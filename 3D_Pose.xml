<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 19 Sep 2023 06:00:18 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling</title><link>http://arxiv.org/abs/2306.17201v1</link><description>Estimating 3D human poses only from a 2D human pose sequence is thoroughlyexplored in recent years. Yet, prior to this, no such work has attempted tounify 2D and 3D pose representations in the shared feature space. In thispaper, we propose MPM, a unified 2D-3D human pose representation framework viamasked pose modeling. We treat 2D and 3D poses as two different modalities likevision and language and build a single-stream transformer-based architecture.We apply three pretext tasks, which are masked 2D pose modeling, masked 3D posemodeling, and masked 2D pose lifting to pre-train our network and usefull-supervision to perform further fine-tuning. A high masking ratio of 72.5%in total with a spatio-temporal mask sampling strategy leading to betterrelation modeling both in spatial and temporal domains. MPM can handle multipletasks including 3D human pose estimation, 3D pose estimation from occluded 2Dpose, and 3D pose completion in a single framework. We conduct extensiveexperiments and ablation studies on several widely used human pose datasets andachieve state-of-the-art performance on Human3.6M and MPI-INF-3DHP. Codes andmodel checkpoints are available at https://github.com/vvirgooo2/MPM</description><author>Zhenyu Zhang, Wenhao Chai, Zhongyu Jiang, Tian Ye, Mingli Song, Jenq-Neng Hwang, Gaoang Wang</author><pubDate>Thu, 29 Jun 2023 11:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17201v1</guid></item><item><title>Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation</title><link>http://arxiv.org/abs/2303.11579v2</link><description>In this paper, a novel Diffusion-based 3D Pose estimation (D3DP) method withJoint-wise reProjection-based Multi-hypothesis Aggregation (JPMA) is proposedfor probabilistic 3D human pose estimation. On the one hand, D3DP generatesmultiple possible 3D pose hypotheses for a single 2D observation. It graduallydiffuses the ground truth 3D poses to a random distribution, and learns adenoiser conditioned on 2D keypoints to recover the uncontaminated 3D poses.The proposed D3DP is compatible with existing 3D pose estimators and supportsusers to balance efficiency and accuracy during inference through twocustomizable parameters. On the other hand, JPMA is proposed to assemblemultiple hypotheses generated by D3DP into a single 3D pose for practical use.It reprojects 3D pose hypotheses to the 2D camera plane, selects the besthypothesis joint-by-joint based on the reprojection errors, and combines theselected joints into the final pose. The proposed JPMA conducts aggregation atthe joint level and makes use of the 2D prior information, both of which havebeen overlooked by previous approaches. Extensive experiments on Human3.6M andMPI-INF-3DHP datasets show that our method outperforms the state-of-the-artdeterministic and probabilistic approaches by 1.5% and 8.9%, respectively. Codeis available at https://github.com/paTRICK-swk/D3DP.</description><author>Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, Wen Gao</author><pubDate>Wed, 23 Aug 2023 04:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11579v2</guid></item><item><title>PoseFix: Correcting 3D Human Poses with Natural Language</title><link>http://arxiv.org/abs/2309.08480v1</link><description>Automatically producing instructions to modify one's posture could open thedoor to endless applications, such as personalized coaching and in-homephysical therapy. Tackling the reverse problem (i.e., refining a 3D pose basedon some natural language feedback) could help for assisted 3D characteranimation or robot teaching, for instance. Although a few recent works explorethe connections between natural language and 3D human pose, none focus ondescribing 3D body pose differences. In this paper, we tackle the problem ofcorrecting 3D human poses with natural language. To this end, we introduce thePoseFix dataset, which consists of several thousand paired 3D poses and theircorresponding text feedback, that describe how the source pose needs to bemodified to obtain the target pose. We demonstrate the potential of thisdataset on two tasks: (1) text-based pose editing, that aims at generatingcorrected 3D body poses given a query pose and a text modifier; and (2)correctional text generation, where instructions are generated based on thedifferences between two body poses.</description><author>Ginger Delmas, Philippe Weinzaepfel, Francesc Moreno-Noguer, Grégory Rogez</author><pubDate>Fri, 15 Sep 2023 16:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08480v1</guid></item><item><title>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</title><link>http://arxiv.org/abs/2308.11737v1</link><description>Accurately estimating the 3D pose and shape is an essential step towardsunderstanding animal behavior, and can potentially benefit many downstreamapplications, such as wildlife conservation. However, research in this area isheld back by the lack of a comprehensive and diverse dataset with high-quality3D pose and shape annotations. In this paper, we propose Animal3D, the firstcomprehensive dataset for mammal animal 3D pose and shape estimation. Animal3Dconsists of 3379 images collected from 40 mammal species, high-qualityannotations of 26 keypoints, and importantly the pose and shape parameters ofthe SMAL model. All annotations were labeled and checked manually in amulti-stage process to ensure highest quality results. Based on the Animal3Ddataset, we benchmark representative shape and pose estimation models at: (1)supervised learning from only the Animal3D data, (2) synthetic to real transferfrom synthetically generated images, and (3) fine-tuning human pose and shapeestimation models. Our experimental results demonstrate that predicting the 3Dshape and pose of animals across species remains a very challenging task,despite significant advances in human pose estimation. Our results furtherdemonstrate that synthetic pre-training is a viable strategy to boost the modelperformance. Overall, Animal3D opens new directions for facilitating futureresearch in animal 3D pose and shape estimation, and is publicly available.</description><author>Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, Adam Kortylewski</author><pubDate>Tue, 22 Aug 2023 19:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11737v1</guid></item><item><title>DiffuPose: Monocular 3D Human Pose Estimation via Denoising Diffusion Probabilistic Model</title><link>http://arxiv.org/abs/2212.02796v3</link><description>Thanks to the development of 2D keypoint detectors, monocular 3D human poseestimation (HPE) via 2D-to-3D uplifting approaches have achieved remarkableimprovements. Still, monocular 3D HPE is a challenging problem due to theinherent depth ambiguities and occlusions. To handle this problem, manyprevious works exploit temporal information to mitigate such difficulties.However, there are many real-world applications where frame sequences are notaccessible. This paper focuses on reconstructing a 3D pose from a single 2Dkeypoint detection. Rather than exploiting temporal information, we alleviatethe depth ambiguity by generating multiple 3D pose candidates which can bemapped to an identical 2D keypoint. We build a novel diffusion-based frameworkto effectively sample diverse 3D poses from an off-the-shelf 2D detector. Byconsidering the correlation between human joints by replacing the conventionaldenoising U-Net with graph convolutional network, our approach accomplishesfurther performance improvements. We evaluate our method on the widely adoptedHuman3.6M and HumanEva-I datasets. Comprehensive experiments are conducted toprove the efficacy of the proposed method, and they confirm that our modeloutperforms state-of-the-art multi-hypothesis 3D HPE methods.</description><author>Jeongjun Choi, Dongseok Shim, H. Jin Kim</author><pubDate>Thu, 03 Aug 2023 10:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02796v3</guid></item><item><title>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2308.09678v1</link><description>The current 3D human pose estimators face challenges in adapting to newdatasets due to the scarcity of 2D-3D pose pairs in target domain trainingsets. We present the \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis\textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to overcomethis issue without extensive target domain annotation. Utilizing adiffusion-centric structure, PoSynDA simulates the 3D pose distribution in thetarget domain, filling the data diversity gap. By incorporating amulti-hypothesis network, it creates diverse pose hypotheses and aligns themwith the target domain. Target-specific source augmentation obtains the targetdomain distribution data from the source domain by decoupling the scale andposition parameters. The teacher-student paradigm and low-rank adaptationfurther refine the process. PoSynDA demonstrates competitive performance onbenchmarks, such as Human3.6M, MPI-INF-3DHP, and 3DPW, even comparable with thetarget-trained MixSTE model~\cite{zhang2022mixste}. This work paves the way forthe practical application of 3D human pose estimation. The code is available athttps://github.com/hbing-l/PoSynDA.</description><author>Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Fri, 18 Aug 2023 17:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09678v1</guid></item><item><title>POCO: 3D Pose and Shape Estimation with Confidence</title><link>http://arxiv.org/abs/2308.12965v1</link><description>The regression of 3D Human Pose and Shape (HPS) from an image is becomingincreasingly accurate. This makes the results useful for downstream tasks likehuman action recognition or 3D graphics. Yet, no regressor is perfect, andaccuracy can be affected by ambiguous image evidence or by poses and appearancethat are unseen during training. Most current HPS regressors, however, do notreport the confidence of their outputs, meaning that downstream tasks cannotdifferentiate accurate estimates from inaccurate ones. To address this, wedevelop POCO, a novel framework for training HPS regressors to estimate notonly a 3D human body, but also their confidence, in a single feed-forward pass.Specifically, POCO estimates both the 3D body pose and a per-sample variance.The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressinguncertainty that is highly correlated to pose reconstruction quality. The POCOframework can be applied to any HPS regressor and here we evaluate it bymodifying HMR, PARE, and CLIFF. In all cases, training the network to reasonabout uncertainty helps it learn to more accurately estimate 3D pose. Whilethis was not our goal, the improvement is modest but consistent. Our mainmotivation is to provide uncertainty estimates for downstream tasks; wedemonstrate this in two ways: (1) We use the confidence estimates to bootstrapHPS training. Given unlabelled image data, we take the confident estimates of aPOCO-trained regressor as pseudo ground truth. Retraining with thisautomatically-curated data improves accuracy. (2) We exploit uncertainty invideo pose estimation by automatically identifying uncertain frames (e.g. dueto occlusion) and inpainting these from confident frames. Code and models willbe available for research at https://poco.is.tue.mpg.de.</description><author>Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas</author><pubDate>Thu, 24 Aug 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12965v1</guid></item><item><title>Improving 3D Pose Estimation for Sign Language</title><link>http://arxiv.org/abs/2308.09525v1</link><description>This work addresses 3D human pose reconstruction in single images. We presenta method that combines Forward Kinematics (FK) with neural networks to ensure afast and valid prediction of 3D pose. Pose is represented as a hierarchicaltree/graph with nodes corresponding to human joints that model their physicallimits. Given a 2D detection of keypoints in the image, we lift the skeleton to3D using neural networks to predict both the joint rotations and bone lengths.These predictions are then combined with skeletal constraints using an FK layerimplemented as a network layer in PyTorch. The result is a fast and accurateapproach to the estimation of 3D skeletal pose. Through quantitative andqualitative evaluation, we demonstrate the method is significantly moreaccurate than MediaPipe in terms of both per joint positional error and visualappearance. Furthermore, we demonstrate generalization over different datasets.The implementation in PyTorch runs at between 100-200 milliseconds per image(including CNN detection) using CPU only.</description><author>Maksym Ivashechkin, Oscar Mendez, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 14:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09525v1</guid></item><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</title><link>http://arxiv.org/abs/2308.10305v1</link><description>Despite significant progress in single image-based 3D human mesh recovery,accurately and smoothly recovering 3D human motion from a video remainschallenging. Existing video-based methods generally recover human mesh byestimating the complex pose and shape parameters from coupled image features,whose high complexity and low representation ability often result ininconsistent pose motion and limited shape patterns. To alleviate this issue,we introduce 3D pose as the intermediary and propose a Pose and MeshCo-Evolution network (PMCE) that decouples this task into two parts: 1)video-based 3D human pose estimation and 2) mesh vertices regression from theestimated 3D pose and temporal image feature. Specifically, we propose atwo-stream encoder that estimates mid-frame 3D pose and extracts a temporalimage feature from the input image sequence. In addition, we design aco-evolution decoder that performs pose and mesh interactions with theimage-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit thehuman body shape. Extensive experiments demonstrate that the proposed PMCEoutperforms previous state-of-the-art methods in terms of both per-frameaccuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M,and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.</description><author>Yingxuan You, Hong Liu, Ti Wang, Wenhao Li, Runwei Ding, Xia Li</author><pubDate>Sun, 20 Aug 2023 17:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10305v1</guid></item><item><title>Dual-Side Feature Fusion 3D Pose Transfer</title><link>http://arxiv.org/abs/2305.14951v1</link><description>3D pose transfer solves the problem of additional input and correspondence oftraditional deformation transfer, only the source and target meshes need to beinput, and the pose of the source mesh can be transferred to the target mesh.Some lightweight methods proposed in recent years consume less memory but causespikes and distortions for some unseen poses, while others are costly intraining due to the inclusion of large matrix multiplication and adversarialnetworks. In addition, the meshes with different numbers of vertices alsoincrease the difficulty of pose transfer. In this work, we propose a Dual-SideFeature Fusion Pose Transfer Network to improve the pose transfer accuracy ofthe lightweight method. Our method takes the pose features as one of the sideinputs to the decoding network and fuses them into the target mesh layer bylayer at multiple scales. Our proposed Feature Fusion Adaptive InstanceNormalization has the characteristic of having two side input channels thatfuse pose features and identity features as denormalization parameters, thusenhancing the pose transfer capability of the network. Extensive experimentalresults show that our proposed method has stronger pose transfer capabilitythan state-of-the-art methods while maintaining a lightweight networkstructure, and can converge faster.</description><author>Jue Liu, Feipeng Da</author><pubDate>Wed, 24 May 2023 10:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14951v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video</title><link>http://arxiv.org/abs/2307.05853v2</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 14% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively). GitHub:https://github.com/bruceyo/GLA-GCN.</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Sat, 22 Jul 2023 02:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v2</guid></item><item><title>Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2303.16456v2</link><description>When applying a pre-trained 2D-to-3D human pose lifting model to a targetunseen dataset, large performance degradation is commonly encountered due todomain shift issues. We observe that the degradation is caused by two factors:1) the large distribution gap over global positions of poses between the sourceand target datasets due to variant camera parameters and settings, and 2) thedeficient diversity of local structures of poses in training. To this end, wecombine \textbf{global adaptation} and \textbf{local generalization} in\textit{PoseDA}, a simple yet effective framework of unsupervised domainadaptation for 3D human pose estimation. Specifically, global adaptation aimsto align global positions of poses from the source domain to the target domainwith a proposed global position alignment (GPA) module. And localgeneralization is designed to enhance the diversity of 2D-3D pose mapping witha local pose augmentation (LPA) module. These modules bring significantperformance improvement without introducing additional learnable parameters. Inaddition, we propose local pose augmentation (LPA) to enhance the diversity of3D poses following an adversarial training scheme consisting of 1) aaugmentation generator that generates the parameters of pre-defined posetransformations and 2) an anchor discriminator to ensure the reality andquality of the augmented data. Our approach can be applicable to almost all2D-3D lifting models. \textit{PoseDA} achieves 61.3 mm of MPJPE on MPI-INF-3DHPunder a cross-dataset evaluation setup, improving upon the previousstate-of-the-art method by 10.2\%.</description><author>Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang, Gaoang Wang</author><pubDate>Thu, 17 Aug 2023 07:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16456v2</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v2</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenxuan Guo, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Sun, 06 Aug 2023 15:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v2</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v1</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Tue, 01 Aug 2023 16:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v1</guid></item><item><title>Weakly-supervised 3D Pose Transfer with Keypoints</title><link>http://arxiv.org/abs/2307.13459v1</link><description>The main challenges of 3D pose transfer are: 1) Lack of paired training datawith different characters performing the same pose; 2) Disentangling pose andshape information from the target mesh; 3) Difficulty in applying to mesheswith different topologies. We thus propose a novel weakly-supervisedkeypoint-based framework to overcome these difficulties. Specifically, we use atopology-agnostic keypoint detector with inverse kinematics to computetransformations between the source and target meshes. Our method only requiressupervision on the keypoints, can be applied to meshes with differenttopologies and is shape-invariant for the target which allows extraction ofpose-only information from the target meshes without transferring shapeinformation. We further design a cycle reconstruction to performself-supervised pose transfer without the need for ground truth deformed meshwith the same pose and shape as the target and source, respectively. Weevaluate our approach on benchmark human and animal datasets, where we achievesuperior performance compared to the state-of-the-art unsupervised approachesand even comparable performance with the fully supervised approaches. We teston the more challenging Mixamo dataset to verify our approach's ability inhandling meshes with different topologies and complex clothes. Cross-datasetevaluation further shows the strong generalization ability of our approach.</description><author>Jinnan Chen, Chen Li, Gim Hee Lee</author><pubDate>Tue, 25 Jul 2023 13:40:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13459v1</guid></item><item><title>Weakly-supervised 3D Pose Transfer with Keypoints</title><link>http://arxiv.org/abs/2307.13459v2</link><description>The main challenges of 3D pose transfer are: 1) Lack of paired training datawith different characters performing the same pose; 2) Disentangling pose andshape information from the target mesh; 3) Difficulty in applying to mesheswith different topologies. We thus propose a novel weakly-supervisedkeypoint-based framework to overcome these difficulties. Specifically, we use atopology-agnostic keypoint detector with inverse kinematics to computetransformations between the source and target meshes. Our method only requiressupervision on the keypoints, can be applied to meshes with differenttopologies and is shape-invariant for the target which allows extraction ofpose-only information from the target meshes without transferring shapeinformation. We further design a cycle reconstruction to performself-supervised pose transfer without the need for ground truth deformed meshwith the same pose and shape as the target and source, respectively. Weevaluate our approach on benchmark human and animal datasets, where we achievesuperior performance compared to the state-of-the-art unsupervised approachesand even comparable performance with the fully supervised approaches. We teston the more challenging Mixamo dataset to verify our approach's ability inhandling meshes with different topologies and complex clothes. Cross-datasetevaluation further shows the strong generalization ability of our approach.</description><author>Jinnan Chen, Chen Li, Gim Hee Lee</author><pubDate>Thu, 17 Aug 2023 07:02:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13459v2</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields</title><link>http://arxiv.org/abs/2212.02493v3</link><description>Coordinate-based implicit neural networks, or neural fields, have emerged asuseful representations of shape and appearance in 3D computer vision. Despiteadvances, however, it remains challenging to build neural fields for categoriesof objects without datasets like ShapeNet that provide "canonicalized" objectinstances that are consistently aligned for their 3D position and orientation(pose). We present Canonical Field Network (CaFi-Net), a self-supervised methodto canonicalize the 3D pose of instances from an object category represented asneural fields, specifically neural radiance fields (NeRFs). CaFi-Net directlylearns from continuous and noisy radiance fields using a Siamese networkarchitecture that is designed to extract equivariant field features forcategory-level canonicalization. During inference, our method takes pre-trainedneural radiance fields of novel object instances at arbitrary 3D pose andestimates a canonical field with consistent 3D pose across the entire category.Extensive experiments on a new dataset of 1300 NeRF models across 13 objectcategories show that our method matches or exceeds the performance of 3D pointcloud-based methods.</description><author>Rohith Agaram, Shaurya Dewan, Rahul Sajnani, Adrien Poulenard, Madhava Krishna, Srinath Sridhar</author><pubDate>Wed, 17 May 2023 12:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02493v3</guid></item><item><title>Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet</title><link>http://arxiv.org/abs/2207.04320v3</link><description>Multi-person pose understanding from RGB videos involves three complex tasks:pose estimation, tracking and motion forecasting. Intuitively, accuratemulti-person pose estimation facilitates robust tracking, and robust trackingbuilds crucial history for correct motion forecasting. Most existing workseither focus on a single task or employ multi-stage approaches to solvingmultiple tasks separately, which tends to make sub-optimal decision at eachstage and also fail to exploit correlations among the three tasks. In thispaper, we propose Snipper, a unified framework to perform multi-person 3D poseestimation, tracking, and motion forecasting simultaneously in a single stage.We propose an efficient yet powerful deformable attention mechanism toaggregate spatiotemporal information from the video snippet. Building upon thisdeformable attention, a video transformer is learned to encode thespatiotemporal features from the multi-frame snippet and to decode informativepose features for multi-person pose queries. Finally, these pose queries areregressed to predict multi-person pose trajectories and future motions in asingle shot. In the experiments, we show the effectiveness of Snipper on threechallenging public datasets where our generic model rivals specializedstate-of-art baselines for pose estimation, tracking, and forecasting.</description><author>Shihao Zou, Yuanlu Xu, Chao Li, Lingni Ma, Li Cheng, Minh Vo</author><pubDate>Tue, 12 Sep 2023 22:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.04320v3</guid></item><item><title>Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving</title><link>http://arxiv.org/abs/2307.14889v1</link><description>Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomousvehicles (AVs) to make informed decisions and respond proactively in criticalroad scenarios. Promising results of 3D HPE have been gained in several domainssuch as human-computer interaction, robotics, sports and medical analytics,often based on data collected in well-controlled laboratory environments.Nevertheless, the transfer of 3D HPE methods to AVs has received limitedresearch attention, due to the challenges posed by obtaining accurate 3D poseannotations and the limited suitability of data from other domains. We present a simple yet efficient weakly supervised approach for 3D HPE inthe AV context by employing a high-level sensor fusion between camera and LiDARdata. The weakly supervised setting enables training on the target datasetswithout any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractorand pseudo labels generated from LiDAR to image projections. Our approachoutperforms state-of-the-art results by up to $\sim$ 13% on the Waymo OpenDataset in the weakly supervised setting and achieves state-of-the-art resultsin the supervised setting.</description><author>Peter Bauer, Arij Bouazizi, Ulrich Kressel, Fabian B. Flohr</author><pubDate>Thu, 27 Jul 2023 15:28:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14889v1</guid></item><item><title>Human Part-wise 3D Motion Context Learning for Sign Language Recognition</title><link>http://arxiv.org/abs/2308.09305v1</link><description>In this paper, we propose P3D, the human part-wise motion context learningframework for sign language recognition. Our main contributions lie in twodimensions: learning the part-wise motion context and employing the poseensemble to utilize 2D and 3D pose jointly. First, our empirical observationimplies that part-wise context encoding benefits the performance of signlanguage recognition. While previous methods of sign language recognitionlearned motion context from the sequence of the entire pose, we argue that suchmethods cannot exploit part-specific motion context. In order to utilizepart-wise motion context, we propose the alternating combination of a part-wiseencoding Transformer (PET) and a whole-body encoding Transformer (WET). PETencodes the motion contexts from a part sequence, while WET merges them into aunified context. By learning part-wise motion context, our P3D achievessuperior performance on WLASL compared to previous state-of-the-art methods.Second, our framework is the first to ensemble 2D and 3D poses for signlanguage recognition. Since the 3D pose holds rich motion context and depthinformation to distinguish the words, our P3D outperformed the previousstate-of-the-art methods employing a pose ensemble.</description><author>Taeryung Lee, Yeonguk Oh, Kyoung Mu Lee</author><pubDate>Fri, 18 Aug 2023 06:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09305v1</guid></item><item><title>PoseGraphNet++: Enriching 3D Human Pose with Orientation Estimation</title><link>http://arxiv.org/abs/2308.11440v1</link><description>Existing kinematic skeleton-based 3D human pose estimation methods onlypredict joint positions. Although this is sufficient to compute the yaw andpitch of the bone rotations, the roll around the axis of the bones remainsunresolved by these methods. In this paper, we propose a novel 2D-to-3D liftingGraph Convolution Network named PoseGraphNet++ to predict the complete humanpose including the joint positions and the bone orientations. We employ nodeand edge convolutions to utilize the joint and bone features. Our model isevaluated on multiple benchmark datasets, and its performance is either on parwith or better than the state-of-the-art in terms of both position and rotationmetrics. Through extensive ablation studies, we show that PoseGraphNet++benefits from exploiting the mutual relationship between the joints and thebones.</description><author>Soubarna Banik, Edvard Avagyan, Alejandro Mendoza Gracia, Alois Knoll</author><pubDate>Tue, 22 Aug 2023 14:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11440v1</guid></item><item><title>EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors</title><link>http://arxiv.org/abs/2306.09615v1</link><description>Transformer is popular in recent 3D human pose estimation, which utilizeslong-term modeling to lift 2D keypoints into the 3D space. However, currenttransformer-based methods do not fully exploit the prior knowledge of the humanskeleton provided by the kinematic structure. In this paper, we propose a noveltransformer-based model EvoPose to introduce the human body prior knowledge for3D human pose estimation effectively. Specifically, a Structural PriorsRepresentation (SPR) module represents human priors as structural featurescarrying rich body patterns, e.g. joint relationships. The structural featuresare interacted with 2D pose sequences and help the model to achieve moreinformative spatiotemporal features. Moreover, a Recursive Refinement (RR)module is applied to refine the 3D pose outputs by utilizing estimated resultsand further injects human priors simultaneously. Extensive experimentsdemonstrate the effectiveness of EvoPose which achieves a new state of the arton two most popular benchmarks, Human3.6M and MPI-INF-3DHP.</description><author>Yaqi Zhang, Yan Lu, Bin Liu, Zhiwei Zhao, Qi Chu, Nenghai Yu</author><pubDate>Fri, 16 Jun 2023 05:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09615v1</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v1</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Sat, 29 Jul 2023 21:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v1</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v2</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Mon, 07 Aug 2023 23:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v2</guid></item><item><title>FreeMan: Towards Benchmarking 3D Human Pose Estimation in the Wild</title><link>http://arxiv.org/abs/2309.05073v2</link><description>Estimating the 3D structure of the human body from natural scenes is afundamental aspect of visual perception. This task carries great importance forfields like AIGC and human-robot interaction. In practice, 3D human poseestimation in real-world settings is a critical initial step in solving thisproblem. However, the current datasets, often collected under controlledlaboratory conditions using complex motion capture equipment and unvaryingbackgrounds, are insufficient. The absence of real-world datasets is stallingthe progress of this crucial task. To facilitate the development of 3D poseestimation, we present FreeMan, the first large-scale, real-world multi-viewdataset. FreeMan was captured by synchronizing 8 smartphones across diversescenarios. It comprises 11M frames from 8000 sequences, viewed from differentperspectives. These sequences cover 40 subjects across 10 different scenarios,each with varying lighting conditions. We have also established an automated,precise labeling pipeline that allows for large-scale processing efficiently.We provide comprehensive evaluation baselines for a range of tasks, underliningthe significant challenges posed by FreeMan. Further evaluations of standardindoor/outdoor human sensing datasets reveal that FreeMan offers robustrepresentation transferability in real and complex scenes. FreeMan is nowpublicly available at https://wangjiongw.github.io/freeman.</description><author>Jiong Wang, Fengyu Yang, Wenbo Gou, Bingliang Li, Danqi Yan, Ailing Zeng, Yijun Gao, Junle Wang, Ruimao Zhang</author><pubDate>Tue, 12 Sep 2023 16:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05073v2</guid></item><item><title>3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking</title><link>http://arxiv.org/abs/2308.15316v1</link><description>Markerless methods for animal posture tracking have been developing recently,but frameworks and benchmarks for tracking large animal groups in 3D are stilllacking. To overcome this gap in the literature, we present 3D-MuPPET, aframework to estimate and track 3D poses of up to 10 pigeons at interactivespeed using multiple-views. We train a pose estimator to infer 2D keypoints andbounding boxes of multiple pigeons, then triangulate the keypoints to 3D. Forcorrespondence matching, we first dynamically match 2D detections to globalidentities in the first frame, then use a 2D tracker to maintaincorrespondences accross views in subsequent frames. We achieve comparableaccuracy to a state of the art 3D pose estimator for Root Mean Square Error(RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel usecase where our model trained with data of single pigeons provides comparableresults on data containing multiple pigeons. This can simplify the domain shiftto new species because annotating single animal data is less labour intensivethan multi-animal data. Additionally, we benchmark the inference speed of3D-MuPPET, with up to 10 fps in 2D and 1.5 fps in 3D, and perform quantitativetracking evaluation, which yields encouraging results. Finally, we show that3D-MuPPET also works in natural environments without model fine-tuning onadditional annotations. To the best of our knowledge we are the first topresent a framework for 2D/3D posture and trajectory tracking that works inboth indoor and outdoor environments.</description><author>Urs Waldmann, Alex Hoi Hang Chan, Hemal Naik, Máté Nagy, Iain D. Couzin, Oliver Deussen, Bastian Goldluecke, Fumihiro Kano</author><pubDate>Tue, 29 Aug 2023 15:02:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15316v1</guid></item><item><title>HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.14581v2</link><description>2D-to-3D human pose lifting is fundamental for 3D human pose estimation(HPE). Graph Convolutional Network (GCN) has been proven inherently suitable tomodel the human skeletal topology. However, current GCN-based 3D HPE methodsupdate the node features by aggregating their neighbors' information withoutconsidering the interaction of joints in different motion patterns. Althoughsome studies import limb information to learn the movement patterns, the latentsynergies among joints, such as maintaining balance in the motion are seldominvestigated. We propose a hop-wise GraphFormer with intragroup jointrefinement (HopFIR) to tackle the 3D HPE problem. The HopFIR mainly consists ofa novel Hop-wise GraphFormer(HGF) module and an Intragroup JointRefinement(IJR) module which leverages the prior limb information forperipheral joints refinement. The HGF module groups the joints by $k$-hopneighbors and utilizes a hop-wise transformer-like attention mechanism amongthese groups to discover latent joint synergy. Extensive experimental resultsshow that HopFIR outperforms the SOTA methods with a large margin (on theHuman3.6M dataset, the mean per joint position error (MPJPE) is 32.67mm).Furthermore, it is also demonstrated that previous SOTA GCN-based methods canbenefit from the proposed hop-wise attention mechanism efficiently withsignificant performance promotion, such as SemGCN and MGCN are improved by 8.9%and 4.5%, respectively.</description><author>Kai Zhai, Qiang Nie, Bo Ouyang, Xiang Li, ShanLin Yang</author><pubDate>Tue, 18 Jul 2023 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14581v2</guid></item><item><title>Denoising Diffusion for 3D Hand Pose Estimation from Images</title><link>http://arxiv.org/abs/2308.09523v1</link><description>Hand pose estimation from a single image has many applications. However,approaches to full 3D body pose estimation are typically trained on day-to-dayactivities or actions. As such, detailed hand-to-hand interactions are poorlyrepresented, especially during motion. We see this in the failure cases oftechniques such as OpenPose or MediaPipe. However, accurate hand poseestimation is crucial for many applications where the global body motion isless important than accurate hand pose estimation. This paper addresses the problem of 3D hand pose estimation from monocularimages or sequences. We present a novel end-to-end framework for 3D handregression that employs diffusion models that have shown excellent ability tocapture the distribution of data for generative purposes. Moreover, we enforcekinematic constraints to ensure realistic poses are generated by incorporatingan explicit forward kinematic layer as part of the network. The proposed modelprovides state-of-the-art performance when lifting a 2D single-hand image to3D. However, when sequence data is available, we add a Transformer module overa temporal window of consecutive frames to refine the results, overcomingjittering and further increasing accuracy. The method is quantitatively and qualitatively evaluated showingstate-of-the-art robustness, generalization, and accuracy on several differentdatasets.</description><author>Maksym Ivashechkin, Oscar Mendez, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 13:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09523v1</guid></item><item><title>Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.01365v2</link><description>Accurately estimating the 3D pose of humans in video sequences requires bothaccuracy and a well-structured architecture. With the success of transformers,we introduce the Refined Temporal Pyramidal Compression-and-Amplification(RTPCA) transformer. Exploiting the temporal dimension, RTPCA extendsintra-block temporal modeling via its Temporal PyramidalCompression-and-Amplification (TPCA) structure and refines inter-block featureinteraction with a Cross-Layer Refinement (XLR) module. In particular, TPCAblock exploits a temporal pyramid paradigm, reinforcing key and valuerepresentation capabilities and seamlessly extracting spatial semantics frommotion sequences. We stitch these TPCA blocks with XLR that promotes richsemantic representation through continuous interaction of queries, keys, andvalues. This strategy embodies early-stage information with current flows,addressing typical deficits in detail and stability seen in othertransformer-based methods. We demonstrate the effectiveness of RTPCA byachieving state-of-the-art results on Human3.6M, HumanEva-I, and MPI-INF-3DHPbenchmarks with minimal computational overhead. The source code is available athttps://github.com/hbing-l/RTPCA.</description><author>Hanbing Liu, Wangmeng Xiang, Jun-Yan He, Zhi-Qi Cheng, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Wed, 06 Sep 2023 03:18:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01365v2</guid></item><item><title>Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.01365v1</link><description>Accurately estimating the 3D pose of humans in video sequences requires bothaccuracy and a well-structured architecture. With the success of transformers,we introduce the Refined Temporal Pyramidal Compression-and-Amplification(RTPCA) transformer. Exploiting the temporal dimension, RTPCA extendsintra-block temporal modeling via its Temporal PyramidalCompression-and-Amplification (TPCA) structure and refines inter-block featureinteraction with a Cross-Layer Refinement (XLR) module. In particular, TPCAblock exploits a temporal pyramid paradigm, reinforcing key and valuerepresentation capabilities and seamlessly extracting spatial semantics frommotion sequences. We stitch these TPCA blocks with XLR that promotes richsemantic representation through continuous interaction of queries, keys, andvalues. This strategy embodies early-stage information with current flows,addressing typical deficits in detail and stability seen in othertransformer-based methods. We demonstrate the effectiveness of RTPCA byachieving state-of-the-art results on Human3.6M, HumanEva-I, and MPI-INF-3DHPbenchmarks with minimal computational overhead. The source code is available athttps://github.com/hbing-l/RTPCA.</description><author>Hanbing Li, Wangmeng Xiang, Jun-Yan He, Zhi-Qi Cheng, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 04 Sep 2023 06:25:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01365v1</guid></item><item><title>Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.03833v2</link><description>Learning-based methods have dominated the 3D human pose estimation (HPE)tasks with significantly better performance in most benchmarks than traditionaloptimization-based methods. Nonetheless, 3D HPE in the wild is still thebiggest challenge of learning-based models, whether with 2D-3D lifting,image-to-3D, or diffusion-based methods, since the trained networks implicitlylearn camera intrinsic parameters and domain-based 3D human pose distributionsand estimate poses by statistical average. On the other hand, theoptimization-based methods estimate results case-by-case, which can predictmore diverse and sophisticated human poses in the wild. By combining theadvantages of optimization-based and learning-based methods, we propose theZero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve theproblem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDOachieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mmwithout training with any 2D-3D or image-3D pairs. Moreover, oursingle-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE$42.6$mm on cross-dataset evaluation, which even outperforms learning-basedmethods trained on 3DPW.</description><author>Zhongyu Jiang, Zhuoran Zhou, Lei Li, Wenhao Chai, Cheng-Yen Yang, Jenq-Neng Hwang</author><pubDate>Wed, 23 Aug 2023 18:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03833v2</guid></item><item><title>Double-chain Constraints for 3D Human Pose Estimation in Images and Videos</title><link>http://arxiv.org/abs/2308.05298v1</link><description>Reconstructing 3D poses from 2D poses lacking depth information isparticularly challenging due to the complexity and diversity of human motion.The key is to effectively model the spatial constraints between joints toleverage their inherent dependencies. Thus, we propose a novel model, calledDouble-chain Graph Convolutional Transformer (DC-GCT), to constrain the posethrough a double-chain design consisting of local-to-global and global-to-localchains to obtain a complex representation more suitable for the current humanpose. Specifically, we combine the advantages of GCN and Transformer and designa Local Constraint Module (LCM) based on GCN and a Global Constraint Module(GCM) based on self-attention mechanism as well as a Feature Interaction Module(FIM). The proposed method fully captures the multi-level dependencies betweenhuman body joints to optimize the modeling capability of the model. Moreover,we propose a method to use temporal information into the single-frame model byguiding the video sequence embedding through the joint embedding of the targetframe, with negligible increase in computational cost. Experimental resultsdemonstrate that DC-GCT achieves state-of-the-art performance on twochallenging datasets (Human3.6M and MPI-INF-3DHP). Notably, our model achievesstate-of-the-art performance on all action categories in the Human3.6M datasetusing detected 2D poses from CPN, and our code is available at:https://github.com/KHB1698/DC-GCT.</description><author>Hongbo Kang, Yong Wang, Mengyuan Liu, Doudou Wu, Peng Liu, Wenming Yang</author><pubDate>Thu, 10 Aug 2023 03:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05298v1</guid></item><item><title>HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation</title><link>http://arxiv.org/abs/2305.06968v1</link><description>Monocular 3D human pose and shape estimation is an ill-posed problem sincemultiple 3D solutions can explain a 2D image of a subject. Recent approachespredict a probability distribution over plausible 3D pose and shape parametersconditioned on the image. We show that these approaches exhibit a trade-offbetween three key properties: (i) accuracy - the likelihood of the ground-truth3D solution under the predicted distribution, (ii) sample-input consistency -the extent to which 3D samples from the predicted distribution match thevisible 2D image evidence, and (iii) sample diversity - the range of plausible3D solutions modelled by the predicted distribution. Our method, HuManiFlow,predicts simultaneously accurate, consistent and diverse distributions. We usethe human kinematic tree to factorise full body pose into ancestor-conditionedper-body-part pose distributions in an autoregressive manner. Per-body-partdistributions are implemented using normalising flows that respect the manifoldstructure of SO(3), the Lie group of per-body-part poses. We show thatill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,and employ only probabilistic training losses. Code is available at:https://github.com/akashsengupta1997/HuManiFlow.</description><author>Akash Sengupta, Ignas Budvytis, Roberto Cipolla</author><pubDate>Thu, 11 May 2023 17:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06968v1</guid></item><item><title>NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation</title><link>http://arxiv.org/abs/2305.08590v1</link><description>With the progress of 3D human pose and shape estimation, state-of-the-artmethods can either be robust to occlusions or obtain pixel-aligned accuracy innon-occlusion cases. However, they cannot obtain robustness and mesh-imagealignment at the same time. In this work, we present NIKI (Neural InverseKinematics with Invertible Neural Network), which models bi-directional errorsto improve the robustness to occlusions and obtain pixel-aligned accuracy. NIKIcan learn from both the forward and inverse processes with invertible networks.In the inverse process, the model separates the error from the plausible 3Dpose manifold for a robust 3D human pose estimation. In the forward process, weenforce the zero-error boundary conditions to improve the sensitivity toreliable joint positions for better mesh-image alignment. Furthermore, NIKIemulates the analytical inverse kinematics algorithms with the twist-and-swingdecomposition for better interpretability. Experiments on standard andocclusion-specific benchmarks demonstrate the effectiveness of NIKI, where weexhibit robust and well-aligned results simultaneously. Code is available athttps://github.com/Jeff-sjtu/NIKI</description><author>Jiefeng Li, Siyuan Bian, Qi Liu, Jiasheng Tang, Fan Wang, Cewu Lu</author><pubDate>Mon, 15 May 2023 13:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08590v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human</title><link>http://arxiv.org/abs/2307.05853v1</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Wed, 12 Jul 2023 01:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v1</guid></item><item><title>Spatio-temporal MLP-graph network for 3D human pose estimation</title><link>http://arxiv.org/abs/2308.15313v1</link><description>Graph convolutional networks and their variants have shown significantpromise in 3D human pose estimation. Despite their success, most of thesemethods only consider spatial correlations between body joints and do not takeinto account temporal correlations, thereby limiting their ability to capturerelationships in the presence of occlusions and inherent ambiguity. To addressthis potential weakness, we propose a spatio-temporal network architecturecomposed of a joint-mixing multi-layer perceptron block that facilitatescommunication among different joints and a graph weighted Jacobi network blockthat enables communication among various feature channels. The major novelty ofour approach lies in a new weighted Jacobi feature propagation rule obtainedthrough graph filtering with implicit fairing. We leverage temporal informationfrom the 2D pose sequences, and integrate weight modulation into the model toenable untangling of the feature transformations of distinct nodes. We alsoemploy adjacency modulation with the aim of learning meaningful correlationsbeyond defined linkages between body joints by altering the graph topologythrough a learnable modulation matrix. Extensive experiments on two benchmarkdatasets demonstrate the effectiveness of our model, outperforming recentstate-of-the-art methods for 3D human pose estimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 29 Aug 2023 15:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15313v1</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v4</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Wed, 06 Sep 2023 22:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v4</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v3</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Thu, 11 May 2023 00:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v3</guid></item><item><title>H3WB: Human3.6M 3D WholeBody Dataset and Benchmark</title><link>http://arxiv.org/abs/2211.15692v2</link><description>We present a benchmark for 3D human whole-body pose estimation, whichinvolves identifying accurate 3D keypoints on the entire human body, includingface, hands, body, and feet. Currently, the lack of a fully annotated andaccurate 3D whole-body dataset results in deep networks being trainedseparately on specific body parts, which are combined during inference. Or theyrely on pseudo-groundtruth provided by parametric body models which are not asaccurate as detection based methods. To overcome these issues, we introduce theHuman3.6M 3D WholeBody (H3WB) dataset, which provides whole-body annotationsfor the Human3.6M dataset using the COCO Wholebody layout. H3WB comprises 133whole-body keypoint annotations on 100K images, made possible by our newmulti-view pipeline. We also propose three tasks: i) 3D whole-body pose liftingfrom 2D complete whole-body pose, ii) 3D whole-body pose lifting from 2Dincomplete whole-body pose, and iii) 3D whole-body pose estimation from asingle RGB image. Additionally, we report several baselines from popularmethods for these tasks. Furthermore, we also provide automated 3D whole-bodyannotations of TotalCapture and experimentally show that when used with H3WB ithelps to improve the performance. Code and dataset is available athttps://github.com/wholebody3d/wholebody3d</description><author>Yue Zhu, Nermin Samet, David Picard</author><pubDate>Wed, 06 Sep 2023 13:22:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15692v2</guid></item><item><title>LInKs "Lifting Independent Keypoints" -- Partial Pose Lifting for Occlusion Handling with Improved Accuracy in 2D-3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.07243v1</link><description>We present LInKs, a novel unsupervised learning method to recover 3D humanposes from 2D kinematic skeletons obtained from a single image, even whenocclusions are present. Our approach follows a unique two-step process, whichinvolves first lifting the occluded 2D pose to the 3D domain, followed byfilling in the occluded parts using the partially reconstructed 3D coordinates.This lift-then-fill approach leads to significantly more accurate resultscompared to models that complete the pose in 2D space alone. Additionally, weimprove the stability and likelihood estimation of normalising flows through acustom sampling function replacing PCA dimensionality reduction previously usedin prior work. Furthermore, we are the first to investigate if different partsof the 2D kinematic skeleton can be lifted independently which we find byitself reduces the error of current lifting approaches. We attribute this tothe reduction of long-range keypoint correlations. In our detailed evaluation,we quantify the error under various realistic occlusion scenarios, showcasingthe versatility and applicability of our model. Our results consistentlydemonstrate the superiority of handling all types of occlusions in 3D spacewhen compared to others that complete the pose in 2D space. Our approach alsoexhibits consistent accuracy in scenarios without occlusion, as evidenced by a7.9% reduction in reconstruction error compared to prior works on the Human3.6Mdataset. Furthermore, our method excels in accurately retrieving complete 3Dposes even in the presence of occlusions, making it highly applicable insituations where complete 2D pose information is unavailable.</description><author>Peter Hardy, Hansung Kim</author><pubDate>Wed, 13 Sep 2023 19:28:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07243v1</guid></item><item><title>Regular Splitting Graph Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2305.05785v1</link><description>In human pose estimation methods based on graph convolutional architectures,the human skeleton is usually modeled as an undirected graph whose nodes arebody joints and edges are connections between neighboring joints. However, mostof these methods tend to focus on learning relationships between body joints ofthe skeleton using first-order neighbors, ignoring higher-order neighbors andhence limiting their ability to exploit relationships between distant joints.In this paper, we introduce a higher-order regular splitting graph network(RS-Net) for 2D-to-3D human pose estimation using matrix splitting inconjunction with weight and adjacency modulation. The core idea is to capturelong-range dependencies between body joints using multi-hop neighborhoods andalso to learn different modulation vectors for different body joints as well asa modulation matrix added to the adjacency matrix associated to the skeleton.This learnable modulation matrix helps adjust the graph structure by addingextra graph edges in an effort to learn additional connections between bodyjoints. Instead of using a shared weight matrix for all neighboring bodyjoints, the proposed RS-Net model applies weight unsharing before aggregatingthe feature vectors associated to the joints in order to capture the differentrelations between them. Experiments and ablations studies performed on twobenchmark datasets demonstrate the effectiveness of our model, achievingsuperior performance over recent state-of-the-art methods for 3D human poseestimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 09 May 2023 23:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05785v1</guid></item><item><title>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.01825v2</link><description>Human pose estimation is a challenging task due to its structured datasequence nature. Existing methods primarily focus on pair-wise interaction ofbody joints, which is insufficient for scenarios involving overlapping jointsand rapidly changing poses. To overcome these issues, we introduce a novelapproach, the High-order Directed Transformer (HDFormer), which leverageshigh-order bone and joint relationships for improved pose estimation.Specifically, HDFormer incorporates both self-attention and high-orderattention to formulate a multi-order attention module. This module facilitatesfirst-order "joint$\leftrightarrow$joint", second-order"bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint"interactions, effectively addressing issues in complex and occlusion-heavysituations. In addition, modern CNN techniques are integrated into thetransformer-based architecture, balancing the trade-off between performance andefficiency. HDFormer significantly outperforms state-of-the-art (SOTA) modelson Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parametersand significantly lower computational costs. Moreover, HDFormer demonstratesbroad real-world applicability, enabling real-time, accurate 3D poseestimation. The source code is in https://github.com/hyer/HDFormer</description><author>Hanyuan Chen, Jun-Yan He, Wangmeng Xiang, Zhi-Qi Cheng, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 22 May 2023 07:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01825v2</guid></item><item><title>3DPortraitGAN: Learning One-Quarter Headshot 3D GANs from a Single-View Portrait Dataset with Diverse Body Poses</title><link>http://arxiv.org/abs/2307.14770v2</link><description>3D-aware face generators are typically trained on 2D real-life face imagedatasets that primarily consist of near-frontal face data, and as such, theyare unable to construct one-quarter headshot 3D portraits with complete head,neck, and shoulder geometry. Two reasons account for this issue: First,existing facial recognition methods struggle with extracting facial datacaptured from large camera angles or back views. Second, it is challenging tolearn a distribution of 3D portraits covering the one-quarter headshot regionfrom single-view data due to significant geometric deformation caused bydiverse body poses. To this end, we first create the dataset360{\deg}-Portrait-HQ (360{\deg}PHQ for short) which consists of high-qualitysingle-view real portraits annotated with a variety of camera parameters (theyaw angles span the entire 360{\deg} range) and body poses. We then propose3DPortraitGAN, the first 3D-aware one-quarter headshot portrait generator thatlearns a canonical 3D avatar distribution from the 360{\deg}PHQ dataset withbody pose self-learning. Our model can generate view-consistent portrait imagesfrom all camera angles with a canonical one-quarter headshot 3D representation.Our experiments show that the proposed framework can accurately predictportrait body poses and generate view-consistent, realistic portrait imageswith complete geometry from all camera angles.</description><author>Yiqian Wu, Hao Xu, Xiangjun Tang, Hongbo Fu, Xiaogang Jin</author><pubDate>Mon, 21 Aug 2023 07:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14770v2</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Learning to Predict Scene-Level Implicit 3D from Posed RGBD Data</title><link>http://arxiv.org/abs/2306.08671v1</link><description>We introduce a method that can learn to predict scene-level implicitfunctions for 3D reconstruction from posed RGBD data. At test time, our systemmaps a previously unseen RGB image to a 3D reconstruction of a scene viaimplicit functions. While implicit functions for 3D reconstruction have oftenbeen tied to meshes, we show that we can train one using only a set of posedRGBD images. This setting may help 3D reconstruction unlock the sea ofaccelerometer+RGBD data that is coming with new phones. Our system, D2-DRDF,can match and sometimes outperform current methods that use mesh supervisionand shows better robustness to sparse data.</description><author>Nilesh Kulkarni, Linyi Jin, Justin Johnson, David F. Fouhey</author><pubDate>Wed, 14 Jun 2023 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08671v1</guid></item><item><title>EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild</title><link>http://arxiv.org/abs/2308.16894v1</link><description>We present EMDB, the Electromagnetic Database of Global 3D Human Pose andShape in the Wild. EMDB is a novel dataset that contains high-quality 3D SMPLpose and shape parameters with global body and camera trajectories forin-the-wild videos. We use body-worn, wireless electromagnetic (EM) sensors anda hand-held iPhone to record a total of 58 minutes of motion data, distributedover 81 indoor and outdoor sequences and 10 participants. Together withaccurate body poses and shapes, we also provide global camera poses and bodyroot trajectories. To construct EMDB, we propose a multi-stage optimizationprocedure, which first fits SMPL to the 6-DoF EM measurements and then refinesthe poses via image observations. To achieve high-quality results, we leveragea neural implicit avatar model to reconstruct detailed human surface geometryand appearance, which allows for improved alignment and smoothness via a densepixel-level objective. Our evaluations, conducted with a multi-view volumetriccapture system, indicate that EMDB has an expected accuracy of 2.3 cmpositional and 10.6 degrees angular error, surpassing the accuracy of previousin-the-wild datasets. We evaluate existing state-of-the-art monocular RGBmethods for camera-relative and global pose estimation on EMDB. EMDB ispublicly available under https://ait.ethz.ch/emdb</description><author>Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan Zarate, Otmar Hilliges</author><pubDate>Thu, 31 Aug 2023 18:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16894v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gschoßmann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision</title><link>http://arxiv.org/abs/2303.02862v2</link><description>Event camera shows great potential in 3D hand pose estimation, especiallyaddressing the challenges of fast motion and high dynamic range in a low-powerway. However, due to the asynchronous differential imaging mechanism, it ischallenging to design event representation to encode hand motion informationespecially when the hands are not moving (causing motion ambiguity), and it isinfeasible to fully annotate the temporally dense event stream. In this paper,we propose EvHandPose with novel hand flow representations in Event-to-Posemodule for accurate hand pose estimation and alleviating the motion ambiguityissue. To solve the problem under sparse annotation, we design contrastmaximization and hand-edge constraints in Pose-to-IWE (Image with WarpedEvents) module and formulate EvHandPose in a weakly-supervision framework. Wefurther build EvRealHands, the first large-scale real-world event-based handpose dataset on several challenging scenes to bridge the real-synthetic domaingap. Experiments on EvRealHands demonstrate that EvHandPose outperformsprevious event-based methods under all evaluation scenes, achieves accurate andstable hand pose estimation with high temporal resolution in fast motion andstrong light scenes compared with RGB-based methods, generalizes well tooutdoor scenes and another type of event camera, and shows the potential forthe hand gesture recognition task.</description><author>Jianping Jiang, Jiahe Li, Baowen Zhang, Xiaoming Deng, Boxin Shi</author><pubDate>Wed, 30 Aug 2023 04:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02862v2</guid></item><item><title>Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems</title><link>http://arxiv.org/abs/2308.08511v2</link><description>Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucialtechnologies in the field of medical imaging. Score-based models have proven tobe effective in addressing different inverse problems encountered in CT andMRI, such as sparse-view CT and fast MRI reconstruction. However, these modelsface challenges in achieving accurate three dimensional (3D) volumetricreconstruction. The existing score-based models primarily focus onreconstructing two dimensional (2D) data distribution, leading toinconsistencies between adjacent slices in the reconstructed 3D volumetricimages. To overcome this limitation, we propose a novel two-and-a-half orderscore-based model (TOSM). During the training phase, our TOSM learns datadistributions in 2D space, which reduces the complexity of training compared todirectly working on 3D volumes. However, in the reconstruction phase, the TOSMupdates the data distribution in 3D space, utilizing complementary scores alongthree directions (sagittal, coronal, and transaxial) to achieve a more precisereconstruction. The development of TOSM is built on robust theoreticalprinciples, ensuring its reliability and efficacy. Through extensiveexperimentation on large-scale sparse-view CT and fast MRI datasets, our methoddemonstrates remarkable advancements and attains state-of-the-art results insolving 3D ill-posed inverse problems. Notably, the proposed TOSM effectivelyaddresses the inter-slice inconsistency issue, resulting in high-quality 3Dvolumetric reconstruction.</description><author>Zirong Li, Yanyang Wang, Jianjia Zhang, Weiwen Wu, Hengyong Yu</author><pubDate>Thu, 17 Aug 2023 06:09:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08511v2</guid></item><item><title>Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems</title><link>http://arxiv.org/abs/2308.08511v1</link><description>Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucialtechnologies in the field of medical imaging. Score-based models have proven tobe effective in addressing different inverse problems encountered in CT andMRI, such as sparse-view CT and fast MRI reconstruction. However, these modelsface challenges in achieving accurate three dimensional (3D) volumetricreconstruction. The existing score-based models primarily focus onreconstructing two dimensional (2D) data distribution, leading toinconsistencies between adjacent slices in the reconstructed 3D volumetricimages. To overcome this limitation, we propose a novel two-and-a-half orderscore-based model (TOSM). During the training phase, our TOSM learns datadistributions in 2D space, which reduces the complexity of training compared todirectly working on 3D volumes. However, in the reconstruction phase, the TOSMupdates the data distribution in 3D space, utilizing complementary scores alongthree directions (sagittal, coronal, and transaxial) to achieve a more precisereconstruction. The development of TOSM is built on robust theoreticalprinciples, ensuring its reliability and efficacy. Through extensiveexperimentation on large-scale sparse-view CT and fast MRI datasets, our methoddemonstrates remarkable advancements and attains state-of-the-art results insolving 3D ill-posed inverse problems. Notably, the proposed TOSM effectivelyaddresses the inter-slice inconsistency issue, resulting in high-quality 3Dvolumetric reconstruction.</description><author>Zirong Li, Yanyang Wang, Jianjia Zhang, Weiwen Wu, Hengyong Yu</author><pubDate>Wed, 16 Aug 2023 18:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08511v1</guid></item><item><title>TAPE: Temporal Attention-based Probabilistic human pose and shape Estimation</title><link>http://arxiv.org/abs/2305.00181v1</link><description>Reconstructing 3D human pose and shape from monocular videos is awell-studied but challenging problem. Common challenges include occlusions, theinherent ambiguities in the 2D to 3D mapping and the computational complexityof video processing. Existing methods ignore the ambiguities of thereconstruction and provide a single deterministic estimate for the 3D pose. Inorder to address these issues, we present a Temporal Attention basedProbabilistic human pose and shape Estimation method (TAPE) that operates on anRGB video. More specifically, we propose to use a neural network to encodevideo frames to temporal features using an attention-based neural network.Given these features, we output a per-frame but temporally-informed probabilitydistribution for the human pose using Normalizing Flows. We show that TAPEoutperforms state-of-the-art methods in standard benchmarks and serves as aneffective video-based prior for optimization-based human pose and shapeestimation. Code is available at: https: //github.com/nikosvasilik/TAPE</description><author>Nikolaos Vasilikopoulos, Nikos Kolotouros, Aggeliki Tsoli, Antonis Argyros</author><pubDate>Sat, 29 Apr 2023 07:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00181v1</guid></item><item><title>Optimal and Robust Category-level Perception: Object Pose and Shape Estimation from 2D and 3D Semantic Keypoints</title><link>http://arxiv.org/abs/2206.12498v2</link><description>We consider a category-level perception problem, where one is given 2D or 3Dsensor data picturing an object of a given category (e.g., a car), and has toreconstruct the 3D pose and shape of the object despite intra-class variability(i.e., different car models have different shapes). We consider an active shapemodel, where -for an object category- we are given a library of potential CADmodels describing objects in that category, and we adopt a standard formulationwhere pose and shape are estimated from 2D or 3D keypoints via non-convexoptimization. Our first contribution is to develop PACE3D* and PACE2D*, thefirst certifiably optimal solvers for pose and shape estimation using 3D and 2Dkeypoints, respectively. Both solvers rely on the design of tight (i.e., exact)semidefinite relaxations. Our second contribution is to develop outlier-robustversions of both solvers, named PACE3D# and PACE2D#. Towards this goal, wepropose ROBIN, a general graph-theoretic framework to prune outliers, whichuses compatibility hypergraphs to model measurements' compatibility. We showthat in category-level perception problems these hypergraphs can be built fromthe winding orders of the keypoints (in 2D) or their convex hulls (in 3D), andmany outliers can be filtered out via maximum hyperclique computation. The lastcontribution is an extensive experimental evaluation. Besides providing anablation study on simulated datasets and on the PASCAL3D+ dataset, we combineour solver with a deep keypoint detector, and show that PACE3D# improves overthe state of the art in vehicle pose estimation in the ApolloScape datasets,and its runtime is compatible with practical applications. We release our codeat https://github.com/MIT-SPARK/PACE.</description><author>Jingnan Shi, Heng Yang, Luca Carlone</author><pubDate>Mon, 15 May 2023 04:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12498v2</guid></item><item><title>3D Human Pose Estimation via Intuitive Physics</title><link>http://arxiv.org/abs/2303.18246v3</link><description>Estimating 3D humans from images often produces implausible bodies that lean,float, or penetrate the floor. Such methods ignore the fact that bodies aretypically supported by the scene. A physics engine can be used to enforcephysical plausibility, but these are not differentiable, rely on unrealisticproxy bodies, and are difficult to integrate into existing optimization andlearning frameworks. In contrast, we exploit novel intuitive-physics (IP) termsthat can be inferred from a 3D SMPL body interacting with the scene. Inspiredby biomechanics, we infer the pressure heatmap on the body, the Center ofPressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). Withthese, we develop IPMAN, to estimate a 3D body from a color image in a "stable"configuration by encouraging plausible floor contact and overlapping CoP andCoM. Our IP terms are intuitive, easy to implement, fast to compute,differentiable, and can be integrated into existing optimization and regressionmethods. We evaluate IPMAN on standard datasets and MoYo, a new dataset withsynchronized multi-view images, ground-truth 3D bodies with complex poses,body-floor contact, CoM and pressure. IPMAN produces more plausible resultsthan the state of the art, improving accuracy for static poses, while nothurting dynamic ones. Code and data are available for research athttps://ipman.is.tue.mpg.de.</description><author>Shashank Tripathi, Lea Müller, Chun-Hao P. Huang, Omid Taheri, Michael J. Black, Dimitrios Tzionas</author><pubDate>Mon, 24 Jul 2023 06:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.18246v3</guid></item><item><title>Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification</title><link>http://arxiv.org/abs/2308.10658v2</link><description>Long-Term Person Re-Identification (LT-ReID) has become increasingly crucialin computer vision and biometrics. In this work, we aim to extend LT-ReIDbeyond pedestrian recognition to include a wider range of real-world humanactivities while still accounting for cloth-changing scenarios over large timegaps. This setting poses additional challenges due to the geometricmisalignment and appearance ambiguity caused by the diversity of human pose andclothing. To address these challenges, we propose a new approach 3DInvarReIDfor (i) disentangling identity from non-identity components (pose, clothingshape, and texture) of 3D clothed humans, and (ii) reconstructing accurate 3Dclothed body shapes and learning discriminative features of naked body shapesfor person ReID in a joint manner. To better evaluate our study of LT-ReID, wecollect a real-world dataset called CCDA, which contains a wide variety ofhuman activities and clothing changes. Experimentally, we show the superiorperformance of our approach for person ReID.</description><author>Feng Liu, Minchul Kim, ZiAng Gu, Anil Jain, Xiaoming Liu</author><pubDate>Tue, 29 Aug 2023 11:37:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10658v2</guid></item><item><title>Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification</title><link>http://arxiv.org/abs/2308.10658v1</link><description>Long-Term Person Re-Identification (LT-ReID) has become increasingly crucialin computer vision and biometrics. In this work, we aim to extend LT-ReIDbeyond pedestrian recognition to include a wider range of real-world humanactivities while still accounting for cloth-changing scenarios over large timegaps. This setting poses additional challenges due to the geometricmisalignment and appearance ambiguity caused by the diversity of human pose andclothing. To address these challenges, we propose a new approach 3DInvarReIDfor (i) disentangling identity from non-identity components (pose, clothingshape, and texture) of 3D clothed humans, and (ii) reconstructing accurate 3Dclothed body shapes and learning discriminative features of naked body shapesfor person ReID in a joint manner. To better evaluate our study of LT-ReID, wecollect a real-world dataset called CCDA, which contains a wide variety ofhuman activities and clothing changes. Experimentally, we show the superiorperformance of our approach for person ReID.</description><author>Feng Liu, Minchul Kim, ZiAng Gu, Anil Jian, Xiaoming Liu</author><pubDate>Mon, 21 Aug 2023 12:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10658v1</guid></item><item><title>Implicit 3D Human Mesh Recovery using Consistency with Pose and Shape from Unseen-view</title><link>http://arxiv.org/abs/2306.17651v1</link><description>From an image of a person, we can easily infer the natural 3D pose and shapeof the person even if ambiguity exists. This is because we have a mental modelthat allows us to imagine a person's appearance at different viewing directionsfrom a given image and utilize the consistency between them for inference.However, existing human mesh recovery methods only consider the direction inwhich the image was taken due to their structural limitations. Hence, wepropose "Implicit 3D Human Mesh Recovery (ImpHMR)" that can implicitly imaginea person in 3D space at the feature-level via Neural Feature Fields. In ImpHMR,feature fields are generated by CNN-based image encoder for a given image.Then, the 2D feature map is volume-rendered from the feature field for a givenviewing direction, and the pose and shape parameters are regressed from thefeature. To utilize consistency with pose and shape from unseen-view, if thereare 3D labels, the model predicts results including the silhouette from anarbitrary direction and makes it equal to the rotated ground-truth. In the caseof only 2D labels, we perform self-supervised learning through the constraintthat the pose and shape parameters inferred from different directions should bethe same. Extensive evaluations show the efficacy of the proposed method.</description><author>Hanbyel Cho, Yooshin Cho, Jaesung Ahn, Junmo Kim</author><pubDate>Fri, 30 Jun 2023 14:37:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17651v1</guid></item><item><title>Implicit 3D Human Mesh Recovery using Consistency with Pose and Shape from Unseen-view</title><link>http://arxiv.org/abs/2306.17651v2</link><description>From an image of a person, we can easily infer the natural 3D pose and shapeof the person even if ambiguity exists. This is because we have a mental modelthat allows us to imagine a person's appearance at different viewing directionsfrom a given image and utilize the consistency between them for inference.However, existing human mesh recovery methods only consider the direction inwhich the image was taken due to their structural limitations. Hence, wepropose "Implicit 3D Human Mesh Recovery (ImpHMR)" that can implicitly imaginea person in 3D space at the feature-level via Neural Feature Fields. In ImpHMR,feature fields are generated by CNN-based image encoder for a given image.Then, the 2D feature map is volume-rendered from the feature field for a givenviewing direction, and the pose and shape parameters are regressed from thefeature. To utilize consistency with pose and shape from unseen-view, if thereare 3D labels, the model predicts results including the silhouette from anarbitrary direction and makes it equal to the rotated ground-truth. In the caseof only 2D labels, we perform self-supervised learning through the constraintthat the pose and shape parameters inferred from different directions should bethe same. Extensive evaluations show the efficacy of the proposed method.</description><author>Hanbyel Cho, Yooshin Cho, Jaesung Ahn, Junmo Kim</author><pubDate>Mon, 03 Jul 2023 02:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17651v2</guid></item><item><title>Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception</title><link>http://arxiv.org/abs/2306.06362v2</link><description>We introduce the Aria Digital Twin (ADT) - an egocentric dataset capturedusing Aria glasses with extensive object, environment, and human level groundtruth. This ADT release contains 200 sequences of real-world activitiesconducted by Aria wearers in two real indoor scenes with 398 object instances(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of twomonochrome camera streams, one RGB camera stream, two IMU streams; b) completesensor calibration; c) ground truth data including continuous6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eyegaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)photo-realistic synthetic renderings. To the best of our knowledge, there is noexisting egocentric dataset with a level of accuracy, photo-realism andcomprehensiveness comparable to ADT. By contributing ADT to the researchcommunity, our mission is to set a new standard for evaluation in theegocentric machine perception domain, which includes very challenging researchproblems such as 3D object detection and tracking, scene reconstruction andunderstanding, sim-to-real learning, human pose prediction - while alsoinspiring new machine perception tasks for augmented reality (AR) applications.To kick start exploration of the ADT research use cases, we evaluated severalexisting state-of-the-art methods for object detection, segmentation and imagetranslation tasks that demonstrate the usefulness of ADT as a benchmarkingdataset.</description><author>Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, Carl Yuheng Ren</author><pubDate>Tue, 13 Jun 2023 07:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06362v2</guid></item><item><title>A Horse with no Labels: Self-Supervised Horse Pose Estimation from Unlabelled Images and Synthetic Prior</title><link>http://arxiv.org/abs/2308.03411v1</link><description>Obtaining labelled data to train deep learning methods for estimating animalpose is challenging. Recently, synthetic data has been widely used for poseestimation tasks, but most methods still rely on supervised learning paradigmsutilising synthetic images and labels. Can training be fully unsupervised? Is atiny synthetic dataset sufficient? What are the minimum assumptions that wecould make for estimating animal pose? Our proposal addresses these questionsthrough a simple yet effective self-supervised method that only assumes theavailability of unlabelled images and a small set of synthetic 2D poses. Wecompletely remove the need for any 3D or 2D pose annotations (or complex 3Danimal models), and surprisingly our approach can still learn accurate 3D and2D poses simultaneously. We train our method with unlabelled images of horsesmainly collected for YouTube videos and a prior consisting of 2D syntheticposes. The latter is three times smaller than the number of images needed fortraining. We test our method on a challenging set of horse images and evaluatethe predicted 3D and 2D poses. We demonstrate that it is possible to learnaccurate animal poses even with as few assumptions as unlabelled images and asmall set of 2D poses generated from synthetic data. Given the minimumrequirements and the abundance of unlabelled data, our method could be easilydeployed to different animals.</description><author>Jose Sosa, David Hogg</author><pubDate>Mon, 07 Aug 2023 10:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03411v1</guid></item><item><title>An Efficient Convex Hull-based Vehicle Pose Estimation Method for 3D LiDAR</title><link>http://arxiv.org/abs/2302.01034v2</link><description>Vehicle pose estimation with LiDAR is essential in the perception technologyof autonomous driving. However, due to incomplete observation measurements andsparsity of the LiDAR point cloud, it is challenging to achieve satisfactorypose extraction based on 3D LiDAR by using the existing pose estimationmethods. In addition, the requirement for real-time performance furtherincreases the difficulty of the pose estimation task. In this paper, weproposed a novel convex hull-based vehicle pose estimation method. Theextracted 3D cluster is reduced to the convex hull, reducing the computationburden and retaining contour information. Then a novel criterion based on theminimum occlusion area is developed for the search-based algorithm, which canachieve accurate pose estimation. This criterion also makes the proposedalgorithm especially suitable for obstacle avoidance. The proposed algorithm isvalidated on the KITTI dataset and a manually labeled dataset acquired at anindustrial park. The results show that our proposed method can achieve betteraccuracy than the state-of-the-art pose estimation method while maintainingreal-time speed.</description><author>Ningning Ding</author><pubDate>Sun, 02 Jul 2023 00:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01034v2</guid></item><item><title>PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds</title><link>http://arxiv.org/abs/2308.14492v1</link><description>Human pose and shape estimation (HPS) has attracted increasing attention inrecent years. While most existing studies focus on HPS from 2D images or videoswith inherent depth ambiguity, there are surging need to investigate HPS from3D point clouds as depth sensors have been frequently employed in commercialdevices. However, real-world sensory 3D points are usually noisy andincomplete, and also human bodies could have different poses of high diversity.To tackle these challenges, we propose a principled framework, PointHPS, foraccurate 3D HPS from point clouds captured in real-world settings, whichiteratively refines point features through a cascaded architecture.Specifically, each stage of PointHPS performs a series of downsampling andupsampling operations to extract and collate both local and global cues, whichare further enhanced by two novel modules: 1) Cross-stage Feature Fusion (CFF)for multi-scale feature propagation that allows information to flow effectivelythrough the stages, and 2) Intermediate Feature Enhancement (IFE) forbody-aware feature aggregation that improves feature quality after each stage.To facilitate a comprehensive study under various scenarios, we conduct ourexperiments on two large-scale benchmarks, comprising i) a dataset thatfeatures diverse subjects and actions captured by real commercial sensors in alaboratory environment, and ii) controlled synthetic data generated withrealistic considerations such as clothed humans in crowded outdoor scenes.Extensive experiments demonstrate that PointHPS, with its powerful pointfeature extraction and processing scheme, outperforms State-of-the-Art methodsby significant margins across the board. Homepage:https://caizhongang.github.io/projects/PointHPS/.</description><author>Zhongang Cai, Liang Pan, Chen Wei, Wanqi Yin, Fangzhou Hong, Mingyuan Zhang, Chen Change Loy, Lei Yang, Ziwei Liu</author><pubDate>Mon, 28 Aug 2023 12:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14492v1</guid></item><item><title>Learning from Abstract Images: on the Importance of Occlusion in a Minimalist Encoding of Human Poses</title><link>http://arxiv.org/abs/2307.09893v1</link><description>Existing 2D-to-3D pose lifting networks suffer from poor performance incross-dataset benchmarks. Although the use of 2D keypoints joined by"stick-figure" limbs has shown promise as an intermediate step, stick-figuresdo not account for occlusion information that is often inherent in an image. Inthis paper, we propose a novel representation using opaque 3D limbs thatpreserves occlusion information while implicitly encoding joint locations.Crucially, when training on data with accurate three-dimensional keypoints andwithout part-maps, this representation allows training on abstract syntheticimages, with occlusion, from as many synthetic viewpoints as desired. Theresult is a pose defined by limb angles rather than joint positions$\unicode{x2013}$ because poses are, in the real world, independent of cameras$\unicode{x2013}$ allowing us to predict poses that are completely independentof camera viewpoint. The result provides not only an improvement insame-dataset benchmarks, but a "quantum leap" in cross-dataset benchmarks.</description><author>Saad Manzur, Wayne Hayes</author><pubDate>Wed, 19 Jul 2023 11:45:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09893v1</guid></item><item><title>3D-Aware Object Localization using Gaussian Implicit Occupancy Function</title><link>http://arxiv.org/abs/2303.02058v2</link><description>To automatically localize a target object in an image is crucial for manycomputer vision applications. To represent the 2D object, ellipse labels haverecently been identified as a promising alternative to axis-aligned boundingboxes. This paper further considers 3D-aware ellipse labels, \textit{i.e.},ellipses which are projections of a 3D ellipsoidal approximation of the object,for 2D target localization. Indeed, projected ellipses carry more geometricinformation about the object geometry and pose (3D awareness) than traditional3D-agnostic bounding box labels. Moreover, such a generic 3D ellipsoidal modelallows for approximating known to coarsely known targets. We then propose tohave a new look at ellipse regression and replace the discontinuous geometricellipse parameters with the parameters of an implicit Gaussian distributionencoding object occupancy in the image. The models are trained to regress thevalues of this bivariate Gaussian distribution over the image pixels using astatistical loss function. We introduce a novel non-trainable differentiablelayer, E-DSNT, to extract the distribution parameters. Also, we describe how toreadily generate consistent 3D-aware Gaussian occupancy parameters using onlycoarse dimensions of the target and relative pose labels. We extend threeexisting spacecraft pose estimation datasets with 3D-aware Gaussian occupancylabels to validate our hypothesis. Labels and source code are publiclyaccessible here: https://cvi2.uni.lu/3d-aware-obj-loc/.</description><author>Vincent Gaudillière, Leo Pauly, Arunkumar Rathinam, Albert Garcia Sanchez, Mohamed Adel Musallam, Djamila Aouada</author><pubDate>Wed, 02 Aug 2023 15:21:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02058v2</guid></item><item><title>MVP-Human Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames</title><link>http://arxiv.org/abs/2204.11184v2</link><description>In this paper, we consider a novel problem of reconstructing a 3D humanavatar from multiple unconstrained frames, independent of assumptions on cameracalibration, capture space, and constrained actions. The problem should beaddressed by a framework that takes multiple unconstrained images as inputs,and generates a shape-with-skinning avatar in the canonical space, finished inone feed-forward pass. To this end, we present 3D Avatar Reconstruction in thewild (ARwild), which first reconstructs the implicit skinning fields in amulti-level manner, by which the image features from multiple images arealigned and integrated to estimate a pixel-aligned implicit function thatrepresents the clothed shape. To enable the training and testing of the newframework, we contribute a large-scale dataset, MVP-Human (Multi-View andmulti-Pose 3D Human), which contains 400 subjects, each of which has 15 scansin different poses and 8-view images for each pose, providing 6,000 3D scansand 48,000 images in total. Overall, benefits from the specific networkarchitecture and the diverse data, the trained model enables 3D avatarreconstruction from unconstrained frames and achieves state-of-the-artperformance.</description><author>Xiangyu Zhu, Tingting Liao, Jiangjing Lyu, Xiang Yan, Yunfeng Wang, Kan Guo, Qiong Cao, Stan Z. Li, Zhen Lei</author><pubDate>Wed, 17 May 2023 11:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.11184v2</guid></item><item><title>FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models</title><link>http://arxiv.org/abs/2308.05733v1</link><description>3D scene reconstruction is a long-standing vision task. Existing approachescan be categorized into geometry-based and learning-based methods. The formerleverages multi-view geometry but can face catastrophic failures due to thereliance on accurate pixel correspondence across views. The latter wasproffered to mitigate these issues by learning 2D or 3D representationdirectly. However, without a large-scale video or 3D training data, it canhardly generalize to diverse real-world scenarios due to the presence of tensof millions or even billions of optimization parameters in the deep network.Recently, robust monocular depth estimation models trained with large-scaledatasets have been proven to possess weak 3D geometry prior, but they areinsufficient for reconstruction due to the unknown camera parameters, theaffine-invariant property, and inter-frame inconsistency. Here, we propose anovel test-time optimization approach that can transfer the robustness ofaffine-invariant depth models such as LeReS to challenging diverse scenes whileensuring inter-frame consistency, with only dozens of parameters to optimizeper video frame. Specifically, our approach involves freezing the pre-trainedaffine-invariant depth model's depth predictions, rectifying them by optimizingthe unknown scale-shift values with a geometric consistency alignment module,and employing the resulting scale-consistent depth maps to robustly obtaincamera poses and achieve dense scene reconstruction, even in low-textureregions. Experiments show that our method achieves state-of-the-artcross-dataset reconstruction on five zero-shot testing datasets.</description><author>Guangkai Xu, Wei Yin, Hao Chen, Chunhua Shen, Kai Cheng, Feng Zhao</author><pubDate>Thu, 10 Aug 2023 18:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05733v1</guid></item><item><title>Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training</title><link>http://arxiv.org/abs/2206.11215v4</link><description>We consider a certifiable object pose estimation problem, where -- given apartial point cloud of an object -- the goal is to not only estimate the objectpose, but also to provide a certificate of correctness for the resultingestimate. Our first contribution is a general theory of certification forend-to-end perception models. In particular, we introduce the notion of$\zeta$-correctness, which bounds the distance between an estimate and theground truth. We show that $\zeta$-correctness can be assessed by implementingtwo certificates: (i) a certificate of observable correctness, that asserts ifthe model output is consistent with the input data and prior information, (ii)a certificate of non-degeneracy, that asserts whether the input data issufficient to compute a unique estimate. Our second contribution is to applythis theory and design a new learning-based certifiable pose estimator. Wepropose C-3PO, a semantic-keypoint-based pose estimation model, augmented withthe two certificates, to solve the certifiable pose estimation problem. C-3POalso includes a keypoint corrector, implemented as a differentiableoptimization layer, that can correct large detection errors (e.g. due to thesim-to-real gap). Our third contribution is a novel self-supervised trainingapproach that uses our certificate of observable correctness to provide thesupervisory signal to C-3PO during training. In it, the model trains only onthe observably correct input-output pairs, in each training iteration. Astraining progresses, we see that the observably correct input-output pairsgrow, eventually reaching near 100% in many cases. Our experiments show that(i) standard semantic-keypoint-based methods outperform more recentalternatives, (ii) C-3PO further improves performance and significantlyoutperforms all the baselines, and (iii) C-3PO's certificates are able todiscern correct pose estimates.</description><author>Rajat Talak, Lisa Peng, Luca Carlone</author><pubDate>Fri, 28 Apr 2023 20:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11215v4</guid></item><item><title>Few-View Object Reconstruction with Unknown Categories and Camera Poses</title><link>http://arxiv.org/abs/2212.04492v2</link><description>While object reconstruction has made great strides in recent years, currentmethods typically require densely captured images and/or known camera poses,and generalize poorly to novel object categories. To step toward objectreconstruction in the wild, this work explores reconstructing generalreal-world objects from a few images without known camera poses or objectcategories. The crux of our work is solving two fundamental 3D vision problems-- shape reconstruction and pose estimation -- in a unified approach. Ourapproach captures the synergies of these two problems: reliable camera poseestimation gives rise to accurate shape reconstruction, and the accuratereconstruction, in turn, induces robust correspondence between different viewsand facilitates pose estimation. Our method FORGE predicts 3D features fromeach view and leverages them in conjunction with the input images to establishcross-view correspondence for estimating relative camera poses. The 3D featuresare then transformed by the estimated poses into a shared space and are fusedinto a neural radiance field. The reconstruction results are rendered by volumerendering techniques, enabling us to train the model without 3D shapeground-truth. Our experiments show that FORGE reliably reconstructs objectsfrom five views. Our pose estimation method outperforms existing ones by alarge margin. The reconstruction results under predicted poses are comparableto the ones using ground-truth poses. The performance on novel testingcategories matches the results on categories seen during training. Projectpage: https://ut-austin-rpl.github.io/FORGE/</description><author>Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, Yuke Zhu</author><pubDate>Tue, 12 Sep 2023 20:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04492v2</guid></item><item><title>ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with Unsupervised Implicit Pose Embedding</title><link>http://arxiv.org/abs/2304.14005v1</link><description>Although 3D-aware GANs based on neural radiance fields have achievedcompetitive performance, their applicability is still limited to objects orscenes with the ground-truths or prediction models for clearly definedcanonical camera poses. To extend the scope of applicable datasets, we proposea novel 3D-aware GAN optimization technique through contrastive learning withimplicit pose embeddings. To this end, we first revise the discriminator designand remove dependency on ground-truth camera poses. Then, to capture complexand challenging 3D scene structures more effectively, we make the discriminatorestimate a high-dimensional implicit pose embedding from a given image andperform contrastive learning on the pose embedding. The proposed approach canbe employed for the dataset, where the canonical camera pose is ill-definedbecause it does not look up or estimate camera poses. Experimental results showthat our algorithm outperforms existing methods by large margins on thedatasets with multiple object categories and inconsistent canonical cameraposes.</description><author>Mijeoong Kim, Hyunjoon Lee, Bohyung Han</author><pubDate>Thu, 27 Apr 2023 08:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14005v1</guid></item><item><title>ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with Unsupervised Implicit Pose Embedding</title><link>http://arxiv.org/abs/2304.14005v2</link><description>Although 3D-aware GANs based on neural radiance fields have achievedcompetitive performance, their applicability is still limited to objects orscenes with the ground-truths or prediction models for clearly definedcanonical camera poses. To extend the scope of applicable datasets, we proposea novel 3D-aware GAN optimization technique through contrastive learning withimplicit pose embeddings. To this end, we first revise the discriminator designand remove dependency on ground-truth camera poses. Then, to capture complexand challenging 3D scene structures more effectively, we make the discriminatorestimate a high-dimensional implicit pose embedding from a given image andperform contrastive learning on the pose embedding. The proposed approach canbe employed for the dataset, where the canonical camera pose is ill-definedbecause it does not look up or estimate camera poses. Experimental results showthat our algorithm outperforms existing methods by large margins on thedatasets with multiple object categories and inconsistent canonical cameraposes.</description><author>Mijeong Kim, Hyunjoon Lee, Bohyung Han</author><pubDate>Mon, 03 Jul 2023 12:34:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14005v2</guid></item><item><title>BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos</title><link>http://arxiv.org/abs/2212.07401v3</link><description>Quantifying motion in 3D is important for studying the behavior of humans andother animals, but manual pose annotations are expensive and time-consuming toobtain. Self-supervised keypoint discovery is a promising strategy forestimating 3D poses without annotations. However, current keypoint discoveryapproaches commonly process single 2D views and do not operate in the 3D space.We propose a new method to perform self-supervised keypoint discovery in 3Dfrom multi-view videos of behaving agents, without any keypoint or bounding boxsupervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoderarchitecture with a 3D volumetric heatmap, trained to reconstructspatiotemporal differences across multiple views, in addition to joint lengthconstraints on a learned 3D skeleton of the subject. In this way, we discoverkeypoints without requiring manual supervision in videos of humans and rats,demonstrating the potential of 3D keypoint discovery for studying behavior.</description><author>Jennifer J. Sun, Lili Karashchuk, Amil Dravid, Serim Ryou, Sonia Fereidooni, John Tuthill, Aggelos Katsaggelos, Bingni W. Brunton, Georgia Gkioxari, Ann Kennedy, Yisong Yue, Pietro Perona</author><pubDate>Fri, 02 Jun 2023 06:03:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07401v3</guid></item><item><title>BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos</title><link>http://arxiv.org/abs/2212.07401v2</link><description>Quantifying motion in 3D is important for studying the behavior of humans andother animals, but manual pose annotations are expensive and time-consuming toobtain. Self-supervised keypoint discovery is a promising strategy forestimating 3D poses without annotations. However, current keypoint discoveryapproaches commonly process single 2D views and do not operate in the 3D space.We propose a new method to perform self-supervised keypoint discovery in 3Dfrom multi-view videos of behaving agents, without any keypoint or bounding boxsupervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoderarchitecture with a 3D volumetric heatmap, trained to reconstructspatiotemporal differences across multiple views, in addition to joint lengthconstraints on a learned 3D skeleton of the subject. In this way, we discoverkeypoints without requiring manual supervision in videos of humans and rats,demonstrating the potential of 3D keypoint discovery for studying behavior.</description><author>Jennifer J. Sun, Lili Karashchuk, Amil Dravid, Serim Ryou, Sonia Fereidooni, John Tuthill, Aggelos Katsaggelos, Bingni W. Brunton, Georgia Gkioxari, Ann Kennedy, Yisong Yue, Pietro Perona</author><pubDate>Sun, 07 May 2023 00:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07401v2</guid></item><item><title>POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities</title><link>http://arxiv.org/abs/2307.10387v1</link><description>The surgical usage of Mixed Reality (MR) has received growing attention inareas such as surgical navigation systems, skill assessment, and robot-assistedsurgeries. For such applications, pose estimation for hand and surgicalinstruments from an egocentric perspective is a fundamental task and has beenstudied extensively in the computer vision field in recent years. However, thedevelopment of this field has been impeded by a lack of datasets, especially inthe surgical field, where bloody gloves and reflective metallic tools make ithard to obtain 3D pose annotations for hands and objects using conventionalmethods. To address this issue, we propose POV-Surgery, a large-scale,synthetic, egocentric dataset focusing on pose estimation for hands withdifferent surgical gloves and three orthopedic surgical instruments, namelyscalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329frames, featuring high-resolution RGB-D video streams with activityannotations, accurate 3D and 2D annotations for hand-object pose, and 2Dhand-object segmentation masks. We fine-tune the current SOTA methods onPOV-Surgery and further show the generalizability when applying to real-lifecases with surgical gloves and tools by extensive evaluations. The code and thedataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.</description><author>Rui Wang, Sophokles Ktistakis, Siwei Zhang, Mirko Meboldt, Quentin Lohmeyer</author><pubDate>Wed, 19 Jul 2023 19:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10387v1</guid></item><item><title>Robot Pose Nowcasting: Forecast the Future to Improve the Present</title><link>http://arxiv.org/abs/2308.12914v1</link><description>In recent years, the effective and safe collaboration between humans andmachines has gained significant importance, particularly in the Industry 4.0scenario. A critical prerequisite for realizing this collaborative paradigm isprecisely understanding the robot's 3D pose within its environment. Therefore,in this paper, we introduce a novel vision-based system leveraging depth datato accurately establish the 3D locations of robotic joints. Specifically, weprove the ability of the proposed system to enhance its current pose estimationaccuracy by jointly learning to forecast future poses. Indeed, we introduce theconcept of Pose Nowcasting, denoting the capability of a system to exploit thelearned knowledge of the future to improve the estimation of the present. Theexperimental evaluation is conducted on two different datasets, providingstate-of-the-art and real-time performance and confirming the validity of theproposed method on both the robotic and human scenarios.</description><author>Alessandro Simoni, Francesco Marchetti, Guido Borghi, Federico Becattini, Lorenzo Seidenari, Roberto Vezzani, Alberto Del Bimbo</author><pubDate>Thu, 24 Aug 2023 17:40:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12914v1</guid></item><item><title>Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction</title><link>http://arxiv.org/abs/2305.09510v1</link><description>Robotic manipulation systems operating in complex environments rely onperception systems that provide information about the geometry (pose and 3Dshape) of the objects in the scene along with other semantic information suchas object labels. This information is then used for choosing the feasiblegrasps on relevant objects. In this paper, we present a novel method to providethis geometric and semantic information of all objects in the scene as well asfeasible grasps on those objects simultaneously. The main advantage of ourmethod is its speed as it avoids sequential perception and grasp planningsteps. With detailed quantitative analysis, we show that our method deliverscompetitive performance compared to the state-of-the-art dedicated methods forobject shape, pose, and grasp predictions while providing fast inference at 30frames per second speed.</description><author>Shubham Agrawal, Nikhil Chavan-Dafle, Isaac Kasahara, Selim Engin, Jinwook Huh, Volkan Isler</author><pubDate>Tue, 16 May 2023 16:03:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09510v1</guid></item><item><title>EgoHumans: An Egocentric 3D Multi-Human Benchmark</title><link>http://arxiv.org/abs/2305.16487v1</link><description>We present EgoHumans, a new multi-view multi-human video benchmark to advancethe state-of-the-art of egocentric human 3D pose estimation and tracking.Existing egocentric benchmarks either capture single subject or indoor-onlyscenarios, which limit the generalization of computer vision algorithms forreal-world applications. We propose a novel 3D capture setup to construct acomprehensive egocentric multi-human benchmark in the wild with annotations tosupport diverse tasks such as human detection, tracking, 2D/3D pose estimation,and mesh recovery. We leverage consumer-grade wearable camera-equipped glassesfor the egocentric view, which enables us to capture dynamic activities likeplaying soccer, fencing, volleyball, etc. Furthermore, our multi-view setupgenerates accurate 3D ground truth even under severe or complete occlusion. Thedataset consists of more than 125k egocentric images, spanning diverse sceneswith a particular focus on challenging and unchoreographed multi-humanactivities and fast-moving egocentric views. We rigorously evaluate existingstate-of-the-art methods and highlight their limitations in the egocentricscenario, specifically on multi-human tracking. To address such limitations, wepropose EgoFormer, a novel approach with a multi-stream transformerarchitecture and explicit 3D spatial reasoning to estimate and track the humanpose. EgoFormer significantly outperforms prior art by 13.6% IDF1 and 9.3 HOTAon the EgoHumans dataset.</description><author>Rawal Khirodkar, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, Kris Kitani</author><pubDate>Thu, 25 May 2023 22:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16487v1</guid></item><item><title>Reconstructing Three-Dimensional Models of Interacting Humans</title><link>http://arxiv.org/abs/2308.01854v2</link><description>Understanding 3d human interactions is fundamental for fine-grained sceneanalysis and behavioural modeling. However, most of the existing models predictincorrect, lifeless 3d estimates, that miss the subtle human contactaspects--the essence of the event--and are of little use for detailedbehavioral understanding. This paper addresses such issues with severalcontributions: (1) we introduce models for interaction signature estimation(ISP) encompassing contact detection, segmentation, and 3d contact signatureprediction; (2) we show how such components can be leveraged to ensure contactconsistency during 3d reconstruction; (3) we construct several large datasetsfor learning and evaluating 3d contact prediction and reconstruction methods;specifically, we introduce CHI3D, a lab-based accurate 3d motion capturedataset with 631 sequences containing $2,525$ contact events, $728,664$ groundtruth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with$14,081$ processed pairs of people, and $81,233$ facet-level surfacecorrespondences. Finally, (4) we propose methodology for recovering theground-truth pose and shape of interacting people in a controlled setup and (5)annotate all 3d interaction motions in CHI3D with textual descriptions. Motiondata in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) ismade available for research purposes at \url{https://ci3d.imar.ro}, togetherwith an evaluation server and a public benchmark.</description><author>Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, Cristian Sminchisescu</author><pubDate>Fri, 04 Aug 2023 09:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01854v2</guid></item><item><title>Reconstructing Three-Dimensional Models of Interacting Humans</title><link>http://arxiv.org/abs/2308.01854v1</link><description>Understanding 3d human interactions is fundamental for fine-grained sceneanalysis and behavioural modeling. However, most of the existing models predictincorrect, lifeless 3d estimates, that miss the subtle human contactaspects--the essence of the event--and are of little use for detailedbehavioral understanding. This paper addresses such issues with severalcontributions: (1) we introduce models for interaction signature estimation(ISP) encompassing contact detection, segmentation, and 3d contact signatureprediction; (2) we show how such components can be leveraged to ensure contactconsistency during 3d reconstruction; (3) we construct several large datasetsfor learning and evaluating 3d contact prediction and reconstruction methods;specifically, we introduce CHI3D, a lab-based accurate 3d motion capturedataset with 631 sequences containing $2,525$ contact events, $728,664$ groundtruth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with$14,081$ processed pairs of people, and $81,233$ facet-level surfacecorrespondences. Finally, (4) we propose methodology for recovering theground-truth pose and shape of interacting people in a controlled setup and (5)annotate all 3d interaction motions in CHI3D with textual descriptions. Motiondata in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) ismade available for research purposes at \url{https://ci3d.imar.ro}, togetherwith an evaluation server and a public benchmark.</description><author>Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, Cristian Sminchisescu</author><pubDate>Thu, 03 Aug 2023 17:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01854v1</guid></item><item><title>EgoCOL: Egocentric Camera pose estimation for Open-world 3D object Localization @Ego4D challenge 2023</title><link>http://arxiv.org/abs/2306.16606v1</link><description>We present EgoCOL, an egocentric camera pose estimation method for open-world3D object localization. Our method leverages sparse camera pose reconstructionsin a two-fold manner, video and scan independently, to estimate the camera poseof egocentric frames in 3D renders with high recall and precision. Weextensively evaluate our method on the Visual Query (VQ) 3D object localizationEgo4D benchmark. EgoCOL can estimate 62% and 59% more camera poses than theEgo4D baseline in the Ego4D Visual Queries 3D Localization challenge at CVPR2023 in the val and test sets, respectively. Our code is publicly available athttps://github.com/BCV-Uniandes/EgoCOL</description><author>Cristhian Forigua, Maria Escobar, Jordi Pont-Tuset, Kevis-Kokitsi Maninis, Pablo Arbeláez</author><pubDate>Thu, 29 Jun 2023 01:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16606v1</guid></item><item><title>Learning from Synthetic Human Group Activities</title><link>http://arxiv.org/abs/2306.16772v1</link><description>The understanding of complex human interactions and group activities hasgarnered attention in human-centric computer vision. However, the advancementof the related tasks is hindered due to the difficulty of obtaining large-scalelabeled real-world datasets. To mitigate the issue, we propose M3Act, amulti-view multi-group multi-person human atomic action and group activity datagenerator. Powered by the Unity engine, M3Act contains simulation-ready 3Dscenes and human assets, configurable lighting and camera systems, highlyparameterized modular group activities, and a large degree of domainrandomization during the data generation process. Our data generator is capableof generating large-scale datasets of human activities with multipleviewpoints, modalities (RGB images, 2D poses, 3D motions), and high-qualityannotations for individual persons and multi-person groups (2D bounding boxes,instance segmentation masks, individual actions and group activity categories).Using M3Act, we perform synthetic data pre-training for 2D skeleton-based groupactivity recognition and RGB-based multi-person pose tracking. The resultsindicate that learning from our synthetic datasets largely improves the modelperformances on real-world datasets, with the highest gain of 5.59% and 7.32%respectively in group and person recognition accuracy on CAD2, as well as animprovement of 6.63 in MOTP on HiEve. Pre-training with our synthetic data alsoleads to faster model convergence on downstream tasks (up to 6.8% faster).Moreover, M3Act opens new research problems for 3D group activity generation.We release M3Act3D, an 87.6-hour 3D motion dataset of human activities withlarger group sizes and higher complexity of inter-person interactions thanprevious multi-person datasets. We define multiple metrics and propose acompetitive baseline for the novel task.</description><author>Che-Jui Chang, Honglu Zhou, Parth Goel, Aditya Bhat, Seonghyeon Moon, Samuel S. Sohn, Sejong Yoon, Vladimir Pavlovic, Mubbasir Kapadia</author><pubDate>Thu, 29 Jun 2023 09:13:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16772v1</guid></item><item><title>Learning from Synthetic Human Group Activities</title><link>http://arxiv.org/abs/2306.16772v2</link><description>The understanding of complex human interactions and group activities hasgarnered attention in human-centric computer vision. However, the advancementof the related tasks is hindered due to the difficulty of obtaining large-scalelabeled real-world datasets. To mitigate the issue, we propose M3Act, amulti-view multi-group multi-person human atomic action and group activity datagenerator. Powered by the Unity engine, M3Act contains simulation-ready 3Dscenes and human assets, configurable lighting and camera systems, highlyparameterized modular group activities, and a large degree of domainrandomization during the data generation process. Our data generator is capableof generating large-scale datasets of human activities with multipleviewpoints, modalities (RGB images, 2D poses, 3D motions), and high-qualityannotations for individual persons and multi-person groups (2D bounding boxes,instance segmentation masks, individual actions and group activity categories).Using M3Act, we perform synthetic data pre-training for 2D skeleton-based groupactivity recognition and RGB-based multi-person pose tracking. The resultsindicate that learning from our synthetic datasets largely improves the modelperformances on real-world datasets, with the highest gain of 5.59% and 7.32%respectively in group and person recognition accuracy on CAD2, as well as animprovement of 6.63 in MOTP on HiEve. Pre-training with our synthetic data alsoleads to faster model convergence on downstream tasks (up to 6.8% faster).Moreover, M3Act opens new research problems for 3D group activity generation.We release M3Act3D, an 87.6-hour 3D motion dataset of human activities withlarger group sizes and higher complexity of inter-person interactions thanprevious multi-person datasets. We define multiple metrics and propose acompetitive baseline for the novel task.</description><author>Che-Jui Chang, Honglu Zhou, Parth Goel, Aditya Bhat, Seonghyeon Moon, Samuel S. Sohn, Sejong Yoon, Vladimir Pavlovic, Mubbasir Kapadia</author><pubDate>Sun, 16 Jul 2023 06:44:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16772v2</guid></item><item><title>OOD-CV-v2: An extended Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images</title><link>http://arxiv.org/abs/2304.10266v2</link><description>Enhancing the robustness of vision algorithms in real-world scenarios ischallenging. One reason is that existing robustness benchmarks are limited, asthey either rely on synthetic data or ignore the effects of individual nuisancefactors. We introduce OOD-CV-v2, a benchmark dataset that includesout-of-distribution examples of 10 object categories in terms of pose, shape,texture, context and the weather conditions, and enables benchmarking of modelsfor image classification, object detection, and 3D pose estimation. In additionto this novel dataset, we contribute extensive experiments using popularbaseline methods, which reveal that: 1) Some nuisance factors have a muchstronger negative effect on the performance compared to others, also dependingon the vision task. 2) Current approaches to enhance robustness have onlymarginal effects, and can even reduce robustness. 3) We do not observesignificant differences between convolutional and transformer architectures. Webelieve our dataset provides a rich test bed to study robustness and will helppush forward research in this area. Our dataset can be accessed from https://bzhao.me/OOD-CV/</description><author>Bingchen Zhao, Jiahao Wang, Wufei Ma, Artur Jesslen, Siwei Yang, Shaozuo Yu, Oliver Zendel, Christian Theobalt, Alan Yuille, Adam Kortylewski</author><pubDate>Wed, 26 Jul 2023 19:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10266v2</guid></item><item><title>Equivariant Single View Pose Prediction Via Induced and Restricted Representations</title><link>http://arxiv.org/abs/2307.03704v1</link><description>Learning about the three-dimensional world from two-dimensional images is afundamental problem in computer vision. An ideal neural network architecturefor such tasks would leverage the fact that objects can be rotated andtranslated in three dimensions to make predictions about novel images. However,imposing SO(3)-equivariance on two-dimensional inputs is difficult because thegroup of three-dimensional rotations does not have a natural action on thetwo-dimensional plane. Specifically, it is possible that an element of SO(3)will rotate an image out of plane. We show that an algorithm that learns athree-dimensional representation of the world from two dimensional images mustsatisfy certain geometric consistency properties which we formulate asSO(2)-equivariance constraints. We use the induced and restrictedrepresentations of SO(2) on SO(3) to construct and classify architectures whichsatisfy these geometric consistency constraints. We prove that any architecturewhich respects said consistency constraints can be realized as an instance ofour construction. We show that three previously proposed neural architecturesfor 3D pose prediction are special cases of our construction. We propose a newalgorithm that is a learnable generalization of previously considered methods.We test our architecture on three pose predictions task and achieve SOTAresults on both the PASCAL3D+ and SYMSOL pose estimation tasks.</description><author>Owen Howell, David Klee, Ondrej Biza, Linfeng Zhao, Robin Walters</author><pubDate>Fri, 07 Jul 2023 17:30:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03704v1</guid></item><item><title>Object pop-up: Can we infer 3D objects and their poses from human interactions alone?</title><link>http://arxiv.org/abs/2306.00777v1</link><description>The intimate entanglement between objects affordances and human poses is oflarge interest, among others, for behavioural sciences, cognitive psychology,and Computer Vision communities. In recent years, the latter has developedseveral object-centric approaches: starting from items, learning pipelinessynthesizing human poses and dynamics in a realistic way, satisfying bothgeometrical and functional expectations. However, the inverse perspective issignificantly less explored: Can we infer 3D objects and their poses from humaninteractions alone? Our investigation follows this direction, showing that ageneric 3D human point cloud is enough to pop up an unobserved object, evenwhen the user is just imitating a functionality (e.g., looking through abinocular) without involving a tangible counterpart. We validate our methodqualitatively and quantitatively, with synthetic data and sequences acquiredfor the task, showing applicability for XR/VR. The code is available athttps://github.com/ptrvilya/object-popup.</description><author>Ilya A. Petrov, Riccardo Marin, Julian Chibane, Gerard Pons-Moll</author><pubDate>Thu, 01 Jun 2023 16:08:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00777v1</guid></item><item><title>Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction</title><link>http://arxiv.org/abs/2209.05135v3</link><description>Learning fine-grained movements is a challenging topic in robotics,particularly in the context of robotic hands. One specific instance of thischallenge is the acquisition of fingerspelling sign language in robots. In thispaper, we propose an approach for learning dexterous motor imitation from videoexamples without additional information. To achieve this, we first build a URDFmodel of a robotic hand with a single actuator for each joint. We then leveragepre-trained deep vision models to extract the 3D pose of the hand from RGBvideos. Next, using state-of-the-art reinforcement learning algorithms formotion imitation (namely, proximal policy optimization and soft actor-critic),we train a policy to reproduce the movement extracted from the demonstrations.We identify the optimal set of hyperparameters for imitation based on areference motion. Finally, we demonstrate the generalizability of our approachby testing it on six different tasks, corresponding to fingerspelled letters.Our results show that our approach is able to successfully imitate thesefine-grained movements without additional information, highlighting itspotential for real-world applications in robotics.</description><author>Federico Tavella, Aphrodite Galata, Angelo Cangelosi</author><pubDate>Mon, 05 Jun 2023 13:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05135v3</guid></item><item><title>Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models</title><link>http://arxiv.org/abs/2211.09707v2</link><description>Diffusion models have experienced a surge of interest as highly expressiveyet efficiently trainable probabilistic models. We show that these models arean excellent fit for synthesising human motion that co-occurs with audio, e.g.,dancing and co-speech gesticulation, since motion is complex and highlyambiguous given audio, calling for a probabilistic description. Specifically,we adapt the DiffWave architecture to model 3D pose sequences, puttingConformers in place of dilated convolutions for improved modelling power. Wealso demonstrate control over motion style, using classifier-free guidance toadjust the strength of the stylistic expression. Experiments on gesture anddance generation confirm that the proposed method achieves top-of-the-linemotion quality, with distinctive styles whose expression can be made more orless pronounced. We also synthesise path-driven locomotion using the same modelarchitecture. Finally, we generalise the guidance procedure to obtainproduct-of-expert ensembles of diffusion models and demonstrate how these maybe used for, e.g., style interpolation, a contribution we believe is ofindependent interest. Seehttps://www.speech.kth.se/research/listen-denoise-action/ for video examples,data, and code.</description><author>Simon Alexanderson, Rajmund Nagy, Jonas Beskow, Gustav Eje Henter</author><pubDate>Tue, 16 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09707v2</guid></item><item><title>DiVA-360: The Dynamic Visuo-Audio Dataset for Immersive Neural Fields</title><link>http://arxiv.org/abs/2307.16897v1</link><description>Advances in neural fields are enabling high-fidelity capture of the shape andappearance of static and dynamic scenes. However, their capabilities lag behindthose offered by representations such as pixels or meshes due to algorithmicchallenges and the lack of large-scale real-world datasets. We address thedataset limitation with DiVA-360, a real-world 360 dynamic visual-audio datasetwith synchronized multimodal visual, audio, and textual information abouttable-scale scenes. It contains 46 dynamic scenes, 30 static scenes, and 95static objects spanning 11 categories captured using a new hardware systemusing 53 RGB cameras at 120 FPS and 6 microphones for a total of 8.6M imageframes and 1360 s of dynamic data. We provide detailed text descriptions forall scenes, foreground-background segmentation masks, category-specific 3D posealignment for static objects, as well as metrics for comparison. Our data,hardware and software, and code are available at https://diva360.github.io/.</description><author>Cheng-You Lu, Peisen Zhou, Angela Xing, Chandradeep Pokhariya, Arnab Dey, Ishaan Shah, Rugved Mavidipalli, Dylan Hu, Andrew Comport, Kefan Chen, Srinath Sridhar</author><pubDate>Mon, 31 Jul 2023 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16897v1</guid></item><item><title>Pyramid Deep Fusion Network for Two-Hand Reconstruction from RGB-D Images</title><link>http://arxiv.org/abs/2307.06038v1</link><description>Accurately recovering the dense 3D mesh of both hands from monocular imagesposes considerable challenges due to occlusions and projection ambiguity. Mostof the existing methods extract features from color images to estimate theroot-aligned hand meshes, which neglect the crucial depth and scale informationin the real world. Given the noisy sensor measurements with limited resolution,depth-based methods predict 3D keypoints rather than a dense mesh. Theselimitations motivate us to take advantage of these two complementary inputs toacquire dense hand meshes on a real-world scale. In this work, we propose anend-to-end framework for recovering dense meshes for both hands, which employsingle-view RGB-D image pairs as input. The primary challenge lies ineffectively utilizing two different input modalities to mitigate the blurringeffects in RGB images and noises in depth images. Instead of directly treatingdepth maps as additional channels for RGB images, we encode the depthinformation into the unordered point cloud to preserve more geometric details.Specifically, our framework employs ResNet50 and PointNet++ to derive featuresfrom RGB and point cloud, respectively. Additionally, we introduce a novelpyramid deep fusion network (PDFNet) to aggregate features at different scales,which demonstrates superior efficacy compared to previous fusion strategies.Furthermore, we employ a GCN-based decoder to process the fused features andrecover the corresponding 3D pose and dense mesh. Through comprehensiveablation experiments, we have not only demonstrated the effectiveness of ourproposed fusion algorithm but also outperformed the state-of-the-art approacheson publicly available datasets. To reproduce the results, we will make oursource code and models publicly available at{\url{https://github.com/zijinxuxu/PDFNet}}.</description><author>Jinwei Ren, Jianke Zhu</author><pubDate>Wed, 12 Jul 2023 10:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06038v1</guid></item></channel></rss>