<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 14 Apr 2024 06:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Hypergraph-based Multi-View Action Recognition using Event Cameras</title><link>http://arxiv.org/abs/2403.19316v1</link><description>Action recognition from video data forms a cornerstone with wide-rangingapplications. Single-view action recognition faces limitations due to itsreliance on a single viewpoint. In contrast, multi-view approaches capturecomplementary information from various viewpoints for improved accuracy.Recently, event cameras have emerged as innovative bio-inspired sensors,leading to advancements in event-based action recognition. However, existingworks predominantly focus on single-view scenarios, leaving a gap in multi-viewevent data exploitation, particularly in challenges like information deficitand semantic misalignment. To bridge this gap, we introduce HyperMV, amulti-view event-based action recognition framework. HyperMV converts discreteevent data into frame-like representations and extracts view-related featuresusing a shared convolutional network. By treating segments as vertices andconstructing hyperedges using rule-based and KNN-based strategies, a multi-viewhypergraph neural network that captures relationships across viewpoint andtemporal features is established. The vertex attention hypergraph propagationis also introduced for enhanced feature fusion. To prompt research in thisarea, we present the largest multi-view event-based action dataset$\text{THU}^{\text{MV-EACT}}\text{-50}$, comprising 50 actions from 6viewpoints, which surpasses existing datasets by over tenfold. Experimentalresults show that HyperMV significantly outperforms baselines in bothcross-subject and cross-view scenarios, and also exceeds the state-of-the-artsin frame-based multi-view action recognition.</description><author>Yue Gao, Jiaxuan Lu, Siqi Li, Yipeng Li, Shaoyi Du</author><pubDate>Thu, 28 Mar 2024 12:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19316v1</guid></item><item><title>SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition</title><link>http://arxiv.org/abs/2403.09508v1</link><description>Skeleton-based action recognition, which classifies human actions based onthe coordinates of joints and their connectivity within skeleton data, iswidely utilized in various scenarios. While Graph Convolutional Networks (GCNs)have been proposed for skeleton data represented as graphs, they suffer fromlimited receptive fields constrained by joint connectivity. To address thislimitation, recent advancements have introduced transformer-based methods.However, capturing correlations between all joints in all frames requiressubstantial memory resources. To alleviate this, we propose a novel approachcalled Skeletal-Temporal Transformer (SkateFormer) that partitions joints andframes based on different types of skeletal-temporal relation (Skate-Type) andperforms skeletal-temporal self-attention (Skate-MSA) within each partition. Wecategorize the key skeletal-temporal relations for action recognition into atotal of four distinct types. These types combine (i) two skeletal relationtypes based on physically neighboring and distant joints, and (ii) two temporalrelation types based on neighboring and distant frames. Through thispartition-specific attention strategy, our SkateFormer can selectively focus onkey joints and frames crucial for action recognition in an action-adaptivemanner with efficient computation. Extensive experiments on various benchmarkdatasets validate that our SkateFormer outperforms recent state-of-the-artmethods.</description><author>Jeonghyeok Do, Munchurl Kim</author><pubDate>Thu, 14 Mar 2024 16:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09508v1</guid></item><item><title>ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos</title><link>http://arxiv.org/abs/2404.06243v1</link><description>Human action or activity recognition in videos is a fundamental task incomputer vision with applications in surveillance and monitoring, self-drivingcars, sports analytics, human-robot interaction and many more. Traditionalsupervised methods require large annotated datasets for training, which areexpensive and time-consuming to acquire. This work proposes a novel approachusing Cross-Architecture Pseudo-Labeling with contrastive learning forsemi-supervised action recognition. Our framework leverages both labeled andunlabelled data to robustly learn action representations in videos, combiningpseudo-labeling with contrastive learning for effective learning from bothtypes of samples. We introduce a novel cross-architecture approach where 3DConvolutional Neural Networks (3D CNNs) and video transformers (VIT) areutilised to capture different aspects of action representations; hence we callit ActNetFormer. The 3D CNNs excel at capturing spatial features and localdependencies in the temporal domain, while VIT excels at capturing long-rangedependencies across frames. By integrating these complementary architectureswithin the ActNetFormer framework, our approach can effectively capture bothlocal and global contextual information of an action. This comprehensiverepresentation learning enables the model to achieve better performance insemi-supervised action recognition tasks by leveraging the strengths of each ofthese architectures. Experimental results on standard action recognitiondatasets demonstrate that our approach performs better than the existingmethods, achieving state-of-the-art performance with only a fraction of labeleddata. The official website of this work is available at:https://github.com/rana2149/ActNetFormer.</description><author>Sharana Dharshikgan Suresh Dass, Hrishav Bakul Barua, Ganesh Krishnasamy, Raveendran Paramesran, Raphael C. -W. Phan</author><pubDate>Tue, 09 Apr 2024 13:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06243v1</guid></item><item><title>Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in Videos</title><link>http://arxiv.org/abs/2404.07645v1</link><description>Skeleton Action Recognition (SAR) involves identifying human actions usingskeletal joint coordinates and their interconnections. While plain Transformershave been attempted for this task, they still fall short compared to thecurrent leading methods, which are rooted in Graph Convolutional Networks(GCNs) due to the absence of structural priors. Recently, a novel selectivestate space model, Mamba, has surfaced as a compelling alternative to theattention mechanism in Transformers, offering efficient modeling of longsequences. In this work, to the utmost extent of our awareness, we present thefirst SAR framework incorporating Mamba. Each fundamental block of our modeladopts a novel U-ShiftGCN architecture with Mamba as its core component. Theencoder segment of the U-ShiftGCN is devised to extract spatial features fromthe skeletal data using downsampling vanilla Shift S-GCN blocks. These spatialfeatures then undergo intermediate temporal modeling facilitated by the Mambablock before progressing to the encoder section, which comprises vanillaupsampling Shift S-GCN blocks. Additionally, a Shift T-GCN (ShiftTCN) temporalmodeling unit is employed before the exit of each fundamental block to refinetemporal representations. This particular integration of downsampling spatial,intermediate temporal, upsampling spatial, and ultimate temporal subunitsyields promising results for skeleton action recognition. We dub the resultingmodel \textbf{Simba}, which attains state-of-the-art performance across threewell-known benchmark skeleton action recognition datasets: NTU RGB+D, NTU RGB+D120, and Northwestern-UCLA. Interestingly, U-ShiftGCN (Simba withoutIntermediate Mamba Block) by itself is capable of performing reasonably welland surpasses our baseline.</description><author>Soumyabrata Chaudhuri, Saumik Bhattacharya</author><pubDate>Thu, 11 Apr 2024 12:07:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07645v1</guid></item><item><title>SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based Action Recognition System</title><link>http://arxiv.org/abs/2311.08094v2</link><description>Skeleton-based action recognition receives the attention of many researchersas it is robust to viewpoint and illumination changes, and its processing ismuch more efficient than the processing of video frames. With the emergence ofdeep learning models, it has become very popular to represent the skeleton datain pseudo-image form and apply CNN for action recognition. Thereafter, studiesconcentrated on finding effective methods for forming pseudo-images. Recently,attention networks, more specifically transformers have provided promisingresults in various vision problems. In this study, the effectiveness of VIT forskeleton-based action recognition is examined and its robustness on thepseudo-image representation scheme is investigated. To this end, a three-levelarchitecture, SkelVit is proposed, which forms a set of pseudo images, appliesa classifier on each of the representations, and combines their results to findthe final action class. The performance of SkelVit is examined thoroughly via aset of experiments. First, the sensitivity of the system to representation isinvestigated by comparing it with two of the state-of-the-art pseudo-imagerepresentation methods. Then, the classifiers of SkelVit are realized in twoexperimental setups by CNNs and VITs, and their performances are compared. Inthe final experimental setup, the contribution of combining classifiers isexamined by applying the model with a different number of classifiers.Experimental studies reveal that the proposed system with its lightweightrepresentation scheme achieves better results than the state-of-the-artmethods. It is also observed that the vision transformer is less sensitive tothe initial pseudo-image representation compared to CNN. Nevertheless, evenwith the vision transformer, the recognition performance can be furtherimproved by the consensus of classifiers.</description><author>Ozge Oztimur Karadag</author><pubDate>Thu, 07 Mar 2024 07:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08094v2</guid></item><item><title>Emotion Recognition from the perspective of Activity Recognition</title><link>http://arxiv.org/abs/2403.16263v1</link><description>Applications of an efficient emotion recognition system can be found inseveral domains such as medicine, driver fatigue surveillance, social robotics,and human-computer interaction. Appraising human emotional states, behaviors,and reactions displayed in real-world settings can be accomplished using latentcontinuous dimensions. Continuous dimensional models of human affect, such asthose based on valence and arousal are more accurate in describing a broadrange of spontaneous everyday emotions than more traditional models of discretestereotypical emotion categories (e.g. happiness, surprise). Most of the priorwork on estimating valence and arousal considers laboratory settings and acteddata. But, for emotion recognition systems to be deployed and integrated intoreal-world mobile and computing devices, we need to consider data collected inthe world. Action recognition is a domain of Computer Vision that involvescapturing complementary information on appearance from still frames and motionbetween frames. In this paper, we treat emotion recognition from theperspective of action recognition by exploring the application of deep learningarchitectures specifically designed for action recognition, for continuousaffect recognition. We propose a novel three-stream end-to-end deep learningregression pipeline with an attention mechanism, which is an ensemble designbased on sub-modules of multiple state-of-the-art action recognition systems.The pipeline constitutes a novel data pre-processing approach with a spatialself-attention mechanism to extract keyframes. The optical flow ofhigh-attention regions of the face is extracted to capture temporal context.AFEW-VA in-the-wild dataset has been used to conduct comparative experiments.Quantitative analysis shows that the proposed model outperforms multiplestandard baselines of both emotion recognition and action recognition models.</description><author>Savinay Nagendra, Prapti Panigrahi</author><pubDate>Sun, 24 Mar 2024 19:53:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16263v1</guid></item><item><title>Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2402.12706v1</link><description>Few-shot action recognition aims at quickly adapting a pre-trained model tothe novel data with a distribution shift using only a limited number ofsamples. Key challenges include how to identify and leverage the transferableknowledge learned by the pre-trained model. Our central hypothesis is thattemporal invariance in the dynamic system between latent variables lends itselfto transferability (domain-invariance). We therefore propose DITeD, orDomain-Invariant Temporal Dynamics for knowledge transfer. To detect thetemporal invariance part, we propose a generative framework with a two-stagetraining strategy during pre-training. Specifically, we explicitly modelinvariant dynamics including temporal dynamic generation and transitions, andthe variant visual and domain encoders. Then we pre-train the model with theself-supervised signals to learn the representation. After that, we fix thewhole representation model and tune the classifier. During adaptation, we fixthe transferable temporal dynamics and update the image encoder. The efficacyof our approach is revealed by the superior accuracy of DITeD over leadingalternatives across standard few-shot action recognition datasets. Moreover, wevalidate that the learned temporal dynamic transition and temporal dynamicgeneration modules possess transferable qualities.</description><author>Yuke Li, Guangyi Chen, Ben Abramowitz, Stefano Anzellott, Donglai Wei</author><pubDate>Tue, 20 Feb 2024 04:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12706v1</guid></item><item><title>Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition</title><link>http://arxiv.org/abs/2404.07487v1</link><description>Skeleton-based zero-shot action recognition aims to recognize unknown humanactions based on the learned priors of the known skeleton-based actions and asemantic descriptor space shared by both known and unknown categories. However,previous works focus on establishing the bridges between the known skeletonrepresentation space and semantic descriptions space at the coarse-grainedlevel for recognizing unknown action categories, ignoring the fine-grainedalignment of these two spaces, resulting in suboptimal performance indistinguishing high-similarity action categories. To address these challenges,we propose a novel method via Side information and dual-prompts learning forskeleton-based zero-shot action recognition (STAR) at the fine-grained level.Specifically, 1) we decompose the skeleton into several parts based on itstopology structure and introduce the side information concerning multi-partdescriptions of human body movements for alignment between the skeleton and thesemantic space at the fine-grained level; 2) we design the visual-attribute andsemantic-part prompts to improve the intra-class compactness within theskeleton space and inter-class separability within the semantic space,respectively, to distinguish the high-similarity actions. Extensive experimentsshow that our method achieves state-of-the-art performance in ZSL and GZSLsettings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets.</description><author>Yang Chen, Jingcai Guo, Tian He, Ling Wang</author><pubDate>Thu, 11 Apr 2024 06:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07487v1</guid></item><item><title>Benchmarking Micro-action Recognition: Dataset, Methods, and Applications</title><link>http://arxiv.org/abs/2403.05234v1</link><description>Micro-action is an imperceptible non-verbal behaviour characterised bylow-intensity movement. It offers insights into the feelings and intentions ofindividuals and is important for human-oriented applications such as emotionrecognition and psychological assessment. However, the identification,differentiation, and understanding of micro-actions pose challenges due to theimperceptible and inaccessible nature of these subtle human behaviors ineveryday life. In this study, we innovatively collect a new micro-actiondataset designated as Micro-action-52 (MA-52), and propose a benchmark namedmicro-action network (MANet) for micro-action recognition (MAR) task. Uniquely,MA-52 provides the whole-body perspective including gestures, upper- andlower-limb movements, attempting to reveal comprehensive micro-action cues. Indetail, MA-52 contains 52 micro-action categories along with seven body partlabels, and encompasses a full array of realistic and natural micro-actions,accounting for 205 participants and 22,422 video instances collated from thepsychological interviews. Based on the proposed dataset, we assess MANet andother nine prevalent action recognition methods. MANet incorporates squeeze-andexcitation (SE) and temporal shift module (TSM) into the ResNet architecturefor modeling the spatiotemporal characteristics of micro-actions. Then ajoint-embedding loss is designed for semantic matching between video and actionlabels; the loss is used to better distinguish between visually similar yetdistinct micro-action categories. The extended application in emotionrecognition has demonstrated one of the important values of our proposeddataset and method. In the future, further exploration of human behaviour,emotion, and psychological assessment will be conducted in depth. The datasetand source code are released at https://github.com/VUT-HFUT/Micro-Action.</description><author>Dan Guo, Kun Li, Bin Hu, Yan Zhang, Meng Wang</author><pubDate>Fri, 08 Mar 2024 11:48:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05234v1</guid></item><item><title>Selective, Interpretable, and Motion Consistent Privacy Attribute Obfuscation for Action Recognition</title><link>http://arxiv.org/abs/2403.12710v1</link><description>Concerns for the privacy of individuals captured in public imagery have ledto privacy-preserving action recognition. Existing approaches often suffer fromissues arising through obfuscation being applied globally and a lack ofinterpretability. Global obfuscation hides privacy sensitive regions, but alsocontextual regions important for action recognition. Lack of interpretabilityerodes trust in these new technologies. We highlight the limitations of currentparadigms and propose a solution: Human selected privacy templates that yieldinterpretability by design, an obfuscation scheme that selectively hidesattributes and also induces temporal consistency, which is important in actionrecognition. Our approach is architecture agnostic and directly modifies inputimagery, while existing approaches generally require architecture training. Ourapproach offers more flexibility, as no retraining is required, and outperformsalternatives on three widely used datasets.</description><author>Filip Ilic, He Zhao, Thomas Pock, Richard P. Wildes</author><pubDate>Tue, 19 Mar 2024 14:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12710v1</guid></item><item><title>On the Utility of 3D Hand Poses for Action Recognition</title><link>http://arxiv.org/abs/2403.09805v1</link><description>3D hand poses are an under-explored modality for action recognition. Posesare compact yet informative and can greatly benefit applications with limitedcompute budgets. However, poses alone offer an incomplete understanding ofactions, as they cannot fully capture objects and environments with whichhumans interact. To efficiently model hand-object interactions, we proposeHandFormer, a novel multimodal transformer. HandFormer combines 3D hand posesat a high temporal resolution for fine-grained motion modeling with sparselysampled RGB frames for encoding scene semantics. Observing the uniquecharacteristics of hand poses, we temporally factorize hand modeling andrepresent each joint by its short-term trajectories. This factorized poserepresentation combined with sparse RGB samples is remarkably efficient andachieves high accuracy. Unimodal HandFormer with only hand poses outperformsexisting skeleton-based methods at 5x fewer FLOPs. With RGB, we achieve newstate-of-the-art performance on Assembly101 and H2O with significantimprovements in egocentric action recognition.</description><author>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</author><pubDate>Thu, 14 Mar 2024 19:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09805v1</guid></item><item><title>Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines</title><link>http://arxiv.org/abs/2403.09056v1</link><description>On modern industrial assembly lines, many intelligent algorithms have beendeveloped to replace or supervise workers. However, we found that there werebottlenecks in both training datasets and real-time performance when deployingalgorithms on actual assembly line. Therefore, we developed a promisingstrategy for expanding industrial datasets, which utilized large models withstrong generalization abilities to achieve efficient, high-quality, andlarge-scale dataset expansion, solving the problem of insufficient andlow-quality industrial datasets. We also applied this strategy to video actionrecognition. We proposed a method of converting hand action recognitionproblems into hand skeletal trajectory classification problems, which solvedthe real-time performance problem of industrial algorithms. In the "handmovements during wire insertion" scenarios on the actual assembly line, theaccuracy of hand action recognition reached 98.8\%. We conducted detailedexperimental analysis to demonstrate the effectiveness and superiority of themethod, and deployed the entire process on Midea's actual assembly line.</description><author>Liang Wu, X. -G. Ma</author><pubDate>Thu, 14 Mar 2024 03:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09056v1</guid></item><item><title>TIM: A Time Interval Machine for Audio-Visual Action Recognition</title><link>http://arxiv.org/abs/2404.05559v1</link><description>Diverse actions give rise to rich audio-visual signals in long videos. Recentworks showcase that the two modalities of audio and video exhibit differenttemporal extents of events and distinct labels. We address the interplaybetween the two modalities in long videos by explicitly modelling the temporalextents of audio and visual events. We propose the Time Interval Machine (TIM)where a modality-specific time interval poses as a query to a transformerencoder that ingests a long video input. The encoder then attends to thespecified interval, as well as the surrounding context in both modalities, inorder to recognise the ongoing action. We test TIM on three long audio-visual video datasets: EPIC-KITCHENS,Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. OnEPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantlylarger pre-training by 2.9% top-1 action recognition accuracy. Additionally, weshow that TIM can be adapted for action detection, using dense multi-scaleinterval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, andshowing strong performance on the Perception Test. Our ablations show thecritical role of integrating the two modalities and modelling their timeintervals in achieving this performance. Code and models at:https://github.com/JacobChalk/TIM</description><author>Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, Dima Damen</author><pubDate>Mon, 08 Apr 2024 15:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05559v1</guid></item><item><title>TIM: A Time Interval Machine for Audio-Visual Action Recognition</title><link>http://arxiv.org/abs/2404.05559v2</link><description>Diverse actions give rise to rich audio-visual signals in long videos. Recentworks showcase that the two modalities of audio and video exhibit differenttemporal extents of events and distinct labels. We address the interplaybetween the two modalities in long videos by explicitly modelling the temporalextents of audio and visual events. We propose the Time Interval Machine (TIM)where a modality-specific time interval poses as a query to a transformerencoder that ingests a long video input. The encoder then attends to thespecified interval, as well as the surrounding context in both modalities, inorder to recognise the ongoing action. We test TIM on three long audio-visual video datasets: EPIC-KITCHENS,Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. OnEPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantlylarger pre-training by 2.9% top-1 action recognition accuracy. Additionally, weshow that TIM can be adapted for action detection, using dense multi-scaleinterval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, andshowing strong performance on the Perception Test. Our ablations show thecritical role of integrating the two modalities and modelling their timeintervals in achieving this performance. Code and models at:https://github.com/JacobChalk/TIM</description><author>Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, Dima Damen</author><pubDate>Tue, 09 Apr 2024 08:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05559v2</guid></item><item><title>GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2403.15212v1</link><description>Skeleton-based action recognition (SAR) in videos is an important butchallenging task in computer vision. The recent state-of-the-art models for SARare primarily based on graph convolutional neural networks (GCNs), which arepowerful in extracting the spatial information of skeleton data. However, it isyet clear that such GCN-based models can effectively capture the temporaldynamics of human action sequences. To this end, we propose the DevLSTM module,which exploits the path development -- a principled and parsimoniousrepresentation for sequential data by leveraging the Lie group structure. Thepath development, originated from Rough path theory, can effectively capturethe order of events in high-dimensional stream data with massive dimensionreduction and consequently enhance the LSTM module substantially. Our proposedG-DevLSTM module can be conveniently plugged into the temporal graph,complementing existing advanced GCN-based models. Our empirical studies on theNTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybridmodel significantly outperforms the current best-performing methods in SARtasks. The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM.</description><author>Lei Jiang, Weixin Yang, Xin Zhang, Hao Ni</author><pubDate>Fri, 22 Mar 2024 14:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15212v1</guid></item><item><title>Skeleton-Based Human Action Recognition with Noisy Labels</title><link>http://arxiv.org/abs/2403.09975v1</link><description>Understanding human actions from body poses is critical for assistive robotssharing space with humans in order to make informed and safe decisions aboutthe next interaction. However, precise temporal localization and annotation ofactivity sequences is time-consuming and the resulting labels are often noisy.If not effectively addressed, label noise negatively affects the model'straining, resulting in lower recognition quality. Despite its importance,addressing label noise for skeleton-based action recognition has beenoverlooked so far. In this study, we bridge this gap by implementing aframework that augments well-established skeleton-based human actionrecognition methods with label-denoising strategies from various research areasto serve as the initial benchmark. Observations reveal that these baselinesyield only marginal performance when dealing with sparse skeleton data.Consequently, we introduce a novel methodology, NoiseEraSAR, which integratesglobal sample selection, co-teaching, and Cross-Modal Mixture-of-Experts(CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise.Our proposed approach demonstrates better performance on the establishedbenchmark, setting new state-of-the-art standards. The source code for thisstudy will be made accessible at https://github.com/xuyizdby/NoiseEraSAR.</description><author>Yi Xu, Kunyu Peng, Di Wen, Ruiping Liu, Junwei Zheng, Yufan Chen, Jiaming Zhang, Alina Roitberg, Kailun Yang, Rainer Stiefelhagen</author><pubDate>Fri, 15 Mar 2024 03:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09975v1</guid></item><item><title>Deep Learning Approaches for Human Action Recognition in Video Data</title><link>http://arxiv.org/abs/2403.06810v1</link><description>Human action recognition in videos is a critical task with significantimplications for numerous applications, including surveillance, sportsanalytics, and healthcare. The challenge lies in creating models that are bothprecise in their recognition capabilities and efficient enough for practicaluse. This study conducts an in-depth analysis of various deep learning modelsto address this challenge. Utilizing a subset of the UCF101 Videos dataset, wefocus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks(RNNs), and Two-Stream ConvNets. The research reveals that while CNNseffectively capture spatial features and RNNs encode temporal sequences,Two-Stream ConvNets exhibit superior performance by integrating spatial andtemporal dimensions. These insights are distilled from the evaluation metricsof accuracy, precision, recall, and F1-score. The results of this studyunderscore the potential of composite models in achieving robust human actionrecognition and suggest avenues for future research in optimizing these modelsfor real-world deployment.</description><author>Yufei Xie</author><pubDate>Mon, 11 Mar 2024 16:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06810v1</guid></item><item><title>ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More</title><link>http://arxiv.org/abs/2403.12534v1</link><description>Event cameras have recently been shown beneficial for practical vision tasks,such as action recognition, thanks to their high temporal resolution, powerefficiency, and reduced privacy concerns. However, current research is hinderedby 1) the difficulty in processing events because of their prolonged durationand dynamic actions with complex and ambiguous semantics and 2) the redundantaction depiction of the event frame representation with fixed stacks. We findlanguage naturally conveys abundant semantic information, rendering itstunningly superior in reducing semantic uncertainty. In light of this, wepropose ExACT, a novel approach that, for the first time, tackles event-basedaction recognition from a cross-modal conceptualizing perspective. Our ExACTbrings two technical contributions. Firstly, we propose an adaptivefine-grained event (AFE) representation to adaptively filter out the repeatedevents for the stationary objects while preserving dynamic ones. This subtlyenhances the performance of ExACT without extra computational cost. Then, wepropose a conceptual reasoning-based uncertainty estimation module, whichsimulates the recognition process to enrich the semantic representation. Inparticular, conceptual reasoning builds the temporal relation based on theaction semantics, and uncertainty estimation tackles the semantic uncertaintyof actions based on the distributional representation. Experiments show thatour ExACT achieves superior recognition accuracy of 94.83%(+2.23%),90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively.</description><author>Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, Lin Wang</author><pubDate>Tue, 19 Mar 2024 09:15:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12534v1</guid></item><item><title>Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating</title><link>http://arxiv.org/abs/2307.02730v3</link><description>The fine-grained action analysis of the existing action datasets ischallenged by insufficient action categories, low fine granularities, limitedmodalities, and tasks. In this paper, we propose a Multi-modality andMulti-task dataset of Figure Skating (MMFS) which was collected from the WorldFigure Skating Championships. MMFS, which possesses action recognition andaction quality assessment, captures RGB, skeleton, and is collected the scoreof actions from 11671 clips with 256 categories including spatial and temporallabels. The key contributions of our dataset fall into three aspects asfollows. (1) Independently spatial and temporal categories are first proposedto further explore fine-grained action recognition and quality assessment. (2)MMFS first introduces the skeleton modality for complex fine-grained actionquality assessment. (3) Our multi-modality and multi-task dataset encouragemore action analysis models. To benchmark our dataset, we adopt RGB-based andskeleton-based baseline methods for action recognition and action qualityassessment.</description><author>Sheng-Lan Liu, Yu-Ning Ding, Gang Yan, Si-Fan Zhang, Jin-Rong Zhang, Wen-Yue Chen, Xue-Hai Xu</author><pubDate>Tue, 09 Apr 2024 14:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02730v3</guid></item><item><title>SiT-MLP: A Simple MLP with Point-wise Topology Feature Learning for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v4</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, previous GCN-based methods rely onelaborate human priors excessively and construct complex feature aggregationmechanisms, which limits the generalizability and effectiveness of networks. Tosolve these problems, we propose a novel Spatial Topology Gating Unit (STGU),an MLP-based variant without extra priors, to capture the co-occurrencetopology features that encode the spatial dependency across all joints. InSTGU, to learn the point-wise topology features, a new gate-based featureinteraction mechanism is introduced to activate the features point-to-point bythe attention map generated from the input sample. Based on the STGU, wepropose the first MLP-based model, SiT-MLP, for skeleton-based actionrecognition in this work. Compared with previous methods on three large-scaledatasets, SiT-MLP achieves competitive performance. In addition, SiT-MLPreduces the parameters significantly with favorable results. The code will beavailable at https://github.com/BUPTSJZhang/SiT?MLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Mon, 08 Apr 2024 15:09:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v4</guid></item><item><title>CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner</title><link>http://arxiv.org/abs/2403.10082v1</link><description>Most existing one-shot skeleton-based action recognition focuses on rawlow-level information (e.g., joint location), and may suffer from localinformation loss and low generalization ability. To alleviate these, we proposeto leverage text description generated from large language models (LLM) thatcontain high-level human knowledge, to guide feature learning, in aglobal-local-global way. Particularly, during training, we design $2$ promptsto gain global and local text descriptions of each action from an LLM. We firstutilize the global text description to guide the skeleton encoder focus oninformative joints (i.e.,global-to-local). Then we build non-local interactionbetween local text and joint features, to form the final global representation(i.e., local-to-global). To mitigate the asymmetry issue between the trainingand inference phases, we further design a dual-branch architecture that allowsthe model to perform novel class inference without any text input, also makingthe additional inference cost neglectable compared with the base skeletonencoder. Extensive experiments on three different benchmarks show that CrossGLGconsistently outperforms the existing SOTA methods with large margins, and theinference cost (model size) is only $2.8$\% than the previous SOTA. CrossGLGcan also serve as a plug-and-play module that can substantially enhance theperformance of different SOTA skeleton encoders with a neglectable cost duringinference. The source code will be released soon.</description><author>Tingbing Yan, Wenzheng Zeng, Yang Xiao, Xingyu Tong, Bo Tan, Zhiwen Fang, Zhiguo Cao, Joey Tianyi Zhou</author><pubDate>Fri, 15 Mar 2024 08:51:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10082v1</guid></item><item><title>Leveraging YOLO-World and GPT-4V LMMs for Zero-Shot Person Detection and Action Recognition in Drone Imagery</title><link>http://arxiv.org/abs/2404.01571v1</link><description>In this article, we explore the potential of zero-shot Large MultimodalModels (LMMs) in the domain of drone perception. We focus on person detectionand action recognition tasks and evaluate two prominent LMMs, namely YOLO-Worldand GPT-4V(ision) using a publicly available dataset captured from aerialviews. Traditional deep learning approaches rely heavily on large andhigh-quality training datasets. However, in certain robotic settings, acquiringsuch datasets can be resource-intensive or impractical within a reasonabletimeframe. The flexibility of prompt-based Large Multimodal Models (LMMs) andtheir exceptional generalization capabilities have the potential torevolutionize robotics applications in these scenarios. Our findings suggestthat YOLO-World demonstrates good detection performance. GPT-4V struggles withaccurately classifying action classes but delivers promising results infiltering out unwanted region proposals and in providing a general descriptionof the scenery. This research represents an initial step in leveraging LMMs fordrone perception and establishes a foundation for future investigations in thisarea.</description><author>Christian Limberg, Artur Gon√ßalves, Bastien Rigault, Helmut Prendinger</author><pubDate>Tue, 02 Apr 2024 03:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01571v1</guid></item><item><title>TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition</title><link>http://arxiv.org/abs/2402.08875v1</link><description>The increasing variety and quantity of tagged multimedia content on platformssuch as TikTok provides an opportunity to advance computer vision modeling. Wehave curated a distinctive dataset of 283,582 unique video clips categorizedunder 386 hashtags relating to modern human actions. We release this dataset asa valuable resource for building domain-specific foundation models for humanmovement modeling tasks such as action recognition. To validate this dataset,which we name TikTokActions, we perform two sets of experiments. First, wepretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone onTikTokActions subset, and then fine-tune and evaluate on popular datasets suchas UCF101 and the HMDB51. We find that the performance of the model pre-trainedusing our Tik-Tok dataset is comparable to models trained on larger actionrecognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, ourinvestigation into the relationship between pre-training dataset size andfine-tuning performance reveals that beyond a certain threshold, theincremental benefit of larger training sets diminishes. This work introduces auseful TikTok video dataset that is available for public use and providesinsights into the marginal benefit of increasing pre-training dataset sizes forvideo-based foundation models.</description><author>Yang Qian, Yinan Sun, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Pingyi Chen, Zain Jabbar, Dennis Paul Wall, Peter Washington</author><pubDate>Wed, 14 Feb 2024 00:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08875v1</guid></item><item><title>Low-Cost and Real-Time Industrial Human Action Recognitions Based on Large-Scale Foundation Models</title><link>http://arxiv.org/abs/2403.08420v1</link><description>Industrial managements, including quality control, cost and safetyoptimization, etc., heavily rely on high quality industrial human actionrecognitions (IHARs) which were hard to be implemented in large-scaleindustrial scenes due to their high costs and poor real-time performance. Inthis paper, we proposed a large-scale foundation model(LSFM)-based IHAR method,wherein various LSFMs and lightweight methods were jointly used, for the firsttime, to fulfill low-cost dataset establishment and real-time IHARs.Comprehensive tests on in-situ large-scale industrial manufacturing lineselucidated that the proposed method realized great reduction on employmentcosts, superior real-time performance, and satisfactory accuracy andgeneralization capabilities, indicating its great potential as a backbone IHARmethod, especially for large-scale industrial applications.</description><author>Wensheng Liang, Ruiyan Zhuang, Xianwei Shi, Shuai Li, Zhicheng Wang, Xiaoguang Ma</author><pubDate>Wed, 13 Mar 2024 12:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08420v1</guid></item><item><title>Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2404.02624v1</link><description>Skeleton-based gesture recognition methods have achieved high success usingGraph Convolutional Network (GCN). In addition, context-dependent adaptivetopology as a neighborhood vertex information and attention mechanism leveragesa model to better represent actions. In this paper, we propose self-attentionGCN hybrid model, Multi-Scale Spatial-Temporal self-attention (MSST)-GCN toeffectively improve modeling ability to achieve state-of-the-art results onseveral datasets. We utilize spatial self-attention module with adaptivetopology to understand intra-frame interactions within a frame among differentbody parts, and temporal self-attention module to examine correlations betweenframes of a node. These two are followed by multi-scale convolution networkwith dilations, which not only captures the long-range temporal dependencies ofjoints but also the long-range spatial dependencies (i.e., long-distancedependencies) of node temporal behaviors. They are combined into high-levelspatial-temporal representations and output the predicted action with thesoftmax classifier.</description><author>Ikuo Nakamura</author><pubDate>Wed, 03 Apr 2024 11:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02624v1</guid></item><item><title>Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition</title><link>http://arxiv.org/abs/2311.15619v2</link><description>Large-scale visual-language pre-trained models have achieved significantsuccess in various video tasks. However, most existing methods follow an "adaptthen align" paradigm, which adapts pre-trained image encoders to modelvideo-level representations and utilizes one-hot or text embedding of theaction labels for supervision. This paradigm overlooks the challenge of mappingfrom static images to complicated activity concepts. In this paper, we proposea novel "Align before Adapt" (ALT) paradigm. Prior to adapting to videorepresentation learning, we exploit the entity-to-region alignments for eachframe. The alignments are fulfilled by matching the region-aware imageembeddings to an offline-constructed text corpus. With the aligned entities, wefeed their text embeddings to a transformer-based video adapter as the queries,which can help extract the semantics of the most important entities from avideo to a vector. This paradigm reuses the visual-language alignment of VLPduring adaptation and tries to explain an action by the underlying entities.This helps understand actions by bridging the gap with complex activitysemantics, particularly when facing unfamiliar or unseen categories. ALTdemonstrates competitive performance while maintaining remarkably lowcomputational costs. In fully supervised experiments, it achieves 88.1% top-1accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms theprevious state-of-the-art methods in both zero-shot and few-shot experiments,emphasizing its superior generalizability across various learning scenarios.</description><author>Yifei Chen, Dapeng Chen, Ruijin Liu, Sai Zhou, Wenyuan Xue, Wei Peng</author><pubDate>Tue, 19 Mar 2024 18:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15619v2</guid></item><item><title>MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable Trajectory Generation</title><link>http://arxiv.org/abs/2311.08393v3</link><description>The learn-from-observation (LfO) paradigm is a human-inspired mode for arobot to learn to perform a task simply by watching it being performed. LfO canfacilitate robot integration on factory floors by minimizing disruption andreducing tedious programming. A key component of the LfO pipeline is atransformation of the depth camera frames to the corresponding task state andaction pairs, which are then relayed to learning techniques such as imitationor inverse reinforcement learning for understanding the task parameters. Whileseveral existing computer vision models analyze videos for activityrecognition, SA-Net specifically targets robotic LfO from RGB-D data. However,SA-Net and many other models analyze frame data captured from a singleviewpoint. Their analysis is therefore highly sensitive to occlusions of theobserved task, which are frequent in deployments. An obvious way of reducingocclusions is to simultaneously observe the task from multiple viewpoints andsynchronously fuse the multiple streams in the model. Toward this, we presentmulti-view SA-Net, which generalizes the SA-Net model to allow the perceptionof multiple viewpoints of the task activity, integrate them, and betterrecognize the state and action in each frame. Performance evaluations on twodistinct domains establish that MVSA-Net recognizes the state-action pairsunder occlusion more accurately compared to single-view MVSA-Net and otherbaselines. Our ablation studies further evaluate its performance underdifferent ambient conditions and establish the contribution of the architecturecomponents. As such, MVSA-Net offers a significantly more robust and deployablestate-action trajectory generation compared to previous methods.</description><author>Ehsan Asali, Prashant Doshi, Jin Sun</author><pubDate>Mon, 08 Apr 2024 03:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08393v3</guid></item><item><title>Density-Guided Label Smoothing for Temporal Localization of Driving Actions</title><link>http://arxiv.org/abs/2403.06616v1</link><description>Temporal localization of driving actions plays a crucial role in advanceddriver-assistance systems and naturalistic driving studies. However, this is achallenging task due to strict requirements for robustness, reliability andaccurate localization. In this work, we focus on improving the overallperformance by efficiently utilizing video action recognition networks andadapting these to the problem of action localization. To this end, we firstdevelop a density-guided label smoothing technique based on label probabilitydistributions to facilitate better learning from boundary video-segments thattypically include multiple labels. Second, we design a post-processing step toefficiently fuse information from video-segments and multiple camera views intoscene-level predictions, which facilitates elimination of false positives. Ourmethodology yields a competitive performance on the A2 test set of thenaturalistic driving action recognition track of the 2022 NVIDIA AI CityChallenge with an F1 score of 0.271.</description><author>Tunc Alkanat, Erkut Akdag, Egor Bondarev, Peter H. N. De With</author><pubDate>Mon, 11 Mar 2024 12:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06616v1</guid></item><item><title>Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition</title><link>http://arxiv.org/abs/2204.07935v2</link><description>Subject-invariant facial action unit (AU) recognition remains challenging forthe reason that the data distribution varies among subjects. In this paper, wepropose a causal inference framework for subject-invariant facial action unitrecognition. To illustrate the causal effect existing in AU recognition task,we formulate the causalities among facial images, subjects, latent AU semanticrelations, and estimated AU occurrence probabilities via a structural causalmodel. By constructing such a causal diagram, we clarify the causal effectamong variables and propose a plug-in causal intervention module, CIS, todeconfound the confounder \emph{Subject} in the causal diagram. Extensiveexperiments conducted on two commonly used AU benchmark datasets, BP4D andDISFA, show the effectiveness of our CIS, and the model with CIS inserted,CISNet, has achieved state-of-the-art performance.</description><author>Yingjie Chen, Diqi Chen, Tao Wang, Yizhou Wang, Yun Liang</author><pubDate>Wed, 03 Apr 2024 03:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.07935v2</guid></item><item><title>Multi-View Video-Based Learning: Leveraging Weak Labels for Frame-Level Perception</title><link>http://arxiv.org/abs/2403.11616v1</link><description>For training a video-based action recognition model that accepts multi-viewvideo, annotating frame-level labels is tedious and difficult. However, it isrelatively easy to annotate sequence-level labels. This kind of coarseannotations are called as weak labels. However, training a multi-viewvideo-based action recognition model with weak labels for frame-levelperception is challenging. In this paper, we propose a novel learningframework, where the weak labels are first used to train a multi-viewvideo-based base model, which is subsequently used for downstream frame-levelperception tasks. The base model is trained to obtain individual latentembeddings for each view in the multi-view input. For training the model usingthe weak labels, we propose a novel latent loss function. We also propose amodel that uses the view-specific latent embeddings for downstream frame-levelaction recognition and detection tasks. The proposed framework is evaluatedusing the MM Office dataset by comparing several baseline algorithms. Theresults show that the proposed base model is effectively trained using weaklabels and the latent embeddings help the downstream models improve accuracy.</description><author>Vijay John, Yasutomo Kawanishi</author><pubDate>Mon, 18 Mar 2024 10:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11616v1</guid></item><item><title>Multi-View Video-Based Learning: Leveraging Weak Labels for Frame-Level Perception</title><link>http://arxiv.org/abs/2403.11616v2</link><description>For training a video-based action recognition model that accepts multi-viewvideo, annotating frame-level labels is tedious and difficult. However, it isrelatively easy to annotate sequence-level labels. This kind of coarseannotations are called as weak labels. However, training a multi-viewvideo-based action recognition model with weak labels for frame-levelperception is challenging. In this paper, we propose a novel learningframework, where the weak labels are first used to train a multi-viewvideo-based base model, which is subsequently used for downstream frame-levelperception tasks. The base model is trained to obtain individual latentembeddings for each view in the multi-view input. For training the model usingthe weak labels, we propose a novel latent loss function. We also propose amodel that uses the view-specific latent embeddings for downstream frame-levelaction recognition and detection tasks. The proposed framework is evaluatedusing the MM Office dataset by comparing several baseline algorithms. Theresults show that the proposed base model is effectively trained using weaklabels and the latent embeddings help the downstream models improve accuracy.</description><author>Vijay John, Yasutomo Kawanishi</author><pubDate>Tue, 19 Mar 2024 06:49:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11616v2</guid></item><item><title>Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition</title><link>http://arxiv.org/abs/2404.06443v1</link><description>Human facial action units (AUs) are mutually related in a hierarchicalmanner, as not only they are associated with each other in both spatial andtemporal domains but also AUs located in the same/close facial regions showstronger relationships than those of different facial regions. While none ofexisting approach thoroughly model such hierarchical inter-dependencies amongAUs, this paper proposes to comprehensively model multi-scale AU-relateddynamic and hierarchical spatio-temporal relationship among AUs for theiroccurrences recognition. Specifically, we first propose a novel multi-scaletemporal differencing network with an adaptive weighting block to explicitlycapture facial dynamics across frames at different spatial scales, whichspecifically considers the heterogeneity of range and magnitude in differentAUs' activation. Then, a two-stage strategy is introduced to hierarchicallymodel the relationship among AUs based on their spatial distribution (i.e.,local and cross-region AU relationship modelling). Experimental resultsachieved on BP4D and DISFA show that our approach is the new state-of-the-artin the field of AU occurrence recognition. Our code is publicly available athttps://github.com/CVI-SZU/MDHR.</description><author>Zihan Wang, Siyang Song, Cheng Luo, Songhe Deng, Weicheng Xie, Linlin Shen</author><pubDate>Tue, 09 Apr 2024 17:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06443v1</guid></item><item><title>Human Goal Recognition as Bayesian Inference: Investigating the Impact of Actions, Timing, and Goal Solvability</title><link>http://arxiv.org/abs/2402.10510v1</link><description>Goal recognition is a fundamental cognitive process that enables individualsto infer intentions based on available cues. Current goal recognitionalgorithms often take only observed actions as input, but here we use aBayesian framework to explore the role of actions, timing, and goal solvabilityin goal recognition. We analyze human responses to goal-recognition problems inthe Sokoban domain, and find that actions are assigned most importance, butthat timing and solvability also influence goal recognition in some cases,especially when actions are uninformative. We leverage these findings todevelop a goal recognition model that matches human inferences more closelythan do existing algorithms. Our work provides new insight into human goalrecognition and takes a step towards more human-like AI models.</description><author>Chenyuan Zhang, Charles Kemp, Nir Lipovetzky</author><pubDate>Fri, 16 Feb 2024 08:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10510v1</guid></item><item><title>3DInAction: Understanding Human Actions in 3D Point Clouds</title><link>http://arxiv.org/abs/2303.06346v2</link><description>We propose a novel method for 3D point cloud action recognition.Understanding human actions in RGB videos has been widely studied in recentyears, however, its 3D point cloud counterpart remains under-explored. This ismostly due to the inherent limitation of the point cloud data modality -- lackof structure, permutation invariance, and varying number of points -- whichmakes it difficult to learn a spatio-temporal representation. To address thislimitation, we propose the 3DinAction pipeline that first estimates patchesmoving in time (t-patches) as a key building block, alongside a hierarchicalarchitecture that learns an informative spatio-temporal representation. We showthat our method achieves improved performance on existing datasets, includingDFAUST and IKEA ASM. Code is publicly available athttps://github.com/sitzikbs/3dincaction.</description><author>Yizhak Ben-Shabat, Oren Shrout, Stephen Gould</author><pubDate>Fri, 29 Mar 2024 16:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06346v2</guid></item><item><title>PREGO: online mistake detection in PRocedural EGOcentric videos</title><link>http://arxiv.org/abs/2404.01933v1</link><description>Promptly identifying procedural errors from egocentric videos in an onlinesetting is highly challenging and valuable for detecting mistakes as soon asthey happen. This capability has a wide range of applications across variousfields, such as manufacturing and healthcare. The nature of procedural mistakesis open-set since novel types of failures might occur, which calls forone-class classifiers trained on correctly executed procedures. However, notechnique can currently detect open-set procedural mistakes online. We proposePREGO, the first online one-class classification model for mistake detection inPRocedural EGOcentric videos. PREGO is based on an online action recognitioncomponent to model the current action, and a symbolic reasoning module topredict the next actions. Mistake detection is performed by comparing therecognized current action with the expected future one. We evaluate PREGO ontwo procedural egocentric video datasets, Assembly101 and Epic-tent, which weadapt for online benchmarking of procedural mistake detection to establishsuitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets,respectively.</description><author>Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso</author><pubDate>Tue, 02 Apr 2024 14:27:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01933v1</guid></item><item><title>Affective Behaviour Analysis via Integrating Multi-Modal Knowledge</title><link>http://arxiv.org/abs/2403.10825v1</link><description>Affective Behavior Analysis aims to facilitate technology emotionally smart,creating a world where devices can understand and react to our emotions ashumans do. To comprehensively evaluate the authenticity and applicability ofemotional behavior analysis techniques in natural environments, the 6thcompetition on Affective Behavior Analysis in-the-wild (ABAW) utilizes theAff-Wild2, Hume-Vidmimic2, and C-EXPR-DB datasets to set up five competitivetracks, i.e., Valence-Arousal (VA) Estimation, Expression (EXPR) Recognition,Action Unit (AU) Detection, Compound Expression (CE) Recognition, and EmotionalMimicry Intensity (EMI) Estimation. In this paper, we present our methoddesigns for the five tasks. Specifically, our design mainly includes threeaspects: 1) Utilizing a transformer-based feature fusion module to fullyintegrate emotional information provided by audio signals, visual images, andtranscripts, offering high-quality expression features for the downstreamtasks. 2) To achieve high-quality facial feature representations, we employMasked-Auto Encoder as the visual features extraction model and fine-tune itwith our facial dataset. 3) Considering the complexity of the video collectionscenes, we conduct a more detailed dataset division based on scenecharacteristics and train the classifier for each scene. Extensive experimentsdemonstrate the superiority of our designs.</description><author>Wei Zhang, Feng Qiu, Chen Liu, Lincheng Li, Heming Du, Tiancheng Guo, Xin Yu</author><pubDate>Sat, 16 Mar 2024 07:26:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10825v1</guid></item><item><title>Real-Time Multimodal Cognitive Assistant for Emergency Medical Services</title><link>http://arxiv.org/abs/2403.06734v1</link><description>Emergency Medical Services (EMS) responders often operate undertime-sensitive conditions, facing cognitive overload and inherent risks,requiring essential skills in critical thinking and rapid decision-making. Thispaper presents CognitiveEMS, an end-to-end wearable cognitive assistant systemthat can act as a collaborative virtual partner engaging in the real-timeacquisition and analysis of multimodal data from an emergency scene andinteracting with EMS responders through Augmented Reality (AR) smart glasses.CognitiveEMS processes the continuous streams of data in real-time andleverages edge computing to provide assistance in EMS protocol selection andintervention recognition. We address key technical challenges in real-timecognitive assistance by introducing three novel components: (i) a SpeechRecognition model that is fine-tuned for real-world medical emergencyconversations using simulated EMS audio recordings, augmented with syntheticdata generated by large language models (LLMs); (ii) an EMS Protocol Predictionmodel that combines state-of-the-art (SOTA) tiny language models with EMSdomain knowledge using graph-based attention mechanisms; (iii) an EMS ActionRecognition module which leverages multimodal audio and video data and protocolpredictions to infer the intervention/treatment actions taken by the respondersat the incident scene. Our results show that for speech recognition we achievesuperior performance compared to SOTA (WER of 0.290 vs. 0.618) onconversational data. Our protocol prediction component also significantlyoutperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognitionachieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78sfor protocol prediction on the edge and 0.31s on the server.</description><author>Keshara Weerasinghe, Saahith Janapati, Xueren Ge, Sion Kim, Sneha Iyer, John A. Stankovic, Homa Alemzadeh</author><pubDate>Mon, 11 Mar 2024 14:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06734v1</guid></item><item><title>OmniVid: A Generative Framework for Universal Video Understanding</title><link>http://arxiv.org/abs/2403.17935v1</link><description>The core of video understanding tasks, such as recognition, captioning, andtracking, is to automatically detect objects or actions in a video and analyzetheir temporal evolution. Despite sharing a common goal, different tasks oftenrely on distinct model architectures and annotation formats. In contrast,natural language processing benefits from a unified output space, i.e., textsequences, which simplifies the training of powerful foundational languagemodels, such as GPT-3, with extensive training corpora. Inspired by this, weseek to unify the output space of video understanding tasks by using languagesas labels and additionally introducing time and box tokens. In this way, avariety of video tasks could be formulated as video-grounded token generation.This enables us to address various types of video tasks, includingclassification (such as action recognition), captioning (covering clipcaptioning, video question answering, and dense video captioning), andlocalization tasks (such as visual object tracking) within a fully sharedencoder-decoder architecture, following a generative framework. Throughcomprehensive experiments, we demonstrate such a simple and straightforwardidea is quite effective and can achieve state-of-the-art or competitive resultson seven video benchmarks, providing a novel perspective for more universalvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.</description><author>Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Tue, 26 Mar 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17935v1</guid></item><item><title>Language Model Guided Interpretable Video Action Reasoning</title><link>http://arxiv.org/abs/2404.01591v1</link><description>While neural networks have excelled in video action recognition tasks, theirblack-box nature often obscures the understanding of their decision-makingprocesses. Recent approaches used inherently interpretable models to analyzevideo actions in a manner akin to human reasoning. These models, however,usually fall short in performance compared to their black-box counterparts. Inthis work, we present a new framework named Language-guided InterpretableAction Recognition framework (LaIAR). LaIAR leverages knowledge from languagemodels to enhance both the recognition capabilities and the interpretability ofvideo models. In essence, we redefine the problem of understanding video modeldecisions as a task of aligning video and language models. Using the logicalreasoning captured by the language model, we steer the training of the videomodel. This integrated approach not only improves the video model'sadaptability to different domains but also boosts its overall performance.Extensive experiments on two complex video action datasets, Charades &amp; CAD-120,validates the improved performance and interpretability of our LaIAR framework.The code of LaIAR is available at https://github.com/NingWang2049/LaIAR.</description><author>Ning Wang, Guangming Zhu, HS Li, Liang Zhang, Syed Afaq Ali Shah, Mohammed Bennamoun</author><pubDate>Tue, 02 Apr 2024 03:31:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01591v1</guid></item><item><title>LLMs are Good Action Recognizers</title><link>http://arxiv.org/abs/2404.00532v1</link><description>Skeleton-based action recognition has attracted lots of research attention.Recently, to build an accurate skeleton-based action recognizer, a variety ofworks have been proposed. Among them, some works use large model architecturesas backbones of their recognizers to boost the skeleton data representationcapability, while some other works pre-train their recognizers on external datato enrich the knowledge. In this work, we observe that large language modelswhich have been extensively used in various natural language processing tasksgenerally hold both large model architectures and rich implicit knowledge.Motivated by this, we propose a novel LLM-AR framework, in which we investigatetreating the Large Language Model as an Action Recognizer. In our framework, wepropose a linguistic projection process to project each input action signal(i.e., each skeleton sequence) into its ``sentence format'' (i.e., an ``actionsentence''). Moreover, we also incorporate our framework with several designsto further facilitate this linguistic projection process. Extensive experimentsdemonstrate the efficacy of our proposed framework.</description><author>Haoxuan Qu, Yujun Cai, Jun Liu</author><pubDate>Sun, 31 Mar 2024 03:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00532v1</guid></item><item><title>EventRPG: Event Data Augmentation with Relevance Propagation Guidance</title><link>http://arxiv.org/abs/2403.09274v1</link><description>Event camera, a novel bio-inspired vision sensor, has drawn a lot ofattention for its low latency, low power consumption, and high dynamic range.Currently, overfitting remains a critical problem in event-based classificationtasks for Spiking Neural Network (SNN) due to its relatively weak spatialrepresentation capability. Data augmentation is a simple but efficient methodto alleviate overfitting and improve the generalization ability of neuralnetworks, and saliency-based augmentation methods are proven to be effective inthe image processing field. However, there is no approach available forextracting saliency maps from SNNs. Therefore, for the first time, we presentSpiking Layer-Time-wise Relevance Propagation rule (SLTRP) and SpikingLayer-wise Relevance Propagation rule (SLRP) in order for SNN to generatestable and accurate CAMs and saliency maps. Based on this, we propose EventRPG,which leverages relevance propagation on the spiking neural network for moreefficient augmentation. Our proposed method has been evaluated on several SNNstructures, achieving state-of-the-art performance in object recognition tasksincluding N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, aswell as action recognition task SL-Animals with an accuracy of 91.59%. Our codeis available at https://github.com/myuansun/EventRPG.</description><author>Mingyuan Sun, Donghao Zhang, Zongyuan Ge, Jiaxu Wang, Jia Li, Zheng Fang, Renjing Xu</author><pubDate>Thu, 14 Mar 2024 11:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09274v1</guid></item><item><title>AutoGCN -- Towards Generic Human Activity Recognition with Neural Architecture Search</title><link>http://arxiv.org/abs/2402.01313v3</link><description>This paper introduces AutoGCN, a generic Neural Architecture Search (NAS)algorithm for Human Activity Recognition (HAR) using Graph Convolution Networks(GCNs). HAR has gained attention due to advances in deep learning, increaseddata availability, and enhanced computational capabilities. At the same time,GCNs have shown promising results in modeling relationships between body keypoints in a skeletal graph. While domain experts often craft dataset-specificGCN-based methods, their applicability beyond this specific context is severelylimited. AutoGCN seeks to address this limitation by simultaneously searchingfor the ideal hyperparameters and architecture combination within a versatilesearch space using a reinforcement controller while balancing optimalexploration and exploitation behavior with a knowledge reservoir during thesearch process. We conduct extensive experiments on two large-scale datasetsfocused on skeleton-based action recognition to assess the proposed algorithm'sperformance. Our experimental results underscore the effectiveness of AutoGCNin constructing optimal GCN architectures for HAR, outperforming conventionalNAS and GCN methods, as well as random search. These findings highlight thesignificance of a diverse search space and an expressive input representationto enhance the network performance and generalizability.</description><author>Felix Tempel, Inga Str√ºmke, Espen Alexander F. Ihlen</author><pubDate>Tue, 12 Mar 2024 11:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01313v3</guid></item><item><title>Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment</title><link>http://arxiv.org/abs/2402.04599v2</link><description>Video sequences exhibit significant nuisance variations (undesired effects)of speed of actions, temporal locations, and subjects' poses, leading totemporal-viewpoint misalignment when comparing two sets of frames or evaluatingthe similarity of two sequences. Thus, we propose Joint tEmporal and cAmeraviewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3Dskeleton sequences whose camera and subjects' poses can be easily manipulatedin 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), wherematching well temporal blocks (temporal chunks that make up a sequence) ofsupport-query sequence pairs (by factoring out nuisance variations) isessential due to limited samples of novel classes. Given a query sequence, wecreate its several views by simulating several camera locations. For a supportsequence, we match it with view-simulated query sequences, as in the popularDynamic Time Warping (DTW). Specifically, each support temporal block can bematched to the query temporal block with the same or adjacent (next) temporalindex, and adjacent camera views to achieve joint local temporal-viewpointwarping. JEANIE selects the smallest distance among matching paths withdifferent temporal-viewpoint warping patterns, an advantage over DTW which onlyperforms temporal alignment. We also propose an unsupervised FSAR akin toclustering of sequences with JEANIE as a distance measure. JEANIE achievesstate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3DMultiview Activity II on supervised and unsupervised FSAR, and theirmeta-learning inspired fusion.</description><author>Lei Wang, Jun Liu, Liang Zheng, Tom Gedeon, Piotr Koniusz</author><pubDate>Mon, 25 Mar 2024 14:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04599v2</guid></item><item><title>Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects</title><link>http://arxiv.org/abs/2403.16428v1</link><description>We interact with the world with our hands and see it through our own(egocentric) perspective. A holistic 3D understanding of such interactions fromegocentric views is important for tasks in robotics, AR/VR, action recognitionand motion generation. Accurately reconstructing such interactions in 3D ischallenging due to heavy occlusion, viewpoint bias, camera distortion, andmotion blur from the head movement. To this end, we designed the HANDS23challenge based on the AssemblyHands and ARCTIC datasets with carefullydesigned training and testing splits. Based on the results of the top submittedmethods and more recent baselines on the leaderboards, we perform a thoroughanalysis on 3D hand(-object) reconstruction tasks. Our analysis demonstratesthe effectiveness of addressing distortion specific to egocentric cameras,adopting high-capacity transformers to learn complex hand-object interactions,and fusing predictions from different views. Our study further revealschallenging scenarios intractable with state-of-the-art methods, such as fasthand motion, object reconstruction from narrow egocentric views, and closecontact between two hands and objects. Our efforts will enrich the community'sknowledge foundation and facilitate future hand studies on egocentrichand-object interactions.</description><author>Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao</author><pubDate>Mon, 25 Mar 2024 06:12:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16428v1</guid></item><item><title>Understanding Video Transformers via Universal Concept Discovery</title><link>http://arxiv.org/abs/2401.10831v3</link><description>This paper studies the problem of concept-based interpretability oftransformer representations for videos. Concretely, we seek to explain thedecision-making process of video transformers based on high-level,spatiotemporal concepts that are automatically discovered. Prior research onconcept-based interpretability has concentrated solely on image-level tasks.Comparatively, video models deal with the added temporal dimension, increasingcomplexity and posing challenges in identifying dynamic concepts over time. Inthis work, we systematically address these challenges by introducing the firstVideo Transformer Concept Discovery (VTCD) algorithm. To this end, we proposean efficient approach for unsupervised identification of units of videotransformer representations - concepts, and ranking their importance to theoutput of a model. The resulting concepts are highly interpretable, revealingspatio-temporal reasoning mechanisms and object-centric representations inunstructured video models. Performing this analysis jointly over a diverse setof supervised and self-supervised representations, we discover that some ofthese mechanism are universal in video transformers. Finally, we show that VTCDcan be used for fine-grained action recognition and video object segmentation.</description><author>Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos G. Derpanis, Pavel Tokmakov</author><pubDate>Wed, 10 Apr 2024 16:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10831v3</guid></item><item><title>STRIDE: Single-video based Temporally Continuous Occlusion Robust 3D Pose Estimation</title><link>http://arxiv.org/abs/2312.16221v2</link><description>The capability to accurately estimate 3D human poses is crucial for diversefields such as action recognition, gait recognition, and virtual/augmentedreality. However, a persistent and significant challenge within this field isthe accurate prediction of human poses under conditions of severe occlusion.Traditional image-based estimators struggle with heavy occlusions due to a lackof temporal context, resulting in inconsistent predictions. While video-basedmodels benefit from processing temporal data, they encounter limitations whenfaced with prolonged occlusions that extend over multiple frames. Thischallenge arises because these models struggle to generalize beyond theirtraining datasets, and the variety of occlusions is hard to capture in thetraining data. Addressing these challenges, we propose STRIDE (Single-videobased TempoRally contInuous occlusion Robust 3D Pose Estimation), a novelTest-Time Training (TTT) approach to fit a human motion prior for each video.This approach specifically handles occlusions that were not encountered duringthe model's training. By employing STRIDE, we can refine a sequence of noisyinitial pose estimates into accurate, temporally coherent poses during testtime, effectively overcoming the limitations of prior methods. Our frameworkdemonstrates flexibility by being model-agnostic, allowing us to use anyoff-the-shelf 3D pose estimation method for improving robustness and temporalconsistency. We validate STRIDE's efficacy through comprehensive experiments onchallenging datasets like Occluded Human3.6M, Human3.6M, and OCMotion, where itnot only outperforms existing single-image and video-based pose estimationmodels but also showcases superior handling of substantial occlusions,achieving fast, robust, accurate, and temporally consistent 3D pose estimates.</description><author>Rohit Lal, Saketh Bachu, Yash Garg, Arindam Dutta, Calvin-Khang Ta, Dripta S. Raychaudhuri, Hannah Dela Cruz, M. Salman Asif, Amit K. Roy-Chowdhury</author><pubDate>Thu, 14 Mar 2024 04:36:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16221v2</guid></item><item><title>Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data</title><link>http://arxiv.org/abs/2402.05892v3</link><description>In recent years, Transformers have become the de-facto architecture forsequence modeling on text and a variety of multi-dimensional data, such asimages and video. However, the use of self-attention layers in a Transformerincurs prohibitive compute and memory complexity that scales quadraticallyw.r.t. the sequence length. A recent architecture, Mamba, based on state spacemodels has been shown to achieve comparable performance for modeling textsequences, while scaling linearly with the sequence length. In this work, wepresent Mamba-ND, a generalized design extending the Mamba architecture toarbitrary multi-dimensional data. Our design alternatively unravels the inputdata across different dimensions following row-major orderings. We provide asystematic comparison of Mamba-ND with several other alternatives, based onprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.Empirically, we show that Mamba-ND demonstrates performance competitive withthe state-of-the-art on a variety of multi-dimensional benchmarks, includingImageNet-1K classification, HMDB-51 action recognition, and ERA5 weatherforecasting.</description><author>Shufan Li, Harkanwar Singh, Aditya Grover</author><pubDate>Thu, 14 Mar 2024 17:16:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05892v3</guid></item><item><title>Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation</title><link>http://arxiv.org/abs/2310.05737v2</link><description>While Large Language Models (LLMs) are the dominant models for generativetasks in language, they do not perform as well as diffusion models on image andvideo generation. To effectively use LLMs for visual generation, one crucialcomponent is the visual tokenizer that maps pixel-space inputs to discretetokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, avideo tokenizer designed to generate concise and expressive tokens for bothvideos and images using a common token vocabulary. Equipped with this newtokenizer, we show that LLMs outperform diffusion models on standard image andvideo generation benchmarks including ImageNet and Kinetics. In addition, wedemonstrate that our tokenizer surpasses the previously top-performing videotokenizer on two more tasks: (1) video compression comparable to thenext-generation video codec (VCC) according to human evaluations, and (2)learning effective representations for action recognition tasks.</description><author>Lijun Yu, Jos√© Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang</author><pubDate>Wed, 13 Mar 2024 06:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05737v2</guid></item><item><title>Koala: Key frame-conditioned long video-LLM</title><link>http://arxiv.org/abs/2404.04346v1</link><description>Long video question answering is a challenging task that involves recognizingshort-term activities and reasoning about their fine-grained relationships.State-of-the-art video Large Language Models (vLLMs) hold promise as a viablesolution due to their demonstrated emergent capabilities on new tasks. However,despite being trained on millions of short seconds-long videos, vLLMs areunable to understand minutes-long videos and accurately answer questions aboutthem. To address this limitation, we propose a lightweight and self-supervisedapproach, Key frame-conditioned long video-LLM (Koala), that introduceslearnable spatiotemporal queries to adapt pretrained vLLMs for generalizing tolonger videos. Our approach introduces two new tokenizers that condition onvisual tokens computed from sparse video key frames for understanding short andlong video moments. We train our proposed approach on HowTo100M and demonstrateits effectiveness on zero-shot long video understanding benchmarks, where itoutperforms state-of-the-art large models by 3 - 6% in absolute accuracy acrossall tasks. Surprisingly, we also empirically show that our approach not onlyhelps a pretrained vLLM to understand long videos but also improves itsaccuracy on short-term action recognition.</description><author>Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A. Plummer, Bryan Russell, Kate Saenko</author><pubDate>Fri, 05 Apr 2024 19:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04346v1</guid></item><item><title>Hierarchical NeuroSymbolic Approach for Action Quality Assessment</title><link>http://arxiv.org/abs/2403.13798v1</link><description>Action quality assessment (AQA) applies computer vision to quantitativelyassess the performance or execution of a human action. Current AQA approachesare end-to-end neural models, which lack transparency and tend to be biasedbecause they are trained on subjective human judgements as ground-truth. Toaddress these issues, we introduce a neuro-symbolic paradigm for AQA, whichuses neural networks to abstract interpretable symbols from video data andmakes quality assessments by applying rules to those symbols. We take diving asthe case study. We found that domain experts prefer our system and find it moreinformative than purely neural approaches to AQA in diving. Our system alsoachieves state-of-the-art action recognition and temporal segmentation, andautomatically generates a detailed report that breaks the dive down into itselements and provides objective scoring with visual evidence. As verified by agroup of domain experts, this report may be used to assist judges in scoring,help train judges, and provide feedback to divers. We will open-source all ofour annotated training data and code for ease of reproducibility.</description><author>Lauren Okamoto, Paritosh Parmar</author><pubDate>Wed, 20 Mar 2024 18:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13798v1</guid></item><item><title>PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</title><link>http://arxiv.org/abs/2404.04430v1</link><description>While current methods have shown promising progress on estimating 3D humanmotion from monocular videos, their motion estimates are often physicallyunrealistic because they mainly consider kinematics. In this paper, weintroduce Physics-aware Pretrained Transformer (PhysPT), which improveskinematics-based motion estimates and infers motion forces. PhysPT exploits aTransformer encoder-decoder backbone to effectively learn human dynamics in aself-supervised manner. Moreover, it incorporates physics principles governinghuman motion. Specifically, we build a physics-based body representation andcontact force model. We leverage them to impose novel physics-inspired traininglosses (i.e., force loss, contact loss, and Euler-Lagrange loss), enablingPhysPT to capture physical properties of the human body and the forces itexperiences. Experiments demonstrate that, once trained, PhysPT can be directlyapplied to kinematics-based estimates to significantly enhance their physicalplausibility and generate favourable motion forces. Furthermore, we show thatthese physically meaningful quantities translate into improved accuracy of animportant downstream task: human action recognition.</description><author>Yufei Zhang, Jeffrey O. Kephart, Zijun Cui, Qiang Ji</author><pubDate>Fri, 05 Apr 2024 23:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04430v1</guid></item><item><title>Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data</title><link>http://arxiv.org/abs/2402.05892v4</link><description>In recent years, Transformers have become the de-facto architecture forsequence modeling on text and a variety of multi-dimensional data, such asimages and video. However, the use of self-attention layers in a Transformerincurs prohibitive compute and memory complexity that scales quadraticallyw.r.t. the sequence length. A recent architecture, Mamba, based on state spacemodels has been shown to achieve comparable performance for modeling textsequences, while scaling linearly with the sequence length. In this work, wepresent Mamba-ND, a generalized design extending the Mamba architecture toarbitrary multi-dimensional data. Our design alternatively unravels the inputdata across different dimensions following row-major orderings. We provide asystematic comparison of Mamba-ND with several other alternatives, based onprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.Empirically, we show that Mamba-ND demonstrates performance competitive withthe state-of-the-art on a variety of multi-dimensional benchmarks, includingImageNet-1K classification, HMDB-51 action recognition, and ERA5 weatherforecasting.</description><author>Shufan Li, Harkanwar Singh, Aditya Grover</author><pubDate>Wed, 20 Mar 2024 01:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05892v4</guid></item><item><title>Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling</title><link>http://arxiv.org/abs/2403.06978v1</link><description>In this paper, we introduce Attention Prompt Tuning (APT) - a computationallyefficient variant of prompt tuning for video-based applications such as actionrecognition. Prompt tuning approaches involve injecting a set of learnableprompts along with data tokens during fine-tuning while keeping the backbonefrozen. This approach greatly reduces the number of learnable parameterscompared to full tuning. For image-based downstream tasks, normally a couple oflearnable prompts achieve results close to those of full tuning. However,videos, which contain more complex spatiotemporal information, require hundredsof tunable prompts to achieve reasonably good results. This reduces theparameter efficiency observed in images and significantly increases latency andthe number of floating-point operations (FLOPs) during inference. To tacklethese issues, we directly inject the prompts into the keys and values of thenon-local attention mechanism within the transformer block. Additionally, weintroduce a novel prompt reparameterization technique to make APT more robustagainst hyperparameter selection. The proposed APT approach greatly reduces thenumber of FLOPs and latency while achieving a significant performance boostover the existing parameter-efficient tuning methods on UCF101, HMDB51, andSSv2 datasets for action recognition. The code and pre-trained models areavailable at https://github.com/wgcban/apt</description><author>Wele Gedara Chaminda Bandara, Vishal M. Patel</author><pubDate>Mon, 11 Mar 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06978v1</guid></item><item><title>Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training</title><link>http://arxiv.org/abs/2312.02914v3</link><description>In this work, we tackle the problem of unsupervised domain adaptation (UDA)for video action recognition. Our approach, which we call UNITE, uses an imageteacher model to adapt a video student model to the target domain. UNITE firstemploys self-supervised pre-training to promote discriminative feature learningon target domain videos using a teacher-guided masked distillation objective.We then perform self-training on masked target data, using the video studentmodel and image teacher model together to generate improved pseudolabels forunlabeled target videos. Our self-training process successfully leverages thestrengths of both models to achieve strong transfer performance across domains.We evaluate our approach on multiple video domain adaptation benchmarks andobserve significant improvements upon previously reported results.</description><author>Arun Reddy, William Paul, Corban Rivera, Ketul Shah, Celso M. de Melo, Rama Chellappa</author><pubDate>Thu, 21 Mar 2024 14:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02914v3</guid></item><item><title>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</title><link>http://arxiv.org/abs/2403.15377v1</link><description>We introduce InternVideo2, a new video foundation model (ViFM) that achievesthe state-of-the-art performance in action recognition, video-text tasks, andvideo-centric dialogue. Our approach employs a progressive training paradigmthat unifies the different self- or weakly-supervised learning frameworks ofmasked video token reconstruction, cross-modal contrastive learning, and nexttoken prediction. Different training stages would guide our model to capturedifferent levels of structure and semantic information through differentpretext tasks. At the data level, we prioritize the spatiotemporal consistencyby semantically segmenting videos and generating video-audio-speech captions.This improves the alignment between video and text. We scale both data andmodel size for our InternVideo2. Through extensive experiments, we validate ourdesigns and demonstrate the state-of-the-art performance on over 60 video andaudio tasks. Notably, our model outperforms others on various video-relatedcaptioning, dialogue, and long video understanding benchmarks, highlighting itsability to reason and comprehend long temporal contexts. Code and models areavailable at https://github.com/OpenGVLab/InternVideo2/.</description><author>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Fri, 22 Mar 2024 18:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15377v1</guid></item><item><title>Video Relationship Detection Using Mixture of Experts</title><link>http://arxiv.org/abs/2403.03994v1</link><description>Machine comprehension of visual information from images and videos by neuralnetworks faces two primary challenges. Firstly, there exists a computationaland inference gap in connecting vision and language, making it difficult toaccurately determine which object a given agent acts on and represent itthrough language. Secondly, classifiers trained by a single, monolithic neuralnetwork often lack stability and generalization. To overcome these challenges,we introduce MoE-VRD, a novel approach to visual relationship detectionutilizing a mixture of experts. MoE-VRD identifies language triplets in theform of &lt; subject, predicate, object&gt; tuples to extract relationships fromvisual processing. Leveraging recent advancements in visual relationshipdetection, MoE-VRD addresses the requirement for action recognition inestablishing relationships between subjects (acting) and objects (being actedupon). In contrast to single monolithic networks, MoE-VRD employs multiplesmall models as experts, whose outputs are aggregated. Each expert in MoE-VRDspecializes in visual relationship learning and object tagging. By utilizing asparsely-gated mixture of experts, MoE-VRD enables conditional computation andsignificantly enhances neural network capacity without increasing computationalcomplexity. Our experimental results demonstrate that the conditionalcomputation capabilities and scalability of the mixture-of-experts approachlead to superior performance in visual relationship detection compared tostate-of-the-art methods.</description><author>Ala Shaabana, Zahra Gharaee, Paul Fieguth</author><pubDate>Wed, 06 Mar 2024 19:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03994v1</guid></item><item><title>Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation</title><link>http://arxiv.org/abs/2310.05737v3</link><description>While Large Language Models (LLMs) are the dominant models for generativetasks in language, they do not perform as well as diffusion models on image andvideo generation. To effectively use LLMs for visual generation, one crucialcomponent is the visual tokenizer that maps pixel-space inputs to discretetokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, avideo tokenizer designed to generate concise and expressive tokens for bothvideos and images using a common token vocabulary. Equipped with this newtokenizer, we show that LLMs outperform diffusion models on standard image andvideo generation benchmarks including ImageNet and Kinetics. In addition, wedemonstrate that our tokenizer surpasses the previously top-performing videotokenizer on two more tasks: (1) video compression comparable to thenext-generation video codec (VCC) according to human evaluations, and (2)learning effective representations for action recognition tasks.</description><author>Lijun Yu, Jos√© Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang</author><pubDate>Fri, 29 Mar 2024 18:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05737v3</guid></item><item><title>Dynamic 3D Point Cloud Sequences as 2D Videos</title><link>http://arxiv.org/abs/2403.01129v1</link><description>Dynamic 3D point cloud sequences serve as one of the most common andpractical representation modalities of dynamic real-world environments.However, their unstructured nature in both spatial and temporal domains posessignificant challenges to effective and efficient processing. Existing deeppoint cloud sequence modeling approaches imitate the mature 2D video learningmechanisms by developing complex spatio-temporal point neighbor grouping andfeature aggregation schemes, often resulting in methods lacking effectiveness,efficiency, and expressive power. In this paper, we propose a novel genericrepresentation called \textit{Structured Point Cloud Videos} (SPCVs).Intuitively, by leveraging the fact that 3D geometric shapes are essentially 2Dmanifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatialsmoothness and temporal consistency, where the pixel values correspond to the3D coordinates of points. The structured nature of our SPCV representationallows for the seamless adaptation of well-established 2D image/videotechniques, enabling efficient and effective processing and analysis of 3Dpoint cloud sequences. To achieve such re-organization, we design aself-supervised learning pipeline that is geometrically regularized and drivenby self-reconstructive and deformation field learning objectives. Additionally,we construct SPCV-based frameworks for both low-level and high-level 3D pointcloud sequence processing and analysis tasks, including action recognition,temporal interpolation, and compression. Extensive experiments demonstrate theversatility and superiority of the proposed SPCV, which has the potential tooffer new possibilities for deep learning on unstructured 3D point cloudsequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.</description><author>Yiming Zeng, Junhui Hou, Qijian Zhang, Siyu Ren, Wenping Wang</author><pubDate>Sat, 02 Mar 2024 08:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01129v1</guid></item><item><title>Disentangled Pre-training for Human-Object Interaction Detection</title><link>http://arxiv.org/abs/2404.01725v1</link><description>Detecting human-object interaction (HOI) has long been limited by the amountof supervised data available. Recent approaches address this issue bypre-training according to pseudo-labels, which align object regions with HOItriplets parsed from image captions. However, pseudo-labeling is tricky andnoisy, making HOI pre-training a complex process. Therefore, we propose anefficient disentangled pre-training method for HOI detection (DP-HOI) toaddress this problem. First, DP-HOI utilizes object detection and actionrecognition datasets to pre-train the detection and interaction decoder layers,respectively. Then, we arrange these decoder layers so that the pre-trainingarchitecture is consistent with the downstream HOI detection task. Thisfacilitates efficient knowledge transfer. Specifically, the detection decoderidentifies reliable human instances in each action recognition dataset image,generates one corresponding query, and feeds it into the interaction decoderfor verb classification. Next, we combine the human instance verb predictionsin the same image and impose image-level supervision. The DP-HOI structure canbe easily adapted to the HOI detection task, enabling effective model parameterinitialization. Therefore, it significantly enhances the performance ofexisting HOI detection models on a broad range of rare categories. The code andpre-trained weight are available at https://github.com/xingaoli/DP-HOI.</description><author>Zhuolong Li, Xingao Li, Changxing Ding, Xiangmin Xu</author><pubDate>Tue, 02 Apr 2024 09:21:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01725v1</guid></item><item><title>Occlusion Aware Student Emotion Recognition based on Facial Action Unit Detection</title><link>http://arxiv.org/abs/2307.09465v2</link><description>Given that approximately half of science, technology, engineering, andmathematics (STEM) undergraduate students in U.S. colleges and universitiesleave by the end of the first year [15], it is crucial to improve the qualityof classroom environments. This study focuses on monitoring students' emotionsin the classroom as an indicator of their engagement and proposes an approachto address this issue. The impact of different facial parts on the performanceof an emotional recognition model is evaluated through experimentation. To testthe proposed model under partial occlusion, an artificially occluded dataset isintroduced. The novelty of this work lies in the proposal of an occlusion-awarearchitecture for facial action units (AUs) extraction, which employs attentionmechanism and adaptive feature learning. The AUs can be used later to classifyfacial expressions in classroom settings. This research paper's findings provide valuable insights into handlingocclusion in analyzing facial images for emotional engagement analysis. Theproposed experiments demonstrate the significance of considering occlusion andenhancing the reliability of facial analysis models in classroom environments.These findings can also be extended to other settings where occlusions areprevalent.</description><author>Shrouk Wally, Ahmed Elsayed, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Thu, 29 Feb 2024 03:17:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09465v2</guid></item><item><title>Facial Affective Behavior Analysis with Instruction Tuning</title><link>http://arxiv.org/abs/2404.05052v1</link><description>Facial affective behavior analysis (FABA) is crucial for understanding humanmental states from images. However, traditional approaches primarily deploymodels to discriminate among discrete emotion categories, and lack the finegranularity and reasoning capability for complex facial behaviors. The adventof Multi-modal Large Language Models (MLLMs) has been proven successful ingeneral visual understanding tasks. However, directly harnessing MLLMs for FABAis challenging due to the scarcity of datasets and benchmarks, neglectingfacial prior knowledge, and low training efficiency. To address thesechallenges, we introduce (i) an instruction-following dataset for two FABAtasks, e.g., emotion and action unit recognition, (ii) a benchmark FABA-Benchwith a new metric considering both recognition and generation ability, and(iii) a new MLLM "EmoLA" as a strong baseline to the community. Our initiativeon the dataset and benchmarks reveal the nature and rationale of facialaffective behaviors, i.e., fine-grained facial movement, interpretability, andreasoning. Moreover, to build an effective and efficient FABA MLLM, weintroduce a facial prior expert module with face structure knowledge and alow-rank adaptation module into pre-trained MLLM. We conduct extensiveexperiments on FABA-Bench and four commonly-used FABA datasets. The resultsdemonstrate that the proposed facial prior expert can boost the performance andEmoLA achieves the best results on our FABA-Bench. On commonly-used FABAdatasets, EmoLA is competitive rivaling task-specific state-of-the-art models.</description><author>Yifan Li, Anh Dao, Wentao Bao, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong</author><pubDate>Sun, 07 Apr 2024 20:23:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05052v1</guid></item><item><title>A Lie Group Approach to Riemannian Batch Normalization</title><link>http://arxiv.org/abs/2403.11261v1</link><description>Manifold-valued measurements exist in numerous applications within computervision and machine learning. Recent studies have extended Deep Neural Networks(DNNs) to manifolds, and concomitantly, normalization techniques have also beenadapted to several manifolds, referred to as Riemannian normalization.Nonetheless, most of the existing Riemannian normalization methods have beenderived in an ad hoc manner and only apply to specific manifolds. This paperestablishes a unified framework for Riemannian Batch Normalization (RBN)techniques on Lie groups. Our framework offers the theoretical guarantee ofcontrolling both the Riemannian mean and variance. Empirically, we focus onSymmetric Positive Definite (SPD) manifolds, which possess three distinct typesof Lie group structures. Using the deformation concept, we generalize theexisting Lie groups on SPD manifolds into three families of parameterized Liegroups. Specific normalization layers induced by these Lie groups are thenproposed for SPD neural networks. We demonstrate the effectiveness of ourapproach through three sets of experiments: radar recognition, human actionrecognition, and electroencephalography (EEG) classification. The code isavailable at https://github.com/GitZH-Chen/LieBN.git.</description><author>Ziheng Chen, Yue Song, Yunmei Liu, Nicu Sebe</author><pubDate>Sun, 17 Mar 2024 17:24:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11261v1</guid></item><item><title>Riemannian Multinomial Logistics Regression for SPD Neural Networks</title><link>http://arxiv.org/abs/2305.11288v2</link><description>Deep neural networks for learning Symmetric Positive Definite (SPD) matricesare gaining increasing attention in machine learning. Despite the significantprogress, most existing SPD networks use traditional Euclidean classifiers onan approximated space rather than intrinsic classifiers that accurately capturethe geometry of SPD manifolds. Inspired by Hyperbolic Neural Networks (HNNs),we propose Riemannian Multinomial Logistics Regression (RMLR) for theclassification layers in SPD networks. We introduce a unified framework forbuilding Riemannian classifiers under the metrics pulled back from theEuclidean space, and showcase our framework under the parameterizedLog-Euclidean Metric (LEM) and Log-Cholesky Metric (LCM). Besides, ourframework offers a novel intrinsic explanation for the most popular LogEigclassifier in existing SPD networks. The effectiveness of our method isdemonstrated in three applications: radar recognition, human actionrecognition, and electroencephalography (EEG) classification. The code isavailable at https://github.com/GitZH-Chen/SPDMLR.git.</description><author>Ziheng Chen, Yue Song, Gaowen Liu, Ramana Rao Kompella, Xiaojun Wu, Nicu Sebe</author><pubDate>Wed, 20 Mar 2024 16:10:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11288v2</guid></item><item><title>Exploring the dynamic interplay of cognitive load and emotional arousal by using multimodal measurements: Correlation of pupil diameter and emotional arousal in emotionally engaging tasks</title><link>http://arxiv.org/abs/2403.00366v1</link><description>Multimodal data analysis and validation based on streams fromstate-of-the-art sensor technology such as eye-tracking or emotion recognitionusing the Facial Action Coding System (FACTs) with deep learning allowseducational researchers to study multifaceted learning and problem-solvingprocesses and to improve educational experiences. This study aims toinvestigate the correlation between two continuous sensor streams, pupildiameter as an indicator of cognitive workload and FACTs with deep learning asan indicator of emotional arousal (RQ 1a), specifically for epochs of high,medium, and low arousal (RQ 1b). Furthermore, the time lag between emotionalarousal and pupil diameter data will be analyzed (RQ 2). 28 participants workedon three cognitively demanding and emotionally engaging everyday moral dilemmaswhile eye-tracking and emotion recognition data were collected. The data werepre-processed in Phyton (synchronization, blink control, downsampling) andanalyzed using correlation analysis and Granger causality tests. The resultsshow negative and statistically significant correlations between the datastreams for emotional arousal and pupil diameter. However, the correlation isnegative and significant only for epochs of high arousal, while positive butnon-significant relationships were found for epochs of medium or low arousal.The average time lag for the relationship between arousal and pupil diameterwas 2.8 ms. In contrast to previous findings without a multimodal approachsuggesting a positive correlation between the constructs, the resultscontribute to the state of research by highlighting the importance ofmultimodal data validation and research on convergent vagility. Future researchshould consider emotional regulation strategies and emotional valence.</description><author>C. Kosel, S. Michel, T. Seidel, M. Foerster</author><pubDate>Fri, 01 Mar 2024 08:49:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00366v1</guid></item><item><title>The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition</title><link>http://arxiv.org/abs/2402.19344v1</link><description>This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW)Competition, which is part of the respective Workshop held in conjunction withIEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges inunderstanding human emotions and behaviors, crucial for the development ofhuman-centered technologies. In more detail, the Competition focuses on affectrelated benchmarking tasks and comprises of five sub-challenges: i)Valence-Arousal Estimation (the target is to estimate two continuous affectdimensions, valence and arousal), ii) Expression Recognition (the target is torecognise between the mutually exclusive classes of the 7 basic expressions and'other'), iii) Action Unit Detection (the target is to detect 12 action units),iv) Compound Expression Recognition (the target is to recognise between the 7mutually exclusive compound expression classes), and v) Emotional MimicryIntensity Estimation (the target is to estimate six continuous emotiondimensions). In the paper, we present these Challenges, describe theirrespective datasets and challenge protocols (we outline the evaluation metrics)and present the baseline systems as well as their obtained performance. Moreinformation for the Competition can be found in:\url{https://affective-behavior-analysis-in-the-wild.github.io/6th}.</description><author>Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Stefanos Zafeiriou, Chunchang Shao, Guanyu Hu</author><pubDate>Thu, 29 Feb 2024 16:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19344v1</guid></item><item><title>The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition</title><link>http://arxiv.org/abs/2402.19344v3</link><description>This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW)Competition, which is part of the respective Workshop held in conjunction withIEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges inunderstanding human emotions and behaviors, crucial for the development ofhuman-centered technologies. In more detail, the Competition focuses on affectrelated benchmarking tasks and comprises of five sub-challenges: i)Valence-Arousal Estimation (the target is to estimate two continuous affectdimensions, valence and arousal), ii) Expression Recognition (the target is torecognise between the mutually exclusive classes of the 7 basic expressions and'other'), iii) Action Unit Detection (the target is to detect 12 action units),iv) Compound Expression Recognition (the target is to recognise between the 7mutually exclusive compound expression classes), and v) Emotional MimicryIntensity Estimation (the target is to estimate six continuous emotiondimensions). In the paper, we present these Challenges, describe theirrespective datasets and challenge protocols (we outline the evaluation metrics)and present the baseline systems as well as their obtained performance. Moreinformation for the Competition can be found in:https://affective-behavior-analysis-in-the-wild.github.io/6th.</description><author>Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Stefanos Zafeiriou, Irene Kotsia, Alice Baird, Chris Gagne, Chunchang Shao, Guanyu Hu</author><pubDate>Tue, 12 Mar 2024 17:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19344v3</guid></item><item><title>ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis</title><link>http://arxiv.org/abs/2402.15952v1</link><description>The immense popularity of racket sports has fueled substantial demand intactical analysis with broadcast videos. However, existing manual methodsrequire laborious annotation, and recent attempts leveraging video perceptionmodels are limited to low-level annotations like ball trajectories, overlookingtactics that necessitate an understanding of stroke techniques.State-of-the-art action segmentation models also struggle with techniquerecognition due to frequent occlusions and motion-induced blurring in racketsports videos. To address these challenges, We propose ViSTec, a Video-basedSports Technique recognition model inspired by human cognition that synergizessparse visual data with rich contextual insights. Our approach integrates agraph to explicitly model strategic knowledge in stroke sequences and enhancetechnique recognition with contextual inductive bias. A two-stage actionperception model is jointly trained to align with the contextual knowledge inthe graph. Experiments demonstrate that our method outperforms existing modelsby a significant margin. Case studies with experts from the Chinese nationaltable tennis team validate our model's capacity to automate analysis fortechnical actions and tactical strategies. More details are available at:https://ViSTec2024.github.io/.</description><author>Yuchen He, Zeqing Yuan, Yihong Wu, Liqi Cheng, Dazhen Deng, Yingcai Wu</author><pubDate>Sun, 25 Feb 2024 02:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15952v1</guid></item><item><title>OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General Video Recognition</title><link>http://arxiv.org/abs/2312.00096v2</link><description>Due to the resource-intensive nature of training vision-language models onexpansive video data, a majority of studies have centered on adaptingpre-trained image-language models to the video domain. Dominant pipelinespropose to tackle the visual discrepancies with additional temporal learnerswhile overlooking the substantial discrepancy for web-scaled descriptivenarratives and concise action category names, leading to less distinct semanticspace and potential performance limitations. In this work, we prioritize therefinement of text knowledge to facilitate generalizable video recognition. Toaddress the limitations of the less distinct semantic space of category names,we prompt a large language model (LLM) to augment action class names intoSpatio-Temporal Descriptors thus bridging the textual discrepancy and servingas a knowledge base for general recognition. Moreover, to assign the bestdescriptors with different video instances, we propose Optimal DescriptorSolver, forming the video recognition problem as solving the optimal matchingflow across frame-level representations and descriptors. Comprehensiveevaluations in zero-shot, few-shot, and fully supervised video recognitionhighlight the effectiveness of our approach. Our best model achieves astate-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.</description><author>Tongjia Chen, Hongshan Yu, Zhengeng Yang, Zechuan Li, Wei Sun, Chen Chen</author><pubDate>Thu, 28 Mar 2024 09:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00096v2</guid></item><item><title>Efficient Surgical Tool Recognition via HMM-Stabilized Deep Learning</title><link>http://arxiv.org/abs/2404.04992v1</link><description>Recognizing various surgical tools, actions and phases from surgery videos isan important problem in computer vision with exciting clinical applications.Existing deep-learning-based methods for this problem either process eachsurgical video as a series of independent images without considering theirdependence, or rely on complicated deep learning models to count for dependenceof video frames. In this study, we revealed from exploratory data analysis thatsurgical videos enjoy relatively simple semantic structure, where the presenceof surgical phases and tools can be well modeled by a compact hidden Markovmodel (HMM). Based on this observation, we propose an HMM-stabilized deeplearning method for tool presence detection. A wide range of experimentsconfirm that the proposed approaches achieve better performance with lowertraining and running costs, and support more flexible ways to construct andutilize training data in scenarios where not all surgery videos of interest areextensively labelled. These results suggest that popular deep learningapproaches with over-complicated model structures may suffer from inefficientutilization of data, and integrating ingredients of deep learning andstatistical learning wisely may lead to more powerful algorithms that enjoycompetitive performance, transparent interpretation and convenient modeltraining simultaneously.</description><author>Haifeng Wang, Hao Xu, Jun Wang, Jian Zhou, Ke Deng</author><pubDate>Sun, 07 Apr 2024 16:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04992v1</guid></item><item><title>Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition</title><link>http://arxiv.org/abs/2307.04132v3</link><description>In this work, following the intuition that adverbs describing scene-sequencesare best identified by reasoning over high-level concepts of object-behavior,we propose the design of a new framework that reasons over object-behavioursextracted from raw-video-clips to recognize the clip's correspondingadverb-types. Importantly, while previous works for general sceneadverb-recognition assume knowledge of the clips underlying action-types, ourmethod is directly applicable in the more general problem setting where theaction-type of a video-clip is unknown. Specifically, we propose a novelpipeline that extracts human-interpretable object-behaviour-facts from rawvideo clips and propose novel symbolic and transformer based reasoning methodsthat operate over these extracted facts to identify adverb-types. Experimentresults demonstrate that our proposed methods perform favourably against theprevious state-of-the-art. Additionally, to support efforts in symbolicvideo-processing, we release two new datasets of object-behaviour-factsextracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.</description><author>Amrit Diggavi Seshadri, Alessandra Russo</author><pubDate>Wed, 27 Mar 2024 19:17:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04132v3</guid></item><item><title>Emotion Recognition Using Transformers with Masked Learning</title><link>http://arxiv.org/abs/2403.13731v1</link><description>In recent years, deep learning has achieved innovative advancements invarious fields, including the analysis of human emotions and behaviors.Initiatives such as the Affective Behavior Analysis in-the-wild (ABAW)competition have been particularly instrumental in driving research in thisarea by providing diverse and challenging datasets that enable preciseevaluation of complex emotional states. This study leverages the VisionTransformer (ViT) and Transformer models to focus on the estimation ofValence-Arousal (VA), which signifies the positivity and intensity of emotions,recognition of various facial expressions, and detection of Action Units (AU)representing fundamental muscle movements. This approach transcends traditionalConvolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) basedmethods, proposing a new Transformer-based framework that maximizes theunderstanding of temporal and spatial features. The core contributions of thisresearch include the introduction of a learning technique through random framemasking and the application of Focal loss adapted for imbalanced data,enhancing the accuracy and applicability of emotion and behavior analysis inreal-world settings. This approach is expected to contribute to the advancementof emotional computing and deep learning methodologies.</description><author>Seongjae Min, Junseok Yang, Sangjun Lim, Junyong Lee, Sangwon Lee, Sejoon Lim</author><pubDate>Tue, 19 Mar 2024 13:26:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13731v1</guid></item><item><title>Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches</title><link>http://arxiv.org/abs/2404.00540v1</link><description>The vulnerability of deep neural networks to adversarial patches hasmotivated numerous defense strategies for boosting model robustness. However,the prevailing defenses depend on single observation or pre-establishedadversary information to counter adversarial patches, often failing to beconfronted with unseen or adaptive adversarial attacks and easily exhibitingunsatisfying performance in dynamic 3D environments. Inspired by active humanperception and recurrent feedback mechanisms, we develop Embodied ActiveDefense (EAD), a proactive defensive strategy that actively contextualizesenvironmental information to address misaligned adversarial patches in 3Dreal-world settings. To achieve this, EAD develops two central recurrentsub-modules, i.e., a perception module and a policy module, to implement twocritical functions of active vision. These models recurrently process a seriesof beliefs and observations, facilitating progressive refinement of theircomprehension of the target object and enabling the development of strategicactions to counter adversarial patches in 3D environments. To optimize learningefficiency, we incorporate a differentiable approximation of environmentaldynamics and deploy patches that are agnostic to the adversary strategies.Extensive experiments demonstrate that EAD substantially enhances robustnessagainst a variety of patches within just a few steps through its action policyin safety-critical tasks (e.g., face recognition and object detection), withoutcompromising standard accuracy. Furthermore, due to the attack-agnosticcharacteristic, EAD facilitates excellent generalization to unseen attacks,diminishing the averaged attack success rate by 95 percent across a range ofunseen adversarial attacks.</description><author>Lingxuan Wu, Xiao Yang, Yinpeng Dong, Liuwei Xie, Hang Su, Jun Zhu</author><pubDate>Sun, 31 Mar 2024 04:02:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00540v1</guid></item><item><title>Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru</title><link>http://arxiv.org/abs/2402.11571v1</link><description>Social robots aim to establish long-term bonds with humans through engagingconversation. However, traditional conversational approaches, reliant onscripted interactions, often fall short in maintaining engaging conversations.This paper addresses this limitation by integrating large language models(LLMs) into social robots to achieve more dynamic and expressive conversations.We introduce a fully-automated conversation system that leverages LLMs togenerate robot responses with expressive behaviors, congruent with the robot'spersonality. We incorporate robot behavior with two modalities: 1) atext-to-speech (TTS) engine capable of various delivery styles, and 2) alibrary of physical actions for the robot. We develop a custom,state-of-the-art emotion recognition model to dynamically select the robot'stone of voice and utilize emojis from LLM output as cues for generating robotactions. A demo of our system is available here. To illuminate design andimplementation issues, we conduct a pilot study where volunteers chat with asocial robot using our proposed system, and we analyze their feedback,conducting a rigorous error analysis of chat transcripts. Feedback wasoverwhelmingly positive, with participants commenting on the robot's empathy,helpfulness, naturalness, and entertainment. Most negative feedback was due toautomatic speech recognition (ASR) errors which had limited impact onconversations. However, we observed a small class of errors, such as the LLMrepeating itself or hallucinating fictitious information and human responses,that have the potential to derail conversations, raising important issues forLLM application.</description><author>Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez</author><pubDate>Sun, 18 Feb 2024 12:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11571v1</guid></item><item><title>X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization</title><link>http://arxiv.org/abs/2403.19811v1</link><description>Lately, there has been growing interest in adapting vision-language models(VLMs) to image and third-person video classification due to their success inzero-shot recognition. However, the adaptation of these models to egocentricvideos has been largely unexplored. To address this gap, we propose a simpleyet effective cross-modal adaptation framework, which we call X-MIC. Using avideo adapter, our pipeline learns to align frozen text embeddings to eachegocentric video directly in the shared embedding space. Our novel adapterarchitecture retains and improves generalization of the pre-trained VLMs bydisentangling learnable temporal modeling and frozen visual encoder. Thisresults in an enhanced alignment of text embeddings to each egocentric video,leading to a significant improvement in cross-dataset generalization. Weevaluate our approach on the Epic-Kitchens, Ego4D, and EGTEA datasets forfine-grained cross-dataset action generalization, demonstrating theeffectiveness of our method. Code is available athttps://github.com/annusha/xmic</description><author>Anna Kukleva, Fadime Sener, Edoardo Remelli, Bugra Tekin, Eric Sauser, Bernt Schiele, Shugao Ma</author><pubDate>Thu, 28 Mar 2024 20:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19811v1</guid></item><item><title>Robust Light-Weight Facial Affective Behavior Recognition with CLIP</title><link>http://arxiv.org/abs/2403.09915v1</link><description>Human affective behavior analysis aims to delve into human expressions andbehaviors to deepen our understanding of human emotions. Basic expressioncategories (EXPR) and Action Units (AUs) are two essential components in thisanalysis, which categorize emotions and break down facial movements intoelemental units, respectively. Despite advancements, existing approaches inexpression classification and AU detection often necessitate complex models andsubstantial computational resources, limiting their applicability in everydaysettings. In this work, we introduce the first lightweight framework adept atefficiently tackling both expression classification and AU detection. Thisframework employs a frozen CLIP image encoder alongside a trainable multilayerperceptron (MLP), enhanced with Conditional Value at Risk (CVaR) for robustnessand a loss landscape flattening strategy for improved generalization.Experimental results on the Aff-wild2 dataset demonstrate superior performancein comparison to the baseline while maintaining minimal computational demands,offering a practical solution for affective behavior analysis. The code isavailable at https://github.com/Purdue-M2/Affective_Behavior_Analysis_M2_PURDUE</description><author>Li Lin, Sarah Papabathini, Xin Wang, Shu Hu</author><pubDate>Fri, 15 Mar 2024 00:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09915v1</guid></item><item><title>Boosting Continuous Emotion Recognition with Self-Pretraining using Masked Autoencoders, Temporal Convolutional Networks, and Transformers</title><link>http://arxiv.org/abs/2403.11440v1</link><description>Human emotion recognition holds a pivotal role in facilitating seamlesshuman-computer interaction. This paper delineates our methodology in tacklingthe Valence-Arousal (VA) Estimation Challenge, Expression (Expr) ClassificationChallenge, and Action Unit (AU) Detection Challenge within the ambit of the 6thWorkshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Ourstudy advocates a novel approach aimed at refining continuous emotionrecognition. We achieve this by initially harnessing pre-training with MaskedAutoencoders (MAE) on facial datasets, followed by fine-tuning on the aff-wild2dataset annotated with expression (Expr) labels. The pre-trained model servesas an adept visual feature extractor, thereby enhancing the model's robustness.Furthermore, we bolster the performance of continuous emotion recognition byintegrating Temporal Convolutional Network (TCN) modules and TransformerEncoder modules into our framework.</description><author>Weiwei Zhou, Jiada Lu, Chenkun Ling, Weifeng Wang, Shaowei Liu</author><pubDate>Mon, 18 Mar 2024 04:28:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11440v1</guid></item><item><title>Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video Understanding</title><link>http://arxiv.org/abs/2311.18773v2</link><description>Learning from videos is an emerging research area that enables robots toacquire skills from human demonstrations, such as procedural videos. To dothis, video-language models must be able to obtain structured understandings,such as the temporal segmentation of a demonstration into sequences of actionsand skills, and to generalize the understandings to novel domains. In pursuitof this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1)step recognition and (2) intra-video retrieval over a dataset of temporallysegmented and labeled tasks in International Space Station spacewalkrecordings. In tandem, the two tasks quantify a model's ability to make use of:(1) out-of-domain visual information; (2) a high temporal context window; and(3) multimodal (e.g. visual and speech) domains. This departs from existingbenchmarks for procedural video understanding, which typically deal with shortcontext lengths and can be solved with a single modality. Spacewalk-18, withits inherent multimodal and long-form complexity, exposes the high difficultyof task recognition and segmentation. We find that state-of-the-art methodsperform poorly on our benchmark, but improvements can be obtained byincorporating information from longer-range temporal context across differentmodalities. Our experiments underscore the need to develop new approaches tothese tasks. Data, model, and code will be released athttps://brown-palm.github.io/Spacewalk-18/.</description><author>Rohan Myer Krishnan, Zitian Tang, Zhiqiu Yu, Chen Sun</author><pubDate>Fri, 22 Mar 2024 02:21:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18773v2</guid></item><item><title>Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2402.14569v1</link><description>Robot navigation has transitioned from prioritizing obstacle avoidance toadopting socially aware navigation strategies that accommodate human presence.As a result, the recognition of socially aware navigation within dynamichuman-centric environments has gained prominence in the field of robotics.Although reinforcement learning technique has fostered the advancement ofsocially aware navigation, defining appropriate reward functions, especially incongested environments, has posed a significant challenge. These rewards,crucial in guiding robot actions, demand intricate human-crafted design due totheir complex nature and inability to be automatically set. The multitude ofmanually designed rewards poses issues with hyperparameter redundancy,imbalance, and inadequate representation of unique object characteristics. Toaddress these challenges, we introduce a transformable gaussian reward function(TGRF). The TGRF significantly reduces the burden of hyperparameter tuning,displays adaptability across various reward functions, and demonstratesaccelerated learning rates, particularly excelling in crowded environmentsutilizing deep reinforcement learning (DRL). We introduce and validate TGRFthrough sections highlighting its conceptual background, characteristics,experiments, and real-world application, paving the way for a more effectiveand adaptable approach in robotics.The complete source code is available onhttps://github.com/JinnnK/TGRF</description><author>Jinyeob Kim, Sumin Kang, Sungwoo Yang, Beomjoon Kim, Jargalbaatar Yura, Donghan Kim</author><pubDate>Thu, 22 Feb 2024 14:20:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14569v1</guid></item><item><title>Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention</title><link>http://arxiv.org/abs/2404.07347v1</link><description>Humans utilize their gaze to concentrate on essential information whileperceiving and interpreting intentions in videos. Incorporating human gaze intocomputational algorithms can significantly enhance model performance in videounderstanding tasks. In this work, we address a challenging and innovative taskin video understanding: predicting the actions of an agent in a video based ona partial video. We introduce the Gaze-guided Action Anticipation algorithm,which establishes a visual-semantic graph from the video input. Our methodutilizes a Graph Neural Network to recognize the agent's intention and predictthe action sequence to fulfill this intention. To assess the efficiency of ourapproach, we collect a dataset containing household activities generated in theVirtualHome environment, accompanied by human gaze data of viewing videos. Ourmethod outperforms state-of-the-art techniques, achieving a 7\% improvement inaccuracy for 18-class intention recognition. This highlights the efficiency ofour method in learning important features from human gaze data.</description><author>Suleyman Ozdel, Yao Rong, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang</author><pubDate>Wed, 10 Apr 2024 22:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07347v1</guid></item><item><title>Non-verbal information in spontaneous speech - towards a new framework of analysis</title><link>http://arxiv.org/abs/2403.03522v1</link><description>Non-verbal signals in speech are encoded by prosody and carry informationthat ranges from conversation action to attitude and emotion. Despite itsimportance, the principles that govern prosodic structure are not yetadequately understood. This paper offers an analytical schema and atechnological proof-of-concept for the categorization of prosodic signals andtheir association with meaning. The schema interprets surface-representationsof multi-layered prosodic events. As a first step towards implementation, wepresent a classification process that disentangles prosodic phenomena of threeorders. It relies on fine-tuning a pre-trained speech recognition model,enabling the simultaneous multi-class/multi-label detection. It generalizesover a large variety of spontaneous data, performing on a par with, or superiorto, human annotation. In addition to a standardized formalization of prosody,disentangling prosodic patterns can direct a theory of communication and speechorganization. A welcome by-product is an interpretation of prosody that willenhance speech- and language-related technologies.</description><author>Tirza Biron, Moshe Barboy, Eran Ben-Artzy, Alona Golubchik, Yanir Marmor, Smadar Szekely, Yaron Winter, David Harel</author><pubDate>Wed, 06 Mar 2024 08:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03522v1</guid></item><item><title>Non-verbal information in spontaneous speech -- towards a new framework of analysis</title><link>http://arxiv.org/abs/2403.03522v2</link><description>Non-verbal signals in speech are encoded by prosody and carry informationthat ranges from conversation action to attitude and emotion. Despite itsimportance, the principles that govern prosodic structure are not yetadequately understood. This paper offers an analytical schema and atechnological proof-of-concept for the categorization of prosodic signals andtheir association with meaning. The schema interprets surface-representationsof multi-layered prosodic events. As a first step towards implementation, wepresent a classification process that disentangles prosodic phenomena of threeorders. It relies on fine-tuning a pre-trained speech recognition model,enabling the simultaneous multi-class/multi-label detection. It generalizesover a large variety of spontaneous data, performing on a par with, or superiorto, human annotation. In addition to a standardized formalization of prosody,disentangling prosodic patterns can direct a theory of communication and speechorganization. A welcome by-product is an interpretation of prosody that willenhance speech- and language-related technologies.</description><author>Tirza Biron, Moshe Barboy, Eran Ben-Artzy, Alona Golubchik, Yanir Marmor, Smadar Szekely, Yaron Winter, David Harel</author><pubDate>Wed, 13 Mar 2024 10:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03522v2</guid></item><item><title>PsyChat: A Client-Centric Dialogue System for Mental Health Support</title><link>http://arxiv.org/abs/2312.04262v2</link><description>Dialogue systems are increasingly integrated into mental health support tohelp clients facilitate exploration, gain insight, take action, and ultimatelyheal themselves. A practical and user-friendly dialogue system should beclient-centric, focusing on the client's behaviors. However, existing dialoguesystems publicly available for mental health support often concentrate solelyon the counselor's strategies rather than the behaviors expressed by clients.This can lead to unreasonable or inappropriate counseling strategies andcorresponding responses generated by the dialogue system. To address thisissue, we propose PsyChat, a client-centric dialogue system that providespsychological support through online chat. The client-centric dialogue systemcomprises five modules: client behavior recognition, counselor strategyselection, input packer, response generator, and response selection. Bothautomatic and human evaluations demonstrate the effectiveness and practicalityof our proposed dialogue system for real-life mental health support.Furthermore, the case study demonstrates that the dialogue system can predictthe client's behaviors, select appropriate counselor strategies, and generateaccurate and suitable responses.</description><author>Huachuan Qiu, Anqi Li, Lizhi Ma, Zhenzhong Lan</author><pubDate>Wed, 20 Mar 2024 02:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04262v2</guid></item><item><title>Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets</title><link>http://arxiv.org/abs/2403.07767v1</link><description>Paralinguistic traits like cognitive load and emotion are increasinglyrecognized as pivotal areas in speech recognition research, often examinedthrough specialized datasets like CLSE and IEMOCAP. However, the integrity ofthese datasets is seldom scrutinized for text-dependency. This paper criticallyevaluates the prevalent assumption that machine learning models trained on suchdatasets genuinely learn to identify paralinguistic traits, rather than merelycapturing lexical features. By examining the lexical overlap in these datasetsand testing the performance of machine learning models, we expose significanttext-dependency in trait-labeling. Our results suggest that some machinelearning models, especially large pre-trained models like HuBERT, mightinadvertently focus on lexical characteristics rather than the intendedparalinguistic features. The study serves as a call to action for the researchcommunity to reevaluate the reliability of existing datasets and methodologies,ensuring that machine learning models genuinely learn what they are designed torecognize.</description><author>Jan Pe≈°√°n, Santosh Kesiraju, Luk√°≈° Burget, Jan ''Honza'' ƒåernock√Ω</author><pubDate>Tue, 12 Mar 2024 16:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07767v1</guid></item><item><title>Systemic Biases in Sign Language AI Research: A Deaf-Led Call to Reevaluate Research Agendas</title><link>http://arxiv.org/abs/2403.02563v1</link><description>Growing research in sign language recognition, generation, and translation AIhas been accompanied by calls for ethical development of such technologies.While these works are crucial to helping individual researchers do better,there is a notable lack of discussion of systemic biases or analysis ofrhetoric that shape the research questions and methods in the field, especiallyas it remains dominated by hearing non-signing researchers. Therefore, weconduct a systematic review of 101 recent papers in sign language AI. Ouranalysis identifies significant biases in the current state of sign language AIresearch, including an overfocus on addressing perceived communicationbarriers, a lack of use of representative datasets, use of annotations lackinglinguistic foundations, and development of methods that build on flawed models.We take the position that the field lacks meaningful input from Deafstakeholders, and is instead driven by what decisions are the most convenientor perceived as important to hearing researchers. We end with a call to action:the field must make space for Deaf researchers to lead the conversation in signlanguage AI.</description><author>Aashaka Desai, Maartje De Meulder, Julie A. Hochgesang, Annemarie Kocab, Alex X. Lu</author><pubDate>Tue, 05 Mar 2024 00:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02563v1</guid></item></channel></rss>