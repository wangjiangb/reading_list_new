<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivaction recognition</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 20 Oct 2023 06:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>InfoGCN++: Learning Representation by Predicting the Future for Online Human Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2310.10547v1</link><description>Skeleton-based action recognition has made significant advancements recently,with models like InfoGCN showcasing remarkable accuracy. However, these modelsexhibit a key limitation: they necessitate complete action observation prior toclassification, which constrains their applicability in real-time situationssuch as surveillance and robotic systems. To overcome this barrier, weintroduce InfoGCN++, an innovative extension of InfoGCN, explicitly developedfor online skeleton-based action recognition. InfoGCN++ augments the abilitiesof the original InfoGCN model by allowing real-time categorization of actiontypes, independent of the observation sequence's length. It transcendsconventional approaches by learning from current and anticipated futuremovements, thereby creating a more thorough representation of the entiresequence. Our approach to prediction is managed as an extrapolation issue,grounded on observed actions. To enable this, InfoGCN++ incorporates NeuralOrdinary Differential Equations, a concept that lets it effectively model thecontinuous evolution of hidden states. Following rigorous evaluations on threeskeleton-based action recognition benchmarks, InfoGCN++ demonstratesexceptional performance in online action recognition. It consistently equals orexceeds existing techniques, highlighting its significant potential to reshapethe landscape of real-time action recognition applications. Consequently, thiswork represents a major leap forward from InfoGCN, pushing the limits of what'spossible in online, skeleton-based action recognition. The code for InfoGCN++is publicly available at https://github.com/stnoah1/infogcn2 for furtherexploration and validation.</description><author>Seunggeun Chi, Hyung-gun Chi, Qixing Huang, Karthik Ramani</author><pubDate>Mon, 16 Oct 2023 17:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10547v1</guid></item><item><title>Balanced Representation Learning for Long-tailed Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.14024v1</link><description>Skeleton-based action recognition has recently made significant progress.However, data imbalance is still a great challenge in real-world scenarios. Theperformance of current action recognition algorithms declines sharply whentraining data suffers from heavy class imbalance. The imbalanced data actuallydegrades the representations learned by these methods and becomes thebottleneck for action recognition. How to learn unbiased representations fromimbalanced action data is the key to long-tailed action recognition. In thispaper, we propose a novel balanced representation learning method to addressthe long-tailed problem in action recognition. Firstly, a spatial-temporalaction exploration strategy is presented to expand the sample spaceeffectively, generating more valuable samples in a rebalanced manner. Secondly,we design a detached action-aware learning schedule to further mitigate thebias in the representation space. The schedule detaches the representationlearning of tail classes from training and proposes an action-aware loss toimpose more effective constraints. Additionally, a skip-modal representation isproposed to provide complementary structural information. The proposed methodis validated on four skeleton datasets, NTU RGB+D 60, NTU RGB+D 120, NW-UCLA,and Kinetics. It not only achieves consistently large improvement compared tothe state-of-the-art (SOTA) methods, but also demonstrates a superiorgeneralization capacity through extensive experiments. Our code is available athttps://github.com/firework8/BRL.</description><author>Hongda Liu, Yunlong Wang, Min Ren, Junxing Hu, Zhengquan Luo, Guangqi Hou, Zhenan Sun</author><pubDate>Sun, 27 Aug 2023 08:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14024v1</guid></item><item><title>Few-shot Action Recognition via Intra- and Inter-Video Information Maximization</title><link>http://arxiv.org/abs/2305.06114v1</link><description>Current few-shot action recognition involves two primary sources ofinformation for classification:(1) intra-video information, determined by framecontent within a single video clip, and (2) inter-video information, measuredby relationships (e.g., feature similarity) among videos. However, existingmethods inadequately exploit these two information sources. In terms ofintra-video information, current sampling operations for input videos may omitcritical action information, reducing the utilization efficiency of video data.For the inter-video information, the action misalignment among videos makes itchallenging to calculate precise relationships. Moreover, how to jointlyconsider both inter- and intra-video information remains under-explored forfew-shot action recognition. To this end, we propose a novel framework, VideoInformation Maximization (VIM), for few-shot video action recognition. VIM isequipped with an adaptive spatial-temporal video sampler and a spatiotemporalaction alignment model to maximize intra- and inter-video information,respectively. The video sampler adaptively selects important frames andamplifies critical spatial regions for each input video based on the task athand. This preserves and emphasizes informative parts of video clips whileeliminating interference at the data level. The alignment model performstemporal and spatial action alignment sequentially at the feature level,leading to more precise measurements of inter-video similarity. Finally, Thesegoals are facilitated by incorporating additional loss terms based on mutualinformation measurement. Consequently, VIM acts to maximize the distinctivenessof video information from limited video data. Extensive experimental results onpublic datasets for few-shot action recognition demonstrate the effectivenessand benefits of our framework.</description><author>Huabin Liu, Weiyao Lin, Tieyuan Chen, Yuxi Li, Shuyuan Li, John See</author><pubDate>Wed, 10 May 2023 14:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06114v1</guid></item><item><title>Hierarchical Compositional Representations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2208.09424v2</link><description>Recently action recognition has received more and more attention for itscomprehensive and practical applications in intelligent surveillance andhuman-computer interaction. However, few-shot action recognition has not beenwell explored and remains challenging because of data scarcity. In this paper,we propose a novel hierarchical compositional representations (HCR) learningapproach for few-shot action recognition. Specifically, we divide a complicatedaction into several sub-actions by carefully designed hierarchical clusteringand further decompose the sub-actions into more fine-grained spatiallyattentional sub-actions (SAS-actions). Although there exist large differencesbetween base classes and novel classes, they can share similar patterns insub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance inthe transportation problem to measure the similarity between video samples interms of sub-action representations. It computes the optimal matching flowsbetween sub-actions as distance metric, which is favorable for comparingfine-grained patterns. Extensive experiments show our method achieves thestate-of-the-art results on HMDB51, UCF101 and Kinetics datasets.</description><author>Changzhen Li, Jie Zhang, Shuzhe Wu, Xin Jin, Shiguang Shan</author><pubDate>Fri, 19 May 2023 03:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09424v2</guid></item><item><title>Local Spherical Harmonics Improve Skeleton-Based Hand Action Recognition</title><link>http://arxiv.org/abs/2308.10557v1</link><description>Hand action recognition is essential. Communication, human-robotinteractions, and gesture control are dependent on it. Skeleton-based actionrecognition traditionally includes hands, which belong to the classes whichremain challenging to correctly recognize to date. We propose a methodspecifically designed for hand action recognition which uses relative angularembeddings and local Spherical Harmonics to create novel hand representations.The use of Spherical Harmonics creates rotation-invariant representations whichmake hand action recognition even more robust against inter-subject differencesand viewpoint changes. We conduct extensive experiments on the hand joints inthe First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand PoseAnnotations, and on the NTU RGB+D 120 dataset, demonstrating the benefit ofusing Local Spherical Harmonics Representations. Our code is available athttps://github.com/KathPra/LSHR_LSHT.</description><author>Katharina Prasse, Steffen Jung, Yuxuan Zhou, Margret Keuper</author><pubDate>Mon, 21 Aug 2023 09:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10557v1</guid></item><item><title>Analysis and Evaluation of Kinect-based Action Recognition Algorithms</title><link>http://arxiv.org/abs/2112.08626v2</link><description>Human action recognition still exists many challenging problems such asdifferent viewpoints, occlusion, lighting conditions, human body size and thespeed of action execution, although it has been widely used in different areas.To tackle these challenges, the Kinect depth sensor has been developed torecord real time depth sequences, which are insensitive to the color of humanclothes and illumination conditions. Many methods on recognizing human actionhave been reported in the literature such as HON4D, HOPC, RBD and HDG, whichuse the 4D surface normals, pointclouds, skeleton-based model and depthgradients respectively to capture discriminative information from depth videosor skeleton data. In this research project, the performance of fouraforementioned algorithms will be analyzed and evaluated using five benchmarkdatasets, which cover challenging issues such as noise, change of viewpoints,background clutters and occlusions. We also implemented and improved the HDGalgorithm, and applied it in cross-view action recognition using the UWA3DMultiview Activity dataset. Moreover, we used different combinations ofindividual feature vectors in HDG for performance evaluation. The experimentalresults show that our improvement of HDG outperforms other threestate-of-the-art algorithms for cross-view action recognition.</description><author>Lei Wang</author><pubDate>Wed, 27 Sep 2023 16:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.08626v2</guid></item><item><title>One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching</title><link>http://arxiv.org/abs/2307.07286v1</link><description>One-shot skeleton action recognition, which aims to learn a skeleton actionrecognition model with a single training sample, has attracted increasinginterest due to the challenge of collecting and annotating large-scale skeletonaction data. However, most existing studies match skeleton sequences bycomparing their feature vectors directly which neglects spatial structures andtemporal orders of skeleton data. This paper presents a novel one-shot skeletonaction recognition technique that handles skeleton action recognition viamulti-scale spatial-temporal feature matching. We represent skeleton data atmultiple spatial and temporal scales and achieve optimal feature matching fromtwo perspectives. The first is multi-scale matching which captures thescale-wise semantic relevance of skeleton data at multiple spatial and temporalscales simultaneously. The second is cross-scale matching which handlesdifferent motion magnitudes and speeds by capturing sample-wise relevanceacross multiple scales. Extensive experiments over three large-scale datasets(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superiorone-shot skeleton action recognition, and it outperforms the state-of-the-artconsistently by large margins.</description><author>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot</author><pubDate>Fri, 14 Jul 2023 12:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07286v1</guid></item><item><title>hear-your-action: human action recognition by ultrasound active sensing</title><link>http://arxiv.org/abs/2309.08087v1</link><description>Action recognition is a key technology for many industrial applications.Methods using visual information such as images are very popular. However,privacy issues prevent widespread usage due to the inclusion of privateinformation, such as visible faces and scene backgrounds, which are notnecessary for recognizing user action. In this paper, we propose aprivacy-preserving action recognition by ultrasound active sensing. As actionrecognition from ultrasound active sensing in a non-invasive manner is not wellinvestigated, we create a new dataset for action recognition and conduct acomparison of features for classification. We calculated feature values byfocusing on the temporal variation of the amplitude of ultrasound reflectedwaves and performed classification using a support vector machine and VGG foreight fundamental action classes. We confirmed that our method achieved anaccuracy of 97.9% when trained and evaluated on the same person and in the sameenvironment. Additionally, our method achieved an accuracy of 89.5% even whentrained and evaluated on different people. We also report the analyses ofaccuracies in various conditions and limitations.</description><author>Risako Tanigawa, Yasunori Ishii</author><pubDate>Fri, 15 Sep 2023 02:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08087v1</guid></item><item><title>Egocentric RGB+Depth Action Recognition in Industry-Like Settings</title><link>http://arxiv.org/abs/2309.13962v1</link><description>Action recognition from an egocentric viewpoint is a crucial perception taskin robotics and enables a wide range of human-robot interactions. While mostcomputer vision approaches prioritize the RGB camera, the Depth modality -which can further amplify the subtleties of actions from an egocentricperspective - remains underexplored. Our work focuses on recognizing actionsfrom egocentric RGB and Depth modalities in an industry-like environment. Tostudy this problem, we consider the recent MECCANO dataset, which provides awide range of assembling actions. Our framework is based on the 3D Video SWINTransformer to encode both RGB and Depth modalities effectively. To address theinherent skewness in real-world multimodal action occurrences, we propose atraining strategy using an exponentially decaying variant of the focal lossmodulating factor. Additionally, to leverage the information in both RGB andDepth modalities, we opt for late fusion to combine the predictions from eachmodality. We thoroughly evaluate our method on the action recognition task ofthe MECCANO dataset, and it significantly outperforms the prior work. Notably,our method also secured first place at the multimodal action recognitionchallenge at ICIAP 2023.</description><author>Jyoti Kini, Sarah Fleischer, Ishan Dave, Mubarak Shah</author><pubDate>Mon, 25 Sep 2023 09:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13962v1</guid></item><item><title>Cross-Modal Learning with 3D Deformable Attention for Action Recognition</title><link>http://arxiv.org/abs/2212.05638v3</link><description>An important challenge in vision-based action recognition is the embedding ofspatiotemporal features with two or more heterogeneous modalities into a singlefeature. In this study, we propose a new 3D deformable transformer for actionrecognition with adaptive spatiotemporal receptive fields and a cross-modallearning scheme. The 3D deformable transformer consists of three attentionmodules: 3D deformability, local joint stride, and temporal stride attention.The two cross-modal tokens are input into the 3D deformable attention module tocreate a cross-attention token with a reflected spatiotemporal correlation.Local joint stride attention is applied to spatially combine attention and posetokens. Temporal stride attention temporally reduces the number of input tokensin the attention module and supports temporal expression learning without thesimultaneous use of all tokens. The deformable transformer iterates L-times andcombines the last cross-modal token for classification. The proposed 3Ddeformable transformer was tested on the NTU60, NTU120, FineGYM, and PennActiondatasets, and showed results better than or similar to pre-trainedstate-of-the-art methods even without a pre-training process. In addition, byvisualizing important joints and correlations during action recognition throughspatial joint and temporal stride attention, the possibility of achieving anexplainable potential for action recognition is presented.</description><author>Sangwon Kim, Dasom Ahn, Byoung Chul Ko</author><pubDate>Thu, 17 Aug 2023 08:23:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05638v3</guid></item><item><title>Flow Dynamics Correction for Action Recognition</title><link>http://arxiv.org/abs/2310.10059v1</link><description>Various research studies indicate that action recognition performance highlydepends on the types of motions being extracted and how accurate the humanactions are represented. In this paper, we investigate different optical flow,and features extracted from these optical flow that capturing both short-termand long-term motion dynamics. We perform power normalization on the magnitudecomponent of optical flow for flow dynamics correction to boost subtle ordampen sudden motions. We show that existing action recognition models whichrely on optical flow are able to get performance boosted with our correctedoptical flow. To further improve performance, we integrate our corrected flowdynamics into popular models through a simple hallucination step by selectingonly the best performing optical flow features, and we show that by'translating' the CNN feature maps into these optical flow features withdifferent scales of motions leads to the new state-of-the-art performance onseveral benchmarks including HMDB-51, YUP++, fine-grained action recognition onMPII Cooking Activities, and large-scale Charades.</description><author>Lei Wang, Piotr Koniusz</author><pubDate>Mon, 16 Oct 2023 05:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10059v1</guid></item><item><title>Action Recognition Utilizing YGAR Dataset</title><link>http://arxiv.org/abs/2310.00831v1</link><description>The scarcity of high quality actions video data is a bottleneck in theresearch and application of action recognition. Although significant effort hasbeen made in this area, there still exist gaps in the range of available datatypes a more flexible and comprehensive data set could help bridge. In thispaper, we present a new 3D actions data simulation engine and generate 3 setsof sample data to demonstrate its current functionalities. With the new datageneration process, we demonstrate its applications to image classifications,action recognitions and potential to evolve into a system that would allow theexploration of much more complex action recognition tasks. In order to show offthese capabilities, we also train and test a list of commonly used models forimage recognition to demonstrate the potential applications and capabilities ofthe data sets and their generation process.</description><author>Shuo Wang, Amiya Ranjan, Lawrence Jiang</author><pubDate>Mon, 02 Oct 2023 01:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00831v1</guid></item><item><title>Modelling Spatio-Temporal Interactions for Compositional Action Recognition</title><link>http://arxiv.org/abs/2305.02673v1</link><description>Humans have the natural ability to recognize actions even if the objectsinvolved in the action or the background are changed. Humans can abstract awaythe action from the appearance of the objects and their context which isreferred to as compositionality of actions. Compositional action recognitiondeals with imparting human-like compositional generalization abilities toaction-recognition models. In this regard, extracting the interactions betweenhumans and objects forms the basis of compositional understanding. Theseinteractions are not affected by the appearance biases of the objects or thecontext. But the context provides additional cues about the interactionsbetween things and stuff. Hence we need to infuse context into the human-objectinteractions for compositional action recognition. To this end, we first designa spatial-temporal interaction encoder that captures the human-object (things)interactions. The encoder learns the spatio-temporal interaction tokensdisentangled from the background context. The interaction tokens are theninfused with contextual information from the video tokens to model theinteractions between things and stuff. The final context-infusedspatio-temporal interaction tokens are used for compositional actionrecognition. We show the effectiveness of our interaction-centric approach onthe compositional Something-Else dataset where we obtain a new state-of-the-artresult of 83.8% top-1 accuracy outperforming recent important object-centricmethods by a significant margin. Our approach of explicit human-object-stuffinteraction modeling is effective even for standard action recognition datasetssuch as Something-Something-V2 and Epic-Kitchens-100 where we obtain comparableor better performance than state-of-the-art.</description><author>Ramanathan Rajendiran, Debaditya Roy, Basura Fernando</author><pubDate>Thu, 04 May 2023 10:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02673v1</guid></item><item><title>M$^3$Net: Multi-view Encoding, Matching, and Fusion for Few-shot Fine-grained Action Recognition</title><link>http://arxiv.org/abs/2308.03063v1</link><description>Due to the scarcity of manually annotated data required for fine-grainedvideo understanding, few-shot fine-grained (FS-FG) action recognition hasgained significant attention, with the aim of classifying novel fine-grainedaction categories with only a few labeled instances. Despite the progress madein FS coarse-grained action recognition, current approaches encounter twochallenges when dealing with the fine-grained action categories: the inabilityto capture subtle action details and the insufficiency of learning from limiteddata that exhibit high intra-class variance and inter-class similarity. Toaddress these limitations, we propose M$^3$Net, a matching-based framework forFS-FG action recognition, which incorporates \textit{multi-view encoding},\textit{multi-view matching}, and \textit{multi-view fusion} to facilitateembedding encoding, similarity matching, and decision making across multipleviewpoints. \textit{Multi-view encoding} captures rich contextual details fromthe intra-frame, intra-video, and intra-episode perspectives, generatingcustomized higher-order embeddings for fine-grained data. \textit{Multi-viewmatching} integrates various matching functions enabling flexible relationmodeling within limited samples to handle multi-scale spatio-temporalvariations by leveraging the instance-specific, category-specific, andtask-specific perspectives. \textit{Multi-view fusion} consists ofmatching-predictions fusion and matching-losses fusion over the above views,where the former promotes mutual complementarity and the latter enhancesembedding generalizability by employing multi-task collaborative learning.Explainable visualizations and experimental results on three challengingbenchmarks demonstrate the superiority of M$^3$Net in capturing fine-grainedaction details and achieving state-of-the-art performance for FS-FG actionrecognition.</description><author>Hao Tang, Jun Liu, Shuanglin Yan, Rui Yan, Zechao Li, Jinhui Tang</author><pubDate>Sun, 06 Aug 2023 10:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03063v1</guid></item><item><title>On the Importance of Spatial Relations for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2308.07119v1</link><description>Deep learning has achieved great success in video recognition, yet stillstruggles to recognize novel actions when faced with only a few examples. Totackle this challenge, few-shot action recognition methods have been proposedto transfer knowledge from a source dataset to a novel target dataset with onlyone or a few labeled videos. However, existing methods mainly focus on modelingthe temporal relations between the query and support videos while ignoring thespatial relations. In this paper, we find that the spatial misalignment betweenobjects also occurs in videos, notably more common than the temporalinconsistency. We are thus motivated to investigate the importance of spatialrelations and propose a more accurate few-shot action recognition method thatleverages both spatial and temporal information. Particularly, a novel SpatialAlignment Cross Transformer (SA-CT) which learns to re-adjust the spatialrelations and incorporates the temporal information is contributed. Experimentsreveal that, even without using any temporal information, the performance ofSA-CT is comparable to temporal based methods on 3/4 benchmarks. To furtherincorporate the temporal information, we propose a simple yet effectiveTemporal Mixer module. The Temporal Mixer enhances the video representation andimproves the performance of the full SA-CT model, achieving very competitiveresults. In this work, we also exploit large-scale pretrained models forfew-shot action recognition, providing useful insights for this researchdirection.</description><author>Yilun Zhang, Yuqian Fu, Xingjun Ma, Lizhe Qi, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Mon, 14 Aug 2023 13:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07119v1</guid></item><item><title>Position and Orientation-Aware One-Shot Learning for Medical Action Recognition from Signal Data</title><link>http://arxiv.org/abs/2309.15635v1</link><description>In this work, we propose a position and orientation-aware one-shot learningframework for medical action recognition from signal data. The proposedframework comprises two stages and each stage includes signal-level imagegeneration (SIG), cross-attention (CsA), dynamic time warping (DTW) modules andthe information fusion between the proposed privacy-preserved position andorientation features. The proposed SIG method aims to transform the rawskeleton data into privacy-preserved features for training. The CsA module isdeveloped to guide the network in reducing medical action recognition bias andmore focusing on important human body parts for each specific action, aimed ataddressing similar medical action related issues. Moreover, the DTW module isemployed to minimize temporal mismatching between instances and further improvemodel performance. Furthermore, the proposed privacy-preservedorientation-level features are utilized to assist the position-level featuresin both of the two stages for enhancing medical action recognition performance.Extensive experimental results on the widely-used and well-known NTU RGB+D 60,NTU RGB+D 120, and PKU-MMD datasets all demonstrate the effectiveness of theproposed method, which outperforms the other state-of-the-art methods withgeneral dataset partitioning by 2.7%, 6.2% and 4.1%, respectively.</description><author>Leiyu Xie, Yuxing Yang, Zeyu Fu, Syed Mohsen Naqvi</author><pubDate>Wed, 27 Sep 2023 14:08:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15635v1</guid></item><item><title>Fourier Analysis on Robustness of Graph Convolutional Neural Networks for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2305.17939v1</link><description>Using Fourier analysis, we explore the robustness and vulnerability of graphconvolutional neural networks (GCNs) for skeleton-based action recognition. Weadopt a joint Fourier transform (JFT), a combination of the graph Fouriertransform (GFT) and the discrete Fourier transform (DFT), to examine therobustness of adversarially-trained GCNs against adversarial attacks and commoncorruptions. Experimental results with the NTU RGB+D dataset reveal thatadversarial training does not introduce a robustness trade-off betweenadversarial attacks and low-frequency perturbations, which typically occursduring image classification based on convolutional neural networks. Thisfinding indicates that adversarial training is a practical approach toenhancing robustness against adversarial attacks and common corruptions inskeleton-based action recognition. Furthermore, we find that the Fourierapproach cannot explain vulnerability against skeletal part occlusioncorruption, which highlights its limitations. These findings extend ourunderstanding of the robustness of GCNs, potentially guiding the development ofmore robust learning methods for skeleton-based action recognition.</description><author>Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 29 May 2023 09:04:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17939v1</guid></item><item><title>M$^2$DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer</title><link>http://arxiv.org/abs/2305.08877v1</link><description>Ensuring traffic safety and preventing accidents is a critical goal in dailydriving, where the advancement of computer vision technologies can be leveragedto achieve this goal. In this paper, we present a multi-view, multi-scaleframework for naturalistic driving action recognition and localization inuntrimmed videos, namely M$^2$DAR, with a particular focus on detectingdistracted driving behaviors. Our system features a weight-sharing, multi-scaleTransformer-based action recognition network that learns robust hierarchicalrepresentations. Furthermore, we propose a new election algorithm consisting ofaggregation, filtering, merging, and selection processes to refine thepreliminary results from the action recognition module across multiple views.Extensive experiments conducted on the 7th AI City Challenge Track 3 datasetdemonstrate the effectiveness of our approach, where we achieved an overlapscore of 0.5921 on the A2 test set. Our source code is available at\url{https://github.com/PurdueDigitalTwin/M2DAR}.</description><author>Yunsheng Ma, Liangqi Yuan, Amr Abdelraouf, Kyungtae Han, Rohit Gupta, Zihao Li, Ziran Wang</author><pubDate>Sat, 13 May 2023 03:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08877v1</guid></item><item><title>Improving Zero-Shot Action Recognition using Human Instruction with Text Description</title><link>http://arxiv.org/abs/2301.08874v2</link><description>Zero-shot action recognition, which recognizes actions in videos withouthaving received any training examples, is gaining wide attention considering itcan save labor costs and training time. Nevertheless, the performance ofzero-shot learning is still unsatisfactory, which limits its practicalapplication. To solve this problem, this study proposes a framework to improvezero-shot action recognition using human instructions with text descriptions.The proposed framework manually describes video contents, which incurs somelabor costs; in many situations, the labor costs are worth it. We manuallyannotate text features for each action, which can be a word, phrase, orsentence. Then by computing the matching degrees between the video and all textfeatures, we can predict the class of the video. Furthermore, the proposedmodel can also be combined with other models to improve its accuracy. Inaddition, our model can be continuously optimized to improve the accuracy byrepeating human instructions. The results with UCF101 and HMDB51 showed thatour model achieved the best accuracy and improved the accuracies of othermodels.</description><author>Nan Wu, Hiroshi Kera, Kazuhiko Kawamoto</author><pubDate>Mon, 12 Jun 2023 09:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08874v2</guid></item><item><title>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2212.04761v2</link><description>Skeleton-based action recognition has attracted considerable attention due toits compact representation of the human body's skeletal sructure. Many recentmethods have achieved remarkable performance using graph convolutional networks(GCNs) and convolutional neural networks (CNNs), which extract spatial andtemporal features, respectively. Although spatial and temporal dependencies inthe human skeleton have been explored separately, spatio-temporal dependency israrely considered. In this paper, we propose the Spatio-Temporal Curve Network(STC-Net) to effectively leverage the spatio-temporal dependency of the humanskeleton. Our proposed network consists of two novel elements: 1) TheSpatio-Temporal Curve (STC) module; and 2) Dilated Kernels for GraphConvolution (DK-GC). The STC module dynamically adjusts the receptive field byidentifying meaningful node connections between every adjacent frame andgenerating spatio-temporal curves based on the identified node connections,providing an adaptive spatio-temporal coverage. In addition, we propose DK-GCto consider long-range dependencies, which results in a large receptive fieldwithout any additional parameters by applying an extended kernel to the givenadjacency matrices of the graph. Our STC-Net combines these two modules andachieves state-of-the-art performance on four skeleton-based action recognitionbenchmarks.</description><author>Jungho Lee, Minhyeok Lee, Suhwan Cho, Sungmin Woo, Sungjun Jang, Sangyoun Lee</author><pubDate>Wed, 19 Jul 2023 03:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04761v2</guid></item><item><title>Baby Physical Safety Monitoring in Smart Home Using Action Recognition System</title><link>http://arxiv.org/abs/2210.12527v2</link><description>Humans are able to intuitively deduce actions that took place between twostates in observations via deductive reasoning. This is because the brainoperates on a bidirectional communication model, which has radically improvedthe accuracy of recognition and prediction based on features connected toprevious experiences. During the past decade, deep learning models for actionrecognition have significantly improved. However, deep neural networks strugglewith these tasks on a smaller dataset for specific Action Recognition (AR)tasks. As with most action recognition tasks, the ambiguity of accuratelydescribing activities in spatial-temporal data is a drawback that can beovercome by curating suitable datasets, including careful annotations andpreprocessing of video data for analyzing various recognition tasks. In thisstudy, we present a novel lightweight framework combining transfer learningtechniques with a Conv2D LSTM layer to extract features from the pre-trainedI3D model on the Kinetics dataset for a new AR task (Smart Baby Care) thatrequires a smaller dataset and less computational resources. Furthermore, wedeveloped a benchmark dataset and an automated model that uses LSTM convolutionwith I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in asmart baby room. Finally, we implemented video augmentation to improve modelperformance on the smart baby care task. Compared to other benchmark models,our experimental framework achieved better performance with less computationalresources.</description><author>Victor Adewopo, Nelly Elsayed, Kelly Anderson</author><pubDate>Sun, 30 Apr 2023 02:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12527v2</guid></item><item><title>SkeleTR: Towrads Skeleton-based Action Recognition in the Wild</title><link>http://arxiv.org/abs/2309.11445v1</link><description>We present SkeleTR, a new framework for skeleton-based action recognition. Incontrast to prior work, which focuses mainly on controlled environments, wetarget more general scenarios that typically involve a variable number ofpeople and various forms of interaction between people. SkeleTR works with atwo-stage paradigm. It first models the intra-person skeleton dynamics for eachskeleton sequence with graph convolutions, and then uses stacked Transformerencoders to capture person interactions that are important for actionrecognition in general scenarios. To mitigate the negative impact of inaccurateskeleton associations, SkeleTR takes relative short skeleton sequences as inputand increases the number of sequences. As a unified solution, SkeleTR can bedirectly applied to multiple skeleton-based action tasks, including video-levelaction classification, instance-level action detection, and group-levelactivity recognition. It also enables transfer learning and joint trainingacross different action tasks and datasets, which result in performanceimprovement. When evaluated on various skeleton-based action recognitionbenchmarks, SkeleTR achieves the state-of-the-art performance.</description><author>Haodong Duan, Mingze Xu, Bing Shuai, Davide Modolo, Zhuowen Tu, Joseph Tighe, Alessandro Bergamo</author><pubDate>Wed, 20 Sep 2023 17:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11445v1</guid></item><item><title>Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization</title><link>http://arxiv.org/abs/2308.03950v1</link><description>Zero-shot skeleton-based action recognition aims to recognize actions ofunseen categories after training on data of seen categories. The key is tobuild the connection between visual and semantic space from seen to unseenclasses. Previous studies have primarily focused on encoding sequences into asingular feature vector, with subsequent mapping the features to an identicalanchor point within the embedded space. Their performance is hindered by 1) theignorance of the global visual/semantic distribution alignment, which resultsin a limitation to capture the true interdependence between the two spaces. 2)the negligence of temporal information since the frame-wise features with richaction clues are directly pooled into a single feature vector. We propose a newzero-shot skeleton-based action recognition method via mutual information (MI)estimation and maximization. Specifically, 1) we maximize the MI between visualand semantic space for distribution alignment; 2) we leverage the temporalinformation for estimating the MI by encouraging MI to increase as more framesare observed. Extensive experiments on three large-scale skeleton actiondatasets confirm the effectiveness of our method. Code:https://github.com/YujieOuO/SMIE.</description><author>Yujie Zhou, Wenwen Qiang, Anyi Rao, Ning Lin, Bing Su, Jiaqi Wang</author><pubDate>Tue, 08 Aug 2023 00:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03950v1</guid></item><item><title>Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2302.13434v2</link><description>Recently, skeleton-based human action has become a hot research topic becausethe compact representation of human skeletons brings new blood to this researchdomain. As a result, researchers began to notice the importance of using RGB orother sensors to analyze human action by extracting skeleton information.Leveraging the rapid development of deep learning (DL), a significant number ofskeleton-based human action approaches have been presented with fine-designedDL structures recently. However, a well-trained DL model always demandshigh-quality and sufficient data, which is hard to obtain without costing highexpenses and human labor. In this paper, we introduce a novel data augmentationmethod for skeleton-based action recognition tasks, which can effectivelygenerate high-quality and diverse sequential actions. In order to obtainnatural and realistic action sequences, we propose denoising diffusionprobabilistic models (DDPMs) that can generate a series of synthetic actionsequences, and their generation process is precisely guided by aspatial-temporal transformer (ST-Trans). Experimental results show that ourmethod outperforms the state-of-the-art (SOTA) motion generation approaches ondifferent naturality and diversity metrics. It proves that its high-qualitysynthetic data can also be effectively deployed to existing action recognitionmodels with significant performance improvement.</description><author>Yifan Jiang, Han Chen, Hanseok Ko</author><pubDate>Tue, 25 Jul 2023 03:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13434v2</guid></item><item><title>MSQNet: Actor-agnostic Action Recognition with Multi-modal Query</title><link>http://arxiv.org/abs/2307.10763v1</link><description>Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code will be released athttps://github.com/mondalanindya/MSQNet.</description><author>Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</author><pubDate>Thu, 20 Jul 2023 11:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10763v1</guid></item><item><title>Unveiling the Hidden Realm: Self-supervised Skeleton-based Action Recognition in Occluded Environments</title><link>http://arxiv.org/abs/2309.12029v1</link><description>To integrate action recognition methods into autonomous robotic systems, itis crucial to consider adverse situations involving target occlusions. Such ascenario, despite its practical relevance, is rarely addressed in existingself-supervised skeleton-based action recognition methods. To empower robotswith the capacity to address occlusion, we propose a simple and effectivemethod. We first pre-train using occluded skeleton sequences, then use k-meansclustering (KMeans) on sequence embeddings to group semantically similarsamples. Next, we employ K-nearest-neighbor (KNN) to fill in missing skeletondata based on the closest sample neighbors. Imputing incomplete skeletonsequences to create relatively complete sequences as input provides significantbenefits to existing skeleton-based self-supervised models. Meanwhile, buildingon the state-of-the-art Partial Spatio-Temporal Learning (PSTL), we introducean Occluded Partial Spatio-Temporal Learning (OPSTL) framework. Thisenhancement utilizes Adaptive Spatial Masking (ASM) for better use ofhigh-quality, intact skeletons. The effectiveness of our imputation methods isverified on the challenging occluded versions of the NTURGB+D 60 and NTURGB+D120. The source code will be made publicly available athttps://github.com/cyfml/OPSTL.</description><author>Yifei Chen, Kunyu Peng, Alina Roitberg, David Schneider, Jiaming Zhang, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang, Rainer Stiefelhagen</author><pubDate>Thu, 21 Sep 2023 13:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12029v1</guid></item><item><title>Graph Contrastive Learning for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2301.10900v2</link><description>In the field of skeleton-based action recognition, current top-performinggraph convolutional networks (GCNs) exploit intra-sequence context to constructadaptive graphs for feature aggregation. However, we argue that such context isstill \textit{local} since the rich cross-sequence relations have not beenexplicitly investigated. In this paper, we propose a graph contrastive learningframework for skeleton-based action recognition (\textit{SkeletonGCL}) toexplore the \textit{global} context across all sequences. In specific,SkeletonGCL associates graph learning across sequences by enforcing graphs tobe class-discriminative, \emph{i.e.,} intra-class compact and inter-classdispersed, which improves the GCN capacity to distinguish various actionpatterns. Besides, two memory banks are designed to enrich cross-sequencecontext from two complementary levels, \emph{i.e.,} instance and semanticlevels, enabling graph contrastive learning in multiple context scales.Consequently, SkeletonGCL establishes a new training paradigm, and it can beseamlessly incorporated into current GCNs. Without loss of generality, wecombine SkeletonGCL with three GCNs (2S-ACGN, CTR-GCN, and InfoGCN), andachieve consistent improvements on NTU60, NTU120, and NW-UCLA benchmarks. Thesource code will be available at\url{https://github.com/OliverHxh/SkeletonGCL}.</description><author>Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jingdong Wang, Xinggang Wang, Wenyu Liu, Bin Feng</author><pubDate>Sat, 10 Jun 2023 11:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10900v2</guid></item><item><title>Video BagNet: short temporal receptive fields increase robustness in long-term action recognition</title><link>http://arxiv.org/abs/2308.11249v1</link><description>Previous work on long-term video action recognition relies on deep3D-convolutional models that have a large temporal receptive field (RF). Weargue that these models are not always the best choice for temporal modeling invideos. A large temporal receptive field allows the model to encode the exactsub-action order of a video, which causes a performance decrease when testingvideos have a different sub-action order. In this work, we investigate whetherwe can improve the model robustness to the sub-action order by shrinking thetemporal receptive field of action recognition models. For this, we designVideo BagNet, a variant of the 3D ResNet-50 model with the temporal receptivefield size limited to 1, 9, 17 or 33 frames. We analyze Video BagNet onsynthetic and real-world video datasets and experimentally compare models withvarying temporal receptive fields. We find that short receptive fields arerobust to sub-action order changes, while larger temporal receptive fields aresensitive to the sub-action order.</description><author>Ombretta Strafforello, Xin Liu, Klamer Schutte, Jan van Gemert</author><pubDate>Tue, 22 Aug 2023 08:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11249v1</guid></item><item><title>IndGIC: Supervised Action Recognition under Low Illumination</title><link>http://arxiv.org/abs/2308.15345v1</link><description>Technologies of human action recognition in the dark are gaining more andmore attention as huge demand in surveillance, motion control andhuman-computer interaction. However, because of limitation in image enhancementmethod and low-lighting video datasets, e.g. labeling cost, existing methodsmeet some problems. Some video-based approached are effect and efficient inspecific datasets but cannot generalize to most cases while others methodsusing multiple sensors rely heavily to prior knowledge to deal with noisynature from video stream. In this paper, we proposes action recognition methodusing deep multi-input network. Furthermore, we proposed a Independent GammaIntensity Corretion (Ind-GIC) to enhance poor-illumination video, generatingone gamma for one frame to increase enhancement performance. To prove ourmethod is effective, there is some evaluation and comparison between our methodand existing methods. Experimental results show that our model achieves highaccuracy in on ARID dataset.</description><author>Jingbo Zeng</author><pubDate>Tue, 29 Aug 2023 15:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15345v1</guid></item><item><title>Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective</title><link>http://arxiv.org/abs/2305.15699v1</link><description>Understanding action recognition in egocentric videos has emerged as a vitalresearch topic with numerous practical applications. With the limitation in thescale of egocentric data collection, learning robust deep learning-based actionrecognition models remains difficult. Transferring knowledge learned from thelarge-scale exocentric data to the egocentric data is challenging due to thedifference in videos across views. Our work introduces a novel cross-viewlearning approach to action recognition (CVAR) that effectively transfersknowledge from the exocentric to the egocentric view. First, we introduce anovel geometric-based constraint into the self-attention mechanism inTransformer based on analyzing the camera positions between two views. Then, wepropose a new cross-view self-attention loss learned on unpaired cross-viewdata to enforce the self-attention mechanism learning to transfer knowledgeacross views. Finally, to further improve the performance of our cross-viewlearning approach, we present the metrics to measure the correlations in videosand attention maps effectively. Experimental results on standard egocentricaction recognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, andEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-artperformance.</description><author>Thanh-Dat Truong, Khoa Luu</author><pubDate>Thu, 25 May 2023 05:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15699v1</guid></item><item><title>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly</title><link>http://arxiv.org/abs/2307.09238v1</link><description>As collaborative robots (cobots) continue to gain popularity in industrialmanufacturing, effective human-robot collaboration becomes crucial. Cobotsshould be able to recognize human actions to assist with assembly tasks and actautonomously. To achieve this, skeleton-based approaches are often used due totheir ability to generalize across various people and environments. Althoughbody skeleton approaches are widely used for action recognition, they may notbe accurate enough for assembly actions where the worker's fingers and handsplay a significant role. To address this limitation, we propose a method inwhich less detailed body skeletons are combined with highly detailed handskeletons. We investigate CNNs and transformers, the latter of which areparticularly adept at extracting and combining important information from bothskeleton types using attention. This paper demonstrates the effectiveness ofour proposed approach in enhancing action recognition in assembly scenarios.</description><author>Dustin Aganian, Mona Khler, Benedict Stephan, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Tue, 18 Jul 2023 14:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09238v1</guid></item><item><title>How Object Information Improves Skeleton-based Human Action Recognition in Assembly Tasks</title><link>http://arxiv.org/abs/2306.05844v1</link><description>As the use of collaborative robots (cobots) in industrial manufacturingcontinues to grow, human action recognition for effective human-robotcollaboration becomes increasingly important. This ability is crucial forcobots to act autonomously and assist in assembly tasks. Recently,skeleton-based approaches are often used as they tend to generalize better todifferent people and environments. However, when processing skeletons alone,information about the objects a human interacts with is lost. Therefore, wepresent a novel approach of integrating object information into skeleton-basedaction recognition. We enhance two state-of-the-art methods by treating objectcenters as further skeleton joints. Our experiments on the assembly datasetIKEA ASM show that our approach improves the performance of thesestate-of-the-art methods to a large extent when combining skeleton joints withobjects predicted by a state-of-the-art instance segmentation model. Ourresearch sheds light on the benefits of combining skeleton joints with objectinformation for human action recognition in assembly tasks. We analyze theeffect of the object detector on the combination for action classification anddiscuss the important factors that must be taken into account.</description><author>Dustin Aganian, Mona Khler, Sebastian Baake, Markus Eisenbach, Horst-Michael Gross</author><pubDate>Fri, 09 Jun 2023 13:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05844v1</guid></item><item><title>SOAR: Scene-debiasing Open-set Action Recognition</title><link>http://arxiv.org/abs/2309.01265v1</link><description>Deep learning models have a risk of utilizing spurious clues to makepredictions, such as recognizing actions based on the background scene. Thisissue can severely degrade the open-set action recognition performance when thetesting samples have different scene distributions from the training samples.To mitigate this problem, we propose a novel method, called Scene-debiasingOpen-set Action Recognition (SOAR), which features an adversarial scenereconstruction module and an adaptive adversarial scene classification module.The former prevents the decoder from reconstructing the video background givenvideo features, and thus helps reduce the background information in featurelearning. The latter aims to confuse scene type classification given videofeatures, with a specific emphasis on the action foreground, and helps to learnscene-invariant information. In addition, we design an experiment to quantifythe scene bias. The results indicate that the current open-set actionrecognizers are biased toward the scene, and our proposed SOAR method bettermitigates such bias. Furthermore, our extensive experiments demonstrate thatour method outperforms state-of-the-art methods, and the ablation studiesconfirm the effectiveness of our proposed modules.</description><author>Yuanhao Zhai, Ziyi Liu, Zhenyu Wu, Yi Wu, Chunluan Zhou, David Doermann, Junsong Yuan, Gang Hua</author><pubDate>Sun, 03 Sep 2023 21:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01265v1</guid></item><item><title>FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</title><link>http://arxiv.org/abs/2306.11046v1</link><description>Existing skeleton-based action recognition methods typically follow acentralized learning paradigm, which can pose privacy concerns when exposinghuman-related videos. Federated Learning (FL) has attracted much attention dueto its outstanding advantages in privacy-preserving. However, directly applyingFL approaches to skeleton videos suffers from unstable training. In this paper,we investigate and discover that the heterogeneous human topology graphstructure is the crucial factor hindering training stability. To address thislimitation, we pioneer a novel Federated Skeleton-based Action Recognition(FSAR) paradigm, which enables the construction of a globally generalized modelwithout accessing local sensitive data. Specifically, we introduce an AdaptiveTopology Structure (ATS), separating generalization and personalization bylearning a domain-invariant topology shared across clients and adomain-specific topology decoupled from global model aggregation.Furthermore,we explore Multi-grain Knowledge Distillation (MKD) to mitigate the discrepancybetween clients and server caused by distinct updating patterns throughaligning shallow block-wise motion features. Extensive experiments on multipledatasets demonstrate that FSAR outperforms state-of-the-art FL-based methodswhile inherently protecting privacy.</description><author>Jingwen Guo, Hong Liu, Shitong Sun, Tianyu Guo, Min Zhang, Chenyang Si</author><pubDate>Mon, 19 Jun 2023 17:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11046v1</guid></item><item><title>SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-supervised Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2309.05834v1</link><description>Contrastive learning has achieved great success in skeleton-based actionrecognition. However, most existing approaches encode the skeleton sequences asentangled spatiotemporal representations and confine the contrasts to the samelevel of representation. Instead, this paper introduces a novel contrastivelearning framework, namely Spatiotemporal Clues Disentanglement Network(SCD-Net). Specifically, we integrate the decoupling module with a featureextractor to derive explicit clues from spatial and temporal domainsrespectively. As for the training of SCD-Net, with a constructed global anchor,we encourage the interaction between the anchor and extracted clues. Further,we propose a new masking strategy with structural constraints to strengthen thecontextual associations, leveraging the latest development from masked imagemodelling into the proposed SCD-Net. We conduct extensive evaluations on theNTU-RGB+D (60&amp;120) and PKU-MMD (I&amp;II) datasets, covering various downstreamtasks such as action recognition, action retrieval, transfer learning, andsemi-supervised learning. The experimental results demonstrate theeffectiveness of our method, which outperforms the existing state-of-the-art(SOTA) approaches significantly.</description><author>Cong Wu, Xiao-Jun Wu, Josef Kittler, Tianyang Xu, Sara Atito, Muhammad Awais, Zhenhua Feng</author><pubDate>Mon, 11 Sep 2023 22:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05834v1</guid></item><item><title>DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition</title><link>http://arxiv.org/abs/2308.12501v1</link><description>Graph Convolutional Networks (GCNs) have been widely used in skeleton-basedhuman action recognition. In GCN-based methods, the spatio-temporal graph isfundamental for capturing motion patterns. However, existing approaches ignorethe physical dependency and synchronized spatio-temporal correlations betweenjoints, which limits the representation capability of GCNs. To solve theseproblems, we construct the directed diffusion graph for action modeling andintroduce the activity partition strategy to optimize the weight sharingmechanism of graph convolution kernels. In addition, we present thespatio-temporal synchronization encoder to embed synchronized spatio-temporalsemantics. Finally, we propose Directed Diffusion Graph Convolutional Network(DD-GCN) for action recognition, and the experiments on three public datasets:NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-artperformance of our method.</description><author>Chang Li, Qian Huang, Yingchi Mao</author><pubDate>Thu, 24 Aug 2023 02:53:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12501v1</guid></item><item><title>Topology-aware MLP for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v1</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, existing previous GCN-based methodshave relied excessively on elaborate human body priors and constructed complexfeature aggregation mechanisms, which limits the generalizability of networks.To solve these problems, we propose a novel Spatial Topology Gating Unit(STGU), which is an MLP-based variant without extra priors, to capture theco-occurrence topology features that encode the spatial dependency across alljoints. In STGU, to model the sample-specific and completely independentpoint-wise topology attention, a new gate-based feature interaction mechanismis introduced to activate the features point-to-point by the attention mapgenerated from the input. Based on the STGU, in this work, we propose the firsttopology-aware MLP-based model, Ta-MLP, for skeleton-based action recognition.In comparison with existing previous methods on three large-scale datasets,Ta-MLP achieves competitive performance. In addition, Ta-MLP reduces theparameters by up to 62.5% with favorable results. Compared with previousstate-of-the-art (SOAT) approaches, Ta-MLP pushes the frontier of real-timeaction recognition. The code will be available athttps://github.com/BUPTSJZhang/Ta-MLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Wed, 30 Aug 2023 14:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v1</guid></item><item><title>Topology-aware MLP for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v2</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, existing previous GCN-based methodshave relied excessively on elaborate human body priors and constructed complexfeature aggregation mechanisms, which limits the generalizability of networks.To solve these problems, we propose a novel Spatial Topology Gating Unit(STGU), which is an MLP-based variant without extra priors, to capture theco-occurrence topology features that encode the spatial dependency across alljoints. In STGU, to model the sample-specific and completely independentpoint-wise topology attention, a new gate-based feature interaction mechanismis introduced to activate the features point-to-point by the attention mapgenerated from the input. Based on the STGU, in this work, we propose the firsttopology-aware MLP-based model, Ta-MLP, for skeleton-based action recognition.In comparison with existing previous methods on three large-scale datasets,Ta-MLP achieves competitive performance. In addition, Ta-MLP reduces theparameters by up to 62.5% with favorable results. Compared with previousstate-of-the-art (SOAT) approaches, Ta-MLP pushes the frontier of real-timeaction recognition. The code will be available athttps://github.com/BUPTSJZhang/Ta-MLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Mon, 04 Sep 2023 08:08:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v2</guid></item><item><title>SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder and Transformer Network</title><link>http://arxiv.org/abs/2306.17574v1</link><description>Recent advancements in technology have expanded the possibilities of humanaction recognition by leveraging 3D data, which offers a richer representationof actions through the inclusion of depth information, enabling more accurateanalysis of spatial and temporal characteristics. However, 3D human actionrecognition is a challenging task due to the irregularity and Disarrangement ofthe data points in action sequences. In this context, we present our novelmodel for human action recognition from fixed topology mesh sequences based onSpiral Auto-encoder and Transformer Network, namely SpATr. The proposed methodfirst disentangles space and time in the mesh sequences. Then, an auto-encoderis utilized to extract spatial geometrical features, and tiny transformer isused to capture the temporal evolution of the sequence. Previous methods eitheruse 2D depth images, sample skeletons points or they require a huge amount ofmemory leading to the ability to process short sequences only. In this work, weshow competitive recognition rate and high memory efficiency by building ourauto-encoder based on spiral convolutions, which are light weight convolutiondirectly applied to mesh data with fixed topologies, and by modeling temporalevolution using a attention, that can handle large sequences. The proposedmethod is evaluated on on two 3D human action datasets: MoVi and BMLrub fromthe Archive of Motion Capture As Surface Shapes (AMASS). The results analysisshows the effectiveness of our method in 3D human action recognition whilemaintaining high memory efficiency. The code will soon be made publiclyavailable.</description><author>Hamza Bouzid, Lahoucine Ballihi</author><pubDate>Fri, 30 Jun 2023 12:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17574v1</guid></item><item><title>Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations</title><link>http://arxiv.org/abs/2211.13466v3</link><description>Contrastive learning has been proven beneficial for self-supervisedskeleton-based action recognition. Most contrastive learning methods utilizecarefully designed augmentations to generate different movement patterns ofskeletons for the same semantics. However, it is still a pending issue to applystrong augmentations, which distort the images/skeletons' structures and causesemantic loss, due to their resulting unstable training. In this paper, weinvestigate the potential of adopting strong augmentations and propose ageneral hierarchical consistent contrastive learning framework (HiCLR) forskeleton-based action recognition. Specifically, we first design a gradualgrowing augmentation policy to generate multiple ordered positive pairs, whichguide to achieve the consistency of the learned representation from differentviews. Then, an asymmetric loss is proposed to enforce the hierarchicalconsistency via a directional clustering operation in the feature space,pulling the representations from strongly augmented views closer to those fromweakly augmented views for better generalizability. Meanwhile, we propose andevaluate three kinds of strong augmentations for 3D skeletons to demonstratethe effectiveness of our method. Extensive experiments show that HiCLRoutperforms the state-of-the-art methods notably on three large-scale datasets,i.e., NTU60, NTU120, and PKUMMD.</description><author>Jiahang Zhang, Lilang Lin, Jiaying Liu</author><pubDate>Mon, 10 Jul 2023 11:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13466v3</guid></item><item><title>Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition</title><link>http://arxiv.org/abs/2308.07571v1</link><description>This paper presents Ske2Grid, a new representation learning framework forimproved skeleton-based action recognition. In Ske2Grid, we define a regularconvolution operation upon a novel grid representation of human skeleton, whichis a compact image-like grid patch constructed and learned through three noveldesigns. Specifically, we propose a graph-node index transform (GIT) toconstruct a regular grid patch through assigning the nodes in the skeletongraph one by one to the desired grid cells. To ensure that GIT is a bijectionand enrich the expressiveness of the grid representation, an up-samplingtransform (UPT) is learned to interpolate the skeleton graph nodes for fillingthe grid patch to the full. To resolve the problem when the one-step UPT isaggressive and further exploit the representation capability of the grid patchwith increasing spatial size, a progressive learning strategy (PLS) is proposedwhich decouples the UPT into multiple steps and aligns them to multiple pairedGITs through a compact cascaded design learned progressively. We constructnetworks upon prevailing graph convolution networks and conduct experiments onsix mainstream skeleton-based action recognition datasets. Experiments showthat our Ske2Grid significantly outperforms existing GCN-based solutions underdifferent benchmark settings, without bells and whistles. Code and models areavailable at https://github.com/OSVAI/Ske2Grid</description><author>Dongqi Cai, Yangyuxuan Kang, Anbang Yao, Yurong Chen</author><pubDate>Tue, 15 Aug 2023 05:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07571v1</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v1</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Fri, 19 May 2023 07:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v1</guid></item><item><title>Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action Recognition through Redefined Skeletal Topology Awareness</title><link>http://arxiv.org/abs/2305.11468v2</link><description>Graph Convolutional Networks (GCNs) have long defined the state-of-the-art inskeleton-based action recognition, leveraging their ability to unravel thecomplex dynamics of human joint topology through the graph's adjacency matrix.However, an inherent flaw has come to light in these cutting-edge models: theytend to optimize the adjacency matrix jointly with the model weights. Thisprocess, while seemingly efficient, causes a gradual decay of bone connectivitydata, culminating in a model indifferent to the very topology it sought to map.As a remedy, we propose a threefold strategy: (1) We forge an innovativepathway that encodes bone connectivity by harnessing the power of graphdistances. This approach preserves the vital topological nuances often lost inconventional GCNs. (2) We highlight an oft-overlooked feature - the temporalmean of a skeletal sequence, which, despite its modest guise, carries highlyaction-specific information. (3) Our investigation revealed strong variationsin joint-to-joint relationships across different actions. This finding exposesthe limitations of a single adjacency matrix in capturing the variations ofrelational configurations emblematic of human movement, which we remedy byproposing an efficient refinement to Graph Convolutions (GC) - the BlockGC.This evolution slashes parameters by a substantial margin (above 40%), whileelevating performance beyond original GCNs. Our full model, the BlockGCN,establishes new standards in skeleton-based action recognition for small modelsizes. Its high accuracy, notably on the large-scale NTU RGB+D 120 dataset,stand as compelling proof of the efficacy of BlockGCN. The source code andmodel can be found at https://github.com/ZhouYuxuanYX/BlockGCN.</description><author>Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong Xie, Margret Keuper</author><pubDate>Thu, 25 May 2023 19:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11468v2</guid></item><item><title>Selective Volume Mixup for Video Action Recognition</title><link>http://arxiv.org/abs/2309.09534v1</link><description>The recent advances in Convolutional Neural Networks (CNNs) and VisionTransformers have convincingly demonstrated high learning capability for videoaction recognition on large datasets. Nevertheless, deep models often sufferfrom the overfitting effect on small-scale datasets with a limited number oftraining videos. A common solution is to exploit the existing imageaugmentation strategies for each frame individually including Mixup, Cutmix,and RandAugment, which are not particularly optimized for video data. In thispaper, we propose a novel video augmentation strategy named Selective VolumeMixup (SV-Mix) to improve the generalization ability of deep models withlimited training videos. SV-Mix devises a learnable selective module to choosethe most informative volumes from two videos and mixes the volumes up toachieve a new training video. Technically, we propose two new modules, i.e., aspatial selective module to select the local patches for each spatial position,and a temporal selective module to mix the entire frames for each timestamp andmaintain the spatial pattern. At each time, we randomly choose one of the twomodules to expand the diversity of training samples. The selective modules arejointly optimized with the video action recognition framework to find theoptimal augmentation strategy. We empirically demonstrate the merits of theSV-Mix augmentation on a wide range of video action recognition benchmarks andconsistently boot the performances of both CNN-based and transformer-basedmodels.</description><author>Yi Tan, Zhaofan Qiu, Yanbin Hao, Ting Yao, Xiangnan He, Tao Mei</author><pubDate>Mon, 18 Sep 2023 08:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09534v1</guid></item><item><title>FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition</title><link>http://arxiv.org/abs/2305.18479v1</link><description>3D Convolutional Neural Networks are gaining increasing attention fromresearchers and practitioners and have found applications in many domains, suchas surveillance systems, autonomous vehicles, human monitoring systems, andvideo retrieval. However, their widespread adoption is hindered by their highcomputational and memory requirements, especially when resource-constrainedsystems are targeted. This paper addresses the problem of mapping X3D, astate-of-the-art model in Human Action Recognition that achieves accuracy of95.5\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflowgenerates an optimised stream-based hardware system, taking into account theavailable resources and off-chip memory characteristics of the FPGA device. Thegenerated designs push further the current performance-accuracy pareto front,and enable for the first time the targeting of such complex model architecturesfor the Human Action Recognition task.</description><author>Petros Toupas, Christos-Savvas Bouganis, Dimitrios Tzovaras</author><pubDate>Mon, 29 May 2023 12:17:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18479v1</guid></item><item><title>Deep Neural Networks in Video Human Action Recognition: A Review</title><link>http://arxiv.org/abs/2305.15692v1</link><description>Currently, video behavior recognition is one of the most foundational tasksof computer vision. The 2D neural networks of deep learning are built forrecognizing pixel-level information such as images with RGB, RGB-D, or opticalflow formats, with the current increasingly wide usage of surveillance videoand more tasks related to human action recognition. There are increasing tasksrequiring temporal information for frames dependency analysis. The researchershave widely studied video-based recognition rather thanimage-based(pixel-based) only to extract more informative elements fromgeometry tasks. Our current related research addresses multiple novel proposedresearch works and compares their advantages and disadvantages between thederived deep learning frameworks rather than machine learning frameworks. Thecomparison happened between existing frameworks and datasets, which are videoformat data only. Due to the specific properties of human actions and theincreasingly wide usage of deep neural networks, we collected all researchworks within the last three years between 2020 to 2022. In our article, theperformance of deep neural networks surpassed most of the techniques in thefeature learning and extraction tasks, especially video action recognition.</description><author>Zihan Wang, Yang Yang, Zhi Liu, Yifan Zheng</author><pubDate>Thu, 25 May 2023 04:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15692v1</guid></item><item><title>TransNet: A Transfer Learning-Based Network for Human Action Recognition</title><link>http://arxiv.org/abs/2309.06951v1</link><description>Human action recognition (HAR) is a high-level and significant research areain computer vision due to its ubiquitous applications. The main limitations ofthe current HAR models are their complex structures and lengthy training time.In this paper, we propose a simple yet versatile and effective end-to-end deeplearning architecture, coined as TransNet, for HAR. TransNet decomposes thecomplex 3D-CNNs into 2D- and 1D-CNNs, where the 2D- and 1D-CNN componentsextract spatial features and temporal patterns in videos, respectively.Benefiting from its concise architecture, TransNet is ideally compatible withany pretrained state-of-the-art 2D-CNN models in other fields, beingtransferred to serve the HAR task. In other words, it naturally leverages thepower and success of transfer learning for HAR, bringing huge advantages interms of efficiency and effectiveness. Extensive experimental results and thecomparison with the state-of-the-art models demonstrate the superiorperformance of the proposed TransNet in HAR in terms of flexibility, modelcomplexity, training speed and classification accuracy.</description><author>K. Alomar, X. Cai</author><pubDate>Wed, 13 Sep 2023 14:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06951v1</guid></item><item><title>Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization</title><link>http://arxiv.org/abs/2304.08799v3</link><description>3D Skeleton-based human action recognition has attracted increasing attentionin recent years. Most of the existing work focuses on supervised learning whichrequires a large number of labeled action sequences that are often expensiveand time-consuming to annotate. In this paper, we address self-supervised 3Daction representation learning for skeleton-based action recognition. Weinvestigate self-supervised representation learning and design a novel skeletoncloud colorization technique that is capable of learning spatial and temporalskeleton representations from unlabeled skeleton sequence data. We represent askeleton action sequence as a 3D skeleton cloud and colorize each point in thecloud according to its temporal and spatial orders in the original(unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud,we design an auto-encoder framework that can learn spatial-temporal featuresfrom the artificial color labels of skeleton joints effectively. Specifically,we design a two-steam pretraining network that leverages fine-grained andcoarse-grained colorization to learn multi-scale spatial-temporal features. Inaddition, we design a Masked Skeleton Cloud Repainting task that can pretrainthe designed auto-encoder framework to learn informative representations. Weevaluate our skeleton cloud colorization approach with linear classifierstrained under different configurations, including unsupervised,semi-supervised, fully-supervised, and transfer learning settings. Extensiveexperiments on NTU RGB+D, NTU RGB+D 120, PKU-MMD, NW-UCLA, and UWA3D datasetsshow that the proposed method outperforms existing unsupervised andsemi-supervised 3D action recognition methods by large margins and achievescompetitive performance in supervised 3D action recognition as well.</description><author>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Yongjian Hu, Alex C. Kot</author><pubDate>Mon, 16 Oct 2023 09:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08799v3</guid></item><item><title>MAiVAR-T: Multimodal Audio-image and Video Action Recognizer using Transformers</title><link>http://arxiv.org/abs/2308.03741v1</link><description>In line with the human capacity to perceive the world by simultaneouslyprocessing and integrating high-dimensional inputs from multiple modalitieslike vision and audio, we propose a novel model, MAiVAR-T (MultimodalAudio-Image to Video Action Recognition Transformer). This model employs anintuitive approach for the combination of audio-image and video modalities,with a primary aim to escalate the effectiveness of multimodal human actionrecognition (MHAR). At the core of MAiVAR-T lies the significance of distillingsubstantial representations from the audio modality and transmuting these intothe image domain. Subsequently, this audio-image depiction is fused with thevideo modality to formulate a unified representation. This concerted approachstrives to exploit the contextual richness inherent in both audio and videomodalities, thereby promoting action recognition. In contrast to existingstate-of-the-art strategies that focus solely on audio or video modalities,MAiVAR-T demonstrates superior performance. Our extensive empirical evaluationsconducted on a benchmark action recognition dataset corroborate the model'sremarkable performance. This underscores the potential enhancements derivedfrom integrating audio and video modalities for action recognition purposes.</description><author>Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, Naveed Akhtar</author><pubDate>Tue, 01 Aug 2023 12:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03741v1</guid></item><item><title>CDFSL-V: Cross-Domain Few-Shot Learning for Videos</title><link>http://arxiv.org/abs/2309.03989v1</link><description>Few-shot video action recognition is an effective approach to recognizing newcategories with only a few labeled examples, thereby reducing the challengesassociated with collecting and annotating large-scale video datasets. Existingmethods in video action recognition rely on large labeled datasets from thesame domain. However, this setup is not realistic as novel categories may comefrom different data domains that may have different spatial and temporalcharacteristics. This dissimilarity between the source and target domains canpose a significant challenge, rendering traditional few-shot action recognitiontechniques ineffective. To address this issue, in this work, we propose a novelcross-domain few-shot video action recognition method that leveragesself-supervised learning and curriculum learning to balance the informationfrom the source and target domains. To be particular, our method employs amasked autoencoder-based self-supervised training objective to learn from bothsource and target data in a self-supervised manner. Then a progressivecurriculum balances learning the discriminative information from the sourcedataset with the generic information learned from the target domain. Initially,our curriculum utilizes supervised learning to learn class discriminativefeatures from the source data. As the training progresses, we transition tolearning target-domain-specific features. We propose a progressive curriculumto encourage the emergence of rich features in the target domain based on classdiscriminative supervised features in the source domain. %a schedule that helpswith this transition. We evaluate our method on several challenging benchmarkdatasets and demonstrate that our approach outperforms existing cross-domainfew-shot learning techniques. Our code is available at\hyperlink{https://github.com/Sarinda251/CDFSL-V}{https://github.com/Sarinda251/CDFSL-V}</description><author>Sarinda Samarasinghe, Mamshad Nayeem Rizve, Navid Kardan, Mubarak Shah</author><pubDate>Thu, 07 Sep 2023 20:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03989v1</guid></item><item><title>CDFSL-V: Cross-Domain Few-Shot Learning for Videos</title><link>http://arxiv.org/abs/2309.03989v2</link><description>Few-shot video action recognition is an effective approach to recognizing newcategories with only a few labeled examples, thereby reducing the challengesassociated with collecting and annotating large-scale video datasets. Existingmethods in video action recognition rely on large labeled datasets from thesame domain. However, this setup is not realistic as novel categories may comefrom different data domains that may have different spatial and temporalcharacteristics. This dissimilarity between the source and target domains canpose a significant challenge, rendering traditional few-shot action recognitiontechniques ineffective. To address this issue, in this work, we propose a novelcross-domain few-shot video action recognition method that leveragesself-supervised learning and curriculum learning to balance the informationfrom the source and target domains. To be particular, our method employs amasked autoencoder-based self-supervised training objective to learn from bothsource and target data in a self-supervised manner. Then a progressivecurriculum balances learning the discriminative information from the sourcedataset with the generic information learned from the target domain. Initially,our curriculum utilizes supervised learning to learn class discriminativefeatures from the source data. As the training progresses, we transition tolearning target-domain-specific features. We propose a progressive curriculumto encourage the emergence of rich features in the target domain based on classdiscriminative supervised features in the source domain. We evaluate our methodon several challenging benchmark datasets and demonstrate that our approachoutperforms existing cross-domain few-shot learning techniques. Our code isavailable at https://github.com/Sarinda251/CDFSL-V</description><author>Sarinda Samarasinghe, Mamshad Nayeem Rizve, Navid Kardan, Mubarak Shah</author><pubDate>Fri, 15 Sep 2023 18:24:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03989v2</guid></item><item><title>Video Action Recognition Collaborative Learning with Dynamics via PSO-ConvNet Transformer</title><link>http://arxiv.org/abs/2302.09187v3</link><description>Recognizing human actions in video sequences, known as Human ActionRecognition (HAR), is a challenging task in pattern recognition. WhileConvolutional Neural Networks (ConvNets) have shown remarkable success in imagerecognition, they are not always directly applicable to HAR, as temporalfeatures are critical for accurate classification. In this paper, we propose anovel dynamic PSO-ConvNet model for learning actions in videos, building on ourrecent work in image recognition. Our approach leverages a framework where theweight vector of each neural network represents the position of a particle inphase space, and particles share their current weight vectors and gradientestimates of the Loss function. To extend our approach to video, we integrateConvNets with state-of-the-art temporal methods such as Transformer andRecurrent Neural Networks. Our experimental results on the UCF-101 datasetdemonstrate substantial improvements of up to 9% in accuracy, which confirmsthe effectiveness of our proposed method. In addition, we conducted experimentson larger and more variety of datasets including Kinetics-400 and HMDB-51 andobtained preference for Collaborative Learning in comparison withNon-Collaborative Learning (Individual Learning). Overall, our dynamicPSO-ConvNet model provides a promising direction for improving HAR by bettercapturing the spatio-temporal dynamics of human actions in videos. The code isavailable athttps://github.com/leonlha/Video-Action-Recognition-Collaborative-Learning-with-Dynamics-via-PSO-ConvNet-Transformer.</description><author>Nguyen Huu Phong, Bernardete Ribeiro</author><pubDate>Thu, 21 Sep 2023 09:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09187v3</guid></item><item><title>Universal Prototype Transport for Zero-Shot Action Recognition and Localization</title><link>http://arxiv.org/abs/2203.03971v2</link><description>This work addresses the problem of recognizing action categories in videoswhen no training examples are available. The current state-of-the-art enablessuch a zero-shot recognition by learning universal mappings from videos to asemantic space, either trained on large-scale seen actions or on objects. Whileeffective, we find that universal action and object mappings are biased tospecific regions in the semantic space. These biases lead to a fundamentalproblem: many unseen action categories are simply never inferred duringtesting. For example on UCF-101, a quarter of the unseen actions are out ofreach with a state-of-the-art universal action model. To that end, this paperintroduces universal prototype transport for zero-shot action recognition. Themain idea is to re-position the semantic prototypes of unseen actions bymatching them to the distribution of all test videos. For universal actionmodels, we propose to match distributions through a hyperspherical optimaltransport from unseen action prototypes to the set of all projected testvideos. The resulting transport couplings in turn determine the targetprototype for each unseen action. Rather than directly using the targetprototype as final result, we re-position unseen action prototypes along thegeodesic spanned by the original and target prototypes as a form of semanticregularization. For universal object models, we outline a variant that definestarget prototypes based on an optimal transport between unseen actionprototypes and object prototypes. Empirically, we show that universal prototypetransport diminishes the biased selection of unseen action prototypes andboosts both universal action and object models for zero-shot classification andspatio-temporal localization.</description><author>Pascal Mettes</author><pubDate>Tue, 01 Aug 2023 10:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.03971v2</guid></item><item><title>What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations</title><link>http://arxiv.org/abs/2306.08713v1</link><description>We propose and address a new generalisation problem: can a model trained foraction recognition successfully classify actions when they are performed withina previously unseen scenario and in a previously unseen location? To answerthis question, we introduce the Action Recognition Generalisation Overscenarios and locations dataset (ARGO1M), which contains 1.1M video clips fromthe large-scale Ego4D dataset, across 10 scenarios and 13 locations. Wedemonstrate recognition models struggle to generalise over 10 proposed testsplits, each of an unseen scenario in an unseen location. We thus propose CIR,a method to represent each video as a Cross-Instance Reconstruction of videosfrom other domains. Reconstructions are paired with text narrations to guidethe learning of a domain generalisable representation. We provide extensiveanalysis and ablations on ARGO1M that show CIR outperforms prior domaingeneralisation works on all test splits. Code and data:https://chiaraplizz.github.io/what-can-a-cook/.</description><author>Chiara Plizzari, Toby Perrett, Barbara Caputo, Dima Damen</author><pubDate>Wed, 14 Jun 2023 20:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08713v1</guid></item><item><title>What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations</title><link>http://arxiv.org/abs/2306.08713v2</link><description>We propose and address a new generalisation problem: can a model trained foraction recognition successfully classify actions when they are performed withina previously unseen scenario and in a previously unseen location? To answerthis question, we introduce the Action Recognition Generalisation Overscenarios and locations dataset (ARGO1M), which contains 1.1M video clips fromthe large-scale Ego4D dataset, across 10 scenarios and 13 locations. Wedemonstrate recognition models struggle to generalise over 10 proposed testsplits, each of an unseen scenario in an unseen location. We thus propose CIR,a method to represent each video as a Cross-Instance Reconstruction of videosfrom other domains. Reconstructions are paired with text narrations to guidethe learning of a domain generalisable representation. We provide extensiveanalysis and ablations on ARGO1M that show CIR outperforms prior domaingeneralisation works on all test splits. Code and data:https://chiaraplizz.github.io/what-can-a-cook/.</description><author>Chiara Plizzari, Toby Perrett, Barbara Caputo, Dima Damen</author><pubDate>Thu, 24 Aug 2023 11:06:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08713v2</guid></item><item><title>Cross-view Action Recognition via Contrastive View-invariant Representation</title><link>http://arxiv.org/abs/2305.01733v1</link><description>Cross view action recognition (CVAR) seeks to recognize a human action whenobserved from a previously unseen viewpoint. This is a challenging problemsince the appearance of an action changes significantly with the viewpoint.Applications of CVAR include surveillance and monitoring of assisted livingfacilities where is not practical or feasible to collect large amounts oftraining data when adding a new camera. We present a simple yet efficient CVARframework to learn invariant features from either RGB videos, 3D skeleton data,or both. The proposed approach outperforms the current state-of-the-artachieving similar levels of performance across input modalities: 99.4% (RGB)and 99.9% (3D skeletons), 99.4% (RGB) and 99.9% (3D Skeletons), 97.3% (RGB),and 99.2% (3D skeletons), and 84.4%(RGB) for the N-UCLA, NTU-RGB+D 60,NTU-RGB+D 120, and UWA3DII datasets, respectively.</description><author>Yuexi Zhang, Dan Luo, Balaji Sundareshan, Octavia Camps, Mario Sznaier</author><pubDate>Tue, 02 May 2023 20:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01733v1</guid></item><item><title>Multi-Semantic Fusion Model for Generalized Zero-Shot Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2309.09592v1</link><description>Generalized zero-shot skeleton-based action recognition (GZSSAR) is a newchallenging problem in computer vision community, which requires models torecognize actions without any training samples. Previous studies only utilizethe action labels of verb phrases as the semantic prototypes for learning themapping from skeleton-based actions to a shared semantic space. However, thelimited semantic information of action labels restricts the generalizationability of skeleton features for recognizing unseen actions. In order to solvethis dilemma, we propose a multi-semantic fusion (MSF) model for improving theperformance of GZSSAR, where two kinds of class-level textual descriptions(i.e., action descriptions and motion descriptions), are collected as auxiliarysemantic information to enhance the learning efficacy of generalizable skeletonfeatures. Specially, a pre-trained language encoder takes the actiondescriptions, motion descriptions and original class labels as inputs to obtainrich semantic features for each action class, while a skeleton encoder isimplemented to extract skeleton features. Then, a variational autoencoder (VAE)based generative module is performed to learn a cross-modal alignment betweenskeleton and semantic features. Finally, a classification module is built torecognize the action categories of input samples, where a seen-unseenclassification gate is adopted to predict whether the sample comes from seenaction classes or not in GZSSAR. The superior performance in comparisons withprevious models validates the effectiveness of the proposed MSF model onGZSSAR.</description><author>Ming-Zhe Li, Zhen Jia, Zhang Zhang, Zhanyu Ma, Liang Wang</author><pubDate>Mon, 18 Sep 2023 10:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09592v1</guid></item><item><title>Video Action Recognition with Attentive Semantic Units</title><link>http://arxiv.org/abs/2303.09756v2</link><description>Visual-Language Models (VLMs) have significantly advanced action videorecognition. Supervised by the semantics of action labels, recent works adaptthe visual branch of VLMs to learn video representations. Despite theeffectiveness proved by these works, we believe that the potential of VLMs hasyet to be fully harnessed. In light of this, we exploit the semantic units (SU)hiding behind the action labels and leverage their correlations withfine-grained items in frames for more accurate action recognition. SUs areentities extracted from the language descriptions of the entire action set,including body parts, objects, scenes, and motions. To further enhance thealignments between visual contents and the SUs, we introduce a multi-regionmodule (MRA) to the visual branch of the VLM. The MRA allows the perception ofregion-aware visual features beyond the original global feature. Our methodadaptively attends to and selects relevant SUs with visual features of frames.With a cross-modal decoder, the selected SUs serve to decode spatiotemporalvideo representations. In summary, the SUs as the medium can boostdiscriminative ability and transferability. Specifically, in fully-supervisedlearning, our method achieved 87.8% top-1 accuracy on Kinetics-400. In K=2few-shot experiments, our method surpassed the previous state-of-the-art by+7.1% and +15.0% on HMDB-51 and UCF-101, respectively.</description><author>Yifei Chen, Dapeng Chen, Ruijin Liu, Hao Li, Wei Peng</author><pubDate>Tue, 10 Oct 2023 14:31:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09756v2</guid></item><item><title>Action Recognition with Multi-stream Motion Modeling and Mutual Information Maximization</title><link>http://arxiv.org/abs/2306.07576v1</link><description>Action recognition has long been a fundamental and intriguing problem inartificial intelligence. The task is challenging due to the high dimensionalitynature of an action, as well as the subtle motion details to be considered.Current state-of-the-art approaches typically learn from articulated motionsequences in the straightforward 3D Euclidean space. However, the vanillaEuclidean space is not efficient for modeling important motion characteristicssuch as the joint-wise angular acceleration, which reveals the driving forcebehind the motion. Moreover, current methods typically attend to each channelequally and lack theoretical constrains on extracting task-relevant featuresfrom the input. In this paper, we seek to tackle these challenges from three aspects: (1) Wepropose to incorporate an acceleration representation, explicitly modeling thehigher-order variations in motion. (2) We introduce a novel Stream-GCN networkequipped with multi-stream components and channel attention, where differentrepresentations (i.e., streams) supplement each other towards a more preciseaction recognition while attention capitalizes on those important channels. (3)We explore feature-level supervision for maximizing the extraction oftask-relevant information and formulate this into a mutual information loss.Empirically, our approach sets the new state-of-the-art performance on threebenchmark datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. Our code isanonymously released at https://github.com/ActionR-Group/Stream-GCN, hoping toinspire the community.</description><author>Yuheng Yang, Haipeng Chen, Zhenguang Liu, Yingda Lyu, Beibei Zhang, Shuang Wu, Zhibo Wang, Kui Ren</author><pubDate>Tue, 13 Jun 2023 07:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07576v1</guid></item><item><title>FHA-Kitchens: A Novel Dataset for Fine-Grained Hand Action Recognition in Kitchen Scenes</title><link>http://arxiv.org/abs/2306.10858v1</link><description>A typical task in the field of video understanding is hand actionrecognition, which has a wide range of applications. Existing works eithermainly focus on full-body actions, or the defined action categories arerelatively coarse-grained. In this paper, we propose FHA-Kitchens, a noveldataset of fine-grained hand actions in kitchen scenes. In particular, we focuson human hand interaction regions and perform deep excavation to further refinehand action information and interaction regions. Our FHA-Kitchens datasetconsists of 2,377 video clips and 30,047 images collected from 8 differenttypes of dishes, and all hand interaction regions in each image are labeledwith high-quality fine-grained action classes and bounding boxes. We representthe action information in each hand interaction region as a triplet, resultingin a total of 878 action triplets. Based on the constructed dataset, webenchmark representative action recognition and detection models on thefollowing three tracks: (1) supervised learning for hand interaction region andobject detection, (2) supervised learning for fine-grained hand actionrecognition, and (3) intra- and inter-class domain generalization for handinteraction region detection. The experimental results offer compellingempirical evidence that highlights the challenges inherent in fine-grained handaction recognition, while also shedding light on potential avenues for futureresearch, particularly in relation to pre-training strategy, model design, anddomain generalization. The dataset will be released athttps://github.com/tingZ123/FHA-Kitchens.</description><author>Ting Zhe, Yongqian Li, Jing Zhang, Yong Luo, Han Hu, Bo Du, Yonggang Wen, Dacheng Tao</author><pubDate>Mon, 19 Jun 2023 12:21:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10858v1</guid></item><item><title>Sample Less, Learn More: Efficient Action Recognition via Frame Feature Restoration</title><link>http://arxiv.org/abs/2307.14866v1</link><description>Training an effective video action recognition model poses significantcomputational challenges, particularly under limited resource budgets. Currentmethods primarily aim to either reduce model size or utilize pre-trainedmodels, limiting their adaptability to various backbone architectures. Thispaper investigates the issue of over-sampled frames, a prevalent problem inmany approaches yet it has received relatively little attention. Despite theuse of fewer frames being a potential solution, this approach often results ina substantial decline in performance. To address this issue, we propose a novelmethod to restore the intermediate features for two sparsely sampled andadjacent video frames. This feature restoration technique brings a negligibleincrease in computational requirements compared to resource-intensive imageencoders, such as ViT. To evaluate the effectiveness of our method, we conductextensive experiments on four public datasets, including Kinetics-400,ActivityNet, UCF-101, and HMDB-51. With the integration of our method, theefficiency of three commonly used baselines has been improved by over 50%, witha mere 0.5% reduction in recognition accuracy. In addition, our method alsosurprisingly helps improve the generalization ability of the models underzero-shot settings.</description><author>Harry Cheng, Yangyang Guo, Liqiang Nie, Zhiyong Cheng, Mohan Kankanhalli</author><pubDate>Thu, 27 Jul 2023 14:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14866v1</guid></item><item><title>Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives</title><link>http://arxiv.org/abs/2309.12067v1</link><description>Action scene understanding in soccer is a challenging task due to the complexand dynamic nature of the game, as well as the interactions between players.This article provides a comprehensive overview of this task divided into actionrecognition, spotting, and spatio-temporal action localization, with aparticular emphasis on the modalities used and multimodal methods. We explorethe publicly available data sources and metrics used to evaluate models'performance. The article reviews recent state-of-the-art methods that leveragedeep learning techniques and traditional methods. We focus on multimodalmethods, which integrate information from multiple sources, such as video andaudio data, and also those that represent one source in various ways. Theadvantages and limitations of methods are discussed, along with their potentialfor improving the accuracy and robustness of models. Finally, the articlehighlights some of the open research questions and future directions in thefield of soccer action recognition, including the potential for multimodalmethods to advance this field. Overall, this survey provides a valuableresource for researchers interested in the field of action scene understandingin soccer.</description><author>Karolina Seweryn, Anna Wrblewska, Szymon ukasik</author><pubDate>Thu, 21 Sep 2023 14:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12067v1</guid></item><item><title>Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating</title><link>http://arxiv.org/abs/2307.02730v1</link><description>The fine-grained action analysis of the existing action datasets ischallenged by insufficient action categories, low fine granularities, limitedmodalities, and tasks. In this paper, we propose a Multi-modality andMulti-task dataset of Figure Skating (MMFS) which was collected from the WorldFigure Skating Championships. MMFS, which possesses action recognition andaction quality assessment, captures RGB, skeleton, and is collected the scoreof actions from 11671 clips with 256 categories including spatial and temporallabels. The key contributions of our dataset fall into three aspects asfollows. (1) Independently spatial and temporal categories are first proposedto further explore fine-grained action recognition and quality assessment. (2)MMFS first introduces the skeleton modality for complex fine-grained actionquality assessment. (3) Our multi-modality and multi-task dataset encouragemore action analysis models. To benchmark our dataset, we adopt RGB-based andskeleton-based baseline methods for action recognition and action qualityassessment.</description><author>Sheng-Lan Liu, Yu-Ning Ding, Si-Fan Zhang, Wen-Yue Chen, Ning Zhou, Hao Liu, Gui-Hong Lao</author><pubDate>Thu, 06 Jul 2023 03:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02730v1</guid></item><item><title>SiT-MLP: A Simple MLP with Point-wise Topology Feature Learning for Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2308.16018v3</link><description>Graph convolution networks (GCNs) have achieved remarkable performance inskeleton-based action recognition. However, previous GCN-based methods rely onelaborate human priors excessively and construct complex feature aggregationmechanisms, which limits the generalizability and effectiveness of networks. Tosolve these problems, we propose a novel Spatial Topology Gating Unit (STGU),an MLP-based variant without extra priors, to capture the co-occurrencetopology features that encode the spatial dependency across all joints. InSTGU, to learn the point-wise topology features, a new gate-based featureinteraction mechanism is introduced to activate the features point-to-point bythe attention map generated from the input sample. Based on the STGU, wepropose the first MLP-based model, SiTMLP, for skeleton-based actionrecognition in this work. Compared with previous methods on three large-scaledatasets, SiTMLP achieves competitive performance. In addition, SiT-MLP reducesthe parameters by up to 62.5% with favorable results. The code will beavailable at https://github.com/BUPTSJZhang/SiTMLP.</description><author>Shaojie Zhang, Jianqin Yin, Yonghao Dang, Jiajun Fu</author><pubDate>Wed, 27 Sep 2023 03:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16018v3</guid></item><item><title>A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023</title><link>http://arxiv.org/abs/2307.06569v1</link><description>In this technical report, we present our findings from a study conducted onthe EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for ActionRecognition. Our research focuses on the innovative application of adifferentiable logic loss in the training to leverage the co-occurrencerelations between verb and noun, as well as the pre-trained Large LanguageModels (LLMs) to generate the logic rules for the adaptation to unseen actionlabels. Specifically, the model's predictions are treated as the truthassignment of a co-occurrence logic formula to compute the logic loss, whichmeasures the consistency between the predictions and the logic constraints. Byusing the verb-noun co-occurrence matrix generated from the dataset, we observea moderate improvement in model performance compared to our baseline framework.To further enhance the model's adaptability to novel action labels, weexperiment with rules generated using GPT-3.5, which leads to a slight decreasein performance. These findings shed light on the potential and challenges ofincorporating differentiable logic and LLMs for knowledge extraction inunsupervised domain adaptation for action recognition. Our final submission(entitled `NS-LLM') achieved the first place in terms of top-1 actionrecognition accuracy.</description><author>Yi Cheng, Ziwei Xu, Fen Fang, Dongyun Lin, Hehe Fan, Yongkang Wong, Ying Sun, Mohan Kankanhalli</author><pubDate>Thu, 13 Jul 2023 06:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06569v1</guid></item><item><title>Elevating Skeleton-Based Action Recognition with Efficient Multi-Modality Self-Supervision</title><link>http://arxiv.org/abs/2309.12009v1</link><description>Self-supervised representation learning for human action recognition hasdeveloped rapidly in recent years. Most of the existing works are based onskeleton data while using a multi-modality setup. These works overlooked thedifferences in performance among modalities, which led to the propagation oferroneous knowledge between modalities while only three fundamental modalities,i.e., joints, bones, and motions are used, hence no additional modalities areexplored. In this work, we first propose an Implicit Knowledge Exchange Module (IKEM)which alleviates the propagation of erroneous knowledge between low-performancemodalities. Then, we further propose three new modalities to enrich thecomplementary information between modalities. Finally, to maintain efficiencywhen introducing new modalities, we propose a novel teacher-student frameworkto distill the knowledge from the secondary modalities into the mandatorymodalities considering the relationship constrained by anchors, positives, andnegatives, named relational cross-modality knowledge distillation. Theexperimental results demonstrate the effectiveness of our approach, unlockingthe efficient use of skeleton-based multi-modality data. Source code will bemade publicly available at https://github.com/desehuileng0o0/IKEM.</description><author>Yiping Wei, Kunyu Peng, Alina Roitberg, Jiaming Zhang, Junwei Zheng, Ruiping Liu, Yufan Chen, Kailun Yang, Rainer Stiefelhagen</author><pubDate>Thu, 21 Sep 2023 13:27:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12009v1</guid></item><item><title>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2208.10741v3</link><description>Graph convolutional networks (GCNs) are the most commonly used methods forskeleton-based action recognition and have achieved remarkable performance.Generating adjacency matrices with semantically meaningful edges isparticularly important for this task, but extracting such edges is challengingproblem. To solve this, we propose a hierarchically decomposed graphconvolutional network (HD-GCN) architecture with a novel hierarchicallydecomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes everyjoint node into several sets to extract major structurally adjacent and distantedges, and uses them to construct an HD-Graph containing those edges in thesame semantic spaces of a human skeleton. In addition, we introduce anattention-guided hierarchy aggregation (A-HA) module to highlight the dominanthierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-wayensemble method, which uses only joint and bone stream without any motionstream. The proposed model is evaluated and achieves state-of-the-artperformance on four large, popular datasets. Finally, we demonstrate theeffectiveness of our model with various comparative experiments.</description><author>Jungho Lee, Minhyeok Lee, Dogyoon Lee, Sangyoun Lee</author><pubDate>Wed, 19 Jul 2023 10:15:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10741v3</guid></item><item><title>Temporal-Distributed Backdoor Attack Against Video Based Action Recognition</title><link>http://arxiv.org/abs/2308.11070v1</link><description>Deep neural networks (DNNs) have achieved tremendous success in variousapplications including video action recognition, yet remain vulnerable tobackdoor attacks (Trojans). The backdoor-compromised model will mis-classify tothe target class chosen by the attacker when a test instance (from a non-targetclass) is embedded with a specific trigger, while maintaining high accuracy onattack-free instances. Although there are extensive studies on backdoor attacksagainst image data, the susceptibility of video-based systems under backdoorattacks remains largely unexplored. Current studies are direct extensions ofapproaches proposed for image data, e.g., the triggers are\textbf{independently} embedded within the frames, which tend to be detectableby existing defenses. In this paper, we introduce a \textit{simple} yet\textit{effective} backdoor attack against video data. Our proposed attack,adding perturbations in a transformed domain, plants an \textbf{imperceptible,temporally distributed} trigger across the video frames, and is shown to beresilient to existing defensive strategies. The effectiveness of the proposedattack is demonstrated by extensive experiments with various well-known modelson two video recognition benchmarks, UCF101 and HMDB51, and a sign languagerecognition benchmark, Greek Sign Language (GSL) dataset. We delve into theimpact of several influential factors on our proposed attack and identify anintriguing effect termed "collateral damage" through extensive studies.</description><author>Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis</author><pubDate>Mon, 21 Aug 2023 23:31:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11070v1</guid></item><item><title>Temporal-Distributed Backdoor Attack Against Video Based Action Recognition</title><link>http://arxiv.org/abs/2308.11070v2</link><description>Deep neural networks (DNNs) have achieved tremendous success in variousapplications including video action recognition, yet remain vulnerable tobackdoor attacks (Trojans). The backdoor-compromised model will mis-classify tothe target class chosen by the attacker when a test instance (from a non-targetclass) is embedded with a specific trigger, while maintaining high accuracy onattack-free instances. Although there are extensive studies on backdoor attacksagainst image data, the susceptibility of video-based systems under backdoorattacks remains largely unexplored. Current studies are direct extensions ofapproaches proposed for image data, e.g., the triggers are independentlyembedded within the frames, which tend to be detectable by existing defenses.In this paper, we introduce a simple yet effective backdoor attack againstvideo data. Our proposed attack, adding perturbations in a transformed domain,plants an imperceptible, temporally distributed trigger across the videoframes, and is shown to be resilient to existing defensive strategies. Theeffectiveness of the proposed attack is demonstrated by extensive experimentswith various well-known models on two video recognition benchmarks, UCF101 andHMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL)dataset. We delve into the impact of several influential factors on ourproposed attack and identify an intriguing effect termed "collateral damage"through extensive studies.</description><author>Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis</author><pubDate>Fri, 01 Sep 2023 02:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11070v2</guid></item><item><title>Part Aware Contrastive Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2305.00666v2</link><description>In recent years, remarkable results have been achieved in self-supervisedaction recognition using skeleton sequences with contrastive learning. It hasbeen observed that the semantic distinction of human action features is oftenrepresented by local body parts, such as legs or hands, which are advantageousfor skeleton-based action recognition. This paper proposes an attention-basedcontrastive learning framework for skeleton representation learning, calledSkeAttnCLR, which integrates local similarity and global features forskeleton-based action representations. To achieve this, a multi-head attentionmask module is employed to learn the soft attention mask features from theskeletons, suppressing non-salient local features while accentuating localsalient features, thereby bringing similar local features closer in the featurespace. Additionally, ample contrastive pairs are generated by expandingcontrastive pairs based on salient and non-salient features with globalfeatures, which guide the network to learn the semantic representations of theentire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLRlearns local features under different data augmentation views. The experimentresults demonstrate that the inclusion of local feature similaritysignificantly enhances skeleton-based action representation. Our proposedSkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, andPKU-MMD datasets.</description><author>Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen Chen, Shiqian Wu</author><pubDate>Thu, 11 May 2023 08:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00666v2</guid></item><item><title>MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge</title><link>http://arxiv.org/abs/2303.08914v2</link><description>Large scale Vision-Language (VL) models have shown tremendous success inaligning representations between visual and text modalities. This enablesremarkable progress in zero-shot recognition, image generation &amp; editing, andmany other exciting tasks. However, VL models tend to over-represent objectswhile paying much less attention to verbs, and require additional tuning onvideo data for best zero-shot action recognition performance. While previouswork relied on large-scale, fully-annotated data, in this work we propose anunsupervised approach. We adapt a VL model for zero-shot and few-shot actionrecognition using a collection of unlabeled videos and an unpaired actiondictionary. Based on that, we leverage Large Language Models and VL models tobuild a text bag for each unlabeled video via matching, text expansion andcaptioning. We use those bags in a Multiple Instance Learning setup to adapt animage-text backbone to video data. Although finetuned on unlabeled video data,our resulting models demonstrate high transferability to numerous unseenzero-shot downstream tasks, improving the base VL model performance by up to14\%, and even comparing favorably to fully-supervised baselines in bothzero-shot and few-shot video recognition transfer. The code will be releasedlater at \url{https://github.com/wlin-at/MAXI}.</description><author>Wei Lin, Leonid Karlinsky, Nina Shvetsova, Horst Possegger, Mateusz Kozinski, Rameswar Panda, Rogerio Feris, Hilde Kuehne, Horst Bischof</author><pubDate>Sat, 22 Jul 2023 10:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08914v2</guid></item><item><title>Proving the Potential of Skeleton Based Action Recognition to Automate the Analysis of Manual Processes</title><link>http://arxiv.org/abs/2310.08451v1</link><description>In manufacturing sectors such as textiles and electronics, manual processesare a fundamental part of production. The analysis and monitoring of theprocesses is necessary for efficient production design. Traditional methods foranalyzing manual processes are complex, expensive, and inflexible. Compared toestablished approaches such as Methods-Time-Measurement (MTM), machine learning(ML) methods promise: Higher flexibility, self-sufficient &amp; permanent use,lower costs. In this work, based on a video stream, the current motion class ina manual assembly process is detected. With information on the current motion,Key-Performance-Indicators (KPIs) can be derived easily. A skeleton-basedaction recognition approach is taken, as this field recently shows majorsuccess in machine vision tasks. For skeleton-based action recognition inmanual assembly, no sufficient pre-work could be found. Therefore, a MLpipeline is developed, to enable extensive research on different (pre-)processing methods and neural nets. Suitable well generalizing approaches arefound, proving the potential of ML to enhance analyzation of manual processes.Models detect the current motion, performed by an operator in manual assembly,but the results can be transferred to all kinds of manual processes.</description><author>Marlin Berger, Frederik Cloppenburg, Jens Eufinger, Thomas Gries</author><pubDate>Thu, 12 Oct 2023 17:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08451v1</guid></item><item><title>Few-shot Action Recognition with Captioning Foundation Models</title><link>http://arxiv.org/abs/2310.10125v1</link><description>Transferring vision-language knowledge from pretrained multimodal foundationmodels to various downstream tasks is a promising direction. However, mostcurrent few-shot action recognition methods are still limited to a singlevisual modality input due to the high cost of annotating additional textualdescriptions. In this paper, we develop an effective plug-and-play frameworkcalled CapFSAR to exploit the knowledge of multimodal models without manuallyannotating text. To be specific, we first utilize a captioning foundation model(i.e., BLIP) to extract visual features and automatically generate associatedcaptions for input videos. Then, we apply a text encoder to the syntheticcaptions to obtain representative text embeddings. Finally, a visual-textaggregation module based on Transformer is further designed to incorporatecross-modal spatio-temporal complementary information for reliable few-shotmatching. In this way, CapFSAR can benefit from powerful multimodal knowledgeof pretrained foundation models, yielding more comprehensive classification inthe low-shot regime. Extensive experiments on multiple standard few-shotbenchmarks demonstrate that the proposed CapFSAR performs favorably againstexisting methods and achieves state-of-the-art performance. The code will bemade publicly available.</description><author>Xiang Wang, Shiwei Zhang, Hangjie Yuan, Yingya Zhang, Changxin Gao, Deli Zhao, Nong Sang</author><pubDate>Mon, 16 Oct 2023 08:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10125v1</guid></item><item><title>Joint Adversarial and Collaborative Learning for Self-Supervised Action Recognition</title><link>http://arxiv.org/abs/2307.07791v1</link><description>Considering the instance-level discriminative ability, contrastive learningmethods, including MoCo and SimCLR, have been adapted from the original imagerepresentation learning task to solve the self-supervised skeleton-based actionrecognition task. These methods usually use multiple data streams (i.e., joint,motion, and bone) for ensemble learning, meanwhile, how to construct adiscriminative feature space within a single stream and effectively aggregatethe information from multiple streams remains an open problem. To this end, wefirst apply a new contrastive learning method called BYOL to learn fromskeleton data and formulate SkeletonBYOL as a simple yet effective baseline forself-supervised skeleton-based action recognition. Inspired by SkeletonBYOL, wefurther present a joint Adversarial and Collaborative Learning (ACL) framework,which combines Cross-Model Adversarial Learning (CMAL) and Cross-StreamCollaborative Learning (CSCL). Specifically, CMAL learns single-streamrepresentation by cross-model adversarial loss to obtain more discriminativefeatures. To aggregate and interact with multi-stream information, CSCL isdesigned by generating similarity pseudo label of ensemble learning assupervision and guiding feature generation for individual streams. Exhaustiveexperiments on three datasets verify the complementary properties between CMALand CSCL and also verify that our method can perform favorably againststate-of-the-art methods using various evaluation protocols. Our code andmodels are publicly available at \url{https://github.com/Levigty/ACL}.</description><author>Tianyu Guo, Mengyuan Liu, Hong Liu, Wenhao Li, Jingwen Guo, Tao Wang, Yidi Li</author><pubDate>Sat, 15 Jul 2023 13:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07791v1</guid></item><item><title>Task-Specific Alignment and Multiple Level Transformer for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2307.01985v1</link><description>In the research field of few-shot learning, the main difference betweenimage-based and video-based is the additional temporal dimension for videos. Inrecent years, many approaches for few-shot action recognition have followed themetric-based methods, especially, since some works use the Transformer to getthe cross-attention feature of the videos or the enhanced prototype, and theresults are competitive. However, they do not mine enough information from theTransformer because they only focus on the feature of a single level. In ourpaper, we have addressed this problem. We propose an end-to-end method named"Task-Specific Alignment and Multiple Level Transformer Network (TSA-MLT)". Inour model, the Multiple Level Transformer focuses on the multiple-level featureof the support video and query video. Especially before Multiple LevelTransformer, we use task-specific TSA to filter unimportant or misleadingframes as a pre-processing. Furthermore, we adopt a fusion loss using two kindsof distance, the first is L2 sequence distance, which focuses on temporal orderalignment. The second one is Optimal transport distance, which focuses onmeasuring the gap between the appearance and semantics of the videos. Using asimple fusion network, we fuse the two distances element-wise, then use thecross-entropy loss as our fusion loss. Extensive experiments show our methodachieves state-of-the-art results on the HMDB51 and UCF101 datasets and acompetitive result on the benchmark of Kinetics and something-2-something V2datasets. Our code will be available at the URL:https://github.com/cofly2014/tsa-mlt.git</description><author>Fei Guo, Li Zhu, YiWang Wang</author><pubDate>Wed, 05 Jul 2023 03:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01985v1</guid></item><item><title>Human Action Recognition in Still Images Using ConViT</title><link>http://arxiv.org/abs/2307.08994v1</link><description>Understanding the relationship between different parts of the image plays acrucial role in many visual recognition tasks. Despite the fact thatConvolutional Neural Networks (CNNs) have demonstrated impressive results indetecting single objects, they lack the capability to extract the relationshipbetween various regions of an image, which is a crucial factor in human actionrecognition. To address this problem, this paper proposes a new module thatfunctions like a convolutional layer using Vision Transformer (ViT). Theproposed action recognition model comprises two components: the first part is adeep convolutional network that extracts high-level spatial features from theimage, and the second component of the model utilizes a Vision Transformer thatextracts the relationship between various regions of the image using thefeature map generated by the CNN output. The proposed model has been evaluatedon the Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5%mAP and 91.5% mAP results, respectively, which are promising compared to otherstate-of-the-art methods.</description><author>Seyed Rohollah Hosseyni, Hasan Taheri, Sanaz Seyedin, Ali Ahmad Rahmani</author><pubDate>Tue, 18 Jul 2023 07:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08994v1</guid></item><item><title>Optimizing ViViT Training: Time and Memory Reduction for Action Recognition</title><link>http://arxiv.org/abs/2306.04822v1</link><description>In this paper, we address the challenges posed by the substantial trainingtime and memory consumption associated with video transformers, focusing on theViViT (Video Vision Transformer) model, in particular the Factorised Encoderversion, as our baseline for action recognition tasks. The factorised encodervariant follows the late-fusion approach that is adopted by many state of theart approaches. Despite standing out for its favorable speed/accuracy tradeoffsamong the different variants of ViViT, its considerable training time andmemory requirements still pose a significant barrier to entry. Our method isdesigned to lower this barrier and is based on the idea of freezing the spatialtransformer during training. This leads to a low accuracy model if naivelydone. But we show that by (1) appropriately initializing the temporaltransformer (a module responsible for processing temporal information) (2)introducing a compact adapter model connecting frozen spatial representations((a module that selectively focuses on regions of the input image) to thetemporal transformer, we can enjoy the benefits of freezing the spatialtransformer without sacrificing accuracy. Through extensive experimentationover 6 benchmarks, we demonstrate that our proposed training strategysignificantly reduces training costs (by $\sim 50\%$) and memory consumptionwhile maintaining or slightly improving performance by up to 1.79\% compared tothe baseline model. Our approach additionally unlocks the capability to utilizelarger image transformer models as our spatial transformer and access moreframes with the same memory consumption.</description><author>Shreyank N Gowda, Anurag Arnab, Jonathan Huang</author><pubDate>Thu, 08 Jun 2023 00:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04822v1</guid></item><item><title>Telling Stories for Common Sense Zero-Shot Action Recognition</title><link>http://arxiv.org/abs/2309.17327v1</link><description>Video understanding has long suffered from reliance on large labeleddatasets, motivating research into zero-shot learning. Recent progress inlanguage modeling presents opportunities to advance zero-shot video analysis,but constructing an effective semantic space relating action classes remainschallenging. We address this by introducing a novel dataset, Stories, whichcontains rich textual descriptions for diverse action classes extracted fromWikiHow articles. For each class, we extract multi-sentence narrativesdetailing the necessary steps, scenes, objects, and verbs that characterize theaction. This contextual data enables modeling of nuanced relationships betweenactions, paving the way for zero-shot transfer. We also propose an approachthat harnesses Stories to improve feature generation for training zero-shotclassification. Without any target dataset fine-tuning, our method achieves newstate-of-the-art on multiple benchmarks, improving top-1 accuracy by up to6.1%. We believe Stories provides a valuable resource that can catalyzeprogress in zero-shot action recognition. The textual narratives forgeconnections between seen and unseen classes, overcoming the bottleneck oflabeled data that has long impeded advancements in this exciting domain. Thedata can be found here: https://github.com/kini5gowda/Stories .</description><author>Shreyank N Gowda, Laura Sevilla-Lara</author><pubDate>Fri, 29 Sep 2023 16:34:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17327v1</guid></item><item><title>SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition</title><link>http://arxiv.org/abs/2310.07189v1</link><description>Event cameras are bio-inspired sensors that respond to local changes in lightintensity and feature low latency, high energy efficiency, and high dynamicrange. Meanwhile, Spiking Neural Networks (SNNs) have gained significantattention due to their remarkable efficiency and fault tolerance. Bysynergistically harnessing the energy efficiency inherent in event cameras andthe spike-based processing capabilities of SNNs, their integration could enableultra-low-power application scenarios, such as action recognition tasks.However, existing approaches often entail converting asynchronous events intoconventional frames, leading to additional data mapping efforts and a loss ofsparsity, contradicting the design concept of SNNs and event cameras. Toaddress this challenge, we propose SpikePoint, a novel end-to-end point-basedSNN architecture. SpikePoint excels at processing sparse event cloud data,effectively extracting both global and local features through a singular-stagestructure. Leveraging the surrogate training method, SpikePoint achieves highaccuracy with few parameters and maintains low power consumption, specificallyemploying the identity mapping feature extractor on diverse datasets.SpikePoint achieves state-of-the-art (SOTA) performance on four event-basedaction recognition datasets using only 16 timesteps, surpassing other SNNmethods. Moreover, it also achieves SOTA performance across all methods onthree datasets, utilizing approximately 0.3\% of the parameters and 0.5\% ofpower consumption employed by artificial neural networks (ANNs). These resultsemphasize the significance of Point Cloud and pave the way for manyultra-low-power event-based data processing applications.</description><author>Hongwei Ren, Yue Zhou, Yulong Huang, Haotian Fu, Xiaopeng Lin, Jie Song, Bojun Cheng</author><pubDate>Wed, 11 Oct 2023 05:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07189v1</guid></item><item><title>Multimodal Distillation for Egocentric Action Recognition</title><link>http://arxiv.org/abs/2307.07483v2</link><description>The focal point of egocentric video understanding is modelling hand-objectinteractions. Standard models, e.g. CNNs or Vision Transformers, which receiveRGB frames as input perform well. However, their performance improves furtherby employing additional input modalities that provide complementary cues, suchas object detections, optical flow, audio, etc. The added complexity of themodality-specific modules, on the other hand, makes these models impracticalfor deployment. The goal of this work is to retain the performance of such amultimodal approach, while using only the RGB frames as input at inferencetime. We demonstrate that for egocentric action recognition on theEpic-Kitchens and the Something-Something datasets, students which are taughtby multimodal teachers tend to be more accurate and better calibrated thanarchitecturally equivalent models trained on ground truth labels in a unimodalor multimodal fashion. We further adopt a principled multimodal knowledgedistillation framework, allowing us to deal with issues which occur whenapplying multimodal knowledge distillation in a naive manner. Lastly, wedemonstrate the achieved reduction in computational complexity, and show thatour approach maintains higher performance with the reduction of the number ofinput views. We release our code athttps://github.com/gorjanradevski/multimodal-distillation.</description><author>Gorjan Radevski, Dusan Grujicic, Marie-Francine Moens, Matthew Blaschko, Tinne Tuytelaars</author><pubDate>Tue, 18 Jul 2023 10:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07483v2</guid></item><item><title>Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints</title><link>http://arxiv.org/abs/2306.01075v1</link><description>Accurate understanding and prediction of human behaviors are criticalprerequisites for autonomous vehicles, especially in highly dynamic andinteractive scenarios such as intersections in dense urban areas. In this work,we aim at identifying crossing pedestrians and predicting their futuretrajectories. To achieve these goals, we not only need the context informationof road geometry and other traffic participants but also need fine-grainedinformation of the human pose, motion and activity, which can be inferred fromhuman keypoints. In this paper, we propose a novel multi-task learningframework for pedestrian crossing action recognition and trajectory prediction,which utilizes 3D human keypoints extracted from raw sensor data to capturerich information on human pose and activity. Moreover, we propose to apply twoauxiliary tasks and contrastive learning to enable auxiliary supervisions toimprove the learned keypoints representation, which further enhances theperformance of major tasks. We validate our approach on a large-scale in-housedataset, as well as a public benchmark dataset, and show that our approachachieves state-of-the-art performance on a wide range of evaluation metrics.The effectiveness of each model component is validated in a detailed ablationstudy.</description><author>Jiachen Li, Xinwei Shi, Feiyu Chen, Jonathan Stroud, Zhishuai Zhang, Tian Lan, Junhua Mao, Jeonhyung Kang, Khaled S. Refaat, Weilong Yang, Eugene Ie, Congcong Li</author><pubDate>Thu, 01 Jun 2023 19:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01075v1</guid></item><item><title>Spiking Neural Networks for event-based action recognition: A new task to understand their advantage</title><link>http://arxiv.org/abs/2209.14915v2</link><description>Spiking Neural Networks (SNN) are characterised by their unique temporaldynamics, but the properties and advantages of such computations are still notwell understood. In order to provide answers, in this work we demonstrate howSpiking neurons can enable temporal feature extraction in feed-forward neuralnetworks without the need for recurrent synapses, showing how theirbio-inspired computing principles can be successfully exploited beyond energyefficiency gains and evidencing their differences with respect to conventionalneurons. This is demonstrated by proposing a new task, DVS-Gesture-Chain(DVS-GC), which allows, for the first time, to evaluate the perception oftemporal dependencies in a real event-based action recognition dataset. Ourstudy proves how the widely used DVS Gesture benchmark could be solved bynetworks without temporal feature extraction, unlike the new DVS-GC whichdemands an understanding of the ordering of the events. Furthermore, this setupallowed us to unveil the role of the leakage rate in spiking neurons fortemporal processing tasks and demonstrated the benefits of "hard reset"mechanisms. Additionally, we also show how time-dependent weights andnormalization can lead to understanding order by means of temporal attention.</description><author>Alex Vicente-Sola, Davide L. Manna, Paul Kirkland, Gaetano Di Caterina, Trevor Bihl</author><pubDate>Tue, 08 Aug 2023 11:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.14915v2</guid></item><item><title>High-Performance Inference Graph Convolutional Networks for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2305.18710v1</link><description>Recently, significant achievements have been made in skeleton-based humanaction recognition with the emergence of graph convolutional networks (GCNs).However, the state-of-the-art (SOTA) models used for this task focus onconstructing more complex higher-order connections between joint nodes todescribe skeleton information, which leads to complex inference processes andhigh computational costs, resulting in reduced model's practicality. To addressthe slow inference speed caused by overly complex model structures, weintroduce re-parameterization and over-parameterization techniques to GCNs, andpropose two novel high-performance inference graph convolutional networks,namely HPI-GCN-RP and HPI-GCN-OP. HPI-GCN-RP uses re-parameterization techniqueto GCNs to achieve a higher inference speed with competitive model performance.HPI-GCN-OP further utilizes over-parameterization technique to bringsignificant performance improvement with inference speed slightly decreased.Experimental results on the two skeleton-based action recognition datasetsdemonstrate the effectiveness of our approach. Our HPI-GCN-OP achieves anaccuracy of 93% on the cross-subject split of the NTU-RGB+D 60 dataset, and90.1% on the cross-subject benchmark of the NTU-RGB+D 120 dataset and is 4.5times faster than HD-GCN at the same accuracy.</description><author>Ziao Li, Junyi Wang, Guhong Nie</author><pubDate>Tue, 30 May 2023 04:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18710v1</guid></item><item><title>High-order Tensor Pooling with Attention for Action Recognition</title><link>http://arxiv.org/abs/2110.05216v3</link><description>We aim at capturing high-order statistics of feature vectors formed by aneural network, and propose end-to-end second- and higher-order pooling to forma tensor descriptor. Tensor descriptors require a robust similarity measure dueto low numbers of aggregated vectors and the burstiness phenomenon, when agiven feature appears more/less frequently than statistically expected. TheHeat Diffusion Process (HDP) on a graph Laplacian is closely related to theEigenvalue Power Normalization (EPN) of the covariance/autocorrelation matrix,whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPNplay the same role, i.e., to boost or dampen the magnitude of the eigenspectrumthus preventing the burstiness. We equip higher-order tensors with EPN whichacts as a spectral detector of higher-order occurrences to prevent burstiness.We also prove that for a tensor of order r built from d dimensional featuredescriptors, such a detector gives the likelihood if at least one higher-orderoccurrence is 'projected' into one of binom(d,r) subspaces represented by thetensor; thus forming a tensor power normalization metric endowed withbinom(d,r) such 'detectors'. For experimental contributions, we apply severalsecond- and higher-order pooling variants to action recognition, providepreviously not presented comparisons of such pooling variants, and showstate-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.</description><author>Lei Wang, Piotr Koniusz, Ke Sun</author><pubDate>Mon, 16 Oct 2023 06:05:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.05216v3</guid></item><item><title>High-order Tensor Pooling with Attention for Action Recognition</title><link>http://arxiv.org/abs/2110.05216v2</link><description>We aim at capturing high-order statistics of feature vectors formed by aneural network, and propose end-to-end second- and higher-order pooling to forma tensor descriptor. Tensor descriptors require a robust similarity measure dueto low numbers of aggregated vectors and the burstiness phenomenon, when agiven feature appears more/less frequently than statistically expected. TheHeat Diffusion Process (HDP) on a graph Laplacian is closely related to theEigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix,whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPNplay the same role, i.e., to boost or dampen the magnitude of the eigenspectrumthus preventing the burstiness. We equip higher-order tensors with EPN whichacts as a spectral detector of higher-order occurrences to prevent burstiness.We also prove that for a tensor of order r built from d dimensional featuredescriptors, such a detector gives the likelihood if at least one higher-orderoccurrence is 'projected' into one of binom(d,r) subspaces represented by thetensor; thus forming a tensor power normalization metric endowed withbinom(d,r) such 'detectors'. For experimental contributions, we apply severalsecond- and higher-order pooling variants to action recognition, providepreviously not presented comparisons of such pooling variants, and showstate-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.</description><author>Piotr Koniusz, Lei Wang, Ke Sun</author><pubDate>Thu, 20 Jul 2023 15:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.05216v2</guid></item></channel></rss>