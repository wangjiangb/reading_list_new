<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivdialogue summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 10 Sep 2025 13:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems</title><link>http://arxiv.org/abs/2402.18013v2</link><description>This survey provides a comprehensive review of research on multi-turndialogue systems, with a particular focus on multi-turn dialogue systems basedon large language models (LLMs). This paper aims to (a) give a summary ofexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)elaborate recent advances in multi-turn dialogue systems, covering bothLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,along with datasets and evaluation metrics; (c) discuss some future emphasisand recent research problems arising from the development of LLMs and theincreasing demands on multi-turn dialogue systems.</description><author>Zihao Yi, Jiarui Ouyang, Zhe Xu, Yuwen Liu, Tianhao Liao, Haohao Luo, Ying Shen</author><pubDate>Fri, 15 Aug 2025 03:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18013v2</guid></item><item><title>Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking</title><link>http://arxiv.org/abs/2403.09717v2</link><description>Depression-diagnosis-oriented chat aims to guide patients in self-expressionto collect key symptoms for depression detection. Recent work focuses oncombining task-oriented dialogue and chitchat to simulate the interview-baseddepression diagnosis. Whereas, these methods can not well capture the changinginformation, feelings, or symptoms of the patient during dialogues. Moreover,no explicit framework has been explored to guide the dialogue, which results insome useless communications that affect the experience. In this paper, wepropose to integrate Psychological State Tracking (POST) within the largelanguage model (LLM) to explicitly guide depression-diagnosis-oriented chat.Specifically, the state is adapted from a psychological theoretical model,which consists of four components, namely Stage, Information, Summary and Next.We fine-tune an LLM model to generate the dynamic psychological state, which isfurther used to assist response generation at each turn to simulate thepsychiatrist. Experimental results on the existing benchmark show that ourproposed method boosts the performance of all subtasks indepression-diagnosis-oriented chat.</description><author>Yiyang Gu, Yougen Zhou, Qin Chen, Ningning Zhou, Jie Zhou, Aimin Zhou, Liang He</author><pubDate>Wed, 20 Aug 2025 02:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09717v2</guid></item><item><title>Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models</title><link>http://arxiv.org/abs/2308.15022v4</link><description>Recently, large language models (LLMs), such as GPT-4, stand out remarkableconversational abilities, enabling them to engage in dynamic and contextuallyrelevant dialogues across a wide range of topics. However, given a longconversation, these chatbots fail to recall past information and tend togenerate inconsistent responses. To address this, we propose to recursivelygenerate summaries/ memory using large language models (LLMs) to enhancelong-term memory ability. Specifically, our method first stimulates LLMs tomemorize small dialogue contexts and then recursively produce new memory usingprevious memory and following contexts. Finally, the chatbot can easilygenerate a highly consistent response with the help of the latest memory. Weevaluate our method on both open and closed LLMs, and the experiments on thewidely-used public dataset show that our method can generate more consistentresponses in a long-context conversation. Also, we show that our strategy couldnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhancedLLMs, bringing further long-term dialogue performance. Notably, our method is apotential solution to enable the LLM to model the extremely long context. Thecode and scripts are released.</description><author>Qingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian, Liang Ding</author><pubDate>Mon, 25 Aug 2025 14:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15022v4</guid></item><item><title>"Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues</title><link>http://arxiv.org/abs/2508.19077v1</link><description>In a doctor-patient dialogue, the primary objective of physicians is todiagnose patients and propose a treatment plan. Medical doctors guide theseconversations through targeted questioning to efficiently gather theinformation required to provide the best possible outcomes for patients. To thebest of our knowledge, this is the first work that studies physician intenttrajectories in doctor-patient dialogues. We use the `Ambient ClinicalIntelligence Benchmark' (Aci-bench) dataset for our study. We collaborate withmedical professionals to develop a fine-grained taxonomy of physician intentsbased on the SOAP framework (Subjective, Objective, Assessment, and Plan). Wethen conduct a large-scale annotation effort to label over 5000 doctor-patientturns with the help of a large number of medical experts recruited usingProlific, a popular crowd-sourcing platform. This large labeled dataset is animportant resource contribution that we use for benchmarking thestate-of-the-art generative and encoder models for medical intentclassification tasks. Our findings show that our models understand the generalstructure of medical dialogues with high accuracy, but often fail to identifytransitions between SOAP categories. We also report for the first time commontrajectories in medical dialogue structures that provide valuable insights fordesigning `differential diagnosis' systems. Finally, we extensively study theimpact of intent filtering for medical dialogue summarization and observe asignificant boost in performance. We make the codes and data, includingannotation guidelines, publicly available athttps://github.com/DATEXIS/medical-intent-classification.</description><author>Tom Röhr, Soumyadeep Roy, Fares Al Mohamad, Jens-Michalis Papaioannou, Wolfgang Nejdl, Felix Gers, Alexander Löser</author><pubDate>Tue, 26 Aug 2025 14:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19077v1</guid></item><item><title>HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention</title><link>http://arxiv.org/abs/2509.07475v1</link><description>Detecting content that contradicts or is unsupported by a given source textis a critical challenge for the safe deployment of generative language models.We introduce HALT-RAG, a post-hoc verification system designed to identifyhallucinations in the outputs of Retrieval-Augmented Generation (RAG)pipelines. Our flexible and task-adaptable framework uses a universal featureset derived from an ensemble of two frozen, off-the-shelf Natural LanguageInference (NLI) models and lightweight lexical signals. These features are usedto train a simple, calibrated, and task-adapted meta-classifier. Using arigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage andproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.By pairing our universal feature set with a lightweight, task-adaptedclassifier and a precision-constrained decision policy, HALT-RAG achievesstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,and dialogue tasks, respectively. The system's well-calibrated probabilitiesenable a practical abstention mechanism, providing a reliable tool forbalancing model performance with safety requirements.</description><author>Saumya Goswami, Siddharth Kurra</author><pubDate>Tue, 09 Sep 2025 07:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07475v1</guid></item></channel></rss>