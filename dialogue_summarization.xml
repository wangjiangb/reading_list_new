<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivdialogue summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 27 Aug 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems</title><link>http://arxiv.org/abs/2402.18013v2</link><description>This survey provides a comprehensive review of research on multi-turndialogue systems, with a particular focus on multi-turn dialogue systems basedon large language models (LLMs). This paper aims to (a) give a summary ofexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)elaborate recent advances in multi-turn dialogue systems, covering bothLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,along with datasets and evaluation metrics; (c) discuss some future emphasisand recent research problems arising from the development of LLMs and theincreasing demands on multi-turn dialogue systems.</description><author>Zihao Yi, Jiarui Ouyang, Zhe Xu, Yuwen Liu, Tianhao Liao, Haohao Luo, Ying Shen</author><pubDate>Fri, 15 Aug 2025 03:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18013v2</guid></item><item><title>Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking</title><link>http://arxiv.org/abs/2403.09717v2</link><description>Depression-diagnosis-oriented chat aims to guide patients in self-expressionto collect key symptoms for depression detection. Recent work focuses oncombining task-oriented dialogue and chitchat to simulate the interview-baseddepression diagnosis. Whereas, these methods can not well capture the changinginformation, feelings, or symptoms of the patient during dialogues. Moreover,no explicit framework has been explored to guide the dialogue, which results insome useless communications that affect the experience. In this paper, wepropose to integrate Psychological State Tracking (POST) within the largelanguage model (LLM) to explicitly guide depression-diagnosis-oriented chat.Specifically, the state is adapted from a psychological theoretical model,which consists of four components, namely Stage, Information, Summary and Next.We fine-tune an LLM model to generate the dynamic psychological state, which isfurther used to assist response generation at each turn to simulate thepsychiatrist. Experimental results on the existing benchmark show that ourproposed method boosts the performance of all subtasks indepression-diagnosis-oriented chat.</description><author>Yiyang Gu, Yougen Zhou, Qin Chen, Ningning Zhou, Jie Zhou, Aimin Zhou, Liang He</author><pubDate>Wed, 20 Aug 2025 02:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09717v2</guid></item><item><title>Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models</title><link>http://arxiv.org/abs/2308.15022v4</link><description>Recently, large language models (LLMs), such as GPT-4, stand out remarkableconversational abilities, enabling them to engage in dynamic and contextuallyrelevant dialogues across a wide range of topics. However, given a longconversation, these chatbots fail to recall past information and tend togenerate inconsistent responses. To address this, we propose to recursivelygenerate summaries/ memory using large language models (LLMs) to enhancelong-term memory ability. Specifically, our method first stimulates LLMs tomemorize small dialogue contexts and then recursively produce new memory usingprevious memory and following contexts. Finally, the chatbot can easilygenerate a highly consistent response with the help of the latest memory. Weevaluate our method on both open and closed LLMs, and the experiments on thewidely-used public dataset show that our method can generate more consistentresponses in a long-context conversation. Also, we show that our strategy couldnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhancedLLMs, bringing further long-term dialogue performance. Notably, our method is apotential solution to enable the LLM to model the extremely long context. Thecode and scripts are released.</description><author>Qingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian, Liang Ding</author><pubDate>Mon, 25 Aug 2025 14:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15022v4</guid></item></channel></rss>