<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivdialogue summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 18 Aug 2025 18:39:15 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Cultural Commonsense Knowledge for Intercultural Dialogues</title><link>http://arxiv.org/abs/2402.10689v3</link><description>Despite recent progress, large language models (LLMs) still face thechallenge of appropriately reacting to the intricacies of social and culturalconventions. This paper presents MANGO, a methodology for distillinghigh-accuracy, high-recall assertions of cultural knowledge. We judiciously anditeratively prompt LLMs for this purpose from two entry points, concepts andcultures. Outputs are consolidated via clustering and generative summarization.Running the MANGO method with GPT-3.5 as underlying LLM yields 167Khigh-accuracy assertions for 30K concepts and 11K cultures, surpassing priorresources by a large margin in quality and size. In an extrinsic evaluation forintercultural dialogues, we explore augmenting dialogue systems with culturalknowledge assertions. Notably, despite LLMs inherently possessing culturalknowledge, we find that adding knowledge from MANGO improves the overallquality, specificity, and cultural sensitivity of dialogue responses, as judgedby human annotators. Data and code are available for download.</description><author>Tuan-Phong Nguyen, Simon Razniewski, Gerhard Weikum</author><pubDate>Tue, 23 Jul 2024 10:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10689v3</guid></item><item><title>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</title><link>http://arxiv.org/abs/2305.18290v3</link><description>While large-scale unsupervised language models (LMs) learn broad worldknowledge and some reasoning skills, achieving precise control of theirbehavior is difficult due to the completely unsupervised nature of theirtraining. Existing methods for gaining such steerability collect human labelsof the relative quality of model generations and fine-tune the unsupervised LMto align with these preferences, often with reinforcement learning from humanfeedback (RLHF). However, RLHF is a complex and often unstable procedure, firstfitting a reward model that reflects the human preferences, and thenfine-tuning the large unsupervised LM using reinforcement learning to maximizethis estimated reward without drifting too far from the original model. In thispaper we introduce a new parameterization of the reward model in RLHF thatenables extraction of the corresponding optimal policy in closed form, allowingus to solve the standard RLHF problem with only a simple classification loss.The resulting algorithm, which we call Direct Preference Optimization (DPO), isstable, performant, and computationally lightweight, eliminating the need forsampling from the LM during fine-tuning or performing significanthyperparameter tuning. Our experiments show that DPO can fine-tune LMs to alignwith human preferences as well as or better than existing methods. Notably,fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment ofgenerations, and matches or improves response quality in summarization andsingle-turn dialogue while being substantially simpler to implement and train.</description><author>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn</author><pubDate>Mon, 29 Jul 2024 22:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18290v3</guid></item><item><title>Instructive Dialogue Summarization with Query Aggregations</title><link>http://arxiv.org/abs/2310.10981v3</link><description>Conventional dialogue summarization methods directly generate summaries anddo not consider user's specific interests. This poses challenges in cases wherethe users are more focused on particular topics or aspects. With theadvancement of instruction-finetuned language models, we introduceinstruction-tuning to dialogues to expand the capability set of dialoguesummarization models. To overcome the scarcity of instructive dialoguesummarization data, we propose a three-step approach to synthesize high-qualityquery-based summarization triples. This process involves summary-anchored querygeneration, query filtering, and query-based summary generation. By training aunified model called InstructDS (Instructive Dialogue Summarization) on threesummarization datasets with multi-purpose instructive triples, we expand thecapability of dialogue summarization models. We evaluate our method on fourdatasets, including dialogue summarization and dialogue reading comprehension.Experimental results show that our approach outperforms the state-of-the-artmodels and even models with larger sizes. Additionally, our model exhibitshigher generalizability and faithfulness, as confirmed by human subjectiveevaluations.</description><author>Bin Wang, Zhengyuan Liu, Nancy F. Chen</author><pubDate>Thu, 01 Aug 2024 09:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10981v3</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v7</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Tue, 06 Aug 2024 14:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v7</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v8</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Fri, 09 Aug 2024 14:48:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v8</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v9</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Thu, 22 Aug 2024 10:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v9</guid></item><item><title>MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues</title><link>http://arxiv.org/abs/2408.14418v1</link><description>Automatic Speech Recognition (ASR) systems are pivotal in transcribing speechinto text, yet the errors they introduce can significantly degrade theperformance of downstream tasks like summarization. This issue is particularlypronounced in clinical dialogue summarization, a low-resource domain wheresupervised data for fine-tuning is scarce, necessitating the use of ASR modelsas black-box solutions. Employing conventional data augmentation for enhancingthe noise robustness of summarization models is not feasible either due to theunavailability of sufficient medical dialogue audio recordings andcorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,an approach for generating synthetic samples for data augmentation using LargeLanguage Models (LLMs). Specifically, we leverage the in-context learningcapabilities of LLMs and instruct them to generate ASR-like errors based on afew available medical dialogue examples with audio recordings. Experimentalresults show that LLMs can effectively model ASR noise, and incorporating thisnoisy data into the training process significantly improves the robustness andaccuracy of medical dialogue summarization systems. This approach addresses thechallenges of noisy ASR outputs in critical applications, offering a robustsolution to enhance the reliability of clinical dialogue summarization.</description><author>Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler</author><pubDate>Mon, 26 Aug 2024 17:04:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14418v1</guid></item><item><title>Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system</title><link>http://arxiv.org/abs/2307.15793v2</link><description>Meetings play a critical infrastructural role in the coordination of work. Inrecent years, due to shift to hybrid and remote work, more meetings are movingto online Computer Mediated Spaces. This has led to new problems (e.g. moretime spent in less engaging meetings) and new opportunities (e.g. automatedtranscription/captioning and recap support). Recent advances in large languagemodels (LLMs) for dialog summarization have the potential to improve theexperience of meetings by reducing individuals' meeting load and increasing theclarity and alignment of meeting outputs. Despite this potential, they facetechnological limitation due to long transcripts and inability to capturediverse recap needs based on user's context. To address these gaps, we design,implement and evaluate in-context a meeting recap system. We firstconceptualize two salient recap representations -- important highlights, and astructured, hierarchical minutes view. We develop a system to operationalizethe representations with dialogue summarization as its building blocks.Finally, we evaluate the effectiveness of the system with seven users in thecontext of their work meetings. Our findings show promise in using LLM-baseddialogue summarization for meeting recap and the need for both representationsin different contexts. However, we find that LLM-based recap still lacks anunderstanding of whats personally relevant to participants, can miss importantdetails, and mis-attributions can be detrimental to group dynamics. We identifycollaboration opportunities such as a shared recap document that a high qualityrecap enables. We report on implications for designing AI systems to partnerwith users to learn and improve from natural interactions to overcome thelimitations related to personal relevance and summarization quality.</description><author>Sumit Asthana, Sagih Hilleli, Pengcheng He, Aaron Halfaker</author><pubDate>Thu, 29 Aug 2024 00:32:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15793v2</guid></item><item><title>E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning</title><link>http://arxiv.org/abs/2409.06679v1</link><description>In the realm of Large Language Models (LLMs), the ability to process longcontexts is increasingly crucial for tasks such as multi-round dialogues, codegeneration, and document summarization. This paper addresses the challenges ofenhancing the long-context performance, reducing computational complexity, andleveraging pretrained models collectively termed the "impossible triangle." Weintroduce E2LLM (Encoder Elongated Large Language Models), a novel approachthat effectively navigates this paradox. The method involves splitting longcontexts into chunks, compressing each into embedding vectors via a pretrainedtext encoder, and utilizing an adapter to align these representations with adecoder-only LLM. Two training objectives, focusing on reconstruction of theencoder output and long-context instruction fine-tuning, are employed tofacilitate the understanding of soft prompts by the LLM. Experimental resultsdemonstrate that E2LLM achieves superior performance in long-context scenarioswhile balancing efficiency, performance, and compatibility with pretrainedmodels. Our framework thus represents a significant advancement in the field,contributing to effective long-text modeling.</description><author>Zihan Liao, Jun Wang, Hang Yu, Lingxiao Wei, Jianguo Li, Jun Wang, Wei Zhang</author><pubDate>Tue, 10 Sep 2024 17:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06679v1</guid></item><item><title>Increasing faithfulness in human-human dialog summarization with Spoken Language Understanding tasks</title><link>http://arxiv.org/abs/2409.10070v1</link><description>Dialogue summarization aims to provide a concise and coherent summary ofconversations between multiple speakers. While recent advancements in languagemodels have enhanced this process, summarizing dialogues accurately andfaithfully remains challenging due to the need to understand speakerinteractions and capture relevant information. Indeed, abstractive models usedfor dialog summarization may generate summaries that contain inconsistencies.We suggest using the semantic information proposed for performing SpokenLanguage Understanding (SLU) in human-machine dialogue systems forgoal-oriented human-human dialogues to obtain a more semantically faithfulsummary regarding the task. This study introduces three key contributions:First, we propose an exploration of how incorporating task-related informationcan enhance the summarization process, leading to more semantically accuratesummaries. Then, we introduce a new evaluation criterion based on tasksemantics. Finally, we propose a new dataset version with increased annotateddata standardized for research on task-oriented dialogue summarization. Thestudy evaluates these methods using the DECODA corpus, a collection of Frenchspoken dialogues from a call center. Results show that integrating models withtask-related information improves summary accuracy, even with varying worderror rates.</description><author>Eunice Akani, Benoit Favre, Frederic Bechet, Romain Gemignani</author><pubDate>Mon, 16 Sep 2024 08:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10070v1</guid></item><item><title>Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation</title><link>http://arxiv.org/abs/2409.09324v1</link><description>Scientific research indicates that for every hour spent in direct patientcare, physicians spend nearly two additional hours on administrative tasks,particularly on electronic health records (EHRs) and desk work. This excessiveadministrative burden not only reduces the time available for patient care butalso contributes to physician burnout and inefficiencies in healthcaredelivery. To address these challenges, this study introduces MediGen, afine-tuned large language model (LLM) designed to automate the generation ofmedical reports from medical dialogues. By leveraging state-of-the-artmethodologies for fine-tuning open-source pretrained models, includingLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizingclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promisingresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicatingits effectiveness in generating accurate and clinically relevant medicalreports. These findings suggest that MediGen has the potential to significantlyreduce the administrative workload on physicians, improving both healthcareefficiency and physician well-being.</description><author>Hui Yi Leong, Yi Fan Gao, Ji Shuai, Uktu Pamuksuz</author><pubDate>Sat, 14 Sep 2024 06:02:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09324v1</guid></item><item><title>CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation for Meeting Summarization</title><link>http://arxiv.org/abs/2409.10883v1</link><description>Large Language Models (LLMs) have spurred interest in automatic evaluationmethods for summarization, offering a faster, more cost-effective alternativeto human evaluation. However, existing methods often fall short when applied tocomplex tasks like long-context summarizations and dialogue-based meetingsummarizations. In this paper, we introduce CREAM (Comparison-BasedReference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), anovel framework that addresses the unique challenges of evaluating meetingsummaries. CREAM leverages a combination of chain-of-thought reasoning and keyfacts alignment to assess conciseness and completeness of model-generatedsummaries without requiring reference. By employing an ELO ranking system, ourapproach provides a robust mechanism for comparing the quality of differentmodels or prompt configurations.</description><author>Ziwei Gong, Lin Ai, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Zehui Wu, Ahmad Emami, Julia Hirschberg</author><pubDate>Tue, 17 Sep 2024 04:39:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10883v1</guid></item><item><title>Natural Language Processing for Dialects of a Language: A Survey</title><link>http://arxiv.org/abs/2401.05632v3</link><description>State-of-the-art natural language processing (NLP) models are trained onmassive training corpora, and report a superlative performance on evaluationdatasets. This survey delves into an important attribute of these datasets: thedialect of a language. Motivated by the performance degradation of NLP modelsfor dialectic datasets and its implications for the equity of languagetechnologies, we survey past research in NLP for dialects in terms of datasets,and approaches. We describe a wide range of NLP tasks in terms of twocategories: natural language understanding (NLU) (for tasks such as dialectclassification, sentiment analysis, parsing, and NLU benchmarks) and naturallanguage generation (NLG) (for summarisation, machine translation, and dialoguesystems). The survey is also broad in its coverage of languages which includeEnglish, Arabic, German among others. We observe that past work in NLPconcerning dialects goes deeper than mere dialect classification, and . Thisincludes early approaches that used sentence transduction that lead to therecent approaches that integrate hypernetworks into LoRA. We expect that thissurvey will be useful to NLP researchers interested in building equitablelanguage technologies by rethinking LLM benchmarks and model architectures.</description><author>Aditya Joshi, Raj Dabre, Diptesh Kanojia, Zhuang Li, Haolan Zhan, Gholamreza Haffari, Doris Dippold</author><pubDate>Wed, 18 Sep 2024 00:02:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05632v3</guid></item><item><title>SPL: A Socratic Playground for Learning Powered by Large Language Model</title><link>http://arxiv.org/abs/2406.13919v4</link><description>Dialogue-based Intelligent Tutoring Systems (ITSs) have significantlyadvanced adaptive and personalized learning by automating sophisticated humantutoring strategies within interactive dialogues. However, replicating thenuanced patterns of expert human communication remains a challenge in NaturalLanguage Processing (NLP). Recent advancements in NLP, particularly LargeLanguage Models (LLMs) such as OpenAI's GPT-4, offer promising solutions byproviding human-like and context-aware responses based on extensive pre-trainedknowledge. Motivated by the effectiveness of LLMs in various educational tasks(e.g., content creation and summarization, problem-solving, and automatedfeedback provision), our study introduces the Socratic Playground for Learning(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs theSocratic teaching method to foster critical thinking among learners. Throughextensive prompt engineering, SPL can generate specific learning scenarios andfacilitates efficient multi-turn tutoring dialogues. The SPL system aims toenhance personalized and adaptive learning experiences tailored to individualneeds, specifically focusing on improving critical thinking skills. Our pilotexperimental results from essay writing tasks demonstrate SPL has the potentialto improve tutoring interactions and further enhance dialogue-based ITSfunctionalities. Our study, exemplified by SPL, demonstrates how LLMs enhancedialogue-based ITSs and expand the accessibility and efficacy of educationaltechnologies.</description><author>Liang Zhang, Jionghao Lin, Ziyi Kuang, Sheng Xu, Xiangen Hu</author><pubDate>Wed, 25 Sep 2024 01:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13919v4</guid></item><item><title>MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions</title><link>http://arxiv.org/abs/2410.02743v1</link><description>Reinforcement learning from human feedback (RLHF) has demonstratedeffectiveness in aligning large language models (LLMs) with human preferences.However, token-level RLHF suffers from the credit assignment problem over longsequences, where delayed rewards make it challenging for the model to discernwhich actions contributed to successful outcomes. This hinders learningefficiency and slows convergence. In this paper, we propose MA-RLHF, a simpleyet effective RLHF framework that incorporates macro actions -- sequences oftokens or higher-level language constructs -- into the learning process. Byoperating at this higher level of abstraction, our approach reduces thetemporal distance between actions and rewards, facilitating faster and moreaccurate credit assignment. This results in more stable policy gradientestimates and enhances learning efficiency within each episode, all withoutincreasing computational complexity during training or inference. We validateour approach through extensive experiments across various model sizes andtasks, including text summarization, dialogue generation, question answering,and program synthesis. Our method achieves substantial performance improvementsover standard RLHF, with performance gains of up to 30% in text summarizationand code generation, 18% in dialogue, and 8% in question answering tasks.Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster interms of training time and continues to outperform it with further training. Wewill make our code and data publicly available athttps://github.com/ernie-research/MA-RLHF .</description><author>Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, Hua Wu</author><pubDate>Thu, 03 Oct 2024 17:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02743v1</guid></item><item><title>SparsePO: Controlling Preference Alignment of LLMs via Sparse Token Masks</title><link>http://arxiv.org/abs/2410.05102v1</link><description>Preference Optimization (PO) has proven an effective step for aligninglanguage models to human-desired behaviors. Current variants, following theoffline Direct Preference Optimization objective, have focused on a strictsetting where all tokens are contributing signals of KL divergence and rewardsto the loss function. However, human preference is not affected by each word ina sequence equally but is often dependent on specific words or phrases, e.g.existence of toxic terms leads to non-preferred responses. Based on thisobservation, we argue that not all tokens should be weighted equally during POand propose a flexible objective termed SparsePO, that aims to automaticallylearn to weight the KL divergence and reward corresponding to each token duringPO training. We propose two different variants of weight-masks that can eitherbe derived from the reference model itself or learned on the fly. Notably, ourmethod induces sparsity in the learned masks, allowing the model to learn howto best weight reward and KL divergence contributions at the token level,learning an optimal level of mask sparsity. Extensive experiments on multipledomains, including sentiment control, dialogue, text summarization andtext-to-code generation, illustrate that our approach assigns meaningfulweights to tokens according to the target task, generates more responses withthe desired preference and improves reasoning tasks by up to 2 percentagepoints compared to other token- and response-level PO methods.</description><author>Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang</author><pubDate>Mon, 07 Oct 2024 15:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05102v1</guid></item><item><title>A Novel LLM-based Two-stage Summarization Approach for Long Dialogues</title><link>http://arxiv.org/abs/2410.06520v1</link><description>Long document summarization poses a significant challenge in natural languageprocessing due to input lengths that exceed the capacity of moststate-of-the-art pre-trained language models. This study proposes ahierarchical framework that segments and condenses information from longdocuments, subsequently fine-tuning the processed text with an abstractivesummarization model. Unsupervised topic segmentation methods identifysemantically appropriate breakpoints. The condensation stage utilizes anunsupervised generation model to generate condensed data, and our currentexperiments employ ChatGPT(v3.5). The summarization stage fine-tunes theabstractive summarization model on the condensed data to generate the finalresults. This framework enables long documents to be processed on models evenwhen the document length exceeds the model's maximum input size. The exclusionof the entire document from the summarization model reduces the time andcomputational resources required for training, making the framework suitablefor contexts with constrained local computational resources.</description><author>Yuan-Jhe Yin, Bo-Yu Chen, Berlin Chen</author><pubDate>Wed, 09 Oct 2024 03:42:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06520v1</guid></item><item><title>SparsePO: Controlling Preference Alignment of LLMs via Sparse Token Masks</title><link>http://arxiv.org/abs/2410.05102v2</link><description>Preference Optimization (PO) has proven an effective step for aligninglanguage models to human-desired behaviors. Current variants, following theoffline Direct Preference Optimization objective, have focused on a strictsetting where all tokens are contributing signals of KL divergence and rewardsto the loss function. However, human preference is not affected by each word ina sequence equally but is often dependent on specific words or phrases, e.g.existence of toxic terms leads to non-preferred responses. Based on thisobservation, we argue that not all tokens should be weighted equally during POand propose a flexible objective termed SparsePO, that aims to automaticallylearn to weight the KL divergence and reward corresponding to each token duringPO training. We propose two different variants of weight-masks that can eitherbe derived from the reference model itself or learned on the fly. Notably, ourmethod induces sparsity in the learned masks, allowing the model to learn howto best weight reward and KL divergence contributions at the token level,learning an optimal level of mask sparsity. Extensive experiments on multipledomains, including sentiment control, dialogue, text summarization andtext-to-code generation, illustrate that our approach assigns meaningfulweights to tokens according to the target task, generates more responses withthe desired preference and improves reasoning tasks by up to 2 percentagepoints compared to other token- and response-level PO methods.</description><author>Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang</author><pubDate>Tue, 08 Oct 2024 15:53:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05102v2</guid></item><item><title>DiaSynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications</title><link>http://arxiv.org/abs/2409.19020v2</link><description>The scarcity of domain-specific dialogue datasets limits the development ofdialogue systems across applications. Existing research is constrained bygeneral or niche datasets that lack sufficient scale for training dialoguesystems. To address this gap, we introduce DiaSynth - a synthetic dialoguegeneration framework capable of generating high-quality, contextually richdialogues across a wide range of domains. Unlike existing frameworks, DiaSynthuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning togenerate dynamic, domain-specific dialogues with simulated personas and diverseconversational features. We perform our experiments by generating syntheticdata using different LLMs and few-shot examples from DialogSum and SAMSum. Thepretrained language models fine-tuned on the synthetic data outperform the basemodels by 16.47% on dialogue summarization, while the comparison between modelsfine-tuned on in-domain data and synthetic data shows that the synthetic datais able to capture 90.48% of the performance distribution of the in-domain dataon dialogue summarization. The quality of the data generated also increases aswe increase the size of LLM from 3B to 8B. These results validate DiaSynth'spotential as a robust alternative to traditional data collection methods. Weopen source the code and data generated for future research.</description><author>Sathya Krishnan Suresh, Wu Mengjun, Tushar Pranav, Eng Siong Chng</author><pubDate>Tue, 15 Oct 2024 12:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19020v2</guid></item><item><title>An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation</title><link>http://arxiv.org/abs/2410.12265v1</link><description>With the rapid development of large language models (LLMs), how toefficiently evaluate them has become an important research question. Existingevaluation methods often suffer from high costs, limited test formats, the needof human references, and systematic evaluation biases. To address theselimitations, our study introduces the Auto-PRE, an automatic LLM evaluationframework based on peer review. In contrast to previous studies that rely onhuman annotations, Auto-PRE selects evaluator LLMs automatically based on theirinherent traits including consistency, self-confidence, and pertinence. Weconduct extensive experiments on three tasks: summary generation, non-factoidquestion-answering, and dialogue generation. Experimental results indicate ourAuto-PRE achieves state-of-the-art performance at a lower cost. Moreover, ourstudy highlights the impact of prompt strategies and evaluation formats onevaluation performance, offering guidance for method optimization in thefuture.</description><author>Junjie Chen, Weihang Su, Zhumin Chu, Haitao Li, Qinyao Ai, Yiqun Liu, Min Zhang, Shaoping Ma</author><pubDate>Wed, 16 Oct 2024 06:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12265v1</guid></item><item><title>ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization</title><link>http://arxiv.org/abs/2410.13667v1</link><description>Dialogue agents have been receiving increasing attention for years, and thistrend has been further boosted by the recent progress of large language models(LLMs). Stance detection and dialogue summarization are two core tasks ofdialogue agents in application scenarios that involve argumentative dialogues.However, research on these tasks is limited by the insufficiency of publicdatasets, especially for non-English languages. To address this languageresource gap in Chinese, we present ORCHID (Oral Chinese Debate), the firstChinese dataset for benchmarking target-independent stance detection and debatesummarization. Our dataset consists of 1,218 real-world debates that wereconducted in Chinese on 476 unique topics, containing 2,436 stance-specificsummaries and 14,133 fully annotated utterances. Besides providing a versatiletestbed for future research, we also conduct an empirical study on the datasetand propose an integrated task. The results show the challenging nature of thedataset and suggest a potential of incorporating stance detection insummarization for argumentative dialogue.</description><author>Xiutian Zhao, Ke Wang, Wei Peng</author><pubDate>Thu, 17 Oct 2024 15:28:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13667v1</guid></item><item><title>DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph</title><link>http://arxiv.org/abs/2410.14666v1</link><description>Summarizing movie screenplays presents a unique set of challenges compared tostandard document summarization. Screenplays are not only lengthy, but alsofeature a complex interplay of characters, dialogues, and scenes, with numerousdirect and subtle relationships and contextual nuances that are difficult formachine learning models to accurately capture and comprehend. Recent attemptsat screenplay summarization focus on fine-tuning transformer-based pre-trainedmodels, but these models often fall short in capturing long-term dependenciesand latent relationships, and frequently encounter the "lost in the middle"issue. To address these challenges, we introduce DiscoGraMS, a novel resourcethat represents movie scripts as a movie character-aware discourse graph (CaDGraph). This approach is well-suited for various downstream tasks, such assummarization, question-answering, and salience detection. The model aims topreserve all salient information, offering a more comprehensive and faithfulrepresentation of the screenplay's content. We further explore a baselinemethod that combines the CaD Graph with the corresponding movie script througha late fusion of graph and text modalities, and we present very initialpromising results.</description><author>Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra</author><pubDate>Fri, 18 Oct 2024 17:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14666v1</guid></item><item><title>Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization</title><link>http://arxiv.org/abs/2410.14545v1</link><description>Meeting summarization is crucial in digital communication, but existingsolutions struggle with salience identification to generate personalized,workable summaries, and context understanding to fully comprehend the meetings'content. Previous attempts to address these issues by considering relatedsupplementary resources (e.g., presentation slides) alongside transcripts arehindered by models' limited context sizes and handling the additionalcomplexities of the multi-source tasks, such as identifying relevantinformation in additional files and seamlessly aligning it with the meetingcontent. This work explores multi-source meeting summarization consideringsupplementary materials through a three-stage large language model approach:identifying transcript passages needing additional context, inferring relevantdetails from supplementary materials and inserting them into the transcript,and generating a summary from this enriched transcript. Our multi-sourceapproach enhances model understanding, increasing summary relevance by ~9% andproducing more content-rich outputs. We introduce a personalization protocolthat extracts participant characteristics and tailors summaries accordingly,improving informativeness by ~10%. This work further provides insights onperformance-cost trade-offs across four leading model families, includingedge-device capable options. Our approach can be extended to similar complexgenerative tasks benefitting from additional resources and personalization,such as dialogue systems and action planning.</description><author>Frederic Kirstein, Terry Ruas, Robert Kratel, Bela Gipp</author><pubDate>Fri, 18 Oct 2024 15:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14545v1</guid></item><item><title>Systematic Exploration of Dialogue Summarization Approaches for Reproducibility, Comparative Assessment, and Methodological Innovations for Advancing Natural Language Processing in Abstractive Summarization</title><link>http://arxiv.org/abs/2410.15962v1</link><description>Reproducibility in scientific research, particularly within the realm ofnatural language processing (NLP), is essential for validating and verifyingthe robustness of experimental findings. This paper delves into thereproduction and evaluation of dialogue summarization models, focusingspecifically on the discrepancies observed between original studies and ourreproduction efforts. Dialogue summarization is a critical aspect of NLP,aiming to condense conversational content into concise and informativesummaries, thus aiding in efficient information retrieval and decision-makingprocesses. Our research involved a thorough examination of several dialoguesummarization models using the AMI (Augmented Multi-party Interaction) dataset.The models assessed include Hierarchical Memory Networks (HMNet) and variousversions of Pointer-Generator Networks (PGN), namely PGN(DKE), PGN(DRD),PGN(DTS), and PGN(DALL). The primary objective was to evaluate theinformativeness and quality of the summaries generated by these models throughhuman assessment, a method that introduces subjectivity and variability in theevaluation process. The analysis began with Dataset 1, where the samplestandard deviation of 0.656 indicated a moderate dispersion of data pointsaround the mean.</description><author>Yugandhar Reddy Gogireddy, Jithendra Reddy Gogireddy</author><pubDate>Mon, 21 Oct 2024 12:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15962v1</guid></item><item><title>Assessment of Transformer-Based Encoder-Decoder Model for Human-Like Summarization</title><link>http://arxiv.org/abs/2410.16842v1</link><description>In recent times, extracting valuable information from large text is makingsignificant progress. Especially in the current era of social media, peopleexpect quick bites of information. Automatic text summarization seeks to tacklethis by slimming large texts down into more manageable summaries. Thisimportant research area can aid in decision-making by digging out salientcontent from large text. With the progress in deep learning models, significantwork in language models has emerged. The encoder-decoder framework in deeplearning has become the central approach for automatic text summarization. Thiswork leverages transformer-based BART model for human-like summarization whichis an open-ended problem with many challenges. On training and fine-tuning theencoder-decoder model, it is tested with diverse sample articles and thequality of summaries of diverse samples is assessed based on human evaluationparameters. Further, the finetuned model performance is compared with thebaseline pretrained model based on evaluation metrics like ROUGE score andBERTScore. Additionally, domain adaptation of the model is required forimproved performance of abstractive summarization of dialogues betweeninterlocutors. On investigating, the above popular evaluation metrics are foundto be insensitive to factual errors. Further investigation of the summariesgenerated by finetuned model is done using the contemporary evaluation metricsof factual consistency like WeCheck and SummaC. Empirical results on BBC Newsarticles highlight that the gold standard summaries written by humans are morefactually consistent by 17% than the abstractive summaries generated byfinetuned model.</description><author>Sindhu Nair, Y. S. Rao, Radha Shankarmani</author><pubDate>Tue, 22 Oct 2024 09:25:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16842v1</guid></item><item><title>Context-Aware LLM Translation System Using Conversation Summarization and Dialogue History</title><link>http://arxiv.org/abs/2410.16775v1</link><description>Translating conversational text, particularly in customer support contexts,presents unique challenges due to its informal and unstructured nature. Wepropose a context-aware LLM translation system that leverages conversationsummarization and dialogue history to enhance translation quality for theEnglish-Korean language pair. Our approach incorporates the two most recentdialogues as raw data and a summary of earlier conversations to manage contextlength effectively. We demonstrate that this method significantly improvestranslation accuracy, maintaining coherence and consistency acrossconversations. This system offers a practical solution for customer supporttranslation tasks, addressing the complexities of conversational text.</description><author>Mingi Sung, Seungmin Lee, Jiwon Kim, Sejoon Kim</author><pubDate>Tue, 22 Oct 2024 07:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16775v1</guid></item><item><title>Unstructured Text Enhanced Open-domain Dialogue System: A Systematic Survey</title><link>http://arxiv.org/abs/2411.09166v1</link><description>Incorporating external knowledge into dialogue generation has been proven tobenefit the performance of an open-domain Dialogue System (DS), such asgenerating informative or stylized responses, controlling conversation topics.In this article, we study the open-domain DS that uses unstructured text asexternal knowledge sources (\textbf{U}nstructured \textbf{T}ext\textbf{E}nhanced \textbf{D}ialogue \textbf{S}ystem, \textbf{UTEDS}). Theexistence of unstructured text entails distinctions between UTEDS andtraditional data-driven DS and we aim to analyze these differences. We firstgive the definition of the UTEDS related concepts, then summarize the recentlyreleased datasets and models. We categorize UTEDS into Retrieval and Generativemodels and introduce them from the perspective of model components. Theretrieval models consist of Fusion, Matching, and Ranking modules, while thegenerative models comprise Dialogue and Knowledge Encoding, KnowledgeSelection, and Response Generation modules. We further summarize the evaluationmethods utilized in UTEDS and analyze the current models' performance. At last,we discuss the future development trends of UTEDS, hoping to inspire newresearch in this field.</description><author>Longxuan Ma, Mingda Li, Weinan Zhang, Jiapeng Li, Ting Liu</author><pubDate>Thu, 14 Nov 2024 03:54:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09166v1</guid></item><item><title>Key-Element-Informed sLLM Tuning for Document Summarization</title><link>http://arxiv.org/abs/2406.04625v3</link><description>Remarkable advances in large language models (LLMs) have enabled high-qualitytext summarization. However, this capability is currently accessible onlythrough LLMs of substantial size or proprietary LLMs with usage fees. Inresponse, smaller-scale LLMs (sLLMs) of easy accessibility and low costs havebeen extensively studied, yet they often suffer from missing key informationand entities, i.e., low relevance, in particular, when input documents arelong. We hence propose a key-element-informed instruction tuning forsummarization, so-called KEITSum, which identifies key elements in documentsand instructs sLLM to generate summaries capturing these key elements.Experimental results on dialogue and news datasets demonstrate that sLLM withKEITSum indeed provides high-quality summarization with higher relevance andless hallucinations, competitive to proprietary LLM.</description><author>Sangwon Ryu, Heejin Do, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok</author><pubDate>Tue, 19 Nov 2024 12:41:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04625v3</guid></item><item><title>CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs</title><link>http://arxiv.org/abs/2412.02602v1</link><description>This paper analyzes the performance of Small Language Models (SLMs) andVision Language Models (VLMs) and evaluates the trade-off between modelperformance and carbon emissions across 4 essential tasks: Image Captioning,Visual Question Answering (VQA), Dialogue Summarization and Text-to-SQLconversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecturefamily are chosen and variants based on model size in terms of the number ofparameters, quantization level and fine-tuning parameters are evaluated. Themodel variant's performance and carbon emissions are calculated. To quantifythe trade-off between model performance and carbon emissions, we introduce anovel metric called CEGI (Carbon Efficient Gain Index). This metric representsthe carbon emission per unit percentage gain per million trainable parameters .This metric provides a normalized measure to compare model's efficiency interms of performance improvement relative to their environmental cost. Theexperiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieveperformance levels comparable to Large Language Models (LLMs) while producingsignificantly less carbon emissions. Our findings suggest that the marginalgains in accuracy from larger models do not justify the substantial increase incarbon emissions. Leveraging lower-bit quantization levels, the proposed metricfurther enhances energy efficiency without compromising performance. This studyhighlights balancing high performance and environmental sustainability. Itoffers a valuable metric for selecting models suitable forenvironmentally-friendly AI development.</description><author>Abhas Kumar, Kapil Pathak, Rajesh Kavuru, Prabhakar Srinivasan</author><pubDate>Tue, 03 Dec 2024 17:32:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02602v1</guid></item><item><title>Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models</title><link>http://arxiv.org/abs/2412.05167v1</link><description>Large Audio-Language Models (LALMs) have unclocked audio dialoguecapabilities, where audio dialogues are a direct exchange of spoken languagebetween LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMsin back-and-forth audio dialogues with humans. This progression not onlyunderscores the potential of LALMs but also broadens their applicability acrossa wide range of practical scenarios supported by audio dialogues. However,given these advancements, a comprehensive benchmark to evaluate the performanceof LALMs in the open-ended audio dialogue understanding remains absentcurrently. To address this gap, we propose an Audio Dialogue UnderstandingBenchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess theopen-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills,9 multilingual languages, and 4 categories of ambiguity handling. Notably, wefirstly propose the evaluation of ambiguity handling in audio dialogues thatexpresses different intentions beyond the same literal meaning of sentences,e.g., "Really!?" with different intonations. In summary, ADU-Bench includesover 20,000 open-ended audio dialogues for the assessment of LALMs. Throughextensive experiments conducted on 13 LALMs, our analysis reveals that there isstill considerable room for improvement in the audio dialogue understandingabilities of existing LALMs. In particular, they struggle with mathematicalsymbols and formulas, understanding human behavior such as roleplay,comprehending multiple languages, and handling audio dialogue ambiguities fromdifferent phonetic elements, such as intonations, pause positions, andhomophones.</description><author>Kuofeng Gao, Shu-Tao Xia, Ke Xu, Philip Torr, Jindong Gu</author><pubDate>Fri, 06 Dec 2024 16:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05167v1</guid></item><item><title>Large Language Model Benchmarks in Medical Tasks</title><link>http://arxiv.org/abs/2410.21348v2</link><description>With the increasing application of large language models (LLMs) in themedical domain, evaluating these models' performance using benchmark datasetshas become crucial. This paper presents a comprehensive survey of variousbenchmark datasets employed in medical LLM tasks. These datasets span multiplemodalities including text, image, and multimodal benchmarks, focusing ondifferent aspects of medical knowledge such as electronic health records(EHRs), doctor-patient dialogues, medical question-answering, and medical imagecaptioning. The survey categorizes the datasets by modality, discussing theirsignificance, data structure, and impact on the development of LLMs forclinical tasks such as diagnosis, report generation, and predictive decisionsupport. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, andCheXpert, which have facilitated advancements in tasks like medical reportgeneration, clinical summarization, and synthetic data generation. The papersummarizes the challenges and opportunities in leveraging these benchmarks foradvancing multimodal medical intelligence, emphasizing the need for datasetswith a greater degree of language diversity, structured omics data, andinnovative approaches to synthesis. This work also provides a foundation forfuture research in the application of LLMs in medicine, contributing to theevolving field of medical artificial intelligence.</description><author>Lawrence K. Q. Yan, Qian Niu, Ming Li, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Benji Peng, Ziqian Bi, Pohsun Feng, Keyu Chen, Tianyang Wang, Yunze Wang, Silin Chen, Ming Liu, Junyu Liu</author><pubDate>Mon, 09 Dec 2024 10:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21348v2</guid></item><item><title>Integrating automatic speech recognition into remote healthcare interpreting: A pilot study of its impact on interpreting quality</title><link>http://arxiv.org/abs/2502.03381v1</link><description>This paper reports on the results from a pilot study investigating the impactof automatic speech recognition (ASR) technology on interpreting quality inremote healthcare interpreting settings. Employing a within-subjects experimentdesign with four randomised conditions, this study utilises scripted medicalconsultations to simulate dialogue interpreting tasks. It involves four traineeinterpreters with a language combination of Chinese and English. It alsogathers participants' experience and perceptions of ASR support through cuedretrospective reports and semi-structured interviews. Preliminary data suggestthat the availability of ASR, specifically the access to full ASR transcriptsand to ChatGPT-generated summaries based on ASR, effectively improvedinterpreting quality. Varying types of ASR output had different impacts on thedistribution of interpreting error types. Participants reported similarinteractive experiences with the technology, expressing their preference forfull ASR transcripts. This pilot study shows encouraging results of applyingASR to dialogue-based healthcare interpreting and offers insights into theoptimal ways to present ASR output to enhance interpreter experience andperformance. However, it should be emphasised that the main purpose of thisstudy was to validate the methodology and that further research with a largersample size is necessary to confirm these findings.</description><author>Shiyi Tan, Constantin Orăsan, Sabine Braun</author><pubDate>Wed, 05 Feb 2025 17:17:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.03381v1</guid></item><item><title>DiaSynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications</title><link>http://arxiv.org/abs/2409.19020v3</link><description>The scarcity of domain-specific dialogue datasets limits the development ofdialogue systems across applications. Existing research is constrained bygeneral or niche datasets that lack sufficient scale for training dialoguesystems. To address this gap, we introduce DiaSynth - a synthetic dialoguegeneration framework capable of generating high-quality, contextually richdialogues across a wide range of domains. Unlike existing frameworks, DiaSynthuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning togenerate dynamic, domain-specific dialogues with simulated personas and diverseconversational features. We perform our experiments by generating syntheticdata using different LLMs and few-shot examples from DialogSum and SAMSum. Thepretrained language models fine-tuned on the synthetic data outperform the basemodels by 16.47% on dialogue summarization, while the comparison between modelsfine-tuned on in-domain data and synthetic data shows that the synthetic datais able to capture 90.48% of the performance distribution of the in-domain dataon dialogue summarization. The quality of the data generated also increases aswe increase the size of LLM from 3B to 8B. These results validate DiaSynth'spotential as a robust alternative to traditional data collection methods. Weopen source the code and data generated for future research.</description><author>Sathya Krishnan Suresh, Wu Mengjun, Tushar Pranav, Eng Siong Chng</author><pubDate>Mon, 10 Feb 2025 16:42:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19020v3</guid></item><item><title>Hello Again! LLM-powered Personalized Agent for Long-term Dialogue</title><link>http://arxiv.org/abs/2406.05925v2</link><description>Open-domain dialogue systems have seen remarkable advancements with thedevelopment of large language models (LLMs). Nonetheless, most existingdialogue systems predominantly focus on brief single-session interactions,neglecting the real-world demands for long-term companionship and personalizedinteractions with chatbots. Crucial to addressing this real-world need areevent summary and persona management, which enable reasoning for appropriatelong-term dialogue responses. Recent progress in the human-like cognitive andreasoning capabilities of LLMs suggests that LLM-based agents couldsignificantly enhance automated perception, decision-making, andproblem-solving. In response to this potential, we introduce a model-agnosticframework, the Long-term Dialogue Agent (LD-Agent), which incorporates threeindependently tunable modules dedicated to event perception, personaextraction, and response generation. For the event memory module, long andshort-term memory banks are employed to separately focus on historical andongoing sessions, while a topic-based retrieval mechanism is introduced toenhance the accuracy of memory retrieval. Furthermore, the persona moduleconducts dynamic persona modeling for both users and agents. The integration ofretrieved memories and extracted personas is subsequently fed into thegenerator to induce appropriate responses. The effectiveness, generality, andcross-domain capabilities of LD-Agent are empirically demonstrated acrossvarious illustrative benchmarks, models, and tasks. The code is released athttps://github.com/leolee99/LD-Agent.</description><author>Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua</author><pubDate>Thu, 13 Feb 2025 18:02:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05925v2</guid></item></channel></rss>