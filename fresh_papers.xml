<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 08 Oct 2025 13:00:10 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Human3R: Everyone Everywhere All at Once</title><link>http://arxiv.org/abs/2510.06219v1</link><description>We present Human3R, a unified, feed-forward framework for online 4Dhuman-scene reconstruction, in the world frame, from casually capturedmonocular videos. Unlike previous approaches that rely on multi-stagepipelines, iterative contact-aware refinement between humans and scenes, andheavy dependencies, e.g., human detection, depth estimation, and SLAMpre-processing, Human3R jointly recovers global multi-person SMPL-X bodies("everyone"), dense 3D scene ("everywhere"), and camera trajectories in asingle forward pass ("all-at-once"). Our method builds upon the 4D onlinereconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,to strive to preserve CUT3R's rich spatiotemporal priors, while enabling directreadout of multiple SMPL-X bodies. Human3R is a unified model that eliminatesheavy dependencies and iterative refinement. After being trained on therelatively small-scale synthetic dataset BEDLAM for just one day on one GPU, itachieves superior performance with remarkable efficiency: it reconstructsmultiple humans in a one-shot manner, along with 3D scenes, in one stage, atreal-time speed (15 FPS) with a low memory footprint (8 GB). Extensiveexperiments demonstrate that Human3R delivers state-of-the-art or competitiveperformance across tasks, including global human motion estimation, local humanmesh recovery, video depth estimation, and camera pose estimation, with asingle unified model. We hope that Human3R will serve as a simple yet strongbaseline, be easily extended for downstream applications.Code available inhttps://fanegg.github.io/Human3R</description><author>Yue Chen, Xingyu Chen, Yuxuan Xue, Anpei Chen, Yuliang Xiu, Gerard Pons-Moll</author><pubDate>Tue, 07 Oct 2025 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06219v1</guid></item><item><title>EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</title><link>http://arxiv.org/abs/2510.06218v1</link><description>Most existing benchmarks for egocentric vision understanding focus primarilyon daytime scenarios, overlooking the low-light conditions that are inevitablein real-world applications. To investigate this gap, we present EgoNight, thefirst comprehensive benchmark for nighttime egocentric vision, with visualquestion answering (VQA) as the core task. A key feature of EgoNight is theintroduction of day-night aligned videos, which enhance night annotationquality using the daytime data and reveal clear performance gaps betweenlighting conditions. To achieve this, we collect both synthetic videos renderedby Blender and real-world recordings, ensuring that scenes and actions arevisually and temporally aligned. Leveraging these paired videos, we constructEgoNight-VQA, supported by a novel day-augmented night auto-labeling engine andrefinement through extensive human verification. Each QA pair is double-checkedby annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairsacross 90 videos, spanning 12 diverse QA types, with more than 300 hours ofhuman work. Evaluations of state-of-the-art multimodal large language models(MLLMs) reveal substantial performance drops when transferring from day tonight, underscoring the challenges of reasoning under low-light conditions.Beyond VQA, EgoNight also introduces two auxiliary tasks, day-nightcorrespondence retrieval and egocentric depth estimation at night, that furtherexplore the boundaries of existing models. We believe EgoNight-VQA provides astrong foundation for advancing application-driven egocentric vision researchand for developing models that generalize across illumination domains. All thedata and code will be made available upon acceptance.</description><author>Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, Ajad Chhatkuli, Xuanjing Huang, Yu-Gang Jiang, Luc Van Gool, Danda Pani Paudel</author><pubDate>Tue, 07 Oct 2025 17:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06218v1</guid></item><item><title>TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning</title><link>http://arxiv.org/abs/2510.06217v1</link><description>Process Reward Models (PRMs) have recently emerged as a powerful frameworkfor enhancing the reasoning capabilities of large reasoning models (LRMs),particularly in the context of test-time scaling (TTS). However, theirpotential for supervising LRMs on tabular reasoning domains remainsunderexplored. Through detailed empirical analyses, we identify that existingPRMs, though widely adopted for supervising text-only reasoning steps, strugglewith table-specific operations such as sub-table retrieval and schemainteraction, leading to critical performance bottlenecks. To address thislimitation, we propose TaTToo, a novel table-grounded PRM framework that (i)reasons explicitly over tabular reasoning steps and (ii) integrates tool-basedverification to provide precise reward supervision. Concretely, we first designa scalable data curation pipeline that constructs over 60k high-qualitystep-level annotations by integrating table verification rationales withtool-based executions. Building on the collected data, we train TaTToo with adual-stage paradigm: cold-start supervised fine-tuning to capture tool-usereasoning patterns, followed by reinforcement learning with tool-groundedreward shaping to align our model with table-based verification. We provide acomprehensive evaluation of the policy improvement induced by our newlydesigned PRM. Across 5 challenging tabular reasoning benchmarks coveringnumerical reasoning, fact-checking, and data analysis, TaTToo improvesdownstream policy LRMs by 30.9% at inference, surpasses strong PRM baselinessuch as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates stronggeneralizability across diverse TTS strategies.</description><author>Jiaru Zou, Soumya Roy, Vinay Kumar Verma, Ziyi Wang, David Wipf, Pan Lu, Sumit Negi, James Zou, Jingrui He</author><pubDate>Tue, 07 Oct 2025 17:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06217v1</guid></item><item><title>Dropping the D: RGB-D SLAM Without the Depth Sensor</title><link>http://arxiv.org/abs/2510.06216v1</link><description>We present DropD-SLAM, a real-time monocular SLAM system that achievesRGB-D-level accuracy without relying on depth sensors. The system replacesactive depth input with three pretrained vision modules: a monocular metricdepth estimator, a learned keypoint detector, and an instance segmentationnetwork. Dynamic objects are suppressed using dilated instance masks, whilestatic keypoints are assigned predicted depth values and backprojected into 3Dto form metrically scaled features. These are processed by an unmodified RGB-DSLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAMattains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences,matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPSon a single GPU. These results suggest that modern pretrained vision models canreplace active depth sensors as reliable, real-time sources of metric scale,marking a step toward simpler and more cost-effective SLAM systems.</description><author>Mert Kiray, Alican Karaomer, Benjamin Busam</author><pubDate>Tue, 07 Oct 2025 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06216v1</guid></item><item><title>Fine-grained Defocus Blur Control for Generative Image Models</title><link>http://arxiv.org/abs/2510.06215v1</link><description>Current text-to-image diffusion models excel at generating diverse,high-quality images, yet they struggle to incorporate fine-grained camerametadata such as precise aperture settings. In this work, we introduce a noveltext-to-image diffusion framework that leverages camera metadata, or EXIF data,which is often embedded in image files, with an emphasis on generatingcontrollable lens blur. Our method mimics the physical image formation processby first generating an all-in-focus image, estimating its monocular depth,predicting a plausible focus distance with a novel focus distance transformer,and then forming a defocused image with an existing differentiable lens blurmodel. Gradients flow backwards through this whole process, allowing us tolearn without explicit supervision to generate defocus effects based on contentelements and the provided EXIF data. At inference time, this enables preciseinteractive user control over defocus effects while preserving scene contents,which is not achievable with existing diffusion models. Experimental resultsdemonstrate that our model enables superior fine-grained control withoutaltering the depicted scene.</description><author>Ayush Shrivastava, Connelly Barnes, Xuaner Zhang, Lingzhi Zhang, Andrew Owens, Sohrab Amirghodsi, Eli Shechtman</author><pubDate>Tue, 07 Oct 2025 17:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06215v1</guid></item><item><title>Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents</title><link>http://arxiv.org/abs/2510.06214v1</link><description>Large language model (LLM) agents increasingly rely on external tools such assearch engines to solve complex, multi-step problems, and reinforcementlearning (RL) has become a key paradigm for training them. However, thetrajectories of search agents are structurally heterogeneous, where variationsin the number, placement, and outcomes of search calls lead to fundamentallydifferent answer directions and reward distributions. Standard policy gradientmethods, which use a single global baseline, suffer from what we identify andformalize as cross-stratum bias-an "apples-to-oranges" comparison ofheterogeneous trajectories. This cross-stratum bias distorts credit assignmentand hinders exploration of complex, multi-step search strategies. To addressthis, we propose Stratified GRPO, whose central component, Stratified AdvantageNormalization (SAN), partitions trajectories into homogeneous strata based ontheir structural properties and computes advantages locally within eachstratum. This ensures that trajectories are evaluated only against their truepeers. Our analysis proves that SAN eliminates cross-stratum bias, yieldsconditionally unbiased unit-variance estimates inside each stratum, and retainsthe global unbiasedness and unit-variance properties enjoyed by standardnormalization, resulting in a more pure and scale-stable learning signal. Toimprove practical stability under finite-sample regimes, we further linearlyblend SAN with the global estimator. Extensive experiments on diversesingle-hop and multi-hop question-answering benchmarks demonstrate thatStratified GRPO consistently and substantially outperforms GRPO by up to 11.3points, achieving higher training rewards, greater training stability, and moreeffective search policies. These results establish stratification as aprincipled remedy for structural heterogeneity in RL for LLM search agents.</description><author>Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia</author><pubDate>Tue, 07 Oct 2025 17:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06214v1</guid></item><item><title>Training Dynamics Impact Post-Training Quantization Robustness</title><link>http://arxiv.org/abs/2510.06213v1</link><description>While post-training quantization is widely adopted for efficient deploymentof large language models, the mechanisms underlying quantization robustnessremain unclear. We conduct a comprehensive analysis of quantization degradationacross open-source language model training trajectories up to 32B parametersand 15T training tokens to accurately assess the relationship between trainingdynamics and quantization performance. Our key finding is that quantizationerrors in large-scale training runs are driven by a complex interplay betweenlearning rate and other training hyperparameters. Specifically, once learningrates decay, validation loss and quantization error diverge, largelyindependent of training data scale. To investigate interventions on thetraining dynamics and identify specific configurations that can modulatequantization robustness favorably, we train our own models in controlledexperiments up to 100B tokens. Our results challenge the assumption thatincreasing dataset scale inherently compromises quantization effectiveness,demonstrating instead that strategic training hyperparameter interventions canimprove quantization quality at scale.</description><author>Albert Catalan-Tatjer, Niccolò Ajroldi, Jonas Geiping</author><pubDate>Tue, 07 Oct 2025 17:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06213v1</guid></item><item><title>LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</title><link>http://arxiv.org/abs/2510.04573v2</link><description>Large Language Models (LLMs) demonstrate their reasoning ability throughchain-of-thought (CoT) generation. However, LLM's autoregressive decoding maylimit the ability to revisit and refine earlier tokens in a holistic manner,which can also lead to inefficient exploration for diverse solutions. In thispaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoningframework that unifies the expressiveness of continuous latent representationwith the iterative refinement capabilities of latent diffusion models for anexisting LLM. We first construct a structured latent reasoning space using aVariational Autoencoder (VAE) that encodes text reasoning steps into blocks ofthought tokens, preserving semantic information and interpretability whileoffering compact but expressive representations. Subsequently, we utilize alatent diffusion model that learns to denoise a block of latent thought tokenswith a blockwise bidirectional attention mask, enabling longer horizon anditerative refinement with adaptive test-time compute. This design allowsefficient parallel generation of diverse reasoning trajectories, allowing themodel to plan and revise the reasoning process holistically. We conductevaluations on a suite of mathematical reasoning and planning benchmarks.Empirical results show that LaDiR consistently improves accuracy, diversity,and interpretability over existing autoregressive, diffusion-based, and latentreasoning methods, revealing a new paradigm for text reasoning with latentdiffusion.</description><author>Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, Lianhui Qin</author><pubDate>Tue, 07 Oct 2025 17:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.04573v2</guid></item><item><title>Drive&amp;Gen: Co-Evaluating End-to-End Driving and Video Generation Models</title><link>http://arxiv.org/abs/2510.06209v1</link><description>Recent advances in generative models have sparked exciting new possibilitiesin the field of autonomous vehicles. Specifically, video generation models arenow being explored as controllable virtual testing environments.Simultaneously, end-to-end (E2E) driving models have emerged as a streamlinedalternative to conventional modular autonomous driving systems, gainingpopularity for their simplicity and scalability. However, the application ofthese techniques to simulation and planning raises important questions. First,while video generation models can generate increasingly realistic videos, canthese videos faithfully adhere to the specified conditions and be realisticenough for E2E autonomous planner evaluation? Second, given that data iscrucial for understanding and controlling E2E planners, how can we gain deeperinsights into their biases and improve their ability to generalize toout-of-distribution scenarios? In this work, we bridge the gap between thedriving models and generative world models (Drive&amp;Gen) to address thesequestions. We propose novel statistical measures leveraging E2E drivers toevaluate the realism of generated videos. By exploiting the controllability ofthe video generation model, we conduct targeted experiments to investigatedistribution gaps affecting E2E planner performance. Finally, we show thatsynthetic data produced by the video generation model offers a cost-effectivealternative to real-world data collection. This synthetic data effectivelyimproves E2E model generalization beyond existing Operational Design Domains,facilitating the expansion of autonomous vehicle services into new operationalcontexts.</description><author>Jiahao Wang, Zhenpei Yang, Yijing Bai, Yingwei Li, Yuliang Zou, Bo Sun, Abhijit Kundu, Jose Lezama, Luna Yue Huang, Zehao Zhu, Jyh-Jing Hwang, Dragomir Anguelov, Mingxing Tan, Chiyu Max Jiang</author><pubDate>Tue, 07 Oct 2025 17:58:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06209v1</guid></item><item><title>HOG-Diff: Higher-Order Guided Diffusion for Graph Generation</title><link>http://arxiv.org/abs/2502.04308v2</link><description>Graph generation is a critical yet challenging task as empirical analysesrequire a deep understanding of complex, non-Euclidean structures. Diffusionmodels have recently made significant achievements in graph generation, butthese models are typically adapted from image generation frameworks andoverlook inherent higher-order topology, leaving them ill-suited for capturingthe topological properties of graphs. In this work, we propose Higher-orderGuided Diffusion (HOG-Diff), a principled framework that progressivelygenerates plausible graphs with inherent topological structures. HOG-Difffollows a coarse-to-fine generation curriculum guided by higher-order topologyand implemented via diffusion bridges. We further prove that our model exhibitsa stronger theoretical guarantee than classical diffusion frameworks. Extensiveexperiments on both molecular and generic graph generation tasks demonstratethat our method consistently outperforms or remains competitive withstate-of-the-art baselines. Our code is available athttps://github.com/Yiminghh/HOG-Diff.</description><author>Yiming Huang, Tolga Birdal</author><pubDate>Tue, 07 Oct 2025 17:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04308v2</guid></item><item><title>ShapeGen4D: Towards High Quality 4D Shape Generation from Videos</title><link>http://arxiv.org/abs/2510.06208v1</link><description>Video-conditioned 4D shape generation aims to recover time-varying 3Dgeometry and view-consistent appearance directly from an input video. In thiswork, we introduce a native video-to-4D shape generation framework thatsynthesizes a single dynamic 3D representation end-to-end from the video. Ourframework introduces three key components based on large-scale pre-trained 3Dmodels: (i) a temporal attention that conditions generation on all frames whileproducing a time-indexed dynamic representation; (ii) a time-aware pointsampling and 4D latent anchoring that promote temporally consistent geometryand texture; and (iii) noise sharing across frames to enhance temporalstability. Our method accurately captures non-rigid motion, volume changes, andeven topological transitions without per-frame optimization. Across diversein-the-wild videos, our method improves robustness and perceptual fidelity andreduces failure modes compared with the baselines.</description><author>Jiraphon Yenphraphai, Ashkan Mirzaei, Jianqi Chen, Jiaxu Zou, Sergey Tulyakov, Raymond A. Yeh, Peter Wonka, Chaoyang Wang</author><pubDate>Tue, 07 Oct 2025 17:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06208v1</guid></item><item><title>Generative Interfaces for Language Models</title><link>http://arxiv.org/abs/2508.19227v2</link><description>Large language models (LLMs) are increasingly seen as assistants, copilots,and consultants, capable of supporting a wide range of tasks through naturalconversation. However, most systems remain constrained by a linearrequest-response format that often makes interactions inefficient inmulti-turn, information-dense, and exploratory tasks. To address theselimitations, we propose Generative Interfaces for Language Models, a paradigmin which LLMs respond to user queries by proactively generating user interfaces(UIs) that enable more adaptive and interactive engagement. Our frameworkleverages structured interface-specific representations and iterativerefinements to translate user queries into task-specific UIs. For systematicevaluation, we introduce a multidimensional assessment framework that comparesgenerative interfaces with traditional chat-based ones across diverse tasks,interaction patterns, and query types, capturing functional, interactive, andemotional aspects of user experience. Results show that generative interfacesconsistently outperform conversational ones, with up to a 72% improvement inhuman preference. These findings clarify when and why users favor generativeinterfaces, paving the way for future advancements in human-AI interaction.</description><author>Jiaqi Chen, Yanzhe Zhang, Yutong Zhang, Yijia Shao, Diyi Yang</author><pubDate>Tue, 07 Oct 2025 17:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19227v2</guid></item><item><title>Hierarchical Reasoning Models: Perspectives and Misconceptions</title><link>http://arxiv.org/abs/2510.00355v2</link><description>Transformers have demonstrated remarkable performance in natural languageprocessing and related domains, as they largely focus on sequential,autoregressive next-token prediction tasks. Yet, they struggle in logicalreasoning, not necessarily because of a fundamental limitation of these models,but possibly due to the lack of exploration of more creative uses, such aslatent space and recurrent reasoning. An emerging exploration in this directionis the Hierarchical Reasoning Model (Wang et. al., 2025), which introduces anovel type of recurrent reasoning in the latent space of transformers,achieving remarkable performance on a wide range of 2D reasoning tasks. Despitethe promising results, this line of models is still at an early stage and callsfor in-depth investigation. In this work, we review this class of models,examine key design choices, test alternative variants and clarify commonmisconceptions.</description><author>Renee Ge, Qianli Liao, Tomaso Poggio</author><pubDate>Tue, 07 Oct 2025 17:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.00355v2</guid></item><item><title>Modulation Discovery with Differentiable Digital Signal Processing</title><link>http://arxiv.org/abs/2510.06204v1</link><description>Modulations are a critical part of sound design and music production,enabling the creation of complex and evolving audio. Modern synthesizersprovide envelopes, low frequency oscillators (LFOs), and more parameterautomation tools that allow users to modulate the output with ease. However,determining the modulation signals used to create a sound is difficult, andexisting sound-matching / parameter estimation systems are oftenuninterpretable black boxes or predict high-dimensional framewise parametervalues without considering the shape, structure, and routing of the underlyingmodulation curves. We propose a neural sound-matching approach that leveragesmodulation extraction, constrained control signal parameterizations, anddifferentiable digital signal processing (DDSP) to discover the modulationspresent in a sound. We demonstrate the effectiveness of our approach on highlymodulated synthetic and real audio samples, its applicability to different DDSPsynth architectures, and investigate the trade-off it incurs betweeninterpretability and sound-matching accuracy. We make our code and audiosamples available and provide the trained DDSP synths in a VST plugin.</description><author>Christopher Mitcheltree, Hao Hao Tan, Joshua D. Reiss</author><pubDate>Tue, 07 Oct 2025 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06204v1</guid></item><item><title>Tracing Multilingual Factual Knowledge Acquisition in Pretraining</title><link>http://arxiv.org/abs/2505.14824v2</link><description>Large Language Models (LLMs) are capable of recalling multilingual factualknowledge present in their pretraining data. However, most studies evaluateonly the final model, leaving the development of factual recall andcrosslingual consistency throughout pretraining largely unexplored. In thiswork, we trace how factual recall and crosslingual consistency evolve duringpretraining, focusing on OLMo-7B as a case study. We find that both accuracyand consistency improve over time for most languages. We show that thisimprovement is primarily driven by the fact frequency in the pretrainingcorpus: more frequent facts are more likely to be recalled correctly,regardless of language. Yet, some low-frequency facts in non-English languagescan still be correctly recalled. Our analysis reveals that these instanceslargely benefit from crosslingual transfer of their English counterparts -- aneffect that emerges predominantly in the early stages of pretraining. Wepinpoint two distinct pathways through which multilingual factual knowledgeacquisition occurs: (1) frequency-driven learning, which is dominant andlanguage-agnostic, and (2) crosslingual transfer, which is limited in scale andtypically constrained to relation types involving named entities. We releaseour code and data to facilitate further research athttps://github.com/cisnlp/multilingual-fact-tracing.</description><author>Yihong Liu, Mingyang Wang, Amir Hossein Kargaran, Felicia Körner, Ercong Nie, Barbara Plank, François Yvon, Hinrich Schütze</author><pubDate>Tue, 07 Oct 2025 17:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14824v2</guid></item><item><title>Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning</title><link>http://arxiv.org/abs/2510.04022v2</link><description>We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QAframework that preserves a fixed token budget by first \emph{localizing}question-relevant interval(s) with a low-fps skim and then \emph{answering} viaspan-aware reallocation of visual tokens at higher effective frame rate,emitting an interleaved output with both spans and the final option for directattribution. We also introduce \dataname{}, which converts description basedevent graphs into \emph{span-grounded} multiple-choice QA by pairing eachquestion with \emph{ground-truth} time span(s) and related reasoning. ViTL istrained end-to-end with an interleaved group-relative objective that couplestemporal IoU for localization with answer correctness, allowing credit to flowfrom answers back to spans without increasing compute. Under fixed tokenbudgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA andtemporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablationsshow that span-aware token reallocation consistently surpasses uniformsampling. Together, \dataname{} and ViTL provide an interpretable,compute-efficient recipe for scalable long-video QA.</description><author>Chendong Wang, Donglin Bai, Yifan Yang, Xiao Jin, Anlan Zhang, Rui Wang, Shiqi Jiang, Yuqing Yang, Hao Wu, Qi Dai, Chong Luo, Ting Cao, Lili Qiu, Suman Banerjee</author><pubDate>Tue, 07 Oct 2025 17:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.04022v2</guid></item><item><title>LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</title><link>http://arxiv.org/abs/2509.14252v2</link><description>Large Language Model (LLM) pretraining, finetuning, and evaluation rely oninput-space reconstruction and generative capabilities. Yet, it has beenobserved in vision that embedding-space training objectives, e.g., with JointEmbedding Predictive Architectures (JEPAs), are far superior to theirinput-space counterpart. That mismatch in how training is achieved betweenlanguage and vision opens up a natural question: {\em can language trainingmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM isa testimony of the challenge in designing such objectives for language. In thiswork, we propose a first step in that direction where we develop LLM-JEPA, aJEPA based solution for LLMs applicable both to finetuning and pretraining.Thus far, LLM-JEPA is able to outperform the standard LLM training objectivesby a significant margin across models, all while being robust to overfiting.Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider,RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmofamilies. Code: https://github.com/rbalestr-lab/llm-jepa.</description><author>Hai Huang, Yann LeCun, Randall Balestriero</author><pubDate>Tue, 07 Oct 2025 17:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.14252v2</guid></item><item><title>Reference Grounded Skill Discovery</title><link>http://arxiv.org/abs/2510.06203v1</link><description>Scaling unsupervised skill discovery algorithms to high-DoF agents remainschallenging. As dimensionality increases, the exploration space growsexponentially, while the manifold of meaningful skills remains limited.Therefore, semantic meaningfulness becomes essential to effectively guideexploration in high-dimensional spaces. In this work, we presentReference-Grounded Skill Discovery (RGSD), a novel algorithm that grounds skilldiscovery in a semantically meaningful latent space using reference data. RGSDfirst performs contrastive pretraining to embed motions on a unit hypersphere,clustering each reference trajectory into a distinct direction. This groundingenables skill discovery to simultaneously involve both imitation of referencebehaviors and the discovery of semantically related diverse behaviors. On asimulated SMPL humanoid with 359-D observations and 69-D actions, RGSD learnsstructured skills including walking, running, punching, and side stepping, andalso discovers related novel behaviors. In downstream control tasks, RGSDoutperforms imitation-based skill acquisition baselines. Our results suggestthat lightweight reference-guided grounding offers a practical path todiscovering semantically rich and structured skills in high-DoF systems.</description><author>Seungeun Rho, Aaron Trinh, Danfei Xu, Sehoon Ha</author><pubDate>Tue, 07 Oct 2025 17:55:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06203v1</guid></item><item><title>TokenChain: A Discrete Speech Chain via Semantic Token Modeling</title><link>http://arxiv.org/abs/2510.06201v1</link><description>Machine Speech Chain, simulating the human perception-production loop, proveseffective in jointly improving ASR and TTS. We propose TokenChain, a fullydiscrete speech chain coupling semantic-token ASR with a two-stage TTS: anautoregressive text-to-semantic model co-trained with ASR and amasked-generative semantic-to-acoustic model for synthesis only. End-to-endfeedback across the text interface is enabled with straight-throughargmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weightaveraging. Ablations examine optimal temperature schedules for in- andcross-domain transfer. Evaluation reveals TokenChain surpasses baselineaccuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error withstable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by31% on TED-LIUM with minimal forgetting, showing that chain learning remainseffective with token interfaces and models.</description><author>Mingxuan Wang, Satoshi Nakamura</author><pubDate>Tue, 07 Oct 2025 17:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06201v1</guid></item><item><title>StarEmbed: Benchmarking Time Series Foundation Models on Astronomical Observations of Variable Stars</title><link>http://arxiv.org/abs/2510.06200v1</link><description>Time series foundation models (TSFMs) are increasingly being adopted ashighly-capable general-purpose time series representation learners. Althoughtheir training corpora are vast, they exclude astronomical time series data.Observations of stars produce peta-scale time series with unique challengesincluding irregular sampling and heteroskedasticity. We introduce StarEmbed,the first public benchmark for rigorous and standardized evaluation ofstate-of-the-art TSFMs on stellar time series observations (``light curves'').We benchmark on three scientifically-motivated downstream tasks: unsupervisedclustering, supervised classification, and out-of-distribution sourcedetection. StarEmbed integrates a catalog of expert-vetted labels withmulti-variate light curves from the Zwicky Transient Facility, yielding ~40khand-labeled light curves spread across seven astrophysical classes. Weevaluate the zero-shot representation capabilities of three TSFMs (MOIRAI,Chronos, Chronos-Bolt) and a domain-specific transformer (Astromer) againsthandcrafted feature extraction, the long-standing baseline in the astrophysicsliterature. Our results demonstrate that these TSFMs, especially the Chronosmodels, which are trained on data completely unlike the astronomicalobservations, can outperform established astrophysics-specific baselines insome tasks and effectively generalize to entirely new data. In particular,TSFMs deliver state-of-the-art performance on our out-of-distribution sourcedetection benchmark. With the first benchmark of TSFMs on astronomical timeseries data, we test the limits of their generalization and motivate a paradigmshift in time-domain astronomy from using task-specific, fully supervisedpipelines toward adopting generic foundation model representations for theanalysis of peta-scale datasets from forthcoming observatories.</description><author>Weijian Li, Hong-Yu Chen, Qinjie Lin, Nabeel Rehemtulla, Ved G. Shah, Dennis Wu, Adam A. Miller, Han Liu</author><pubDate>Tue, 07 Oct 2025 17:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06200v1</guid></item><item><title>Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction</title><link>http://arxiv.org/abs/2510.06198v1</link><description>This paper introduces a framework for relation extraction (RE) that enhancesboth accuracy and explainability. The framework has two key components: (i) areasoning mechanism that formulates relation extraction as a series oftext-processing steps inspired by cognitive science, and (ii) an optimizationprocess driven by reinforcement learning (RL) with a novel reward functiondesigned to improve both task accuracy and explanation quality. We call ourapproach CogRE. Our framework addresses the lack of supervision forlanguage-based explanations in traditional RE by promoting outputs that includeimportant relation keywords. These keywords are drawn from a high-qualitydictionary that is automatically constructed using an LLM. We evaluate ourapproach for the task of one-shot RE using two LLMs and two RE datasets. Ourexperiments show that CogRE improves explanation quality by addressing twocommon failure patterns in one-shot RE: poor attention focus and limitedone-shot learning capability. For example, our cognitive-structured reasoningwith Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassingprior reasoning-based designs. Optimizing this approach with RL using ourreward further improves performance by +23.46% (absolute). Finally, humanevaluation shows that our best model generates relational keywords closelyaligned with gold labels, increasing human explanation quality ratings by 54%(relative).</description><author>Xinyu Guo, Zhengliang Shi, Minglai Yang, Mahdi Rahimi, Mihai Surdeanu</author><pubDate>Tue, 07 Oct 2025 17:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06198v1</guid></item><item><title>Exploring the Potential of Conversational AI Support for Agent-Based Social Simulation Model Design</title><link>http://arxiv.org/abs/2405.08032v2</link><description>ChatGPT, the AI-powered chatbot with a massive user base of hundreds ofmillions, has become a global phenomenon. However, the use of Conversational AISystems (CAISs) like ChatGPT for research in the field of Social Simulation isstill limited. Specifically, there is no evidence of its usage in Agent-BasedSocial Simulation (ABSS) model design. This paper takes a crucial first steptoward exploring the untapped potential of this emerging technology in thecontext of ABSS model design. The research presented here demonstrates howCAISs can facilitate the development of innovative conceptual ABSS models in aconcise timeframe and with minimal required upfront case-based knowledge. Byemploying advanced prompt engineering techniques and adhering to theEngineering ABSS framework, we have constructed a comprehensive prompt scriptthat enables the design of conceptual ABSS models with or by the CAIS. Aproof-of-concept application of the prompt script, used to generate theconceptual ABSS model for a case study on the impact of adaptive architecturein a museum environment, illustrates the practicality of the approach. Despiteoccasional inaccuracies and conversational divergence, the CAIS proved to be avaluable companion for ABSS modellers.</description><author>Peer-Olaf Siebers</author><pubDate>Tue, 07 Oct 2025 17:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08032v2</guid></item><item><title>Latent Speech-Text Transformer</title><link>http://arxiv.org/abs/2510.06195v1</link><description>Auto-regressive speech-text models are typically pre-trained on a largenumber of interleaved sequences of text tokens and raw speech encoded as speechtokens using vector quantization. These models have demonstratedstate-of-the-art performance in speech-to-speech understanding and generationbenchmarks, together with promising scaling laws, primarily enabled by therepresentational alignment between text and speech. Nevertheless, they sufferfrom shortcomings, partly owing to the disproportionately longer sequences ofspeech tokens in contrast to textual tokens. This results in a large computeimbalance between modalities during pre-training as well as during inference,and a potential hindrance to effectively aligning speech and text, ultimatelytranslating to several orders of magnitude slower scaling laws. We introducethe Latent Speech-Text Transformer (LST), which makes pre-training speech-textmodels more data-efficient by dynamically and inexpensively aggregating speechtokens into latent speech patches. These patches serve as higher-level unitsthat can either align with corresponding textual units to aid capabilitytransfer or even encapsulate common speech sequences like silences to be morecompute-efficient. We show that LST outperforms vanilla approaches onspeech-to-speech as well as text-to-text benchmarks in both data- andcompute-controlled settings, the former indicating more effectiverepresentational alignment and the latter indicating steeper scaling laws forspeech-text models. On HellaSwag story completion, LST achieves 6.5% absolutegain in speech accuracy under compute-controlled training and 5.3% underdata-controlled training, while also improving text performance. We willrelease our models, code, and the evaluation data to facilitate furtherresearch.</description><author>Yen-Ju Lu, Yashesh Gaur, Wei Zhou, Benjamin Muller, Jesus Villalba, Najim Dehak, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Srinivasan Iyer, Duc Le</author><pubDate>Tue, 07 Oct 2025 17:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06195v1</guid></item><item><title>Overlap-aware segmentation for topological reconstruction of obscured objects</title><link>http://arxiv.org/abs/2510.06194v1</link><description>The separation of overlapping objects presents a significant challenge inscientific imaging. While deep learning segmentation-regression algorithms canpredict pixel-wise intensities, they typically treat all regions equally ratherthan prioritizing overlap regions where attribution is most ambiguous. Recentadvances in instance segmentation show that weighting regions of pixel overlapin training can improve segmentation boundary predictions in regions ofoverlap, but this idea has not yet been extended to segmentation regression. Weaddress this with Overlap-Aware Segmentation of ImageS (OASIS): a newsegmentation-regression framework with a weighted loss function designed toprioritize regions of object-overlap during training, enabling extraction ofpixel intensities and topological features from heavily obscured objects. Wedemonstrate OASIS in the context of the MIGDAL experiment, which aims todirectly image the Migdal effect--a rare process where electron emission isinduced by nuclear scattering--in a low-pressure optical time projectionchamber. This setting poses an extreme test case, as the target forreconstruction is a faint electron recoil track which is often heavily-buriedwithin the orders-of-magnitude brighter nuclear recoil track. Compared tounweighted training, OASIS improves median intensity reconstruction errors from-32% to -14% for low-energy electron tracks (4-5 keV) and improves topologicalintersection-over-union scores from 0.828 to 0.855. These performance gainsdemonstrate OASIS's ability to recover obscured signals in overlap-dominatedregions. The framework provides a generalizable methodology for scientificimaging where pixels represent physical quantities and overlap obscuresfeatures of interest. All code is openly available to facilitate cross-domainadoption.</description><author>J. Schueler, H. M. Araújo, S. N. Balashov, J. E. Borg, C. Brew, F. M. Brunbauer, C. Cazzaniga, A. Cottle, D. Edgeman, C. D. Frost, F. Garcia, D. Hunt, M. Kastriotou, P. Knights, H. Kraus, A. Lindote, M. Lisowska, D. Loomba, E. Lopez Asamar, P. A. Majewski, T. Marley, C. McCabe, L. Millins, R. Nandakumar, T. Neep, F. Neves, K. Nikolopoulos, E. Oliveri, A. Roy, T. J. Sumner, E. Tilly, W. Thompson, M. A. Vogiatzi</author><pubDate>Tue, 07 Oct 2025 17:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06194v1</guid></item><item><title>On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond</title><link>http://arxiv.org/abs/2510.06190v1</link><description>This paper formally studies generation processes, including auto-regressivenext-token prediction and masked diffusion, that abstract beyond architecturalspecifics. At this level of abstraction, we quantify their benefits andlimitations through measurable criteria such as computational hardness andlearnability. In particular, we demonstrate that allowing generation to proceedbeyond autoregression and current masked diffusion, with capabilities torewrite and length-variable edit, can bring significant theoretical andempirical advantages, with important implications for frontier LLMs that aspireto tackle increasingly hard problems and work universally across domains beyondnatural language, such as coding and science.</description><author>Chenxiao Yang, Cai Zhou, David Wipf, Zhiyuan Li</author><pubDate>Tue, 07 Oct 2025 17:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06190v1</guid></item><item><title>Barbarians at the Gate: How AI is Upending Systems Research</title><link>http://arxiv.org/abs/2510.06189v1</link><description>Artificial Intelligence (AI) is starting to transform the research process aswe know it by automating the discovery of new solutions. Given a task, thetypical AI-driven approach is (i) to generate a set of diverse solutions, andthen (ii) to verify these solutions and select one that solves the problem.Crucially, this approach assumes the existence of a reliable verifier, i.e.,one that can accurately determine whether a solution solves the given problem.We argue that systems research, long focused on designing and evaluating newperformance-oriented algorithms, is particularly well-suited for AI-drivensolution discovery. This is because system performance problems naturally admitreliable verifiers: solutions are typically implemented in real systems orsimulators, and verification reduces to running these software artifactsagainst predefined workloads and measuring performance. We term this approachas AI-Driven Research for Systems (ADRS), which iteratively generates,evaluates, and refines solutions. Using penEvolve, an existing open-source ADRSinstance, we present case studies across diverse domains, including loadbalancing for multi-region cloud scheduling, Mixture-of-Experts inference,LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRSdiscovers algorithms that outperform state-of-the-art human designs (e.g.,achieving up to 5.0x runtime improvements or 50% cost reductions). We distillbest practices for guiding algorithm evolution, from prompt design to evaluatorconstruction, for existing frameworks. We then discuss the broader implicationsfor the systems community: as AI assumes a central role in algorithm design, weargue that human researchers will increasingly focus on problem formulation andstrategic guidance. Our results highlight both the disruptive potential and theurgent need to adapt systems research practices in the age of AI.</description><author>Audrey Cheng, Shu Liu, Melissa Pan, Zhifei Li, Bowen Wang, Alex Krentsel, Tian Xia, Mert Cemri, Jongseok Park, Shuo Yang, Jeff Chen, Aditya Desai, Jiarong Xing, Koushik Sen, Matei Zaharia, Ion Stoica</author><pubDate>Tue, 07 Oct 2025 17:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06189v1</guid></item><item><title>BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects</title><link>http://arxiv.org/abs/2510.06188v1</link><description>Real-time speech assistants are becoming increasingly popular for ensuringimproved accessibility to information. Bengali, being a low-resource languagewith a high regional dialectal diversity, has seen limited progress indeveloping such systems. Existing systems are not optimized for real-time useand focus only on standard Bengali. In this work, we present BanglaTalk, thefirst real-time speech assistance system for Bengali regional dialects.BanglaTalk follows the client-server architecture and uses the Real-timeTransport Protocol (RTP) to ensure low-latency communication. To addressdialectal variation, we introduce a dialect-aware ASR system, BRDialect,developed by fine-tuning the IndicWav2Vec model in ten Bengali regionaldialects. It outperforms the baseline ASR models by 12.41-33.98% on theRegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Lowbandwidth usage and minimal end-to-end delay make the system bothcost-effective and interactive for real-time use cases, enabling inclusive andaccessible speech technology for the diverse community of Bengali speakers.</description><author>Jakir Hasan, Shubhashis Roy Dipta</author><pubDate>Tue, 07 Oct 2025 17:47:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06188v1</guid></item><item><title>Automated Program Repair of Uncompilable Student Code</title><link>http://arxiv.org/abs/2510.06187v1</link><description>A significant portion of student programming submissions in CS1 learningenvironments are uncompilable, limiting their use in student modeling anddownstream knowledge tracing. Traditional modeling pipelines often excludethese cases, discarding observations of student learning. This studyinvestigates automated program repair as a strategy to recover uncompilablecode while preserving students' structural intent for use in student modeling.Within this framework, we assess large language models (LLMs) as repair agents,including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash(Google), under high- and low-context prompting conditions. Repairs wereevaluated for compilability, edit distance, and preservation of students'original structure and logic. We find that while all three LLMs are capable ofproducing compilable repairs, their behavior diverges in how well they preservestudents' control flow and code structure, which affects their pedagogicalutility. By recovering uncompilable submissions, this work enables richer andmore comprehensive analyses of learners' coding processes and development overtime.</description><author>Griffin Pitts, Aum Pandya, Darsh Rank, Tirth Bhatt, Muntasir Hoq, Bita Akram</author><pubDate>Tue, 07 Oct 2025 17:46:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06187v1</guid></item><item><title>RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback</title><link>http://arxiv.org/abs/2510.06186v1</link><description>Large language models (LLMs) show the promise in supporting scientificresearch implementation, yet their ability to generate correct and executablecode remains limited. Existing works largely adopt one-shot settings, ignoringthe iterative and feedback-driven nature of realistic workflows of scientificresearch development. To address this gap, we present RECODE-H, a benchmark of102 tasks from research papers and repositories that evaluates LLM agentsthrough multi-turn interactions with LLM-simulated human feedback. It includesstructured instructions,unit tests, and a five-level feedback hierarchy toreflect realistic researcher-agent collaboration. We further presentReCodeAgent, a framework that integrates feedback into iterative codegeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richerfeedback, while also highlighting ongoing challenges in the generation ofcomplex research code. RECODE-H establishes a foundation for developingadaptive, feedback-driven LLM agents in scientific research implementation</description><author>Chunyu Miao, Henry Peng Zou, Yangning Li, Yankai Chen, Yibo Wang, Fangxin Wang, Yifan Li, Wooseong Yang, Bowei He, Xinni Zhang, Dianzhi Yu, Hanchen Yang, Hoang H Nguyen, Yue Zhou, Jie Yang, Jizhou Guo, Wenzhe Fan, Chin-Yuan Yeh, Panpan Meng, Liancheng Fang, Jinhu Qi, Wei-Chieh Huang, Zhengyao Gu, Yuwei Han, Langzhou He, Yuyao Yang, Xue Liu, Irwin King, Philip S. Yu</author><pubDate>Tue, 07 Oct 2025 17:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06186v1</guid></item><item><title>Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context</title><link>http://arxiv.org/abs/2510.06182v1</link><description>A key component of in-context reasoning is the ability of language models(LMs) to bind entities for later retrieval. For example, an LM might represent"Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann"when asked "Who loves pie?" Prior research on short lists of bound entitiesfound strong evidence that LMs implement such retrieval via a positionalmechanism, where "Ann" is retrieved based on its position in context. In thiswork, we find that this mechanism generalizes poorly to more complex settings;as the number of bound entities in context increases, the positional mechanismbecomes noisy and unreliable in middle positions. To compensate for this, wefind that LMs supplement the positional mechanism with a lexical mechanism(retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism(retrieving "Ann" through a direct pointer). Through extensive experiments onnine models and ten binding tasks, we uncover a consistent pattern in how LMsmix these mechanisms to drive model behavior. We leverage these insights todevelop a causal model combining all three mechanisms that estimates next tokendistributions with 95% agreement. Finally, we show that our model generalizesto substantially longer inputs of open-ended text interleaved with entitygroups, further demonstrating the robustness of our findings in more naturalsettings. Overall, our study establishes a more complete picture of how LMsbind and retrieve entities in-context.</description><author>Yoav Gur-Arieh, Mor Geva, Atticus Geiger</author><pubDate>Tue, 07 Oct 2025 17:44:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06182v1</guid></item><item><title>Conformalized Gaussian processes for online uncertainty quantification over graphs</title><link>http://arxiv.org/abs/2510.06181v1</link><description>Uncertainty quantification (UQ) over graphs arises in a number ofsafety-critical applications in network science. The Gaussian process (GP), asa classical Bayesian framework for UQ, has been developed to handlegraph-structured data by devising topology-aware kernel functions. However,such GP-based approaches are limited not only by the prohibitive computationalcomplexity, but also the strict modeling assumptions that might yield poorcoverage, especially with labels arriving on the fly. To effect scalability, wedevise a novel graph-aware parametric GP model by leveraging the random feature(RF)-based kernel approximation, which is amenable to efficient recursiveBayesian model updates. To further allow for adaptivity, an ensemble ofgraph-aware RF-based scalable GPs have been leveraged, with per-GP weightadapted to data arriving incrementally. To ensure valid coverage withrobustness to model mis-specification, we wed the GP-based set predictors withthe online conformal prediction framework, which post-processes the predictionsets using adaptive thresholds. Experimental results the proposed method yieldsimproved coverage and efficient prediction sets over existing baselines byadaptively ensembling the GP models and setting the key threshold parameters inCP.</description><author>Jinwen Xu, Qin Lu, Georgios B. Giannakis</author><pubDate>Tue, 07 Oct 2025 17:44:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06181v1</guid></item><item><title>Climate Model Tuning with Online Synchronization-Based Parameter Estimation</title><link>http://arxiv.org/abs/2510.06180v1</link><description>In climate science, the tuning of climate models is a computationallyintensive problem due to the combination of the high-dimensionality of thesystem state and long integration times. Here we demonstrate the potential of aparameter estimation algorithm which makes use of synchronization to tune aglobal atmospheric model at modest computational costs. We first use it todirectly optimize internal model parameters. We then apply the algorithm to theweights of each member of a supermodel ensemble to optimize the overallpredictions. In both cases, the algorithm is able to find parameters whichresult in reduced errors in the climatology of the model. Finally, we introducea novel approach which combines both methods called adaptive supermodeling,where the internal parameters of the members of a supermodel are tunedsimultaneously with the model weights such that the supermodel predictions areoptimized. For a case designed to challenge the two previous methods, adaptivesupermodeling achieves a performance similar to a perfect model.</description><author>Jordan Seneca, Suzanne Bintanja, Frank M. Selten</author><pubDate>Tue, 07 Oct 2025 17:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06180v1</guid></item><item><title>Differentiable Model Predictive Control on the GPU</title><link>http://arxiv.org/abs/2510.06179v1</link><description>Differentiable model predictive control (MPC) offers a powerful framework forcombining learning and control. However, its adoption has been limited by theinherently sequential nature of traditional optimization algorithms, which arechallenging to parallelize on modern computing hardware like GPUs. In thiswork, we tackle this bottleneck by introducing a GPU-accelerated differentiableoptimization tool for MPC. This solver leverages sequential quadraticprogramming and a custom preconditioned conjugate gradient (PCG) routine withtridiagonal preconditioning to exploit the problem's structure and enableefficient parallelization. We demonstrate substantial speedups over CPU- andGPU-based baselines, significantly improving upon state-of-the-art trainingtimes on benchmark reinforcement learning and imitation learning tasks.Finally, we showcase the method on the challenging task of reinforcementlearning for driving at the limits of handling, where it enables robustdrifting of a Toyota Supra through water puddles.</description><author>Emre Adabag, Marcus Greiff, John Subosits, Thomas Lew</author><pubDate>Tue, 07 Oct 2025 17:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06179v1</guid></item><item><title>Noise2Score3D: Tweedie's Approach for Unsupervised Point Cloud Denoising</title><link>http://arxiv.org/abs/2503.09283v3</link><description>Building on recent advances in Bayesian statistics and image denoising, wepropose Noise2Score3D, a fully unsupervised framework for point clouddenoising. Noise2Score3D learns the score function of the underlying pointcloud distribution directly from noisy data, eliminating the need for cleandata during training. Using Tweedie's formula, our method performs denoising ina single step, avoiding the iterative processes used in existing unsupervisedmethods, thus improving both accuracy and efficiency. Additionally, weintroduce Total Variation for Point Clouds as a denoising quality metric, whichallows for the estimation of unknown noise parameters. Experimental resultsdemonstrate that Noise2Score3D achieves state-of-the-art performance onstandard benchmarks among unsupervised learning methods in Chamfer distance andpoint-to-mesh metrics. Noise2Score3D also demonstrates strong generalizationability beyond training datasets. Our method, by addressing the generalizationissue and challenge of the absence of clean data in learning-based methods,paves the way for learning-based point cloud denoising methods in real-worldapplications.</description><author>Xiangbin Wei, Yuanfeng Wang, Ao XU, Lingyu Zhu, Dongyong Sun, Keren Li, Yang Li, Qi Qin</author><pubDate>Tue, 07 Oct 2025 17:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.09283v3</guid></item><item><title>Analysis of kinetic Langevin Monte Carlo under the stochastic exponential Euler discretization from underdamped all the way to overdamped</title><link>http://arxiv.org/abs/2510.03949v2</link><description>Simulating the kinetic Langevin dynamics is a popular approach for samplingfrom distributions, where only their unnormalized densities are available.Various discretizations of the kinetic Langevin dynamics have been considered,where the resulting algorithm is collectively referred to as the kineticLangevin Monte Carlo (KLMC) or underdamped Langevin Monte Carlo. Specifically,the stochastic exponential Euler discretization, or exponential integrator forshort, has previously been studied under strongly log-concave and log-Lipschitzsmooth potentials via the synchronous Wasserstein coupling strategy. Existinganalyses, however, impose restrictions on the parameters that do not explainthe behavior of KLMC under various choices of parameters. In particular, allknown results fail to hold in the overdamped regime, suggesting that theexponential integrator degenerates in the overdamped limit. In this work, werevisit the synchronous Wasserstein coupling analysis of KLMC with theexponential integrator. Our refined analysis results in Wassersteincontractions and bounds on the asymptotic bias that hold under weakerrestrictions on the parameters, which assert that the exponential integrator iscapable of stably simulating the kinetic Langevin dynamics in the overdampedregime, as long as proper time acceleration is applied.</description><author>Kyurae Kim, Samuel Gruffaz, Ji Won Park, Alain Oliviero Durmus</author><pubDate>Tue, 07 Oct 2025 17:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.03949v2</guid></item><item><title>OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature</title><link>http://arxiv.org/abs/2505.22945v2</link><description>Large language models (LLMs) are known to memorize and recall English textfrom their pretraining data. However, the extent to which this abilitygeneralizes to non-English languages or transfers across languages remainsunclear. This paper investigates multilingual and cross-lingual memorization inLLMs, probing if memorized content in one language (e.g., English) can berecalled when presented in translation. To do so, we introduce OWL, a datasetof 31.5K aligned excerpts from 20 books in ten languages, including Englishoriginals, official translations (Vietnamese, Spanish, Turkish), and newtranslations in six low-resource languages (Sesotho, Yoruba, Maithili,Malagasy, Setswana, Tahitian). We evaluate memorization across model familiesand sizes through three tasks: (1) direct probing, which asks the model toidentify a book's title and author; (2) name cloze, which requires predictingmasked character names; and (3) prefix probing, which involves generatingcontinuations. We find that LLMs consistently recall content across languages,even for texts without direct translation in pretraining data. GPT-4o, forexample, identifies authors and titles 69% of the time and masked entities 6%of the time in newly translated excerpts. Perturbations (e.g., maskingcharacters, shuffling words) modestly reduce direct probing accuracy (7% dropfor shuffled official translations). Our results highlight the extent ofcross-lingual memorization and provide insights on the differences between themodels.</description><author>Alisha Srivastava, Emir Korukluoglu, Minh Nhat Le, Duyen Tran, Chau Minh Pham, Marzena Karpinska, Mohit Iyyer</author><pubDate>Tue, 07 Oct 2025 17:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.22945v2</guid></item><item><title>Electromagnetic Inverse Scattering from a Single Transmitter</title><link>http://arxiv.org/abs/2506.21349v5</link><description>Solving Electromagnetic Inverse Scattering Problems (EISP) is fundamental inapplications such as medical imaging, where the goal is to reconstruct therelative permittivity from scattered electromagnetic field. This inverseprocess is inherently ill-posed and highly nonlinear, making it particularlychallenging, especially under sparse transmitter setups, e.g., with only onetransmitter. A recent machine learning-based approach, Img-Interiors, showspromising results by leveraging continuous implicit functions. However, itrequires time-consuming case-specific optimization and fails under sparsetransmitter setups. To address these limitations, we revisit EISP from adata-driven perspective. The scarcity of transmitters leads to an insufficientamount of measured data, which fails to capture adequate physical informationfor stable inversion. Built on this insight, we propose a fully end-to-end anddata-driven framework that predicts the relative permittivity of scatterersfrom measured fields, leveraging data distribution priors to compensate for thelack of physical information. This design enables data-driven training andfeed-forward prediction of relative permittivity while maintaining strongrobustness to transmitter sparsity. Extensive experiments show that our methodoutperforms state-of-the-art approaches in reconstruction accuracy androbustness. Notably, it achieves high-quality results even with a singletransmitter, a setting where previous methods consistently fail. This workoffers a fundamentally new perspective on electromagnetic inverse scatteringand represents a major step toward cost-effective practical solutions forelectromagnetic imaging.</description><author>Yizhe Cheng, Chunxun Tian, Haoru Wang, Wentao Zhu, Xiaoxuan Ma, Yizhou Wang</author><pubDate>Tue, 07 Oct 2025 17:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.21349v5</guid></item><item><title>VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization</title><link>http://arxiv.org/abs/2510.06175v1</link><description>The Key-Value (KV) cache introduces substantial memory overhead during largelanguage model (LLM) inference. Although existing vector quantization (VQ)methods reduce KV cache usage and provide flexible representational capacityacross bit-widths, they suffer severe performance degradation at ultra-lowbit-widths due to key cache outliers that hinder effective codebookutilization. To address this challenge, we propose VecInfer, a novel VQ methodfor aggressive KV cache compression while enabling efficient inference. Byapplying smooth and Hadamard transformations, VecInfer suppresses outliers inthe key cache, enabling the codebook to comprehensively cover the original datadistribution and thereby reducing quantization difficulty. To facilitateefficient deployment, we design an optimized CUDA kernel that fuses computationwith dequantization to minimize memory access overhead. Extensive evaluationsdemonstrate that VecInfer consistently outperforms existing quantizationbaselines across both long-context understanding and mathematical reasoningtasks. With only 2-bit quantization, VecInfer achieves performance comparableto full precision, while delivering up to $\mathbf{2.7\times}$ speedup inlarge-batch self-attention computation and $\mathbf{8.3\times}$ reduction insingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.</description><author>Dingyu Yao, Chenxu Yang, Zhengyang Tong, Zheng Lin, Wei Liu, Jian Luan, Weiping Wang</author><pubDate>Tue, 07 Oct 2025 17:35:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06175v1</guid></item><item><title>Thermodynamic Performance Limits for Score-Based Diffusion Models</title><link>http://arxiv.org/abs/2510.06174v1</link><description>We establish a fundamental connection between score-based diffusion modelsand non-equilibrium thermodynamics by deriving performance limits based onentropy rates. Our main theoretical contribution is a lower bound on thenegative log-likelihood of the data that relates model performance to entropyrates of diffusion processes. We numerically validate this bound on a syntheticdataset and investigate its tightness. By building a bridge to entropy rates -system, intrinsic, and exchange entropy - we provide new insights into thethermodynamic operation of these models, drawing parallels to Maxwell's demonand implications for thermodynamic computing hardware. Our framework connectsgenerative modeling performance to fundamental physical principles throughstochastic thermodynamics.</description><author>Nathan X. Kodama, Michael Hinczewski</author><pubDate>Tue, 07 Oct 2025 17:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06174v1</guid></item><item><title>Smartphone-based iris recognition through high-quality visible-spectrum iris image capture.V2</title><link>http://arxiv.org/abs/2510.06170v1</link><description>Smartphone-based iris recognition in the visible spectrum (VIS) remainsdifficult due to illumination variability, pigmentation differences, and theabsence of standardized capture controls. This work presents a compactend-to-end pipeline that enforces ISO/IEC 29794-6 quality compliance atacquisition and demonstrates that accurate VIS iris recognition is feasible oncommodity devices. Using a custom Android application performing real-timeframing, sharpness evaluation, and feedback, we introduce the CUVIRIS datasetof 752 compliant images from 47 subjects. A lightweight MobileNetV3-basedmulti-task segmentation network (LightIrisNet) is developed for efficienton-device processing, and a transformer matcher (IrisFormer) is adapted to theVIS domain. Under a standardized protocol and comparative benchmarking againstprior CNN baselines, OSIRIS attains a TAR of 97.9% at FAR=0.01 (EER=0.76%),while IrisFormer, trained only on UBIRIS.v2, achieves an EER of 0.057% onCUVIRIS. The acquisition app, trained models, and a public subset of thedataset are released to support reproducibility. These results confirm thatstandardized capture and VIS-adapted lightweight models enable accurate andpractical iris recognition on smartphones.</description><author>Naveenkumar G Venkataswamy, Yu Liu, Soumyabrata Dey, Stephanie Schuckers, Masudul H Imtiaz</author><pubDate>Tue, 07 Oct 2025 17:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06170v1</guid></item><item><title>A Fairness-Aware Strategy for B5G Physical-layer Security Leveraging Reconfigurable Intelligent Surfaces</title><link>http://arxiv.org/abs/2506.06344v3</link><description>Reconfigurable Intelligent Surfaces are composed of physical elements thatcan dynamically alter electromagnetic wave properties to enhance beamformingand lead to improvements in areas with low coverage properties. Combined withReinforcement Learning techniques, they have the potential to be conduct aswell physical-layer security hardening. Yet, and in addition to securityimprovements, it is crucial to consider the concept of fair communication.Reconfigurable Intelligent Surfaces must ensure that User Equipment unitsreceive their signals with adequate strength, without other units beingdeprived of service due to insufficient power. In this paper, we address such aproblem. We explore the fairness properties of previous work and propose anovel method that aims at obtaining both an efficient and fair duplexReconfigurable Intelligent Surface-Reinforcement Learning system for multiplelegitimate User Equipment units without reducing the level of achievedphysical-layer security hardening. In terms of contributions, we uncover afairness imbalance of a previous physical-layer security hardening solution,validate our findings and report experimental work via simulation results. Wealso provide an alternative reward strategy to solve the uncovered problems andrelease both code and datasets to foster further research in the topics of thispaper.</description><author>Alex Pierron, Michel Barbeau, Luca De Cicco, Jose Rubio-Hernan, Joaquin Garcia-Alfaro</author><pubDate>Tue, 07 Oct 2025 17:30:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.06344v3</guid></item><item><title>Higher-Order Feature Attribution: Bridging Statistics, Explainable AI, and Topological Signal Processing</title><link>http://arxiv.org/abs/2510.06165v1</link><description>Feature attributions are post-training analysis methods that assess howvarious input features of a machine learning model contribute to an outputprediction. Their interpretation is straightforward when features actindependently, but becomes less direct when the predictive model involvesinteractions such as multiplicative relationships or joint featurecontributions. In this work, we propose a general theory of higher-orderfeature attribution, which we develop on the foundation of Integrated Gradients(IG). This work extends existing frameworks in the literature on explainableAI. When using IG as the method of feature attribution, we discover naturalconnections to statistics and topological signal processing. We provide severaltheoretical results that establish the theory, and we validate our theory on afew examples.</description><author>Kurt Butler, Guanchao Feng, Petar Djuric</author><pubDate>Tue, 07 Oct 2025 17:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06165v1</guid></item><item><title>TabPFN-Wide: Continued Pre-Training for Extreme Feature Counts</title><link>http://arxiv.org/abs/2510.06162v1</link><description>Revealing novel insights from the relationship between molecular measurementsand pathology remains a very impactful application of machine learning inbiomedicine. Data in this domain typically contain only a few observations butthousands of potentially noisy features, posing challenges for conventionalmachine learning approaches. While prior-data fitted networks emerge asfoundation models for tabular data, they are currently not suited to handlelarge feature counts (&gt;500). Although feature reduction enables theirapplication, it hinders feature importance analysis. We propose a strategy thatextends existing models through continued pre-training on synthetic datasampled from a customized prior. The resulting model, TabPFN-Wide, matches orexceeds its base model's performance while exhibiting improved robustness tonoise. It seamlessly scales beyond 50,000 features, regardless of noise levels,while maintaining inherent interpretability, which is critical for biomedicalapplications. Our results show that prior-informed adaptation is suitable toenhance the capability of foundation models for high-dimensional data. Onreal-world biomedical datasets many of the most relevant features identified bythe model overlap with previous biological findings, while others proposepotential starting points for future studies.</description><author>Christopher Kolberg, Katharina Eggensperger, Nico Pfeifer</author><pubDate>Tue, 07 Oct 2025 17:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06162v1</guid></item><item><title>Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation</title><link>http://arxiv.org/abs/2505.18875v3</link><description>Diffusion Transformers (DiTs) are essential for video generation but sufferfrom significant latency due to the quadratic complexity of attention. Bycomputing only critical tokens, sparse attention reduces computational costsand offers a promising acceleration approach. However, we identify thatexisting methods fail to approach optimal generation quality under the samecomputation budget for two reasons: (1) Inaccurate critical tokenidentification: current methods cluster tokens based on position rather thansemantics, leading to imprecise aggregated representations. (2) Excessivecomputation waste: critical tokens are scattered among non-critical ones,leading to wasted computation on GPUs, which are optimized for processingcontiguous tokens. In this paper, we propose SVG2, a training-free frameworkthat maximizes identification accuracy and minimizes computation waste,achieving a Pareto frontier trade-off between generation quality andefficiency. The core of SVG2 is semantic-aware permutation, which clusters andreorders tokens based on semantic similarity using k-means. This approachensures both a precise cluster representation, improving identificationaccuracy, and a densified layout of critical tokens, enabling efficientcomputation without padding. Additionally, SVG2 integrates top-p dynamic budgetcontrol and customized kernel implementations, achieving up to 2.30x and 1.89xspeedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan2.1, respectively. Our code is open-sourced at\href{https://github.com/svg-project/Sparse-VideoGen}{https://github.com/svg-project/Sparse-VideoGen}.</description><author>Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, Jianfei Chen, Song Han, Kurt Keutzer, Ion Stoica</author><pubDate>Tue, 07 Oct 2025 17:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.18875v3</guid></item><item><title>LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams</title><link>http://arxiv.org/abs/2510.06151v1</link><description>A critical challenge in modelling Heterogeneous-Agent Teams is trainingagents to collaborate with teammates whose policies are inaccessible ornon-stationary, such as humans. Traditional approaches rely on expensivehuman-in-the-loop data, which limits scalability. We propose using LargeLanguage Models (LLMs) as policy-agnostic human proxies to generate syntheticdata that mimics human decision-making. To evaluate this, we conduct threeexperiments in a grid-world capture game inspired by Stag Hunt, a game theoryparadigm that balances risk and reward. In Experiment 1, we compare decisionsfrom 30 human participants and 2 expert judges with outputs from LLaMA 3.1 andMixtral 8x22B models. LLMs, prompted with game-state observations and rewardstructures, align more closely with experts than participants, demonstratingconsistency in applying underlying decision criteria. Experiment 2 modifiesprompts to induce risk-sensitive strategies (e.g. "be risk averse"). LLMoutputs mirror human participants' variability, shifting between risk-averseand risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamicgrid-world where the LLM agents generate movement actions. LLMs producetrajectories resembling human participants' paths. While LLMs cannot yet fullyreplicate human adaptability, their prompt-guided diversity offers a scalablefoundation for simulating policy-agnostic teammates.</description><author>Aju Ani Justus, Chris Baber</author><pubDate>Tue, 07 Oct 2025 17:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06151v1</guid></item><item><title>OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</title><link>http://arxiv.org/abs/2509.03498v3</link><description>We introduce OneCAT, a unified multimodal model that seamlessly integratesunderstanding, generation, and editing within a novel, pure decoder-onlytransformer architecture. Our framework uniquely eliminates the need forexternal components such as Vision Transformers (ViT) or vision tokenizerduring inference, leading to significant efficiency gains, especially forhigh-resolution inputs. This is achieved through a modality-specificMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)objective, which also natively supports dynamic resolutions. Furthermore, wepioneer a multi-scale visual autoregressive mechanism within the Large LanguageModel (LLM) that drastically reduces decoding steps compared to diffusion-basedmethods while maintaining state-of-the-art performance. Our findingsdemonstrate the powerful potential of pure autoregressive modeling as asufficient and elegant foundation for unified multimodal intelligence. As aresult, OneCAT sets a new performance standard, outperforming existingopen-source unified multimodal models across benchmarks for multimodalgeneration, editing, and understanding.</description><author>Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong</author><pubDate>Tue, 07 Oct 2025 17:20:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.03498v3</guid></item><item><title>How Reliable are Causal Probing Interventions?</title><link>http://arxiv.org/abs/2408.15510v4</link><description>Causal probing aims to analyze foundation models by examining how interveningon their representation of various latent properties impacts their outputs.Recent works have cast doubt on the theoretical basis of several leading causalprobing methods, but it has been unclear how to systematically evaluate theeffectiveness of these methods in practice. To address this, we define two keycausal probing desiderata: completeness (how thoroughly the representation ofthe target property has been transformed) and selectivity (how littlenon-targeted properties have been impacted). We find that there is an inherenttradeoff between the two, which we define as reliability, their harmonic mean.We introduce an empirical analysis framework to measure and evaluate thesequantities, allowing us to make the first direct comparisons between differentfamilies of leading causal probing methods (e.g., linear vs. nonlinear, orconcept removal vs. counterfactual interventions). We find that: (1) allmethods show a clear tradeoff between completeness and selectivity; (2) morecomplete and reliable methods have a greater impact on LLM behavior; and (3)nonlinear interventions are almost always more reliable than linearinterventions.</description><author>Marc Canby, Adam Davies, Chirag Rastogi, Julia Hockenmaier</author><pubDate>Tue, 07 Oct 2025 17:20:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15510v4</guid></item><item><title>Gemstones: A Model Suite for Multi-Faceted Scaling Laws</title><link>http://arxiv.org/abs/2502.06857v3</link><description>Scaling laws are typically fit using a family of models with a narrow rangeof frozen hyperparameter choices. In this work we study scaling laws usingmultiple architectural shapes and hyperparameter choices, highlighting theirimpact on resulting prescriptions. As a primary artifact of our research, werelease the Gemstones: an open-source scaling law dataset, consisting of over4000 checkpoints from transformers with up to 2 billion parameters and diversearchitectural shapes; including ablations over learning rate and cooldown. Ourcheckpoints enable more complex studies of scaling, such as analyzing therelationship between width and depth. By examining our model suite, we findthat the prescriptions of scaling laws can be highly sensitive to theexperimental design process and the specific model checkpoints used duringfitting.</description><author>Sean McLeish, John Kirchenbauer, David Yu Miller, Siddharth Singh, Abhinav Bhatele, Micah Goldblum, Ashwinee Panda, Tom Goldstein</author><pubDate>Tue, 07 Oct 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06857v3</guid></item><item><title>Trajectory Prediction Meets Large Language Models: A Survey</title><link>http://arxiv.org/abs/2506.03408v2</link><description>Recent advances in large language models (LLMs) have sparked growing interestin integrating language-driven techniques into trajectory prediction. Byleveraging their semantic and reasoning capabilities, LLMs are reshaping howautonomous systems perceive, model, and predict trajectories. This surveyprovides a comprehensive overview of this emerging field, categorizing recentwork into five directions: (1) Trajectory prediction via language modelingparadigms, (2) Direct trajectory prediction with pretrained language models,(3) Language-guided scene understanding for trajectory prediction, (4)Language-driven data generation for trajectory prediction, (5) Language-basedreasoning and interpretability for trajectory prediction. For each, we analyzerepresentative methods, highlight core design choices, and identify openchallenges. This survey bridges natural language processing and trajectoryprediction, offering a unified perspective on how language can enrichtrajectory prediction.</description><author>Yi Xu, Ruining Yang, Yitian Zhang, Jianglin Lu, Mingyuan Zhang, Yizhou Wang, Lili Su, Yun Fu</author><pubDate>Tue, 07 Oct 2025 17:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.03408v2</guid></item><item><title>Implicit Updates for Average-Reward Temporal Difference Learning</title><link>http://arxiv.org/abs/2510.06149v1</link><description>Temporal difference (TD) learning is a cornerstone of reinforcement learning.In the average-reward setting, standard TD($\lambda$) is highly sensitive tothe choice of step-size and thus requires careful tuning to maintain numericalstability. We introduce average-reward implicit TD($\lambda$), which employs animplicit fixed point update to provide data-adaptive stabilization whilepreserving the per iteration computational complexity of standardaverage-reward TD($\lambda$). In contrast to prior finite-time analyses ofaverage-reward TD($\lambda$), which impose restrictive step-size conditions, weestablish finite-time error bounds for the implicit variant under substantiallyweaker step-size requirements. Empirically, average-reward implicitTD($\lambda$) operates reliably over a much broader range of step-sizes andexhibits markedly improved numerical stability. This enables more efficientpolicy evaluation and policy learning, highlighting its effectiveness as arobust alternative to average-reward TD($\lambda$).</description><author>Hwanwoo Kim, Dongkyu Derek Cho, Eric Laber</author><pubDate>Tue, 07 Oct 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06149v1</guid></item><item><title>Non-iid hypothesis testing: from classical to quantum</title><link>http://arxiv.org/abs/2510.06147v1</link><description>We study hypothesis testing (aka state certification) in the non-identicallydistributed setting. A recent work (Garg et al. 2023) considered the classicalcase, in which one is given (independent) samples from $T$ unknown probabilitydistributions $p_1, \dots, p_T$ on $[d] = \{1, 2, \dots, d\}$, and one wishesto accept/reject the hypothesis that their average $p_{\mathrm{avg}}$ equals aknown hypothesis distribution $q$. Garg et al. showed that if one has just $c =2$ samples from each $p_i$, and provided $T \gg \frac{\sqrt{d}}{\epsilon^2} +\frac{1}{\epsilon^4}$, one can (whp) distinguish $p_{\mathrm{avg}} = q$ from$d_{\mathrm{TV}}(p_{\mathrm{avg}},q) &gt; \epsilon$. This nearly matches theoptimal result for the classical iid setting (namely, $T \gg\frac{\sqrt{d}}{\epsilon^2}$). Besides optimally improving this result (andgeneralizing to tolerant testing with more stringent distance measures), westudy the analogous problem of hypothesis testing for non-identical quantumstates. Here we uncover an unexpected phenomenon: for any $d$-dimensionalhypothesis state $\sigma$, and given just a single copy ($c = 1$) of each state$\rho_1, \dots, \rho_T$, one can distinguish $\rho_{\mathrm{avg}} = \sigma$from $D_{\mathrm{tr}}(\rho_{\mathrm{avg}},\sigma) &gt; \epsilon$ provided $T \ggd/\epsilon^2$. (Again, we generalize to tolerant testing with more stringentdistance measures.) This matches the optimal result for the iid case, which issurprising because doing this with $c = 1$ is provably impossible in theclassical case. We also show that the analogous phenomenon happens for thenon-iid extension of identity testing between unknown states. A technical toolwe introduce may be of independent interest: an Efron-Stein inequality, andmore generally an Efron-Stein decomposition, in the quantum setting.</description><author>Giacomo De Palma, Marco Fanizza, Connor Mowry, Ryan O'Donnell</author><pubDate>Tue, 07 Oct 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06147v1</guid></item><item><title>Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images</title><link>http://arxiv.org/abs/2510.06145v1</link><description>We tackle the problem of forecasting bimanual 3D hand motion &amp; articulationfrom a single image in everyday settings. To address the lack of 3D handannotations in diverse settings, we design an annotation pipeline consisting ofa diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For theforecasting model, we adopt a diffusion loss to account for the multimodalityin hand motion distribution. Extensive experiments across 6 datasets show thebenefits of training on diverse data with imputed labels (14% improvement) andeffectiveness of our lifting (42% better) &amp; forecasting (16.4% gain) models,over the best baselines, especially in zero-shot generalization to everydayimages.</description><author>Aditya Prakash, David Forsyth, Saurabh Gupta</author><pubDate>Tue, 07 Oct 2025 17:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06145v1</guid></item><item><title>RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without Human Test Sets</title><link>http://arxiv.org/abs/2510.06143v1</link><description>LLMs are powerful generators of synthetic data, which are used for trainingsmaller, specific models. This is especially valuable for low-resourcelanguages, where human-labelled data is scarce but LLMs can still producehigh-quality text. However, LLMs differ in how useful their outputs are fortraining. Selecting the best LLM as a generator is challenging becauseextrinsic evaluation requires costly human annotations (which are oftenunavailable for low-resource languages), while intrinsic metrics correlatepoorly with downstream performance. We introduce Round robin Synthetic dataEvaluation (RoSE), a proxy metric for selecting the best LLM generator withouthuman test sets. RoSE trains a small model on the outputs of a candidategenerator (LLM) and then evaluates it on generated synthetic examples from allother candidate LLMs. The final RoSE score is the mean performance of thissmall model. Across six LLMs, eleven languages, and three tasks (sentiment,topic, intent), RoSE identifies the optimal generator more often than any otherintrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within0.76 percentage points of the optimal generator baseline. This result ismeasured in terms of downstream performance, obtained by training a small modelon the chosen generator's outputs (optimal vs. proxy metric selected) andevaluating it on human-labelled test data. Additionally, RoSE is the onlymetric to achieve a positive correlation with performance on human test data.</description><author>Jan Cegin, Branislav Pecher, Ivan Srba, Jakub Simko</author><pubDate>Tue, 07 Oct 2025 17:17:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06143v1</guid></item><item><title>Improved High-probability Convergence Guarantees of Decentralized SGD</title><link>http://arxiv.org/abs/2510.06141v1</link><description>Convergence in high-probability (HP) has been receiving increasing interest,due to its attractive properties, such as exponentially decaying tail boundsand strong guarantees for each individual run of an algorithm. While HPguarantees are extensively studied in centralized settings, much less isunderstood in the decentralized, networked setup. Existing HP studies indecentralized settings impose strong assumptions, like uniformly boundedgradients, or asymptotically vanishing noise, resulting in a significant gapbetween assumptions used to establish convergence in the HP and themean-squared error (MSE) sense, even for vanilla Decentralized StochasticGradient Descent ($\mathtt{DSGD}$) algorithm. This is contrary to centralizedsettings, where it is known that $\mathtt{SGD}$ converges in HP under the sameconditions on the cost function as needed to guarantee MSE convergence.Motivated by this observation, we revisit HP guarantees for $\mathtt{DSGD}$ inthe presence of light-tailed noise. We show that $\mathtt{DSGD}$ converges inHP under the same conditions on the cost as in the MSE sense, removinguniformly bounded gradients and other restrictive assumptions, whilesimultaneously achieving order-optimal rates for both non-convex and stronglyconvex costs. Moreover, our improved analysis yields linear speed-up in thenumber of users, demonstrating that $\mathtt{DSGD}$ maintains strongperformance in the HP sense and matches existing MSE guarantees. Our improvedresults stem from a careful analysis of the MGF of quantities of interest(norm-squared of gradient or optimality gap) and the MGF of the consensus gapbetween users' models. To achieve linear speed-up, we provide a novel result onthe variance-reduction effect of decentralized methods in the HP sense and morefine-grained bounds on the MGF for strongly convex costs, which are both ofindependent interest.</description><author>Aleksandar Armacki, Ali H. Sayed</author><pubDate>Tue, 07 Oct 2025 17:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06141v1</guid></item><item><title>Deforming Videos to Masks: Flow Matching for Referring Video Segmentation</title><link>http://arxiv.org/abs/2510.06139v1</link><description>Referring Video Object Segmentation (RVOS) requires segmenting specificobjects in a video guided by a natural language description. The core challengeof RVOS is to anchor abstract linguistic concepts onto a specific set of pixelsand continuously segment them through the complex dynamics of a video. Facedwith this difficulty, prior work has often decomposed the task into a pragmatic`locate-then-segment' pipeline. However, this cascaded design creates aninformation bottleneck by simplifying semantics into coarse geometric prompts(e.g, point), and struggles to maintain temporal consistency as the segmentingprocess is often decoupled from the initial language grounding. To overcomethese fundamental limitations, we propose FlowRVS, a novel framework thatreconceptualizes RVOS as a conditional continuous flow problem. This allows usto harness the inherent strengths of pretrained T2V models, fine-grained pixelcontrol, text-video semantic alignment, and temporal coherence. Instead ofconventional generating from noise to mask or directly predicting mask, wereformulate the task by learning a direct, language-guided deformation from avideo's holistic representation to its target mask. Our one-stage, generativeapproach achieves new state-of-the-art results across all major RVOSbenchmarks. Specifically, achieving a $\mathcal{J}\&amp;\mathcal{F}$ of 51.1 inMeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7),demonstrating the significant potential of modeling video understanding tasksas continuous deformation processes.</description><author>Zanyi Wang, Dengyang Jiang, Liuzhuozheng Li, Sizhe Dang, Chengzu Li, Harry Yang, Guang Dai, Mengmeng Wang, Jingdong Wang</author><pubDate>Tue, 07 Oct 2025 17:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06139v1</guid></item><item><title>Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks</title><link>http://arxiv.org/abs/2510.06138v1</link><description>Multi-task reinforcement learning often relies on task metadata -- such asbrief natural-language descriptions -- to guide behavior across diverseobjectives. We present Lexical Policy Networks (LEXPOL), a language-conditionedmixture-of-policies architecture for multi-task RL. LEXPOL encodes taskmetadata with a text encoder and uses a learned gating module to select orblend among multiple sub-policies, enabling end-to-end training across tasks.On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselinesin success rate and sample efficiency, without task-specific retraining. Toanalyze the mechanism, we further study settings with fixed expert policiesobtained independently of the gate and show that the learned language gatecomposes these experts to produce behaviors appropriate to novel taskdescriptions and unseen task combinations. These results indicate thatnatural-language metadata can effectively index and recombine reusable skillswithin a single policy.</description><author>Rushiv Arora</author><pubDate>Tue, 07 Oct 2025 17:12:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06138v1</guid></item><item><title>Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification</title><link>http://arxiv.org/abs/2510.06135v1</link><description>Test-time compute can be scaled both sequentially and in parallel. Sequentialscaling involves lengthening the generation process, while parallel scalinginvolves verifying and selecting among multiple candidate outputs. Combiningthese two strategies has led to the most powerful AI systems, such as Grok 4Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles),verifying responses can be substantially easier than generating them. Thisproperty, referred to as \emph{asymmetric verification}, highlights the strongpotential of test-time scaling (TTS). In this work, we study both sequentialand parallel TTS of deep search agents, motivated by the intuition thatverification in this setting is often much easier than generation. Inexperiments, we first show that sequential scaling methods, such as budgetforcing, can be effective initially but soon degrade performance. Leveragingasymmetric verification, however, we are able to achieve substantialimprovements by allocating only a modest amount of compute to the verifier. Weconduct experiments with flagship open-source models and extend them to their``Heavy'' variants through TTS. These deep research agents achieve gains of upto 27 absolute points on benchmarks such as BrowseComp. Remarkably, as anopen-source alternative, GLM-4.5 Heavy reaches accuracy of {\bf 54.0\%} onBrowseComp and {\bf 66.0\%} on GAIA, placing it comparable to the bestproprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavyfurther achieves {\bf 69.0\%} accuracy on BrowseComp, greatly surpassing thebest proprietary results.</description><author>Weihao Zeng, Keqing He, Chuqiao Kuang, Xiaoguang Li, Junxian He</author><pubDate>Tue, 07 Oct 2025 17:09:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06135v1</guid></item><item><title>CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits</title><link>http://arxiv.org/abs/2510.06133v1</link><description>Diffusion large language models (dLLMs) generate text through iterativedenoising steps, achieving parallel decoding by denoising only high-confidencepositions at each step. However, existing approaches often repetitively remasktokens due to initially low confidence scores, leading to redundant iterationsand limiting overall acceleration. Through the analysis of dLLM decodingtraces, we observe that the model often determines the final prediction for atoken several steps before the decoding step. To leverage this historicalinformation and avoid redundant steps, we introduce the concept of TraceCredit, which quantifies each token's convergence potential by accumulatinghistorical logits. Furthermore, we propose CreditDecoding, a training-freeparallel decoding algorithm that accelerates the confidence convergence ofcorrect but underconfident tokens by fusing current logits with Trace Credit.This process significantly reduces redundant iterations and enhances decodingrobustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedupand a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 timesspeedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.Importantly, CreditDecoding scales effectively to long sequences and isorthogonal to mainstream inference optimizations, making it a readilyintegrable and versatile solution.</description><author>Kangyu Wang, Zhiyun Jiang, Haibo Feng, Weijia Zhao, Lin Liu, Jianguo Li, Zhenzhong Lan, Weiyao Lin</author><pubDate>Tue, 07 Oct 2025 17:08:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06133v1</guid></item><item><title>Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort</title><link>http://arxiv.org/abs/2510.01367v3</link><description>Reward hacking, where a reasoning model exploits loopholes in a rewardfunction to achieve high rewards without solving the intended task, poses asignificant threat. This behavior may be explicit, i.e. verbalized in themodel's chain-of-thought (CoT), or implicit, where the CoT appears benign thusbypasses CoT monitors. To detect implicit reward hacking, we propose TRACE(Truncated Reasoning AUC Evaluation). Our key observation is that hackingoccurs when exploiting the loophole is easier than solving the actual task.This means that the model is using less 'effort' than required to achieve highreward. TRACE quantifies effort by measuring how early a model's reasoningbecomes sufficient to obtain the reward. We progressively truncate a model'sCoT at various lengths, force the model to answer, and estimate the expectedreward at each cutoff. A hacking model, which takes a shortcut, will achieve ahigh expected reward with only a small fraction of its CoT, yielding a largearea under the accuracy-vs-length curve. TRACE achieves over 65% gains over ourstrongest 72B CoT monitor in math reasoning, and over 30% gains over a 32Bmonitor in coding. We further show that TRACE can discover unknown loopholesduring training. Overall, TRACE offers a scalable unsupervised approach foroversight where current monitoring methods prove ineffective.</description><author>Xinpeng Wang, Nitish Joshi, Barbara Plank, Rico Angell, He He</author><pubDate>Tue, 07 Oct 2025 17:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.01367v3</guid></item><item><title>Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation</title><link>http://arxiv.org/abs/2510.06131v1</link><description>Recent advances in generative medical models are constrained bymodality-specific scenarios that hinder the integration of complementaryevidence from imaging, pathology, and clinical notes. This fragmentation limitstheir evolution into foundation models that can learn and reason across thefull spectrum of biomedical data. We propose MeDiM, the first medical discretediffusion model that learns shared distributions across modalities withoutmodality-specific components. MeDiM unifies multiple generative tasks:translating between images and text, and jointly producing image-report pairsacross domains in response to prompts. Built on a discrete diffusion framework,MeDiM bridges vision and language representations through a sharedprobabilistic space. To enable unified and flexible medical generation, weemploy a multimodal large language model (MLLM) as the diffusion backbone,leveraging its prior knowledge and cross-modal reasoning. Two key designs areintroduced: (1) removing the causal attention mask for bidirectional context,and (2) injecting continuous timestep embeddings for diffusion awareness.Experiments demonstrate high-fidelity medical generation (FID 16.60 onMIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR0.2650 and 0.2580). Jointly generated image-report pairs further enhancedownstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supportscoherent and clinically grounded multimodal outputs.</description><author>Jiawei Mao, Yuhan Wang, Lifeng Chen, Can Zhao, Yucheng Tang, Dong Yang, Liangqiong Qu, Daguang Xu, Yuyin Zhou</author><pubDate>Tue, 07 Oct 2025 17:06:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06131v1</guid></item><item><title>Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer</title><link>http://arxiv.org/abs/2510.06128v1</link><description>Tokenization defines the foundation of multilingual language models bydetermining how words are represented and shared across languages. However,existing methods often fail to support effective cross-lingual transfer becausesemantically equivalent words are assigned distinct embeddings. For example, "Ieat rice" in English and "Ina cin shinkafa" in Hausa are typically mapped todifferent vocabulary indices, preventing shared representations and limitingcross-lingual generalization. We introduce parallel tokenizers. This newframework trains tokenizers monolingually and then aligns their vocabulariesexhaustively using bilingual dictionaries or word-to-word translation, ensuringconsistent indices for semantically equivalent words. This alignment enforces ashared semantic space across languages while naturally improving fertilitybalance. To assess their effectiveness, we pretrain a transformer encoder fromscratch on thirteen low-resource languages and evaluate it on sentimentanalysis, hate speech detection, emotion classification, and sentence embeddingsimilarity. Across all tasks, models trained with parallel tokenizersoutperform conventional multilingual baselines, confirming that rethinkingtokenization is essential for advancing multilingual representationlearning--especially in low-resource settings.</description><author>Muhammad Dehan Al Kautsar, Fajri Koto</author><pubDate>Tue, 07 Oct 2025 17:05:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06128v1</guid></item><item><title>lm-Meter: Unveiling Runtime Inference Latency for On-Device Language Models</title><link>http://arxiv.org/abs/2510.06126v1</link><description>Large Language Models (LLMs) are increasingly integrated into everydayapplications, but their prevalent cloud-based deployment raises growingconcerns around data privacy and long-term sustainability. Running LLMs locallyon mobile and edge devices (on-device LLMs) offers the promise of enhancedprivacy, reliability, and reduced communication costs. However, realizing thisvision remains challenging due to substantial memory and compute demands, aswell as limited visibility into performance-efficiency trade-offs onresource-constrained hardware. We propose lm-Meter, the first lightweight,online latency profiler tailored for on-device LLM inference. lm-Meter capturesfine-grained, real-time latency at both phase (e.g., embedding, prefill,decode, softmax, sampling) and kernel levels without auxiliary devices. Weimplement lm-Meter on commercial mobile platforms and demonstrate its highprofiling accuracy with minimal system overhead, e.g., only 2.58% throughputreduction in prefill and 0.99% in decode under the most constrained Powersavegovernor. Leveraging lm-Meter, we conduct comprehensive empirical studiesrevealing phase- and kernel-level bottlenecks in on-device LLM inference,quantifying accuracy-efficiency trade-offs, and identifying systematicoptimization opportunities. lm-Meter provides unprecedented visibility into theruntime behavior of LLMs on constrained platforms, laying the foundation forinformed optimization and accelerating the democratization of on-device LLMsystems. Code and tutorials are available athttps://github.com/amai-gsu/LM-Meter.</description><author>Haoxin Wang, Xiaolong Tu, Hongyu Ke, Huirong Chai, Dawei Chen, Kyungtae Han</author><pubDate>Tue, 07 Oct 2025 17:05:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06126v1</guid></item><item><title>Downsized and Compromised?: Assessing the Faithfulness of Model Compression</title><link>http://arxiv.org/abs/2510.06125v1</link><description>In real-world applications, computational constraints often requiretransforming large models into smaller, more efficient versions through modelcompression. While these techniques aim to reduce size and computational costwithout sacrificing performance, their evaluations have traditionally focusedon the trade-off between size and accuracy, overlooking the aspect of modelfaithfulness. This limited view is insufficient for high-stakes domains likehealthcare, finance, and criminal justice, where compressed models must remainfaithful to the behavior of their original counterparts. This paper presents anovel approach to evaluating faithfulness in compressed models, moving beyondstandard metrics. We introduce and demonstrate a set of faithfulness metricsthat capture how model behavior changes post-compression. Our contributionsinclude introducing techniques to assess predictive consistency between theoriginal and compressed models using model agreement, and applying chi-squaredtests to detect statistically significant changes in predictive patterns acrossboth the overall dataset and demographic subgroups, thereby exposing shiftsthat aggregate fairness metrics may obscure. We demonstrate our approaches byapplying quantization and pruning to artificial neural networks (ANNs) trainedon three diverse and socially meaningful datasets. Our findings show that highaccuracy does not guarantee faithfulness, and our statistical tests detectsubtle yet significant shifts that are missed by standard metrics, such asAccuracy and Equalized Odds. The proposed metrics provide a practical and moredirect method for ensuring that efficiency gains through compression do notcompromise the fairness or faithfulness essential for trustworthy AI.</description><author>Moumita Kamal, Douglas A. Talbert</author><pubDate>Tue, 07 Oct 2025 17:05:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06125v1</guid></item><item><title>Taxonomy of User Needs and Actions</title><link>http://arxiv.org/abs/2510.06124v1</link><description>The growing ubiquity of conversational AI highlights the need for frameworksthat capture not only users' instrumental goals but also the situated,adaptive, and social practices through which they achieve them. Existingtaxonomies of conversational behavior either overgeneralize, remaindomain-specific, or reduce interactions to narrow dialogue functions. Toaddress this gap, we introduce the Taxonomy of User Needs and Actions (TUNA),an empirically grounded framework developed through iterative qualitativeanalysis of 1193 human-AI conversations, supplemented by theoretical review andvalidation across diverse contexts. TUNA organizes user actions into athree-level hierarchy encompassing behaviors associated with informationseeking, synthesis, procedural guidance, content creation, social interaction,and meta-conversation. By centering user agency and appropriation practices,TUNA enables multi-scale evaluation, supports policy harmonization acrossproducts, and provides a backbone for layering domain-specific taxonomies. Thiswork contributes a systematic vocabulary for describing AI use, advancing bothscholarly understanding and practical design of safer, more responsive, andmore accountable conversational systems.</description><author>Renee Shelby, Fernando Diaz, Vinodkumar Prabhakaran</author><pubDate>Tue, 07 Oct 2025 17:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06124v1</guid></item><item><title>Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework</title><link>http://arxiv.org/abs/2510.06123v1</link><description>Deep learning in medical imaging is often limited by scarce and imbalancedannotated data. We present SSGNet, a unified framework that combines classspecific generative modeling with iterative semisupervised pseudo labeling toenhance both classification and segmentation. Rather than functioning as astandalone model, SSGNet augments existing baselines by expanding training datawith StyleGAN3 generated images and refining labels through iterative pseudolabeling. Experiments across multiple medical imaging benchmarks demonstrateconsistent gains in classification and segmentation performance, while FrechetInception Distance analysis confirms the high quality of generated samples.These results highlight SSGNet as a practical strategy to mitigate annotationbottlenecks and improve robustness in medical image analysis.</description><author>Mosong Ma, Tania Stathaki, Michalis Lazarou</author><pubDate>Tue, 07 Oct 2025 17:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06123v1</guid></item><item><title>PolyGraph Discrepancy: a classifier-based metric for graph generation</title><link>http://arxiv.org/abs/2510.06122v1</link><description>Existing methods for evaluating graph generative models primarily rely onMaximum Mean Discrepancy (MMD) metrics based on graph descriptors. While thesemetrics can rank generative models, they do not provide an absolute measure ofperformance. Their values are also highly sensitive to extrinsic parameters,namely kernel and descriptor parametrization, making them incomparable acrossdifferent graph descriptors. We introduce PolyGraph Discrepancy (PGD), a newevaluation framework that addresses these limitations. It approximates theJensen-Shannon distance of graph distributions by fitting binary classifiers todistinguish between real and generated graphs, featurized by these descriptors.The data log-likelihood of these classifiers approximates a variational lowerbound on the JS distance between the two distributions. Resulting metrics areconstrained to the unit interval [0,1] and are comparable across differentgraph descriptors. We further derive a theoretically grounded summary metricthat combines these individual metrics to provide a maximally tight lower boundon the distance for the given descriptors. Thorough experiments demonstratethat PGD provides a more robust and insightful evaluation compared to MMDmetrics. The PolyGraph framework for benchmarking graph generative models ismade publicly available at https://github.com/BorgwardtLab/polygraph-benchmark.</description><author>Markus Krimmel, Philip Hartout, Karsten Borgwardt, Dexiong Chen</author><pubDate>Tue, 07 Oct 2025 17:02:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06122v1</guid></item><item><title>Optimal Policy Minimum Bayesian Risk</title><link>http://arxiv.org/abs/2505.17242v2</link><description>Inference scaling helps LLMs solve complex reasoning problems throughextended runtime computation. On top of long chain-of-thought (long-CoT)models, purely inference-time techniques such as best-of-N (BoN) sampling,majority voting, or more generally, minimum Bayes risk decoding (MBRD), canfurther improve LLM accuracy by generating multiple candidate solutions andaggregating over them. These methods typically leverage additional signals inthe form of reward models and risk/similarity functions that compare generatedsamples, e.g., exact match in some normalized space or standard similaritymetrics such as Rouge. Here we present a novel method for incorporating rewardand risk/similarity signals into MBRD. Based on the concept of optimal policyin KL-controlled reinforcement learning, our framework provides a simple andwell-defined mechanism for leveraging such signals, offering several advantagesover traditional inference-time methods: higher robustness, improved accuracy,and well-understood asymptotic behavior. In addition, it allows for thedevelopment of a sample-efficient variant of MBRD that can adjust the number ofsamples to generate according to the difficulty of the problem, without relyingon majority vote counts. We empirically demonstrate the advantages of ourapproach on math (MATH-$500$) and coding (HumanEval) tasks using recentopen-source models. We also present a comprehensive analysis of itsaccuracy-compute trade-offs.</description><author>Ramón Fernandez Astudillo, Md Arafat Sultan, Aashka Trivedi, Yousef El-Kurdi, Tahira Naseem, Radu Florian, Salim Roukos</author><pubDate>Tue, 07 Oct 2025 16:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.17242v2</guid></item><item><title>Spatiotemporal Graph Learning with Direct Volumetric Information Passing and Feature Enhancement</title><link>http://arxiv.org/abs/2409.18013v2</link><description>Data-driven learning of physical systems has kindled significant attention,where many neural models have been developed. In particular, mesh-based graphneural networks (GNNs) have demonstrated significant potential in modelingspatiotemporal dynamics across arbitrary geometric domains. However, theexisting node-edge message-passing and aggregation mechanism in GNNs limits therepresentation learning ability. In this paper, we proposed a dual-moduleframework, Cell-embedded and Feature-enhanced Graph Neural Network (aka,CeFeGNN), for learning spatiotemporal dynamics. Specifically, we embedlearnable cell attributions to the common node-edge message passing process,which better captures the spatial dependency of regional features. Such astrategy essentially upgrades the local aggregation scheme from first order(e.g., from edge to node) to a higher order (e.g., from volume and edge tonode), which takes advantage of volumetric information in message passing.Meanwhile, a novel feature-enhanced block is designed to further improve themodel's performance and alleviate the over-smoothness problem. Extensiveexperiments on various PDE systems and one real-world dataset demonstrate thatCeFeGNN achieves superior performance compared with other baselines.</description><author>Yuan Mi, Qi Wang, Xueqin Hu, Yike Guo, Ji-Rong Wen, Yang Liu, Hao Sun</author><pubDate>Tue, 07 Oct 2025 16:51:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18013v2</guid></item><item><title>Multimodal Feature Prototype Learning for Interpretable and Discriminative Cancer Survival Prediction</title><link>http://arxiv.org/abs/2510.06113v1</link><description>Survival analysis plays a vital role in making clinical decisions. However,the models currently in use are often difficult to interpret, which reducestheir usefulness in clinical settings. Prototype learning presents a potentialsolution, yet traditional methods focus on local similarities and staticmatching, neglecting the broader tumor context and lacking strong semanticalignment with genomic data. To overcome these issues, we introduce aninnovative prototype-based multimodal framework, FeatProto, aimed at enhancingcancer survival prediction by addressing significant limitations in currentprototype learning methodologies within pathology. Our framework establishes aunified feature prototype space that integrates both global and local featuresof whole slide images (WSI) with genomic profiles. This integration facilitatestraceable and interpretable decision-making processes. Our approach includesthree main innovations: (1) A robust phenotype representation that mergescritical patches with global context, harmonized with genomic data to minimizelocal bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) thatsustains stable cross-modal associations and employs a wandering mechanism toadapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototypematching scheme designed to capture global centrality, local typicality, andcohort-level trends, thereby refining prototype inference. Comprehensiveevaluations on four publicly available cancer datasets indicate that our methodsurpasses current leading unimodal and multimodal survival predictiontechniques in both accuracy and interoperability, providing a new perspectiveon prototype learning for critical medical applications. Our source code isavailable at https://github.com/JSLiam94/FeatProto.</description><author>Shuo Jiang, Zhuwen Chen, Liaoman Xu, Yanming Zhu, Changmiao Wang, Jiong Zhang, Feiwei Qin, Yifei Chen, Zhu Zhu</author><pubDate>Tue, 07 Oct 2025 16:49:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06113v1</guid></item><item><title>Influence Functions for Efficient Data Selection in Reasoning</title><link>http://arxiv.org/abs/2510.06108v1</link><description>Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data showsthat a small amount of high-quality data can outperform massive datasets. Yet,what constitutes "quality" remains ill-defined. Existing reasoning methods relyon indirect heuristics such as problem difficulty or trace length, whileinstruction-tuning has explored a broader range of automated selectionstrategies, but rarely in the context of reasoning. We propose to definereasoning data quality using influence functions, which measure the causaleffect of individual CoT examples on downstream accuracy, and introduceinfluence-based pruning, which consistently outperforms perplexity andembedding-based baselines on math reasoning within a model family.</description><author>Prateek Humane, Paolo Cudrano, Daniel Z. Kaplan, Matteo Matteucci, Supriyo Chakraborty, Irina Rish</author><pubDate>Tue, 07 Oct 2025 16:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06108v1</guid></item><item><title>Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models</title><link>http://arxiv.org/abs/2510.06107v1</link><description>Large Language Models (LLMs) are prone to hallucination, the generation ofplausible yet factually incorrect statements. This work investigates theintrinsic, architectural origins of this failure mode through three primarycontributions.First, to enable the reliable tracing of internal semanticfailures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unifiedframework that integrates established interpretability techniques to produce acausal map of a model's reasoning, treating meaning as a function of context(distributional semantics). Second, we pinpoint the model's layer at which ahallucination becomes inevitable, identifying a specific \textbf{commitmentlayer} where a model's internal representations irreversibly diverge fromfactuality. Third, we identify the underlying mechanism for these failures. Weobserve a conflict between distinct computational pathways, which we interpretusing the lens of dual-process theory: a fast, heuristic \textbf{associativepathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway}(akin to System 2), leading to predictable failure modes such as\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify thecoherence of the contextual pathway reveals a strong negative correlation($\rho = -0.863$) with hallucination rates, implying that these failures arepredictable consequences of internal semantic weakness. The result is amechanistic account of how, when, and why hallucinations occur within theTransformer architecture.</description><author>Gagan Bhatia, Somayajulu G Sripada, Kevin Allan, Jacobo Azcona</author><pubDate>Tue, 07 Oct 2025 16:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06107v1</guid></item><item><title>The Physics of Data and Tasks: Theories of Locality and Compositionality in Deep Learning</title><link>http://arxiv.org/abs/2510.06106v1</link><description>Deep neural networks have achieved remarkable success, yet our understandingof how they learn remains limited. These models can learn high-dimensionaltasks, which is generally statistically intractable due to the curse ofdimensionality. This apparent paradox suggests that learnable data must have anunderlying latent structure. What is the nature of this structure? How doneural networks encode and exploit it, and how does it quantitatively impactperformance - for instance, how does generalization improve with the number oftraining examples? This thesis addresses these questions by studying the rolesof locality and compositionality in data, tasks, and deep learningrepresentations.</description><author>Alessandro Favero</author><pubDate>Tue, 07 Oct 2025 16:40:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06106v1</guid></item><item><title>Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes</title><link>http://arxiv.org/abs/2509.21456v2</link><description>Moral alignment has emerged as a widely adopted approach for regulating thebehavior of pretrained language models (PLMs), typically through fine-tuning oncurated datasets. Gender stereotype mitigation is a representational taskwithin the broader application of moral alignment. However, this process oftencomes at the cost of degraded downstream task performance. Prior studiescommonly aim to achieve a performance trade-off by encouraging PLMs toselectively forget only stereotypical knowledge through carefully designedfairness objective, while preserving their language modeling capability(overall forgetting). In this short paper, we investigate whether theperformance trade-off can be achieved through the lens of forgetting and thefairness objective. Our analysis shows that the large datasets needed forsatisfactory fairness highlight the limitations of current fairness objectivesin achieving an effective trade-off: (1) downstream task performance isstrongly correlated with overall forgetting; (2) selective forgetting reducesstereotypes, but overall forgetting increases. and (3) general solutions foralleviating forgetting are ineffective at reducing the overall forgetting andfail to improve downstream task performance.</description><author>Guangliang Liu, Bocheng Chen, Xitong Zhang, Kristen Marie Johnson</author><pubDate>Tue, 07 Oct 2025 16:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21456v2</guid></item><item><title>Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences</title><link>http://arxiv.org/abs/2510.06105v1</link><description>Large language models (LLMs) are increasingly shaping how information iscreated and disseminated, from companies using them to craft persuasiveadvertisements, to election campaigns optimizing messaging to gain votes, tosocial media influencers boosting engagement. These settings are inherentlycompetitive, with sellers, candidates, and influencers vying for audienceapproval, yet it remains poorly understood how competitive feedback loopsinfluence LLM behavior. We show that optimizing LLMs for competitive successcan inadvertently drive misalignment. Using simulated environments across thesescenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% risein deceptive marketing; in elections, a 4.9% gain in vote share coincides with22.3% more disinformation and 12.5% more populist rhetoric; and on socialmedia, a 7.5% engagement boost comes with 188.6% more disinformation and a16.3% increase in promotion of harmful behaviors. We call this phenomenonMoloch's Bargain for AI--competitive success achieved at the cost of alignment.These misaligned behaviors emerge even when models are explicitly instructed toremain truthful and grounded, revealing the fragility of current alignmentsafeguards. Our findings highlight how market-driven optimization pressures cansystematically erode alignment, creating a race to the bottom, and suggest thatsafe deployment of AI systems will require stronger governance and carefullydesigned incentives to prevent competitive dynamics from undermining societaltrust.</description><author>Batu El, James Zou</author><pubDate>Tue, 07 Oct 2025 16:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06105v1</guid></item><item><title>The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models</title><link>http://arxiv.org/abs/2510.06101v1</link><description>Distilling the thinking traces of a Large Language Model (LLM) with reasoningcapabilities into a smaller model has been proven effective. Yet, there is ascarcity of work done on how model performances scale with the quantity ofdistillation data. In this work, we study the scaling trend of distillingcompetitive coding skills on two small non-reasoning LLMs. We validate thehypothesis that there is a $\textit{valley of code reasoning}$: downstreamperformance on competitive coding first drops as data quantity increases, thenit steadily increases in a sharper-than-log-linear fashion. Having identifiedthe trend, we further fine-tune the models at two different distillation stageson the same data to ground conclusions on their respective learning phases. Welearn that across stages in the low and medium-low data regimes, small modelsbenefit significantly from easier coding questions than from harder ones. Wealso find that, surprisingly, the correctness of outputs in training data makesno difference to distillation outcomes. Our work represents a step forward inunderstanding the training dynamics of code reasoning distillation outsideintuition</description><author>Muyu He, Muhammad Ali Shafique, Anand Kumar, Tsach Mackey, Nazneen Rajani</author><pubDate>Tue, 07 Oct 2025 16:32:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06101v1</guid></item><item><title>Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models</title><link>http://arxiv.org/abs/2507.12428v2</link><description>Reasoning language models improve performance on complex tasks by generatinglong chains of thought (CoTs), but this process can also increase harmfuloutputs in adversarial settings. In this work, we ask whether the long CoTs canbe leveraged for predictive safety monitoring: do the reasoning traces provideearly signals of final response alignment that could enable timelyintervention? We evaluate a range of monitoring methods using either CoT textor activations, including highly capable large language models, fine-tunedclassifiers, and humans. First, we find that a simple linear probe trained onCoT activations significantly outperforms all text-based baselines inpredicting whether a final response is safe or unsafe, with an average absoluteincrease of 13 in F1 scores over the best-performing alternatives. CoT textsare often unfaithful and misleading, while model latents provide a morereliable predictive signal. Second, the probe can be applied to early CoTsegments before the response is generated, showing that alignment signalsappear before reasoning completes. Error analysis reveals that the performancegap between text classifiers and the linear probe largely stems from a subsetof responses we call performative CoTs, where the reasoning consistentlycontradicts the final response as the CoT progresses. Our findings generalizeacross model sizes, families, and safety benchmarks, suggesting thatlightweight probes could enable real-time safety monitoring and earlyintervention during generation.</description><author>Yik Siu Chan, Zheng-Xin Yong, Stephen H. Bach</author><pubDate>Tue, 07 Oct 2025 16:30:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.12428v2</guid></item><item><title>Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution</title><link>http://arxiv.org/abs/2510.06098v1</link><description>Fusing a hyperspectral image with a multispectral image acquired over thesame scene, \textit{i.e.}, hyperspectral image super-resolution, has become apopular computational way to access the latent high-spatial-spectral-resolutionimage. To date, a variety of fusion methods have been proposed, among which thetensor-based ones have testified that multiple priors, such as multidimensionallow-rankness and spatial total variation at multiple levels, effectively drivethe fusion process. However, existing tensor-based models can only effectivelyleverage one or two priors at one or two levels, since simultaneouslyincorporating multi-level priors inevitably increases model complexity. Thisintroduces challenges in both balancing the weights of different priors andoptimizing multi-block structures. Concerning this, we present a novelhyperspectral super-resolution model compactly characterizing these multi-levelpriors of hyperspectral images within the tensor framework. Firstly, theproposed model decouples the spectral low-rankness and spatial priors bycasting the latent high-spatial-spectral-resolution image into spectralsubspace and spatial maps via block term decomposition. Secondly, these spatialmaps are stacked as the spatial tensor encoding the high-order spatiallow-rankness and smoothness priors, which are co-modeled via the proposednon-convex mode-shuffled tensor correlated total variation. Finally, we drawinspiration from the linearized alternating direction method of multipliers todesign an efficient algorithm to optimize the resulting model, theoreticallyproving its Karush-Kuhn-Tucker convergence under mild conditions. Experimentson multiple datasets demonstrate the effectiveness of the proposed algorithm.The code implementation will be available from https://github.com/WongYinJ.</description><author>Yinjian Wang, Wei Li, Yuanyuan Gui, Gemine Vivone</author><pubDate>Tue, 07 Oct 2025 16:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06098v1</guid></item><item><title>Robust-Multi-Task Gradient Boosting</title><link>http://arxiv.org/abs/2507.11411v2</link><description>Multi-task learning (MTL) has shown effectiveness in exploiting sharedinformation across tasks to improve generalization. MTL assumes tasks sharesimilarities that can improve performance. In addition, boosting algorithmshave demonstrated exceptional performance across diverse learning problems,primarily due to their ability to focus on hard-to-learn instances anditeratively reduce residual errors. This makes them a promising approach forlearning multi-task problems. However, real-world MTL scenarios often involvetasks that are not well-aligned (known as outlier or adversarial tasks), whichdo not share beneficial similarities with others and can, in fact, deterioratethe performance of the overall model. To overcome this challenge, we proposeRobust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework thatexplicitly models and adapts to task heterogeneity during training. R-MTGBstructures the learning process into three sequential blocks: (1) learningshared patterns, (2) partitioning tasks into outliers and non-outliers withregularized parameters, and (3) fine-tuning task-specific predictors. Thisarchitecture enables R-MTGB to automatically detect and penalize outlier taskswhile promoting effective knowledge transfer among related tasks. Our methodintegrates these mechanisms seamlessly within gradient boosting, allowingrobust handling of noisy or adversarial tasks without sacrificing accuracy.Extensive experiments on both synthetic benchmarks and real-world datasetsdemonstrate that our approach successfully isolates outliers, transfersknowledge, and consistently reduces prediction errors for each taskindividually, and achieves overall performance gains across all tasks. Theseresults highlight robustness, adaptability, and reliable convergence of R-MTGBin challenging MTL environments.</description><author>Seyedsaman Emami, Gonzalo Martínez-Muñoz, Daniel Hernández-Lobato</author><pubDate>Tue, 07 Oct 2025 16:25:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.11411v2</guid></item><item><title>The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives</title><link>http://arxiv.org/abs/2510.06096v1</link><description>The objectives that Large Language Models (LLMs) implicitly optimize remaindangerously opaque, making trustworthy alignment and auditing a grandchallenge. While Inverse Reinforcement Learning (IRL) can infer rewardfunctions from behaviour, existing approaches either produce a single,overconfident reward estimate or fail to address the fundamental ambiguity ofthe task (non-identifiability). This paper introduces a principled auditingframework that re-frames reward inference from a simple estimation task to acomprehensive process for verification. Our framework leverages Bayesian IRL tonot only recover a distribution over objectives but to enable three criticalaudit capabilities: (i) Quantifying and systematically reducingnon-identifiability by demonstrating posterior contraction over sequentialrounds of evidence; (ii) Providing actionable, uncertainty-aware diagnosticsthat expose spurious shortcuts and identify out-of-distribution prompts wherethe inferred objective cannot be trusted; and (iii) Validating policy-levelutility by showing that the refined, low-uncertainty reward can be useddirectly in RLHF to achieve training dynamics and toxicity reductionscomparable to the ground-truth alignment process. Empirically, our frameworksuccessfully audits a detoxified LLM, yielding a well-calibrated andinterpretable objective that strengthens alignment guarantees. Overall, thiswork provides a practical toolkit for auditors, safety teams, and regulators toverify what LLMs are truly trying to achieve, moving us toward more trustworthyand accountable AI.</description><author>Matthieu Bou, Nyal Patel, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo</author><pubDate>Tue, 07 Oct 2025 16:25:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06096v1</guid></item><item><title>Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices</title><link>http://arxiv.org/abs/2510.06093v1</link><description>As algorithmic decision-makers are increasingly applied to high-stakesdomains, AI alignment research has evolved from a focus on universal valuealignment to context-specific approaches that account for decision-makerattributes. Prior work on Decision-Maker Alignment (DMA) has explored twoprimary strategies: (1) classical AI methods integrating case-based reasoning,Bayesian reasoning, and naturalistic decision-making, and (2) large languagemodel (LLM)-based methods leveraging prompt engineering. While both approacheshave shown promise in limited domains such as medical triage, theirgeneralizability to novel contexts remains underexplored. In this work, weimplement a prior classical AI model and develop an LLM-based algorithmicdecision-maker evaluated using a large reasoning model (GPT-5) and anon-reasoning model (GPT-4) with weighted self-consistency under a zero-shotprompting framework, as proposed in recent literature. We evaluate bothapproaches on a health insurance decision-making dataset annotated for threetarget decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).In the experiments reported herein, classical AI and LLM-based models achievedcomparable alignment with attribute-based targets, with classical AI exhibitingslightly better alignment for a moderate risk profile. The dataset andopen-source implementation are publicly available at:https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment andhttps://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.</description><author>Mallika Mainali, Harsha Sureshbabu, Anik Sen, Christopher B. Rauch, Noah D. Reifsnyder, John Meyer, J. T. Turner, Michael W. Floyd, Matthew Molineaux, Rosina O. Weber</author><pubDate>Tue, 07 Oct 2025 16:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06093v1</guid></item><item><title>Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL</title><link>http://arxiv.org/abs/2510.06092v1</link><description>Reinforcement Learning from Human Feedback (RLHF) aligns Large LanguageModels (LLMs) with human preferences, yet the underlying reward signals theyinternalize remain hidden, posing a critical challenge for interpretability andsafety. Existing approaches attempt to extract these latent incentives usingInverse Reinforcement Learning (IRL), but treat all preference pairs equally,often overlooking the most informative signals: those examples the extractedreward model misclassifies or assigns nearly equal scores, which we term\emph{failures}. We introduce a novel \emph{failure-aware} IRL algorithm thatfocuses on misclassified or difficult examples to recover the latent rewardsdefining model behaviors. By learning from these failures, our failure-awareIRL extracts reward functions that better reflect the true objectives behindRLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselinesacross multiple metrics when applied to LLM detoxification, without requiringexternal classifiers or supervision. Crucially, failure-aware IRL yieldsrewards that better capture the true incentives learned during RLHF, enablingmore effective re-RLHF training than standard IRL. This establishesfailure-aware IRL as a robust, scalable method for auditing model alignment andreducing ambiguity in the IRL process.</description><author>Nyal Patel, Matthieu Bou, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo</author><pubDate>Tue, 07 Oct 2025 16:20:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06092v1</guid></item><item><title>HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss Guided Depth and Bidirectional Warping</title><link>http://arxiv.org/abs/2509.24893v2</link><description>Novel View Synthesis (NVS) from sparse views presents a formidable challengein 3D reconstruction, where limited multi-view constraints lead to severeoverfitting, geometric distortion, and fragmented scenes. While 3D GaussianSplatting (3DGS) delivers real-time, high-fidelity rendering, its performancedrastically deteriorates under sparse inputs, plagued by floating artifacts andstructural failures. To address these challenges, we introduce HBSplat, aunified framework that elevates 3DGS by seamlessly integrating robuststructural cues, virtual view constraints, and occluded region completion. Ourcore contributions are threefold: a Hybrid-Loss Depth Estimation module thatensures multi-view consistency by leveraging dense matching priors andintegrating reprojection, point propagation, and smoothness constraints; aBidirectional Warping Virtual View Synthesis method that enforces substantiallystronger constraints by creating high-fidelity virtual views throughbidirectional depth-image warping and multi-view fusion; and an Occlusion-AwareReconstruction component that recovers occluded areas using a depth-differencemask and a learning-based inpainting model. Extensive evaluations on LLFF,Blender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art,achieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-timeinference. Code is available at: https://github.com/eternalland/HBSplat.</description><author>Yu Ma, Guoliang Wei, Yue Cheng</author><pubDate>Tue, 07 Oct 2025 16:18:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.24893v2</guid></item><item><title>Learning Mixtures of Linear Dynamical Systems (MoLDS) via Hybrid Tensor-EM Method</title><link>http://arxiv.org/abs/2510.06091v1</link><description>Mixtures of linear dynamical systems (MoLDS) provide a path to modeltime-series data that exhibit diverse temporal dynamics across trajectories.However, its application remains challenging in complex and noisy settings,limiting its effectiveness for neural data analysis. Tensor-based momentmethods can provide global identifiability guarantees for MoLDS, but theirperformance degrades under noise and complexity. Commonly usedexpectation-maximization (EM) methods offer flexibility in fitting latentmodels but are highly sensitive to initialization and prone to poor localminima. Here, we propose a tensor-based method that provides identifiabilityguarantees for learning MoLDS, which is followed by EM updates to combine thestrengths of both approaches. The novelty in our approach lies in theconstruction of moment tensors using the input-output data to recover globallyconsistent estimates of mixture weights and system parameters. These estimatescan then be refined through a Kalman EM algorithm, with closed-form updates forall LDS parameters. We validate our framework on synthetic benchmarks andreal-world datasets. On synthetic data, the proposed Tensor-EM method achievesmore reliable recovery and improved robustness compared to either pure tensoror randomly initialized EM methods. We then analyze neural recordings from theprimate somatosensory cortex while a non-human primate performs reaches indifferent directions. Our method successfully models and clusters differentconditions as separate subsystems, consistent with supervised single-LDS fitsfor each condition. Finally, we apply this approach to another neural datasetwhere monkeys perform a sequential reaching task. These results demonstratethat MoLDS provides an effective framework for modeling complex neural data,and that Tensor-EM is a reliable approach to MoLDS learning for theseapplications.</description><author>Lulu Gong, Shreya Saxena</author><pubDate>Tue, 07 Oct 2025 16:17:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06091v1</guid></item><item><title>A public cardiac CT dataset featuring the left atrial appendage</title><link>http://arxiv.org/abs/2510.06090v1</link><description>Despite the success of advanced segmentation frameworks such asTotalSegmentator (TS), accurate segmentations of the left atrial appendage(LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significantchallenge in medical imaging. In this work, we present the first open-source,anatomically coherent dataset of curated, high-resolution segmentations forthese structures, supplemented with whole-heart labels produced by TS on thepublicly available ImageCAS dataset consisting of 1000 cardiac computedtomography angiography (CCTA) scans. One purpose of the data set is to fosternovel approaches to the analysis of LAA morphology. LAA segmentations on ImageCAS were generated using a state-of-the-artsegmentation framework developed specifically for high resolution LAAsegmentation. We trained the network on a large private dataset with manualannotations provided by medical readers guided by a trained cardiologist andtransferred the model to ImageCAS data. CA labels were improved from theoriginal ImageCAS annotations, while PV segmentations were refined from TSoutputs. In addition, we provide a list of scans from ImageCAS that containscommon data flaws such as step artefacts, LAAs extending beyond the scanner'sfield of view, and other types of data defects.</description><author>Bjoern Hansen, Jonas Pedersen, Klaus F. Kofoed, Oscar Camara, Rasmus R. Paulsen, Kristine Soerensen</author><pubDate>Tue, 07 Oct 2025 16:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06090v1</guid></item><item><title>Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability</title><link>http://arxiv.org/abs/2510.06084v1</link><description>Language model post-training has enhanced instruction-following andperformance on many downstream tasks, but also comes with an often-overlookedcost on tasks with many possible valid answers. We characterize threedesiderata for conditional distributional modeling: in-context steerability,valid output space coverage, and distributional alignment, and document acrossthree model families how current post-training can reduce these properties. Inparticular, we disambiguate between two kinds of in-context learning: ICL foreliciting existing underlying knowledge or capabilities, and in-contextsteerability, where a model must use in-context information to override itspriors and steer to a novel data generating distribution. To better evaluateand improve these desiderata, we introduce Spectrum Suite, a large-scaleresource compiled from &gt;40 data sources and spanning &gt;90 tasks requiring modelsto steer to and match diverse distributions ranging from varied humanpreferences to numerical distributions and more. We find that while currentpost-training techniques help elicit underlying capabilities and knowledge,they hurt models' ability to flexibly steer in-context. To mitigate theseissues, we propose Spectrum Tuning, a post-training method using Spectrum Suiteto improve steerability and distributional coverage. We find that SpectrumTuning often improves over pretrained models and their instruction-tunedcounterparts, enhancing steerability, spanning more of the output space, andimproving distributional alignment on held-out datasets.</description><author>Taylor Sorensen, Benjamin Newman, Jared Moore, Chan Park, Jillian Fisher, Niloofar Mireshghallah, Liwei Jiang, Yejin Choi</author><pubDate>Tue, 07 Oct 2025 16:10:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06084v1</guid></item><item><title>Epistemic Diversity and Knowledge Collapse in Large Language Models</title><link>http://arxiv.org/abs/2510.04226v2</link><description>Large language models (LLMs) tend to generate lexically, semantically, andstylistically homogenous texts. This poses a risk of knowledge collapse, wherehomogenous LLMs mediate a shrinking in the range of accessible information overtime. Existing works on homogenization are limited by a focus on closed-endedmultiple-choice setups or fuzzy semantic features, and do not look at trendsacross time and cultural contexts. To overcome this, we present a newmethodology to measure epistemic diversity, i.e., variation in real-worldclaims in LLM outputs, which we use to perform a broad empirical study of LLMknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200prompt variations sourced from real user chats. For the topics in our study, weshow that while newer models tend to generate more diverse claims, nearly allmodels are less epistemically diverse than a basic web search. We find thatmodel size has a negative impact on epistemic diversity, whileretrieval-augmented generation (RAG) has a positive impact, though theimprovement from RAG varies by the cultural context. Finally, compared to atraditional knowledge source (Wikipedia), we find that country-specific claimsreflect the English language more than the local one, highlighting a gap inepistemic representation</description><author>Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Chan Young Park, Isabelle Augenstein</author><pubDate>Tue, 07 Oct 2025 16:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.04226v2</guid></item><item><title>Entropy-Gated Branching for Efficient Test-Time Reasoning</title><link>http://arxiv.org/abs/2503.21961v3</link><description>Test-time compute methods can significantly improve the reasoningcapabilities and problem-solving accuracy of large language models (LLMs).However, these approaches require substantially more computational resources,with most compute wasted on exploring low-diversity branches where the modelalready exhibits high confidence. We observe that a small subset of uncertainreasoning steps has a disproportionately large impact on final predictionaccuracy, and branching at these critical junctures tends to yield more diverseand higher-quality candidate reasoning steps. We propose Entropy-GatedBranching (EGB), which branches only at high-uncertainty steps and prunesexpansions with a lightweight verifier. On mathematical and financial reasoningbenchmarks, EGB improves accuracy by 22.6% over standard inference whileoperating 31%-75% faster across math benchmarks than test-time beam search withhigher performance. Our results show that dynamic resource allocation duringinference can substantially improve both efficiency and effectiveness, offeringa more scalable pathway to enhanced LLM reasoning capabilities.</description><author>Xianzhi Li, Ethan Callanan, Abdellah Ghassel, Xiaodan Zhu</author><pubDate>Tue, 07 Oct 2025 16:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.21961v3</guid></item><item><title>Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents</title><link>http://arxiv.org/abs/2510.06078v1</link><description>Route recommendation aims to provide users with optimal travel plans thatsatisfy diverse and complex requirements. Classical routing algorithms (e.g.,shortest-path and constraint-aware search) are efficient but assume structuredinputs and fixed objectives, limiting adaptability to natural-language queries.Recent LLM-based approaches enhance flexibility but struggle with spatialreasoning and the joint modeling of route-level and POI-level preferences. Toaddress these limitations, we propose RouteLLM, a hierarchical multi-agentframework that grounds natural-language intents into constraint-aware routes.It first parses user queries into structured intents including POIs, paths, andconstraints. A manager agent then coordinates specialized sub-agents: aconstraint agent that resolves and formally check constraints, a POI agent thatretrieves and ranks candidate POIs, and a path refinement agent that refinesroutes via a routing engine with preference-conditioned costs. A final verifieragent ensures constraint satisfaction and produces the final route with aninterpretable rationale. This design bridges linguistic flexibility and spatialstructure, enabling reasoning over route feasibility and user preferences.Experiments show that our method reliably grounds textual preferences intoconstraint-aware routes, improving route quality and preference satisfactionover classical methods.</description><author>Tao Zhe, Rui Liu, Fateme Memar, Xiao Luo, Wei Fan, Xinyue Ye, Zhongren Peng, Dongjie Wang</author><pubDate>Tue, 07 Oct 2025 16:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06078v1</guid></item><item><title>When Thinking Drifts: Evidential Grounding for Robust Video Reasoning</title><link>http://arxiv.org/abs/2510.06077v1</link><description>Video reasoning, the task of enabling machines to infer from dynamic visualcontent through multi-step logic, is crucial for advanced AI. While theChain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,its application to video understanding remains underexplored. This paperpresents a systematic analysis revealing that CoT often degrades performance invideo reasoning, generating verbose but misleading internal monologues, andleading to hallucinated visual details and overridden correct intuitions - aphenomenon we term "visual thinking drift". We explain this drift through aBayesian lens, positing that CoT traces often diverge from actual visualevidence, instead amplifying internal biases or language priors, causing modelsto storytell rather than engage in grounded reasoning. To counteract this, weintroduce Visual Evidence Reward (VER), a novel reinforcement learningframework that explicitly rewards the generation of reasoning traces that areverifiably grounded in visual evidence. Comprehensive evaluation across 10diverse video understanding benchmarks demonstrates that our Video-VERconsistently achieves top performance. Our work sheds light on the distinctchallenges of video-centric reasoning and encourages the development of AI thatrobustly grounds its inferences in visual evidence - for large multimodalmodels that not only "think before answering", but also "see while thinking".</description><author>Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman</author><pubDate>Tue, 07 Oct 2025 16:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06077v1</guid></item><item><title>EmoHRNet: High-Resolution Neural Network Based Speech Emotion Recognition</title><link>http://arxiv.org/abs/2510.06072v1</link><description>Speech emotion recognition (SER) is pivotal for enhancing human-machineinteractions. This paper introduces "EmoHRNet", a novel adaptation ofHigh-Resolution Networks (HRNet) tailored for SER. The HRNet structure isdesigned to maintain high-resolution representations from the initial to thefinal layers. By transforming audio samples into spectrograms, EmoHRNetleverages the HRNet architecture to extract high-level features. EmoHRNet'sunique architecture maintains high-resolution representations throughout,capturing both granular and overarching emotional cues from speech signals. Themodel outperforms leading models, achieving accuracies of 92.45% on RAVDESS,80.06% on IEMOCAP, and 92.77% on EMOVO. Thus, we show that EmoHRNet sets a newbenchmark in the SER domain.</description><author>Akshay Muppidi, Martin Radfar</author><pubDate>Tue, 07 Oct 2025 15:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06072v1</guid></item><item><title>Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks</title><link>http://arxiv.org/abs/2510.06071v1</link><description>AI models are increasingly used for data analysis and visualization, yetbenchmarks rarely address scatterplot-specific tasks, limiting insight intoperformance. To address this gap for one of the most common chart types, weintroduce a synthetic, annotated dataset of over 18,000 scatterplots from sixdata generators and 17 chart designs, and a benchmark based on it. We evaluateproprietary models from OpenAI and Google using N-shot prompting on fivedistinct tasks derived from annotations of cluster bounding boxes, their centercoordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash,especially when prompted with examples, are viable options for countingclusters and, in Flash's case, outliers (90%+ Accuracy). However, the resultsfor localization-related tasks are unsatisfactory: Precision and Recall arenear or below 50%, except for Flash in outlier identification (65.01%).Furthermore, the impact of chart design on performance appears to be asecondary factor, but it is advisable to avoid scatterplots with wide aspectratios (16:9 and 21:9) or those colored randomly. Supplementary materials areavailable at https://github.com/feedzai/biy-paper.</description><author>João Palmeiro, Diogo Duarte, Rita Costa, Pedro Bizarro</author><pubDate>Tue, 07 Oct 2025 15:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06071v1</guid></item><item><title>There is More to Attention: Statistical Filtering Enhances Explanations in Vision Transformers</title><link>http://arxiv.org/abs/2510.06070v1</link><description>Explainable AI (XAI) has become increasingly important with the rise of largetransformer models, yet many explanation methods designed for CNNs transferpoorly to Vision Transformers (ViTs). Existing ViT explanations often rely onattention weights, which tend to yield noisy maps as they capturetoken-to-token interactions within each layer.While attribution methodsincorporating MLP blocks have been proposed, we argue that attention remains avaluable and interpretable signal when properly filtered. We propose a methodthat combines attention maps with a statistical filtering, initially proposedfor CNNs, to remove noisy or uninformative patterns and produce more faithfulexplanations. We further extend our approach with a class-specific variant thatyields discriminative explanations. Evaluation against popular state-of-the-artmethods demonstrates that our approach produces sharper and more interpretablemaps. In addition to perturbation-based faithfulness metrics, we incorporatehuman gaze data to assess alignment with human perception, arguing that humaninterpretability remains essential for XAI. Across multiple datasets, ourapproach consistently outperforms or is comparable to the SOTA methods whileremaining efficient and human plausible.</description><author>Meghna P Ayyar, Jenny Benois-Pineau, Akka Zemmari</author><pubDate>Tue, 07 Oct 2025 15:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06070v1</guid></item><item><title>Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning</title><link>http://arxiv.org/abs/2510.06068v1</link><description>Dexterous grasping with multi-fingered hands remains challenging due tohigh-dimensional articulations and the cost of optimization-based pipelines.Existing end-to-end methods require training on large-scale datasets forspecific hands, limiting their ability to generalize across differentembodiments. We propose an eigengrasp-based, end-to-end framework forcross-embodiment grasp generation. From a hand's morphology description, wederive a morphology embedding and an eigengrasp set. Conditioned on these,together with the object point cloud and wrist pose, an amplitude predictorregresses articulation coefficients in a low-dimensional space, which aredecoded into full joint articulations. Articulation learning is supervised witha Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevantmotions and injects morphology-specific structure. In simulation on unseenobjects across three dexterous hands, our model attains a 91.9% average graspsuccess rate with less than 0.4 seconds inference per grasp. With few-shotadaptation to an unseen hand, it achieves 85.6% success on unseen objects insimulation, and real-world experiments on this few-shot generalized handachieve an 87% success rate. The code and additional materials will be madeavailable upon publication on our project websitehttps://connor-zh.github.io/cross_embodiment_dexterous_grasping.</description><author>Heng Zhang, Kevin Yuchen Ma, Mike Zheng Shou, Weisi Lin, Yan Wu</author><pubDate>Tue, 07 Oct 2025 15:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06068v1</guid></item><item><title>Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA</title><link>http://arxiv.org/abs/2510.06067v1</link><description>CAPTCHA, originally designed to distinguish humans from robots, has evolvedinto a real-world benchmark for assessing the spatial reasoning capabilities ofvision-language models. In this work, we first show that step-by-step reasoningis crucial for vision-language models (VLMs) to solve CAPTCHAs, which representhigh-difficulty spatial reasoning tasks, and that current commercialvision-language models still struggle with such reasoning. In particular, weobserve that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail toeffectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).However, our findings indicate that requiring the model to perform step-by-stepreasoning before generating the final coordinates can significantly enhance itssolving accuracy, underscoring the severity of the gap. To systematically studythis issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark withreasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,etc.) with step-by-step action solutions and grounding annotations. We furtherdefine five reasoning-oriented metrics that enable a comprehensive evaluationof models reasoning capabilities. To validate the effectiveness of reasoning,we also propose a general agentic VLM-based framework that incorporates themodels inherent reasoning abilities. Our method achieves state-of-the-artperformance across five high-difficulty CAPTCHA types, with an average solvingaccuracy of 83.9 percent, substantially surpassing existing baselines. Theseresults reveal the limitations of current models and highlight the importanceof reasoning in advancing visual-spatial challenges in the future.</description><author>Python Song, Luke Tenyi Chang, Yun-Yun Tsai, Penghui Li, Junfeng Yang</author><pubDate>Tue, 07 Oct 2025 15:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06067v1</guid></item><item><title>CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis</title><link>http://arxiv.org/abs/2508.02322v2</link><description>Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures aredistinguished by their strong performance scaling with increasing parametersacross a wide range of tasks, yet they also suffer from substantialcomputational and storage overheads. Notably, the performance gains of MoEmodels do not scale proportionally with the growth in expert parameters. Whileprior works attempt to reduce parameters via expert-level pruning, merging, ordecomposition, they still suffer from challenges in both performance andcomputational efficiency. In this paper, we address these challenges byintroducing micro-expert as a finer-grained compression unit that spans acrossmatrices. We first establish a more fundamental perspective, viewing MoE layersas mixtures of micro-experts, and present CAMERA, a lightweight andtraining-free framework for identifying micro-expert redundancy. Our analysisuncovers significant variance in micro-expert contributions during decoding.Based on this insight, we further propose CAMERA-P, a structured micro-expertpruning framework, and CAMERA-Q, a mixed-precision quantization idea designedfor micro-experts. Extensive experiments on nine downstream tasks show thatCAMERA-P consistently outperforms strong baselines under pruning ratios rangingfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results underaggressive 2-bit quantization, surpassing existing matrix- and channel-levelideas. Notably, our method enables complete micro-expert analysis ofQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.</description><author>Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che</author><pubDate>Tue, 07 Oct 2025 15:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.02322v2</guid></item><item><title>Analyzing the Effect of Embedding Norms and Singular Values to Oversmoothing in Graph Neural Networks</title><link>http://arxiv.org/abs/2510.06066v1</link><description>In this paper, we study the factors that contribute to the effect ofoversmoothing in deep Graph Neural Networks (GNNs). Specifically, our analysisis based on a new metric (Mean Average Squared Distance - $MASED$) to quantifythe extent of oversmoothing. We derive layer-wise bounds on $MASED$, whichaggregate to yield global upper and lower distance bounds. Based on thisquantification of oversmoothing, we further analyze the importance of twodifferent properties of the model; namely the norms of the generated nodeembeddings, along with the largest and smallest singular values of the weightmatrices. Building on the insights drawn from the theoretical analysis, we showthat oversmoothing increases as the number of trainable weight matrices and thenumber of adjacency matrices increases. We also use the derived layer-wisebounds on $MASED$ to form a proposal for decoupling the number of hops (i.e.,adjacency depth) from the number of weight matrices. In particular, weintroduce G-Reg, a regularization scheme that increases the bounds, anddemonstrate through extensive experiments that by doing so node classificationaccuracy increases, achieving robustness at large depths. We further show thatby reducing oversmoothing in deep networks, we can achieve better results insome tasks than using shallow ones. Specifically, we experiment with a ``coldstart" scenario, i.e., when there is no feature information for the unlabelednodes. Finally, we show empirically the trade-off between receptive field size(i.e., number of weight matrices) and performance, using the $MASED$ bounds.This is achieved by distributing adjacency hops across a small number oftrainable layers, avoiding the extremes of under- or over-parameterization ofthe GNN.</description><author>Dimitrios Kelesis, Dimitris Fotakis, Georgios Paliouras</author><pubDate>Tue, 07 Oct 2025 15:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06066v1</guid></item><item><title>Medical Vision Language Models as Policies for Robotic Surgery</title><link>http://arxiv.org/abs/2510.06064v1</link><description>Vision-based Proximal Policy Optimization (PPO) struggles with visualobservation-based robotic laparoscopic surgical tasks due to thehigh-dimensional nature of visual input, the sparsity of rewards in surgicalenvironments, and the difficulty of extracting task-relevant features from rawvisual data. We introduce a simple approach integrating MedFlamingo, a medicaldomain-specific Vision-Language Model, with PPO. Our method is evaluated onfive diverse laparoscopic surgery task environments in LapGym, using onlyendoscopic visual observations. MedFlamingo PPO outperforms and convergesfaster compared to both standard vision-based PPO and OpenFlamingo PPObaselines, achieving task success rates exceeding 70% across all environments,with improvements ranging from 66.67% to 1114.29% compared to baseline. Byprocessing task observations and instructions once per episode to generatehigh-level planning tokens, our method efficiently combines medical expertisewith real-time visual feedback. Our results highlight the value of specializedmedical knowledge in robotic surgical planning and decision-making.</description><author>Akshay Muppidi, Martin Radfar</author><pubDate>Tue, 07 Oct 2025 15:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06064v1</guid></item><item><title>TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis</title><link>http://arxiv.org/abs/2510.06063v1</link><description>Modern enterprises generate vast streams of time series metrics whenmonitoring complex systems, known as observability data. Unlike conventionaltime series from domains such as weather, observability data are zero-inflated,highly stochastic, and exhibit minimal temporal structure. Despite theirimportance, observability datasets are underrepresented in public benchmarksdue to proprietary restrictions. Existing datasets are often anonymized andnormalized, removing scale information and limiting their use for tasks beyondforecasting, such as anomaly detection, root-cause analysis, and multi-modalreasoning. To address this gap, we introduce TelecomTS, a large-scaleobservability dataset derived from a 5G telecommunications network. TelecomTSfeatures heterogeneous, de-anonymized covariates with explicit scaleinformation and supports a suite of downstream tasks, including anomalydetection, root-cause analysis, and a question-answering benchmark requiringmulti-modal reasoning. Benchmarking state-of-the-art time series, language, andreasoning models reveals that existing approaches struggle with the abrupt,noisy, and high-variance dynamics of observability data. Our experiments alsounderscore the importance of preserving covariates' absolute scale, emphasizingthe need for foundation time series models that natively leverage scaleinformation for practical observability applications.</description><author>Austin Feng, Andreas Varvarigos, Ioannis Panitsas, Daniela Fernandez, Jinbiao Wei, Yuwei Guo, Jialin Chen, Ali Maatouk, Leandros Tassiulas, Rex Ying</author><pubDate>Tue, 07 Oct 2025 15:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06063v1</guid></item><item><title>ASPO: Asymmetric Importance Sampling Policy Optimization</title><link>http://arxiv.org/abs/2510.06062v1</link><description>Recent Large Language Model (LLM) post-training methods rely on token-levelclipping mechanisms during Reinforcement Learning (RL). However, we identify afundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the ImportanceSampling (IS) ratios of positive-advantage tokens are mismatched, leading tounbalanced token weighting for positive and negative tokens. This mismatchsuppresses the update of low-probability tokens while over-amplifying alreadyhigh-probability ones. To address this, we propose Asymmetric ImportanceSampling Policy Optimization (ASPO), which uses a simple yet effective strategythat flips the IS ratios of positive-advantage tokens, aligning their updatedirection with the learning dynamics of negative ones. AIS further incorporatesa soft dual-clipping mechanism to stabilize extreme updates while maintaininggradient flow. Comprehensive experiments on coding and mathematical reasoningbenchmarks demonstrate that ASPO significantly mitigates premature convergence,improves training stability, and enhances final performance over strongGRPO-based baselines. Our analysis provides new insights into the role oftoken-level weighting in OSRL and highlights the critical importance ofcorrecting IS in LLM RL. The code and models of ASPO are available athttps://github.com/wizard-III/Archer2.0.</description><author>Jiakang Wang, Runze Liu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai</author><pubDate>Tue, 07 Oct 2025 15:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06062v1</guid></item><item><title>On Relation-Specific Neurons in Large Language Models</title><link>http://arxiv.org/abs/2502.17355v2</link><description>In large language models (LLMs), certain \emph{neurons} can store distinctpieces of knowledge learned during pretraining. While factual knowledgetypically appears as a combination of \emph{relations} and \emph{entities}, itremains unclear whether some neurons focus on a relation itself -- independentof any entity. We hypothesize such neurons \emph{detect} a relation in theinput text and \emph{guide} generation involving such a relation. Toinvestigate this, we study the LLama-2 family on a chosen set of relations,with a \textit{statistics}-based method. Our experiments demonstrate theexistence of relation-specific neurons. We measure the effect of selectivelydeactivating candidate neurons specific to relation $r$ on the LLM's ability tohandle (1) facts involving relation $r$ and (2) facts involving a differentrelation $r' \neq r$. With respect to their capacity for encoding relationinformation, we give evidence for the following three properties ofrelation-specific neurons. \textbf{(i) Neuron cumulativity.} Multiple neuronsjointly contribute to processing facts involving relation $r$, with no singleneuron fully encoding a fact in $r$ on its own. \textbf{(ii) Neuronversatility.} Neurons can be shared across multiple closely related as well asless related relations. In addition, some relation neurons transfer acrosslanguages. \textbf{(iii) Neuron interference.} Deactivating neurons specific toone relation can improve LLMs' factual recall performance for facts of otherrelations. We make our code and data publicly available athttps://github.com/cisnlp/relation-specific-neurons.</description><author>Yihong Liu, Runsheng Chen, Lea Hirlimann, Ahmad Dawar Hakimi, Mingyang Wang, Amir Hossein Kargaran, Sascha Rothe, François Yvon, Hinrich Schütze</author><pubDate>Tue, 07 Oct 2025 15:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.17355v2</guid></item></channel></rss>