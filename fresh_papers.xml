<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 10 Dec 2024 13:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>[MASK] is All You Need</title><link>http://arxiv.org/abs/2412.06787v1</link><description>In generative models, two paradigms have gained attraction in variousapplications: next-set prediction-based Masked Generative Models and next-noiseprediction-based Non-Autoregressive Models, e.g., Diffusion Models. In thiswork, we propose using discrete-state models to connect them and explore theirscalability in the vision domain. First, we conduct a step-by-step analysis ina unified design space across two types of models includingtimestep-independence, noise schedule, temperature, guidance strength, etc in ascalable manner. Second, we re-cast typical discriminative tasks, e.g., imagesegmentation, as an unmasking process from [MASK]tokens on a discrete-statemodel. This enables us to perform various sampling processes, includingflexible conditional sampling by only training once to model the jointdistribution. All aforementioned explorations lead to our framework namedDiscrete Interpolants, which enables us to achieve state-of-the-art orcompetitive performance compared to previous discrete-state based methods invarious benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics.In summary, by leveraging [MASK] in discrete-state models, we can bridge MaskedGenerative and Non-autoregressive Diffusion models, as well as generative anddiscriminative tasks.</description><author>Vincent Tao Hu, Björn Ommer</author><pubDate>Mon, 09 Dec 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06787v1</guid></item><item><title>Retrieving Semantics from the Deep: an RAG Solution for Gesture Synthesis</title><link>http://arxiv.org/abs/2412.06786v1</link><description>Non-verbal communication often comprises of semantically rich gestures thathelp convey the meaning of an utterance. Producing such semantic co-speechgestures has been a major challenge for the existing neural systems that cangenerate rhythmic beat gestures, but struggle to produce semanticallymeaningful gestures. Therefore, we present RAG-Gesture, a diffusion-basedgesture generation approach that leverages Retrieval Augmented Generation (RAG)to produce natural-looking and semantically rich gestures. Our neuro-explicitgesture generation approach is designed to produce semantic gestures groundedin interpretable linguistic knowledge. We achieve this by using explicit domainknowledge to retrieve exemplar motions from a database of co-speech gestures.Once retrieved, we then inject these semantic exemplar gestures into ourdiffusion-based gesture generation pipeline using DDIM inversion and retrievalguidance at the inference time without any need of training. Further, wepropose a control paradigm for guidance, that allows the users to modulate theamount of influence each retrieval insertion has over the generated sequence.Our comparative evaluations demonstrate the validity of our approach againstrecent gesture generation approaches. The reader is urged to explore theresults on our project page.</description><author>M. Hamza Mughal, Rishabh Dabral, Merel C. J. Scholman, Vera Demberg, Christian Theobalt</author><pubDate>Mon, 09 Dec 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06786v1</guid></item><item><title>Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation</title><link>http://arxiv.org/abs/2412.06785v1</link><description>3D generation methods have shown visually compelling results powered bydiffusion image priors. However, they often fail to produce realistic geometricdetails, resulting in overly smooth surfaces or geometric details inaccuratelybaked in albedo maps. To address this, we introduce a new method thatincorporates touch as an additional modality to improve the geometric detailsof generated 3D assets. We design a lightweight 3D texture field to synthesizevisual and tactile textures, guided by 2D diffusion model priors on both visualand tactile domains. We condition the visual texture generation onhigh-resolution tactile normals and guide the patch-based tactile texturerefinement with a customized TextureDreambooth. We further present a multi-partgeneration pipeline that enables us to synthesize different textures acrossvarious regions. To our knowledge, we are the first to leverage high-resolutiontactile sensing to enhance geometric details for 3D generation tasks. Weevaluate our method in both text-to-3D and image-to-3D settings. Ourexperiments demonstrate that our method provides customized and realistic finegeometric textures while maintaining accurate alignment between two modalitiesof vision and touch.</description><author>Ruihan Gao, Kangle Deng, Gengshan Yang, Wenzhen Yuan, Jun-Yan Zhu</author><pubDate>Mon, 09 Dec 2024 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06785v1</guid></item><item><title>P3-PO: Prescriptive Point Priors for Visuo-Spatial Generalization of Robot Policies</title><link>http://arxiv.org/abs/2412.06784v1</link><description>Developing generalizable robot policies that can robustly handle variedenvironmental conditions and object instances remains a fundamental challengein robot learning. While considerable efforts have focused on collecting largerobot datasets and developing policy architectures to learn from such data,naively learning from visual inputs often results in brittle policies that failto transfer beyond the training data. This work presents Prescriptive PointPriors for Policies or P3-PO, a novel framework that constructs a unique staterepresentation of the environment leveraging recent advances in computer visionand robot learning to achieve improved out-of-distribution generalization forrobot manipulation. This representation is obtained through two steps. First, ahuman annotator prescribes a set of semantically meaningful points on a singledemonstration frame. These points are then propagated through the dataset usingoff-the-shelf vision models. The derived points serve as an input tostate-of-the-art policy architectures for policy learning. Our experimentsacross four real-world tasks demonstrate an overall 43% absolute improvementover prior methods when evaluated in identical settings as training. Further,P3-PO exhibits 58% and 80% gains across tasks for new object instances and morecluttered environments respectively. Videos illustrating the robot'sperformance are best viewed at point-priors.github.io.</description><author>Mara Levy, Siddhant Haldar, Lerrel Pinto, Abhinav Shirivastava</author><pubDate>Mon, 09 Dec 2024 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06784v1</guid></item><item><title>CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction</title><link>http://arxiv.org/abs/2412.06782v1</link><description>In robotic visuomotor policy learning, diffusion-based models have achievedsignificant success in improving the accuracy of action trajectory generationcompared to traditional autoregressive models. However, they suffer frominefficiency due to multiple denoising steps and limited flexibility fromcomplex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressivePolicy (CARP), a novel paradigm for visuomotor policy learning that redefinesthe autoregressive action generation process as a coarse-to-fine, next-scaleapproach. CARP decouples action generation into two stages: first, an actionautoencoder learns multi-scale representations of the entire action sequence;then, a GPT-style transformer refines the sequence prediction through acoarse-to-fine autoregressive process. This straightforward and intuitiveapproach produces highly accurate and smooth actions, matching or evensurpassing the performance of diffusion-based policies while maintainingefficiency on par with autoregressive policies. We conduct extensiveevaluations across diverse settings, including single-task and multi-taskscenarios on state-based and image-based simulation benchmarks, as well asreal-world tasks. CARP achieves competitive success rates, with up to a 10%improvement, and delivers 10x faster inference compared to state-of-the-artpolicies, establishing a high-performance, efficient, and flexible paradigm foraction generation in robotic tasks.</description><author>Zhefei Gong, Pengxiang Ding, Shangke Lyu, Siteng Huang, Mingyang Sun, Wei Zhao, Zhaoxin Fan, Donglin Wang</author><pubDate>Mon, 09 Dec 2024 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06782v1</guid></item><item><title>Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation</title><link>http://arxiv.org/abs/2412.06781v1</link><description>Global visual geolocation predicts where an image was captured on Earth.Since images vary in how precisely they can be localized, this task inherentlyinvolves a significant degree of ambiguity. However, existing approaches aredeterministic and overlook this aspect. In this paper, we aim to close the gapbetween traditional geolocalization and modern generative methods. We proposethe first generative geolocation approach based on diffusion and Riemannianflow matching, where the denoising process operates directly on the Earth'ssurface. Our model achieves state-of-the-art performance on three visualgeolocation benchmarks: OpenStreetView-5M, YFCC-100M, and iNat21. In addition,we introduce the task of probabilistic visual geolocation, where the modelpredicts a probability distribution over all possible locations instead of asingle point. We introduce new metrics and baselines for this task,demonstrating the advantages of our diffusion-based approach. Codes and modelswill be made available.</description><author>Nicolas Dufour, David Picard, Vicky Kalogeiton, Loic Landrieu</author><pubDate>Mon, 09 Dec 2024 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06781v1</guid></item><item><title>Diverse Score Distillation</title><link>http://arxiv.org/abs/2412.06780v1</link><description>Score distillation of 2D diffusion models has proven to be a powerfulmechanism to guide 3D optimization, for example enabling text-based 3Dgeneration or single-view reconstruction. A common limitation of existing scoredistillation formulations, however, is that the outputs of the (mode-seeking)optimization are limited in diversity despite the underlying diffusion modelbeing capable of generating diverse samples. In this work, inspired by thesampling process in denoising diffusion, we propose a score formulation thatguides the optimization to follow generation paths defined by random initialseeds, thus ensuring diversity. We then present an approximation to adopt thisformulation for scenarios where the optimization may not precisely follow thegeneration paths (e.g. a 3D representation whose renderings evolve in aco-dependent manner). We showcase the applications of our `Diverse ScoreDistillation' (DSD) formulation across tasks such as 2D optimization,text-based 3D inference, and single-view reconstruction. We also empiricallyvalidate DSD against prior score distillation formulations and show that itsignificantly improves sample diversity while preserving fidelity.</description><author>Yanbo Xu, Jayanth Srinivasa, Gaowen Liu, Shubham Tulsiani</author><pubDate>Mon, 09 Dec 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06780v1</guid></item><item><title>AnyBimanual: Transferring Unimanual Policy for General Bimanual Manipulation</title><link>http://arxiv.org/abs/2412.06779v1</link><description>Performing general language-conditioned bimanual manipulation tasks is ofgreat importance for many applications ranging from household service toindustrial assembly. However, collecting bimanual manipulation data isexpensive due to the high-dimensional action space, which poses challenges forconventional methods to handle general bimanual manipulation tasks. Incontrast, unimanual policy has recently demonstrated impressivegeneralizability across a wide range of tasks because of scaled modelparameters and training data, which can provide sharable manipulation knowledgefor bimanual systems. To this end, we propose a plug-and-play method namedAnyBimanual, which transfers pre-trained unimanual policy to general bimanualmanipulation policy with few bimanual demonstrations. Specifically, we firstintroduce a skill manager to dynamically schedule the skill representationsdiscovered from pre-trained unimanual policy for bimanual manipulation tasks,which linearly combines skill primitives with task-oriented compensation torepresent the bimanual manipulation instruction. To mitigate the observationdiscrepancy between unimanual and bimanual systems, we present a visual alignerto generate soft masks for visual embedding of the workspace, which aims toalign visual input of unimanual policy model for each arm with those duringpretraining stage. AnyBimanual shows superiority on 12 simulated tasks fromRLBench2 with a sizable 12.67% improvement in success rate over previousmethods. Experiments on 9 real-world tasks further verify its practicality withan average success rate of 84.62%.</description><author>Guanxing Lu, Tengbo Yu, Haoyuan Deng, Season Si Chen, Yansong Tang, Ziwei Wang</author><pubDate>Mon, 09 Dec 2024 18:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06779v1</guid></item><item><title>Data Attribution for Text-to-Image Models by Unlearning Synthesized Images</title><link>http://arxiv.org/abs/2406.09408v2</link><description>The goal of data attribution for text-to-image models is to identify thetraining images that most influence the generation of a new image. Influence isdefined such that, for a given output, if a model is retrained from scratchwithout the most influential images, the model would fail to reproduce the sameoutput. Unfortunately, directly searching for these influential images iscomputationally infeasible, since it would require repeatedly retraining modelsfrom scratch. In our work, we propose an efficient data attribution method bysimulating unlearning the synthesized image. We achieve this by increasing thetraining loss on the output image, without catastrophic forgetting of other,unrelated concepts. We then identify training images with significant lossdeviations after the unlearning process and label these as influential. Weevaluate our method with a computationally intensive but "gold-standard"retraining from scratch and demonstrate our method's advantages over previousmethods.</description><author>Sheng-Yu Wang, Aaron Hertzmann, Alexei A. Efros, Jun-Yan Zhu, Richard Zhang</author><pubDate>Mon, 09 Dec 2024 18:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09408v2</guid></item><item><title>Towards Foundation Models for 3D Vision: How Close Are We?</title><link>http://arxiv.org/abs/2410.10799v2</link><description>Building a foundation model for 3D vision is a complex challenge that remainsunsolved. Towards that goal, it is important to understand the 3D reasoningcapabilities of current models as well as identify the gaps between thesemodels and humans. Therefore, we construct a new 3D visual understandingbenchmark named UniQA-3D. UniQA-3D covers fundamental 3D vision tasks in theVisual Question Answering (VQA) format. We evaluate state-of-the-artVision-Language Models (VLMs), specialized models, and human subjects on it.Our results show that VLMs generally perform poorly, while the specializedmodels are accurate but not robust, failing under geometric perturbations. Incontrast, human vision continues to be the most reliable 3D visual system. Wefurther demonstrate that neural networks align more closely with human 3Dvision mechanisms compared to classical computer vision methods, andTransformer-based networks such as ViT align more closely with human 3D visionmechanisms than CNNs. We hope our study will benefit the future development offoundation models for 3D vision. Code is available athttps://github.com/princeton-vl/UniQA-3D .</description><author>Yiming Zuo, Karhan Kayan, Maggie Wang, Kevin Jeon, Jia Deng, Thomas L. Griffiths</author><pubDate>Mon, 09 Dec 2024 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10799v2</guid></item><item><title>Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving</title><link>http://arxiv.org/abs/2412.06777v1</link><description>Realtime 4D reconstruction for dynamic scenes remains a crucial challenge forautonomous driving perception. Most existing methods rely on depth estimationthrough self-supervision or multi-modality sensor fusion. In this paper, wepropose Driv3R, a DUSt3R-based framework that directly regresses per-framepoint maps from multi-view image sequences. To achieve streaming densereconstruction, we maintain a memory pool to reason both spatial relationshipsacross sensors and dynamic temporal contexts to enhance multi-view 3Dconsistency and temporal integration. Furthermore, we employ a 4D flowpredictor to identify moving objects within the scene to direct our networkfocus more on reconstructing these dynamic regions. Finally, we align allper-frame pointmaps consistently to the world coordinate system in anoptimization-free manner. We conduct extensive experiments on the large-scalenuScenes dataset to evaluate the effectiveness of our method. Driv3Routperforms previous frameworks in 4D dynamic scene reconstruction, achieving15x faster inference speed compared to methods requiring global alignment.Code: https://github.com/Barrybarry-Smith/Driv3R.</description><author>Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, Jiwen Lu</author><pubDate>Mon, 09 Dec 2024 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06777v1</guid></item><item><title>Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models</title><link>http://arxiv.org/abs/2412.06775v1</link><description>While large vision-language models (LVLMs) have shown impressive capabilitiesin generating plausible responses correlated with input visual contents, theystill suffer from hallucinations, where the generated text inaccuratelyreflects visual contents. To address this, recent approaches apply contrastivedecoding to calibrate the model's response via contrasting output distributionswith original and visually distorted samples, demonstrating promisinghallucination mitigation in a training-free manner. However, the potential ofchanging information in visual inputs is not well-explored, so a deeperinvestigation into the behaviors of visual contrastive decoding is of greatinterest. In this paper, we first explore various methods for contrastivedecoding to change visual contents, including image downsampling and editing.Downsampling images reduces the detailed textual information while editingyields new contents in images, providing new aspects as visual contrastivesamples. To further study benefits by using different contrastive samples, weanalyze probability-level metrics, including entropy and distribution distance.Interestingly, the effect of these samples in mitigating hallucinations variesa lot across LVLMs and benchmarks. Based on our analysis, we propose a simpleyet effective method to combine contrastive samples, offering a practicalsolution for applying contrastive decoding across various scenarios. Extensiveexperiments are conducted to validate the proposed fusion method amongdifferent benchmarks.</description><author>Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu</author><pubDate>Mon, 09 Dec 2024 18:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06775v1</guid></item><item><title>SCADE: Scalable Framework for Anomaly Detection in High-Performance System</title><link>http://arxiv.org/abs/2412.04259v2</link><description>As command-line interfaces remain integral to high-performance computingenvironments, the risk of exploitation through stealthy and complexcommand-line abuse grows. Conventional security solutions struggle to detectthese anomalies due to their context-specific nature, lack of labeled data, andthe prevalence of sophisticated attacks like Living-off-the-Land (LOL). Toaddress this gap, we introduce the Scalable Command-Line Anomaly DetectionEngine (SCADE), a framework that combines global statistical models with localcontext-specific analysis for unsupervised anomaly detection. SCADE leveragesnovel statistical methods, including BM25 and Log Entropy, alongside dynamicthresholding to adaptively detect rare, malicious command-line patterns in lowsignal-to-noise ratio (SNR) environments. Experimental results show that SCADEachieves above 98% SNR in identifying anomalous behavior while minimizing falsepositives. Designed for scalability and precision, SCADE provides aninnovative, metadata-enriched approach to anomaly detection, offering a robustsolution for cybersecurity in high-computation environments. This work presentsSCADE's architecture, detection methodology, and its potential for enhancinganomaly detection in enterprise systems. We argue that SCADE represents asignificant advancement in unsupervised anomaly detection, offering a robust,adaptive framework for security analysts and researchers seeking to enhancedetection accuracy in high-computation environments.</description><author>Vaishali Vinay, Anjali Mangal</author><pubDate>Mon, 09 Dec 2024 18:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04259v2</guid></item><item><title>Visual Lexicon: Rich Image Features in Language Space</title><link>http://arxiv.org/abs/2412.06774v1</link><description>We present Visual Lexicon, a novel visual language that encodes rich imageinformation into the text space of vocabulary tokens while retaining intricatevisual details that are often challenging to convey in natural language. Unliketraditional methods that prioritize either high-level semantics (e.g., CLIP) orpixel-level reconstruction (e.g., VAE), ViLex simultaneously captures richsemantic content and fine visual details, enabling high-quality imagegeneration and comprehensive visual scene understanding. Through aself-supervised learning pipeline, ViLex generates tokens optimized forreconstructing input images using a frozen text-to-image (T2I) diffusion model,preserving the detailed information necessary for high-fidelity semantic-levelreconstruction. As an image embedding in the language space, ViLex tokensleverage the compositionality of natural languages, allowing them to be usedindependently as "text tokens" or combined with natural language tokens toprompt pretrained T2I models with both visual and textual inputs, mirroring howwe interact with vision-language models (VLMs). Experiments demonstrate thatViLex achieves higher fidelity in image reconstruction compared to textembeddings--even with a single ViLex token. Moreover, ViLex successfullyperforms various DreamBooth tasks in a zero-shot, unsupervised manner withoutfine-tuning T2I models. Additionally, ViLex serves as a powerful visionencoder, consistently improving vision-language model performance across 15benchmarks relative to a strong SigLIP baseline.</description><author>XuDong Wang, Xingyi Zhou, Alireza Fathi, Trevor Darrell, Cordelia Schmid</author><pubDate>Mon, 09 Dec 2024 18:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06774v1</guid></item><item><title>Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty</title><link>http://arxiv.org/abs/2412.06771v1</link><description>User prompts for generative AI models are often underspecified, leading tosub-optimal responses. This problem is particularly evident in text-to-image(T2I) generation, where users commonly struggle to articulate their preciseintent. This disconnect between the user's vision and the model'sinterpretation often forces users to painstakingly and repeatedly refine theirprompts. To address this, we propose a design for proactive T2I agents equippedwith an interface to (1) actively ask clarification questions when uncertain,and (2) present their understanding of user intent as an understandable beliefgraph that a user can edit. We build simple prototypes for such agents andverify their effectiveness through both human studies and automated evaluation.We observed that at least 90% of human subjects found these agents and theirbelief graphs helpful for their T2I workflow. Moreover, we develop a scalableautomated evaluation approach using two agents, one with a ground truth imageand the other tries to ask as few questions as possible to align with theground truth. On DesignBench, a benchmark we created for artists and designers,the COCO dataset (Lin et al., 2014), and ImageInWords (Garg et al., 2024), weobserved that these T2I agents were able to ask informative questions andelicit crucial information to achieve successful alignment with at least 2times higher VQAScore (Lin et al., 2024) than the standard single-turn T2Igeneration. Demo: https://github.com/google-deepmind/proactive_t2i_agents.</description><author>Meera Hahn, Wenjun Zeng, Nithish Kannen, Rich Galt, Kartikeya Badola, Been Kim, Zi Wang</author><pubDate>Mon, 09 Dec 2024 18:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06771v1</guid></item><item><title>Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view Event Cameras</title><link>http://arxiv.org/abs/2412.06770v1</link><description>Volumetric reconstruction of dynamic scenes is an important problem incomputer vision. It is especially challenging in poor lighting and with fastmotion. It is partly due to the limitations of RGB cameras: To capture fastmotion without much blur, the framerate must be increased, which in turnrequires more lighting. In contrast, event cameras, which record changes inpixel brightness asynchronously, are much less dependent on lighting, makingthem more suitable for recording fast motion. We hence propose the first methodto spatiotemporally reconstruct a scene from sparse multi-view event streamsand sparse RGB frames. We train a sequence of cross-faded time-conditioned NeRFmodels, one per short recording segment. The individual segments are supervisedwith a set of event- and RGB-based losses and sparse-view regularisation. Weassemble a real-world multi-view camera rig with six static event camerasaround the object and record a benchmark multi-view event stream dataset ofchallenging motions. Our work outperforms RGB-based baselines, producingstate-of-the-art results, and opens up the topic of multi-view event-basedreconstruction as a new path for fast scene capture beyond RGB cameras. Thecode and the data will be released soon athttps://4dqv.mpi-inf.mpg.de/DynEventNeRF/</description><author>Viktor Rudnev, Gereon Fox, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik</author><pubDate>Mon, 09 Dec 2024 18:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06770v1</guid></item><item><title>Training Large Language Models to Reason in a Continuous Latent Space</title><link>http://arxiv.org/abs/2412.06769v1</link><description>Large language models (LLMs) are restricted to reason in the "languagespace", where they typically express the reasoning process with achain-of-thought (CoT) to solve a complex reasoning problem. However, we arguethat language space may not always be optimal for reasoning. For example, mostword tokens are primarily for textual coherence and not essential forreasoning, while some critical tokens require complex planning and pose hugechallenges to LLMs. To explore the potential of LLM reasoning in anunrestricted latent space instead of using natural language, we introduce a newparadigm Coconut (Chain of Continuous Thought). We utilize the last hiddenstate of the LLM as a representation of the reasoning state (termed "continuousthought"). Rather than decoding this into a word token, we feed it back to theLLM as the subsequent input embedding directly in the continuous space.Experiments show that Coconut can effectively augment the LLM on severalreasoning tasks. This novel latent reasoning paradigm leads to emergentadvanced reasoning patterns: the continuous thought can encode multiplealternative next reasoning steps, allowing the model to perform a breadth-firstsearch (BFS) to solve the problem, rather than prematurely committing to asingle deterministic path like CoT. Coconut outperforms CoT in certain logicalreasoning tasks that require substantial backtracking during planning, withfewer thinking tokens during inference. These findings demonstrate the promiseof latent reasoning and offer valuable insights for future research.</description><author>Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, Yuandong Tian</author><pubDate>Mon, 09 Dec 2024 18:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06769v1</guid></item><item><title>MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views</title><link>http://arxiv.org/abs/2412.06767v1</link><description>We present a novel appearance model that simultaneously realizes explicithigh-quality 3D surface mesh recovery and photorealistic novel view synthesisfrom sparse view samples. Our key idea is to model the underlying scenegeometry Mesh as an Atlas of Charts which we render with 2D Gaussian surfels(MAtCha Gaussians). MAtCha distills high-frequency scene surface details froman off-the-shelf monocular depth estimator and refines it through Gaussiansurfel rendering. The Gaussian surfels are attached to the charts on the fly,satisfying photorealism of neural volumetric rendering and crisp geometry of amesh model, i.e., two seemingly contradicting goals in a single model. At thecore of MAtCha lies a novel neural deformation model and a structure loss thatpreserve the fine surface details distilled from learned monocular depths whileaddressing their fundamental scale ambiguities. Results of extensiveexperimental validation demonstrate MAtCha's state-of-the-art quality ofsurface reconstruction and photorealism on-par with top contenders but withdramatic reduction in the number of input views and computational time. Webelieve MAtCha will serve as a foundational tool for any visual application invision, graphics, and robotics that require explicit geometry in addition tophotorealism. Our project page is the following:https://anttwo.github.io/matcha/</description><author>Antoine Guédon, Tomoki Ichikawa, Kohei Yamashita, Ko Nishino</author><pubDate>Mon, 09 Dec 2024 18:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06767v1</guid></item><item><title>From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design</title><link>http://arxiv.org/abs/2311.12668v3</link><description>Engineering design is undergoing a transformative shift with the advent ofAI, marking a new era in how we approach product, system, and service planning.Large language models have demonstrated impressive capabilities in enablingthis shift. Yet, with text as their only input modality, they cannot leveragethe large body of visual artifacts that engineers have used for centuries andare accustomed to. This gap is addressed with the release of multimodalvision-language models (VLMs), such as GPT-4V, enabling AI to impact many moretypes of tasks. Our work presents a comprehensive evaluation of VLMs across aspectrum of engineering design tasks, categorized into four main areas:Conceptual Design, System-Level and Detailed Design, Manufacturing andInspection, and Engineering Education Tasks. Specifically in this paper, weassess the capabilities of two VLMs, GPT-4V and LLaVA 1.6 34B, in design taskssuch as sketch similarity analysis, CAD generation, topology optimization,manufacturability assessment, and engineering textbook problems. Through thisstructured evaluation, we not only explore VLMs' proficiency in handlingcomplex design challenges but also identify their limitations in complexengineering design applications. Our research establishes a foundation forfuture assessments of vision language models. It also contributes a set ofbenchmark testing datasets, with more than 1000 queries, for ongoingadvancements and applications in this field.</description><author>Cyril Picard, Kristen M. Edwards, Anna C. Doris, Brandon Man, Giorgio Giannone, Md Ferdous Alam, Faez Ahmed</author><pubDate>Mon, 09 Dec 2024 18:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12668v3</guid></item><item><title>Ranking-aware adapter for text-driven image ordering with CLIP</title><link>http://arxiv.org/abs/2412.06760v1</link><description>Recent advances in vision-language models (VLMs) have made significantprogress in downstream tasks that require quantitative concepts such as facialage estimation and image quality assessment, enabling VLMs to exploreapplications like image ranking and retrieval. However, existing studiestypically focus on the reasoning based on a single image and heavily depend ontext prompting, limiting their ability to learn comprehensive understandingfrom multiple images. To address this, we propose an effective yet efficientapproach that reframes the CLIP model into a learning-to-rank task andintroduces a lightweight adapter to augment CLIP for text-guided image ranking.Specifically, our approach incorporates learnable prompts to adapt to newinstructions for ranking purposes and an auxiliary branch with ranking-awareattention, leveraging text-conditioned visual differences for additionalsupervision in image ranking. Our ranking-aware adapter consistentlyoutperforms fine-tuned CLIPs on various tasks and achieves competitive resultscompared to state-of-the-art models designed for specific tasks like facial ageestimation and image quality assessment. Overall, our approach primarilyfocuses on ranking images with a single instruction, which provides a naturaland generalized way of learning from visual differences across images,bypassing the need for extensive text prompts tailored to individual tasks.Code is available: https://github.com/uynaes/RankingAwareCLIP.</description><author>Wei-Hsiang Yu, Yen-Yu Lin, Ming-Hsuan Yang, Yi-Hsuan Tsai</author><pubDate>Mon, 09 Dec 2024 18:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06760v1</guid></item><item><title>XRZoo: A Large-Scale and Versatile Dataset of Extended Reality (XR) Applications</title><link>http://arxiv.org/abs/2412.06759v1</link><description>The rapid advancement of Extended Reality (XR, encompassing AR, MR, and VR)and spatial computing technologies forms a foundational layer for the emergingMetaverse, enabling innovative applications across healthcare, education,manufacturing, and entertainment. However, research in this area is oftenlimited by the lack of large, representative, and highquality applicationdatasets that can support empirical studies and the development of newapproaches benefiting XR software processes. In this paper, we introduce XRZoo,a comprehensive and curated dataset of XR applications designed to bridge thisgap. XRZoo contains 12,528 free XR applications, spanning nine app stores,across all XR techniques (i.e., AR, MR, and VR) and use cases, with detailedmetadata on key aspects such as application descriptions, applicationcategories, release dates, user review numbers, and hardware specifications,etc. By making XRZoo publicly available, we aim to foster reproducible XRsoftware engineering and security research, enable cross-disciplinaryinvestigations, and also support the development of advanced XR systems byproviding examples to developers. Our dataset serves as a valuable resource forresearchers and practitioners interested in improving the scalability,usability, and effectiveness of XR applications. XRZoo will be released andactively maintained.</description><author>Shuqing Li, Chenran Zhang, Cuiyun Gao, Michael R. Lyu</author><pubDate>Mon, 09 Dec 2024 18:49:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06759v1</guid></item><item><title>Predictive Models in Sequential Recommendations: Bridging Performance Laws with Data Quality Insights</title><link>http://arxiv.org/abs/2412.00430v3</link><description>Sequential Recommendation (SR) plays a critical role in predicting users'sequential preferences. Despite its growing prominence in various industries,the increasing scale of SR models incurs substantial computational costs andunpredictability, challenging developers to manage resources efficiently. Underthis predicament, Scaling Laws have achieved significant success by examiningthe loss as models scale up. However, there remains a disparity between lossand model performance, which is of greater concern in practical applications.Moreover, as data continues to expand, it incorporates repetitive andinefficient data. In response, we introduce the Performance Law for SR models,which aims to theoretically investigate and model the relationship betweenmodel performance and data quality. Specifically, we first fit the HR and NDCGmetrics to transformer-based SR models. Subsequently, we propose ApproximateEntropy (ApEn) to assess data quality, presenting a more nuanced approachcompared to traditional data quantity metrics. Our method enables accuratepredictions across various dataset scales and model sizes, demonstrating astrong correlation in large SR models and offering insights into achievingoptimal performance for any given model configuration.</description><author>Tingjia Shen, Hao Wang, Chuhan Wu, Jin Yao Chin, Wei Guo, Yong Liu, Huifeng Guo, Defu Lian, Ruiming Tang, Enhong Chen</author><pubDate>Mon, 09 Dec 2024 18:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00430v3</guid></item><item><title>InstantRestore: Single-Step Personalized Face Restoration with Shared-Image Attention</title><link>http://arxiv.org/abs/2412.06753v1</link><description>Face image restoration aims to enhance degraded facial images whileaddressing challenges such as diverse degradation types, real-time processingdemands, and, most crucially, the preservation of identity-specific features.Existing methods often struggle with slow processing times and suboptimalrestoration, especially under severe degradation, failing to accuratelyreconstruct finer-level identity details. To address these issues, we introduceInstantRestore, a novel framework that leverages a single-step image diffusionmodel and an attention-sharing mechanism for fast and personalized facerestoration. Additionally, InstantRestore incorporates a novel landmarkattention loss, aligning key facial landmarks to refine the attention maps,enhancing identity preservation. At inference time, given a degraded input anda small (~4) set of reference images, InstantRestore performs a single forwardpass through the network to achieve near real-time performance. Unlike priorapproaches that rely on full diffusion processes or per-identity model tuning,InstantRestore offers a scalable solution suitable for large-scaleapplications. Extensive experiments demonstrate that InstantRestore outperformsexisting methods in quality and speed, making it an appealing choice foridentity-preserving face restoration.</description><author>Howard Zhang, Yuval Alaluf, Sizhuo Ma, Achuta Kadambi, Jian Wang, Kfir Aberman</author><pubDate>Mon, 09 Dec 2024 18:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06753v1</guid></item><item><title>ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using LLMs</title><link>http://arxiv.org/abs/2401.14279v3</link><description>Technical Q&amp;A sites are valuable for software developers seeking knowledge,but the code snippets they provide are often uncompilable and incomplete due tounresolved types and missing libraries. This poses a challenge for users whowish to reuse or analyze these snippets. Existing methods either do not focuson creating compilable code or have low success rates. To address this, wepropose ZS4C, a lightweight approach for zero-shot synthesis of compilable codefrom incomplete snippets using Large Language Models (LLMs). ZS4C operates intwo stages: first, it uses an LLM, like GPT-3.5, to identify missing importstatements in a snippet; second, it collaborates with a validator (e.g.,compiler) to fix compilation errors caused by incorrect imports and syntaxissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,Python-SO, which includes 539 Python snippets from Stack Overflow across the 20most popular Python libraries. ZS4C significantly outperforms existing methods,improving the compilation rate from 63% to 95.1% compared to thestate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infermore accurate import statements (with an F1 score of 0.98) than SnR, with animprovement of 8.5% in the F1.</description><author>Azmain Kabir, Shaowei Wang, Yuan Tian, Tse-Hsun Chen, Muhammad Asaduzzaman, Wenbin Zhang</author><pubDate>Mon, 09 Dec 2024 18:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14279v3</guid></item><item><title>Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models</title><link>http://arxiv.org/abs/2412.06748v1</link><description>A key component of building safe and reliable language models is enabling themodels to appropriately refuse to follow certain instructions or answer certainquestions. We may want models to output refusal messages for various categoriesof user queries, for example, ill-posed questions, instructions for committingillegal acts, or queries which require information past the model's knowledgehorizon. Engineering models that refuse to answer such questions is complicatedby the fact that an individual may want their model to exhibit varying levelsof sensitivity for refusing queries of various categories, and different usersmay want different refusal rates. The current default approach involvestraining multiple models with varying proportions of refusal messages from eachcategory to achieve the desired refusal rates, which is computationallyexpensive and may require training a new model to accommodate each user'sdesired preference over refusal rates. To address these challenges, we proposerefusal tokens, one such token for each refusal category or a single refusaltoken, which are prepended to the model's responses during training. We thenshow how to increase or decrease the probability of generating the refusaltoken for each category during inference to steer the model's refusal behavior.Refusal tokens enable controlling a single model's refusal rates without theneed of any further fine-tuning, but only by selectively intervening duringgeneration.</description><author>Neel Jain, Aditya Shrivastava, Chenyang Zhu, Daben Liu, Alfy Samuel, Ashwinee Panda, Anoop Kumar, Micah Goldblum, Tom Goldstein</author><pubDate>Mon, 09 Dec 2024 18:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06748v1</guid></item><item><title>Croissant: A Metadata Format for ML-Ready Datasets</title><link>http://arxiv.org/abs/2403.19546v3</link><description>Data is a critical resource for machine learning (ML), yet working with dataremains a key friction point. This paper introduces Croissant, a metadataformat for datasets that creates a shared representation across ML tools,frameworks, and platforms. Croissant makes datasets more discoverable,portable, and interoperable, thereby addressing significant challenges in MLdata management. Croissant is already supported by several popular datasetrepositories, spanning hundreds of thousands of datasets, enabling easy loadinginto the most commonly-used ML frameworks, regardless of where the data isstored. Our initial evaluation by human raters shows that Croissant metadata isreadable, understandable, complete, yet concise.</description><author>Mubashara Akhtar, Omar Benjelloun, Costanza Conforti, Luca Foschini, Joan Giner-Miguelez, Pieter Gijsbers, Sujata Goswami, Nitisha Jain, Michalis Karamousadakis, Michael Kuchnik, Satyapriya Krishna, Sylvain Lesage, Quentin Lhoest, Pierre Marcenac, Manil Maskey, Peter Mattson, Luis Oala, Hamidah Oderinwale, Pierre Ruyssen, Tim Santos, Rajat Shinde, Elena Simperl, Arjun Suresh, Goeffry Thomas, Slava Tykhonov, Joaquin Vanschoren, Susheel Varma, Jos van der Velde, Steffen Vogler, Carole-Jean Wu, Luyao Zhang</author><pubDate>Mon, 09 Dec 2024 18:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19546v3</guid></item><item><title>Deep-Learning Based Docking Methods: Fair Comparisons to Conventional Docking Workflows</title><link>http://arxiv.org/abs/2412.02889v2</link><description>The diffusion learning method, DiffDock, for docking small-molecule ligandsinto protein binding sites was recently introduced. Results includedcomparisons to more conventional docking approaches, with DiffDock showingsuperior performance. Here, we employ a fully automatic workflow using theSurflex-Dock methods to generate a fair baseline for conventional dockingapproaches. Results were generated for the common and expected situation wherea binding site location is known and also for the condition of an unknownbinding site. For the known binding site condition, Surflex-Dock success ratesat 2.0 Angstroms RMSD far exceeded those for DiffDock (Top-1/Top-5 successrates, respectively, were 68/81% compared with 45/51%). Glide performed withsimilar success rates (67/73%) to Surflex-Dock for the known binding sitecondition, and results for AutoDock Vina and Gnina followed this pattern. Forthe unknown binding site condition, using an automated method to identifymultiple binding pockets, Surflex-Dock success rates again exceeded those ofDiffDock, but by a somewhat lesser margin. DiffDock made use of roughly 17,000co-crystal structures for learning (98% of PDBBind version 2020, pre-2019structures) for a training set in order to predict on 363 test cases (2% ofPDBBind 2020) from 2019 forward. DiffDock's performance was inextricably linkedwith the presence of near-neighbor cases of close to identical protein-ligandcomplexes in the training set for over half of the test set cases. DiffDockexhibited a 40 percentage point difference on near-neighbor cases (two-thirdsof all test cases) compared with cases with no near-neighbor training case.DiffDock has apparently encoded a type of table-lookup during its learningprocess, rendering meaningful applications beyond its reach. Further, it doesnot perform even close to competitively with a competently run modern dockingworkflow.</description><author>Ajay N. Jain, Ann E. Cleves, W. Patrick Walters</author><pubDate>Mon, 09 Dec 2024 18:37:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02889v2</guid></item><item><title>ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities</title><link>http://arxiv.org/abs/2412.06745v1</link><description>Traditional fixed test sets fall short in evaluating open-ended capabilitiesof foundation models. To address this, we propose ONEBench(OpeN-EndedBenchmarking), a new testing paradigm that consolidates individual evaluationdatasets into a unified, ever-expanding sample pool. ONEBench allows users togenerate custom, open-ended evaluation benchmarks from this pool, correspondingto specific capabilities of interest. By aggregating samples across test sets,ONEBench enables the assessment of diverse capabilities beyond those covered bythe original test sets, while mitigating overfitting and dataset bias. Mostimportantly, it frames model evaluation as a collective process of selectingand aggregating sample-level tests. The shift from task-specific benchmarks to ONEBench introduces twochallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to theaggregation over diverse metrics, while incompleteness describes comparingmodels evaluated on different data subsets. To address these challenges, weexplore algorithms to aggregate sparse measurements into reliable model scores.Our aggregation algorithm ensures identifiability(asymptotically recoveringground-truth scores) and rapid convergence, enabling accurate model rankingwith less data. On homogenous datasets, we show our aggregation algorithmprovides rankings that highly correlate with those produced by average scores.We also demonstrate robustness to ~95% of measurements missing, reducingevaluation cost by up to 20x with little-to-no change in model rankings. Weintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-languagemodels, unifying evaluations across these domains. Overall, we present atechnique for open-ended evaluation, which can aggregate over incomplete,heterogeneous sample-level measurements to continually grow a benchmarkalongside the rapidly developing foundation models.</description><author>Adhiraj Ghosh, Sebastian Dziadzio, Ameya Prabhu, Vishaal Udandarao, Samuel Albanie, Matthias Bethge</author><pubDate>Mon, 09 Dec 2024 18:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06745v1</guid></item><item><title>3D Graph Attention Networks for High Fidelity Pediatric Glioma Segmentation</title><link>http://arxiv.org/abs/2412.06743v1</link><description>Pediatric brain tumors, particularly gliomas, represent a significant causeof cancer related mortality in children with complex infiltrative growthpatterns that complicate treatment. Early, accurate segmentation of thesetumors in neuroimaging data is crucial for effective diagnosis and interventionplanning. This study presents a novel 3D UNet architecture with a spatialattention mechanism tailored for automated segmentation of pediatric gliomas.Using the BraTS pediatric glioma dataset with multiparametric MRI data, theproposed model captures multi-scale features and selectively attends to tumorrelevant regions, enhancing segmentation precision and reducing interferencefrom surrounding tissue. The model's performance is quantitatively evaluatedusing the Dice similarity coefficient and HD95, demonstrating improveddelineation of complex glioma structured. This approach offers a promisingadvancement in automating pediatric glioma segmentation, with the potential toimprove clinical decision making and outcomes.</description><author>Harish Thangaraj, Diya Katariya, Eshaan Joshi, Sangeetha N</author><pubDate>Mon, 09 Dec 2024 18:36:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06743v1</guid></item><item><title>ContRail: A Framework for Realistic Railway Image Synthesis using ControlNet</title><link>http://arxiv.org/abs/2412.06742v1</link><description>Deep Learning became an ubiquitous paradigm due to its extraordinaryeffectiveness and applicability in numerous domains. However, the approachsuffers from the high demand of data required to achieve the potential of thistype of model. An ever-increasing sub-field of Artificial Intelligence, ImageSynthesis, aims to address this limitation through the design of intelligentmodels capable of creating original and realistic images, endeavour which coulddrastically reduce the need for real data. The Stable Diffusion generationparadigm recently propelled state-of-the-art approaches to exceed all previousbenchmarks. In this work, we propose the ContRail framework based on the novelStable Diffusion model ControlNet, which we empower through a multi-modalconditioning method. We experiment with the task of synthetic railway imagegeneration, where we improve the performance in rail-specific tasks, such asrail semantic segmentation by enriching the dataset with realistic syntheticimages.</description><author>Andrei-Robert Alexandrescu, Razvan-Gabriel Petec, Alexandru Manole, Laura-Silvia Diosan</author><pubDate>Mon, 09 Dec 2024 18:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06742v1</guid></item><item><title>Convolution goes higher-order: a biologically inspired mechanism empowers image classification</title><link>http://arxiv.org/abs/2412.06740v1</link><description>We propose a novel approach to image classification inspired by complexnonlinear biological visual processing, whereby classical convolutional neuralnetworks (CNNs) are equipped with learnable higher-order convolutions. Ourmodel incorporates a Volterra-like expansion of the convolution operator,capturing multiplicative interactions akin to those observed in early andadvanced stages of biological visual processing. We evaluated this approach onsynthetic datasets by measuring sensitivity to testing higher-ordercorrelations and performance in standard benchmarks (MNIST, FashionMNIST,CIFAR10, CIFAR100 and Imagenette). Our architecture outperforms traditional CNNbaselines, and achieves optimal performance with expansions up to 3rd/4thorder, aligning remarkably well with the distribution of pixel intensities innatural images. Through systematic perturbation analysis, we validate thisalignment by isolating the contributions of specific image statistics to modelperformance, demonstrating how different orders of convolution process distinctaspects of visual information. Furthermore, Representational SimilarityAnalysis reveals distinct geometries across network layers, indicatingqualitatively different modes of visual information processing. Our workbridges neuroscience and deep learning, offering a path towards more effective,biologically inspired computer vision models. It provides insights into visualinformation processing and lays the groundwork for neural networks that bettercapture complex visual patterns, particularly in resource-constrainedscenarios.</description><author>Simone Azeglio, Olivier Marre, Peter Neri, Ulisse Ferrari</author><pubDate>Mon, 09 Dec 2024 18:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06740v1</guid></item><item><title>LLM Pruning and Distillation in Practice: The Minitron Approach</title><link>http://arxiv.org/abs/2408.11796v4</link><description>We present a comprehensive report on compressing the Llama 3.1 8B and MistralNeMo 12B models to 4B and 8B parameters, respectively, using pruning anddistillation. We explore two distinct pruning strategies: (1) depth pruning and(2) joint hidden/attention/MLP (width) pruning, and evaluate the results oncommon benchmarks from the LM Evaluation Harness. The models are then alignedwith NeMo Aligner and tested in instruct-tuned versions. This approach producesa compelling 4B model from Llama 3.1 8B and a state-of-the-artMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo12B. We found that with no access to the original data, it is beneficial toslightly fine-tune teacher models on the distillation dataset. We open-sourceour base model weights on Hugging Face with a permissive license.</description><author>Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil Mahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, Chenhan Yu, Wei-Chun Chen, Hayley Ross, Oluwatobi Olabiyi, Ashwath Aithal, Oleksii Kuchaiev, Daniel Korzekwa, Pavlo Molchanov, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, Bryan Catanzaro</author><pubDate>Mon, 09 Dec 2024 18:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11796v4</guid></item><item><title>The broader spectrum of in-context learning</title><link>http://arxiv.org/abs/2412.03782v2</link><description>The ability of language models to learn a task from a few examples in contexthas generated substantial interest. Here, we provide a perspective thatsituates this type of supervised few-shot learning within a much broaderspectrum of meta-learned in-context learning. Indeed, we suggest that anydistribution of sequences in which context non-trivially decreases loss onsubsequent predictions can be interpreted as eliciting a kind of in-contextlearning. We suggest that this perspective helps to unify the broad set ofin-context abilities that language models exhibit $\unicode{x2014}$ such asadapting to tasks from instructions or role play, or extrapolating time series.This perspective also sheds light on potential roots of in-context learning inlower-level processing of linguistic dependencies (e.g. coreference or parallelstructures). Finally, taking this perspective highlights the importance ofgeneralization, which we suggest can be studied along several dimensions: notonly the ability to learn something novel, but also flexibility in learningfrom different presentations, and in applying what is learned. We discussbroader connections to past literature in meta-learning and goal-conditionedagents, and other perspectives on learning and adaptation. We close bysuggesting that research on in-context learning should consider this broaderspectrum of in-context capabilities and types of generalization.</description><author>Andrew Kyle Lampinen, Stephanie C. Y. Chan, Aaditya K. Singh, Murray Shanahan</author><pubDate>Mon, 09 Dec 2024 18:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03782v2</guid></item><item><title>JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset Generation with LLM</title><link>http://arxiv.org/abs/2412.06738v1</link><description>Recently some studies have highlighted the potential of Large Language Models(LLMs) as effective generators of supervised training data, offering advantagessuch as enhanced inference efficiency and reduced costs associated with datacollection. However, these studies have predominantly focused on Englishlanguage tasks. In this paper, we address the fundamental research question:Can LLMs serve as proficient training data generators for other language tasks?Specifically, we leverage LLMs to synthesize supervised training data underfew-shot and zero-shot learning scenarios across six diverse Japanesedownstream tasks. Subsequently, we utilize this synthesized data to traincompact models (e.g., BERT). This novel methodology is termed JAPAGEN. Ourexperimental findings underscore that JAPAGEN achieves robust performance inclassification tasks that necessitate formal text inputs, demonstratingcompetitive results compared to conventional LLM prompting strategies.</description><author>Takuro Fujii, Satoru Katsumata</author><pubDate>Mon, 09 Dec 2024 18:27:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06738v1</guid></item><item><title>How Many Languages Make Good Multilingual Instruction Tuning? A Case Study on BLOOM</title><link>http://arxiv.org/abs/2404.04850v2</link><description>Instruction tuning a large language model with multiple languages can prepareit for multilingual downstream tasks. Nonetheless, it is yet to be determinedwhether having a handful of languages is sufficient, or whether the benefitsincrease with the inclusion of more. By fine-tuning large multilingual modelson 1 to 52 languages, we present a case study on BLOOM to understand threepertinent factors affecting performance: the number of languages, languageexposure, and similarity between training and test languages. Overall we foundthat 1) expanding language coverage in multilingual instruction tuning provesto be beneficial; 2) accuracy often significantly boots if the test languageappears in the instruction mixture; 3) languages' genetic features correlatewith cross-lingual transfer more than merely the number of language butdifferent languages benefit to various degrees.</description><author>Shaoxiong Ji, Pinzhen Chen</author><pubDate>Mon, 09 Dec 2024 18:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04850v2</guid></item><item><title>OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and Captioning</title><link>http://arxiv.org/abs/2404.03657v2</link><description>We propose the new task 'open-world video instance segmentation andcaptioning'. It requires to detect, segment, track and describe with richcaptions never before seen objects. This challenging task can be addressed bydeveloping "abstractors" which connect a vision model and a language foundationmodel. Concretely, we connect a multi-scale visual feature extractor and alarge language model (LLM) by developing an object abstractor and anobject-to-text abstractor. The object abstractor, consisting of a promptencoder and transformer blocks, introduces spatially-diverse open-world objectqueries to discover never before seen objects in videos. An inter-querycontrastive loss further encourages the diversity of object queries. Theobject-to-text abstractor is augmented with masked cross-attention and acts asa bridge between the object queries and a frozen LLM to generate rich anddescriptive object-centric captions for each detected object. Our generalizedapproach surpasses the baseline that jointly addresses the tasks of open-worldvideo instance segmentation and dense video object captioning by 13% on neverbefore seen objects, and by 10% on object-centric captions.</description><author>Anwesa Choudhuri, Girish Chowdhary, Alexander G. Schwing</author><pubDate>Mon, 09 Dec 2024 18:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03657v2</guid></item><item><title>Take Fake as Real: Realistic-like Robust Black-box Adversarial Attack to Evade AIGC Detection</title><link>http://arxiv.org/abs/2412.06727v1</link><description>The security of AI-generated content (AIGC) detection based on GANs anddiffusion models is closely related to the credibility of multimedia content.Malicious adversarial attacks can evade these developing AIGC detection.However, most existing adversarial attacks focus only on GAN-generated facialimages detection, struggle to be effective on multi-class natural images anddiffusion-based detectors, and exhibit poor invisibility. To fill this gap, wefirst conduct an in-depth analysis of the vulnerability of AIGC detectors anddiscover the feature that detectors vary in vulnerability to differentpost-processing. Then, considering the uncertainty of detectors in real-worldscenarios, and based on the discovery, we propose a Realistic-like RobustBlack-box Adversarial attack (R$^2$BA) with post-processing fusionoptimization. Unlike typical perturbations, R$^2$BA uses real-worldpost-processing, i.e., Gaussian blur, JPEG compression, Gaussian noise andlight spot to generate adversarial examples. Specifically, we use a stochasticparticle swarm algorithm with inertia decay to optimize post-processing fusionintensity and explore the detector's decision boundary. Guided by thedetector's fake probability, R$^2$BA enhances/weakens thedetector-vulnerable/detector-robust post-processing intensity to strike abalance between adversariality and invisibility. Extensive experiments onpopular/commercial AIGC detectors and datasets demonstrate that R$^2$BAexhibits impressive anti-detection performance, excellent invisibility, andstrong robustness in GAN-based and diffusion-based cases. Compared tostate-of-the-art white-box and black-box attacks, R$^2$BA shows significantimprovements of 15% and 21% in anti-detection performance under the originaland robust scenario respectively, offering valuable insights for the securityof AIGC detection in real-world applications.</description><author>Caiyun Xie, Dengpan Ye, Yunming Zhang, Long Tang, Yunna Lv, Jiacheng Deng, Jiawei Song</author><pubDate>Mon, 09 Dec 2024 18:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06727v1</guid></item><item><title>AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark</title><link>http://arxiv.org/abs/2412.06724v1</link><description>We investigate the reasoning capabilities of large language models (LLMs) forautomatically generating data-cleaning workflows. To evaluate LLMs' ability tocomplete data-cleaning tasks, we implemented a pipeline for LLM-based Auto DataCleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operationsto repair three types of data quality issues: duplicates, missing values, andinconsistent data formats. Given a dirty table and a purpose (expressed as aquery), this pipeline generates a minimal, clean table sufficient to addressthe purpose and the data cleaning workflow used to produce the table. Theplanning process involves three main LLM-driven components: (1) Select TargetColumns: Identifies a set of target columns related to the purpose. (2) InspectColumn Quality: Assesses the data quality for each target column and generatesa Data Quality Report as operation objectives. (3) Generate Operation &amp;Arguments: Predicts the next operation and arguments based on the data qualityreport results. Additionally, we propose a data cleaning benchmark to evaluatethe capability of LLM agents to automatically generate workflows that addressdata cleaning purposes of varying difficulty levels. The benchmark comprisesthe annotated datasets as a collection of purpose, raw table, clean table, datacleaning workflow, and answer set. In our experiments, we evaluated three LLMsthat auto-generate purpose-driven data cleaning workflows. The results indicatethat LLMs perform well in planning and generating data-cleaning workflowswithout the need for fine-tuning.</description><author>Lan Li, Liri Fang, Vetle I. Torvik</author><pubDate>Mon, 09 Dec 2024 18:13:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06724v1</guid></item><item><title>VP-MEL: Visual Prompts Guided Multimodal Entity Linking</title><link>http://arxiv.org/abs/2412.06720v1</link><description>Multimodal Entity Linking (MEL) is extensively utilized in the domains ofinformation retrieval. However, existing MEL methods typically utilize mentionwords as mentions for retrieval. This results in a significant dependence ofMEL on mention words, thereby constraining its capacity to effectively leverageinformation from both images and text. In situations where mention words areabsent, MEL methods struggle to leverage image-text pairs for entity linking.To solve these issues, we introduce a Visual Prompts guided Multimodal EntityLinking (VP-MEL) task. VP-MEL directly marks specific regions within the image.These markers are referred to as visual prompts in VP-MEL. Without mentionwords, VP-MEL aims to utilize marked image-text pairs to align visual promptswith specific entities in the knowledge bases. A new dataset for the VP-MELtask, VPWiki, is proposed in this paper. Moreover, we propose a framework namedFBMEL, which enhances the significance of visual prompts and fully leveragesthe information in image-text pairs. Experimental results on the VPWiki datasetdemonstrate that FBMEL outperforms baseline methods across multiple benchmarksfor the VP-MEL task.</description><author>Hongze Mi, Jinyuan Li, Xuying Zhang, Haoran Cheng, Jiahao Wang, Di Sun, Gang Pan</author><pubDate>Mon, 09 Dec 2024 18:06:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06720v1</guid></item><item><title>MVAD: A Multiple Visual Artifact Detector for Video Streaming</title><link>http://arxiv.org/abs/2406.00212v2</link><description>Visual artifacts are often introduced into streamed video content, due toprevailing conditions during content production and delivery. Since these candegrade the quality of the user's experience, it is important to automaticallyand accurately detect them in order to enable effective quality measurement andenhancement. Existing detection methods often focus on a single type ofartifact and/or determine the presence of an artifact through thresholdingobjective quality indices. Such approaches have been reported to offerinconsistent prediction performance and are also impractical for real-worldapplications where multiple artifacts co-exist and interact. In this paper, wepropose a Multiple Visual Artifact Detector, MVAD, for video streaming which,for the first time, is able to detect multiple artifacts using a singleframework that is not reliant on video quality assessment models. Our approachemploys a new Artifact-aware Dynamic Feature Extractor (ADFE) to obtainartifact-relevant spatial features within each frame for multiple artifacttypes. The extracted features are further processed by a Recurrent MemoryVision Transformer (RMViT) module, which captures both short-term and long-termtemporal information within the input video. The proposed network architectureis optimized in an end-to-end manner based on a new, large and diverse trainingdatabase that is generated by simulating the video streaming pipeline and basedon Adversarial Data Augmentation. This model has been evaluated on two videoartifact databases, Maxwell and BVI-Artifact, and achieves consistent andimproved prediction results for ten target visual artifacts when compared toseven existing single and multiple artifact detectors. The source code andtraining database will be available athttps://chenfeng-bristol.github.io/MVAD/.</description><author>Chen Feng, Duolikun Danier, Fan Zhang, Alex Mackin, Andrew Collins, David Bull</author><pubDate>Mon, 09 Dec 2024 18:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00212v2</guid></item><item><title>Toward Non-Invasive Diagnosis of Bankart Lesions with Deep Learning</title><link>http://arxiv.org/abs/2412.06717v1</link><description>Bankart lesions, or anterior-inferior glenoid labral tears, arediagnostically challenging on standard MRIs due to their subtle imagingfeatures-often necessitating invasive MRI arthrograms (MRAs). This studydevelops deep learning (DL) models to detect Bankart lesions on both standardMRIs and MRAs, aiming to improve diagnostic accuracy and reduce reliance onMRAs. We curated a dataset of 586 shoulder MRIs (335 standard, 251 MRAs) from558 patients who underwent arthroscopy. Ground truth labels were derived fromintraoperative findings, the gold standard for Bankart lesion diagnosis.Separate DL models for MRAs and standard MRIs were trained using the SwinTransformer architecture, pre-trained on a public knee MRI dataset. Predictionsfrom sagittal, axial, and coronal views were ensembled to optimize performance.The models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71standard MRIs). Bankart lesions were identified in 31.9% of MRAs and 8.6% ofstandard MRIs. The models achieved AUCs of 0.87 (86% accuracy, 83% sensitivity,86% specificity) and 0.90 (85% accuracy, 82% sensitivity, 86% specificity) onstandard MRIs and MRAs, respectively. These results match or surpassradiologist performance on our dataset and reported literature metrics.Notably, our model's performance on non-invasive standard MRIs matched orsurpassed the radiologists interpreting MRAs. This study demonstrates thefeasibility of using DL to address the diagnostic challenges posed by subtlepathologies like Bankart lesions. Our models demonstrate potential to improvediagnostic confidence, reduce reliance on invasive imaging, and enhanceaccessibility to care.</description><author>Sahil Sethi, Sai Reddy, Mansi Sakarvadia, Jordan Serotte, Darlington Nwaudo, Nicholas Maassen, Lewis Shi</author><pubDate>Mon, 09 Dec 2024 18:04:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06717v1</guid></item><item><title>ALEN: An Adaptive Dual-Approach for Enhancing Uniform and Non-Uniform Low-Light Images</title><link>http://arxiv.org/abs/2407.19708v2</link><description>Low-light image enhancement is vital for improving the visibility and qualityof images captured under suboptimal lighting conditions. Traditional methodsoften fail to adequately capture local lighting variations and enhance bothtextural and chromatic details. Recent deep learning-based approaches, whileeffective, still struggle with generalization across diverse datasets, leadingto noise amplification and unnatural color saturation. To address thesechallenges, the Adaptive Light Enhancement Network (ALEN) is introduced, anovel method that utilizes a classification mechanism to determine whetherlocal or global illumination enhancement is required. ALEN integrates the SwinLight-Classification Transformer (SLCformer) for illuminance categorization,complemented by the Single-Channel Network (SCNet) and Multi-Channel Network(MCNet) for precise illumination and color estimation, respectively. Extensiveexperiments on publicly available datasets demonstrate ALEN's robustgeneralization capabilities, outperforming state-of-the-art methods in bothquantitative metrics and qualitative assessments. Furthermore, ALEN not onlyenhances image quality but also improves the performance of high-level visiontasks such as semantic segmentation, showcasing its broader applicability andpotential impact. The code for this method and the datasets are available athttps://github.com/xingyumex/ALEN}{https://github.com/xingyumex/ALEN</description><author>Ezequiel Perez-Zarate, Oscar Ramos-Soto, Chunxiao Liu, Diego Oliva, Marco Perez-Cisneros</author><pubDate>Mon, 09 Dec 2024 18:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19708v2</guid></item><item><title>How to Merge Your Multimodal Models Over Time?</title><link>http://arxiv.org/abs/2412.06712v1</link><description>Model merging combines multiple expert models - finetuned from a basefoundation model on diverse tasks and domains - into a single, more capablemodel. However, most existing model merging approaches assume that all expertsare available simultaneously. In reality, new tasks and domains emergeprogressively over time, requiring strategies to integrate the knowledge ofexpert models as they become available: a process we call temporal modelmerging. The temporal dimension introduces unique challenges not addressed inprior work, raising new questions such as: when training for a new task, shouldthe expert model start from the merged past experts or from the original basemodel? Should we merge all models at each time step? Which merging techniquesare best suited for temporal merging? Should different strategies be used toinitialize the training and deploy the model? To answer these questions, wepropose a unified framework called TIME - Temporal Integration of ModelExpertise - which defines temporal model merging across three axes: (1)Initialization Phase, (2) Deployment Phase, and (3) Merging Technique. UsingTIME, we study temporal model merging across model sizes, compute budgets, andlearning horizons on the FoMo-in-Flux benchmark. Our comprehensive suite ofexperiments across TIME allows us to uncover key insights for temporal modelmerging, offering a better understanding of current challenges and bestpractices for effective temporal model merging.</description><author>Sebastian Dziadzio, Vishaal Udandarao, Karsten Roth, Ameya Prabhu, Zeynep Akata, Samuel Albanie, Matthias Bethge</author><pubDate>Mon, 09 Dec 2024 18:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06712v1</guid></item><item><title>MISFEAT: Feature Selection for Subgroups with Systematic Missing Data</title><link>http://arxiv.org/abs/2412.06711v1</link><description>We investigate the problem of selecting features for datasets that can benaturally partitioned into subgroups (e.g., according to socio-demographicgroups and age), each with its own dominant set of features. Within thissubgroup-oriented framework, we address the challenge of systematic missingdata, a scenario in which some feature values are missing for all tuples of asubgroup, due to flawed data integration, regulatory constraints, or privacyconcerns. Feature selection is governed by finding mutual Information, apopular quantification of correlation, between features and a target variable.Our goal is to identify top-K feature subsets of some fixed size with thehighest joint mutual information with a target variable. In the presence ofsystematic missing data, the closed form of mutual information could not simplybe applied. We argue that in such a setting, leveraging relationships betweenavailable feature mutual information within a subgroup or across subgroups canassist inferring missing mutual information values. We propose a generalizablemodel based on heterogeneous graph neural network to identify interdependenciesbetween feature-subgroup-target variable connections by modeling it as amultiplex graph, and employing information propagation between its nodes. Weaddress two distinct scalability challenges related to training and proposeprincipled solutions to tackle them. Through an extensive empirical evaluation,we demonstrate the efficacy of the proposed solutions both qualitatively andrunning time wise.</description><author>Bar Genossar, Thinh On, Md. Mouinul Islam, Ben Eliav, Senjuti Basu Roy, Avigdor Gal</author><pubDate>Mon, 09 Dec 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06711v1</guid></item><item><title>Parkinson's Disease Diagnosis Through Deep Learning: A Novel LSTM-Based Approach for Freezing of Gait Detection</title><link>http://arxiv.org/abs/2412.06709v1</link><description>Deep learning holds tremendous potential in healthcare for uncovering hiddenpatterns within extensive clinical datasets, aiding in the diagnosis of variousdiseases. Parkinson's disease (PD) is a neurodegenerative conditioncharacterized by the deterioration of brain function. In the initial stages ofPD, automatic diagnosis poses a challenge due to the similarity in behaviorbetween individuals with PD and those who are healthy. Our objective is topropose an effective model that can aid in the early detection of Parkinson'sdisease. We employed the VGRF gait signal dataset sourced from Physionet fordistinguishing between healthy individuals and those diagnosed with Parkinson'sdisease. This paper introduces a novel deep learning architecture based on theLSTM network for automatically detecting freezing of gait episodes inParkinson's disease patients. In contrast to conventional machine learningalgorithms, this method eliminates manual feature engineering and proficientlycaptures prolonged temporal dependencies in gait patterns, thereby improvingthe diagnosis of Parkinson's disease. The LSTM network resolves the issue ofvanishing gradients by employing memory blocks in place of self-connectedhidden units, allowing for optimal information assimilation. To preventoverfitting, dropout and L2 regularization techniques have been employed.Additionally, the stochastic gradient-based optimizer Adam is used for theoptimization process. The results indicate that our proposed approach surpassescurrent state-of-the-art models in FOG episode detection, achieving an accuracyof 97.71%, sensitivity of 99%, precision of 98%, and specificity of 96%. Thisdemonstrates its potential as a superior classification method for Parkinson'sdisease detection.</description><author>Aqib Nazir Mir, Iqra Nissar, Mumtaz Ahmed, Sarfaraz Masood, Danish Raza Rizvi</author><pubDate>Mon, 09 Dec 2024 17:58:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06709v1</guid></item><item><title>FlexEvent: Event Camera Object Detection at Arbitrary Frequencies</title><link>http://arxiv.org/abs/2412.06708v1</link><description>Event cameras offer unparalleled advantages for real-time perception indynamic environments, thanks to their microsecond-level temporal resolution andasynchronous operation. Existing event-based object detection methods, however,are limited by fixed-frequency paradigms and fail to fully exploit thehigh-temporal resolution and adaptability of event cameras. To address theselimitations, we propose FlexEvent, a novel event camera object detectionframework that enables detection at arbitrary frequencies. Our approachconsists of two key components: FlexFuser, an adaptive event-frame fusionmodule that integrates high-frequency event data with rich semantic informationfrom RGB frames, and FAL, a frequency-adaptive learning mechanism thatgenerates frequency-adjusted labels to enhance model generalization acrossvarying operational frequencies. This combination allows our method to detectobjects with high accuracy in both fast-moving and static scenarios, whileadapting to dynamic environments. Extensive experiments on large-scale eventcamera datasets demonstrate that our approach surpasses state-of-the-artmethods, achieving significant improvements in both standard and high-frequencysettings. Notably, our method maintains robust performance when scaling from 20Hz to 90 Hz and delivers accurate detection up to 180 Hz, proving itseffectiveness in extreme conditions. Our framework sets a new benchmark forevent-based object detection and paves the way for more adaptable, real-timevision systems.</description><author>Dongyue Lu, Lingdong Kong, Gim Hee Lee, Camille Simon Chane, Wei Tsang Ooi</author><pubDate>Mon, 09 Dec 2024 17:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06708v1</guid></item><item><title>FabuLight-ASD: Unveiling Speech Activity via Body Language</title><link>http://arxiv.org/abs/2411.13674v2</link><description>Active speaker detection (ASD) in multimodal environments is crucial forvarious applications, from video conferencing to human-robot interaction. Thispaper introduces FabuLight-ASD, an advanced ASD model that integrates facial,audio, and body pose information to enhance detection accuracy and robustness.Our model builds upon the existing Light-ASD framework by incorporating humanpose data, represented through skeleton graphs, which minimises computationaloverhead. Using the Wilder Active Speaker Detection (WASD) dataset, renownedfor reliable face and body bounding box annotations, we demonstrateFabuLight-ASD's effectiveness in real-world scenarios. Achieving an overallmean average precision (mAP) of 94.3%, FabuLight-ASD outperforms Light-ASD,which has an overall mAP of 93.7% across various challenging scenarios. Theincorporation of body pose information shows a particularly advantageousimpact, with notable improvements in mAP observed in scenarios with speechimpairment, face occlusion, and human voice background noise. Furthermore,efficiency analysis indicates only a modest increase in parameter count (27.3%)and multiply-accumulate operations (up to 2.4%), underscoring the model'sefficiency and feasibility. These findings validate the efficacy ofFabuLight-ASD in enhancing ASD performance through the integration of body posedata. FabuLight-ASD's code and model weights are available athttps://github.com/knowledgetechnologyuhh/FabuLight-ASD.</description><author>Hugo Carneiro, Stefan Wermter</author><pubDate>Mon, 09 Dec 2024 17:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.13674v2</guid></item><item><title>FIARSE: Model-Heterogeneous Federated Learning via Importance-Aware Submodel Extraction</title><link>http://arxiv.org/abs/2407.19389v3</link><description>In federated learning (FL), accommodating clients' varied computationalcapacities poses a challenge, often limiting the participation of those withconstrained resources in global model training. To address this issue, theconcept of model heterogeneity through submodel extraction has emerged,offering a tailored solution that aligns the model's complexity with eachclient's computational capacity. In this work, we propose FederatedImportance-Aware Submodel Extraction (FIARSE), a novel approach thatdynamically adjusts submodels based on the importance of model parameters,thereby overcoming the limitations of previous static and dynamic submodelextraction methods. Compared to existing works, the proposed method offers atheoretical foundation for the submodel extraction and eliminates the need foradditional information beyond the model parameters themselves to determineparameter importance, significantly reducing the overhead on clients. Extensiveexperiments are conducted on various datasets to showcase the superiorperformance of the proposed FIARSE.</description><author>Feijie Wu, Xingchen Wang, Yaqing Wang, Tianci Liu, Lu Su, Jing Gao</author><pubDate>Mon, 09 Dec 2024 17:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19389v3</guid></item><item><title>Source Separation &amp; Automatic Transcription for Music</title><link>http://arxiv.org/abs/2412.06703v1</link><description>Source separation is the process of isolating individual sounds in anauditory mixture of multiple sounds [1], and has a variety of applicationsranging from speech enhancement and lyric transcription [2] to digital audioproduction for music. Furthermore, Automatic Music Transcription (AMT) is theprocess of converting raw music audio into sheet music that musicians can read[3]. Historically, these tasks have faced challenges such as significant audionoise, long training times, and lack of free-use data due to copyrightrestrictions. However, recent developments in deep learning have brought newpromising approaches to building low-distortion stems and generating sheetmusic from audio signals [4]. Using spectrogram masking, deep neural networks,and the MuseScore API, we attempt to create an end-to-end pipeline that allowsfor an initial music audio mixture (e.g...wav file) to be separated intoinstrument stems, converted into MIDI files, and transcribed into sheet musicfor each component instrument.</description><author>Bradford Derby, Lucas Dunker, Samarth Galchar, Shashank Jarmale, Akash Setti</author><pubDate>Mon, 09 Dec 2024 17:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06703v1</guid></item><item><title>You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale</title><link>http://arxiv.org/abs/2412.06699v1</link><description>Recent 3D generation models typically rely on limited-scale 3D `gold-labels'or 2D diffusion priors for 3D content creation. However, their performance isupper-bounded by constrained 3D priors due to the lack of scalable learningparadigms. In this work, we present See3D, a visual-conditional multi-viewdiffusion model trained on large-scale Internet videos for open-world 3Dcreation. The model aims to Get 3D knowledge by solely Seeing the visualcontents from the vast and rapidly growing video data -- You See it, You Gotit. To achieve this, we first scale up the training data using a proposed datacuration pipeline that automatically filters out multi-view inconsistencies andinsufficient observations from source videos. This results in a high-quality,richly diverse, large-scale dataset of multi-view images, termed WebVi3D,containing 320M frames from 16M video clips. Nevertheless, learning generic 3Dpriors from videos without explicit 3D geometry or camera pose annotations isnontrivial, and annotating poses for web-scale videos is prohibitivelyexpensive. To eliminate the need for pose conditions, we introduce aninnovative visual-condition - a purely 2D-inductive visual signal generated byadding time-dependent noise to the masked video data. Finally, we introduce anovel visual-conditional 3D generation framework by integrating See3D into awarping-based pipeline for high-fidelity 3D generation. Our numerical andvisual comparisons on single and sparse reconstruction benchmarks show thatSee3D, trained on cost-effective and scalable video data, achieves notablezero-shot and open-world generation capabilities, markedly outperforming modelstrained on costly and constrained 3D datasets. Please refer to our project pageat: https://vision.baai.ac.cn/see3d</description><author>Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, Xinlong Wang</author><pubDate>Mon, 09 Dec 2024 17:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06699v1</guid></item><item><title>Gen-3Diffusion: Realistic Image-to-3D Generation via 2D &amp; 3D Diffusion Synergy</title><link>http://arxiv.org/abs/2412.06698v1</link><description>Creating realistic 3D objects and clothed avatars from a single RGB image isan attractive yet challenging problem. Due to its ill-posed nature, recentworks leverage powerful prior from 2D diffusion models pretrained on largedatasets. Although 2D diffusion models demonstrate strong generalizationcapability, they cannot guarantee the generated multi-view images are 3Dconsistent. In this paper, we propose Gen-3Diffusion: Realistic Image-to-3DGeneration via 2D &amp; 3D Diffusion Synergy. We leverage a pre-trained 2Ddiffusion model and a 3D diffusion model via our elegantly designed processthat synchronizes two diffusion models at both training and sampling time. Thesynergy between the 2D and 3D diffusion models brings two major advantages: 1)2D helps 3D in generalization: the pretrained 2D model has stronggeneralization ability to unseen images, providing strong shape priors for the3D diffusion model; 2) 3D helps 2D in multi-view consistency: the 3D diffusionmodel enhances the 3D consistency of 2D multi-view sampling process, resultingin more accurate multi-view generation. We validate our idea through extensiveexperiments in image-based objects and clothed avatar generation tasks. Resultsshow that our method generates realistic 3D objects and avatars withhigh-fidelity geometry and texture. Extensive ablations also validate ourdesign choices and demonstrate the strong generalization ability to diverseclothing and compositional shapes. Our code and pretrained models will bepublicly released on https://yuxuan-xue.com/gen-3diffusion.</description><author>Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll</author><pubDate>Mon, 09 Dec 2024 17:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06698v1</guid></item><item><title>Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization</title><link>http://arxiv.org/abs/2311.05546v3</link><description>Multi-Agent Reinforcement Learning is becoming increasingly more important intimes of autonomous driving and other smart industrial applications.Simultaneously a promising new approach to Reinforcement Learning arises usingthe inherent properties of quantum mechanics, reducing the trainable parametersof a model significantly. However, gradient-based Multi-Agent QuantumReinforcement Learning methods often have to struggle with barren plateaus,holding them back from matching the performance of classical approaches. Webuild upon an existing approach for gradient free Quantum ReinforcementLearning and propose three genetic variations with Variational Quantum Circuitsfor Multi-Agent Reinforcement Learning using evolutionary optimization. Weevaluate our genetic variations in the Coin Game environment and also comparethem to classical approaches. We showed that our Variational Quantum Circuitapproaches perform significantly better compared to a neural network with asimilar amount of trainable parameters. Compared to the larger neural network,our approaches archive similar results using $97.88\%$ less parameters.</description><author>Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien</author><pubDate>Mon, 09 Dec 2024 17:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05546v3</guid></item><item><title>Digital Transformation in the Water Distribution System based on the Digital Twins Concept</title><link>http://arxiv.org/abs/2412.06694v1</link><description>Digital Twins have emerged as a disruptive technology with great potential;they can enhance WDS by offering real-time monitoring, predictive maintenance,and optimization capabilities. This paper describes the development of astate-of-the-art DT platform for WDS, introducing advanced technologies such asthe Internet of Things, Artificial Intelligence, and Machine Learning models.This paper provides insight into the architecture of the proposedplatform-CAUCCES-that, informed by both historical and meteorological data,effectively deploys AI/ML models like LSTM networks, Prophet, LightGBM, andXGBoost in trying to predict water consumption patterns. Furthermore, we delveinto how optimization in the maintenance of WDS can be achieved by formulatinga Constraint Programming problem for scheduling, hence minimizing theoperational cost efficiently with reduced environmental impacts. It alsofocuses on cybersecurity and protection to ensure the integrity and reliabilityof the DT platform. In this view, the system will contribute to improvements indecision-making capabilities, operational efficiency, and system reliability,with reassurance being drawn from the important role it can play towardsustainable management of water resources.</description><author>MohammadHossein Homaei, Agustín Javier Di Bartolo, Mar Ávila, Óscar Mogollón-Gutiérrez, Andrés Caro</author><pubDate>Mon, 09 Dec 2024 17:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06694v1</guid></item><item><title>OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions</title><link>http://arxiv.org/abs/2412.06693v1</link><description>The rapid advancements in Large Language Models (LLMs) have significantlyexpanded their applications, ranging from multilingual support todomain-specific tasks and multimodal integration. In this paper, we presentOmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and theiromni-extensions across multilingual, multidomain, and multimodal capabilities.Unlike existing benchmarks that often focus on a single aspect, OmniEvalKitprovides a modular, lightweight, and automated evaluation system. It isstructured with a modular architecture comprising a Static Builder and DynamicData Flow, promoting the seamless integration of new models and datasets.OmniEvalKit supports over 100 LLMs and 50 evaluation datasets, coveringcomprehensive evaluations across thousands of model-dataset combinations.OmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployableevaluation framework, making downstream applications more convenient andversatile for the AI community.</description><author>Yi-Kai Zhang, Xu-Xiang Zhong, Shiyin Lu, Qing-Guo Chen, De-Chuan Zhan, Han-Jia Ye</author><pubDate>Mon, 09 Dec 2024 17:39:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06693v1</guid></item><item><title>Wake Vision: A Tailored Dataset and Benchmark Suite for TinyML Computer Vision Applications</title><link>http://arxiv.org/abs/2405.00892v4</link><description>Tiny machine learning (TinyML) for low-power devices lacks robust datasetsfor development. We present Wake Vision, a large-scale dataset for persondetection that contains over 6 million quality-filtered images. We provide twovariants: Wake Vision (Large) and Wake Vision (Quality), leveraging the largevariant for pretraining and knowledge distillation, while the higher-qualitylabels drive final model performance. The manually labeled validation and testsets reduce error rates from 7.8% to 2.2% compared to previous standards. Inaddition, we introduce five detailed benchmark sets to evaluate modelperformance in real-world scenarios, including varying lighting, cameradistances, and demographic characteristics. Training with Wake Vision improvesaccuracy by 1.93% over existing datasets, demonstrating the importance ofdataset quality for low-capacity models and dataset size for high-capacitymodels. The dataset, benchmarks, code, and models are available under the CC-BY4.0 license, maintained by the Edge AI Foundation.</description><author>Colby Banbury, Emil Njor, Andrea Mattia Garavagno, Matthew Stewart, Pete Warden, Manjunath Kudlur, Nat Jeffries, Xenofon Fafoutis, Vijay Janapa Reddi</author><pubDate>Mon, 09 Dec 2024 17:35:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00892v4</guid></item><item><title>GeoSAM: Fine-tuning SAM with Multi-Modal Prompts for Mobility Infrastructure Segmentation</title><link>http://arxiv.org/abs/2311.11319v3</link><description>In geographical image segmentation, performance is often constrained by thelimited availability of training data and a lack of generalizability,particularly for segmenting mobility infrastructure such as roads, sidewalks,and crosswalks. Vision foundation models like the Segment Anything Model (SAM),pre-trained on millions of natural images, have demonstrated impressivezero-shot segmentation performance, providing a potential solution. However,SAM struggles with geographical images, such as aerial and satellite imagery,due to its training being confined to natural images and the narrow featuresand textures of these objects blending into their surroundings. To addressthese challenges, we propose Geographical SAM (GeoSAM), a SAM-based frameworkthat fine-tunes SAM with automatically generated multi-modal prompts, combiningpoint prompts from a pre-trained task-specific model as primary visual guidanceand text prompts from a large language model as secondary semantic guidance toenhance model comprehension. GeoSAM outperforms existing approaches formobility infrastructure segmentation in both familiar and completely unseenregions by at least 5\% in mIoU, representing a significant leap in leveragingfoundation models to segment mobility infrastructure, including both road andpedestrian infrastructure in geographical images. The source code can be foundin this GitHub Repository: https://github.com/rafiibnsultan/GeoSAM.</description><author>Rafi Ibn Sultan, Chengyin Li, Hui Zhu, Prashant Khanduri, Marco Brocanelli, Dongxiao Zhu</author><pubDate>Mon, 09 Dec 2024 17:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11319v3</guid></item><item><title>FedSynthCT-Brain: A Federated Learning Framework for Multi-Institutional Brain MRI-to-CT Synthesis</title><link>http://arxiv.org/abs/2412.06690v1</link><description>The generation of Synthetic Computed Tomography (sCT) images has become apivotal methodology in modern clinical practice, particularly in the context ofRadiotherapy (RT) treatment planning. The use of sCT enables the calculation ofdoses, pushing towards Magnetic Resonance Imaging (MRI) guided radiotherapytreatments. Moreover, with the introduction of MRI-Positron Emission Tomography(PET) hybrid scanners, the derivation of sCT from MRI can improve theattenuation correction of PET images. Deep learning methods for MRI-to-sCT haveshown promising results, but their reliance on single-centre training datasetlimits generalisation capabilities to diverse clinical settings. Moreover,creating centralised multicentre datasets may pose privacy concerns. To solve the issues, this study introduces FedSynthCT-Brain, a frameworkbased on the Federated Learning (FL) paradigm for MRI-to-sCT in brain imaging. We reproduced a federation across four European and American centres using aU-Net-based model. The approach was implemented using data from centresbelonging the federation and it was tested on an unseen dataset from a centreoutside the federation. In the case of the unseen centre, the federated model achieved a median MeanAbsolute Error (MAE) of 102.0 HU across 23 patients, with an interquartilerange of 96.7-110.5 HU. The median (interquartile range) for the StructuralSimilarity Index (SSIM) and the Peak Signal to Noise Ratio (PNSR) were 0.89(0.86-0.89) and 26.58 (25.52-27.42), respectively. The analysis of the results showed acceptable performances of the federatedapproach, thus highlighting the potential of FL to enhance MRI-to-sCT toimprove generalisability and advancing safe and equitable clinical applicationswhile fostering collaboration and preserving data privacy.</description><author>Ciro Benito Raggio, Mathias Krohmer Zabaleta, Nils Skupien, Oliver Blanck, Francesco Cicone, Giuseppe Lucio Cascini, Paolo Zaffino, Lucia Migliorelli, Maria Francesca Spadea</author><pubDate>Mon, 09 Dec 2024 17:32:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06690v1</guid></item><item><title>Impact of Privacy Parameters on Deep Learning Models for Image Classification</title><link>http://arxiv.org/abs/2412.06689v1</link><description>The project aims to develop differentially private deep learning models forimage classification on CIFAR-10 datasets \cite{cifar10} and analyze the impactof various privacy parameters on model accuracy. We have implemented fivedifferent deep learning models, namely ConvNet, ResNet18, EfficientNet, ViT,and DenseNet121 and three supervised classifiers namely K-Nearest Neighbors,Naive Bayes Classifier and Support Vector Machine. We evaluated the performanceof these models under varying settings. Our best performing model to date isEfficientNet with test accuracy of $59.63\%$ with the following parameters(Adam optimizer, batch size 256, epoch size 100, epsilon value 5.0, learningrate $1e-3$, clipping threshold 1.0, and noise multiplier 0.912).</description><author>Basanta Chaulagain</author><pubDate>Mon, 09 Dec 2024 17:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06689v1</guid></item><item><title>FullStack Bench: Evaluating LLMs as Full Stack Coders</title><link>http://arxiv.org/abs/2412.00535v3</link><description>As the capabilities of code large language models (LLMs) continue to expand,their applications across diverse code intelligence domains are rapidlyincreasing. However, most existing datasets only evaluate limited applicationdomains. To address this gap, we have developed a comprehensive code evaluationdataset FullStack Bench focusing on full-stack programming, which encompasses awide range of application domains (e.g., basic programming, data analysis,software engineering, mathematics, and machine learning). Besides, to assessmultilingual programming capabilities, in FullStack Bench, we design real-worldinstructions and corresponding unit test cases from 16 widely-used programminglanguages to reflect real-world usage scenarios rather than simpletranslations. Moreover, we also release an effective code sandbox executiontool (i.e., SandboxFusion) supporting various programming languages andpackages to evaluate the performance of our FullStack Bench efficiently.Comprehensive experimental results on our FullStack Bench demonstrate thenecessity and effectiveness of our FullStack Bench and SandboxFusion.</description><author>Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, Z. Y. Peng, Shukai Liu, Zhaoxiang Zhang, Jing Mai, Ge Zhang, Wenhao Huang, Kai Shen, Liang Xiang</author><pubDate>Mon, 09 Dec 2024 17:31:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00535v3</guid></item><item><title>Some Best Practices in Operator Learning</title><link>http://arxiv.org/abs/2412.06686v1</link><description>Hyperparameters searches are computationally expensive. This paper studiessome general choices of hyperparameters and training methods specifically foroperator learning. It considers the architectures DeepONets, Fourier neuraloperators and Koopman autoencoders for several differential equations to findrobust trends. Some options considered are activation functions, dropout andstochastic weight averaging.</description><author>Dustin Enyeart, Guang Lin</author><pubDate>Mon, 09 Dec 2024 17:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06686v1</guid></item><item><title>DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation</title><link>http://arxiv.org/abs/2411.18562v2</link><description>Dexterous manipulation with contact-rich interactions is crucial for advancedrobotics. While recent diffusion-based planning approaches show promise forsimpler manipulation tasks, they often produce unrealistic ghost states (e.g.,the object automatically moves without hand contact) or lack adaptability whenhandling complex sequential interactions. In this work, we introduceDexDiffuser, an interaction-aware diffusion planning framework for adaptivedexterous manipulation. DexDiffuser models joint state-action dynamics througha dual-phase diffusion process which consists of pre-interaction contactalignment and post-contact goal-directed control, enabling goal-adaptivegeneralizable dexterous manipulation. Additionally, we incorporate dynamicsmodel-based dual guidance and leverage large language models for automatedguidance function generation, enhancing generalizability for physicalinteractions and facilitating diverse goal adaptation through language cues.Experiments on physical interaction tasks such as door opening, pen and blockre-orientation, and hammer striking demonstrate DexDiffuser's effectiveness ongoals outside training distributions, achieving over twice the average successrate (59.2% vs. 29.5%) compared to existing methods. Our framework achieves70.0% success on 30-degree door opening, 40.0% and 36.7% on pen and blockhalf-side re-orientation respectively, and 46.7% on hammer nail half drive,highlighting its robustness and flexibility in contact-rich manipulation.</description><author>Zhixuan Liang, Yao Mu, Yixiao Wang, Tianxing Chen, Wenqi Shao, Wei Zhan, Masayoshi Tomizuka, Ping Luo, Mingyu Ding</author><pubDate>Mon, 09 Dec 2024 17:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18562v2</guid></item><item><title>Policy Agnostic RL: Offline RL and Online RL Fine-Tuning of Any Class and Backbone</title><link>http://arxiv.org/abs/2412.06685v1</link><description>Recent advances in learning decision-making policies can largely beattributed to training expressive policy models, largely via imitationlearning. While imitation learning discards non-expert data, reinforcementlearning (RL) can still learn from suboptimal data. However, instantiating RLtraining of a new policy class often presents a different challenge: most deepRL machinery is co-developed with assumptions on the policy class and backbone,resulting in poor performance when the policy class changes. For instance, SACutilizes a low-variance reparameterization policy gradient for Gaussianpolicies, but this is unstable for diffusion policies and intractable forautoregressive categorical policies. To address this issue, we develop anoffline RL and online fine-tuning approach called policy-agnostic RL (PA-RL)that can effectively train multiple policy classes, with varying architecturesand sizes. We build off the basic idea that a universal supervised learningloss can replace the policy improvement step in RL, as long as it is applied on"optimized" actions. To obtain these optimized actions, we first samplemultiple actions from a base policy, and run global optimization (i.e.,re-ranking multiple action samples using the Q-function) and local optimization(i.e., running gradient steps on an action sample) to maximize the critic onthese candidates. PA-RL enables fine-tuning diffusion and transformer policieswith either autoregressive tokens or continuous action outputs, at differentsizes, entirely via actor-critic RL. Moreover, PA-RL improves the performanceand sample-efficiency by up to 2 times compared to existing offline RL andonline fine-tuning methods. We show the first result that successfullyfine-tunes OpenVLA, a 7B generalist robot policy, autonomously with Cal-QL, anonline RL fine-tuning algorithm, improving from 40% to 70% in the real world in40 minutes.</description><author>Max Sobol Mark, Tian Gao, Georgia Gabriela Sampaio, Mohan Kumar Srirama, Archit Sharma, Chelsea Finn, Aviral Kumar</author><pubDate>Mon, 09 Dec 2024 17:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06685v1</guid></item><item><title>Exploring Critical Testing Scenarios for Decision-Making Policies: An LLM Approach</title><link>http://arxiv.org/abs/2412.06684v1</link><description>Recent years have witnessed surprising achievements of decision-makingpolicies across various fields, such as autonomous driving and robotics.Testing for decision-making policies is crucial with the existence of criticalscenarios that may threaten their reliability. Numerous research efforts havebeen dedicated to testing these policies. However, there are still significantchallenges, such as low testing efficiency and diversity due to the complexityof the policies and environments under test. Inspired by the remarkablecapabilities of large language models (LLMs), in this paper, we propose anLLM-driven online testing framework for efficiently testing decision-makingpolicies. The main idea is to employ an LLM-based test scenario generator tointelligently generate challenging test cases through contemplation andreasoning. Specifically, we first design a "generate-test-feedback" pipelineand apply templated prompt engineering to fully leverage the knowledge andreasoning abilities of LLMs. Then, we introduce a multi-scale scenariogeneration strategy to address the inherent challenges LLMs face in making fineadjustments, further enhancing testing efficiency. Finally, we evaluate theLLM-driven approach on five widely used benchmarks. The experimental resultsdemonstrate that our method significantly outperforms baseline approaches inuncovering both critical and diverse scenarios.</description><author>Weichao Xu, Huaxin Pei, Jingxuan Yang, Yuchen Shi, Yi Zhang</author><pubDate>Mon, 09 Dec 2024 17:27:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06684v1</guid></item><item><title>Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework</title><link>http://arxiv.org/abs/2412.06681v1</link><description>In transportation system demand modeling and simulation, agent-based modelsand microsimulations are current state-of-the-art approaches. However, existingagent-based models still have some limitations on behavioral realism andresource demand that limit their applicability. In this study, leveraging theemerging technology of large language models (LLMs) and LLM-based agents, wepropose a general LLM-agent-based modeling framework for transportationsystems. We argue that LLM agents not only possess the essential capabilitiesto function as agents but also offer promising solutions to overcome somelimitations of existing agent-based models. Our conceptual framework designclosely replicates the decision-making and interaction processes and traits ofhuman travelers within transportation networks, and we demonstrate that theproposed systems can meet critical behavioral criteria for decision-making andlearning behaviors using related studies and a demonstrative example of LLMagents' learning and adjustment in the bottleneck setting. Although furtherrefinement of the LLM-agent-based modeling framework is necessary, we believethat this approach has the potential to improve transportation system modelingand simulation.</description><author>Tianming Liu, Jirong Yang, Yafeng Yin</author><pubDate>Mon, 09 Dec 2024 17:24:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06681v1</guid></item><item><title>I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token</title><link>http://arxiv.org/abs/2412.06676v1</link><description>Large Language Models are known to capture real-world knowledge, allowingthem to excel in many downstream tasks. Despite recent advances, these modelsare still prone to what are commonly known as hallucinations, causing them toemit unwanted and factually incorrect text. In this work, we propose a novelcalibration method that can be used to combat hallucinations. We add a special[IDK] ("I don't know") token to the model's vocabulary and introduce anobjective function that shifts probability mass to the [IDK] token forincorrect predictions. This approach allows the model to express uncertainty inits output explicitly. We evaluate our proposed method across multiple modelarchitectures and factual downstream tasks. We find that models trained withour method are able to express uncertainty in places where they wouldpreviously make mistakes while suffering only a small loss of encodedknowledge. We further perform extensive ablation studies of multiple variationsof our approach and provide a detailed analysis of the precision-recalltradeoff of our method.</description><author>Roi Cohen, Konstantin Dobler, Eden Biran, Gerard de Melo</author><pubDate>Mon, 09 Dec 2024 17:13:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06676v1</guid></item><item><title>FLAASH: Flow-Attention Adaptive Semantic Hierarchical Fusion for Multi-Modal Tobacco Content Analysis</title><link>http://arxiv.org/abs/2410.19896v2</link><description>The proliferation of tobacco-related content on social media platforms posessignificant challenges for public health monitoring and intervention. Thispaper introduces a novel multi-modal deep learning framework namedFlow-Attention Adaptive Semantic Hierarchical Fusion (FLAASH) designed toanalyze tobacco-related video content comprehensively. FLAASH addresses thecomplexities of integrating visual and textual information in short-form videosby leveraging a hierarchical fusion mechanism inspired by flow network theory.Our approach incorporates three key innovations, including a flow-attentionmechanism that captures nuanced interactions between visual and textualmodalities, an adaptive weighting scheme that balances the contribution ofdifferent hierarchical levels, and a gating mechanism that selectivelyemphasizes relevant features. This multi-faceted approach enables FLAASH toeffectively process and analyze diverse tobacco-related content, from productshowcases to usage scenarios. We evaluate FLAASH on the Multimodal TobaccoContent Analysis Dataset (MTCAD), a large-scale collection of tobacco-relatedvideos from popular social media platforms. Our results demonstrate significantimprovements over existing methods, outperforming state-of-the-art approachesin classification accuracy, F1 score, and temporal consistency. The proposedmethod also shows strong generalization capabilities when tested on standardvideo question-answering datasets, surpassing current models. This workcontributes to the intersection of public health and artificial intelligence,offering an effective tool for analyzing tobacco promotion in digital media.</description><author>Naga VS Raviteja Chappa, Page Daniel Dobbs, Bhiksha Raj, Khoa Luu</author><pubDate>Mon, 09 Dec 2024 17:12:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19896v2</guid></item><item><title>EMOv2: Pushing 5M Vision Model Frontier</title><link>http://arxiv.org/abs/2412.06674v1</link><description>This work focuses on developing parameter-efficient and lightweight modelsfor dense predictions while trading off parameters, FLOPs, and performance. Ourgoal is to set up the new frontier of the 5M magnitude lightweight model onvarious downstream tasks. Inverted Residual Block (IRB) serves as theinfrastructure for lightweight CNNs, but no counterparts have been recognizedby attention-based design. Our work rethinks the lightweight infrastructure ofefficient IRB and practical components in Transformer from a unifiedperspective, extending CNN-based IRB to attention-based models and abstractinga one-residual Meta Mobile Block (MMBlock) for lightweight model design.Following neat but effective design criterion, we deduce a modern ImprovedInverted Residual Mobile Block (i2RMB) and improve a hierarchical EfficientMOdel (EMOv2) with no elaborate complex structures. Considering theimperceptible latency for mobile users when downloading models under 4G/5Gbandwidth and ensuring model performance, we investigate the performance upperlimit of lightweight models with a magnitude of 5M. Extensive experiments onvarious vision recognition, dense prediction, and image generation tasksdemonstrate the superiority of our EMOv2 over state-of-the-art methods, e.g.,EMOv2-1M/2M/5M achieve 72.3, 75.8, and 79.4 Top-1 that surpass equal-orderCNN-/Attention-based models significantly. At the same time, EMOv2-5M equippedRetinaNet achieves 41.5 mAP for object detection tasks that surpasses theprevious EMO-5M by +2.6. When employing the more robust training recipe, ourEMOv2-5M eventually achieves 82.9 Top-1 accuracy, which elevates theperformance of 5M magnitude models to a new level. Code is available athttps://github.com/zhangzjn/EMOv2.</description><author>Jiangning Zhang, Teng Hu, Haoyang He, Zhucun Xue, Yabiao Wang, Chengjie Wang, Yong Liu, Xiangtai Li, Dacheng Tao</author><pubDate>Mon, 09 Dec 2024 17:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06674v1</guid></item><item><title>Distributed Thompson sampling under constrained communication</title><link>http://arxiv.org/abs/2410.15543v2</link><description>In Bayesian optimization, a black-box function is maximized via the use of asurrogate model. We apply distributed Thompson sampling, using a Gaussianprocess as a surrogate model, to approach the multi-agent Bayesian optimizationproblem. In our distributed Thompson sampling implementation, each agentreceives sampled points from neighbors, where the communication network isencoded in a graph; each agent utilizes their own Gaussian process to model theobjective function. We demonstrate theoretical bounds on Bayesian simple regretand Bayesian average regret, where the bound depends on the structure of thecommunication graph. Unlike in batch Bayesian optimization, this bound isapplicable in cases where the communication graph amongst agents isconstrained. When compared to sequential single-agent Thompson sampling, ourbound guarantees faster convergence with respect to time as long as thecommunication graph is connected. We confirm the efficacy of our algorithm withnumerical simulations on traditional optimization test functions, illustratingthe significance of graph connectivity on improving regret convergence.</description><author>Saba Zerefa, Zhaolin Ren, Haitong Ma, Na Li</author><pubDate>Mon, 09 Dec 2024 17:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15543v2</guid></item><item><title>ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance</title><link>http://arxiv.org/abs/2412.06673v1</link><description>In this paper, we introduce ILLUME, a unified multimodal large language model(MLLM) that seamlessly integrates multimodal understanding and generationcapabilities within a single large language model through a unified next-tokenprediction formulation. To address the large dataset size typically requiredfor image-text alignment, we propose to enhance data efficiency through thedesign of a vision tokenizer that incorporates semantic information and aprogressive multi-stage training procedure. This approach reduces the datasetsize to just 15M for pretraining -- over four times fewer than what istypically needed -- while achieving competitive or even superior performancewith existing unified MLLMs, such as Janus. Additionally, to promotesynergistic enhancement between understanding and generation capabilities,which is under-explored in previous works, we introduce a novel self-enhancingmultimodal alignment scheme. This scheme supervises the MLLM to self-assess theconsistency between text descriptions and self-generated images, facilitatingthe model to interpret images more accurately and avoid unrealistic andincorrect predictions caused by misalignment in image generation. Based onextensive experiments, our proposed ILLUME stands out and competes withstate-of-the-art unified MLLMs and specialized models across various benchmarksfor multimodal understanding, generation, and editing.</description><author>Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, Hang Xu</author><pubDate>Mon, 09 Dec 2024 17:11:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06673v1</guid></item><item><title>Optimistic Query Routing in Clustering-based Approximate Maximum Inner Product Search</title><link>http://arxiv.org/abs/2405.12207v2</link><description>Clustering-based nearest neighbor search is an effective method in whichpoints are partitioned into geometric shards to form an index, with only a fewshards searched during query processing to find a set of top-$k$ vectors. Eventhough the search efficacy is heavily influenced by the algorithm thatidentifies the shards to probe, it has received little attention in theliterature. This work bridges that gap by studying routing in clustering-basedmaximum inner product search. We unpack existing routers and notice thesurprising contribution of optimism. We then take a page from the sequentialdecision making literature and formalize that insight following the principleof ``optimism in the face of uncertainty.'' In particular, we present aframework that incorporates the moments of the distribution of inner productswithin each shard to estimate the maximum inner product. We then present aninstance of our algorithm that uses only the first two moments to reach thesame accuracy as state-of-the-art routers such as ScaNN by probing up to $50\%$fewer points on benchmark datasets. Our algorithm is also space-efficient: wedesign a sketch of the second moment whose size is independent of the number ofpoints and requires $\mathcal{O}(1)$ vectors per shard.</description><author>Sebastian Bruch, Aditya Krishnan, Franco Maria Nardini</author><pubDate>Mon, 09 Dec 2024 17:08:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12207v2</guid></item><item><title>Diff5T: Benchmarking Human Brain Diffusion MRI with an Extensive 5.0 Tesla K-Space and Spatial Dataset</title><link>http://arxiv.org/abs/2412.06666v1</link><description>Diffusion magnetic resonance imaging (dMRI) provides critical insights intothe microstructural and connectional organization of the human brain. However,the availability of high-field, open-access datasets that include raw k-spacedata for advanced research remains limited. To address this gap, we introduceDiff5T, a first comprehensive 5.0 Tesla diffusion MRI dataset focusing on thehuman brain. This dataset includes raw k-space data and reconstructed diffusionimages, acquired using a variety of imaging protocols. Diff5T is designed tosupport the development and benchmarking of innovative methods in artifactcorrection, image reconstruction, image preprocessing, diffusion modelling andtractography. The dataset features a wide range of diffusion parameters,including multiple b-values and gradient directions, allowing extensiveresearch applications in studying human brain microstructure and connectivity.With its emphasis on open accessibility and detailed benchmarks, Diff5T servesas a valuable resource for advancing human brain mapping research usingdiffusion MRI, fostering reproducibility, and enabling collaboration across theneuroscience and medical imaging communities.</description><author>Shanshan Wang, Shoujun Yu, Jian Cheng, Sen Jia, Changjun Tie, Jiayu Zhu, Haohao Peng, Yijing Dong, Jianzhong He, Fan Zhang, Yaowen Xing, Xiuqin Jia, Qi Yang, Qiyuan Tian, Hua Guo, Guobin Li, Hairong Zheng</author><pubDate>Mon, 09 Dec 2024 17:04:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06666v1</guid></item><item><title>Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing Image Segmentation</title><link>http://arxiv.org/abs/2412.06664v1</link><description>Fine-grained remote sensing image segmentation is essential for accuratelyidentifying detailed objects in remote sensing images. Recently, visiontransformer models (VTM) pretrained on large-scale datasets have shown strongzero-shot generalization, indicating that they have learned the generalknowledge of object understanding. We introduce a novel end-to-end learningparadigm combining knowledge guidance with domain refinement to enhanceperformance. We present two key components: the Feature Alignment Module (FAM)and the Feature Modulation Module (FMM). FAM aligns features from a CNN-basedbackbone with those from the pretrained VTM's encoder using channeltransformation and spatial interpolation, and transfers knowledge via KLdivergence and L2 normalization constraint. FMM further adapts the knowledge tothe specific domain to address domain shift. We also introduce a fine-grainedgrass segmentation dataset and demonstrate, through experiments on twodatasets, that our method achieves a significant improvement of 2.57 mIoU onthe grass dataset and 3.73 mIoU on the cloud dataset. The results highlight thepotential of combining knowledge transfer and domain adaptation to overcomedomain-related challenges and data limitations. The project page is availableat https://xavierjiezou.github.io/KTDA/.</description><author>Shun Zhang, Xuechao Zou, Kai Li, Congyan Lang, Shiying Wang, Pin Tao, Tengfei Cao</author><pubDate>Mon, 09 Dec 2024 17:01:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06664v1</guid></item><item><title>Efficiency Meets Fidelity: A Novel Quantization Framework for Stable Diffusion</title><link>http://arxiv.org/abs/2412.06661v1</link><description>Text-to-image generation of Stable Diffusion models has achieved notablesuccess due to its remarkable generation ability. However, the repetitivedenoising process is computationally intensive during inference, which rendersDiffusion models less suitable for real-world applications that require lowlatency and scalability. Recent studies have employed post-trainingquantization (PTQ) and quantization-aware training (QAT) methods to compressDiffusion models. Nevertheless, prior research has often neglected to examinethe consistency between results generated by quantized models and those fromfloating-point models. This consistency is crucial in fields such as contentcreation, design, and edge deployment, as it can significantly enhance bothefficiency and system stability for practitioners. To ensure that quantizedmodels generate high-quality and consistent images, we propose an efficientquantization framework for Stable Diffusion models. Our approach features aSerial-to-Parallel calibration pipeline that addresses the consistency of boththe calibration and inference processes, as well as ensuring trainingstability. Based on this pipeline, we further introduce a mix-precisionquantization strategy, multi-timestep activation quantization, and timeinformation precalculation techniques to ensure high-fidelity generation incomparison to floating-point models. Through extensive experiments with StableDiffusion v1-4, v2-1, and XL 1.0, we have demonstrated that our methodoutperforms the current state-of-the-art techniques when tested on prompts fromthe COCO validation dataset and the Stable-Diffusion-Prompts dataset. UnderW4A8 quantization settings, our approach enhances both distribution similarityand visual similarity by 45%-60%.</description><author>Shuaiting Li, Juncan Deng, Zeyu Wang, Hong Gu, Kedong Xu, Haibin Shen, Kejie Huang</author><pubDate>Mon, 09 Dec 2024 17:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06661v1</guid></item><item><title>Break a Lag: Triple Exponential Moving Average for Enhanced Optimization</title><link>http://arxiv.org/abs/2306.01423v3</link><description>The performance of deep learning models is critically dependent onsophisticated optimization strategies. While existing optimizers have shownpromising results, many rely on first-order Exponential Moving Average (EMA)techniques, which often limit their ability to track complex gradient trendsaccurately. This fact can lead to a significant lag in trend identification andsuboptimal optimization, particularly in highly dynamic gradient behavior. Toaddress this fundamental limitation, we introduce Fast Adaptive MomentEstimation (FAME), a novel optimization technique that leverages the power ofTriple Exponential Moving Average. By incorporating an advanced trackingmechanism, FAME enhances responsiveness to data dynamics, mitigates trendidentification lag, and optimizes learning efficiency. Our comprehensiveevaluation encompasses different computer vision tasks including imageclassification, object detection, and semantic segmentation, integrating FAMEinto 30 distinct architectures ranging from lightweight CNNs to VisionTransformers. Through rigorous benchmarking against state-of-the-artoptimizers, FAME demonstrates superior accuracy and robustness. Notably, itoffers high scalability, delivering substantial improvements across diversemodel complexities, architectures, tasks, and benchmarks.</description><author>Roi Peleg, Yair Smadar, Teddy Lazebnik, Assaf Hoogi</author><pubDate>Mon, 09 Dec 2024 16:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01423v3</guid></item><item><title>Off-Policy Maximum Entropy RL with Future State and Action Visitation Measures</title><link>http://arxiv.org/abs/2412.06655v1</link><description>We introduce a new maximum entropy reinforcement learning framework based onthe distribution of states and actions visited by a policy. More precisely, anintrinsic reward function is added to the reward function of the Markovdecision process that shall be controlled. For each state and action, thisintrinsic reward is the relative entropy of the discounted distribution ofstates and actions (or features from these states and actions) visited duringthe next time steps. We first prove that an optimal exploration policy, whichmaximizes the expected discounted sum of intrinsic rewards, is also a policythat maximizes a lower bound on the state-action value function of the decisionprocess under some assumptions. We also prove that the visitation distributionused in the intrinsic reward definition is the fixed point of a contractionoperator. Following, we describe how to adapt existing algorithms to learn thisfixed point and compute the intrinsic rewards to enhance exploration. A newpractical off-policy maximum entropy reinforcement learning algorithm isfinally introduced. Empirically, exploration policies have good state-actionspace coverage, and high-performing control policies are computed efficiently.</description><author>Adrien Bolland, Gaspard Lambrechts, Damien Ernst</author><pubDate>Mon, 09 Dec 2024 16:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06655v1</guid></item><item><title>GEAR: A Simple GENERATE, EMBED, AVERAGE AND RANK Approach for Unsupervised Reverse Dictionary</title><link>http://arxiv.org/abs/2412.06654v1</link><description>Reverse Dictionary (RD) is the task of obtaining the most relevant word orset of words given a textual description or dictionary definition. Effective RDmethods have applications in accessibility, translation or writing supportsystems. Moreover, in NLP research we find RD to be used to benchmark textencoders at various granularities, as it often requires word, definition andsentence embeddings. In this paper, we propose a simple approach to RD thatleverages LLMs in combination with embedding models. Despite its simplicity,this approach outperforms supervised baselines in well studied RD datasets,while also showing less over-fitting. We also conduct a number of experimentson different dictionaries and analyze how different styles, registers andtarget audiences impact the quality of RD systems. We conclude that, onaverage, untuned embeddings alone fare way below an LLM-only baseline (althoughthey are competitive in highly technical dictionaries), but are crucial forboosting performance in combined methods.</description><author>Fatemah Almeman, Luis Espinosa-Anke</author><pubDate>Mon, 09 Dec 2024 16:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06654v1</guid></item><item><title>How transformers learn structured data: insights from hierarchical filtering</title><link>http://arxiv.org/abs/2408.15138v2</link><description>Understanding the learning process and the embedded computation intransformers is becoming a central goal for the development of interpretableAI. In the present study, we introduce a hierarchical filtering procedure forgenerative models of sequences on trees, allowing us to hand-tune the range ofpositional correlations in the data. Leveraging this controlled setting, weprovide evidence that vanilla encoder-only transformers can approximate theexact inference algorithm when trained on root classification and maskedlanguage modeling tasks, and study how this computation is discovered andimplemented. We find that correlations at larger distances, corresponding toincreasing layers of the hierarchy, are sequentially included by the networkduring training. Moreover, by comparing attention maps from models trained withvarying degrees of filtering and by probing the different encoder levels, wefind clear evidence of a reconstruction of correlations on successive lengthscales corresponding to the various levels of the hierarchy, which we relate toa plausible implementation of the exact inference algorithm within the samearchitecture.</description><author>Jerome Garnier-Brun, Marc Mézard, Emanuele Moscato, Luca Saglietti</author><pubDate>Mon, 09 Dec 2024 16:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15138v2</guid></item><item><title>Instructional Video Generation</title><link>http://arxiv.org/abs/2412.04189v2</link><description>Despite the recent strides in video generation, state-of-the-art methodsstill struggle with elements of visual detail. One particularly challengingcase is the class of egocentric instructional videos in which the intricatemotion of the hand coupled with a mostly stable and non-distracting environmentis necessary to convey the appropriate visual action instruction. To addressthese challenges, we introduce a new method for instructional video generation.Our diffusion-based method incorporates two distinct innovations. First, wepropose an automatic method to generate the expected region of motion, guidedby both the visual context and the action text. Second, we introduce a criticalhand structure loss to guide the diffusion model to focus on smooth andconsistent hand poses. We evaluate our method on augmented instructionaldatasets based on EpicKitchens and Ego4D, demonstrating significantimprovements over state-of-the-art methods in terms of instructional clarity,especially of the hand motion in the target region, across diverse environmentsand actions. Video results can be found on the project webpage:https://excitedbutter.github.io/Instructional-Video-Generation/</description><author>Yayuan Li, Zhi Cao, Jason J. Corso</author><pubDate>Mon, 09 Dec 2024 16:45:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04189v2</guid></item><item><title>Semantic Search and Recommendation Algorithm</title><link>http://arxiv.org/abs/2412.06649v1</link><description>This paper introduces a new semantic search algorithm that uses Word2Vec andAnnoy Index to improve the efficiency of information retrieval from largedatasets. The proposed approach addresses the limitations of traditional searchmethods by offering enhanced speed, accuracy, and scalability. Testing ondatasets up to 100GB demonstrates the method's effectiveness in processing vastamounts of data while maintaining high precision and performance.</description><author>Aryan Duhan, Aryan Singhal, Shourya Sharma, Neeraj, Arti MK</author><pubDate>Mon, 09 Dec 2024 16:43:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06649v1</guid></item><item><title>Can tweets predict article retractions? A comparison between human and LLM labelling</title><link>http://arxiv.org/abs/2403.16851v2</link><description>Quickly detecting problematic research articles is crucial to safeguardingthe integrity of scientific research. This study explores whether Twittermentions of retracted articles can signal potential problems with the articlesprior to their retraction, potentially serving as an early warning system forscholars. To investigate this, we analysed a dataset of 4,354 Twitter mentionsassociated with 504 retracted articles. The effectiveness of Twitter mentionsin predicting article retractions was evaluated by both manual and LargeLanguage Model (LLM) labelling. Manual labelling results indicated that 25.7%of tweets signalled problems before retraction. Using the manual labellingresults as the baseline, we found that LLMs (GPT-4o-mini, Gemini 1.5 Flash, andClaude-3.5-Haiku) outperformed lexicon-based sentiment analysis tools (e.g.,TextBlob) in detecting potential problems, suggesting that automatic detectionof problematic articles from social media using LLMs is technically feasible.Nevertheless, since only a small proportion of retracted articles (11.1%) werecriticised on Twitter prior to retraction, such automatic systems would detectonly a minority of problematic articles. Overall, this study offers insightsinto how social media data, coupled with emerging generative AI techniques, cansupport research integrity.</description><author>Er-Te Zheng, Hui-Zhen Fu, Mike Thelwall, Zhichao Fang</author><pubDate>Mon, 09 Dec 2024 16:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16851v2</guid></item><item><title>Extraction Propagation</title><link>http://arxiv.org/abs/2402.15883v4</link><description>Running backpropagation end to end on large neural networks is fraught withdifficulties like vanishing gradients and degradation. In this paper we presentan alternative architecture composed of many small neural networks thatinteract with one another. Instead of propagating gradients back through thearchitecture we propagate vector-valued messages computed via forward passes,which are then used to update the parameters. Currently the performance isconjectured as we are yet to implement the architecture. However, we do back itup with some theory. A previous version of this paper was entitled "Fusionencoder networks" and detailed a slightly different architecture.</description><author>Stephen Pasteris, Chris Hicks, Vasilios Mavroudis</author><pubDate>Mon, 09 Dec 2024 16:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15883v4</guid></item><item><title>Object Detection using Event Camera: A MoE Heat Conduction based Detector and A New Benchmark Dataset</title><link>http://arxiv.org/abs/2412.06647v1</link><description>Object detection in event streams has emerged as a cutting-edge researcharea, demonstrating superior performance in low-light conditions, scenarioswith motion blur, and rapid movements. Current detectors leverage spikingneural networks, Transformers, or convolutional neural networks as their corearchitectures, each with its own set of limitations including restrictedperformance, high computational overhead, or limited local receptive fields.This paper introduces a novel MoE (Mixture of Experts) heat conduction-basedobject detection algorithm that strikingly balances accuracy and computationalefficiency. Initially, we employ a stem network for event data embedding,followed by processing through our innovative MoE-HCO blocks. Each blockintegrates various expert modules to mimic heat conduction within eventstreams. Subsequently, an IoU-based query selection module is utilized forefficient token extraction, which is then channeled into a detection head forthe final object detection process. Furthermore, we are pleased to introduceEvDET200K, a novel benchmark dataset for event-based object detection. Capturedwith a high-definition Prophesee EVK4-HD event camera, this dataset encompasses10 distinct categories, 200,000 bounding boxes, and 10,054 samples, eachspanning 2 to 5 seconds. We also provide comprehensive results from over 15state-of-the-art detectors, offering a solid foundation for future research andcomparison. The source code of this paper will be released on:https://github.com/Event-AHU/OpenEvDET</description><author>Xiao Wang, Yu Jin, Wentao Wu, Wei Zhang, Lin Zhu, Bo Jiang, Yonghong Tian</author><pubDate>Mon, 09 Dec 2024 16:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06647v1</guid></item><item><title>The Narrow Gate: Localized Image-Text Communication in Vision-Language Models</title><link>http://arxiv.org/abs/2412.06646v1</link><description>Recent advances in multimodal training have significantly improved theintegration of image understanding and generation within a unified model. Thisstudy investigates how vision-language models (VLMs) handle image-understandingtasks, specifically focusing on how visual information is processed andtransferred to the textual domain. We compare VLMs that generate both imagesand text with those that output only text, highlighting key differences ininformation flow. We find that in models with multimodal outputs, image andtext embeddings are more separated within the residual stream. Additionally,models vary in how information is exchanged from visual to textual tokens. VLMsthat only output text exhibit a distributed communication pattern, whereinformation is exchanged through multiple image tokens. In contrast, modelstrained for image and text generation rely on a single token that acts as anarrow gate for the visual information. We demonstrate that ablating thissingle token significantly deteriorates performance on image understandingtasks. Furthermore, modifying this token enables effective steering of theimage semantics, showing that targeted, local interventions can reliablycontrol the model's global behavior.</description><author>Alessandro Serra, Francesco Ortu, Emanuele Panizon, Lucrezia Valeriani, Lorenzo Basile, Alessio Ansuini, Diego Doimo, Alberto Cazzaniga</author><pubDate>Mon, 09 Dec 2024 16:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06646v1</guid></item><item><title>Detecting Facial Image Manipulations with Multi-Layer CNN Models</title><link>http://arxiv.org/abs/2412.06643v1</link><description>The rapid evolution of digital image manipulation techniques posessignificant challenges for content verification, with models such as stablediffusion and mid-journey producing highly realistic, yet synthetic, imagesthat can deceive human perception. This research develops and evaluatesconvolutional neural networks (CNNs) specifically tailored for the detection ofthese manipulated images. The study implements a comparative analysis of threeprogressively complex CNN architectures, assessing their ability to classifyand localize manipulations across various facial image modifications.Regularization and optimization techniques were systematically incorporated toimprove feature extraction and performance. The results indicate that theproposed models achieve an accuracy of up to 76\% in distinguishing manipulatedimages from genuine ones, surpassing traditional approaches. This research notonly highlights the potential of CNNs in enhancing the robustness of digitalmedia verification tools, but also provides insights into effectivearchitectural adaptations and training strategies for low-computationenvironments. Future work will build on these findings by extending thearchitectures to handle more diverse manipulation techniques and integratingmulti-modal data for improved detection capabilities.</description><author>Alejandro Marco Montejano, Angela Sanchez Perez, Javier Barrachina, David Ortiz-Perez, Manuel Benavent-Lledo, Jose Garcia-Rodriguez</author><pubDate>Mon, 09 Dec 2024 16:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06643v1</guid></item><item><title>Class Balance Matters to Active Class-Incremental Learning</title><link>http://arxiv.org/abs/2412.06642v1</link><description>Few-Shot Class-Incremental Learning has shown remarkable efficacy inefficient learning new concepts with limited annotations. Nevertheless, theheuristic few-shot annotations may not always cover the most informativesamples, which largely restricts the capability of incremental learner. We aimto start from a pool of large-scale unlabeled data and then annotate the mostinformative samples for incremental learning. Based on this premise, this paperintroduces the Active Class-Incremental Learning (ACIL). The objective of ACILis to select the most informative samples from the unlabeled pool toeffectively train an incremental learner, aiming to maximize the performance ofthe resulting model. Note that vanilla active learning algorithms suffer fromclass-imbalanced distribution among annotated samples, which restricts theability of incremental learning. To achieve both class balance andinformativeness in chosen samples, we propose Class-Balanced Selection (CBS)strategy. Specifically, we first cluster the features of all unlabeled imagesinto multiple groups. Then for each cluster, we employ greedy selectionstrategy to ensure that the Gaussian distribution of the sampled featuresclosely matches the Gaussian distribution of all unlabeled features within thecluster. Our CBS can be plugged and played into those CIL methods which arebased on pretrained models with prompts tunning technique. Extensiveexperiments under ACIL protocol across five diverse datasets demonstrate thatCBS outperforms both random selection and other SOTA active learningapproaches. Code is publicly available at https://github.com/1170300714/CBS.</description><author>Zitong Huang, Ze Chen, Yuanze Li, Bowen Dong, Erjin Zhou, Yong Liu, Rick Siow Mong Goh, Chun-Mei Feng, Wangmeng Zuo</author><pubDate>Mon, 09 Dec 2024 16:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06642v1</guid></item><item><title>SigKAN: Signature-Weighted Kolmogorov-Arnold Networks for Time Series</title><link>http://arxiv.org/abs/2406.17890v2</link><description>We propose a novel approach that enhances multivariate function approximationusing learnable path signatures and Kolmogorov-Arnold networks (KANs). Weenhance the learning capabilities of these networks by weighting the valuesobtained by KANs using learnable path signatures, which capture importantgeometric features of paths. This combination allows for a more comprehensiveand flexible representation of sequential and temporal data. We demonstratethrough studies that our SigKANs with learnable path signatures perform betterthan conventional methods across a range of function approximation challenges.By leveraging path signatures in neural networks, this method offers intriguingopportunities to enhance performance in time series analysis and time seriesforecasting, among other fields.</description><author>Hugo Inzirillo, Remi Genet</author><pubDate>Mon, 09 Dec 2024 16:37:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17890v2</guid></item><item><title>How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis</title><link>http://arxiv.org/abs/2411.04105v3</link><description>Large language models (LLMs) have shown amazing performance on tasks thatrequire planning and reasoning. Motivated by this, we investigate the internalmechanisms that underpin a network's ability to perform complex logicalreasoning. We first construct a synthetic propositional logic problem thatserves as a concrete test-bed for network training and evaluation. Crucially,this problem demands nontrivial planning to solve. We perform our study on twofronts. First, we pursue an understanding of precisely how a three-layertransformer, trained from scratch and attains perfect test accuracy, solvesthis problem. We are able to identify certain "planning" and "reasoning"mechanisms in the network that necessitate cooperation between the attentionblocks to implement the desired logic. Second, we study how pretrained LLMs,namely Mistral-7B and Gemma-2-9B, solve this problem. We characterize theirreasoning circuits through causal intervention experiments, providing necessityand sufficiency evidence for the circuits. We find evidence suggesting that thetwo models' latent reasoning strategies are surprisingly similar, andhuman-like. Overall, our work systemically uncovers novel aspects of small andlarge transformers, and continues the study of how they plan and reason.</description><author>Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Xin Wang, Rina Panigrahy</author><pubDate>Mon, 09 Dec 2024 16:36:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04105v3</guid></item><item><title>Beyond Scalars: Concept-Based Alignment Analysis in Vision Transformers</title><link>http://arxiv.org/abs/2412.06639v1</link><description>Vision transformers (ViTs) can be trained using various learning paradigms,from fully supervised to self-supervised. Diverse training protocols oftenresult in significantly different feature spaces, which are usually comparedthrough alignment analysis. However, current alignment measures quantify thisrelationship in terms of a single scalar value, obscuring the distinctionsbetween common and unique features in pairs of representations that share thesame scalar alignment. We address this limitation by combining alignmentanalysis with concept discovery, which enables a breakdown of alignment intosingle concepts encoded in feature space. This fine-grained comparison revealsboth universal and unique concepts across different representations, as well asthe internal structure of concepts within each of them. Our methodologicalcontributions address two key prerequisites for concept-based alignment: 1) Fora description of the representation in terms of concepts that faithfullycapture the geometry of the feature space, we define concepts as the mostgeneral structure they can possibly form - arbitrary manifolds, allowing hiddenfeatures to be described by their proximity to these manifolds. 2) To measuredistances between concept proximity scores of two representations, we use ageneralized Rand index and partition it for alignment between pairs ofconcepts. We confirm the superiority of our novel concept definition foralignment analysis over existing linear baselines in a sanity check. Theconcept-based alignment analysis of representations from four different ViTsreveals that increased supervision correlates with a reduction in the semanticstructure of learned representations.</description><author>Johanna Vielhaben, Dilyara Bareeva, Jim Berend, Wojciech Samek, Nils Strodthoff</author><pubDate>Mon, 09 Dec 2024 16:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06639v1</guid></item><item><title>RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models</title><link>http://arxiv.org/abs/2412.02830v3</link><description>This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), aversatile extension to the mutual reasoning framework (rStar), aimed atenhancing reasoning accuracy and factual integrity across large language models(LLMs) for complex, knowledge-intensive tasks such as commonsense and medicalreasoning. RARE incorporates two innovative actions within the Monte Carlo TreeSearch (MCTS) framework: A6, which generates search queries based on theinitial problem statement, performs information retrieval using those queries,and augments reasoning with the retrieved data to formulate the final answer;and A7, which leverages information retrieval specifically for generatedsub-questions and re-answers these sub-questions with the relevant contextualinformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposedto replace the original discriminator, prioritizing reasoning paths that meethigh standards of factuality. Experimental results with LLaMA 3.1 show thatRARE enables open-source LLMs to achieve competitive performance with topopen-source models like GPT-4 and GPT-4o. This research establishes RARE as ascalable solution for improving LLMs in domains where logical coherence andfactual integrity are critical.</description><author>Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, Hong Yu</author><pubDate>Mon, 09 Dec 2024 16:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02830v3</guid></item><item><title>MAVias: Mitigate any Visual Bias</title><link>http://arxiv.org/abs/2412.06632v1</link><description>Mitigating biases in computer vision models is an essential step towards thetrustworthiness of artificial intelligence models. Existing bias mitigationmethods focus on a small set of predefined biases, limiting their applicabilityin visual datasets where multiple, possibly unknown biases exist. To addressthis limitation, we introduce MAVias, an open-set bias mitigation approachleveraging foundation models to discover spurious associations between visualattributes and target classes. MAVias first captures a wide variety of visualfeatures in natural language via a foundation image tagging model, and thenleverages a large language model to select those visual features defining thetarget class, resulting in a set of language-coded potential visual biases. Wethen translate this set of potential biases into vision-language embeddings andintroduce an in-processing bias mitigation approach to prevent the model fromencoding information related to them. Our experiments on diverse datasets,including CelebA, Waterbirds, ImageNet, and UrbanCars, show that MAViaseffectively detects and mitigates a wide range of biases in visual recognitiontasks outperforming current state-of-the-art.</description><author>Ioannis Sarridis, Christos Koutlis, Symeon Papadopoulos, Christos Diou</author><pubDate>Mon, 09 Dec 2024 16:23:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06632v1</guid></item><item><title>PolytopeWalk: Sparse MCMC Sampling over Polytopes</title><link>http://arxiv.org/abs/2412.06629v1</link><description>High dimensional sampling is an important computational tool in statisticsand other computational disciplines, with applications ranging from Bayesianstatistical uncertainty quantification, metabolic modeling in systems biologyto volume computation. We present $\textsf{PolytopeWalk}$, a new scalablePython library designed for uniform sampling over polytopes. The libraryprovides an end-to-end solution, which includes preprocessing algorithms suchas facial reduction and initialization methods. Six state-of-the-art MCMCalgorithms on polytopes are implemented, including the Dikin, Vaidya, and JohnWalk. Additionally, we introduce novel sparse constrained formulations of thesealgorithms, enabling efficient sampling from sparse polytopes of the form $K_2= \{x \in \mathbb{R}^d \ | \ Ax = b, x \succeq_k 0\}$. This implementationmaintains sparsity in $A$, ensuring scalability to high dimensional settings$(d &gt; 10^5)$. We demonstrate the improved sampling efficiency and per-iterationcost on both Netlib datasets and structured polytopes. $\textsf{PolytopeWalk}$is available at github.com/ethz-randomwalk/polytopewalk with documentation atpolytopewalk.readthedocs.io .</description><author>Benny Sun, Yuansi Chen</author><pubDate>Mon, 09 Dec 2024 16:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06629v1</guid></item><item><title>Fundus Image-based Visual Acuity Assessment with PAC-Guarantees</title><link>http://arxiv.org/abs/2412.06624v1</link><description>Timely detection and treatment are essential for maintaining eye health.Visual acuity (VA), which measures the clarity of vision at a distance, is acrucial metric for managing eye health. Machine learning (ML) techniques havebeen introduced to assist in VA measurement, potentially alleviatingclinicians' workloads. However, the inherent uncertainties in ML models makerelying solely on them for VA prediction less than ideal. The VA predictiontask involves multiple sources of uncertainty, requiring more robustapproaches. A promising method is to build prediction sets or intervals ratherthan point estimates, offering coverage guarantees through techniques likeconformal prediction and Probably Approximately Correct (PAC) prediction sets.Despite the potential, to date, these approaches have not been applied to theVA prediction task.To address this, we propose a method for deriving predictionintervals for estimating visual acuity from fundus images with a PAC guarantee.Our experimental results demonstrate that the PAC guarantees are upheld, withperformance comparable to or better than that of two prior works that do notprovide such guarantees.</description><author>Sooyong Jang, Kuk Jin Jang, Hyonyoung Choi, Yong-Seop Han, Seongjin Lee, Jin-hyun Kim, Insup Lee</author><pubDate>Mon, 09 Dec 2024 16:16:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06624v1</guid></item><item><title>Copyright-Protected Language Generation via Adaptive Model Fusion</title><link>http://arxiv.org/abs/2412.06619v1</link><description>The risk of language models reproducing copyrighted material from theirtraining data has led to the development of various protective measures. Amongthese, inference-time strategies that impose constraints via post-processinghave shown promise in addressing the complexities of copyright regulation.However, they often incur prohibitive computational costs or suffer fromperformance trade-offs. To overcome these limitations, we introduceCopyright-Protecting Model Fusion (CP-Fuse), a novel approach that combinesmodels trained on disjoint sets of copyrighted material during inference. Inparticular, CP-Fuse adaptively aggregates the model outputs to minimize thereproduction of copyrighted content, adhering to a crucial balancing propertythat prevents the regurgitation of memorized data. Through extensiveexperiments, we show that CP-Fuse significantly reduces the reproduction ofprotected material without compromising the quality of text and codegeneration. Moreover, its post-hoc nature allows seamless integration withother protective measures, further enhancing copyright safeguards. Lastly, weshow that CP-Fuse is robust against common techniques for extracting trainingdata.</description><author>Javier Abad, Konstantin Donhauser, Francesco Pinto, Fanny Yang</author><pubDate>Mon, 09 Dec 2024 16:13:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06619v1</guid></item><item><title>AI TrackMate: Finally, Someone Who Will Give Your Music More Than Just "Sounds Great!"</title><link>http://arxiv.org/abs/2412.06617v1</link><description>The rise of "bedroom producers" has democratized music creation, whilechallenging producers to objectively evaluate their work. To address this, wepresent AI TrackMate, an LLM-based music chatbot designed to provideconstructive feedback on music productions. By combining LLMs' inherent musicalknowledge with direct audio track analysis, AI TrackMate offersproduction-specific insights, distinguishing it from text-only approaches. Ourframework integrates a Music Analysis Module, an LLM-Readable Music Report, andMusic Production-Oriented Feedback Instruction, creating a plug-and-play,training-free system compatible with various LLMs and adaptable to futureadvancements. We demonstrate AI TrackMate's capabilities through an interactiveweb interface and present findings from a pilot study with a music producer. Bybridging AI capabilities with the needs of independent producers, AI TrackMateoffers on-demand analytical feedback, potentially supporting the creativeprocess and skill development in music production. This system addresses thegrowing demand for objective self-assessment tools in the evolving landscape ofindependent music production.</description><author>Yi-Lin Jiang, Chia-Ho Hsiung, Yen-Tung Yeh, Lu-Rong Chen, Bo-Yu Chen</author><pubDate>Mon, 09 Dec 2024 16:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06617v1</guid></item><item><title>Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection</title><link>http://arxiv.org/abs/2412.04455v2</link><description>Automatic detection and prevention of open-set failures are crucial inclosed-loop robotic systems. Recent studies often struggle to simultaneouslyidentify unexpected failures reactively after they occur and preventforeseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), anovel paradigm leveraging the vision-language model (VLM) for both open-setreactive and proactive failure detection. The core of our method is toformulate both tasks as a unified set of spatio-temporal constraintsatisfaction problems and use VLM-generated code to evaluate them for real-timemonitoring. To enhance the accuracy and efficiency of monitoring, we furtherintroduce constraint elements that abstract constraint-related entities ortheir parts into compact geometric elements. This approach offers greatergenerality, simplifies tracking, and facilitates constraint-aware visualprogramming by leveraging these elements as visual prompts. Experiments showthat CaM achieves a 28.7% higher success rate and reduces execution time by31.8% under severe disturbances compared to baselines across three simulatorsand a real-world setting. Moreover, CaM can be integrated with open-loopcontrol policies to form closed-loop systems, enabling long-horizon tasks incluttered scenes with dynamic environments.</description><author>Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, He Wang</author><pubDate>Mon, 09 Dec 2024 16:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04455v2</guid></item><item><title>Social Media Informatics for Sustainable Cities and Societies: An Overview of the Applications, associated Challenges, and Potential Solutions</title><link>http://arxiv.org/abs/2412.03600v2</link><description>In the modern world, our cities and societies face several technological andsocietal challenges, such as rapid urbanization, global warming &amp; climatechange, the digital divide, and social inequalities, increasing the need formore sustainable cities and societies. Addressing these challenges requires amultifaceted approach involving all the stakeholders, sustainable planning,efficient resource management, innovative solutions, and modern technologies.Like other modern technologies, social media informatics also plays its part indeveloping more sustainable and resilient cities and societies. Despite itslimitations, social media informatics has proven very effective in varioussustainable cities and society applications. In this paper, we review andanalyze the role of social media informatics in sustainable cities and societyby providing a detailed overview of its applications, associated challenges,and potential solutions. This work is expected to provide a baseline for futureresearch in the domain.</description><author>Jebran Khan, Kashif Ahmad, Senthil Kumar Jagatheesaperumal, Nasir Ahmad, Kyung-Ah Sohn</author><pubDate>Mon, 09 Dec 2024 16:06:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03600v2</guid></item><item><title>MVReward: Better Aligning and Evaluating Multi-View Diffusion Models with Human Preferences</title><link>http://arxiv.org/abs/2412.06614v1</link><description>Recent years have witnessed remarkable progress in 3D content generation.However, corresponding evaluation methods struggle to keep pace. Automaticapproaches have proven challenging to align with human preferences, and themixed comparison of text- and image-driven methods often leads to unfairevaluations. In this paper, we present a comprehensive framework to betteralign and evaluate multi-view diffusion models with human preferences. To beginwith, we first collect and filter a standardized image prompt set fromDALL$\cdot$E and Objaverse, which we then use to generate multi-view assetswith several multi-view diffusion models. Through a systematic ranking pipelineon these assets, we obtain a human annotation dataset with 16k expert pairwisecomparisons and train a reward model, coined MVReward, to effectively encodehuman preferences. With MVReward, image-driven 3D methods can be evaluatedagainst each other in a more fair and transparent manner. Building on this, wefurther propose Multi-View Preference Learning (MVP), a plug-and-playmulti-view diffusion tuning strategy. Extensive experiments demonstrate thatMVReward can serve as a reliable metric and MVP consistently enhances thealignment of multi-view diffusion models with human preferences.</description><author>Weitao Wang, Haoran Xu, Yuxiao Yang, Zhifang Liu, Jun Meng, Haoqian Wang</author><pubDate>Mon, 09 Dec 2024 16:05:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06614v1</guid></item><item><title>3D Spatial Understanding in MLLMs: Disambiguation and Evaluation</title><link>http://arxiv.org/abs/2412.06613v1</link><description>Multimodal Large Language Models (MLLMs) have made significant progress intasks such as image captioning and question answering. However, while thesemodels can generate realistic captions, they often struggle with providingprecise instructions, particularly when it comes to localizing anddisambiguating objects in complex 3D environments. This capability is criticalas MLLMs become more integrated with collaborative robotic systems. Inscenarios where a target object is surrounded by similar objects (distractors),robots must deliver clear, spatially-aware instructions to guide humanseffectively. We refer to this challenge as contextual object localization anddisambiguation, which imposes stricter constraints than conventional 3D densecaptioning, especially regarding ensuring target exclusivity. In response, wepropose simple yet effective techniques to enhance the model's ability tolocalize and disambiguate target objects. Our approach not only achievesstate-of-the-art performance on conventional metrics that evaluate sentencesimilarity, but also demonstrates improved 3D spatial understanding through 3Dvisual grounding model.</description><author>Chun-Peng Chang, Alain Pagani, Didier Stricker</author><pubDate>Mon, 09 Dec 2024 16:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06613v1</guid></item><item><title>APOLLO: SGD-like Memory, AdamW-level Performance</title><link>http://arxiv.org/abs/2412.05270v2</link><description>Large language models (LLMs) are notoriously memory-intensive duringtraining, particularly with the popular AdamW optimizer. This memory burdennecessitates using more or higher-end GPUs or reducing batch sizes, limitingtraining scalability and throughput. To address this, various memory-efficientoptimizers have been proposed to reduce optimizer memory usage. However, theyface critical challenges: (i) reliance on costly SVD operations; (ii)significant performance trade-offs compared to AdamW; and (iii) stillsubstantial optimizer memory overhead to maintain competitive performance. In this work, we identify that AdamW's learning rate adaptation rule can beeffectively coarsened as a structured learning rate update. Based on thisinsight, we propose Approximated Gradient Scaling for Memory-Efficient LLMOptimization (APOLLO), which approximates learning rate scaling using anauxiliary low-rank optimizer state based on pure random projection. Thisstructured learning rate update rule makes APOLLO highly tolerant to furthermemory reductions while delivering comparable pre-training performance. Evenits rank-1 variant, APOLLO-Mini, achieves superior pre-training performancecompared to AdamW with SGD-level memory costs. Extensive experiments demonstrate that the APOLLO series performs on-par withor better than AdamW, while achieving greater memory savings by nearlyeliminating the optimization states of AdamW. These savings provide significantsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GBsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved ModelScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs withoutsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-trainingLLaMA-7B on a single GPU using less than 12 GB of memory with weightquantization.</description><author>Hanqing Zhu, Zhenyu Zhang, Wenyan Cong, Xi Liu, Sem Park, Vikas Chandra, Bo Long, David Z. Pan, Zhangyang Wang, Jinwon Lee</author><pubDate>Mon, 09 Dec 2024 16:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05270v2</guid></item><item><title>Enhancing predictive imaging biomarker discovery through treatment effect analysis</title><link>http://arxiv.org/abs/2406.02534v2</link><description>Identifying predictive covariates, which forecast individual treatmenteffectiveness, is crucial for decision-making across different disciplines suchas personalized medicine. These covariates, referred to as biomarkers, areextracted from pre-treatment data, often within randomized controlled trials,and should be distinguished from prognostic biomarkers, which are independentof treatment assignment. Our study focuses on discovering predictive imagingbiomarkers, specific image features, by leveraging pre-treatment images touncover new causal relationships. Unlike labor-intensive approaches relying onhandcrafted features prone to bias, we present a novel task of directlylearning predictive features from images. We propose an evaluation protocol toassess a model's ability to identify predictive imaging biomarkers anddifferentiate them from purely prognostic ones by employing statistical testingand a comprehensive analysis of image feature attribution. We explore thesuitability of deep learning models originally developed for estimating theconditional average treatment effect (CATE) for this task, which have beenassessed primarily for their precision of CATE estimation while overlooking theevaluation of imaging biomarker discovery. Our proof-of-concept analysisdemonstrates the feasibility and potential of our approach in discovering andvalidating predictive imaging biomarkers from synthetic outcomes and real-worldimage datasets. Our code is available at\url{https://github.com/MIC-DKFZ/predictive_image_biomarker_analysis}.</description><author>Shuhan Xiao, Lukas Klein, Jens Petersen, Philipp Vollmuth, Paul F. Jaeger, Klaus H. Maier-Hein</author><pubDate>Mon, 09 Dec 2024 15:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02534v2</guid></item></channel></rss>