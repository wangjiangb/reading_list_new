<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 03 Nov 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>URAvatar: Universal Relightable Gaussian Codec Avatars</title><link>http://arxiv.org/abs/2410.24223v1</link><description>We present a new approach to creating photorealistic and relightable headavatars from a phone scan with unknown illumination. The reconstructed avatarscan be animated and relit in real time with the global illumination of diverseenvironments. Unlike existing approaches that estimate parametric reflectanceparameters via inverse rendering, our approach directly models learnableradiance transfer that incorporates global light transport in an efficientmanner for real-time rendering. However, learning such a complex lighttransport that can generalize across identities is non-trivial. A phone scan ina single environment lacks sufficient information to infer how the head wouldappear in general environments. To address this, we build a universalrelightable avatar model represented by 3D Gaussians. We train on hundreds ofhigh-quality multi-view human scans with controllable point lights.High-resolution geometric guidance further enhances the reconstruction accuracyand generalization. Once trained, we finetune the pretrained model on a phonescan using inverse rendering to obtain a personalized relightable avatar. Ourexperiments establish the efficacy of our design, outperforming existingapproaches while retaining real-time rendering capability.</description><author>Junxuan Li, Chen Cao, Gabriel Schwartz, Rawal Khirodkar, Christian Richardt, Tomas Simon, Yaser Sheikh, Shunsuke Saito</author><pubDate>Thu, 31 Oct 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24223v1</guid></item><item><title>Robust Gaussian Processes via Relevance Pursuit</title><link>http://arxiv.org/abs/2410.24222v1</link><description>Gaussian processes (GPs) are non-parametric probabilistic regression modelsthat are popular due to their flexibility, data efficiency, and well-calibrateduncertainty estimates. However, standard GP models assume homoskedasticGaussian noise, while many real-world applications are subject to non-Gaussiancorruptions. Variants of GPs that are more robust to alternative noise modelshave been proposed, and entail significant trade-offs between accuracy androbustness, and between computational requirements and theoretical guarantees.In this work, we propose and study a GP model that achieves robustness againstsparse outliers by inferring data-point-specific noise levels with a sequentialselection procedure maximizing the log marginal likelihood that we refer to asrelevance pursuit. We show, surprisingly, that the model can be parameterizedsuch that the associated log marginal likelihood is strongly concave in thedata-point-specific noise variances, a property rarely found in either robustregression objectives or GP marginal likelihoods. This in turn implies the weaksubmodularity of the corresponding subset selection problem, and thereby provesapproximation guarantees for the proposed algorithm. We compare the model'sperformance relative to other approaches on diverse regression and Bayesianoptimization tasks, including the challenging but common setting of sparsecorruptions of the labels within or close to the function range.</description><author>Sebastian Ament, Elizabeth Santorella, David Eriksson, Ben Letham, Maximilian Balandat, Eytan Bakshy</author><pubDate>Thu, 31 Oct 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24222v1</guid></item><item><title>EgoMimic: Scaling Imitation Learning via Egocentric Video</title><link>http://arxiv.org/abs/2410.24221v1</link><description>The scale and diversity of demonstration data required for imitation learningis a significant challenge. We present EgoMimic, a full-stack framework whichscales manipulation via human embodiment data, specifically egocentric humanvideos paired with 3D hand tracking. EgoMimic achieves this through: (1) asystem to capture human embodiment data using the ergonomic Project Ariaglasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gapto human data, (3) cross-domain data alignment techniques, and (4) an imitationlearning architecture that co-trains on human and robot data. Compared to priorworks that only extract high-level intent from human videos, our approachtreats human and robot data equally as embodied demonstration data and learns aunified policy from both data sources. EgoMimic achieves significantimprovement on a diverse set of long-horizon, single-arm and bimanualmanipulation tasks over state-of-the-art imitation learning methods and enablesgeneralization to entirely new scenes. Finally, we show a favorable scalingtrend for EgoMimic, where adding 1 hour of additional hand data issignificantly more valuable than 1 hour of additional robot data. Videos andadditional information can be found at https://egomimic.github.io/</description><author>Simar Kareer, Dhruv Patel, Ryan Punamiya, Pranay Mathur, Shuo Cheng, Chen Wang, Judy Hoffman, Danfei Xu</author><pubDate>Thu, 31 Oct 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24221v1</guid></item><item><title>Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning</title><link>http://arxiv.org/abs/2410.24219v1</link><description>Despite advancements in Text-to-Video (T2V) generation, producing videos withrealistic motion remains challenging. Current models often yield static orminimally dynamic outputs, failing to capture complex motions described bytext. This issue stems from the internal biases in text encoding, whichoverlooks motions, and inadequate conditioning mechanisms in T2V generationmodels. To address this, we propose a novel framework called DEcomposed MOtion(DEMO), which enhances motion synthesis in T2V generation by decomposing bothtext encoding and conditioning into content and motion components. Our methodincludes a content encoder for static elements and a motion encoder fortemporal dynamics, alongside separate content and motion conditioningmechanisms. Crucially, we introduce text-motion and video-motion supervision toimprove the model's understanding and generation of motion. Evaluations onbenchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBenchdemonstrate DEMO's superior ability to produce videos with enhanced motiondynamics while maintaining high visual quality. Our approach significantlyadvances T2V generation by integrating comprehensive motion understandingdirectly from textual descriptions. Project page:https://PR-Ryan.github.io/DEMO-project/</description><author>Penghui Ruan, Pichao Wang, Divya Saxena, Jiannong Cao, Yuhui Shi</author><pubDate>Thu, 31 Oct 2024 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24219v1</guid></item><item><title>Bridging Geometric States via Geometric Diffusion Bridge</title><link>http://arxiv.org/abs/2410.24220v1</link><description>The accurate prediction of geometric state evolution in complex systems iscritical for advancing scientific domains such as quantum chemistry andmaterial modeling. Traditional experimental and computational methods facechallenges in terms of environmental constraints and computational demands,while current deep learning approaches still fall short in terms of precisionand generality. In this work, we introduce the Geometric Diffusion Bridge(GDB), a novel generative modeling framework that accurately bridges initialand target geometric states. GDB leverages a probabilistic approach to evolvegeometric state distributions, employing an equivariant diffusion bridgederived by a modified version of Doob's $h$-transform for connecting geometricstates. This tailored diffusion process is anchored by initial and targetgeometric states as fixed endpoints and governed by equivariant transitionkernels. Moreover, trajectory data can be seamlessly leveraged in our GDBframework by using a chain of equivariant diffusion bridges, providing a moredetailed and accurate characterization of evolution dynamics. Theoretically, weconduct a thorough examination to confirm our framework's ability to preservejoint distributions of geometric states and capability to completely model theunderlying dynamics inducing trajectory distributions with negligible error.Experimental evaluations across various real-world scenarios show that GDBsurpasses existing state-of-the-art approaches, opening up a new pathway foraccurately bridging geometric states and tackling crucial scientific challengeswith improved accuracy and applicability.</description><author>Shengjie Luo, Yixian Xu, Di He, Shuxin Zheng, Tie-Yan Liu, Liwei Wang</author><pubDate>Thu, 31 Oct 2024 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24220v1</guid></item><item><title>Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use</title><link>http://arxiv.org/abs/2410.24218v1</link><description>In real-world scenarios, it is desirable for embodied agents to have theability to leverage human language to gain explicit or implicit knowledge forlearning tasks. Despite recent progress, most previous approaches adopt simplelow-level instructions as language inputs, which may not reflect natural humancommunication. It's not clear how to incorporate rich language use tofacilitate task learning. To address this question, this paper studiesdifferent types of language inputs in facilitating reinforcement learning (RL)embodied agents. More specifically, we examine how different levels of languageinformativeness (i.e., feedback on past behaviors and future guidance) anddiversity (i.e., variation of language expressions) impact agent learning andinference. Our empirical results based on four RL benchmarks demonstrate thatagents trained with diverse and informative language feedback can achieveenhanced generalization and fast adaptation to new tasks. These findingshighlight the pivotal role of language use in teaching embodied agents newtasks in an open world. Project website:https://github.com/sled-group/Teachable_RL</description><author>Jiajun Xi, Yinong He, Jianing Yang, Yinpei Dai, Joyce Chai</author><pubDate>Thu, 31 Oct 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24218v1</guid></item><item><title>CaAdam: Improving Adam optimizer using connection aware methods</title><link>http://arxiv.org/abs/2410.24216v1</link><description>We introduce a new method inspired by Adam that enhances convergence speedand achieves better loss function minima. Traditional optimizers, includingAdam, apply uniform or globally adjusted learning rates across neural networkswithout considering their architectural specifics. This architecture-agnosticapproach is deeply embedded in most deep learning frameworks, where optimizersare implemented as standalone modules without direct access to the network'sstructural information. For instance, in popular frameworks like Keras orPyTorch, optimizers operate solely on gradients and parameters, withoutknowledge of layer connectivity or network topology. Our algorithm, CaAdam,explores this overlooked area by introducing connection-aware optimizationthrough carefully designed proxies of architectural information. We proposemultiple scaling methodologies that dynamically adjust learning rates based oneasily accessible structural properties such as layer depth, connection counts,and gradient distributions. This approach enables more granular optimizationwhile working within the constraints of current deep learning frameworks.Empirical evaluations on standard datasets (e.g., CIFAR-10, Fashion MNIST) showthat our method consistently achieves faster convergence and higher accuracycompared to standard Adam optimizer, demonstrating the potential benefits ofincorporating architectural awareness in optimization strategies.</description><author>Remi Genet, Hugo Inzirillo</author><pubDate>Thu, 31 Oct 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24216v1</guid></item><item><title>ARQ: A Mixed-Precision Quantization Framework for Accurate and Certifiably Robust DNNs</title><link>http://arxiv.org/abs/2410.24214v1</link><description>Mixed precision quantization has become an important technique for enablingthe execution of deep neural networks (DNNs) on limited resource computingplatforms. Traditional quantization methods have primarily concentrated onmaintaining neural network accuracy, either ignoring the impact of quantizationon the robustness of the network, or using only empirical techniques forimproving robustness. In contrast, techniques for robustness certification,which can provide strong guarantees about the robustness of DNNs have not beenused during quantization due to their high computation cost. This paper introduces ARQ, an innovative mixed-precision quantization methodthat not only preserves the clean accuracy of the smoothed classifiers but alsomaintains their certified robustness. ARQ uses reinforcement learning to findaccurate and robust DNN quantization, while efficiently leveraging randomizedsmoothing, a popular class of statistical DNN verification algorithms, to guidethe search process. We compare ARQ with multiple state-of-the-art quantization techniques onseveral DNN architectures commonly used in quantization studies: ResNet-20 onCIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. We demonstratethat ARQ consistently performs better than these baselines across all thebenchmarks and the input perturbation levels. In many cases, the performance ofARQ quantized networks can reach that of the original DNN with floating-pointweights, but with only 1.5% instructions.</description><author>Yuchen Yang, Shubham Ugare, Yifan Zhao, Gagandeep Singh, Sasa Misailovic</author><pubDate>Thu, 31 Oct 2024 17:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24214v1</guid></item><item><title>Learning Video Representations without Natural Videos</title><link>http://arxiv.org/abs/2410.24213v1</link><description>In this paper, we show that useful video representations can be learned fromsynthetic videos and natural images, without incorporating natural videos inthe training. We propose a progression of video datasets synthesized by simplegenerative processes, that model a growing set of natural video properties(e.g. motion, acceleration, and shape transformations). The downstreamperformance of video models pre-trained on these generated datasets graduallyincreases with the dataset progression. A VideoMAE model pre-trained on oursynthetic videos closes 97.2% of the performance gap on UCF101 actionclassification between training from scratch and self-supervised pre-trainingfrom natural videos, and outperforms the pre-trained model on HMDB51.Introducing crops of static images to the pre-training stage results in similarperformance to UCF101 pre-training and outperforms the UCF101 pre-trained modelon 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing thelow-level properties of the datasets, we identify correlations between framediversity, frame similarity to natural data, and downstream performance. Ourapproach provides a more controllable and transparent alternative to video datacuration processes for pre-training.</description><author>Xueyang Yu, Xinlei Chen, Yossi Gandelsman</author><pubDate>Thu, 31 Oct 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24213v1</guid></item><item><title>DELTA: Dense Efficient Long-range 3D Tracking for any video</title><link>http://arxiv.org/abs/2410.24211v1</link><description>Tracking dense 3D motion from monocular videos remains challenging,particularly when aiming for pixel-level precision over long sequences. Weintroduce \Approach, a novel method that efficiently tracks every pixel in 3Dspace, enabling accurate motion estimation across entire videos. Our approachleverages a joint global-local attention mechanism for reduced-resolutiontracking, followed by a transformer-based upsampler to achieve high-resolutionpredictions. Unlike existing methods, which are limited by computationalinefficiency or sparse tracking, \Approach delivers dense 3D tracking at scale,running over 8x faster than previous methods while achieving state-of-the-artaccuracy. Furthermore, we explore the impact of depth representation ontracking performance and identify log-depth as the optimal choice. Extensiveexperiments demonstrate the superiority of \Approach on multiple benchmarks,achieving new state-of-the-art results in both 2D and 3D dense tracking tasks.Our method provides a robust solution for applications requiring fine-grained,long-term motion tracking in 3D space.</description><author>Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, Chaoyang Wang</author><pubDate>Thu, 31 Oct 2024 17:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24211v1</guid></item><item><title>TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling</title><link>http://arxiv.org/abs/2410.24210v1</link><description>Deep learning architectures for supervised learning on tabular data rangefrom simple multilayer perceptrons (MLP) to sophisticated Transformers andretrieval-augmented methods. This study highlights a major, yet so faroverlooked opportunity for substantially improving tabular MLPs: namely,parameter-efficient ensembling -- a paradigm for implementing an ensemble ofmodels as one model producing multiple predictions. We start by developing TabM-- a simple model based on MLP and our variations of BatchEnsemble (an existingtechnique). Then, we perform a large-scale evaluation of tabular DLarchitectures on public benchmarks in terms of both task performance andefficiency, which renders the landscape of tabular DL in a new light.Generally, we show that MLPs, including TabM, form a line of stronger and morepractical models compared to attention- and retrieval-based architectures. Inparticular, we find that TabM demonstrates the best performance among tabularDL models. Lastly, we conduct an empirical analysis on the ensemble-like natureof TabM. For example, we observe that the multiple predictions of TabM are weakindividually, but powerful collectively. Overall, our work brings an impactfultechnique to tabular DL, analyses its behaviour, and advances theperformance-efficiency trade-off with TabM -- a simple and powerful baselinefor researchers and practitioners.</description><author>Yury Gorishniy, Akim Kotelnikov, Artem Babenko</author><pubDate>Thu, 31 Oct 2024 17:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24210v1</guid></item><item><title>NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking</title><link>http://arxiv.org/abs/2406.15349v2</link><description>Benchmarking vision-based driving policies is challenging. On one hand,open-loop evaluation with real data is easy, but these results do not reflectclosed-loop performance. On the other, closed-loop evaluation is possible insimulation, but is hard to scale due to its significant computational demands.Further, the simulators available today exhibit a large domain gap to realdata. This has resulted in an inability to draw clear conclusions from therapidly growing body of research on end-to-end autonomous driving. In thispaper, we present NAVSIM, a middle ground between these evaluation paradigms,where we use large datasets in combination with a non-reactive simulator toenable large-scale real-world benchmarking. Specifically, we gathersimulation-based metrics, such as progress and time to collision, by unrollingbird's eye view abstractions of the test scenes for a short simulation horizon.Our simulation is non-reactive, i.e., the evaluated policy and environment donot influence each other. As we demonstrate empirically, this decoupling allowsopen-loop metric computation while being better aligned with closed-loopevaluations than traditional displacement errors. NAVSIM enabled a newcompetition held at CVPR 2024, where 143 teams submitted 463 entries, resultingin several new insights. On a large set of challenging scenarios, we observethat simple methods with moderate compute requirements such as TransFuser canmatch recent large-scale end-to-end driving architectures such as UniAD. Ourmodular framework can potentially be extended with new datasets, data curationstrategies, and metrics, and will be continually maintained to host futurechallenges. Our code is available athttps://github.com/autonomousvision/navsim.</description><author>Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta</author><pubDate>Thu, 31 Oct 2024 17:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.15349v2</guid></item><item><title>No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images</title><link>http://arxiv.org/abs/2410.24207v1</link><description>We introduce NoPoSplat, a feed-forward model capable of reconstructing 3Dscenes parameterized by 3D Gaussians from \textit{unposed} sparse multi-viewimages. Our model, trained exclusively with photometric loss, achievesreal-time 3D Gaussian reconstruction during inference. To eliminate the needfor accurate pose input during reconstruction, we anchor one input view's localcamera coordinates as the canonical space and train the network to predictGaussian primitives for all views within this space. This approach obviates theneed to transform Gaussian primitives from local coordinates into a globalcoordinate system, thus avoiding errors associated with per-frame Gaussians andpose estimation. To resolve scale ambiguity, we design and compare variousintrinsic embedding methods, ultimately opting to convert camera intrinsicsinto a token embedding and concatenate it with image tokens as input to themodel, enabling accurate scene scale prediction. We utilize the reconstructed3D Gaussians for novel view synthesis and pose estimation tasks and propose atwo-stage coarse-to-fine pipeline for accurate pose estimation. Experimentalresults demonstrate that our pose-free approach can achieve superior novel viewsynthesis quality compared to pose-required methods, particularly in scenarioswith limited input image overlap. For pose estimation, our method, trainedwithout ground truth depth or explicit matching loss, significantly outperformsthe state-of-the-art methods with substantial improvements. This work makessignificant advances in pose-free generalizable 3D reconstruction anddemonstrates its applicability to real-world scenarios. Code and trained modelsare available at https://noposplat.github.io/.</description><author>Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, Songyou Peng</author><pubDate>Thu, 31 Oct 2024 17:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24207v1</guid></item><item><title>Understanding Optimization in Deep Learning with Central Flows</title><link>http://arxiv.org/abs/2410.24206v1</link><description>Optimization in deep learning remains poorly understood, even in the simplesetting of deterministic (i.e. full-batch) training. A key difficulty is thatmuch of an optimizer's behavior is implicitly determined by complex oscillatorydynamics, referred to as the "edge of stability." The main contribution of thispaper is to show that an optimizer's implicit behavior can be explicitlycaptured by a "central flow:" a differential equation which models thetime-averaged optimization trajectory. We show that these flows can empiricallypredict long-term optimization trajectories of generic neural networks with ahigh degree of numerical accuracy. By interpreting these flows, we reveal forthe first time 1) the precise sense in which RMSProp adapts to the local losslandscape, and 2) an "acceleration via regularization" mechanism, whereinadaptive optimizers implicitly navigate towards low-curvature regions in whichthey can take larger steps. This mechanism is key to the efficacy of theseadaptive optimizers. Overall, we believe that central flows constitute apromising tool for reasoning about optimization in deep learning.</description><author>Jeremy M. Cohen, Alex Damian, Ameet Talwalkar, Zico Kolter, Jason D. Lee</author><pubDate>Thu, 31 Oct 2024 17:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24206v1</guid></item><item><title>Zonal RL-RRT: Integrated RL-RRT Path Planning with Collision Probability and Zone Connectivity</title><link>http://arxiv.org/abs/2410.24205v1</link><description>Path planning in high-dimensional spaces poses significant challenges,particularly in achieving both time efficiency and a fair success rate. Toaddress these issues, we introduce a novel path-planning algorithm, ZonalRL-RRT, that leverages kd-tree partitioning to segment the map into zones whileaddressing zone connectivity, ensuring seamless transitions between zones. Bybreaking down the complex environment into multiple zones and using Q-learningas the high-level decision-maker, our algorithm achieves a 3x improvement intime efficiency compared to basic sampling methods such as RRT and RRT* inforest-like maps. Our approach outperforms heuristic-guided methods like BIT*and Informed RRT* by 1.5x in terms of runtime while maintaining robust andreliable success rates across 2D to 6D environments. Compared to learning-basedmethods like NeuralRRT* and MPNetSMP, as well as the heuristic RRT*J, ouralgorithm demonstrates, on average, 1.5x better performance in the sameenvironments. We also evaluate the effectiveness of our approach throughsimulations of the UR10e arm manipulator in the MuJoCo environment. A keyobservation of our approach lies in its use of zone partitioning andReinforcement Learning (RL) for adaptive high-level planning allowing thealgorithm to accommodate flexible policies across diverse environments, makingit a versatile tool for advanced path planning.</description><author>AmirMohammad Tahmasbi, MohammadSaleh Faghfoorian, Saeed Khodaygan, Aniket Bera</author><pubDate>Thu, 31 Oct 2024 17:57:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24205v1</guid></item><item><title>GeoSplatting: Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering</title><link>http://arxiv.org/abs/2410.24204v1</link><description>We consider the problem of physically-based inverse rendering using 3DGaussian Splatting (3DGS) representations. While recent 3DGS methods haveachieved remarkable results in novel view synthesis (NVS), accurately capturinghigh-fidelity geometry, physically interpretable materials and lighting remainschallenging, as it requires precise geometry modeling to provide accuratesurface normals, along with physically-based rendering (PBR) techniques toensure correct material and lighting disentanglement. Previous 3DGS methodsresort to approximating surface normals, but often struggle with noisy localgeometry, leading to inaccurate normal estimation and suboptimalmaterial-lighting decomposition. In this paper, we introduce GeoSplatting, anovel hybrid representation that augments 3DGS with explicit geometric guidanceand differentiable PBR equations. Specifically, we bridge isosurface and 3DGStogether, where we first extract isosurface mesh from a scalar field, thenconvert it into 3DGS points and formulate PBR equations for them in a fullydifferentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,enabling precise surface normal modeling, which facilitates the use of PBRframeworks for material decomposition. This approach further maintains theefficiency and quality of NVS from 3DGS while ensuring accurate geometry fromthe isosurface. Comprehensive evaluations across diverse datasets demonstratethe superiority of GeoSplatting, consistently outperforming existing methodsboth quantitatively and qualitatively.</description><author>Kai Ye, Chong Gao, Guanbin Li, Wenzheng Chen, Baoquan Chen</author><pubDate>Thu, 31 Oct 2024 17:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24204v1</guid></item><item><title>DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion</title><link>http://arxiv.org/abs/2410.24203v1</link><description>Diffusion-based methods have achieved remarkable achievements in 2D image or3D object generation, however, the generation of 3D scenes and even$360^{\circ}$ images remains constrained, due to the limited number of scenedatasets, the complexity of 3D scenes themselves, and the difficulty ofgenerating consistent multi-view images. To address these issues, we firstestablish a large-scale panoramic video-text dataset containing millions ofconsecutive panoramic keyframes with corresponding panoramic depths, cameraposes, and text descriptions. Then, we propose a novel text-driven panoramicgeneration framework, termed DiffPano, to achieve scalable, consistent, anddiverse panoramic scene generation. Specifically, benefiting from the powerfulgenerative capabilities of stable diffusion, we fine-tune a single-viewtext-to-panorama diffusion model with LoRA on the established panoramicvideo-text dataset. We further design a spherical epipolar-aware multi-viewdiffusion model to ensure the multi-view consistency of the generated panoramicimages. Extensive experiments demonstrate that DiffPano can generate scalable,consistent, and diverse panoramic images with given unseen text descriptionsand camera poses.</description><author>Weicai Ye, Chenhao Ji, Zheng Chen, Junyao Gao, Xiaoshui Huang, Song-Hai Zhang, Wanli Ouyang, Tong He, Cairong Zhao, Guofeng Zhang</author><pubDate>Thu, 31 Oct 2024 17:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24203v1</guid></item><item><title>P-Masking: Power Law Masking Improves Multi-attribute Controlled Generation</title><link>http://arxiv.org/abs/2410.24201v1</link><description>We introduce LingGen, a novel approach for controlled text generation thatoffers precise control over a wide array of linguistic attributes, even as thenumber of attributes varies. LingGen employs a dynamic P-MASKING strategy,which samples masking rates from a power law distribution during training. Thisinnovative approach enables the model to develop robust representations andadapt its attribute control capabilities across a variable number ofattributes, from a single attribute to multiple complex configurations. TheP-MASKING technique enhances LingGen's ability to manage different levels ofattribute visibility, resulting in superior performance in multi-attributegeneration tasks. Our experiments demonstrate that LingGen surpasses currentstate-of-the-art models in both attribute control accuracy and text fluency,particularly excelling in scenarios with varying attribute demands.Additionally, our ablation studies highlight the effectiveness of P-MASKING andthe influence of different base language models on performance. These findingsdemonstrate LingGen's potential for applications requiring precise andadaptable control over multiple linguistic attributes in text generation.</description><author>Mohamed Elgaar, Hadi Amiri</author><pubDate>Thu, 31 Oct 2024 17:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24201v1</guid></item><item><title>Length-Induced Embedding Collapse in Transformer-based Models</title><link>http://arxiv.org/abs/2410.24200v1</link><description>Text embeddings enable various applications, but their performancedeteriorates on longer texts. In this paper, we find that the performancedegradation is due to a phenomenon called Length Collapse, where longer textembeddings collapse into a narrow space. This collapse results in adistributional inconsistency between embeddings of different text lengths,ultimately hurting the performance of downstream tasks. Theoretically, byconsidering the self-attention mechanism inherently functions as a low-passfilter, we prove that long sequences increase the attenuation rate of thelow-pass filter effect of the self-attention mechanism. With layers goingdeeper, excessive low-pass filtering causes the token signals to retain onlytheir Direct-Current (DC) component, which means the input token feature mapswill collapse into a narrow space, especially in long texts. Based on the aboveanalysis, we propose to mitigate the undesirable length collapse limitation byintroducing a temperature in softmax(), which achieves a higher low-filterattenuation rate. The tuning-free method, called TempScale, can be plugged intomultiple transformer-based embedding models. Empirically, we demonstrate thatTempScale can improve existing embedding models, especially on long textinputs, bringing up to 0.53% performance gains on 40 datasets from Massive TextEmbedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets fromLongEmbed, which specifically focuses on long context retrieval.</description><author>Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu</author><pubDate>Thu, 31 Oct 2024 17:55:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24200v1</guid></item><item><title>Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation</title><link>http://arxiv.org/abs/2410.24199v1</link><description>We present a novel approach to paraphrase generation that enables precisecontrol and fine-tuning of 40 linguistic attributes for English. Our model isan encoder-decoder architecture that takes as input a source sentence anddesired linguistic attributes, and produces paraphrases of the source thatsatisfy the desired attributes. To guarantee high-quality outputs at inferencetime, our method is equipped with a quality control mechanism that graduallyadjusts the embedding of linguistic attributes to find the nearest and mostattainable configuration of desired attributes for paraphrase generation. Weevaluate the effectiveness of our method by comparing it to recent controllablegeneration models. Experimental results demonstrate that the proposed modeloutperforms baselines in generating paraphrases that satisfy desired linguisticattributes.</description><author>Mohamed Elgaar, Hadi Amiri</author><pubDate>Thu, 31 Oct 2024 17:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24199v1</guid></item><item><title>SelfCodeAlign: Self-Alignment for Code Generation</title><link>http://arxiv.org/abs/2410.24198v1</link><description>Instruction tuning is a supervised fine-tuning approach that significantlyimproves the ability of large language models (LLMs) to follow humaninstructions. We propose SelfCodeAlign, the first fully transparent andpermissive pipeline for self-aligning code LLMs without extensive humanannotations or distillation. SelfCodeAlign employs the same base model forinference throughout the data generation process. It first extracts diversecoding concepts from high-quality seed snippets to generate new tasks. It thensamples multiple responses per task, pairs each with test cases, and validatesthem in a sandbox environment. Finally, passing examples are selected forinstruction tuning. In our primary experiments, we use SelfCodeAlign withCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 onHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.Across all benchmarks, this finetuned model consistently outperforms theoriginal version trained with OctoPack, the previous state-of-the-art methodfor instruction tuning without human annotations or distillation. Additionally,we show that SelfCodeAlign is effective across LLMs of various sizes, from 3Bto 33B, and that the base models can benefit more from alignment with their owndata distribution. We further validate each component's effectiveness in ourpipeline, showing that SelfCodeAlign outperforms both direct distillation fromGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct andEvol-Instruct. SelfCodeAlign has also led to the creation ofStarCoder2-Instruct, the first fully transparent, permissively licensed, andself-aligned code LLM that achieves state-of-the-art coding performance.</description><author>Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang</author><pubDate>Thu, 31 Oct 2024 17:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24198v1</guid></item><item><title>Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters</title><link>http://arxiv.org/abs/2410.24190v1</link><description>How could LLMs influence our democracy? We investigate LLMs' politicalleanings and the potential influence of LLMs on voters by conducting multipleexperiments in a U.S. presidential election context. Through a votingsimulation, we first demonstrate 18 open- and closed-weight LLMs' politicalpreference for a Democratic nominee over a Republican nominee. We show how thisleaning towards the Democratic nominee becomes more pronounced ininstruction-tuned models compared to their base versions by analyzing theirresponses to candidate-policy related questions. We further explore thepotential impact of LLMs on voter choice by conducting an experiment with 935U.S. registered voters. During the experiments, participants interacted withLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment resultsshow a shift in voter choices towards the Democratic nominee following LLMinteraction, widening the voting margin from 0.7% to 4.6%, even though LLMswere not asked to persuade users to support the Democratic nominee during thediscourse. This effect is larger than many previous studies on thepersuasiveness of political campaigns, which have shown minimal effects inpresidential elections. Many users also expressed a desire for furtherpolitical interaction with LLMs. Which aspects of LLM interactions drove theseshifts in voter choice requires further study. Lastly, we explore how a safetymethod can make LLMs more politically neutral, while leaving some openquestions.</description><author>Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, Dawn Song</author><pubDate>Thu, 31 Oct 2024 17:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24190v1</guid></item><item><title>Chasing Better Deep Image Priors between Over- and Under-parameterization</title><link>http://arxiv.org/abs/2410.24187v1</link><description>Deep Neural Networks (DNNs) are well-known to act as over-parameterized deepimage priors (DIP) that regularize various image inverse problems. Meanwhile,researchers also proposed extremely compact, under-parameterized image priors(e.g., deep decoder) that are strikingly competent for image restoration too,despite a loss of accuracy. These two extremes push us to think whether thereexists a better solution in the middle: between over- and under-parameterizedimage priors, can one identify "intermediate" parameterized image priors thatachieve better trade-offs between performance, efficiency, and even preservingstrong transferability? Drawing inspirations from the lottery ticket hypothesis(LTH), we conjecture and study a novel "lottery image prior" (LIP) byexploiting DNN inherent sparsity, stated as: given an over-parameterizedDNN-based image prior, it will contain a sparse subnetwork that can be trainedin isolation, to match the original DNN's performance when being applied as aprior to various image inverse problems. Our results validate the superiorityof LIPs: we can successfully locate the LIP subnetworks from over-parameterizedDIPs at substantial sparsity ranges. Those LIP subnetworks significantlyoutperform deep decoders under comparably compact model sizes (by often fullypreserving the effectiveness of their over-parameterized counterparts), andthey also possess high transferability across different images as well asrestoration task types. Besides, we also extend LIP to compressive sensingimage reconstruction, where a pre-trained GAN generator is used as the prior(in contrast to untrained DIP or deep decoder), and confirm its validity inthis setting too. To our best knowledge, this is the first time that LTH isdemonstrated to be relevant in the context of inverse problems or image priors.</description><author>Qiming Wu, Xiaohan Chen, Yifan Jiang, Zhangyang Wang</author><pubDate>Thu, 31 Oct 2024 17:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24187v1</guid></item><item><title>DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning</title><link>http://arxiv.org/abs/2410.24185v1</link><description>Imitation learning from human demonstrations is an effective means to teachrobots manipulation skills. But data acquisition is a major bottleneck inapplying this paradigm more broadly, due to the amount of cost and human effortinvolved. There has been significant interest in imitation learning forbimanual dexterous robots, like humanoids. Unfortunately, data collection iseven more challenging here due to the challenges of simultaneously controllingmultiple arms and multi-fingered hands. Automated data generation in simulationis a compelling, scalable alternative to fuel this need for data. To this end,we introduce DexMimicGen, a large-scale automated data generation system thatsynthesizes trajectories from a handful of human demonstrations for humanoidrobots with dexterous hands. We present a collection of simulation environmentsin the setting of bimanual dexterous manipulation, spanning a range ofmanipulation behaviors and different requirements for coordination among thetwo arms. We generate 21K demos across these tasks from just 60 source humandemos and study the effect of several data generation and policy learningdecisions on agent performance. Finally, we present a real-to-sim-to-realpipeline and deploy it on a real-world humanoid can sorting task. Videos andmore are at https://dexmimicgen.github.io/</description><author>Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, Yuke Zhu</author><pubDate>Thu, 31 Oct 2024 17:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24185v1</guid></item><item><title>Quantized neural network for complex hologram generation</title><link>http://arxiv.org/abs/2409.06711v2</link><description>Computer-generated holography (CGH) is a promising technology for augmentedreality displays, such as head-mounted or head-up displays. However, its highcomputational demand makes it impractical for implementation. Recent efforts tointegrate neural networks into CGH have successfully accelerated computingspeed, demonstrating the potential to overcome the trade-off betweencomputational cost and image quality. Nevertheless, deploying neuralnetwork-based CGH algorithms on computationally limited embedded systemsrequires more efficient models with lower computational cost, memory footprint,and power consumption. In this study, we developed a lightweight model forcomplex hologram generation by introducing neural network quantization.Specifically, we built a model based on tensor holography and quantized it from32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Ourperformance evaluation shows that the proposed INT8 model achieves hologramquality comparable to that of the FP32 model while reducing the model size byapproximately 70% and increasing the speed fourfold. Additionally, weimplemented the INT8 model on a system-on-module to demonstrate itsdeployability on embedded platforms and high power efficiency.</description><author>Yutaka Endo, Minoru Oikawa, Timothy D. Wilkinson, Tomoyoshi Shimobaba, Tomoyoshi Ito</author><pubDate>Thu, 31 Oct 2024 17:48:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06711v2</guid></item><item><title>ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization</title><link>http://arxiv.org/abs/2406.04312v2</link><description>Text-to-Image (T2I) models have made significant advancements in recentyears, but they still struggle to accurately capture intricate detailsspecified in complex compositional prompts. While fine-tuning T2I models withreward objectives has shown promise, it suffers from "reward hacking" and maynot generalize well to unseen prompt distributions. In this work, we proposeReward-based Noise Optimization (ReNO), a novel approach that enhances T2Imodels at inference by optimizing the initial noise based on the signal fromone or multiple human preference reward models. Remarkably, solving thisoptimization problem with gradient ascent for 50 iterations yields impressiveresults on four different one-step models across two competitive benchmarks,T2I-CompBench and GenEval. Within a computational budget of 20-50 seconds,ReNO-enhanced one-step models consistently surpass the performance of allcurrent open-source Text-to-Image models. Extensive user studies demonstratethat our model is preferred nearly twice as often compared to the popular SDXLmodel and is on par with the proprietary Stable Diffusion 3 with 8B parameters.Moreover, given the same computational resources, a ReNO-optimized one-stepmodel outperforms widely-used open-source models such as SDXL andPixArt-$\alpha$, highlighting the efficiency and effectiveness of ReNO inenhancing T2I model performance at inference time. Code is available athttps://github.com/ExplainableML/ReNO.</description><author>Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, Zeynep Akata</author><pubDate>Thu, 31 Oct 2024 17:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04312v2</guid></item><item><title>Invisible Image Watermarks Are Provably Removable Using Generative AI</title><link>http://arxiv.org/abs/2306.01953v3</link><description>Invisible watermarks safeguard images' copyrights by embedding hiddenmessages only detectable by owners. They also prevent people from misusingimages, especially those generated by AI models. We propose a family ofregeneration attacks to remove these invisible watermarks. The proposed attackmethod first adds random noise to an image to destroy the watermark and thenreconstructs the image. This approach is flexible and can be instantiated withmany existing image-denoising algorithms and pre-trained generative models suchas diffusion models. Through formal proofs and extensive empirical evaluations,we demonstrate that pixel-level invisible watermarks are vulnerable to thisregeneration attack. Our results reveal that, across four different pixel-levelwatermarking schemes, the proposed method consistently achieves superiorperformance compared to existing attack techniques, with lower detection ratesand higher image quality. However, watermarks that keep the image semanticallysimilar can be an alternative defense against our attacks. Our findingunderscores the need for a shift in research/industry emphasis from invisiblewatermarks to semantic-preserving watermarks. Code is available athttps://github.com/XuandongZhao/WatermarkAttacker</description><author>Xuandong Zhao, Kexun Zhang, Zihao Su, Saastha Vasan, Ilya Grishchenko, Christopher Kruegel, Giovanni Vigna, Yu-Xiang Wang, Lei Li</author><pubDate>Thu, 31 Oct 2024 17:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01953v3</guid></item><item><title>Group Crosscoders for Mechanistic Analysis of Symmetry</title><link>http://arxiv.org/abs/2410.24184v1</link><description>We introduce group crosscoders, an extension of crosscoders thatsystematically discover and analyse symmetrical features in neural networks.While neural networks often develop equivariant representations withoutexplicit architectural constraints, understanding these emergent symmetries hastraditionally relied on manual analysis. Group crosscoders automate thisprocess by performing dictionary learning across transformed versions of inputsunder a symmetry group. Applied to InceptionV1's mixed3b layer using thedihedral group $\mathrm{D}_{32}$, our method reveals several key insights:First, it naturally clusters features into interpretable families thatcorrespond to previously hypothesised feature types, providing more preciseseparation than standard sparse autoencoders. Second, our transform blockanalysis enables the automatic characterisation of feature symmetries,revealing how different geometric features (such as curves versus lines)exhibit distinct patterns of invariance and equivariance. These resultsdemonstrate that group crosscoders can provide systematic insights into howneural networks represent symmetry, offering a promising new tool formechanistic interpretability.</description><author>Liv Gorton</author><pubDate>Thu, 31 Oct 2024 17:47:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24184v1</guid></item><item><title>Extended Object Tracking and Classification based on Linear Splines</title><link>http://arxiv.org/abs/2410.24183v1</link><description>This paper introduces a framework based on linear splines for 2-dimensionalextended object tracking and classification. Unlike state of the art models,linear splines allow to represent extended objects whose contour is anarbitrarily complex curve. An exact likelihood is derived for the case in whichnoisy measurements can be scattered from any point on the contour of theextended object, while an approximate Monte Carlo likelihood is provided forthe case wherein scattering points can be anywhere, i.e. inside or on thecontour, on the object surface. Exploiting such likelihood to measure how wellthe observed data fit a given shape, a suitable estimator is developed. Theproposed estimator models the extended object in terms of a kinematic state,providing object position and orientation, along with a shape vector,characterizing object contour and surface. The kinematic state is estimated viaa nonlinear Kalman filter, while the shape vector is estimated via a Bayesianclassifier so that classification is implicitly solved during shape estimation.Numerical experiments are provided to assess, compared to state of the artextended object estimators, the effectiveness of the proposed one.</description><author>Matteo Tesori, Giorgio Battistelli, Luigi Chisci</author><pubDate>Thu, 31 Oct 2024 17:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24183v1</guid></item><item><title>Federated Black-Box Adaptation for Semantic Segmentation</title><link>http://arxiv.org/abs/2410.24181v1</link><description>Federated Learning (FL) is a form of distributed learning that allowsmultiple institutions or clients to collaboratively learn a global model tosolve a task. This allows the model to utilize the information from everyinstitute while preserving data privacy. However, recent studies show that thepromise of protecting the privacy of data is not upheld by existing methods andthat it is possible to recreate the training data from the differentinstitutions. This is done by utilizing gradients transferred between theclients and the global server during training or by knowing the modelarchitecture at the client end. In this paper, we propose a federated learningframework for semantic segmentation without knowing the model architecture nortransferring gradients between the client and the server, thus enabling betterprivacy preservation. We propose BlackFed - a black-box adaptation of neuralnetworks that utilizes zero order optimization (ZOO) to update the client modelweights and first order optimization (FOO) to update the server weights. Weevaluate our approach on several computer vision and medical imaging datasetsto demonstrate its effectiveness. To the best of our knowledge, this work isone of the first works in employing federated learning for segmentation, devoidof gradients or model information exchange. Code:https://github.com/JayParanjape/blackfed/tree/master</description><author>Jay N. Paranjape, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel</author><pubDate>Thu, 31 Oct 2024 17:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24181v1</guid></item><item><title>AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal Properties</title><link>http://arxiv.org/abs/2410.24178v1</link><description>Anomaly detection is widely used for identifying critical errors andsuspicious behaviors, but current methods lack interpretability. We leveragecommon properties of existing methods and recent advances in generative modelsto introduce counterfactual explanations for anomaly detection. Given an input,we generate its counterfactual as a diffusion-based repair that shows what anon-anomalous version should have looked like. A key advantage of this approachis that it enables a domain-independent formal specification of explainabilitydesiderata, offering a unified framework for generating and evaluatingexplanations. We demonstrate the effectiveness of our anomaly explainabilityframework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI)anomaly datasets. The code used for the experiments is accessible at:https://github.com/xjiae/arpro.</description><author>Xiayan Ji, Anton Xue, Eric Wong, Oleg Sokolsky, Insup Lee</author><pubDate>Thu, 31 Oct 2024 17:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24178v1</guid></item><item><title>DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models</title><link>http://arxiv.org/abs/2410.24177v1</link><description>Spoken language models (SLMs) have gained increasing attention withadvancements in text-based, decoder-only language models. SLMs process text andspeech, enabling simultaneous speech understanding and generation. This paperpresents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims toimprove speech tokenization by bridging audio signals and SLM tokens. DC-Spinextracts speaker-invariant tokens rich in phonetic information and resilient toinput variations, enhancing zero-shot SLM tasks and speech resynthesis. Wepropose a chunk-wise approach to enable streamable DC-Spin without retrainingand degradation. Comparisons of tokenization methods (self-supervised andneural audio codecs), model scalability, and downstream task proxies show thattokens easily modeled by an n-gram LM or aligned with phonemes offer strongperformance, providing insights for designing speech tokenizers for SLMs.</description><author>Heng-Jui Chang, Hongyu Gong, Changhan Wang, James Glass, Yu-An Chung</author><pubDate>Thu, 31 Oct 2024 17:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24177v1</guid></item><item><title>Constraint Back-translation Improves Complex Instruction Following of Large Language Models</title><link>http://arxiv.org/abs/2410.24175v1</link><description>Large language models (LLMs) struggle to follow instructions with complexconstraints in format, length, etc. Following the conventionalinstruction-tuning practice, previous works conduct post-training on complexinstruction-response pairs generated by feeding complex instructions toadvanced LLMs. However, even advanced LLMs cannot follow complex instructionswell, thus limiting the quality of generated data. In this work, we find thatexisting datasets inherently contain implicit complex constraints and propose anovel data generation technique, constraint back-translation. Specifically, wetake the high-quality instruction-response pairs in existing datasets and onlyadopt advanced LLMs to add complex constraints already met by the responses tothe instructions, which naturally reduces costs and data noise. In theexperiments, we adopt Llama3-70B-Instruct to back-translate constraints andcreate a high-quality complex instruction-response dataset, named CRAB. Wepresent that post-training on CRAB improves multiple backbone LLMs' complexinstruction-following ability, evaluated on extensive instruction-followingbenchmarks. We further find that constraint back-translation also serves as auseful auxiliary training objective in post-training. Our code, data, andmodels will be released to facilitate future research.</description><author>Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li</author><pubDate>Thu, 31 Oct 2024 17:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24175v1</guid></item><item><title>Novel Architecture for Distributed Travel Data Integration and Service Provision Using Microservices</title><link>http://arxiv.org/abs/2410.24174v1</link><description>This paper introduces a microservices architecture for the purpose ofenhancing the flexibility and performance of an airline reservation system. Thearchitectural design incorporates Redis cache technologies, two differentmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, andPostgreSQL). It also introduces authorization techniques, including securecommunication through OAuth2 and JWT which is essential with the management ofhigh-demand travel services. According to selected indicators, the architectureprovides an impressive level of data consistency at 99.5% and a latency of datapropagation of less than 75 ms allowing rapid and reliable intercommunicationbetween microservices. A system throughput of 1050 events per second wasachieved so that the acceptability level was maintained even during peak time.Redis caching reduced a 92% cache hit ratio on the database thereby loweringthe burden on the database and increasing the speed of response. Furtherimprovement of the systems scalability was done through the use of Docker andKubernetes which enabled services to be expanded horizontally to cope with thechanges in demand. The error rates were very low, at 0.2% further enhancing theefficiency of the system in handling real-time data integration. This approachis suggested to meet the specific needs of the airline reservation system. Itis secure, fast, scalable, all serving to improve the user experience as wellas the efficiency of operations. The low latency and high data integrationlevels and prevaiing efficient usage of the resources demonstrates thearchitecture ability to offer continued support in the ever growing high demandsituations.</description><author>Biman Barua, M. Shamim Kaiser</author><pubDate>Thu, 31 Oct 2024 17:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24174v1</guid></item><item><title>MoVA: Adapting Mixture of Vision Experts to Multimodal Context</title><link>http://arxiv.org/abs/2404.13046v2</link><description>As the key component in multimodal large language models (MLLMs), the abilityof the visual encoder greatly affects MLLM's understanding on diverse imagecontent. Although some large-scale pretrained vision encoders such as visionencoders in CLIP and DINOv2 have brought promising performance, we found thatthere is still no single vision encoder that can dominate various image contentunderstanding, e.g., the CLIP vision encoder leads to outstanding results ongeneral image understanding but poor performance on document or chart content.To alleviate the bias of CLIP vision encoder, we first delve into the inherentbehavior of different pre-trained vision encoders and then propose the MoVA, apowerful and novel MLLM, adaptively routing and fusing task-specific visionexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we designa context-aware expert routing strategy to dynamically select the most suitablevision experts according to the user instruction, input image, and expertise ofvision experts. This benefits from the powerful model function understandingability of the large language model (LLM). In the fine-grained stage, weelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) toextract and fuse task-specific knowledge from various experts. Thiscoarse-to-fine paradigm effectively leverages representations from expertsbased on multimodal context and model expertise, further enhancing thegeneralization ability. We conduct extensive experiments to evaluate theeffectiveness of the proposed approach. Without any bells and whistles, MoVAcan achieve significant performance gains over current state-of-the-art methodsin a wide range of challenging multimodal benchmarks.</description><author>Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, Yu Liu</author><pubDate>Thu, 31 Oct 2024 17:39:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13046v2</guid></item><item><title>The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains</title><link>http://arxiv.org/abs/2410.24169v1</link><description>Scaling has been critical in improving model performance and generalizationin machine learning. It involves how a model's performance changes withincreases in model size or input data, as well as how efficiently computationalresources are utilized to support this growth. Despite successes in otherareas, the study of scaling in Neural Network Interatomic Potentials (NNIPs)remains limited. NNIPs act as surrogate models for ab initio quantum mechanicalcalculations. The dominant paradigm here is to incorporate many physical domainconstraints into the model, such as rotational equivariance. We contend thatthese complex constraints inhibit the scaling ability of NNIPs, and are likelyto lead to performance plateaus in the long run. In this work, we take analternative approach and start by systematically studying NNIP scalingstrategies. Our findings indicate that scaling the model through attentionmechanisms is efficient and improves model expressivity. These insightsmotivate us to develop an NNIP architecture designed for scalability: theEfficiently Scaled Attention Interatomic Potential (EScAIP). EScAIP leverages amulti-head self-attention formulation within graph neural networks, applyingattention at the neighbor-level representations. Implemented withhighly-optimized attention GPU kernels, EScAIP achieves substantial gains inefficiency--at least 10x faster inference, 5x less memory usage--compared toexisting NNIPs. EScAIP also achieves state-of-the-art performance on a widerange of datasets including catalysts (OC20 and OC22), molecules (SPICE), andmaterials (MPTrj). We emphasize that our approach should be thought of as aphilosophy rather than a specific model, representing a proof-of-concept fordeveloping general-purpose NNIPs that achieve better expressivity throughscaling, and continue to scale efficiently with increased computationalresources and training data.</description><author>Eric Qu, Aditi S. Krishnapriyan</author><pubDate>Thu, 31 Oct 2024 17:35:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24169v1</guid></item><item><title>Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level</title><link>http://arxiv.org/abs/2403.04690v3</link><description>Neighborhood attention reduces the cost of self attention by restricting eachtoken's attention span to its nearest neighbors. This restriction,parameterized by a window size and dilation factor, draws a spectrum ofpossible attention patterns between linear projection and self attention.Neighborhood attention, and more generally sliding window attention patterns,have long been bounded by infrastructure, particularly in higher-rank spaces(2-D and 3-D), calling for the development of custom kernels, which have beenlimited in either functionality, or performance, if not both. In this work, weaim to massively improve upon existing infrastructure by providing two newmethods for implementing neighborhood attention. We first show thatneighborhood attention can be represented as a batched GEMM problem, similar tostandard attention, and implement it for 1-D and 2-D neighborhood attention.These kernels on average provide 895% and 272% improvement in full precisionruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhoodattention respectively. We find that aside from being heavily bound by memorybandwidth, certain inherent inefficiencies exist in all unfused implementationsof neighborhood attention, which in most cases undo their theoreticalefficiency gain. Motivated by the progress made into fused dot-productattention kernels, we developed fused neighborhood attention; an adaptation offused dot-product attention kernels that allow fine-grained control overattention across different spatial axes. Known for reducing the quadratic timecomplexity of self attention to a linear complexity, neighborhood attention cannow enjoy a reduced and constant memory footprint, and record-breaking halfprecision runtime. We observe that our fused implementation successfullycircumvents some of the unavoidable inefficiencies in unfusedimplementations...</description><author>Ali Hassani, Wen-Mei Hwu, Humphrey Shi</author><pubDate>Thu, 31 Oct 2024 17:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04690v3</guid></item><item><title>Ab Initio Structure Solutions from Nanocrystalline Powder Diffraction Data</title><link>http://arxiv.org/abs/2406.10796v2</link><description>A major challenge in materials science is the determination of the structureof nanometer sized objects. Here we present a novel approach that uses agenerative machine learning model based on diffusion processes that is trainedon 45,229 known structures. The model factors both the measured diffractionpattern as well as relevant statistical priors on the unit cell of atomiccluster structures. Conditioned only on the chemical formula and theinformation-scarce finite-size broadened powder diffraction pattern, we findthat our model, PXRDnet, can successfully solve simulated nanocrystals as smallas 10 angstroms across 200 materials of varying symmetry and complexity,including structures from all seven crystal systems. We show that our model cansuccessfully and verifiably determine structural candidates four out of fivetimes, with average error among these candidates being only 7% (as measured bypost-Rietveld refinement R-factor). Furthermore, PXRDnet is capable of solvingstructures from noisy diffraction patterns gathered in real-world experiments.We suggest that data driven approaches, bootstrapped from theoreticalsimulation, will ultimately provide a path towards determining the structure ofpreviously unsolved nano-materials.</description><author>Gabe Guo, Tristan Saidi, Maxwell Terban, Michele Valsecchi, Simon JL Billinge, Hod Lipson</author><pubDate>Thu, 31 Oct 2024 17:29:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10796v2</guid></item><item><title>Approaches to human activity recognition via passive radar</title><link>http://arxiv.org/abs/2410.24166v1</link><description>The thesis explores novel methods for Human Activity Recognition (HAR) usingpassive radar with a focus on non-intrusive Wi-Fi Channel State Information(CSI) data. Traditional HAR approaches often use invasive sensors like camerasor wearables, raising privacy issues. This study leverages the non-intrusivenature of CSI, using Spiking Neural Networks (SNN) to interpret signalvariations caused by human movements. These networks, integrated with symbolicreasoning frameworks such as DeepProbLog, enhance the adaptability andinterpretability of HAR systems. SNNs offer reduced power consumption, idealfor privacy-sensitive applications. Experimental results demonstrate SNN-basedneurosymbolic models achieve high accuracy making them a promising alternativefor HAR across various domains.</description><author>Christian Bresciani, Federico Cerutti, Marco Cominelli</author><pubDate>Thu, 31 Oct 2024 17:28:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24166v1</guid></item><item><title>$_0$: A Vision-Language-Action Flow Model for General Robot Control</title><link>http://arxiv.org/abs/2410.24164v1</link><description>Robot learning holds tremendous promise to unlock the full potential offlexible, general, and dexterous robot systems, as well as to address some ofthe deepest questions in artificial intelligence. However, bringing robotlearning to the level of generality required for effective real-world systemsfaces major obstacles in terms of data, generalization, and robustness. In thispaper, we discuss how generalist robot policies (i.e., robot foundation models)can address these challenges, and how we can design effective generalist robotpolicies for complex and highly dexterous tasks. We propose a novel flowmatching architecture built on top of a pre-trained vision-language model (VLM)to inherit Internet-scale semantic knowledge. We then discuss how this modelcan be trained on a large and diverse dataset from multiple dexterous robotplatforms, including single-arm robots, dual-arm robots, and mobilemanipulators. We evaluate our model in terms of its ability to perform tasks inzero shot after pre-training, follow language instructions from people and froma high-level VLM policy, and its ability to acquire new skills via fine-tuning.Our results cover a wide variety of tasks, such as laundry folding, tablecleaning, and assembling boxes.</description><author>Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky</author><pubDate>Thu, 31 Oct 2024 17:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24164v1</guid></item><item><title>Models Can and Should Embrace the Communicative Nature of Human-Generated Math</title><link>http://arxiv.org/abs/2409.17005v2</link><description>Math is constructed by people for people: just as natural language corporareflect not just propositions but the communicative goals of language users,the math data that models are trained on reflects not just idealizedmathematical entities but rich communicative intentions. While there areimportant advantages to treating math in a purely symbolic manner, we herehypothesize that there are benefits to treating math as situated linguisticcommunication and that language models are well suited for this goal, in waysthat are not fully appreciated. We illustrate these points with two casestudies. First, we ran an experiment in which we found that language modelsinterpret the equals sign in a humanlike way -- generating systematicallydifferent word problems for the same underlying equation arranged in differentways. Second, we found that language models prefer proofs to be ordered innaturalistic ways, even though other orders would be logically equivalent. Weadvocate for AI systems that learn from and represent the communicativeintentions latent in human-generated math.</description><author>Sasha Boguraev, Ben Lipkin, Leonie Weissweiler, Kyle Mahowald</author><pubDate>Thu, 31 Oct 2024 17:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17005v2</guid></item><item><title>Conformalized Prediction of Post-Fault Voltage Trajectories Using Pre-trained and Finetuned Attention-Driven Neural Operators</title><link>http://arxiv.org/abs/2410.24162v1</link><description>This paper proposes a new data-driven methodology for predicting intervals ofpost-fault voltage trajectories in power systems. We begin by introducing theQuantile Attention-Fourier Deep Operator Network (QAF-DeepONet), designed tocapture the complex dynamics of voltage trajectories and reliably estimatequantiles of the target trajectory without any distributional assumptions. Theproposed operator regression model maps the observed portion of the voltagetrajectory to its unobserved post-fault trajectory. Our methodology employs apre-training and fine-tuning process to address the challenge of limited dataavailability. To ensure data privacy in learning the pre-trained model, we usemerging via federated learning with data from neighboring buses, enabling themodel to learn the underlying voltage dynamics from such buses without directlysharing their data. After pre-training, we fine-tune the model with data fromthe target bus, allowing it to adapt to unique dynamics and operatingconditions. Finally, we integrate conformal prediction into the fine-tunedmodel to ensure coverage guarantees for the predicted intervals. We evaluatedthe performance of the proposed methodology using the New England 39-bus testsystem considering detailed models of voltage and frequency controllers. Twometrics, Prediction Interval Coverage Probability (PICP) and PredictionInterval Normalized Average Width (PINAW), are used to numerically assess themodel's performance in predicting intervals. The results show that the proposedapproach offers practical and reliable uncertainty quantification in predictingthe interval of post-fault voltage trajectories.</description><author>Amirhossein Mollaali, Gabriel Zufferey, Gonzalo Constante-Flores, Christian Moya, Can Li, Guang Lin, Meng Yue</author><pubDate>Thu, 31 Oct 2024 17:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24162v1</guid></item><item><title>Redefining &lt;Creative&gt; in Dictionary: Towards a Enhanced Semantic Understanding of Creative Generation</title><link>http://arxiv.org/abs/2410.24160v1</link><description>Creativity, both in human and diffusion models, remains an inherentlyabstract concept; thus, simply adding "creative" to a prompt does not yieldreliable semantic recognition by the model. In this work, we concretize theabstract notion of "creative" through the TP2O task, which aims to merge twounrelated concepts, and introduce CreTok, redefining "creative" as the token$\texttt{&lt;CreTok&gt;}$. This redefinition offers a more concrete and universallyadaptable representation for concept blending. This redefinition occurscontinuously, involving the repeated random sampling of text pairs withdifferent concepts and optimizing cosine similarity between target and constantprompts. This approach enables $\texttt{&lt;CreTok&gt;}$ to learn a method forcreative concept fusion. Extensive experiments demonstrate that the creativecapability enabled by $\texttt{&lt;CreTok&gt;}$ substantially surpasses recent SOTAdiffusion models and achieves superior creative generation. CreTok exhibitsgreater flexibility and reduced time overhead, as $\texttt{&lt;CreTok&gt;}$ canfunction as a universal token for any concept, facilitating creative generationwithout retraining.</description><author>Fu Feng, Yucheng Xie, Jing Wang, Xin Geng</author><pubDate>Thu, 31 Oct 2024 17:19:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24160v1</guid></item><item><title>TSI-Bench: Benchmarking Time Series Imputation</title><link>http://arxiv.org/abs/2406.12747v2</link><description>Effective imputation is a crucial preprocessing step for time seriesanalysis. Despite the development of numerous deep learning algorithms for timeseries imputation, the community lacks standardized and comprehensive benchmarkplatforms to effectively evaluate imputation performance across differentsettings. Moreover, although many deep learning forecasting algorithms havedemonstrated excellent performance, whether their modelling achievements can betransferred to time series imputation tasks remains unexplored. To bridge thesegaps, we develop TSI-Bench, the first (to our knowledge) comprehensivebenchmark suite for time series imputation utilizing deep learning techniques.The TSI-Bench pipeline standardizes experimental settings to enable fairevaluation of imputation algorithms and identification of meaningful insightsinto the influence of domain-appropriate missing rates and patterns on modelperformance. Furthermore, TSI-Bench innovatively provides a systematic paradigmto tailor time series forecasting algorithms for imputation purposes. Ourextensive study across 34,804 experiments, 28 algorithms, and 8 datasets withdiverse missingness scenarios demonstrates TSI-Bench's effectiveness in diversedownstream tasks and potential to unlock future directions in time seriesimputation research and analysis. All source code and experiment logs arereleased at https://github.com/WenjieDu/AwesomeImputation.</description><author>Wenjie Du, Jun Wang, Linglong Qian, Yiyuan Yang, Zina Ibrahim, Fanxing Liu, Zepu Wang, Haoxin Liu, Zhiyuan Zhao, Yingjie Zhou, Wenjia Wang, Kaize Ding, Yuxuan Liang, B. Aditya Prakash, Qingsong Wen</author><pubDate>Thu, 31 Oct 2024 17:18:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12747v2</guid></item><item><title>GPT or BERT: why not both?</title><link>http://arxiv.org/abs/2410.24159v1</link><description>We present a simple way to merge masked language modeling with causallanguage modeling. This hybrid training objective results in a model thatcombines the strengths of both modeling paradigms within a single transformerstack: GPT-BERT can be transparently used like any standard causal or maskedlanguage model. We test the pretraining process that enables this flexiblebehavior on the BabyLM Challenge 2024. The results show that the hybridpretraining outperforms masked-only or causal-only models. We openly releasethe models, training corpora and code.</description><author>Lucas Georges Gabriel Charpentier, David Samuel</author><pubDate>Thu, 31 Oct 2024 17:18:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24159v1</guid></item><item><title>Visual place recognition for aerial imagery: A survey</title><link>http://arxiv.org/abs/2406.00885v2</link><description>Aerial imagery and its direct application to visual localization is anessential problem for many Robotics and Computer Vision tasks. While GlobalNavigation Satellite Systems (GNSS) are the standard default solution forsolving the aerial localization problem, it is subject to a number oflimitations, such as, signal instability or solution unreliability that makethis option not so desirable. Consequently, visual geolocalization is emergingas a viable alternative. However, adapting Visual Place Recognition (VPR) taskto aerial imagery presents significant challenges, including weather variationsand repetitive patterns. Current VPR reviews largely neglect the specificcontext of aerial data. This paper introduces a methodology tailored forevaluating VPR techniques specifically in the domain of aerial imagery,providing a comprehensive assessment of various methods and their performance.However, we not only compare various VPR methods, but also demonstrate theimportance of selecting appropriate zoom and overlap levels when constructingmap tiles to achieve maximum efficiency of VPR algorithms in the case of aerialimagery. The code is available on our GitHub repository --https://github.com/prime-slam/aero-vloc.</description><author>Ivan Moskalenko, Anastasiia Kornilova, Gonzalo Ferrer</author><pubDate>Thu, 31 Oct 2024 17:12:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00885v2</guid></item><item><title>Debiasing Alternative Data for Credit Underwriting Using Causal Inference</title><link>http://arxiv.org/abs/2410.22382v2</link><description>Alternative data provides valuable insights for lenders to evaluate aborrower's creditworthiness, which could help expand credit access tounderserved groups and lower costs for borrowers. But some forms of alternativedata have historically been excluded from credit underwriting because it couldact as an illegal proxy for a protected class like race or gender, causingredlining. We propose a method for applying causal inference to a supervisedmachine learning model to debias alternative data so that it might be used forcredit underwriting. We demonstrate how our algorithm can be used against apublic credit dataset to improve model accuracy across different racial groups,while providing theoretically robust nondiscrimination guarantees.</description><author>Chris Lam</author><pubDate>Thu, 31 Oct 2024 17:12:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22382v2</guid></item><item><title>Thought Space Explorer: Navigating and Expanding Thought Space for Large Language Model Reasoning</title><link>http://arxiv.org/abs/2410.24155v1</link><description>Recent advances in large language models (LLMs) have demonstrated theirpotential in handling complex reasoning tasks, which are usually achieved byconstructing a thought chain to guide the model to solve the problem withmulti-step thinking. However, existing methods often remain confined topreviously explored solution spaces and thus overlook the critical blind spotwithin LLMs' cognitive range. To address these issues, we design the ThoughtSpace Explorer (TSE), a novel framework to expand and optimize thoughtstructures to guide LLMs to explore their blind spots of thinking. Bygenerating new reasoning steps and branches based on the original thoughtstructure with various designed strategies, TSE broadens the thought space andalleviates the impact of blind spots for LLM reasoning. Experimental results onmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We alsoconduct extensive analysis to understand how structured and expansive thoughtcan contribute to unleashing the potential of LLM reasoning capabilities.</description><author>Jinghan Zhang, Fengran Mo, Xiting Wang, Kunpeng Liu</author><pubDate>Thu, 31 Oct 2024 17:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24155v1</guid></item><item><title>Dense Associative Memory Through the Lens of Random Features</title><link>http://arxiv.org/abs/2410.24153v1</link><description>Dense Associative Memories are high storage capacity variants of the Hopfieldnetworks that are capable of storing a large number of memory patterns in theweights of the network of a given size. Their common formulations typicallyrequire storing each pattern in a separate set of synaptic weights, which leadsto the increase of the number of synaptic weights when new patterns areintroduced. In this work we propose an alternative formulation of this class ofmodels using random features, commonly used in kernel methods. In thisformulation the number of network's parameters remains fixed. At the same time,new memories can be added to the network by modifying existing weights. We showthat this novel network closely approximates the energy function and dynamicsof conventional Dense Associative Memories and shares their desirablecomputational properties.</description><author>Benjamin Hoover, Duen Horng Chau, Hendrik Strobelt, Parikshit Ram, Dmitry Krotov</author><pubDate>Thu, 31 Oct 2024 17:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24153v1</guid></item><item><title>Scaling Concept With Text-Guided Diffusion Models</title><link>http://arxiv.org/abs/2410.24151v1</link><description>Text-guided diffusion models have revolutionized generative tasks byproducing high-fidelity content from text descriptions. They have also enabledan editing paradigm where concepts can be replaced through text conditioning(e.g., a dog to a tiger). In this work, we explore a novel approach: instead ofreplacing a concept, can we enhance or suppress the concept itself? Through anempirical study, we identify a trend where concepts can be decomposed intext-guided diffusion models. Leveraging this insight, we introduceScalingConcept, a simple yet effective method to scale decomposed concepts upor down in real input without introducing new elements. To systematicallyevaluate our approach, we present the WeakConcept-10 dataset, where conceptsare imperfect and need to be enhanced. More importantly, ScalingConcept enablesa variety of novel zero-shot applications across image and audio domains,including tasks such as canonical pose generation and generative soundhighlighting or removal.</description><author>Chao Huang, Susan Liang, Yunlong Tang, Yapeng Tian, Anurag Kumar, Chenliang Xu</author><pubDate>Thu, 31 Oct 2024 17:09:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24151v1</guid></item><item><title>Exploring Vision Language Models for Facial Attribute Recognition: Emotion, Race, Gender, and Age</title><link>http://arxiv.org/abs/2410.24148v1</link><description>Technologies for recognizing facial attributes like race, gender, age, andemotion have several applications, such as surveillance, advertising content,sentiment analysis, and the study of demographic trends and social behaviors.Analyzing demographic characteristics based on images and analyzing facialexpressions have several challenges due to the complexity of humans' facialattributes. Traditional approaches have employed CNNs and various other deeplearning techniques, trained on extensive collections of labeled images. Whilethese methods demonstrated effective performance, there remains potential forfurther enhancements. In this paper, we propose to utilize vision languagemodels (VLMs) such as generative pre-trained transformer (GPT), GEMINI, largelanguage and vision assistant (LLAVA), PaliGemma, and Microsoft Florence2 torecognize facial attributes such as race, gender, age, and emotion from imageswith human faces. Various datasets like FairFace, AffectNet, and UTKFace havebeen utilized to evaluate the solutions. The results show that VLMs arecompetitive if not superior to traditional techniques. Additionally, we propose"FaceScanPaliGemma"--a fine-tuned PaliGemma model--for race, gender, age, andemotion recognition. The results show an accuracy of 81.1%, 95.8%, 80%, and59.4% for race, gender, age group, and emotion classification, respectively,outperforming pre-trained version of PaliGemma, other VLMs, and SotA methods.Finally, we propose "FaceScanGPT", which is a GPT-4o model to recognize theabove attributes when several individuals are present in the image using aprompt engineered for a person with specific facial and/or physical attributes.The results underscore the superior multitasking capability of FaceScanGPT todetect the individual's attributes like hair cut, clothing color, postures,etc., using only a prompt to drive the detection and recognition tasks.</description><author>Nouar AlDahoul, Myles Joshua Toledo Tan, Harishwar Reddy Kasireddy, Yasir Zaki</author><pubDate>Thu, 31 Oct 2024 17:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24148v1</guid></item><item><title>Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias</title><link>http://arxiv.org/abs/2406.17947v2</link><description>The variations between in-group and out-group speech (intergroup bias) aresubtle and could underlie many social phenomena like stereotype perpetuationand implicit bias. In this paper, we model the intergroup bias as a taggingtask on English sports comments from forums dedicated to fandom for NFL teams.We curate a unique dataset of over 6 million game-time comments from opposingperspectives (the teams in the game), each comment grounded in a non-linguisticdescription of the events that precipitated these comments (live winprobabilities for each team). Expert and crowd annotations justify modeling thebias through tagging of implicit and explicit referring expressions and revealthe rich, contextual understanding of language and the world required for thistask. For large-scale analysis of intergroup variation, we use LLMs forautomated tagging, and discover that some LLMs perform best when prompted withlinguistic descriptions of the win probability at the time of the comment,rather than numerical probability. Further, large-scale tagging of commentsusing LLMs uncovers linear variations in the form of referent across winprobabilities that distinguish in-group and out-group utterances. Code and dataare available at https://github.com/venkatasg/intergroup-nfl .</description><author>Venkata S Govindarajan, Matianyu Zang, Kyle Mahowald, David Beaver, Junyi Jessy Li</author><pubDate>Thu, 31 Oct 2024 17:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17947v2</guid></item><item><title>Conformal prediction of circular data</title><link>http://arxiv.org/abs/2410.24145v1</link><description>Split conformal prediction techniques are applied to regression problems withcircular responses by introducing a suitable conformity score, leading toprediction sets with adaptive arc length and finite-sample coverage guaranteesfor any circular predictive model under exchangeable data. Leveraging the highperformance of existing predictive models designed for linear responses, weanalyze a general projection procedure that converts any linear responseregression model into one suitable for circular responses. When random forestsserve as basis models in this projection procedure, we harness the out-of-bagdynamics to eliminate the necessity for a separate calibration sample in theconstruction of prediction sets. For synthetic and real datasets the resultingprojected random forests model produces more efficient out-of-bag conformalprediction sets, with shorter median arc length, when compared to the splitconformal prediction sets generated by two existing alternative models.</description><author>Paulo C. Marques F., Rinaldo Artes, Helton Graziadei</author><pubDate>Thu, 31 Oct 2024 17:05:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24145v1</guid></item><item><title>SPO: Sequential Monte Carlo Policy Optimisation</title><link>http://arxiv.org/abs/2402.07963v3</link><description>Leveraging planning during learning and decision-making is central to thelong-term development of intelligent agents. Recent works have successfullycombined tree-based search methods and self-play learning mechanisms to thisend. However, these methods typically face scaling challenges due to thesequential nature of their search. While practical engineering solutions canpartly overcome this, they often result in a negative impact on performance. Inthis paper, we introduce SPO: Sequential Monte Carlo Policy Optimisation, amodel-based reinforcement learning algorithm grounded within the ExpectationMaximisation (EM) framework. We show that SPO provides robust policyimprovement and efficient scaling properties. The sample-based search makes itdirectly applicable to both discrete and continuous action spaces withoutmodifications. We demonstrate statistically significant improvements inperformance relative to model-free and model-based baselines across bothcontinuous and discrete environments. Furthermore, the parallel nature of SPO'ssearch enables effective utilisation of hardware accelerators, yieldingfavourable scaling laws.</description><author>Matthew V Macfarlane, Edan Toledo, Donal Byrne, Paul Duckworth, Alexandre Laterre</author><pubDate>Thu, 31 Oct 2024 17:05:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07963v3</guid></item><item><title>HoloChrome: Polychromatic Illumination for Speckle Reduction in Holographic Near-Eye Displays</title><link>http://arxiv.org/abs/2410.24144v1</link><description>Holographic displays hold the promise of providing authentic depth cues,resulting in enhanced immersive visual experiences for near-eye applications.However, current holographic displays are hindered by speckle noise, whichlimits accurate reproduction of color and texture in displayed images. Wepresent HoloChrome, a polychromatic holographic display framework designed tomitigate these limitations. HoloChrome utilizes an ultrafast,wavelength-adjustable laser and a dual-Spatial Light Modulator (SLM)architecture, enabling the multiplexing of a large set of discrete wavelengthsacross the visible spectrum. By leveraging spatial separation in our dual-SLMsetup, we independently manipulate speckle patterns across multiplewavelengths. This novel approach effectively reduces speckle noise throughincoherent averaging achieved by wavelength multiplexing. Our method iscomplementary to existing speckle reduction techniques, offering a new pathwayto address this challenge. Furthermore, the use of polychromatic illuminationbroadens the achievable color gamut compared to traditional three-color primaryholographic displays. Our simulations and tabletop experiments validate that HoloChromesignificantly reduces speckle noise and expands the color gamut. Theseadvancements enhance the performance of holographic near-eye displays, movingus closer to practical, immersive next-generation visual experiences.</description><author>Florian Schiffers, Grace Kuo, Nathan Matsuda, Douglas Lanman, Oliver Cossairt</author><pubDate>Thu, 31 Oct 2024 17:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24144v1</guid></item><item><title>Don't Touch My Diacritics</title><link>http://arxiv.org/abs/2410.24140v1</link><description>The common practice of preprocessing text before feeding it into NLP modelsintroduces many decision points which have unintended consequences on modelperformance. In this opinion piece, we focus on the handling of diacritics intexts originating in many languages and scripts. We demonstrate, throughseveral case studies, the adverse effects of inconsistent encoding ofdiacritized characters and of removing diacritics altogether. We call on thecommunity to adopt simple but necessary steps across all models and toolkits inorder to improve handling of diacritized text and, by extension, increaseequity in multilingual NLP.</description><author>Kyle Gorman, Yuval Pinter</author><pubDate>Thu, 31 Oct 2024 17:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24140v1</guid></item><item><title>COSNet: A Novel Semantic Segmentation Network using Enhanced Boundaries in Cluttered Scenes</title><link>http://arxiv.org/abs/2410.24139v1</link><description>Automated waste recycling aims to efficiently separate the recyclable objectsfrom the waste by employing vision-based systems. However, the presence ofvarying shaped objects having different material types makes it a challengingproblem, especially in cluttered environments. Existing segmentation methodsperform reasonably on many semantic segmentation datasets by employingmulti-contextual representations, however, their performance is degraded whenutilized for waste object segmentation in cluttered scenarios. In addition,plastic objects further increase the complexity of the problem due to theirtranslucent nature. To address these limitations, we introduce an efficacioussegmentation network, named COSNet, that uses boundary cues along withmulti-contextual information to accurately segment the objects in clutteredscenes. COSNet introduces novel components including feature sharpening block(FSB) and boundary enhancement module (BEM) for enhancing the features andhighlighting the boundary information of irregular waste objects in clutteredenvironment. Extensive experiments on three challenging datasets includingZeroWaste-f, SpectralWaste, and ADE20K demonstrate the effectiveness of theproposed method. Our COSNet achieves a significant gain of 1.8% on ZeroWaste-fand 2.1% on SpectralWaste datasets respectively in terms of mIoU metric.</description><author>Muhammad Ali, Mamoona Javaid, Mubashir Noman, Mustansar Fiaz, Salman Khan</author><pubDate>Thu, 31 Oct 2024 17:03:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24139v1</guid></item><item><title>Learning Cooperative Trajectory Representations for Motion Forecasting</title><link>http://arxiv.org/abs/2311.00371v2</link><description>Motion forecasting is an essential task for autonomous driving, and utilizinginformation from infrastructure and other vehicles can enhance forecastingcapabilities. Existing research mainly focuses on leveraging single-framecooperative information to enhance the limited perception capability of the egovehicle, while underutilizing the motion and interaction context of trafficparticipants observed from cooperative devices. In this paper, we propose aforecasting-oriented representation paradigm to utilize motion and interactionfeatures from cooperative information. Specifically, we present V2X-Graph, arepresentative framework to achieve interpretable and end-to-end trajectoryfeature fusion for cooperative motion forecasting. V2X-Graph is evaluated onV2X-Seq in vehicle-to-infrastructure (V2I) scenarios. To further evaluate onvehicle-to-everything (V2X) scenario, we construct the first real-world V2Xmotion forecasting dataset V2X-Traj, which contains multiple autonomousvehicles and infrastructure in every scenario. Experimental results on bothV2X-Seq and V2X-Traj show the advantage of our method. We hope both V2X-Graphand V2X-Traj will benefit the further development of cooperative motionforecasting. Find the project at https://github.com/AIR-THU/V2X-Graph.</description><author>Hongzhi Ruan, Haibao Yu, Wenxian Yang, Siqi Fan, Zaiqing Nie</author><pubDate>Thu, 31 Oct 2024 17:01:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00371v2</guid></item><item><title>Implicit Optimization Bias of Next-Token Prediction in Linear Models</title><link>http://arxiv.org/abs/2402.18551v2</link><description>We initiate an investigation into the optimization properties of next-tokenprediction (NTP), the dominant training paradigm for modern language models.Specifically, we study the structural properties of the solutions selected bygradient-based optimizers among the many possible minimizers of the NTPobjective. By framing NTP as cross-entropy minimization across distinctcontexts, each tied with a sparse conditional probability distribution across afinite vocabulary of tokens, we introduce "NTP-separability conditions" thatenable reaching the data-entropy lower bound. With this setup, and focusing onlinear models with fixed context embeddings, we characterize the optimizationbias of gradient descent (GD): Within the data subspace defined by the sparsitypatterns of distinct contexts, GD selects parameters that equate the logits'differences of in-support tokens to their log-odds. In the orthogonal subspace,the GD parameters diverge in norm and select the direction that maximizes amargin specific to NTP. These findings extend previous research on implicitbias in one-hot classification to the NTP setting, highlighting key differencesand prompting further research into the optimization and generalizationproperties of NTP, irrespective of the specific architecture used to generatethe context embeddings.</description><author>Christos Thrampoulidis</author><pubDate>Thu, 31 Oct 2024 17:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18551v2</guid></item><item><title>On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs)</title><link>http://arxiv.org/abs/2407.01079v3</link><description>We investigate the statistical and computational limits of latent DiffusionTransformers (DiTs) under the low-dimensional linear latent space assumption.Statistically, we study the universal approximation and sample complexity ofthe DiTs score function, as well as the distribution recovery property of theinitial data. Specifically, under mild data assumptions, we derive anapproximation error bound for the score network of latent DiTs, which issub-linear in the latent space dimension. Additionally, we derive thecorresponding sample complexity bound and show that the data distributiongenerated from the estimated score function converges toward a proximate areaof the original one. Computationally, we characterize the hardness of bothforward inference and backward computation of latent DiTs, assuming the StrongExponential Time Hypothesis (SETH). For forward inference, we identifyefficient criteria for all possible latent DiTs inference algorithms andshowcase our theory by pushing the efficiency toward almost-linear timeinference. For backward computation, we leverage the low-rank structure withinthe gradient computation of DiTs training for possible algorithmic speedup.Specifically, we show that such speedup achieves almost-linear time latent DiTstraining by casting the DiTs gradient as a series of chained low-rankapproximations with bounded error. Under the low-dimensional assumption, weshow that the statistical rates and the computational efficiency are alldominated by the dimension of the subspace, suggesting that latent DiTs havethe potential to bypass the challenges associated with the high dimensionalityof initial data.</description><author>Jerry Yao-Chieh Hu, Weimin Wu, Zhao Song, Han Liu</author><pubDate>Thu, 31 Oct 2024 16:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01079v3</guid></item><item><title>Benchmarking LLMs via Uncertainty Quantification</title><link>http://arxiv.org/abs/2401.12794v3</link><description>The proliferation of open-source Large Language Models (LLMs) from variousinstitutions has highlighted the urgent need for comprehensive evaluationmethods. However, current evaluation platforms, such as the widely recognizedHuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,which is vital for thoroughly assessing LLMs. To bridge this gap, we introducea new benchmarking approach for LLMs that integrates uncertaintyquantification. Our examination involves nine LLMs (LLM series) spanning fiverepresentative natural language processing tasks. Our findings reveal that: I)LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMsmay display greater uncertainty compared to their smaller counterparts; andIII) Instruction-finetuning tends to increase the uncertainty of LLMs. Theseresults underscore the significance of incorporating uncertainty in theevaluation of LLMs.</description><author>Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F. Wong, Emine Yilmaz, Shuming Shi, Zhaopeng Tu</author><pubDate>Thu, 31 Oct 2024 16:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12794v3</guid></item><item><title>Separation-based distance measures for causal graphs</title><link>http://arxiv.org/abs/2402.04952v3</link><description>Assessing the accuracy of the output of causal discovery algorithms iscrucial in developing and comparing novel methods. Common evaluation metricssuch as the structural Hamming distance are useful for assessing individuallinks of causal graphs. However, many state-of-the-art causal discovery methodsdo not output single causal graphs, but rather their Markov equivalence classes(MECs) which encode all of the graph's separation and connection statements. Inthis work, we propose additional measures of distance that capture thedifference in separations of two causal graphs which link-based distances arenot fit to assess. The proposed distances have low polynomial time complexityand are applicable to directed acyclic graphs (DAGs) as well as to maximalancestral graph (MAGs) that may contain bidirected edges. We complement ourtheoretical analysis with toy examples and empirical experiments that highlightthe differences to existing comparison metrics.</description><author>Jonas Wahl, Jakob Runge</author><pubDate>Thu, 31 Oct 2024 16:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04952v3</guid></item><item><title>Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource Programming and Formal Languages</title><link>http://arxiv.org/abs/2406.03636v4</link><description>Recent advances in large language models (LLMs) for code applications havedemonstrated remarkable zero-shot fluency and instruction following onchallenging code related tasks ranging from test case generation toself-repair. Unsurprisingly, however, models struggle to compose syntacticallyvalid programs in programming languages unrepresented in pre-training, referredto as very low-resource Programming Languages (VLPLs). VLPLs appear in crucialsettings, including domain-specific languages for internal tools, tool-chainsfor legacy languages, and formal verification frameworks. Inspired by atechnique called natural programming elicitation, we propose designing anintermediate language that LLMs "naturally" know how to use and which can beautomatically compiled to a target VLPL. When LLMs generate code that liesoutside of this intermediate language, we use compiler techniques to repair thecode into programs in the intermediate language. Overall, we introduce\emph{synthetic programming elicitation and compilation} (SPEAC), an approachthat enables LLMs to generate syntactically valid code even for VLPLs. Weempirically evaluate the performance of SPEAC in a case study for the UCLID5formal verification language and find that, compared to existing retrieval andfine-tuning baselines, SPEAC produces syntactically correct programs morefrequently and without sacrificing semantic correctness.</description><author>Federico Mora, Justin Wong, Haley Lepe, Sahil Bhatia, Karim Elmaaroufi, George Varghese, Joseph E. Gonzalez, Elizabeth Polgreen, Sanjit A. Seshia</author><pubDate>Thu, 31 Oct 2024 16:54:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03636v4</guid></item><item><title>De-Confusing Pseudo-Labels in Source-Free Domain Adaptation</title><link>http://arxiv.org/abs/2401.01650v3</link><description>Source-free domain adaptation aims to adapt a source-trained model to anunlabeled target domain without access to the source data. It has attractedgrowing attention in recent years, where existing approaches focus onself-training that usually includes pseudo-labeling techniques. In this paper,we introduce a novel noise-learning approach tailored to address noisedistribution in domain adaptation settings and learn to de-confuse thepseudo-labels. More specifically, we learn a noise transition matrix of thepseudo-labels to capture the label corruption of each class and learn theunderlying true label distribution. Estimating the noise transition matrixenables a better true class-posterior estimation, resulting in betterprediction accuracy. We demonstrate the effectiveness of our approach whencombined with several source-free domain adaptation methods: SHOT, SHOT++, andAaD. We obtain state-of-the-art results on three domain adaptation datasets:VisDA, DomainNet, and OfficeHome.</description><author>Idit Diamant, Amir Rosenfeld, Idan Achituve, Jacob Goldberger, Arnon Netzer</author><pubDate>Thu, 31 Oct 2024 16:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01650v3</guid></item><item><title>Q-learning for Quantile MDPs: A Decomposition, Performance, and Convergence Analysis</title><link>http://arxiv.org/abs/2410.24128v1</link><description>In Markov decision processes (MDPs), quantile risk measures such asValue-at-Risk are a standard metric for modeling RL agents' preferences forcertain outcomes. This paper proposes a new Q-learning algorithm for quantileoptimization in MDPs with strong convergence and performance guarantees. Thealgorithm leverages a new, simple dynamic program (DP) decomposition forquantile MDPs. Compared with prior work, our DP decomposition requires neitherknown transition probabilities nor solving complex saddle point equations andserves as a suitable foundation for other model-free RL algorithms. Ournumerical results in tabular domains show that our Q-learning algorithmconverges to its DP variant and outperforms earlier algorithms.</description><author>Jia Lin Hau, Erick Delage, Esther Derman, Mohammad Ghavamzadeh, Marek Petrik</author><pubDate>Thu, 31 Oct 2024 16:53:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24128v1</guid></item><item><title>Multi-environment Topic Models</title><link>http://arxiv.org/abs/2410.24126v1</link><description>Probabilistic topic models are a powerful tool for extracting latent themesfrom large text datasets. In many text datasets, we also observe per-documentcovariates (e.g., source, style, political affiliation) that act asenvironments that modulate a "global" (environment-agnostic) topicrepresentation. Accurately learning these representations is important forprediction on new documents in unseen environments and for estimating thecausal effect of topics on real-world outcomes. To this end, we introduce theMulti-environment Topic Model (MTM), an unsupervised probabilistic model thatseparates global and environment-specific terms. Through experimentation onvarious political content, from ads to tweets and speeches, we show that theMTM produces interpretable global topics with distinct environment-specificwords. On multi-environment data, the MTM outperforms strong baselines in andout-of-distribution. It also enables the discovery of accurate causal effects.</description><author>Dominic Sobhani, Amir Feder, David Blei</author><pubDate>Thu, 31 Oct 2024 16:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24126v1</guid></item><item><title>Text-Aware Diffusion for Policy Learning</title><link>http://arxiv.org/abs/2407.01903v2</link><description>Training an agent to achieve particular goals or perform desired behaviors isoften accomplished through reinforcement learning, especially in the absence ofexpert demonstrations. However, supporting novel goals or behaviors throughreinforcement learning requires the ad-hoc design of appropriate rewardfunctions, which quickly becomes intractable. To address this challenge, wepropose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses apretrained, frozen text-conditioned diffusion model to compute dense zero-shotreward signals for text-aligned policy learning. We hypothesize thatlarge-scale pretrained generative models encode rich priors that can supervisea policy to behave not only in a text-aligned manner, but also in alignmentwith a notion of naturalness summarized from internet-scale training data. Inour experiments, we demonstrate that TADPoLe is able to learn policies fornovel goal-achievement and continuous locomotion behaviors specified by naturallanguage, in both Humanoid and Dog environments. The behaviors are learnedzero-shot without ground-truth rewards or expert demonstrations, and arequalitatively more natural according to human evaluation. We further show thatTADPoLe performs competitively when applied to robotic manipulation tasks inthe Meta-World environment, without having access to any in-domaindemonstrations.</description><author>Calvin Luo, Mandy He, Zilai Zeng, Chen Sun</author><pubDate>Thu, 31 Oct 2024 16:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01903v2</guid></item><item><title>BERTs are Generative In-Context Learners</title><link>http://arxiv.org/abs/2406.04823v2</link><description>While in-context learning is commonly associated with causal language models,such as GPT, we demonstrate that this capability also 'emerges' in maskedlanguage models. Through an embarrassingly simple inference technique, weenable an existing masked model, DeBERTa, to perform generative tasks withoutadditional training or architectural changes. Our evaluation reveals that themasked and causal language models behave very differently, as they clearlyoutperform each other on different categories of tasks. These complementarystrengths suggest that the field's focus on causal models for in-contextlearning may be limiting - both architectures can develop these capabilities,but with distinct advantages; pointing toward promising hybrid approaches thatcombine the strengths of both objectives.</description><author>David Samuel</author><pubDate>Thu, 31 Oct 2024 16:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04823v2</guid></item><item><title>Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing</title><link>http://arxiv.org/abs/2410.24119v1</link><description>The emergence of foundational models and generative artificial intelligence(GenAI) is poised to transform productivity in scientific computing, especiallyin code development, refactoring, and translating from one programming languageto another. However, because the output of GenAI cannot be guaranteed to becorrect, manual intervention remains necessary. Some of this intervention canbe automated through task-specific tools, alongside additional methodologiesfor correctness verification and effective prompt development. We explored theapplication of GenAI in assisting with code translation, languageinteroperability, and codebase inspection within a legacy Fortran codebase usedto simulate particle interactions at the Large Hadron Collider (LHC). In theprocess, we developed a tool, CodeScribe, which combines prompt engineeringwith user supervision to establish an efficient process for code conversion. Inthis paper, we demonstrate how CodeScribe assists in converting Fortran code toC++, generating Fortran-C APIs for integrating legacy systems with modern C++libraries, and providing developer support for code organization and algorithmimplementation. We also address the challenges of AI-driven code translationand highlight its benefits for enhancing productivity in scientific computingworkflows.</description><author>Akash Dhruv, Anshu Dubey</author><pubDate>Thu, 31 Oct 2024 16:48:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24119v1</guid></item><item><title>An Optimism-based Approach to Online Evaluation of Generative Models</title><link>http://arxiv.org/abs/2406.07451v2</link><description>Existing frameworks for evaluating and comparing generative models typicallytarget an offline setting, where the evaluator has access to full batches ofdata produced by the models. However, in many practical scenarios, the goal isto identify the best model using the fewest generated samples to minimize thecosts of querying data from the models. Such an online comparison ischallenging with current offline assessment methods. In this work, we proposean online evaluation framework to find the generative model that maximizes astandard assessment score among a group of available models. Our method uses anoptimism-based multi-armed bandit framework to identify the model producingdata with the highest evaluation score, quantifying the quality and diversityof generated data. Specifically, we study the online assessment of generativemodels based on the Fr\'echet Inception Distance (FID) and Inception Score (IS)metrics and propose the FID-UCB and IS-UCB algorithms leveraging the upperconfidence bound approach in online learning. We prove sub-linear regret boundsfor these algorithms and present numerical results on standard image datasets,demonstrating their effectiveness in identifying the score-maximizinggenerative model.</description><author>Xiaoyan Hu, Ho-fung Leung, Farzan Farnia</author><pubDate>Thu, 31 Oct 2024 16:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07451v2</guid></item><item><title>Repository-Level Compositional Code Translation and Validation</title><link>http://arxiv.org/abs/2410.24117v1</link><description>Code translation transforms programs from one programming language (PL) toanother. Several rule-based transpilers have been designed to automate codetranslation between different pairs of PLs. However, the rules can becomeobsolete as the PLs evolve and cannot generalize to other PLs. Recent studieshave explored the automation of code translation using Large Language Models(LLMs). One key observation is that such techniques may work well for craftedbenchmarks but fail to generalize to the scale and complexity of real-worldprojects with dependencies, custom types, PL-specific features, etc. We propose AlphaTrans, a neuro-symbolic approach to automate repository-levelcode translation. AlphaTrans translates both source and test code, and employsmultiple levels of validation to ensure the translation preserves thefunctionality of the source program. To break down the problem for LLMs,AlphaTrans leverages program analysis to decompose the program into fragmentsand translates them in the reverse call order. We leveraged AlphaTrans totranslate ten real-world open-source projects consisting of &lt;836, 8575, 2719&gt;classes, methods, and tests. AlphaTrans translated the entire repository ofthese projects consisting of 6899 source code fragments. 99.1% of thetranslated code fragments are syntactically correct, and AlphaTrans validatesthe translations' runtime behavior and functional correctness for 25.8%. Onaverage, the integrated translation and validation take 36 hours to translate aproject, showing its scalability in practice. For the syntactically orsemantically incorrect translations, AlphaTrans generates a report includingexisting translation, stack trace, test errors, or assertion failures. Weprovided these artifacts to two developers to fix the translation bugs in fourprojects. They were able to fix the issues in 20.1 hours on average and achieveall passing tests.</description><author>Ali Reza Ibrahimzada, Kaiyao Ke, Mrigank Pawagi, Muhammad Salman Abid, Rangeet Pan, Saurabh Sinha, Reyhaneh Jabbarvand</author><pubDate>Thu, 31 Oct 2024 16:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24117v1</guid></item><item><title>AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization</title><link>http://arxiv.org/abs/2410.24116v1</link><description>Image labeling is a critical bottleneck in the development of computer visiontechnologies, often constraining the potential of machine learning models dueto the time-intensive nature of manual annotations. This work introduces anovel approach that leverages outpainting to address the problem of annotateddata scarcity by generating artificial contexts and annotations, significantlyreducing manual labeling efforts. We apply this technique to a particularlyacute challenge in autonomous driving, urban planning, and environmentalmonitoring: the lack of diverse, eye-level vehicle images in desired classes.Our dataset comprises AI-generated vehicle images obtained by detecting andcropping vehicles from manually selected seed images, which are then outpaintedonto larger canvases to simulate varied real-world conditions. The outpaintedimages include detailed annotations, providing high-quality ground truth data.Advanced outpainting techniques and image quality assessments ensure visualfidelity and contextual relevance. Augmentation with outpainted vehiclesimproves overall performance metrics by up to 8\% and enhances prediction ofunderrepresented classes by up to 20\%. This approach, exemplifying outpaintingas a self-annotating paradigm, presents a solution that enhances datasetversatility across multiple domains of machine learning. The code and links todatasets used in this study are available for further research and replicationat https://github.com/amir-kazemi/aidovecl.</description><author>Amir Kazemi, Qurat ul ain Fatima, Volodymyr Kindratenko, Christopher Tessum</author><pubDate>Thu, 31 Oct 2024 16:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24116v1</guid></item><item><title>Nearest Neighbor Normalization Improves Multimodal Retrieval</title><link>http://arxiv.org/abs/2410.24114v1</link><description>Multimodal models leverage large-scale pre-training to achieve strong butstill imperfect performance on tasks such as image captioning, visual questionanswering, and cross-modal retrieval. In this paper, we present a simple andefficient method for correcting errors in trained contrastive image-textretrieval models with no additional training, called Nearest NeighborNormalization (NNN). We show an improvement on retrieval metrics in both textretrieval and image retrieval for all of the contrastive models that we tested(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used(MS-COCO and Flickr30k). NNN requires a reference database, but does notrequire any training on this database, and can even increase the retrievalaccuracy of a model after finetuning.</description><author>Neil Chowdhury, Franklin Wang, Sumedh Shenoy, Douwe Kiela, Sarah Schwettmann, Tristan Thrush</author><pubDate>Thu, 31 Oct 2024 16:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24114v1</guid></item><item><title>Self-Calibrating Conformal Prediction</title><link>http://arxiv.org/abs/2402.07307v3</link><description>In machine learning, model calibration and predictive inference are essentialfor producing reliable predictions and quantifying uncertainty to supportdecision-making. Recognizing the complementary roles of point and intervalpredictions, we introduce Self-Calibrating Conformal Prediction, a method thatcombines Venn-Abers calibration and conformal prediction to deliver calibratedpoint predictions alongside prediction intervals with finite-sample validityconditional on these predictions. To achieve this, we extend the originalVenn-Abers procedure from binary classification to regression. Our theoreticalframework supports analyzing conformal prediction methods that involvecalibrating model predictions and subsequently constructing conditionally validprediction intervals on the same data, where the conditioning set or conformityscores may depend on the calibrated predictions. Real-data experiments showthat our method improves interval efficiency through model calibration andoffers a practical alternative to feature-conditional validity.</description><author>Lars van der Laan, Ahmed M. Alaa</author><pubDate>Thu, 31 Oct 2024 16:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07307v3</guid></item><item><title>Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers</title><link>http://arxiv.org/abs/2410.24108v1</link><description>Decision Transformers have recently emerged as a new and compelling paradigmfor offline Reinforcement Learning (RL), completing a trajectory in anautoregressive way. While improvements have been made to overcome initialshortcomings, online finetuning of decision transformers has been surprisinglyunder-explored. The widely adopted state-of-the-art Online Decision Transformer(ODT) still struggles when pretrained with low-reward offline data. In thispaper, we theoretically analyze the online-finetuning of the decisiontransformer, showing that the commonly used Return-To-Go (RTG) that's far fromthe expected return hampers the online fine-tuning process. This problem,however, is well-addressed by the value function and advantage of standard RLalgorithms. As suggested by our analysis, in our experiments, we hence findthat simply adding TD3 gradients to the finetuning process of ODT effectivelyimproves the online finetuning performance of ODT, especially if ODT ispretrained with low-reward offline data. These findings provide new directionsto further improve decision transformers.</description><author>Kai Yan, Alexander G. Schwing, Yu-Xiong Wang</author><pubDate>Thu, 31 Oct 2024 16:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24108v1</guid></item><item><title>On Sampling Strategies for Spectral Model Sharding</title><link>http://arxiv.org/abs/2410.24106v1</link><description>The problem of heterogeneous clients in federated learning has recently drawna lot of attention. Spectral model sharding, i.e., partitioning the modelparameters into low-rank matrices based on the singular value decomposition,has been one of the proposed solutions for more efficient on-device training insuch settings. In this work, we present two sampling strategies for suchsharding, obtained as solutions to specific optimization problems. The firstproduces unbiased estimators of the original weights, while the second aims tominimize the squared approximation error. We discuss how both of theseestimators can be incorporated in the federated learning loop and practicalconsiderations that arise during local training. Empirically, we demonstratethat both of these methods can lead to improved performance on various commonlyused datasets.</description><author>Denis Korzhenkov, Christos Louizos</author><pubDate>Thu, 31 Oct 2024 16:37:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24106v1</guid></item><item><title>Mini-Sequence Transformer: Optimizing Intermediate Memory for Long Sequences Training</title><link>http://arxiv.org/abs/2407.15892v3</link><description>We introduce Mini-Sequence Transformer (MsT), a simple and effectivemethodology for highly efficient and accurate LLM training with extremely longsequences. MsT partitions input sequences and iteratively processesmini-sequences to reduce intermediate memory usage. Integrated with activationrecomputation, it enables significant memory savings in both forward andbackward passes. In experiments with the Llama3-8B model, with MsT, we measureno degradation in throughput or convergence even with 12x longer sequences thanstandard implementations. MsT is fully general, implementation-agnostic, andrequires minimal code changes to integrate with existing LLM trainingframeworks. Integrated with the huggingface library, MsT successfully extendsthe maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x.</description><author>Cheng Luo, Jiawei Zhao, Zhuoming Chen, Beidi Chen, Anima Anandkumar</author><pubDate>Thu, 31 Oct 2024 16:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15892v3</guid></item><item><title>Adversarial Score identity Distillation: Rapidly Surpassing the Teacher in One Step</title><link>http://arxiv.org/abs/2410.14919v2</link><description>Score identity Distillation (SiD) is a data-free method that has achievedstate-of-the-art performance in image generation by leveraging only apretrained diffusion model, without requiring any training data. However, theultimate performance of SiD is constrained by the accuracy with which thepretrained model captures the true data scores at different stages of thediffusion process. In this paper, we introduce SiDA (SiD with AdversarialLoss), which not only enhances generation quality but also improvesdistillation efficiency by incorporating real images and adversarial loss. SiDAutilizes the encoder from the generator's score network as a discriminator,boosting its ability to distinguish between real images and those generated bySiD. The adversarial loss is batch-normalized within each GPU and then combinedwith the original SiD loss. This integration effectively incorporates theaverage "fakeness" per GPU batch into the pixel-based SiD loss, enabling SiDAto distill a single-step generator either from scratch or by fine-tuning anexisting one. SiDA converges significantly faster than its predecessor whentrained from scratch, and swiftly improves upon the original model'sperformance after an initial warmup period during fine-tuning from apre-distilled SiD generator. This one-step adversarial distillation methodestablishes new benchmarks in generation performance when distilling EDMdiffusion models pretrained on CIFAR-10 (32x32) and ImageNet (64x64), achievingFID score of 1.110 on ImageNet 64x64. It sets record-low FID scores whendistilling EDM2 models trained on ImageNet (512x512), surpassing even thelargest teacher model, EDM2-XXL. Our SiDA's results record FID scores of 2.156for EDM2-XS, 1.669 for EDM2-S, 1.488 for EDM2-M, and 1.465 for EDM2-L,demonstrating significant improvements across all model sizes. Our open-sourcecode will be integrated into the SiD codebase.</description><author>Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, Hai Huang</author><pubDate>Thu, 31 Oct 2024 16:36:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14919v2</guid></item><item><title>Matchmaker: Self-Improving Large Language Model Programs for Schema Matching</title><link>http://arxiv.org/abs/2410.24105v1</link><description>Schema matching -- the task of finding matches between attributes acrossdisparate data sources with different tables and hierarchies -- is critical forcreating interoperable machine learning (ML)-ready data. Addressing thisfundamental data-centric problem has wide implications, especially in domainslike healthcare, finance and e-commerce -- but also has the potential tobenefit ML models more generally, by increasing the data available for ML modeltraining. However, schema matching is a challenging ML task due tostructural/hierarchical and semantic heterogeneity between different schemas.Previous ML approaches to automate schema matching have either requiredsignificant labeled data for model training, which is often unrealistic orsuffer from poor zero-shot performance. To this end, we propose Matchmaker - acompositional language model program for schema matching, comprised ofcandidate generation, refinement and confidence scoring. Matchmaker alsoself-improves in a zero-shot manner without the need for labeled demonstrationsvia a novel optimization approach, which constructs synthetic in-contextdemonstrations to guide the language model's reasoning process. Empirically, wedemonstrate on real-world medical schema matching benchmarks that Matchmakeroutperforms previous ML-based approaches, highlighting its potential toaccelerate data integration and interoperability of ML-ready data.</description><author>Nabeel Seedat, Mihaela van der Schaar</author><pubDate>Thu, 31 Oct 2024 16:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24105v1</guid></item><item><title>Clustering to Minimize Cluster-Aware Norm Objectives</title><link>http://arxiv.org/abs/2410.24104v1</link><description>We initiate the study of the following general clustering problem. We seek topartition a given set $P$ of data points into $k$ clusters by finding a set $X$of $k$ centers and assigning each data point to one of the centers. The cost ofa cluster, represented by a center $x\in X$, is a monotone, symmetric norm $f$(inner norm) of the vector of distances of points assigned to $x$. The goal isto minimize a norm $g$ (outer norm) of the vector of cluster costs. Thisproblem, which we call $(f,g)$-Clustering, generalizes many fundamentalclustering problems such as $k$-Center, $k$-Median , Min-Sum of Radii, andMin-Load $k$-Clustering . A recent line of research (Chakrabarty, Swamy[STOC'19]) studies norm objectives that are oblivious to the cluster structuresuch as $k$-Median and $k$-Center. In contrast, our problem modelscluster-aware objectives including Min-Sum of Radii and Min-Load$k$-Clustering. Our main results are as follows. First, we design a constant-factorapproximation algorithm for $(\textsf{top}_\ell,\mathcal{L}_1)$-Clusteringwhere the inner norm ($\textsf{top}_\ell$) sums over the $\ell$ largestdistances. Second, we design a constant-factor approximation\ for$(\mathcal{L}_\infty,\textsf{Ord})$-Clustering where the outer norm is a convexcombination of $\textsf{top}_\ell$ norms (ordered weighted norm).</description><author>Martin G. Herold, Evangelos Kipouridis, Joachim Spoerhase</author><pubDate>Thu, 31 Oct 2024 16:33:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24104v1</guid></item><item><title>Benchmark Data Repositories for Better Benchmarking</title><link>http://arxiv.org/abs/2410.24100v1</link><description>In machine learning research, it is common to evaluate algorithms via theirperformance on standard benchmark datasets. While a growing body of workestablishes guidelines for -- and levies criticisms at -- data and benchmarkingpractices in machine learning, comparatively less attention has been paid tothe data repositories where these datasets are stored, documented, and shared.In this paper, we analyze the landscape of these $\textit{benchmark datarepositories}$ and the role they can play in improving benchmarking. This roleincludes addressing issues with both datasets themselves (e.g.,representational harms, construct validity) and the manner in which evaluationis carried out using such datasets (e.g., overemphasis on a few datasets andmetrics, lack of reproducibility). To this end, we identify and discuss a setof considerations surrounding the design and use of benchmark datarepositories, with a focus on improving benchmarking practices in machinelearning.</description><author>Rachel Longjohn, Markelle Kelly, Sameer Singh, Padhraic Smyth</author><pubDate>Thu, 31 Oct 2024 16:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24100v1</guid></item><item><title>Offline Multitask Representation Learning for Reinforcement Learning</title><link>http://arxiv.org/abs/2403.11574v2</link><description>We study offline multitask representation learning in reinforcement learning(RL), where a learner is provided with an offline dataset from different tasksthat share a common representation and is asked to learn the sharedrepresentation. We theoretically investigate offline multitask low-rank RL, andpropose a new algorithm called MORL for offline multitask representationlearning. Furthermore, we examine downstream RL in reward-free, offline andonline scenarios, where a new task is introduced to the agent that shares thesame representation as the upstream offline tasks. Our theoretical resultsdemonstrate the benefits of using the learned representation from the upstreamoffline task instead of directly learning the representation of the low-rankmodel.</description><author>Haque Ishfaq, Thanh Nguyen-Tang, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, Doina Precup</author><pubDate>Thu, 31 Oct 2024 16:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11574v2</guid></item><item><title>Parameter choices in HaarPSI for IQA with medical images</title><link>http://arxiv.org/abs/2410.24098v1</link><description>When developing machine learning models, image quality assessment (IQA)measures are a crucial component for evaluation. However, commonly used IQAmeasures have been primarily developed and optimized for natural images. Inmany specialized settings, such as medical images, this poses anoften-overlooked problem regarding suitability. In previous studies, the IQAmeasure HaarPSI showed promising behavior for natural and medical images.HaarPSI is based on Haar wavelet representations and the framework allowsoptimization of two parameters. So far, these parameters have been aligned fornatural images. Here, we optimize these parameters for two annotated medicaldata sets, a photoacoustic and a chest X-Ray data set. We observe that they aremore sensitive to the parameter choices than the employed natural images, andon the other hand both medical data sets lead to similar parameter values whenoptimized. We denote the optimized setting, which improves the performance forthe medical images notably, by HaarPSI$_{MED}$. The results suggest thatadapting common IQA measures within their frameworks for medical images canprovide a valuable, generalizable addition to the employment of more specifictask-based measures.</description><author>Clemens Karner, Janek Grhl, Ian Selby, Judith Babar, Jake Beckford, Thomas R Else, Timothy J Sadler, Shahab Shahipasand, Arthikkaa Thavakumar, Michael Roberts, James H. F. Rudd, Carola-Bibiane Schnlieb, Jonathan R Weir-McCall, Anna Breger</author><pubDate>Thu, 31 Oct 2024 16:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24098v1</guid></item><item><title>Progressive Safeguards for Safe and Model-Agnostic Reinforcement Learning</title><link>http://arxiv.org/abs/2410.24096v1</link><description>In this paper we propose a formal, model-agnostic meta-learning framework forsafe reinforcement learning. Our framework is inspired by how parents safeguardtheir children across a progression of increasingly riskier tasks, imparting asense of safety that is carried over from task to task. We model this as ameta-learning process where each task is synchronized with a safeguard thatmonitors safety and provides a reward signal to the agent. The safeguard isimplemented as a finite-state machine based on a safety specification; thereward signal is formally shaped around this specification. The safetyspecification and its corresponding safeguard can be arbitrarily complex andnon-Markovian, which adds flexibility to the training process andexplainability to the learned policy. The design of the safeguard is manual butit is high-level and model-agnostic, which gives rise to an end-to-end safelearning approach with wide applicability, from pixel-level game control tolanguage model fine-tuning. Starting from a given set of safety specifications(tasks), we train a model such that it can adapt to new specifications usingonly a small number of training samples. This is made possible by our methodfor efficiently transferring safety bias between tasks, which effectivelyminimizes the number of safety violations. We evaluate our framework in aMinecraft-inspired Gridworld, a VizDoom game environment, and an LLMfine-tuning application. Agents trained with our approach achieve near-minimalsafety violations, while baselines are shown to underperform.</description><author>Nabil Omi, Hosein Hasanbeig, Hiteshi Sharma, Sriram K. Rajamani, Siddhartha Sen</author><pubDate>Thu, 31 Oct 2024 16:28:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24096v1</guid></item><item><title>3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing</title><link>http://arxiv.org/abs/2410.24091v1</link><description>Tactile and visual perception are both crucial for humans to performfine-grained interactions with their environment. Developing similarmulti-modal sensing capabilities for robots can significantly enhance andexpand their manipulation skills. This paper introduces \textbf{3D-ViTac}, amulti-modal sensing and learning system designed for dexterous bimanualmanipulation. Our system features tactile sensors equipped with dense sensingunits, each covering an area of 3$mm^2$. These sensors are low-cost andflexible, providing detailed and extensive coverage of physical contacts,effectively complementing visual information. To integrate tactile and visualdata, we fuse them into a unified 3D representation space that preserves their3D structures and spatial relationships. The multi-modal representation canthen be coupled with diffusion policies for imitation learning. Throughconcrete hardware experiments, we demonstrate that even low-cost robots canperform precise manipulations and significantly outperform vision-onlypolicies, particularly in safe interactions with fragile items and executinglong-horizon tasks involving in-hand manipulation. Our project page isavailable at \url{https://binghao-huang.github.io/3D-ViTac/}.</description><author>Binghao Huang, Yixuan Wang, Xinyi Yang, Yiyue Luo, Yunzhu Li</author><pubDate>Thu, 31 Oct 2024 16:22:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24091v1</guid></item><item><title>Demystifying Linear MDPs and Novel Dynamics Aggregation Framework</title><link>http://arxiv.org/abs/2410.24089v1</link><description>In this work, we prove that, in linear MDPs, the feature dimension $d$ islower bounded by $S/U$ in order to aptly represent transition probabilities,where $S$ is the size of the state space and $U$ is the maximum size ofdirectly reachable states. Hence, $d$ can still scale with $S$ depending on thedirect reachability of the environment. To address this limitation of linearMDPs, we propose a novel structural aggregation framework based on dynamics,named as the "dynamics aggregation". For this newly proposed framework, wedesign a provably efficient hierarchical reinforcement learning algorithm inlinear function approximation that leverages aggregated sub-structures. Ourproposed algorithm exhibits statistical efficiency, achieving a regret of $\tilde{O} ( d_{\psi}^{3/2} H^{3/2}\sqrt{ N T} )$, where $d_{\psi}$ representsthe feature dimension of aggregated subMDPs and $N$ signifies the number ofaggregated subMDPs. We establish that the condition $d_{\psi}^3 N \ll d^{3}$ isreadily met in most real-world environments with hierarchical structures,enabling a substantial improvement in the regret bound compared to LSVI-UCB,which enjoys a regret of $ \tilde{O} (d^{3/2} H^{3/2} \sqrt{ T})$. To the bestof our knowledge, this work presents the first HRL algorithm with linearfunction approximation that offers provable guarantees.</description><author>Joongkyu Lee, Min-hwan Oh</author><pubDate>Thu, 31 Oct 2024 16:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24089v1</guid></item><item><title>EraW-Net: Enhance-Refine-Align W-Net for Scene-Associated Driver Attention Estimation</title><link>http://arxiv.org/abs/2408.08570v2</link><description>Associating driver attention with driving scene across two fields of views(FOVs) is a hard cross-domain perception problem, which requires comprehensiveconsideration of cross-view mapping, dynamic driving scene analysis, and driverstatus tracking. Previous methods typically focus on a single view or mapattention to the scene via estimated gaze, failing to exploit the implicitconnection between them. Moreover, simple fusion modules are insufficient formodeling the complex relationships between the two views, making informationintegration challenging. To address these issues, we propose a novel method forend-to-end scene-associated driver attention estimation, called EraW-Net. Thismethod enhances the most discriminative dynamic cues, refines featurerepresentations, and facilitates semantically aligned cross-domain integrationthrough a W-shaped architecture, termed W-Net. Specifically, a Dynamic AdaptiveFilter Module (DAF-Module) is proposed to address the challenges of frequentlychanging driving environments by extracting vital regions. It suppresses theindiscriminately recorded dynamics and highlights crucial ones by innovativejoint frequency-spatial analysis, enhancing the model's ability to parsecomplex dynamics. Additionally, to track driver states during non-fixed facialposes, we propose a Global Context Sharing Module (GCS-Module) to constructrefined feature representations by capturing hierarchical features that adaptto various scales of head and eye movements. Finally, W-Net achieves systematiccross-view information integration through its "Encoding-Independent PartialDecoding-Fusion Decoding" structure, addressing semantic misalignment inheterogeneous data integration. Experiments demonstrate that the proposedmethod robustly and accurately estimates the mapping of driver attention inscene on large public datasets.</description><author>Jun Zhou, Chunsheng Liu, Faliang Chang, Wenqian Wang, Penghui Hao, Yiming Huang, Zhiqiang Yang</author><pubDate>Thu, 31 Oct 2024 16:20:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08570v2</guid></item><item><title>In-Context Fine-Tuning for Time-Series Foundation Models</title><link>http://arxiv.org/abs/2410.24087v1</link><description>Motivated by the recent success of time-series foundation models forzero-shot forecasting, we present a methodology for $\textit{in-contextfine-tuning}$ of a time-series foundation model. In particular, we design apretrained foundation model that can be prompted (at inference time) withmultiple time-series examples, in order to forecast a target time-series intothe future. Our foundation model is specifically trained to utilize examplesfrom multiple related time-series in its context window (in addition to thehistory of the target time-series) to help it adapt to the specificdistribution of the target domain at inference time. We show that such afoundation model that uses in-context examples at inference time can obtainmuch better performance on popular forecasting benchmarks compared tosupervised deep learning methods, statistical models, as well as othertime-series foundation models. Interestingly, our in-context fine-tuningapproach even rivals the performance of a foundation model that is explicitlyfine-tuned on the target domain.</description><author>Abhimanyu Das, Matthew Faw, Rajat Sen, Yichen Zhou</author><pubDate>Thu, 31 Oct 2024 16:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24087v1</guid></item><item><title>An Efficient Dynamic Resource Allocation Framework for Evolutionary Bilevel Optimization</title><link>http://arxiv.org/abs/2410.24081v1</link><description>Bilevel optimization problems are characterized by an interactivehierarchical structure, where the upper level seeks to optimize its strategywhile simultaneously considering the response of the lower level. Evolutionaryalgorithms are commonly used to solve complex bilevel problems in practicalscenarios, but they face significant resource consumption challenges due to thenested structure imposed by the implicit lower-level optimality condition. Thischallenge becomes even more pronounced as problem dimensions increase. Althoughrecent methods have enhanced bilevel convergence through task-level knowledgesharing, further efficiency improvements are still hindered by redundantlower-level iterations that consume excessive resources while generatingunpromising solutions. To overcome this challenge, this paper proposes anefficient dynamic resource allocation framework for evolutionary bileveloptimization, named DRC-BLEA. Compared to existing approaches, DRC-BLEAintroduces a novel competitive quasi-parallel paradigm, in which multiplelower-level optimization tasks, derived from different upper-level individuals,compete for resources. A continuously updated selection probability is used toprioritize execution opportunities to promising tasks. Additionally, acooperation mechanism is integrated within the competitive framework to furtherenhance efficiency and prevent premature convergence. Experimental resultscompared with chosen state-of-the-art algorithms demonstrate the effectivenessof the proposed method. Specifically, DRC-BLEA achieves competitive accuracyacross diverse problem sets and real-world scenarios, while significantlyreducing the number of function evaluations and overall running time.</description><author>Dejun Xu, Kai Ye, Zimo Zheng, Tao Zhou, Gary G. Yen, Min Jiang</author><pubDate>Thu, 31 Oct 2024 16:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24081v1</guid></item><item><title>Graph Learning for Numeric Planning</title><link>http://arxiv.org/abs/2410.24080v1</link><description>Graph learning is naturally well suited for use in symbolic, object-centricplanning due to its ability to exploit relational structures exhibited inplanning domains and to take as input planning instances with arbitrary numbersof objects. Numeric planning is an extension of symbolic planning in whichstates may now also exhibit numeric variables. In this work, we proposedata-efficient and interpretable machine learning models for learning to solvenumeric planning tasks. This involves constructing a new graph kernel forgraphs with both continuous and categorical attributes, as well as newoptimisation methods for learning heuristic functions for numeric planning.Experiments show that our graph kernels are vastly more efficient andgeneralise better than graph neural networks for numeric planning, and alsoyield competitive coverage performance compared to domain-independent numericplanners. Code is available at https://github.com/DillonZChen/goose</description><author>Dillon Z. Chen, Sylvie Thibaux</author><pubDate>Thu, 31 Oct 2024 16:16:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24080v1</guid></item><item><title>Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models</title><link>http://arxiv.org/abs/2410.24079v1</link><description>Bayesian reasoning in linear mixed-effects models (LMMs) is challenging andoften requires advanced sampling techniques like Markov chain Monte Carlo(MCMC). A common approach is to write the model in a probabilistic programminglanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there aremany ways a user can transform a model that make inference more or lessefficient. In particular, marginalizing some variables can greatly improveinference but is difficult for users to do manually. We develop an algorithm toeasily marginalize random effects in LMMs. A naive approach introduces cubictime operations within an inference algorithm like HMC, but we reduce therunning time to linear using fast linear algebra techniques. We show thatmarginalization is always beneficial when applicable and highlight improvementsin various models, especially ones from cognitive sciences.</description><author>Jinlin Lai, Daniel Sheldon, Justin Domke</author><pubDate>Thu, 31 Oct 2024 16:16:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24079v1</guid></item><item><title>Efficient Federated Learning against Heterogeneous and Non-stationary Client Unavailability</title><link>http://arxiv.org/abs/2409.17446v2</link><description>Addressing intermittent client availability is critical for the real-worlddeployment of federated learning algorithms. Most prior work either overlooksthe potential non-stationarity in the dynamics of client unavailability orrequires substantial memory/computation overhead. We study federated learningin the presence of heterogeneous and non-stationary client availability, whichmay occur when the deployment environments are uncertain, or the clients aremobile. The impacts of heterogeneity and non-stationarity on clientunavailability can be significant, as we illustrate using FedAvg, the mostwidely adopted federated learning algorithm. We propose FedAPM, which includesnovel algorithmic structures that (i) compensate for missed computations due tounavailability with only $O(1)$ additional memory and computation with respectto standard FedAvg, and (ii) evenly diffuse local updates within the federatedlearning system through implicit gossiping, despite being agnostic tonon-stationary dynamics. We show that FedAPM converges to a stationary point ofeven non-convex objectives while achieving the desired linear speedup property.We corroborate our analysis with numerical experiments over diversified clientunavailability dynamics on real-world data sets.</description><author>Ming Xiang, Stratis Ioannidis, Edmund Yeh, Carlee Joe-Wong, Lili Su</author><pubDate>Thu, 31 Oct 2024 16:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17446v2</guid></item><item><title>Identifying Spatio-Temporal Drivers of Extreme Events</title><link>http://arxiv.org/abs/2410.24075v1</link><description>The spatio-temporal relations of impacts of extreme events and their driversin climate data are not fully understood and there is a need of machinelearning approaches to identify such spatio-temporal relations from data. Thetask, however, is very challenging since there are time delays between extremesand their drivers, and the spatial response of such drivers is inhomogeneous.In this work, we propose a first approach and benchmarks to tackle thischallenge. Our approach is trained end-to-end to predict spatio-temporallyextremes and spatio-temporally drivers in the physical input variables jointly.By enforcing the network to predict extremes from spatio-temporal binary masksof identified drivers, the network successfully identifies drivers that arecorrelated with extremes. We evaluate our approach on three newly createdsynthetic benchmarks, where two of them are based on remote sensing orreanalysis climate data, and on two real-world reanalysis datasets. The sourcecode and datasets are publicly available at the project pagehttps://hakamshams.github.io/IDE.</description><author>Mohamad Hakam Shams Eddin, Juergen Gall</author><pubDate>Thu, 31 Oct 2024 16:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24075v1</guid></item><item><title>TaskBench: Benchmarking Large Language Models for Task Automation</title><link>http://arxiv.org/abs/2311.18760v3</link><description>In recent years, the remarkable progress of large language models (LLMs) hassparked interest in task automation, which involves decomposing complex tasksdescribed by user instructions into sub-tasks and invoking external tools toexecute them, playing a central role in autonomous agents. However, there is alack of systematic and standardized benchmarks to promote the development ofLLMs in task automation. To address this, we introduce TaskBench, acomprehensive framework to evaluate the capability of LLMs in task automation.Specifically, task automation can be divided into three critical stages: taskdecomposition, tool selection, and parameter prediction. To tackle thecomplexities inherent in these stages, we introduce the concept of Tool Graphto represent decomposed tasks and adopt a back-instruct method to generatehigh-quality user instructions. We propose TaskEval, a multi-faceted evaluationmethodology that assesses LLM performance across these three stages. Ourapproach combines automated construction with rigorous human verification,ensuring high consistency with human evaluation. Experimental resultsdemonstrate that TaskBench effectively reflects the capabilities of variousLLMs in task automation. It provides insights into model performance acrossdifferent task complexities and domains, pushing the boundaries of what currentmodels can achieve. TaskBench offers a scalable, adaptable, and reliablebenchmark for advancing LLM-based autonomous agents.</description><author>Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang</author><pubDate>Thu, 31 Oct 2024 16:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18760v3</guid></item><item><title>Univariate Conditional Variational Autoencoder for Morphogenic Patterns Design in Frontal Polymerization-Based Manufacturing</title><link>http://arxiv.org/abs/2410.17518v2</link><description>Under some initial and boundary conditions, the rapid reaction-thermaldiffusion process taking place during frontal polymerization (FP) destabilizesthe planar mode of front propagation, leading to spatially varying, complexhierarchical patterns in thermoset polymeric materials. Although modernreaction-diffusion models can predict the patterns resulting from unstable FP,the inverse design of patterns, which aims to retrieve process conditions thatproduce a desired pattern, remains an open challenge due to the non-unique andnon-intuitive mapping between process conditions and manufactured patterns. Inthis work, we propose a probabilistic generative model named univariateconditional variational autoencoder (UcVAE) for the inverse design ofhierarchical patterns in FP-based manufacturing. Unlike the cVAE, which encodesboth the design space and the design target, the UcVAE encodes only the designspace. In the encoder of the UcVAE, the number of training parameters issignificantly reduced compared to the cVAE, resulting in a shorter trainingtime while maintaining comparable performance. Given desired pattern images,the trained UcVAE can generate multiple process condition solutions thatproduce high-fidelity hierarchical patterns.</description><author>Qibang Liu, Pengfei Cai, Diab Abueidda, Sagar Vyas, Seid Koric, Rafael Gomez-Bombarelli, Philippe Geubelle</author><pubDate>Thu, 31 Oct 2024 16:10:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17518v2</guid></item><item><title>LaCour!: Enabling Research on Argumentation in Hearings of the European Court of Human Rights</title><link>http://arxiv.org/abs/2312.05061v3</link><description>Why does an argument end up in the final court decision? Was it deliberatedor questioned during the oral hearings? Was there something in the hearingsthat triggered a particular judge to write a dissenting opinion? Despite theavailability of the final judgments of the European Court of Human Rights(ECHR), none of these legal research questions can currently be answered as theECHR's multilingual oral hearings are not transcribed, structured, orspeaker-attributed. We address this fundamental gap by presenting LaCour!, thefirst corpus of textual oral arguments of the ECHR, consisting of 154 fullhearings (2.1 million tokens from over 267 hours of video footage) in English,French, and other court languages, each linked to the corresponding finaljudgment documents. In addition to the transcribed and partially manuallycorrected text from the video, we provide sentence-level timestamps andmanually annotated role and language labels. We also showcase LaCour! in a setof preliminary experiments that explore the interplay between questions anddissenting opinions. Apart from the use cases in legal NLP, we hope that lawstudents or other interested parties will also use LaCour! as a learningresource, as it is freely available in various formats athttps://huggingface.co/datasets/TrustHLT/LaCour.</description><author>Lena Held, Ivan Habernal</author><pubDate>Thu, 31 Oct 2024 16:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05061v3</guid></item><item><title>Local Linearity: the Key for No-regret Reinforcement Learning in Continuous MDPs</title><link>http://arxiv.org/abs/2410.24071v1</link><description>Achieving the no-regret property for Reinforcement Learning (RL) problems incontinuous state and action-space environments is one of the major openproblems in the field. Existing solutions either work under very specificassumptions or achieve bounds that are vacuous in some regimes. Furthermore,many structural assumptions are known to suffer from a provably unavoidableexponential dependence on the time horizon $H$ in the regret, which makes anypossible solution unfeasible in practice. In this paper, we identify locallinearity as the feature that makes Markov Decision Processes (MDPs) bothlearnable (sublinear regret) and feasible (regret that is polynomial in $H$).We define a novel MDP representation class, namely Locally Linearizable MDPs,generalizing other representation classes like Linear MDPs and MDPS with lowinherent Belmman error. Then, i) we introduce Cinderella, a no-regret algorithmfor this general representation class, and ii) we show that all known learnableand feasible MDP families are representable in this class. We first show thatall known feasible MDPs belong to a family that we call Mildly Smooth MDPs.Then, we show how any mildly smooth MDP can be represented as a LocallyLinearizable MDP by an appropriate choice of representation. This way,Cinderella is shown to achieve state-of-the-art regret bounds for allpreviously known (and some new) continuous MDPs for which RL is learnable andfeasible.</description><author>Davide Maran, Alberto Maria Metelli, Matteo Papini, Marcello Restelli</author><pubDate>Thu, 31 Oct 2024 16:07:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24071v1</guid></item><item><title>Dynamical similarity analysis uniquely captures how computations develop in RNNs</title><link>http://arxiv.org/abs/2410.24070v1</link><description>Methods for analyzing representations in neural systems are increasinglypopular tools in neuroscience and mechanistic interpretability. Measurescomparing neural activations across conditions, architectures, and species givescalable ways to understand information transformation within different neuralnetworks. However, recent findings show that some metrics respond to spurioussignals, leading to misleading results. Establishing benchmark test cases isthus essential for identifying the most reliable metric and potentialimprovements. We propose that compositional learning in recurrent neuralnetworks (RNNs) can provide a test case for dynamical representation alignmentmetrics. Implementing this case allows us to evaluate if metrics can identifyrepresentations that develop throughout learning and determine ifrepresentations identified by metrics reflect the network's actualcomputations. Building both attractor and RNN based test cases, we show thatthe recently proposed Dynamical Similarity Analysis (DSA) is more noise robustand reliably identifies behaviorally relevant representations compared to priormetrics (Procrustes, CKA). We also demonstrate how such test cases can extendbeyond metric evaluation to study new architectures. Specifically, testing DSAin modern (Mamba) state space models suggests that these models, unlike RNNs,may not require changes in recurrent dynamics due to their expressive hiddenstates. Overall, we develop test cases that showcase how DSA's enhanced abilityto detect dynamical motifs makes it highly effective for identifying ongoingcomputations in RNNs and revealing how networks learn tasks.</description><author>Quentin Guilhot, Jascha Achterberg, Micha Wjcik, Rui Ponte Costa</author><pubDate>Thu, 31 Oct 2024 16:07:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24070v1</guid></item><item><title>Revealing Fine-Grained Values and Opinions in Large Language Models</title><link>http://arxiv.org/abs/2406.19238v2</link><description>Uncovering latent values and opinions embedded in large language models(LLMs) can help identify biases and mitigate potential harm. Recently, this hasbeen approached by prompting LLMs with survey questions and quantifying thestances in the outputs towards morally and politically charged statements.However, the stances generated by LLMs can vary greatly depending on how theyare prompted, and there are many ways to argue for or against a given position.In this work, we propose to address this by analysing a large and robustdataset of 156k LLM responses to the 62 propositions of the Political CompassTest (PCT) generated by 6 LLMs using 420 prompt variations. We performcoarse-grained analysis of their generated stances and fine-grained analysis ofthe plain text justifications for those stances. For fine-grained analysis, wepropose to identify tropes in the responses: semantically similar phrases thatare recurrent and consistent across different prompts, revealing naturalpatterns in the text that a given LLM is prone to produce. We find thatdemographic features added to prompts significantly affect outcomes on the PCT,reflecting bias, as well as disparities between the results of tests wheneliciting closed-form vs. open domain responses. Additionally, patterns in theplain text rationales via tropes show that similar justifications arerepeatedly generated across models and prompts even with disparate stances.</description><author>Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein</author><pubDate>Thu, 31 Oct 2024 16:06:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19238v2</guid></item><item><title>FairSkin: Fair Diffusion for Skin Disease Image Generation</title><link>http://arxiv.org/abs/2410.22551v2</link><description>Image generation is a prevailing technique for clinical data augmentation foradvancing diagnostic accuracy and reducing healthcare disparities. DiffusionModel (DM) has become a leading method in generating synthetic medical images,but it suffers from a critical twofold bias: (1) The quality of imagesgenerated for Caucasian individuals is significantly higher, as measured by theFrechet Inception Distance (FID). (2) The ability of the downstream-tasklearner to learn critical features from disease images varies across differentskin tones. These biases pose significant risks, particularly in skin diseasedetection, where underrepresentation of certain skin tones can lead tomisdiagnosis or neglect of specific conditions. To address these challenges, wepropose FairSkin, a novel DM framework that mitigates these biases through athree-level resampling mechanism, ensuring fairer representation across racialand disease categories. Our approach significantly improves the diversity andquality of generated images, contributing to more equitable skin diseasedetection in clinical settings.</description><author>Ruichen Zhang, Yuguang Yao, Zhen Tan, Zhiming Li, Pan Wang, Huan Liu, Jingtong Hu, Sijia Liu, Tianlong Chen</author><pubDate>Thu, 31 Oct 2024 16:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22551v2</guid></item></channel></rss>