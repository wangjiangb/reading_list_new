<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 25 Oct 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Synthetic Data as Validation</title><link>http://arxiv.org/abs/2310.16052v1</link><description>This study leverages synthetic data as a validation set to reduce overfittingand ease the selection of the best model in AI development. While syntheticdata have been used for augmenting the training set, we find that syntheticdata can also significantly diversify the validation set, offering markedadvantages in domains like healthcare, where data are typically limited,sensitive, and from out-domain sources (i.e., hospitals). In this study, weillustrate the effectiveness of synthetic data for early cancer detection incomputed tomography (CT) volumes, where synthetic tumors are generated andsuperimposed onto healthy organs, thereby creating an extensive dataset forrigorous validation. Using synthetic data as validation can improve AIrobustness in both in-domain and out-domain test sets. Furthermore, weestablish a new continual learning framework that continuously trains AI modelson a stream of out-domain data with synthetic tumors. The AI model trained andvalidated in dynamically expanding synthetic data can consistently outperformmodels trained and validated exclusively on real-world data. Specifically, theDSC score for liver tumor segmentation improves from 26.7% (95% CI:22.6%-30.9%) to 34.5% (30.8%-38.2%) when evaluated on an in-domain dataset andfrom 31.1% (26.0%-36.2%) to 35.4% (32.1%-38.7%) on an out-domain dataset.Importantly, the performance gain is particularly significant in identifyingvery tiny liver tumors (radius &lt; 5mm) in CT volumes, with Sensitivity improvingfrom 33.1% to 55.4% on an in-domain dataset and 33.9% to 52.3% on an out-domaindataset, justifying the efficacy in early detection of cancer. The applicationof synthetic data, from both training and validation perspectives, underlines apromising avenue to enhance AI robustness when dealing with data from varyingdomains.</description><author>Qixin Hu, Alan Yuille, Zongwei Zhou</author><pubDate>Tue, 24 Oct 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16052v1</guid></item><item><title>Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width</title><link>http://arxiv.org/abs/2302.12250v2</link><description>We systematically analyze optimization dynamics in deep neural networks(DNNs) trained with stochastic gradient descent (SGD) and study the effect oflearning rate $\eta$, depth $d$, and width $w$ of the neural network. Byanalyzing the maximum eigenvalue $\lambda^H_t$ of the Hessian of the loss,which is a measure of sharpness of the loss landscape, we find that thedynamics can show four distinct regimes: (i) an early time transient regime,(ii) an intermediate saturation regime, (iii) a progressive sharpening regime,and (iv) a late time ``edge of stability" regime. The early and intermediateregimes (i) and (ii) exhibit a rich phase diagram depending on $\eta \equiv c /\lambda_0^H $, $d$, and $w$. We identify several critical values of $c$, whichseparate qualitatively distinct phenomena in the early time dynamics oftraining loss and sharpness. Notably, we discover the opening up of a``sharpness reduction" phase, where sharpness decreases at early times, as $d$and $1/w$ are increased.</description><author>Dayal Singh Kalra, Maissam Barkeshli</author><pubDate>Tue, 24 Oct 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12250v2</guid></item><item><title>MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning</title><link>http://arxiv.org/abs/2310.16049v1</link><description>While large language models (LLMs) equipped with techniques likechain-of-thought prompting have demonstrated impressive capabilities, theystill fall short in their ability to reason robustly in complex settings.However, evaluating LLM reasoning is challenging because system capabilitiescontinue to grow while benchmark datasets for tasks like logical deduction haveremained static. We introduce MuSR, a dataset for evaluating language models onmultistep soft reasoning tasks specified in a natural language narrative. Thisdataset has two crucial features. First, it is created through a novelneurosymbolic synthetic-to-natural generation algorithm, enabling theconstruction of complex reasoning instances that challenge GPT-4 (e.g., murdermysteries roughly 1000 words in length) and which can be scaled further as morecapable LLMs are released. Second, our dataset instances are free textnarratives corresponding to real-world domains of reasoning; this makes itsimultaneously much more challenging than other synthetically-craftedbenchmarks while remaining realistic and tractable for human annotators tosolve with high accuracy. We evaluate a range of LLMs and prompting techniqueson this dataset and characterize the gaps that remain for techniques likechain-of-thought to perform robust reasoning.</description><author>Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett</author><pubDate>Tue, 24 Oct 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16049v1</guid></item><item><title>AI Alignment and Social Choice: Fundamental Limitations and Policy Implications</title><link>http://arxiv.org/abs/2310.16048v1</link><description>Aligning AI agents to human intentions and values is a key bottleneck inbuilding safe and deployable AI applications. But whose values should AI agentsbe aligned with? Reinforcement learning with human feedback (RLHF) has emergedas the key framework for AI alignment. RLHF uses feedback from humanreinforcers to fine-tune outputs; all widely deployed large language models(LLMs) use RLHF to align their outputs to human values. It is critical tounderstand the limitations of RLHF and consider policy challenges arising fromthese limitations. In this paper, we investigate a specific challenge inbuilding RLHF systems that respect democratic norms. Building on impossibilityresults in social choice theory, we show that, under fairly broad assumptions,there is no unique voting protocol to universally align AI systems using RLHFthrough democratic processes. Further, we show that aligning AI agents with thevalues of all individuals will always violate certain private ethicalpreferences of an individual user i.e., universal AI alignment using RLHF isimpossible. We discuss policy implications for the governance of AI systemsbuilt using RLHF: first, the need for mandating transparent voting rules tohold model builders accountable. Second, the need for model builders to focuson developing AI agents that are narrowly aligned to specific user groups.</description><author>Abhilash Mishra</author><pubDate>Tue, 24 Oct 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16048v1</guid></item><item><title>From Posterior Sampling to Meaningful Diversity in Image Restoration</title><link>http://arxiv.org/abs/2310.16047v1</link><description>Image restoration problems are typically ill-posed in the sense that eachdegraded image can be restored in infinitely many valid ways. To accommodatethis, many works generate a diverse set of outputs by attempting to randomlysample from the posterior distribution of natural images given the degradedinput. Here we argue that this strategy is commonly of limited practical valuebecause of the heavy tail of the posterior distribution. Consider for exampleinpainting a missing region of the sky in an image. Since there is a highprobability that the missing region contains no object but clouds, any set ofsamples from the posterior would be entirely dominated by (practicallyidentical) completions of sky. However, arguably, presenting users with onlyone clear sky completion, along with several alternative solutions such asairships, birds, and balloons, would better outline the set of possibilities.In this paper, we initiate the study of meaningfully diverse image restoration.We explore several post-processing approaches that can be combined with anydiverse image restoration method to yield semantically meaningful diversity.Moreover, we propose a practical approach for allowing diffusion based imagerestoration methods to generate meaningfully diverse outputs, while incurringonly negligent computational overhead. We conduct extensive user studies toanalyze the proposed techniques, and find the strategy of reducing similaritybetween outputs to be significantly favorable over posterior sampling. Code andexamples are available in https://noa-cohen.github.io/MeaningfulDiversityInIR</description><author>Noa Cohen, Hila Manor, Yuval Bahat, Tomer Michaeli</author><pubDate>Tue, 24 Oct 2023 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16047v1</guid></item><item><title>Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty</title><link>http://arxiv.org/abs/2308.02019v2</link><description>We present our submission to the BabyLM challenge, whose goal was to improvethe sample efficiency of language models. We trained an ensemble consisting ofa GPT-2 and small LLaMA models on the developmentally-plausible, 10M-wordBabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model,which exceeds in performance both of its teachers as well as a similar modeltrained without distillation. This suggests that distillation can not onlyretain the full performance of the teacher model when the latter is trained ona sufficiently small dataset; it can exceed it, and lead to significantlybetter performance than direct training.</description><author>Inar Timiryasov, Jean-Loup Tastet</author><pubDate>Tue, 24 Oct 2023 18:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02019v2</guid></item><item><title>A Unified, Scalable Framework for Neural Population Decoding</title><link>http://arxiv.org/abs/2310.16046v1</link><description>Our ability to use deep learning approaches to decipher neural activity wouldlikely benefit from greater scale, in terms of both model size and datasets.However, the integration of many neural recordings into one unified model ischallenging, as each recording contains the activity of different neurons fromdifferent individual animals. In this paper, we introduce a training frameworkand architecture designed to model the population dynamics of neural activityacross diverse, large-scale neural recordings. Our method first tokenizesindividual spikes within the dataset to build an efficient representation ofneural events that captures the fine temporal structure of neural activity. Wethen employ cross-attention and a PerceiverIO backbone to further construct alatent tokenization of neural population activities. Utilizing thisarchitecture and training framework, we construct a large-scale multi-sessionmodel trained on large datasets from seven nonhuman primates, spanning over 158different sessions of recording from over 27,373 neural units and over 100hours of recordings. In a number of different tasks, we demonstrate that ourpretrained model can be rapidly adapted to new, unseen sessions withunspecified neuron correspondence, enabling few-shot performance with minimallabels. This work presents a powerful new approach for building deep learningtools to analyze neural data and stakes out a clear path to training at scale.</description><author>Mehdi Azabou, Vinam Arora, Venkataramana Ganesh, Ximeng Mao, Santosh Nachimuthu, Michael J. Mendelson, Blake Richards, Matthew G. Perich, Guillaume Lajoie, Eva L. Dyer</author><pubDate>Tue, 24 Oct 2023 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16046v1</guid></item><item><title>Woodpecker: Hallucination Correction for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2310.16045v1</link><description>Hallucination is a big shadow hanging over the rapidly evolving MultimodalLarge Language Models (MLLMs), referring to the phenomenon that the generatedtext is inconsistent with the image content. In order to mitigatehallucinations, existing studies mainly resort to an instruction-tuning mannerthat requires retraining the models with specific data. In this paper, we pavea different way, introducing a training-free method named Woodpecker. Like awoodpecker heals trees, it picks out and corrects hallucinations from thegenerated text. Concretely, Woodpecker consists of five stages: key conceptextraction, question formulation, visual knowledge validation, visual claimgeneration, and hallucination correction. Implemented in a post-remedy manner,Woodpecker can easily serve different MLLMs, while being interpretable byaccessing intermediate outputs of the five stages. We evaluate Woodpecker bothquantitatively and qualitatively and show the huge potential of this newparadigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvementin accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is releasedat https://github.com/BradyFU/Woodpecker.</description><author>Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen</author><pubDate>Tue, 24 Oct 2023 18:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16045v1</guid></item><item><title>Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark</title><link>http://arxiv.org/abs/2310.16044v1</link><description>We introduce Stanford-ORB, a new real-world 3D Object inverse RenderingBenchmark. Recent advances in inverse rendering have enabled a wide range ofreal-world applications in 3D content generation, moving rapidly from researchand commercial use cases to consumer devices. While the results continue toimprove, there is no real-world benchmark that can quantitatively assess andcompare the performance of various inverse rendering methods. Existingreal-world datasets typically only consist of the shape and multi-view imagesof objects, which are not sufficient for evaluating the quality of materialrecovery and object relighting. Methods capable of recovering material andlighting often resort to synthetic data for quantitative evaluation, which onthe other hand does not guarantee generalization to complex real-worldenvironments. We introduce a new dataset of real-world objects captured under avariety of natural scenes with ground-truth 3D scans, multi-view images, andenvironment lighting. Using this dataset, we establish the first comprehensivereal-world evaluation benchmark for object inverse rendering tasks fromin-the-wild scenes, and compare the performance of various existing methods.All data, code, and models can be accessed at https://stanfordorb.github.io/.</description><author>Zhengfei Kuang, Yunzhi Zhang, Hong-Xing Yu, Samir Agarwala, Shangzhe Wu, Jiajun Wu</author><pubDate>Tue, 24 Oct 2023 18:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16044v1</guid></item><item><title>WebWISE: Web Interface Control and Sequential Exploration with Large Language Models</title><link>http://arxiv.org/abs/2310.16042v1</link><description>The paper investigates using a Large Language Model (LLM) to automaticallyperform web software tasks using click, scroll, and text input operations.Previous approaches, such as reinforcement learning (RL) or imitation learning,are inefficient to train and task-specific. Our method uses filtered DocumentObject Model (DOM) elements as observations and performs tasks step-by-step,sequentially generating small programs based on the current observations. Weuse in-context learning, either benefiting from a single manually providedexample, or an automatically generated example based on a successful zero-shottrial. We evaluate the proposed method on the MiniWob++ benchmark. With onlyone in-context example, our WebWISE method achieves similar or betterperformance than other methods that require many demonstrations or trials.</description><author>Heyi Tao, Sethuraman T V, Michal Shlapentokh-Rothman, Derek Hoiem, Heng Ji</author><pubDate>Tue, 24 Oct 2023 18:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16042v1</guid></item><item><title>Instruct and Extract: Instruction Tuning for On-Demand Information Extraction</title><link>http://arxiv.org/abs/2310.16040v1</link><description>Large language models with instruction-following capabilities open the doorto a wider group of users. However, when it comes to information extraction - aclassic task in natural language processing - most task-specific systems cannotalign well with long-tail ad hoc extraction use cases for non-expert users. Toaddress this, we propose a novel paradigm, termed On-Demand InformationExtraction, to fulfill the personalized demands of real-world users. Our taskaims to follow the instructions to extract the desired content from theassociated text and present it in a structured tabular format. The tableheaders can either be user-specified or inferred contextually by the model. Tofacilitate research in this emerging area, we present a benchmark namedInstructIE, inclusive of both automatically generated training data, as well asthe human-annotated test set. Building on InstructIE, we further develop anOn-Demand Information Extractor, ODIE. Comprehensive evaluations on ourbenchmark reveal that ODIE substantially outperforms the existing open-sourcemodels of similar size. Our code and dataset are released onhttps://github.com/yzjiao/On-Demand-IE.</description><author>Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Heng Ji, Jiawei Han</author><pubDate>Tue, 24 Oct 2023 18:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16040v1</guid></item><item><title>Consistent Optimal Transport with Empirical Conditional Measures</title><link>http://arxiv.org/abs/2305.15901v3</link><description>Given samples from two joint distributions, we consider the problem ofOptimal Transportation (OT) between them when conditioned on a common variable.We focus on the general setting where the conditioned variable may becontinuous, and the marginals of this variable in the two joint distributionsmay not be the same. In such settings, standard OT variants cannot be employed,and novel estimation techniques are necessary. Since the main challenge is thatthe conditional distributions are not explicitly available, the key idea in ourOT formulation is to employ kernelized-least-squares terms computed over thejoint samples, which implicitly match the transport plan's marginals with theempirical conditionals. Under mild conditions, we prove that our estimatedtransport plans, as a function of the conditioned variable, are asymptoticallyoptimal. For finite samples, we show that the deviation in terms of ourregularized objective is bounded by $O(1/m^{1/4})$, where $m$ is the number ofsamples. We also discuss how the conditional transport plan could be modelledusing explicit probabilistic models as well as using implicit generative ones.We empirically verify the consistency of our estimator on synthetic datasets,where the optimal plan is analytically known. When employed in applicationslike prompt learning for few-shot classification and conditional-generation inthe context of predicting cell responses to treatment, our methodology improvesupon state-of-the-art methods.</description><author>Piyushi Manupriya, Rachit Keerti Das, Sayantan Biswas, Saketha Nath Jagarlapudi</author><pubDate>Tue, 24 Oct 2023 18:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15901v3</guid></item><item><title>What's Left? Concept Grounding with Logic-Enhanced Foundation Models</title><link>http://arxiv.org/abs/2310.16035v1</link><description>Recent works such as VisProg and ViperGPT have smartly composed foundationmodels for visual reasoning-using large language models (LLMs) to produceprograms that can be executed by pre-trained vision-language models. However,they operate in limited domains, such as 2D images, not fully exploiting thegeneralization of language: abstract concepts like "left" can also be groundedin 3D, temporal, and action data, as in moving to your left. This limitedgeneralization stems from these inference-only methods' inability to learn oradapt pre-trained models to a new domain. We propose the Logic-EnhancedFoundation Model (LEFT), a unified framework that learns to ground and reasonwith concepts across domains with a differentiable, domain-independent,first-order logic-based program executor. LEFT has an LLM interpreter thatoutputs a program represented in a general, logic-based reasoning language,which is shared across all domains and tasks. LEFT's executor then executes theprogram with trainable domain-specific grounding modules. We show that LEFTflexibly learns concepts in four domains: 2D images, 3D scenes, human motions,and robotic manipulation. It exhibits strong reasoning ability in a widevariety of tasks, including those that are complex and not seen duringtraining, and can be easily applied to new domains.</description><author>Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Jiajun Wu</author><pubDate>Tue, 24 Oct 2023 18:50:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16035v1</guid></item><item><title>Dolphin: A Challenging and Diverse Benchmark for Arabic NLG</title><link>http://arxiv.org/abs/2305.14989v2</link><description>We present Dolphin, a novel benchmark that addresses the need for a naturallanguage generation (NLG) evaluation framework dedicated to the wide collectionof Arabic languages and varieties. The proposed benchmark encompasses a broadrange of 13 different NLG tasks, including dialogue generation, questionanswering, machine translation, summarization, among others. Dolphin comprisesa substantial corpus of 40 diverse and representative public datasets across 50test splits, carefully curated to reflect real-world scenarios and thelinguistic richness of Arabic. It sets a new standard for evaluating theperformance and generalization capabilities of Arabic and multilingual models,promising to enable researchers to push the boundaries of currentmethodologies. We provide an extensive analysis of Dolphin, highlighting itsdiversity and identifying gaps in current Arabic NLG research. We also offer apublic leaderboard that is both interactive and modular and evaluate severalmodels on our benchmark, allowing us to set strong baselines against whichresearchers can compare.</description><author>El Moatez Billah Nagoudi, AbdelRahim Elmadany, Ahmed El-Shangiti, Muhammad Abdul-Mageed</author><pubDate>Tue, 24 Oct 2023 18:48:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14989v2</guid></item><item><title>Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models</title><link>http://arxiv.org/abs/2310.16033v1</link><description>Multimodal Large Language Models (LLMs) have recently achieved promisingzero-shot accuracy on visual question answering (VQA) -- a fundamental taskaffecting various downstream applications and domains. Given the greatpotential for the broad use of these models, it is important to investigatetheir limitations in dealing with different image and question properties. Inthis work, we investigate whether multimodal LLMs can perceive small details aswell as large details in images. In particular, we show that their zero-shotaccuracy in answering visual questions is very sensitive to the size of thevisual subject of the question, declining up to $46\%$ with size. Furthermore,we show that this effect is causal by observing that human visual cropping cansignificantly mitigate their sensitivity to size. Inspired by the usefulness ofhuman cropping, we then propose three automatic visual cropping methods asinference time mechanisms to improve the zero-shot performance of multimodalLLMs. We study their effectiveness on four popular VQA datasets, and a subsetof the VQAv2 dataset tailored towards fine visual details. Our findings suggestthat multimodal LLMs should be used with caution in detail-sensitive VQAapplications, and that visual cropping is a promising direction to improvetheir zero-shot performance. Our code and data are publicly available.</description><author>Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski</author><pubDate>Tue, 24 Oct 2023 18:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16033v1</guid></item><item><title>Finetuning Offline World Models in the Real World</title><link>http://arxiv.org/abs/2310.16029v1</link><description>Reinforcement Learning (RL) is notoriously data-inefficient, which makestraining on a real robot difficult. While model-based RL algorithms (worldmodels) improve data-efficiency to some extent, they still require hours ordays of interaction to learn skills. Recently, offline RL has been proposed asa framework for training RL policies on pre-existing datasets without anyonline interaction. However, constraining an algorithm to a fixed datasetinduces a state-action distribution shift between training and inference, andlimits its applicability to new tasks. In this work, we seek to get the best ofboth worlds: we consider the problem of pretraining a world model with offlinedata collected on a real robot, and then finetuning the model on online datacollected by planning with the learned model. To mitigate extrapolation errorsduring online interaction, we propose to regularize the planner at test-time bybalancing estimated returns and (epistemic) model uncertainty. We evaluate ourmethod on a variety of visuo-motor control tasks in simulation and on a realrobot, and find that our method enables few-shot finetuning to seen and unseentasks even when offline data is limited. Videos, code, and data are availableat https://yunhaifeng.com/FOWM .</description><author>Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, Xiaolong Wang</author><pubDate>Tue, 24 Oct 2023 18:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16029v1</guid></item><item><title>What Algorithms can Transformers Learn? A Study in Length Generalization</title><link>http://arxiv.org/abs/2310.16028v1</link><description>Large language models exhibit surprising emergent generalization properties,yet also struggle on many simple reasoning tasks such as arithmetic and parity.This raises the question of if and when Transformer models can learn the truealgorithm for solving a task. We study the scope of Transformers' abilities inthe specific setting of length generalization on algorithmic tasks. Here, wepropose a unifying framework to understand when and how Transformers canexhibit strong length generalization on a given task. Specifically, we leverageRASP (Weiss et al., 2021) -- a programming language designed for thecomputational model of a Transformer -- and introduce the RASP-GeneralizationConjecture: Transformers tend to length generalize on a task if the task can besolved by a short RASP program which works for all input lengths. This simpleconjecture remarkably captures most known instances of length generalization onalgorithmic tasks. Moreover, we leverage our insights to drastically improvegeneralization performance on traditionally hard tasks (such as parity andaddition). On the theoretical side, we give a simple example where the"min-degree-interpolator" model of learning from Abbe et al. (2023) does notcorrectly predict Transformers' out-of-distribution behavior, but ourconjecture does. Overall, our work provides a novel perspective on themechanisms of compositional generalization and the algorithmic capabilities ofTransformers.</description><author>Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, Preetum Nakkiran</author><pubDate>Tue, 24 Oct 2023 18:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16028v1</guid></item><item><title>TimewarpVAE: Simultaneous Time-Warping and Representation Learning of Trajectories</title><link>http://arxiv.org/abs/2310.16027v1</link><description>Human demonstrations of trajectories are an important source of training datafor many machine learning problems. However, the difficulty of collecting humandemonstration data for complex tasks makes learning efficient representationsof those trajectories challenging. For many problems, such as for handwritingor for quasistatic dexterous manipulation, the exact timings of thetrajectories should be factored from their spatial path characteristics. Inthis work, we propose TimewarpVAE, a fully differentiable manifold-learningalgorithm that incorporates Dynamic Time Warping (DTW) to simultaneously learnboth timing variations and latent factors of spatial variation. We show how theTimewarpVAE algorithm learns appropriate time alignments and meaningfulrepresentations of spatial variations in small handwriting and forkmanipulation datasets. Our results have lower spatial reconstruction test errorthan baseline approaches and the learned low-dimensional representations can beused to efficiently generate semantically meaningful novel trajectories.</description><author>Travers Rhodes, Daniel D. Lee</author><pubDate>Tue, 24 Oct 2023 18:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16027v1</guid></item><item><title>Data Selection for Language Models via Importance Resampling</title><link>http://arxiv.org/abs/2302.03169v2</link><description>Selecting a suitable pretraining dataset is crucial for both general-domain(e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). Weformalize this problem as selecting a subset of a large raw unlabeled datasetto match a desired target distribution given some unlabeled target samples. Dueto the large scale and dimensionality of the raw text data, existing methodsuse simple heuristics or use experts to manually curate data. Instead, weextend the classic importance resampling approach used in low-dimensions for LMdata selection. We propose Data Selection with Importance Resampling (DSIR), anefficient and scalable framework that estimates importance weights in a reducedfeature space for tractability and selects data with importance resamplingaccording to these weights. To determine an appropriate feature space, we showthat KL reduction, a data metric that measures the proximity between selectedpretraining data and the target in a feature space, has high correlation withaverage downstream accuracy (r=0.89) when computed with simple n-gram features.This motivates our instantiation of DSIR using n-gram features. When performingcontinued pretraining towards a specific domain, DSIR performs comparably toexpert curation across 8 target distributions. When pretraining general-domainmodels (target is Wikipedia + books), DSIR improves over random selection andheuristic filtering baselines by 2-2.5% on the GLUE benchmark.</description><author>Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy Liang</author><pubDate>Tue, 24 Oct 2023 18:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03169v2</guid></item><item><title>ConvBKI: Real-Time Probabilistic Semantic Mapping Network with Quantifiable Uncertainty</title><link>http://arxiv.org/abs/2310.16020v1</link><description>In this paper, we develop a modular neural network for real-time semanticmapping in uncertain environments, which explicitly updates per-voxelprobabilistic distributions within a neural network layer. Our approachcombines the reliability of classical probabilistic algorithms with theperformance and efficiency of modern neural networks. Although roboticperception is often divided between modern differentiable methods and classicalexplicit methods, a union of both is necessary for real-time and trustworthyperformance. We introduce a novel Convolutional Bayesian Kernel Inference(ConvBKI) layer which incorporates semantic segmentation predictions onlineinto a 3D map through a depthwise convolution layer by leveraging conjugatepriors. We compare ConvBKI against state-of-the-art deep learning approachesand probabilistic algorithms for mapping to evaluate reliability andperformance. We also create a Robot Operating System (ROS) package of ConvBKIand test it on real-world perceptually challenging off-road driving data.</description><author>Joey Wilson, Yuewei Fu, Joshua Friesen, Parker Ewen, Andrew Capodieci, Paramsothy Jayakumar, Kira Barton, Maani Ghaffari</author><pubDate>Tue, 24 Oct 2023 18:30:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16020v1</guid></item><item><title>Differentiable Earth Mover's Distance for Data Compression at the High-Luminosity LHC</title><link>http://arxiv.org/abs/2306.04712v2</link><description>The Earth mover's distance (EMD) is a useful metric for image recognition andclassification, but its usual implementations are not differentiable or tooslow to be used as a loss function for training other algorithms via gradientdescent. In this paper, we train a convolutional neural network (CNN) to learna differentiable, fast approximation of the EMD and demonstrate that it can beused as a substitute for computing-intensive EMD implementations. We apply thisdifferentiable approximation in the training of an autoencoder-inspired neuralnetwork (encoder NN) for data compression at the high-luminosity LHC at CERN.The goal of this encoder NN is to compress the data while preserving theinformation related to the distribution of energy deposits in particledetectors. We demonstrate that the performance of our encoder NN trained usingthe differentiable EMD CNN surpasses that of training with loss functions basedon mean squared error.</description><author>Rohan Shenoy, Javier Duarte, Christian Herwig, James Hirschauer, Daniel Noonan, Maurizio Pierini, Nhan Tran, Cristina Mantilla Suarez</author><pubDate>Tue, 24 Oct 2023 18:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04712v2</guid></item><item><title>Provably Valid and Diverse Mutations of Real-World Media Data for DNN Testing</title><link>http://arxiv.org/abs/2112.01956v2</link><description>Deep neural networks (DNNs) often accept high-dimensional media data (e.g.,photos, text, and audio) and understand their perceptual content (e.g., a cat).To test DNNs, diverse inputs are needed to trigger mis-predictions. Somepreliminary works use byte-level mutations or domain-specific filters (e.g.,foggy), whose enabled mutations may be limited and likely error-prone. SOTAworks employ deep generative models to generate (infinite) inputs. Also, tokeep the mutated inputs perceptually valid (e.g., a cat remains a "cat" aftermutation), existing efforts rely on imprecise and less generalizableheuristics. This study revisits two key objectives in media input mutation - perceptiondiversity (DIV) and validity (VAL) - in a rigorous manner based on manifold, awell-developed theory capturing perceptions of high-dimensional media data in alow-dimensional space. We show important results that DIV and VAL inextricablybound each other, and prove that SOTA generative model-based methodsfundamentally fail to mutate real-world media data (either sacrificing DIV orVAL). In contrast, we discuss the feasibility of mutating real-world media datawith provably high DIV and VAL based on manifold. We concretize the technical solution of mutating media data of variousformats (images, audios, text) via a unified manner based on manifold.Specifically, when media data are projected into a low-dimensional manifold,the data can be mutated by walking on the manifold with certain directions andstep sizes. When contrasted with the input data, the mutated data exhibitencouraging DIV in the perceptual traits (e.g., lying vs. standing dog) whileretaining reasonably high VAL (i.e., a dog remains a dog). We implement ourtechniques in DEEPWALK for testing DNNs. DEEPWALK outperforms prior methods intesting comprehensiveness and can find more error-triggering inputs with higherquality.</description><author>Yuanyuan Yuan, Qi Pang, Shuai Wang</author><pubDate>Tue, 24 Oct 2023 18:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.01956v2</guid></item><item><title>Intelligent Debris Mass Estimation Model for Autonomous Underwater Vehicle</title><link>http://arxiv.org/abs/2309.10617v2</link><description>Marine debris poses a significant threat to the survival of marine wildlife,often leading to entanglement and starvation, ultimately resulting in death.Therefore, removing debris from the ocean is crucial to restore the naturalbalance and allow marine life to thrive. Instance segmentation is an advancedform of object detection that identifies objects and precisely locates andseparates them, making it an essential tool for autonomous underwater vehicles(AUVs) to navigate and interact with their underwater environment effectively.AUVs use image segmentation to analyze images captured by their cameras tonavigate underwater environments. In this paper, we use instance segmentationto calculate the area of individual objects within an image, we use YOLOV7 inRoboflow to generate a set of bounding boxes for each object in the image witha class label and a confidence score for every detection. A segmentation maskis then created for each object by applying a binary mask to the object'sbounding box. The masks are generated by applying a binary threshold to theoutput of a convolutional neural network trained to segment objects from thebackground. Finally, refining the segmentation mask for each object is done byapplying post-processing techniques such as morphological operations andcontour detection, to improve the accuracy and quality of the mask. The processof estimating the area of instance segmentation involves calculating the areaof each segmented instance separately and then summing up the areas of allinstances to obtain the total area. The calculation is carried out usingstandard formulas based on the shape of the object, such as rectangles andcircles. In cases where the object is complex, the Monte Carlo method is usedto estimate the area. This method provides a higher degree of accuracy thantraditional methods, especially when using a large number of samples.</description><author>Mohana Sri S, Swethaa S, Aouthithiye Barathwaj SR Y, Sai Ganesh CS</author><pubDate>Tue, 24 Oct 2023 18:23:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10617v2</guid></item><item><title>A mean-field games laboratory for generative modeling</title><link>http://arxiv.org/abs/2304.13534v5</link><description>We demonstrate the versatility of mean-field games (MFGs) as a mathematicalframework for explaining, enhancing, and designing generative models. Ingenerative flows, a Lagrangian formulation is used where each particle(generated sample) aims to minimize a loss function over its simulated path.The loss, however, is dependent on the paths of other particles, which leads toa competition among the population of particles. The asymptotic behavior ofthis competition yields a mean-field game. We establish connections betweenMFGs and major classes of generative flows and diffusions includingcontinuous-time normalizing flows, score-based generative models (SGM), andWasserstein gradient flows. Furthermore, we study the mathematical propertiesof each generative model by studying their associated MFG's optimalitycondition, which is a set of coupled forward-backward nonlinear partialdifferential equations. The mathematical structure described by the MFGoptimality conditions identifies the inductive biases of generative flows. Weinvestigate the well-posedness and structure of normalizing flows, unravel themathematical structure of SGMs, and derive a MFG formulation of Wassersteingradient flows. From an algorithmic perspective, the optimality conditionsyields Hamilton-Jacobi-Bellman (HJB) regularizers for enhanced training ofgenerative models. In particular, we propose and demonstrate an HJB-regularizedSGM with improved performance over standard SGMs. We present this framework asan MFG laboratory which serves as a platform for revealing new avenues ofexperimentation and invention of generative models.</description><author>Benjamin J. Zhang, Markos A. Katsoulakis</author><pubDate>Tue, 24 Oct 2023 18:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13534v5</guid></item><item><title>Rule Enforcing Through Ordering</title><link>http://arxiv.org/abs/2303.17971v2</link><description>In many real world situations, like minor traffic offenses in big cities, acentral authority is tasked with periodic administering punishments to a largenumber of individuals. Common practice is to give each individual a chance tosuffer a smaller fine and be guaranteed to avoid the legal process withprobable considerably larger punishment. However, thanks to the large number ofoffenders and a limited capacity of the central authority, the individual riskis typically small and a rational individual will not choose to pay the fine.Here we show that if the central authority processes the offenders in apublicly known order, it properly incentives the offenders to pay the fine. Weshow analytically and on realistic experiments that our mechanism promotesnon-cooperation and incentives individuals to pay. Moreover, the same holds foran arbitrary coalition. We quantify the expected total payment the centralauthority receives, and show it increases considerably.</description><author>David Sychrovsk√Ω, Sameer Desai, Martin Loebl</author><pubDate>Tue, 24 Oct 2023 18:16:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17971v2</guid></item><item><title>Provable Guarantees for Generative Behavior Cloning: Bridging Low-Level Stability and High-Level Behavior</title><link>http://arxiv.org/abs/2307.14619v5</link><description>We propose a theoretical framework for studying behavior cloning of complexexpert demonstrations using generative modeling. Our framework invokeslow-level controllers - either learned or implicit in position-command control- to stabilize imitation around expert demonstrations. We show that with (a) asuitable low-level stability guarantee and (b) a powerful enough generativemodel as our imitation learner, pure supervised behavior cloning can generatetrajectories matching the per-time step distribution of essentially arbitraryexpert trajectories in an optimal transport cost. Our analysis relies on astochastic continuity property of the learned policy we call "total variationcontinuity" (TVC). We then show that TVC can be ensured with minimaldegradation of accuracy by combining a popular data-augmentation regimen with anovel algorithmic trick: adding augmentation noise at execution time. Weinstantiate our guarantees for policies parameterized by diffusion models andprove that if the learner accurately estimates the score of the(noise-augmented) expert policy, then the distribution of imitator trajectoriesis close to the demonstrator distribution in a natural optimal transportdistance. Our analysis constructs intricate couplings between noise-augmentedtrajectories, a technique that may be of independent interest. We conclude byempirically validating our algorithmic recommendations, and discussingimplications for future research directions for better behavior cloning withgenerative modeling.</description><author>Adam Block, Ali Jadbabaie, Daniel Pfrommer, Max Simchowitz, Russ Tedrake</author><pubDate>Tue, 24 Oct 2023 18:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14619v5</guid></item><item><title>Human-in-the-Loop Task and Motion Planning for Imitation Learning</title><link>http://arxiv.org/abs/2310.16014v1</link><description>Imitation learning from human demonstrations can teach robots complexmanipulation skills, but is time-consuming and labor intensive. In contrast,Task and Motion Planning (TAMP) systems are automated and excel at solvinglong-horizon tasks, but they are difficult to apply to contact-rich tasks. Inthis paper, we present Human-in-the-Loop Task and Motion Planning (HITL-TAMP),a novel system that leverages the benefits of both approaches. The systememploys a TAMP-gated control mechanism, which selectively gives and takescontrol to and from a human teleoperator. This enables the human teleoperatorto manage a fleet of robots, maximizing data collection efficiency. Thecollected human data is then combined with an imitation learning framework totrain a TAMP-gated policy, leading to superior performance compared to trainingon full task demonstrations. We compared HITL-TAMP to a conventionalteleoperation system -- users gathered more than 3x the number of demos giventhe same time budget. Furthermore, proficient agents (75\%+ success) could betrained from just 10 minutes of non-expert teleoperation data. Finally, wecollected 2.1K demos with HITL-TAMP across 12 contact-rich, long-horizon tasksand show that the system often produces near-perfect agents. Videos andadditional results at https://hitltamp.github.io .</description><author>Ajay Mandlekar, Caelan Garrett, Danfei Xu, Dieter Fox</author><pubDate>Tue, 24 Oct 2023 18:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16014v1</guid></item><item><title>Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series</title><link>http://arxiv.org/abs/2310.14017v2</link><description>Contrastive representation learning is crucial in medical time seriesanalysis as it alleviates dependency on labor-intensive, domain-specific, andscarce expert annotations. However, existing contrastive learning methodsprimarily focus on one single data level, which fails to fully exploit theintricate nature of medical time series. To address this issue, we presentCOMET, an innovative hierarchical framework that leverages data consistenciesat all inherent levels in medical time series. Our meticulously designed modelsystematically captures data consistency from four potential levels:observation, sample, trial, and patient levels. By developing contrastive lossat multiple levels, we can learn effective representations that preservecomprehensive data consistency, maximizing information utilization in aself-supervised manner. We conduct experiments in the challengingpatient-independent setting. We compare COMET against six baselines using threediverse datasets, which include ECG signals for myocardial infarction and EEGsignals for Alzheimer's and Parkinson's diseases. The results demonstrate thatCOMET consistently outperforms all baselines, particularly in setup with 10%and 1% labeled data fractions across all datasets. These results underscore thesignificant impact of our framework in advancing contrastive representationlearning techniques for medical time series. The source code is available athttps://github.com/DL4mHealth/COMET.</description><author>Yihe Wang, Yu Han, Haishuai Wang, Xiang Zhang</author><pubDate>Tue, 24 Oct 2023 18:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14017v2</guid></item><item><title>Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks</title><link>http://arxiv.org/abs/2310.11398v2</link><description>In the realm of deep learning, the self-attention mechanism has substantiatedits pivotal role across a myriad of tasks, encompassing natural languageprocessing and computer vision. Despite achieving success across diverseapplications, the traditional self-attention mechanism primarily leverageslinear transformations for the computation of query, key, and value (QKV),which may not invariably be the optimal choice under specific circumstances.This paper probes into a novel methodology for QKV computation-implementing aspecially-designed neural network structure for the calculation. Utilizing amodified Marian model, we conducted experiments on the IWSLT 2017German-English translation task dataset and juxtaposed our method with theconventional approach. The experimental results unveil a significantenhancement in BLEU scores with our method. Furthermore, our approach alsomanifested superiority when training the Roberta model with the Wikitext-103dataset, reflecting a notable reduction in model perplexity compared to itsoriginal counterpart. These experimental outcomes not only validate theefficacy of our method but also reveal the immense potential in optimizing theself-attention mechanism through neural network-based QKV computation, pavingthe way for future research and practical applications. The source code andimplementation details for our proposed method can be accessed athttps://github.com/ocislyjrti/NeuralAttention.</description><author>Muhan Zhang</author><pubDate>Tue, 24 Oct 2023 18:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11398v2</guid></item><item><title>Towards Understanding Sycophancy in Language Models</title><link>http://arxiv.org/abs/2310.13548v2</link><description>Reinforcement learning from human feedback (RLHF) is a popular technique fortraining high-quality AI assistants. However, RLHF may also encourage modelresponses that match user beliefs over truthful responses, a behavior known assycophancy. We investigate the prevalence of sycophancy in RLHF-trained modelsand whether human preference judgements are responsible. We first demonstratethat five state-of-the-art AI assistants consistently exhibit sycophanticbehavior across four varied free-form text-generation tasks. To understand ifhuman preferences drive this broadly observed behavior of RLHF models, weanalyze existing human preference data. We find that when a response matches auser's views, it is more likely to be preferred. Moreover, both humans andpreference models (PMs) prefer convincingly-written sycophantic responses overcorrect ones a non-negligible fraction of the time. Optimizing model outputsagainst PMs also sometimes sacrifices truthfulness in favor of sycophancy.Overall, our results indicate that sycophancy is a general behavior of RLHFmodels, likely driven in part by human preference judgements favoringsycophantic responses.</description><author>Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez</author><pubDate>Tue, 24 Oct 2023 18:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13548v2</guid></item><item><title>MLFMF: Data Sets for Machine Learning for Mathematical Formalization</title><link>http://arxiv.org/abs/2310.16005v1</link><description>We introduce MLFMF, a collection of data sets for benchmarking recommendationsystems used to support formalization of mathematics with proof assistants.These systems help humans identify which previous entries (theorems,constructions, datatypes, and postulates) are relevant in proving a new theoremor carrying out a new construction. Each data set is derived from a library offormalized mathematics written in proof assistants Agda or Lean. The collectionincludes the largest Lean~4 library Mathlib, and some of the largest Agdalibraries: the standard library, the library of univalent mathematicsAgda-unimath, and the TypeTopology library. Each data set represents thecorresponding library in two ways: as a heterogeneous network, and as a list ofs-expressions representing the syntax trees of all the entries in the library.The network contains the (modular) structure of the library and the referencesbetween entries, while the s-expressions give complete and easily parsedinformation about every entry. We report baseline results using standard graphand word embeddings, tree ensembles, and instance-based learning algorithms.The MLFMF data sets provide solid benchmarking support for furtherinvestigation of the numerous machine learning approaches to formalizedmathematics. The methodology used to extract the networks and the s-expressionsreadily applies to other libraries, and is applicable to other proofassistants. With more than $250\,000$ entries in total, this is currently thelargest collection of formalized mathematical knowledge in machine learnableformat.</description><author>Andrej Bauer, Matej Petkoviƒá, Ljupƒço Todorovski</author><pubDate>Tue, 24 Oct 2023 18:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16005v1</guid></item><item><title>CVPR 2023 Text Guided Video Editing Competition</title><link>http://arxiv.org/abs/2310.16003v1</link><description>Humans watch more than a billion hours of video per day. Most of this videowas edited manually, which is a tedious process. However, AI-enabledvideo-generation and video-editing is on the rise. Building on text-to-imagemodels like Stable Diffusion and Imagen, generative AI has improveddramatically on video tasks. But it's hard to evaluate progress in these videotasks because there is no standard benchmark. So, we propose a new dataset fortext-guided video editing (TGVE), and we run a competition at CVPR to evaluatemodels on our TGVE dataset. In this paper we present a retrospective on thecompetition and describe the winning method. The competition dataset isavailable at https://sites.google.com/view/loveucvpr23/track4.</description><author>Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, Rui He, Feng Hu, Junhua Hu, Hai Huang, Hanyu Zhu, Xu Cheng, Jie Tang, Mike Zheng Shou, Kurt Keutzer, Forrest Iandola</author><pubDate>Tue, 24 Oct 2023 17:56:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16003v1</guid></item><item><title>Learning from Mistakes via Cooperative Study Assistant for Large Language Models</title><link>http://arxiv.org/abs/2305.13829v3</link><description>Large language models (LLMs) have demonstrated their potential to refinetheir generation based on their own feedback. However, the feedback from LLMitself is often inaccurate, thereby limiting its benefits. In this paper, wepropose Study Assistant for Large LAnguage Model (SALAM), a novel frameworkwith an auxiliary agent to assist the main LLM in learning from mistakesthrough interactive cooperation. In the gathering phase, the student assistantagent probes the main LLM, analyzes its errors, and collects the interaction ina mistake memory. During the examination phase, the study assistant providesguidelines by retrieving relevant cases to help the main LLM anticipate andavoid similar errors. We first investigate the effectiveness of a general studyassistant and then customize it to provide LLM-specific guidance throughimitation learning from successful guidance experiences. Our experiments onthree LLMs using two challenging frameworks demonstrate that SALAM cansignificantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 onBBQ.</description><author>Danqing Wang, Lei Li</author><pubDate>Tue, 24 Oct 2023 17:55:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13829v3</guid></item><item><title>Integrating View Conditions for Image Synthesis</title><link>http://arxiv.org/abs/2310.16002v1</link><description>In the field of image processing, applying intricate semantic modificationswithin existing images remains an enduring challenge. This paper introduces apioneering framework that integrates viewpoint information to enhance thecontrol of image editing tasks. By surveying existing object editingmethodologies, we distill three essential criteria, consistency,controllability, and harmony, that should be met for an image editing method.In contrast to previous approaches, our method takes the lead in satisfying allthree requirements for addressing the challenge of image synthesis. Throughcomprehensive experiments, encompassing both quantitative assessments andqualitative comparisons with contemporary state-of-the-art methods, we presentcompelling evidence of our framework's superior performance across multipledimensions. This work establishes a promising avenue for advancing imagesynthesis techniques and empowering precise object modifications whilepreserving the visual coherence of the entire composition.</description><author>Jinbin Bai, Zhen Dong, Aosong Feng, Xiao Zhang, Tian Ye, Kaicheng Zhou, Mike Zheng Shou</author><pubDate>Tue, 24 Oct 2023 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16002v1</guid></item><item><title>Transitivity Recovering Decompositions: Interpretable and Robust Fine-Grained Relationships</title><link>http://arxiv.org/abs/2310.15999v1</link><description>Recent advances in fine-grained representation learning leveragelocal-to-global (emergent) relationships for achieving state-of-the-artresults. The relational representations relied upon by such methods, however,are abstract. We aim to deconstruct this abstraction by expressing them asinterpretable graphs over image views. We begin by theoretically showing thatabstract relational representations are nothing but a way of recoveringtransitive relationships among local views. Based on this, we designTransitivity Recovering Decompositions (TRD), a graph-space search algorithmthat identifies interpretable equivalents of abstract emergent relationships atboth instance and class levels, and with no post-hoc computations. Weadditionally show that TRD is provably robust to noisy views, with empiricalevidence also supporting this finding. The latter allows TRD to perform at paror even better than the state-of-the-art, while being fully interpretable.Implementation is available at https://github.com/abhrac/trd.</description><author>Abhra Chaudhuri, Massimiliano Mancini, Zeynep Akata, Anjan Dutta</author><pubDate>Tue, 24 Oct 2023 17:48:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15999v1</guid></item><item><title>White-box Compiler Fuzzing Empowered by Large Language Models</title><link>http://arxiv.org/abs/2310.15991v1</link><description>Compiler correctness is crucial, as miscompilation falsifying the programbehaviors can lead to serious consequences. In the literature, fuzzing has beenextensively studied to uncover compiler defects. However, compiler fuzzingremains challenging: Existing arts focus on black- and grey-box fuzzing, whichgenerates tests without sufficient understanding of internal compilerbehaviors. As such, they often fail to construct programs to exerciseconditions of intricate optimizations. Meanwhile, traditional white-boxtechniques are computationally inapplicable to the giant codebase of compilers.Recent advances demonstrate that Large Language Models (LLMs) excel in codegeneration/understanding tasks and have achieved state-of-the-art performancein black-box fuzzing. Nonetheless, prompting LLMs with compiler source-codeinformation remains a missing piece of research in compiler testing. To this end, we propose WhiteFox, the first white-box compiler fuzzer usingLLMs with source-code information to test compiler optimization. WhiteFoxadopts a dual-model framework: (i) an analysis LLM examines the low-leveloptimization source code and produces requirements on the high-level testprograms that can trigger the optimization; (ii) a generation LLM produces testprograms based on the summarized requirements. Additionally,optimization-triggering tests are used as feedback to further enhance the testgeneration on the fly. Our evaluation on four popular compilers shows thatWhiteFox can generate high-quality tests to exercise deep optimizationsrequiring intricate conditions, practicing up to 80 more optimizations thanstate-of-the-art fuzzers. To date, WhiteFox has found in total 96 bugs, with 80confirmed as previously unknown and 51 already fixed. Beyond compiler testing,WhiteFox can also be adapted for white-box fuzzing of other complex, real-worldsoftware systems in general.</description><author>Chenyuan Yang, Yinlin Deng, Runyu Lu, Jiayi Yao, Jiawei Liu, Reyhaneh Jabbarvand, Lingming Zhang</author><pubDate>Tue, 24 Oct 2023 17:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15991v1</guid></item><item><title>Quantification of Uncertainty with Adversarial Models</title><link>http://arxiv.org/abs/2307.03217v2</link><description>Quantifying uncertainty is important for actionable predictions in real-worldapplications. A crucial part of predictive uncertainty quantification is theestimation of epistemic uncertainty, which is defined as an integral of theproduct between a divergence function and the posterior. Current methods suchas Deep Ensembles or MC dropout underperform at estimating the epistemicuncertainty, since they primarily consider the posterior when sampling models.We suggest Quantification of Uncertainty with Adversarial Models (QUAM) tobetter estimate the epistemic uncertainty. QUAM identifies regions where thewhole product under the integral is large, not just the posterior.Consequently, QUAM has lower approximation error of the epistemic uncertaintycompared to previous methods. Models for which the product is large correspondto adversarial models (not adversarial examples!). Adversarial models have botha high posterior as well as a high divergence between their predictions andthat of a reference model. Our experiments show that QUAM excels in capturingepistemic uncertainty for deep learning models and outperforms previous methodson challenging tasks in the vision domain.</description><author>Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, G√ºnter Klambauer, Sepp Hochreiter</author><pubDate>Tue, 24 Oct 2023 17:37:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03217v2</guid></item><item><title>Dissecting In-Context Learning of Translations in GPTs</title><link>http://arxiv.org/abs/2310.15987v1</link><description>Most of the recent work in leveraging Large Language Models (LLMs) such asGPT-3 for Machine Translation (MT) has focused on selecting the few-shotsamples for prompting. In this work, we try to better understand the role ofdemonstration attributes for the in-context learning of translations throughperturbations of high-quality, in-domain demonstrations. We find thatasymmetric perturbation of the source-target mappings yield vastly differentresults. We show that the perturbation of the source side has surprisinglylittle impact, while target perturbation can drastically reduce translationquality, suggesting that it is the output text distribution that provides themost important learning signal during in-context learning of translations. Wepropose a method named Zero-Shot-Context to add this signal automatically inZero-Shot prompting. We demonstrate that it improves upon the zero-shottranslation performance of GPT-3, even making it competitive with few-shotprompted translations.</description><author>Vikas Raunak, Hany Hassan Awadalla, Arul Menezes</author><pubDate>Tue, 24 Oct 2023 17:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15987v1</guid></item><item><title>Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning</title><link>http://arxiv.org/abs/2310.15985v1</link><description>This paper presents a novel approach to Single-Positive Multi-label Learning.In general multi-label learning, a model learns to predict multiple labels orcategories for a single input image. This is in contrast with standardmulti-class image classification, where the task is predicting a single labelfrom many possible labels for an image. Single-Positive Multi-label Learning(SPML) specifically considers learning to predict multiple labels when there isonly a single annotation per image in the training data. Multi-label learningis in many ways a more realistic task than single-label learning as real-worlddata often involves instances belonging to multiple categories simultaneously;however, most common computer vision datasets predominantly contain singlelabels due to the inherent complexity and cost of collecting multiple highquality annotations for each instance. We propose a novel approach calledVision-Language Pseudo-Labeling (VLPL), which uses a vision-language model tosuggest strong positive and negative pseudo-labels, and outperforms the currentSOTA methods by 5.5% on Pascal VOC, 18.4% on MS-COCO, 15.2% on NUS-WIDE, and8.4% on CUB-Birds. Our code and data are available athttps://github.com/mvrl/VLPL.</description><author>Xin Xing, Zhexiao Xiong, Abby Stylianou, Srikumar Sastry, Liyu Gong, Nathan Jacobs</author><pubDate>Tue, 24 Oct 2023 17:36:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15985v1</guid></item><item><title>Large-scale Bayesian Structure Learning for Gaussian Graphical Models using Marginal Pseudo-likelihood</title><link>http://arxiv.org/abs/2307.00127v2</link><description>Bayesian methods for learning Gaussian graphical models offer a robustframework that addresses model uncertainty and incorporates prior knowledge.Despite their theoretical strengths, the applicability of Bayesian methods isoften constrained by computational needs, especially in modern contextsinvolving thousands of variables. To overcome this issue, we introduce twonovel Markov chain Monte Carlo (MCMC) search algorithms that have asignificantly lower computational cost than leading Bayesian approaches. Ourproposed MCMC-based search algorithms use the marginal pseudo-likelihoodapproach to bypass the complexities of computing intractable normalizingconstants and iterative precision matrix sampling. These algorithms can deliverreliable results in mere minutes on standard computers, even for large-scaleproblems with one thousand variables. Furthermore, our proposed method iscapable of addressing model uncertainty by efficiently exploring the fullposterior graph space. Our simulation study indicates that the proposedalgorithms, particularly for large-scale sparse graphs, outperform the leadingBayesian approaches in terms of computational efficiency and precision. Theimplementation supporting the new approach is available through the R packageBDgraph.</description><author>Reza Mohammadi, Marit Schoonhoven, Lucas Vogels, S. Ilker Birbil</author><pubDate>Tue, 24 Oct 2023 17:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00127v2</guid></item><item><title>Geometry-Aware Video Quality Assessment for Dynamic Digital Human</title><link>http://arxiv.org/abs/2310.15984v1</link><description>Dynamic Digital Humans (DDHs) are 3D digital models that are animated usingpredefined motions and are inevitably bothered by noise/shift during thegeneration process and compression distortion during the transmission process,which needs to be perceptually evaluated. Usually, DDHs are displayed as 2Drendered animation videos and it is natural to adapt video quality assessment(VQA) methods to DDH quality assessment (DDH-QA) tasks. However, the VQAmethods are highly dependent on viewpoints and less sensitive to geometry-baseddistortions. Therefore, in this paper, we propose a novel no-reference (NR)geometry-aware video quality assessment method for DDH-QA challenge. Geometrycharacteristics are described by the statistical parameters estimated from theDDHs' geometry attribute distributions. Spatial and temporal features areacquired from the rendered videos. Finally, all kinds of features areintegrated and regressed into quality values. Experimental results show thatthe proposed method achieves state-of-the-art performance on the DDH-QAdatabase.</description><author>Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Guangtao Zhai</author><pubDate>Tue, 24 Oct 2023 17:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15984v1</guid></item><item><title>NA-SODINN: a deep learning algorithm for exoplanet image detection based on residual noise regimes</title><link>http://arxiv.org/abs/2302.02854v2</link><description>Supervised deep learning was recently introduced in high-contrast imaging(HCI) through the SODINN algorithm, a convolutional neural network designed forexoplanet detection in angular differential imaging (ADI) datasets. Thebenchmarking of HCI algorithms within the Exoplanet Imaging Data Challenge(EIDC) showed that (i) SODINN can produce a high number of false positives inthe final detection maps, and (ii) algorithms processing images in a more localmanner perform better. This work aims to improve the SODINN detectionperformance by introducing new local processing approaches and adapting itslearning process accordingly. We propose NA-SODINN, a new deep learning binaryclassifier based on a convolutional neural network (CNN) that better capturesimage noise correlations in ADI-processed frames by identifying noise regimes.Our new approach was tested against its predecessor, as well as twoSODINN-based hybrid models and a more standard annular-PCA approach, throughlocal receiving operating characteristics (ROC) analysis of ADI sequences fromthe VLT/SPHERE and Keck/NIRC-2 instruments. Results show that NA-SODINNenhances SODINN in both sensitivity and specificity, especially in thespeckle-dominated noise regime. NA-SODINN is also benchmarked against thecomplete set of submitted detection algorithms in EIDC, in which we show thatits final detection score matches or outperforms the most powerful detectionalgorithms.Throughout the supervised machine learning case, this studyillustrates and reinforces the importance of adapting the task of detection tothe local content of processed images.</description><author>Carles Cantero, Olivier Absil, Carl-Henrik Dahlqvist, Marc Van Droogenbroeck</author><pubDate>Tue, 24 Oct 2023 17:32:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02854v2</guid></item><item><title>RADAR: Robust AI-Text Detection via Adversarial Learning</title><link>http://arxiv.org/abs/2307.03838v2</link><description>Recent advances in large language models (LLMs) and the intensifyingpopularity of ChatGPT-like applications have blurred the boundary ofhigh-quality text generation between humans and machines. However, in additionto the anticipated revolutionary changes to our technology and society, thedifficulty of distinguishing LLM-generated texts (AI-text) from human-generatedtexts poses new challenges of misuse and fairness, such as fake contentgeneration, plagiarism, and false accusations of innocent writers. Whileexisting works show that current AI-text detectors are not robust to LLM-basedparaphrasing, this paper aims to bridge this gap by proposing a new frameworkcalled RADAR, which jointly trains a robust AI-text detector via adversariallearning. RADAR is based on adversarial training of a paraphraser and adetector. The paraphraser's goal is to generate realistic content to evadeAI-text detection. RADAR uses the feedback from the detector to update theparaphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,experimental results show that RADAR significantly outperforms existing AI-textdetection methods, especially when paraphrasing is in place. We also identifythe strong transferability of RADAR from instruction-tuned LLMs to other LLMs,and evaluate the improved capability of RADAR via GPT-3.5-Turbo.</description><author>Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho</author><pubDate>Tue, 24 Oct 2023 17:31:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03838v2</guid></item><item><title>Graph Deep Learning for Time Series Forecasting</title><link>http://arxiv.org/abs/2310.15978v1</link><description>Graph-based deep learning methods have become popular tools to processcollections of correlated time series. Differently from traditionalmultivariate forecasting methods, neural graph-based predictors take advantageof pairwise relationships by conditioning forecasts on a (possibly dynamic)graph spanning the time series collection. The conditioning can take the formof an architectural inductive bias on the neural forecasting architecture,resulting in a family of deep learning models called spatiotemporal graphneural networks. Such relational inductive biases enable the training of globalforecasting models on large time-series collections, while at the same timelocalizing predictions w.r.t. each element in the set (i.e., graph nodes) byaccounting for local correlations among them (i.e., graph edges). Indeed,recent theoretical and practical advances in graph neural networks and deeplearning for time series forecasting make the adoption of such processingframeworks appealing and timely. However, most of the studies in the literaturefocus on proposing variations of existing neural architectures by takingadvantage of modern deep learning practices, while foundational andmethodological aspects have not been subject to systematic investigation. Tofill the gap, this paper aims to introduce a comprehensive methodologicalframework that formalizes the forecasting problem and provides designprinciples for graph-based predictive models and methods to assess theirperformance. At the same time, together with an overview of the field, weprovide design guidelines, recommendations, and best practices, as well as anin-depth discussion of open challenges and future research directions.</description><author>Andrea Cini, Ivan Marisca, Daniele Zambon, Cesare Alippi</author><pubDate>Tue, 24 Oct 2023 17:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15978v1</guid></item><item><title>Data-driven Traffic Simulation: A Comprehensive Review</title><link>http://arxiv.org/abs/2310.15975v1</link><description>Autonomous vehicles (AVs) have the potential to significantly revolutionizesociety by providing a secure and efficient mode of transportation. Recentyears have witnessed notable advance-ments in autonomous driving perception andprediction, but the challenge of validating the performance of AVs remainslargely unresolved. Data-driven microscopic traffic simulation has be-come animportant tool for autonomous driving testing due to 1) availability ofhigh-fidelity traffic data; 2) its advantages of ena-bling large-scale testingand scenario reproducibility; and 3) its potential in reactive and realistictraffic simulation. However, a comprehensive review of this topic is currentlylacking. This pa-per aims to fill this gap by summarizing relevant studies. Theprimary objective of this paper is to review current research ef-forts andprovide a futuristic perspective that will benefit future developments in thefield. It introduces the general issues of data-driven traffic simulation andoutlines key concepts and terms. After overviewing traffic simulation, variousdatasets and evalua-tion metrics commonly used are reviewed. The paper thenoffers a comprehensive evaluation of imitation learning, reinforcementlearning, generative and deep learning methods, summarizing each and analyzingtheir advantages and disadvantages in detail. Moreover, it evaluates thestate-of-the-art, existing challenges, and future research directions.</description><author>Di Chen, Meixin Zhu, Hao Yang, Xuesong Wang, Yinhai Wang</author><pubDate>Tue, 24 Oct 2023 17:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15975v1</guid></item><item><title>Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex Optimization</title><link>http://arxiv.org/abs/2310.15976v1</link><description>signSGD is popular in nonconvex optimization due to its communicationefficiency. Yet, existing analyses of signSGD rely on assuming that data aresampled with replacement in each iteration, contradicting the practicalimplementation where data are randomly reshuffled and sequentially fed into thealgorithm. We bridge this gap by proving the first convergence result ofsignSGD with random reshuffling (SignRR) for nonconvex optimization. Given thedataset size $n$, the number of epochs of data passes $T$, and the variancebound of a stochastic gradient $\sigma^2$, we show that SignRR has the sameconvergence rate $O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ as signSGD\citep{bernstein2018signsgd}. We then present SignRVR and SignRVM, whichleverage variance-reduced gradients and momentum updates respectively, bothconverging at $O(\log(nT)/\sqrt{nT})$. In contrast with the analysis ofsignSGD, our results do not require an extremely large batch size in eachiteration to be of the same order as the total number of iterations\citep{bernstein2018signsgd} or the signs of stochastic and true gradientsmatch element-wise with a minimum probability of 1/2\citep{safaryan2021stochastic}. We also extend our algorithms to cases wheredata are distributed across different machines, yielding dist-SignRVR anddist-SignRVM, both converging at $O(\log(n_0T)/\sqrt{n_0T})$, where $n_0$ isthe dataset size of a single machine. We back up our theoretical findingsthrough experiments on simulated and real-world problems, verifying thatrandomly reshuffled sign methods match or surpass existing baselines.</description><author>Zhen Qin, Zhishuai Liu, Pan Xu</author><pubDate>Tue, 24 Oct 2023 17:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15976v1</guid></item><item><title>Nonlinear model reduction for slow-fast stochastic systems near unknown invariant manifolds</title><link>http://arxiv.org/abs/2104.02120v2</link><description>We introduce a nonlinear stochastic model reduction technique forhigh-dimensional stochastic dynamical systems that have a low-dimensionalinvariant effective manifold with slow dynamics, and high-dimensional, largefast modes. Given only access to a black box simulator from which short burstsof simulation can be obtained, we design an algorithm that outputs an estimateof the invariant manifold, a process of the effective stochastic dynamics onit, which has averaged out the fast modes, and a simulator thereof. Thissimulator is efficient in that it exploits of the low dimension of theinvariant manifold, and takes time steps of size dependent on the regularity ofthe effective process, and therefore typically much larger than that of theoriginal simulator, which had to resolve the fast modes. The algorithm and theestimation can be performed on-the-fly, leading to efficient exploration of theeffective state space, without losing consistency with the underlying dynamics.This construction enables fast and efficient simulation of paths of theeffective dynamics, together with estimation of crucial features andobservables of such dynamics, including the stationary distribution,identification of metastable states, and residence times and transition ratesbetween them.</description><author>Felix X. -F. Ye, Sichen Yang, Mauro Maggioni</author><pubDate>Tue, 24 Oct 2023 17:23:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.02120v2</guid></item><item><title>DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization</title><link>http://arxiv.org/abs/2208.09708v3</link><description>Efficiently deploying deep neural networks on low-resource edge devices ischallenging due to their ever-increasing resource requirements. To address thisissue, researchers have proposed multiplication-free neural networks, such asPower-of-Two quantization, or also known as Shift networks, which aim to reducememory usage and simplify computation. However, existing low-bit Shift networksare not as accurate as their full-precision counterparts, typically sufferingfrom limited weight range encoding schemes and quantization loss. In thispaper, we propose the DenseShift network, which significantly improves theaccuracy of Shift networks, achieving competitive performance to full-precisionnetworks for vision and speech applications. In addition, we introduce a methodto deploy an efficient DenseShift network using non-quantized floating-pointactivations, while obtaining 1.6X speed-up over existing methods. To achievethis, we demonstrate that zero-weight values in low-bit Shift networks do notcontribute to model capacity and negatively impact inference computation. Toaddress this issue, we propose a zero-free shifting mechanism that simplifiesinference and increases model capacity. We further propose a sign-scaledecomposition design to enhance training efficiency and a low-variance randominitialization strategy to improve the model's transfer learning performance.Our extensive experiments on various computer vision and speech tasksdemonstrate that DenseShift outperforms existing low-bit multiplication-freenetworks and achieves competitive performance compared to full-precisionnetworks. Furthermore, our proposed approach exhibits strong transfer learningperformance without a drop in accuracy. Our code was released on GitHub.</description><author>Xinlin Li, Bang Liu, Rui Heng Yang, Vanessa Courville, Chao Xing, Vahid Partovi Nia</author><pubDate>Tue, 24 Oct 2023 17:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09708v3</guid></item><item><title>Minimax Forward and Backward Learning of Evolving Tasks with Performance Guarantees</title><link>http://arxiv.org/abs/2310.15974v1</link><description>For a sequence of classification tasks that arrive over time, it is commonthat tasks are evolving in the sense that consecutive tasks often have a highersimilarity. The incremental learning of a growing sequence of tasks holdspromise to enable accurate classification even with few samples per task byleveraging information from all the tasks in the sequence (forward and backwardlearning). However, existing techniques developed for continual learning andconcept drift adaptation are either designed for tasks with time-independentsimilarities or only aim to learn the last task in the sequence. This paperpresents incremental minimax risk classifiers (IMRCs) that effectively exploitforward and backward learning and account for evolving tasks. In addition, weanalytically characterize the performance improvement provided by forward andbackward learning in terms of the tasks' expected quadratic change and thenumber of tasks. The experimental evaluation shows that IMRCs can result in asignificant performance improvement, especially for reduced sample sizes.</description><author>Ver√≥nica √Ålvarez, Santiago Mazuelas, Jose A. Lozano</author><pubDate>Tue, 24 Oct 2023 17:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15974v1</guid></item><item><title>Ask Language Model to Clean Your Noisy Translation Data</title><link>http://arxiv.org/abs/2310.13469v3</link><description>Transformer models have demonstrated remarkable performance in neural machinetranslation (NMT). However, their vulnerability to noisy input poses asignificant challenge in practical implementation, where generating cleanoutput from noisy input is crucial. The MTNT dataset is widely used as abenchmark for evaluating the robustness of NMT models against noisy input.Nevertheless, its utility is limited due to the presence of noise in both thesource and target sentences. To address this limitation, we focus on cleaningthe noise from the target sentences in MTNT, making it more suitable as abenchmark for noise evaluation. Leveraging the capabilities of large languagemodels (LLMs), we observe their impressive abilities in noise removal. Forexample, they can remove emojis while considering their semantic meaning.Additionally, we show that LLM can effectively rephrase slang, jargon, andprofanities. The resulting datasets, called C-MTNT, exhibit significantly lessnoise in the target sentences while preserving the semantic integrity of theoriginal sentences. Our human and GPT-4 evaluations also lead to a consistentconclusion that LLM performs well on this task. Lastly, experiments on C-MTNTshowcased its effectiveness in evaluating the robustness of NMT models,highlighting the potential of advanced language models for data cleaning andemphasizing C-MTNT as a valuable resource.</description><author>Quinten Bolding, Baohao Liao, Brandon James Denis, Jun Luo, Christof Monz</author><pubDate>Tue, 24 Oct 2023 17:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13469v3</guid></item><item><title>Accented Speech Recognition With Accent-specific Codebooks</title><link>http://arxiv.org/abs/2310.15970v1</link><description>Speech accents pose a significant challenge to state-of-the-art automaticspeech recognition (ASR) systems. Degradation in performance acrossunderrepresented accents is a severe deterrent to the inclusive adoption ofASR. In this work, we propose a novel accent adaptation approach for end-to-endASR systems using cross-attention with a trainable set of codebooks. Theselearnable codebooks capture accent-specific information and are integratedwithin the ASR encoder layers. The model is trained on accented English speech,while the test data also contained accents which were not seen during training.On the Mozilla Common Voice multi-accented dataset, we show that our proposedapproach yields significant performance gains not only on the seen Englishaccents (up to $37\%$ relative improvement in word error rate) but also on theunseen accents (up to $5\%$ relative improvement in WER). Further, weillustrate benefits for a zero-shot transfer setup on the L2Artic dataset. Wealso compare the performance with other approaches based on accent adversarialtraining.</description><author>Darshan Prabhu, Preethi Jyothi, Sriram Ganapathy, Vinit Unni</author><pubDate>Tue, 24 Oct 2023 17:10:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15970v1</guid></item><item><title>Constructing and Machine Learning Calabi-Yau Five-folds</title><link>http://arxiv.org/abs/2310.15966v1</link><description>We construct all possible complete intersection Calabi-Yau five-folds in aproduct of four or less complex projective spaces, with up to four constraints.We obtain $27068$ spaces, which are not related by permutations of rows andcolumns of the configuration matrix, and determine the Euler number for all ofthem. Excluding the $3909$ product manifolds among those, we calculate thecohomological data for $12433$ cases, i.e. $53.7 \%$ of the non-product spaces,obtaining $2375$ different Hodge diamonds. The dataset containing all the aboveinformation is available athttps://www.dropbox.com/scl/fo/z7ii5idt6qxu36e0b8azq/h?rlkey=0qfhx3tykytduobpld510gsfy&amp;dl=0. The distributions of the invariants are presented, and a comparison with thelower-dimensional analogues is discussed. Supervised machine learning isperformed on the cohomological data, via classifier and regressor (both fullyconnected and convolutional) neural networks. We find that $h^{1,1}$ can belearnt very efficiently, with very high $R^2$ score and an accuracy of $96\%$,i.e. $96 \%$ of the predictions exactly match the correct values. For$h^{1,4},h^{2,3}, \eta$, we also find very high $R^2$ scores, but the accuracyis lower, due to the large ranges of possible values.</description><author>R. Alawadhi, D. Angella, A. Leonardo, T. Schettini Gherardini</author><pubDate>Tue, 24 Oct 2023 17:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15966v1</guid></item><item><title>Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation</title><link>http://arxiv.org/abs/2310.15961v1</link><description>Despite the promise of Mixture of Experts (MoE) models in increasingparameter counts of Transformer models while maintaining training and inferencecosts, their application carries notable drawbacks. The key strategy of thesemodels is to, for each processed token, activate at most a few experts -subsets of an extensive feed-forward layer. But this approach is not withoutits challenges. The operation of matching experts and tokens is discrete, whichmakes MoE models prone to issues like training instability and uneven expertutilization. Existing techniques designed to address these concerns, such asauxiliary losses or balance-aware matching, result either in lower modelperformance or are more difficult to train. In response to these issues, wepropose Mixture of Tokens, a fully-differentiable model that retains thebenefits of MoE architectures while avoiding the aforementioned difficulties.Rather than routing tokens to experts, this approach mixes tokens fromdifferent examples prior to feeding them to experts, enabling the model tolearn from all token-expert combinations. Importantly, this mixing can bedisabled to avoid mixing of different sequences during inference. Crucially,this method is fully compatible with both masked and causal Large LanguageModel training and inference.</description><author>Szymon Antoniak, Sebastian Jaszczur, Micha≈Ç Krutul, Maciej Pi√≥ro, Jakub Krajewski, Jan Ludziejewski, Tomasz Odrzyg√≥≈∫d≈∫, Marek Cygan</author><pubDate>Tue, 24 Oct 2023 17:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15961v1</guid></item><item><title>Towards Visual Saliency Explanations of Face Verification</title><link>http://arxiv.org/abs/2305.08546v4</link><description>In the past years, deep convolutional neural networks have been pushing thefrontier of face recognition (FR) techniques in both verification andidentification scenarios. Despite the high accuracy, they are often criticizedfor lacking explainability. There has been an increasing demand forunderstanding the decision-making process of deep face recognition systems.Recent studies have investigated the usage of visual saliency maps as anexplanation, but they often lack a discussion and analysis in the context offace recognition. This paper concentrates on explainable face verificationtasks and conceives a new explanation framework. Firstly, a definition of thesaliency-based explanation method is provided, which focuses on the decisionsmade by the deep FR model. Secondly, a new model-agnostic explanation methodnamed CorrRISE is proposed to produce saliency maps, which reveal both thesimilar and dissimilar regions of any given pair of face images. Then, anevaluation methodology is designed to measure the performance of general visualsaliency explanation methods in face verification. Finally, substantial visualand quantitative results have shown that the proposed CorrRISE methoddemonstrates promising results in comparison with other state-of-the-artexplainable face verification approaches.</description><author>Yuhang Lu, Zewei Xu, Touradj Ebrahimi</author><pubDate>Tue, 24 Oct 2023 17:02:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08546v4</guid></item><item><title>PuoBERTa: Training and evaluation of a curated language model for Setswana</title><link>http://arxiv.org/abs/2310.09141v2</link><description>Natural language processing (NLP) has made significant progress forwell-resourced languages such as English but lagged behind for low-resourcelanguages like Setswana. This paper addresses this gap by presenting PuoBERTa,a customised masked language model trained specifically for Setswana. We coverhow we collected, curated, and prepared diverse monolingual texts to generate ahigh-quality corpus for PuoBERTa's training. Building upon previous efforts increating monolingual resources for Setswana, we evaluated PuoBERTa acrossseveral NLP tasks, including part-of-speech (POS) tagging, named entityrecognition (NER), and news categorisation. Additionally, we introduced a newSetswana news categorisation dataset and provided the initial benchmarks usingPuoBERTa. Our work demonstrates the efficacy of PuoBERTa in fostering NLPcapabilities for understudied languages like Setswana and paves the way forfuture research directions.</description><author>Vukosi Marivate, Moseli Mots'Oehli, Valencia Wagner, Richard Lastrucci, Isheanesu Dzingirai</author><pubDate>Tue, 24 Oct 2023 17:01:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09141v2</guid></item><item><title>NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes</title><link>http://arxiv.org/abs/2310.15959v1</link><description>The detailed clinical records drafted by doctors after each patient's visitare crucial for medical practitioners and researchers. Automating the creationof these notes with language models can reduce the workload of doctors.However, training such models can be difficult due to the limited publicavailability of conversations between patients and doctors. In this paper, weintroduce NoteChat, a cooperative multi-agent framework leveraging LargeLanguage Models (LLMs) for generating synthetic doctor-patient conversationsconditioned on clinical notes. NoteChat consists of Planning, Roleplay, andPolish modules. We provide a comprehensive automatic and human evaluation ofNoteChat, comparing it with state-of-the-art models, including OpenAI's ChatGPTand GPT-4. Results demonstrate that NoteChat facilitates high-quality syntheticdoctor-patient conversations, underscoring the untapped potential of LLMs inhealthcare. This work represents the first instance of multiple LLMscooperating to complete a doctor-patient conversation conditioned on clinicalnotes, offering promising avenues for the intersection of AI and healthcare</description><author>Junda Wang, Zonghai Yao, Zhichao Yang, Huixue Zhou, Rumeng Li, Xun Wang, Yucheng Xu, Hong Yu</author><pubDate>Tue, 24 Oct 2023 16:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15959v1</guid></item><item><title>Learning to Generate Parameters of ConvNets for Unseen Image Data</title><link>http://arxiv.org/abs/2310.11862v2</link><description>Typical Convolutional Neural Networks (ConvNets) depend heavily on largeamounts of image data and resort to an iterative optimization algorithm (e.g.,SGD or Adam) to learn network parameters, which makes training very time- andresource-intensive. In this paper, we propose a new training paradigm andformulate the parameter learning of ConvNets into a prediction task: given aConvNet architecture, we observe there exists correlations between imagedatasets and their corresponding optimal network parameters, and explore if wecan learn a hyper-mapping between them to capture the relations, such that wecan directly predict the parameters of the network for an image dataset neverseen during the training phase. To do this, we put forward a new hypernetworkbased model, called PudNet, which intends to learn a mapping between datasetsand their corresponding network parameters, and then predicts parameters forunseen data with only a single forward propagation. Moreover, our modelbenefits from a series of adaptive hyper recurrent units sharing weights tocapture the dependencies of parameters among different network layers.Extensive experiments demonstrate that our proposed method achieves goodefficacy for unseen image datasets on two kinds of settings: Intra-datasetprediction and Inter-dataset prediction. Our PudNet can also well scale up tolarge-scale datasets, e.g., ImageNet-1K. It takes 8967 GPU seconds to trainResNet-18 on the ImageNet-1K using GC from scratch and obtain a top-5 accuracyof 44.65 %. However, our PudNet costs only 3.89 GPU seconds to predict thenetwork parameters of ResNet-18 achieving comparable performance (44.92 %),more than 2,300 times faster than the traditional training paradigm.</description><author>Shiye Wang, Kaituo Feng, Changsheng Li, Ye Yuan, Guoren Wang</author><pubDate>Tue, 24 Oct 2023 16:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11862v2</guid></item><item><title>Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation</title><link>http://arxiv.org/abs/2310.14892v2</link><description>Controllable text generation (CTG) aims to generate text with desiredattributes, and decoding-time-based methods have shown promising performance onthis task. However, in this paper, we identify the phenomenon of AttributeCollapse for the first time. It causes the fluency of generated text to rapidlydecrease when the control strength exceeds a critical value, rendering the textcompletely unusable. This limitation hinders the effectiveness of decodingmethods in achieving high levels of controllability. To address this problem,we propose a novel lightweight decoding framework named Air-Decoding. Its mainidea is reconstructing the attribute distributions to balance the weightsbetween attribute words and non-attribute words to generate more fluent text.Specifically, we train prefixes by prefix-tuning to obtain attributedistributions. Then we design a novel attribute distribution reconstructionmethod to balance the obtained distributions and use the reconstructeddistributions to guide language models for generation, effectively avoiding theissue of Attribute Collapse. Experiments on multiple CTG tasks prove that ourmethod achieves a new state-of-the-art control performance.</description><author>Tianqi Zhong, Quan Wang, Jingxuan Han, Yongdong Zhang, Zhendong Mao</author><pubDate>Tue, 24 Oct 2023 16:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14892v2</guid></item><item><title>Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection</title><link>http://arxiv.org/abs/2310.15955v1</link><description>The introduction of DETR represents a new paradigm for object detection.However, its decoder conducts classification and box localization using sharedqueries and cross-attention layers, leading to suboptimal results. We observethat different regions of interest in the visual feature map are suitable forperforming query classification and box localization tasks, even for the sameobject. Salient regions provide vital information for classification, while theboundaries around them are more favorable for box regression. Unfortunately,such spatial misalignment between these two tasks greatly hinders DETR'straining. Therefore, in this work, we focus on decoupling localization andclassification tasks in DETR. To achieve this, we introduce a new design schemecalled spatially decoupled DETR (SD-DETR), which includes a task-aware querygeneration module and a disentangled feature learning process. We elaboratelydesign the task-aware query initialization process and divide thecross-attention block in the decoder to allow the task-aware queries to matchdifferent visual regions. Meanwhile, we also observe that the predictionmisalignment problem for high classification confidence and preciselocalization exists, so we propose an alignment loss to further guide thespatially decoupled DETR training. Through extensive experiments, wedemonstrate that our approach achieves a significant improvement in MSCOCOdatasets compared to previous work. For instance, we improve the performance ofConditional DETR by 4.5 AP. By spatially disentangling the two tasks, ourmethod overcomes the misalignment problem and greatly improves the performanceof DETR for object detection.</description><author>Manyuan Zhang, Guanglu Song, Yu Liu, Hongsheng Li</author><pubDate>Tue, 24 Oct 2023 16:54:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15955v1</guid></item><item><title>Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles</title><link>http://arxiv.org/abs/2310.15952v1</link><description>While deep learning models have achieved remarkable success across a range ofmedical image analysis tasks, deployment of these models in real clinicalcontexts requires that they be robust to variability in the acquired images.While many methods apply predefined transformations to augment the trainingdata to enhance test-time robustness, these transformations may not ensure themodel's robustness to the diverse variability seen in patient images. In thispaper, we introduce a novel three-stage approach based on transformers coupledwith conditional diffusion models, with the goal of improving model robustnessto the kinds of imaging variability commonly encountered in practice withoutthe need for pre-determined data augmentation strategies. To this end, multipleimage encoders first learn hierarchical feature representations to builddiscriminative latent spaces. Next, a reverse diffusion process, guided by thelatent code, acts on an informative prior and proposes prediction candidates ina generative manner. Finally, several prediction candidates are aggregated in abi-level aggregation protocol to produce the final output. Through extensiveexperiments on medical imaging benchmark datasets, we show that our methodimproves upon state-of-the-art methods in terms of robustness and confidencecalibration. Additionally, we introduce a strategy to quantify the predictionuncertainty at the instance level, increasing their trustworthiness toclinicians using them in clinical practice.</description><author>Xing Shen, Hengguan Huang, Brennan Nichyporuk, Tal Arbel</author><pubDate>Tue, 24 Oct 2023 16:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15952v1</guid></item><item><title>Weighted Distance Nearest Neighbor Condensing</title><link>http://arxiv.org/abs/2310.15951v1</link><description>The problem of nearest neighbor condensing has enjoyed a long history ofstudy, both in its theoretical and practical aspects. In this paper, weintroduce the problem of weighted distance nearest neighbor condensing, whereone assigns weights to each point of the condensed set, and then new points arelabeled based on their weighted distance nearest neighbor in the condensed set. We study the theoretical properties of this new model, and show that it canproduce dramatically better condensing than the standard nearest neighbor rule,yet is characterized by generalization bounds almost identical to the latter.We then suggest a condensing heuristic for our new problem. We demonstrateBayes consistency for this heuristic, and also show promising empiricalresults.</description><author>Lee-Ad Gottlieb, Timor Sharabi, Roi Weiss</author><pubDate>Tue, 24 Oct 2023 16:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15951v1</guid></item><item><title>Representation Learning with Large Language Models for Recommendation</title><link>http://arxiv.org/abs/2310.15950v1</link><description>Recommender systems have seen significant advancements with the influence ofdeep learning and graph neural networks, particularly in capturing complexuser-item relationships. However, these graph-based recommenders heavily dependon ID-based data, potentially disregarding valuable textual informationassociated with users and items, resulting in less informative learnedrepresentations. Moreover, the utilization of implicit feedback data introducespotential noise and bias, posing challenges for the effectiveness of userpreference learning. While the integration of large language models (LLMs) intotraditional ID-based recommenders has gained attention, challenges such asscalability issues, limitations in text-only reliance, and prompt inputconstraints need to be addressed for effective implementation in practicalrecommender systems. To address these challenges, we propose a model-agnosticframework RLMRec that aims to enhance existing recommenders with LLM-empoweredrepresentation learning. It proposes a recommendation paradigm that integratesrepresentation learning with LLMs to capture intricate semantic aspects of userbehaviors and preferences. RLMRec incorporates auxiliary textual signals,develops a user/item profiling paradigm empowered by LLMs, and aligns thesemantic space of LLMs with the representation space of collaborativerelational signals through a cross-view alignment framework. This work furtherestablish a theoretical foundation demonstrating that incorporating textualsignals through mutual information maximization enhances the quality ofrepresentations. In our evaluation, we integrate RLMRec with state-of-the-artrecommender models, while also analyzing its efficiency and robustness to noisedata. Our implementation codes are available athttps://github.com/HKUDS/RLMRec.</description><author>Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang</author><pubDate>Tue, 24 Oct 2023 16:51:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15950v1</guid></item><item><title>Language-driven Scene Synthesis using Multi-conditional Diffusion Model</title><link>http://arxiv.org/abs/2310.15948v1</link><description>Scene synthesis is a challenging problem with several industrialapplications. Recently, substantial efforts have been directed to synthesizethe scene using human motions, room layouts, or spatial graphs as the input.However, few studies have addressed this problem from multiple modalities,especially combining text prompts. In this paper, we propose a language-drivenscene synthesis task, which is a new task that integrates text prompts, humanmotion, and existing objects for scene synthesis. Unlike other single-conditionsynthesis tasks, our problem involves multiple conditions and requires astrategy for processing and encoding them into a unified space. To address thechallenge, we present a multi-conditional diffusion model, which differs fromthe implicit unification approach of other diffusion literature by explicitlypredicting the guiding points for the original data distribution. Wedemonstrate that our approach is theoretically supportive. The intensiveexperiment results illustrate that our method outperforms state-of-the-artbenchmarks and enables natural scene editing applications. The source code anddataset can be accessed at https://lang-scene-synth.github.io/.</description><author>An Vuong, Minh Nhat Vu, Toan Tien Nguyen, Baoru Huang, Dzung Nguyen, Thieu Vo, Anh Nguyen</author><pubDate>Tue, 24 Oct 2023 16:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15948v1</guid></item><item><title>Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations</title><link>http://arxiv.org/abs/2309.13837v3</link><description>This article introduces an advanced analytical approach for predictingbackorders in inventory management. Backorder refers to an order that cannot beimmediately fulfilled due to stock depletion. Multiple classificationtechniques, including Balanced Bagging Classifiers, Fuzzy Logic, VariationalAutoencoder - Generative Adversarial Networks, and Multi-layer Perceptronclassifiers, are assessed in this work using performance evaluation metricssuch as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit functionand misclassification costs, considering the financial implications and costsassociated with inventory management and backorder handling. The study suggeststhat a combination of modeling approaches, including ensemble techniques andVAE, can effectively address imbalanced datasets in inventory management,emphasizing interpretability and reducing false positives and false negatives.This research contributes to the advancement of predictive analytics and offersvaluable insights for future investigations in backorder forecasting andinventory control optimization for decision-making.</description><author>Sarit Maitra, Sukanya Kundu</author><pubDate>Tue, 24 Oct 2023 16:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13837v3</guid></item><item><title>ShARc: Shape and Appearance Recognition for Person Identification In-the-wild</title><link>http://arxiv.org/abs/2310.15946v1</link><description>Identifying individuals in unconstrained video settings is a valuable yetchallenging task in biometric analysis due to variations in appearances,environments, degradations, and occlusions. In this paper, we present ShARc, amultimodal approach for video-based person identification in uncontrolledenvironments that emphasizes 3-D body shape, pose, and appearance. We introducetwo encoders: a Pose and Shape Encoder (PSE) and an Aggregated AppearanceEncoder (AAE). PSE encodes the body shape via binarized silhouettes, skeletonmotions, and 3-D body shape, while AAE provides two levels of temporalappearance feature aggregation: attention-based feature aggregation andaveraging aggregation. For attention-based feature aggregation, we employspatial and temporal attention to focus on key areas for person distinction.For averaging aggregation, we introduce a novel flattening layer afteraveraging to extract more distinguishable information and reduce overfitting ofattention. We utilize centroid feature averaging for gallery registration. Wedemonstrate significant improvements over existing state-of-the-art methods onpublic datasets, including CCVID, MEVID, and BRIAR.</description><author>Haidong Zhu, Wanrong Zheng, Zhaoheng Zheng, Ram Nevatia</author><pubDate>Tue, 24 Oct 2023 16:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15946v1</guid></item><item><title>AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement</title><link>http://arxiv.org/abs/2309.08030v2</link><description>Speech enhancement systems are typically trained using pairs of clean andnoisy speech. In audio-visual speech enhancement (AVSE), there is not as muchground-truth clean data available; most audio-visual datasets are collected inreal-world environments with background noise and reverberation, hampering thedevelopment of AVSE. In this work, we introduce AV2Wav, a resynthesis-basedaudio-visual speech enhancement approach that can generate clean speech despitethe challenges of real-world training data. We obtain a subset of nearly cleanspeech from an audio-visual corpus using a neural quality estimator, and thentrain a diffusion model on this subset to generate waveforms conditioned oncontinuous speech representations from AV-HuBERT with noise-robust training. Weuse continuous rather than discrete representations to retain prosody andspeaker information. With this vocoding task alone, the model can performspeech enhancement better than a masking-based baseline. We further fine-tunethe diffusion model on clean/noisy utterance pairs to improve the performance.Our approach outperforms a masking-based baseline in terms of both automaticmetrics and a human listening test and is close in quality to the target speechin the listening test. Audio samples can be found athttps://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.</description><author>Ju-Chieh Chou, Chung-Ming Chien, Karen Livescu</author><pubDate>Tue, 24 Oct 2023 16:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08030v2</guid></item><item><title>Lifelong Robot Learning with Human Assisted Language Planners</title><link>http://arxiv.org/abs/2309.14321v2</link><description>Large Language Models (LLMs) have been shown to act like planners that candecompose high-level instructions into a sequence of executable instructions.However, current LLM-based planners are only able to operate with a fixed setof skills. We overcome this critical limitation and present a method for usingLLM-based planners to query new skills and teach robots these skills in a dataand time-efficient manner for rigid object manipulation. Our system can re-usenewly acquired skills for future tasks, demonstrating the potential of openworld and lifelong learning. We evaluate the proposed framework on multipletasks in simulation and the real world. Videos are available at:https://sites.google.com/mit.edu/halp-robot-learning.</description><author>Meenal Parakh, Alisha Fong, Anthony Simeonov, Tao Chen, Abhishek Gupta, Pulkit Agrawal</author><pubDate>Tue, 24 Oct 2023 16:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14321v2</guid></item><item><title>This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models</title><link>http://arxiv.org/abs/2310.15941v1</link><description>Although large language models (LLMs) have apparently acquired a certainlevel of grammatical knowledge and the ability to make generalizations, theyfail to interpret negation, a crucial step in Natural Language Processing. Wetry to clarify the reasons for the sub-optimal performance of LLMsunderstanding negation. We introduce a large semi-automatically generateddataset of circa 400,000 descriptive sentences about commonsense knowledge thatcan be true or false in which negation is present in about 2/3 of the corpus indifferent forms. We have used our dataset with the largest available open LLMsin a zero-shot approach to grasp their generalization and inference capabilityand we have also fine-tuned some of the models to assess whether theunderstanding of negation can be trained. Our findings show that, while LLMsare proficient at classifying affirmative sentences, they struggle withnegative sentences and lack a deep understanding of negation, often relying onsuperficial cues. Although fine-tuning the models on negative sentencesimproves their performance, the lack of generalization in handling negation ispersistent, highlighting the ongoing challenges of LLMs regarding negationunderstanding and generalization. The dataset and code are publicly available.</description><author>Iker Garc√≠a-Ferrero, Bego√±a Altuna, Javier √Ålvez, Itziar Gonzalez-Dios, German Rigau</author><pubDate>Tue, 24 Oct 2023 16:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15941v1</guid></item><item><title>Combining Behaviors with the Successor Features Keyboard</title><link>http://arxiv.org/abs/2310.15940v1</link><description>The Option Keyboard (OK) was recently proposed as a method for transferringbehavioral knowledge across tasks. OK transfers knowledge by adaptivelycombining subsets of known behaviors using Successor Features (SFs) andGeneralized Policy Improvement (GPI). However, it relies on hand-designedstate-features and task encodings which are cumbersome to design for every newenvironment. In this work, we propose the "Successor Features Keyboard" (SFK),which enables transfer with discovered state-features and task encodings. Toenable discovery, we propose the "Categorical Successor Feature Approximator"(CSFA), a novel learning algorithm for estimating SFs while jointly discoveringstate-features and task encodings. With SFK and CSFA, we achieve the firstdemonstration of transfer with SFs in a challenging 3D environment where allthe necessary representations are discovered. We first compare CSFA againstother methods for approximating SFs and show that only CSFA discoversrepresentations compatible with SF&amp;GPI at this scale. We then compare SFKagainst transfer learning baselines and show that it transfers most quickly tolong-horizon tasks.</description><author>Wilka Carvalho, Andre Saraiva, Angelos Filos, Andrew Kyle Lampinen, Loic Matthey, Richard L. Lewis, Honglak Lee, Satinder Singh, Danilo J. Rezende, Daniel Zoran</author><pubDate>Tue, 24 Oct 2023 16:35:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15940v1</guid></item><item><title>Zero-Shot Cross-Lingual Summarization via Large Language Models</title><link>http://arxiv.org/abs/2302.14229v4</link><description>Given a document in a source language, cross-lingual summarization (CLS) aimsto generate a summary in a different target language. Recently, the emergenceof Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, hasattracted wide attention from the computational linguistics community. However,it is not yet known the performance of LLMs on CLS. In this report, weempirically use various prompts to guide LLMs to perform zero-shot CLS fromdifferent paradigms (i.e., end-to-end and pipeline), and provide a preliminaryevaluation on the generated summaries. We find that ChatGPT and GPT-4originally prefer to produce lengthy summaries with detailed information. Thesetwo LLMs can further balance informativeness and conciseness with the help ofan interactive prompt, significantly improving their CLS performance.Experimental results on three widely-used CLS datasets show that GPT-4 achievesstate-of-the-art zero-shot CLS performance, and performs competitively comparedwith the fine-tuned mBART-50. Moreover, we also find some multi-lingual andbilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limitedzero-shot CLS ability. Due to the composite nature of CLS, which requiresmodels to perform summarization and translation simultaneously, accomplishingthis task in a zero-shot manner is even a challenge for LLMs. Therefore, wesincerely hope and recommend future LLM research could use CLS as a testbed.</description><author>Jiaan Wang, Yunlong Liang, Fandong Meng, Beiqi Zou, Zhixu Li, Jianfeng Qu, Jie Zhou</author><pubDate>Tue, 24 Oct 2023 16:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14229v4</guid></item><item><title>ABKD: Graph Neural Network Compression with Attention-Based Knowledge Distillation</title><link>http://arxiv.org/abs/2310.15938v1</link><description>Graph Neural Networks (GNNs) have proven to be quite versatile for a varietyof applications, including recommendation systems, fake news detection, drugdiscovery, and even computer vision. Due to the expanding size ofgraph-structured data, GNN models have also increased in complexity, leading tosubstantial latency issues. This is primarily attributed to the irregularstructure of graph data and its access pattern into memory. The naturalsolution to reduce latency is to compress large GNNs into small GNNs. One wayto do this is via knowledge distillation (KD). However, most KD approaches forGNNs only consider the outputs of the last layers and do not consider theoutputs of the intermediate layers of the GNNs; these layers may containimportant inductive biases indicated by the graph structure. To address thisshortcoming, we propose a novel KD approach to GNN compression that we callAttention-Based Knowledge Distillation (ABKD). ABKD is a KD approach that usesattention to identify important intermediate teacher-student layer pairs andfocuses on aligning their outputs. ABKD enables higher compression of GNNs witha smaller accuracy dropoff compared to existing KD approaches. On average, weachieve a 1.79% increase in accuracy with a 32.3x compression ratio onOGBN-Mag, a large graph dataset, compared to state-of-the-art approaches.</description><author>Anshul Ahluwalia, Rohit Das, Payman Behnam, Alind Khare, Pan Li, Alexey Tumanov</author><pubDate>Tue, 24 Oct 2023 16:34:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15938v1</guid></item><item><title>Improving Fairness in Deepfake Detection</title><link>http://arxiv.org/abs/2306.16635v2</link><description>Despite the development of effective deepfake detection models in recentyears, several recent studies have demonstrated that biases in the trainingdata utilized to develop deepfake detection models can lead to unfairperformance for demographic groups of different races and/or genders. Such canresult in these groups being unfairly targeted or excluded from detection,allowing misclassified deepfakes to manipulate public opinion and erode trustin the model. While these studies have focused on identifying and evaluatingthe unfairness in deepfake detection, no methods have been developed to addressthe fairness issue of deepfake detection at the algorithm level. In this work,we make the first attempt to improve deepfake detection fairness by proposingnovel loss functions to train fair deepfake detection models in ways that areagnostic or aware of demographic factors. Extensive experiments on fourdeepfake datasets and five deepfake detectors demonstrate the effectiveness andflexibility of our approach in improving the deepfake detection fairness.</description><author>Yan Ju, Shu Hu, Shan Jia, George H. Chen, Siwei Lyu</author><pubDate>Tue, 24 Oct 2023 16:33:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16635v2</guid></item><item><title>Online Robust Mean Estimation</title><link>http://arxiv.org/abs/2310.15932v1</link><description>We study the problem of high-dimensional robust mean estimation in an onlinesetting. Specifically, we consider a scenario where $n$ sensors are measuringsome common, ongoing phenomenon. At each time step $t=1,2,\ldots,T$, the$i^{th}$ sensor reports its readings $x^{(i)}_t$ for that time step. Thealgorithm must then commit to its estimate $\mu_t$ for the true mean value ofthe process at time $t$. We assume that most of the sensors observe independentsamples from some common distribution $X$, but an $\epsilon$-fraction of themmay instead behave maliciously. The algorithm wishes to compute a goodapproximation $\mu$ to the true mean $\mu^\ast := \mathbf{E}[X]$. We note thatif the algorithm is allowed to wait until time $T$ to report its estimate, thisreduces to the well-studied problem of robust mean estimation. However, therequirement that our algorithm produces partial estimates as the data is comingin substantially complicates the situation. We prove two main results about online robust mean estimation in this model.First, if the uncorrupted samples satisfy the standard condition of$(\epsilon,\delta)$-stability, we give an efficient online algorithm thatoutputs estimates $\mu_t$, $t \in [T],$ such that with high probability itholds that $\|\mu-\mu^\ast\|_2 = O(\delta \log(T))$, where $\mu = (\mu_t)_{t\in [T]}$. We note that this error bound is nearly competitive with the bestoffline algorithms, which would achieve $\ell_2$-error of $O(\delta)$. Oursecond main result shows that with additional assumptions on the input (mostnotably that $X$ is a product distribution) there are inefficient algorithmswhose error does not depend on $T$ at all.</description><author>Daniel M. Kane, Ilias Diakonikolas, Hanshen Xiao, Sihan Liu</author><pubDate>Tue, 24 Oct 2023 16:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15932v1</guid></item><item><title>Provenance for the Description Logic ELHr</title><link>http://arxiv.org/abs/2001.07541v3</link><description>We address the problem of handling provenance information in ELHr ontologies.We consider a setting recently introduced for ontology-based data access, basedon semirings and extending classical data provenance, in which ontology axiomsare annotated with provenance tokens. A consequence inherits the provenance ofthe axioms involved in deriving it, yielding a provenance polynomial as anannotation. We analyse the semantics for the ELHr case and show that thepresence of conjunctions poses various difficulties for handling provenance,some of which are mitigated by assuming multiplicative idempotency of thesemiring. Under this assumption, we study three problems: ontology completionwith provenance, computing the set of relevant axioms for a consequence, andquery answering.</description><author>Camille Bourgaux, Ana Ozaki, Rafael Pe√±aloza, Livia Predoiu</author><pubDate>Tue, 24 Oct 2023 16:27:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2001.07541v3</guid></item><item><title>E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity</title><link>http://arxiv.org/abs/2310.15929v1</link><description>Traditional pruning methods are known to be challenging to work in LargeLanguage Models (LLMs) for Generative AI because of their unaffordable trainingprocess and large computational demands. For the first time, we introduce theinformation entropy of hidden state features into a pruning metric design,namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparseemploys the information richness to leverage the channel importance, andfurther incorporates several novel techniques to put it into effect: (1) itintroduces information entropy to enhance the significance of parameter weightsand input feature norms as a novel pruning metric, and performs N:M sparsitywithout modifying the remaining weights. (2) it designs global naive shuffleand local block shuffle to quickly optimize the information distribution andadequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse isimplemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA AmpereGPUs. Extensive experiments on the LLaMA family and OPT models show thatE-Sparse can significantly speed up the model inference over the dense model(up to 1.53X) and obtain significant memory saving (up to 43.52%), withacceptable accuracy loss.</description><author>Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, Zhanhui Kang</author><pubDate>Tue, 24 Oct 2023 16:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15929v1</guid></item><item><title>Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words</title><link>http://arxiv.org/abs/2310.15921v1</link><description>The performance of sentence encoders can be significantly improved throughthe simple practice of fine-tuning using contrastive loss. A natural questionarises: what characteristics do models acquire during contrastive learning?This paper theoretically and experimentally shows that contrastive-basedsentence encoders implicitly weight words based on information-theoreticquantities; that is, more informative words receive greater weight, whileothers receive less. The theory states that, in the lower bound of the optimalvalue of the contrastive learning objective, the norm of word embeddingreflects the information gain associated with the distribution of surroundingwords. We also conduct comprehensive experiments using various models, multipledatasets, two methods to measure the implicit weighting of models (IntegratedGradients and SHAP), and two information-theoretic quantities (information gainand self-information). The results provide empirical evidence that contrastivefine-tuning emphasizes informative words.</description><author>Hiroto Kurita, Goro Kobayashi, Sho Yokoi, Kentaro Inui</author><pubDate>Tue, 24 Oct 2023 16:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15921v1</guid></item><item><title>SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents</title><link>http://arxiv.org/abs/2305.13040v4</link><description>Task-oriented dialogue (TOD) models have made significant progress in recentyears. However, previous studies primarily focus on datasets written byannotators, which has resulted in a gap between academic research andreal-world spoken conversation scenarios. While several small-scale spoken TODdatasets are proposed to address robustness issues such as ASR errors, theyignore the unique challenges in spoken conversation. To tackle the limitations,we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios fromhuman-to-human spoken conversations. SpokenWOZ further incorporates commonspoken characteristics such as word-by-word processing and reasoning in spokenlanguage. Based on these characteristics, we present cross-turn slot andreasoning slot detection as new challenges. We conduct experiments on variousbaselines, including text-modal models, newly proposed dual-modal models, andLLMs, e.g., ChatGPT. The results show that the current models still havesubstantial room for improvement in spoken conversation, where the mostadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy andthe SOTA end-to-end model only correctly completes the user request in 52.1% ofdialogues. The dataset, code, and leaderboard are available:https://spokenwoz.github.io/SpokenWOZ-github.io/.</description><author>Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li</author><pubDate>Tue, 24 Oct 2023 16:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13040v4</guid></item><item><title>TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models</title><link>http://arxiv.org/abs/2310.10180v2</link><description>Automated theorem proving (ATP) has become an appealing domain for exploringthe reasoning ability of the recent successful generative language models.However, current ATP benchmarks mainly focus on symbolic inference, but rarelyinvolve the understanding of complex number combination reasoning. In thiswork, we propose TRIGO, an ATP benchmark that not only requires a model toreduce a trigonometric expression with step-by-step proofs but also evaluates agenerative LM's reasoning ability on formulas and its capability to manipulate,group, and factor number terms. We gather trigonometric expressions and theirreduced forms from the web, annotate the simplification process manually, andtranslate it into the Lean formal language system. We then automaticallygenerate additional examples from the annotated samples to expand the dataset.Furthermore, we develop an automatic generator based on Lean-Gym to createdataset splits of varying difficulties and distributions in order to thoroughlyanalyze the model's generalization ability. Our extensive experiments show ourproposed TRIGO poses a new challenge for advanced generative LM's includingGPT-4 which is pre-trained on a considerable amount of open-source formaltheorem-proving language data, and provide a new tool to study the generativeLM's ability on both formal and mathematical reasoning.</description><author>Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, Chuanyang Zheng, Xiaodan Liang, Ming Zhang, Qun Liu</author><pubDate>Tue, 24 Oct 2023 16:17:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10180v2</guid></item><item><title>In-Context Learning Creates Task Vectors</title><link>http://arxiv.org/abs/2310.15916v1</link><description>In-context learning (ICL) in Large Language Models (LLMs) has emerged as apowerful new learning paradigm. However, its underlying mechanism is still notwell understood. In particular, it is challenging to map it to the "standard"machine learning framework, where one uses a training set $S$ to find abest-fitting function $f(x)$ in some hypothesis class. Here we make progress onthis problem by showing that the functions learned by ICL often have a verysimple structure: they correspond to the transformer LLM whose only inputs arethe query $x$ and a single "task vector" calculated from the training set.Thus, ICL can be seen as compressing $S$ into a single task vector$\boldsymbol{\theta}(S)$ and then using this task vector to modulate thetransformer to produce the output. We support the above claim via comprehensiveexperiments across a range of models and tasks.</description><author>Roee Hendel, Mor Geva, Amir Globerson</author><pubDate>Tue, 24 Oct 2023 16:17:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15916v1</guid></item><item><title>Mitigate Domain Shift by Primary-Auxiliary Objectives Association for Generalizing Person ReID</title><link>http://arxiv.org/abs/2310.15913v1</link><description>While deep learning has significantly improved ReID model accuracy under theindependent and identical distribution (IID) assumption, it has also becomeclear that such models degrade notably when applied to an unseen novel domaindue to unpredictable/unknown domain shift. Contemporary domain generalization(DG) ReID models struggle in learning domain-invariant representation solelythrough training on an instance classification objective. We consider that adeep learning model is heavily influenced and therefore biased towardsdomain-specific characteristics, e.g., background clutter, scale and viewpointvariations, limiting the generalizability of the learned model, and hypothesizethat the pedestrians are domain invariant owning they share the same structuralcharacteristics. To enable the ReID model to be less domain-specific from thesepure pedestrians, we introduce a method that guides model learning of theprimary ReID instance classification objective by a concurrent auxiliarylearning objective on weakly labeled pedestrian saliency detection. To solvethe problem of conflicting optimization criteria in the model parameter spacebetween the two learning objectives, we introduce a Primary-AuxiliaryObjectives Association (PAOA) mechanism to calibrate the loss gradients of theauxiliary task towards the primary learning task gradients. Benefiting from theharmonious multitask learning design, our model can be extended with the recenttest-time diagram to form the PAOA+, which performs on-the-fly optimizationagainst the auxiliary objective in order to maximize the model's generativecapacity in the test target domain. Experiments demonstrate the superiority ofthe proposed PAOA model.</description><author>Qilei Li, Shaogang Gong</author><pubDate>Tue, 24 Oct 2023 16:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15913v1</guid></item><item><title>Meta learning with language models: Challenges and opportunities in the classification of imbalanced text</title><link>http://arxiv.org/abs/2310.15019v2</link><description>Detecting out of policy speech (OOPS) content is important but difficult.While machine learning is a powerful tool to tackle this challenging task, itis hard to break the performance ceiling due to factors like quantity andquality limitations on training data and inconsistencies in OOPS definition anddata labeling. To realize the full potential of available limited resources, wepropose a meta learning technique (MLT) that combines individual models builtwith different text representations. We analytically show that the resultingtechnique is numerically stable and produces reasonable combining weights. Wecombine the MLT with a threshold-moving (TM) technique to further improve theperformance of the combined predictor on highly-imbalanced in-distribution andout-of-distribution datasets. We also provide computational results to show thestatistically significant advantages of the proposed MLT approach. All authors contributed equally to this work.</description><author>Apostol Vassilev, Honglan Jin, Munawar Hasan</author><pubDate>Tue, 24 Oct 2023 16:15:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15019v2</guid></item><item><title>Climate Change Impact on Agricultural Land Suitability: An Interpretable Machine Learning-Based Eurasia Case Study</title><link>http://arxiv.org/abs/2310.15912v1</link><description>The United Nations has identified improving food security and reducing hungeras essential components of its sustainable development goals. As of 2021,approximately 828 million people worldwide are experiencing hunger andmalnutrition, with numerous fatalities reported. Climate change significantlyimpacts agricultural land suitability, potentially leading to severe foodshortages and subsequent social and political conflicts. To address thispressing issue, we have developed a machine learning-based approach to predictthe risk of substantial land suitability degradation and changes in irrigationpatterns. Our study focuses on Central Eurasia, a region burdened with economicand social challenges. This study represents a pioneering effort in utilizing machine learningmethods to assess the impact of climate change on agricultural land suitabilityunder various carbon emissions scenarios. Through comprehensive featureimportance analysis, we unveil specific climate and terrain characteristicsthat exert influence on land suitability. Our approach achieves remarkableaccuracy, offering policymakers invaluable insights to facilitate informeddecisions aimed at averting a humanitarian crisis, including strategies such asthe provision of additional water and fertilizers. This research underscoresthe tremendous potential of machine learning in addressing global challenges,with a particular emphasis on mitigating hunger and malnutrition.</description><author>Valeriy Shevchenko, Daria Taniushkina, Aleksander Lukashevich, Aleksandr Bulkin, Roland Grinis, Kirill Kovalev, Veronika Narozhnaia, Nazar Sotiriadi, Alexander Krenke, Yury Maximov</author><pubDate>Tue, 24 Oct 2023 16:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15912v1</guid></item><item><title>Characterizing Mechanisms for Factual Recall in Language Models</title><link>http://arxiv.org/abs/2310.15910v1</link><description>Language Models (LMs) often must integrate facts they memorized inpretraining with new information that appears in a given context. These twosources can disagree, causing competition within the model, and it is unclearhow an LM will resolve the conflict. On a dataset that queries for knowledge ofworld capitals, we investigate both distributional and mechanistic determinantsof LM behavior in such situations. Specifically, we measure the proportion ofthe time an LM will use a counterfactual prefix (e.g., "The capital of Polandis London") to overwrite what it learned in pretraining ("Warsaw"). On Pythiaand GPT2, the training frequency of both the query country ("Poland") and thein-context city ("London") highly affect the models' likelihood of using thecounterfactual. We then use head attribution to identify individual attentionheads that either promote the memorized answer or the in-context answer in thelogits. By scaling up or down the value vector of these heads, we can controlthe likelihood of using the in-context answer on new data. This method canincrease the rate of generating the in-context answer to 88\% of the timesimply by scaling a single head at runtime. Our work contributes to a body ofevidence showing that we can often localize model behaviors to specificcomponents and provides a proof of concept for how future methods might controlmodel behavior dynamically at runtime.</description><author>Qinan Yu, Jack Merullo, Ellie Pavlick</author><pubDate>Tue, 24 Oct 2023 16:15:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15910v1</guid></item><item><title>Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding Spaces</title><link>http://arxiv.org/abs/2310.15905v1</link><description>The ability to identify and control different kinds of linguistic informationencoded in vector representations of words has many use cases, especially forexplainability and bias removal. This is usually done via a set of simpleclassification tasks, termed probes, to evaluate the information encoded in theembedding space. However, the involvement of a trainable classifier leads toentanglement between the probe's results and the classifier's nature. As aresult, contemporary works on probing include tasks that do not involvetraining of auxiliary models. In this work we introduce the term indicatortasks for non-trainable tasks which are used to query embedding spaces for theexistence of certain properties, and claim that this kind of tasks may point toa direction opposite to probes, and that this contradiction complicates thedecision on whether a property exists in an embedding space. We demonstrate ourclaims with two test cases, one dealing with gender debiasing and another withthe erasure of morphological information from embedding spaces. We show thatthe application of a suitable indicator provides a more accurate picture of theinformation captured and removed compared to probes. We thus conclude thatindicator tasks should be implemented and taken into consideration wheneliciting information from embedded representations.</description><author>Tal Levy, Omer Goldman, Reut Tsarfaty</author><pubDate>Tue, 24 Oct 2023 16:08:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15905v1</guid></item><item><title>SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in NER</title><link>http://arxiv.org/abs/2209.01646v3</link><description>Named Entity Recognition is the task to locate and classify the entities inthe text. However, Unlabeled Entity Problem in NER datasets seriously hindersthe improvement of NER performance. This paper proposes SCL-RAI to cope withthis problem. Firstly, we decrease the distance of span representations withthe same label while increasing it for different ones via span-basedcontrastive learning, which relieves the ambiguity among entities and improvesthe robustness of the model over unlabeled entities. Then we propose retrievalaugmented inference to mitigate the decision boundary shifting problem. Ourmethod significantly outperforms the previous SOTA method by 4.21% and 8.64%F1-score on two real-world datasets.</description><author>Shuzheng Si, Shuang Zeng, Jiaxing Lin, Baobao Chang</author><pubDate>Tue, 24 Oct 2023 16:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01646v3</guid></item><item><title>Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition</title><link>http://arxiv.org/abs/2310.15904v1</link><description>Recent developments in generative AI have shone a spotlight onhigh-performance synthetic text generation technologies. The now wideavailability and ease of use of such models highlights the urgent need toprovide equally powerful technologies capable of identifying synthetic text.With this in mind, we draw inspiration from psychological studies which suggestthat people can be driven by emotion and encode emotion in the text theycompose. We hypothesize that pretrained language models (PLMs) have anaffective deficit because they lack such an emotional driver when generatingtext and consequently may generate synthetic text which has affectiveincoherence i.e. lacking the kind of emotional coherence present inhuman-authored text. We subsequently develop an emotionally aware detector byfine-tuning a PLM on emotion. Experiment results indicate that ouremotionally-aware detector achieves improvements across a range of synthetictext generators, various sized models, datasets, and domains. Finally, wecompare our emotionally-aware synthetic text detector to ChatGPT in the task ofidentification of its own output and show substantial gains, reinforcing thepotential of emotion as a signal to identify synthetic text. Code, models, anddatasets are available at https: //github.com/alanagiasi/emoPLMsynth</description><author>Alan Cowap, Yvette Graham, Jennifer Foster</author><pubDate>Tue, 24 Oct 2023 16:07:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15904v1</guid></item><item><title>Neural Collapse in Multi-label Learning with Pick-all-label Loss</title><link>http://arxiv.org/abs/2310.15903v1</link><description>We study deep neural networks for the multi-label classification (MLab) taskthrough the lens of neural collapse (NC). Previous works have been restrictedto the multi-class classification setting and discovered a prevalent NCphenomenon comprising of the following properties for the last-layer features:(i) the variability of features within every class collapses to zero, (ii) theset of feature means form an equi-angular tight frame (ETF), and (iii) the lastlayer classifiers collapse to the feature mean upon some scaling. We generalizethe study to multi-label learning, and prove for the first time that ageneralized NC phenomenon holds with the "pick-all-label'' formulation. Underthe natural analog of the unconstrained feature model (UFM), we establish thatthe only global classifier of the pick-all-label cross entropy loss display thesame ETF geometry which further collapse to multiplicity-1 feature class means.Besides, we discover a combinatorial property in generalized NC which is uniquefor multi-label learning that we call ``tag-wise average'' property, where thefeature class-means of samples with multiple labels are scaled average of thefeature class-means of single label tags. Theoretically, we establish globaloptimality result for the pick-all-label cross-entropy risk for the UFM.Additionally, We also provide empirical evidence to support our investigationinto training deep neural networks on multi-label datasets, resulting inimproved training efficiency.</description><author>Pengyu Li, Yutong Wang, Xiao Li, Qing Qu</author><pubDate>Tue, 24 Oct 2023 16:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15903v1</guid></item><item><title>YOLO-Angio: An Algorithm for Coronary Anatomy Segmentation</title><link>http://arxiv.org/abs/2310.15898v1</link><description>Coronary angiography remains the gold standard for diagnosis of coronaryartery disease, the most common cause of death worldwide. While this procedureis performed more than 2 million times annually, there remain few methods forfast and accurate automated measurement of disease and localization of coronaryanatomy. Here, we present our solution to the Automatic Region-based CoronaryArtery Disease diagnostics using X-ray angiography images (ARCADE) challengeheld at MICCAI 2023. For the artery segmentation task, our three-stage approachcombines preprocessing and feature selection by classical computer vision toenhance vessel contrast, followed by an ensemble model based on YOLOv8 topropose possible vessel candidates by generating a vessel map. A finalsegmentation is based on a logic-based approach to reconstruct the coronarytree in a graph-based sorting method. Our entry to the ARCADE challenge placed3rd overall. Using the official metric for evaluation, we achieved an F1 scoreof 0.422 and 0.4289 on the validation and hold-out sets respectively.</description><author>Tom Liu, Hui Lin, Aggelos K. Katsaggelos, Adrienne Kline</author><pubDate>Tue, 24 Oct 2023 16:02:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15898v1</guid></item><item><title>Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions</title><link>http://arxiv.org/abs/2303.12484v2</link><description>Deep learning has seen rapid growth in recent years and achievedstate-of-the-art performance in a wide range of applications. However, trainingmodels typically requires expensive and time-consuming collection of largequantities of labeled data. This is particularly true within the scope ofmedical imaging analysis (MIA), where data are limited and labels are expensiveto be acquired. Thus, label-efficient deep learning methods are developed tomake comprehensive use of the labeled data as well as the abundance ofunlabeled and weak-labeled data. In this survey, we extensively investigatedover 300 recent papers to provide a comprehensive overview of recent progresson label-efficient learning strategies in MIA. We first present the backgroundof label-efficient learning and categorize the approaches into differentschemes. Next, we examine the current state-of-the-art methods in detailthrough each scheme. Specifically, we provide an in-depth investigation,covering not only canonical semi-supervised, self-supervised, andmulti-instance learning schemes, but also recently emerged active andannotation-efficient learning strategies. Moreover, as a comprehensivecontribution to the field, this survey not only elucidates the commonalitiesand unique features of the surveyed methods but also presents a detailedanalysis of the current challenges in the field and suggests potential avenuesfor future research.</description><author>Cheng Jin, Zhengrui Guo, Yi Lin, Luyang Luo, Hao Chen</author><pubDate>Tue, 24 Oct 2023 16:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12484v2</guid></item><item><title>AnglE-optimized Text Embeddings</title><link>http://arxiv.org/abs/2309.12871v5</link><description>High-quality text embedding is pivotal in improving semantic textualsimilarity (STS) tasks, which are crucial components in Large Language Model(LLM) applications. However, a common challenge existing text embedding modelsface is the problem of vanishing gradients, primarily due to their reliance onthe cosine function in the optimization objective, which has saturation zones.To address this issue, this paper proposes a novel angle-optimized textembedding model called AnglE. The core idea of AnglE is to introduce angleoptimization in a complex space. This novel approach effectively mitigates theadverse effects of the saturation zone in the cosine function, which can impedegradient and hinder optimization processes. To set up a comprehensive STSevaluation, we experimented on existing short-text STS datasets and a newlycollected long-text STS dataset from GitHub Issues. Furthermore, we examinedomain-specific STS scenarios with limited labeled data and explore how AnglEworks with LLM-annotated data. Extensive experiments were conducted on varioustasks including short-text STS, long-text STS, and domain-specific STS tasks.The results show that AnglE outperforms the state-of-the-art (SOTA) STS modelsthat ignore the cosine saturation zone. These findings demonstrate the abilityof AnglE to generate high-quality text embeddings and the usefulness of angleoptimization in STS.</description><author>Xianming Li, Jing Li</author><pubDate>Tue, 24 Oct 2023 15:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12871v5</guid></item><item><title>Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining</title><link>http://arxiv.org/abs/2310.12172v2</link><description>This paper presents an overview of the ImageArg shared task, the firstmultimodal Argument Mining shared task co-located with the 10th Workshop onArgument Mining at EMNLP 2023. The shared task comprises two classificationsubtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: ImagePersuasiveness Classification. The former determines the stance of a tweetcontaining an image and a piece of text toward a controversial topic (e.g., guncontrol and abortion). The latter determines whether the image makes the tweettext more persuasive. The shared task received 31 submissions for Subtask-A and21 submissions for Subtask-B from 9 different teams across 6 countries. The topsubmission in Subtask-A achieved an F1-score of 0.8647 while the bestsubmission in Subtask-B achieved an F1-score of 0.5561.</description><author>Zhexiong Liu, Mohamed Elaraby, Yang Zhong, Diane Litman</author><pubDate>Tue, 24 Oct 2023 15:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12172v2</guid></item><item><title>BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT</title><link>http://arxiv.org/abs/2310.15896v1</link><description>Large language models (LLMs) have performed well in providing general andextensive health suggestions in single-turn conversations, exemplified bysystems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, thelimited information provided by users during single turn results in inadequatepersonalization and targeting of the generated suggestions, which requiresusers to independently select the useful part. It is mainly caused by themissing ability to engage in multi-turn questioning. In real-world medicalconsultations, doctors usually employ a series of iterative inquiries tocomprehend the patient's condition thoroughly, enabling them to provideeffective and personalized suggestions subsequently, which can be defined aschain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we proposeBianQue, a ChatGLM-based LLM finetuned with the self-constructed healthconversation dataset BianQueCorpus that is consist of multiple turns ofquestioning and health suggestions polished by ChatGPT. Experimental resultsdemonstrate that the proposed BianQue can simultaneously balance thecapabilities of both questioning and health suggestions, which will helppromote the research and application of LLMs in the field of proactive health.</description><author>Yirong Chen, Zhenyu Wang, Xiaofen Xing, huimin zheng, Zhipei Xu, Kai Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, Xiangmin Xu</author><pubDate>Tue, 24 Oct 2023 15:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15896v1</guid></item><item><title>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</title><link>http://arxiv.org/abs/2303.04048v3</link><description>Recently, the emergence of ChatGPT has attracted wide attention from thecomputational linguistics community. Many prior studies have shown that ChatGPTachieves remarkable performance on various NLP tasks in terms of automaticevaluation metrics. However, the ability of ChatGPT to serve as an evaluationmetric is still underexplored. Considering assessing the quality of naturallanguage generation (NLG) models is an arduous task and NLG metrics notoriouslyshow their poor correlation with human judgments, we wonder whether ChatGPT isa good NLG evaluation metric. In this report, we provide a preliminarymeta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail,we regard ChatGPT as a human evaluator and give task-specific (e.g.,summarization) and aspect-specific (e.g., relevance) instruction to promptChatGPT to evaluate the generated results of NLG models. We conduct experimentson five NLG meta-evaluation datasets (including summarization, story generationand data-to-text tasks). Experimental results show that compared with previousautomatic metrics, ChatGPT achieves state-of-the-art or competitive correlationwith human judgments in most cases. In addition, we find that the effectivenessof the ChatGPT evaluator might be influenced by the creation method of themeta-evaluation datasets. For the meta-evaluation datasets which are createdgreatly depending on the reference and thus are biased, the ChatGPT evaluatormight lose its effectiveness. We hope our preliminary study could prompt theemergence of a general-purposed reliable NLG metric.</description><author>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou</author><pubDate>Tue, 24 Oct 2023 15:56:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04048v3</guid></item><item><title>nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources</title><link>http://arxiv.org/abs/2309.02373v2</link><description>State-of-the-art language models like T5 have revolutionized the NLPlandscape, but their computational demands hinder a large portion of theresearch community. To address this challenge, we present nanoT5, aspecially-optimized PyTorch framework for efficient pre-training andfine-tuning of T5 models. Drawing on insights from optimizer differences andprioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on asingle GPU in just 16 hours, without any loss in performance. With theintroduction of this open-source framework, we hope to widen the accessibilityto language modelling research and cater to the community's demand for moreuser-friendly T5 (Encoder-Decoder) implementations. We make our contributions,including configurations, codebase, pre-training insights, and pre-trainedmodels, available to the public.</description><author>Piotr Nawrot</author><pubDate>Tue, 24 Oct 2023 15:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02373v2</guid></item><item><title>Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data</title><link>http://arxiv.org/abs/2310.15890v1</link><description>The current state-of-the-art decentralized learning algorithms mostly assumethe data distribution to be Independent and Identically Distributed (IID).However, in practical scenarios, the distributed datasets can havesignificantly heterogeneous data distributions across the agents. In this work,we present a novel approach for decentralized learning on heterogeneous data,where data-free knowledge distillation through contrastive loss oncross-features is utilized to improve performance. Cross-features for a pair ofneighboring agents are the features (i.e., last hidden layer activations)obtained from the data of an agent with respect to the model parameters of theother agent. We demonstrate the effectiveness of the proposed technique throughan exhaustive set of experiments on various Computer Vision datasets (CIFAR-10,CIFAR-100, Fashion MNIST, and ImageNet), model architectures, and networktopologies. Our experiments show that the proposed method achieves superiorperformance (0.2-4% improvement in test accuracy) compared to other existingtechniques for decentralized learning on heterogeneous data.</description><author>Sai Aparna Aketi, Kaushik Roy</author><pubDate>Tue, 24 Oct 2023 15:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15890v1</guid></item><item><title>Explainable machine learning-based prediction model for diabetic nephropathy</title><link>http://arxiv.org/abs/2309.16730v2</link><description>The aim of this study is to analyze the effect of serum metabolites ondiabetic nephropathy (DN) and predict the prevalence of DN through a machinelearning approach. The dataset consists of 548 patients from April 2018 toApril 2019 in Second Affiliated Hospital of Dalian Medical University (SAHDMU).We select the optimal 38 features through a Least absolute shrinkage andselection operator (LASSO) regression model and a 10-fold cross-validation. Wecompare four machine learning algorithms, including eXtreme Gradient Boosting(XGB), random forest, decision tree and logistic regression, by AUC-ROC curves,decision curves, calibration curves. We quantify feature importance andinteraction effects in the optimal predictive model by Shapley AdditiveexPlanations (SHAP) method. The XGB model has the best performance to screenfor DN with the highest AUC value of 0.966. The XGB model also gains moreclinical net benefits than others and the fitting degree is better. Inaddition, there are significant interactions between serum metabolites andduration of diabetes. We develop a predictive model by XGB algorithm to screenfor DN. C2, C5DC, Tyr, Ser, Met, C24, C4DC, and Cys have great contribution inthe model, and can possibly be biomarkers for DN.</description><author>Jing-Mei Yin, Yang Li, Jun-Tang Xue, Guo-Wei Zong, Zhong-Ze Fang, Lang Zou</author><pubDate>Tue, 24 Oct 2023 15:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16730v2</guid></item><item><title>State Sequences Prediction via Fourier Transform for Representation Learning</title><link>http://arxiv.org/abs/2310.15888v1</link><description>While deep reinforcement learning (RL) has been demonstrated effective insolving complex control tasks, sample efficiency remains a key challenge due tothe large amounts of data required for remarkable performance. Existingresearch explores the application of representation learning for data-efficientRL, e.g., learning predictive representations by predicting long-term futurestates. However, many existing methods do not fully exploit the structuralinformation inherent in sequential state signals, which can potentially improvethe quality of long-term decision-making but is difficult to discern in thetime domain. To tackle this problem, we propose State Sequences Prediction viaFourier Transform (SPF), a novel method that exploits the frequency domain ofstate sequences to extract the underlying patterns in time series data forlearning expressive representations efficiently. Specifically, we theoreticallyanalyze the existence of structural information in state sequences, which isclosely related to policy performance and signal regularity, and then proposeto predict the Fourier transform of infinite-step future state sequences toextract such information. One of the appealing features of SPF is that it issimple to implement while not requiring storage of infinite-step future statesas prediction targets. Experiments demonstrate that the proposed methodoutperforms several state-of-the-art algorithms in terms of both sampleefficiency and performance.</description><author>Mingxuan Ye, Yufei Kuang, Jie Wang, Rui Yang, Wengang Zhou, Houqiang Li, Feng Wu</author><pubDate>Tue, 24 Oct 2023 15:47:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15888v1</guid></item><item><title>AdaptiX -- A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics</title><link>http://arxiv.org/abs/2310.15887v1</link><description>With the ongoing efforts to empower people with mobility impairments and theincrease in technological acceptance by the general public, assistivetechnologies, such as collaborative robotic arms, are gaining popularity. Yet,their widespread success is limited by usability issues, specifically thedisparity between user input and software control along the autonomy continuum.To address this, shared control concepts provide opportunities to combine thetargeted increase of user autonomy with a certain level of computer assistance.This paper presents the free and open-source AdaptiX XR framework fordeveloping and evaluating shared control applications in a high-resolutionsimulation environment. The initial framework consists of a simulated roboticarm with an example scenario in Virtual Reality (VR), multiple standard controlinterfaces, and a specialized recording/replay system. AdaptiX can easily beextended for specific research needs, allowing Human-Robot Interaction (HRI)researchers to rapidly design and test novel interaction methods, interventionstrategies, and multi-modal feedback techniques, without requiring an actualphysical robotic arm during the early phases of ideation, prototyping, andevaluation. Also, a Robot Operating System (ROS) integration enables thecontrolling of a real robotic arm in a PhysicalTwin approach without anysimulation-reality gap. Here, we review the capabilities and limitations ofAdaptiX in detail and present three bodies of research based on the framework.AdaptiX can be accessed at https://adaptix.robot-research.de.</description><author>Max Pascher, Felix Ferdinand Goldau, Kirill Kronhardt, Udo Frese, Jens Gerken</author><pubDate>Tue, 24 Oct 2023 15:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15887v1</guid></item><item><title>Fully Adaptive Composition in Differential Privacy</title><link>http://arxiv.org/abs/2203.05481v3</link><description>Composition is a key feature of differential privacy. Well-known advancedcomposition theorems allow one to query a private database quadratically moretimes than basic privacy composition would permit. However, these resultsrequire that the privacy parameters of all algorithms be fixed beforeinteracting with the data. To address this, Rogers et al. introduced fullyadaptive composition, wherein both algorithms and their privacy parameters canbe selected adaptively. They defined two probabilistic objects to measureprivacy in adaptive composition: privacy filters, which provide differentialprivacy guarantees for composed interactions, and privacy odometers,time-uniform bounds on privacy loss. There are substantial gaps betweenadvanced composition and existing filters and odometers. First, existingfilters place stronger assumptions on the algorithms being composed. Second,these odometers and filters suffer from large constants, making themimpractical. We construct filters that match the rates of advanced composition,including constants, despite allowing for adaptively chosen privacy parameters.En route we also derive a privacy filter for approximate zCDP. We alsoconstruct several general families of odometers. These odometers match thetightness of advanced composition at an arbitrary, preselected point in time,or at all points in time simultaneously, up to a doubly-logarithmic factor. Weobtain our results by leveraging advances in martingale concentration. In sum,we show that fully adaptive privacy is obtainable at almost no loss.</description><author>Justin Whitehouse, Aaditya Ramdas, Ryan Rogers, Zhiwei Steven Wu</author><pubDate>Tue, 24 Oct 2023 15:37:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.05481v3</guid></item><item><title>Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views</title><link>http://arxiv.org/abs/2303.15206v3</link><description>Neural view synthesis (NVS) is one of the most successful techniques forsynthesizing free viewpoint videos, capable of achieving high fidelity fromonly a sparse set of captured images. This success has led to many variants ofthe techniques, each evaluated on a set of test views typically using imagequality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of researchon how NVS methods perform with respect to perceived video quality. We presentthe first study on perceptual evaluation of NVS and NeRF variants. For thisstudy, we collected two datasets of scenes captured in a controlled labenvironment as well as in-the-wild. In contrast to existing datasets, thesescenes come with reference video sequences, allowing us to test for temporalartifacts and subtle distortions that are easily overlooked when viewing onlystatic images. We measured the quality of videos synthesized by several NVSmethods in a well-controlled perceptual quality assessment experiment as wellas with many existing state-of-the-art image/video quality metrics. We presenta detailed analysis of the results and recommendations for dataset and metricselection for NVS evaluation.</description><author>Hanxue Liang, Tianhao Wu, Param Hanji, Francesco Banterle, Hongyun Gao, Rafal Mantiuk, Cengiz Oztireli</author><pubDate>Tue, 24 Oct 2023 15:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15206v3</guid></item></channel></rss>