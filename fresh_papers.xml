<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 01 Sep 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge</title><link>http://arxiv.org/abs/2508.21072v1</link><description>Content watermarking is an important tool for the authentication andcopyright protection of digital media. However, it is unclear whether existingwatermarks are robust against adversarial attacks. We present the winningsolution to the NeurIPS 2024 Erasing the Invisible challenge, whichstress-tests watermark robustness under varying degrees of adversary knowledge.The challenge consisted of two tracks: a black-box and beige-box track,depending on whether the adversary knows which watermarking method was used bythe provider. For the beige-box track, we leverage an adaptive VAE-basedevasion attack, with a test-time optimization and color-contrast restoration inCIELAB space to preserve the image's quality. For the black-box track, we firstcluster images based on their artifacts in the spatial or frequency-domain.Then, we apply image-to-image diffusion models with controlled noise injectionand semantic priors from ChatGPT-generated captions to each cluster withoptimized parameter settings. Empirical evaluations demonstrate that our methodsuccessfully achieves near-perfect watermark removal (95.7%) with negligibleimpact on the residual image's quality. We hope that our attacks inspire thedevelopment of more robust image watermarking methods.</description><author>Fahad Shamshad, Tameem Bakr, Yahia Shaaban, Noor Hussein, Karthik Nandakumar, Nils Lukas</author><pubDate>Thu, 28 Aug 2025 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21072v1</guid></item><item><title>Dress&amp;Dance: Dress up and Dance as You Like It - Technical Preview</title><link>http://arxiv.org/abs/2508.21070v1</link><description>We present Dress&amp;Dance, a video diffusion framework that generates highquality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of auser wearing desired garments while moving in accordance with a given referencevideo. Our approach requires a single user image and supports a range of tops,bottoms, and one-piece garments, as well as simultaneous tops and bottomstry-on in a single pass. Key to our framework is CondNet, a novel conditioningnetwork that leverages attention to unify multi-modal inputs (text, images, andvideos), thereby enhancing garment registration and motion fidelity. CondNet istrained on heterogeneous training data, combining limited video data and alarger, more readily available image dataset, in a multistage progressivemanner. Dress&amp;Dance outperforms existing open source and commercial solutionsand enables a high quality and flexible try-on experience.</description><author>Jun-Kun Chen, Aayush Bansal, Minh Phuoc Vo, Yu-Xiong Wang</author><pubDate>Thu, 28 Aug 2025 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21070v1</guid></item><item><title>OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning</title><link>http://arxiv.org/abs/2508.21066v1</link><description>In this paper, we introduce OneReward, a unified reinforcement learningframework that enhances the model's generative capabilities across multipletasks under different evaluation criteria using only \textit{One Reward} model.By employing a single vision-language model (VLM) as the generative rewardmodel, which can distinguish the winner and loser for a given task and a givenevaluation criterion, it can be effectively applied to multi-task generationmodels, particularly in contexts with varied data and diverse task objectives.We utilize OneReward for mask-guided image generation, which can be furtherdivided into several sub-tasks such as image fill, image extend, objectremoval, and text rendering, involving a binary mask as the edit area. Althoughthese domain-specific tasks share same conditioning paradigm, they differsignificantly in underlying data distributions and evaluation metrics. Existingmethods often rely on task-specific supervised fine-tuning (SFT), which limitsgeneralization and training efficiency. Building on OneReward, we developSeedream 3.0 Fill, a mask-guided generation model trained via multi-taskreinforcement learning directly on a pre-trained base model, eliminating theneed for task-specific SFT. Experimental results demonstrate that our unifiededit model consistently outperforms both commercial and open-sourcecompetitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], acrossmultiple evaluation dimensions. Code and model are available at:https://one-reward.github.io</description><author>Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu</author><pubDate>Thu, 28 Aug 2025 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21066v1</guid></item><item><title>Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs</title><link>http://arxiv.org/abs/2405.14862v2</link><description>Decoder-only large language models typically rely solely on masked causalattention, which limits their expressiveness by restricting information flow toone direction. We propose Bitune, a method that enhances pretraineddecoder-only LLMs by incorporating bidirectional attention into promptprocessing. We evaluate Bitune in instruction-tuning and question-answeringsettings, showing significant improvements in performance on commonsensereasoning, arithmetic, and language understanding tasks. Furthermore, extensiveablation studies validate the role of each component of the method, anddemonstrate that Bitune is compatible with various parameter-efficientfinetuning techniques and full model finetuning.</description><author>Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano</author><pubDate>Thu, 28 Aug 2025 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14862v2</guid></item><item><title>Prompt-to-Product: Generative Assembly via Bimanual Manipulation</title><link>http://arxiv.org/abs/2508.21063v1</link><description>Creating assembly products demands significant manual effort and expertknowledge in 1) designing the assembly and 2) constructing the product. Thispaper introduces Prompt-to-Product, an automated pipeline that generatesreal-world assembly products from natural language prompts. Specifically, weleverage LEGO bricks as the assembly platform and automate the process ofcreating brick assembly structures. Given the user design requirements,Prompt-to-Product generates physically buildable brick designs, and thenleverages a bimanual robotic system to construct the real assembly products,bringing user imaginations into the real world. We conduct a comprehensive userstudy, and the results demonstrate that Prompt-to-Product significantly lowersthe barrier and reduces manual effort in creating assembly products fromimaginative ideas.</description><author>Ruixuan Liu, Philip Huang, Ava Pun, Kangle Deng, Shobhit Aggarwal, Kevin Tang, Michelle Liu, Deva Ramanan, Jun-Yan Zhu, Jiaoyang Li, Changliu Liu</author><pubDate>Thu, 28 Aug 2025 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21063v1</guid></item><item><title>OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models</title><link>http://arxiv.org/abs/2508.21061v1</link><description>As multi-turn dialogues with large language models (LLMs) grow longer andmore complex, how can users better evaluate and review progress on theirconversational goals? We present OnGoal, an LLM chat interface that helps usersbetter manage goal progress. OnGoal provides real-time feedback on goalalignment through LLM-assisted evaluation, explanations for evaluation resultswith examples, and overviews of goal progression over time, enabling users tonavigate complex dialogues more effectively. Through a study with 20participants on a writing task, we evaluate OnGoal against a baseline chatinterface without goal tracking. Using OnGoal, participants spent less time andeffort to achieve their goals while exploring new prompting strategies toovercome miscommunication, suggesting tracking and visualizing goals canenhance engagement and resilience in LLM dialogues. Our findings inspireddesign implications for future LLM chat interfaces that improve goalcommunication, reduce cognitive load, enhance interactivity, and enablefeedback to improve LLM performance.</description><author>Adam Coscia, Shunan Guo, Eunyee Koh, Alex Endert</author><pubDate>Thu, 28 Aug 2025 17:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21061v1</guid></item><item><title>Multi-View 3D Point Tracking</title><link>http://arxiv.org/abs/2508.21060v1</link><description>We introduce the first data-driven multi-view 3D point tracker, designed totrack arbitrary points in dynamic scenes using multiple camera views. Unlikeexisting monocular trackers, which struggle with depth ambiguities andocclusion, or prior multi-camera methods that require over 20 cameras andtedious per-sequence optimization, our feed-forward model directly predicts 3Dcorrespondences using a practical number of cameras (e.g., four), enablingrobust and accurate online tracking. Given known camera poses and eithersensor-based or estimated multi-view depth, our tracker fuses multi-viewfeatures into a unified point cloud and applies k-nearest-neighbors correlationalongside a transformer-based update to reliably estimate long-range 3Dcorrespondences, even under occlusion. We train on 5K synthetic multi-viewKubric sequences and evaluate on two real-world benchmarks: Panoptic Studio andDexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.Our method generalizes well to diverse camera setups of 1-8 views with varyingvantage points and video lengths of 24-150 frames. By releasing our trackeralongside training and evaluation datasets, we aim to set a new standard formulti-view 3D tracking research and provide a practical tool for real-worldapplications. Project page available at https://ethz-vlg.github.io/mvtracker.</description><author>Frano Rajič, Haofei Xu, Marko Mihajlovic, Siyuan Li, Irem Demir, Emircan Gündoğdu, Lei Ke, Sergey Prokudin, Marc Pollefeys, Siyu Tang</author><pubDate>Thu, 28 Aug 2025 17:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21060v1</guid></item><item><title>Mixture of Contexts for Long Video Generation</title><link>http://arxiv.org/abs/2508.21058v1</link><description>Long video generation is fundamentally a long context memory problem: modelsmust retain and retrieve salient events across a long range without collapsingor drifting. However, scaling diffusion transformers to generate long-contextvideos is fundamentally limited by the quadratic cost of self-attention, whichmakes memory and computation intractable and difficult to optimize for longsequences. We recast long-context video generation as an internal informationretrieval task and propose a simple, learnable sparse attention routing module,Mixture of Contexts (MoC), as an effective long-term memory retrieval engine.In MoC, each query dynamically selects a few informative chunks plus mandatoryanchors (caption, local windows) to attend to, with causal routing thatprevents loop closures. As we scale the data and gradually sparsify therouting, the model allocates compute to salient history, preserving identities,actions, and scenes over minutes of content. Efficiency follows as a byproductof retrieval (near-linear scaling), which enables practical training andsynthesis, and the emergence of memory and consistency at the scale of minutes.</description><author>Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, Maneesh Agrawala, Lu Jiang, Gordon Wetzstein</author><pubDate>Thu, 28 Aug 2025 17:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21058v1</guid></item><item><title>FakeParts: a New Family of AI-Generated DeepFakes</title><link>http://arxiv.org/abs/2508.21052v1</link><description>We introduce FakeParts, a new class of deepfakes characterized by subtle,localized manipulations to specific spatial regions or temporal segments ofotherwise authentic videos. Unlike fully synthetic content, these partialmanipulations, ranging from altered facial expressions to object substitutionsand background modifications, blend seamlessly with real elements, making themparticularly deceptive and difficult to detect. To address the critical gap indetection capabilities, we present FakePartsBench, the first large-scalebenchmark dataset specifically designed to capture the full spectrum of partialdeepfakes. Comprising over 25K videos with pixel-level and frame-levelmanipulation annotations, our dataset enables comprehensive evaluation ofdetection methods. Our user studies demonstrate that FakeParts reduces humandetection accuracy by over 30% compared to traditional deepfakes, with similarperformance degradation observed in state-of-the-art detection models. Thiswork identifies an urgent vulnerability in current deepfake detectionapproaches and provides the necessary resources to develop more robust methodsfor partial video manipulations.</description><author>Gaetan Brison, Soobash Daiboo, Samy Aimeur, Awais Hussain Sani, Xi Wang, Gianni Franchi, Vicky Kalogeiton</author><pubDate>Thu, 28 Aug 2025 17:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21052v1</guid></item><item><title>Enabling Equitable Access to Trustworthy Financial Reasoning</title><link>http://arxiv.org/abs/2508.21051v1</link><description>According to the United States Internal Revenue Service, ''the averageAmerican spends $\$270$ and 13 hours filing their taxes''. Even beyond theU.S., tax filing requires complex reasoning, combining application ofoverlapping rules with numerical calculations. Because errors can incur costlypenalties, any automated system must deliver high accuracy and auditability,making modern large language models (LLMs) poorly suited for this task. Wepropose an approach that integrates LLMs with a symbolic solver to calculatetax obligations. We evaluate variants of this system on the challengingStAtutory Reasoning Assessment (SARA) dataset, and include a novel method forestimating the cost of deploying such a system based on real-world penaltiesfor tax errors. We further show how combining up-front translation ofplain-text rules into formal logic programs, combined with intelligentlyretrieved exemplars for formal case representations, can dramatically improveperformance on this task and reduce costs to well below real-world averages.Our results demonstrate the promise and economic feasibility of neuro-symbolicarchitectures for increasing equitable access to reliable tax assistance.</description><author>William Jurayj, Nils Holzenberger, Benjamin Van Durme</author><pubDate>Thu, 28 Aug 2025 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21051v1</guid></item><item><title>Estimating Machine Translation Difficulty</title><link>http://arxiv.org/abs/2508.10175v2</link><description>Machine translation quality has steadily improved over the years, achievingnear-perfect translations in recent benchmarks. These high-quality outputs makeit difficult to distinguish between state-of-the-art models and to identifyareas for future improvement. In this context, automatically identifying textswhere machine translation systems struggle holds promise for developing morediscriminative evaluations and guiding future research. In this work, we address this gap by formalizing the task of translationdifficulty estimation, defining a text's difficulty based on the expectedquality of its translations. We introduce a new metric to evaluate difficultyestimators and use it to assess both baselines and novel approaches. Finally,we demonstrate the practical utility of difficulty estimators by using them toconstruct more challenging benchmarks for machine translation. Our results showthat dedicated models outperform both heuristic-based methods andLLM-as-a-judge approaches, with Sentinel-src achieving the best performance.Thus, we release two improved models for difficulty estimation, Sentinel-src-24and Sentinel-src-25, which can be used to scan large collections of texts andselect those most likely to challenge contemporary machine translation systems.</description><author>Lorenzo Proietti, Stefano Perrella, Vilém Zouhar, Roberto Navigli, Tom Kocmi</author><pubDate>Thu, 28 Aug 2025 17:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10175v2</guid></item><item><title>Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm</title><link>http://arxiv.org/abs/2508.21049v1</link><description>Sentential relation extraction (RE) is an important task in natural languageprocessing (NLP). In this paper we propose to do sentential RE with dynamicrouting in capsules. We first show that the proposed approach outperform stateof the art on common sentential relation extraction datasets Tacred, Tacredrev,Retacred, and Conll04. We then investigate potential reasons for its goodperformance on the mentioned datasets, and yet low performance on anothersimilar, yet larger sentential RE dataset, Wikidata. As such, we identify noisein Wikidata labels as one of the reasons that can hinder performance.Additionally, we show associativity of better performance with betterre-representation, a term from neuroscience referred to change ofrepresentation in human brain to improve the match at comparison time. Asexample, in the given analogous terms King:Queen::Man:Woman, at comparisontime, and as a result of re-representation, the similarity between related headterms (King,Man), and tail terms (Queen,Woman) increases. As such, ourobservation show that our proposed model can do re-representation better thanthe vanilla model compared with. To that end, beside noise in the labels of thedistantly supervised RE datasets, we propose re-representation as a challengein sentential RE.</description><author>Ramazan Ali Bahrami, Ramin Yahyapour</author><pubDate>Thu, 28 Aug 2025 17:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21049v1</guid></item><item><title>Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning</title><link>http://arxiv.org/abs/2508.21048v1</link><description>Deepfake detection remains a formidable challenge due to the complex andevolving nature of fake content in real-world scenarios. However, existingacademic benchmarks suffer from severe discrepancies from industrial practice,typically featuring homogeneous training sources and low-quality testingimages, which hinder the practical deployments of current detectors. Tomitigate this gap, we introduce HydraFake, a dataset that simulates real-worldchallenges with hierarchical generalization testing. Specifically, HydraFakeinvolves diversified deepfake techniques and in-the-wild forgeries, along withrigorous training and evaluation protocol, covering unseen model architectures,emerging forgery techniques and novel data domains. Building on this resource,we propose Veritas, a multi-modal large language model (MLLM) based deepfakedetector. Different from vanilla chain-of-thought (CoT), we introducepattern-aware reasoning that involves critical reasoning patterns such as"planning" and "self-reflection" to emulate human forensic process. We furtherpropose a two-stage training pipeline to seamlessly internalize such deepfakereasoning capacities into current MLLMs. Experiments on HydraFake datasetreveal that although previous detectors show great generalization oncross-model scenarios, they fall short on unseen forgeries and data domains.Our Veritas achieves significant gains across different OOD scenarios, and iscapable of delivering transparent and faithful detection outputs.</description><author>Hao Tan, Jun Lan, Zichang Tan, Ajian Liu, Chuanbiao Song, Senyuan Shi, Huijia Zhu, Weiqiang Wang, Jun Wan, Zhen Lei</author><pubDate>Thu, 28 Aug 2025 17:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21048v1</guid></item><item><title>CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing &amp; Sparsification</title><link>http://arxiv.org/abs/2508.21046v1</link><description>Recent Vision-Language-Action (VLA) models built on pre-trainedVision-Language Models (VLMs) require extensive post-training, resulting inhigh computational overhead that limits scalability and deployment.We proposeCogVLA, a Cognition-Aligned Vision-Language-Action framework that leveragesinstruction-driven routing and sparsification to improve both efficiency andperformance. CogVLA draws inspiration from human multimodal coordination andintroduces a 3-stage progressive architecture. 1) Encoder-FiLM basedAggregation Routing (EFA-Routing) injects instruction information into thevision encoder to selectively aggregate and compress dual-stream visual tokens,forming a instruction-aware latent representation. 2) Building upon thiscompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)introduces action intent into the language model by pruninginstruction-irrelevant visually grounded tokens, thereby achieving token-levelsparsity. 3) To ensure that compressed perception inputs can still supportaccurate and coherent action generation, we introduce V-L-A Coupled Attention(CAtten), which combines causal vision-language attention with bidirectionalaction parallel decoding. Extensive experiments on the LIBERO benchmark andreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-artperformance with success rates of 97.4% and 70.0%, respectively, while reducingtraining costs by 2.5-fold and decreasing inference latency by 2.8-foldcompared to OpenVLA. CogVLA is open-sourced and publicly available athttps://github.com/JiuTian-VL/CogVLA.</description><author>Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie</author><pubDate>Thu, 28 Aug 2025 17:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21046v1</guid></item><item><title>MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs</title><link>http://arxiv.org/abs/2508.21044v1</link><description>Video Large Language Models (VLLMs) excel in video understanding, but theirexcessive visual tokens pose a significant computational challenge forreal-world applications. Current methods aim to enhance inference efficiency byvisual token pruning. However, they do not consider the dynamic characteristicsand temporal dependencies of video frames, as they perceive video understandingas a multi-frame task. To address these challenges, we propose MMG-Vid, a noveltraining-free visual token pruning framework that removes redundancy byMaximizing Marginal Gains at both segment-level and token-level. Specifically,we first divide the video into segments based on frame similarity, and thendynamically allocate the token budget for each segment to maximize the marginalgain of each segment. Subsequently, we propose a temporal-guided DPC algorithmthat jointly models inter-frame uniqueness and intra-frame diversity, therebymaximizing the marginal gain of each token. By combining both stages, MMG-Vidcan maximize the utilization of the limited token budget, significantlyimproving efficiency while maintaining strong performance. Extensiveexperiments demonstrate that MMG-Vid can maintain over 99.5% of the originalperformance, while effectively reducing 75% visual tokens and accelerating theprefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.</description><author>Junpeng Ma, Qizhe Zhang, Ming Lu, Zhibin Wang, Qiang Zhou, Jun Song, Shanghang Zhang</author><pubDate>Thu, 28 Aug 2025 17:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21044v1</guid></item><item><title>Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025</title><link>http://arxiv.org/abs/2508.21041v1</link><description>Atypical mitotic figures (AMFs) are markers of abnormal cell divisionassociated with poor prognosis, yet their detection remains difficult due tolow prevalence, subtle morphology, and inter-observer variability. The MIDOG2025 challenge introduces a benchmark for AMF classification across multipledomains. In this work, we evaluate the recently published DINOv3-H+ visiontransformer, pretrained on natural images, which we fine-tuned using low-rankadaptation (LoRA, 650k trainable parameters) and extensive augmentation.Despite the domain gap, DINOv3 transfers effectively to histopathology,achieving a balanced accuracy of 0.8871 on the preliminary test set. Theseresults highlight the robustness of DINOv3 pretraining and show that, whencombined with parameter-efficient fine-tuning, it provides a strong baselinefor atypical mitosis classification in MIDOG 2025.</description><author>Guillaume Balezo, Raphaël Bourgade, Thomas Walter</author><pubDate>Thu, 28 Aug 2025 17:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21041v1</guid></item><item><title>From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement</title><link>http://arxiv.org/abs/2508.10950v2</link><description>Fiber orientation distribution (FOD) is an advanced diffusion MRI modelingtechnique that represents complex white matter fiber configurations, and a keystep for subsequent brain tractography and connectome analysis. Its reliabilityand accuracy, however, heavily rely on the quality of the MRI acquisition andthe subsequent estimation of the FODs at each voxel. Generating reliable FODsfrom widely available clinical protocols with single-shell andlow-angular-resolution acquisitions remains challenging but could potentiallybe addressed with recent advances in deep learning-based enhancementtechniques. Despite advancements, existing methods have predominantly beenassessed on healthy subjects, which have proved to be a major hurdle for theirclinical adoption. In this work, we validate a newly optimized enhancementframework, FastFOD-Net, across healthy controls and six neurological disorders.This accelerated end-to-end deep learning framework enhancing FODs withsuperior performance and delivering training/inference efficiency for clinicaluse ($60\times$ faster comparing to its predecessor). With the mostcomprehensive clinical evaluation to date, our work demonstrates the potentialof FastFOD-Net in accelerating clinical neuroscience research, empoweringdiffusion MRI analysis for disease differentiation, improving interpretabilityin connectome applications, and reducing measurement errors to lower samplesize requirements. Critically, this work will facilitate the more widespreadadoption of, and build clinical trust in, deep learning based methods fordiffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis ofreal-world, clinical diffusion MRI data, comparable to that achievable withhigh-quality research acquisitions.</description><author>Xinyi Wang, Michael Barnett, Frederique Boonstra, Yael Barnett, Mariano Cabezas, Arkiev D'Souza, Matthew C. Kiernan, Kain Kyle, Meng Law, Lynette Masters, Zihao Tang, Stephen Tisch, Sicong Tu, Anneke Van Der Walt, Dongang Wang, Fernando Calamante, Weidong Cai, Chenyu Wang</author><pubDate>Thu, 28 Aug 2025 17:44:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10950v2</guid></item><item><title>FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator</title><link>http://arxiv.org/abs/2508.21040v1</link><description>Labeled handwriting data is often scarce, limiting the effectiveness ofrecognition systems that require diverse, style-consistent training samples.Handwriting synthesis offers a promising solution by generating artificial datato augment training. However, current methods face two major limitations.First, most are built on conventional convolutional architectures, whichstruggle to model long-range dependencies and complex stroke patterns. Second,they largely ignore the crucial role of frequency information, which isessential for capturing fine-grained stylistic and structural details inhandwriting. To address these challenges, we propose FW-GAN, a one-shothandwriting synthesis framework that generates realistic, writer-consistenttext from a single example. Our generator integrates a phase-aware Wave-MLP tobetter capture spatial relationships while preserving subtle stylistic cues. Wefurther introduce a frequency-guided discriminator that leverageshigh-frequency components to enhance the authenticity detection of generatedsamples. Additionally, we introduce a novel Frequency Distribution Loss thataligns the frequency characteristics of synthetic and real handwriting, therebyenhancing visual fidelity. Experiments on Vietnamese and English handwritingdatasets demonstrate that FW-GAN generates high-quality, style-consistenthandwriting, making it a valuable tool for augmenting data in low-resourcehandwriting recognition (HTR) pipelines. Official implementation is availableat https://github.com/DAIR-Group/FW-GAN</description><author>Huynh Tong Dang Khoa, Dang Hoai Nam, Vo Nguyen Le Duy</author><pubDate>Thu, 28 Aug 2025 17:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21040v1</guid></item><item><title>On the Theoretical Limitations of Embedding-Based Retrieval</title><link>http://arxiv.org/abs/2508.21038v1</link><description>Vector embeddings have been tasked with an ever-increasing set of retrievaltasks over the years, with a nascent rise in using them for reasoning,instruction-following, coding, and more. These new benchmarks push embeddingsto work for any query and any notion of relevance that could be given. Whileprior works have pointed out theoretical limitations of vector embeddings,there is a common assumption that these difficulties are exclusively due tounrealistic queries, and those that are not can be overcome with bettertraining data and larger models. In this work, we demonstrate that we mayencounter these theoretical limitations in realistic settings with extremelysimple queries. We connect known results in learning theory, showing that thenumber of top-k subsets of documents capable of being returned as the result ofsome query is limited by the dimension of the embedding. We empirically showthat this holds true even if we restrict to k=2, and directly optimize on thetest set with free parameterized embeddings. We then create a realistic datasetcalled LIMIT that stress tests models based on these theoretical results, andobserve that even state-of-the-art models fail on this dataset despite thesimple nature of the task. Our work shows the limits of embedding models underthe existing single vector paradigm and calls for future research to developmethods that can resolve this fundamental limitation.</description><author>Orion Weller, Michael Boratko, Iftekhar Naim, Jinhyuk Lee</author><pubDate>Thu, 28 Aug 2025 17:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21038v1</guid></item><item><title>Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop</title><link>http://arxiv.org/abs/2508.21036v1</link><description>Generative AI (GenAI) radically expands the scope and capability ofautomation for work, education, and everyday tasks, a transformation posingboth risks and opportunities for human cognition. How will human cognitionchange, and what opportunities are there for GenAI to augment it? Whichtheories, metrics, and other tools are needed to address these questions? TheCHI 2025 workshop on Tools for Thought aimed to bridge an emerging science ofhow the use of GenAI affects human thought, from metacognition to criticalthinking, memory, and creativity, with an emerging design practice for buildingGenAI tools that both protect and augment human thought. Fifty-six researchers,designers, and thinkers from across disciplines as well as industry andacademia, along with 34 papers and portfolios, seeded a day of discussion,ideation, and community-building. We synthesize this material here to beginmapping the space of research and design opportunities and to catalyze amultidisciplinary community around this pressing area of research.</description><author>Lev Tankelevitch, Elena L. Glassman, Jessica He, Aniket Kittur, Mina Lee, Srishti Palani, Advait Sarkar, Gonzalo Ramos, Yvonne Rogers, Hari Subramonyam</author><pubDate>Thu, 28 Aug 2025 17:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21036v1</guid></item><item><title>A multi-task neural network for atypical mitosis recognition under domain shift</title><link>http://arxiv.org/abs/2508.21035v1</link><description>Recognizing atypical mitotic figures in histopathology images allowsphysicians to correctly assess tumor aggressiveness. Although machine learningmodels could be exploited for automatically performing such a task, underdomain shift these models suffer from significative performance drops. In thiswork, an approach based on multi-task learning is proposed for addressing thisproblem. By exploiting auxiliary tasks, correlated to the main classificationtask, the proposed approach, submitted to the track 2 of the MItosis DOmainGeneralization (MIDOG) challenge, aims to aid the model to focus only on theobject to classify, ignoring the domain varying background of the image. Theproposed approach shows promising performance in a preliminary evaluationconducted on three distinct datasets, i.e., the MIDOG 2025 Atypical TrainingSet, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25challenge.</description><author>Gennaro Percannella, Mattia Sarno, Francesco Tortorella, Mario Vento</author><pubDate>Thu, 28 Aug 2025 17:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21035v1</guid></item><item><title>Mitosis detection in domain shift scenarios: a Mamba-based approach</title><link>http://arxiv.org/abs/2508.21033v1</link><description>Mitosis detection in histopathology images plays a key role in tumorassessment. Although machine learning algorithms could be exploited for aidingphysicians in accurately performing such a task, these algorithms suffer fromsignificative performance drop when evaluated on images coming from domainsthat are different from the training ones. In this work, we propose aMamba-based approach for mitosis detection under domain shift, inspired by thepromising performance demonstrated by Mamba in medical imaging segmentationtasks. Specifically, our approach exploits a VM-UNet architecture for carryingout the addressed task, as well as stain augmentation operations for furtherimproving model robustness against domain shift. Our approach has beensubmitted to the track 1 of the MItosis DOmain Generalization (MIDOG)challenge. Preliminary experiments, conducted on the MIDOG++ dataset, showlarge room for improvement for the proposed method.</description><author>Gennaro Percannella, Mattia Sarno, Francesco Tortorella, Mario Vento</author><pubDate>Thu, 28 Aug 2025 17:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21033v1</guid></item><item><title>Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets</title><link>http://arxiv.org/abs/2508.21032v1</link><description>Text-to-image diffusion models enable high-quality image generation but arecomputationally expensive. While prior work optimizes per-inference efficiency,we explore an orthogonal approach: reducing redundancy across correlatedprompts. Our method leverages the coarse-to-fine nature of diffusion models,where early denoising steps capture shared structures among similar prompts. Wepropose a training-free approach that clusters prompts based on semanticsimilarity and shares computation in early diffusion steps. Experiments showthat for models trained conditioned on image embeddings, our approachsignificantly reduces compute cost while improving image quality. By leveragingUnClip's text-to-image prior, we enhance diffusion step allocation for greaterefficiency. Our method seamlessly integrates with existing pipelines, scaleswith prompt sets, and reduces the environmental and financial burden oflarge-scale text-to-image generation. Project page:https://ddecatur.github.io/hierarchical-diffusion/</description><author>Dale Decatur, Thibault Groueix, Wang Yifan, Rana Hanocka, Vladimir Kim, Matheus Gadelha</author><pubDate>Thu, 28 Aug 2025 17:35:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21032v1</guid></item><item><title>Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions</title><link>http://arxiv.org/abs/2508.19945v2</link><description>We present an inverse dynamic game-based algorithm to learn parametricconstraints from a given dataset of local generalized Nash equilibriuminteractions between multiple agents. Specifically, we introduce mixed-integerlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of theinteracting agents, which recover constraints consistent with the Nashstationarity of the interaction demonstrations. We establish theoreticalguarantees that our method learns inner approximations of the true safe andunsafe sets, as well as limitations of constraint learnability fromdemonstrations of Nash equilibrium interactions. We also use the interactionconstraints recovered by our method to design motion plans that robustlysatisfy the underlying constraints. Across simulations and hardwareexperiments, our methods proved capable of inferring constraints and designinginteractive motion plans for various classes of constraints, both convex andnon-convex, from interaction demonstrations of agents with nonlinear dynamics.</description><author>Zhouyu Zhang, Chih-Yuan Chiu, Glen Chou</author><pubDate>Thu, 28 Aug 2025 17:30:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19945v2</guid></item><item><title>Probing Pre-Trained Language Models for Cross-Cultural Differences in Values</title><link>http://arxiv.org/abs/2203.13722v3</link><description>Language embeds information about social, cultural, and political valuespeople hold. Prior work has explored social and potentially harmful biasesencoded in Pre-Trained Language models (PTLMs). However, there has been nosystematic study investigating how values embedded in these models vary acrosscultures. In this paper, we introduce probes to study which values acrosscultures are embedded in these models, and whether they align with existingtheories and cross-cultural value surveys. We find that PTLMs capturedifferences in values across cultures, but those only weakly align withestablished value surveys. We discuss implications of using mis-aligned modelsin cross-cultural settings, as well as ways of aligning PTLMs with valuesurveys.</description><author>Arnav Arora, Lucie-Aimée Kaffee, Isabelle Augenstein</author><pubDate>Thu, 28 Aug 2025 17:30:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.13722v3</guid></item><item><title>The Ramon Llull's Thinking Machine for Automated Ideation</title><link>http://arxiv.org/abs/2508.19200v2</link><description>This paper revisits Ramon Llull's Ars combinatoria - a medieval framework forgenerating knowledge through symbolic recombination - as a conceptualfoundation for building a modern Llull's thinking machine for researchideation. Our approach defines three compositional axes: Theme (e.g.,efficiency, adaptivity), Domain (e.g., question answering, machinetranslation), and Method (e.g., adversarial training, linear attention). Theseelements represent high-level abstractions common in scientific work -motivations, problem settings, and technical approaches - and serve as buildingblocks for LLM-driven exploration. We mine elements from human experts orconference papers and show that prompting LLMs with curated combinationsproduces research ideas that are diverse, relevant, and grounded in currentliterature. This modern thinking machine offers a lightweight, interpretabletool for augmenting scientific creativity and suggests a path towardcollaborative ideation between humans and AI.</description><author>Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu</author><pubDate>Thu, 28 Aug 2025 17:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19200v2</guid></item><item><title>An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs</title><link>http://arxiv.org/abs/2508.21024v1</link><description>Retrieval-Augmented Generation (RAG) has emerged as a powerful solution tomitigate the limitations of Large Language Models (LLMs), such ashallucinations and outdated knowledge. However, deploying RAG-based tools inSmall and Medium Enterprises (SMEs) remains a challenge due to their limitedresources and lack of expertise in natural language processing (NLP). Thispaper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, astructured, agile method designed to facilitate the deployment of RAG systemsin industrial SME contexts. EASI-RAG is based on method engineering principlesand comprises well-defined roles, activities, and techniques. The method wasvalidated through a real-world case study in an environmental testinglaboratory, where a RAG tool was implemented to answer operators queries usingdata extracted from operational procedures. The system was deployed in under amonth by a team with no prior RAG experience and was later iteratively improvedbased on user feedback. Results demonstrate that EASI-RAG supports fastimplementation, high user adoption, delivers accurate answers, and enhances thereliability of underlying data. This work highlights the potential of RAGdeployment in industrial SMEs. Future works include the need for generalizationacross diverse use cases and further integration with fine-tuned models.</description><author>Mathieu Bourdin, Anas Neumann, Thomas Paviot, Robert Pellerin, Samir Lamouri</author><pubDate>Thu, 28 Aug 2025 17:27:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21024v1</guid></item><item><title>Fast Convergence Rates for Subsampled Natural Gradient Algorithms on Quadratic Model Problems</title><link>http://arxiv.org/abs/2508.21022v1</link><description>Subsampled natural gradient descent (SNGD) has shown impressive results forparametric optimization tasks in scientific machine learning, such as neuralnetwork wavefunctions and physics-informed neural networks, but it has lacked atheoretical explanation. We address this gap by analyzing the convergence ofSNGD and its accelerated variant, SPRING, for idealized parametric optimizationproblems where the model is linear and the loss function is strongly convex andquadratic. In the special case of a least-squares loss, namely the standardlinear least-squares problem, we prove that SNGD is equivalent to a regularizedKaczmarz method while SPRING is equivalent to an accelerated regularizedKaczmarz method. As a result, by leveraging existing analyses we obtain undermild conditions (i) the first fast convergence rate for SNGD, (ii) the firstconvergence guarantee for SPRING in any setting, and (iii) the first proof thatSPRING can accelerate SNGD. In the case of a general strongly convex quadraticloss, we extend the analysis of the regularized Kaczmarz method to obtain afast convergence rate for SNGD under stronger conditions, providing the firstexplanation for the effectiveness of SNGD outside of the least-squares setting.Overall, our results illustrate how tools from randomized linear algebra canshed new light on the interplay between subsampling and curvature-awareoptimization strategies.</description><author>Gil Goldshlager, Jiang Hu, Lin Lin</author><pubDate>Thu, 28 Aug 2025 17:24:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21022v1</guid></item><item><title>POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models</title><link>http://arxiv.org/abs/2508.21019v1</link><description>The field of video diffusion generation faces critical bottlenecks insampling efficiency, especially for large-scale models and long sequences.Existing video acceleration methods adopt image-based techniques but sufferfrom fundamental limitations: they neither model the temporal coherence ofvideo frames nor provide single-step distillation for large-scale video models.To bridge this gap, we propose POSE (Phased One-Step Equilibrium), adistillation framework that reduces the sampling steps of large-scale videodiffusion models, enabling the generation of high-quality videos in a singlestep. POSE employs a carefully designed two-phase process to distill videomodels:(i) stability priming: a warm-up mechanism to stabilize adversarialdistillation that adapts the high-quality trajectory of the one-step generatorfrom high to low signal-to-noise ratio regimes, optimizing the video quality ofsingle-step mappings near the endpoints of flow trajectories. (ii) unifiedadversarial equilibrium: a flexible self-adversarial distillation mechanismthat promotes stable single-step adversarial training towards a Nashequilibrium within the Gaussian noise space, generating realistic single-stepvideos close to real videos. For conditional video generation, we propose (iii)conditional adversarial consistency, a method to improve both semanticconsistency and frame consistency between conditional frames and generatedframes. Comprehensive experiments demonstrate that POSE outperforms otheracceleration methods on VBench-I2V by average 7.15% in semantic alignment,temporal conference and frame quality, reducing the latency of the pre-trainedmodel by 100$\times$, from 1000 seconds to 10 seconds, while maintainingcompetitive performance.</description><author>Jiaxiang Cheng, Bing Ma, Xuhua Ren, Hongyi Jin, Kai Yu, Peng Zhang, Wenyue Li, Yuan Zhou, Tianxiang Zheng, Qinglin Lu</author><pubDate>Thu, 28 Aug 2025 17:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21019v1</guid></item><item><title>DanceGRPO: Unleashing GRPO on Visual Generation</title><link>http://arxiv.org/abs/2505.07818v4</link><description>Recent advances in generative AI have revolutionized visual content creation,yet aligning model outputs with human preferences remains a critical challenge.While Reinforcement Learning (RL) has emerged as a promising approach forfine-tuning generative models, existing methods like DDPO and DPOK facefundamental limitations - particularly their inability to maintain stableoptimization when scaling to large and diverse prompt sets, severelyrestricting their practical utility. This paper presents DanceGRPO, a frameworkthat addresses these limitations through an innovative adaptation of GroupRelative Policy Optimization (GRPO) for visual generation tasks. Our keyinsight is that GRPO's inherent stability mechanisms uniquely position it toovercome the optimization challenges that plague prior RL-based approaches onvisual generation. DanceGRPO establishes several significant advances: First,it demonstrates consistent and stable policy optimization across multiplemodern generative paradigms, including both diffusion models and rectifiedflows. Second, it maintains robust performance when scaling to complex,real-world scenarios encompassing three key tasks and four foundation models.Third, it shows remarkable versatility in optimizing for diverse humanpreferences as captured by five distinct reward models assessing image/videoaesthetics, text-image alignment, video motion quality, and binary feedback.Our comprehensive experiments reveal that DanceGRPO outperforms baselinemethods by up to 181\% across multiple established benchmarks, includingHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Our results establish DanceGRPOas a robust and versatile solution for scaling Reinforcement Learning fromHuman Feedback (RLHF) tasks in visual generation, offering new insights intoharmonizing reinforcement learning and visual synthesis.</description><author>Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo</author><pubDate>Thu, 28 Aug 2025 17:19:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.07818v4</guid></item><item><title>Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance</title><link>http://arxiv.org/abs/2508.21016v1</link><description>Denoising-based generative models, particularly diffusion and flow matchingalgorithms, have achieved remarkable success. However, aligning their outputdistributions with complex downstream objectives, such as human preferences,compositional accuracy, or data compressibility, remains challenging. Whilereinforcement learning (RL) fine-tuning methods, inspired by advances in RLfrom human feedback (RLHF) for large language models, have been adapted tothese generative frameworks, current RL approaches are suboptimal for diffusionmodels and offer limited flexibility in controlling alignment strength afterfine-tuning. In this work, we reinterpret RL fine-tuning for diffusion modelsthrough the lens of stochastic differential equations and implicit rewardconditioning. We introduce Reinforcement Learning Guidance (RLG), aninference-time method that adapts Classifier-Free Guidance (CFG) by combiningthe outputs of the base and RL fine-tuned models via a geometric average. Ourtheoretical analysis shows that RLG's guidance scale is mathematicallyequivalent to adjusting the KL-regularization coefficient in standard RLobjectives, enabling dynamic control over the alignment-quality trade-offwithout further training. Extensive experiments demonstrate that RLGconsistently improves the performance of RL fine-tuned models across variousarchitectures, RL algorithms, and downstream tasks, including humanpreferences, compositional control, compressibility, and text rendering.Furthermore, RLG supports both interpolation and extrapolation, therebyoffering unprecedented flexibility in controlling generative alignment. Ourapproach provides a practical and theoretically sound solution for enhancingand controlling diffusion model alignment at inference. The source code for RLGis publicly available at the Github:https://github.com/jinluo12345/Reinforcement-learning-guidance.</description><author>Luozhijie Jin, Zijie Qiu, Jie Liu, Zijie Diao, Lifeng Qiao, Ning Ding, Alex Lamb, Xipeng Qiu</author><pubDate>Thu, 28 Aug 2025 17:18:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21016v1</guid></item><item><title>Expert Routing with Synthetic Data for Continual Learning</title><link>http://arxiv.org/abs/2412.17009v3</link><description>In many real-world settings, regulations and economic incentives permit thesharing of models but not data across institutional boundaries. In suchscenarios, practitioners might hope to adapt models to new domains, withoutlosing performance on previous domains (so-called catastrophic forgetting).While any single model may struggle to achieve this goal, learning an ensembleof domain-specific experts offers the potential to adapt more closely to eachindividual institution. However, a core challenge in this context isdetermining which expert to deploy at test time. In this paper, we proposeGenerate to Discriminate (G2D), a domain-incremental continual learning methodthat leverages synthetic data to train a domain-discriminator that routessamples at inference time to the appropriate expert. Surprisingly, we find thatleveraging synthetic data in this capacity is more effective than using thesamples to \textit{directly} train the downstream classifier (the more commonapproach to leveraging synthetic data in the lifelong learning literature). Weobserve that G2D outperforms competitive domain-incremental learning methods ontasks in both vision and language modalities, providing a new perspective onthe use of synthetic data in the lifelong learning literature.</description><author>Yewon Byun, Sanket Vaibhav Mehta, Saurabh Garg, Emma Strubell, Michael Oberst, Bryan Wilder, Zachary C. Lipton</author><pubDate>Thu, 28 Aug 2025 17:13:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17009v3</guid></item><item><title>OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset</title><link>http://arxiv.org/abs/2301.06375v2</link><description>Inspired by humans comprehending speech in a multi-modal manner, variousaudio-visual datasets have been constructed. However, most existing datasetsfocus on English, induce dependencies with various prediction models duringdataset preparation, and have only a small number of multi-view videos. Tomitigate the limitations, we recently developed the Open Large-scale KoreanAudio-Visual Speech (OLKAVS) dataset, which is the largest among publiclyavailable audio-visual speech datasets. The dataset contains 1,150 hours oftranscribed audio from 1,107 Korean speakers in a studio setup with ninedifferent viewpoints and various noise situations. We also provide thepre-trained baseline models for two tasks, audio-visual speech recognition andlip reading. We conducted experiments based on the models to verify theeffectiveness of multi-modal and multi-view training over uni-modal andfrontal-view-only training. We expect the OLKAVS dataset to facilitatemulti-modal research in broader areas such as Korean speech recognition,speaker recognition, pronunciation level classification, and mouth motionanalysis.</description><author>Jeongkyun Park, Jung-Wook Hwang, Kwanghee Choi, Seung-Hyun Lee, Jun Hwan Ahn, Rae-Hong Park, Hyung-Min Park</author><pubDate>Thu, 28 Aug 2025 17:12:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06375v2</guid></item><item><title>ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering</title><link>http://arxiv.org/abs/2508.21010v1</link><description>Existing Causal-Why Video Question Answering (VideoQA) models often strugglewith higher-order reasoning, relying on opaque, monolithic pipelines thatentangle video understanding, causal inference, and answer generation. Theseblack-box approaches offer limited interpretability and tend to depend onshallow heuristics. We propose a novel, modular framework that explicitlydecouples causal reasoning from answer generation, introducing natural languagecausal chains as interpretable intermediate representations. Inspired by humancognitive models, these structured cause-effect sequences bridge low-levelvideo content with high-level causal reasoning, enabling transparent andlogically coherent inference. Our two-stage architecture comprises a CausalChain Extractor (CCE) that generates causal chains from video-question pairs,and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded inthese chains. To address the lack of annotated reasoning traces, we introduce ascalable method for generating high-quality causal chains from existingdatasets using large language models. We also propose CauCo, a new evaluationmetric for causality-oriented captioning. Experiments on three large-scalebenchmarks demonstrate that our approach not only outperforms state-of-the-artmodels, but also yields substantial gains in explainability, user trust, andgeneralization -- positioning the CCE as a reusable causal reasoning engineacross diverse domains. Project page:https://paritoshparmar.github.io/chainreaction/</description><author>Paritosh Parmar, Eric Peh, Basura Fernando</author><pubDate>Thu, 28 Aug 2025 17:10:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21010v1</guid></item><item><title>Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution</title><link>http://arxiv.org/abs/2508.21004v1</link><description>Large language models (LLMs) have seen significant advancements, achievingsuperior performance in various Natural Language Processing (NLP) tasks.However, they remain vulnerable to backdoor attacks, where models behavenormally for standard queries but generate harmful responses or unintendedoutput when specific triggers are activated. Existing backdoor defenses eitherlack comprehensiveness, focusing on narrow trigger settings, detection-onlymechanisms, and limited domains, or fail to withstand advanced scenarios likemodel-editing-based, multi-trigger, and triggerless attacks. In this paper, wepresent LETHE, a novel method to eliminate backdoor behaviors from LLMs throughknowledge dilution using both internal and external mechanisms. Internally,LETHE leverages a lightweight dataset to train a clean model, which is thenmerged with the backdoored model to neutralize malicious behaviors by dilutingthe backdoor impact within the model's parametric memory. Externally, LETHEincorporates benign and semantically relevant evidence into the prompt todistract LLM's attention from backdoor features. Experimental results onclassification and generation domains across 5 widely used LLMs demonstratethat LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoorattacks. LETHE reduces the attack success rate of advanced backdoor attacks byup to 98% while maintaining model utility. Furthermore, LETHE has proven to becost-efficient and robust against adaptive backdoor attacks.</description><author>Chen Chen, Yuchen Sun, Jiaxin Gao, Xueluan Gong, Qian Wang, Ziyao Wang, Yongsen Zheng, Kwok-Yan Lam</author><pubDate>Thu, 28 Aug 2025 17:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21004v1</guid></item><item><title>InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity</title><link>http://arxiv.org/abs/2508.21003v1</link><description>In this paper, we introduce InSQuAD, designed to enhance the performance ofIn-Context Learning (ICL) models through Submodular Mutual Information} (SMI)enforcing Quality and Diversity among in-context exemplars. InSQuAD achievesthis through two principal strategies: First, we model the ICL task as atargeted selection problem and introduce a unified selection strategy based onSMIs which mines relevant yet diverse in-context examples encapsulating thenotions of quality and diversity. Secondly, we address a common pitfall inexisting retrieval models which model query relevance, often overlookingdiversity, critical for ICL. InSQuAD introduces a combinatorial trainingparadigm which learns the parameters of an SMI function to enforce both qualityand diversity in the retrieval model through a novel likelihood-based loss. Tofurther aid the learning process we augment an existing multi-hop questionanswering dataset with synthetically generated paraphrases. Adopting theretrieval model trained using this strategy alongside the novel targetedselection formulation for ICL on nine benchmark datasets shows significantimprovements validating the efficacy of our approach.</description><author>Souradeep Nanda, Anay Majee, Rishabh Iyer</author><pubDate>Thu, 28 Aug 2025 17:04:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21003v1</guid></item><item><title>Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees</title><link>http://arxiv.org/abs/2508.21001v1</link><description>Kinodynamic motion planning is concerned with computing collision-freetrajectories while abiding by the robot's dynamic constraints. This criticalproblem is often tackled using sampling-based planners (SBPs) that explore therobot's high-dimensional state space by constructing a search tree via actionpropagations. Although SBPs can offer global guarantees on completeness andsolution quality, their performance is often hindered by slow exploration dueto uninformed action sampling. Learning-based approaches can yieldsignificantly faster runtimes, yet they fail to generalize toout-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,thus limiting their deployment on physical robots. We present Diffusion Tree(DiTree): a \emph{provably-generalizable} framework leveraging diffusionpolicies (DPs) as informed samplers to efficiently guide state-space searchwithin SBPs. DiTree combines DP's ability to model complex distributions ofexpert trajectories, conditioned on local observations, with the completenessof SBPs to yield \emph{provably-safe} solutions within a few action propagationiterations for complex dynamical systems. We demonstrate DiTree's power with animplementation combining the popular RRT planner with a DP action samplertrained on a \emph{single environment}. In comprehensive evaluations on OODscenarios, % DiTree has comparable runtimes to a standalone DP (3x faster thanclassical SBPs), while improving the average success rate over DP and SBPs.DiTree is on average 3x faster than classical SBPs, and outperforms all otherapproaches by achieving roughly 30\% higher success rate. Project webpage:https://sites.google.com/view/ditree.</description><author>Yaniv Hassidof, Tom Jurgenson, Kiril Solovey</author><pubDate>Thu, 28 Aug 2025 17:04:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21001v1</guid></item><item><title>ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery</title><link>http://arxiv.org/abs/2508.20996v1</link><description>Substance use disorders (SUDs) affect over 36 million people worldwide, yetfew receive effective care due to stigma, motivational barriers, and limitedpersonalized support. Although large language models (LLMs) show promise formental-health assistance, most systems lack tight integration with clinicallyvalidated strategies, reducing effectiveness in addiction recovery. We presentChatThero, a multi-agent conversational framework that couples dynamic patientmodeling with context-sensitive therapeutic dialogue and adaptive persuasivestrategies grounded in cognitive behavioral therapy (CBT) and motivationalinterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,Medium, and Hard resistance levels, and train ChatThero with a two-stagepipeline comprising supervised fine-tuning (SFT) followed by direct preferenceoptimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain inpatient motivation, a 0.49\% increase in treatment confidence, and resolveshard cases with 26\% fewer turns than GPT-4o, and both automated and humanclinical assessments rate it higher in empathy, responsiveness, and behavioralrealism. The framework supports rigorous, privacy-preserving study oftherapeutic conversation and provides a robust, replicable basis for researchand clinical translation.</description><author>Junda Wang, Zonghai Yao, Zhichao Yang, Lingxi Li, Junhui Qian, Hong Yu</author><pubDate>Thu, 28 Aug 2025 16:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20996v1</guid></item><item><title>ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts</title><link>http://arxiv.org/abs/2508.20991v1</link><description>Simulating detector responses is a crucial part of understanding the innerworkings of particle collisions in the Large Hadron Collider at CERN. Suchsimulations are currently performed with statistical Monte Carlo methods, whichare computationally expensive and put a significant strain on CERN'scomputational grid. Therefore, recent proposals advocate for generative machinelearning methods to enable more efficient simulations. However, thedistribution of the data varies significantly across the simulations, which ishard to capture with out-of-the-box methods. In this study, we presentExpertSim - a deep learning simulation approach tailored for the Zero DegreeCalorimeter in the ALICE experiment. Our method utilizes aMixture-of-Generative-Experts architecture, where each expert specializes insimulating a different subset of the data. This allows for a more precise andefficient generation process, as each expert focuses on a specific aspect ofthe calorimeter response. ExpertSim not only improves accuracy, but alsoprovides a significant speedup compared to the traditional Monte-Carlo methods,offering a promising solution for high-efficiency detector simulations inparticle physics experiments at CERN. We make the code available athttps://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.</description><author>Patryk Będkowski, Jan Dubiński, Filip Szatkowski, Kamil Deja, Przemysław Rokita, Tomasz Trzciński</author><pubDate>Thu, 28 Aug 2025 16:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20991v1</guid></item><item><title>ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation</title><link>http://arxiv.org/abs/2312.05407v4</link><description>Unsupervised domain adaptive segmentation typically relies on self-trainingusing pseudo labels predicted by a pre-trained network on an unlabeled targetdataset. However, the noisy nature of such pseudo-labels presents a majorbottleneck in adapting a network to the distribution shift between source andtarget datasets. This challenge is exaggerated when the network encounters anincoming data stream in online fashion, where the network is constrained toadapt to incoming streams of target domain data in exactly one round of forwardand backward passes. In this scenario, relying solely on inaccuratepseudo-labels can lead to low-quality segmentation, which is detrimental tomedical image analysis where accuracy and precision are of utmost priority. Wehypothesize that a small amount of pixel-level annotation obtained from anexpert can address this problem, thereby enhancing the performance of domainadaptation of online streaming data, even in the absence of dedicated trainingdata. We call our method ODES: Domain Adaptation with Expert Guidance forOnline Medical Image Segmentation that adapts to each incoming data batch in anonline setup, incorporating feedback from an expert through active learning.Through active learning, the most informative pixels in each image can beselected for expert annotation. However, the acquisition of pixel-levelannotations across all images in a batch often leads to redundant informationwhile increasing temporal overhead in online learning. To reduce the annotationacquisition time and make the adaptation process more online-friendly, wefurther propose a novel image-pruning strategy that selects the most usefulsubset of images from the current batch for active learning. Our proposedapproach outperforms existing online adaptation approaches and producescompetitive results compared to offline domain adaptive active learningmethods.</description><author>Md Shazid Islam, Sayak Nag, Arindam Dutta, Miraj Ahmed, Fahim Faisal Niloy, Shreyangshu Bera, Amit K. Roy-Chowdhury</author><pubDate>Thu, 28 Aug 2025 16:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05407v4</guid></item><item><title>InterAct-Video: Reasoning-Rich Video QA for Urban Traffic</title><link>http://arxiv.org/abs/2507.14743v3</link><description>Traffic monitoring is crucial for urban mobility, road safety, andintelligent transportation systems (ITS). Deep learning has advancedvideo-based traffic monitoring through video question answering (VideoQA)models, enabling structured insight extraction from traffic videos. However,existing VideoQA models struggle with the complexity of real-world trafficscenes, where multiple concurrent events unfold across spatiotemporaldimensions. To address these challenges, this paper introduces \textbf{InterActVideoQA}, a curated dataset designed to benchmark and enhance VideoQA modelsfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours ofreal-world traffic footage collected from diverse intersections, segmented into10-second video clips, with over 25,000 question-answer (QA) pairs coveringspatiotemporal dynamics, vehicle interactions, incident detection, and othercritical traffic attributes. State-of-the-art VideoQA models are evaluated onInterAct VideoQA, exposing challenges in reasoning over fine-grainedspatiotemporal dependencies within complex traffic scenarios. Additionally,fine-tuning these models on InterAct VideoQA yields notable performanceimprovements, demonstrating the necessity of domain-specific datasets forVideoQA. InterAct VideoQA is publicly available as a benchmark dataset tofacilitate future research in real-world deployable VideoQA models forintelligent transportation systems. GitHub Repo:https://github.com/joe-rabbit/InterAct_VideoQA</description><author>Joseph Raj Vishal, Divesh Basina, Rutuja Patil, Manas Srinivas Gowda, Katha Naik, Yezhou Yang, Bharatesh Chakravarthi</author><pubDate>Thu, 28 Aug 2025 16:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14743v3</guid></item><item><title>Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation</title><link>http://arxiv.org/abs/2508.20987v1</link><description>Images manipulated using image editing tools can mislead viewers and posesignificant risks to social security. However, accurately localizing themanipulated regions within an image remains a challenging problem. One of themain barriers in this area is the high cost of data acquisition and the severelack of high-quality annotated datasets. To address this challenge, weintroduce novel methods that mitigate data scarcity by leveraging readilyavailable web data. We utilize a large collection of manually forged imagesfrom the web, as well as automatically generated annotations derived from asimpler auxiliary task, constrained image manipulation localization.Specifically, we introduce a new paradigm CAAAv2, which automatically andaccurately annotates manipulated regions at the pixel level. To further improveannotation quality, we propose a novel metric, QES, which filters outunreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, alarge-scale, diverse, and high-quality dataset containing 246,212 manuallyforged images with pixel-level mask annotations. This is over 120x larger thanexisting handcrafted datasets like IMD20. Additionally, we introduce ObjectJitter, a technique that further enhances model training by generatinghigh-quality manipulation artifacts. Building on these advances, we develop anew model, Web-IML, designed to effectively leverage web-scale supervision forthe image manipulation localization task. Extensive experiments demonstratethat our approach substantially alleviates the data scarcity problem andsignificantly improves the performance of various models on multiple real-worldforgery benchmarks. With the proposed web supervision, Web-IML achieves astriking performance gain of 31% and surpasses previous SOTA TruFor by 24.1average IoU points. The dataset and code will be made publicly available athttps://github.com/qcf-568/MIML.</description><author>Chenfan Qu, Yiwu Zhong, Bin Li, Lianwen Jin</author><pubDate>Thu, 28 Aug 2025 16:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20987v1</guid></item><item><title>LGDE: Local Graph-based Dictionary Expansion</title><link>http://arxiv.org/abs/2405.07764v4</link><description>We present Local Graph-based Dictionary Expansion (LGDE), a method fordata-driven discovery of the semantic neighbourhood of words using tools frommanifold learning and network science. At the heart of LGDE lies the creationof a word similarity graph from the geometry of word embeddings followed bylocal community detection based on graph diffusion. The diffusion in the localgraph manifold allows the exploration of the complex nonlinear geometry of wordembeddings to capture word similarities based on paths of semantic association,over and above direct pairwise similarities. Exploiting such semanticneighbourhoods enables the expansion of dictionaries of pre-selected keywords,an important step for tasks in information retrieval, such as database queriesand online data collection. We validate LGDE on two user-generatedEnglish-language corpora and show that LGDE enriches the list of keywords withimproved performance relative to methods based on direct word similarities orco-occurrences. We further demonstrate our method through a real-world use casefrom communication science, where LGDE is evaluated quantitatively on theexpansion of a conspiracy-related dictionary from online data collected andanalysed by domain experts. Our empirical results and expert user assessmentindicate that LGDE expands the seed dictionary with more useful keywords due tothe manifold-learning-based similarity network.</description><author>Juni Schindler, Sneha Jha, Xixuan Zhang, Kilian Buehling, Annett Heft, Mauricio Barahona</author><pubDate>Thu, 28 Aug 2025 16:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07764v4</guid></item><item><title>FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference</title><link>http://arxiv.org/abs/2407.15161v3</link><description>Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands frompartial observations remains a critical challenge in robot learning. Priorgenerative methods struggle to model the intricate grasp distribution ofdexterous hands and often fail to reason about shape uncertainty inherent inpartial point clouds, leading to unreliable or overly conservative grasps. Wepropose FFHFlow, a flow-based variational framework that generates diverse,robust multi-finger grasps while explicitly quantifying perceptual uncertaintyin the partial point clouds. Our approach leverages a normalizing flow-baseddeep latent variable model to learn a hierarchical grasp manifold, overcomingthe mode collapse and rigid prior limitations of conditional VariationalAutoencoders (cVAEs). By exploiting the invertibility and exact likelihoods offlows, FFHFlow introspects shape uncertainty in partial observations andidentifies novel object structures, enabling risk-aware grasp synthesis. Tofurther enhance reliability, we integrate a discriminative grasp evaluator withthe flow likelihoods, formulating an uncertainty-aware ranking strategy thatprioritizes grasps robust to shape ambiguity. Extensive experiments insimulation and real-world setups demonstrate that FFHFlow outperformsstate-of-the-art baselines (including diffusion models) in grasp diversity andsuccess rate, while achieving run-time efficient sampling. We also showcase itspractical value in cluttered and confined environments, where diversity-drivensampling excels by mitigating collisions (Project Page:https://sites.google.com/view/ffhflow/home/).</description><author>Qian Feng, Jianxiang Feng, Zhaopeng Chen, Rudolph Triebel, Alois Knoll</author><pubDate>Thu, 28 Aug 2025 16:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15161v3</guid></item><item><title>Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets</title><link>http://arxiv.org/abs/2508.20986v1</link><description>Data has become a foundational asset driving innovation across domains suchas finance, healthcare, and e-commerce. In these areas, predictive modelingover relational tables is commonly employed, with increasing emphasis onreducing manual effort through automated machine learning (AutoML) techniques.This raises an interesting question: can feature augmentation itself beautomated and identify and utilize task-related relational signals? To address this challenge, we propose an end-to-end automated featureaugmentation framework, ReCoGNN, which enhances initial datasets using featuresextracted from multiple relational tables to support predictive tasks. ReCoGNNfirst captures semantic dependencies within each table by modeling intra-tableattribute relationships, enabling it to partition tables into structured,semantically coherent segments. It then constructs a heterogeneous weightedgraph that represents inter-row relationships across all segments. Finally,ReCoGNN leverages message-passing graph neural networks to propagateinformation through the graph, guiding feature selection and augmenting theoriginal dataset. Extensive experiments conducted on ten real-life andsynthetic datasets demonstrate that ReCoGNN consistently outperforms existingmethods on both classification and regression tasks.</description><author>Lianpeng Qiao, Ziqi Cao, Kaiyu Feng, Ye Yuan, Guoren Wang</author><pubDate>Thu, 28 Aug 2025 16:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20986v1</guid></item><item><title>Dynamic Context Compression for Efficient RAG</title><link>http://arxiv.org/abs/2507.22931v2</link><description>Retrieval-augmented generation (RAG) enhances large language models (LLMs)with external knowledge but incurs significant inference costs due to lengthyretrieved contexts. While context compression mitigates this issue, existingmethods apply fixed compression rates, over-compressing simple queries orunder-compressing complex ones. We propose Adaptive Context Compression for RAG(ACC-RAG), a framework that dynamically adjusts compression rates based oninput complexity, optimizing inference efficiency without sacrificing accuracy.ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) witha context selector to retain minimal sufficient information, akin to humanskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperformsfixed-rate methods and matches/unlocks over 4 times faster inference versusstandard RAG while maintaining or improving accuracy.</description><author>Shuyu Guo, Zhaochun Ren</author><pubDate>Thu, 28 Aug 2025 16:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22931v2</guid></item><item><title>Program Semantic Inequivalence Game with Large Language Models</title><link>http://arxiv.org/abs/2505.03818v2</link><description>Large Language Models (LLMs) can achieve strong performance on everydaycoding tasks, but they can fail on complex tasks that require non-trivialreasoning about program semantics. Finding training examples to teach LLMs tosolve these tasks can be challenging. In this work, we explore a method to synthetically generate code reasoningtraining data based on a semantic inequivalence game SInQ: a generator agentcreates program variants that are semantically distinct, derived from a datasetof real-world programming tasks, while an evaluator agent has to identify inputexamples that cause the original programs and the generated variants to divergein their behaviour, with the agents training each other semi-adversarially. Weprove that this setup enables theoretically unlimited improvement throughself-play in the limit of infinite computational resources. We evaluated our approach on multiple code generation and understandingbenchmarks, including cross-language vulnerability detection (Lu et al., 2021),where our method improves vulnerability detection in C/C++ code despite beingtrained exclusively on Python code, and the challenging Python builtinidentifier swap benchmark (Miceli-Barone et al., 2023), showing that whereasmodern LLMs still struggle with this benchmark, our approach yields substantialimprovements. We release the code needed to replicate the experiments, as well as thegenerated synthetic data, which can be used to fine-tune LLMs.</description><author>Antonio Valerio Miceli-Barone, Vaishak Belle, Ali Payani</author><pubDate>Thu, 28 Aug 2025 16:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.03818v2</guid></item><item><title>Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System</title><link>http://arxiv.org/abs/2508.20983v1</link><description>The SAFE Challenge evaluates synthetic speech detection across three tasks:unmodified audio, processed audio with compression artifacts, and launderedaudio designed to evade detection. We systematically explore self-supervisedlearning (SSL) front-ends, training data compositions, and audio lengthconfigurations for robust deepfake detection. Our AASIST-based approachincorporates WavLM large frontend with RawBoost augmentation, trained on amultilingual dataset of 256,600 samples spanning 9 languages and over 70 TTSsystems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS.Through extensive experimentation with different SSL front-ends, three trainingdata versions, and two audio lengths, we achieved second place in both Task 1(unmodified audio detection) and Task 3 (laundered audio detection),demonstrating strong generalization and robustness.</description><author>Hashim Ali, Surya Subramani, Lekha Bollinani, Nithin Sai Adupa, Sali El-Loh, Hafiz Malik</author><pubDate>Thu, 28 Aug 2025 16:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20983v1</guid></item><item><title>ActLoc: Learning to Localize on the Move via Active Viewpoint Selection</title><link>http://arxiv.org/abs/2508.20981v1</link><description>Reliable localization is critical for robot navigation, yet most existingsystems implicitly assume that all viewing directions at a location are equallyinformative. In practice, localization becomes unreliable when the robotobserves unmapped, ambiguous, or uninformative regions. To address this, wepresent ActLoc, an active viewpoint-aware planning framework for enhancinglocalization accuracy for general robot navigation tasks. At its core, ActLocemploys a largescale trained attention-based model for viewpoint selection. Themodel encodes a metric map and the camera poses used during map construction,and predicts localization accuracy across yaw and pitch directions at arbitrary3D locations. These per-point accuracy distributions are incorporated into apath planner, enabling the robot to actively select camera orientations thatmaximize localization robustness while respecting task and motion constraints.ActLoc achieves stateof-the-art results on single-viewpoint selection andgeneralizes effectively to fulltrajectory planning. Its modular design makes itreadily applicable to diverse robot navigation and inspection tasks.</description><author>Jiajie Li, Boyang Sun, Luca Di Giammarino, Hermann Blum, Marc Pollefeys</author><pubDate>Thu, 28 Aug 2025 16:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20981v1</guid></item><item><title>Efficient Neuro-Symbolic Learning of Constraints and Objective</title><link>http://arxiv.org/abs/2508.20978v1</link><description>In the ongoing quest for hybridizing discrete reasoning with neural nets,there is an increasing interest in neural architectures that can learn how tosolve discrete reasoning or optimization problems from natural inputs, a taskthat Large Language Models seem to struggle with. Objectives: We introduce a differentiable neuro-symbolic architecture and aloss function dedicated to learning how to solve NP-hard reasoning problems. Methods: Our new probabilistic loss allows for learning both the constraintsand the objective, thus delivering a complete model that can be scrutinized andcompleted with side constraints. By pushing the combinatorial solver out of thetraining loop, our architecture also offers scalable training while exactinference gives access to maximum accuracy. Results: We empirically show that it can efficiently learn how to solveNP-hard reasoning problems from natural inputs. On three variants of the Sudokubenchmark -- symbolic, visual, and many-solution --, our approach requires afraction of training time of other hybrid methods. On a visual Min-Cut/Max-cuttask, it optimizes the regret better than a Decision-Focused-Learningregret-dedicated loss. Finally, it efficiently learns the energy optimizationformulation of the large real-world problem of designing proteins.</description><author>Marianne Defresne, Romain Gambardella, Sophie Barbe, Thomas Schiex</author><pubDate>Thu, 28 Aug 2025 16:33:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20978v1</guid></item><item><title>WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations</title><link>http://arxiv.org/abs/2508.20976v1</link><description>Large audio language models (LALMs) extend language understanding into theauditory domain, yet their ability to perform low-level listening, such aspitch and duration detection, remains underexplored. However, low-levellistening is critical for real-world, out-of-distribution tasks where modelsmust reason about unfamiliar sounds based on fine-grained acoustic cues. Toaddress this gap, we introduce the World-of-Whale benchmark (WoW-Bench) toevaluate low-level auditory perception and cognition using marine mammalvocalizations. WoW-bench is composed of a Perception benchmark for categorizingnovel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assessthe abilities to remember, understand, apply, and analyze sound events. For theCognition benchmark, we additionally introduce distractor questions to evaluatewhether models are truly solving problems through listening rather than relyingon other heuristics. Experiments with state-of-the-art LALMs show performancefar below human levels, indicating a need for stronger auditory grounding inLALMs.</description><author>Jaeyeon Kim, Heeseung Yun, Sang Hoon Woo, Chao-Han Huck Yang, Gunhee Kim</author><pubDate>Thu, 28 Aug 2025 16:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20976v1</guid></item><item><title>ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents</title><link>http://arxiv.org/abs/2508.20973v1</link><description>Proactive dialogue has emerged as a critical and challenging research problemin advancing large language models (LLMs). Existing works predominantly focuson domain-specific or task-oriented scenarios, which leads to fragmentedevaluations and limits the comprehensive exploration of models' proactiveconversation abilities. In this work, we propose ProactiveEval, a unifiedframework designed for evaluating proactive dialogue capabilities of LLMs. Thisframework decomposes proactive dialogue into target planning and dialogueguidance, establishing evaluation metrics across various domains. Moreover, italso enables the automatic generation of diverse and challenging evaluationdata. Based on the proposed framework, we develop 328 evaluation environmentsspanning 6 distinct domains. Through experiments with 22 different types ofLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptionalperformance on target planning and dialogue guidance tasks, respectively.Finally, we investigate how reasoning capabilities influence proactivebehaviors and discuss their implications for future model development.</description><author>Tianjian Liu, Fanqi Wan, Jiajian Guo, Xiaojun Quan</author><pubDate>Thu, 28 Aug 2025 16:26:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20973v1</guid></item><item><title>DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes</title><link>http://arxiv.org/abs/2508.20965v1</link><description>We present DrivingGaussian++, an efficient and effective framework forrealistic reconstructing and controllable editing of surrounding dynamicautonomous driving scenes. DrivingGaussian++ models the static background usingincremental 3D Gaussians and reconstructs moving objects with a compositedynamic Gaussian graph, ensuring accurate positions and occlusions. Byintegrating a LiDAR prior, it achieves detailed and consistent scenereconstruction, outperforming existing methods in dynamic scene reconstructionand photorealistic surround-view synthesis. DrivingGaussian++ supportstraining-free controllable editing for dynamic driving scenes, includingtexture modification, weather simulation, and object manipulation, leveragingmulti-view images and depth priors. By integrating large language models (LLMs)and controllable editing, our method can automatically generate dynamic objectmotion trajectories and enhance their realism during the optimization process.DrivingGaussian++ demonstrates consistent and realistic editing results andgenerates dynamic multi-view driving scenarios, while significantly enhancingscene diversity. More results and code can be found at the project site:https://xiong-creator.github.io/DrivingGaussian_plus.github.io</description><author>Yajiao Xiong, Xiaoyu Zhou, Yongtao Wan, Deqing Sun, Ming-Hsuan Yang</author><pubDate>Thu, 28 Aug 2025 16:22:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20965v1</guid></item><item><title>SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline Dataset</title><link>http://arxiv.org/abs/2504.07612v2</link><description>The primary goal of a news headline is to summarize an event in as few wordsas possible. Depending on the media outlet, a headline can serve as a means toobjectively deliver a summary or improve its visibility. For the latter,specific publications may employ stylistic approaches that incorporate the useof sarcasm, irony, and exaggeration, key elements of a satirical approach. Assuch, even the headline must reflect the tone of the satirical main content.Current approaches for the Romanian language tend to detect thenon-conventional tone (i.e., satire and clickbait) of the news content bycombining both the main article and the headline. Because we consider aheadline to be merely a brief summary of the main article, we investigate inthis paper the presence of satirical tone in headlines alone, testing multiplebaselines ranging from standard machine learning algorithms to deep learningmodels. Our experiments show that Bidirectional Transformer models outperformboth standard machine-learning approaches and Large Language Models (LLMs),particularly when the meta-learning Reptile approach is employed.</description><author>Mihnea-Alexandru Vîrlan, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel</author><pubDate>Thu, 28 Aug 2025 16:22:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.07612v2</guid></item><item><title>A multimodal dataset for understanding the impact of mobile phones on remote online virtual education</title><link>http://arxiv.org/abs/2412.14195v3</link><description>This work presents the IMPROVE dataset, a multimodal resource designed toevaluate the effects of mobile phone usage on learners during online education.It includes behavioral, biometric, physiological, and academic performance datacollected from 120 learners divided into three groups with different levels ofphone interaction, enabling the analysis of the impact of mobile phone usageand related phenomena such as nomophobia. A setup involving 16 synchronizedsensors-including EEG, eye tracking, video cameras, smartwatches, and keystrokedynamics-was used to monitor learner activity during 30-minute sessionsinvolving educational videos, document reading, and multiple-choice tests.Mobile phone usage events, including both controlled interventions anduncontrolled interactions, were labeled by supervisors and refined through asemi-supervised re-labeling process. Technical validation confirmed signalquality, and statistical analyses revealed biometric changes associated withphone usage. The dataset is publicly available for research through GitHub andScience Data Bank, with synchronized recordings from three platforms (edBB,edX, and LOGGE), provided in standard formats (.csv, .mp4, .wav, and .tsv), andaccompanied by a detailed guide.</description><author>Roberto Daza, Alvaro Becerra, Ruth Cobos, Julian Fierrez, Aythami Morales</author><pubDate>Thu, 28 Aug 2025 16:18:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14195v3</guid></item><item><title>E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections</title><link>http://arxiv.org/abs/2508.20955v1</link><description>Many high-performance networks were not designed with lightweight applicationscenarios in mind from the outset, which has greatly restricted their scope ofapplication. This paper takes ConvNeXt as the research object and significantlyreduces the parameter scale and network complexity of ConvNeXt by integratingthe Cross Stage Partial Connections mechanism and a series of optimizeddesigns. The new network is named E-ConvNeXt, which can maintain high accuracyperformance under different complexity configurations. The three coreinnovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network(CSPNet) with ConvNeXt and adjusting the network structure, which reduces themodel's network complexity by up to 80%; (2) Optimizing the Stem and Blockstructures to enhance the model's feature expression capability and operationalefficiency; (3) Replacing Layer Scale with channel attention. Experimentalvalidation on ImageNet classification demonstrates E-ConvNeXt's superioraccuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transferlearning tests on object detection tasks further confirm its generalizationcapability.</description><author>Fang Wang, Huitao Li, Wenhan Chao, Zheng Zhuo, Yiran Ji, Chang Peng, Yupeng Sun</author><pubDate>Thu, 28 Aug 2025 16:17:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20955v1</guid></item><item><title>Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement</title><link>http://arxiv.org/abs/2508.20954v1</link><description>In the context of proven climate change, maintaining olive biodiversitythrough early anomaly detection and treatment using remote sensing technologyis crucial, offering effective management solutions. This paper presents aninnovative approach to olive tree segmentation from satellite images. Byleveraging foundational models and advanced segmentation techniques, the studyintegrates the Segment Anything Model (SAM) to accurately identify and segmentolive trees in agricultural plots. The methodology includes SAM segmentationand corrections based on trees alignement in the field and a learanbleconstraint about the shape and the size. Our approach achieved a 98\% accuracyrate, significantly surpassing the initial SAM performance of 82\%.</description><author>Amir Jmal, Chaima Chtourou, Mahdi Louati, Abdelaziz Kallel, Houda Khmila</author><pubDate>Thu, 28 Aug 2025 16:16:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20954v1</guid></item><item><title>A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling</title><link>http://arxiv.org/abs/2508.20953v1</link><description>Workforce scheduling in the healthcare sector is a significant operationalchallenge, characterized by fluctuating patient loads, diverse clinical skills,and the critical need to control labor costs while upholding high standards ofpatient care. This problem is inherently multi-objective, demanding a delicatebalance between competing goals: minimizing payroll, ensuring adequate staffingfor patient needs, and accommodating staff preferences to mitigate burnout. Wepropose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospitalunit workforce scheduling problem as a multi-objective optimization task. Ourmodel incorporates real-world complexities, including hourly appointment-drivendemand and the use of modular shifts for a multi-skilled workforce. By definingobjective functions for cost, patient care coverage, and staff satisfaction,the GA navigates the vast search space to identify a set of high-quality,non-dominated solutions. Demonstrated on datasets representing a typicalhospital unit, the results show that our MOO-GA generates robust and balancedschedules. On average, the schedules produced by our algorithm showed a 66\%performance improvement over a baseline that simulates a conventional, manualscheduling process. This approach effectively manages trade-offs betweencritical operational and staff-centric objectives, providing a practicaldecision support tool for nurse managers and hospital administrators.</description><author>Vipul Patel, Anirudh Deodhar, Dagnachew Birru</author><pubDate>Thu, 28 Aug 2025 16:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20953v1</guid></item><item><title>Transformers Meet In-Context Learning: A Universal Approximation Theory</title><link>http://arxiv.org/abs/2506.05200v2</link><description>Large language models are capable of in-context learning, the ability toperform new tasks at test time using a handful of input-output examples,without parameter updates. We develop a universal approximation theory toelucidate how transformers enable in-context learning. For a general class offunctions (each representing a distinct task), we demonstrate how to constructa transformer that, without any further weight updates, can predict based on afew noisy in-context examples with vanishingly small risk. Unlike prior workthat frames transformers as approximators of optimization algorithms (e.g.,gradient descent) for statistical learning tasks, we integrate Barron'suniversal function approximation theory with the algorithm approximatorviewpoint. Our approach yields approximation guarantees that are notconstrained by the effectiveness of the optimization algorithms being mimicked,extending far beyond convex problems like linear regression. The key is to showthat (i) any target function can be nearly linearly represented, with small$\ell_1$-norm, over a set of universal features, and (ii) a transformer can beconstructed to find the linear representation -- akin to solving Lasso -- attest time.</description><author>Gen Li, Yuchen Jiao, Yu Huang, Yuting Wei, Yuxin Chen</author><pubDate>Thu, 28 Aug 2025 16:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05200v2</guid></item><item><title>Efficient Large-Scale Cross-Domain Sequential Recommendation with Dynamic State Representations</title><link>http://arxiv.org/abs/2508.20945v1</link><description>Recently, autoregressive recommendation models (ARMs), such as Meta's HSTUmodel, have emerged as a major breakthrough over traditional Deep LearningRecommendation Models (DLRMs), exhibiting the highly sought-after scaling lawbehaviour. However, when applied to multi-domain scenarios, the transformerarchitecture's attention maps become a computational bottleneck, as they attendto all items across every domain. To tackle this challenge, systems mustefficiently balance inter and intra-domain knowledge transfer. In this work, weintroduce a novel approach for scalable multi-domain recommendation systems byreplacing full inter-domain attention with two innovative mechanisms: 1)Transition-Aware Positional Embeddings (TAPE): We propose novel positionalembeddings that account for domain-transition specific information. This allowsattention to be focused solely on intra-domain items, effectively reducing theunnecessary computational cost associated with attending to irrelevant domains.2) Dynamic Domain State Representation (DDSR): We introduce a dynamic staterepresentation for each domain, which is stored and accessed during subsequenttoken predictions. This enables the efficient transfer of relevant domaininformation without relying on full attention maps. Our method offers ascalable solution to the challenges posed by large-scale, multi-domainrecommendation systems and demonstrates significant improvements in retrievaltasks by separately modelling and combining inter- and intra-domainrepresentations.</description><author>Manuel V. Loureiro, Steven Derby, Aleksei Medvedev, Alejandro Ariza-Casabona, Gonzalo Fiz Pontiveros, Tri Kurniawan Wijaya</author><pubDate>Thu, 28 Aug 2025 16:05:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20945v1</guid></item><item><title>STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment</title><link>http://arxiv.org/abs/2508.20944v1</link><description>In-Context Learning (ICL) has become a powerful paradigm that enables LLMs toperform a wide range of tasks without task-specific fine-tuning. However, theeffectiveness of ICL heavily depends on the quality of exemplar selection. Inparticular, for structured prediction tasks such as semantic parsing, existingICL selection strategies often overlook structural alignment, leading tosuboptimal performance and poor generalization. To address this issue, wepropose a novel two-stage exemplar selection strategy that achieves a strongbalance between efficiency, generalizability, and performance. First, wefine-tune a BERT-based retriever using structure-aware supervision, guiding itto select exemplars that are both semantically relevant and structurallyaligned. Then, we enhance the retriever with a plug-in module, which amplifiessyntactically meaningful information in the hidden representations. Thisplug-in is model-agnostic, requires minimal overhead, and can be seamlesslyintegrated into existing pipelines. Experiments on four benchmarks spanningthree semantic parsing tasks demonstrate that our method consistentlyoutperforms existing baselines with multiple recent LLMs as inference-timemodels.</description><author>Jiaqian Li, Qisheng Hu, Jing Li, Wenya Wang</author><pubDate>Thu, 28 Aug 2025 16:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20944v1</guid></item><item><title>Transfer Learning for Classification under Decision Rule Drift with Application to Optimal Individualized Treatment Rule Estimation</title><link>http://arxiv.org/abs/2508.20942v1</link><description>In this paper, we extend the transfer learning classification framework fromregression function-based methods to decision rules. We propose a novelmethodology for modeling posterior drift through Bayes decision rules. Byexploiting the geometric transformation of the Bayes decision boundary, ourmethod reformulates the problem as a low-dimensional empirical riskminimization problem. Under mild regularity conditions, we establish theconsistency of our estimators and derive the risk bounds. Moreover, weillustrate the broad applicability of our method by adapting it to theestimation of optimal individualized treatment rules. Extensive simulationstudies and analyses of real-world data further demonstrate both superiorperformance and robustness of our approach.</description><author>Xiaohan Wang, Yang Ning</author><pubDate>Thu, 28 Aug 2025 16:03:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20942v1</guid></item><item><title>High-Dimensional Gaussian Process Regression with Soft Kernel Interpolation</title><link>http://arxiv.org/abs/2410.21419v3</link><description>We introduce Soft Kernel Interpolation (SoftKI), a method that combinesaspects of Structured Kernel Interpolation (SKI) and variational inducing pointmethods, to achieve scalable Gaussian Process (GP) regression onhigh-dimensional datasets. SoftKI approximates a kernel via softmaxinterpolation from a smaller number of interpolation points learned byoptimizing a combination of the SoftKI marginal log-likelihood (MLL), and whenneeded, an approximate MLL for improved numerical stability. Consequently, itcan overcome the dimensionality scaling challenges that SKI faces wheninterpolating from a dense and static lattice while retaining the flexibilityof variational methods to adapt inducing points to the dataset. We demonstratethe effectiveness of SoftKI across various examples and show that it iscompetitive with other approximated GP methods when the data dimensionality ismodest (around 10).</description><author>Chris Camaño, Daniel Huang</author><pubDate>Thu, 28 Aug 2025 15:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21419v3</guid></item><item><title>How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench</title><link>http://arxiv.org/abs/2508.20931v1</link><description>Recent advances in reasoning and planning capabilities of large languagemodels (LLMs) have enabled their potential as autonomous agents capable of tooluse in dynamic environments. However, in multi-turn conversational environmentslike $\tau$-bench, these agents often struggle with consistent reasoning,adherence to domain-specific policies, and extracting correct information overa long horizon of tool-calls and conversation. To capture and mitigate thesefailures, we conduct a comprehensive manual analysis of the common errorsoccurring in the conversation trajectories. We then experiment withreformulations of inputs to the tool-calling agent for improvement in agentdecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)framework, which automatically reformulates user queries augmented withrelevant domain rules and tool suggestions for the tool-calling agent to focuson. The results show that IRMA significantly outperforms ReAct, FunctionCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, inoverall pass^5 scores. These findings highlight the superior reliability andconsistency of IRMA compared to other methods in dynamic environments.</description><author>Venkatesh Mishra, Amir Saeidi, Satyam Raj, Mutsumi Nakamura, Jayanth Srinivasa, Gaowen Liu, Ali Payani, Chitta Baral</author><pubDate>Thu, 28 Aug 2025 15:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20931v1</guid></item><item><title>Finite-Time Guarantees for Multi-Agent Combinatorial Bandits with Nonstationary Rewards</title><link>http://arxiv.org/abs/2508.20923v1</link><description>We study a sequential resource allocation problem where a decision makerselects subsets of agents at each period to maximize overall outcomes withoutprior knowledge of individual-level effects. Our framework applies to settingssuch as community health interventions, targeted digital advertising, andworkforce retention programs, where intervention effects evolve dynamically.Agents may exhibit habituation (diminished response from frequent selection) orrecovery (enhanced response from infrequent selection). The technical challengecenters on nonstationary reward distributions that lead to changingintervention effects over time. The problem requires balancing two keycompeting objectives: heterogeneous individual rewards and theexploration-exploitation tradeoff in terms of learning for improved futuredecisions as opposed to maximizing immediate outcomes. Our contributionintroduces the first framework incorporating this form of nonstationary rewardsin the combinatorial multi-armed bandit literature. We develop algorithms withtheoretical guarantees on dynamic regret and demonstrate practical efficacythrough a diabetes intervention case study. Our personalized communityintervention algorithm achieved up to three times as much improvement inprogram enrollment compared to baseline approaches, validating the framework'spotential for real-world applications. This work bridges theoretical advancesin adaptive learning with practical challenges in population-level behavioralchange interventions.</description><author>Katherine B. Adams, Justin J. Boutilier, Qinyang He, Yonatan Mintz</author><pubDate>Thu, 28 Aug 2025 15:51:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20923v1</guid></item><item><title>Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics</title><link>http://arxiv.org/abs/2508.11017v2</link><description>Large language models (LLMs) struggle with cross-lingual knowledge transfer:they hallucinate when asked in one language about facts expressed in adifferent language during training. This work introduces a controlled settingto study the causes and dynamics of this phenomenon by training smallTransformer models from scratch on synthetic multilingual datasets. We identifya learning phase wherein a model develops either separate or unifiedrepresentations of the same facts across languages, and show that unificationis essential for cross-lingual transfer. We also show that the degree ofunification depends on mutual information between facts and training datalanguage, and on how easy it is to extract that language. Based on theseinsights, we develop methods to modulate the level of cross-lingual transfer bymanipulating data distribution and tokenization, and we introduce metrics andvisualizations to formally characterize their effects on unification. Our workshows how controlled settings can shed light on pre-training dynamics andsuggests new directions for improving cross-lingual transfer in LLMs.</description><author>Carter Blum, Katja Filippova, Ann Yuan, Asma Ghandeharioun, Julian Zimmert, Fred Zhang, Jessica Hoffmann, Tal Linzen, Martin Wattenberg, Lucas Dixon, Mor Geva</author><pubDate>Thu, 28 Aug 2025 15:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11017v2</guid></item><item><title>COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans</title><link>http://arxiv.org/abs/2508.20920v1</link><description>In the era of Industry 5.0, monitoring human activity is essential forensuring both ergonomic safety and overall well-being. While multi-cameracentralized setups improve pose estimation accuracy, they often suffer fromhigh computational costs and bandwidth requirements, limiting scalability andreal-time applicability. Distributing processing across edge devices can reducenetwork bandwidth and computational load. On the other hand, the constrainedresources of edge devices lead to accuracy degradation, and the distribution ofcomputation leads to temporal and spatial inconsistencies. We address thischallenge by proposing COMETH (Convex Optimization for Multiview Estimation andTracking of Humans), a lightweight algorithm for real-time multi-view humanpose fusion that relies on three concepts: it integrates kinematic andbiomechanical constraints to increase the joint positioning accuracy; itemploys convex optimization-based inverse kinematics for spatial fusion; and itimplements a state observer to improve temporal consistency. We evaluate COMETHon both public and industrial datasets, where it outperforms state-of-the-artmethods in localization, detection, and tracking accuracy. The proposed fusionpipeline enables accurate and scalable human motion tracking, making itwell-suited for industrial and safety-critical applications. The code ispublicly available at https://github.com/PARCO-LAB/COMETH.</description><author>Enrico Martini, Ho Jin Choi, Nadia Figueroa, Nicola Bombieri</author><pubDate>Thu, 28 Aug 2025 15:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20920v1</guid></item><item><title>Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement</title><link>http://arxiv.org/abs/2508.20919v1</link><description>Mitotic figures (MFs) are relevant biomarkers in tumor grading.Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,as manual annotation is time-consuming and subjective. In this work an ensembleof ConvNeXtBase models was trained with AUCMEDI and extend with a rule-basedrefinement (RBR) module. On the MIDOG25 preliminary test set, the ensembleachieved a balanced accuracy of 84.02%. While the RBR increased specificity, itreduced sensitivity and overall performance. The results show that deepensembles perform well for AMF classification. RBR can increase specificmetrics but requires further research.</description><author>Sara Krauss, Ellena Spieß, Daniel Hieber, Frank Kramer, Johannes Schobel, Dominik Müller</author><pubDate>Thu, 28 Aug 2025 15:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20919v1</guid></item><item><title>SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement</title><link>http://arxiv.org/abs/2508.20916v1</link><description>Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational tonatural human-computer interaction, enabling end-to-end spoken dialoguesystems. However, evaluating these models remains a fundamental challenge. Wepropose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speechLLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approachesthat disregard acoustic features, SageLM jointly assesses both semantic andacoustic dimensions. Second, it leverages rationale-based supervision toenhance explainability and guide model learning, achieving superior alignmentwith evaluation outcomes compared to rule-based reinforcement learning methods.Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,and employ a two-stage training paradigm to mitigate the scarcity of speechpreference data. Trained on both semantic and acoustic dimensions, SageLMachieves an 82.79\% agreement rate with human evaluators, outperformingcascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.</description><author>Yuan Ge, Junxiang Zhang, Xiaoqian Liu, Bei Li, Xiangnan Ma, Chenglong Wang, Kaiyang Ye, Yangfan Du, Linfeng Zhang, Yuxin Huang, Tong Xiao, Zhengtao Yu, JingBo Zhu</author><pubDate>Thu, 28 Aug 2025 15:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20916v1</guid></item><item><title>An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation</title><link>http://arxiv.org/abs/2508.11803v3</link><description>This study investigates whether second-order geometric cues - planarcurvature magnitude, curvature sign, and gradient orientation - are sufficienton their own to drive a multilayer perceptron (MLP) classifier for handwrittencharacter recognition (HCR), offering an alternative to convolutional neuralnetworks (CNNs). Using these three handcrafted feature maps as inputs, ourcurvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89percent on EMNIST letters. These results underscore the discriminative power ofcurvature-based representations for handwritten character images anddemonstrate that the advantages of deep learning can be realized even withinterpretable, hand-engineered features.</description><author>Azam Nouri</author><pubDate>Thu, 28 Aug 2025 15:47:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11803v3</guid></item><item><title>A Sobel-Gradient MLP Baseline for Handwritten Character Recognition</title><link>http://arxiv.org/abs/2508.11902v3</link><description>We revisit the classical Sobel operator to ask a simple question: Arefirst-order edge maps sufficient to drive an all-dense multilayer perceptron(MLP) for handwritten character recognition (HCR), as an alternative toconvolutional neural networks (CNNs)? Using only horizontal and vertical Sobelderivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite itsextreme simplicity, the resulting network reaches 98% accuracy on MNIST digitsand 92% on EMNIST letters -- approaching CNNs while offering a smaller memoryfootprint and transparent features. Our findings highlight that much of theclass-discriminative information in handwritten character images is alreadycaptured by first-order gradients, making edge-aware MLPs a compelling optionfor HCR.</description><author>Azam Nouri</author><pubDate>Thu, 28 Aug 2025 15:44:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11902v3</guid></item><item><title>Learning Robust Spatial Representations from Binaural Audio through Feature Distillation</title><link>http://arxiv.org/abs/2508.20914v1</link><description>Recently, deep representation learning has shown strong performance inmultiple audio tasks. However, its use for learning spatial representationsfrom multichannel audio is underexplored. We investigate the use of apretraining stage based on feature distillation to learn a robust spatialrepresentation of binaural speech without the need for data labels. In thisframework, spatial features are computed from clean binaural speech samples toform prediction labels. These clean features are then predicted fromcorresponding augmented speech using a neural network. After pretraining, wethrow away the spatial feature predictor and use the learned encoder weights toinitialize a DoA estimation model which we fine-tune for DoA estimation. Ourexperiments demonstrate that the pretrained models show improved performance innoisy and reverberant environments after fine-tuning for direction-of-arrivalestimation, when compared to fully supervised models and classic signalprocessing methods.</description><author>Holger Severin Bovbjerg, Jan Østergaard, Jesper Jensen, Shinji Watanabe, Zheng-Hua Tan</author><pubDate>Thu, 28 Aug 2025 15:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20914v1</guid></item><item><title>Research Challenges in Relational Database Management Systems for LLM Queries</title><link>http://arxiv.org/abs/2508.20912v1</link><description>Large language models (LLMs) have become essential for applications such astext summarization, sentiment analysis, and automated question-answering.Recently, LLMs have also been integrated into relational database managementsystems to enhance querying and support advanced data processing. Companiessuch as Amazon, Databricks, Google, and Snowflake offer LLM invocation directlywithin SQL, denoted as LLM queries, to boost data insights. However,open-source solutions currently have limited functionality and poorperformance. In this work, we present an early exploration of two open-sourcesystems and one enterprise platform, using five representative queries toexpose functional, performance, and scalability limits in today's SQL-invokedLLM integrations. We identify three main issues: enforcing structured outputs,optimizing resource utilization, and improving query planning. We implementedinitial solutions and observed improvements in accommodating LLM powered SQLqueries. These early gains demonstrate that tighter integration of LLM+DBMS isthe key to scalable and efficient processing of LLM queries.</description><author>Kerem Akillioglu, Anurag Chakraborty, Sairaj Voruganti, M. Tamer Özsu</author><pubDate>Thu, 28 Aug 2025 15:41:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20912v1</guid></item><item><title>Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation</title><link>http://arxiv.org/abs/2508.20909v1</link><description>Foundation models pre-trained on large-scale natural image datasets offer apowerful paradigm for medical image segmentation. However, effectivelytransferring their learned representations for precise clinical applicationsremains a challenge. In this work, we propose Dino U-Net, a novelencoder-decoder architecture designed to exploit the high-fidelity densefeatures of the DINOv3 vision foundation model. Our architecture introduces anencoder built upon a frozen DINOv3 backbone, which employs a specializedadapter to fuse the model's rich semantic features with low-level spatialdetails. To preserve the quality of these representations during dimensionalityreduction, we design a new fidelity-aware projection module (FAPM) thateffectively refines and projects the features for the decoder. We conductedextensive experiments on seven diverse public medical image segmentationdatasets. Our results show that Dino U-Net achieves state-of-the-artperformance, consistently outperforming previous methods across various imagingmodalities. Our framework proves to be highly scalable, with segmentationaccuracy consistently improving as the backbone model size increases up to the7-billion-parameter variant. The findings demonstrate that leveraging thesuperior, dense-pretrained features from a general-purpose foundation modelprovides a highly effective and parameter-efficient approach to advance theaccuracy of medical image segmentation. The code is available athttps://github.com/yifangao112/DinoUNet.</description><author>Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, Xin Gao</author><pubDate>Thu, 28 Aug 2025 15:38:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20909v1</guid></item><item><title>Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant</title><link>http://arxiv.org/abs/2508.20907v1</link><description>Qiskit is an open-source quantum computing framework that allows users todesign, simulate, and run quantum circuits on real quantum hardware. We explorepost-training techniques for LLMs to assist in writing Qiskit code. Weintroduce quantum verification as an effective method for ensuring code qualityand executability on quantum hardware. To support this, we developed asynthetic data pipeline that generates quantum problem-unit test pairs and usedit to create preference data for aligning LLMs with DPO. Additionally, wetrained models using GRPO, leveraging quantum-verifiable rewards provided bythe quantum hardware. Our best-performing model, combining DPO and GRPO,surpasses the strongest open-source baselines on the challengingQiskit-HumanEval-hard benchmark.</description><author>Nicolas Dupuis, Adarsh Tiwari, Youssef Mroueh, David Kremer, Ismael Faro, Juan Cruz-Benito</author><pubDate>Thu, 28 Aug 2025 15:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20907v1</guid></item><item><title>Inferring processes within dynamic forest models using hybrid modeling</title><link>http://arxiv.org/abs/2508.01228v2</link><description>Modeling forest dynamics under novel climatic conditions requires a carefulbalance between process-based understanding and empirical flexibility. DynamicVegetation Models (DVM) represent ecological processes mechanistically, buttheir performance is prone to misspecified assumptions about functional forms.Inferring the structure of these processes and their functional forms correctlyfrom data remains a major challenge because current approaches, such as plug-inestimators, have proven ineffective. We introduce Forest Informed NeuralNetworks (FINN), a hybrid modeling approach that combines a forest gap modelwith deep neural networks (DNN). FINN replaces processes with DNNs, which arethen calibrated alongside the other mechanistic components in one unified step.In a case study on the Barro Colorado Island 50-ha plot we demonstrate thatreplacing the growth process with a DNN improves predictive performance andsuccession trajectories compared to a mechanistic version of FINN. Furthermore,we discovered that the DNN learned an ecologically plausible, improvedfunctional form of the growth process, which we extracted from the DNN usingexplainable AI. In conclusion, our new hybrid modeling approach offers aversatile opportunity to infer forest dynamics from data and to improveforecasts of ecosystem trajectories under unprecedented environmental change.</description><author>Maximilian Pichler, Yannek Käber</author><pubDate>Thu, 28 Aug 2025 15:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.01228v2</guid></item><item><title>Turning Tabular Foundation Models into Graph Foundation Models</title><link>http://arxiv.org/abs/2508.20906v1</link><description>While foundation models have revolutionized such fields as natural languageprocessing and computer vision, their application and potential within graphmachine learning remain largely unexplored. One of the key challenges indesigning graph foundation models (GFMs) is handling diverse node features thatcan vary across different graph datasets. Although many works on GFMs have beenfocused exclusively on text-attributed graphs, the problem of handlingarbitrary features of other types in GFMs has not been fully addressed.However, this problem is not unique to the graph domain, as it also arises inthe field of machine learning for tabular data. In this work, motivated by therecent success of tabular foundation models like TabPFNv2, we propose G2T-FM, asimple graph foundation model that employs TabPFNv2 as a backbone.Specifically, G2T-FM augments the original node features with neighborhoodfeature aggregation, adds structural embeddings, and then applies TabPFNv2 tothe constructed node representations. Even in a fully in-context regime, ourmodel achieves strong results, significantly outperforming publicly availableGFMs and performing on par with well-tuned GNNs trained from scratch. Moreover,after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting thepotential of the proposed approach. More broadly, our paper reveals apreviously overlooked direction of utilizing tabular foundation models forgraph machine learning tasks.</description><author>Dmitry Eremeev, Gleb Bazhenov, Oleg Platonov, Artem Babenko, Liudmila Prokhorenkova</author><pubDate>Thu, 28 Aug 2025 15:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20906v1</guid></item><item><title>Improving Quantization with Post-Training Model Expansion</title><link>http://arxiv.org/abs/2503.17513v2</link><description>The size of a model has been a strong predictor of its quality, as well asits cost. As such, the trade-off between model cost and quality has beenwell-studied. Post-training optimizations like quantization and pruning havetypically focused on reducing the overall volume of pre-trained models toreduce inference costs while maintaining model quality. However, recentadvancements have introduced optimization techniques that, interestingly,expand models post-training, increasing model size to improve quality whenreducing volume. For instance, to enable 4-bit weight and activationquantization, incoherence processing often necessitates inserting onlineHadamard rotations in the compute graph, and preserving highly sensitiveweights often calls for additional higher precision computations. However, ifapplication requirements cannot be met, the prevailing solution is to relaxquantization constraints. In contrast, we demonstrate post-training modelexpansion is a viable strategy to improve model quality within a quantizationco-design space, and provide theoretical justification. We show it is possibleto progressively and selectively expand the size of a pre-trained largelanguage model (LLM) to improve model quality without end-to-end retraining. Inparticular, when quantizing the weights and activations to 4 bits for Llama31B, we reduce the gap to full-precision perplexity by an average of 9% relativeto both QuaRot and SpinQuant with only 5% more parameters, which is still a3.8% reduction in volume relative to a BF16 reference model.</description><author>Giuseppe Franco, Pablo Monteagudo-Lago, Ian Colbert, Nicholas Fraser, Michaela Blott</author><pubDate>Thu, 28 Aug 2025 15:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.17513v2</guid></item><item><title>CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems</title><link>http://arxiv.org/abs/2508.20898v1</link><description>Collaborative learning enhances the performance and adaptability ofmulti-robot systems in complex tasks but faces significant challenges due tohigh communication overhead and data heterogeneity inherent in multi-robottasks. To this end, we propose CoCoL, a Communication efficient decentralizedCollaborative Learning method tailored for multi-robot systems withheterogeneous local datasets. Leveraging a mirror descent framework, CoCoLachieves remarkable communication efficiency with approximate Newton-typeupdates by capturing the similarity between objective functions of robots, andreduces computational costs through inexact sub-problem solutions. Furthermore,the integration of a gradient tracking scheme ensures its robustness againstdata heterogeneity. Experimental results on three representative multi robotcollaborative learning tasks show the superiority of the proposed CoCoL insignificantly reducing both the number of communication rounds and totalbandwidth consumption while maintaining state-of-the-art accuracy. Thesebenefits are particularly evident in challenging scenarios involving non-IID(non-independent and identically distributed) data distribution, streamingdata, and time-varying network topologies.</description><author>Jiaxi Huang, Yan Huang, Yixian Zhao, Wenchao Meng, Jinming Xu</author><pubDate>Thu, 28 Aug 2025 15:25:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20898v1</guid></item><item><title>The Uneven Impact of Post-Training Quantization in Machine Translation</title><link>http://arxiv.org/abs/2508.20893v1</link><description>Quantization is essential for deploying large language models (LLMs) onresource-constrained hardware, but its implications for multilingual tasksremain underexplored. We conduct the first large-scale evaluation ofpost-training quantization (PTQ) on machine translation across 55 languagesusing five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals thatwhile 4-bit quantization often preserves translation quality for high-resourcelanguages and large models, significant degradation occurs for low-resource andtypologically diverse languages, particularly in 2-bit settings. We comparefour quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showingthat algorithm choice and model size jointly determine robustness. GGUFvariants provide the most consistent performance, even at 2-bit precision.Additionally, we quantify the interactions between quantization, decodinghyperparameters, and calibration languages, finding that language-matchedcalibration offers benefits primarily in low-bit scenarios. Our findings offeractionable insights for deploying multilingual LLMs for machine translationunder quantization constraints, especially in low-resource settings.</description><author>Benjamin Marie, Atsushi Fujita</author><pubDate>Thu, 28 Aug 2025 15:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20893v1</guid></item><item><title>To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software</title><link>http://arxiv.org/abs/2508.20892v1</link><description>Autonomous vehicle perception typically relies on modular pipelines thatdecompose the task into detection, tracking, and prediction. Whileinterpretable, these pipelines suffer from error accumulation and limitedinter-task synergy. Unified perception has emerged as a promising paradigm thatintegrates these sub-tasks within a shared architecture, potentially improvingrobustness, contextual reasoning, and efficiency while retaining interpretableoutputs. In this survey, we provide a comprehensive overview of unifiedperception, introducing a holistic and systemic taxonomy that categorizesmethods along task integration, tracking formulation, and representation flow.We define three paradigms -Early, Late, and Full Unified Perception- andsystematically review existing methods, their architectures, trainingstrategies, datasets used, and open-source availability, while highlightingfuture research directions. This work establishes the first comprehensiveframework for understanding and advancing unified perception, consolidatesfragmented efforts, and guides future research toward more robust,generalizable, and interpretable perception.</description><author>Loïc Stratil, Felix Fent, Esteban Rivera, Markus Lienkamp</author><pubDate>Thu, 28 Aug 2025 15:20:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20892v1</guid></item><item><title>A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task</title><link>http://arxiv.org/abs/2507.17232v2</link><description>Large Language Models (LLMs) are trained on a vast amount of proceduraltexts, but they do not directly observe real-world phenomena. In the context ofcooking recipes, this poses a challenge, as intermediate states of ingredientsare often omitted, making it difficult for models to track ingredient statesand understand recipes accurately. In this paper, we apply state probing, amethod for evaluating a language model's understanding of the world, to thedomain of cooking. We propose a new task and dataset for evaluating how wellLLMs can recognize intermediate ingredient states during cooking procedures. Wefirst construct a new Japanese recipe dataset with clear and accurateannotations of ingredient state changes, collected from well-structured andcontrolled recipe texts. Using this dataset, we design three novel tasks toevaluate whether LLMs can track ingredient state transitions and identifyingredients present at intermediate steps. Our experiments with widely usedLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient stateknowledge improves their understanding of cooking processes, achievingperformance comparable to commercial LLMs. The dataset are publicly availableat: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1</description><author>Mashiro Toyooka, Kiyoharu Aizawa, Yoko Yamakata</author><pubDate>Thu, 28 Aug 2025 15:15:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.17232v2</guid></item><item><title>Polynomial Chaos Expansion for Operator Learning</title><link>http://arxiv.org/abs/2508.20886v1</link><description>Operator learning (OL) has emerged as a powerful tool in scientific machinelearning (SciML) for approximating mappings between infinite-dimensionalfunctional spaces. One of its main applications is learning the solutionoperator of partial differential equations (PDEs). While much of the progressin this area has been driven by deep neural network-based approaches such asDeep Operator Networks (DeepONet) and Fourier Neural Operator (FNO), recentwork has begun to explore traditional machine learning methods for OL. In thiswork, we introduce polynomial chaos expansion (PCE) as an OL method. PCE hasbeen widely used for uncertainty quantification (UQ) and has recently gainedattention in the context of SciML. For OL, we establish a mathematicalframework that enables PCE to approximate operators in both purely data-drivenand physics-informed settings. The proposed framework reduces the task oflearning the operator to solving a system of equations for the PCEcoefficients. Moreover, the framework provides UQ by simply post-processing thePCE coefficients, without any additional computational cost. We apply theproposed method to a diverse set of PDE problems to demonstrate itscapabilities. Numerical results demonstrate the strong performance of theproposed method in both OL and UQ tasks, achieving excellent numerical accuracyand computational efficiency.</description><author>Himanshu Sharma, Lukáš Novák, Michael D. Shields</author><pubDate>Thu, 28 Aug 2025 15:14:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20886v1</guid></item><item><title>SMARTe-VR: Student Monitoring and Adaptive Response Technology for e-Learning in Virtual Reality</title><link>http://arxiv.org/abs/2501.10977v3</link><description>This work introduces SMARTe-VR, a platform for student monitoring in animmersive virtual reality environment designed for online education. SMARTe-VRaims to collect data for adaptive learning, focusing on facial biometrics andlearning metadata. The platform allows instructors to create customizedlearning sessions with video lectures, featuring an interface with an AutoQAsystem to evaluate understanding, interaction tools (for example, textbookhighlighting and lecture tagging), and real-time feedback. Furthermore, wereleased a dataset that contains 5 research challenges with data from 10 usersin VR-based TOEIC sessions. This data set, which spans more than 25 hours,includes facial features, learning metadata, 450 responses, difficulty levelsof the questions, concept tags, and understanding labels. Alongside thedatabase, we present preliminary experiments using Item Response Theory models,adapted for understanding detection using facial features. Two architectureswere explored: a Temporal Convolutional Network for local features and aMultilayer Perceptron for global features.</description><author>Roberto Daza, Lin Shengkai, Aythami Morales, Julian Fierrez, Katashi Nagao</author><pubDate>Thu, 28 Aug 2025 15:13:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.10977v3</guid></item><item><title>Understanding and evaluating computer vision models through the lens of counterfactuals</title><link>http://arxiv.org/abs/2508.20881v1</link><description>Counterfactual reasoning -- the practice of asking ``what if'' by varyinginputs and observing changes in model behavior -- has become central tointerpretable and fair AI. This thesis develops frameworks that usecounterfactuals to explain, audit, and mitigate bias in vision classifiers andgenerative models. By systematically altering semantically meaningfulattributes while holding others fixed, these methods uncover spuriouscorrelations, probe causal dependencies, and help build more robust systems. The first part addresses vision classifiers. CAVLI integrates attribution(LIME) with concept-level analysis (TCAV) to quantify how strongly decisionsrely on human-interpretable concepts. With localized heatmaps and a ConceptDependency Score, CAVLI shows when models depend on irrelevant cues likebackgrounds. Extending this, ASAC introduces adversarial counterfactuals thatperturb protected attributes while preserving semantics. Through curriculumlearning, ASAC fine-tunes biased models for improved fairness and accuracywhile avoiding stereotype-laden artifacts. The second part targets generative Text-to-Image (TTI) models. TIBET providesa scalable pipeline for evaluating prompt-sensitive biases by varyingidentity-related terms, enabling causal auditing of how race, gender, and ageaffect image generation. To capture interactions, BiasConnect builds causalgraphs diagnosing intersectional biases. Finally, InterMit offers a modular,training-free algorithm that mitigates intersectional bias via causalsensitivity scores and user-defined fairness goals. Together, these contributions show counterfactuals as a unifying lens forinterpretability, fairness, and causality in both discriminative and generativemodels, establishing principled, scalable methods for socially responsible biasevaluation and mitigation.</description><author>Pushkar Shukla</author><pubDate>Thu, 28 Aug 2025 15:11:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20881v1</guid></item><item><title>Explaining word embeddings with perfect fidelity: Case study in research impact prediction</title><link>http://arxiv.org/abs/2409.15912v3</link><description>The best-performing approaches for scholarly document quality prediction arebased on embedding models. In addition to their performance when used inclassifiers, embedding models can also provide predictions even for words thatwere not contained in the labelled training data for the classification model,which is important in the context of the ever-evolving research terminology.Although model-agnostic explanation methods, such as Local interpretablemodel-agnostic explanations, can be applied to explain machine learningclassifiers trained on embedding models, these produce results withquestionable correspondence to the model. We introduce a new feature importancemethod, Self-model Rated Entities (SMER), for logistic regression-basedclassification models trained on word embeddings. We show that SMER hastheoretically perfect fidelity with the explained model, as the average oflogits of SMER scores for individual words (SMER explanation) exactlycorresponds to the logit of the prediction of the explained model. Quantitativeand qualitative evaluation is performed through five diverse experimentsconducted on 50,000 research articles (papers) from the CORD-19 corpus. Throughan AOPC curve analysis, we experimentally demonstrate that SMER produces betterexplanations than LIME, SHAP and global tree surrogates.</description><author>Lucie Dvorackova, Marcin P. Joachimiak, Michal Cerny, Adriana Kubecova, Vilem Sklenak, Tomas Kliegr</author><pubDate>Thu, 28 Aug 2025 15:08:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15912v3</guid></item><item><title>Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis</title><link>http://arxiv.org/abs/2508.20877v1</link><description>Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal formsof cancer, with a five-year survival rate below 10% primarily due to latedetection. This research develops and validates a deep learning framework forearly PDAC detection through analysis of dual-modality imaging:autofluorescence and second harmonic generation (SHG). We analyzed 40 uniquepatient samples to create a specialized neural network capable ofdistinguishing between normal, fibrotic, and cancerous tissue. Our methodologyevaluated six distinct deep learning architectures, comparing traditionalConvolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).Through systematic experimentation, we identified and overcome significantchallenges in medical image analysis, including limited dataset size and classimbalance. The final optimized framework, based on a modified ResNetarchitecture with frozen pre-trained layers and class-weighted training,achieved over 90% accuracy in cancer detection. This represents a significantimprovement over current manual analysis methods an demonstrates potential forclinical deployment. This work establishes a robust pipeline for automated PDACdetection that can augment pathologists' capabilities while providing afoundation for future expansion to other cancer types. The developedmethodology also offers valuable insights for applying deep learning tolimited-size medical imaging datasets, a common challenge in clinicalapplications.</description><author>Dennis Slobodzian, Karissa Tilbury, Amir Kordijazi</author><pubDate>Thu, 28 Aug 2025 15:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20877v1</guid></item><item><title>LeMat-Traj: A Scalable and Unified Dataset of Materials Trajectories for Atomistic Modeling</title><link>http://arxiv.org/abs/2508.20875v1</link><description>The development of accurate machine learning interatomic potentials (MLIPs)is limited by the fragmented availability and inconsistent formatting ofquantum mechanical trajectory datasets derived from Density Functional Theory(DFT). These datasets are expensive to generate yet difficult to combine due tovariations in format, metadata, and accessibility. To address this, weintroduce LeMat-Traj, a curated dataset comprising over 120 million atomicconfigurations aggregated from large-scale repositories, including theMaterials Project, Alexandria, and OQMD. LeMat-Traj standardizes datarepresentation, harmonizes results and filters for high-quality configurationsacross widely used DFT functionals (PBE, PBESol, SCAN, r2SCAN). Itsignificantly lowers the barrier for training transferrable and accurate MLIPs.LeMat-Traj spans both relaxed low-energy states and high-energy, high-forcestructures, complementing molecular dynamics and active learning datasets. Byfine-tuning models pre-trained on high-force data with LeMat-Traj, we achieve asignificant reduction in force prediction errors on relaxation tasks. We alsopresent LeMaterial-Fetcher, a modular and extensible open-source librarydeveloped for this work, designed to provide a reproducible framework for thecommunity to easily incorporate new data sources and ensure the continuedevolution of large-scale materials datasets. LeMat-Traj and LeMaterial-Fetcherare publicly available at https://huggingface.co/datasets/LeMaterial/LeMat-Trajand https://github.com/LeMaterial/lematerial-fetcher.</description><author>Ali Ramlaoui, Martin Siron, Inel Djafar, Joseph Musielewicz, Amandine Rossello, Victor Schmidt, Alexandre Duval</author><pubDate>Thu, 28 Aug 2025 15:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20875v1</guid></item><item><title>Automatic Inspection Based on Switch Sounds of Electric Point Machines</title><link>http://arxiv.org/abs/2508.20870v1</link><description>Since 2018, East Japan Railway Company and Hitachi, Ltd. have been working toreplace human inspections with IoT-based monitoring. The purpose isLabor-saving required for equipment inspections and provide appropriatepreventive maintenance. As an alternative to visual inspection, it has beendifficult to substitute electrical characteristic monitoring, and theintroduction of new high-performance sensors has been costly. In 2019, weimplemented cameras and microphones in an ``NS'' electric point machines toreduce downtime from equipment failures, allowing for remote monitoring oflock-piece conditions. This method for detecting turnout switching errors basedon sound information was proposed, and the expected test results were obtained.The proposed method will make it possible to detect equipment failures in realtime, thereby reducing the need for visual inspections. This paper presents theresults of our technical studies aimed at automating the inspection ofelectronic point machines using sound, specifically focusing on ``switchsound'' beginning in 2019.</description><author>Ayano Shibata, Toshiki Gunji, Mitsuaki Tsuda, Takashi Endo, Kota Dohi, Tomoya Nishida, Satoko Nomoto</author><pubDate>Thu, 28 Aug 2025 15:01:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20870v1</guid></item><item><title>OLMoASR: Open Models and Data for Training Robust Speech Recognition Models</title><link>http://arxiv.org/abs/2508.20869v1</link><description>Improvements in training data scale and quality have led to significantadvances, yet its influence in speech recognition remains underexplored. Inthis paper, we present a large-scale dataset, OLMoASR-Pool, and series ofmodels, OLMoASR, to study and develop robust zero-shot speech recognitionmodels. Beginning from OLMoASR-Pool, a collection of 3M hours of English audioand 17M transcripts, we design text heuristic filters to remove low-quality ormistranscribed data. Our curation pipeline produces a new dataset containing 1Mhours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We useOLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASRachieves comparable average performance to OpenAI's Whisper on short andlong-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a12.8\% and 11.0\% word error rate (WER) that is on par with Whisper's largestEnglish-only model Whisper-medium.en's 12.4\% and 10.5\% WER for short andlong-form recognition respectively (at equivalent parameter count).OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code willbe made publicly available to further research on robust speech processing.</description><author>Huong Ngo, Matt Deitke, Martijn Bartelds, Sarah Pratt, Josh Gardner, Matt Jordan, Ludwig Schmidt</author><pubDate>Thu, 28 Aug 2025 15:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20869v1</guid></item><item><title>MSRS: Evaluating Multi-Source Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2508.20867v1</link><description>Retrieval-augmented systems are typically evaluated in settings whereinformation required to answer the query can be found within a single source orthe answer is short-form or factoid-based. However, many real-worldapplications demand the ability to integrate and summarize informationscattered across multiple sources, where no single source is sufficient torespond to the user's question. In such settings, the retrieval component of aRAG pipeline must recognize a variety of relevance signals, and the generationcomponent must connect and synthesize information across multiple sources. Wepresent a scalable framework for constructing evaluation benchmarks thatchallenge RAG systems to integrate information across distinct sources andgenerate long-form responses. Using our framework, we build two new benchmarkson Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representingnarrative synthesis and summarization tasks, respectively, that requireretrieval from large collections. Our extensive experiments with various RAGpipelines -- including sparse and dense retrievers combined with frontier LLMs-- reveal that generation quality is highly dependent on retrievaleffectiveness, which varies greatly by task. While multi-source synthesisproves challenging even in an oracle retrieval setting, we find that reasoningmodels significantly outperform standard LLMs at this distinct step.</description><author>Rohan Phanse, Yijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, Arman Cohan</author><pubDate>Thu, 28 Aug 2025 14:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20867v1</guid></item><item><title>Superstate Quantum Mechanics</title><link>http://arxiv.org/abs/2502.00037v2</link><description>We introduce Superstate Quantum Mechanics (SQM) as a theory that considersstates in Hilbert space subject to multiple quadratic constraints. Traditionalquantum mechanics corresponds to a single quadratic constraint of wavefunctionnormalization. In its simplest form, SQM considers states in the form ofunitary operators, where the quadratic constraints are conditions of unitarity.In this case, the stationary SQM problem is a quantum inverse problem withmultiple applications in physics, machine learning, and artificialintelligence. The SQM stationary problem is equivalent to a new algebraicproblem that we address in this paper. The SQM non-stationary problem considersthe evolution of a quantum system itself, distinct from the explicit timedependence of the Hamiltonian, $H(t)$. Two options for the SQM dynamic equationare considered: (1) within the framework of linear maps from higher-orderquantum theory, where 2D-type quantum circuits are introduced to transform onequantum system into another; and (2) in the form of a Gross-Pitaevskii-typenonlinear map. Although no known physical process currently describes such 2Ddynamics, this approach naturally bridges direct and inverse quantum mechanicsproblems, allowing for the development of a new type of computer algorithms.Beyond computer modeling, the developed theory could be directly applied if orwhen a physical process capable of solving a quantum inverse problem in asingle measurement act (analogous to how an eigenvalue arises from ameasurement in traditional quantum mechanics) is discovered in the future.</description><author>Mikhail Gennadievich Belov, Victor Victorovich Dubov, Vadim Konstantinovich Ivanov, Alexander Yurievich Maslov, Olga Vladimirovna Proshina, Vladislav Gennadievich Malyshkin</author><pubDate>Thu, 28 Aug 2025 14:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.00037v2</guid></item><item><title>AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning</title><link>http://arxiv.org/abs/2508.20866v1</link><description>The increasing complexity of software systems and the sophistication ofcyber-attacks have underscored the critical need for effective automatedvulnerability detection and repair systems. Traditional methods, such as staticprogram analysis, face significant challenges related to scalability,adaptability, and high false-positive and false-negative rates. AI-drivenapproaches, particularly those using machine learning and deep learning models,show promise but are heavily reliant on the quality and quantity of trainingdata. This paper introduces a novel framework designed to automaticallyintroduce realistic, category-specific vulnerabilities into secure C/C++codebases to generate datasets. The proposed approach coordinates multiple AIagents that simulate expert reasoning, along with function agents andtraditional code analysis tools. It leverages Retrieval-Augmented Generationfor contextual grounding and employs Low-Rank approximation of weights forefficient model fine-tuning. Our experimental study on 116 code samples fromthree different benchmarks suggests that our approach outperforms othertechniques with regard to dataset accuracy, achieving between 89\% and 95\%success rates in injecting vulnerabilities at function level.</description><author>Amine Lbath, Massih-Reza Amini, Aurelien Delaitre, Vadim Okun</author><pubDate>Thu, 28 Aug 2025 14:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20866v1</guid></item><item><title>Random Feature Representation Boosting</title><link>http://arxiv.org/abs/2501.18283v4</link><description>We introduce Random Feature Representation Boosting (RFRBoost), a novelmethod for constructing deep residual random feature neural networks (RFNNs)using boosting theory. RFRBoost uses random features at each layer to learn thefunctional gradient of the network representation, enhancing performance whilepreserving the convex optimization benefits of RFNNs. In the case of MSE loss,we obtain closed-form solutions to greedy layer-wise boosting with randomfeatures. For general loss functions, we show that fitting random featureresidual blocks reduces to solving a quadratically constrained least squaresproblem. Through extensive numerical experiments on tabular datasets for bothregression and classification, we show that RFRBoost significantly outperformsRFNNs and end-to-end trained MLP ResNets in the small- to medium-scale regimewhere RFNNs are typically applied. Moreover, RFRBoost offers substantialcomputational benefits, and theoretical guarantees stemming from boostingtheory.</description><author>Nikita Zozoulenko, Thomas Cass, Lukas Gonon</author><pubDate>Thu, 28 Aug 2025 14:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18283v4</guid></item><item><title>Practical Physical Layer Authentication for Mobile Scenarios Using a Synthetic Dataset Enhanced Deep Learning Approach</title><link>http://arxiv.org/abs/2508.20861v1</link><description>The Internet of Things (IoT) is ubiquitous thanks to the rapid development ofwireless technologies. However, the broadcast nature of wireless transmissionsresults in great vulnerability to device authentication. Physical layerauthentication emerges as a promising approach by exploiting the unique channelcharacteristics. However, a practical scheme applicable to dynamic channelvariations is still missing. In this paper, we proposed a deep learning-basedphysical layer channel state information (CSI) authentication for mobilescenarios and carried out comprehensive simulation and experimental evaluationusing IEEE 802.11n. Specifically, a synthetic training dataset was generatedbased on the WLAN TGn channel model and the autocorrelation and the distancecorrelation of the channel, which can significantly reduce the overhead ofmanually collecting experimental datasets. A convolutional neural network(CNN)-based Siamese network was exploited to learn the temporal and spatialcorrelation between the CSI pair and output a score to measure theirsimilarity. We adopted a synergistic methodology involving both simulation andexperimental evaluation. The experimental testbed consisted of WiFi IoTdevelopment kits and a few typical scenarios were specifically considered. Bothsimulation and experimental evaluation demonstrated excellent generalizationperformance of our proposed deep learning-based approach and excellentauthentication performance. Demonstrated by our practical measurement results,our proposed scheme improved the area under the curve (AUC) by 0.03 compared tothe fully connected network-based (FCN-based) Siamese model and by 0.06compared to the correlation-based benchmark algorithm.</description><author>Yijia Guo, Junqing Zhang, Y. -W. Peter Hong</author><pubDate>Thu, 28 Aug 2025 14:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20861v1</guid></item><item><title>Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models</title><link>http://arxiv.org/abs/2503.11519v3</link><description>Current Cross-Modality Generation Models (GMs) demonstrate remarkablecapabilities in various generative tasks. Given the ubiquity and informationrichness of vision modality inputs in real-world scenarios, Cross-Vision tasks,encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), haveattracted significant attention. Large Vision Language Models (LVLMs) and I2IGeneration Models (GMs) are employed to handle VLP and I2I tasks, respectively.Previous research indicates that printing typographic words into input imagessignificantly induces LVLMs and I2I GMs to produce disruptive outputs that aresemantically aligned with those words. Additionally, visual prompts, as a moresophisticated form of typography, are also revealed to pose security risks tovarious applications of cross-vision tasks. However, the specificcharacteristics of the threats posed by visual prompts remain underexplored. Inthis paper, to comprehensively investigate the performance impact induced byTypographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, wepropose the Typographic Visual Prompts Injection Dataset and thoroughlyevaluate the TVPI security risks on various open-source and closed-source LVLMsand I2I GMs under visual prompts with different target semantics, deepening theunderstanding of TVPI threats.</description><author>Hao Cheng, Erjia Xiao, Yichi Wang, Lingfeng Zhang, Qiang Zhang, Jiahang Cao, Kaidi Xu, Mengshu Sun, Xiaoshuai Hao, Jindong Gu, Renjing Xu</author><pubDate>Thu, 28 Aug 2025 14:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.11519v3</guid></item><item><title>PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis</title><link>http://arxiv.org/abs/2508.20851v1</link><description>Deep learning based automated pathological diagnosis has markedly improveddiagnostic efficiency and reduced variability between observers, yet itsclinical adoption remains limited by opaque model decisions and a lack oftraceable rationale. To address this, recent multimodal visual reasoningarchitectures provide a unified framework that generates segmentation masks atthe pixel level alongside semantically aligned textual explanations. Bylocalizing lesion regions and producing expert style diagnostic narratives,these models deliver the transparent and interpretable insights necessary fordependable AI assisted pathology. Building on these advancements, we proposePathMR, a cell-level Multimodal visual Reasoning framework for Pathologicalimage analysis. Given a pathological image and a textual query, PathMRgenerates expert-level diagnostic explanations while simultaneously predictingcell distribution patterns. To benchmark its performance, we evaluated ourapproach on the publicly available PathGen dataset as well as on our newlydeveloped GADVR dataset. Extensive experiments on these two datasetsdemonstrate that PathMR consistently outperforms state-of-the-art visualreasoning methods in text generation quality, segmentation accuracy, andcross-modal alignment. These results highlight the potential of PathMR forimproving interpretability in AI-driven pathological diagnosis. The code willbe publicly available in https://github.com/zhangye-zoe/PathMR.</description><author>Ye Zhang, Yu Zhou, Jingwen Qi, Yongbing Zhang, Simon Puettmann, Finn Wichmann, Larissa Pereira Ferreira, Lara Sichward, Julius Keyl, Sylvia Hartmann, Shuo Zhao, Hongxiao Wang, Xiaowei Xu, Jianxu Chen</author><pubDate>Thu, 28 Aug 2025 14:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20851v1</guid></item><item><title>Encoding Tactile Stimuli for Organoid Intelligence in Braille Recognition</title><link>http://arxiv.org/abs/2508.20850v1</link><description>This study proposes a generalizable encoding strategy that maps tactilesensor data to electrical stimulation patterns, enabling neural organoids toperform an open-loop artificial tactile Braille classification task. Humanforebrain organoids cultured on a low-density microelectrode array (MEA) aresystematically stimulated to characterize the relationship between electricalstimulation parameters (number of pulse, phase amplitude, phase duration, andtrigger delay) and organoid responses, measured as spike activity and spatialdisplacement of the center of activity. Implemented on event-based tactileinputs recorded from the Evetac sensor, our system achieved an average Brailleletter classification accuracy of 61 percent with a single organoid, whichincreased significantly to 83 percent when responses from a three-organoidensemble were combined. Additionally, the multi-organoid configurationdemonstrated enhanced robustness against various types of artificiallyintroduced noise. This research demonstrates the potential of organoids aslow-power, adaptive bio-hybrid computational elements and provides afoundational encoding framework for future scalable bio-hybrid computingarchitectures.</description><author>Tianyi Liu, Hemma Philamore, Benjamin Ward-Cherrier</author><pubDate>Thu, 28 Aug 2025 14:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20850v1</guid></item><item><title>JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring</title><link>http://arxiv.org/abs/2508.20848v1</link><description>Accurately determining whether a jailbreak attempt has succeeded is afundamental yet unresolved challenge. Existing evaluation methods rely onmisaligned proxy indicators or naive holistic judgments. They frequentlymisinterpret model responses, leading to inconsistent and subjectiveassessments that misalign with human perception. To address this gap, weintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universaljailbreak evaluation framework. Its key mechanism is to automatically decomposean input harmful question into a set of weighted sub-questions, score eachsub-answer, and weight-aggregate the sub-scores into a final decision. JADESalso incorporates an optional fact-checking module to strengthen the detectionof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, anewly introduced benchmark proposed in this work, consisting of 400 pairs ofjailbreak prompts and responses, each meticulously annotated by humans. In abinary setting (success/failure), JADES achieves 98.5% agreement with humanevaluators, outperforming strong baselines by over 9%. Re-evaluating fivepopular attacks on four LLMs reveals substantial overestimation (e.g., LAA'sattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results showthat JADES could deliver accurate, consistent, and interpretable evaluations,providing a reliable basis for measuring future jailbreak attacks.</description><author>Junjie Chu, Mingjie Li, Ziqing Yang, Ye Leng, Chenhao Lin, Chao Shen, Michael Backes, Yun Shen, Yang Zhang</author><pubDate>Thu, 28 Aug 2025 14:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20848v1</guid></item><item><title>Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving</title><link>http://arxiv.org/abs/2508.14926v2</link><description>Autonomous vehicles hold great promise for reducing traffic fatalities andimproving transportation efficiency, yet their widespread adoption hinges onembedding robust ethical reasoning into routine and emergency maneuvers,particularly to protect vulnerable road users (VRUs) such as pedestrians andcyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL)framework that explicitly integrates moral considerations with standard drivingobjectives. At the decision level, a Safe RL agent is trained using a compositeethical risk cost, combining collision probability and harm severity, togenerate high-level motion targets. A dynamic Prioritized Experience Replaymechanism amplifies learning from rare but critical, high-risk events. At theexecution level, polynomial path planning coupled withProportional-Integral-Derivative (PID) and Stanley controllers translates thesetargets into smooth, feasible trajectories, ensuring both accuracy and comfort.We train and validate our approach on rich, real-world traffic datasetsencompassing diverse vehicles, cyclists, and pedestrians, and demonstrate thatit outperforms baseline methods in reducing ethical risk and maintainingdriving performance. To our knowledge, this is the first study of ethicaldecision-making for autonomous vehicles via Safe RL evaluated on real-world,human-mixed traffic scenarios. Our results highlight the potential of combiningformal control theory and data-driven learning to advance ethically accountableautonomy that explicitly protects those most at risk in urban trafficenvironments.</description><author>Dianzhao Li, Ostap Okhrin</author><pubDate>Thu, 28 Aug 2025 14:35:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14926v2</guid></item></channel></rss>