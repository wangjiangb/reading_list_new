<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 21 Apr 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>On the Content Bias in Fréchet Video Distance</title><link>http://arxiv.org/abs/2404.12391v1</link><description>Fr\'echet Video Distance (FVD), a prominent metric for evaluating videogeneration models, is known to conflict with human perception occasionally. Inthis paper, we aim to explore the extent of FVD's bias toward per-frame qualityover temporal realism and identify its sources. We first quantify the FVD'ssensitivity to the temporal axis by decoupling the frame and motion quality andfind that the FVD increases only slightly with large temporal corruption. Wethen analyze the generated videos and show that via careful sampling from alarge set of generated videos that do not contain motions, one can drasticallydecrease FVD without improving the temporal quality. Both studies suggest FVD'sbias towards the quality of individual frames. We further observe that the biascan be attributed to the features extracted from a supervised video classifiertrained on the content-biased dataset. We show that FVD with features extractedfrom the recent large-scale self-supervised video models is less biased towardimage quality. Finally, we revisit a few real-world examples to validate ourhypothesis.</description><author>Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, Jia-Bin Huang</author><pubDate>Thu, 18 Apr 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12391v1</guid></item><item><title>NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields</title><link>http://arxiv.org/abs/2404.01300v2</link><description>Neural fields excel in computer vision and robotics due to their ability tounderstand the 3D visual world such as inferring semantics, geometry, anddynamics. Given the capabilities of neural fields in densely representing a 3Dscene from 2D images, we ask the question: Can we scale their self-supervisedpretraining, specifically using masked autoencoders, to generate effective 3Drepresentations from posed RGB images. Owing to the astounding success ofextending transformers to novel data modalities, we employ standard 3D VisionTransformers to suit the unique formulation of NeRFs. We leverage NeRF'svolumetric grid as a dense input to the transformer, contrasting it with other3D representations such as pointclouds where the information density can beuneven, and the representation is irregular. Due to the difficulty of applyingmasked autoencoders to an implicit representation, such as NeRF, we opt forextracting an explicit representation that canonicalizes scenes across domainsby employing the camera trajectory for sampling. Our goal is made possible bymasking random patches from NeRF's radiance and density grid and employing astandard 3D Swin Transformer to reconstruct the masked patches. In doing so,the model can learn the semantic and spatial structure of complete scenes. Wepretrain this representation at scale on our proposed curated posed-RGB data,totaling over 1.6 million images. Once pretrained, the encoder is used foreffective 3D transfer learning. Our novel self-supervised pretraining forNeRFs, NeRF-MAE, scales remarkably well and improves performance on variouschallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRFscene understanding baselines on Front3D and ScanNet datasets with an absoluteperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.</description><author>Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</author><pubDate>Thu, 18 Apr 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01300v2</guid></item><item><title>BLINK: Multimodal Large Language Models Can See but Not Perceive</title><link>http://arxiv.org/abs/2404.12390v1</link><description>We introduce Blink, a new benchmark for multimodal language models (LLMs)that focuses on core visual perception abilities not found in otherevaluations. Most of the Blink tasks can be solved by humans "within a blink"(e.g., relative depth estimation, visual correspondence, forensics detection,and multi-view reasoning). However, we find these perception-demanding taskscast significant challenges for current multimodal LLMs because they resistmediation through natural language. Blink reformats 14 classic computer visiontasks into 3,807 multiple-choice questions, paired with single or multipleimages and visual prompting. While humans get 95.70% accuracy on average, Blinkis surprisingly challenging for existing multimodal LLMs: even thebest-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only13.17% and 7.63% higher than random guessing, indicating that such perceptionabilities have not "emerged" yet in recent multimodal LLMs. Our analysis alsohighlights that specialist CV models could solve these problems much better,suggesting potential pathways for future improvements. We believe Blink willstimulate the community to help multimodal LLMs catch up with human-levelvisual perception.</description><author>Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna</author><pubDate>Thu, 18 Apr 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12390v1</guid></item><item><title>VideoGigaGAN: Towards Detail-rich Video Super-Resolution</title><link>http://arxiv.org/abs/2404.12388v1</link><description>Video super-resolution (VSR) approaches have shown impressive temporalconsistency in upsampled videos. However, these approaches tend to generateblurrier results than their image counterparts as they are limited in theirgenerative capability. This raises a fundamental question: can we extend thesuccess of a generative image upsampler to the VSR task while preserving thetemporal consistency? We introduce VideoGigaGAN, a new generative VSR modelthat can produce videos with high-frequency details and temporal consistency.VideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN. Simplyinflating GigaGAN to a video model by adding temporal modules produces severetemporal flickering. We identify several key issues and propose techniques thatsignificantly improve the temporal consistency of upsampled videos. Ourexperiments show that, unlike previous VSR methods, VideoGigaGAN generatestemporally consistent videos with more fine-grained appearance details. Wevalidate the effectiveness of VideoGigaGAN by comparing it withstate-of-the-art VSR models on public datasets and showcasing video resultswith $8\times$ super-resolution.</description><author>Yiran Xu, Taesung Park, Richard Zhang, Yang Zhou, Eli Shechtman, Feng Liu, Jia-Bin Huang, Difan Liu</author><pubDate>Thu, 18 Apr 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12388v1</guid></item><item><title>Moving Object Segmentation: All You Need Is SAM (and Flow)</title><link>http://arxiv.org/abs/2404.12389v1</link><description>The objective of this paper is motion segmentation -- discovering andsegmenting the moving objects in a video. This is a much studied area withnumerous careful,and sometimes complex, approaches and training schemesincluding: self-supervised learning, learning from synthetic datasets,object-centric representations, amodal representations, and many more. Ourinterest in this paper is to determine if the Segment Anything model (SAM) cancontribute to this task. We investigate two models for combining SAM withoptical flow that harness the segmentation power of SAM with the ability offlow to discover and group moving objects. In the first model, we adapt SAM totake optical flow, rather than RGB, as an input. In the second, SAM takes RGBas an input, and flow is used as a segmentation prompt. These surprisinglysimple methods, without any further modifications, outperform all previousapproaches by a considerable margin in both single and multi-object benchmarks.We also extend these frame-level segmentations to sequence-level segmentationsthat maintain object identity. Again, this simple model outperforms previousmethods on multiple video object segmentation benchmarks.</description><author>Junyu Xie, Charig Yang, Weidi Xie, Andrew Zisserman</author><pubDate>Thu, 18 Apr 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12389v1</guid></item><item><title>Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models</title><link>http://arxiv.org/abs/2404.12387v1</link><description>We introduce Reka Core, Flash, and Edge, a series of powerful multimodallanguage models trained from scratch by Reka. Reka models are able to processand reason with text, images, video, and audio inputs. This technical reportdiscusses details of training some of these models and provides comprehensiveevaluation results. We show that Reka Edge and Reka Flash are not onlystate-of-the-art but also outperform many much larger models, deliveringoutsized values for their respective compute class. Meanwhile, our most capableand largest model, Reka Core, approaches the best frontier models on bothautomatic evaluations and blind human evaluations. On image question answeringbenchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.Meanwhile, on multimodal chat, Core ranks as the second most preferred modelunder a blind third-party human evaluation setup, outperforming other modelssuch as Claude 3 Opus. On text benchmarks, Core not only performs competitivelyto other frontier models on a set of well-established benchmarks (e.g. MMLU,GSM8K) but also outperforms GPT4-0613 on human evaluation. On video questionanswering (Perception-Test), Core outperforms Gemini Ultra. Models are shippedin production at http://chat.reka.ai . A showcase of non cherry pickedqualitative examples can also be found at http://showcase.reka.ai .</description><author>Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, Zhihui Xie</author><pubDate>Thu, 18 Apr 2024 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12387v1</guid></item><item><title>SOHES: Self-supervised Open-world Hierarchical Entity Segmentation</title><link>http://arxiv.org/abs/2404.12386v1</link><description>Open-world entity segmentation, as an emerging computer vision task, aims atsegmenting entities in images without being restricted by pre-defined classes,offering impressive generalization capabilities on unseen images and concepts.Despite its promise, existing entity segmentation methods like Segment AnythingModel (SAM) rely heavily on costly expert annotators. This work presentsSelf-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novelapproach that eliminates the need for human annotations. SOHES operates inthree phases: self-exploration, self-instruction, and self-correction. Given apre-trained self-supervised representation, we produce abundant high-qualitypseudo-labels through visual feature clustering. Then, we train a segmentationmodel on the pseudo-labels, and rectify the noises in pseudo-labels via ateacher-student mutual-learning procedure. Beyond segmenting entities, SOHESalso captures their constituent parts, providing a hierarchical understandingof visual entities. Using raw images as the sole training data, our methodachieves unprecedented performance in self-supervised open-world segmentation,marking a significant milestone towards high-quality open-world entitysegmentation in the absence of human-annotated masks. Project page:https://SOHES.github.io.</description><author>Shengcao Cao, Jiuxiang Gu, Jason Kuen, Hao Tan, Ruiyi Zhang, Handong Zhao, Ani Nenkova, Liang-Yan Gui, Tong Sun, Yu-Xiong Wang</author><pubDate>Thu, 18 Apr 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12386v1</guid></item><item><title>MeshLRM: Large Reconstruction Model for High-Quality Mesh</title><link>http://arxiv.org/abs/2404.12385v1</link><description>We propose MeshLRM, a novel LRM-based approach that can reconstruct ahigh-quality mesh from merely four input images in less than one second.Different from previous large reconstruction models (LRMs) that focus onNeRF-based reconstruction, MeshLRM incorporates differentiable mesh extractionand rendering within the LRM framework. This allows for end-to-end meshreconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.Moreover, we improve the LRM architecture by simplifying several complexdesigns in previous LRMs. MeshLRM's NeRF initialization is sequentially trainedwith low- and high-resolution images; this new LRM training strategy enablessignificantly faster convergence and thereby leads to better quality with lesscompute. Our approach achieves state-of-the-art mesh reconstruction fromsparse-view inputs and also allows for many downstream applications, includingtext-to-3D and single-image-to-3D generation. Project page:https://sarahweiii.github.io/meshlrm/</description><author>Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, Zexiang Xu</author><pubDate>Thu, 18 Apr 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12385v1</guid></item><item><title>G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis</title><link>http://arxiv.org/abs/2404.12383v1</link><description>We propose G-HOP, a denoising diffusion based generative prior forhand-object interactions that allows modeling both the 3D object and a humanhand, conditioned on the object category. To learn a 3D spatial diffusion modelthat can capture this joint distribution, we represent the human hand via askeletal distance field to obtain a representation aligned with the (latent)signed distance field for the object. We show that this hand-object prior canthen serve as generic guidance to facilitate other tasks like reconstructionfrom interaction clip and human grasp synthesis. We believe that our model,trained by aggregating seven diverse real-world interaction datasets spanningacross 155 categories, represents a first approach that allows jointlygenerating both hand and object. Our empirical evaluations demonstrate thebenefit of this joint prior in video-based reconstruction and human graspsynthesis, outperforming current task-specific baselines. Project website: https://judyye.github.io/ghop-www</description><author>Yufei Ye, Abhinav Gupta, Kris Kitani, Shubham Tulsiani</author><pubDate>Thu, 18 Apr 2024 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12383v1</guid></item><item><title>Lazy Diffusion Transformer for Interactive Image Editing</title><link>http://arxiv.org/abs/2404.12382v1</link><description>We introduce a novel diffusion transformer, LazyDiffusion, that generatespartial image updates efficiently. Our approach targets interactive imageediting applications in which, starting from a blank canvas or an image, a userspecifies a sequence of localized image modifications using binary masks andtext prompts. Our generator operates in two phases. First, a context encoderprocesses the current canvas and user mask to produce a compact global contexttailored to the region to generate. Second, conditioned on this context, adiffusion-based transformer decoder synthesizes the masked pixels in a "lazy"fashion, i.e., it only generates the masked region. This contrasts withprevious works that either regenerate the full canvas, wasting time andcomputation, or confine processing to a tight rectangular crop around the mask,ignoring the global image context altogether. Our decoder's runtime scales withthe mask size, which is typically small, while our encoder introducesnegligible overhead. We demonstrate that our approach is competitive withstate-of-the-art inpainting methods in terms of quality and fidelity whileproviding a 10x speedup for typical user interactions, where the editing maskrepresents 10% of the image.</description><author>Yotam Nitzan, Zongze Wu, Richard Zhang, Eli Shechtman, Daniel Cohen-Or, Taesung Park, Michaël Gharbi</author><pubDate>Thu, 18 Apr 2024 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12382v1</guid></item><item><title>6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction</title><link>http://arxiv.org/abs/2404.12378v1</link><description>Current 3D reconstruction techniques struggle to infer unbounded scenes froma few images faithfully. Specifically, existing methods have high computationaldemands, require detailed pose information, and cannot reconstruct occludedregions reliably. We introduce 6Img-to-3D, an efficient, scalabletransformer-based encoder-renderer method for single-shot image to 3Dreconstruction. Our method outputs a 3D-consistent parameterized triplane fromonly six outward-facing input images for large-scale, unbounded outdoor drivingscenarios. We take a step towards resolving existing shortcomings by combiningcontracted custom cross- and self-attention mechanisms for triplaneparameterization, differentiable volume rendering, scene contraction, and imagefeature projection. We showcase that six surround-view vehicle images from asingle timestamp without global pose information are enough to reconstruct360$^{\circ}$ scenes during inference time, taking 395 ms. Our method allows,for example, rendering third-person images and birds-eye views. Our code isavailable at https://github.com/continental/6Img-to-3D, and more examples canbe found at our website here https://6Img-to-3D.GitHub.io/.</description><author>Théo Gieruc, Marius Kästingschäfer, Sebastian Bernhard, Mathieu Salzmann</author><pubDate>Thu, 18 Apr 2024 18:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12378v1</guid></item><item><title>Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular Videos</title><link>http://arxiv.org/abs/2404.12379v1</link><description>Modern 3D engines and graphics pipelines require mesh as a memory-efficientrepresentation, which allows efficient rendering, geometry processing, textureediting, and many other downstream operations. However, it is still highlydifficult to obtain high-quality mesh in terms of structure and detail frommonocular visual observations. The problem becomes even more challenging fordynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh(DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent meshgiven a single monocular video. Our work leverages the recent advancement in 3DGaussian Splatting to construct the mesh sequence with temporal consistencyfrom a video. Building on top of this representation, DG-Mesh recovershigh-quality meshes from the Gaussian points and can track the mesh verticesover time, which enables applications such as texture editing on dynamicobjects. We introduce the Gaussian-Mesh Anchoring, which encourages evenlydistributed Gaussians, resulting better mesh reconstruction through mesh-guideddensification and pruning on the deformed Gaussians. By applyingcycle-consistent deformation between the canonical and the deformed space, wecan project the anchored Gaussian back to the canonical space and optimizeGaussians across all time frames. During the evaluation on different datasets,DG-Mesh provides significantly better mesh reconstruction and rendering thanbaselines.</description><author>Isabella Liu, Hao Su, Xiaolong Wang</author><pubDate>Thu, 18 Apr 2024 18:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12379v1</guid></item><item><title>Matching the Statistical Query Lower Bound for k-sparse Parity Problems with Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2404.12376v1</link><description>The $k$-parity problem is a classical problem in computational complexity andalgorithmic theory, serving as a key benchmark for understanding computationalclasses. In this paper, we solve the $k$-parity problem with stochasticgradient descent (SGD) on two-layer fully-connected neural networks. Wedemonstrate that SGD can efficiently solve the $k$-sparse parity problem on a$d$-dimensional hypercube ($k\le O(\sqrt{d})$) with a sample complexity of$\tilde{O}(d^{k-1})$ using $2^{\Theta(k)}$ neurons, thus matching theestablished $\Omega(d^{k})$ lower bounds of Statistical Query (SQ) models. Ourtheoretical analysis begins by constructing a good neural network capable ofcorrectly solving the $k$-parity problem. We then demonstrate how a trainedneural network with SGD can effectively approximate this good network, solvingthe $k$-parity problem with small statistical errors. Our theoretical resultsand findings are supported by empirical evidence, showcasing the efficiency andefficacy of our approach.</description><author>Yiwen Kou, Zixiang Chen, Quanquan Gu, Sham M. Kakade</author><pubDate>Thu, 18 Apr 2024 18:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12376v1</guid></item><item><title>MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale</title><link>http://arxiv.org/abs/2404.12372v1</link><description>Medical Visual Question Answering (MedVQA), which offers language responsesto image-based medical inquiries, represents a challenging task and significantadvancement in healthcare. It assists medical experts to swiftly interpretmedical images, thereby enabling faster and more accurate diagnoses. However,the model interpretability and transparency of existing MedVQA solutions areoften limited, posing challenges in understanding their decision-makingprocesses. To address this issue, we devise a semi-automated annotation processto streamlining data preparation and build new benchmark MedVQA datasets R-RADand R-SLAKE. The R-RAD and R-SLAKE datasets provide intermediate medicaldecision-making rationales generated by multimodal large language models andhuman annotations for question-answering pairs in existing MedVQA datasets,i.e., VQA-RAD and SLAKE. Moreover, we design a novel framework which finetuneslightweight pretrained generative models by incorporating medicaldecision-making rationales into the training process. The framework includesthree distinct strategies to generate decision outcomes and correspondingrationales, thereby clearly showcasing the medical decision-making processduring reasoning. Extensive experiments demonstrate that our method can achievean accuracy of 83.5% on R-RAD and 86.3% on R-SLAKE, significantly outperformingexisting state-of-the-art baselines. Dataset and code will be released.</description><author>Xiaotang Gai, Chenyi Zhou, Jiaxiang Liu, Yang Feng, Jian Wu, Zuozhu Liu</author><pubDate>Thu, 18 Apr 2024 18:53:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12372v1</guid></item><item><title>KDk: A Defense Mechanism Against Label Inference Attacks in Vertical Federated Learning</title><link>http://arxiv.org/abs/2404.12369v1</link><description>Vertical Federated Learning (VFL) is a category of Federated Learning inwhich models are trained collaboratively among parties with verticallypartitioned data. Typically, in a VFL scenario, the labels of the samples arekept private from all the parties except for the aggregating server, that isthe label owner. Nevertheless, recent works discovered that by exploitinggradient information returned by the server to bottom models, with theknowledge of only a small set of auxiliary labels on a very limited subset oftraining data points, an adversary can infer the private labels. These attacksare known as label inference attacks in VFL. In our work, we propose a novelframework called KDk, that combines Knowledge Distillation and k-anonymity toprovide a defense mechanism against potential label inference attacks in a VFLscenario. Through an exhaustive experimental campaign we demonstrate that byapplying our approach, the performance of the analyzed label inference attacksdecreases consistently, even by more than 60%, maintaining the accuracy of thewhole VFL almost unaltered.</description><author>Marco Arazzi, Serena Nicolazzo, Antonino Nocera</author><pubDate>Thu, 18 Apr 2024 18:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12369v1</guid></item><item><title>Gradient-Regularized Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2404.12368v1</link><description>One of the challenges for neural networks in real-life applications is theoverconfident errors these models make when the data is not from the originaltraining distribution. Addressing this issue is known as Out-of-Distribution (OOD) detection. Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogatefor OOD data during training to achieve improved performance. However, these methods fail to fully exploit the local information embeddedin the auxiliary dataset. In this work, we propose the idea of leveraging the information embedded inthe gradient of the loss function during training to enable the network to notonly learn a desired OOD score for each sample but also to exhibit similarbehavior in a local neighborhood around each sample. We also develop a novel energy-based sampling method to allow the network tobe exposed to more informative OOD samples during the training phase. This isespecially important when the auxiliary dataset is large. We demonstrate theeffectiveness of our method through extensive experiments on several OODbenchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNetexperiment. We further provide a theoretical analysis through the lens of certifiedrobustness and Lipschitz analysis to showcase the theoretical foundation of ourwork. We will publicly release our code after the review process.</description><author>Sina Sharifi, Taha Entesari, Bardia Safaei, Vishal M. Patel, Mahyar Fazlyab</author><pubDate>Thu, 18 Apr 2024 18:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12368v1</guid></item><item><title>Information theory unifies atomistic machine learning, uncertainty quantification, and materials thermodynamics</title><link>http://arxiv.org/abs/2404.12367v1</link><description>An accurate description of information is relevant for a range of problems inatomistic modeling, such as sampling methods, detecting rare events, analyzingdatasets, or performing uncertainty quantification (UQ) in machine learning(ML)-driven simulations. Although individual methods have been proposed foreach of these tasks, they lack a common theoretical background integratingtheir solutions. Here, we introduce an information theoretical framework thatunifies predictions of phase transformations, kinetic events, datasetoptimality, and model-free UQ from atomistic simulations, thus bridgingmaterials modeling, ML, and statistical mechanics. We first demonstrate that,for a proposed representation, the information entropy of a distribution ofatom-centered environments is a surrogate value for thermodynamic entropy.Using molecular dynamics (MD) simulations, we show that information entropydifferences from trajectories can be used to build phase diagrams, identifyrare events, and recover classical theories of nucleation. Building on theseresults, we use this general concept of entropy to quantify information indatasets for ML interatomic potentials (IPs), informing compression, explainingtrends in testing errors, and evaluating the efficiency of active learningstrategies. Finally, we propose a model-free UQ method for MLIPs usinginformation entropy, showing it reliably detects extrapolation regimes, scalesto millions of atoms, and goes beyond model errors. This method is madeavailable as the package QUESTS: Quick Uncertainty and Entropy via STructuralSimilarity, providing a new unifying theory for data-driven atomistic modelingand combining efforts in ML, first-principles thermodynamics, and simulations.</description><author>Daniel Schwalbe-Koda, Sebastien Hamel, Babak Sadigh, Fei Zhou, Vincenzo Lordi</author><pubDate>Thu, 18 Apr 2024 18:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12367v1</guid></item><item><title>Accounting for AI and Users Shaping One Another: The Role of Mathematical Models</title><link>http://arxiv.org/abs/2404.12366v1</link><description>As AI systems enter into a growing number of societal domains, these systemsincreasingly shape and are shaped by user preferences, opinions, and behaviors.However, the design of AI systems rarely accounts for how AI and users shapeone another. In this position paper, we argue for the development of formalinteraction models which mathematically specify how AI and users shape oneanother. Formal interaction models can be leveraged to (1) specify interactionsfor implementation, (2) monitor interactions through empirical analysis, (3)anticipate societal impacts via counterfactual analysis, and (4) controlsocietal impacts via interventions. The design space of formal interactionmodels is vast, and model design requires careful consideration of factors suchas style, granularity, mathematical complexity, and measurability. Usingcontent recommender systems as a case study, we critically examine the nascentliterature of formal interaction models with respect to these use-cases anddesign axes. More broadly, we call for the community to leverage formalinteraction models when designing, evaluating, or auditing any AI system whichinteracts with users.</description><author>Sarah Dean, Evan Dong, Meena Jagadeesan, Liu Leqi</author><pubDate>Thu, 18 Apr 2024 18:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12366v1</guid></item><item><title>When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes</title><link>http://arxiv.org/abs/2404.12365v1</link><description>We present FastFit, a method, and a Python package design to provide fast andaccurate few-shot classification, especially for scenarios with manysemantically similar classes. FastFit utilizes a novel approach integratingbatch contrastive learning and token-level similarity score. Compared toexisting few-shot learning packages, such as SetFit, Transformers, or few-shotprompting of large language models via API calls, FastFit significantlyimproves multiclass classification performance in speed and accuracy acrossFewMany, our newly curated English benchmark, and Multilingual datasets.FastFit demonstrates a 3-20x improvement in training speed, completing trainingin just a few seconds. The FastFit package is now available on GitHub and PyPi,presenting a user-friendly solution for NLP practitioners.</description><author>Asaf Yehudai, Elron Bendel</author><pubDate>Thu, 18 Apr 2024 18:48:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12365v1</guid></item><item><title>Transformer tricks: Removing weights for skipless transformers</title><link>http://arxiv.org/abs/2404.12362v1</link><description>He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without theV and P (post-attention projection) linear layers, which reduces the totalnumber of weights. However, this scheme is only applicable to MHA (multi-headattention), but not for MQA (multi-query attention) and GQA (grouped-queryattention). The latter schemes are used by many popular LLMs such as Llama 2,Mistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposesmathematically equivalent versions that are suitable for MQA and GQA. Forexample, removing Q and P from a skipless version of Mistral-7B would remove15% of its weights (and thus reduce its compute and memory complexity). SeearXiv:2402.13388 and https://github.com/OpenMachine-ai/transformer-tricks forcode and more transformer tricks.</description><author>Nils Graef</author><pubDate>Thu, 18 Apr 2024 18:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12362v1</guid></item><item><title>Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI using Diffusion Models</title><link>http://arxiv.org/abs/2404.12361v1</link><description>Deep learning methods for accelerated MRI achieve state-of-the-art resultsbut largely ignore additional speedups possible with noncartesian samplingtrajectories. To address this gap, we created a generative diffusionmodel-based reconstruction algorithm for multi-coil highly undersampled spiralMRI. This model uses conditioning during training as well as frequency-basedguidance to ensure consistency between images and measurements. Evaluated onretrospective data, we show high quality (structural similarity &gt; 0.87) inreconstructed images with ultrafast scan times (0.02 seconds for a 2D image).We use this algorithm to identify a set of optimal variable-density spiraltrajectories and show large improvements in image quality compared toconventional reconstruction using the non-uniform fast Fourier transform. Bycombining efficient spiral sampling trajectories, multicoil imaging, and deeplearning reconstruction, these methods could enable the extremely highacceleration factors needed for real-time 3D imaging.</description><author>Trevor J. Chan, Chamith S. Rajapakse</author><pubDate>Thu, 18 Apr 2024 18:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12361v1</guid></item><item><title>Inverse Neural Rendering for Explainable Multi-Object Tracking</title><link>http://arxiv.org/abs/2404.12359v1</link><description>Today, most methods for image understanding tasks rely on feed-forward neuralnetworks. While this approach has allowed for empirical accuracy, efficiency,and task adaptation via fine-tuning, it also comes with fundamentaldisadvantages. Existing networks often struggle to generalize across differentdatasets, even on the same task. By design, these networks ultimately reasonabout high-dimensional scene features, which are challenging to analyze. Thisis true especially when attempting to predict 3D information based on 2Dimages. We propose to recast 3D multi-object tracking from RGB cameras as an\emph{Inverse Rendering (IR)} problem, by optimizing via a differentiablerendering pipeline over the latent space of pre-trained 3D objectrepresentations and retrieve the latents that best represent object instancesin a given input image. To this end, we optimize an image loss over generativelatent spaces that inherently disentangle shape and appearance properties. Weinvestigate not only an alternate take on tracking but our method also enablesexamining the generated objects, reasoning about failure situations, andresolving ambiguous cases. We validate the generalization and scalingcapabilities of our method by learning the generative prior exclusively fromsynthetic data and assessing camera-based 3D tracking on the nuScenes and Waymodatasets. Both these datasets are completely unseen to our method and do notrequire fine-tuning. Videos and code are available athttps://light.princeton.edu/inverse-rendering-tracking/.</description><author>Julian Ost, Tanushree Banerjee, Mario Bijelic, Felix Heide</author><pubDate>Thu, 18 Apr 2024 18:37:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12359v1</guid></item><item><title>From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function</title><link>http://arxiv.org/abs/2404.12358v1</link><description>Reinforcement Learning From Human Feedback (RLHF) has been a critical to thesuccess of the latest generation of generative AI models. In response to thecomplex nature of the classical RLHF pipeline, direct alignment algorithms suchas Direct Preference Optimization (DPO) have emerged as an alternativeapproach. Although DPO solves the same objective as the standard RLHF setup,there is a mismatch between the two approaches. Standard RLHF deploysreinforcement learning in a specific token-level MDP, while DPO is derived as abandit problem in which the whole response of the model is treated as a singlearm. In this work we rectify this difference, first we theoretically show thatwe can derive DPO in the token-level MDP as a general inverse Q-learningalgorithm, which satisfies the Bellman equation. Using our theoretical results,we provide three concrete empirical insights. First, we show that because ofits token level interpretation, DPO is able to perform some type of creditassignment. Next, we prove that under the token level formulation, classicalsearch-based algorithms, such as MCTS, which have recently been applied to thelanguage generation space, are equivalent to likelihood-based search on a DPOpolicy. Empirically we show that a simple beam search yields meaningfulimprovement over the base DPO policy. Finally, we show how the choice ofreference policy causes implicit rewards to decline during training. Weconclude by discussing applications of our work, including informationelicitation in multi-tun dialogue, reasoning, agentic applications andend-to-end training of multi-model systems.</description><author>Rafael Rafailov, Joey Hejna, Ryan Park, Chelsea Finn</author><pubDate>Thu, 18 Apr 2024 18:37:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12358v1</guid></item><item><title>Visually grounded few-shot word learning in low-resource settings</title><link>http://arxiv.org/abs/2306.11371v3</link><description>We propose a visually grounded speech model that learns new words and theirvisual depictions from just a few word-image example pairs. Given a set of testimages and a spoken query, we ask the model which image depicts the query word.Previous work has simplified this few-shot learning problem by either using anartificial setting with digit word-image pairs or by using a large number ofexamples per class. Moreover, all previous studies were performed using Englishspeech-image data. We propose an approach that can work on natural word-imagepairs but with less examples, i.e. fewer shots, and then illustrate how thisapproach can be applied for multimodal few-shot learning in a real low-resourcelanguage, Yor\`ub\'a. Our approach involves using the given word-image examplepairs to mine new unsupervised word-image training pairs from large collectionsof unlabelled speech and images. Additionally, we use a word-to-image attentionmechanism to determine word-image similarity. With this new model, we achievebetter performance with fewer shots than previous approaches on an existingEnglish benchmark. Many of the model's mistakes are due to confusion betweenvisual concepts co-occurring in similar contexts. The experiments on Yor\`ub\'ashow the benefit of transferring knowledge from a multimodal model trained on alarger set of English speech-image data.</description><author>Leanne Nortje, Dan Oneata, Herman Kamper</author><pubDate>Thu, 18 Apr 2024 18:36:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11371v3</guid></item><item><title>Private graphon estimation via sum-of-squares</title><link>http://arxiv.org/abs/2403.12213v2</link><description>We develop the first pure node-differentially-private algorithms for learningstochastic block models and for graphon estimation with polynomial running timefor any constant number of blocks. The statistical utility guarantees matchthose of the previous best information-theoretic (exponential-time)node-private mechanisms for these problems. The algorithm is based on anexponential mechanism for a score function defined in terms of a sum-of-squaresrelaxation whose level depends on the number of blocks. The key ingredients ofour results are (1) a characterization of the distance between the blockgraphons in terms of a quadratic optimization over the polytope of doublystochastic matrices, (2) a general sum-of-squares convergence result forpolynomial optimization over arbitrary polytopes, and (3) a general approach toperform Lipschitz extensions of score functions as part of the sum-of-squaresalgorithmic paradigm.</description><author>Hongjie Chen, Jingqiu Ding, Tommaso d'Orsi, Yiding Hua, Chih-Hung Liu, David Steurer</author><pubDate>Thu, 18 Apr 2024 18:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12213v2</guid></item><item><title>Improving the interpretability of GNN predictions through conformal-based graph sparsification</title><link>http://arxiv.org/abs/2404.12356v1</link><description>Graph Neural Networks (GNNs) have achieved state-of-the-art performance insolving graph classification tasks. However, most GNN architectures aggregateinformation from all nodes and edges in a graph, regardless of their relevanceto the task at hand, thus hindering the interpretability of their predictions.In contrast to prior work, in this paper we propose a GNN \emph{training}approach that jointly i) finds the most predictive subgraph by removing edgesand/or nodes -- -\emph{without making assumptions about the subgraph structure}-- while ii) optimizing the performance of the graph classification task. Tothat end, we rely on reinforcement learning to solve the resulting bi-leveloptimization with a reward function based on conformal predictions to accountfor the current in-training uncertainty of the classifier. Our empiricalresults on nine different graph classification datasets show that our methodcompetes in performance with baselines while relying on significantly sparsersubgraphs, leading to more interpretable GNN-based predictions.</description><author>Pablo Sanchez-Martin, Kinaan Aamir Khan, Isabel Valera</author><pubDate>Thu, 18 Apr 2024 18:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12356v1</guid></item><item><title>Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation</title><link>http://arxiv.org/abs/2404.12355v1</link><description>Foundation models, such as large language models, have demonstrated successin addressing various language and image processing tasks. In this work, weintroduce a multi-modal foundation model for scientific problems, namedPROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is amulti-operator learning approach which can predict future states ofspatiotemporal systems while concurrently learning the underlying governingequations of the physical system. Specifically, we focus on multi-operatorlearning by training distinct one-dimensional time-dependent nonlinear constantcoefficient partial differential equations, with potential applications to manyphysical applications including physics, geology, and biology. Moreimportantly, we provide three extrapolation studies to demonstrate thatPROSE-PDE can generalize physical features through the robust training ofmultiple operators and that the proposed model can extrapolate to predict PDEsolutions whose models or data were unseen during the training. Furthermore, weshow through systematic numerical experiments that the utilization of thesymbolic modality in our model effectively resolves the well-posedness problemswith training multiple operators and thus enhances our model's predictivecapabilities.</description><author>Jingmin Sun, Yuxuan Liu, Zecheng Zhang, Hayden Schaeffer</author><pubDate>Thu, 18 Apr 2024 18:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12355v1</guid></item><item><title>V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning</title><link>http://arxiv.org/abs/2404.12353v1</link><description>Video summarization aims to create short, accurate, and cohesive summaries oflonger videos. Despite the existence of various video summarization datasets, anotable limitation is their limited amount of source videos, which hampers theeffective fine-tuning of advanced large vision-language models (VLMs).Additionally, most existing datasets are created for video-to-videosummarization, overlooking the contemporary need for multimodal video contentsummarization. Recent efforts have been made to expand from unimodal tomultimodal video summarization, categorizing the task into three sub-tasksbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), anda combination of video and text summarization (V2VT). However, the textualsummaries in previous multimodal datasets are inadequate. To address theseissues, we introduce Instruct-V2Xum, a cross-modal video summarization datasetfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from40 to 940 seconds and an average summarization ratio of 16.39\%. Each videosummary in Instruct-V2Xum is paired with a textual summary that referencesspecific frame indexes, facilitating the generation of aligned video andtextual summaries. In addition, we propose a new video summarization frameworknamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is thefirst framework that unifies different video summarization tasks into one largelanguage model's (LLM) text decoder and achieves task-controllable videosummarization with temporal prompts and task instructions. Experiments showthat V2Xum-LLaMA outperforms strong baseline models on multiple videosummarization tasks. Furthermore, we propose an enhanced evaluation metric forV2V and V2VT summarization tasks.</description><author>Hang Hua, Yunlong Tang, Chenliang Xu, Jiebo Luo</author><pubDate>Thu, 18 Apr 2024 18:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12353v1</guid></item><item><title>Point-In-Context: Understanding Point Cloud via In-Context Learning</title><link>http://arxiv.org/abs/2404.12352v1</link><description>With the emergence of large-scale models trained on diverse datasets,in-context learning has emerged as a promising paradigm for multitasking,notably in natural language processing and image processing. However, itsapplication in 3D point cloud tasks remains largely unexplored. In this work,we introduce Point-In-Context (PIC), a novel framework for 3D point cloudunderstanding via in-context learning. We address the technical challenge ofeffectively extending masked point modeling to 3D point clouds by introducing aJoint Sampling module and proposing a vanilla version of PIC calledPoint-In-Context-Generalist (PIC-G). PIC-G is designed as a generalist modelfor various 3D point cloud tasks, with inputs and outputs modeled ascoordinates. In this paradigm, the challenging segmentation task is achieved byassigning label points with XYZ coordinates for each category; the finalprediction is then chosen based on the label point closest to the predictions.To break the limitation by the fixed label-coordinate assignment, which haspoor generalization upon novel classes, we propose two novel trainingstrategies, In-Context Labeling and In-Context Enhancing, forming an extendedversion of PIC named Point-In-Context-Segmenter (PIC-S), targeting improvingdynamic context labeling and model training. By utilizing dynamic in-contextlabels and extra in-context pairs, PIC-S achieves enhanced performance andgeneralization capability in and across part segmentation datasets. PIC is ageneral framework so that other tasks or datasets can be seamlessly introducedinto our PIC through a unified data format. We conduct extensive experiments tovalidate the versatility and adaptability of our proposed methods in handling awide range of tasks and segmenting multi-datasets. Our PIC-S is capable ofgeneralizing unseen datasets and performing novel part segmentation bycustomizing prompts.</description><author>Mengyuan Liu, Zhongbin Fang, Xia Li, Joachim M. Buhmann, Xiangtai Li, Chen Change Loy</author><pubDate>Thu, 18 Apr 2024 18:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12352v1</guid></item><item><title>Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey</title><link>http://arxiv.org/abs/2403.01255v2</link><description>Recent advancements in deep learning (DL) have posed a significant challengefor automatic speech recognition (ASR). ASR relies on extensive trainingdatasets, including confidential ones, and demands substantial computationaland storage resources. Enabling adaptive systems improves ASR performance indynamic environments. DL techniques assume training and testing data originatefrom the same domain, which is not always true. Advanced DL techniques likedeep transfer learning (DTL), federated learning (FL), and reinforcementlearning (RL) address these issues. DTL allows high-performance models usingsmall yet related datasets, FL enables training on confidential data withoutdataset possession, and RL optimizes decision-making in dynamic environments,reducing computation costs. This survey offers a comprehensive review of DTL,FL, and RL-based ASR frameworks, aiming to provide insights into the latestdevelopments and aid researchers and professionals in understanding the currentchallenges. Additionally, transformers, which are advanced DL techniquesheavily used in proposed ASR frameworks, are considered in this survey fortheir ability to capture extensive dependencies in the input ASR sequence. Thepaper starts by presenting the background of DTL, FL, RL, and Transformers andthen adopts a well-designed taxonomy to outline the state-of-the-artapproaches. Subsequently, a critical analysis is conducted to identify thestrengths and weaknesses of each framework. Additionally, a comparative studyis presented to highlight the existing challenges, paving the way for futureresearch opportunities.</description><author>Hamza Kheddar, Mustapha Hemis, Yassine Himeur</author><pubDate>Thu, 18 Apr 2024 18:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01255v2</guid></item><item><title>Beyond Known Clusters: Probe New Prototypes for Efficient Generalized Class Discovery</title><link>http://arxiv.org/abs/2404.08995v2</link><description>Generalized Class Discovery (GCD) aims to dynamically assign labels tounlabelled data partially based on knowledge learned from labelled data, wherethe unlabelled data may come from known or novel classes. The prevailingapproach generally involves clustering across all data and learning conceptionsby prototypical contrastive learning. However, existing methods largely hingeon the performance of clustering algorithms and are thus subject to theirinherent limitations. Firstly, the estimated cluster number is often smallerthan the ground truth, making the existing methods suffer from the lack ofprototypes for comprehensive conception learning. To address this issue, wepropose an adaptive probing mechanism that introduces learnable potentialprototypes to expand cluster prototypes (centers). As there is no ground truthfor the potential prototype, we develop a self-supervised prototype learningframework to optimize the potential prototype in an end-to-end fashion.Secondly, clustering is computationally intensive, and the conventionalstrategy of clustering both labelled and unlabelled instances exacerbates thisissue. To counteract this inefficiency, we opt to cluster only the unlabelledinstances and subsequently expand the cluster prototypes with our introducedpotential prototypes to fast explore novel classes. Despite the simplicity ofour proposed method, extensive empirical analysis on a wide range of datasetsconfirms that our method consistently delivers state-of-the-art results.Specifically, our method surpasses the nearest competitor by a significantmargin of \textbf{9.7}$\%$ within the Stanford Cars dataset and\textbf{12$\times$} clustering efficiency within the Herbarium 19 dataset. Wewill make the code and checkpoints publicly available at\url{https://github.com/xjtuYW/PNP.git}.</description><author>Ye Wang, Yaxiong Wang, Yujiao Wu, Bingchen Zhao, Xueming Qian</author><pubDate>Thu, 18 Apr 2024 18:26:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08995v2</guid></item><item><title>Evaluating AI for Law: Bridging the Gap with Open-Source Solutions</title><link>http://arxiv.org/abs/2404.12349v1</link><description>This study evaluates the performance of general-purpose AI, like ChatGPT, inlegal question-answering tasks, highlighting significant risks to legalprofessionals and clients. It suggests leveraging foundational models enhancedby domain-specific knowledge to overcome these issues. The paper advocates forcreating open-source legal AI systems to improve accuracy, transparency, andnarrative diversity, addressing general AI's shortcomings in legal contexts.</description><author>Rohan Bhambhoria, Samuel Dahan, Jonathan Li, Xiaodan Zhu</author><pubDate>Thu, 18 Apr 2024 18:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12349v1</guid></item><item><title>AniClipart: Clipart Animation with Text-to-Video Priors</title><link>http://arxiv.org/abs/2404.12347v1</link><description>Clipart, a pre-made graphic art form, offers a convenient and efficient wayof illustrating visual content. Traditional workflows to convert static clipartimages into motion sequences are laborious and time-consuming, involvingnumerous intricate steps like rigging, key animation and in-betweening. Recentadvancements in text-to-video generation hold great potential in resolving thisproblem. Nevertheless, direct application of text-to-video generation modelsoften struggles to retain the visual identity of clipart images or generatecartoon-style motions, resulting in unsatisfactory animation outcomes. In thispaper, we introduce AniClipart, a system that transforms static clipart imagesinto high-quality motion sequences guided by text-to-video priors. To generatecartoon-style and smooth motion, we first define B\'{e}zier curves overkeypoints of the clipart image as a form of motion regularization. We thenalign the motion trajectories of the keypoints with the provided text prompt byoptimizing the Video Score Distillation Sampling (VSDS) loss, which encodesadequate knowledge of natural motion within a pretrained text-to-videodiffusion model. With a differentiable As-Rigid-As-Possible shape deformationalgorithm, our method can be end-to-end optimized while maintaining deformationrigidity. Experimental results show that the proposed AniClipart consistentlyoutperforms existing image-to-video generation models, in terms of text-videoalignment, visual identity preservation, and motion consistency. Furthermore,we showcase the versatility of AniClipart by adapting it to generate a broaderarray of animation formats, such as layered animation, which allows topologicalchanges.</description><author>Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao</author><pubDate>Thu, 18 Apr 2024 18:24:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12347v1</guid></item><item><title>Large Language Models in Targeted Sentiment Analysis</title><link>http://arxiv.org/abs/2404.12342v1</link><description>In this paper we investigate the use of decoder-based generative transformersfor extracting sentiment towards the named entities in Russian news articles.We study sentiment analysis capabilities of instruction-tuned large languagemodels (LLMs). We consider the dataset of RuSentNE-2023 in our study. The firstgroup of experiments was aimed at the evaluation of zero-shot capabilities ofLLMs with closed and open transparencies. The second covers the fine-tuning ofFlan-T5 using the "chain-of-thought" (CoT) three-hop reasoning framework(THoR). We found that the results of the zero-shot approaches are similar tothe results achieved by baseline fine-tuned encoder-based transformers(BERT-base). Reasoning capabilities of the fine-tuned Flan-T5 models with THoRachieve at least 5% increment with the base-size model compared to the resultsof the zero-shot experiment. The best results of sentiment analysis onRuSentNE-2023 were achieved by fine-tuned Flan-T5-xl, which surpassed theresults of previous state-of-the-art transformer-based classifiers. Our CoTapplication framework is publicly available:https://github.com/nicolay-r/Reasoning-for-Sentiment-Analysis-Framework</description><author>Nicolay Rusnachenko, Anton Golubev, Natalia Loukachevitch</author><pubDate>Thu, 18 Apr 2024 18:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12342v1</guid></item><item><title>One-shot Empirical Privacy Estimation for Federated Learning</title><link>http://arxiv.org/abs/2302.03098v5</link><description>Privacy estimation techniques for differentially private (DP) algorithms areuseful for comparing against analytical bounds, or to empirically measureprivacy loss in settings where known analytical bounds are not tight. However,existing privacy auditing techniques usually make strong assumptions on theadversary (e.g., knowledge of intermediate model iterates or the training datadistribution), are tailored to specific tasks, model architectures, or DPalgorithm, and/or require retraining the model many times (typically on theorder of thousands). These shortcomings make deploying such techniques at scaledifficult in practice, especially in federated settings where model trainingcan take days or weeks. In this work, we present a novel "one-shot" approachthat can systematically address these challenges, allowing efficient auditingor estimation of the privacy loss of a model during the same, single trainingrun used to fit model parameters, and without requiring any a priori knowledgeabout the model architecture, task, or DP training algorithm. We show that ourmethod provides provably correct estimates for the privacy loss under theGaussian mechanism, and we demonstrate its performance on well-established FLbenchmark datasets under several adversarial threat models.</description><author>Galen Andrew, Peter Kairouz, Sewoong Oh, Alina Oprea, H. Brendan McMahan, Vinith M. Suriyakumar</author><pubDate>Thu, 18 Apr 2024 18:14:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03098v5</guid></item><item><title>Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models</title><link>http://arxiv.org/abs/2404.02124v3</link><description>Multiple-choice questions (MCQs) are ubiquitous in almost all levels ofeducation since they are easy to administer, grade, and are a reliable formatin assessments and practices. One of the most important aspects of MCQs is thedistractors, i.e., incorrect options that are designed to target common errorsor misconceptions among real students. To date, the task of craftinghigh-quality distractors largely remains a labor and time-intensive process forteachers and learning content designers, which has limited scalability. In thiswork, we study the task of automated distractor generation in the domain ofmath MCQs and explore a wide variety of large language model (LLM)-basedapproaches, from in-context learning to fine-tuning. We conduct extensiveexperiments using a real-world math MCQ dataset and find that although LLMs cangenerate some mathematically valid distractors, they are less adept atanticipating common errors or misconceptions among real students.</description><author>Wanyong Feng, Jaewook Lee, Hunter McNichols, Alexander Scarlatos, Digory Smith, Simon Woodhead, Nancy Otero Ornelas, Andrew Lan</author><pubDate>Thu, 18 Apr 2024 18:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02124v3</guid></item><item><title>JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks</title><link>http://arxiv.org/abs/2404.03027v2</link><description>With the rapid advancements in Multimodal Large Language Models (MLLMs),securing these models against malicious inputs while aligning them with humanvalues has emerged as a critical challenge. In this paper, we investigate animportant and unexplored question of whether techniques that successfullyjailbreak Large Language Models (LLMs) can be equally effective in jailbreakingMLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneeringbenchmark designed to assess the transferability of LLM jailbreak techniques toMLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreakattacks. Utilizing a dataset of 2, 000 malicious queries that is also proposedin this paper, we generate 20, 000 text-based jailbreak prompts using advancedjailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs fromrecent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 testcases across a spectrum of adversarial scenarios. Our evaluation of 10open-source MLLMs reveals a notably high Attack Success Rate (ASR) for attackstransferred from LLMs, highlighting a critical vulnerability in MLLMs thatstems from their text-processing capabilities. Our findings underscore theurgent need for future research to address alignment vulnerabilities in MLLMsfrom both textual and visual inputs.</description><author>Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, Chaowei Xiao</author><pubDate>Thu, 18 Apr 2024 18:11:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03027v2</guid></item><item><title>Measuring Feature Dependency of Neural Networks by Collapsing Feature Dimensions in the Data Manifold</title><link>http://arxiv.org/abs/2404.12341v1</link><description>This paper introduces a new technique to measure the feature dependency ofneural network models. The motivation is to better understand a model byquerying whether it is using information from human-understandable features,e.g., anatomical shape, volume, or image texture. Our method is based on theprinciple that if a model is dependent on a feature, then removal of thatfeature should significantly harm its performance. A targeted feature is"removed" by collapsing the dimension in the data distribution that correspondsto that feature. We perform this by moving data points along the featuredimension to a baseline feature value while staying on the data manifold, asestimated by a deep generative model. Then we observe how the model'sperformance changes on the modified test data set, with the target featuredimension removed. We test our method on deep neural network models trained onsynthetic image data with known ground truth, an Alzheimer's disease predictiontask using MRI and hippocampus segmentations from the OASIS-3 dataset, and acell nuclei classification task using the Lizard dataset.</description><author>Yinzhu Jin, Matthew B. Dwyer, P. Thomas Fletcher</author><pubDate>Thu, 18 Apr 2024 18:10:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12341v1</guid></item><item><title>SPOT: Point Cloud Based Stereo Visual Place Recognition for Similar and Opposing Viewpoints</title><link>http://arxiv.org/abs/2404.12339v1</link><description>Recognizing places from an opposing viewpoint during a return trip is acommon experience for human drivers. However, the analogous roboticscapability, visual place recognition (VPR) with limited field of view camerasunder 180 degree rotations, has proven to be challenging to achieve. To addressthis problem, this paper presents Same Place Opposing Trajectory (SPOT), atechnique for opposing viewpoint VPR that relies exclusively on structureestimated through stereo visual odometry (VO). The method extends recentadvances in lidar descriptors and utilizes a novel double (similar andopposing) distance matrix sequence matching method. We evaluate SPOT on apublicly available dataset with 6.7-7.6 km routes driven in similar andopposing directions under various lighting conditions. The proposed algorithmdemonstrates remarkable improvement over the state-of-the-art, achieving up to91.7% recall at 100% precision in opposing viewpoint cases, while requiringless storage than all baselines tested and running faster than all but one.Moreover, the proposed method assumes no a priori knowledge of whether theviewpoint is similar or opposing, and also demonstrates competitive performancein similar viewpoint cases.</description><author>Spencer Carmichael, Rahul Agrawal, Ram Vasudevan, Katherine A. Skinner</author><pubDate>Thu, 18 Apr 2024 18:09:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12339v1</guid></item><item><title>Low-rank tensor completion via tensor joint rank with logarithmic composite norm</title><link>http://arxiv.org/abs/2309.16208v2</link><description>Low-rank tensor completion (LRTC) aims to recover a complete low-rank tensorfrom incomplete observed tensor, attracting extensive attention in variouspractical applications such as image processing and computer vision. However,current methods often perform well only when there is a sufficient of observedinformation, and they perform poorly or may fail when the observed informationis less than 5\%. In order to improve the utilization of observed information,a new method called the tensor joint rank with logarithmic composite norm(TJLC) method is proposed. This method simultaneously exploits two types oftensor low-rank structures, namely tensor Tucker rank and tubal rank, therebyenhancing the inherent correlations between known and missing elements. Toaddress the challenge of applying two tensor ranks with significantly differentdirectly to LRTC, a new tensor Logarithmic composite norm is further proposed.Subsequently, the TJLC model and algorithm for the LRTC problem are proposed.Additionally, theoretical convergence guarantees for the TJLC method areprovided. Experiments on various real datasets demonstrate that the proposedmethod outperforms state-of-the-art methods significantly. Particularly, theproposed method achieves satisfactory recovery even when the observedinformation is as low as 1\%, and the recovery performance improvessignificantly as the observed information increases.</description><author>Hongbing Zhang</author><pubDate>Thu, 18 Apr 2024 18:08:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16208v2</guid></item><item><title>Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models</title><link>http://arxiv.org/abs/2308.02022v2</link><description>While reaching for NLP systems that maximize accuracy, other importantmetrics of system performance are often overlooked. Prior models are easilyforgotten despite their possible suitability in settings where large computingresources are unavailable or relatively more costly. In this paper, we performa broad comparative evaluation of document-level sentiment analysis models witha focus on resource costs that are important for the feasibility of modeldeployment and general climate consciousness. Our experiments considerdifferent feature extraction techniques, the effect of ensembling,task-specific deep learning modeling, and domain-independent large languagemodels (LLMs). We find that while a fine-tuned LLM achieves the best accuracy,some alternate configurations provide huge (up to 24, 283 *) resource savingsfor a marginal (&lt;1%) loss in accuracy. Furthermore, we find that for smallerdatasets, the differences in accuracy shrink while the difference in resourceconsumption grows further.</description><author>Mahammed Kamruzzaman, Gene Louis Kim</author><pubDate>Thu, 18 Apr 2024 18:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02022v2</guid></item><item><title>The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations</title><link>http://arxiv.org/abs/2212.11737v2</link><description>Machine learning (ML) models are nowadays used in complex applications invarious domains, such as medicine, bioinformatics, and other sciences. Due totheir black box nature, however, it may sometimes be hard to understand andtrust the results they provide. This has increased the demand for reliablevisualization tools related to enhancing trust in ML models, which has become aprominent topic of research in the visualization community over the pastdecades. To provide an overview and present the frontiers of current researchon the topic, we present a State-of-the-Art Report (STAR) on enhancing trust inML models with the use of interactive visualization. We define and describe thebackground of the topic, introduce a categorization for visualizationtechniques that aim to accomplish this goal, and discuss insights andopportunities for future research directions. Among our contributions is acategorization of trust against different facets of interactive ML, expandedand improved from previous research. Our results are investigated fromdifferent analytical perspectives: (a) providing a statistical overview, (b)summarizing key findings, (c) performing topic analyses, and (d) exploring thedata sets used in the individual papers, all with the support of an interactiveweb-based survey browser. We intend this survey to be beneficial forvisualization researchers whose interests involve making ML models moretrustworthy, as well as researchers and practitioners from other disciplines intheir search for effective visualization techniques suitable for solving theirtasks with confidence and conveying meaning to their data.</description><author>A. Chatzimparmpas, R. Martins, I. Jusufi, K. Kucher, Fabrice Rossi, A. Kerren</author><pubDate>Thu, 18 Apr 2024 18:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.11737v2</guid></item><item><title>Customizing Text-to-Image Diffusion with Camera Viewpoint Control</title><link>http://arxiv.org/abs/2404.12333v1</link><description>Model customization introduces new concepts to existing text-to-image models,enabling the generation of the new concept in novel contexts. However, suchmethods lack accurate camera view control w.r.t the object, and users mustresort to prompt engineering (e.g., adding "top-view") to achieve coarse viewcontrol. In this work, we introduce a new task -- enabling explicit control ofcamera viewpoint for model customization. This allows us to modify objectproperties amongst various background scenes via text prompts, all whileincorporating the target camera pose as additional control. This new taskpresents significant challenges in merging a 3D representation from themulti-view images of the new concept with a general, 2D text-to-image model. Tobridge this gap, we propose to condition the 2D diffusion process on rendered,view-dependent features of the new object. During training, we jointly adaptthe 2D diffusion modules and 3D feature predictions to reconstruct the object'sappearance and geometry while reducing overfitting to the input multi-viewimages. Our method outperforms existing image editing and model personalizationbaselines in preserving the custom object's identity while following the inputtext prompt and the object's camera pose.</description><author>Nupur Kumari, Grace Su, Richard Zhang, Taesung Park, Eli Shechtman, Jun-Yan Zhu</author><pubDate>Thu, 18 Apr 2024 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12333v1</guid></item><item><title>VisRuler: Visual Analytics for Extracting Decision Rules from Bagged and Boosted Decision Trees</title><link>http://arxiv.org/abs/2112.00334v5</link><description>Bagging and boosting are two popular ensemble methods in machine learning(ML) that produce many individual decision trees. Due to the inherent ensemblecharacteristic of these methods, they typically outperform single decisiontrees or other ML models in predictive performance. However, numerous decisionpaths are generated for each decision tree, increasing the overall complexityof the model and hindering its use in domains that require trustworthy andexplainable decisions, such as finance, social care, and health care. Thus, theinterpretability of bagging and boosting algorithms, such as random forest andadaptive boosting, reduces as the number of decisions rises. In this paper, wepropose a visual analytics tool that aims to assist users in extractingdecisions from such ML models via a thorough visual inspection workflow thatincludes selecting a set of robust and diverse models (originating fromdifferent ensemble learning algorithms), choosing important features accordingto their global contribution, and deciding which decisions are essential forglobal explanation (or locally, for specific cases). The outcome is a finaldecision based on the class agreement of several models and the explored manualdecisions exported by users. We evaluated the applicability and effectivenessof VisRuler via a use case, a usage scenario, and a user study. The evaluationrevealed that most users managed to successfully use our system to exploredecision rules visually, performing the proposed tasks and answering the givenquestions in a satisfying way.</description><author>Angelos Chatzimparmpas, Rafael M. Martins, Andreas Kerren</author><pubDate>Thu, 18 Apr 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.00334v5</guid></item><item><title>A Perspective on Deep Vision Performance with Standard Image and Video Codecs</title><link>http://arxiv.org/abs/2404.12330v1</link><description>Resource-constrained hardware, such as edge devices or cell phones, oftenrely on cloud servers to provide the required computational resources forinference in deep vision models. However, transferring image and video datafrom an edge or mobile device to a cloud server requires coding to deal withnetwork constraints. The use of standardized codecs, such as JPEG or H.264, isprevalent and required to ensure interoperability. This paper aims to examinethe implications of employing standardized codecs within deep vision pipelines.We find that using JPEG and H.264 coding significantly deteriorates theaccuracy across a broad range of vision tasks and models. For instance, strongcompression rates reduce semantic segmentation accuracy by more than 80% inmIoU. In contrast to previous findings, our analysis extends beyond image andaction classification to localization and dense prediction tasks, thusproviding a more comprehensive perspective.</description><author>Christoph Reich, Oliver Hahn, Daniel Cremers, Stefan Roth, Biplob Debnath</author><pubDate>Thu, 18 Apr 2024 17:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12330v1</guid></item><item><title>Generalizable Face Landmarking Guided by Conditional Face Warping</title><link>http://arxiv.org/abs/2404.12322v1</link><description>As a significant step for human face modeling, editing, and generation, facelandmarking aims at extracting facial keypoints from images. A generalizableface landmarker is required in practice because real-world facial images, e.g.,the avatars in animations and games, are often stylized in various ways.However, achieving generalizable face landmarking is challenging due to thediversity of facial styles and the scarcity of labeled stylized faces. In thisstudy, we propose a simple but effective paradigm to learn a generalizable facelandmarker based on labeled real human faces and unlabeled stylized faces. Ourmethod learns the face landmarker as the key module of a conditional facewarper. Given a pair of real and stylized facial images, the conditional facewarper predicts a warping field from the real face to the stylized one, inwhich the face landmarker predicts the ending points of the warping field andprovides us with high-quality pseudo landmarks for the corresponding stylizedfacial images. Applying an alternating optimization strategy, we learn the facelandmarker to minimize $i)$ the discrepancy between the stylized faces and thewarped real ones and $ii)$ the prediction errors of both real and pseudolandmarks. Experiments on various datasets show that our method outperformsexisting state-of-the-art domain adaptation methods in face landmarking tasks,leading to a face landmarker with better generalizability. Code is available athttps://plustwo0.github.io/project-face-landmarker}{https://plustwo0.github.io/project-face-landmarker.</description><author>Jiayi Liang, Haotian Liu, Hongteng Xu, Dixin Luo</author><pubDate>Thu, 18 Apr 2024 17:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12322v1</guid></item><item><title>Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment</title><link>http://arxiv.org/abs/2404.12318v1</link><description>Aligning language models (LMs) based on human-annotated preference data is acrucial step in obtaining practical and performant LM-based systems. However,multilingual human preference data are difficult to obtain at scale, making itchallenging to extend this framework to diverse languages. In this work, weevaluate a simple approach for zero-shot cross-lingual alignment, where areward model is trained on preference data in one source language and directlyapplied to other target languages. On summarization and open-ended dialoggeneration, we show that this method is consistently successful undercomprehensive evaluation settings, including human evaluation: cross-linguallyaligned models are preferred by humans over unaligned models on up to &gt;70% ofevaluation instances. We moreover find that a different-language reward modelsometimes yields better aligned models than a same-language reward model. Wealso identify best practices when there is no language-specific data for evensupervised finetuning, another component in alignment.</description><author>Zhaofeng Wu, Ananth Balashankar, Yoon Kim, Jacob Eisenstein, Ahmad Beirami</author><pubDate>Thu, 18 Apr 2024 17:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12318v1</guid></item><item><title>Large Language Models for Synthetic Participatory Planning of Shared Automated Electric Mobility Systems</title><link>http://arxiv.org/abs/2404.12317v1</link><description>Unleashing the synergies of rapidly evolving mobility technologies in amulti-stakeholder landscape presents unique challenges and opportunities foraddressing urban transportation problems. This paper introduces a novelsynthetic participatory method, critically leveraging large language models(LLMs) to create digital avatars representing diverse stakeholders to planshared automated electric mobility systems (SAEMS). These calibratable agentscollaboratively identify objectives, envision and evaluate SAEMS alternatives,and strategize implementation under risks and constraints. The results of aMontreal case study indicate that a structured and parameterized workflowprovides outputs with high controllability and comprehensiveness on an SAEMSplan than generated using a single LLM-enabled expert agent. Consequently, theapproach provides a promising avenue for cost-efficiently improving theinclusivity and interpretability of multi-objective transportation planning,suggesting a paradigm shift in how we envision and strategize for sustainableand equitable transportation systems.</description><author>Jiangbo Yu</author><pubDate>Thu, 18 Apr 2024 17:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12317v1</guid></item><item><title>Adjoint Sensitivities of Chaotic Flows without Adjoint Solvers: A Data-Driven Approach</title><link>http://arxiv.org/abs/2404.12315v1</link><description>In one calculation, adjoint sensitivity analysis provides the gradient of aquantity of interest with respect to all system's parameters. Conventionally,adjoint solvers need to be implemented by differentiating computational models,which can be a cumbersome task and is code-specific. To propose an adjointsolver that is not code-specific, we develop a data-driven strategy. Wedemonstrate its application on the computation of gradients of long-timeaverages of chaotic flows. First, we deploy a parameter-aware echo statenetwork (ESN) to accurately forecast and simulate the dynamics of a dynamicalsystem for a range of system's parameters. Second, we derive the adjoint of theparameter-aware ESN. Finally, we combine the parameter-aware ESN with itsadjoint version to compute the sensitivities to the system parameters. Weshowcase the method on a prototypical chaotic system. Because adjointsensitivities in chaotic regimes diverge for long integration times, we analysethe application of ensemble adjoint method to the ESN. We find that the adjointsensitivities obtained from the ESN match closely with the original system.This work opens possibilities for sensitivity analysis without code-specificadjoint solvers.</description><author>Defne E. Ozan, Luca Magri</author><pubDate>Thu, 18 Apr 2024 17:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12315v1</guid></item><item><title>Guided Discrete Diffusion for Electronic Health Record Generation</title><link>http://arxiv.org/abs/2404.12314v1</link><description>Electronic health records (EHRs) are a pivotal data source that enablesnumerous applications in computational medicine, e.g., disease progressionprediction, clinical trial design, and health economics and outcomes research.Despite wide usability, their sensitive nature raises privacy andconfidentially concerns, which limit potential use cases. To tackle thesechallenges, we explore the use of generative models to synthesize artificial,yet realistic EHRs. While diffusion-based methods have recently demonstratedstate-of-the-art performance in generating other data modalities and overcomethe training instability and mode collapse issues that plague previousGAN-based approaches, their applications in EHR generation remainunderexplored. The discrete nature of tabular medical code data in EHRs poseschallenges for high-quality data generation, especially for continuousdiffusion models. To this end, we introduce a novel tabular EHR generationmethod, EHR-D3PM, which enables both unconditional and conditional generationusing the discrete diffusion model. Our experiments demonstrate that EHR-D3PMsignificantly outperforms existing generative baselines on comprehensivefidelity and utility metrics while maintaining less membership vulnerabilityrisks. Furthermore, we show EHR-D3PM is effective as a data augmentation methodand enhances performance on downstream tasks when combined with real data.</description><author>Zixiang Chen, Jun Han, Yongqian Li, Yiwen Kou, Eran Halperin, Robert E. Tillman, Quanquan Gu</author><pubDate>Thu, 18 Apr 2024 17:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12314v1</guid></item><item><title>SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization</title><link>http://arxiv.org/abs/2402.13919v3</link><description>Large Language Models (LLMs) such as GPT &amp; Llama have demonstratedsignificant achievements in summarization tasks but struggle with factualinaccuracies, a critical issue in clinical NLP applications where errors couldlead to serious consequences. To counter the high costs and limitedavailability of expert-annotated data for factual alignment, this studyintroduces an innovative pipeline that utilizes &gt;100B parameter GPT variantslike GPT-3.5 &amp; GPT-4 to act as synthetic experts to generate high-qualitysynthetics feedback aimed at enhancing factual consistency in clinical notesummarization. Our research primarily focuses on edit feedback generated bythese synthetic feedback experts without additional human annotations,mirroring and optimizing the practical scenario in which medical professionalsrefine AI system outputs. Although such 100B+ parameter GPT variants haveproven to demonstrate expertise in various clinical NLP tasks, such as theMedical Licensing Examination, there is scant research on their capacity to actas synthetic feedback experts and deliver expert-level edit feedback forimproving the generation quality of weaker (&lt;10B parameter) LLMs like GPT-2(1.5B) &amp; Llama 2 (7B) in clinical domain. So in this work, we leverage 100B+GPT variants to act as synthetic feedback experts offering expert-level editfeedback, that is used to reduce hallucinations and align weaker (&lt;10Bparameter) LLMs with medical facts using two distinct alignment algorithms (DPO&amp; SALT), endeavoring to narrow the divide between AI-generated content andfactual accuracy. This highlights the substantial potential of LLM-basedsynthetic edits in enhancing the alignment of clinical factuality.</description><author>Prakamya Mishra, Zonghai Yao, Parth Vashisht, Feiyun Ouyang, Beining Wang, Vidhi Dhaval Mody, Hong Yu</author><pubDate>Thu, 18 Apr 2024 17:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13919v3</guid></item><item><title>DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps</title><link>http://arxiv.org/abs/2304.00133v5</link><description>As the complexity of machine learning (ML) models increases and theirapplication in different (and critical) domains grows, there is a strong demandfor more interpretable and trustworthy ML. A direct, model-agnostic, way tointerpret such models is to train surrogate models-such as rule sets anddecision trees-that sufficiently approximate the original ones while beingsimpler and easier-to-explain. Yet, rule sets can become very lengthy, withmany if-else statements, and decision tree depth grows rapidly when accuratelyemulating complex ML models. In such cases, both approaches can fail to meettheir core goal-providing users with model interpretability. To tackle this, wepropose DeforestVis, a visual analytics tool that offers summarization of thebehaviour of complex ML models by providing surrogate decision stumps(one-level decision trees) generated with the Adaptive Boosting (AdaBoost)technique. DeforestVis helps users to explore the complexity versus fidelitytrade-off by incrementally generating more stumps, creating attribute-basedexplanations with weighted stumps to justify decision making, and analysing theimpact of rule overriding on training instance allocation between one or morestumps. An independent test set allows users to monitor the effectiveness ofmanual rule changes and form hypotheses based on case-by-case analyses. We showthe applicability and usefulness of DeforestVis with two use cases and expertinterviews with data analysts and model developers.</description><author>Angelos Chatzimparmpas, Rafael M. Martins, Alexandru C. Telea, Andreas Kerren</author><pubDate>Thu, 18 Apr 2024 17:46:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00133v5</guid></item><item><title>A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to Functional Conditional Moment Equations</title><link>http://arxiv.org/abs/2404.12312v1</link><description>We study minimax optimization problems defined over infinite-dimensionalfunction classes. In particular, we restrict the functions to the class ofoverparameterized two-layer neural networks and study (i) the convergence ofthe gradient descent-ascent algorithm and (ii) the representation learning ofthe neural network. As an initial step, we consider the minimax optimizationproblem stemming from estimating a functional equation defined by conditionalexpectations via adversarial estimation, where the objective function isquadratic in the functional space. For this problem, we establish convergenceunder the mean-field regime by considering the continuous-time andinfinite-width limit of the optimization dynamics. Under this regime, gradientdescent-ascent corresponds to a Wasserstein gradient flow over the space ofprobability measures defined over the space of neural network parameters. Weprove that the Wasserstein gradient flow converges globally to a stationarypoint of the minimax objective at a $\mathcal{O}(T^{-1} + \alpha^{-1} ) $sublinear rate, and additionally finds the solution to the functional equationwhen the regularizer of the minimax objective is strongly convex. Here $T$denotes the time and $\alpha$ is a scaling parameter of the neural network. Interms of representation learning, our results show that the featurerepresentation induced by the neural networks is allowed to deviate from theinitial one by the magnitude of $\mathcal{O}(\alpha^{-1})$, measured in termsof the Wasserstein distance. Finally, we apply our general results to concreteexamples including policy evaluation, nonparametric instrumental variableregression, and asset pricing.</description><author>Yuchen Zhu, Yufeng Zhang, Zhaoran Wang, Zhuoran Yang, Xiaohong Chen</author><pubDate>Thu, 18 Apr 2024 17:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12312v1</guid></item><item><title>Transferability Ranking of Adversarial Examples</title><link>http://arxiv.org/abs/2208.10878v2</link><description>Adversarial transferability in black-box scenarios presents a uniquechallenge: while attackers can employ surrogate models to craft adversarialexamples, they lack assurance on whether these examples will successfullycompromise the target model. Until now, the prevalent method to ascertainsuccess has been trial and error-testing crafted samples directly on the victimmodel. This approach, however, risks detection with every attempt, forcingattackers to either perfect their first try or face exposure. Our paperintroduces a ranking strategy that refines the transfer attack process,enabling the attacker to estimate the likelihood of success without repeatedtrials on the victim's system. By leveraging a set of diverse surrogate models,our method can predict transferability of adversarial examples. This strategycan be used to either select the best sample to use in an attack or the bestperturbation to apply to a specific sample. Using our strategy, we were able toraise the transferability of adversarial examples from a mere 20% - akin torandom selection-up to near upper-bound levels, with some scenarios evenwitnessing a 100% success rate. This substantial improvement not only shedslight on the shared susceptibilities across diverse architectures but alsodemonstrates that attackers can forego the detectable trial-and-error tacticsraising increasing the threat of surrogate-based attacks.</description><author>Mosh Levy, Guy Amit, Yuval Elovici, Yisroel Mirsky</author><pubDate>Thu, 18 Apr 2024 17:41:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10878v2</guid></item><item><title>iRAG: An Incremental Retrieval Augmented Generation System for Videos</title><link>http://arxiv.org/abs/2404.12309v1</link><description>Retrieval augmented generation (RAG) systems combine the strengths oflanguage generation and information retrieval to power many real-worldapplications like chatbots. Use of RAG for combined understanding of multimodaldata such as text, images and videos is appealing but two critical limitationsexist: one-time, upfront capture of all content in large multimodal data astext descriptions entails high processing times, and not all information in therich multimodal data is typically in the text descriptions. Since the userqueries are not known apriori, developing a system for multimodal to textconversion and interactive querying of multimodal data is challenging. To address these limitations, we propose iRAG, which augments RAG with anovel incremental workflow to enable interactive querying of large corpus ofmultimodal data. Unlike traditional RAG, iRAG quickly indexes largerepositories of multimodal data, and in the incremental workflow, it uses theindex to opportunistically extract more details from select portions of themultimodal data to retrieve context relevant to an interactive user query. Suchan incremental workflow avoids long multimodal to text conversion times,overcomes information loss issues by doing on-demand query-specific extractionof details in multimodal data, and ensures high quality of responses tointeractive user queries that are often not known apriori. To the best of ourknowledge, iRAG is the first system to augment RAG with an incremental workflowto support efficient interactive querying of large, real-world multimodal data.Experimental results on real-world long videos demonstrate 23x to 25x fastervideo to text ingestion, while ensuring that quality of responses tointeractive user queries is comparable to responses from a traditional RAGwhere all video data is converted to text upfront before any querying.</description><author>Md Adnan Arefeen, Biplob Debnath, Md Yusuf Sarwar Uddin, Srimat Chakradhar</author><pubDate>Thu, 18 Apr 2024 17:38:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12309v1</guid></item><item><title>HardVis: Visual Analytics to Handle Instance Hardness Using Undersampling and Oversampling Techniques</title><link>http://arxiv.org/abs/2203.15753v4</link><description>Despite the tremendous advances in machine learning (ML), training withimbalanced data still poses challenges in many real-world applications. Among aseries of diverse techniques to solve this problem, sampling algorithms areregarded as an efficient solution. However, the problem is more fundamental,with many works emphasizing the importance of instance hardness. This issuerefers to the significance of managing unsafe or potentially noisy instancesthat are more likely to be misclassified and serve as the root cause of poorclassification performance. This paper introduces HardVis, a visual analyticssystem designed to handle instance hardness mainly in imbalanced classificationscenarios. Our proposed system assists users in visually comparing differentdistributions of data types, selecting types of instances based on localcharacteristics that will later be affected by the active sampling method, andvalidating which suggestions from undersampling or oversampling techniques arebeneficial for the ML model. Additionally, rather than uniformlyundersampling/oversampling a specific class, we allow users to find and sampleeasy and difficult to classify training instances from all classes. Users canexplore subsets of data from different perspectives to decide all thoseparameters, while HardVis keeps track of their steps and evaluates the model'spredictive performance in a test set separately. The end result is awell-balanced data set that boosts the predictive power of the ML model. Theefficacy and effectiveness of HardVis are demonstrated with a hypotheticalusage scenario and a use case. Finally, we also look at how useful our systemis based on feedback we received from ML experts.</description><author>Angelos Chatzimparmpas, Fernando V. Paulovich, Andreas Kerren</author><pubDate>Thu, 18 Apr 2024 17:37:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.15753v4</guid></item><item><title>ASID: Active Exploration for System Identification in Robotic Manipulation</title><link>http://arxiv.org/abs/2404.12308v1</link><description>Model-free control strategies such as reinforcement learning have shown theability to learn control strategies without requiring an accurate model orsimulator of the world. While this is appealing due to the lack of modelingrequirements, such methods can be sample inefficient, making them impracticalin many real-world domains. On the other hand, model-based control techniquesleveraging accurate simulators can circumvent these challenges and use a largeamount of cheap simulation data to learn controllers that can effectivelytransfer to the real world. The challenge with such model-based techniques isthe requirement for an extremely accurate simulation, requiring both thespecification of appropriate simulation assets and physical parameters. Thisrequires considerable human effort to design for every environment beingconsidered. In this work, we propose a learning system that can leverage asmall amount of real-world data to autonomously refine a simulation model andthen plan an accurate control strategy that can be deployed in the real world.Our approach critically relies on utilizing an initial (possibly inaccurate)simulator to design effective exploration policies that, when deployed in thereal world, collect high-quality data. We demonstrate the efficacy of thisparadigm in identifying articulation, mass, and other physical parameters inseveral challenging robotic manipulation tasks, and illustrate that only asmall amount of real-world data can allow for effective sim-to-real transfer.Project website at https://weirdlabuw.github.io/asid</description><author>Marius Memmel, Andrew Wagenmaker, Chuning Zhu, Patrick Yin, Dieter Fox, Abhishek Gupta</author><pubDate>Thu, 18 Apr 2024 17:35:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12308v1</guid></item><item><title>Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair</title><link>http://arxiv.org/abs/2404.12299v1</link><description>In Simultaneous Machine Translation (SiMT) systems, training with asimultaneous interpretation (SI) corpus is an effective method for achievinghigh-quality yet low-latency systems. However, it is very challenging to curatesuch a corpus due to limitations in the abilities of annotators, and hence,existing SI corpora are limited. Therefore, we propose a method to convertexisting speech translation corpora into interpretation-style data, maintainingthe original word order and preserving the entire source content using LargeLanguage Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models intext-to-text and speech-to-text settings with the LLM-SI-Corpus reduceslatencies while maintaining the same level of quality as the models trainedwith offline datasets. The LLM-SI-Corpus is available at\url{https://github.com/yusuke1997/LLM-SI-Corpus}.</description><author>Yusuke Sakai, Mana Makinae, Hidetaka Kamigaito, Taro Watanabe</author><pubDate>Thu, 18 Apr 2024 17:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12299v1</guid></item><item><title>VisEvol: Visual Analytics to Support Hyperparameter Search through Evolutionary Optimization</title><link>http://arxiv.org/abs/2012.01205v4</link><description>During the training phase of machine learning (ML) models, it is usuallynecessary to configure several hyperparameters. This process is computationallyintensive and requires an extensive search to infer the best hyperparameter setfor the given problem. The challenge is exacerbated by the fact that most MLmodels are complex internally, and training involves trial-and-error processesthat could remarkably affect the predictive result. Moreover, eachhyperparameter of an ML algorithm is potentially intertwined with the others,and changing it might result in unforeseeable impacts on the remaininghyperparameters. Evolutionary optimization is a promising method to try andaddress those issues. According to this method, performant models are stored,while the remainder are improved through crossover and mutation processesinspired by genetic algorithms. We present VisEvol, a visual analytics toolthat supports interactive exploration of hyperparameters and intervention inthis evolutionary procedure. In summary, our proposed tool helps the user togenerate new models through evolution and eventually explore powerfulhyperparameter combinations in diverse regions of the extensive hyperparameterspace. The outcome is a voting ensemble (with equal rights) that boosts thefinal predictive performance. The utility and applicability of VisEvol aredemonstrated with two use cases and interviews with ML experts who evaluatedthe effectiveness of the tool.</description><author>Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas Kerren</author><pubDate>Thu, 18 Apr 2024 17:23:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.01205v4</guid></item><item><title>Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens</title><link>http://arxiv.org/abs/2402.15758v2</link><description>Large language models (LLMs) have demonstrated remarkable capabilities acrossvarious tasks. However, their widespread application is hindered by theresource-intensive decoding process. To address this challenge, currentapproaches have incorporated additional decoding heads to enable parallelprediction of multiple subsequent tokens, thereby achieving inferenceacceleration. Nevertheless, the accuracy of these decoding heads falls short ofthe auto-regressive decoding approach. In light of these limitations, we propose Chimera, a novel frameworkspecifically designed for speculative sampling. Within this framework, weintroduce a lightweight draft model that effectively utilizes previouslygenerated tokens to predict subsequent words. To ensure both accuracy andefficiency, we present two strategies within the lightweight draft model.Firstly, we focus on capturing short-range dependencies at the bottom layer.Secondly, we leverage the readily available representations from the originalLLM.Through empirical evaluation on the Vicuna and LlaMA-2 series, Chimerademonstrates impressive results, achieving an average latency speedup ratio of2.7x compared to the vanilla auto-regressive decoding approach. This highlightsthe potential of our proposed framework in significantly improving theefficiency of large language models during the decoding process.</description><author>Ziqian Zeng, Jiahong Yu, Qianshi Pang, Zihao Wang, Huiping Zhuang, Hongen Shao, Xiaofeng Zou</author><pubDate>Thu, 18 Apr 2024 17:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15758v2</guid></item><item><title>InstructIE: A Bilingual Instruction-based Information Extraction Dataset</title><link>http://arxiv.org/abs/2305.11527v3</link><description>Large language models can perform well on general natural language tasks, buttheir effectiveness is still not optimal for information extraction. Recentworks indicate that the main reason lies in the lack of extensive data oninformation extraction instructions. Note that the existing datasets oninformation extraction instructions not only have limited coverage but alsoinvolve high construction costs. To address this issue, we introduceInstructIE, a bilingual instruction-based information extraction dataset, whichcovers 12 diverse domains. Specifically, we propose KG2Instruction, a frameworkspecifically for the automatic generation of such datasets. Experimentalresults demonstrate that large language models trained with InstructIE can notonly obtain better information extraction capabilities but also enhancezero-shot performance compared with baselines.</description><author>Honghao Gui, Shuofei Qiao, Jintian Zhang, Hongbin Ye, Mengshu Sun, Lei Liang, Jeff Z. Pan, Huajun Chen, Ningyu Zhang</author><pubDate>Thu, 18 Apr 2024 17:20:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11527v3</guid></item><item><title>When Medical Imaging Met Self-Attention: A Love Story That Didn't Quite Work Out</title><link>http://arxiv.org/abs/2404.12295v1</link><description>A substantial body of research has focused on developing systems that assistmedical professionals during labor-intensive early screening processes, manybased on convolutional deep-learning architectures. Recently, multiple studiesexplored the application of so-called self-attention mechanisms in the visiondomain. These studies often report empirical improvements over fullyconvolutional approaches on various datasets and tasks. To evaluate this trendfor medical imaging, we extend two widely adopted convolutional architectureswith different self-attention variants on two different medical datasets. Withthis, we aim to specifically evaluate the possible advantages of additionalself-attention. We compare our models with similarly sized convolutional andattention-based baselines and evaluate performance gains statistically.Additionally, we investigate how including such layers changes the featureslearned by these models during the training. Following a hyperparameter search,and contrary to our expectations, we observe no significant improvement inbalanced accuracy over fully convolutional models. We also find that importantfeatures, such as dermoscopic structures in skin lesion images, are still notlearned by employing self-attention. Finally, analyzing local explanations, weconfirm biased feature usage. We conclude that merely incorporating attentionis insufficient to surpass the performance of existing fully convolutionalmethods.</description><author>Tristan Piater, Niklas Penzel, Gideon Stein, Joachim Denzler</author><pubDate>Thu, 18 Apr 2024 17:18:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12295v1</guid></item><item><title>floZ: Evidence estimation from posterior samples with normalizing flows</title><link>http://arxiv.org/abs/2404.12294v1</link><description>We propose a novel method (floZ), based on normalizing flows, for estimatingthe Bayesian evidence (and its numerical uncertainty) from a set of samplesdrawn from the unnormalized posterior distribution. We validate it ondistributions whose evidence is known analytically, up to 15 parameter spacedimensions, and compare with two state-of-the-art techniques for estimating theevidence: nested sampling (which computes the evidence as its main target) anda k-nearest-neighbors technique that produces evidence estimates from posteriorsamples. Provided representative samples from the target posterior areavailable, our method is more robust to posterior distributions with sharpfeatures, especially in higher dimensions. It has wide applicability, e.g., toestimate the evidence from variational inference, Markov-chain Monte Carlosamples, or any other method that delivers samples from the unnormalizedposterior density.</description><author>Rahul Srinivasan, Marco Crisostomi, Roberto Trotta, Enrico Barausse, Matteo Breschi</author><pubDate>Thu, 18 Apr 2024 17:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12294v1</guid></item><item><title>MetaStackVis: Visually-Assisted Performance Evaluation of Metamodels</title><link>http://arxiv.org/abs/2212.03539v3</link><description>Stacking (or stacked generalization) is an ensemble learning method with onemain distinctiveness from the rest: even though several base models are trainedon the original data set, their predictions are further used as input data forone or more metamodels arranged in at least one extra layer. Composing a stackof models can produce high-performance outcomes, but it usually involves atrial-and-error process. Therefore, our previously developed visual analyticssystem, StackGenVis, was mainly designed to assist users in choosing a set oftop-performing and diverse models by measuring their predictive performance.However, it only employs a single logistic regression metamodel. In this paper,we investigate the impact of alternative metamodels on the performance ofstacking ensembles using a novel visualization tool, called MetaStackVis. Ourinteractive tool helps users to visually explore different singular and pairsof metamodels according to their predictive probabilities and multiplevalidation metrics, as well as their ability to predict specific problematicdata instances. MetaStackVis was evaluated with a usage scenario based on amedical data set and via expert interviews.</description><author>Ilya Ploshchik, Angelos Chatzimparmpas, Andreas Kerren</author><pubDate>Thu, 18 Apr 2024 17:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.03539v3</guid></item><item><title>Singular-limit analysis of gradient descent with noise injection</title><link>http://arxiv.org/abs/2404.12293v1</link><description>We study the limiting dynamics of a large class of noisy gradient descentsystems in the overparameterized regime. In this regime the set of globalminimizers of the loss is large, and when initialized in a neighbourhood ofthis zero-loss set a noisy gradient descent algorithm slowly evolves along thisset. In some cases this slow evolution has been related to bettergeneralisation properties. We characterize this evolution for the broad classof noisy gradient descent systems in the limit of small step size. Our resultsshow that the structure of the noise affects not just the form of the limitingprocess, but also the time scale at which the evolution takes place. We applythe theory to Dropout, label noise and classical SGD (minibatching) noise, andshow that these evolve on different two time scales. Classical SGD even yieldsa trivial evolution on both time scales, implying that additional noise isrequired for regularization. The results are inspired by the training of neuralnetworks, but the theorems apply to noisy gradient descent of any loss that hasa non-trivial zero-loss set.</description><author>Anna Shalova, André Schlichting, Mark Peletier</author><pubDate>Thu, 18 Apr 2024 17:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12293v1</guid></item><item><title>Reducing Bias in Pre-trained Models by Tuning while Penalizing Change</title><link>http://arxiv.org/abs/2404.12292v1</link><description>Deep models trained on large amounts of data often incorporate implicitbiases present during training time. If later such a bias is discovered duringinference or deployment, it is often necessary to acquire new data and retrainthe model. This behavior is especially problematic in critical areas such asautonomous driving or medical decision-making. In these scenarios, new data isoften expensive and hard to come by. In this work, we present a method based onchange penalization that takes a pre-trained model and adapts the weights tomitigate a previously detected bias. We achieve this by tuning azero-initialized copy of a frozen pre-trained network. Our method needs veryfew, in extreme cases only a single, examples that contradict the bias toincrease performance. Additionally, we propose an early stopping criterion tomodify baselines and reduce overfitting. We evaluate our approach on awell-known bias in skin lesion classification and three other datasets from thedomain shift literature. We find that our approach works especially well withvery few images. Simple fine-tuning combined with our early stopping also leadsto performance benefits for a larger number of tuning samples.</description><author>Niklas Penzel, Gideon Stein, Joachim Denzler</author><pubDate>Thu, 18 Apr 2024 17:12:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12292v1</guid></item><item><title>Augmenting emotion features in irony detection with Large language modeling</title><link>http://arxiv.org/abs/2404.12291v1</link><description>This study introduces a novel method for irony detection, applying LargeLanguage Models (LLMs) with prompt-based learning to facilitate emotion-centrictext augmentation. Traditional irony detection techniques typically fall shortdue to their reliance on static linguistic features and predefined knowledgebases, often overlooking the nuanced emotional dimensions integral to irony. Incontrast, our methodology augments the detection process by integrating subtleemotional cues, augmented through LLMs, into three benchmark pre-trained NLPmodels - BERT, T5, and GPT-2 - which are widely recognized as foundational inirony detection. We assessed our method using the SemEval-2018 Task 3 datasetand observed substantial enhancements in irony detection capabilities.</description><author>Yucheng Lin, Yuhan Xia, Yunfei Long</author><pubDate>Thu, 18 Apr 2024 17:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12291v1</guid></item><item><title>Debiased Distribution Compression</title><link>http://arxiv.org/abs/2404.12290v1</link><description>Modern compression methods can summarize a target distribution $\mathbb{P}$more succinctly than i.i.d. sampling but require access to a low-bias inputsequence like a Markov chain converging quickly to $\mathbb{P}$. We introduce anew suite of compression methods suitable for compression with biased inputsequences. Given $n$ points targeting the wrong distribution and quadratictime, Stein Kernel Thinning (SKT) returns $\sqrt{n}$ equal-weighted points with$\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\mathbb {P}$. Forlarger-scale compression tasks, Low-rank SKT achieves the same feat insub-quadratic time using an adaptive low-rank debiasing procedure that may beof independent interest. For downstream tasks that support simplex orconstant-preserving weights, Stein Recombination and Stein Cholesky achieveeven greater parsimony, matching the guarantees of SKT with as few as$\operatorname{poly-log}(n)$ weighted points. Underlying these advances are newguarantees for the quality of simplex-weighted coresets, the spectral decay ofkernel matrices, and the covering numbers of Stein kernel Hilbert spaces. Inour experiments, our techniques provide succinct and accurate posteriorsummaries while overcoming biases due to burn-in, approximate Markov chainMonte Carlo, and tempering.</description><author>Lingxiao Li, Raaz Dwivedi, Lester Mackey</author><pubDate>Thu, 18 Apr 2024 17:11:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12290v1</guid></item><item><title>Resilience through Scene Context in Visual Referring Expression Generation</title><link>http://arxiv.org/abs/2404.12289v1</link><description>Scene context is well known to facilitate humans' perception of visibleobjects. In this paper, we investigate the role of context in ReferringExpression Generation (REG) for objects in images, where existing research hasoften focused on distractor contexts that exert pressure on the generator. Wetake a new perspective on scene context in REG and hypothesize that contextualinformation can be conceived of as a resource that makes REG models moreresilient and facilitates the generation of object descriptions, and objecttypes in particular. We train and test Transformer-based REG models with targetrepresentations that have been artificially obscured with noise to varyingdegrees. We evaluate how properties of the models' visual context affect theirprocessing and performance. Our results show that even simple scene contextsmake models surprisingly resilient to perturbations, to the extent that theycan identify referent types even when visual information about the target iscompletely missing.</description><author>Simeon Junker, Sina Zarrieß</author><pubDate>Thu, 18 Apr 2024 17:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12289v1</guid></item><item><title>Performance Evaluation of Segment Anything Model with Variational Prompting for Application to Non-Visible Spectrum Imagery</title><link>http://arxiv.org/abs/2404.12285v1</link><description>The Segment Anything Model (SAM) is a deep neural network foundational modeldesigned to perform instance segmentation which has gained significantpopularity given its zero-shot segmentation ability. SAM operates by generatingmasks based on various input prompts such as text, bounding boxes, points, ormasks, introducing a novel methodology to overcome the constraints posed bydataset-specific scarcity. While SAM is trained on an extensive dataset,comprising ~11M images, it mostly consists of natural photographic images withonly very limited images from other modalities. Whilst the rapid progress invisual infrared surveillance and X-ray security screening imaging technologies,driven forward by advances in deep learning, has significantly enhanced theability to detect, classify and segment objects with high accuracy, it is notevident if the SAM zero-shot capabilities can be transferred to suchmodalities. This work assesses SAM capabilities in segmenting objects ofinterest in the X-ray/infrared modalities. Our approach reuses the pre-trainedSAM with three different prompts: bounding box, centroid and random points. Wepresent quantitative/qualitative results to showcase the performance onselected datasets. Our results show that SAM can segment objects in the X-raymodality when given a box prompt, but its performance varies for point prompts.Specifically, SAM performs poorly in segmenting slender objects and organicmaterials, such as plastic bottles. We find that infrared objects are alsochallenging to segment with point prompts given the low-contrast nature of thismodality. This study shows that while SAM demonstrates outstanding zero-shotcapabilities with box prompts, its performance ranges from moderate to poor forpoint prompts, indicating that special consideration on the cross-modalgeneralisation of SAM is needed when considering use on X-ray/infrared imagery.</description><author>Yona Falinie A. Gaus, Neelanjan Bhowmik, Brian K. S. Isaac-Medina, Toby P. Breckon</author><pubDate>Thu, 18 Apr 2024 17:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12285v1</guid></item><item><title>t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections</title><link>http://arxiv.org/abs/2002.06910v5</link><description>t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization ofmultidimensional data has proven to be a popular approach, with successfulapplications in a wide range of domains. Despite their usefulness, t-SNEprojections can be hard to interpret or even misleading, which hurts thetrustworthiness of the results. Understanding the details of t-SNE itself andthe reasons behind specific patterns in its output may be a daunting task,especially for non-experts in dimensionality reduction. In this work, wepresent t-viSNE, an interactive tool for the visual exploration of t-SNEprojections that enables analysts to inspect different aspects of theiraccuracy and meaning, such as the effects of hyper-parameters, distance andneighborhood preservation, densities and costs of specific neighborhoods, andthe correlations between dimensions and visual patterns. We propose a coherent,accessible, and well-integrated collection of different views for thevisualization of t-SNE projections. The applicability and usability of t-viSNEare demonstrated through hypothetical usage scenarios with real data sets.Finally, we present the results of a user study where the tool's effectivenesswas evaluated. By bringing to light information that would normally be lostafter running t-SNE, we hope to support analysts in using t-SNE and making itsresults better understandable.</description><author>Angelos Chatzimparmpas, Rafael M. Martins, Andreas Kerren</author><pubDate>Thu, 18 Apr 2024 17:03:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2002.06910v5</guid></item><item><title>StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics</title><link>http://arxiv.org/abs/2005.01575v9</link><description>In machine learning (ML), ensemble methods such as bagging, boosting, andstacking are widely-established approaches that regularly achieve top-notchpredictive performance. Stacking (also called "stacked generalization") is anensemble method that combines heterogeneous base models, arranged in at leastone layer, and then employs another metamodel to summarize the predictions ofthose models. Although it may be a highly-effective approach for increasing thepredictive performance of ML, generating a stack of models from scratch can bea cumbersome trial-and-error process. This challenge stems from the enormousspace of available solutions, with different sets of data instances andfeatures that could be used for training, several algorithms to choose from,and instantiations of these algorithms using diverse parameters (i.e., models)that perform differently according to various metrics. In this work, we presenta knowledge generation model, which supports ensemble learning with the use ofvisualization, and a visual analytics system for stacked generalization. Oursystem, StackGenVis, assists users in dynamically adapting performance metrics,managing data instances, selecting the most important features for a given dataset, choosing a set of top-performant and diverse algorithms, and measuring thepredictive performance. In consequence, our proposed tool helps users to decidebetween distinct models and to reduce the complexity of the resulting stack byremoving overpromising and underperforming models. The applicability andeffectiveness of StackGenVis are demonstrated with two use cases: a real-worldhealthcare data set and a collection of data related to sentiment/stancedetection in texts. Finally, the tool has been evaluated through interviewswith three ML experts.</description><author>Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas Kerren</author><pubDate>Thu, 18 Apr 2024 17:02:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.01575v9</guid></item><item><title>FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches</title><link>http://arxiv.org/abs/2103.14539v4</link><description>The machine learning (ML) life cycle involves a series of iterative steps,from the effective gathering and preparation of the data, including complexfeature engineering processes, to the presentation and improvement of results,with various algorithms to choose from in every step. Feature engineering inparticular can be very beneficial for ML, leading to numerous improvements suchas boosting the predictive results, decreasing computational times, reducingexcessive noise, and increasing the transparency behind the decisions takenduring the training. Despite that, while several visual analytics tools existto monitor and control the different stages of the ML life cycle (especiallythose related to data and algorithms), feature engineering support remainsinadequate. In this paper, we present FeatureEnVi, a visual analytics systemspecifically designed to assist with the feature engineering process. Ourproposed system helps users to choose the most important feature, to transformthe original features into powerful alternatives, and to experiment withdifferent feature generation combinations. Additionally, data space slicingallows users to explore the impact of features on both local and global scales.FeatureEnVi utilizes multiple automatic feature selection techniques;furthermore, it visually guides users with statistical evidence about theinfluence of each feature (or subsets of features). The final outcome is theextraction of heavily engineered features, evaluated by multiple validationmetrics. The usefulness and applicability of FeatureEnVi are demonstrated withtwo use cases and a case study. We also report feedback from interviews withtwo ML experts and a visualization researcher who assessed the effectiveness ofour system.</description><author>Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas Kerren</author><pubDate>Thu, 18 Apr 2024 17:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.14539v4</guid></item><item><title>Enhancing Embedding Performance through Large Language Model-based Text Enrichment and Rewriting</title><link>http://arxiv.org/abs/2404.12283v1</link><description>Embedding models are crucial for various natural language processing tasksbut can be limited by factors such as limited vocabulary, lack of context, andgrammatical errors. This paper proposes a novel approach to improve embeddingperformance by leveraging large language models (LLMs) to enrich and rewriteinput text before the embedding process. By utilizing ChatGPT 3.5 to provideadditional context, correct inaccuracies, and incorporate metadata, theproposed method aims to enhance the utility and accuracy of embedding models.The effectiveness of this approach is evaluated on three datasets:Banking77Classification, TwitterSemEval 2015, and Amazon Counter-factualClassification. Results demonstrate significant improvements over the baselinemodel on the TwitterSemEval 2015 dataset, with the best-performing promptachieving a score of 85.34 compared to the previous best of 81.52 on theMassive Text Embedding Benchmark (MTEB) Leaderboard. However, performance onthe other two datasets was less impressive, highlighting the importance ofconsidering domain-specific characteristics. The findings suggest thatLLM-based text enrichment has shown promising results to improve embeddingperformance, particularly in certain domains. Hence, numerous limitations inthe process of embedding can be avoided.</description><author>Nicholas Harris, Anand Butani, Syed Hashmy</author><pubDate>Thu, 18 Apr 2024 16:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12283v1</guid></item><item><title>Investigating Guiding Information for Adaptive Collocation Point Sampling in PINNs</title><link>http://arxiv.org/abs/2404.12282v1</link><description>Physics-informed neural networks (PINNs) provide a means of obtainingapproximate solutions of partial differential equations and systems through theminimisation of an objective function which includes the evaluation of aresidual function at a set of collocation points within the domain. The qualityof a PINNs solution depends upon numerous parameters, including the number anddistribution of these collocation points. In this paper we consider a number ofstrategies for selecting these points and investigate their impact on theoverall accuracy of the method. In particular, we suggest that no singleapproach is likely to be ``optimal'' but we show how a number of importantmetrics can have an impact in improving the quality of the results obtainedwhen using a fixed number of residual evaluations. We illustrate theseapproaches through the use of two benchmark test problems: Burgers' equationand the Allen-Cahn equation.</description><author>Jose Florido, He Wang, Amirul Khan, Peter K. Jimack</author><pubDate>Thu, 18 Apr 2024 16:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12282v1</guid></item><item><title>Struggle with Adversarial Defense? Try Diffusion</title><link>http://arxiv.org/abs/2404.08273v2</link><description>Adversarial attacks induce misclassification by introducing subtleperturbations. Recently, diffusion models are applied to the image classifiersto improve adversarial robustness through adversarial training or by purifyingadversarial noise. However, diffusion-based adversarial training oftenencounters convergence challenges and high computational expenses.Additionally, diffusion-based purification inevitably causes data shift and isdeemed susceptible to stronger adaptive attacks. To tackle these issues, wepropose the Truth Maximization Diffusion Classifier (TMDC), a generativeBayesian classifier that builds upon pre-trained diffusion models and theBayesian theorem. Unlike data-driven classifiers, TMDC, guided by Bayesianprinciples, utilizes the conditional likelihood from diffusion models todetermine the class probabilities of input images, thereby insulating againstthe influences of data shift and the limitations of adversarial training.Moreover, to enhance TMDC's resilience against more potent adversarial attacks,we propose an optimization strategy for diffusion classifiers. This strategyinvolves post-training the diffusion model on perturbed datasets withground-truth labels as conditions, guiding the diffusion model to learn thedata distribution and maximizing the likelihood under the ground-truth labels.The proposed method achieves state-of-the-art performance on the CIFAR10dataset against heavy white-box attacks and strong adaptive attacks.Specifically, TMDC achieves robust accuracies of 82.81% against $l_{\infty}$norm-bounded perturbations and 86.05% against $l_{2}$ norm-boundedperturbations, respectively, with $\epsilon=0.05$.</description><author>Yujie Li, Yanbin Wang, Haitao Xu, Bin Liu, Jianguo Sun, Zhenhao Guo, Wenrui Ma</author><pubDate>Thu, 18 Apr 2024 16:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08273v2</guid></item><item><title>DF-DM: A foundational process model for multimodal data fusion in the artificial intelligence era</title><link>http://arxiv.org/abs/2404.12278v1</link><description>In the big data era, integrating diverse data modalities poses significantchallenges, particularly in complex fields like healthcare. This paperintroduces a new process model for multimodal Data Fusion for Data Mining,integrating embeddings and the Cross-Industry Standard Process for Data Miningwith the existing Data Fusion Information Group model. Our model aims todecrease computational costs, complexity, and bias while improving efficiencyand reliability. We also propose "disentangled dense fusion", a novel embeddingfusion method designed to optimize mutual information and facilitate denseinter-modality feature interaction, thereby minimizing redundant information. We demonstrate the model's efficacy through three use cases: predictingdiabetic retinopathy using retinal images and patient metadata, domesticviolence prediction employing satellite imagery, internet, and census data, andidentifying clinical and demographic features from radiography images andclinical notes. The model achieved a Macro F1 score of 0.92 in diabeticretinopathy prediction, an R-squared of 0.854 and sMAPE of 24.868 in domesticviolence prediction, and a macro AUC of 0.92 and 0.99 for disease predictionand sex classification, respectively, in radiological analysis. These results underscore the Data Fusion for Data Mining model's potential tosignificantly impact multimodal data processing, promoting its adoption indiverse, resource-constrained settings.</description><author>David Restrepo, Chenwei Wu, Constanza Vásquez-Venegas, Luis Filipe Nakayama, Leo Anthony Celi, Diego M López</author><pubDate>Thu, 18 Apr 2024 16:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12278v1</guid></item><item><title>A new dataset for measuring the performance of blood vessel segmentation methods under distribution shifts</title><link>http://arxiv.org/abs/2301.04517v4</link><description>Creating a dataset for training supervised machine learning algorithms can bea demanding task. This is especially true for medical image segmentation sinceone or more specialists are usually required for image annotation, and creatingground truth labels for just a single image can take up to several hours. Inaddition, it is paramount that the annotated samples represent well thedifferent conditions that might affect the imaged tissues as well as possiblechanges in the image acquisition process. This can only be achieved byconsidering samples that are typical in the dataset as well as atypical, oreven outlier, samples. We introduce VessMAP, a heterogeneous blood vesselsegmentation dataset acquired by carefully sampling relevant images from alarger non-annotated dataset. A methodology was developed to select bothprototypical and atypical samples from the base dataset, thus defining anassorted set of images that can be used for measuring the performance ofsegmentation algorithms on samples that are highly distinct from each other. Todemonstrate the potential of the new dataset, we show that the validationperformance of a neural network changes significantly depending on the splitsused for training the network.</description><author>Matheus Viana da Silva, Natália de Carvalho Santos, Julie Ouellette, Baptiste Lacoste, Cesar Henrique Comin</author><pubDate>Thu, 18 Apr 2024 16:50:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04517v4</guid></item><item><title>Advancing the Robustness of Large Language Models through Self-Denoised Smoothing</title><link>http://arxiv.org/abs/2404.12274v1</link><description>Although large language models (LLMs) have achieved significant success,their vulnerability to adversarial perturbations, including recent jailbreakattacks, has raised considerable concerns. However, the increasing size ofthese models and their limited access make improving their robustness achallenging task. Among various defense strategies, randomized smoothing hasshown great potential for LLMs, as it does not require full access to themodel's parameters or fine-tuning via adversarial training. However, randomizedsmoothing involves adding noise to the input before model prediction, and thefinal model's robustness largely depends on the model's performance on thesenoise corrupted data. Its effectiveness is often limited by the model'ssub-optimal performance on noisy data. To address this issue, we propose toleverage the multitasking nature of LLMs to first denoise the noisy inputs andthen to make predictions based on these denoised versions. We call thisprocedure self-denoised smoothing. Unlike previous denoised smoothingtechniques in computer vision, which require training a separate model toenhance the robustness of LLMs, our method offers significantly betterefficiency and flexibility. Our experimental results indicate that our methodsurpasses existing methods in both empirical and certified robustness indefending against adversarial attacks for both downstream tasks and humanalignments (i.e., jailbreak attacks). Our code is publicly available athttps://github.com/UCSB-NLP-Chang/SelfDenoise</description><author>Jiabao Ji, Bairu Hou, Zhen Zhang, Guanhua Zhang, Wenqi Fan, Qing Li, Yang Zhang, Gaowen Liu, Sijia Liu, Shiyu Chang</author><pubDate>Thu, 18 Apr 2024 16:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12274v1</guid></item><item><title>FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom</title><link>http://arxiv.org/abs/2404.12273v1</link><description>Federated Learning (FL) has emerged as a promising solution for collaborativetraining of large language models (LLMs). However, the integration of LLMs intoFL introduces new challenges, particularly concerning the evaluation of LLMs.Traditional evaluation methods that rely on labeled test sets andsimilarity-based metrics cover only a subset of the acceptable answers, therebyfailing to accurately reflect the performance of LLMs on generative tasks.Meanwhile, although automatic evaluation methods that leverage advanced LLMspresent potential, they face critical risks of data leakage due to the need totransmit data to external servers and suboptimal performance on downstreamtasks due to the lack of domain knowledge. To address these issues, we proposea Federated Evaluation framework of Large Language Models, named FedEval-LLM,that provides reliable performance measurements of LLMs on downstream taskswithout the reliance on labeled test sets and external tools, thus ensuringstrong privacy-preserving capability. FedEval-LLM leverages a consortium ofpersonalized LLMs from participants as referees to provide domain knowledge andcollective evaluation capability, thus aligning to the respective downstreamtasks and mitigating uncertainties and biases associated with a single referee.Experimental results demonstrate a significant improvement in the evaluationcapability of personalized evaluation models on downstream tasks. When appliedto FL, these evaluation models exhibit strong agreement with human preferenceand RougeL-score on meticulously curated test sets. FedEval-LLM effectivelyovercomes the limitations of traditional metrics and the reliance on externalservices, making it a promising framework for the evaluation of LLMs withincollaborative training scenarios.</description><author>Yuanqin He, Yan Kang, Lixin Fan, Qiang Yang</author><pubDate>Thu, 18 Apr 2024 16:46:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12273v1</guid></item><item><title>Can We Edit Multimodal Large Language Models?</title><link>http://arxiv.org/abs/2310.08475v5</link><description>In this paper, we focus on editing Multimodal Large Language Models (MLLMs).Compared to editing single-modal LLMs, multimodal model editing is morechallenging, which demands a higher level of scrutiny and careful considerationin the editing process. To facilitate research in this area, we construct a newbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suiteof innovative metrics for evaluation. We conduct comprehensive experimentsinvolving various model editing baselines and analyze the impact of editingdifferent components for multimodal LLMs. Empirically, we notice that previousbaselines can implement editing multimodal LLMs to some extent, but the effectis still barely satisfactory, indicating the potential difficulty of this task.We hope that our work can provide the NLP community with insights. Code anddataset are available in https://github.com/zjunlp/EasyEdit.</description><author>Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, Ningyu Zhang</author><pubDate>Thu, 18 Apr 2024 16:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08475v5</guid></item><item><title>Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</title><link>http://arxiv.org/abs/2404.12272v1</link><description>Due to the cumbersome nature of human evaluation and limitations ofcode-based evaluation, Large Language Models (LLMs) are increasingly being usedto assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simplyinherit all the problems of the LLMs they evaluate, requiring further humanvalidation. We present a mixed-initiative approach to ``validate thevalidators'' -- aligning LLM-generated evaluation functions (be it prompts orcode) with human requirements. Our interface, EvalGen, provides automatedassistance to users in generating evaluation criteria and implementingassertions. While generating candidate implementations (Python functions, LLMgrader prompts), EvalGen asks humans to grade a subset of LLM outputs; thisfeedback is used to select implementations that better align with user grades.A qualitative study finds overall support for EvalGen but underscores thesubjectivity and iterative process of alignment. In particular, we identify aphenomenon we dub \emph{criteria drift}: users need criteria to grade outputs,but grading outputs helps users define criteria. What is more, some criteriaappears \emph{dependent} on the specific LLM outputs observed (rather thanindependent criteria that can be defined \emph{a priori}), raising seriousquestions for approaches that assume the independence of evaluation fromobservation of model outputs. We present our interface and implementationdetails, a comparison of our algorithm with a baseline approach, andimplications for the design of future LLM evaluation assistants.</description><author>Shreya Shankar, J. D. Zamfirescu-Pereira, Björn Hartmann, Aditya G. Parameswaran, Ian Arawjo</author><pubDate>Thu, 18 Apr 2024 16:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12272v1</guid></item><item><title>Online Advertisements with LLMs: Opportunities and Challenges</title><link>http://arxiv.org/abs/2311.07601v3</link><description>This paper explores the potential for leveraging Large Language Models (LLM)in the realm of online advertising systems. We delve into essentialrequirements including privacy, latency, reliability as well as thesatisfaction of users and advertisers that such a system must fulfill. Wefurther introduce a general framework for LLM advertisement, consisting ofmodification, bidding, prediction, and auction modules. Different designconsiderations for each module are presented. Fundamental questions regardingpracticality, efficiency, and implementation challenges of these designs areraised for future research. Finally, we explore the prospect of LLM-baseddynamic creative optimization as a means to significantly enhance the appeal ofadvertisements to users and discuss its additional challenges.</description><author>Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei, Suho Shin</author><pubDate>Thu, 18 Apr 2024 16:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07601v3</guid></item><item><title>How Population Diversity Influences the Efficiency of Crossover</title><link>http://arxiv.org/abs/2404.12268v1</link><description>Our theoretical understanding of crossover is limited by our ability toanalyze how population diversity evolves. In this study, we provide one of thefirst rigorous analyses of population diversity and optimization time in asetting where large diversity and large population sizes are required to speedup progress. We give a formal and general criterion which amount of diversityis necessary and sufficient to speed up the $(\mu+1)$ Genetic Algorithm onLeadingOnes. We show that the naturally evolving diversity falls short ofgiving a substantial speed-up for any $\mu=O(\sqrt{n}/\log^2 n)$. On the otherhand, we show that even for $\mu=2$, if we simply break ties in favor ofdiversity then this increases diversity so much that optimization isaccelerated by a constant factor.</description><author>Sacha Cerf, Johannes Lengler</author><pubDate>Thu, 18 Apr 2024 16:41:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12268v1</guid></item><item><title>Physics-integrated generative modeling using attentive planar normalizing flow based variational autoencoder</title><link>http://arxiv.org/abs/2404.12267v1</link><description>Physics-integrated generative modeling is a class of hybrid or grey-boxmodeling in which we augment the the data-driven model with the physicsknowledge governing the data distribution. The use of physics knowledge allowsthe generative model to produce output in a controlled way, so that the output,by construction, complies with the physical laws. It imparts improvedgeneralization ability to extrapolate beyond the training distribution as wellas improved interpretability because the model is partly grounded in firmdomain knowledge. In this work, we aim to improve the fidelity ofreconstruction and robustness to noise in the physics integrated generativemodel. To this end, we use variational-autoencoder as a generative model. Toimprove the reconstruction results of the decoder, we propose to learn thelatent posterior distribution of both the physics as well as the trainabledata-driven components using planar normalizng flow. Normalizng flow basedposterior distribution harnesses the inherent dynamical structure of the datadistribution, hence the learned model gets closer to the true underlying datadistribution. To improve the robustness of generative model against noiseinjected in the model, we propose a modification in the encoder part of thenormalizing flow based VAE. We designed the encoder to incorporate scaled dotproduct attention based contextual information in the noisy latent vector whichwill mitigate the adverse effect of noise in the latent vector and make themodel more robust. We empirically evaluated our models on human locomotiondataset [33] and the results validate the efficacy of our proposed models interms of improvement in reconstruction quality as well as robustness againstnoise injected in the model.</description><author>Sheikh Waqas Akhtar</author><pubDate>Thu, 18 Apr 2024 16:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12267v1</guid></item><item><title>Exposing Image Splicing Traces in Scientific Publications via Uncertainty-guided Refinement</title><link>http://arxiv.org/abs/2309.16388v2</link><description>Recently, a surge in scientific publications suspected of image manipulationhas led to numerous retractions, bringing the issue of image integrity intosharp focus. Although research on forensic detectors for image plagiarism andimage synthesis exists, the detection of image splicing traces in scientificpublications remains unexplored. Compared to image duplication and synthesis,image splicing detection is more challenging due to the lack of referenceimages and the typically small tampered areas. Furthermore, disruptive factorsin scientific images, such as artifacts from digital compression, abnormalpatterns, and noise from physical operations, present misleading features likesplicing traces, significantly increasing the difficulty of this task.Moreover, the scarcity of high-quality datasets of spliced scientific imageslimits potential advancements. In this work, we propose an Uncertainty-guidedRefinement Network (URN) to mitigate the impact of these disruptive factors.Our URN can explicitly suppress the propagation of unreliable information flowcaused by disruptive factors between regions, thus obtaining robust splicingfeatures. Additionally, the URN is designed to concentrate improvements inuncertain prediction areas during the decoding phase. We also construct adataset for image splicing detection (SciSp) containing 1,290 spliced images.Compared to existing datasets, SciSp includes the largest number of splicedimages and the most diverse sources. Comprehensive experiments conducted onthree benchmark datasets demonstrate the superiority of our approach. We alsovalidate the URN's generalisability in resisting cross-dataset domain shiftsand its robustness against various post-processing techniques, includingadvanced deep-learning-based inpainting.</description><author>Xun Lin, Wenzhong Tang, Haoran Wang, Yizhong Liu, Yakun Ju, Shuai Wang, Zitong Yu</author><pubDate>Thu, 18 Apr 2024 16:32:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16388v2</guid></item><item><title>A Survey on the Densest Subgraph Problem and Its Variants</title><link>http://arxiv.org/abs/2303.14467v2</link><description>The Densest Subgraph Problem requires to find, in a given graph, a subset ofvertices whose induced subgraph maximizes a measure of density. The problem hasreceived a great deal of attention in the algorithmic literature since theearly 1970s, with many variants proposed and many applications built on top ofthis basic definition. Recent years have witnessed a revival of researchinterest in this problem with several important contributions, including somegroundbreaking results, published in 2022 and 2023. This survey provides a deepoverview of the fundamental results and an exhaustive coverage of the manyvariants proposed in the literature, with a special attention to the mostrecent results. The survey also presents a comprehensive overview ofapplications and discusses some interesting open problems for this evergreenresearch topic.</description><author>Tommaso Lanciano, Atsushi Miyauchi, Adriano Fazzone, Francesco Bonchi</author><pubDate>Thu, 18 Apr 2024 16:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14467v2</guid></item><item><title>State Space Models for Event Cameras</title><link>http://arxiv.org/abs/2402.15584v3</link><description>Today, state-of-the-art deep neural networks that process event-camera datafirst convert a temporal window of events into dense, grid-like inputrepresentations. As such, they exhibit poor generalizability when deployed athigher inference frequencies (i.e., smaller temporal windows) than the onesthey were trained on. We address this challenge by introducing state-spacemodels (SSMs) with learnable timescale parameters to event-based vision. Thisdesign adapts to varying frequencies without the need to retrain the network atdifferent frequencies. Additionally, we investigate two strategies tocounteract aliasing effects when deploying the model at higher frequencies. Wecomprehensively evaluate our approach against existing methods based on RNN andTransformer architectures across various benchmarks, including Gen1 and 1 Mpxevent camera datasets. Our results demonstrate that SSM-based models train 33%faster and also exhibit minimal performance degradation when tested at higherfrequencies than the training input. Traditional RNN and Transformer modelsexhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.76mAP, highlighting the effectiveness of SSMs in event-based vision tasks.</description><author>Nikola Zubić, Mathias Gehrig, Davide Scaramuzza</author><pubDate>Thu, 18 Apr 2024 16:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15584v3</guid></item><item><title>Alleviating Catastrophic Forgetting in Facial Expression Recognition with Emotion-Centered Models</title><link>http://arxiv.org/abs/2404.12260v1</link><description>Facial expression recognition is a pivotal component in machine learning,facilitating various applications. However, convolutional neural networks(CNNs) are often plagued by catastrophic forgetting, impeding theiradaptability. The proposed method, emotion-centered generative replay (ECgr),tackles this challenge by integrating synthetic images from generativeadversarial networks. Moreover, ECgr incorporates a quality assurance algorithmto ensure the fidelity of generated images. This dual approach enables CNNs toretain past knowledge while learning new tasks, enhancing their performance inemotion recognition. The experimental results on four diverse facial expressiondatasets demonstrate that incorporating images generated by ourpseudo-rehearsal method enhances training on the targeted dataset and thesource dataset while making the CNN retain previously learned knowledge.</description><author>Israel A. Laurensi, Alceu de Souza Britto Jr., Jean Paul Barddal, Alessandro Lameiras Koerich</author><pubDate>Thu, 18 Apr 2024 16:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12260v1</guid></item><item><title>Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM</title><link>http://arxiv.org/abs/2404.12259v1</link><description>Data analysts have long sought to turn unstructured text data into meaningfulconcepts. Though common, topic modeling and clustering focus on lower-levelkeywords and require significant interpretative work. We introduce conceptinduction, a computational process that instead produces high-level concepts,defined by explicit inclusion criteria, from unstructured text. For a datasetof toxic online comments, where a state-of-the-art BERTopic model outputs"women, power, female," concept induction produces high-level concepts such as"Criticism of traditional gender roles" and "Dismissal of women's concerns." Wepresent LLooM, a concept induction algorithm that leverages large languagemodels to iteratively synthesize sampled text and propose human-interpretableconcepts of increasing generality. We then instantiate LLooM in amixed-initiative text analysis tool, enabling analysts to shift their attentionfrom interpreting topics to engaging in theory-driven analysis. Throughtechnical evaluations and four analysis scenarios ranging from literaturereview to content moderation, we find that LLooM's concepts improve upon theprior art of topic models in terms of quality and data coverage. In expert casestudies, LLooM helped researchers to uncover new insights even from familiardatasets, for example by suggesting a previously unnoticed concept of attackson out-party stances in a political social media dataset.</description><author>Michelle S. Lam, Janice Teoh, James Landay, Jeffrey Heer, Michael S. Bernstein</author><pubDate>Thu, 18 Apr 2024 16:26:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12259v1</guid></item><item><title>DeepLocalization: Using change point detection for Temporal Action Localization</title><link>http://arxiv.org/abs/2404.12258v1</link><description>In this study, we introduce DeepLocalization, an innovative framework devisedfor the real-time localization of actions tailored explicitly for monitoringdriver behavior. Utilizing the power of advanced deep learning methodologies,our objective is to tackle the critical issue of distracted driving-asignificant factor contributing to road accidents. Our strategy employs a dualapproach: leveraging Graph-Based Change-Point Detection for pinpointing actionsin time alongside a Video Large Language Model (Video-LLM) for preciselycategorizing activities. Through careful prompt engineering, we customize theVideo-LLM to adeptly handle driving activities' nuances, ensuring itsclassification efficacy even with sparse data. Engineered to be lightweight,our framework is optimized for consumer-grade GPUs, making it vastly applicablein practical scenarios. We subjected our method to rigorous testing on theSynDD2 dataset, a complex benchmark for distracted driving behaviors, where itdemonstrated commendable performance-achieving 57.5% accuracy in eventclassification and 51% in event detection. These outcomes underscore thesubstantial promise of DeepLocalization in accurately identifying diversedriver behaviors and their temporal occurrences, all within the bounds oflimited computational resources.</description><author>Mohammed Shaiqur Rahman, Ibne Farabi Shihab, Lynna Chu, Anuj Sharma</author><pubDate>Thu, 18 Apr 2024 16:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12258v1</guid></item><item><title>Food Portion Estimation via 3D Object Scaling</title><link>http://arxiv.org/abs/2404.12257v1</link><description>Image-based methods to analyze food images have alleviated the user burdenand biases associated with traditional methods. However, accurate portionestimation remains a major challenge due to the loss of 3D information in the2D representation of foods captured by smartphone cameras or wearable devices.In this paper, we propose a new framework to estimate both food volume andenergy from 2D images by leveraging the power of 3D food models and physicalreference in the eating scene. Our method estimates the pose of the camera andthe food object in the input image and recreates the eating occasion byrendering an image of a 3D model of the food with the estimated poses. We alsointroduce a new dataset, SimpleFood45, which contains 2D images of 45 fooditems and associated annotations including food volume, weight, and energy. Ourmethod achieves an average error of 31.10 kCal (17.67%) on this dataset,outperforming existing portion estimation methods.</description><author>Gautham Vinod, Jiangpeng He, Zeman Shao, Fengqing Zhu</author><pubDate>Thu, 18 Apr 2024 16:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12257v1</guid></item><item><title>An Online Spatial-Temporal Graph Trajectory Planner for Autonomous Vehicles</title><link>http://arxiv.org/abs/2404.12256v1</link><description>The autonomous driving industry is expected to grow by over 20 times in thecoming decade and, thus, motivate researchers to delve into it. The primaryfocus of their research is to ensure safety, comfort, and efficiency. Anautonomous vehicle has several modules responsible for one or more of theaforementioned items. Among these modules, the trajectory planner plays apivotal role in the safety of the vehicle and the comfort of its passengers.The module is also responsible for respecting kinematic constraints and anyapplicable road constraints. In this paper, a novel online spatial-temporalgraph trajectory planner is introduced to generate safe and comfortabletrajectories. First, a spatial-temporal graph is constructed using theautonomous vehicle, its surrounding vehicles, and virtual nodes along the roadwith respect to the vehicle itself. Next, the graph is forwarded into asequential network to obtain the desired states. To support the planner, asimple behavioral layer is also presented that determines kinematic constraintsfor the planner. Furthermore, a novel potential function is also proposed totrain the network. Finally, the proposed planner is tested on three differentcomplex driving tasks, and the performance is compared with two frequently usedmethods. The results show that the proposed planner generates safe and feasibletrajectories while achieving similar or longer distances in the forwarddirection and comparable comfort ride.</description><author>Jilan Samiuddin, Benoit Boulet, Di Wu</author><pubDate>Thu, 18 Apr 2024 16:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12256v1</guid></item><item><title>Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing</title><link>http://arxiv.org/abs/2404.12253v1</link><description>Despite the impressive capabilities of Large Language Models (LLMs) onvarious tasks, they still struggle with scenarios that involves complexreasoning and planning. Recent work proposed advanced prompting techniques andthe necessity of fine-tuning with high-quality data to augment LLMs' reasoningabilities. However, these approaches are inherently constrained by dataavailability and quality. In light of this, self-correction and self-learningemerge as viable solutions, employing strategies that allow LLMs to refinetheir outputs and learn from self-assessed rewards. Yet, the efficacy of LLMsin self-refining its response, particularly in complex reasoning and planningtask, remains dubious. In this paper, we introduce AlphaLLM for theself-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) withLLMs to establish a self-improving loop, thereby enhancing the capabilities ofLLMs without additional annotations. Drawing inspiration from the success ofAlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLMfor self-improvement, including data scarcity, the vastness search spaces oflanguage tasks, and the subjective nature of feedback in language tasks.AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approachtailored for language tasks, and a trio of critic models for precise feedback.Our experimental results in mathematical reasoning tasks demonstrate thatAlphaLLM significantly enhances the performance of LLMs without additionalannotations, showing the potential for self-improvement in LLMs.</description><author>Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, Dong Yu</author><pubDate>Thu, 18 Apr 2024 16:21:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12253v1</guid></item><item><title>Deep Gaussian mixture model for unsupervised image segmentation</title><link>http://arxiv.org/abs/2404.12252v1</link><description>The recent emergence of deep learning has led to a great deal of work ondesigning supervised deep semantic segmentation algorithms. As in many taskssufficient pixel-level labels are very difficult to obtain, we propose a methodwhich combines a Gaussian mixture model (GMM) with unsupervised deep learningtechniques. In the standard GMM the pixel values with each sub-region aremodelled by a Gaussian distribution. In order to identify the differentregions, the parameter vector that minimizes the negative log-likelihood (NLL)function regarding the GMM has to be approximated. For this task, usuallyiterative optimization methods such as the expectation-maximization (EM)algorithm are used. In this paper, we propose to estimate these parametersdirectly from the image using a convolutional neural network (CNN). We thuschange the iterative procedure in the EM algorithm replacing theexpectation-step by a gradient-step with regard to the networks parameters.This means that the network is trained to minimize the NLL function of the GMMwhich comes with at least two advantages. As once trained, the network is ableto predict label probabilities very quickly compared with time consumingiterative optimization methods. Secondly, due to the deep image prior ourmethod is able to partially overcome one of the main disadvantages of GMM,which is not taking into account correlation between neighboring pixels, as itassumes independence between them. We demonstrate the advantages of our methodin various experiments on the example of myocardial infarct segmentation onmulti-sequence MRI images.</description><author>Matthias Schwab, Agnes Mayr, Markus Haltmeier</author><pubDate>Thu, 18 Apr 2024 16:20:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12252v1</guid></item><item><title>Visualization for Trust in Machine Learning Revisited: The State of the Field in 2023</title><link>http://arxiv.org/abs/2403.12005v2</link><description>Visualization for explainable and trustworthy machine learning remains one ofthe most important and heavily researched fields within informationvisualization and visual analytics with various application domains, such asmedicine, finance, and bioinformatics. After our 2020 state-of-the-art reportcomprising 200 techniques, we have persistently collected peer-reviewedarticles describing visualization techniques, categorized them based on thepreviously established categorization schema consisting of 119 categories, andprovided the resulting collection of 542 techniques in an online surveybrowser. In this survey article, we present the updated findings of newanalyses of this dataset as of fall 2023 and discuss trends, insights, andeight open challenges for using visualizations in machine learning. Our resultscorroborate the rapidly growing trend of visualization techniques forincreasing trust in machine learning models in the past three years, withvisualization found to help improve popular model explainability methods andcheck new deep learning architectures, for instance.</description><author>Angelos Chatzimparmpas, Kostiantyn Kucher, Andreas Kerren</author><pubDate>Thu, 18 Apr 2024 16:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12005v2</guid></item><item><title>Dynamic Modality and View Selection for Multimodal Emotion Recognition with Missing Modalities</title><link>http://arxiv.org/abs/2404.12251v1</link><description>The study of human emotions, traditionally a cornerstone in fields likepsychology and neuroscience, has been profoundly impacted by the advent ofartificial intelligence (AI). Multiple channels, such as speech (voice) andfacial expressions (image), are crucial in understanding human emotions.However, AI's journey in multimodal emotion recognition (MER) is marked bysubstantial technical challenges. One significant hurdle is how AI modelsmanage the absence of a particular modality - a frequent occurrence inreal-world situations. This study's central focus is assessing the performanceand resilience of two strategies when confronted with the lack of one modality:a novel multimodal dynamic modality and view selection and a cross-attentionmechanism. Results on the RECOLA dataset show that dynamic selection-basedmethods are a promising approach for MER. In the missing modalities scenarios,all dynamic selection-based methods outperformed the baseline. The studyconcludes by emphasizing the intricate interplay between audio and videomodalities in emotion prediction, showcasing the adaptability of dynamicselection methods in handling missing modalities.</description><author>Luciana Trinkaus Menon, Luiz Carlos Ribeiro Neduziak, Jean Paul Barddal, Alessandro Lameiras Koerich, Alceu de Souza Britto Jr</author><pubDate>Thu, 18 Apr 2024 16:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12251v1</guid></item><item><title>Blind Localization and Clustering of Anomalies in Textures</title><link>http://arxiv.org/abs/2404.12246v1</link><description>Anomaly detection and localization in images is a growing field in computervision. In this area, a seemingly understudied problem is anomaly clustering,i.e., identifying and grouping different types of anomalies in a fullyunsupervised manner. In this work, we propose a novel method for clusteringanomalies in largely stationary images (textures) in a blind setting. That is,the input consists of normal and anomalous images without distinction andwithout labels. What contributes to the difficulty of the task is thatanomalous regions are often small and may present only subtle changes inappearance, which can be easily overshadowed by the genuine variance in thetexture. Moreover, each anomaly type may have a complex appearancedistribution. We introduce a novel scheme for solving this task using acombination of blind anomaly localization and contrastive learning. Byidentifying the anomalous regions with high fidelity, we can restrict our focusto those regions of interest; then, contrastive learning is employed toincrease the separability of different anomaly types and reduce the intra-classvariation. Our experiments show that the proposed solution yields significantlybetter results compared to prior work, setting a new state of the art. Projectpage: https://reality.tf.fau.de/pub/ardelean2024blind.html.</description><author>Andrei-Timotei Ardelean, Tim Weyrich</author><pubDate>Thu, 18 Apr 2024 16:11:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12246v1</guid></item><item><title>Reciprocal Attention Mixing Transformer for Lightweight Image Restoration</title><link>http://arxiv.org/abs/2305.11474v4</link><description>Although many recent works have made advancements in the image restoration(IR) field, they often suffer from an excessive number of parameters. Anotherissue is that most Transformer-based IR methods focus only on either local orglobal features, leading to limited receptive fields or deficient parameterissues. To address these problems, we propose a lightweight IR network,Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposeddimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, whichcompute bi-dimensional (spatial and channel) self-attentions in parallel withdifferent numbers of multi-heads. The bi-dimensional attentions help each otherto complement their counterpart's drawbacks and are then mixed. Additionally,we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer thatcompensates for pixel-level information losses and utilizes semanticinformation while maintaining an efficient hierarchical structure. Furthermore,we revisit and modify MobileNet V1 and V2 to attach efficient convolutions toour proposed components. The experimental results demonstrate that RAMiTachieves state-of-the-art performance on multiple lightweight IR tasks,including super-resolution, color denoising, grayscale denoising, low-lightenhancement, and deraining. Codes are available athttps://github.com/rami0205/RAMiT.</description><author>Haram Choi, Cheolwoong Na, Jihyeon Oh, Seungjae Lee, Jinseop Kim, Subeen Choe, Jeongmin Lee, Taehoon Kim, Jihoon Yang</author><pubDate>Thu, 18 Apr 2024 16:10:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11474v4</guid></item><item><title>Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks</title><link>http://arxiv.org/abs/2311.01949v2</link><description>In-context learning (ICL) ability has emerged with the increasing scale oflarge language models (LLMs), enabling them to learn input-label mappings fromdemonstrations and perform well on downstream tasks. However, under thestandard ICL setting, LLMs may sometimes neglect query-related information indemonstrations, leading to incorrect predictions. To address this limitation,we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) toexplore the power of ICL in open-domain question answering, an important formin knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extractquery-related knowledge from demonstrations, then concatenates the knowledge toprompt LLMs in a more explicit way. Furthermore, we track the source of thisknowledge to identify specific examples, and introduce a Hint-related ExampleRetriever (HER) to select informative examples for enhanced demonstrations. Weevaluate HICL with HER on 3 open-domain QA benchmarks, and observe averageperformance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EMscore and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.</description><author>Yifan Wang, Qingyan Guo, Xinzhe Ni, Chufan Shi, Lemao Liu, Haiyun Jiang, Yujiu Yang</author><pubDate>Thu, 18 Apr 2024 16:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01949v2</guid></item></channel></rss>