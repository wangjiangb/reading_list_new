<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 11 Feb 2024 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</title><link>http://arxiv.org/abs/2402.05937v1</link><description>In this paper, we introduce a novel paradigm to enhance the ability of objectdetector, e.g., expanding categories or improving detection performance, bytraining on synthetic dataset generated from diffusion models. Specifically, weintegrate an instance-level grounding head into a pre-trained, generativediffusion model, to augment it with the ability of localising arbitraryinstances in the generated images. The grounding head is trained to align thetext embedding of category names with the regional visual feature of thediffusion model, using supervision from an off-the-shelf object detector, and anovel self-training scheme on (novel) categories not covered by the detector.This enhanced version of diffusion model, termed as InstaGen, can serve as adata synthesizer for object detection. We conduct thorough experiments to showthat, object detector can be enhanced while training on the synthetic datasetfrom InstaGen, demonstrating superior performance over existingstate-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to5.2 AP) scenarios.</description><author>Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma</author><pubDate>Thu, 08 Feb 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05937v1</guid></item><item><title>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2402.05935v1</link><description>We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)series developed upon SPHINX. To improve the architecture and trainingefficiency, we modify the SPHINX framework by removing redundant visualencoders, bypassing fully-padded sub-images with skip tokens, and simplifyingmulti-stage training into a one-stage all-in-one paradigm. To fully unleash thepotential of MLLMs, we assemble a comprehensive multi-domain and multimodaldataset covering publicly available resources in language, vision, andvision-language tasks. We further enrich this collection with our curated OCRintensive and Set-of-Mark datasets, extending the diversity and generality. Bytraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary inparameter size and multilingual capabilities. Comprehensive benchmarkingreveals a strong correlation between the multi-modal performance with the dataand parameter scales. Code and models are released athttps://github.com/Alpha-VLLM/LLaMA2-Accessory</description><author>Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, Yu Qiao</author><pubDate>Thu, 08 Feb 2024 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05935v1</guid></item><item><title>Classifying Nodes in Graphs without GNNs</title><link>http://arxiv.org/abs/2402.05934v1</link><description>Graph neural networks (GNNs) are the dominant paradigm for classifying nodesin a graph, but they have several undesirable attributes stemming from theirmessage passing architecture. Recently, distillation methods succeeded ineliminating the use of GNNs at test time but they still require them duringtraining. We perform a careful analysis of the role that GNNs play indistillation methods. This analysis leads us to propose a fully GNN-freeapproach for node classification, not requiring them at train or test time. Ourmethod consists of three key components: smoothness constraints,pseudo-labeling iterations and neighborhood-label histograms. Our finalapproach can match the state-of-the-art accuracy on standard popular benchmarkssuch as citation and co-purchase networks, without training a GNN.</description><author>Daniel Winter, Niv Cohen, Yedid Hoshen</author><pubDate>Thu, 08 Feb 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05934v1</guid></item><item><title>Time Series Diffusion in the Frequency Domain</title><link>http://arxiv.org/abs/2402.05933v1</link><description>Fourier analysis has been an instrumental tool in the development of signalprocessing. This leads us to wonder whether this framework could similarlybenefit generative modelling. In this paper, we explore this question throughthe scope of time series diffusion models. More specifically, we analyzewhether representing time series in the frequency domain is a useful inductivebias for score-based diffusion models. By starting from the canonical SDEformulation of diffusion in the time domain, we show that a dual diffusionprocess occurs in the frequency domain with an important nuance: Brownianmotions are replaced by what we call mirrored Brownian motions, characterizedby mirror symmetries among their components. Building on this insight, we showhow to adapt the denoising score matching approach to implement diffusionmodels in the frequency domain. This results in frequency diffusion models,which we compare to canonical time diffusion models. Our empirical evaluationon real-world datasets, covering various domains like healthcare and finance,shows that frequency diffusion models better capture the training distributionthan time diffusion models. We explain this observation by showing that timeseries from these datasets tend to be more localized in the frequency domainthan in the time domain, which makes them easier to model in the former case.All our observations point towards impactful synergies between Fourier analysisand diffusion models.</description><author>Jonathan Crabbé, Nicolas Huynh, Jan Stanczuk, Mihaela van der Schaar</author><pubDate>Thu, 08 Feb 2024 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05933v1</guid></item><item><title>Driving Everywhere with Large Language Model Policy Adaptation</title><link>http://arxiv.org/abs/2402.05932v1</link><description>Adapting driving behavior to new environments, customs, and laws is along-standing problem in autonomous driving, precluding the widespreaddeployment of autonomous vehicles (AVs). In this paper, we present LLaDA, asimple yet powerful tool that enables human drivers and autonomous vehiclesalike to drive everywhere by adapting their tasks and motion plans to trafficrules in new locations. LLaDA achieves this by leveraging the impressivezero-shot generalizability of large language models (LLMs) in interpreting thetraffic rules in the local driver handbook. Through an extensive user study, weshow that LLaDA's instructions are useful in disambiguating in-the-wildunexpected situations. We also demonstrate LLaDA's ability to adapt AV motionplanning policies in real-world datasets; LLaDA outperforms baseline planningapproaches on all our metrics. Please check our website for more details:https://boyiliee.github.io/llada.</description><author>Boyi Li, Yue Wang, Jiageng Mao, Boris Ivanovic, Sushant Veer, Karen Leung, Marco Pavone</author><pubDate>Thu, 08 Feb 2024 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05932v1</guid></item><item><title>The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information</title><link>http://arxiv.org/abs/2102.10019v5</link><description>Critical decisions like hiring, college admissions, and loan approvals areguided by predictions made in the presence of uncertainty. While uncertaintyimparts errors across all demographic groups, this paper shows that the typesof errors vary systematically: Groups with higher average outcomes aretypically assigned higher false positive rates, while those with lower averageoutcomes are assigned higher false negative rates. We characterize theconditions that give rise to this disparate impact and explain why theintuitive remedy to omit demographic variables from datasets does not correctit. Instead of data omission, this paper examines how data enrichment canbroaden access to opportunity. The strategy, which we call "AffirmativeInformation," could stand as an alternative to Affirmative Action.</description><author>Claire Lazar Reich</author><pubDate>Thu, 08 Feb 2024 18:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.10019v5</guid></item><item><title>An Interactive Agent Foundation Model</title><link>http://arxiv.org/abs/2402.05929v1</link><description>The development of artificial intelligence systems is transitioning fromcreating static, task-specific models to dynamic, agent-based systems capableof performing well in a wide range of applications. We propose an InteractiveAgent Foundation Model that uses a novel multi-task agent training paradigm fortraining AI agents across a wide range of domains, datasets, and tasks. Ourtraining paradigm unifies diverse pre-training strategies, including visualmasked auto-encoders, language modeling, and next-action prediction, enabling aversatile and adaptable AI framework. We demonstrate the performance of ourframework across three separate domains -- Robotics, Gaming AI, and Healthcare.Our model demonstrates its ability to generate meaningful and contextuallyrelevant outputs in each area. The strength of our approach lies in itsgenerality, leveraging a variety of data sources such as robotics sequences,gameplay data, large-scale video datasets, and textual information foreffective multimodal and multi-task learning. Our approach provides a promisingavenue for developing generalist, action-taking, multimodal systems.</description><author>Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori, Yusuke Noda, Paul Tang, Ehsan Adeli, Shrinidhi Kowshika Lakshmikanth, Kevin Schulman, Arnold Milstein, Demetri Terzopoulos, Ade Famoti, Noboru Kuno, Ashley Llorens, Hoi Vo, Katsu Ikeuchi, Li Fei-Fei, Jianfeng Gao, Naoki Wake, Qiuyuan Huang</author><pubDate>Thu, 08 Feb 2024 18:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05929v1</guid></item><item><title>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</title><link>http://arxiv.org/abs/2402.05930v1</link><description>We propose the problem of conversational web navigation, where a digitalagent controls a web browser and follows user instructions to solve real-worldtasks in a multi-turn dialogue fashion. To support this problem, we introduceWEBLINX - a large-scale benchmark of 100K interactions across 2300 expertdemonstrations of conversational web navigation. Our benchmark covers a broadrange of patterns on over 150 real-world websites and can be used to train andevaluate agents in diverse scenarios. Due to the magnitude of informationpresent, Large Language Models (LLMs) cannot process entire web pages inreal-time. To solve this bottleneck, we design a retrieval-inspired model thatefficiently prunes HTML pages by ranking relevant elements. We use the selectedelements, along with screenshots and action history, to assess a variety ofmodels for their ability to replicate human behavior when navigating the web.Our experiments span from small text-only to proprietary multimodal LLMs. Wefind that smaller finetuned decoders surpass the best zero-shot LLMs (includingGPT-4V), but also larger finetuned multimodal models which were explicitlypretrained on screenshots. However, all finetuned models struggle to generalizeto unseen websites. Our findings highlight the need for large multimodal modelsthat can generalize to novel settings. Our code, data and models are availablefor research: https://mcgill-nlp.github.io/weblinx</description><author>Xing Han Lù, Zdeněk Kasner, Siva Reddy</author><pubDate>Thu, 08 Feb 2024 18:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05930v1</guid></item><item><title>Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss</title><link>http://arxiv.org/abs/2402.05928v1</link><description>In this work, we study statistical learning with dependent ($\beta$-mixing)data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p}\|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by thesearch for a sharp noise interaction term, or variance proxy, in learning withdependent data. Absent any realizability assumption, typical non-asymptoticresults exhibit variance proxies that are deflated \emph{multiplicatively} bythe mixing time of the underlying covariates process. We show that whenever thetopologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class$\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class:$\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- theempirical risk minimizer achieves a rate that only depends on the complexity ofthe class and second order statistics in its leading term. Our result holdswhether the problem is realizable or not and we refer to this as a \emph{nearmixing-free rate}, since direct dependence on mixing is relegated to anadditive higher order term. We arrive at our result by combining the abovenotion of a weakly sub-Gaussian class with mixed tail generic chaining. Thiscombination allows us to compute sharp, instance-optimal rates for a wide rangeof problems. %Our approach, reliant on mixed tail generic chaining, allows usto obtain sharp, instance-optimal rates. Examples that satisfy our frameworkinclude sub-Gaussian linear regression, more general smoothly parameterizedfunction classes, finite hypothesis classes, and bounded smoothness classes.</description><author>Ingvar Ziemann, Stephen Tu, George J. Pappas, Nikolai Matni</author><pubDate>Thu, 08 Feb 2024 18:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05928v1</guid></item><item><title>On the Convergence of Zeroth-Order Federated Tuning in Large Language Models</title><link>http://arxiv.org/abs/2402.05926v1</link><description>The confluence of Federated Learning (FL) and Large Language Models (LLMs) isushering in a new era in privacy-preserving natural language processing.However, the intensive memory requirements for fine-tuning LLMs posesignificant challenges, especially when deploying on edge devices with limitedcomputational resources. To circumvent this, we explore the novel integrationof Memory-efficient Zeroth-Order Optimization within a federated setting, asynergy we denote as FedMeZO. Our study is the first to examine the theoreticalunderpinnings of FedMeZO in the context of LLMs, tackling key questionsregarding the influence of large parameter spaces on optimization behavior, theestablishment of convergence properties, and the identification of criticalparameters for convergence to inform personalized federated strategies. Ourextensive empirical evidence supports the theory, showing that FedMeZO not onlyconverges faster than traditional first-order methods such as SGD but alsosignificantly reduces GPU memory usage during training to levels comparable tothose during inference. Moreover, the proposed personalized FL strategy that isbuilt upon the theoretical insights to customize the client-wise learning ratecan effectively accelerate loss reduction. We hope our work can help to bridgetheoretical and practical aspects of federated fine-tuning for LLMs andfacilitate further development and research.</description><author>Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen</author><pubDate>Thu, 08 Feb 2024 18:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05926v1</guid></item><item><title>Collaborative Control for Geometry-Conditioned PBR Image Generation</title><link>http://arxiv.org/abs/2402.05919v1</link><description>Current 3D content generation builds on generative models that output RGBimages. Modern graphics pipelines, however, require physically-based rendering(PBR) material properties. We propose to model the PBR image distributiondirectly to avoid photometric inaccuracies in RGB generation and the inherentambiguity in extracting PBR from RGB. Existing paradigms for cross-modalfinetuning are not suited for PBR generation due to a lack of data and the highdimensionality of the output modalities: we overcome both challenges byretaining a frozen RGB model and tightly linking a newly trained PBR modelusing a novel cross-network communication paradigm. As the base RGB model isfully frozen, the proposed method does not risk catastrophic forgetting duringfinetuning and remains compatible with techniques such as IPAdapter pretrainedfor the base RGB model. We validate our design choices, robustness to datasparsity, and compare against existing paradigms with an extensive experimentalsection.</description><author>Shimon Vainer, Mark Boss, Mathias Parger, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Nicolas Perony, Simon Donné</author><pubDate>Thu, 08 Feb 2024 18:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05919v1</guid></item><item><title>Point-VOS: Pointing Up Video Object Segmentation</title><link>http://arxiv.org/abs/2402.05917v1</link><description>Current state-of-the-art Video Object Segmentation (VOS) methods rely ondense per-object mask annotations both during training and testing. Thisrequires time-consuming and costly video annotation mechanisms. We propose anovel Point-VOS task with a spatio-temporally sparse point-wise annotationscheme that substantially reduces the annotation effort. We apply ourannotation scheme to two large-scale video datasets with text descriptions andannotate over 19M points across 133K objects in 32K videos. Based on ourannotations, we propose a new Point-VOS benchmark, and a correspondingpoint-based training mechanism, which we use to establish strong baselineresults. We show that existing VOS methods can easily be adapted to leverageour point annotations during training, and can achieve results close to thefully-supervised performance when trained on pseudo-masks generated from thesepoints. In addition, we show that our data can be used to improve models thatconnect vision and language, by evaluating it on the Video Narrative Grounding(VNG) task. We will make our code and annotations available athttps://pointvos.github.io.</description><author>Idil Esen Zulfikar, Sabarinath Mahadevan, Paul Voigtlaender, Bastian Leibe</author><pubDate>Thu, 08 Feb 2024 18:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05917v1</guid></item><item><title>GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory</title><link>http://arxiv.org/abs/2402.05916v1</link><description>We present GenEFT: an effective theory framework for shedding light on thestatics and dynamics of neural network generalization, and illustrate it withgraph learning examples. We first investigate the generalization phasetransition as data size increases, comparing experimental results withinformation-theory-based approximations. We find generalization in a Goldilockszone where the decoder is neither too weak nor too powerful. We then introducean effective theory for the dynamics of representation learning, wherelatent-space representations are modeled as interacting particles (repons), andfind that it explains our experimentally observed phase transition betweengeneralization and overfitting as encoder and decoder learning rates arescanned. This highlights the power of physics-inspired effective theories forbridging the gap between theoretical predictions and practice in machinelearning.</description><author>David D. Baek, Ziming Liu, Max Tegmark</author><pubDate>Thu, 08 Feb 2024 18:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05916v1</guid></item><item><title>Efficient Stagewise Pretraining via Progressive Subnetworks</title><link>http://arxiv.org/abs/2402.05913v1</link><description>Recent developments in large language models have sparked interest inefficient pretraining methods. A recent effective paradigm is to performstage-wise training, where the size of the model is gradually increased overthe course of training (e.g. gradual stacking (Reddi et al., 2023)). While theresource and wall-time savings are appealing, it has limitations, particularlythe inability to evaluate the full model during earlier stages, and degradationin model quality due to smaller model capacity in the initial stages. In thiswork, we propose an alternative framework, progressive subnetwork training,that maintains the full model throughout training, but only trains subnetworkswithin the model in each step. We focus on a simple instantiation of thisframework, Random Path Training (RaPTr) that only trains a sub-path of layersin each step, progressively increasing the path lengths in stages. RaPTrachieves better pre-training loss for BERT and UL2 language models whilerequiring 20-33% fewer FLOPs compared to standard training, and is competitiveor better than other efficient training methods. Furthermore, RaPTr showsbetter downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5%compared to standard training and stacking. Finally, we provide a theoreticalbasis for RaPTr to justify (a) the increasing complexity of subnetworks instages, and (b) the stability in loss across stage transitions due to residualconnections and layer norm.</description><author>Abhishek Panigrahi, Nikunj Saunshi, Kaifeng Lyu, Sobhan Miryoosefi, Sashank Reddi, Satyen Kale, Sanjiv Kumar</author><pubDate>Thu, 08 Feb 2024 18:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05913v1</guid></item><item><title>Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games</title><link>http://arxiv.org/abs/2402.05906v1</link><description>Classical multi-agent reinforcement learning (MARL) assumes risk neutralityand complete objectivity for agents. However, in settings where agents need toconsider or model human economic or social preferences, a notion of risk mustbe incorporated into the RL optimization problem. This will be of greaterimportance in MARL where other human or non-human agents are involved, possiblywith their own risk-sensitive policies. In this work, we considerrisk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT),a non-convex risk measure and a generalization of coherent measures of risk.CPT is capable of explaining loss aversion in humans and their tendency tooverestimate/underestimate small/large probabilities. We propose a distributedsampling-based actor-critic (AC) algorithm with CPT risk for networkaggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC.Under a set of assumptions, we prove the convergence of the algorithm to asubjective notion of Markov perfect Nash equilibrium in NAMGs. The experimentalresults show that subjective CPT policies obtained by our algorithm can bedifferent from the risk-neutral ones, and agents with a higher loss aversionare more inclined to socially isolate themselves in an NAMG.</description><author>Hafez Ghaemi, Hamed Kebriaei, Alireza Ramezani Moghaddam, Majid Nili Ahamdabadi</author><pubDate>Thu, 08 Feb 2024 18:43:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05906v1</guid></item><item><title>FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs</title><link>http://arxiv.org/abs/2402.05904v1</link><description>Our society is facing rampant misinformation harming public health and trust.To address the societal challenge, we introduce FACT-GPT, a system leveragingLarge Language Models (LLMs) to automate the claim matching stage offact-checking. FACT-GPT, trained on a synthetic dataset, identifies socialmedia content that aligns with, contradicts, or is irrelevant to previouslydebunked claims. Our evaluation shows that our specialized LLMs can match theaccuracy of larger models in identifying related claims, closely mirroringhuman judgment. This research provides an automated solution for efficientclaim matching, demonstrates the potential of LLMs in supporting fact-checkers,and offers valuable resources for further research in the field.</description><author>Eun Cheol Choi, Emilio Ferrara</author><pubDate>Thu, 08 Feb 2024 18:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05904v1</guid></item><item><title>ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation</title><link>http://arxiv.org/abs/2402.05902v1</link><description>The newly released Segment Anything Model (SAM) is a popular tool used inimage processing due to its superior segmentation accuracy, variety of inputprompts, training capabilities, and efficient model design. However, itscurrent model is trained on a diverse dataset not tailored to medical images,particularly ultrasound images. Ultrasound images tend to have a lot of noise,making it difficult to segment out important structures. In this project, wedeveloped ClickSAM, which fine-tunes the Segment Anything Model using clickprompts for ultrasound images. ClickSAM has two stages of training: the firststage is trained on single-click prompts centered in the ground-truth contours,and the second stage focuses on improving the model performance throughadditional positive and negative click prompts. By comparing the first stagepredictions to the ground-truth masks, true positive, false positive, and falsenegative segments are calculated. Positive clicks are generated using the truepositive and false negative segments, and negative clicks are generated usingthe false positive segments. The Centroidal Voronoi Tessellation algorithm isthen employed to collect positive and negative click prompts in each segmentthat are used to enhance the model performance during the second stage oftraining. With click-train methods, ClickSAM exhibits superior performancecompared to other existing models for ultrasound image segmentation.</description><author>Aimee Guo, Gace Fei, Hemanth Pasupuletic, Jing Wang</author><pubDate>Thu, 08 Feb 2024 18:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05902v1</guid></item><item><title>EfficientAD: Accurate Visual Anomaly Detection at Millisecond-Level Latencies</title><link>http://arxiv.org/abs/2303.14535v3</link><description>Detecting anomalies in images is an important task, especially in real-timecomputer vision applications. In this work, we focus on computationalefficiency and propose a lightweight feature extractor that processes an imagein less than a millisecond on a modern GPU. We then use a student-teacherapproach to detect anomalous features. We train a student network to predictthe extracted features of normal, i.e., anomaly-free training images. Thedetection of anomalies at test time is enabled by the student failing topredict their features. We propose a training loss that hinders the studentfrom imitating the teacher feature extractor beyond the normal images. Itallows us to drastically reduce the computational cost of the student-teachermodel, while improving the detection of anomalous features. We furthermoreaddress the detection of challenging logical anomalies that involve invalidcombinations of normal local features, for example, a wrong ordering ofobjects. We detect these anomalies by efficiently incorporating an autoencoderthat analyzes images globally. We evaluate our method, called EfficientAD, on32 datasets from three industrial anomaly detection dataset collections.EfficientAD sets new standards for both the detection and the localization ofanomalies. At a latency of two milliseconds and a throughput of six hundredimages per second, it enables a fast handling of anomalies. Together with itslow error rate, this makes it an economical solution for real-worldapplications and a fruitful basis for future research.</description><author>Kilian Batzner, Lars Heckler, Rebecca König</author><pubDate>Thu, 08 Feb 2024 18:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14535v3</guid></item><item><title>SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach</title><link>http://arxiv.org/abs/2402.02275v2</link><description>This paper introduces SudokuSens, a generative framework for automatedgeneration of training data in machine-learning-based Internet-of-Things (IoT)applications, such that the generated synthetic data mimic experimentalconfigurations not encountered during actual sensor data collection. Theframework improves the robustness of resulting deep learning models, and isintended for IoT applications where data collection is expensive. The work ismotivated by the fact that IoT time-series data entangle the signatures ofobserved objects with the confounding intrinsic properties of the surroundingenvironment and the dynamic environmental disturbances experienced. Toincorporate sufficient diversity into the IoT training data, one thereforeneeds to consider a combinatorial explosion of training cases that aremultiplicative in the number of objects considered and the possibleenvironmental conditions in which such objects may be encountered. Ourframework substantially reduces these multiplicative training needs. Todecouple object signatures from environmental conditions, we employ aConditional Variational Autoencoder (CVAE) that allows us to reduce datacollection needs from multiplicative to (nearly) linear, while syntheticallygenerating (data for) the missing conditions. To obtain robustness with respectto dynamic disturbances, a session-aware temporal contrastive learning approachis taken. Integrating the aforementioned two approaches, SudokuSenssignificantly improves the robustness of deep learning for IoT applications. Weexplore the degree to which SudokuSens benefits downstream inference tasks indifferent data sets and discuss conditions under which the approach isparticularly effective.</description><author>Tianshi Wang, Jinyang Li, Ruijie Wang, Denizhan Kara, Shengzhong Liu, Davis Wertheimer, Antoni Viros-i-Martin, Raghu Ganti, Mudhakar Srivatsa, Tarek Abdelzaher</author><pubDate>Thu, 08 Feb 2024 18:35:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02275v2</guid></item><item><title>Large Language Model Meets Graph Neural Network in Knowledge Distillation</title><link>http://arxiv.org/abs/2402.05894v1</link><description>Despite recent community revelations about the advancements and potential ofLarge Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), thedeployment of LLMs for production is hindered by their high computational andstorage requirements, as well as long latencies during inference.Simultaneously, although traditional Graph Neural Networks (GNNs) are lightweight and adept at learning structural features of graphs, their ability tograsp the complex semantics in TAGs is somewhat constrained for realapplications. To address these limitations, we concentrate on the downstreamtask of node classification in TAG and propose a novel graph knowledgedistillation framework, termed Linguistic Graph Knowledge Distillation(LinguGKD), using LLMs as teacher models and GNNs as student models forknowledge distillation. It involves TAG-oriented instruction tuning of LLM ondesigned node classification prompts, followed by aligning the hierarchicallylearned node features of the teacher LLM and the student GNN in latent space,employing a layer-adaptive contrastive learning strategy. Through extensiveexperiments on a variety of LLM and GNN models and multiple benchmark datasets,the proposed LinguGKD significantly boosts the student GNN's predictiveaccuracy and convergence rate, without the need of extra data or modelparameters. Compared to teacher LLM, distilled GNN achieves superior inferencespeed equipped with much fewer computing and storage demands, when surpassingthe teacher LLM's classification performance on some of benchmark datasets.</description><author>Shengxiang Hu, Guobing Zou, Song Yang, Bofeng Zhang, Yixin Chen</author><pubDate>Thu, 08 Feb 2024 18:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05894v1</guid></item><item><title>Learning Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding</title><link>http://arxiv.org/abs/2308.05893v2</link><description>Multi-agent pathfinding (MAPF) is a critical field in many large-scalerobotic applications, often being the fundamental step in multi-agent systems.The increasing complexity of MAPF in complex and crowded environments, however,critically diminishes the effectiveness of existing solutions. In contrast toother studies that have either presented a general overview of the recentadvancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL)within multi-agent system settings independently, our work presented in thisreview paper focuses on highlighting the integration of DRL-based approaches inMAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutionsby addressing the lack of unified evaluation metrics and providingcomprehensive clarification on these metrics. Finally, our paper discusses thepotential of model-based DRL as a promising future direction and provides itsrequired foundational understanding to address current challenges in MAPF. Ourobjective is to assist readers in gaining insight into the current researchdirection, providing unified metrics for comparing different MAPF algorithmsand expanding their knowledge of model-based DRL to address the existingchallenges in MAPF.</description><author>Jaehoon Chung, Jamil Fayyad, Younes Al Younes, Homayoun Najjaran</author><pubDate>Thu, 08 Feb 2024 18:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05893v2</guid></item><item><title>Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data</title><link>http://arxiv.org/abs/2402.05892v1</link><description>In recent years, Transformers have become the de-facto architecture forsequence modeling on text and a variety of multi-dimensional data, such asimages and video. However, the use of self-attention layers in a Transformerincurs prohibitive compute and memory complexity that scales quadraticallyw.r.t. the sequence length. A recent architecture, Mamba, based on state spacemodels has been shown to achieve comparable performance for modeling textsequences, while scaling linearly with the sequence length. In this work, wepresent Mamba-ND, a generalized design extending the Mamba architecture toarbitrary multi-dimensional data. Our design alternatively unravels the inputdata across different dimensions following row-major orderings. We provide asystematic comparison of Mamba-ND with several other alternatives, based onprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.Empirically, we show that Mamba-ND demonstrates performance competitive withthe state-of-the-art on a variety of multi-dimensional benchmarks, includingImageNet-1K classification, HMDB-51 action recognition, and ERA5 weatherforecasting.</description><author>Shufan Li, Harkanwar Singh, Aditya Grover</author><pubDate>Thu, 08 Feb 2024 18:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05892v1</guid></item><item><title>CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion</title><link>http://arxiv.org/abs/2402.05889v1</link><description>Despite impressive advancements in multimodal compositional reasoningapproaches, they are still limited in their flexibility and efficiency byprocessing fixed modality inputs while updating a lot of model parameters. Thispaper tackles these critical challenges and proposes CREMA, an efficient andmodular modality-fusion framework for injecting any new modality into videoreasoning. We first augment multiple informative modalities (such as opticalflow, 3D point cloud, audio) from given videos without extra human annotationby leveraging existing pre-trained models. Next, we introduce a querytransformer with multiple parameter-efficient modules associated with eachaccessible modality. It projects diverse modality features to the LLM tokenembedding space, allowing the model to integrate different data types forresponse generation. Furthermore, we propose a fusion module designed tocompress multimodal queries, maintaining computational efficiency in the LLMwhile combining additional modalities. We validate our method on video-3D,video-audio, and video-language reasoning tasks and achieve better/equivalentperformance against strong multimodal LLMs, including BLIP-2, 3D-LLM, andSeViLA while using 96% fewer trainable parameters. We provide extensiveanalyses of CREMA, including the impact of each modality on reasoning domains,the design of the fusion module, and example visualizations.</description><author>Shoubin Yu, Jaehong Yoon, Mohit Bansal</author><pubDate>Thu, 08 Feb 2024 18:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05889v1</guid></item><item><title>Learning to Rewrite Prompts for Personalized Text Generation</title><link>http://arxiv.org/abs/2310.00152v2</link><description>Facilitated by large language models (LLMs), personalized text generation hasbecome a rapidly growing research direction. Most existing studies focus ondesigning specialized models for a particular domain, or they requirefine-tuning the LLMs to generate personalized text. We consider a typicalscenario in which the large language model, which generates personalizedoutput, is frozen and can only be accessed through APIs. Under this constraint,all one can do is to improve the input text (i.e., text prompts) sent to theLLM, a procedure that is usually done manually. In this paper, we propose anovel method to automatically revise prompts for personalized text generation.The proposed method takes the initial prompts generated by a state-of-the-art,multistage framework for personalized generation and rewrites a few criticalcomponents that summarize and synthesize the personal context. The promptrewriter employs a training paradigm that chains together supervised learning(SL) and reinforcement learning (RL), where SL reduces the search space of RLand RL facilitates end-to-end training of the rewriter. Using datasets fromthree representative domains, we demonstrate that the rewritten promptsoutperform both the original prompts and the prompts optimized via supervisedlearning or reinforcement learning alone. In-depth analysis of the rewrittenprompts shows that they are not only human readable, but also able to guidemanual revision of prompts when there is limited resource to employreinforcement learning to train the prompt rewriter, or when it is costly todeploy an automatic prompt rewriter for inference.</description><author>Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Michael Bendersky</author><pubDate>Thu, 08 Feb 2024 18:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00152v2</guid></item><item><title>EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance</title><link>http://arxiv.org/abs/2402.05885v1</link><description>The need to identify graphs having small structural distance from a queryarises in biology, chemistry, recommender systems, and social network analysis.Among several methods to measure inter graph distance, Graph Edit Distance(GED) is preferred for its comprehensibility, yet hindered by the NP-hardnessof its computation. State-of-the-art GED approximations predominantly employneural methods, which, however, (i) lack an explanatory edit path correspondingto the approximated GED; (ii) require the NP-hard generation of ground-truthGEDs for training; and (iii) necessitate separate training on each dataset. Inthis paper, we propose an efficient algebraic unsuper vised method, EUGENE,that approximates GED and yields edit paths corresponding to the approx imatedcost, while eliminating the need for ground truth generation and data-specifictraining. Extensive experimental evaluation demonstrates that theaforementioned benefits of EUGENE do not come at the cost of efficacy.Specifically, EUGENE consistently ranks among the most accurate methods acrossall of the benchmark datasets and outperforms majority of the neuralapproaches.</description><author>Aditya Bommakanti, Harshith Reddy Vonteri, Sayan Ranu, Panagiotis Karras</author><pubDate>Thu, 08 Feb 2024 18:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05885v1</guid></item><item><title>Learning Collective Behaviors from Observation</title><link>http://arxiv.org/abs/2311.00875v2</link><description>We present a comprehensive examination of learning methodologies employed forthe structural identification of dynamical systems. These techniques aredesigned to elucidate emergent phenomena within intricate systems ofinteracting agents. Our approach not only ensures theoretical convergenceguarantees but also exhibits computational efficiency when handlinghigh-dimensional observational data. The methods adeptly reconstruct bothfirst- and second-order dynamical systems, accommodating observation andstochastic noise, intricate interaction rules, absent interaction features, andreal-world observations in agent systems. The foundational aspect of ourlearning methodologies resides in the formulation of tailored loss functionsusing the variational inverse problem approach, inherently equipping ourmethods with dimension reduction capabilities.</description><author>Jinchao Feng, Ming Zhong</author><pubDate>Thu, 08 Feb 2024 18:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00875v2</guid></item><item><title>Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking</title><link>http://arxiv.org/abs/2402.05880v1</link><description>Large language models (LLMs) powered conversational search systems havealready been used by hundreds of millions of people, and are believed to bringmany benefits over conventional search. However, while decades of research andpublic discourse interrogated the risk of search systems in increasingselective exposure and creating echo chambers -- limiting exposure to diverseopinions and leading to opinion polarization, little is known about such a riskof LLM-powered conversational search. We conduct two experiments toinvestigate: 1) whether and how LLM-powered conversational search increasesselective exposure compared to conventional search; 2) whether and how LLMswith opinion biases that either reinforce or challenge the user's view changethe effect. Overall, we found that participants engaged in more biasedinformation querying with LLM-powered conversational search, and an opinionatedLLM reinforcing their views exacerbated this bias. These results presentcritical implications for the development of LLMs and conversational searchsystems, and the policy governing these technologies.</description><author>Nikhil Sharma, Q. Vera Liao, Ziang Xiao</author><pubDate>Thu, 08 Feb 2024 18:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05880v1</guid></item><item><title>Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits</title><link>http://arxiv.org/abs/2402.05878v1</link><description>We study the problem of Bayesian fixed-budget best-arm identification (BAI)in structured bandits. We propose an algorithm that uses fixed allocationsbased on the prior information and the structure of the environment. We providetheoretical bounds on its performance across diverse models, including thefirst prior-dependent upper bounds for linear and hierarchical BAI. Our keycontribution is introducing new proof methods that result in tighter bounds formulti-armed BAI compared to existing methods. We extensively compare ourapproach to other fixed-budget BAI methods, demonstrating its consistent androbust performance in various settings. Our work improves our understanding ofBayesian fixed-budget BAI in structured bandits and highlights theeffectiveness of our approach in practical scenarios.</description><author>Nicolas Nguyen, Imad Aouali, András György, Claire Vernade</author><pubDate>Thu, 08 Feb 2024 18:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05878v1</guid></item><item><title>Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability</title><link>http://arxiv.org/abs/2307.07084v2</link><description>Reinforcement Learning or optimal control can provide effective reasoning forsequential decision-making problems with variable dynamics. Such reasoning inpractical implementation, however, poses a persistent challenge in interpretingthe reward function and corresponding optimal policy. Consequently, formalizingthe sequential decision-making problems as inference has a considerable value,as probabilistic inference in principle offers diverse and powerfulmathematical tools to infer the stochastic dynamics whilst suggesting aprobabilistic interpretation of the reward design and policy convergence. Inthis study, we propose a novel Adaptive Wasserstein Variational Optimization(AWaVO) to tackle these challenges in sequential decision-making. Our approachutilizes formal methods to provide interpretations of reward design,transparency of training convergence, and probabilistic interpretation ofsequential decisions. To demonstrate practicality, we show convergent trainingwith guaranteed global convergence rates not only in simulation but also inreal robot tasks, and empirically verify a reasonable tradeoff between highperformance and conservative interpretability.</description><author>Yanran Wang, David Boyle</author><pubDate>Thu, 08 Feb 2024 18:09:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07084v2</guid></item><item><title>Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices</title><link>http://arxiv.org/abs/2402.05876v1</link><description>Offline reinforcement learning (RL), which seeks to learn an optimal policyusing offline data, has garnered significant interest due to its potential incritical applications where online data collection is infeasible or expensive.This work explores the benefit of federated learning for offline RL, aiming atcollaboratively leveraging offline datasets at multiple agents. Focusing onfinite-horizon episodic tabular Markov decision processes (MDPs), we designFedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored forfederated offline RL. FedLCB-Q updates local Q-functions at agents with novellearning rate schedules and aggregates them at a central server usingimportance averaging and a carefully designed pessimistic penalty term. Oursample complexity analysis reveals that, with appropriately chosen parametersand synchronization schedules, FedLCB-Q achieves linear speedup in terms of thenumber of agents without requiring high-quality datasets at individual agents,as long as the local datasets collectively cover the state-action space visitedby the optimal policy, highlighting the power of collaboration in the federatedsetting. In fact, the sample complexity almost matches that of the single-agentcounterpart, as if all the data are stored at a central location, up topolynomial factors of the horizon length. Furthermore, FedLCB-Q iscommunication-efficient, where the number of communication rounds is onlylinear with respect to the horizon length up to logarithmic factors.</description><author>Jiin Woo, Laixi Shi, Gauri Joshi, Yuejie Chi</author><pubDate>Thu, 08 Feb 2024 18:09:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05876v1</guid></item><item><title>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</title><link>http://arxiv.org/abs/2307.04725v2</link><description>With the advance of text-to-image (T2I) diffusion models (e.g., StableDiffusion) and corresponding personalization techniques such as DreamBooth andLoRA, everyone can manifest their imagination into high-quality images at anaffordable cost. However, adding motion dynamics to existing high-qualitypersonalized T2Is and enabling them to generate animations remains an openchallenge. In this paper, we present AnimateDiff, a practical framework foranimating personalized T2I models without requiring model-specific tuning. Atthe core of our framework is a plug-and-play motion module that can be trainedonce and seamlessly integrated into any personalized T2Is originating from thesame base T2I. Through our proposed training strategy, the motion moduleeffectively learns transferable motion priors from real-world videos. Oncetrained, the motion module can be inserted into a personalized T2I model toform a personalized animation generator. We further propose MotionLoRA, alightweight fine-tuning technique for AnimateDiff that enables a pre-trainedmotion module to adapt to new motion patterns, such as different shot types, ata low training and data collection cost. We evaluate AnimateDiff and MotionLoRAon several public representative personalized T2I models collected from thecommunity. The results demonstrate that our approaches help these modelsgenerate temporally smooth animation clips while preserving the visual qualityand motion diversity. Codes and pre-trained weights are available athttps://github.com/guoyww/AnimateDiff.</description><author>Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, Bo Dai</author><pubDate>Thu, 08 Feb 2024 18:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04725v2</guid></item><item><title>Multi-Task Learning with Summary Statistics</title><link>http://arxiv.org/abs/2307.02388v2</link><description>Multi-task learning has emerged as a powerful machine learning paradigm forintegrating data from multiple sources, leveraging similarities between tasksto improve overall model performance. However, the application of multi-tasklearning to real-world settings is hindered by data-sharing constraints,especially in healthcare settings. To address this challenge, we propose aflexible multi-task learning framework utilizing summary statistics fromvarious sources. Additionally, we present an adaptive parameter selectionapproach based on a variant of Lepski's method, allowing for data-driven tuningparameter selection when only summary statistics are available. Our systematicnon-asymptotic analysis characterizes the performance of the proposed methodsunder various regimes of the sample complexity and overlap. We demonstrate ourtheoretical findings and the performance of the method through extensivesimulations. This work offers a more flexible tool for training related modelsacross various domains, with practical implications in genetic risk predictionand many other fields.</description><author>Parker Knight, Rui Duan</author><pubDate>Thu, 08 Feb 2024 18:08:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02388v2</guid></item><item><title>Adaptive Surface Normal Constraint for Geometric Estimation from Monocular Images</title><link>http://arxiv.org/abs/2402.05869v1</link><description>We introduce a novel approach to learn geometries such as depth and surfacenormal from images while incorporating geometric context. The difficulty ofreliably capturing geometric context in existing methods impedes their abilityto accurately enforce the consistency between the different geometricproperties, thereby leading to a bottleneck of geometric estimation quality. Wetherefore propose the Adaptive Surface Normal (ASN) constraint, a simple yetefficient method. Our approach extracts geometric context that encodes thegeometric variations present in the input image and correlates depth estimationwith geometric constraints. By dynamically determining reliable local geometryfrom randomly sampled candidates, we establish a surface normal constraint,where the validity of these candidates is evaluated using the geometriccontext. Furthermore, our normal estimation leverages the geometric context toprioritize regions that exhibit significant geometric variations, which makesthe predicted normals accurately capture intricate and detailed geometricinformation. Through the integration of geometric context, our method unifiesdepth and surface normal estimations within a cohesive framework, which enablesthe generation of high-quality 3D geometry from images. We validate thesuperiority of our approach over state-of-the-art methods through extensiveevaluations and comparisons on diverse indoor and outdoor datasets, showcasingits efficiency and robustness.</description><author>Xiaoxiao Long, Yuhang Zheng, Yupeng Zheng, Beiwen Tian, Cheng Lin, Lingjie Liu, Hao Zhao, Guyue Zhou, Wenping Wang</author><pubDate>Thu, 08 Feb 2024 17:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05869v1</guid></item><item><title>PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models</title><link>http://arxiv.org/abs/2402.05868v1</link><description>Cloud-based large language models (LLMs) such as ChatGPT have increasinglybecome integral to daily operations, serving as vital tools across variousapplications. While these models offer substantial benefits in terms ofaccessibility and functionality, they also introduce significant privacyconcerns: the transmission and storage of user data in cloud infrastructurespose substantial risks of data breaches and unauthorized access to sensitiveinformation; even if the transmission and storage of data is encrypted, the LLMservice provider itself still knows the real contents of the data, preventingindividuals or entities from confidently using such LLM services. To addressthese concerns, this paper proposes a simple yet effective mechanismPromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputsbefore sending them to LLM, effectively rendering them indecipherable to humanor LLM's examination while retaining the original intent of the prompt, thusensuring the model's performance remains unaffected. We conduct experiments onthree tasks, personalized recommendation, sentiment analysis, and tabular dataanalysis. Experiment results reveal that PromptCrypt can encrypt personalinformation within prompts in such a manner that not only prevents thediscernment of sensitive data by humans or LLM itself, but also maintains oreven improves the precision without further tuning, achieving comparable oreven better task accuracy than directly prompting the LLM without promptencryption. These results highlight the practicality of adopting encryptionmeasures that safeguard user privacy without compromising the functionalintegrity and performance of LLMs. Code and dataset are available athttps://github.com/agiresearch/PromptCrypt.</description><author>Guo Lin, Wenyue Hua, Yongfeng Zhang</author><pubDate>Thu, 08 Feb 2024 17:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05868v1</guid></item><item><title>Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs</title><link>http://arxiv.org/abs/2402.05864v1</link><description>In this paper, we propose a new decoding method called Permute-and-Flip (PF)decoder. It enjoys robustness properties similar to the standard samplingdecoder, but is provably up to 2x better in its quality-robustness tradeoffthan sampling and never worse than any other decoder. We also design acryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, butnaturally tailored for PF decoder. The watermarking scheme does not change thedistribution to sample, while allowing arbitrarily low false positive rate andhigh recall whenever the generated text has high entropy. Our experiments showthat the PF decoder (and its watermarked counterpart) significantlyoutperform(s) naive sampling (and it's Gumbel watermarked counterpart) in termsof perplexity, while retaining the same robustness (and detectability), hencemaking it a promising new approach for LLM decoding. The code is available athttps://github.com/XuandongZhao/pf-decoding</description><author>Xuandong Zhao, Lei Li, Yu-Xiang Wang</author><pubDate>Thu, 08 Feb 2024 17:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05864v1</guid></item><item><title>How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis</title><link>http://arxiv.org/abs/2402.05863v1</link><description>Negotiation is the basis of social interactions; humans negotiate everythingfrom the price of cars to how to share common resources. With rapidly growinginterest in using large language models (LLMs) to act as agents on behalf ofhuman users, such LLM agents would also need to be able to negotiate. In thispaper, we study how well LLMs can negotiate with each other. We developNegotiationArena: a flexible framework for evaluating and probing thenegotiation abilities of LLM agents. We implemented three types of scenarios inNegotiationArena to assess LLM's behaviors in allocating shared resources(ultimatum games), aggregate resources (trading games) and buy/sell goods(price negotiations). Each scenario allows for multiple turns of flexibledialogues between LLM agents to allow for more complex negotiations.Interestingly, LLM agents can significantly boost their negotiation outcomes byemploying certain behavioral tactics. For example, by pretending to be desolateand desperate, LLMs can improve their payoffs by 20\% when negotiating againstthe standard GPT-4. We also quantify irrational negotiation behaviors exhibitedby the LLM agents, many of which also appear in humans. Together,\NegotiationArena offers a new environment to investigate LLM interactions,enabling new insights into LLM's theory of mind, irrationality, and reasoningabilities.</description><author>Federico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, James Zou</author><pubDate>Thu, 08 Feb 2024 17:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05863v1</guid></item><item><title>Let Your Graph Do the Talking: Encoding Structured Data for LLMs</title><link>http://arxiv.org/abs/2402.05862v1</link><description>How can we best encode structured data into sequential form for use in largelanguage models (LLMs)? In this work, we introduce a parameter-efficient methodto explicitly represent structured data for LLMs. Our method, GraphToken,learns an encoding function to extend prompts with explicit structuredinformation. Unlike other work which focuses on limited domains (e.g. knowledgegraph representation), our work is the first effort focused on the generalencoding of structured data to be used for various reasoning tasks. We showthat explicitly representing the graph structure allows significantimprovements to graph reasoning tasks. Specifically, we see across the boardimprovements - up to 73% points - on node, edge and, graph-level tasks from theGraphQA benchmark.</description><author>Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow</author><pubDate>Thu, 08 Feb 2024 17:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05862v1</guid></item><item><title>Memory Consolidation Enables Long-Context Video Understanding</title><link>http://arxiv.org/abs/2402.05861v1</link><description>Most transformer-based video encoders are limited to short temporal contextsdue to their quadratic complexity. While various attempts have been made toextend this context, this has often come at the cost of both conceptual andcomputational complexity. We propose to instead re-purpose existing pre-trainedvideo transformers by simply fine-tuning them to attend to memories derivednon-parametrically from past activations. By leveraging redundancy reduction,our memory-consolidated vision transformer (MC-ViT) effortlessly extends itscontext far into the past and exhibits excellent scaling behavior when learningfrom longer videos. In doing so, MC-ViT sets a new state-of-the-art inlong-context video understanding on EgoSchema, Perception Test, and Diving48,outperforming methods that benefit from orders of magnitude more parameters.</description><author>Ivana Balažević, Yuge Shi, Pinelopi Papalampidi, Rahma Chaabouni, Skanda Koppula, Olivier J. Hénaff</author><pubDate>Thu, 08 Feb 2024 17:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05861v1</guid></item><item><title>Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model</title><link>http://arxiv.org/abs/2402.00746v2</link><description>Artificial intelligence (AI) in healthcare has significantly advancedintelligent medical treatment. However, traditional intelligent healthcare islimited by static data and unified standards, preventing full integration withindividual situations and other challenges. Hence, a more professional anddetailed intelligent healthcare method is needed for development. To this end,we propose an innovative framework named Heath-LLM, which combines large-scalefeature extraction and medical knowledge trade-off scoring. Compared totraditional health management methods, our approach has three main advantages.First, our method integrates health reports into a large model to providedetailed task information. Second, professional medical expertise is used toadjust the weighted scores of health characteristics. Third, we use asemi-automated feature extraction framework to enhance the analytical power oflanguage models and incorporate expert insights to improve the accuracy ofdisease prediction. We have conducted disease prediction experiments on a largenumber of health reports to assess the effectiveness of Health-LLM. The resultsof the experiments indicate that the proposed method surpasses traditionalmethods and has the potential to revolutionize disease prediction andpersonalized health management. The code is available athttps://github.com/jmyissb/HealthLLM.</description><author>Mingyu Jin, Qinkai Yu, Chong Zhang, Dong Shu, Suiyuan Zhu, Mengnan Du, Yongfeng Zhang, Yanda Meng</author><pubDate>Thu, 08 Feb 2024 17:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00746v2</guid></item><item><title>Incentive-Theoretic Bayesian Inference for Collaborative Science</title><link>http://arxiv.org/abs/2307.03748v2</link><description>Contemporary scientific research is a distributed, collaborative endeavor,carried out by teams of researchers, regulatory institutions, funding agencies,commercial partners, and scientific bodies, all interacting with each other andfacing different incentives. To maintain scientific rigor, statistical methodsshould acknowledge this state of affairs. To this end, we study hypothesistesting when there is an agent (e.g., a researcher or a pharmaceutical company)with a private prior about an unknown parameter and a principal (e.g., apolicymaker or regulator) who wishes to make decisions based on the parametervalue. The agent chooses whether to run a statistical trial based on theirprivate prior and then the result of the trial is used by the principal toreach a decision. We show how the principal can conduct statistical inferencethat leverages the information that is revealed by an agent's strategicbehavior -- their choice to run a trial or not. In particular, we show how theprincipal can design a policy to elucidate partial information about theagent's private prior beliefs and use this to control the posterior probabilityof the null. One implication is a simple guideline for the choice ofsignificance threshold in clinical trials: the type-I error level should be setto be strictly less than the cost of the trial divided by the firm's profit ifthe trial is successful.</description><author>Stephen Bates, Michael I. Jordan, Michael Sklar, Jake A. Soloff</author><pubDate>Thu, 08 Feb 2024 17:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03748v2</guid></item><item><title>Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery</title><link>http://arxiv.org/abs/2402.05860v1</link><description>Deep Neural Networks (DNNs) based semantic segmentation of the roboticinstruments and tissues can enhance the precision of surgical activities inrobot-assisted surgery. However, in biological learning, DNNs cannot learnincremental tasks over time and exhibit catastrophic forgetting, which refersto the sharp decline in performance on previously learned tasks after learninga new one. Specifically, when data scarcity is the issue, the model shows arapid drop in performance on previously learned instruments after learning newdata with new instruments. The problem becomes worse when it limits releasingthe dataset of the old instruments for the old model due to privacy concernsand the unavailability of the data for the new or updated version of theinstruments for the continual learning model. For this purpose, we develop aprivacy-preserving synthetic continual semantic segmentation framework byblending and harmonizing (i) open-source old instruments foreground to thesynthesized background without revealing real patient data in public and (ii)new instruments foreground to extensively augmented real background. To boostthe balanced logit distillation from the old model to the continual learningmodel, we design overlapping class-aware temperature normalization (CAT) bycontrolling model learning utility. We also introduce multi-scaleshifted-feature distillation (SD) to maintain long and short-range spatialrelationships among the semantic objects where conventional short-range spatialfeatures with limited information reduce the power of feature distillation. Wedemonstrate the effectiveness of our framework on the EndoVis 2017 and 2018instrument segmentation dataset with a generalized continual learning setting.Code is available at~\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.</description><author>Mengya Xu, Mobarakol Islam, Long Bai, Hongliang Ren</author><pubDate>Thu, 08 Feb 2024 17:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05860v1</guid></item><item><title>Learning to Route Among Specialized Experts for Zero-Shot Generalization</title><link>http://arxiv.org/abs/2402.05859v1</link><description>Recently, there has been a widespread proliferation of "expert" languagemodels that are specialized to a specific task or domain throughparameter-efficient fine-tuning. How can we recycle large collections of expertlanguage models to improve zero-shot generalization to unseen tasks? In thiswork, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean ofSpecialized Experts (PHATGOOSE), which learns to route among specializedmodules that were produced through parameter-efficient fine-tuning. Unlike pastmethods that learn to route among specialized models, PHATGOOSE explores thepossibility that zero-shot generalization will be improved if different expertscan be adaptively chosen for each token and at each layer in the model.Crucially, our method is post-hoc - it does not require simultaneous access tothe datasets used to create the specialized models and only requires a modestamount of additional compute after each expert model is trained. In experimentscovering a range of specialized model collections and zero-shot generalizationbenchmarks, we find that PHATGOOSE outperforms past methods for post-hocrouting and, in some cases, outperforms explicit multitask training (whichrequires simultaneous data access). To better understand the routing strategylearned by PHATGOOSE, we perform qualitative experiments to validate thatPHATGOOSE's performance stems from its ability to make adaptive per-token andper-module expert choices. We release all of our code to support future work onimproving zero-shot generalization by recycling specialized experts.</description><author>Mohammed Muqeeth, Haokun Liu, Yufan Liu, Colin Raffel</author><pubDate>Thu, 08 Feb 2024 17:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05859v1</guid></item><item><title>Adaptive Experimental Design for Policy Learning</title><link>http://arxiv.org/abs/2401.03756v3</link><description>Evidence-based targeting has been a topic of growing interest among thepractitioners of policy and business. Formulating decision-maker's policylearning as a fixed-budget best arm identification (BAI) problem withcontextual information, we study an optimal adaptive experimental design forpolicy learning with multiple treatment arms. In the sampling stage, theplanner assigns treatment arms adaptively over sequentially arrivingexperimental units upon observing their contextual information (covariates).After the experiment, the planner recommends an individualized assignment ruleto the population. Setting the worst-case expected regret as the performancecriterion of adaptive sampling and recommended policies, we derive itsasymptotic lower bounds, and propose a strategy, Adaptive Sampling-PolicyLearning strategy (PLAS), whose leading factor of the regret upper bound alignswith the lower bound as the size of experimental units increases.</description><author>Masahiro Kato, Kyohei Okumura, Takuya Ishihara, Toru Kitagawa</author><pubDate>Thu, 08 Feb 2024 17:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03756v3</guid></item><item><title>How to train your VAE</title><link>http://arxiv.org/abs/2309.13160v2</link><description>Variational Autoencoders (VAEs) have become a cornerstone in generativemodeling and representation learning within machine learning. This paperexplores a nuanced aspect of VAEs, focusing on interpreting the KullbackLeibler (KL) Divergence, a critical component within the Evidence Lower Bound(ELBO) that governs the trade off between reconstruction accuracy andregularization. Meanwhile, the KL Divergence enforces alignment between latentvariable distributions and a prior imposing a structure on the overall latentspace but leaves individual variable distributions unconstrained. The proposedmethod redefines the ELBO with a mixture of Gaussians for the posteriorprobability, introduces a regularization term to prevent variance collapse, andemploys a PatchGAN discriminator to enhance texture realism. Implementationdetails involve ResNetV2 architectures for both the Encoder and Decoder. Theexperiments demonstrate the ability to generate realistic faces, offering apromising solution for enhancing VAE based generative models.</description><author>Mariano Rivera</author><pubDate>Thu, 08 Feb 2024 17:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13160v2</guid></item><item><title>Personalized PCA: Decoupling Shared and Unique Features</title><link>http://arxiv.org/abs/2207.08041v2</link><description>In this paper, we tackle a significant challenge in PCA: heterogeneity. Whendata are collected from different sources with heterogeneous trends while stillsharing some congruency, it is critical to extract shared knowledge whileretaining the unique features of each source. To this end, we proposepersonalized PCA (PerPCA), which uses mutually orthogonal global and localprincipal components to encode both unique and shared features. We show that,under mild conditions, both unique and shared features can be identified andrecovered by a constrained optimization problem, even if the covariancematrices are immensely different. Also, we design a fully federated algorithminspired by distributed Stiefel gradient descent to solve the problem. Thealgorithm introduces a new group of operations called generalized retractionsto handle orthogonality constraints, and only requires global PCs to be sharedacross sources. We prove the linear convergence of the algorithm under suitableassumptions. Comprehensive numerical experiments highlight PerPCA's superiorperformance in feature extraction and prediction from heterogeneous datasets.As a systematic approach to decouple shared and unique features fromheterogeneous datasets, PerPCA finds applications in several tasks, includingvideo segmentation, topic extraction, and feature clustering.</description><author>Naichen Shi, Raed Al Kontar</author><pubDate>Thu, 08 Feb 2024 17:35:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.08041v2</guid></item><item><title>BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</title><link>http://arxiv.org/abs/2402.03216v2</link><description>In this paper, we present a new embedding model, called M3-Embedding, whichis distinguished for its versatility in Multi-Linguality, Multi-Functionality,and Multi-Granularity. It can support more than 100 working languages, leadingto new state-of-the-art performances on multi-lingual and cross-lingualretrieval tasks. It can simultaneously perform the three common retrievalfunctionalities of embedding model: dense retrieval, multi-vector retrieval,and sparse retrieval, which provides a unified model foundation for real-worldIR applications. It is able to process inputs of different granularities,spanning from short sentences to long documents of up to 8192 tokens. Theeffective training of M3-Embedding involves the following technicalcontributions. We propose a novel self-knowledge distillation approach, wherethe relevance scores from different retrieval functionalities can be integratedas the teacher signal to enhance the training quality. We also optimize thebatching strategy, enabling a large batch size and high training throughput toensure the discriminativeness of embeddings. To the best of our knowledge,M3-Embedding is the first embedding model which realizes such a strongversatility. The model and code will be publicly available athttps://github.com/FlagOpen/FlagEmbedding.</description><author>Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu</author><pubDate>Thu, 08 Feb 2024 17:32:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03216v2</guid></item><item><title>Orthogonal Transforms in Neural Networks Amount to Effective Regularization</title><link>http://arxiv.org/abs/2305.06344v2</link><description>We consider applications of neural networks in nonlinear systemidentification and formulate a hypothesis that adjusting general networkstructure by incorporating frequency information or other known orthogonaltransform, should result in an efficient neural network retaining its universalproperties. We show that such a structure is a universal approximator and thatusing any orthogonal transform in a proposed way implies regularization duringtraining by adjusting the learning rate of each parameter individually. Weempirically show in particular, that such a structure, using the Fouriertransform, outperforms equivalent models without orthogonality support.</description><author>Krzysztof Zając, Wojciech Sopot, Paweł Wachel</author><pubDate>Thu, 08 Feb 2024 17:31:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06344v2</guid></item><item><title>Robust Knowledge Extraction from Large Language Models using Social Choice Theory</title><link>http://arxiv.org/abs/2312.14877v2</link><description>Large-language models (LLMs) can support a wide range of applications likeconversational agents, creative writing or general query answering. However,they are ill-suited for query answering in high-stake domains like medicinebecause they are typically not robust - even the same query can result indifferent answers when prompted multiple times. In order to improve therobustness of LLM queries, we propose using ranking queries repeatedly and toaggregate the queries using methods from social choice theory. We study rankingqueries in diagnostic settings like medical and fault diagnosis and discuss howthe Partial Borda Choice function from the literature can be applied to mergemultiple query results. We discuss some additional interesting properties inour setting and evaluate the robustness of our approach empirically.</description><author>Nico Potyka, Yuqicheng Zhu, Yunjie He, Evgeny Kharlamov, Steffen Staab</author><pubDate>Thu, 08 Feb 2024 17:29:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14877v2</guid></item><item><title>CSL: Class-Agnostic Structure-Constrained Learning for Segmentation Including the Unseen</title><link>http://arxiv.org/abs/2312.05538v2</link><description>Addressing Out-Of-Distribution (OOD) Segmentation and Zero-Shot SemanticSegmentation (ZS3) is challenging, necessitating segmenting unseen classes.Existing strategies adapt the class-agnostic Mask2Former (CA-M2F) tailored tospecific tasks. However, these methods cater to singular tasks, demand trainingfrom scratch, and we demonstrate certain deficiencies in CA-M2F, which affectperformance. We propose the Class-Agnostic Structure-Constrained Learning(CSL), a plug-in framework that can integrate with existing methods, therebyembedding structural constraints and achieving performance gain, including theunseen, specifically OOD, ZS3, and domain adaptation (DA) tasks. There are twoschemes for CSL to integrate with existing methods (1) by distilling knowledgefrom a base teacher network, enforcing constraints across training andinference phrases, or (2) by leveraging established models to obtain per-pixeldistributions without retraining, appending constraints during the inferencephase. We propose soft assignment and mask split methodologies that enhance OODobject segmentation. Empirical evaluations demonstrate CSL's prowess inboosting the performance of existing algorithms spanning OOD segmentation, ZS3,and DA segmentation, consistently transcending the state-of-art across allthree tasks.</description><author>Hao Zhang, Fang Li, Lu Qi, Ming-Hsuan Yang, Narendra Ahuja</author><pubDate>Thu, 08 Feb 2024 17:20:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05538v2</guid></item><item><title>Towards More Efficient Shared Autonomous Mobility: A Learning-Based Fleet Repositioning Approach</title><link>http://arxiv.org/abs/2210.08659v3</link><description>Shared-use autonomous mobility services (SAMS) present new opportunities forimproving accessible and demand-responsive mobility. A fundamental challengethat SAMS face is appropriate positioning of idle fleet vehicles to meet futuredemand - a problem that strongly impacts service quality and efficiency. Thispaper formulates SAMS fleet repositioning as a Markov Decision Process andpresents a reinforcement learning-based repositioning (RLR) approach calledintegrated system-agent repositioning (ISR). The ISR learns a scalable fleetrepositioning strategy in an integrated manner: learning to respond to evolvingdemand patterns without explicit demand forecasting and to cooperate withoptimization-based passenger-to-vehicle assignment. Numerical experiments areconducted using New York City taxi data and an agent-based simulation tool. TheISR is compared to an alternative RLR approach named externally guidedrepositioning (EGR) and a benchmark joint optimization (JO) forpassenger-to-vehicle assignment and repositioning. The results demonstrate theRLR approaches' substantial reductions in passenger wait times, over 50%,relative to the JO approach. The ISR's ability to bypass demand forecasting isalso demonstrated as it maintains comparable performance to EGR in terms ofaverage metrics. The results also demonstrate the model's transferability toevolving conditions, including unseen demand patterns, extended operationalperiods, and changes in the assignment strategy.</description><author>Monika Filipovska, Michael Hyland, Haimanti Bala</author><pubDate>Thu, 08 Feb 2024 17:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.08659v3</guid></item><item><title>Dirichlet Flow Matching with Applications to DNA Sequence Design</title><link>http://arxiv.org/abs/2402.05841v1</link><description>Discrete diffusion or flow models could enable faster and more controllablesequence generation than autoregressive models. We show that na\"ive linearflow matching on the simplex is insufficient toward this goal since it suffersfrom discontinuities in the training target and further pathologies. Toovercome this, we develop Dirichlet flow matching on the simplex based onmixtures of Dirichlet distributions as probability paths. In this framework, wederive a connection between the mixtures' scores and the flow's vector fieldthat allows for classifier and classifier-free guidance. Further, we providedistilled Dirichlet flow matching, which enables one-step sequence generationwith minimal performance hits, resulting in $O(L)$ speedups compared toautoregressive models. On complex DNA sequence generation tasks, we demonstratesuperior performance compared to all baselines in distributional metrics and inachieving desired design targets for generated sequences. Finally, we show thatour classifier-free guidance approach improves unconditional generation and iseffective for generating DNA that satisfies design targets. Code is availableat https://github.com/HannesStark/dirichlet-flow-matching.</description><author>Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, Tommi Jaakkola</author><pubDate>Thu, 08 Feb 2024 17:18:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05841v1</guid></item><item><title>Topological Learning in Multi-Class Data Sets</title><link>http://arxiv.org/abs/2301.09734v4</link><description>We specialize techniques from topological data analysis to the problem ofcharacterizing the topological complexity (as defined in the body of the paper)of a multi-class data set. As a by-product, a topological classifier is definedthat uses an open sub-covering of the data set. This sub-covering can be usedto construct a simplicial complex whose topological features (e.g., Bettinumbers) provide information about the classification problem. We use thesetopological constructs to study the impact of topological complexity onlearning in feedforward deep neural networks (DNNs). We hypothesize thattopological complexity is negatively correlated with the ability of a fullyconnected feedforward deep neural network to learn to classify data correctly.We evaluate our topological classification algorithm on multiple constructedand open source data sets. We also validate our hypothesis regarding therelationship between topological complexity and learning in DNN's on multipledata sets.</description><author>Christopher Griffin, Trevor Karn, Benjamin Apple</author><pubDate>Thu, 08 Feb 2024 17:16:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09734v4</guid></item><item><title>Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults</title><link>http://arxiv.org/abs/2401.01172v2</link><description>Diagnosis of bearing faults is paramount to reducing maintenance costs andoperational breakdowns. Bearing faults are primary contributors to machinevibrations, and analyzing their signal morphology offers insights into theirhealth status. Unfortunately, existing approaches are optimized for controlledenvironments, neglecting realistic conditions such as time-varying rotationalspeeds and the vibration's non-stationary nature. This paper presents a fusionof time-frequency analysis and deep learning techniques to diagnose bearingfaults under time-varying speeds and varying noise levels. First, we formulatethe bearing fault-induced vibrations and discuss the link between theirnon-stationarity and the bearing's inherent and operational parameters. We alsoelucidate quadratic time-frequency distributions and validate theireffectiveness in resolving distinctive dynamic patterns associated withdifferent bearing faults. Based on this, we design a time-frequencyconvolutional neural network (TF-CNN) to diagnose various faults inrolling-element bearings. Our experimental findings undeniably demonstrate thesuperior performance of TF-CNN in comparison to recently developed techniques.They also assert its versatility in capturing fault-relevant non-stationaryfeatures that couple with speed changes and show its exceptional resilience tonoise, consistently surpassing competing methods across various signal-to-noiseratios and performance metrics. Altogether, the TF-CNN achieves substantialaccuracy improvements up to 15%, in severe noise conditions.</description><author>Mohammad Al-Sa'd, Tuomas Jalonen, Serkan Kiranyaz, Moncef Gabbouj</author><pubDate>Thu, 08 Feb 2024 17:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01172v2</guid></item><item><title>How Much is Unseen Depends Chiefly on Information About the Seen</title><link>http://arxiv.org/abs/2402.05835v1</link><description>It might seem counter-intuitive at first: We find that, in expectation, theproportion of data points in an unknown population-that belong to classes thatdo not appear in the training data-is almost entirely determined by the number$f_k$ of classes that do appear in the training data the same number of times.While in theory we show that the difference of the induced estimator decaysexponentially in the size of the sample, in practice the high variance preventsus from using it directly for an estimator of the sample coverage. However, ourprecise characterization of the dependency between $f_k$'s induces a largesearch space of different representations of the expected value, which can bedeterministically instantiated as estimators. Hence, we turn to optimizationand develop a genetic algorithm that, given only the sample, searches for anestimator with minimal mean-squared error (MSE). In our experiments, ourgenetic algorithm discovers estimators that have a substantially smaller MSEthan the state-of-the-art Good-Turing estimator. This holds for over 96% ofruns when there are at least as many samples as classes. Our estimators' MSE isroughly 80% of the Good-Turing estimator's.</description><author>Seongmin Lee, Marcel Böhme</author><pubDate>Thu, 08 Feb 2024 17:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05835v1</guid></item><item><title>Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting</title><link>http://arxiv.org/abs/2402.05830v1</link><description>Time series analysis is vital for numerous applications, and transformershave become increasingly prominent in this domain. Leading methods customizethe transformer architecture from NLP and CV, utilizing a patching technique toconvert continuous signals into segments. Yet, time series data are uniquelychallenging due to significant distribution shifts and intrinsic noise levels.To address these two challenges,we introduce the Sparse Vector QuantizedFFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparsevector quantization technique coupled with Reverse Instance Normalization(RevIN) to reduce noise impact and capture sufficient statistics forforecasting, serving as an alternative to the Feed-Forward layer (FFN) in thetransformer architecture. Our FFN-free approach trims the parameter count,enhancing computational efficiency and reducing overfitting. Throughevaluations across ten benchmark datasets, including the newly introduced CAISOdataset, Sparse-VQ surpasses leading models with a 7.84% and 4.17% decrease inMAE for univariate and multivariate time series forecasting, respectively.Moreover, it can be seamlessly integrated with existing transformer-basedmodels to elevate their performance.</description><author>Yanjun Zhao, Tian Zhou, Chao Chen, Liang Sun, Yi Qian, Rong Jin</author><pubDate>Thu, 08 Feb 2024 17:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05830v1</guid></item><item><title>Limitations of Agents Simulated by Predictive Models</title><link>http://arxiv.org/abs/2402.05829v1</link><description>There is increasing focus on adapting predictive models into agent-likesystems, most notably AI assistants based on language models. We outline twostructural reasons for why these models can fail when turned into agents.First, we discuss auto-suggestive delusions. Prior work has shown theoreticallythat models fail to imitate agents that generated the training data if theagents relied on hidden observations: the hidden observations act asconfounding variables, and the models treat actions they generate as evidencefor nonexistent observations. Second, we introduce and formally study arelated, novel limitation: predictor-policy incoherence. When a model generatesa sequence of actions, the model's implicit prediction of the policy thatgenerated those actions can serve as a confounding variable. The result is thatmodels choose actions as if they expect future actions to be suboptimal,causing them to be overly conservative. We show that both of those failures arefixed by including a feedback loop from the environment, that is, re-trainingthe models on their own actions. We give simple demonstrations of bothlimitations using Decision Transformers and confirm that empirical resultsagree with our conceptual and formal analysis. Our treatment provides aunifying view of those failure modes, and informs the question of whyfine-tuning offline learned policies with online learning makes them moreeffective.</description><author>Raymond Douglas, Jacek Karwowski, Chan Bae, Andis Draguns, Victoria Krakovna</author><pubDate>Thu, 08 Feb 2024 17:08:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05829v1</guid></item><item><title>Discovering Temporally-Aware Reinforcement Learning Algorithms</title><link>http://arxiv.org/abs/2402.05828v1</link><description>Recent advancements in meta-learning have enabled the automatic discovery ofnovel reinforcement learning algorithms parameterized by surrogate objectivefunctions. To improve upon manually designed algorithms, the parameterizationof this learned objective function must be expressive enough to represent novelprinciples of learning (instead of merely recovering already established ones)while still generalizing to a wide range of settings outside of itsmeta-training distribution. However, existing methods focus on discoveringobjective functions that, like many widely used objective functions inreinforcement learning, do not take into account the total number of stepsallowed for training, or "training horizon". In contrast, humans use a plethoraof different learning objectives across the course of acquiring a new ability.For instance, students may alter their studying techniques based on theproximity to exam deadlines and their self-assessed capabilities. This papercontends that ignoring the optimization time horizon significantly restrictsthe expressive potential of discovered learning algorithms. We propose a simpleaugmentation to two existing objective discovery approaches that allows thediscovered algorithm to dynamically update its objective function throughoutthe agent's training procedure, resulting in expressive schedules and increasedgeneralization across different training horizons. In the process, we find thatcommonly used meta-gradient approaches fail to discover such adaptive objectivefunctions while evolution strategies discover highly dynamic learning rules. Wedemonstrate the effectiveness of our approach on a wide range of tasks andanalyze the resulting learned algorithms, which we find effectively balanceexploration and exploitation by modifying the structure of their learning rulesthroughout the agent's lifetime.</description><author>Matthew Thomas Jackson, Chris Lu, Louis Kirsch, Robert Tjarko Lange, Shimon Whiteson, Jakob Nicolaus Foerster</author><pubDate>Thu, 08 Feb 2024 17:07:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05828v1</guid></item><item><title>Is it Possible to Edit Large Language Models Robustly?</title><link>http://arxiv.org/abs/2402.05827v1</link><description>Large language models (LLMs) have played a pivotal role in buildingcommunicative AI to imitate human behaviors but face the challenge of efficientcustomization. To tackle this challenge, recent studies have delved into therealm of model editing, which manipulates specific memories of language modelsand changes the related language generation. However, the robustness of modelediting remains an open question. This work seeks to understand the strengthsand limitations of editing methods, thus facilitating robust, realisticapplications of communicative AI. Concretely, we conduct extensive analysis toaddress the three key research questions. Q1: Can edited LLMs behaveconsistently resembling communicative AI in realistic situations? Q2: To whatextent does the rephrasing of prompts lead LLMs to deviate from the editedknowledge memory? Q3: Which knowledge features are correlated with theperformance and robustness of editing? Our experimental results uncover asubstantial disparity between existing editing methods and the practicalapplication of LLMs. On rephrased prompts that are complex and flexible butcommon in realistic applications, the performance of editing experiences asignificant decline. Further analysis shows that more popular knowledge ismemorized better, easier to recall, and more challenging to edit effectively.</description><author>Xinbei Ma, Tianjie Ju, Jiyang Qiu, Zhuosheng Zhang, Hai Zhao, Lifeng Liu, Yulong Wang</author><pubDate>Thu, 08 Feb 2024 17:06:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05827v1</guid></item><item><title>FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting</title><link>http://arxiv.org/abs/2402.05823v1</link><description>Accurate solar power forecasting is crucial to integrate photovoltaic plantsinto the electric grid, schedule and secure the power grid safety. This problembecomes more demanding for those newly installed solar plants which lacksufficient data. Current research predominantly relies on historical solarpower data or numerical weather prediction in a single-modality format,ignoring the complementary information provided in different modalities. Inthis paper, we propose a multi-modality fusion framework to integratehistorical power data, numerical weather prediction, and satellite images,significantly improving forecast performance. We introduce a vector quantizedframework that aligns modalities with varying information densities, striking abalance between integrating sufficient information and averting modeloverfitting. Our framework demonstrates strong zero-shot forecastingcapability, which is especially useful for those newly installed plants.Moreover, we collect and release a multi-modal solar power (MMSP) dataset fromreal-world plants to further promote the research of multi-modal solarforecasting algorithms. Our extensive experiments show that our model not onlyoperates with robustness but also boosts accuracy in both zero-shot forecastingand scenarios rich with training data, surpassing leading models. We haveincorporated it into our eForecaster platform and deployed it for more than 300solar plants with a capacity of over 15GW.</description><author>Ziqing Ma, Wenwei Wang, Tian Zhou, Chao Chen, Bingqing Peng, Liang Sun, Rong Jin</author><pubDate>Thu, 08 Feb 2024 17:03:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05823v1</guid></item><item><title>S-Agents: self-organizing agents in open-ended environment</title><link>http://arxiv.org/abs/2402.04578v2</link><description>Leveraging large language models (LLMs), autonomous agents have significantlyimproved, gaining the ability to handle a variety of tasks. In open-endedsettings, optimizing collaboration for efficiency and effectiveness demandsflexible adjustments. Despite this, current research mainly emphasizes fixed,task-oriented workflows and overlooks agent-centric organizational structures.Drawing inspiration from human organizational behavior, we introduce aself-organizing agent system (S-Agents) with a "tree of agents" structure fordynamic workflow, an "hourglass agent architecture" for balancing informationpriorities, and a "non-obstructive collaboration" method to allow asynchronoustask execution among agents. This structure can autonomously coordinate a groupof agents, efficiently addressing the challenges of an open and dynamicenvironment without human intervention. Our experiments demonstrate thatS-Agents proficiently execute collaborative building tasks and resourcecollection in the Minecraft environment, validating their effectiveness.</description><author>Jiaqi Chen, Yuxian Jiang, Jiachen Lu, Li Zhang</author><pubDate>Thu, 08 Feb 2024 17:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04578v2</guid></item><item><title>Continuous Multidimensional Scaling</title><link>http://arxiv.org/abs/2402.04436v2</link><description>Multidimensional scaling (MDS) is the act of embedding proximity informationabout a set of $n$ objects in $d$-dimensional Euclidean space. As originallyconceived by the psychometric community, MDS was concerned with embedding afixed set of proximities associated with a fixed set of objects. Modernconcerns, e.g., that arise in developing asymptotic theories for statisticalinference on random graphs, more typically involve studying the limitingbehavior of a sequence of proximities associated with an increasing set ofobjects. Standard results from the theory of point-to-set maps imply that, if$n$ is fixed and a sequence of proximities converges, then the limit of theembedded structures is the embedded structure of the limiting proximities. Butwhat if $n$ increases? It then becomes necessary to reformulate MDS so that theentire sequence of embedding problems can be viewed as a sequence ofoptimization problems in a fixed space. We present such a reformulation andderive some consequences.</description><author>Michael W. Trosset, Carey E. Priebe</author><pubDate>Thu, 08 Feb 2024 17:00:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04436v2</guid></item><item><title>ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms</title><link>http://arxiv.org/abs/2305.04359v2</link><description>Approximate nearest-neighbor search (ANNS) algorithms are a key part of themodern deep learning stack due to enabling efficient similarity search overhigh-dimensional vector space representations (i.e., embeddings) of data. Amongvarious ANNS algorithms, graph-based algorithms are known to achieve the bestthroughput-recall tradeoffs. Despite the large scale of modern ANNS datasets,existing parallel graph based implementations suffer from significantchallenges to scale to large datasets due to heavy use of locks and othersequential bottlenecks, which 1) prevents them from efficiently scaling to alarge number of processors, and 2) results in nondeterminism that isundesirable in certain applications. In this paper, we introduce ParlayANN, a library of deterministic andparallel graph-based approximate nearest neighbor search algorithms, along witha set of useful tools for developing such algorithms. In this library, wedevelop novel parallel implementations for four state-of-the-art graph-basedANNS algorithms that scale to billion-scale datasets. Our algorithms aredeterministic and achieve high scalability across a diverse set of challengingdatasets. In addition to the new algorithmic ideas, we also conduct a detailedexperimental study of our new algorithms as well as two existing non-graphapproaches. Our experimental results both validate the effectiveness of our newtechniques, and lead to a comprehensive comparison among ANNS algorithms onlarge scale datasets with a list of interesting findings.</description><author>Magdalen Dobson Manohar, Zheqi Shen, Guy E. Blelloch, Laxman Dhulipala, Yan Gu, Harsha Vardhan Simhadri, Yihan Sun</author><pubDate>Thu, 08 Feb 2024 17:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04359v2</guid></item><item><title>Guided Evolution with Binary Discriminators for ML Program Search</title><link>http://arxiv.org/abs/2402.05821v1</link><description>How to automatically design better machine learning programs is an openproblem within AutoML. While evolution has been a popular tool to search forbetter ML programs, using learning itself to guide the search has been lesssuccessful and less understood on harder problems but has the promise todramatically increase the speed and final performance of the optimizationprocess. We propose guiding evolution with a binary discriminator, trainedonline to distinguish which program is better given a pair of programs. Thediscriminator selects better programs without having to perform a costlyevaluation and thus speed up the convergence of evolution. Our method canencode a wide variety of ML components including symbolic optimizers, neuralarchitectures, RL loss functions, and symbolic regression equations with thesame directed acyclic graph representation. By combining this representationwith modern GNNs and an adaptive mutation strategy, we demonstrate our methodcan speed up evolution across a set of diverse problems including a 3.7xspeedup on the symbolic search for ML optimizers and a 4x speedup for RL lossfunctions.</description><author>John D. Co-Reyes, Yingjie Miao, George Tucker, Aleksandra Faust, Esteban Real</author><pubDate>Thu, 08 Feb 2024 16:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05821v1</guid></item><item><title>How Far Can Fairness Constraints Help Recover From Biased Data?</title><link>http://arxiv.org/abs/2312.10396v3</link><description>A general belief in fair classification is that fairness constraints incur atrade-off with accuracy, which biased data may worsen. Contrary to this belief,Blum &amp; Stangl (2019) show that fair classification with equal opportunityconstraints even on extremely biased data can recover optimally accurate andfair classifiers on the original data distribution. Their result is interestingbecause it demonstrates that fairness constraints can implicitly rectify databias and simultaneously overcome a perceived fairness-accuracy trade-off. Theirdata bias model simulates under-representation and label bias inunderprivileged population, and they show the above result on a stylized datadistribution with i.i.d. label noise, under simple conditions on the datadistribution and bias parameters. We propose a general approach to extend theresult of Blum &amp; Stangl (2019) to different fairness constraints, data biasmodels, data distributions, and hypothesis classes. We strengthen their result,and extend it to the case when their stylized distribution has labels withMassart noise instead of i.i.d. noise. We prove a similar recovery result forarbitrary data distributions using fair reject option classifiers. We furthergeneralize it to arbitrary data distributions and arbitrary hypothesis classes,i.e., we prove that for any data distribution, if the optimally accurateclassifier in a given hypothesis class is fair and robust, then it can berecovered through fair classification with equal opportunity constraints on thebiased distribution whenever the bias parameters satisfy certain simpleconditions. Finally, we show applications of our technique to time-varying databias in classification and fair machine learning pipelines.</description><author>Mohit Sharma, Amit Deshpande</author><pubDate>Thu, 08 Feb 2024 16:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10396v3</guid></item><item><title>Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model</title><link>http://arxiv.org/abs/2402.05819v1</link><description>Recent advances in self-supervised speech models have shown significantimprovement in many downstream tasks. However, these models predominantlycentered on frame-level training objectives, which can fall short in spokenlanguage understanding tasks that require semantic comprehension. Existingworks often rely on additional speech-text data as intermediate targets, whichis costly in the real-world setting. To address this challenge, we proposePseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-leveltargets into the training process, where the targets are derived from avisually-ground speech model, notably eliminating the need for speech-textpaired data. Our experimental results on four spoken language understanding(SLU) benchmarks suggest the superiority of our model in capturing semanticinformation.</description><author>Hung-Chieh Fang, Nai-Xuan Ye, Yi-Jen Shih, Puyuan Peng, Hsuan-Fu Wang, Layne Berry, Hung-yi Lee, David Harwath</author><pubDate>Thu, 08 Feb 2024 16:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05819v1</guid></item><item><title>Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A Supervised Contrastive Learning</title><link>http://arxiv.org/abs/2402.05817v1</link><description>Introduction This study explores the use of the latest You Only Look Once(YOLO V7) object detection method to enhance kidney detection in medicalimaging by training and testing a modified YOLO V7 on medical image formats.Methods Study includes 878 patients with various subtypes of renal cellcarcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scansfor 1084 patients were retrieved. 326 patients with 1034 tumors recruited froma retrospective maintained database, and bounding boxes were drawn around theirtumors. A primary model was trained on 80% of annotated cases, with 20% savedfor testing (primary test set). The best primary model was then used toidentify tumors in the remaining 861 patients and bounding box coordinates weregenerated on their scans using the model. Ten benchmark training sets werecreated with generated coordinates on not-segmented patients. The final modelused to predict the kidney in the primary test set. We reported the positivepredictive value (PPV), sensitivity, and mean average precision (mAP). ResultsThe primary training set showed an average PPV of 0.94 +/- 0.01, sensitivity of0.87 +/- 0.04, and mAP of 0.91 +/- 0.02. The best primary model yielded a PPVof 0.97, sensitivity of 0.92, and mAP of 0.95. The final model demonstrated anaverage PPV of 0.95 +/- 0.03, sensitivity of 0.98 +/- 0.004, and mAP of 0.95+/- 0.01. Conclusion Using a semi-supervised approach with a medical imagelibrary, we developed a high-performing model for kidney detection. Furtherexternal validation is required to assess the model's generalizability.</description><author>Pouria Yazdian Anari, Fiona Obiezu, Nathan Lay, Fatemeh Dehghani Firouzabadi, Aditi Chaurasia, Mahshid Golagha, Shiva Singh, Fatemeh Homayounieh, Aryan Zahergivar, Stephanie Harmon, Evrim Turkbey, Rabindra Gautam, Kevin Ma, Maria Merino, Elizabeth C. Jones, Mark W. Ball, W. Marston Linehan, Baris Turkbey, Ashkan A. Malayeri</author><pubDate>Thu, 08 Feb 2024 16:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05817v1</guid></item><item><title>The Virtues of Pessimism in Inverse Reinforcement Learning</title><link>http://arxiv.org/abs/2402.02616v2</link><description>Inverse Reinforcement Learning (IRL) is a powerful framework for learningcomplex behaviors from expert demonstrations. However, it traditionallyrequires repeatedly solving a computationally expensive reinforcement learning(RL) problem in its inner loop. It is desirable to reduce the explorationburden by leveraging expert demonstrations in the inner-loop RL. As an example,recent work resets the learner to expert states in order to inform the learnerof high-reward expert states. However, such an approach is infeasible in thereal world. In this work, we consider an alternative approach to speeding upthe RL subroutine in IRL: \emph{pessimism}, i.e., staying close to the expert'sdata distribution, instantiated via the use of offline RL algorithms. Weformalize a connection between offline RL and IRL, enabling us to use anarbitrary offline RL algorithm to improve the sample efficiency of IRL. Wevalidate our theory experimentally by demonstrating a strong correlationbetween the efficacy of an offline RL algorithm and how well it works as partof an IRL procedure. By using a strong offline RL algorithm as part of an IRLprocedure, we are able to find policies that match expert performancesignificantly more efficiently than the prior art.</description><author>David Wu, Gokul Swamy, J. Andrew Bagnell, Zhiwei Steven Wu, Sanjiban Choudhury</author><pubDate>Thu, 08 Feb 2024 16:50:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02616v2</guid></item><item><title>Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models</title><link>http://arxiv.org/abs/2402.05813v1</link><description>The aim of this study is to investigate Machine Unlearning (MU), a burgeoningfield focused on addressing concerns related to neural models inadvertentlyretaining personal or sensitive data. Here, a novel approach is introduced toachieve precise and selective forgetting within language models. Unlikeprevious methodologies that adopt completely opposing training objectives, thisapproach aims to mitigate adverse effects on language model performance,particularly in generation tasks. Furthermore, two innovative evaluationmetrics are proposed: Sensitive Information Extraction Likelihood (S-EL) andSensitive Information Memory Accuracy (S-MA), designed to gauge theeffectiveness of sensitive information elimination. To reinforce the forgettingframework, an effective method for annotating sensitive scopes is presented,involving both online and offline strategies. The online selection mechanismleverages language probability scores to ensure computational efficiency, whilethe offline annotation entails a robust two-stage process based on LargeLanguage Models (LLMs).</description><author>Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, Georg Gottlob</author><pubDate>Thu, 08 Feb 2024 16:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05813v1</guid></item><item><title>FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension</title><link>http://arxiv.org/abs/2402.05812v1</link><description>Frequently Asked Questions (FAQs) refer to the most common inquiries aboutspecific content. They serve as content comprehension aids by simplifyingtopics and enhancing understanding through succinct presentation ofinformation. In this paper, we address FAQ generation as a well-defined NaturalLanguage Processing (NLP) task through the development of an end-to-end systemleveraging text-to-text transformation models. We present a literature reviewcovering traditional question-answering systems, highlighting their limitationswhen applied directly to the FAQ generation task. We propose our system capableof building FAQs from textual content tailored to specific domains, enhancingtheir accuracy and relevance. We utilise self-curated algorithms for obtainingoptimal representation of information to be provided as input and also forranking the question-answer pairs to maximise human comprehension. Qualitativehuman evaluation showcases the generated FAQs to be well-constructed andreadable, while also utilising domain-specific constructs to highlightdomain-based nuances and jargon in the original content.</description><author>Sahil Kale, Gautam Khaire, Jay Patankar</author><pubDate>Thu, 08 Feb 2024 16:49:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05812v1</guid></item><item><title>You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement</title><link>http://arxiv.org/abs/2402.05809v1</link><description>Low-Light Image Enhancement (LLIE) task tends to restore the details andvisual information from corrupted low-light images. Most existing methods learnthe mapping function between low/normal-light images by Deep Neural Networks(DNNs) on sRGB and HSV color space. Nevertheless, enhancement involvesamplifying image signals, and applying these color spaces to low-light imageswith a low signal-to-noise ratio can introduce sensitivity and instability intothe enhancement process. Consequently, this results in the presence of colorartifacts and brightness artifacts in the enhanced images. To alleviate thisproblem, we propose a novel trainable color space, namedHorizontal/Vertical-Intensity (HVI). It not only decouples brightness and colorfrom RGB channels to mitigate the instability during enhancement but alsoadapts to low-light images in different illumination ranges due to thetrainable parameters. Further, we design a novel Color and Intensity DecouplingNetwork (CIDNet) with two branches dedicated to processing the decoupled imagebrightness and color in the HVI space. Within CIDNet, we introduce theLightweight Cross-Attention (LCA) module to facilitate interaction betweenimage structure and content information in both branches, while alsosuppressing noise in low-light images. Finally, we conducted 22 quantitativeand qualitative experiments to show that the proposed CIDNet outperforms thestate-of-the-art methods on 11 datasets. The code will be available athttps://github.com/Fediory/HVI-CIDNet.</description><author>Yixu Feng, Cheng Zhang, Pei Wang, Peng Wu, Qingsen Yan, Yanning Zhang</author><pubDate>Thu, 08 Feb 2024 16:47:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05809v1</guid></item><item><title>Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization</title><link>http://arxiv.org/abs/2305.19838v4</link><description>Bayesian Optimization (BO) is typically used to optimize an unknown function$f$ that is noisy and costly to evaluate, by exploiting an acquisition functionthat must be maximized at each optimization step. Even if provablyasymptotically optimal BO algorithms are efficient at optimizinglow-dimensional functions, scaling them to high-dimensional spaces remains anopen problem, often tackled by assuming an additive structure for $f$. By doingso, BO algorithms typically introduce additional restrictive assumptions on theadditive structure that reduce their applicability domain. This paper containstwo main contributions: (i) we relax the restrictive assumptions on theadditive structure of $f$ without weakening the maximization guarantees of theacquisition function, and (ii) we address the over-exploration problem fordecentralized BO algorithms. To these ends, we propose DuMBO, an asymptoticallyoptimal decentralized BO algorithm that achieves very competitive performanceagainst state-of-the-art BO algorithms, especially when the additive structureof $f$ comprises high-dimensional factors.</description><author>Anthony Bardou, Patrick Thiran, Thomas Begin</author><pubDate>Thu, 08 Feb 2024 16:47:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19838v4</guid></item><item><title>Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning</title><link>http://arxiv.org/abs/2402.05808v1</link><description>In this paper, we propose R$^3$: Learning Reasoning through ReverseCurriculum Reinforcement Learning (RL), a novel method that employs onlyoutcome supervision to achieve the benefits of process supervision for largelanguage models. The core challenge in applying RL to complex reasoning is toidentify a sequence of actions that result in positive rewards and provideappropriate supervision for optimization. Outcome supervision provides sparserewards for final results without identifying error locations, whereas processsupervision offers step-wise rewards but requires extensive manual annotation.R$^3$ overcomes these limitations by learning from correct demonstrations.Specifically, R$^3$ progressively slides the start state of reasoning from ademonstration's end to its beginning, facilitating easier model exploration atall stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcomesupervision to offer step-level signals and precisely pinpoint errors. UsingLlama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$points on average. Notebaly, in program-based reasoning on GSM8K, it exceedsthe baseline by $4.2$ points across three backbone models, and without anyextra data, Codellama-7B + R$^3$ performs comparable to larger models orclosed-source models.</description><author>Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, Xuanjing Huang</author><pubDate>Thu, 08 Feb 2024 16:46:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05808v1</guid></item><item><title>On Calibration and Conformal Prediction of Deep Classifiers</title><link>http://arxiv.org/abs/2402.05806v1</link><description>In many classification applications, the prediction of a deep neural network(DNN) based classifier needs to be accompanied with some confidence indication.Two popular post-processing approaches for that aim are: 1) calibration:modifying the classifier's softmax values such that their maximum (associatedwith the prediction) better estimates the correctness probability; and 2)conformal prediction (CP): devising a score (based on the softmax values) fromwhich a set of predictions with theoretically guaranteed marginal coverage ofthe correct class is produced. While in practice both types of indications canbe desired, so far the interplay between them has not been investigated. Towardfilling this gap, in this paper we study the effect of temperature scaling,arguably the most common calibration technique, on prominent CP methods. Westart with an extensive empirical study that among other insights shows that,surprisingly, calibration has a detrimental effect on popular adaptive CPmethods: it frequently leads to larger prediction sets. Then, we turn totheoretically analyze this behavior. We reveal several mathematical propertiesof the procedure, according to which we provide a reasoning for the phenomenon.Our study suggests that it may be worthwhile to utilize adaptive CP methods,chosen for their enhanced conditional coverage, based on softmax values priorto (or after canceling) temperature scaling calibration.</description><author>Lahav Dabah, Tom Tirer</author><pubDate>Thu, 08 Feb 2024 16:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05806v1</guid></item><item><title>InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write</title><link>http://arxiv.org/abs/2402.05804v1</link><description>Digital note-taking is gaining popularity, offering a durable, editable, andeasily indexable way of storing notes in the vectorized form, known as digitalink. However, a substantial gap remains between this way of note-taking andtraditional pen-and-paper note-taking, a practice still favored by a vastmajority. Our work, InkSight, aims to bridge the gap by empowering physicalnote-takers to effortlessly convert their work (offline handwriting) to digitalink (online handwriting), a process we refer to as Derendering. Prior researchon the topic has focused on the geometric properties of images, resulting inlimited generalization beyond their training domains. Our approach combinesreading and writing priors, allowing training a model in the absence of largeamounts of paired samples, which are difficult to obtain. To our knowledge,this is the first work that effectively derenders handwritten text in arbitraryphotos with diverse visual characteristics and backgrounds. Furthermore, itgeneralizes beyond its training domain into simple sketches. Our humanevaluation reveals that 87% of the samples produced by our model on thechallenging HierText dataset are considered as a valid tracing of the inputimage and 67% look like a pen trajectory traced by a human.</description><author>Blagoj Mitrevski, Arina Rak, Julian Schnitzler, Chengkun Li, Andrii Maksai, Jesse Berent, Claudiu Musat</author><pubDate>Thu, 08 Feb 2024 16:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05804v1</guid></item><item><title>AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning</title><link>http://arxiv.org/abs/2402.05803v1</link><description>We introduce an approach for 3D head avatar generation and editing withmulti-modal conditioning based on a 3D Generative Adversarial Network (GAN) anda Latent Diffusion Model (LDM). 3D GANs can generate high-quality head avatarsgiven a single or no condition. However, it is challenging to generate samplesthat adhere to multiple conditions of different modalities. On the other hand,LDMs excel at learning complex conditional distributions. To this end, wepropose to exploit the conditioning capabilities of LDMs to enable multi-modalcontrol over the latent space of a pre-trained 3D GAN. Our method can generateand edit 3D head avatars given a mixture of control signals such as RGB input,segmentation masks, and global attributes. This provides better control overthe generation and editing of synthetic avatars both globally and locally.Experiments show that our proposed approach outperforms a solely GAN-basedapproach both qualitatively and quantitatively on generation and editing tasks.To the best of our knowledge, our approach is the first to introducemulti-modal conditioning to 3D avatar generation and editing.\\href{avatarmmc-sig24.github.io}{Project Page}</description><author>Wamiq Reyaz Para, Abdelrahman Eldesokey, Zhenyu Li, Pradyumna Reddy, Jiankang Deng, Peter Wonka</author><pubDate>Thu, 08 Feb 2024 16:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05803v1</guid></item><item><title>Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence</title><link>http://arxiv.org/abs/2402.05802v1</link><description>Insufficiently precise diagnosis of clinical disease is likely responsiblefor many treatment failures, even for common conditions and treatments. With alarge enough dataset, it may be possible to use unsupervised machine learningto define clinical disease patterns more precisely. We present an approach tolearning these patterns by using probabilistic independence to disentangle theimprint on the medical record of causal latent sources of disease. We inferreda broad set of 2000 clinical signatures of latent sources from 9195 variablesin 269,099 Electronic Health Records. The learned signatures produced betterdiscrimination than the original variables in a lung cancer prediction taskunknown to the inference algorithm, predicting 3-year malignancy in patientswith no history of cancer before a solitary lung nodule was discovered. Moreimportantly, the signatures' greater explanatory power identified pre-nodulesignatures of apparently undiagnosed cancer in many of those patients.</description><author>Thomas A. Lasko, John M. Still, Thomas Z. Li, Marco Barbero Mota, William W. Stead, Eric V. Strobl, Bennett A. Landman, Fabien Maldonado</author><pubDate>Thu, 08 Feb 2024 16:41:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05802v1</guid></item><item><title>TaE: Task-aware Expandable Representation for Long Tail Class Incremental Learning</title><link>http://arxiv.org/abs/2402.05797v1</link><description>Class-incremental learning (CIL) aims to train classifiers that learn newclasses without forgetting old ones. Most CIL methods focus on balanced datadistribution for each task, overlooking real-world long-tailed distributions.Therefore, Long-Tailed Class-Incremental Learning (LT-CIL) has been introduced,which trains on data where head classes have more samples than tail classes.Existing methods mainly focus on preserving representative samples fromprevious classes to combat catastrophic forgetting. Recently, dynamic networkalgorithms frozen old network structures and expanded new ones, achievingsignificant performance. However, with the introduction of the long-tailproblem, merely extending task-specific parameters can lead to miscalibratedpredictions, while expanding the entire model results in an explosion of memorysize. To address these issues, we introduce a novel Task-aware Expandable (TaE)framework, dynamically allocating and updating task-specific trainableparameters to learn diverse representations from each incremental task, whileresisting forgetting through the majority of frozen model parameters. Tofurther encourage the class-specific feature representation, we develop aCentroid-Enhanced (CEd) method to guide the update of these task-awareparameters. This approach is designed to adaptively minimize the distancesbetween intra-class features while simultaneously maximizing the distancesbetween inter-class features across all seen classes. The utility of thiscentroid-enhanced method extends to all "training from scratch" CIL algorithms.Extensive experiments were conducted on CIFAR-100 and ImageNet100 underdifferent settings, which demonstrates that TaE achieves state-of-the-artperformance.</description><author>Linjie Li, S. Liu, Zhenyu Wu, JI yang</author><pubDate>Thu, 08 Feb 2024 16:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05797v1</guid></item><item><title>Phonetically rich corpus construction for a low-resourced language</title><link>http://arxiv.org/abs/2402.05794v1</link><description>Speech technologies rely on capturing a speaker's voice variability whileobtaining comprehensive language information. Textual prompts and sentenceselection methods have been proposed in the literature to comprise suchadequate phonetic data, referred to as a phonetically rich \textit{corpus}.However, they are still insufficient for acoustic modeling, especially criticalfor languages with limited resources. Hence, this paper proposes a novelapproach and outlines the methodological aspects required to create a\textit{corpus} with broad phonetic coverage for a low-resourced language,Brazilian Portuguese. Our methodology includes text dataset collection up to asentence selection algorithm based on triphone distribution. Furthermore, wepropose a new phonemic classification according to acoustic-articulatory speechfeatures since the absolute number of distinct triphones, or low-probabilitytriphones, does not guarantee an adequate representation of every possiblecombination. Using our algorithm, we achieve a 55.8\% higher percentage ofdistinct triphones -- for samples of similar size -- while the currentlyavailable phonetic-rich corpus, CETUC and TTS-Portuguese, 12.6\% and 12.3\% incomparison to a non-phonetically rich dataset.</description><author>Marcellus Amadeus, William Alberto Cruz Castañeda, Wilmer Lobato, Niasche Aquino</author><pubDate>Thu, 08 Feb 2024 16:36:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05794v1</guid></item><item><title>Sensitivity, Performance, Robustness: Deconstructing the Effect of Sociodemographic Prompting</title><link>http://arxiv.org/abs/2309.07034v2</link><description>Annotators' sociodemographic backgrounds (i.e., the individual compositionsof their gender, age, educational background, etc.) have a strong impact ontheir decisions when working on subjective NLP tasks, such as toxic languagedetection. Often, heterogeneous backgrounds result in high disagreements. Tomodel this variation, recent work has explored sociodemographic prompting, atechnique, which steers the output of prompt-based models towards answers thathumans with specific sociodemographic profiles would give. However, theavailable NLP literature disagrees on the efficacy of this technique - itremains unclear for which tasks and scenarios it can help, and the role of theindividual factors in sociodemographic prompting is still unexplored. Weaddress this research gap by presenting the largest and most comprehensivestudy of sociodemographic prompting today. We analyze its influence on modelsensitivity, performance and robustness across seven datasets and sixinstruction-tuned model families. We show that sociodemographic informationaffects model predictions and can be beneficial for improving zero-shotlearning in subjective NLP tasks. However, its outcomes largely vary fordifferent model types, sizes, and datasets, and are subject to large variancewith regards to prompt formulations. Most importantly, our results show thatsociodemographic prompting should be used with care for sensitive applications,such as toxicity annotation or when studying LLM alignment. Code and data:https://github.com/UKPLab/arxiv2023-sociodemographic-prompting</description><author>Tilman Beck, Hendrik Schuff, Anne Lauscher, Iryna Gurevych</author><pubDate>Thu, 08 Feb 2024 16:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07034v2</guid></item><item><title>Determining the significance and relative importance of parameters of a simulated quenching algorithm using statistical tools</title><link>http://arxiv.org/abs/2402.05791v1</link><description>When search methods are being designed it is very important to know whichparameters have the greatest influence on the behaviour and performance of thealgorithm. To this end, algorithm parameters are commonly calibrated by meansof either theoretic analysis or intensive experimentation. When undertaking adetailed statistical analysis of the influence of each parameter, the designershould pay attention mostly to the parameters that are statisticallysignificant. In this paper the ANOVA (ANalysis Of the VAriance) method is usedto carry out an exhaustive analysis of a simulated annealing based method andthe different parameters it requires. Following this idea, the significance andrelative importance of the parameters regarding the obtained results, as wellas suitable values for each of these, were obtained using ANOVA and post-hocTukey HSD test, on four well known function optimization problems and thelikelihood function that is used to estimate the parameters involved in thelognormal diffusion process. Through this statistical study we have verifiedthe adequacy of parameter values available in the bibliography using parametrichypothesis tests.</description><author>Pedro A. Castillo, Maribel García Arenas, Nuria Rico, Antonio Miguel Mora, Pablo García-Sánchez, Juan Luis Jiménez Laredo, Juan Julián Merelo Guervós</author><pubDate>Thu, 08 Feb 2024 16:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05791v1</guid></item><item><title>A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles</title><link>http://arxiv.org/abs/2402.01703v2</link><description>Interactions between the government officials and civilians affect publicwellbeing and the state legitimacy that is necessary for the functioning ofdemocratic society. Police officers, the most visible and contacted agents ofthe state, interact with the public more than 20 million times a year duringtraffic stops. Today, these interactions are regularly recorded by body-worncameras (BWCs), which are lauded as a means to enhance police accountabilityand improve police-public interactions. However, the timely analysis of theserecordings is hampered by a lack of reliable automated tools that can enablethe analysis of these complex and contested police-public interactions. Thisarticle proposes an approach to developing new multi-perspective, multimodalmachine learning (ML) tools to analyze the audio, video, and transcriptinformation from this BWC footage. Our approach begins by identifying theaspects of communication most salient to different stakeholders, including bothcommunity members and police officers. We move away from modeling approachesbuilt around the existence of a single ground truth and instead utilize newadvances in soft labeling to incorporate variation in how different observersperceive the same interactions. We argue that this inclusive approach to theconceptualization and design of new ML tools is broadly applicable to the studyof communication and development of analytic tools across domains of humaninteraction, including education, medicine, and the workplace.</description><author>Benjamin A. T. Grahama, Lauren Brown, Georgios Chochlakis, Morteza Dehghani, Raquel Delerme, Brittany Friedman, Ellie Graeden, Preni Golazizian, Rajat Hebbar, Parsa Hejabi, Aditya Kommineni, Mayagüez Salinas, Michael Sierra-Arévalo, Jackson Trager, Nicholas Weller, Shrikanth Narayan</author><pubDate>Thu, 08 Feb 2024 16:28:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01703v2</guid></item><item><title>How do Transformers perform In-Context Autoregressive Learning?</title><link>http://arxiv.org/abs/2402.05787v1</link><description>Transformers have achieved state-of-the-art performance in language modelingtasks. However, the reasons behind their tremendous success are still unclear.In this paper, towards a better understanding, we train a Transformer model ona simple next token prediction task, where sequences are generated as afirst-order autoregressive process $s_{t+1} = W s_t$. We show how a trainedTransformer predicts the next token by first learning $W$ in-context, thenapplying a prediction mapping. We call the resulting procedure in-contextautoregressive learning. More precisely, focusing on commuting orthogonalmatrices $W$, we first show that a trained one-layer linear Transformerimplements one step of gradient descent for the minimization of an innerobjective function, when considering augmented tokens. When the tokens are notaugmented, we characterize the global minima of a one-layer diagonal linearmulti-head Transformer. Importantly, we exhibit orthogonality between heads andshow that positional encoding captures trigonometric relations in the data. Onthe experimental side, we consider the general case of non-commuting orthogonalmatrices and generalize our theoretical findings.</description><author>Michael E. Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, Gabriel Peyré</author><pubDate>Thu, 08 Feb 2024 16:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05787v1</guid></item><item><title>Prompting Fairness: Artificial Intelligence as Game Players</title><link>http://arxiv.org/abs/2402.05786v1</link><description>Utilitarian games such as dictator games to measure fairness have beenstudied in the social sciences for decades. These games have given us insightinto not only how humans view fairness but also in what conditions thefrequency of fairness, altruism and greed increase or decrease. While thesegames have traditionally been focused on humans, the rise of AI gives us theability to study how these models play these games. AI is becoming a constantin human interaction and examining how these models portray fairness in gameplay can give us some insight into how AI makes decisions. Over 101 rounds ofthe dictator game, I conclude that AI has a strong sense of fairness that isdependant of it it deems the person it is playing with as trustworthy, framinghas a strong effect on how much AI gives a recipient when designated thetrustee, and there may be evidence that AI experiences inequality aversion justas humans.</description><author>Jazmia Henry</author><pubDate>Thu, 08 Feb 2024 16:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05786v1</guid></item><item><title>Limits of Transformer Language Models on Algorithmic Learning</title><link>http://arxiv.org/abs/2402.05785v1</link><description>We analyze the capabilities of Transformer language models on learningdiscrete algorithms. To this end, we introduce two new tasks demanding thecomposition of several discrete sub-tasks. On both training LLaMA models fromscratch and prompting on GPT-4 and Gemini we measure learning compositions oflearned primitives. We observe that the compositional capabilities ofstate-of-the-art Transformer language models are very limited and sample-wisescale worse than relearning all sub-tasks for a new algorithmic composition. Wealso present a theorem in complexity theory, showing that gradient descent onmemorizing feedforward models can be exponentially data inefficient.</description><author>Jonathan Thomm, Aleksandar Terzic, Geethan Karunaratne, Giacomo Camposampiero, Bernhard Schölkopf, Abbas Rahimi</author><pubDate>Thu, 08 Feb 2024 16:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05785v1</guid></item><item><title>pFedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning</title><link>http://arxiv.org/abs/2402.01350v2</link><description>Federated learning (FL) has been widely adopted for collaborative training ondecentralized data. However, it faces the challenges of data, system, and modelheterogeneity. This has inspired the emergence of model-heterogeneouspersonalized federated learning (MHPFL). Nevertheless, the problem of ensuringdata and model privacy, while achieving good model performance and keepingcommunication and computation costs low remains open in MHPFL. To address thisproblem, we propose a model-heterogeneous personalized Federated learning withMixture of Experts (pFedMoE) method. It assigns a shared homogeneous smallfeature extractor and a local gating network for each client's localheterogeneous large model. Firstly, during local training, the localheterogeneous model's feature extractor acts as a local expert for personalizedfeature (representation) extraction, while the shared homogeneous small featureextractor serves as a global expert for generalized feature extraction. Thelocal gating network produces personalized weights for extractedrepresentations from both experts on each data sample. The three models form alocal heterogeneous MoE. The weighted mixed representation fuses generalizedand personalized features and is processed by the local heterogeneous largemodel's header with personalized prediction information. The MoE and predictionheader are updated simultaneously. Secondly, the trained local homogeneoussmall feature extractors are sent to the server for cross-client informationfusion via aggregation. Overall, pFedMoE enhances local model personalizationat a fine-grained data level, while supporting model heterogeneity.</description><author>Liping Yi, Han Yu, Chao Ren, Heng Zhang, Gang Wang, Xiaoguang Liu, Xiaoxiao Li</author><pubDate>Thu, 08 Feb 2024 16:22:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01350v2</guid></item><item><title>Trainable Transformer in Transformer</title><link>http://arxiv.org/abs/2307.01189v2</link><description>Recent works attribute the capability of in-context learning (ICL) in largepre-trained language models to implicitly simulating and fine-tuning aninternal model (e.g., linear or 2-layer MLP) during inference. However, suchconstructions require large memory overhead, which makes simulation of moresophisticated internal models intractable. In this work, we propose anefficient construction, Transformer in Transformer (in short, TinT), thatallows a transformer to simulate and fine-tune complex models internally duringinference (e.g., pre-trained language models). In particular, we introduceinnovative approximation techniques that allow a TinT model with less than 2billion parameters to simulate and fine-tune a 125 million parametertransformer model within a single forward pass. TinT accommodates many commontransformer variants and its design ideas also improve the efficiency of pastinstantiations of simple models inside transformers. We conduct end-to-endexperiments to validate the internal fine-tuning procedure of TinT on variouslanguage modeling and downstream tasks. For example, even with a limitedone-step budget, we observe TinT for a OPT-125M model improves performance by4-16% absolute on average compared to OPT-125M. These findings suggest thatlarge pre-trained language models are capable of performing intricatesubroutines. To facilitate further work, a modular and extensible codebase forTinT is included.</description><author>Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora</author><pubDate>Thu, 08 Feb 2024 16:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01189v2</guid></item><item><title>Text-to-Code Generation with Modality-relative Pre-training</title><link>http://arxiv.org/abs/2402.05783v1</link><description>Large pre-trained language models have recently been expanded and applied toprogramming language tasks with great success, often through furtherpre-training of a strictly-natural language model--where training sequencestypically contain both natural and (linearised) programming language. Suchapproaches effectively map both modalities of the sequence into the sameembedding space. However, programming language keywords (e.g. ``while'') oftenhave very strictly defined semantics. As such, transfer learning from theirnatural language usage may not necessarily be beneficial to their codeapplication and vise versa. Assuming an already pre-trained language model, inthis work we investigate how sequence tokens can be adapted and representeddifferently, depending on which modality they belong to, and to the ultimatebenefit of the downstream task. We experiment with separating embedding spacesbetween modalities during further model pre-training with modality-relativetraining objectives. We focus on text-to-code generation and observe consistentimprovements across two backbone models and two test sets, measuring pass@$k$and a novel incremental variation.</description><author>Fenia Christopoulou, Guchun Zhang, Gerasimos Lampouras</author><pubDate>Thu, 08 Feb 2024 16:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05783v1</guid></item><item><title>Analysing the Sample Complexity of Opponent Shaping</title><link>http://arxiv.org/abs/2402.05782v1</link><description>Learning in general-sum games often yields collectively sub-optimal results.Addressing this, opponent shaping (OS) methods actively guide the learningprocesses of other agents, empirically leading to improved individual and groupperformances in many settings. Early OS methods use higher-order derivatives toshape the learning of co-players, making them unsuitable for shaping multiplelearning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addressesthese by reframing the OS problem as a meta-game. In contrast to early OSmethods, there is little theoretical understanding of the M-FOS framework.Providing theoretical guarantees for M-FOS is hard because A) there is littleliterature on theoretical sample complexity bounds for meta-reinforcementlearning B) M-FOS operates in continuous state and action spaces, sotheoretical analysis is challenging. In this work, we present R-FOS, a tabularversion of M-FOS that is more suitable for theoretical analysis. R-FOSdiscretises the continuous meta-game MDP into a tabular MDP. Within thisdiscretised MDP, we adapt the $R_{max}$ algorithm, most prominently used toderive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm. Wederive a sample complexity bound that is exponential in the cardinality of theinner state and action space and the number of agents. Our bound guaranteesthat, with high probability, the final policy learned by an R-FOS agent isclose to the optimal policy, apart from a constant factor. Finally, weinvestigate how R-FOS's sample complexity scales in the size of state-actionspace. Our theoretical results on scaling are supported empirically in theMatching Pennies environment.</description><author>Kitty Fung, Qizhen Zhang, Chris Lu, Jia Wan, Timon Willi, Jakob Foerster</author><pubDate>Thu, 08 Feb 2024 16:17:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05782v1</guid></item><item><title>Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images</title><link>http://arxiv.org/abs/2402.05779v1</link><description>Following on recent advances in large language models (LLMs) and subsequentchat models, a new wave of large vision-language models (LVLMs) has emerged.Such models can incorporate images as input in addition to text, and performtasks such as visual question answering, image captioning, story generation,etc. Here, we examine potential gender and racial biases in such systems, basedon the perceived characteristics of the people in the input images. Toaccomplish this, we present a new dataset PAIRS (PArallel Images for eveRydayScenarios). The PAIRS dataset contains sets of AI-generated images of people,such that the images are highly similar in terms of background and visualcontent, but differ along the dimensions of gender (man, woman) and race(Black, white). By querying the LVLMs with such images, we observe significantdifferences in the responses according to the perceived gender or race of theperson depicted.</description><author>Kathleen C. Fraser, Svetlana Kiritchenko</author><pubDate>Thu, 08 Feb 2024 16:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05779v1</guid></item><item><title>Can Large Language Models Detect Rumors on Social Media?</title><link>http://arxiv.org/abs/2402.03916v2</link><description>In this work, we investigate to use Large Language Models (LLMs) for rumordetection on social media. However, it is challenging for LLMs to reason overthe entire propagation information on social media, which contains newscontents and numerous comments, due to LLMs may not concentrate on key clues inthe complex propagation information, and have trouble in reasoning when facingmassive and redundant information. Accordingly, we propose an LLM-empoweredRumor Detection (LeRuD) approach, in which we design prompts to teach LLMs toreason over important clues in news and comments, and divide the entirepropagation information into a Chain-of-Propagation for reducing LLMs' burden.We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuDoutperforms several state-of-the-art rumor detection models by 3.2% to 7.7%.Meanwhile, by applying LLMs, LeRuD requires no data for training, and thusshows more promising rumor detection ability in few-shot or zero-shotscenarios.</description><author>Qiang Liu, Xiang Tao, Junfei Wu, Shu Wu, Liang Wang</author><pubDate>Thu, 08 Feb 2024 16:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03916v2</guid></item><item><title>Survey of Federated Learning Models for Spatial-Temporal Mobility Applications</title><link>http://arxiv.org/abs/2305.05257v4</link><description>Federated learning involves training statistical models over edge devicessuch as mobile phones such that the training data is kept local. FederatedLearning (FL) can serve as an ideal candidate for training spatial temporalmodels that rely on heterogeneous and potentially massive numbers ofparticipants while preserving the privacy of highly sensitive location data.However, there are unique challenges involved with transitioning existingspatial temporal models to decentralized learning. In this survey paper, wereview the existing literature that has proposed FL-based models for predictinghuman mobility, traffic prediction, community detection, location-basedrecommendation systems, and other spatial-temporal tasks. We describe themetrics and datasets these works have been using and create a baseline of theseapproaches in comparison to the centralized settings. Finally, we discuss thechallenges of applying spatial-temporal models in a decentralized setting andby highlighting the gaps in the literature we provide a road map andopportunities for the research community.</description><author>Yacine Belal, Sonia Ben Mokhtar, Hamed Haddadi, Jaron Wang, Afra Mashhadi</author><pubDate>Thu, 08 Feb 2024 16:09:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05257v4</guid></item><item><title>Individualized Multi-Treatment Response Curves Estimation using RBF-net with Shared Neurons</title><link>http://arxiv.org/abs/2401.16571v4</link><description>Heterogeneous treatment effect estimation is an important problem inprecision medicine. Specific interests lie in identifying the differentialeffect of different treatments based on some external covariates. We propose anovel non-parametric treatment effect estimation method in a multi-treatmentsetting. Our non-parametric modeling of the response curves relies on radialbasis function (RBF)-nets with shared hidden neurons. Our model thusfacilitates modeling commonality among the treatment outcomes. The estimationand inference schemes are developed under a Bayesian framework and implementedvia an efficient Markov chain Monte Carlo algorithm, appropriatelyaccommodating uncertainty in all aspects of the analysis. The numericalperformance of the method is demonstrated through simulation experiments.Applying our proposed method to MIMIC data, we obtain several interestingfindings related to the impact of different treatment strategies on the lengthof ICU stay and 12-hour SOFA score for sepsis patients who are home-discharged.</description><author>Peter Chang, Arkaprava Roy</author><pubDate>Thu, 08 Feb 2024 16:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16571v4</guid></item><item><title>LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control</title><link>http://arxiv.org/abs/2401.04855v3</link><description>Coverage control is the problem of navigating a robot swarm tocollaboratively monitor features or a phenomenon of interest not known apriori. The problem is challenging in decentralized settings with robots thathave limited communication and sensing capabilities. We propose a learnablePerception-Action-Communication (LPAC) architecture for the problem, wherein aconvolution neural network (CNN) processes localized perception; a graph neuralnetwork (GNN) facilitates robot communications; finally, a shallow multi-layerperceptron (MLP) computes robot actions. The GNN enables collaboration in therobot swarm by computing what information to communicate with nearby robots andhow to incorporate received information. Evaluations show that the LPAC models-- trained using imitation learning -- outperform standard decentralized andcentralized coverage control algorithms. The learned policy generalizes toenvironments different from the training dataset, transfers to largerenvironments with more robots, and is robust to noisy position estimates. Theresults indicate the suitability of LPAC architectures for decentralizednavigation in robot swarms to achieve collaborative behavior.</description><author>Saurav Agarwal, Ramya Muthukrishnan, Walker Gosrich, Vijay Kumar, Alejandro Ribeiro</author><pubDate>Thu, 08 Feb 2024 16:05:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04855v3</guid></item><item><title>Spectral Clustering with Variance Information for Group Structure Estimation in Panel Data</title><link>http://arxiv.org/abs/2201.01793v2</link><description>Consider a panel data setting where repeated observations on individuals areavailable. Often it is reasonable to assume that there exist groups ofindividuals that share similar effects of observed characteristics, but thegrouping is typically unknown in advance. We first conduct a local analysiswhich reveals that the variances of the individual coefficient estimatescontain useful information for the estimation of group structure. We thenpropose a method to estimate unobserved groupings for general panel data modelsthat explicitly account for the variance information. Our proposed methodremains computationally feasible with a large number of individuals and/orrepeated measurements on each individual. The developed ideas can also beapplied even when individual-level data are not available and only parameterestimates together with some quantification of estimation uncertainty are givento the researcher. A thorough simulation study demonstrates superiorperformance of our method than existing methods and we apply the method to twoempirical applications.</description><author>Lu Yu, Jiaying Gu, Stanislav Volgushev</author><pubDate>Thu, 08 Feb 2024 16:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.01793v2</guid></item><item><title>Stable Autonomous Flow Matching</title><link>http://arxiv.org/abs/2402.05774v1</link><description>In contexts where data samples represent a physically stable state, it isoften assumed that the data points represent the local minima of an energylandscape. In control theory, it is well-known that energy can serve as aneffective Lyapunov function. Despite this, connections between control theoryand generative models in the literature are sparse, even though there areseveral machine learning applications with physically stable data points. Inthis paper, we focus on such data and a recent class of deep generative modelscalled flow matching. We apply tools of stochastic stability fortime-independent systems to flow matching models. In doing so, we characterizethe space of flow matching models that are amenable to this treatment, as wellas draw connections to other control theory principles. We demonstrate ourtheoretical results on two examples.</description><author>Christopher Iliffe Sprague, Arne Elofsson, Hossein Azizpour</author><pubDate>Thu, 08 Feb 2024 16:01:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05774v1</guid></item><item><title>UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery</title><link>http://arxiv.org/abs/2402.05773v1</link><description>Raindrops adhering to the lens of UAVs can obstruct visibility of thebackground scene and degrade image quality. Despite recent progress in imagederaining methods and datasets, there is a lack of focus on raindrop removalfrom UAV aerial imagery due to the unique challenges posed by varying anglesand rapid movement during drone flight. To fill the gap in this research, wefirst construct a new benchmark dataset for removing raindrops from UAV images,called UAV-Rain1k. In this letter, we provide a dataset generation pipeline,which includes modeling raindrop shapes using Blender, collecting backgroundimages from various UAV angles, random sampling of rain masks and etc. Based onthe proposed benchmark, we further present a comprehensive evaluation ofexisting representative image deraining algorithms, and reveal future researchopportunities worth exploring. The proposed dataset will be publicly availableat https://github.com/cschenxiang/UAV-Rain1k.</description><author>Wenhui Chang, Hongming Chen, Xin He, Xiang Chen, Liangduo Shen</author><pubDate>Thu, 08 Feb 2024 16:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05773v1</guid></item><item><title>Data-Driven Identification of Quadratic Representations for Nonlinear Hamiltonian Systems using Weakly Symplectic Liftings</title><link>http://arxiv.org/abs/2308.01084v2</link><description>We present a framework for learning Hamiltonian systems using data. This workis based on a lifting hypothesis, which posits that nonlinear Hamiltoniansystems can be written as nonlinear systems with cubic Hamiltonians. Byleveraging this, we obtain quadratic dynamics that are Hamiltonian in atransformed coordinate system. To that end, for given generalized position andmomentum data, we propose a methodology to learn quadratic dynamical systems,enforcing the Hamiltonian structure in combination with a weakly-enforcedsymplectic auto-encoder. The obtained Hamiltonian structure exhibits long-termstability of the system, while the cubic Hamiltonian function providesrelatively low model complexity. For low-dimensional data, we determine ahigher-dimensional transformed coordinate system, whereas for high-dimensionaldata, we find a lower-dimensional coordinate system with the desiredproperties. We demonstrate the proposed methodology by means of bothlow-dimensional and high-dimensional nonlinear Hamiltonian systems.</description><author>Süleyman Yildiz, Pawan Goyal, Thomas Bendokat, Peter Benner</author><pubDate>Thu, 08 Feb 2024 15:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01084v2</guid></item><item><title>Social Learning: Towards Collaborative Learning with Large Language Models</title><link>http://arxiv.org/abs/2312.11441v2</link><description>We introduce the framework of "social learning" in the context of largelanguage models (LLMs), whereby models share knowledge with each other in aprivacy-aware manner using natural language. We present and evaluate twoapproaches for knowledge transfer between LLMs. In the first scenario, we allowthe model to generate abstract prompts aiming to teach the task. In our secondapproach, models transfer knowledge by generating synthetic examples. Weevaluate these methods across diverse datasets and quantify memorization as aproxy for privacy loss. These techniques inspired by social learning yieldpromising results with low memorization of the original data. In particular, weshow that performance using these methods is comparable to results with the useof original labels and prompts. Our work demonstrates the viability of sociallearning for LLMs, establishes baseline approaches and highlights severalunexplored areas for future work.</description><author>Amirkeivan Mohtashami, Florian Hartmann, Sian Gooding, Lukas Zilka, Matt Sharifi, Blaise Aguera y Arcas</author><pubDate>Thu, 08 Feb 2024 15:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11441v2</guid></item><item><title>Off-policy Distributional Q($λ$): Distributional RL without Importance Sampling</title><link>http://arxiv.org/abs/2402.05766v1</link><description>We introduce off-policy distributional Q($\lambda$), a new addition to thefamily of off-policy distributional evaluation algorithms. Off-policydistributional Q($\lambda$) does not apply importance sampling for off-policylearning, which introduces intriguing interactions with signed measures. Suchunique properties distributional Q($\lambda$) from other existing alternativessuch as distributional Retrace. We characterize the algorithmic properties ofdistributional Q($\lambda$) and validate theoretical insights with tabularexperiments. We show how distributional Q($\lambda$)-C51, a combination ofQ($\lambda$) with the C51 agent, exhibits promising results on deep RLbenchmarks.</description><author>Yunhao Tang, Mark Rowland, Rémi Munos, Bernardo Ávila Pires, Will Dabney</author><pubDate>Thu, 08 Feb 2024 15:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05766v1</guid></item><item><title>RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</title><link>http://arxiv.org/abs/2401.04679v5</link><description>We investigate parameter-efficient fine-tuning (PEFT) methods that canprovide good accuracy under limited computational and memory budgets in thecontext of large language models (LLMs). We present a new PEFT method calledRobust Adaptation (RoSA) inspired by robust principal component analysis thatjointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components ontop of a set of fixed pretrained weights to efficiently approximate theperformance of a full-fine-tuning (FFT) solution. Across a series ofchallenging generative tasks such as grade-school math and SQL querygeneration, which require fine-tuning for good performance, we show that RoSAoutperforms LoRA, pure sparse fine-tuning, and alternative hybrid methods atthe same parameter budget, and can even recover the performance of FFT on sometasks. We provide system support for RoSA to complement the training algorithm,specifically in the form of sparse GPU kernels which enable memory- andcomputationally-efficient training, and show that it is also compatible withlow-precision base weights, resulting in the first joint representationcombining quantization, low-rank and sparse approximations. Our code isaccessible at https://github.com/IST-DASLab/RoSA.</description><author>Mahdi Nikdan, Soroush Tabesh, Elvir Crnčević, Dan Alistarh</author><pubDate>Thu, 08 Feb 2024 15:43:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04679v5</guid></item></channel></rss>