<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 22 Aug 2024 13:00:28 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models</title><link>http://arxiv.org/abs/2408.11817v1</link><description>Large multimodal models (LMMs) have exhibited proficiencies across manyvisual tasks. Although numerous well-known benchmarks exist to evaluate modelperformance, they increasingly have insufficient headroom. As such, there is apressing need for a new generation of benchmarks challenging enough for thenext generation of LMMs. One area that LMMs show potential is graph analysis,specifically, the tasks an analyst might typically perform when interpretingfigures such as estimating the mean, intercepts or correlations of functionsand data series. In this work, we introduce GRAB, a graph analysis benchmark,fit for current and future frontier LMMs. Our benchmark is entirely synthetic,ensuring high-quality, noise-free questions. GRAB is comprised of 2170questions, covering four tasks and 23 graph properties. We evaluate 20 LMMs onGRAB, finding it to be a challenging benchmark, with the highest performingmodel attaining a score of just 21.7%. Finally, we conduct various ablations toinvestigate where the models succeed and struggle. We release GRAB to encourageprogress in this important, growing domain.</description><author>Jonathan Roberts, Kai Han, Samuel Albanie</author><pubDate>Wed, 21 Aug 2024 17:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11817v1</guid></item><item><title>Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction</title><link>http://arxiv.org/abs/2408.11816v1</link><description>In the face of difficult exploration problems in reinforcement learning, westudy whether giving an agent an object-centric mapping (describing a set ofitems and their attributes) allow for more efficient learning. We found thisproblem is best solved hierarchically by modelling items at a higher level ofstate abstraction to pixels, and attribute change at a higher level of temporalabstraction to primitive actions. This abstraction simplifies the transitiondynamic by making specific future states easier to predict. We make use of thisto propose a fully model-based algorithm that learns a discriminative worldmodel, plans to explore efficiently with only a count-based intrinsic reward,and can subsequently plan to reach any discovered (abstract) states. We demonstrate the model's ability to (i) efficiently solve single tasks,(ii) transfer zero-shot and few-shot across item types and environments, and(iii) plan across long horizons. Across a suite of 2D crafting and MiniHackenvironments, we empirically show our model significantly out-performsstate-of-the-art low-level methods (without abstraction), as well as performantmodel-free and model-based methods using the same abstraction. Finally, we showhow to reinforce learn low level object-perturbing policies, as well assupervise learn the object mapping itself.</description><author>Anthony GX-Chen, Kenneth Marino, Rob Fergus</author><pubDate>Wed, 21 Aug 2024 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11816v1</guid></item><item><title>Great Memory, Shallow Reasoning: Limits of $k$NN-LMs</title><link>http://arxiv.org/abs/2408.11815v1</link><description>$K$-nearest neighbor language models ($k$NN-LMs), which integrate retrievalwith next-word prediction, have demonstrated strong performance in languagemodeling as well as downstream NLP benchmarks. These results have ledresearchers to argue that models trained on poor quality or outdated data couldperform well by employing a $k$NN extension that has access to a higher-qualitydatastore. In this work, we ask whether this improved ability to recallinformation really translates into downstream abilities. We extensivelyevaluate $k$NN-LMs on a diverse set of tasks, ranging from sentimentclassification and commonsense reasoning to multi-hop reasoning. Results showthat $k$NN-LMs excel at memory-intensive tasks, where utilizing the patterns inthe input is sufficient for determining the output, but struggle with reasoningtasks that require integrating multiple pieces of information to derive newknowledge. We further demonstrate through oracle experiments and qualitativeanalysis that even with perfect retrieval, $k$NN-LMs still fail to determinethe correct answers, placing an upper bound on their reasoning performance.Code and datastores are released at https://github.com/GSYfate/knnlm-limits/.</description><author>Shangyi Geng, Wenting Zhao, Alexander M Rush</author><pubDate>Wed, 21 Aug 2024 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11815v1</guid></item><item><title>SynPlay: Importing Real-world Diversity for a Synthetic Human Dataset</title><link>http://arxiv.org/abs/2408.11814v1</link><description>We introduce Synthetic Playground (SynPlay), a new synthetic human datasetthat aims to bring out the diversity of human appearance in the real world. Wefocus on two factors to achieve a level of diversity that has not yet been seenin previous works: i) realistic human motions and poses and ii) multiple cameraviewpoints towards human instances. We first use a game engine and itslibrary-provided elementary motions to create games where virtual players cantake less-constrained and natural movements while following the game rules(i.e., rule-guided motion design as opposed to detail-guided design). We thenaugment the elementary motions with real human motions captured with a motioncapture device. To render various human appearances in the games from multipleviewpoints, we use seven virtual cameras encompassing the ground and aerialviews, capturing abundant aerial-vs-ground and dynamic-vs-static attributes ofthe scene. Through extensive and carefully-designed experiments, we show thatusing SynPlay in model training leads to enhanced accuracy over existingsynthetic datasets for human detection and segmentation. The benefit of SynPlaybecomes even greater for tasks in the data-scarce regime, such as few-shot andcross-domain learning tasks. These results clearly demonstrate that SynPlay canbe used as an essential dataset with rich attributes of complex humanappearances and poses suitable for model pretraining. SynPlay datasetcomprising over 73k images and 6.5M human instances, is available for downloadat https://synplaydataset.github.io/.</description><author>Jinsub Yim, Hyungtae Lee, Sungmin Eum, Yi-Ting Shen, Yan Zhang, Heesung Kwon, Shuvra S. Bhattacharyya</author><pubDate>Wed, 21 Aug 2024 17:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11814v1</guid></item><item><title>SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs</title><link>http://arxiv.org/abs/2408.11813v1</link><description>Multimodal Large Language Models (MLLMs) have recently demonstratedremarkable perceptual and reasoning abilities, typically comprising a VisionEncoder, an Adapter, and a Large Language Model (LLM). The adapter serves asthe critical bridge between the visual and language components. However,training adapters with image-level supervision often results in significantmisalignment, undermining the LLMs' capabilities and limiting the potential ofMultimodal LLMs. To address this, we introduce Supervised Embedding Alignment(SEA), a token-level alignment method that leverages vision-languagepre-trained models, such as CLIP, to align visual tokens with the LLM'sembedding space through contrastive learning. This approach ensures a morecoherent integration of visual and language representations, enhancing theperformance and interpretability of multimodal LLMs while preserving theirinherent capabilities. Extensive experiments show that SEA effectively improvesMLLMs, particularly for smaller models, without adding extra data or inferencecomputation. SEA also lays the groundwork for developing more general andadaptable solutions to enhance multimodal systems.</description><author>Yuanyang Yin, Yaqi Zhao, Yajie Zhang, Ke Lin, Jiahao Wang, Xin Tao, Pengfei Wan, Di Zhang, Baoqun Yin, Wentao Zhang</author><pubDate>Wed, 21 Aug 2024 17:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11813v1</guid></item><item><title>Scaling Cross-Embodied Learning: One Policy for Manipulation, Navigation, Locomotion and Aviation</title><link>http://arxiv.org/abs/2408.11812v1</link><description>Modern machine learning systems rely on large datasets to attain broadgeneralization, and this often poses a challenge in robot learning, where eachrobotic platform and task might have only a small dataset. By training a singlepolicy across many different kinds of robots, a robot learning method canleverage much broader and more diverse datasets, which in turn can lead tobetter generalization and robustness. However, training a single policy onmulti-robot data is challenging because robots can have widely varying sensors,actuators, and control frequencies. We propose CrossFormer, a scalable andflexible transformer-based policy that can consume data from any embodiment. Wetrain CrossFormer on the largest and most diverse dataset to date, 900Ktrajectories across 20 different robot embodiments. We demonstrate that thesame network weights can control vastly different robots, including single anddual arm manipulation systems, wheeled robots, quadcopters, and quadrupeds.Unlike prior work, our model does not require manual alignment of theobservation or action spaces. Extensive experiments in the real world show thatour method matches the performance of specialist policies tailored for eachembodiment, while also significantly outperforming the prior state of the artin cross-embodiment learning.</description><author>Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, Sergey Levine</author><pubDate>Wed, 21 Aug 2024 17:57:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11812v1</guid></item><item><title>EmbodiedSAM: Online Segment Any 3D Thing in Real Time</title><link>http://arxiv.org/abs/2408.11811v1</link><description>Embodied tasks require the agent to fully understand 3D scenes simultaneouslywith its exploration, so an online, real-time, fine-grained andhighly-generalized 3D perception model is desperately needed. Sincehigh-quality 3D data is limited, directly training such a model in 3D is almostinfeasible. Meanwhile, vision foundation models (VFM) has revolutionized thefield of 2D computer vision with superior performance, which makes the use ofVFM to assist embodied 3D perception a promising direction. However, mostexisting VFM-assisted 3D perception methods are either offline or too slow thatcannot be applied in practical embodied tasks. In this paper, we aim toleverage Segment Anything Model (SAM) for real-time 3D instance segmentation inan online setting. This is a challenging problem since future frames are notavailable in the input streaming RGB-D video, and an instance may be observedin several frames so object matching between frames is required. To addressthese challenges, we first propose a geometric-aware query lifting module torepresent the 2D masks generated by SAM by 3D-aware queries, which is theniteratively refined by a dual-level query decoder. In this way, the 2D masksare transferred to fine-grained shapes on 3D point clouds. Benefit from thequery representation for 3D masks, we can compute the similarity matrix betweenthe 3D masks from different views by efficient matrix operation, which enablesreal-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScanshow our method achieves leading performance even compared with offlinemethods. Our method also demonstrates great generalization ability in severalzero-shot dataset transferring experiments and show great potential inopen-vocabulary and data-efficient setting. Code and demo are available athttps://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required fortraining and evaluation.</description><author>Xiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu</author><pubDate>Wed, 21 Aug 2024 17:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11811v1</guid></item><item><title>Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion Models</title><link>http://arxiv.org/abs/2408.11810v1</link><description>Diffusion Models have emerged as powerful generative models for high-qualityimage synthesis, with many subsequent image editing techniques based on them.However, the ease of text-based image editing introduces significant risks,such as malicious editing for scams or intellectual property infringement.Previous works have attempted to safeguard images from diffusion-based editingby adding imperceptible perturbations. These methods are costly andspecifically target prevalent Latent Diffusion Models (LDMs), whilePixel-domain Diffusion Models (PDMs) remain largely unexplored and robustagainst such attacks. Our work addresses this gap by proposing a novelattacking framework with a feature representation attack loss that exploitsvulnerabilities in denoising UNets and a latent optimization strategy toenhance the naturalness of protected images. Extensive experiments demonstratethe effectiveness of our approach in attacking dominant PDM-based editingmethods (e.g., SDEdit) while maintaining reasonable protection fidelity androbustness against common defense methods. Additionally, our framework isextensible to LDMs, achieving comparable performance to existing approaches.</description><author>Chun-Yen Shih, Li-Xuan Peng, Jia-Wei Liao, Ernie Chu, Cheng-Fu Chou, Jun-Cheng Chen</author><pubDate>Wed, 21 Aug 2024 17:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11810v1</guid></item><item><title>MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding</title><link>http://arxiv.org/abs/2408.11049v2</link><description>Large Language Models (LLMs) have become more prevalent in long-contextapplications such as interactive chatbots, document analysis, and agentworkflows, but it is challenging to serve long-context requests with lowlatency and high throughput. Speculative decoding (SD) is a widely usedtechnique to reduce latency without sacrificing performance but theconventional wisdom suggests that its efficacy is limited to small batch sizes.In MagicDec, we show that surprisingly SD can achieve speedup even for a highthroughput inference regime for moderate to long sequences. More interestingly,an intelligent drafting strategy can achieve better speedup with increasingbatch size based on our rigorous analysis. MagicDec first identifies thebottleneck shifts with increasing batch size and sequence length, and usesthese insights to deploy speculative decoding more effectively for highthroughput inference. Then, it leverages draft models with sparse KV cache toaddress the KV bottleneck that scales with both sequence length and batch size.This finding underscores the broad applicability of speculative decoding inlong-context serving, as it can enhance throughput and reduce latency withoutcompromising accuracy. For moderate to long sequences, we demonstrate up to 2xspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when servingbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is availableat https://github.com/Infini-AI-Lab/MagicDec/.</description><author>Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen</author><pubDate>Wed, 21 Aug 2024 17:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11049v2</guid></item><item><title>ACE: A Cross-Platform Visual-Exoskeletons System for Low-Cost Dexterous Teleoperation</title><link>http://arxiv.org/abs/2408.11805v1</link><description>Learning from demonstrations has shown to be an effective approach to roboticmanipulation, especially with the recently collected large-scale robot datawith teleoperation systems. Building an efficient teleoperation system acrossdiverse robot platforms has become more crucial than ever. However, there is anotable lack of cost-effective and user-friendly teleoperation systems fordifferent end-effectors, e.g., anthropomorphic robot hands and grippers, thatcan operate across multiple platforms. To address this issue, we develop ACE, across-platform visual-exoskeleton system for low-cost dexterous teleoperation.Our system utilizes a hand-facing camera to capture 3D hand poses and anexoskeleton mounted on a portable base, enabling accurate real-time capture ofboth finger and wrist poses. Compared to previous systems, which often requirehardware customization according to different robots, our single system cangeneralize to humanoid hands, arm-hands, arm-gripper, and quadruped-grippersystems with high-precision teleoperation. This enables imitation learning forcomplex manipulation tasks on diverse platforms.</description><author>Shiqi Yang, Minghuan Liu, Yuzhe Qin, Runyu Ding, Jialong Li, Xuxin Cheng, Ruihan Yang, Sha Yi, Xiaolong Wang</author><pubDate>Wed, 21 Aug 2024 17:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11805v1</guid></item><item><title>Approaching Deep Learning through the Spectral Dynamics of Weights</title><link>http://arxiv.org/abs/2408.11804v1</link><description>We propose an empirical approach centered on the spectral dynamics of weights-- the behavior of singular values and vectors during optimization -- to unifyand clarify several phenomena in deep learning. We identify a consistent biasin optimization across various experiments, from small-scale ``grokking'' tolarge-scale tasks like image classification with ConvNets, image generationwith UNets, speech recognition with LSTMs, and language modeling withTransformers. We also demonstrate that weight decay enhances this bias beyondits role as a norm regularizer, even in practical systems. Moreover, we showthat these spectral dynamics distinguish memorizing networks from generalizingones, offering a novel perspective on this longstanding conundrum.Additionally, we leverage spectral dynamics to explore the emergence ofwell-performing sparse subnetworks (lottery tickets) and the structure of theloss surface through linear mode connectivity. Our findings suggest thatspectral dynamics provide a coherent framework to better understand thebehavior of neural networks across diverse settings.</description><author>David Yunis, Kumar Kshitij Patel, Samuel Wheeler, Pedro Savarese, Gal Vardi, Karen Livescu, Michael Maire, Matthew R. Walter</author><pubDate>Wed, 21 Aug 2024 17:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11804v1</guid></item><item><title>Edge AI as a Service with Coordinated Deep Neural Networks</title><link>http://arxiv.org/abs/2401.00631v2</link><description>As artificial intelligence (AI) applications continue to expand innext-generation networks, there is a growing need for deep neural network (DNN)models. Although DNN models deployed at the edge are promising for providing AIas a service with low latency, their cooperation is yet to be explored. In thispaper, we consider that DNN service providers share their computing resourcesas well as their models' parameters and allow other DNNs to offload theircomputations without mirroring. We propose a novel algorithm called coordinatedDNNs on edge (\textbf{CoDE}) that facilitates coordination among DNN servicesby establishing new inference paths. CoDE aims to find the optimal path, whichis the path with the highest possible reward, by creating multi-task DNNs fromindividual models. The reward reflects the inference throughput and modelaccuracy. With CoDE, DNN models can make new paths for inference by using theirown or other models' parameters. We then evaluate the performance of CoDEthrough numerical experiments. The results demonstrate a $40\%$ increase in theinference throughput while degrading the average accuracy by only $2.3\%$.Experiments show that CoDE enhances the inference throughput and, achieveshigher precision compared to a state-of-the-art existing method.</description><author>Alireza Maleki, Hamed Shah-Mansouri, Babak H. Khalaj</author><pubDate>Wed, 21 Aug 2024 17:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00631v2</guid></item><item><title>LongVILA: Scaling Long-Context Visual Language Models for Long Videos</title><link>http://arxiv.org/abs/2408.10188v3</link><description>Long-context capability is critical for multi-modal foundation models,especially for long video understanding. We introduce LongVILA, a full-stacksolution for long-context visual-language models by co-designing the algorithmand system. For model training, we upgrade existing VLMs to support long videounderstanding by incorporating two additional stages, i.e., long contextextension and long supervised fine-tuning. However, training on long video iscomputationally and memory intensive. We introduce the long-context Multi-ModalSequence Parallelism (MM-SP) system that efficiently parallelizes long videotraining and inference, enabling 2M context length training on 256 GPUs withoutany gradient checkpointing. LongVILA efficiently extends the number of videoframes of VILA from 8 to 1024, improving the long video captioning score from2.00 to 3.26 (out of 5), achieving 99.5% accuracy in 1400-frame (274k contextlength) video needle-in-a-haystack. LongVILA-8B demonstrates consistentaccuracy improvements on long videos in the VideoMME benchmark as the number offrames increases. Besides, MM-SP is 2.1x - 5.7x faster than ring sequenceparallelism and 1.1x - 1.4x faster than Megatron with context parallelism +tensor parallelism. Moreover, it seamlessly integrates with Hugging FaceTransformers.</description><author>Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han</author><pubDate>Wed, 21 Aug 2024 17:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10188v3</guid></item><item><title>Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models</title><link>http://arxiv.org/abs/2408.11801v1</link><description>Traditional visual storytelling is complex, requiring specialized knowledgeand substantial resources, yet often constrained by human creativity andcreation precision. While Large Language Models (LLMs) enhance visualstorytelling, current approaches often limit themselves to 2D visuals oroversimplify stories through motion synthesis and behavioral simulation,failing to create comprehensive, multi-dimensional narratives. To this end, wepresent Story3D-Agent, a pioneering approach that leverages the capabilities ofLLMs to transform provided narratives into 3D-rendered visualizations. Byintegrating procedural modeling, our approach enables precise control overmulti-character actions and motions, as well as diverse decorative elements,ensuring the long-range and dynamic 3D representation. Furthermore, our methodsupports narrative extension through logical reasoning, ensuring that generatedcontent remains consistent with existing conditions. We have thoroughlyevaluated our Story3D-Agent to validate its effectiveness, offering a basicframework to advance 3D story representation.</description><author>Yuzhou Huang, Yiran Qin, Shunlin Lu, Xintao Wang, Rui Huang, Ying Shan, Ruimao Zhang</author><pubDate>Wed, 21 Aug 2024 17:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11801v1</guid></item><item><title>PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting and Permitting domain</title><link>http://arxiv.org/abs/2408.11800v1</link><description>In the rapidly evolving landscape of Natural Language Processing (NLP) andtext generation, the emergence of Retrieval Augmented Generation (RAG) presentsa promising avenue for improving the quality and reliability of generated textby leveraging information retrieved from user specified database. Benchmarkingis essential to evaluate and compare the performance of the different RAGconfigurations in terms of retriever and generator, providing insights intotheir effectiveness, scalability, and suitability for the specific domain andapplications. In this paper, we present a comprehensive framework to generate adomain relevant RAG benchmark. Our framework is based on automaticquestion-answer generation with Human (domain experts)-AI Large Language Model(LLM) teaming. As a case study, we demonstrate the framework by introducingPermitQA, a first-of-its-kind benchmark on the wind siting and permittingdomain which comprises of multiple scientific documents/reports related toenvironmental impact of wind energy projects. Our framework systematicallyevaluates RAG performance using diverse metrics and multiple question typeswith varying complexity level. We also demonstrate the performance of differentmodels on our benchmark.</description><author>Rounak Meyur, Hung Phan, Sridevi Wagle, Jan Strube, Mahantesh Halappanavar, Sameera Horawalavithana, Anurag Acharya, Sai Munikoti</author><pubDate>Wed, 21 Aug 2024 17:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11800v1</guid></item><item><title>Practical token pruning for foundation models in few-shot conversational virtual assistant systems</title><link>http://arxiv.org/abs/2408.11799v1</link><description>In an enterprise Virtual Assistant (VA) system, intent classification is thecrucial component that determines how a user input is handled based on what theuser wants. The VA system is expected to be a cost-efficient SaaS service withlow training and inference time while achieving high accuracy even with a smallnumber of training samples. We pretrain a transformer-based sentence embeddingmodel with a contrastive learning objective and leverage the embedding of themodel as features when training intent classification models. Our approachachieves the state-of-the-art results for few-shot scenarios and performsbetter than other commercial solutions on popular intent classificationbenchmarks. However, generating features via a transformer-based modelincreases the inference time, especially for longer user inputs, due to thequadratic runtime of the transformer's attention mechanism. On top of modeldistillation, we introduce a practical multi-task adaptation approach thatconfigures dynamic token pruning without the need for task-specific trainingfor intent classification. We demonstrate that this approach improves theinference speed of popular sentence transformer models without affecting modelperformance.</description><author>Haode Qi, Cheng Qian, Jian Ni, Pratyush Singh, Reza Fazeli, Gengyu Wang, Zhongzheng Shu, Eric Wayne, Juergen Bross</author><pubDate>Wed, 21 Aug 2024 17:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11799v1</guid></item><item><title>LLM Pruning and Distillation in Practice: The Minitron Approach</title><link>http://arxiv.org/abs/2408.11796v1</link><description>We present a comprehensive report on compressing the Llama 3.1 8B and MistralNeMo 12B models to 4B and 8B parameters, respectively, using pruning anddistillation. We explore two distinct pruning strategies: (1) depth pruning and(2) joint hidden/attention/MLP (width) pruning, and evaluate the results oncommon benchmarks from the LM Evaluation Harness. The models are then alignedwith NeMo Aligner and tested in instruct-tuned versions. This approach producesa compelling 4B model from Llama 3.1 8B and a state-of-the-artMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo12B. We found that with no access to the original data, it is beneficial toslightly fine-tune teacher models on the distillation dataset. We open-sourceour base model weights on Hugging Face with a permissive license.</description><author>Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov</author><pubDate>Wed, 21 Aug 2024 17:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11796v1</guid></item><item><title>EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model</title><link>http://arxiv.org/abs/2408.11795v1</link><description>In the realm of multimodal research, numerous studies leverage substantialimage-text pairs to conduct modal alignment learning, transforming LargeLanguage Models (LLMs) into Multimodal LLMs and excelling in a variety ofvisual-language tasks. The prevailing methodologies primarily fall into twocategories: self-attention-based and cross-attention-based methods. Whileself-attention-based methods offer superior data efficiency due to their simpleMLP architecture, they often suffer from lower computational efficiency due toconcatenating visual and textual tokens as input for LLM. Conversely,cross-attention-based methods, although less data-efficient due to additionallearnable parameters, exhibit higher computational efficiency by avoiding longsequence input for LLM. To address these trade-offs, we introduce theData-Efficient and Compute-Efficient Multimodal Large Language Model (EE-MLLM).Without introducing additional modules or learnable parameters, EE-MLLMachieves both data and compute efficiency. Specifically, we modify the originalself-attention mechanism in MLLM to a composite attention mechanism. Thismechanism has two key characteristics: 1) Eliminating the computationaloverhead of self-attention within visual tokens to achieve compute efficiency,and 2) Reusing the weights on each layer of LLM to facilitate effectivemodality alignment between vision and language for data efficiency.Experimental results demonstrate the effectiveness of EE-MLLM across a range ofbenchmarks, including general-purpose datasets like MMBench and SeedBench, aswell as fine-grained tasks such as TextVQA and DocVQA.</description><author>Feipeng Ma, Yizhou Zhou, Hebei Li, Zilong He, Siying Wu, Fengyun Rao, Yueyi Zhang, Xiaoyan Sun</author><pubDate>Wed, 21 Aug 2024 17:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11795v1</guid></item><item><title>NYU CTF Dataset: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security</title><link>http://arxiv.org/abs/2406.05590v2</link><description>Large Language Models (LLMs) are being deployed across various domains today.However, their capacity to solve Capture the Flag (CTF) challenges incybersecurity has not been thoroughly evaluated. To address this, we develop anovel method to assess LLMs in solving CTF challenges by creating a scalable,open-source benchmark database specifically designed for these applications.This database includes metadata for LLM testing and adaptive learning,compiling a diverse range of CTF challenges from popular competitions.Utilizing the advanced function calling capabilities of LLMs, we build a fullyautomated system with an enhanced workflow and support for external tool calls.Our benchmark dataset and automated framework allow us to evaluate theperformance of five LLMs, encompassing both black-box and open-source models.This work lays the foundation for future research into improving the efficiencyof LLMs in interactive cybersecurity tasks and automated task planning. Byproviding a specialized dataset, our project offers an ideal platform fordeveloping, testing, and refining LLM-based approaches to vulnerabilitydetection and resolution. Evaluating LLMs on these challenges and comparingwith human performance yields insights into their potential for AI-drivencybersecurity solutions to perform real-world threat management. We make ourdataset open source to public https://github.com/NYU-LLM-CTF/LLM_CTF_Databasealong with our playground automated frameworkhttps://github.com/NYU-LLM-CTF/llm_ctf_automation.</description><author>Minghao Shao, Sofija Jancheska, Meet Udeshi, Brendan Dolan-Gavitt, Haoran Xi, Kimberly Milner, Boyuan Chen, Max Yin, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique</author><pubDate>Wed, 21 Aug 2024 17:34:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05590v2</guid></item><item><title>Competence-Based Analysis of Language Models</title><link>http://arxiv.org/abs/2303.00333v4</link><description>Despite the recent successes of large, pretrained neural language models(LLMs), comparatively little is known about the representations of linguisticstructure they learn during pretraining, which can lead to unexpected behaviorsin response to prompt variation or distribution shift. To better understandthese models and behaviors, we introduce a general model analysis framework tostudy LLMs with respect to their representation and use of human-interpretablelinguistic properties. Our framework, CALM (Competence-based Analysis ofLanguage Models), is designed to investigate LLM competence in the context ofspecific tasks by intervening on models' internal representations of differentlinguistic properties using causal probing, and measuring models' alignmentunder these interventions with a given ground-truth causal model of the task.We also develop a new approach for performing causal probing interventionsusing gradient-based adversarial attacks, which can target a broader range ofproperties and representations than prior techniques. Finally, we carry out acase study of CALM using these interventions to analyze and compare LLMcompetence across a variety of lexical inference tasks, showing that CALM canbe used to explain and predict behaviors across these tasks.</description><author>Adam Davies, Jize Jiang, ChengXiang Zhai</author><pubDate>Wed, 21 Aug 2024 17:27:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00333v4</guid></item><item><title>Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design</title><link>http://arxiv.org/abs/2408.11793v1</link><description>Molecular property prediction and generative design via deep learning modelshas been the subject of intense research given its potential to acceleratedevelopment of new, high-performance materials. More recently, these workflowshave been significantly augmented with the advent of large language models(LLMs) and systems of LLM-driven agents capable of utilizing pre-trained modelsto make predictions in the context of more complex research tasks. Whileeffective, there is still room for substantial improvement within the agenticsystems on the retrieval of salient information for material design tasks.Moreover, alternative uses of predictive deep learning models, such asleveraging their latent representations to facilitate cross-modal retrievalaugmented generation within agentic systems to enable task-specific materialsdesign, has remained unexplored. Herein, we demonstrate that large, pre-trainedchemistry foundation models can serve as a basis for enabling semanticchemistry information retrieval for both small-molecules, complex polymericmaterials, and reactions. Additionally, we show the use of chemistry foundationmodels in conjunction with image models such as OpenCLIP facilitateunprecedented queries and information retrieval across multiplecharacterization data domains. Finally, we demonstrate the integration of thesesystems within multi-agent systems to facilitate structure andtopological-based natural language queries and information retrieval forcomplex research tasks.</description><author>Nathaniel H. Park, Tiffany J. Callahan, James L. Hedrick, Tim Erdmann, Sara Capponi</author><pubDate>Wed, 21 Aug 2024 17:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11793v1</guid></item><item><title>Optical ISAC: Fundamental Performance Limits and Transceiver Design</title><link>http://arxiv.org/abs/2408.11792v1</link><description>This paper characterizes the optimal capacity-distortion (C-D) tradeoff in anoptical point-to-point (P2P) system with single-input single-output forcommunication and single-input multiple-output for sensing (SISO-SIMO-C/S)within an integrated sensing and communication (ISAC) framework. We introducepractical, asymptotically optimal maximum a posteriori (MAP) and maximumlikelihood estimators (MLE) for target distance, addressing nonlinearmeasurement-to-state relationships and non-conjugate priors. Our results showthese estimators converge to the Bayesian Cramer-Rao bound (BCRB) as sensingantennas increase. We also demonstrate that the achievable rate-CRB (AR-CRB)serves as an outer bound (OB) for the optimal C-D region. To optimize inputdistribution across the Pareto boundary of the C-D region, we propose twoalgorithms: an iterative Blahut-Arimoto algorithm (BAA)-type method and amemory-efficient closed-form (CF) approach, including a CF optimal distributionfor high optical signal-to-noise ratio (O-SNR) conditions. Additionally, weextend and modify the Deterministic-Random Tradeoff (DRT) to this optical ISACcontext.</description><author>Alireza Ghazavi Khorasgani, Mahtab Mirmohseni, Ahmed Elzanaty</author><pubDate>Wed, 21 Aug 2024 17:25:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11792v1</guid></item><item><title>Critique-out-Loud Reward Models</title><link>http://arxiv.org/abs/2408.11791v1</link><description>Traditionally, reward models used for reinforcement learning from humanfeedback (RLHF) are trained to directly predict preference scores withoutleveraging the generation capabilities of the underlying large language model(LLM). This limits the capabilities of reward models as they must reasonimplicitly about the quality of a response, i.e., preference modeling must beperformed in a single forward pass through the model. To enable reward modelsto reason explicitly about the quality of a response, we introduceCritique-out-Loud (CLoud) reward models. CLoud reward models operate by firstgenerating a natural language critique of the assistant's response that is thenused to predict a scalar reward for the quality of the response. We demonstratethe success of CLoud reward models for both Llama-3-8B and 70B base models:compared to classic reward models CLoud reward models improve pairwisepreference classification accuracy on RewardBench by 4.65 and 5.84 percentagepoints for the 8B and 70B base models respectively. Furthermore, CLoud rewardmodels lead to a Pareto improvement for win rate on ArenaHard when used as thescoring model for Best-of-N. Finally, we explore how to exploit the dynamicinference compute capabilities of CLoud reward models by performingself-consistency decoding for reward prediction.</description><author>Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang, Prithviraj Ammanabrolu</author><pubDate>Wed, 21 Aug 2024 17:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11791v1</guid></item><item><title>Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era</title><link>http://arxiv.org/abs/2404.11457v2</link><description>With the rapid advancements of large language models (LLMs), informationretrieval (IR) systems, such as search engines and recommender systems, haveundergone a significant paradigm shift. This evolution, while heralding newopportunities, introduces emerging challenges, particularly in terms of biasesand unfairness, which may threaten the information ecosystem. In this paper, wepresent a comprehensive survey of existing works on emerging and pressing biasand unfairness issues in IR systems when the integration of LLMs. We firstunify bias and unfairness issues as distribution mismatch problems, providing agroundwork for categorizing various mitigation strategies through distributionalignment. Subsequently, we systematically delve into the specific bias andunfairness issues arising from three critical stages of LLMs integration intoIR systems: data collection, model development, and result evaluation. In doingso, we meticulously review and analyze recent literature, focusing on thedefinitions, characteristics, and corresponding mitigation strategiesassociated with these issues. Finally, we identify and highlight some openproblems and challenges for future work, aiming to inspire researchers andstakeholders in the IR field and beyond to better understand and mitigate biasand unfairness issues of IR in this LLM era. We also consistently maintain aGitHub repository for the relevant papers and resources in this risingdirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.</description><author>Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, Jun Xu</author><pubDate>Wed, 21 Aug 2024 17:23:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11457v2</guid></item><item><title>DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework</title><link>http://arxiv.org/abs/2408.11788v1</link><description>Current video generation models excel at creating short, realistic clips, butstruggle with longer, multi-scene videos. We introduce \texttt{DreamFactory},an LLM-based framework that tackles this challenge. \texttt{DreamFactory}leverages multi-agent collaboration principles and a Key Frames IterationDesign Method to ensure consistency and style across long videos. It utilizesChain of Thought (COT) to address uncertainties inherent in large languagemodels. \texttt{DreamFactory} generates long, stylistically coherent, andcomplex videos. Evaluating these long-form videos presents a challenge. Wepropose novel metrics such as Cross-Scene Face Distance Score and Cross-SceneStyle Consistency Score. To further research in this area, we contribute theMulti-Scene Videos Dataset containing over 150 human-rated videos.</description><author>Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend F. Bissyand, Saad Ezzini</author><pubDate>Wed, 21 Aug 2024 17:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11788v1</guid></item><item><title>NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation</title><link>http://arxiv.org/abs/2408.11787v1</link><description>Domain-generalized nuclei segmentation refers to the generalizability ofmodels to unseen domains based on knowledge learned from source domains and ischallenged by various image conditions, cell types, and stain strategies.Recently, the Segment Anything Model (SAM) has made great success in universalimage segmentation by interactive prompt modes (e.g., point and box). Despiteits strengths, the original SAM presents limited adaptation to medical images.Moreover, SAM requires providing manual bounding box prompts for each object toproduce satisfactory segmentation masks, so it is laborious in nucleisegmentation scenarios. To address these limitations, we propose adomain-generalizable framework for nuclei image segmentation, abbreviated toNuSegDG. Specifically, we first devise a Heterogeneous Space Adapter(HS-Adapter) to learn multi-dimensional feature representations of differentnuclei domains by injecting a small number of trainable parameters into theimage encoder of SAM. To alleviate the labor-intensive requirement of manualprompts, we introduce a Gaussian-Kernel Prompt Encoder (GKP-Encoder) togenerate density maps driven by a single point, which guides segmentationpredictions by mixing position prompts and semantic prompts. Furthermore, wepresent a Two-Stage Mask Decoder (TSM-Decoder) to effectively convert semanticmasks to instance maps without the manual demand for morphological shaperefinement. Based on our experimental evaluations, the proposed NuSegDGdemonstrates state-of-the-art performance in nuclei instance segmentation,exhibiting superior domain generalization capabilities. The source code isavailable at https://github.com/xq141839/NuSegDG.</description><author>Zhenye Lou, Qing Xu, Zekun Jiang, Xiangjian He, Zhen Chen, Yi Wang, Chenxin Li, Maggie M. He, Wenting Duan</author><pubDate>Wed, 21 Aug 2024 17:19:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11787v1</guid></item><item><title>Timeline and Boundary Guided Diffusion Network for Video Shadow Detection</title><link>http://arxiv.org/abs/2408.11785v1</link><description>Video Shadow Detection (VSD) aims to detect the shadow masks with framesequence. Existing works suffer from inefficient temporal learning. Moreover,few works address the VSD problem by considering the characteristic (i.e.,boundary) of shadow. Motivated by this, we propose a Timeline and BoundaryGuided Diffusion (TBGDiff) network for VSD where we take account of thepast-future temporal guidance and boundary information jointly. In detail, wedesign a Dual Scale Aggregation (DSA) module for better temporal understandingby rethinking the affinity of the long-term and short-term frames for theclipped video. Next, we introduce Shadow Boundary Aware Attention (SBAA) toutilize the edge contexts for capturing the characteristics of shadows.Moreover, we are the first to introduce the Diffusion model for VSD in which weexplore a Space-Time Encoded Embedding (STEE) to inject the temporal guidancefor Diffusion to conduct shadow detection. Benefiting from these designs, ourmodel can not only capture the temporal information but also the shadowproperty. Extensive experiments show that the performance of our approachovertakes the state-of-the-art methods, verifying the effectiveness of ourcomponents. We release the codes, weights, and results at\url{https://github.com/haipengzhou856/TBGDiff}.</description><author>Haipeng Zhou, Honqiu Wang, Tian Ye, Zhaohu Xing, Jun Ma, Ping Li, Qiong Wang, Lei Zhu</author><pubDate>Wed, 21 Aug 2024 17:16:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11785v1</guid></item><item><title>RFID based Health Adherence Medicine Case Using Fair Federated Learning</title><link>http://arxiv.org/abs/2408.11782v1</link><description>Medication nonadherence significantly reduces the effectiveness of therapies,yet it remains prevalent among patients. Nonadherence has been linked toadverse outcomes, including increased risks of mortality and hospitalization.Although various methods exist to help patients track medication schedules,such as the Intelligent Drug Administration System (IDAS) and Smart Blister,these tools often face challenges that hinder their commercial viability.Building on the principles of dosage measurement and information communicationin IoT, we introduce the Smart Pill Case a smart health adherence tool thatleverages RFID-based data recording and NFC-based data extraction. This systemincorporates a load cell for precise dosage measurement and features an Androidapp to monitor medication intake, offer suggestions, and issue warnings. Toenhance the effectiveness and personalization of the Smart Pill Case, wepropose integrating federated learning into the system. Federated learningallows the Smart Pill Case to learn from medication adherence patterns acrossmultiple users without compromising individual privacy. By training machinelearning models on decentralized data collected from various Smart Pill Cases,the system can continuously improve its recommendations and warnings, adaptingto the diverse needs and behaviors of users. This approach not only enhancesthe tools ability to support medication adherence but also ensures thatsensitive user data remains secure and private.</description><author>Ali Kamrani khodaei, Sina Hajer Ahmadi</author><pubDate>Wed, 21 Aug 2024 17:12:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11782v1</guid></item><item><title>Personality Alignment of Large Language Models</title><link>http://arxiv.org/abs/2408.11779v1</link><description>Current methods for aligning large language models (LLMs) typically aim toreflect general human values and behaviors, but they often fail to capture theunique characteristics and preferences of individual users. To address thisgap, we introduce the concept of Personality Alignment. This approach tailorsLLMs' responses and decisions to match the specific preferences of individualusers or closely related groups. Inspired by psychometrics, we created thePersonality Alignment with Personality Inventories (PAPI) dataset, whichincludes data from 300,000 real subjects, each providing behavioral preferencesbased on the Big Five Personality Factors. This dataset allows us toquantitatively evaluate the extent to which LLMs can align with each subject'sbehavioral patterns. Recognizing the challenges of personality alignments: suchas limited personal data, diverse preferences, and scalability requirements: wedeveloped an activation intervention optimization method. This method enhancesLLMs' ability to efficiently align with individual behavioral preferences usingminimal data and computational resources. Remarkably, our method, PAS, achievessuperior performance while requiring only 1/5 of the optimization time comparedto DPO, offering practical value for personality alignment. Our work paves theway for future AI systems to make decisions and reason in truly personalityways, enhancing the relevance and meaning of AI interactions for each user andadvancing human-centered artificial intelligence.The code has released in\url{https://github.com/zhu-minjun/PAlign}.</description><author>Minjun Zhu, Linyi Yang, Yue Zhang</author><pubDate>Wed, 21 Aug 2024 17:09:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11779v1</guid></item><item><title>Sum of Squares Circuits</title><link>http://arxiv.org/abs/2408.11778v1</link><description>Designing expressive generative models that support exact and efficientinference is a core question in probabilistic ML. Probabilistic circuits (PCs)offer a framework where this tractability-vs-expressiveness trade-off can beanalyzed theoretically. Recently, squared PCs encoding subtractive mixtures vianegative parameters have emerged as tractable models that can be exponentiallymore expressive than monotonic PCs, i.e., PCs with positive parameters only. Inthis paper, we provide a more precise theoretical characterization of theexpressiveness relationships among these models. First, we prove that squaredPCs can be less expressive than monotonic ones. Second, we formalize a novelclass of PCs -- sum of squares PCs -- that can be exponentially more expressivethan both squared and monotonic PCs. Around sum of squares PCs, we build anexpressiveness hierarchy that allows us to precisely unify and separatedifferent tractable model classes such as Born Machines and PSD models, andother recently introduced tractable probabilistic models by using complexparameters. Finally, we empirically show the effectiveness of sum of squarescircuits in performing distribution estimation.</description><author>Lorenzo Loconte, Stefan Mengel, Antonio Vergari</author><pubDate>Wed, 21 Aug 2024 17:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11778v1</guid></item><item><title>A Novel State Space Model with Local Enhancement and State Sharing for Image Fusion</title><link>http://arxiv.org/abs/2404.09293v2</link><description>In image fusion tasks, images from different sources possess distinctcharacteristics. This has driven the development of numerous methods to explorebetter ways of fusing them while preserving their respectivecharacteristics.Mamba, as a state space model, has emerged in the field ofnatural language processing. Recently, many studies have attempted to extendMamba to vision tasks. However, due to the nature of images different fromcausal language sequences, the limited state capacity of Mamba weakens itsability to model image information. Additionally, the sequence modeling abilityof Mamba is only capable of spatial information and cannot effectively capturethe rich spectral information in images. Motivated by these challenges, wecustomize and improve the vision Mamba network designed for the image fusiontask. Specifically, we propose the local-enhanced vision Mamba block, dubbed asLEVM. The LEVM block can improve local information perception of the networkand simultaneously learn local and global spatial information. Furthermore, wepropose the state sharing technique to enhance spatial details and integratespatial and spectral information. Finally, the overall network is a multi-scalestructure based on vision Mamba, called LE-Mamba. Extensive experiments showthe proposed methods achieve state-of-the-art results on multispectralpansharpening and multispectral and hyperspectral image fusion datasets, anddemonstrate the effectiveness of the proposed approach. Codes can be accessedat \url{https://github.com/294coder/Efficient-MIF}.</description><author>Zihan Cao, Xiao Wu, Liang-Jian Deng, Yu Zhong</author><pubDate>Wed, 21 Aug 2024 17:07:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09293v2</guid></item><item><title>Exploiting Diffusion Prior for Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2406.11105v2</link><description>Out-of-distribution (OOD) detection is crucial for deploying robust machinelearning models, especially in areas where security is critical. However,traditional OOD detection methods often fail to capture complex datadistributions from large scale date. In this paper, we present a novel approachfor OOD detection that leverages the generative ability of diffusion models andthe powerful feature extraction capabilities of CLIP. By using these featuresas conditional inputs to a diffusion model, we can reconstruct the images afterencoding them with CLIP. The difference between the original and reconstructedimages is used as a signal for OOD identification. The practicality andscalability of our method is increased by the fact that it does not requireclass-specific labeled ID data, as is the case with many other methods.Extensive experiments on several benchmark datasets demonstrates the robustnessand effectiveness of our method, which have significantly improved thedetection accuracy.</description><author>Armando Zhu, Jiabei Liu, Keqin Li, Shuying Dai, Bo Hong, Peng Zhao, Changsong Wei</author><pubDate>Wed, 21 Aug 2024 17:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11105v2</guid></item><item><title>No Such Thing as a General Learner: Language models and their dual optimization</title><link>http://arxiv.org/abs/2408.09544v2</link><description>What role can the otherwise successful Large Language Models (LLMs) play inthe understanding of human cognition, and in particular in terms of informinglanguage acquisition debates? To contribute to this question, we first arguethat neither humans nor LLMs are general learners, in a variety of senses. Wemake a novel case for how in particular LLMs follow a dual-optimizationprocess: they are optimized during their training (which is typically comparedto language acquisition), and modern LLMs have also been selected, through aprocess akin to natural selection in a species. From this perspective, we arguethat the performance of LLMs, whether similar or dissimilar to that of humans,does not weigh easily on important debates about the importance of humancognitive biases for language.</description><author>Emmanuel Chemla, Ryan M. Nefdt</author><pubDate>Wed, 21 Aug 2024 17:04:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09544v2</guid></item><item><title>A Survey for Foundation Models in Autonomous Driving</title><link>http://arxiv.org/abs/2402.01105v2</link><description>The advent of foundation models has revolutionized the fields of naturallanguage processing and computer vision, paving the way for their applicationin autonomous driving (AD). This survey presents a comprehensive review of morethan 40 research papers, demonstrating the role of foundation models inenhancing AD. Large language models contribute to planning and simulation inAD, particularly through their proficiency in reasoning, code generation andtranslation. In parallel, vision foundation models are increasingly adapted forcritical tasks such as 3D object detection and tracking, as well as creatingrealistic driving scenarios for simulation and testing. Multi-modal foundationmodels, integrating diverse inputs, exhibit exceptional visual understandingand spatial reasoning, crucial for end-to-end AD. This survey not only providesa structured taxonomy, categorizing foundation models based on their modalitiesand functionalities within the AD domain but also delves into the methodsemployed in current research. It identifies the gaps between existingfoundation models and cutting-edge AD approaches, thereby charting futureresearch directions and proposing a roadmap for bridging these gaps.</description><author>Haoxiang Gao, Zhongruo Wang, Yaqian Li, Kaiwen Long, Ming Yang, Yiqing Shen</author><pubDate>Wed, 21 Aug 2024 17:02:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01105v2</guid></item><item><title>Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards</title><link>http://arxiv.org/abs/2408.11775v1</link><description>Recent studies show that large language models (LLMs) struggle with technicalstandards in telecommunications. We propose a fine-tuned retrieval-augmentedgeneration (RAG) system based on the Phi-2 small language model (SLM) to serveas an oracle for communication networks. Our developed system leveragesforward-looking semantic chunking to adaptively determine parsing breakpointsbased on embedding similarity, enabling effective processing of diversedocument formats. To handle the challenge of multiple similar contexts intechnical standards, we employ a re-ranking algorithm to prioritize the mostrelevant retrieved chunks. Recognizing the limitations of Phi-2's small contextwindow, we implement a recent technique, namely SelfExtend, to expand thecontext window during inference, which not only boosts the performance but alsocan accommodate a wider range of user queries and design requirements fromcustomers to specialized technicians. For fine-tuning, we utilize the low-rankadaptation (LoRA) technique to enhance computational efficiency during trainingand enable effective fine-tuning on small datasets. Our comprehensiveexperiments demonstrate substantial improvements over existingquestion-answering approaches in the telecom domain, achieving performance thatexceeds larger language models such as GPT-4 (which is about 880 times largerin size). This work presents a novel approach to leveraging SLMs forcommunication networks, offering a balance of efficiency and performance. Thiswork can serve as a foundation towards agentic language models for networks.</description><author>Omar Erak, Nouf Alabbasi, Omar Alhussein, Ismail Lotfi, Amr Hussein, Sami Muhaidat, Merouane Debbah</author><pubDate>Wed, 21 Aug 2024 17:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11775v1</guid></item><item><title>Deviations from the Nash equilibrium and emergence of tacit collusion in a two-player optimal execution game with reinforcement learning</title><link>http://arxiv.org/abs/2408.11773v1</link><description>The use of reinforcement learning algorithms in financial trading is becomingincreasingly prevalent. However, the autonomous nature of these algorithms canlead to unexpected outcomes that deviate from traditional game-theoreticalpredictions and may even destabilize markets. In this study, we examine ascenario in which two autonomous agents, modeled with Double Deep Q-Learning,learn to liquidate the same asset optimally in the presence of market impact,using the Almgren-Chriss (2000) framework. Our results show that the strategieslearned by the agents deviate significantly from the Nash equilibrium of thecorresponding market impact game. Notably, the learned strategies exhibit tacitcollusion, closely aligning with the Pareto-optimal solution. We furtherexplore how different levels of market volatility influence the agents'performance and the equilibria they discover, including scenarios wherevolatility differs between the training and testing phases.</description><author>Fabrizio Lillo, Andrea Macr</author><pubDate>Wed, 21 Aug 2024 16:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11773v1</guid></item><item><title>KOSMOS-2.5: A Multimodal Literate Model</title><link>http://arxiv.org/abs/2309.11419v2</link><description>The automatic reading of text-intensive images represents a significantadvancement toward achieving Artificial General Intelligence (AGI). In thispaper we present KOSMOS-2.5, a multimodal literate model for machine reading oftext-intensive images. Pre-trained on a large-scale corpus of text-intensiveimages, KOSMOS-2.5 excels in two distinct yet complementary transcriptiontasks: (1) generating spatially-aware text blocks, where each block of text isassigned spatial coordinates within the image, and (2) producing structuredtext output that captures both style and structure in markdown format. Thisunified multimodal literate capability is achieved through a shareddecoder-only autoregressive Transformer architecture and task-specific prompts.Building on this foundation, we fine-tune KOSMOS-2.5 for document understandingtasks, resulting in a document understanding generalist named KOSMOS-2.5-CHAT.Additionally, a large corpus of 357.4 million document pages spanning diversedomains was curated for pre-training. We evaluate KOSMOS-2.5 on two newlyproposed benchmarks, OCREval and MarkdownEval, for document-level textrecognition and image-to-markdown generation, demonstrating impressive literatecapabilities comparable to GPT-4o. KOSMOS-2.5-CHAT achieves performancecomparable to other state-of-the-art generalists that are five times larger(1.3B vs. 7B) across nine text-rich visual question answering benchmarks.Models and code have been available at \url{https://aka.ms/kosmos25}.</description><author>Tengchao Lv, Yupan Huang, Jingye Chen, Yuzhong Zhao, Yilin Jia, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei</author><pubDate>Wed, 21 Aug 2024 16:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11419v2</guid></item><item><title>Hypergraph: A Unified and Uniform Definition with Application to Chemical Hypergraph and More</title><link>http://arxiv.org/abs/2405.12235v5</link><description>The conventional definition of hypergraph has two major issues: (1) there isnot a standard definition of directed hypergraph and (2) there is not a formaldefinition of nested hypergraph. To resolve these issues, we propose a newdefinition of hypergraph that unifies the concepts of undirected, directed andnested hypergraphs, and that is uniform in using hyperedge as a singleconstruct for representing high-order correlations among things, i.e., nodesand hyperedges. Specifically, we define a hyperedge to be a simple hyperedge, anesting hyperedge, or a directed hyperedge. With this new definition, ahypergraph is nested if it has nesting hyperedge(s), and is directed if it hasdirected hyperedge(s). Otherwise, a hypergraph is a simple hypergraph. Theuniformity and power of this new definition, with visualization, shouldfacilitate the use of hypergraph for representing (hierarchical) high-ordercorrelations in general and chemical systems in particular. Graph has beenwidely used as a mathematical structure for machine learning on molecularstructures and 3D molecular geometries. However, graph has a major limitation:it can represent only pairwise correlations between nodes. Hypergraph extendsgraph with high-order correlations among nodes. This extension is significantor essential for machine learning on chemical systems. For molecules, this issignificant as it allows the direct, explicit representation of multicenterbonds and molecular substructures. For chemical reactions, this is essentialsince most chemical reactions involve multiple participants. We propose the useof chemical hypergraph, a multilevel hypergraph with simple, nesting anddirected hyperedges, as a single mathematical structure for representingchemical systems. We apply the new definition of hypergraph to chemicalhypergraph and, as simplified versions, molecular hypergraph and chemicalreaction hypergraph.</description><author>Daniel T. Chang</author><pubDate>Wed, 21 Aug 2024 16:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12235v5</guid></item><item><title>Embedding Ordinality to Binary Loss Function for Improving Solar Flare Forecasting</title><link>http://arxiv.org/abs/2408.11768v1</link><description>In this paper, we propose a novel loss function aimed at optimizing thebinary flare prediction problem by embedding the intrinsic ordinal flarecharacteristics into the binary cross-entropy (BCE) loss function. Thismodification is intended to provide the model with better guidance based on theordinal characteristics of the data and improve the overall performance of themodels. For our experiments, we employ a ResNet34-based model with transferlearning to predict $\geq$M-class flares by utilizing the shape-based featuresof magnetograms of active region (AR) patches spanning from $-$90$^{\circ}$ to$+$90$^{\circ}$ of solar longitude as our input data. We use a composite skillscore (CSS) as our evaluation metric, which is calculated as the geometric meanof the True Skill Score (TSS) and the Heidke Skill Score (HSS) to rank andcompare our models' performance. The primary contributions of this work are asfollows: (i) We introduce a novel approach to encode ordinality into a binaryloss function showing an application to solar flare prediction, (ii) We enhancesolar flare forecasting by enabling flare predictions for each AR across theentire solar disk, without any longitudinal restrictions, and evaluate andcompare performance. (iii) Our candidate model, optimized with the proposedloss function, shows an improvement of $\sim$7%, $\sim$4%, and $\sim$3% for ARpatches within $\pm$30$^\circ$, $\pm$60$^\circ$, and $\pm$90$^\circ$ of solarlongitude, respectively in terms of CSS, when compared with standard BCE.Additionally, we demonstrate the ability to issue flare forecasts for ARs innear-limb regions (regions between $\pm$60$^{\circ}$ to $\pm$90$^{\circ}$) witha CSS=0.34 (TSS=0.50 and HSS=0.23), expanding the scope of AR-based models forsolar flare prediction. This advances the reliability of solar flare forecasts,leading to more effective prediction capabilities.</description><author>Chetraj Pandey, Anli Ji, Jinsu Hong, Rafal A. Angryk, Berkay Aydin</author><pubDate>Wed, 21 Aug 2024 16:42:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11768v1</guid></item><item><title>Diversity and stylization of the contemporary user-generated visual arts in the complexity-entropy plane</title><link>http://arxiv.org/abs/2408.10356v2</link><description>The advent of computational and numerical methods in recent times hasprovided new avenues for analyzing art historiographical narratives and tracingthe evolution of art styles therein. Here, we investigate an evolutionaryprocess underpinning the emergence and stylization of contemporaryuser-generated visual art styles using the complexity-entropy (C-H) plane,which quantifies local structures in paintings. Informatizing 149,780 imagescurated in DeviantArt and Behance platforms from 2010 to 2020, we analyze therelationship between local information of the C-H space and multi-level imagefeatures generated by a deep neural network and a feature extraction algorithm.The results reveal significant statistical relationships between the C-Hinformation of visual artistic styles and the dissimilarities of themulti-level image features over time within groups of artworks. By disclosing aparticular C-H region where the diversity of image representations isnoticeably manifested, our analyses reveal an empirical condition of emergingstyles that are both novel in the C-H plane and characterized by greaterstylistic diversity. Our research shows that visual art analyses combined withphysics-inspired methodologies and machine learning, can provide macroscopicinsights into quantitatively mapping relevant characteristics of anevolutionary process underpinning the creative stylization of uncharted visualarts of given groups and time.</description><author>Seunghwan Kim, Byunghwee Lee, Wonjae Lee</author><pubDate>Wed, 21 Aug 2024 16:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10356v2</guid></item><item><title>PathMLP: Smooth Path Towards High-order Homophily</title><link>http://arxiv.org/abs/2306.13532v2</link><description>Real-world graphs exhibit increasing heterophily, where nodes no longer tendto be connected to nodes with the same label, challenging the homophilyassumption of classical graph neural networks (GNNs) and impeding theirperformance. Intriguingly, from the observation of heterophilous data, wenotice that certain high-order information exhibits higher homophily, whichmotivates us to involve high-order information in node representation learning.However, common practices in GNNs to acquire high-order information mainlythrough increasing model depth and altering message-passing mechanisms, which,albeit effective to a certain extent, suffer from three shortcomings: 1)over-smoothing due to excessive model depth and propagation times; 2)high-order information is not fully utilized; 3) low computational efficiency.In this regard, we design a similarity-based path sampling strategy to capturesmooth paths containing high-order homophily. Then we propose a lightweightmodel based on multi-layer perceptrons (MLP), named PathMLP, which can encodemessages carried by paths via simple transformation and concatenationoperations, and effectively learn node representations in heterophilous graphsthrough adaptive path aggregation. Extensive experiments demonstrate that ourmethod outperforms baselines on 16 out of 20 datasets, underlining itseffectiveness and superiority in alleviating the heterophily problem. Inaddition, our method is immune to over-smoothing and has high computationalefficiency. The source code will be available inhttps://github.com/Graph4Sec-Team/PathMLP.</description><author>Jiajun Zhou, Chenxuan Xie, Shengbo Gong, Jiaxu Qian, Shanqing Yu, Qi Xuan, Xiaoniu Yang</author><pubDate>Wed, 21 Aug 2024 16:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13532v2</guid></item><item><title>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks</title><link>http://arxiv.org/abs/2311.12786v2</link><description>Fine-tuning large pre-trained models has become the de facto strategy fordeveloping both task-specific and general-purpose machine learning systems,including developing models that are safe to deploy. Despite its clearimportance, there has been minimal work that explains how fine-tuning altersthe underlying capabilities learned by a model during pretraining: doesfine-tuning yield entirely novel capabilities or does it just modulate existingones? We address this question empirically in synthetic, controlled settingswhere we can use mechanistic interpretability tools (e.g., network pruning andprobing) to understand how the model's underlying capabilities are changing. Weperform an extensive analysis of the effects of fine-tuning in these settings,and show that: (i) fine-tuning rarely alters the underlying model capabilities;(ii) a minimal transformation, which we call a 'wrapper', is typically learnedon top of the underlying model capabilities, creating the illusion that theyhave been modified; and (iii) further fine-tuning on a task where such hiddencapabilities are relevant leads to sample-efficient 'revival' of thecapability, i.e., the model begins reusing these capability after only a fewgradient steps. This indicates that practitioners can unintentionally remove amodel's safety wrapper merely by fine-tuning it on a, e.g., superficiallyunrelated, downstream task. We additionally perform analysis on language modelstrained on the TinyStories dataset to support our claims in a more realisticsetup.</description><author>Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktschel, David Scott Krueger</author><pubDate>Wed, 21 Aug 2024 16:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12786v2</guid></item><item><title>D-RMGPT: Robot-assisted collaborative tasks driven by large multimodal models</title><link>http://arxiv.org/abs/2408.11761v1</link><description>Collaborative robots are increasingly popular for assisting humans at workand daily tasks. However, designing and setting up interfaces for human-robotcollaboration is challenging, requiring the integration of multiple components,from perception and robot task control to the hardware itself. Frequently, thisleads to highly customized solutions that rely on large amounts of costlytraining data, diverging from the ideal of flexible and general interfaces thatempower robots to perceive and adapt to unstructured environments where theycan naturally collaborate with humans. To overcome these challenges, this paperpresents the Detection-Robot Management GPT (D-RMGPT), a robot-assistedassembly planner based on Large Multimodal Models (LMM). This system can assistinexperienced operators in assembly tasks without requiring any markers orprevious training. D-RMGPT is composed of DetGPT-V and R-ManGPT. DetGPT-V,based on GPT-4V(vision), perceives the surrounding environment through one-shotanalysis of prompted images of the current assembly stage and the list ofcomponents to be assembled. It identifies which components have already beenassembled by analysing their features and assembly requirements. R-ManGPT,based on GPT-4, plans the next component to be assembled and generates therobot's discrete actions to deliver it to the human co-worker. Experimentaltests on assembling a toy aircraft demonstrated that D-RMGPT is flexible andintuitive to use, achieving an assembly success rate of 83% while reducing theassembly time for inexperienced operators by 33% compared to the manualprocess. http://robotics-and-ai.github.io/LMMmodels/</description><author>M. Forlini, M. Babcinschi, G. Palmieri, P. Neto</author><pubDate>Wed, 21 Aug 2024 16:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11761v1</guid></item><item><title>Accelerating Hopfield Network Dynamics: Beyond Synchronous Updates and Forward Euler</title><link>http://arxiv.org/abs/2311.15673v2</link><description>The Hopfield network serves as a fundamental energy-based model in machinelearning, capturing memory retrieval dynamics through an ordinary differentialequation (ODE). The model's output, the equilibrium point of the ODE, istraditionally computed via synchronous updates using the forward Euler method.This paper aims to overcome some of the disadvantages of this approach. Wepropose a conceptual shift, viewing Hopfield networks as instances of DeepEquilibrium Models (DEQs). The DEQ framework not only allows for the use ofspecialized solvers, but also leads to new insights on an empirical inferencetechnique that we will refer to as 'even-odd splitting'. Our theoreticalanalysis of the method uncovers a parallelizable asynchronous update scheme,which should converge roughly twice as fast as the conventional synchronousupdates. Empirical evaluations validate these findings, showcasing theadvantages of both the DEQ framework and even-odd splitting in digitallysimulating energy minimization in Hopfield networks. The code is available athttps://github.com/cgoemaere/hopdeq</description><author>Cdric Goemaere, Johannes Deleu, Thomas Demeester</author><pubDate>Wed, 21 Aug 2024 16:34:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15673v2</guid></item><item><title>The Tug-of-War Between Deepfake Generation and Detection</title><link>http://arxiv.org/abs/2407.06174v4</link><description>Multimodal generative models are rapidly evolving, leading to a surge in thegeneration of realistic video and audio that offers exciting possibilities butalso serious risks. Deepfake videos, which can convincingly impersonateindividuals, have particularly garnered attention due to their potential misusein spreading misinformation and creating fraudulent content. This survey paperexamines the dual landscape of deepfake video generation and detection,emphasizing the need for effective countermeasures against potential abuses. Weprovide a comprehensive overview of current deepfake generation techniques,including face swapping, reenactment, and audio-driven animation, whichleverage cutting-edge technologies like GANs and diffusion models to producehighly realistic fake videos. Additionally, we analyze various detectionapproaches designed to differentiate authentic from altered videos, fromdetecting visual artifacts to deploying advanced algorithms that pinpointinconsistencies across video and audio signals. The effectiveness of these detection methods heavily relies on the diversityand quality of datasets used for training and evaluation. We discuss theevolution of deepfake datasets, highlighting the importance of robust, diverse,and frequently updated collections to enhance the detection accuracy andgeneralizability. As deepfakes become increasingly indistinguishable fromauthentic content, developing advanced detection techniques that can keep pacewith generation technologies is crucial. We advocate for a proactive approachin the "tug-of-war" between deepfake creators and detectors, emphasizing theneed for continuous research collaboration, standardization of evaluationmetrics, and the creation of comprehensive benchmarks.</description><author>Hannah Lee, Changyeon Lee, Kevin Farhat, Lin Qiu, Steve Geluso, Aerin Kim, Oren Etzioni</author><pubDate>Wed, 21 Aug 2024 16:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06174v4</guid></item><item><title>SBDet: A Symmetry-Breaking Object Detector via Relaxed Rotation-Equivariance</title><link>http://arxiv.org/abs/2408.11760v1</link><description>Introducing Group Equivariant Convolution (GConv) empowers models to exploresymmetries hidden in visual data, improving their performance. However, inreal-world scenarios, objects or scenes often exhibit perturbations of asymmetric system, specifically a deviation from a symmetric architecture, whichcan be characterized by a non-trivial action of a symmetry group, known asSymmetry-Breaking. Traditional GConv methods are limited by the strictoperation rules in the group space, only ensuring features remain strictlyequivariant under limited group transformations, making it difficult to adaptto Symmetry-Breaking or non-rigid transformations. Motivated by this, weintroduce a novel Relaxed Rotation GConv (R2GConv) with our defined RelaxedRotation-Equivariant group $\mathbf{R}_4$. Furthermore, we propose a RelaxedRotation-Equivariant Network (R2Net) as the backbone and further develop theSymmetry-Breaking Object Detector (SBDet) for 2D object detection built uponit. Experiments demonstrate the effectiveness of our proposed R2GConv innatural image classification tasks, and SBDet achieves excellent performance inobject detection tasks with improved generalization capabilities androbustness.</description><author>Zhiqiang Wu, Yingjie Liu, Hanlin Dong, Xuan Tang, Jian Yang, Bo Jin, Mingsong Chen, Xian Wei</author><pubDate>Wed, 21 Aug 2024 16:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11760v1</guid></item><item><title>MambaCSR: Dual-Interleaved Scanning for Compressed Image Super-Resolution With SSMs</title><link>http://arxiv.org/abs/2408.11758v1</link><description>We present MambaCSR, a simple but effective framework based on Mamba for thechallenging compressed image super-resolution (CSR) task. Particularly, thescanning strategies of Mamba are crucial for effective contextual knowledgemodeling in the restoration process despite it relying on selective state spacemodeling for all tokens. In this work, we propose an efficient dual-interleavedscanning paradigm (DIS) for CSR, which is composed of two scanning strategies:(i) hierarchical interleaved scanning is designed to comprehensively captureand utilize the most potential contextual information within an image bysimultaneously taking advantage of the local window-based and sequentialscanning methods; (ii) horizontal-to-vertical interleaved scanning is proposedto reduce the computational cost by leaving the redundancy between the scanningof different directions. To overcome the non-uniform compression artifacts, wealso propose position-aligned cross-scale scanning to model multi-scalecontextual information. Experimental results on multiple benchmarks have shownthe great performance of our MambaCSR in the compressed image super-resolutiontask. The code will be soon availablein~\textcolor{magenta}{\url{https://github.com/renyulin-f/MambaCSR}}.</description><author>Yulin Ren, Xin Li, Mengxi Guo, Bingchen Li, Shijie Zhao, Zhibo Chen</author><pubDate>Wed, 21 Aug 2024 16:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11758v1</guid></item><item><title>Improving the Scan-rescan Precision of AI-based CMR Biomarker Estimation</title><link>http://arxiv.org/abs/2408.11754v1</link><description>Quantification of cardiac biomarkers from cine cardiovascular magneticresonance (CMR) data using deep learning (DL) methods offers many advantages,such as increased accuracy and faster analysis. However, only a few studieshave focused on the scan-rescan precision of the biomarker estimates, which isimportant for reproducibility and longitudinal analysis. Here, we propose acardiac biomarker estimation pipeline that not only focuses on achieving highsegmentation accuracy but also on improving the scan-rescan precision of thecomputed biomarkers, namely left and right ventricular ejection fraction, andleft ventricular myocardial mass. We evaluate two approaches to improve theapical-basal resolution of the segmentations used for estimating thebiomarkers: one based on image interpolation and one based on segmentationinterpolation. Using a database comprising scan-rescan cine CMR data acquiredfrom 92 subjects, we compare the performance of these two methods againstground truth (GT) segmentations and DL segmentations obtained beforeinterpolation (baseline). The results demonstrate that both the image-based andsegmentation-based interpolation methods were able to narrow Bland-Altmanscan-rescan confidence intervals for all biomarkers compared to the GT andbaseline performances. Our findings highlight the importance of focusing notonly on segmentation accuracy but also on the consistency of biomarkers acrossrepeated scans, which is crucial for longitudinal analysis of cardiac function.</description><author>Dewmini Hasara Wickremasinghe, Yiyang Xu, Esther Puyol-Antn, Paul Aljabar, Reza Razavi, Andrew P. King</author><pubDate>Wed, 21 Aug 2024 16:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11754v1</guid></item><item><title>Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks</title><link>http://arxiv.org/abs/2408.11749v1</link><description>Large Language Models (LLMs) are susceptible to malicious influence by cyberattackers through intrusions such as adversarial, backdoor, and embeddinginversion attacks. In response, the burgeoning field of LLM Security aims tostudy and defend against such threats. Thus far, the majority of works in thisarea have focused on monolingual English models, however, emerging researchsuggests that multilingual LLMs may be more vulnerable to various attacks thantheir monolingual counterparts. While previous work has investigated embeddinginversion over a small subset of European languages, it is challenging toextrapolate these findings to languages from different linguistic families andwith differing scripts. To this end, we explore the security of multilingualLLMs in the context of embedding inversion attacks and investigatecross-lingual and cross-script inversion across 20 languages, spanning over 8language families and 12 scripts. Our findings indicate that languages writtenin Arabic script and Cyrillic script are particularly vulnerable to embeddinginversion, as are languages within the Indo-Aryan language family. We furtherobserve that inversion models tend to suffer from language confusion, sometimesgreatly reducing the efficacy of an attack. Accordingly, we systematicallyexplore this bottleneck for inversion models, uncovering predictable patternswhich could be leveraged by attackers. Ultimately, this study aims to furtherthe field's understanding of the outstanding security vulnerabilities facingmultilingual LLMs and raise awareness for the languages most at risk ofnegative impact from these attacks.</description><author>Yiyi Chen, Russa Biswas, Heather Lent, Johannes Bjerva</author><pubDate>Wed, 21 Aug 2024 16:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11749v1</guid></item><item><title>DH-Bench: Probing Depth and Height Perception of Large Visual-Language Models</title><link>http://arxiv.org/abs/2408.11748v1</link><description>Geometric understanding is crucial for navigating and interacting with ourenvironment. While large Vision Language Models (VLMs) demonstrate impressivecapabilities, deploying them in real-world scenarios necessitates a comparablegeometric understanding in visual perception. In this work, we focus on thegeometric comprehension of these models; specifically targeting the depths andheights of objects within a scene. Our observations reveal that, although VLMsexcel in basic geometric properties perception such as shape and size, theyencounter significant challenges in reasoning about the depth and height ofobjects. To address this, we introduce a suite of benchmark datasetsencompassing Synthetic 2D, Synthetic 3D, and Real-World scenarios to rigorouslyevaluate these aspects. We benchmark 17 state-of-the-art VLMs using thesedatasets and find that they consistently struggle with both depth and heightperception. Our key insights include detailed analyses of the shortcomings indepth and height reasoning capabilities of VLMs and the inherent bias presentin these models. This study aims to pave the way for the development of VLMswith enhanced geometric understanding, crucial for real-world applications. Thecode and datasets for our benchmarks will be available at\url{https://tinyurl.com/DH-Bench1}.</description><author>Shehreen Azad, Yash Jain, Rishit Garg, Yogesh S Rawat, Vibhav Vineet</author><pubDate>Wed, 21 Aug 2024 16:16:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11748v1</guid></item><item><title>Open-Ended 3D Point Cloud Instance Segmentation</title><link>http://arxiv.org/abs/2408.11747v1</link><description>Open-Vocab 3D Instance Segmentation methods (OV-3DIS) have recentlydemonstrated their ability to generalize to unseen objects. However, thesemethods still depend on predefined class names during testing, restricting theautonomy of agents. To mitigate this constraint, we propose a novel problemtermed Open-Ended 3D Instance Segmentation (OE-3DIS), which eliminates thenecessity for predefined class names during testing. Moreover, we contribute acomprehensive set of strong baselines, derived from OV-3DIS approaches andleveraging 2D Multimodal Large Language Models. To assess the performance ofour OE-3DIS system, we introduce a novel Open-Ended score, evaluating both thesemantic and geometric quality of predicted masks and their associated classnames, alongside the standard AP score. Our approach demonstrates significantperformance improvements over the baselines on the ScanNet200 and ScanNet++datasets. Remarkably, our method surpasses the performance of Open3DIS, thecurrent state-of-the-art method in OV-3DIS, even in the absence of ground-truthobject class names.</description><author>Phuc D. A. Nguyen, Minh Luu, Anh Tran, Cuong Pham, Khoi Nguyen</author><pubDate>Wed, 21 Aug 2024 16:14:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11747v1</guid></item><item><title>Mixed Sparsity Training: Achieving 4$\times$ FLOP Reduction for Transformer Pretraining</title><link>http://arxiv.org/abs/2408.11746v1</link><description>Large language models (LLMs) have made significant strides in complex tasks,yet their widespread adoption is impeded by substantial computational demands.With hundreds of billion parameters, transformer-based LLMs necessitate monthsof pretraining across a high-end GPU cluster. However, this paper reveals acompelling finding: transformers exhibit considerable redundancy in pretrainingcomputations, which motivates our proposed solution, Mixed Sparsity Training(MST), an efficient pretraining method that can reduce about $75\%$ of FloatingPoint Operations (FLOPs) while maintaining performance. MST integrates dynamicsparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention(HSA) during pretraining, involving three distinct phases: warm-up,ultra-sparsification, and restoration. The warm-up phase transforms the densemodel into a sparse one, and the restoration phase reinstates connections.Throughout these phases, the model is trained with a dynamically evolvingsparse topology and an HSA mechanism to maintain performance and minimizetraining FLOPs concurrently. Our experiment on GPT-2 showcases a FLOP reductionof $4\times$ without compromising performance.</description><author>Pihe Hu, Shaolong Li, Longbo Huang</author><pubDate>Wed, 21 Aug 2024 16:13:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11746v1</guid></item><item><title>FocusLLM: Scaling LLM's Context by Parallel Decoding</title><link>http://arxiv.org/abs/2408.11745v1</link><description>Empowering LLMs with the ability to utilize useful information from a longcontext is crucial for many downstream applications. However, achieving longcontext lengths with the conventional transformer architecture requiressubstantial training and inference resources. In this paper, we presentFocusLLM, a framework designed to extend the context length of any decoder-onlyLLM, enabling the model to focus on relevant information from very longsequences. FocusLLM processes long text inputs by dividing them into chunksbased on the model's original context length to alleviate the issue ofattention distraction. Then, it appends the local context to each chunk as aprompt to extract essential information from each chunk based on a novelparallel decoding mechanism, and ultimately integrates the extractedinformation into the local context. FocusLLM stands out for great trainingefficiency and versatility: trained with an 8K input length with much lesstraining cost than previous methods, FocusLLM exhibits superior performanceacross downstream long-context tasks and maintains strong language modelingability when handling extensive long texts, even up to 400K tokens. Our code isavailable at https://github.com/leezythu/FocusLLM.</description><author>Zhenyu Li, Yike Zhang, Tengyu Pan, Yutao Sun, Zhichao Duan, Junjie Fang, Rong Han, Zixuan Wang, Jianyong Wang</author><pubDate>Wed, 21 Aug 2024 16:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11745v1</guid></item><item><title>JieHua Paintings Style Feature Extracting Model using Stable Diffusion with ControlNet</title><link>http://arxiv.org/abs/2408.11744v1</link><description>This study proposes a novel approach to extract stylistic features of Jiehua:the utilization of the Fine-tuned Stable Diffusion Model with ControlNet(FSDMC) to refine depiction techniques from artists' Jiehua. The training datafor FSDMC is based on the opensource Jiehua artist's work collected from theInternet, which were subsequently manually constructed in the format of(Original Image, Canny Edge Features, Text Prompt). By employing the optimalhyperparameters identified in this paper, it was observed FSDMC outperformsCycleGAN, another mainstream style transfer model. FSDMC achieves FID of 3.27on the dataset and also surpasses CycleGAN in terms of expert evaluation. Thisnot only demonstrates the model's high effectiveness in extracting Jiehua'sstyle features, but also preserves the original pre-trained semanticinformation. The findings of this study suggest that the application of FSDMCwith appropriate hyperparameters can enhance the efficacy of the StableDiffusion Model in the field of traditional art style migration tasks,particularly within the context of Jiehua.</description><author>Yujia Gu, Haofeng Li, Xinyu Fang, Zihan Peng, Yinan Peng</author><pubDate>Wed, 21 Aug 2024 16:11:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11744v1</guid></item><item><title>MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large Language Models</title><link>http://arxiv.org/abs/2408.11743v1</link><description>As inference on Large Language Models (LLMs) emerges as an important workloadin machine learning applications, weight quantization has become a standardtechnique for efficient GPU deployment. Quantization not only reduces modelsize, but has also been shown to yield substantial speedups for single-userinference, due to reduced memory movement, with low accuracy impact. Yet, itremains open whether speedups are achievable also in \emph{batched} settingswith multiple parallel clients, which are highly relevant for practicalserving. It is unclear whether GPU kernels can be designed to remainpractically memory-bound, while supporting the substantially increased computerequirements of batched workloads. This paper resolves this question positively by describing the design ofMixed-precision Auto-Regressive LINear kernels, called MARLIN. Concretely,given a model whose weights are compressed via quantization to, e.g., 4 bitsper element, MARLIN shows that batchsizes up to 16-32 can be supported withclose to maximum ($4\times$) quantization speedup, and larger batchsizes up to64-128 with gradually decreasing, but still significant, acceleration. MARLINaccomplishes this via a combination of techniques, such as asynchronous memoryaccess, complex task scheduling and pipelining, and bespoke quantizationsupport. Our experiments show that MARLIN's near-optimal performance onindividual LLM layers across different scenarios can also lead to end-to-endLLM inference speedups (of up to $2.8\times$) when integrated with the popularvLLM serving engine. Finally, MARLIN is extensible to further compressiontechniques, like NVIDIA 2:4 sparsity, leading to additional speedups.</description><author>Elias Frantar, Roberto L. Castro, Jiale Chen, Torsten Hoefler, Dan Alistarh</author><pubDate>Wed, 21 Aug 2024 16:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11743v1</guid></item><item><title>CluMo: Cluster-based Modality Fusion Prompt for Continual Learning in Visual Question Answering</title><link>http://arxiv.org/abs/2408.11742v1</link><description>Large vision-language models (VLMs) have shown significant performance boostin various application domains. However, adopting them to deal with severalsequentially encountered tasks has been challenging because finetuning a VLM ona task normally leads to reducing its generalization power and the capacity oflearning new tasks as well as causing catastrophic forgetting on previouslylearned tasks. Enabling using VLMs in multimodal continual learning (CL)settings can help to address such scenarios. To improve generalization capacityand prevent catastrophic forgetting, we propose a novel prompt-based CL methodfor VLMs, namely $\textbf{Clu}$ster-based $\textbf{Mo}$dality Fusion Prompt(\textbf{CluMo}). We design a novel \textbf{Key-Key-Prompt} pair, where eachprompt is associated with a visual prompt key and a textual prompt key. Weadopt a two-stage training strategy. During the first stage, the single-modalkeys are trained via $K$-means clustering algorithm to help select the bestsemantically matched prompt. During the second stage, the prompt keys arefrozen, the selected prompt is attached to the input for training the VLM inthe CL scenario. Experiments on two benchmarks demonstrate that our methodachieves SOTA performance.</description><author>Yuliang Cai, Mohammad Rostami</author><pubDate>Wed, 21 Aug 2024 16:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11742v1</guid></item><item><title>SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2407.16344v3</link><description>High frame-rate (HFR) videos of action recognition improve fine-grainedexpression while reducing the spatio-temporal relation and motion informationdensity. Thus, large amounts of video samples are continuously required fortraditional data-driven training. However, samples are not always sufficient inreal-world scenarios, promoting few-shot action recognition (FSAR) research. Weobserve that most recent FSAR works build spatio-temporal relation of videosamples via temporal alignment after spatial feature extraction, cutting apartspatial and temporal features within samples. They also capture motioninformation via narrow perspectives between adjacent frames without consideringdensity, leading to insufficient motion information capturing. Therefore, wepropose a novel plug-and-play architecture for FSAR called Spatio-tempOralfrAme tuPle enhancer (SOAP) in this paper. The model we designed with sucharchitecture refers to SOAP-Net. Temporal connections between different featurechannels and spatio-temporal relation of features are considered instead ofsimple feature extraction. Comprehensive motion information is also captured,using frame tuples with multiple frames containing more motion information thanadjacent frames. Combining frame tuples of diverse frame counts furtherprovides a broader perspective. SOAP-Net achieves new state-of-the-artperformance across well-known benchmarks such as SthSthV2, Kinetics, UCF101,and HMDB51. Extensive empirical evaluations underscore the competitiveness,pluggability, generalization, and robustness of SOAP. The code is released athttps://github.com/wenbohuang1002/SOAP.</description><author>Wenbo Huang, Jinghui Zhang, Xuwei Qian, Zhen Wu, Meng Wang, Lei Zhang</author><pubDate>Wed, 21 Aug 2024 16:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16344v3</guid></item><item><title>Spike-and-slab shrinkage priors for structurally sparse Bayesian neural networks</title><link>http://arxiv.org/abs/2308.09104v2</link><description>Network complexity and computational efficiency have become increasinglysignificant aspects of deep learning. Sparse deep learning addresses thesechallenges by recovering a sparse representation of the underlying targetfunction by reducing heavily over-parameterized deep neural networks.Specifically, deep neural architectures compressed via structured sparsity(e.g. node sparsity) provide low latency inference, higher data throughput, andreduced energy consumption. In this paper, we explore two well-establishedshrinkage techniques, Lasso and Horseshoe, for model compression in Bayesianneural networks. To this end, we propose structurally sparse Bayesian neuralnetworks which systematically prune excessive nodes with (i) Spike-and-SlabGroup Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors,and develop computationally tractable variational inference includingcontinuous relaxation of Bernoulli variables. We establish the contractionrates of the variational posterior of our proposed models as a function of thenetwork topology, layer-wise node cardinalities, and bounds on the networkweights. We empirically demonstrate the competitive performance of our modelscompared to the baseline models in prediction accuracy, model compression, andinference latency.</description><author>Sanket Jantre, Shrijita Bhattacharya, Tapabrata Maiti</author><pubDate>Wed, 21 Aug 2024 16:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09104v2</guid></item><item><title>Clinical Insights: A Comprehensive Review of Language Models in Medicine</title><link>http://arxiv.org/abs/2408.11735v1</link><description>This paper provides a detailed examination of the advancements andapplications of large language models in the healthcare sector, with aparticular emphasis on clinical applications. The study traces the evolution ofLLMs from their foundational technologies to the latest developments indomain-specific models and multimodal integration. It explores the technicalprogression from encoder-based models requiring fine-tuning to sophisticatedapproaches that integrate textual, visual, and auditory data, therebyfacilitating comprehensive AI solutions in healthcare. The paper discusses boththe opportunities these technologies present for enhancing clinical efficiencyand the challenges they pose in terms of ethics, data privacy, andimplementation. Additionally, it critically evaluates the deployment strategiesof LLMs, emphasizing the necessity of open-source models to ensure data privacyand adaptability within healthcare environments. Future research directions areproposed, focusing on empirical studies to evaluate the real-world efficacy ofLLMs in healthcare and the development of open datasets for further research.This review aims to provide a comprehensive resource for both newcomers andmultidisciplinary researchers interested in the intersection of AI andhealthcare.</description><author>Nikita Neveditsin, Pawan Lingras, Vijay Mago</author><pubDate>Wed, 21 Aug 2024 15:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11735v1</guid></item><item><title>Predicting Gradient is Better: Exploring Self-Supervised Learning for SAR ATR with a Joint-Embedding Predictive Architecture</title><link>http://arxiv.org/abs/2311.15153v5</link><description>The growing Synthetic Aperture Radar (SAR) data has the potential to build afoundation model through Self-Supervised Learning (SSL) methods, which canachieve various SAR Automatic Target Recognition (ATR) tasks with pre-trainingin large-scale unlabeled data and fine-tuning in small labeled samples. SSLaims to construct supervision signals directly from the data, which minimizesthe need for expensive expert annotation and maximizes the use of the expandingdata pool for a foundational model. This study investigates an effective SSLmethod for SAR ATR, which can pave the way for a foundation model in SAR ATR.The primary obstacles faced in SSL for SAR ATR are the small targets in remotesensing and speckle noise in SAR images, corresponding to the SSL approach andsignals. To overcome these challenges, we present a novel Joint-EmbeddingPredictive Architecture for SAR ATR (SAR-JEPA), which leverages local maskedpatches to predict the multi-scale SAR gradient representations of unseencontext. The key aspect of SAR-JEPA is integrating SAR domain features toensure high-quality self-supervised signals as target features. Besides, weemploy local masks and multi-scale features to accommodate the various smalltargets in remote sensing. By fine-tuning and evaluating our framework on threetarget recognition datasets (vehicle, ship, and aircraft) with four otherdatasets as pre-training, we demonstrate its outperformance over other SSLmethods and its effectiveness with increasing SAR data. This study showcasesthe potential of SSL for SAR target recognition across diverse targets, scenes,and sensors.Our codes and weights are available in\url{https://github.com/waterdisappear/SAR-JEPA.</description><author>Weijie Li, Yang Wei, Tianpeng Liu, Yuenan Hou, Yuxuan Li, Zhen Liu, Yongxiang Liu, Li Liu</author><pubDate>Wed, 21 Aug 2024 15:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15153v5</guid></item><item><title>Enhancing Cross-Modal Medical Image Segmentation through Compositionality</title><link>http://arxiv.org/abs/2408.11733v1</link><description>Cross-modal medical image segmentation presents a significant challenge, asdifferent imaging modalities produce images with varying resolutions,contrasts, and appearances of anatomical structures. We introducecompositionality as an inductive bias in a cross-modal segmentation network toimprove segmentation performance and interpretability while reducingcomplexity. The proposed network is an end-to-end cross-modal segmentationframework that enforces compositionality on the learned representations usinglearnable von Mises-Fisher kernels. These kernels facilitate content-styledisentanglement in the learned representations, resulting in compositionalcontent representations that are inherently interpretable and effectivelydisentangle different anatomical structures. The experimental resultsdemonstrate enhanced segmentation performance and reduced computational costson multiple medical datasets. Additionally, we demonstrate the interpretabilityof the learned compositional features. Code and checkpoints will be publiclyavailable at:https://github.com/Trustworthy-AI-UU-NKI/Cross-Modal-Segmentation.</description><author>Aniek Eijpe, Valentina Corbetta, Kalina Chupetlovska, Regina Beets-Tan, Wilson Silva</author><pubDate>Wed, 21 Aug 2024 15:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11733v1</guid></item><item><title>CMAB: A First National-Scale Multi-Attribute Building Dataset in China Derived from Open Source Data and GeoAI</title><link>http://arxiv.org/abs/2408.05891v2</link><description>Rapidly acquiring three-dimensional (3D) building data, including geometricattributes like rooftop, height and orientations, as well as indicativeattributes like function, quality, and age, is essential for accurate urbananalysis, simulations, and policy updates. Current building datasets sufferfrom incomplete coverage of building multi-attributes. This paper introduces ageospatial artificial intelligence (GeoAI) framework for large-scale buildingmodeling, presenting the first national-scale Multi-Attribute Building dataset(CMAB), covering 3,667 spatial cities, 29 million buildings, and 21.3 billionsquare meters of rooftops with an F1-Score of 89.93% in OCRNet-basedextraction, totaling 337.7 billion cubic meters of building stock. We trainedbootstrap aggregated XGBoost models with city administrative classifications,incorporating features such as morphology, location, and function. Usingmulti-source data, including billions of high-resolution Google Earth imagesand 60 million street view images (SVIs), we generated rooftop, height,function, age, and quality attributes for each building. Accuracy was validatedthrough model benchmarks, existing similar products, and manual SVI validation,mostly above 80%. Our dataset and results are crucial for global SDGs and urbanplanning.</description><author>Yecheng Zhang, Huimin Zhao, Ying Long</author><pubDate>Wed, 21 Aug 2024 15:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05891v2</guid></item><item><title>Deep Generative Models in Robotics: A Survey on Learning from Multimodal Demonstrations</title><link>http://arxiv.org/abs/2408.04380v3</link><description>Learning from Demonstrations, the field that proposes to learn robot behaviormodels from data, is gaining popularity with the emergence of deep generativemodels. Although the problem has been studied for years under names such asImitation Learning, Behavioral Cloning, or Inverse Reinforcement Learning,classical methods have relied on models that don't capture complex datadistributions well or don't scale well to large numbers of demonstrations. Inrecent years, the robot learning community has shown increasing interest inusing deep generative models to capture the complexity of large datasets. Inthis survey, we aim to provide a unified and comprehensive review of the lastyear's progress in the use of deep generative models in robotics. We presentthe different types of models that the community has explored, such asenergy-based models, diffusion models, action value maps, or generativeadversarial networks. We also present the different types of applications inwhich deep generative models have been used, from grasp generation totrajectory generation or cost learning. One of the most important elements ofgenerative models is the generalization out of distributions. In our survey, wereview the different decisions the community has made to improve thegeneralization of the learned models. Finally, we highlight the researchchallenges and propose a number of future directions for learning deepgenerative models in robotics.</description><author>Julen Urain, Ajay Mandlekar, Yilun Du, Mahi Shafiullah, Danfei Xu, Katerina Fragkiadaki, Georgia Chalvatzaki, Jan Peters</author><pubDate>Wed, 21 Aug 2024 15:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04380v3</guid></item><item><title>Efficient Detection of Toxic Prompts in Large Language Models</title><link>http://arxiv.org/abs/2408.11727v1</link><description>Large language models (LLMs) like ChatGPT and Gemini have significantlyadvanced natural language processing, enabling various applications such aschatbots and automated content generation. However, these models can beexploited by malicious individuals who craft toxic prompts to elicit harmful orunethical responses. These individuals often employ jailbreaking techniques tobypass safety mechanisms, highlighting the need for robust toxic promptdetection methods. Existing detection techniques, both blackbox and whitebox,face challenges related to the diversity of toxic prompts, scalability, andcomputational efficiency. In response, we propose ToxicDetector, a lightweightgreybox method designed to efficiently detect toxic prompts in LLMs.ToxicDetector leverages LLMs to create toxic concept prompts, uses embeddingvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)classifier for prompt classification. Our evaluation on various versions of theLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetectorachieves a high accuracy of 96.39\% and a low false positive rate of 2.00\%,outperforming state-of-the-art methods. Additionally, ToxicDetector'sprocessing time of 0.0780 seconds per prompt makes it highly suitable forreal-time applications. ToxicDetector achieves high accuracy, efficiency, andscalability, making it a practical method for toxic prompt detection in LLMs.</description><author>Yi Liu, Junzhe Yu, Huijia Sun, Ling Shi, Gelei Deng, Yuqi Chen, Yang Liu</author><pubDate>Wed, 21 Aug 2024 15:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11727v1</guid></item><item><title>HYVE: Hybrid Vertex Encoder for Neural Distance Fields</title><link>http://arxiv.org/abs/2310.06644v3</link><description>Neural shape representation generally refers to representing 3D geometryusing neural networks, e.g., computing a signed distance or occupancy value ata specific spatial position. In this paper we present a neural-networkarchitecture suitable for accurate encoding of 3D shapes in a single forwardpass. Our architecture is based on a multi-scale hybrid system incorporatinggraph-based and voxel-based components, as well as a continuouslydifferentiable decoder. The hybrid system includes a novel way of voxelizingpoint-based features in neural networks, which we show can be used incombination with oriented point-clouds to obtain smoother and more detailedreconstructions. Furthermore, our network is trained to solve the eikonalequation and only requires knowledge of the zero-level set for training andinference. This means that in contrast to most previous shape encoderarchitectures, our network is able to output valid signed distance fieldswithout explicit prior knowledge of non-zero distance values or shapeoccupancy. It also requires only a single forward-pass, instead of thelatent-code optimization used in auto-decoder methods. We further propose amodification to the loss function in case that surface normals are not welldefined, e.g., in the context of non-watertight surfaces and non-manifoldgeometry, resulting in an unsigned distance field. Overall, our system can helpto reduce the computational overhead of training and evaluating neural distancefields, as well as enabling the application to difficult geometry.</description><author>Stefan Rhys Jeske, Jonathan Klein, Dominik L. Michels, Jan Bender</author><pubDate>Wed, 21 Aug 2024 15:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06644v3</guid></item><item><title>Iterative Object Count Optimization for Text-to-image Diffusion Models</title><link>http://arxiv.org/abs/2408.11721v1</link><description>We address a persistent challenge in text-to-image models: accuratelygenerating a specified number of objects. Current models, which learn fromimage-text pairs, inherently struggle with counting, as training data cannotdepict every possible number of objects for any given object. To solve this, wepropose optimizing the generated image based on a counting loss derived from acounting model that aggregates an object\'s potential. Employing anout-of-the-box counting model is challenging for two reasons: first, the modelrequires a scaling hyperparameter for the potential aggregation that variesdepending on the viewpoint of the objects, and second, classifier guidancetechniques require modified models that operate on noisy intermediate diffusionsteps. To address these challenges, we propose an iterated online training modethat improves the accuracy of inferred images while altering the textconditioning embedding and dynamically adjusting hyperparameters. Our methodoffers three key advantages: (i) it can consider non-derivable countingtechniques based on detection models, (ii) it is a zero-shot plug-and-playsolution facilitating rapid changes to the counting techniques and imagegeneration methods, and (iii) the optimized counting token can be reused togenerate accurate images without additional optimization. We evaluate thegeneration of various objects and show significant improvements in accuracy.The project page is available at https://ozzafar.github.io/count_token.</description><author>Oz Zafar, Lior Wolf, Idan Schwartz</author><pubDate>Wed, 21 Aug 2024 15:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11721v1</guid></item><item><title>LBC: Language-Based-Classifier for Out-Of-Variable Generalization</title><link>http://arxiv.org/abs/2408.10923v2</link><description>Large Language Models (LLMs) have great success in natural languageprocessing tasks such as response generation. However, their use in tabulardata has been limited due to their inferior performance compared to traditionalmachine learning models (TMLs) such as XGBoost. We find that the pre-trainedknowledge of LLMs enables them to interpret new variables that appear in a testwithout additional training, a capability central to the concept ofOut-of-Variable (OOV). From the findings, we propose aLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits ofLLMs to outperform TMLs on OOV tasks. LBC employs three key methodologicalstrategies: 1) Categorical changes to adjust data to better fit the model'sunderstanding, 2) Advanced order and indicator to enhance data representationto the model, and 3) Using verbalizer to map logit scores to classes duringinference to generate model predictions. These strategies, combined with thepre-trained knowledge of LBC, emphasize the model's ability to effectivelyhandle OOV tasks. We empirically and theoretically validate the superiority ofLBC. LBC is the first study to apply an LLM-based model to OOV tasks. Thesource code is athttps://github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks.</description><author>Kangjun Noh, Baekryun Seong, Hoyoon Byun, Youngjun Choi, Sungjin Song, Kyungwoo Song</author><pubDate>Wed, 21 Aug 2024 15:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10923v2</guid></item><item><title>On Learnable Parameters of Optimal and Suboptimal Deep Learning Models</title><link>http://arxiv.org/abs/2408.11720v1</link><description>We scrutinize the structural and operational aspects of deep learning models,particularly focusing on the nuances of learnable parameters (weight)statistics, distribution, node interaction, and visualization. By establishingcorrelations between variance in weight patterns and overall networkperformance, we investigate the varying (optimal and suboptimal) performancesof various deep-learning models. Our empirical analysis extends across widelyrecognized datasets such as MNIST, Fashion-MNIST, and CIFAR-10, and variousdeep learning models such as deep neural networks (DNNs), convolutional neuralnetworks (CNNs), and vision transformer (ViT), enabling us to pinpointcharacteristics of learnable parameters that correlate with successfulnetworks. Through extensive experiments on the diverse architectures of deeplearning models, we shed light on the critical factors that influence thefunctionality and efficiency of DNNs. Our findings reveal that successfulnetworks, irrespective of datasets or models, are invariably similar to othersuccessful networks in their converged weights statistics and distribution,while poor-performing networks vary in their weights. In addition, our researchshows that the learnable parameters of widely varied deep learning models suchas DNN, CNN, and ViT exhibit similar learning characteristics.</description><author>Ziwei Zheng, Huizhi Liang, Vaclav Snasel, Vito Latora, Panos Pardalos, Giuseppe Nicosia, Varun Ojha</author><pubDate>Wed, 21 Aug 2024 15:50:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11720v1</guid></item><item><title>Analysis of Systems' Performance in Natural Language Processing Competitions</title><link>http://arxiv.org/abs/2403.04693v2</link><description>Collaborative competitions have gained popularity in the scientific andtechnological fields. These competitions involve defining tasks, selectingevaluation scores, and devising result verification methods. In the standardscenario, participants receive a training set and are expected to provide asolution for a held-out dataset kept by organizers. An essential challenge fororganizers arises when comparing algorithms' performance, assessing multipleparticipants, and ranking them. Statistical tools are often used for thispurpose; however, traditional statistical methods often fail to capturedecisive differences between systems' performance. This manuscript describes anevaluation methodology for statistically analyzing competition results andcompetition. The methodology is designed to be universally applicable; however,it is illustrated using eight natural language competitions as case studiesinvolving classification and regression problems. The proposed methodologyoffers several advantages, including off-the-shell comparisons with correctionmechanisms and the inclusion of confidence intervals. Furthermore, we introducemetrics that allow organizers to assess the difficulty of competitions. Ouranalysis shows the potential usefulness of our methodology for effectivelyevaluating competition results.</description><author>Sergio Nava-Muoz, Mario Graff, Hugo Jair Escalante</author><pubDate>Wed, 21 Aug 2024 15:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04693v2</guid></item><item><title>Scalable and non-iterative graphical model estimation</title><link>http://arxiv.org/abs/2408.11718v1</link><description>Graphical models have found widespread applications in many areas of modernstatistics and machine learning. Iterative Proportional Fitting (IPF) and itsvariants have become the default method for undirected graphical modelestimation, and are thus ubiquitous in the field. As the IPF is an iterativeapproach, it is not always readily scalable to modern high-dimensional dataregimes. In this paper we propose a novel and fast non-iterative method forpositive definite graphical model estimation in high dimensions, one thatdirectly addresses the shortcomings of IPF and its variants. In addition, theproposed method has a number of other attractive properties. First, we showformally that as the dimension p grows, the proportion of graphs for which theproposed method will outperform the state-of-the-art in terms of computationalcomplexity and performance tends to 1, affirming its efficacy in modernsettings. Second, the proposed approach can be readily combined with scalablenon-iterative thresholding-based methods for high-dimensional sparsityselection. Third, the proposed method has high-dimensional statisticalguarantees. Moreover, our numerical experiments also show that the proposedmethod achieves scalability without compromising on statistical precision.Fourth, unlike the IPF, which depends on the Gaussian likelihood, the proposedmethod is much more robust.</description><author>Kshitij Khare, Syed Rahman, Bala Rajaratnam, Jiayuan Zhou</author><pubDate>Wed, 21 Aug 2024 15:46:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11718v1</guid></item><item><title>ControlCol: Controllability in Automatic Speaker Video Colorization</title><link>http://arxiv.org/abs/2408.11711v1</link><description>Adding color to black-and-white speaker videos automatically is a highlydesirable technique. It is an artistic process that requires interactivity withhumans for the best results. Many existing automatic video colorization systemsprovide little opportunity for the user to guide the colorization process. Inthis work, we introduce a novel automatic speaker video colorization systemwhich provides controllability to the user while also maintaining highcolorization quality relative to state-of-the-art techniques. We name thissystem ControlCol. ControlCol performs 3.5% better than the previousstate-of-the-art DeOldify on the Grid and Lombard Grid datasets when PSNR,SSIM, FID and FVD are used as metrics. This result is also supported by ourhuman evaluation, where in a head-to-head comparison, ControlCol is preferred90% of the time to DeOldify. Example videos can be seen in the supplementarymaterial.</description><author>Rory Ward, John G. Breslin, Peter Corcoran</author><pubDate>Wed, 21 Aug 2024 15:35:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11711v1</guid></item><item><title>Leveraging Large Language Models for Enhancing the Understandability of Generated Unit Tests</title><link>http://arxiv.org/abs/2408.11710v1</link><description>Automated unit test generators, particularly search-based software testingtools like EvoSuite, are capable of generating tests with high coverage.Although these generators alleviate the burden of writing unit tests, theyoften pose challenges for software engineers in terms of understanding thegenerated tests. To address this, we introduce UTGen, which combinessearch-based software testing and large language models to enhance theunderstandability of automatically generated test cases. We achieve thisenhancement through contextualizing test data, improving identifier naming, andadding descriptive comments. Through a controlled experiment with 32participants from both academia and industry, we investigate how theunderstandability of unit tests affects a software engineer's ability toperform bug-fixing tasks. We selected bug-fixing to simulate a real-worldscenario that emphasizes the importance of understandable test cases. Weobserve that participants working on assignments with UTGen test cases fix upto 33% more bugs and use up to 20% less time when compared to baseline testcases. From the post-test questionnaire, we gathered that participants foundthat enhanced test names, test data, and variable names improved theirbug-fixing process.</description><author>Amirhossein Deljouyi, Roham Koohestani, Maliheh Izadi, Andy Zaidman</author><pubDate>Wed, 21 Aug 2024 15:35:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11710v1</guid></item><item><title>Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for Few-Shot Class-Incremental Learning</title><link>http://arxiv.org/abs/2407.06136v2</link><description>Few-shot class-incremental learning (FSCIL) confronts the challenge ofintegrating new classes into a model with minimal training samples whilepreserving the knowledge of previously learned classes. Traditional methodswidely adopt static adaptation relying on a fixed parameter space to learn fromdata that arrive sequentially, prone to overfitting to the current session.Existing dynamic strategies require the expansion of the parameter spacecontinually, leading to increased complexity. In this study, we explore thepotential of Selective State Space Models (SSMs) for FSCIL, leveraging itsdynamic weights and strong ability in sequence modeling to address thesechallenges. Concretely, we propose a dual selective SSM projector thatdynamically adjusts the projection parameters based on the intermediatefeatures for dynamic adaptation. The dual design enables the model to maintainthe robust features of base classes, while adaptively learning distinctivefeature shifts for novel classes. Additionally, we develop a class-sensitiveselective scan mechanism to guide dynamic adaptation. It minimizes thedisruption to base-class representations caused by training on novel data, andmeanwhile, forces the selective scan to perform in distinct patterns betweenbase and novel classes. Experiments on miniImageNet, CUB-200, and CIFAR-100demonstrate that our framework outperforms the existing state-of-the-artmethods. The code is available at\url{https://github.com/xiaojieli0903/Mamba-FSCIL}.</description><author>Xiaojie Li, Yibo Yang, Jianlong Wu, Bernard Ghanem, Liqiang Nie, Min Zhang</author><pubDate>Wed, 21 Aug 2024 15:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06136v2</guid></item><item><title>FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive Prompt Weighting</title><link>http://arxiv.org/abs/2408.11706v1</link><description>Text-to-image (T2I) diffusion models have demonstrated impressivecapabilities in generating high-quality images given a text prompt. However,ensuring the prompt-image alignment remains a considerable challenge, i.e.,generating images that faithfully align with the prompt's semantics. Recentworks attempt to improve the faithfulness by optimizing the latent code, whichpotentially could cause the latent code to go out-of-distribution and thusproduce unrealistic images. In this paper, we propose FRAP, a simple, yeteffective approach based on adaptively adjusting the per-token prompt weightsto improve prompt-image alignment and authenticity of the generated images. Wedesign an online algorithm to adaptively update each token's weightcoefficient, which is achieved by minimizing a unified objective function thatencourages object presence and the binding of object-modifier pairs. Throughextensive evaluations, we show FRAP generates images with significantly higherprompt-image alignment to prompts from complex datasets, while having a loweraverage latency compared to recent latent code optimization methods, e.g., 4seconds faster than D&amp;B on the COCO-Subject dataset. Furthermore, throughvisual comparisons and evaluation on the CLIP-IQA-Real metric, we show thatFRAP not only improves prompt-image alignment but also generates more authenticimages with realistic appearances. We also explore combining FRAP with promptrewriting LLM to recover their degraded prompt-image alignment, where weobserve improvements in both prompt-image alignment and image quality.</description><author>Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohan Sai Singamsetti, Fengyu Sun, Wei Lu, Di Niu</author><pubDate>Wed, 21 Aug 2024 15:30:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11706v1</guid></item><item><title>FedGS: Federated Gradient Scaling for Heterogeneous Medical Image Segmentation</title><link>http://arxiv.org/abs/2408.11701v1</link><description>Federated Learning (FL) in Deep Learning (DL)-automated medical imagesegmentation helps preserving privacy by enabling collaborative model trainingwithout sharing patient data. However, FL faces challenges with dataheterogeneity among institutions, leading to suboptimal global models.Integrating Disentangled Representation Learning (DRL) in FL can enhancerobustness by separating data into distinct representations. Existing DRLmethods assume heterogeneity lies solely in style features, overlookingcontent-based variability like lesion size and shape. We propose FedGS, a novelFL aggregation method, to improve segmentation performance on small,under-represented targets while maintaining overall efficacy. FedGSdemonstrates superior performance over FedAvg, particularly for small lesions,across PolypGen and LiTS datasets. The code and pre-trained checkpoints areavailable at the following link:https://github.com/Trustworthy-AI-UU-NKI/Federated-Learning-Disentanglement</description><author>Philip Schutte, Valentina Corbetta, Regina Beets-Tan, Wilson Silva</author><pubDate>Wed, 21 Aug 2024 15:26:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11701v1</guid></item><item><title>Vessel-Promoted OCT to OCTA Image Translation by Heuristic Contextual Constraints</title><link>http://arxiv.org/abs/2303.06807v2</link><description>Optical Coherence Tomography Angiography (OCTA) is a crucial tool in theclinical screening of retinal diseases, allowing for accurate 3D imaging ofblood vessels through non-invasive scanning. However, the hardware-basedapproach for acquiring OCTA images presents challenges due to the need forspecialized sensors and expensive devices. In this paper, we introduce a novelmethod called TransPro, which can translate the readily available 3D OpticalCoherence Tomography (OCT) images into 3D OCTA images without requiring anyadditional hardware modifications. Our TransPro method is primarily driven bytwo novel ideas that have been overlooked by prior work. The first idea isderived from a critical observation that the OCTA projection map is generatedby averaging pixel values from its corresponding B-scans along the Z-axis.Hence, we introduce a hybrid architecture incorporating a 3D adversarialgenerative network and a novel Heuristic Contextual Guidance (HCG) module,which effectively maintains the consistency of the generated OCTA imagesbetween 3D volumes and projection maps. The second idea is to improve thevessel quality in the translated OCTA projection maps. As a result, we proposea novel Vessel Promoted Guidance (VPG) module to enhance the attention ofnetwork on retinal vessels. Experimental results on two datasets demonstratethat our TransPro outperforms state-of-the-art approaches, with relativeimprovements around 11.4% in MAE, 2.7% in PSNR, 2% in SSIM, 40% in VDE, and9.1% in VDC compared to the baseline method. The code is available at:https://github.com/ustlsh/TransPro.</description><author>Shuhan Li, Dong Zhang, Xiaomeng Li, Chubin Ou, Lin An, Yanwu Xu, Kwang-Ting Cheng</author><pubDate>Wed, 21 Aug 2024 15:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06807v2</guid></item><item><title>Supervised Representation Learning towards Generalizable Assembly State Recognition</title><link>http://arxiv.org/abs/2408.11700v1</link><description>Assembly state recognition facilitates the execution of assembly procedures,offering feedback to enhance efficiency and minimize errors. However,recognizing assembly states poses challenges in scalability, since parts arefrequently updated, and the robustness to execution errors remainsunderexplored. To address these challenges, this paper proposes an approachbased on representation learning and the novel intermediate-state informed lossfunction modification (ISIL). ISIL leverages unlabeled transitions betweenstates and demonstrates significant improvements in clustering andclassification performance for all tested architectures and losses. Despitebeing trained exclusively on images without execution errors, thorough analysison error states demonstrates that our approach accurately distinguishes betweencorrect states and states with various types of execution errors. Theintegration of the proposed algorithm can offer meaningful assistance toworkers and mitigate unexpected losses due to procedural mishaps in industrialsettings. The code is available at: https://timschoonbeek.github.io/state_rec</description><author>Tim J. Schoonbeek, Goutham Balachandran, Hans Onvlee, Tim Houben, Shao-Hsuan Hung, Jacek Kustra, Peter H. N. de With, Fons van der Sommen</author><pubDate>Wed, 21 Aug 2024 15:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11700v1</guid></item><item><title>Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data</title><link>http://arxiv.org/abs/2404.03862v2</link><description>To trust the fluent generations of large language models (LLMs), humans mustbe able to verify their correctness against trusted, external sources. Recentefforts, such as providing citations via retrieved documents or post-hocprovenance, enhance verifiability but still provide no guarantees on theircorrectness. To address these limitations, we tackle the verifiability goalwith a different philosophy: trivializing the verification process bydeveloping models that quote verbatim statements from trusted sources inpre-training data. We propose Quote-Tuning, and demonstrate it is feasible toalign LLMs to provide quoted statements from data memorized duringpre-training. The core of Quote-Tuning is a fast membership inference function(Marone and Van Durme, 2023) that efficiently verifies text against a trustedcorpus. We leverage this tool to design a reward function to quantify quotes inmodel responses, which is then used to create a dataset for preferencelearning. Experimental results show that Quote-Tuning significantly increasesverbatim quotes from high-quality pre-training documents by 55% to 130%relative to un-tuned models while maintaining response quality. Quote-Tuningalso generalizes quoting to out-of-domain data, is applicable in differenttasks, and provides additional benefits to truthfulness. Our method not onlyserves as a hassle-free method to increase quoting but also opens up avenuesfor improving LLM trustworthiness through better verifiability.</description><author>Jingyu Zhang, Marc Marone, Tianjian Li, Benjamin Van Durme, Daniel Khashabi</author><pubDate>Wed, 21 Aug 2024 15:23:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03862v2</guid></item><item><title>Improving global awareness of linkset predictions using Cross-Attentive Modulation tokens</title><link>http://arxiv.org/abs/2405.19375v3</link><description>Most of multiple link prediction or graph generation techniques rely on theattention mechanism or on Graph Neural Networks (GNNs), which consist inleveraging node-level information exchanges in order to form proper linkpredictions. Such node-level interactions do not process nodes as an orderedsequence, which would imply some kind of natural ordering of the nodes: theyare said to be permutation invariant mechanisms. They are well suited for graphproblems, but struggle at providing a global orchestration of the predictedlinks, which can result in a loss of performance. Some typical issues can bethe difficulty to ensure high-level properties such as global connectedness,fixed diameter or to avoid information bottleneck effects such as oversmoothingand oversquashing, which respectively consist in abundant smoothing in denseareas leading to a loss of information and a tendency to exclude isolated nodesfrom the message passing scheme, and often result in irrelevant, unbalancedlink predictions. To tackle this problem, we hereby present Cross-AttentiveModulation (CAM) tokens, which introduce cross-attentive units used tocondition node and edge-level modulations in order to enable context-awarecomputations that improve the global consistency of the prediction links. Wewill implement it on a few permutation invariant architectures, and showcasebenchmarks that prove the merits of our work.</description><author>Flix Marcoccia, Cdric Adjih, Paul Mhlethaler</author><pubDate>Wed, 21 Aug 2024 15:21:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19375v3</guid></item><item><title>Robust 3D Gaussian Splatting for Novel View Synthesis in Presence of Distractors</title><link>http://arxiv.org/abs/2408.11697v1</link><description>3D Gaussian Splatting has shown impressive novel view synthesis results;nonetheless, it is vulnerable to dynamic objects polluting the input data of anotherwise static scene, so called distractors. Distractors have severe impacton the rendering quality as they get represented as view-dependent effects orresult in floating artifacts. Our goal is to identify and ignore suchdistractors during the 3D Gaussian optimization to obtain a cleanreconstruction. To this end, we take a self-supervised approach that looks atthe image residuals during the optimization to determine areas that have likelybeen falsified by a distractor. In addition, we leverage a pretrainedsegmentation network to provide object awareness, enabling more accurateexclusion of distractors. This way, we obtain segmentation masks of distractorsto effectively ignore them in the loss formulation. We demonstrate that ourapproach is robust to various distractors and strongly improves renderingquality on distractor-polluted scenes, improving PSNR by 1.86dB compared to 3DGaussian Splatting.</description><author>Paul Ungermann, Armin Ettenhofer, Matthias Niener, Barbara Roessle</author><pubDate>Wed, 21 Aug 2024 15:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11697v1</guid></item><item><title>Quantum Inception Score</title><link>http://arxiv.org/abs/2311.12163v4</link><description>Motivated by the great success of classical generative models in machinelearning, enthusiastic exploration of their quantum version has recentlystarted. To depart on this journey, it is important to develop a relevantmetric to evaluate the quality of quantum generative models; in the classicalcase, one such example is the (classical) inception score (cIS). In this paper,as a natural extension of cIS, we propose the quantum inception score (qIS) forquantum generators. Importantly, qIS relates the quality to the Holevoinformation of the quantum channel that classifies a given dataset. In thiscontext, we show several properties of qIS. First, qIS is greater than or equalto the corresponding cIS, which is defined through projection measurements onthe system output. Second, the difference between qIS and cIS arises from thepresence of quantum coherence, as characterized by the resource theory ofasymmetry. Third, when a set of entangled generators is prepared, there existsa classifying process leading to the further enhancement of qIS. Fourth, weharness the quantum fluctuation theorem to characterize the physical limitationof qIS. Finally, we apply qIS to assess the quality of the one-dimensional spinchain model as a quantum generative model, with the quantum convolutionalneural network as a quantum classifier, for the phase classification problem inthe quantum many-body physics.</description><author>Akira Sone, Akira Tanji, Naoki Yamamoto</author><pubDate>Wed, 21 Aug 2024 15:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12163v4</guid></item><item><title>Carbon Connect: An Ecosystem for Sustainable Computing</title><link>http://arxiv.org/abs/2405.13858v2</link><description>Computing is at a moment of profound opportunity. Emerging applications --such as capable artificial intelligence, immersive virtual realities, andpervasive sensor systems -- drive unprecedented demand for computer. Despiterecent advances toward net zero carbon emissions, the computing industry'sgross energy usage continues to rise at an alarming rate, outpacing the growthof new energy installations and renewable energy deployments. A shift towardssustainability is needed to spark a transformation in how computer systems aremanufactured, allocated, and consumed. Carbon Connect envisions coordinatedresearch thrusts that produce design and management strategies for sustainable,next-generation computer systems. These strategies must flatten and thenreverse growth trajectories for computing power and carbon for society's mostrapidly growing applications such as artificial intelligence and virtualspaces. We will require accurate models for carbon accounting in computingtechnology. For embodied carbon, we must re-think conventional designstrategies -- over-provisioned monolithic servers, frequent hardware refreshcycles, custom silicon -- and adopt life-cycle design strategies that moreeffectively reduce, reuse and recycle hardware at scale. For operationalcarbon, we must not only embrace renewable energy but also design systems touse that energy more efficiently. Finally, new hardware design and managementstrategies must be cognizant of economic policy and regulatory landscape,aligning private initiatives with societal goals. Many of these broader goalswill require computer scientists to develop deep, enduring collaborations withresearchers in economics, law, and industrial ecology to spark change inbroader practice.</description><author>Benjamin C. Lee, David Brooks, Arthur van Benthem, Udit Gupta, Gage Hills, Vincent Liu, Benjamin Pierce, Christopher Stewart, Emma Strubell, Gu-Yeon Wei, Adam Wierman, Yuan Yao, Minlan Yu</author><pubDate>Wed, 21 Aug 2024 15:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13858v2</guid></item><item><title>What Makes and Breaks Safety Fine-tuning? A Mechanistic Study</title><link>http://arxiv.org/abs/2407.10264v3</link><description>Safety fine-tuning helps align Large Language Models (LLMs) with humanpreferences for their safe deployment. To better understand the underlyingfactors that make models safe via safety fine-tuning, we design a syntheticdata generation framework that captures salient aspects of an unsafe input bymodeling the interaction between the task the model is asked to perform (e.g.,"design") versus the specific concepts the task is asked to be performed upon(e.g., a "cycle" vs. a "bomb"). Using this, we investigate three well-knownsafety fine-tuning methods -- supervised safety fine-tuning, direct preferenceoptimization, and unlearning -- and provide significant evidence demonstratingthat these methods minimally transform MLP weights to specifically align unsafeinputs into its weights' null space. This yields a clustering of inputs basedon whether the model deems them safe or not. Correspondingly, when anadversarial input (e.g., a jailbreak) is provided, its activations are closerto safer samples, leading to the model processing such an input as if it weresafe. We validate our findings, wherever possible, on real-world models --specifically, Llama-2 7B and Llama-3 8B.</description><author>Samyak Jain, Ekdeep Singh Lubana, Kemal Oksuz, Tom Joy, Philip H. S. Torr, Amartya Sanyal, Puneet K. Dokania</author><pubDate>Wed, 21 Aug 2024 15:12:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10264v3</guid></item><item><title>Physics-informed Discovery of State Variables in Second-Order and Hamiltonian Systems</title><link>http://arxiv.org/abs/2408.11691v1</link><description>The modeling of dynamical systems is a pervasive concern for not onlydescribing but also predicting and controlling natural phenomena and engineeredsystems. Current data-driven approaches often assume prior knowledge of therelevant state variables or result in overparameterized state spaces. BoyuanChen and his co-authors proposed a neural network model that estimates thedegrees of freedom and attempts to discover the state variables of a dynamicalsystem. Despite its innovative approach, this baseline model lacks a connectionto the physical principles governing the systems it analyzes, leading tounreliable state variables. This research proposes a method that leverages the physical characteristicsof second-order Hamiltonian systems to constrain the baseline model. Theproposed model outperforms the baseline model in identifying a minimal set ofnon-redundant and interpretable state variables.</description><author>Flix Chavelli, Zi-Yu Khoo, Dawen Wu, Jonathan Sze Choong Low, Stphane Bressan</author><pubDate>Wed, 21 Aug 2024 15:10:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11691v1</guid></item><item><title>Interpretable Long-term Action Quality Assessment</title><link>http://arxiv.org/abs/2408.11687v1</link><description>Long-term Action Quality Assessment (AQA) evaluates the execution ofactivities in videos. However, the length presents challenges in fine-grainedinterpretability, with current AQA methods typically producing a single scoreby averaging clip features, lacking detailed semantic meanings of individualclips. Long-term videos pose additional difficulty due to the complexity anddiversity of actions, exacerbating interpretability challenges. Whilequery-based transformer networks offer promising long-term modelingcapabilities, their interpretability in AQA remains unsatisfactory due to aphenomenon we term Temporal Skipping, where the model skips self-attentionlayers to prevent output degradation. To address this, we propose an attentionloss function and a query initialization method to enhance performance andinterpretability. Additionally, we introduce a weight-score regression moduledesigned to approximate the scoring patterns observed in human judgments andreplace conventional single-score regression, improving the rationality ofinterpretability. Our approach achieves state-of-the-art results on threereal-world, long-term AQA benchmarks. Our code is available at:https://github.com/dx199771/Interpretability-AQA</description><author>Xu Dong, Xinran Liu, Wanqing Li, Anthony Adeyemi-Ejeye, Andrew Gilbert</author><pubDate>Wed, 21 Aug 2024 15:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11687v1</guid></item><item><title>Plug-in estimation of Schrdinger bridges</title><link>http://arxiv.org/abs/2408.11686v1</link><description>We propose a procedure for estimating the Schr\"odinger bridge between twoprobability distributions. Unlike existing approaches, our method does notrequire iteratively simulating forward and backward diffusions or trainingneural networks to fit unknown drifts. Instead, we show that the potentialsobtained from solving the static entropic optimal transport problem between thesource and target samples can be modified to yield a natural plug-in estimatorof the time-dependent drift that defines the bridge between two measures. Underminimal assumptions, we show that our proposal, which we call the\emph{Sinkhorn bridge}, provably estimates the Schr\"odinger bridge with a rateof convergence that depends on the intrinsic dimensionality of the targetmeasure. Our approach combines results from the areas of sampling, andtheoretical and statistical entropic optimal transport.</description><author>Aram-Alexandre Pooladian, Jonathan Niles-Weed</author><pubDate>Wed, 21 Aug 2024 15:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11686v1</guid></item><item><title>LiFCal: Online Light Field Camera Calibration via Bundle Adjustment</title><link>http://arxiv.org/abs/2408.11682v1</link><description>We propose LiFCal, a novel geometric online calibration pipeline forMLA-based light field cameras. LiFCal accurately determines model parametersfrom a moving camera sequence without precise calibration targets, integratingarbitrary metric scaling constraints. It optimizes intrinsic parameters of thelight field camera model, the 3D coordinates of a sparse set of scene pointsand camera poses in a single bundle adjustment defined directly on micro imagepoints. We show that LiFCal can reliably and repeatably calibrate a focused plenopticcamera using different input sequences, providing intrinsic camera parametersextremely close to state-of-the-art methods, while offering two mainadvantages: it can be applied in a target-free scene, and it is implementedonline in a complete and continuous pipeline. Furthermore, we demonstrate the quality of the obtained camera parameters indownstream tasks like depth estimation and SLAM. Webpage: https://lifcal.github.io/</description><author>Aymeric Fleith, Doaa Ahmed, Daniel Cremers, Niclas Zeller</author><pubDate>Wed, 21 Aug 2024 15:04:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11682v1</guid></item><item><title>S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models</title><link>http://arxiv.org/abs/2310.06715v2</link><description>Scoring sleep stages in polysomnography recordings is a time-consuming taskplagued by significant inter-rater variability. Therefore, it stands to benefitfrom the application of machine learning algorithms. While many algorithms havebeen proposed for this purpose, certain critical architectural decisions havenot received systematic exploration. In this study, we meticulously investigatethese design choices within the broad category of encoder-predictorarchitectures. We identify robust architectures applicable to both time seriesand spectrogram input representations. These architectures incorporatestructured state space models as integral components and achieve statisticallysignificant performance improvements compared to state-of-the-art approaches onthe extensive Sleep Heart Health Study dataset. We anticipate that thearchitectural insights gained from this study along with the refinedmethodology for architecture search demonstrated herein will not only provevaluable for future research in sleep staging but also hold relevance for othertime series annotation tasks.</description><author>Tiezhi Wang, Nils Strodthoff</author><pubDate>Wed, 21 Aug 2024 15:03:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06715v2</guid></item><item><title>Surgical Workflow Recognition and Blocking Effectiveness Detection in Laparoscopic Liver Resections with Pringle Maneuver</title><link>http://arxiv.org/abs/2408.10538v2</link><description>Pringle maneuver (PM) in laparoscopic liver resection aims to reduce bloodloss and provide a clear surgical view by intermittently blocking blood inflowof the liver, whereas prolonged PM may cause ischemic injury. Tocomprehensively monitor this surgical procedure and provide timely warnings ofineffective and prolonged blocking, we suggest two complementary AI-assistedsurgical monitoring tasks: workflow recognition and blocking effectivenessdetection in liver resections. The former presents challenges in real-timecapturing of short-term PM, while the latter involves the intraoperativediscrimination of long-term liver ischemia states. To address these challenges,we meticulously collect a novel dataset, called PmLR50, consisting of 25,037video frames covering various surgical phases from 50 laparoscopic liverresection procedures. Additionally, we develop an online baseline for PmLR50,termed PmNet. This model embraces Masked Temporal Encoding (MTE) and CompressedSequence Modeling (CSM) for efficient short-term and long-term temporalinformation modeling, and embeds Contrastive Prototype Separation (CPS) toenhance action discrimination between similar intraoperative operations.Experimental results demonstrate that PmNet outperforms existingstate-of-the-art surgical workflow recognition methods on the PmLR50 benchmark.Our research offers potential clinical applications for the laparoscopic liversurgery community. Source code and data will be publicly available.</description><author>Diandian Guo, Weixin Si, Zhixi Li, Jialun Pei, Pheng-Ann Heng</author><pubDate>Wed, 21 Aug 2024 15:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10538v2</guid></item><item><title>First line of defense: A robust first layer mitigates adversarial attacks</title><link>http://arxiv.org/abs/2408.11680v1</link><description>Adversarial training (AT) incurs significant computational overhead, leadingto growing interest in designing inherently robust architectures. Wedemonstrate that a carefully designed first layer of the neural network canserve as an implicit adversarial noise filter (ANF). This filter is createdusing a combination of large kernel size, increased convolution filters, and amaxpool operation. We show that integrating this filter as the first layer inarchitectures such as ResNet, VGG, and EfficientNet results in adversariallyrobust networks. Our approach achieves higher adversarial accuracies thanexisting natively robust architectures without AT and is competitive withadversarial-trained architectures across a wide range of datasets. Supportingour findings, we show that (a) the decision regions for our method have bettermargins, (b) the visualized loss surfaces are smoother, (c) the modified peaksignal-to-noise ratio (mPSNR) values at the output of the ANF are higher, (d)high-frequency components are more attenuated, and (e) architecturesincorporating ANF exhibit better denoising in Gaussian noise compared tobaseline architectures. Code for all our experiments are available at\url{https://github.com/janani-suresh-97/first-line-defence.git}.</description><author>Janani Suresh, Nancy Nayak, Sheetal Kalyani</author><pubDate>Wed, 21 Aug 2024 15:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11680v1</guid></item><item><title>Exploring Robustness of Visual State Space model against Backdoor Attacks</title><link>http://arxiv.org/abs/2408.11679v1</link><description>Visual State Space Model (VSS) has demonstrated remarkable performance invarious computer vision tasks. However, in the process of development, backdoorattacks have brought severe challenges to security. Such attacks cause aninfected model to predict target labels when a specific trigger is activated,while the model behaves normally on benign samples. In this paper, we conductsystematic experiments to comprehend on robustness of VSS through the lens ofbackdoor attacks, specifically how the state space model (SSM) mechanismaffects robustness. We first investigate the vulnerability of VSS to differentbackdoor triggers and reveal that the SSM mechanism, which captures contextualinformation within patches, makes the VSS model more susceptible to backdoortriggers compared to models without SSM. Furthermore, we analyze thesensitivity of the VSS model to patch processing techniques and discover thatthese triggers are effectively disrupted. Based on these observations, weconsider an effective backdoor for the VSS model that recurs in each patch toresist patch perturbations. Extensive experiments across three datasets andvarious backdoor attacks reveal that the VSS model performs comparably toTransformers (ViTs) but is less robust than the Gated CNNs, which comprise onlystacked Gated CNN blocks without SSM.</description><author>Cheng-Yi Lee, Cheng-Chang Tsai, Chia-Mu Yu, Chun-Shien Lu</author><pubDate>Wed, 21 Aug 2024 14:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11679v1</guid></item><item><title>Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot</title><link>http://arxiv.org/abs/2401.14176v2</link><description>As one of the most popular dynamic languages, Python experiences a decreasein readability and maintainability when code smells are present. Recentadvancements in Large Language Models have sparked growing interest inAI-enabled tools for both code generation and refactoring. GitHub Copilot isone such tool that has gained widespread usage. Copilot Chat, released inSeptember 2023, functions as an interactive tool aimed at facilitating naturallanguage-powered coding. However, limited attention has been given tounderstanding code smells in Copilot-generated Python code and Copilot Chat'sability to fix the code smells. To this end, we built a dataset comprising 102code smells in Copilot-generated Python code. Our aim is to first explore theoccurrence of code smells in Copilot-generated Python code and then evaluatethe effectiveness of Copilot Chat in fixing these code smells employingdifferent prompts. The results show that 8 out of 10 types of code smells canbe detected in Copilot-generated Python code, among which Multiply-NestedContainer is the most common one. For these code smells, Copilot Chat achievesa highest fixing rate of 87.1%, showing promise in fixing Python code smellsgenerated by Copilot itself. In addition, the effectiveness of Copilot Chat infixing these smells can be improved by providing more detailed prompts.</description><author>Beiqi Zhang, Peng Liang, Qiong Feng, Yujia Fu, Zengyang Li</author><pubDate>Wed, 21 Aug 2024 14:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14176v2</guid></item><item><title>MotionBooth: Motion-Aware Customized Text-to-Video Generation</title><link>http://arxiv.org/abs/2406.17758v2</link><description>In this work, we present MotionBooth, an innovative framework designed foranimating customized subjects with precise control over both object and cameramovements. By leveraging a few images of a specific object, we efficientlyfine-tune a text-to-video model to capture the object's shape and attributesaccurately. Our approach presents subject region loss and video preservationloss to enhance the subject's learning performance, along with a subject tokencross-attention loss to integrate the customized subject with motion controlsignals. Additionally, we propose training-free techniques for managing subjectand camera motions during inference. In particular, we utilize cross-attentionmap manipulation to govern subject motion and introduce a novel latent shiftmodule for camera movement control as well. MotionBooth excels in preservingthe appearance of subjects while simultaneously controlling the motions ingenerated videos. Extensive quantitative and qualitative evaluationsdemonstrate the superiority and effectiveness of our method. Our project pageis at https://jianzongwu.github.io/projects/motionbooth</description><author>Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, Kai Chen</author><pubDate>Wed, 21 Aug 2024 14:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17758v2</guid></item><item><title>VulDetectBench: Evaluating the Deep Capability of Vulnerability Detection with Large Language Models</title><link>http://arxiv.org/abs/2406.07595v4</link><description>Large Language Models (LLMs) have training corpora containing large amountsof program code, greatly improving the model's code comprehension andgeneration capabilities. However, sound comprehensive research on detectingprogram vulnerabilities, a more specific task related to code, and evaluatingthe performance of LLMs in this more specialized scenario is still lacking. Toaddress common challenges in vulnerability analysis, our study introduces a newbenchmark, VulDetectBench, specifically designed to assess the vulnerabilitydetection capabilities of LLMs. The benchmark comprehensively evaluates LLM'sability to identify, classify, and locate vulnerabilities through five tasks ofincreasing difficulty. We evaluate the performance of 17 models (both open- andclosed-source) and find that while existing models can achieve over 80%accuracy on tasks related to vulnerability identification and classification,they still fall short on specific, more detailed vulnerability analysis tasks,with less than 30% accuracy, making it difficult to provide valuable auxiliaryinformation for professional vulnerability mining. Our benchmark effectivelyevaluates the capabilities of various LLMs at different levels in the specifictask of vulnerability detection, providing a foundation for future research andimprovements in this critical area of code security. VulDetectBench is publiclyavailable at https://github.com/Sweetaroo/VulDetectBench.</description><author>Yu Liu, Lang Gao, Mingxin Yang, Yu Xie, Ping Chen, Xiaojin Zhang, Wei Chen</author><pubDate>Wed, 21 Aug 2024 14:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07595v4</guid></item><item><title>MIS-ME: A Multi-modal Framework for Soil Moisture Estimation</title><link>http://arxiv.org/abs/2408.00963v3</link><description>Soil moisture estimation is an important task to enable precision agriculturein creating optimal plans for irrigation, fertilization, and harvest. It iscommon to utilize statistical and machine learning models to estimate soilmoisture from traditional data sources such as weather forecasts, soilproperties, and crop properties. However, there is a growing interest inutilizing aerial and geospatial imagery to estimate soil moisture. Althoughthese images capture high-resolution crop details, they are expensive to curateand challenging to interpret. Imagine, an AI-enhanced software tool thatpredicts soil moisture using visual cues captured by smartphones andstatistical data given by weather forecasts. This work is a first step towardsthat goal of developing a multi-modal approach for soil moisture estimation. Inparticular, we curate a dataset consisting of real-world images taken fromground stations and their corresponding weather data. We also propose MIS-ME -Meteorological &amp; Image based Soil Moisture Estimator, a multi-modal frameworkfor soil moisture estimation. Our extensive analysis shows that MIS-ME achievesa MAPE of 10.14%, outperforming traditional unimodal approaches with areduction of 3.25% in MAPE for meteorological data and 2.15% in MAPE for imagedata, highlighting the effectiveness of tailored multi-modal approaches. Ourcode and dataset will be available athttps://github.com/OSU-Complex-Systems/MIS-ME.git.</description><author>Mohammed Rakib, Adil Aman Mohammed, D. Cole Diggins, Sumit Sharma, Jeff Michael Sadler, Tyson Ochsner, Arun Bagavathi</author><pubDate>Wed, 21 Aug 2024 14:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00963v3</guid></item><item><title>Suppressing unknown disturbances to dynamical systems using machine learning</title><link>http://arxiv.org/abs/2307.03690v5</link><description>Identifying and suppressing unknown disturbances to dynamical systems is aproblem with applications in many different fields. Here we present amodel-free method to identify and suppress an unknown disturbance to an unknownsystem based only on previous observations of the system under the influence ofa known forcing function. We find that, under very mild restrictions on thetraining function, our method is able to robustly identify and suppress a largeclass of unknown disturbances. We illustrate our scheme with the identificationof both deterministic and stochastic unknown disturbances to an analog electricchaotic circuit and with numerical examples where a chaotic disturbance tovarious chaotic dynamical systems is identified and suppressed.</description><author>Juan G. Restrepo, Clayton P. Byers, Per Sebastian Skardal</author><pubDate>Wed, 21 Aug 2024 14:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03690v5</guid></item><item><title>Optimizing Federated Graph Learning with Inherent Structural Knowledge and Dual-Densely Connected GNNs</title><link>http://arxiv.org/abs/2408.11662v1</link><description>Federated Graph Learning (FGL) is an emerging technology that enables clientsto collaboratively train powerful Graph Neural Networks (GNNs) in a distributedmanner without exposing their private data. Nevertheless, FGL still faces thechallenge of the severe non-Independent and Identically Distributed (non-IID)nature of graphs, which possess diverse node and edge structures, especiallyacross varied domains. Thus, exploring the knowledge inherent in thesestructures becomes significantly crucial. Existing methods, however, eitheroverlook the inherent structural knowledge in graph data or capture it at thecost of significantly increased resource demands (e.g., FLOPs and communicationbandwidth), which can be detrimental to distributed paradigms. Inspired bythis, we propose FedDense, a novel FGL framework that optimizes the utilizationefficiency of inherent structural knowledge. To better acquire knowledge ofdiverse and underexploited structures, FedDense first explicitly encodes thestructural knowledge inherent within graph data itself alongside node features.Besides, FedDense introduces a Dual-Densely Connected (DDC) GNN architecturethat exploits the multi-scale (i.e., one-hop to multi-hop) feature andstructure insights embedded in the aggregated feature maps at each layer. Inaddition to the exploitation of inherent structures, we consider resourcelimitations in FGL, devising exceedingly narrow layers atop the DDCarchitecture and adopting a selective parameter sharing strategy to reduceresource costs substantially. We conduct extensive experiments using 15datasets across 4 different domains, demonstrating that FedDense consistentlysurpasses baselines by a large margin in training performance, while demandingminimal resources.</description><author>Longwen Wang, Jianchun Liu, Zhi Liu, Jinyang Huang</author><pubDate>Wed, 21 Aug 2024 14:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11662v1</guid></item><item><title>GNN-SKAN: Harnessing the Power of SwallowKAN to Advance Molecular Representation Learning with GNNs</title><link>http://arxiv.org/abs/2408.01018v2</link><description>Effective molecular representation learning is crucial for advancingmolecular property prediction and drug design. Mainstream molecularrepresentation learning approaches are based on Graph Neural Networks (GNNs).However, these approaches struggle with three significant challenges:insufficient annotations, molecular diversity, and architectural limitationssuch as over-squashing, which leads to the loss of critical structural details.To address these challenges, we introduce a new class of GNNs that integratesthe Kolmogorov-Arnold Networks (KANs), known for their robust data-fittingcapabilities and high accuracy in small-scale AI + Science tasks. Byincorporating KANs into GNNs, our model enhances the representation ofmolecular structures. We further advance this approach with a variant calledSwallowKAN (SKAN), which employs adaptive Radial Basis Functions (RBFs) as thecore of the non-linear neurons. This innovation improves both computationalefficiency and adaptability to diverse molecular structures. Building on thestrengths of SKAN, we propose a new class of GNNs, GNN-SKAN, and its augmentedvariant, GNN-SKAN+, which incorporates a SKAN-based classifier to further boostperformance. To our knowledge, this is the first work to integrate KANs intoGNN architectures tailored for molecular representation learning. Experimentsacross 6 classification datasets, 6 regression datasets, and 4 few-shotlearning datasets demonstrate that our approach achieves new state-of-the-artperformance in terms of accuracy and computational cost.</description><author>Ruifeng Li, Mingqian Li, Wei Liu, Hongyang Chen</author><pubDate>Wed, 21 Aug 2024 14:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01018v2</guid></item><item><title>Tracing Privacy Leakage of Language Models to Training Data via Adjusted Influence Functions</title><link>http://arxiv.org/abs/2408.10468v2</link><description>The responses generated by Large Language Models (LLMs) can include sensitiveinformation from individuals and organizations, leading to potential privacyleakage. This work implements Influence Functions (IFs) to trace privacyleakage back to the training data, thereby mitigating privacy concerns ofLanguage Models (LMs). However, we notice that current IFs struggle toaccurately estimate the influence of tokens with large gradient norms,potentially overestimating their influence. When tracing the most influentialsamples, this leads to frequently tracing back to samples with large gradientnorm tokens, overshadowing the actual most influential samples even if theirinfluences are well estimated. To address this issue, we propose HeuristicallyAdjusted IF (HAIF), which reduces the weight of tokens with large gradientnorms, thereby significantly improving the accuracy of tracing the mostinfluential samples. To establish easily obtained groundtruth for tracingprivacy leakage, we construct two datasets, PII-E and PII-CR, representing twodistinct scenarios: one with identical text in the model outputs andpre-training data, and the other where models leverage their reasoningabilities to generate text divergent from pre-training data. HAIF significantlyimproves tracing accuracy, enhancing it by 20.96\% to 73.71\% on the PII-Edataset and 3.21\% to 45.93\% on the PII-CR dataset, compared to the best SOTAIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFson real-world pretraining data CLUECorpus2020, demonstrating strong robustnessregardless prompt and response lengths.</description><author>Jinxin Liu, Zao Yang</author><pubDate>Wed, 21 Aug 2024 14:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10468v2</guid></item><item><title>Proximal Policy Optimization with Graph Neural Networks for Optimal Power Flow</title><link>http://arxiv.org/abs/2212.12470v2</link><description>Optimal Power Flow (OPF) is a very traditional research area within the powersystems field that seeks for the optimal operation point of electric powerplants, and which needs to be solved every few minutes in real-world scenarios.However, due to the nonconvexities that arise in power generation systems,there is not yet a fast, robust solution technique for the full AlternatingCurrent Optimal Power Flow (ACOPF). In the last decades, power grids haveevolved into a typical dynamic, non-linear and large-scale control system,known as the power system, so searching for better and faster ACOPF solutionsis becoming crucial. Appearance of Graph Neural Networks (GNN) has allowed thenatural use of Machine Learning (ML) algorithms on graph data, such as powernetworks. On the other hand, Deep Reinforcement Learning (DRL) is known for itspowerful capability to solve complex decision-making problems. Althoughsolutions that use these two methods separately are beginning to appear in theliterature, none has yet combined the advantages of both. We propose a novelarchitecture based on the Proximal Policy Optimization algorithm with GraphNeural Networks to solve the Optimal Power Flow. The objective is to design anarchitecture that learns how to solve the optimization problem and that is atthe same time able to generalize to unseen scenarios. We compare our solutionwith the DCOPF in terms of cost after having trained our DRL agent on IEEE 30bus system and then computing the OPF on that base network with topologychanges</description><author>ngela Lpez-Cardona, Guillermo Bernrdez, Pere Barlet-Ros, Albert Cabellos-Aparicio</author><pubDate>Wed, 21 Aug 2024 14:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12470v2</guid></item></channel></rss>