<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 24 Oct 2025 02:03:54 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Fast Inference via Hierarchical Speculative Decoding</title><link>http://arxiv.org/abs/2510.19705v2</link><description>Transformer language models generate text autoregressively, making inferencelatency proportional to the number of tokens generated. Speculative decodingreduces this latency without sacrificing output quality, by leveraging a smalldraft model to propose tokens that the larger target model verifies inparallel. In practice, however, there may exist a set of potential draftmodels- ranging from faster but less inaccurate, to slower yet more reliable.We introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacksthese draft models into a hierarchy, where each model proposes tokens, and thenext larger model verifies them in a single forward pass, until finally thetarget model verifies tokens. We derive an expression for the expected latencyof any such hierarchy and show that selecting the latency-optimal hierarchy canbe done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over thebest single-draft baseline, demonstrating the practicality of our algorithm inreducing generation latency beyond previous techniques.</description><author>Clara Mohri, Haim Kaplan, Tal Schuster, Yishay Mansour, Amir Globerson</author><pubDate>Thu, 23 Oct 2025 14:15:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19705v2</guid></item><item><title>Benchmarking World-Model Learning</title><link>http://arxiv.org/abs/2510.19788v2</link><description>Model-learning agents should gather information to learn world models thatsupport many downstream tasks and inferences, such as predicting unobservedstates, estimating near- and far-term consequences of actions, planning actionsequences, and detecting changes in dynamics. Current methods for learning andevaluating world models diverge from this goal: training and evaluation areanchored to next-frame prediction, and success is scored by reward maximizationin the same environment. We propose WorldTest, a protocol to evaluatemodel-learning agents that separates reward-free interaction from a scored testphase in a different but related environment. WorldTest isopen-ended$\unicode{x2014}$models should support many different tasks unknownahead of time$\unicode{x2014}$and agnostic to model representation, allowingcomparison across approaches. We instantiated WorldTest with AutumnBench, asuite of 43 interactive grid-world environments and 129 tasks across threefamilies: masked-frame prediction, planning, and predicting changes to thecausal dynamics. We compared 517 human participants and three frontier modelson AutumnBench. We found that humans outperform the models, and scaling computeimproves performance only in some environments but not others. WorldTestprovides a novel template$\unicode{x2014}$reward-free exploration, derivedtests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learnabout environment dynamics, and AutumnBench exposes significant headroom inworld-model learning.</description><author>Archana Warrier, Dat Nguyen, Michelangelo Naim, Moksh Jain, Yichao Liang, Karen Schroeder, Cambridge Yang, Joshua B. Tenenbaum, Sebastian Vollmer, Kevin Ellis, Zenna Tavares</author><pubDate>Thu, 23 Oct 2025 11:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19788v2</guid></item><item><title>CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees</title><link>http://arxiv.org/abs/2510.19754v2</link><description>Counterfactual explanations (CFXs) provide human-understandablejustifications for model predictions, enabling actionable recourse andenhancing interpretability. To be reliable, CFXs must avoid regions of highpredictive uncertainty, where explanations may be misleading or inapplicable.However, existing methods often neglect uncertainty or lack principledmechanisms for incorporating it with formal guarantees. We propose CONFEX, anovel method for generating uncertainty-aware counterfactual explanations usingConformal Prediction (CP) and Mixed-Integer Linear Programming (MILP). CONFEXexplanations are designed to provide local coverage guarantees, addressing theissue that CFX generation violates exchangeability. To do so, we develop anovel localised CP procedure that enjoys an efficient MILP encoding byleveraging an offline tree-based partitioning of the input space. This way,CONFEX generates CFXs with rigorous guarantees on both predictive uncertaintyand optimality. We evaluate CONFEX against state-of-the-art methods acrossdiverse benchmarks and metrics, demonstrating that our uncertainty-awareapproach yields robust and plausible explanations.</description><author>Aman Bilkhoo, Mehran Hosseini, Milad Kazemi, Nicola Paoletti</author><pubDate>Thu, 23 Oct 2025 10:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19754v2</guid></item><item><title>Adapting Multilingual Models to Code-Mixed Tasks via Model Merging</title><link>http://arxiv.org/abs/2510.19782v2</link><description>We study model merging as a practical alternative to conventional adaptationstrategies for code-mixed NLP. Starting from a multilingual base model, we: (i)perform continued pre-training (CPT) on unlabeled code-mixed text to obtain anadapted checkpoint, (ii) merge checkpoint with the base model, and (iii)fine-tune (FT) on the downstream task data. We evaluate our approach forsentence classification (sentiment and hate speech) task in English-Hindi(En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Ourresults show that merged models consistently outperform full fine-tuning andCPT-&gt;FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2points over CPT-&gt;FT, indicating that unlabeled data is leveraged moreeffectively via merging than via CPT alone. Zero-/few-shot prompting withlarger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and mergedcheckpoints, underscoring limits of in-context learning for code-mixed inputs.We further test cross-pair transfer by training on En-Hi and evaluating onEn-Ta and En-Ml: merged checkpoints transfer more strongly thanmonolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a morereliable substrate for low-resource pairs. We conclude with adaptation recipesmatched to common data regimes (labeled only; labeled+unlabeled; transfer-only)and discuss limitations and scaling considerations for broader tasks and largermodels.</description><author>Prashant Kodali, Vaishnavi Shivkumar, Swarang Joshi, Monojit Choudhary, Ponnurangam Kumaraguru, Manish Shrivastava</author><pubDate>Thu, 23 Oct 2025 10:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19782v2</guid></item><item><title>Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning</title><link>http://arxiv.org/abs/2510.19733v2</link><description>Large Language Model (LLM) conditioning refers to instructing an LLM togenerate content in accordance with the norms and values of a specific culture,beliefs of a particular political orientation, or any desired text-specifiedsemantic conditioning. Unfortunately, prompt engineering does not ensure thatLLMs behave in accordance with a desired conditioning due to the inductive biasof the pre-training and alignment datasets. Prior works have focused onfine-tuning LLMs by directly conditioning the LoRA weights; however, suchmethods introduce a large number of parameters. As a remedy, we propose Zhyper,a parameter-efficient factorized hypernetwork framework that generatescontext-aware LoRA adapters from textual descriptions. Experiments on multiplebenchmarks show that Zhyper achieves competitive performance with up to 26xfewer parameters than the state-of-the-art baselines. Furthermore, we extendZhyper to cultural alignment, demonstrating improved generalization toout-of-domain settings and a better capturing of fine-grained contextualvalues.</description><author>M. H. I. Abdalla, Zhipin Wang, Christian Frey, Steffen Eger, Josif Grabocka</author><pubDate>Thu, 23 Oct 2025 09:50:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19733v2</guid></item><item><title>A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</title><link>http://arxiv.org/abs/2510.19755v2</link><description>Diffusion Models have become a cornerstone of modern generative AI for theirexceptional generation quality and controllability. However, their inherent\textit{multi-step iterations} and \textit{complex backbone networks} lead toprohibitive computational overhead and generation latency, forming a majorbottleneck for real-time applications. Although existing accelerationtechniques have made progress, they still face challenges such as limitedapplicability, high training costs, or quality degradation. Against this backdrop, \textbf{Diffusion Caching} offers a promisingtraining-free, architecture-agnostic, and efficient inference paradigm. Itscore mechanism identifies and reuses intrinsic computational redundancies inthe diffusion process. By enabling feature-level cross-step reuse andinter-layer scheduling, it reduces computation without modifying modelparameters. This paper systematically reviews the theoretical foundations andevolution of Diffusion Caching and proposes a unified framework for itsclassification and analysis. Through comparative analysis of representative methods, we show thatDiffusion Caching evolves from \textit{static reuse} to \textit{dynamicprediction}. This trend enhances caching flexibility across diverse tasks andenables integration with other acceleration techniques such as samplingoptimization and model distillation, paving the way for a unified, efficientinference framework for future multimodal and interactive applications. Weargue that this paradigm will become a key enabler of real-time and efficientgenerative AI, injecting new vitality into both theory and practice of\textit{Efficient Generative Intelligence}.</description><author>Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</author><pubDate>Thu, 23 Oct 2025 09:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19755v2</guid></item><item><title>DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning</title><link>http://arxiv.org/abs/2510.19562v2</link><description>Comprehending natural language and following human instructions are criticalcapabilities for intelligent agents. However, the flexibility of linguisticinstructions induces substantial ambiguity across language-conditioned tasks,severely degrading algorithmic performance. To address these limitations, wepresent a novel method named DAIL (Distributional Aligned Learning), featuringtwo key components: distributional policy and semantic alignment. Specifically,we provide theoretical results that the value distribution estimation mechanismenhances task differentiability. Meanwhile, the semantic alignment modulecaptures the correspondence between trajectories and linguistic instructions.Extensive experimental results on both structured and visual observationbenchmarks demonstrate that DAIL effectively resolves instruction ambiguities,achieving superior performance to baseline methods. Our implementation isavailable at https://github.com/RunpengXie/Distributional-Aligned-Learning.</description><author>Runpeng Xie, Quanwei Wang, Hao Hu, Zherui Zhou, Ni Mu, Xiyun Li, Yiqin Yang, Shuang Xu, Qianchuan Zhao, Bo XU</author><pubDate>Thu, 23 Oct 2025 07:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19562v2</guid></item><item><title>Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics</title><link>http://arxiv.org/abs/2506.06045v3</link><description>Graph-based learned simulators have emerged as a promising approach forsimulating physical systems on unstructured meshes, offering speed andgeneralization across diverse geometries. However, they often struggle withcapturing global phenomena, such as bending or long-range correlations usuallyoccurring in solid mechanics, and suffer from error accumulation over longrollouts due to their reliance on local message passing and direct next-stepprediction. We address these limitations by introducing the RollingDiffusion-Batched Inference Network (ROBIN), a novel learned simulator thatintegrates two key innovations: (i) Rolling Diffusion-Batched Inference (ROBI),a parallelized inference scheme that amortizes the cost of diffusion-basedrefinement across physical time steps by overlapping denoising steps across atemporal window. (ii) A Hierarchical Graph Neural Network built on algebraicmultigrid coarsening, enabling multiscale message passing across different meshresolutions. This architecture, implemented via Algebraic-hierarchical MessagePassing Networks, captures both fine-scale local dynamics and global structuraleffects critical for phenomena like beam bending or multi-body contact. Wevalidate ROBIN on challenging 2D and 3D solid mechanics benchmarks involvinggeometric, material, and contact nonlinearities. ROBIN achievesstate-of-the-art accuracy on all tasks, substantially outperforming existingnext-step learned simulators while reducing inference time by up to an order ofmagnitude compared to standard diffusion simulators.</description><author>Tobias Würth, Niklas Freymuth, Gerhard Neumann, Luise Kärger</author><pubDate>Thu, 23 Oct 2025 06:14:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.06045v3</guid></item><item><title>CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization</title><link>http://arxiv.org/abs/2510.19597v2</link><description>Image Forgery Localization (IFL) is a crucial task in image forensics, aimedat accurately identifying manipulated or tampered regions within an image atthe pixel level. Existing methods typically generate a single deterministiclocalization map, which often lacks the precision and reliability required forhigh-stakes applications such as forensic analysis and security surveillance.To enhance the credibility of predictions and mitigate the risk of errors, weintroduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given aforged image, CBDiff generates multiple diverse and plausible localizationmaps, thereby offering a richer and more comprehensive representation of theforgery distribution. This approach addresses the uncertainty and variabilityinherent in tampered regions. Furthermore, CBDiff innovatively incorporatesBernoulli noise into the diffusion process to more faithfully reflect theinherent binary and sparse properties of forgery masks. Additionally, CBDiffintroduces a Time-Step Cross-Attention (TSCAttention), which is specificallydesigned to leverage semantic feature guidance with temporal steps to improvemanipulation detection. Extensive experiments on eight publicly benchmarkdatasets demonstrate that CBDiff significantly outperforms existingstate-of-the-art methods, highlighting its strong potential for real-worlddeployment.</description><author>Zhou Lei, Pan Gang, Wang Jiahao, Sun Di</author><pubDate>Thu, 23 Oct 2025 05:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19597v2</guid></item><item><title>Graph Representation Learning with Diffusion Generative Models</title><link>http://arxiv.org/abs/2501.13133v2</link><description>Diffusion models have established themselves as state-of-the-art generativemodels across various data modalities, including images and videos, due totheir ability to accurately approximate complex data distributions. Unliketraditional generative approaches such as VAEs and GANs, diffusion modelsemploy a progressive denoising process that transforms noise into meaningfuldata over multiple iterative steps. This gradual approach enhances theirexpressiveness and generation quality. Not only that, diffusion models havealso been shown to extract meaningful representations from data while learningto generate samples. Despite their success, the application of diffusion modelsto graph-structured data remains relatively unexplored, primarily due to thediscrete nature of graphs, which necessitates discrete diffusion processesdistinct from the continuous methods used in other domains. In this work, weleverage the representational capabilities of diffusion models to learnmeaningful embeddings for graph data. By training a discrete diffusion modelwithin an autoencoder framework, we enable both effective autoencoding andrepresentation learning tailored to the unique characteristics ofgraph-structured data. We extract the representation from the combination ofthe encoder's output and the decoder's first time step hidden embedding. Ourapproach demonstrates the potential of discrete diffusion models to be used forgraph representation learning. The code can be found athttps://github.com/DanielMitiku/Graph-Representation-Learning-with-Diffusion-Generative-Models</description><author>Daniel Wesego</author><pubDate>Wed, 22 Oct 2025 17:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13133v2</guid></item><item><title>LoRA vs Full Fine-tuning: An Illusion of Equivalence</title><link>http://arxiv.org/abs/2410.21228v3</link><description>Fine-tuning is a crucial paradigm for adapting pre-trained large languagemodels to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA)have been shown to effectively fine-tune LLMs with an extreme reduction intrainable parameters. But, \emph{are their learned solutions reallyequivalent?} We study how LoRA and full-finetuning change pre-trained models byanalyzing the model's weight matrices through the lens of their spectralproperties. We find that LoRA and full fine-tuning yield weight matrices whosesingular value decompositions exhibit very different structure: weight matricestrained with LoRA have new, high-ranking singular vectors, which we call\emph{intruder dimensions}, while those trained with full fine-tuning do not.Further, we extend the finding that LoRA forgets less than full fine-tuning andfind its forgetting is vastly localized to the intruder dimension -- bycausally intervening on the intruder dimensions by changing their associatedsingular values post-fine-tuning, we show that they cause forgetting. Moreover,scaling them down significantly improves modeling of the pre-trainingdistribution with a minimal drop in downstream task performance. Given this, weshould expect accumulating intruder dimensions to be harmful and lead to moreforgetting. This will be amplified during continual learning because ofsequentially fine-tuning, and we show that LoRA models do accumulate intruderdimensions here tend to perform worse in this setting, emphasizing thepracticality of our findings.</description><author>Reece Shuttleworth, Jacob Andreas, Antonio Torralba, Pratyusha Sharma</author><pubDate>Wed, 22 Oct 2025 17:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21228v3</guid></item><item><title>Learning Reward Machines from Partially Observed Policies</title><link>http://arxiv.org/abs/2502.03762v3</link><description>Inverse reinforcement learning is the problem of inferring a reward functionfrom an optimal policy or demonstrations by an expert. In this work, it isassumed that the reward is expressed as a reward machine whose transitionsdepend on atomic propositions associated with the state of a Markov DecisionProcess (MDP). Our goal is to identify the true reward machine using finiteinformation. To this end, we first introduce the notion of a prefix tree policywhich associates a distribution of actions to each state of the MDP and eachattainable finite sequence of atomic propositions. Then, we characterize anequivalence class of reward machines that can be identified given the prefixtree policy. Finally, we propose a SAT-based algorithm that uses informationextracted from the prefix tree policy to solve for a reward machine. It isproved that if the prefix tree policy is known up to a sufficient (but finite)depth, our algorithm recovers the exact reward machine up to the equivalenceclass. This sufficient depth is derived as a function of the number of MDPstates and (an upper bound on) the number of states of the reward machine.These results are further extended to the case where we only have access todemonstrations from an optimal policy. Several examples, including discretegrid and block worlds, a continuous state-space robotic arm, and real data fromexperiments with mice, are used to demonstrate the effectiveness and generalityof the approach.</description><author>Mohamad Louai Shehab, Antoine Aspeel, Necmiye Ozay</author><pubDate>Wed, 22 Oct 2025 17:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.03762v3</guid></item><item><title>Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization</title><link>http://arxiv.org/abs/2510.17501v3</link><description>We propose a rubric-guided, pseudo-labeled, and prompt-driven zero-shot videosummarization framework that bridges large language models with structuredsemantic reasoning. A small subset of human annotations is converted intohigh-confidence pseudo labels and organized into dataset-adaptive rubricsdefining clear evaluation dimensions such as thematic relevance, action detail,and narrative progression. During inference, boundary scenes, including theopening and closing segments, are scored independently based on their owndescriptions, while intermediate scenes incorporate concise summaries ofadjacent segments to assess narrative continuity and redundancy. This designenables the language model to balance local salience with global coherencewithout any parameter tuning. Across three benchmarks, the proposed methodachieves stable and competitive results, with F1 scores of 57.58 on SumMe,63.05 on TVSum, and 53.79 on QFVS, surpassing zero-shot baselines by +0.85,+0.84, and +0.37, respectively. These outcomes demonstrate that rubric-guidedpseudo labeling combined with contextual prompting effectively stabilizesLLM-based scoring and establishes a general, interpretable, and training-freeparadigm for both generic and query-focused video summarization.</description><author>Yuanli Wu, Long Zhang, Yue Du, Bin Li</author><pubDate>Wed, 22 Oct 2025 17:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.17501v3</guid></item><item><title>Is This Tracker On? A Benchmark Protocol for Dynamic Tracking</title><link>http://arxiv.org/abs/2510.19819v1</link><description>We introduce ITTO, a challenging new benchmark suite for evaluating anddiagnosing the capabilities and limitations of point tracking methods. Ourvideos are sourced from existing datasets and egocentric real-world recordings,with high-quality human annotations collected through a multi-stage pipeline.ITTO captures the motion complexity, occlusion patterns, and object diversitycharacteristic of real-world scenes -- factors that are largely absent incurrent benchmarks. We conduct a rigorous analysis of state-of-the-art trackingmethods on ITTO, breaking down performance along key axes of motion complexity.Our findings reveal that existing trackers struggle with these challenges,particularly in re-identifying points after occlusion, highlighting criticalfailure modes. These results point to the need for new modeling approachestailored to real-world dynamics. We envision ITTO as a foundation testbed foradvancing point tracking and guiding the development of more robust trackingalgorithms.</description><author>Ilona Demler, Saumya Chauhan, Georgia Gkioxari</author><pubDate>Wed, 22 Oct 2025 17:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19819v1</guid></item><item><title>Semantic World Models</title><link>http://arxiv.org/abs/2510.19818v1</link><description>Planning with world models offers a powerful paradigm for robotic control.Conventional approaches train a model to predict future frames conditioned oncurrent frames and actions, which can then be used for planning. However, theobjective of predicting future pixels is often at odds with the actual planningobjective; strong pixel reconstruction does not always correlate with goodplanning decisions. This paper posits that instead of reconstructing futureframes as pixels, world models only need to predict task-relevant semanticinformation about the future. For such prediction the paper poses worldmodeling as a visual question answering problem about semantic information infuture frames. This perspective allows world modeling to be approached with thesame tools underlying vision language models. Thus vision language models canbe trained as "semantic" world models through a supervised finetuning processon image-action-text data, enabling planning for decision-making whileinheriting many of the generalization and robustness properties from thepretrained vision-language models. The paper demonstrates how such a semanticworld model can be used for policy improvement on open-ended robotics tasks,leading to significant generalization improvements over typical paradigms ofreconstruction-based action-conditional world modeling. Website available athttps://weirdlabuw.github.io/swm.</description><author>Jacob Berg, Chuning Zhu, Yanda Bao, Ishan Durugkar, Abhishek Gupta</author><pubDate>Wed, 22 Oct 2025 17:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19818v1</guid></item><item><title>olmOCR 2: Unit Test Rewards for Document OCR</title><link>http://arxiv.org/abs/2510.19817v1</link><description>We present olmOCR 2, the latest in our family of powerful OCR systems forconverting digitized print documents, like PDFs, into clean, naturally orderedplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B visionlanguage model (VLM) trained using reinforcement learning with verifiablerewards (RLVR), where our rewards are a diverse set of binary unit tests. Toscale unit test creation, we develop a pipeline for generating syntheticdocuments with diverse and challenging layouts, known ground-truth HTML sourcecode, and extracted test cases. We show that RL training on these test casesresults in state-of-the-art performance on olmOCR-Bench, our English-languageOCR benchmark, with the largest improvements in math formula conversion, tableparsing, and multi-column layouts compared to previous versions. We release ourmodel, data and code under permissive open licenses.</description><author>Jake Poznanski, Luca Soldaini, Kyle Lo</author><pubDate>Wed, 22 Oct 2025 17:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19817v1</guid></item><item><title>How to Evaluate Monocular Depth Estimation?</title><link>http://arxiv.org/abs/2510.19814v1</link><description>Monocular depth estimation is an important task with rapid progress, but howto evaluate it remains an open question, as evidenced by a lack ofstandardization in existing literature and a large selection of evaluationmetrics whose trade-offs and behaviors are not well understood. This papercontributes a novel, quantitative analysis of existing metrics in terms oftheir sensitivity to various types of perturbations of ground truth,emphasizing comparison to human judgment. Our analysis reveals that existingmetrics are severely under-sensitive to curvature perturbation such as makingflat surfaces wavy. To remedy this, we introduce a new metric based on relativesurface normals, along with new depth visualization tools and a principledmethod to create composite metrics with better human alignment. Code and dataare available at: https://github.com/princeton-vl/evalmde.</description><author>Siyang Wu, Jack Nugent, Willow Yang, Jia Deng</author><pubDate>Wed, 22 Oct 2025 17:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19814v1</guid></item><item><title>Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in LLM</title><link>http://arxiv.org/abs/2505.24379v3</link><description>Large Language Models are typically trained on datasets collected from theweb, which may inadvertently contain harmful or sensitive personal information.To address growing privacy concerns, unlearning methods have been proposed toremove the influence of specific data from trained models. Of these, exactunlearning -- which retrains the model from scratch without the target data --is widely regarded the gold standard for mitigating privacy risks indeployment. In this paper, we revisit this assumption in a practical deploymentsetting where both the pre- and post-unlearning logits API are exposed, such asin open-weight scenarios. Targeting this setting, we introduce a novel dataextraction attack that leverages signals from the pre-unlearning model to guidethe post-unlearning model, uncovering patterns that reflect the removed datadistribution. Combining model guidance with a token filtering strategy, ourattack significantly improves extraction success rates -- doubling performancein some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.Furthermore, we demonstrate our attack's effectiveness on a simulated medicaldiagnosis dataset to highlight real-world privacy risks associated with exactunlearning. In light of our findings, which suggest that unlearning may, in acontradictory way, increase the risk of privacy leakage during real-worlddeployments, we advocate for evaluation of unlearning methods to considerbroader threat models that account not only for post-unlearning models but alsofor adversarial access to prior checkpoints. Code is publicly available at:https://github.com/Nicholas0228/unlearned_data_extraction_llm.</description><author>Xiaoyu Wu, Yifei Pang, Terrance Liu, Zhiwei Steven Wu</author><pubDate>Wed, 22 Oct 2025 17:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24379v3</guid></item><item><title>Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?</title><link>http://arxiv.org/abs/2509.21087v2</link><description>Machine learning approaches for speech enhancement are becoming increasinglyexpressive, enabling ever more powerful modifications of input signals. In thispaper, we demonstrate that this expressiveness introduces a vulnerability:advanced speech enhancement models can be susceptible to adversarial attacks.Specifically, we show that adversarial noise, carefully crafted andpsychoacoustically masked by the original input, can be injected such that theenhanced speech output conveys an entirely different semantic meaning. Weexperimentally verify that contemporary predictive speech enhancement modelscan indeed be manipulated in this way. Furthermore, we highlight that diffusionmodels with stochastic samplers exhibit inherent robustness to such adversarialattacks by design.</description><author>Rostislav Makarov, Lea Schönherr, Timo Gerkmann</author><pubDate>Wed, 22 Oct 2025 17:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21087v2</guid></item><item><title>Hubble: a Model Suite to Advance the Study of LLM Memorization</title><link>http://arxiv.org/abs/2510.19811v1</link><description>We present Hubble, a suite of fully open-source large language models (LLMs)for the scientific study of LLM memorization. Hubble models come in standardand perturbed variants: standard models are pretrained on a large Englishcorpus, and perturbed models are trained in the same way but with controlledinsertion of text (e.g., book passages, biographies, and test sets) designed toemulate key memorization risks. Our core release includes 8 models -- standardand perturbed models with 1B or 8B parameters, pretrained on 100B or 500Btokens -- establishing that memorization risks are determined by the frequencyof sensitive data relative to size of the training corpus (i.e., a passwordappearing once in a smaller corpus is memorized better than the same passwordin a larger corpus). Our release also includes 6 perturbed models with textinserted at different pretraining phases, showing that sensitive data withoutcontinued exposure can be forgotten. These findings suggest two best practicesfor addressing memorization risks: to dilute sensitive data by increasing thesize of the training corpus, and to order sensitive data to appear earlier intraining. Beyond these general empirical findings, Hubble enables a broad rangeof memorization research; for example, analyzing the biographies reveals howreadily different types of private information are memorized. We alsodemonstrate that the randomized insertions in Hubble make it an ideal testbedfor membership inference and machine unlearning, and invite the community tofurther explore, benchmark, and build upon our work.</description><author>Johnny Tian-Zheng Wei, Ameya Godbole, Mohammad Aflah Khan, Ryan Wang, Xiaoyuan Zhu, James Flemings, Nitya Kashyap, Krishna P. Gummadi, Willie Neiswanger, Robin Jia</author><pubDate>Wed, 22 Oct 2025 17:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19811v1</guid></item><item><title>Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing</title><link>http://arxiv.org/abs/2510.19808v1</link><description>Recent advances in multimodal models have demonstrated remarkable text-guidedimage editing capabilities, with systems like GPT-4o and Nano-Banana settingnew benchmarks. However, the research community's progress remains constrainedby the absence of large-scale, high-quality, and openly accessible datasetsbuilt from real images. We introduce Pico-Banana-400K, a comprehensive400K-image dataset for instruction-based image editing. Our dataset isconstructed by leveraging Nano-Banana to generate diverse edit pairs from realphotographs in the OpenImages collection. What distinguishes Pico-Banana-400Kfrom previous synthetic datasets is our systematic approach to quality anddiversity. We employ a fine-grained image editing taxonomy to ensurecomprehensive coverage of edit types while maintaining precise contentpreservation and instruction faithfulness through MLLM-based quality scoringand careful curation. Beyond single turn editing, Pico-Banana-400K enablesresearch into complex editing scenarios. The dataset includes three specializedsubsets: (1) a 72K-example multi-turn collection for studying sequentialediting, reasoning, and planning across consecutive modifications; (2) a56K-example preference subset for alignment research and reward model training;and (3) paired long-short editing instructions for developing instructionrewriting and summarization capabilities. By providing this large-scale,high-quality, and task-rich resource, Pico-Banana-400K establishes a robustfoundation for training and benchmarking the next generation of text-guidedimage editing models.</description><author>Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, Zhe Gan</author><pubDate>Wed, 22 Oct 2025 17:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19808v1</guid></item><item><title>Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning</title><link>http://arxiv.org/abs/2510.19807v1</link><description>Reinforcement learning from verifiable rewards has emerged as a powerfultechnique for enhancing the complex reasoning abilities of Large LanguageModels (LLMs). However, these methods are fundamentally constrained by the''learning cliff'' phenomenon: when faced with problems far beyond theircurrent capabilities, models consistently fail, yielding a persistentzero-reward signal. In policy optimization algorithms like GRPO, this collapsesthe advantage calculation to zero, rendering these difficult problems invisibleto the learning gradient and stalling progress. To overcome this, we introduceScaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressivetraining framework that strategically provides minimal guidance only when amodel's independent learning has plateaued. The framework first diagnoseslearning stagnation and then intervenes by injecting tiered in-prompt hints,ranging from abstract concepts to concrete steps, enabling the model toconstruct a valid solution by itself. Extensive experiments on challengingmathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting thepass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative44.3% over a vanilla GRPO baseline. This result demonstrates our frameworkprovides a robust and effective methodology for unlocking a model's ability tosolve problems previously beyond its reach, a critical step towards extendingthe frontier of autonomous reasoning in LLM.</description><author>Xichen Zhang, Sitong Wu, Yinghao Zhu, Haoru Tan, Shaozuo Yu, Ziyi He, Jiaya Jia</author><pubDate>Wed, 22 Oct 2025 17:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19807v1</guid></item><item><title>The Art of Asking: Multilingual Prompt Optimization for Synthetic Data</title><link>http://arxiv.org/abs/2510.19806v1</link><description>Synthetic data has become a cornerstone for scaling large language models,yet its multilingual use remains bottlenecked by translation-based prompts.This strategy inherits English-centric framing and style and neglects culturaldimensions, ultimately constraining model generalization. We argue that theoverlooked prompt space-the very inputs that define trainingdistributions-offers a more powerful lever for improving multilingualperformance. We introduce a lightweight framework for prompt-spaceoptimization, where translated prompts are systematically transformed forNaturalness, Cultural Adaptation, and Difficulty Enhancement. Using anoff-the-shelf multilingual LLM, we apply these transformations to prompts for12 languages spanning 7 families. Under identical data conditions, ourapproaches achieve substantial and consistent downstream improvements over thetranslation-only baseline: +4.7% on Global-MMLU accuracy, +2.4% on FloresXCometXL and +35.3% wins in preferences on mArenaHard. We establishprompt-space optimization as a simple yet powerful paradigm for buildingmultilingual LLMs that are more robust, culturally grounded, and globallycapable.</description><author>David Mora, Viraat Aryabumi, Wei-Yin Ko, Sara Hooker, Julia Kreutzer, Marzieh Fadaee</author><pubDate>Wed, 22 Oct 2025 17:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19806v1</guid></item><item><title>Class-Aware Prototype Learning with Negative Contrast for Test-Time Adaptation of Vision-Language Models</title><link>http://arxiv.org/abs/2510.19802v1</link><description>Vision-Language Models (VLMs) demonstrate impressive zero-shot generalizationthrough large-scale image-text pretraining, yet their performance can drop oncethe deployment distribution diverges from the training distribution. To addressthis, Test-Time Adaptation (TTA) methods update models using unlabeled targetdata. However, existing approaches often ignore two key challenges: prototypedegradation in long-tailed distributions and confusion between semanticallysimilar classes. To tackle these issues, we propose \textbf{C}lass-Aware\textbf{P}rototype \textbf{L}earning with \textbf{N}egative\textbf{C}ontrast(\textbf{CPL-NC}), a lightweight TTA framework designedspecifically for VLMs to enhance generalization under distribution shifts.CPL-NC introduces a \textit{Class-Aware Prototype Cache} Module thatdynamically adjusts per-class capacity based on test-time frequency andactivation history, with a rejuvenation mechanism for inactive classes toretain rare-category knowledge. Additionally, a \textit{Negative ContrastiveLearning} Mechanism identifies and constrains hard visual-textual negatives toimprove class separability. The framework employs asymmetric optimization,refining only textual prototypes while anchoring on stable visual features.Experiments on 15 benchmarks show that CPL-NC consistently outperforms priorTTA methods across both ResNet-50 and ViT-B/16 backbones.</description><author>Xiaozhen Qiao, Jingkai Zhao, Yuqiu Jiang, Xianda Guo, Zhe Sun, Hongyuan Zhang, Xuelong Li</author><pubDate>Wed, 22 Oct 2025 17:38:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19802v1</guid></item><item><title>The Feasibility of Training Sovereign Language Models in the Global South: A Study of Brazil and Mexico</title><link>http://arxiv.org/abs/2510.19801v1</link><description>The rapid escalation of computational requirements for training large-scalelanguage models has reinforced structural asymmetries between high-capacityjurisdictions and countries in the Global South. This paper examines thetechnical and fiscal feasibility of sovereign-scale language model training inBrazil and Mexico under conditions of constrained hardware access, energyavailability, and fiscal ceilings. Using a dual-axis design that variesaccelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150days), we estimate compute demand, energy consumption, capital expenditures,and regulatory compatibility for the training of a 10-trillion-token model. Ourfindings show that while all configurations remain below export-control andelectrical infrastructure thresholds, fiscal viability is determined byhardware efficiency. H100-based scenarios achieve training feasibility at atotal cost of 8-14 million USD, while A100 deployments require 19-32 millionUSD due to higher energy and hardware demand. We argue that extending trainingtimelines should be treated as a policy lever to mitigate hardware constraints,enabling the production of usable, auditable, and locally aligned modelswithout competing at the global frontier. This study contributes to thediscourse on AI compute governance and technological sovereignty byhighlighting context-sensitive strategies that allow middle-income countries toestablish sustainable and strategically sufficient AI capabilities.</description><author>Sandra Malagon, Monica A. Ulloa Ruiz, Tatiana Elizabeth Sandoval Plaza, Gabriel Rafael Rosario Bolívar, Valentina García Mesa, Ivanna Alvarado Morales</author><pubDate>Wed, 22 Oct 2025 17:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19801v1</guid></item><item><title>Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation</title><link>http://arxiv.org/abs/2510.19799v1</link><description>Public and nonprofit organizations often hesitate to adopt AI tools becausemost models are opaque even though standard approaches typically analyzeaggregate patterns rather than offering actionable, case-level guidance. Thisstudy tests a practitioner-in-the-loop workflow that pairs transparentdecision-tree models with large language models (LLMs) to improve predictiveaccuracy, interpretability, and the generation of practical insights. Usingdata from an ongoing college-success program, we build interpretable decisiontrees to surface key predictors. We then provide each tree's structure to anLLM, enabling it to reproduce case-level predictions grounded in thetransparent models. Practitioners participate throughout feature engineering,model design, explanation review, and usability assessment, ensuring that fieldexpertise informs the analysis at every stage. Results show that integratingtransparent models, LLMs, and practitioner input yields accurate, trustworthy,and actionable case-level evaluations, offering a viable pathway forresponsible AI adoption in the public and nonprofit sectors.</description><author>Ji Ma, Albert Casella</author><pubDate>Wed, 22 Oct 2025 17:35:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19799v1</guid></item><item><title>Transformers are almost optimal metalearners for linear classification</title><link>http://arxiv.org/abs/2510.19797v1</link><description>Transformers have demonstrated impressive in-context learning (ICL)capabilities, raising the question of whether they can serve as metalearnersthat adapt to new tasks using only a small number of in-context examples,without any further training. While recent theoretical work has studiedtransformers' ability to perform ICL, most of these analyses do not address theformal metalearning setting, where the objective is to solve a collection ofrelated tasks more efficiently than would be possible by solving each taskindividually. In this paper, we provide the first theoretical analysis showingthat a simplified transformer architecture trained via gradient descent can actas a near-optimal metalearner in a linear classification setting. We consider anatural family of tasks where each task corresponds to a class-conditionalGaussian mixture model, with the mean vectors lying in a shared $k$-dimensionalsubspace of $R^d$. After training on a sufficient number of such tasks, we showthat the transformer can generalize to a new task using only $O(k / R^4)$in-context examples, where $R$ denotes the signal strength at test time. Thisperformance (almost) matches that of an optimal learner that knows exactly theshared subspace and significantly outperforms any learner that only has accessto the in-context data, which requires $\Omega(d / R^4)$ examples togeneralize. Importantly, our bounds on the number of training tasks andexamples per task needed to achieve this result are independent of the ambientdimension $d$.</description><author>Roey Magen, Gal Vardi</author><pubDate>Wed, 22 Oct 2025 17:32:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19797v1</guid></item><item><title>Blackbox Model Provenance via Palimpsestic Membership Inference</title><link>http://arxiv.org/abs/2510.19796v1</link><description>Suppose Alice trains an open-weight language model and Bob uses a blackboxderivative of Alice's model to produce text. Can Alice prove that Bob is usingher model, either by querying Bob's derivative model (query setting) or fromthe text alone (observational setting)? We formulate this question as anindependence testing problem--in which the null hypothesis is that Bob's modelor text is independent of Alice's randomized training run--and investigate itthrough the lens of palimpsestic memorization in language models: models aremore likely to memorize data seen later in training, so we can test whether Bobis using Alice's model using test statistics that capture correlation betweenBob's model or text and the ordering of training examples in Alice's trainingrun. If Alice has randomly shuffled her training data, then any significantcorrelation amounts to exactly quantifiable statistical evidence against thenull hypothesis, regardless of the composition of Alice's training data. In thequery setting, we directly estimate (via prompting) the likelihood Bob's modelgives to Alice's training examples and order; we correlate the likelihoods ofover 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to12B parameters with the base model's training data order, achieving a p-valueon the order of at most 1e-8 in all but six cases. In the observationalsetting, we try two approaches based on estimating 1) the likelihood of Bob'stext overlapping with spans of Alice's training examples and 2) the likelihoodof Bob's text with respect to different versions of Alice's model we obtain byrepeating the last phase (e.g., 1%) of her training run on reshuffled data. Thesecond approach can reliably distinguish Bob's text from as little as a fewhundred tokens; the first does not involve any retraining but requires manymore tokens (several hundred thousand) to achieve high power.</description><author>Rohith Kuditipudi, Jing Huang, Sally Zhu, Diyi Yang, Christopher Potts, Percy Liang</author><pubDate>Wed, 22 Oct 2025 17:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19796v1</guid></item><item><title>On Controlled Change: Generative AI's Impact on Professional Authority in Journalism</title><link>http://arxiv.org/abs/2510.19792v1</link><description>Using (generative) artificial intelligence tools and systems in journalism isexpected to increase journalists' production rates, transform newsrooms'economic models, and further personalize the audience's news consumptionpractices. Since its release in 2022, OpenAI's ChatGPT and other large languagemodels have raised the alarms inside news organizations, not only for bringingnew challenges to news reporting and fact-checking but also for what thesetechnologies would mean for journalists' professional authority in journalism.This paper examines how journalists in Dutch media manage the integration of AItechnologies into their daily routines. Drawing from 13 interviews witheditors, journalists, and innovation managers in different news outlets andmedia companies, we propose the concept of controlled change. as a heuristic toexplain how journalists are proactively setting guidelines, experimenting withAI tools, and identifying their limitations and capabilities. Usingprofessional authority as a theoretical framework, we argue that journalistsanticipate and integrate AI technologies in a supervised manner and identifythree primary mechanisms through which journalists manage this integration: (1)developing adaptive guidelines that align AI use with ethical codes, (2)experimenting with AI technologies to determine their necessity and fit, and(3) critically assessing the capabilities and limitations of AI systems.</description><author>Tomás Dodds, Wang Ngai Yeung, Claudia Mellado, Mathias-Felipe de Lima-Santos</author><pubDate>Wed, 22 Oct 2025 17:27:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19792v1</guid></item><item><title>ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers</title><link>http://arxiv.org/abs/2510.19791v1</link><description>Tool calling has become increasingly popular for Large Language Models(LLMs). However, for large tool sets, the resulting tokens would exceed theLLM's context window limit, making it impossible to include every tool. Hence,an external retriever is used to provide LLMs with the most relevant tools fora query. Existing retrieval models rank tools based on the similarity between auser query and a tool description (TD). This leads to suboptimal retrieval asuser requests are often poorly aligned with the language of TD. To remedy theissue, we propose ToolDreamer, a framework to condition retriever models tofetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,description of tools that the LLM feels will be potentially useful for thequery. The framework enables a more natural alignment between queries and toolswithin the language space of TD's. We apply ToolDreamer on the ToolRet datasetand show that our method improves the performance of sparse and denseretrievers with and without training, thus showcasing its flexibility. Throughour proposed framework, our aim is to offload a portion of the reasoning burdento the retriever so that the LLM may effectively handle a large collection oftools without inundating its context window.</description><author>Saptarshi Sengupta, Zhengyu Zhou, Jun Araki, Xingbo Wang, Bingqing Wang, Suhang Wang, Zhe Feng</author><pubDate>Wed, 22 Oct 2025 17:26:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19791v1</guid></item><item><title>OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</title><link>http://arxiv.org/abs/2510.19789v1</link><description>This paper introduces OmniMotion-X, a versatile multimodal framework forwhole-body human motion generation, leveraging an autoregressive diffusiontransformer in a unified sequence-to-sequence manner. OmniMotion-X efficientlysupports diverse multimodal tasks, including text-to-motion, music-to-dance,speech-to-gesture, and global spatial-temporal control scenarios (e.g., motionprediction, in-betweening, completion, and joint/trajectory-guided synthesis),as well as flexible combinations of these tasks. Specifically, we propose theuse of reference motion as a novel conditioning signal, substantially enhancingthe consistency of generated content, style, and temporal dynamics crucial forrealistic animations. To handle multimodal conflicts, we introduce aprogressive weak-to-strong mixed-condition training strategy. To enablehigh-quality multimodal training, we construct OmniMoCap-X, the largest unifiedmultimodal motion dataset to date, integrating 28 publicly available MoCapsources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps.To ensure detailed and consistent annotations, we render sequences into videosand use GPT-4o to automatically generate structured and hierarchical captions,capturing both low-level actions and high-level semantics. Extensiveexperimental evaluations confirm that OmniMotion-X significantly surpassesexisting methods, demonstrating state-of-the-art performance across multiplemultimodal tasks and enabling the interactive generation of realistic,coherent, and controllable long-duration motions.</description><author>Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu</author><pubDate>Wed, 22 Oct 2025 17:25:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19789v1</guid></item><item><title>Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR</title><link>http://arxiv.org/abs/2508.00744v2</link><description>Recent advancements in LiDAR-based 3D object detection have significantlyaccelerated progress toward the realization of fully autonomous driving inreal-world environments. Despite achieving high detection performance, most ofthe approaches still rely on a VGG-based or ResNet-based backbone for featureexploration, which increases the model complexity. Lightweight backbone designis well-explored for 2D object detection, but research on 3D object detectionstill remains limited. In this work, we introduce Dense Backbone, a lightweightbackbone that combines the benefits of high processing speed, lightweightarchitecture, and robust detection accuracy. We adapt multiple SoTA 3d objectdetectors, such as PillarNet, with our backbone and show that with ourbackbone, these models retain most of their detection capability at asignificantly reduced computational cost. To our knowledge, this is the firstdense-layer-based backbone tailored specifically for 3D object detection frompoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%reduction in model parameters and a 28% reduction in latency with just a 2%drop in detection accuracy on the nuScenes test set. Furthermore, DenseBackbone's plug-and-play design allows straightforward integration intoexisting architectures, requiring no modifications to other network components.</description><author>Adwait Chandorkar, Hasan Tercan, Tobias Meisen</author><pubDate>Wed, 22 Oct 2025 17:25:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.00744v2</guid></item><item><title>Benchmarking World-Model Learning</title><link>http://arxiv.org/abs/2510.19788v1</link><description>Model-learning agents should gather information to learn world models thatsupport many downstream tasks and inferences, such as predicting unobservedstates, estimating near- and far-term consequences of actions, planning actionsequences, and detecting changes in dynamics. Current methods for learning andevaluating world models diverge from this goal: training and evaluation areanchored to next-frame prediction, and success is scored by reward maximizationin the same environment. We propose WorldTest, a protocol to evaluatemodel-learning agents that separates reward-free interaction from a scored testphase in a different but related environment. WorldTest isopen-ended$\unicode{x2014}$models should support many different tasks unknownahead of time$\unicode{x2014}$and agnostic to model representation, allowingcomparison across approaches. We instantiated WorldTest with AutumnBench, asuite of 43 interactive grid-world environments and 129 tasks across threefamilies: masked-frame prediction, planning, and predicting changes to thecausal dynamics. We compared 517 human participants and three frontier modelson AutumnBench. We found that humans outperform the models, and scaling computeimproves performance only in some environments but not others. WorldTestprovides a novel template$\unicode{x2014}$reward-free exploration, derivedtests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learnabout environment dynamics, and AutumnBench exposes significant headroom inworld-model learning.</description><author>Archana Warrier, Dat Nyugen, Michelangelo Naim, Moksh Jain, Yichao Liang, Karen Schroeder, Cambridge Yang, Joshua B. Tenenbaum, Sebastian Vollmer, Kevin Ellis, Zenna Tavares</author><pubDate>Wed, 22 Oct 2025 17:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19788v1</guid></item><item><title>Provably Efficient Reward Transfer in Reinforcement Learning with Discrete Markov Decision Processes</title><link>http://arxiv.org/abs/2503.13414v3</link><description>In this paper, we propose a new solution to reward adaptation (RA) inreinforcement learning, where the agent adapts to a target reward functionbased on one or more existing source behaviors learned a priori under the samedomain dynamics but different reward functions. While learning the targetbehavior from scratch is possible, it is often inefficient given the availablesource behaviors. Our work introduces a new approach to RA through themanipulation of Q-functions. Assuming the target reward function is a knownfunction of the source reward functions, we compute bounds on the Q-functionand present an iterative process (akin to value iteration) to tighten thesebounds. Such bounds enable action pruning in the target domain before learningeven starts. We refer to this method as "Q-Manipulation" (Q-M). The iterationprocess assumes access to a lite-model, which is easy to provide or learn. Weformally prove that Q-M, under discrete domains, does not affect the optimalityof the returned policy and show that it is provably efficient in terms ofsample complexity in a probabilistic sense. Q-M is evaluated in a variety ofsynthetic and simulation domains to demonstrate its effectiveness,generalizability, and practicality.</description><author>Kevin Vora, Yu Zhang</author><pubDate>Wed, 22 Oct 2025 17:22:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.13414v3</guid></item><item><title>Environment Inference for Learning Generalizable Dynamical System</title><link>http://arxiv.org/abs/2510.19784v1</link><description>Data-driven methods offer efficient and robust solutions for analyzingcomplex dynamical systems but rely on the assumption of I.I.D. data, drivingthe development of generalization techniques for handling environmentaldifferences. These techniques, however, are limited by their dependence onenvironment labels, which are often unavailable during training due to dataacquisition challenges, privacy concerns, and environmental variability,particularly in large public datasets and privacy-sensitive domains. Inresponse, we propose DynaInfer, a novel method that infers environmentspecifications by analyzing prediction errors from fixed neural networks withineach training round, enabling environment assignments directly from data. Weprove our algorithm effectively solves the alternating optimization problem inunlabeled scenarios and validate it through extensive experiments acrossdiverse dynamical systems. Results show that DynaInfer outperforms existingenvironment assignment techniques, converges rapidly to true labels, and evenachieves superior performance when environment labels are available.</description><author>Shixuan Liu, Yue He, Haotian Wang, Wenjing Yang, Yunfei Wang, Peng Cui, Zhong Liu</author><pubDate>Wed, 22 Oct 2025 17:20:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19784v1</guid></item><item><title>QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training</title><link>http://arxiv.org/abs/2506.00711v2</link><description>Clinical decision-making routinely demands reasoning over heterogeneous data,yet existing multimodal language models (MLLMs) remain largely vision-centricand fail to generalize across clinical specialties. To bridge this gap, weintroduce QoQ-Med-7B/32B, the first open generalist clinical foundation modelthat jointly reasons across medical images, time-series signals, and textreports. QoQ-Med is trained with Domain-aware Relative Policy Optimization(DRPO), a novel reinforcement-learning objective that hierarchically scalesnormalized rewards according to domain rarity and modality difficulty,mitigating performance imbalance caused by skewed clinical data distributions.Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains,we show that DRPO training boosts diagnostic performance by 43% in macro-F1 onaverage across all visual domains as compared to other critic-free trainingmethods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentationdata, it is able to highlight salient regions related to the diagnosis, with anIoU 10x higher than open models while reaching the performance of OpenAIo4-mini. To foster reproducibility and downstream research, we release (i) thefull model weights, (ii) the modular training pipeline, and (iii) allintermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.</description><author>Wei Dai, Peilin Chen, Chanakya Ekbote, Paul Pu Liang</author><pubDate>Wed, 22 Oct 2025 17:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.00711v2</guid></item><item><title>Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience</title><link>http://arxiv.org/abs/2508.00596v3</link><description>In decentralized federated learning (FL), multiple clients collaborativelylearn a shared machine learning (ML) model by leveraging their privately helddatasets distributed across the network, through interactive exchange of theintermediate model updates. To ensure data security, cryptographic techniquesare commonly employed to protect model updates during aggregation. Despitegrowing interest in secure aggregation, existing works predominantly focus onprotocol design and computational guarantees, with limited understanding of thefundamental information-theoretic limits of such systems. Moreover, optimalbounds on communication and key usage remain unknown in decentralized settings,where no central aggregator is available. Motivated by these gaps, we study theproblem of decentralized secure aggregation (DSA) from an information-theoreticperspective. Specifically, we consider a network of $K$ fully-connected users,each holding a private input -- an abstraction of local training data -- whoaim to securely compute the sum of all inputs. The security constraint requiresthat no user learns anything beyond the input sum, even when colluding with upto $T$ other users. We characterize the optimal rate region, which specifiesthe minimum achievable communication and secret key rates for DSA. Inparticular, we show that to securely compute one symbol of the desired inputsum, each user must (i) transmit at least one symbol to others, (ii) hold atleast one symbol of secret key, and (iii) all users must collectively hold nofewer than $K - 1$ independent key symbols. Our results establish thefundamental performance limits of DSA, providing insights for the design ofprovably secure and communication-efficient protocols in distributed learningsystems.</description><author>Xiang Zhang, Zhou Li, Shuangyang Li, Kai Wan, Derrick Wing Kwan Ng, Giuseppe Caire</author><pubDate>Wed, 22 Oct 2025 17:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.00596v3</guid></item><item><title>Adapting Multilingual Models to Code-Mixed Tasks via Model Merging</title><link>http://arxiv.org/abs/2510.19782v1</link><description>We study model merging as a practical alternative to conventional adaptationstrategies for code-mixed NLP. Starting from a multilingual base model, we: (i)perform continued pre-training (CPT) on unlabeled code-mixed text to obtain anadapted checkpoint, (ii) merge checkpoint with the base model, and (iii)fine-tune (FT) on the downstream task data. We evaluate our approach forsentence classification (sentiment and hate speech) task in English-Hindi(En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Ourresults show that merged models consistently outperform full fine-tuning andCPT-&gt;FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2points over CPT-&gt;FT, indicating that unlabeled data is leveraged moreeffectively via merging than via CPT alone. Zero-/few-shot prompting withlarger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and mergedcheckpoints, underscoring limits of in-context learning for code-mixed inputs.We further test cross-pair transfer by training on En-Hi and evaluating onEn-Ta and En-Ml: merged checkpoints transfer more strongly thanmonolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a morereliable substrate for low-resource pairs. We conclude with adaptation recipesmatched to common data regimes (labeled only; labeled+unlabeled; transfer-only)and discuss limitations and scaling considerations for broader tasks and largermodels.</description><author>Prashant Kodali, Vaishnavi Shivkumar, Swarang Joshi, Monojit Choudhary, Ponnurangam Kumaraguru, Manish Shrivastava</author><pubDate>Wed, 22 Oct 2025 17:16:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19782v1</guid></item><item><title>Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents</title><link>http://arxiv.org/abs/2506.08800v2</link><description>Data science aims to extract insights from data to support decision-makingprocesses. Recently, Large Language Models (LLMs) have been increasingly usedas assistants for data science, by suggesting ideas, techniques and small codesnippets, or for the interpretation of results and reporting. Proper automationof some data-science activities is now promised by the rise of LLM agents,i.e., AI systems powered by an LLM equipped with additional affordances--suchas code execution and knowledge bases--that can perform self-directed actionsand interact with digital environments. In this paper, we survey the evaluationof LLM assistants and agents for data science. We find (1) a dominant focus ona small subset of goal-oriented activities, largely ignoring data managementand exploratory activities; (2) a concentration on pure assistance or fullyautonomous agents, without considering intermediate levels of human-AIcollaboration; and (3) an emphasis on human substitution, therefore neglectingthe possibility of higher levels of automation thanks to task transformation.</description><author>Irene Testini, José Hernández-Orallo, Lorenzo Pacchiardi</author><pubDate>Wed, 22 Oct 2025 17:14:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.08800v2</guid></item><item><title>AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</title><link>http://arxiv.org/abs/2510.19779v1</link><description>Speculative Decoding (SD) accelerates large language model inference byemploying a small draft model to generate predictions, which are then verifiedby a larger target model. The effectiveness of SD hinges on the alignmentbetween these models, which is typically enhanced by Knowledge Distillation(KD). However, conventional KD methods aim to minimize the KL divergencebetween the draft and target models across all tokens, a goal that ismisaligned with the true objective of SD, which is to maximize token acceptancerate. Therefore, draft models often struggle to fully assimilate the targetmodel's knowledge due to capacity constraints, leading to suboptimalperformance. To address this challenge, we propose AdaSPEC, a novel method thatincorporates selective token filtering into the KD process. AdaSPEC utilizes areference model to identify and filter out difficult-to-fit tokens, enablingthe distillation of a draft model that better aligns with the target model onsimpler tokens. This approach improves the overall token acceptance ratewithout compromising generation quality. We evaluate AdaSPEC across diversetasks, including arithmetic reasoning, instruction-following, coding, andsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.Our results demonstrate that AdaSPEC consistently outperforms thestate-of-the-art DistillSpec method, achieving higher acceptance rates acrossall tasks (up to 15\%). The code is publicly available athttps://github.com/yuezhouhu/adaspec.</description><author>Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao</author><pubDate>Wed, 22 Oct 2025 17:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19779v1</guid></item><item><title>GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters</title><link>http://arxiv.org/abs/2510.19778v1</link><description>Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning asparse subset of model parameters. However, the effectiveness of sparseadaptation depends on optimally selecting the model parameters to befine-tuned. In this work, we introduce a novel sparse fine-tuning techniquenamed GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, whichfine-tunes only those model parameters which have the largest gradientmagnitudes on downstream tasks and the smallest pre-trained magnitudes,intuitively prioritizing parameters that are highly task-relevant, butminimally disruptive to pre-trained knowledge. Our experimentation with LLaMA38B and Gemma 2B as base models shows that GaLLoP consistently improves ormatches the in-distribution as well as out-of-distribution performance obtainedvia the usage of other leading parameter-efficient fine-tuning techniques,including LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigatescatastrophic forgetting and memorization of task data, as important pre-trainedparameters remain unchanged, and stabilizes performance relative to otherfine-tuning techniques, robustly generalizing across most random seeds.</description><author>Anand Choudhary, Yasser Sulaıman, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux, Antoine Bosselut</author><pubDate>Wed, 22 Oct 2025 17:11:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19778v1</guid></item><item><title>Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics</title><link>http://arxiv.org/abs/2506.06045v2</link><description>Graph-based learned simulators have emerged as a promising approach forsimulating physical systems on unstructured meshes, offering speed andgeneralization across diverse geometries. However, they often struggle withcapturing global phenomena, such as bending or long-range correlations usuallyoccurring in solid mechanics, and suffer from error accumulation over longrollouts due to their reliance on local message passing and direct next-stepprediction. We address these limitations by introducing the RollingDiffusion-Batched Inference Network (ROBIN), a novel learned simulator thatintegrates two key innovations: (i) Rolling Diffusion-Batched Inference (ROBI),a parallelized inference scheme that amortizes the cost of diffusion-basedrefinement across physical time steps by overlapping denoising steps across atemporal window. (ii) A Hierarchical Graph Neural Network built on algebraicmultigrid coarsening, enabling multiscale message passing across different meshresolutions. This architecture, implemented via Algebraic-hierarchical MessagePassing Networks, captures both fine-scale local dynamics and global structuraleffects critical for phenomena like beam bending or multi-body contact. Wevalidate ROBIN on challenging 2D and 3D solid mechanics benchmarks involvinggeometric, material, and contact nonlinearities. ROBIN achievesstate-of-the-art accuracy on all tasks, substantially outperforming existingnext-step learned simulators while reducing inference time by up to an order ofmagnitude compared to standard diffusion simulators.</description><author>Tobias Würth, Niklas Freymuth, Gerhard Neumann, Luise Kärger</author><pubDate>Wed, 22 Oct 2025 17:09:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.06045v2</guid></item><item><title>Online Conformal Prediction with Efficiency Guarantees</title><link>http://arxiv.org/abs/2507.02496v2</link><description>We study the problem of conformal prediction in a novel online framework thatdirectly optimizes efficiency. In our problem, we are given a targetmiscoverage rate $\alpha &gt; 0$, and a time horizon $T$. On each day $t \le T$ analgorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, whilemaintaining efficiency, that is, minimizing the average volume (length) of theintervals played. This problem is an online analogue to the problem ofconstructing efficient confidence intervals. We study this problem over arbitrary and exchangeable (random order) inputsequences. For exchangeable sequences, we show that it is possible to constructintervals that achieve coverage $(1 - \alpha) - o(1)$, while having lengthupper bounded by the best fixed interval that achieves coverage in hindsight.For arbitrary sequences however, we show that any algorithm that achieves a$\mu$-approximation in average length compared to the best fixed intervalachieving coverage in hindsight, must make a multiplicative factor moremistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ andthe aspect ratio of the problem. Our main algorithmic result is a matchingalgorithm that can recover all Pareto-optimal settings of $\mu$ and number ofmistakes. Furthermore, our algorithm is deterministic and therefore robust toan adaptive adversary. This gap between the exchangeable and arbitrary settings is in contrast tothe classical online learning problem. In fact, we show that no singlealgorithm can simultaneously be Pareto-optimal for arbitrary sequences andoptimal for exchangeable sequences. On the algorithmic side, we give analgorithm that achieves the near-optimal tradeoff between the two cases.</description><author>Vaidehi Srinivas</author><pubDate>Wed, 22 Oct 2025 17:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.02496v2</guid></item><item><title>The Tail Tells All: Estimating Model-Level Membership Inference Vulnerability Without Reference Models</title><link>http://arxiv.org/abs/2510.19773v1</link><description>Membership inference attacks (MIAs) have emerged as the standard tool forevaluating the privacy risks of AI models. However, state-of-the-art attacksrequire training numerous, often computationally expensive, reference models,limiting their practicality. We present a novel approach for estimatingmodel-level vulnerability, the TPR at low FPR, to membership inference attackswithout requiring reference models. Empirical analysis shows loss distributionsto be asymmetric and heavy-tailed and suggests that most points at risk fromMIAs have moved from the tail (high-loss region) to the head (low-loss region)of the distribution after training. We leverage this insight to propose amethod to estimate model-level vulnerability from the training and testingdistribution alone: using the absence of outliers from the high-loss region asa predictor of the risk. We evaluate our method, the TNR of a simple lossattack, across a wide range of architectures and datasets and show it toaccurately estimate model-level vulnerability to the SOTA MIA attack (LiRA). Wealso show our method to outperform both low-cost (few reference models) attackssuch as RMIA and other measures of distribution difference. We finally evaluatethe use of non-linear functions to evaluate risk and show the approach to bepromising to evaluate the risk in large-language models.</description><author>Euodia Dodd, Nataša Krčo, Igor Shilov, Yves-Alexandre de Montjoye</author><pubDate>Wed, 22 Oct 2025 17:03:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19773v1</guid></item><item><title>Training-Free Constrained Generation With Stable Diffusion Models</title><link>http://arxiv.org/abs/2502.05625v4</link><description>Stable diffusion models represent the state-of-the-art in data synthesisacross diverse domains and hold transformative potential for applications inscience and engineering, e.g., by facilitating the discovery of novel solutionsand simulating systems that are computationally intractable to modelexplicitly. While there is increasing effort to incorporate physics-basedconstraints into generative models, existing techniques are either limited intheir applicability to latent diffusion frameworks or lack the capability tostrictly enforce domain-specific constraints. To address this limitation thispaper proposes a novel integration of stable diffusion models with constrainedoptimization frameworks, enabling the generation of outputs satisfyingstringent physical and functional requirements. The effectiveness of thisapproach is demonstrated through material design experiments requiringadherence to precise morphometric properties, challenging inverse design tasksinvolving the generation of materials inducing specific stress-strainresponses, and copyright-constrained content generation tasks. All code hasbeen released athttps://github.com/RAISELab-atUVA/Constrained-Stable-Diffusion.</description><author>Stefano Zampini, Jacob K. Christopher, Luca Oneto, Davide Anguita, Ferdinando Fioretto</author><pubDate>Wed, 22 Oct 2025 17:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.05625v4</guid></item><item><title>Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents</title><link>http://arxiv.org/abs/2510.19771v1</link><description>LLM-based agents are increasingly moving towards proactivity: rather thanawaiting instruction, they exercise agency to anticipate user needs and solvethem autonomously. However, evaluating proactivity is challenging; currentbenchmarks are constrained to localized context, limiting their ability to testreasoning across sources and longer time horizons. To address this gap, wepresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposesproactivity as a pipeline of three core capabilities: (1) searching forunspecified issues, (2) identifying specific bottlenecks, and (3) executingappropriate resolutions. We apply PROBE to evaluate leading LLMs and popularagentic frameworks, showing that even state-of-the-art models struggle to solvethis benchmark. Computing our consistent measurements across frontier LLMs andagents, we find that the best end-to-end performance of 40% is achieved by bothGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relativecapabilities of each model and analyze mutual failure modes. Our resultshighlight the current limitations of autonomous action in agentic systems, andexpose promising future research directions.</description><author>Gil Pasternak, Dheeraj Rajagopal, Julia White, Dhruv Atreja, Matthew Thomas, George Hurn-Maloney, Ash Lewis</author><pubDate>Wed, 22 Oct 2025 17:00:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19771v1</guid></item><item><title>Variable Rate Image Compression via N-Gram Context based Swin-transformer</title><link>http://arxiv.org/abs/2510.00058v2</link><description>This paper presents an N-gram context-based Swin Transformer for learnedimage compression. Our method achieves variable-rate compression with a singlemodel. By incorporating N-gram context into the Swin Transformer, we overcomeits limitation of neglecting larger regions during high-resolution imagereconstruction due to its restricted receptive field. This enhancement expandsthe regions considered for pixel restoration, thereby improving the quality ofhigh-resolution reconstructions. Our method increases context awareness acrossneighboring windows, leading to a -5.86\% improvement in BD-Rate over existingvariable-rate learned image compression techniques. Additionally, our modelimproves the quality of regions of interest (ROI) in images, making itparticularly beneficial for object-focused applications in fields such asmanufacturing and industrial vision systems.</description><author>Priyanka Mudgal</author><pubDate>Wed, 22 Oct 2025 16:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.00058v2</guid></item><item><title>SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration</title><link>http://arxiv.org/abs/2510.19767v1</link><description>The long chain-of-thought (LongCoT) capability is central to the recentbreakthroughs achieved by large language models in complex reasoning tasks.However, the accompanying issue of ''underthinking'', where models exhibitshallow reasoning by frequently switching thoughts without sufficientexploration, limits both performance and token efficiency. To address thisproblem, we propose a simple yet effective reasoning strategy: the SmartSwitchinference framework. This framework can be easily integrated into any largelanguage model as a plug-and-play solution, continuously monitoring the model'sreasoning process to detect underthinking and guide it toward deeperexploration of promising but overlooked thoughts. Specifically, the perceptionmodule identifies points where thoughts switch and evaluates the potential ofthe preceding thought using an off-the-shelf process reward model (PRM). If ahigh-potential thought is found to be prematurely abandoned, the interventionmodule interrupts the ongoing inference, backtracks to the point before theswitch, and inserts a "deepening prompt" to encourage further exploration alongthat promising path. Extensive experiments on challenging mathematicalreasoning benchmarks demonstrate that our method significantly enhances theperformance of various large language models of different sizes.</description><author>Xichen Zhang, Sitong Wu, Haoru Tan, Shaozuo Yu, Yinghao Zhu, Ziyi He, Jiaya Jia</author><pubDate>Wed, 22 Oct 2025 16:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19767v1</guid></item><item><title>gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity</title><link>http://arxiv.org/abs/2510.08450v2</link><description>Graph Neural Networks (GNNs) leverage the graph structure to transmitinformation between nodes, typically through the message-passing mechanism.While these models have found a wide variety of applications, they are known tosuffer from over-squashing, where information from a large receptive field ofnode representations is collapsed into a single fixed sized vector, resultingin an information bottleneck. In this paper, we re-examine the over-squashingphenomenon through the lens of model storage and retrieval capacity, which wedefine as the amount of information that can be stored in a node'srepresentation for later use. We study some of the limitations of existingtasks used to measure over-squashing and introduce a new synthetic task todemonstrate that an information bottleneck can saturate this capacity.Furthermore, we adapt ideas from the sequence modeling literature onassociative memories, fast weight programmers, and the xLSTM model to develop anovel GNN architecture with improved capacity. We demonstrate strongperformance of this architecture both on our capacity synthetic task, as wellas a range of real-world graph benchmarks.</description><author>Hugh Blayney, Álvaro Arroyo, Xiaowen Dong, Michael M. Bronstein</author><pubDate>Wed, 22 Oct 2025 16:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.08450v2</guid></item><item><title>A flexible framework for structural plasticity in GPU-accelerated sparse spiking neural networks</title><link>http://arxiv.org/abs/2510.19764v1</link><description>The majority of research in both training Artificial Neural Networks (ANNs)and modeling learning in biological brains focuses on synaptic plasticity,where learning equates to changing the strength of existing connections.However, in biological brains, structural plasticity - where new connectionsare created and others removed - is also vital, not only for effective learningbut also for recovery from damage and optimal resource usage. Inspired bystructural plasticity, pruning is often used in machine learning to remove weakconnections from trained models to reduce the computational requirements ofinference. However, the machine learning frameworks typically used forbackpropagation-based training of both ANNs and Spiking Neural Networks (SNNs)are optimized for dense connectivity, meaning that pruning does not help reducethe training costs of ever-larger models. The GeNN simulator already supportsefficient GPU-accelerated simulation of sparse SNNs for computationalneuroscience and machine learning. Here, we present a new flexible frameworkfor implementing GPU-accelerated structural plasticity rules and demonstratethis first using the e-prop supervised learning rule and DEEP R to trainefficient, sparse SNN classifiers and then, in an unsupervised learningcontext, to learn topographic maps. Compared to baseline dense models, oursparse classifiers reduce training time by up to 10x while the DEEP R rewiringenables them to perform as well as the original models. We demonstratetopographic map formation in faster-than-realtime simulations, provide insightsinto the connectivity evolution, and measure simulation speed versus networksize. The proposed framework will enable further research into achieving andmaintaining sparsity in network structure and neural communication, as well asexploring the computational benefits of sparsity in a range of neuromorphicapplications.</description><author>James C. Knight, Johanna Senk, Thomas Nowotny</author><pubDate>Wed, 22 Oct 2025 16:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19764v1</guid></item><item><title>Phase-driven Domain Generalizable Learning for Nonstationary Time Series</title><link>http://arxiv.org/abs/2402.05960v2</link><description>Pattern recognition is a fundamental task in continuous sensing applications,but real-world scenarios often experience distribution shifts that necessitatelearning generalizable representations for such tasks. This challenge isexacerbated with time-series data, which also exhibit inherentnonstationarity--variations in statistical and spectral properties over time.In this work, we offer a fresh perspective on learning generalizablerepresentations for time-series classification by considering the phaseinformation of a signal as an approximate proxy for nonstationarity and proposea phase-driven generalizable representation learning framework for time-seriesclassification, PhASER. It consists of three key elements: 1) Hilberttransform-based augmentation, which diversifies nonstationarity whilepreserving task-specific discriminatory semantics, 2) separate magnitude-phaseencoding, viewing time-varying magnitude and phase as independent modalities,and 3) phase-residual feature broadcasting, integrating 2D phase features witha residual connection to the 1D signal representation, providing inherentregularization to improve distribution-invariant learning. Extensiveevaluations on five datasets from sleep-stage classification, human activityrecognition, and gesture recognition against 13 state-of-the-art baselinemethods demonstrate that PhASER consistently outperforms the best baselines byan average of 5% and up to 11% in some cases. Additionally, the principles ofPhASER can be broadly applied to enhance the generalizability of existingtime-series representation learning models.</description><author>Payal Mohapatra, Lixu Wang, Qi Zhu</author><pubDate>Wed, 22 Oct 2025 16:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05960v2</guid></item><item><title>Exploring the Effect of DNN Depth on Adversarial Attacks in Network Intrusion Detection Systems</title><link>http://arxiv.org/abs/2510.19761v1</link><description>Adversarial attacks pose significant challenges to Machine Learning (ML)systems and especially Deep Neural Networks (DNNs) by subtly manipulatinginputs to induce incorrect predictions. This paper investigates whetherincreasing the layer depth of deep neural networks affects their robustnessagainst adversarial attacks in the Network Intrusion Detection System (NIDS)domain. We compare the adversarial robustness of various deep neural networksacross both \ac{NIDS} and computer vision domains (the latter being widely usedin adversarial attack experiments). Our experimental results reveal that in theNIDS domain, adding more layers does not necessarily improve their performance,yet it may actually significantly degrade their robustness against adversarialattacks. Conversely, in the computer vision domain, adding more layers exhibitsa more modest impact on robustness. These findings can guide the development ofrobust neural networks for (NIDS) applications and highlight the uniquecharacteristics of network security domains within the (ML) landscape.</description><author>Mohamed ElShehaby, Ashraf Matrawy</author><pubDate>Wed, 22 Oct 2025 16:48:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19761v1</guid></item><item><title>Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks</title><link>http://arxiv.org/abs/2510.19760v1</link><description>Quantization-Aware Training (QAT) is a critical technique for deploying deepneural networks on resource-constrained devices. However, existing methodsoften face two major challenges: the highly non-uniform distribution ofactivations and the static, mismatched codebooks used in weight quantization.To address these challenges, we propose Adaptive Distribution-awareQuantization (ADQ), a mixed-precision quantization framework that employs adifferentiated strategy. The core of ADQ is a novel adaptive weightquantization scheme comprising three key innovations: (1) a quantile-basedinitialization method that constructs a codebook closely aligned with theinitial weight distribution; (2) an online codebook adaptation mechanism basedon Exponential Moving Average (EMA) to dynamically track distributional shifts;and (3) a sensitivity-informed strategy for mixed-precision allocation. Foractivations, we integrate a hardware-friendly non-uniform-to-uniform mappingscheme. Comprehensive experiments validate the effectiveness of our method. OnImageNet, ADQ enables a ResNet-18 to achieve 71.512% Top-1 accuracy with anaverage bit-width of only 2.81 bits, outperforming state-of-the-art methodsunder comparable conditions. Furthermore, detailed ablation studies on CIFAR-10systematically demonstrate the individual contributions of each innovativecomponent, validating the rationale and effectiveness of our design.</description><author>Shaohang Jia, Zhiyong Huang, Zhi Yu, Mingyang Hou, Shuai Miao, Han Yang</author><pubDate>Wed, 22 Oct 2025 16:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19760v1</guid></item><item><title>Interpretable Features for the Assessment of Neurodegenerative Diseases through Handwriting Analysis</title><link>http://arxiv.org/abs/2409.08303v4</link><description>Motor dysfunction is a common sign of neurodegenerative diseases (NDs) suchas Parkinson's disease (PD) and Alzheimer's disease (AD), but may be difficultto detect, especially in the early stages. In this work, we examine thebehavior of a wide array of interpretable features extracted from thehandwriting signals of 113 subjects performing multiple tasks on a digitaltablet, as part of the Neurological Signals dataset. The aim is to measuretheir effectiveness in characterizing NDs, including AD and PD. To this end,task-agnostic and task-specific features are extracted from 14 distinct tasks.Subsequently, through statistical analysis and a series of classificationexperiments, we investigate which features provide greater discriminative powerbetween NDs and healthy controls and amongst different NDs. Preliminary resultsindicate that the tasks at hand can all be effectively leveraged to distinguishbetween the considered set of NDs, specifically by measuring the stability, thespeed of writing, the time spent not writing, and the pressure variationsbetween groups from our handcrafted interpretable features, which shows astatistically significant difference between groups, across multiple tasks.Using various binary classification algorithms on the computed features, weobtain up to 87% accuracy for the discrimination between AD and healthycontrols (CTL), and up to 69% for the discrimination between PD and CTL.</description><author>Thomas Thebaud, Anna Favaro, Casey Chen, Gabrielle Chavez, Laureano Moro-Velazquez, Ankur Butala, Najim Dehak</author><pubDate>Wed, 22 Oct 2025 16:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08303v4</guid></item><item><title>A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</title><link>http://arxiv.org/abs/2510.19755v1</link><description>Diffusion Models have become a cornerstone of modern generative AI for theirexceptional generation quality and controllability. However, their inherent\textit{multi-step iterations} and \textit{complex backbone networks} lead toprohibitive computational overhead and generation latency, forming a majorbottleneck for real-time applications. Although existing accelerationtechniques have made progress, they still face challenges such as limitedapplicability, high training costs, or quality degradation. Against this backdrop, \textbf{Diffusion Caching} offers a promisingtraining-free, architecture-agnostic, and efficient inference paradigm. Itscore mechanism identifies and reuses intrinsic computational redundancies inthe diffusion process. By enabling feature-level cross-step reuse andinter-layer scheduling, it reduces computation without modifying modelparameters. This paper systematically reviews the theoretical foundations andevolution of Diffusion Caching and proposes a unified framework for itsclassification and analysis. Through comparative analysis of representative methods, we show thatDiffusion Caching evolves from \textit{static reuse} to \textit{dynamicprediction}. This trend enhances caching flexibility across diverse tasks andenables integration with other acceleration techniques such as samplingoptimization and model distillation, paving the way for a unified, efficientinference framework for future multimodal and interactive applications. Weargue that this paradigm will become a key enabler of real-time and efficientgenerative AI, injecting new vitality into both theory and practice of\textit{Efficient Generative Intelligence}.</description><author>Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</author><pubDate>Wed, 22 Oct 2025 16:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19755v1</guid></item><item><title>CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees</title><link>http://arxiv.org/abs/2510.19754v1</link><description>Counterfactual explanations (CFXs) provide human-understandablejustifications for model predictions, enabling actionable recourse andenhancing interpretability. To be reliable, CFXs must avoid regions of highpredictive uncertainty, where explanations may be misleading or inapplicable.However, existing methods often neglect uncertainty or lack principledmechanisms for incorporating it with formal guarantees. We propose CONFEX, anovel method for generating uncertainty-aware counterfactual explanations usingConformal Prediction (CP) and Mixed-Integer Linear Programming (MILP). CONFEXexplanations are designed to provide local coverage guarantees, addressing theissue that CFX generation violates exchangeability. To do so, we develop anovel localised CP procedure that enjoys an efficient MILP encoding byleveraging an offline tree-based partitioning of the input space. This way,CONFEX generates CFXs with rigorous guarantees on both predictive uncertaintyand optimality. We evaluate CONFEX against state-of-the-art methods acrossdiverse benchmarks and metrics, demonstrating that our uncertainty-awareapproach yields robust and plausible explanations.</description><author>Aman Bilkhoo, Milad Kazemi, Nicola Paoletti, Mehran Hosseini</author><pubDate>Wed, 22 Oct 2025 16:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19754v1</guid></item><item><title>When Do Transformers Learn Heuristics for Graph Connectivity?</title><link>http://arxiv.org/abs/2510.19753v1</link><description>Transformers often fail to learn generalizable algorithms, instead relying onbrittle heuristics. Using graph connectivity as a testbed, we explain thisphenomenon both theoretically and empirically. We consider a simplifiedTransformer architecture, the disentangled Transformer, and prove that an$L$-layer model has capacity to solve for graphs with diameters up to exactly$3^L$, implementing an algorithm equivalent to computing powers of theadjacency matrix. We analyze the training-dynamics, and show that the learnedstrategy hinges on whether most training instances are within this modelcapacity. Within-capacity graphs (diameter $\leq 3^L$) drive the learning of acorrect algorithmic solution while beyond-capacity graphs drive the learning ofa simple heuristic based on node degrees. Finally, we empirically demonstratethat restricting training data within a model's capacity leads to both standardand disentangled transformers learning the exact algorithm rather than thedegree-based heuristic.</description><author>Qilin Ye, Deqing Fu, Robin Jia, Vatsal Sharan</author><pubDate>Wed, 22 Oct 2025 16:43:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19753v1</guid></item><item><title>Learning Affordances at Inference-Time for Vision-Language-Action Models</title><link>http://arxiv.org/abs/2510.19752v1</link><description>Solving complex real-world control tasks often takes multiple tries: if wefail at first, we reflect on what went wrong, and change our strategyaccordingly to avoid making the same mistake. In robotics,Vision-Language-Action models (VLAs) offer a promising path towards solvingcomplex control tasks, but lack the ability to contextually and dynamicallyreadjust behavior when they fail to accomplish a task. In this work, weintroduce Learning from Inference-Time Execution (LITEN), which connects a VLAlow-level policy to a high-level VLM that conditions on past experiences byincluding them in-context, allowing it to learn the affordances andcapabilities of the low-level VLA. Our approach iterates between a reasoningphase that generates and executes plans for the low-level VLA, and anassessment phase that reflects on the resulting execution and draws usefulconclusions to be included in future reasoning contexts. Unlike similarapproaches to self-refinement in non-robotics domains, LITEN must reflect onunstructured real-world robot trajectories (e.g., raw videos), which requiresstructured guiderails during assessment. Our experimental results demonstrateLITEN is able to effectively learn from past experience to generate plans thatuse high-affordance instructions to accomplish long-horizon tasks.</description><author>Ameesh Shah, William Chen, Adwait Godbole, Federico Mora, Sanjit A. Seshia, Sergey Levine</author><pubDate>Wed, 22 Oct 2025 16:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19752v1</guid></item><item><title>BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models</title><link>http://arxiv.org/abs/2510.19749v1</link><description>Species distribution models (SDMs), which aim to predict species occurrencebased on environmental variables, are widely used to monitor and respond tobiodiversity change. Recent deep learning advances for SDMs have been shown toperform well on complex and heterogeneous datasets, but their effectivenessremains limited by spatial biases in the data. In this paper, we revisit deepSDMs from a Bayesian perspective and introduce BATIS, a novel and practicalframework wherein prior predictions are updated iteratively using limitedobservational data. Models must appropriately capture both aleatoric andepistemic uncertainty to effectively combine fine-grained local insights withbroader ecological patterns. We benchmark an extensive set of uncertaintyquantification approaches on a novel dataset including citizen scienceobservations from the eBird platform. Our empirical study shows how Bayesiandeep learning approaches can greatly improve the reliability of SDMs indata-scarce locations, which can contribute to ecological understanding andconservation efforts.</description><author>Catherine Villeneuve, Benjamin Akera, Mélisande Teng, David Rolnick</author><pubDate>Wed, 22 Oct 2025 16:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19749v1</guid></item><item><title>Video-R1: Reinforcing Video Reasoning in MLLMs</title><link>http://arxiv.org/abs/2503.21776v4</link><description>Inspired by DeepSeek-R1's success in eliciting reasoning abilities throughrule-based reinforcement learning (RL), we introduce Video-R1 as the firstattempt to systematically explore the R1 paradigm for incentivizing videoreasoning within multimodal large language models (MLLMs). However, directlyapplying RL training with the GRPO algorithm to video reasoning presents twoprimary challenges: (i) a lack of temporal modeling for video reasoning, and(ii) the scarcity of high-quality video-reasoning data. To address theseissues, we first propose the T-GRPO algorithm, which encourages models toutilize temporal information in videos for reasoning. Additionally, instead ofrelying solely on video data, we incorporate high-quality image-reasoning datainto the training process. We have constructed two datasets: Video-R1-CoT-165kfor SFT cold start and Video-R1-260k for RL training, both comprising image andvideo data. Experimental results demonstrate that Video-R1 achieves significantimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, aswell as on general video benchmarks including MVBench and TempCompass, etc.Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoningbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. Allcode, models, and data are released in: https://github.com/tulerfeng/Video-R1.</description><author>Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, Xiangyu Yue</author><pubDate>Wed, 22 Oct 2025 16:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.21776v4</guid></item><item><title>CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience</title><link>http://arxiv.org/abs/2507.09024v4</link><description>Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets.CNeuroMod-THINGS meets this need by capturing neural representations for a wideset of semantic concepts using well-characterized images in a newdensely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGSexploits synergies between two existing projects: the THINGS initiative(THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS hasdeveloped a common set of thoroughly annotated images broadly sampling naturaland man-made objects which is used to acquire a growing collection oflarge-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiringhundreds of hours of fMRI data from a core set of participants duringcontrolled and naturalistic tasks, including visual tasks like movie watchingand videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants eachcompleted 33-36 sessions of a continuous recognition paradigm usingapproximately 4000 images from the THINGS stimulus set spanning 720 categories.We report behavioural and neuroimaging metrics that showcase the quality of thedata. By bridging together large existing resources, CNeuroMod-THINGS expandsour capacity to model broad slices of the human visual experience.</description><author>Marie St-Laurent, Basile Pinsard, Oliver Contier, Elizabeth DuPre, Katja Seeliger, Valentina Borghesani, Julie A. Boyle, Lune Bellec, Martin N. Hebart</author><pubDate>Wed, 22 Oct 2025 16:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.09024v4</guid></item><item><title>Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning</title><link>http://arxiv.org/abs/2508.16949v5</link><description>Recent advances in Large Language Models (LLMs) have underscored thepotential of Reinforcement Learning (RL) to facilitate the emergence ofreasoning capabilities. Despite the encouraging results, a fundamental dilemmapersists as RL improvement relies on learning from high-quality samples, yetthe exploration for such samples remains bounded by the inherent limitations ofLLMs. This, in effect, creates an undesirable cycle in which what cannot beexplored cannot be learned. In this work, we propose Rubric-ScaffoldedReinforcement Learning (RuscaRL), a novel instructional scaffolding frameworkdesigned to break the exploration bottleneck for general LLM reasoning.Specifically, RuscaRL introduces checklist-style rubrics as (1) explicitscaffolding for exploration during rollout generation, where different rubricsare provided as external guidance within task instructions to steer diversehigh-quality responses. This guidance is gradually decayed over time,encouraging the model to internalize the underlying reasoning patterns; (2)verifiable rewards for exploitation during model training, where we can obtainrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RLon general reasoning tasks. Extensive experiments demonstrate the superiorityof the proposed RuscaRL across various benchmarks, effectively expandingreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRLsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,surpassing GPT-4.1. Furthermore, our fine-tuned variant onQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leadingLLMs including OpenAI-o3. Our code is available athttps://github.com/IANNXANG/RuscaRL.</description><author>Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Kongcheng Zhang, Jiale Zhao, Jingwen Yang, Yihe Zhou, Jianwei Lv, Tongya Zheng, Hengtong Lu, Wei Chen, Yan Xie, Mingli Song</author><pubDate>Wed, 22 Oct 2025 16:32:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16949v5</guid></item><item><title>Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency Modulation</title><link>http://arxiv.org/abs/2503.14475v2</link><description>The field of Novel View Synthesis has been revolutionized by 3D GaussianSplatting (3DGS), which enables high-quality scene reconstruction that can berendered in real-time. 3DGS-based techniques typically suffer from high GPUmemory and disk storage requirements which limits their practical applicationon consumer-grade devices. We propose Opti3DGS, a novel frequency-modulatedcoarse-to-fine optimization framework that aims to minimize the number ofGaussian primitives used to represent a scene, thus reducing memory and storagedemands. Opti3DGS leverages image frequency modulation, initially enforcing acoarse scene representation and progressively refining it by modulatingfrequency details in the training images. On the baseline 3DGS, we demonstratean average reduction of 62% in Gaussians, a 40% reduction in the training GPUmemory requirements and a 20% reduction in optimization time withoutsacrificing the visual quality. Furthermore, we show that our method integratesseamlessly with many 3DGS-based techniques, consistently reducing the number ofGaussian primitives while maintaining, and often improving, visual quality.Additionally, Opti3DGS inherently produces a level-of-detail scenerepresentation at no extra cost, a natural byproduct of the optimizationpipeline. Results and code will be made publicly available.</description><author>Umar Farooq, Jean-Yves Guillemaut, Adrian Hilton, Marco Volino</author><pubDate>Wed, 22 Oct 2025 16:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.14475v2</guid></item><item><title>[RETRACTED]Evolving Form and Function: Dual-Objective Optimization in Neural Symbolic Regression Networks</title><link>http://arxiv.org/abs/2502.17393v2</link><description>[RETRACTED]Data increasingly abounds, but distilling their underlyingrelationships down to something interpretable remains challenging. One approachis genetic programming, which `symbolically regresses' a data set down into anequation. However, symbolic regression (SR) faces the issue of requiring training fromscratch for each new dataset. To generalize across all datasets, deep learningtechniques have been applied to SR. These networks, however, are only able to be trained using a symbolicobjective: NN-generated and target equations are symbolically compared. Butthis does not consider the predictive power of these equations, which could bemeasured by a behavioral objective that compares the generated equation'spredictions to actual data. Here we introduce a method that combines gradient descent and evolutionarycomputation to yield neural networks that minimize the symbolic and behavioralerrors of the equations they generate from data. As a result, these evolved networks are shown to generate more symbolicallyand behaviorally accurate equations than those generated by networks trained bystate-of-the-art gradient based neural symbolic regression methods. We hope this method suggests that evolutionary algorithms, combined withgradient descent, can improve SR results by yielding equations with moreaccurate form and function.</description><author>Amanda Bertschinger, James Bagrow, Joshua Bongard</author><pubDate>Wed, 22 Oct 2025 16:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.17393v2</guid></item><item><title>Misalignment Bounty: Crowdsourcing AI Agent Misbehavior</title><link>http://arxiv.org/abs/2510.19738v1</link><description>Advanced AI systems sometimes act in ways that differ from human intent. Togather clear, reproducible examples, we ran the Misalignment Bounty: acrowdsourced project that collected cases of agents pursuing unintended orunsafe goals. The bounty received 295 submissions, of which nine were awarded. This report explains the program's motivation and evaluation criteria, andwalks through the nine winning submissions step by step.</description><author>Rustem Turtayev, Natalia Fedorova, Oleg Serikov, Sergey Koldyba, Lev Avagyan, Dmitrii Volkov</author><pubDate>Wed, 22 Oct 2025 16:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19738v1</guid></item><item><title>SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks</title><link>http://arxiv.org/abs/2506.11791v2</link><description>Rigorous security-focused evaluation of large language model (LLM) agents isimperative for establishing trust in their safe deployment throughout thesoftware development lifecycle. However, existing benchmarks largely rely onsynthetic challenges or simplified vulnerability datasets that fail to capturethe complexity and ambiguity encountered by security engineers in practice. Weintroduce SEC-bench, the first fully automated benchmarking framework forevaluating LLM agents on authentic security engineering tasks. SEC-benchemploys a novel multi-agent scaffold that automatically constructs coderepositories with harnesses, reproduces vulnerabilities in isolatedenvironments, and generates gold patches for reliable evaluation. Our frameworkautomatically creates high-quality software vulnerability datasets withreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,we implement two critical software security tasks to rigorously evaluate LLMagents' capabilities: proof-of-concept (PoC) generation and vulnerabilitypatching. A comprehensive evaluation of state-of-the-art LLM code agentsreveals significant performance gaps, achieving at most 18.0% success in PoCgeneration and 34.0% in vulnerability patching on our complete dataset. Theseresults highlight the crucial steps needed toward developing LLM agents thatare more practical, intelligent, and autonomous for security engineering.</description><author>Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, Lingming Zhang</author><pubDate>Wed, 22 Oct 2025 16:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.11791v2</guid></item><item><title>Learning when to rank: Estimation of partial rankings from sparse, noisy comparisons</title><link>http://arxiv.org/abs/2501.02505v3</link><description>Ranking items based on pairwise comparisons is common, from using matchoutcomes to rank sports teams to using purchase or survey data to rank consumerproducts. Statistical inference-based methods such as the Bradley-Terry model,which extract rankings based on an underlying generative model, have emerged asflexible and powerful tools to tackle ranking in empirical data. In situationswith limited and/or noisy comparisons, it is often challenging to confidentlydistinguish the performance of different items based on the evidence availablein the data. However, most inference-based ranking methods choose to assigneach item to a unique rank or score, suggesting a meaningful distinction whenthere is none. Here, we develop a principled nonparametric Bayesian method,adaptable to any statistical ranking method, for learning partial rankings(rankings with ties) that distinguishes among the ranks of different items onlywhen there is sufficient evidence available in the data. We develop a fastagglomerative algorithm to perform Maximum A Posteriori (MAP) inference ofpartial rankings under our framework and examine the performance of our methodon a variety of real and synthetic network datasets, finding that it frequentlygives a more parsimonious summary of the data than traditional ranking,particularly when observations are sparse.</description><author>Sebastian Morel-Balbi, Alec Kirkley</author><pubDate>Wed, 22 Oct 2025 16:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.02505v3</guid></item><item><title>Preconditioned Norms: A Unified Framework for Steepest Descent, Quasi-Newton and Adaptive Methods</title><link>http://arxiv.org/abs/2510.10777v2</link><description>Optimization lies at the core of modern deep learning, yet existing methodsoften face a fundamental trade-off between adapting to problem geometry andleveraging curvature utilization. Steepest descent algorithms adapt todifferent geometries through norm choices but remain strictly first-order,whereas quasi-Newton and adaptive optimizers incorporate curvature informationbut are restricted to Frobenius geometry, limiting their applicability acrossdiverse architectures. In this work, we propose a unified frameworkgeneralizing steepest descent, quasi-Newton methods, and adaptive methodsthrough the novel notion of preconditioned matrix norms. This abstractionreveals that widely used optimizers such as SGD and Adam, as well as moreadvanced approaches like Muon and KL-Shampoo, and recent hybrids including SOAPand SPlus, all emerge as special cases of the same principle. Within thisframework, we provide the first systematic treatment of affine and scaleinvariance in the matrix-parameterized setting, establishing necessary andsufficient conditions under generalized norms. Building on this foundation, weintroduce two new methods, $\texttt{MuAdam}$ and $\texttt{MuAdam-SANIA}$, whichcombine the spectral geometry of Muon with Adam-style preconditioning. Ourexperiments demonstrate that these optimizers are competitive with, and in somecases outperform, existing state-of-the-art methods. Our code is available athttps://github.com/brain-lab-research/LIB/tree/quasi_descent</description><author>Andrey Veprikov, Arman Bolatov, Samuel Horváth, Aleksandr Beznosikov, Martin Takáč, Slavomir Hanzely</author><pubDate>Wed, 22 Oct 2025 16:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.10777v2</guid></item><item><title>Statistical Inference for Linear Functionals of Online Least-squares SGD when $t \gtrsim d^{1+δ}$</title><link>http://arxiv.org/abs/2510.19734v1</link><description>Stochastic Gradient Descent (SGD) has become a cornerstone method in moderndata science. However, deploying SGD in high-stakes applications necessitatesrigorous quantification of its inherent uncertainty. In this work, we establish\emph{non-asymptotic Berry--Esseen bounds} for linear functionals of onlineleast-squares SGD, thereby providing a Gaussian Central Limit Theorem (CLT) ina \emph{growing-dimensional regime}. Existing approaches to high-dimensionalinference for projection parameters, such as~\cite{chang2023inference}, rely oninverting empirical covariance matrices and require at least $t \gtrsimd^{3/2}$ iterations to achieve finite-sample Berry--Esseen guarantees,rendering them computationally expensive and restrictive in the allowabledimensional scaling. In contrast, we show that a CLT holds for SGD iterateswhen the number of iterations grows as $t \gtrsim d^{1+\delta}$ for any $\delta&gt; 0$, significantly extending the dimensional regime permitted by prior workswhile improving computational efficiency. The proposed online SGD-basedprocedure operates in $\mathcal{O}(td)$ time and requires only $\mathcal{O}(d)$memory, in contrast to the $\mathcal{O}(td^2 + d^3)$ runtime ofcovariance-inversion methods. To render the theory practically applicable, wefurther develop an \emph{online variance estimator} for the asymptotic varianceappearing in the CLT and establish \emph{high-probability deviation bounds} forthis estimator. Collectively, these results yield the first fully online anddata-driven framework for constructing confidence intervals for SGD iterates inthe near-optimal scaling regime $t \gtrsim d^{1+\delta}$.</description><author>Bhavya Agrawalla, Krishnakumar Balasubramanian, Promit Ghosal</author><pubDate>Wed, 22 Oct 2025 16:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19734v1</guid></item><item><title>Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning</title><link>http://arxiv.org/abs/2510.19733v1</link><description>Large Language Model (LLM) conditioning refers to instructing an LLM togenerate content in accordance with the norms and values of a specific culture,beliefs of a particular political orientation, or any desired text-specifiedsemantic conditioning. Unfortunately, prompt engineering does not ensure thatLLMs behave in accordance with a desired conditioning due to the inductive biasof the pre-training and alignment datasets. Prior works have focused onfine-tuning LLMs by directly conditioning the LoRA weights; however, suchmethods introduce a large number of parameters. As a remedy, we propose Zhyper,a parameter-efficient factorized hypernetwork framework that generatescontext-aware LoRA adapters from textual descriptions. Experiments on multiplebenchmarks show that Zhyper achieves competitive performance with up to 26xfewer parameters than the state-of-the-art baselines. Furthermore, we extendZhyper to cultural alignment, demonstrating improved generalization toout-of-domain settings and a better capturing of fine-grained contextualvalues.</description><author>M. H. I. Abdalla, Zhipin Wang, Christian Frey, Steffen Eger, Josif Grabocka</author><pubDate>Wed, 22 Oct 2025 16:25:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19733v1</guid></item><item><title>Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking</title><link>http://arxiv.org/abs/2505.18495v2</link><description>Masked diffusion models (MDM) are powerful generative models for discretedata that generate samples by progressively unmasking tokens in a sequence.Each token can take one of two states: masked or unmasked. We observe thattoken sequences often remain unchanged between consecutive sampling steps;consequently, the model repeatedly processes identical inputs, leading toredundant computation. To address this inefficiency, we propose the Partialmasking scheme (Prime), which augments MDM by allowing tokens to takeintermediate states interpolated between the masked and unmasked states. Thisdesign enables the model to make predictions based on partially observed tokeninformation, and facilitates a fine-grained denoising process. We derive avariational training objective and introduce a simple architectural design toaccommodate intermediate-state inputs. Our method demonstrates superiorperformance across a diverse set of generative modeling tasks. On text data, itachieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM(21.52), autoregressive models (17.54), and their hybrid variants (17.58),without relying on an autoregressive formulation. On image data, it attainscompetitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparableto leading continuous generative models.</description><author>Chen-Hao Chao, Wei-Fang Sun, Hanwen Liang, Chun-Yi Lee, Rahul G. Krishnan</author><pubDate>Wed, 22 Oct 2025 16:25:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.18495v2</guid></item><item><title>Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning</title><link>http://arxiv.org/abs/2510.19732v1</link><description>To enable embodied agents to operate effectively over extended timeframes, itis crucial to develop models that form and access memories to staycontextualized in their environment. In the current paradigm of trainingtransformer-based policies for embodied sequential decision-making tasks,visual inputs often overwhelm the context limits of transformers, while humanscan maintain and utilize a lifetime of experience compressed as memories.Significant compression is possible in principle, as much of the input isirrelevant and can be abstracted. However, existing approaches predominantlyfocus on either recurrent models with fixed-size memory or transformers withfull-context reliance. In this work, we propose Memo, a transformer-basedarchitecture and training recipe for reinforcement learning (RL) onmemory-intensive, long-horizon tasks. Memo incorporates the creation andretrieval of memory by interleaving periodic summarization tokens with theinputs of a model during training. We demonstrate Memo's effectiveness on agridworld meta-RL benchmark and a multi-object navigation task inphoto-realistic indoor settings. Memo outperforms naive long-contexttransformer baselines while being more compute and storage efficient.Additionally, Memo generalizes better to longer contexts at inference time andremains robust in streaming settings, where historical context must betruncated to fit inference constraints.</description><author>Gunshi Gupta, Karmesh Yadav, Zsolt Kira, Yarin Gal, Rahaf Aljundi</author><pubDate>Wed, 22 Oct 2025 16:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19732v1</guid></item><item><title>Scalable Boltzmann Generators for equilibrium sampling of large-scale materials</title><link>http://arxiv.org/abs/2509.25486v2</link><description>The use of generative models to sample equilibrium distributions of many-bodysystems, as first demonstrated by Boltzmann Generators, has attractedsubstantial interest due to their ability to produce unbiased and uncorrelatedsamples in `one shot'. Despite their promise and impressive results across thenatural sciences, scaling these models to large systems remains a majorchallenge. In this work, we introduce a Boltzmann Generator architecture thataddresses this scalability bottleneck with a focus on applications in materialsscience. We leverage augmented coupling flows in combination with graph neuralnetworks to base the generation process on local environmental information,while allowing for energy-based training and fast inference. Compared toprevious architectures, our model trains significantly faster, requires farless computational resources, and achieves superior sampling efficiencies.Crucially, the architecture is transferable to larger system sizes, whichallows for the efficient sampling of materials with simulation cells ofunprecedented size. We demonstrate the potential of our approach by applying itto several materials systems, including Lennard-Jones crystals, ice phases ofmW water, and the phase diagram of silicon, for system sizes well above onethousand atoms. The trained Boltzmann Generators produce highly accurateequilibrium ensembles for various crystal structures, as well as Helmholtz andGibbs free energies across a range of system sizes, able to reach scales wherefinite-size effects become negligible.</description><author>Maximilian Schebek, Frank Noé, Jutta Rogal</author><pubDate>Wed, 22 Oct 2025 16:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.25486v2</guid></item><item><title>Bridging Earth and Space: A Survey on HAPS for Non-Terrestrial Networks</title><link>http://arxiv.org/abs/2510.19731v1</link><description>HAPS are emerging as key enablers in the evolution of 6G wireless networks,bridging terrestrial and non-terrestrial infrastructures. Operating in thestratosphere, HAPS can provide wide-area coverage, low-latency,energy-efficient broadband communications with flexible deployment options fordiverse applications. This survey delivers a comprehensive overview of HAPS usecases, technologies, and integration strategies within the 6G ecosystem. Theroles of HAPS in extending connectivity to underserved regions, supportingdynamic backhauling, enabling massive IoT, and delivering reliable low-latencycommunications for autonomous and immersive services are discussed. The paperreviews state-of-the-art architectures for terrestrial and non-terrestrialnetwork integration, highlights recent field trials. Furthermore, key enablingtechnologies such as channel modeling, AI-driven resource allocation,interference control, mobility management, and energy-efficient communicationsare examined. The paper also outlines open research challenges. By addressingexisting gaps in the literature, this survey positions HAPS as a foundationalcomponent of globally integrated, resilient, and sustainable 6G networks.</description><author>G. Svistunov, A. Akhtarshenas, D. López-Pérez, M. Giordani, G. Geraci, H. Yanikomeroglu</author><pubDate>Wed, 22 Oct 2025 16:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19731v1</guid></item><item><title>Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models</title><link>http://arxiv.org/abs/2510.02629v2</link><description>Context utilisation, the ability of Language Models (LMs) to incorporaterelevant information from the provided context when generating responses,remains largely opaque to users, who cannot determine whether models draw fromparametric memory or provided context, nor identify which specific contextpieces inform the response. Highlight explanations (HEs) offer a naturalsolution as they can point the exact context pieces and tokens that influencedmodel outputs. However, no existing work evaluates their effectiveness inaccurately explaining context utilisation. We address this gap by introducingthe first gold standard HE evaluation framework for context attribution, usingcontrolled test cases with known ground-truth context usage, which avoids thelimitations of existing indirect proxy evaluations. To demonstrate theframework's broad applicability, we evaluate four HE methods -- threeestablished techniques and MechLight, a mechanistic interpretability approachwe adapt for this task -- across four context scenarios, four datasets, andfive LMs. Overall, we find that MechLight performs best across all contextscenarios. However, all methods struggle with longer contexts and exhibitpositional biases, pointing to fundamental challenges in explanation accuracythat require new approaches to deliver reliable context utilisationexplanations at scale.</description><author>Jingyi Sun, Pepa Atanasova, Sagnik Ray Choudhury, Sekh Mainul Islam, Isabelle Augenstein</author><pubDate>Wed, 22 Oct 2025 16:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02629v2</guid></item><item><title>ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable Multimodal Model Serving</title><link>http://arxiv.org/abs/2502.00937v3</link><description>Large multimodal models (LMMs) demonstrate impressive capabilities inunderstanding images, videos, and audio beyond text. However, efficientlyserving LMMs in production environments poses significant challenges due totheir complex architectures and heterogeneous characteristics across theirmulti-stage inference pipelines. We present the first comprehensive systemsanalysis of two prominent LMM architectures, decoder-only and cross-attention,across six representative open-source models, revealing key systems designimplications. We also present an in-depth analysis of production LMM inferencetraces, uncovering unique workload characteristics, including variable,heavy-tailed request distributions and bursty traffic patterns. Based on theseinsights, we propose ModServe, a modular LMM serving system that decouplesstages for independent optimization and adaptive scaling. ModServe dynamicallyreconfigures stages and handles bursty traffic with modality-aware schedulingand autoscaling to meet tail latency SLOs while minimizing costs. ModServeachieves 3.3-5.5x higher throughput (leading to 25-41.3% cost saving) whilemeeting SLOs on a 128-GPU cluster with production traces.</description><author>Haoran Qiu, Anish Biswas, Zihan Zhao, Jayashree Mohan, Alind Khare, Esha Choukse, Íñigo Goiri, Zeyu Zhang, Haiying Shen, Chetan Bansal, Ramachandran Ramjee, Rodrigo Fonseca</author><pubDate>Wed, 22 Oct 2025 16:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.00937v3</guid></item><item><title>Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series</title><link>http://arxiv.org/abs/2510.19728v1</link><description>We present a novel framework for leveraging synthetic ICU time-series datanot only to train but also to rigorously and trustworthily evaluate predictivemodels, both at the population level and within fine-grained demographicsubgroups. Building on prior diffusion and VAE-based generators (TimeDiff,HealthGen, TimeAutoDiff), we introduce \textit{Enhanced TimeAutoDiff}, whichaugments the latent diffusion objective with distribution-alignment penalties.We extensively benchmark all models on MIMIC-III and eICU, on 24-hour mortalityand binary length-of-stay tasks. Our results show that Enhanced TimeAutoDiffreduces the gap between real-on-synthetic and real-on-real evaluation (``TRTSgap'') by over 70\%, achieving $\Delta_{TRTS} \leq 0.014$ AUROC, whilepreserving training utility ($\Delta_{TSTR} \approx 0.01$). Crucially, for 32intersectional subgroups, large synthetic cohorts cut subgroup-level AUROCestimation error by up to 50\% relative to small real test sets, and outperformthem in 72--84\% of subgroups. This work provides a practical,privacy-preserving roadmap for trustworthy, granular model evaluation incritical care, enabling robust and reliable performance analysis across diversepatient populations without exposing sensitive EHR data, contributing to theoverall trustworthiness of Medical AI.</description><author>Mahmoud Ibrahim, Bart Elen, Chang Sun, Gökhan Ertaylan, Michel Dumontier</author><pubDate>Wed, 22 Oct 2025 16:17:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19728v1</guid></item><item><title>WikiVideo: Article Generation from Multiple Videos</title><link>http://arxiv.org/abs/2504.00939v2</link><description>We introduce the task of grounded article generation with the goal ofcreating a Wikipedia-style article from multiple diverse videos aboutreal-world events -- from natural disasters to political elections -- where allthe information in the article is supported by video evidence. Videos areintuitive sources for retrieval-augmented generation (RAG), but mostcontemporary RAG workflows focus heavily on text while existing methods forvideo-based summarization focus on low-level scene understanding rather thanhigh-level event semantics. To close this gap, we introduce WikiVideo, abenchmark consisting of expert-written articles and densely annotated videosthat provide evidence for articles' claims, facilitating the integration ofvideo into RAG pipelines and enabling the creation of in-depth content that isgrounded in multimodal sources. We further propose Collaborative ArticleGeneration (CAG), a novel interactive method for article creation from multiplevideos. CAG leverages an iterative interaction between an r1-style reasoningmodel and a VideoLLM to draw higher-level inferences about the target eventthan is possible with VideoLLMs alone, which fixate on low-level visualfeatures. We benchmark state-of-the-art VideoLLMs and CAG in both oracleretrieval and RAG settings and find that CAG consistently outperformsalternative methods, while suggesting intriguing avenues for future work.</description><author>Alexander Martin, Reno Kriz, William Gantt Walden, Kate Sanders, Hannah Recknor, Eugene Yang, Francis Ferraro, Benjamin Van Durme</author><pubDate>Wed, 22 Oct 2025 16:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.00939v2</guid></item><item><title>The Coverage Principle: How Pre-Training Enables Post-Training</title><link>http://arxiv.org/abs/2510.15020v2</link><description>Language models demonstrate remarkable abilities when pre-trained on largetext corpora and fine-tuned for specific tasks, but how and why pre-trainingshapes the success of the final model remains poorly understood. Notably,although pre-training success is often quantified by cross-entropy loss,cross-entropy can be a poor predictor of downstream performance. Instead, weprovide a theoretical perspective on this relationship through the lens of\emph{coverage}, which quantifies the probability mass the pre-trained modelplaces on high-quality responses and which is necessary and sufficient forpost-training and test-time scaling methods such as Best-of-N to succeed. Ourmain results develop an understanding of \emph{the coverage principle}, aphenomenon whereby next-token prediction (more generally, maximum likelihood)implicitly optimizes toward a model with good coverage. In particular, weuncover a mechanism that explains the power of coverage in predictingdownstream performance: \emph{coverage generalizes faster than cross-entropy},avoiding spurious dependence on problem-dependent parameters such as thesequence length. We also study practical algorithmic interventions withprovable benefits for improving coverage, including (i) model/checkpointselection procedures, (ii) gradient normalization schemes, and (iii) test-timedecoding strategies.</description><author>Fan Chen, Audrey Huang, Noah Golowich, Sadhika Malladi, Adam Block, Jordan T. Ash, Akshay Krishnamurthy, Dylan J. Foster</author><pubDate>Wed, 22 Oct 2025 16:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.15020v2</guid></item><item><title>3D Visual Illusion Depth Estimation</title><link>http://arxiv.org/abs/2505.13061v4</link><description>3D visual illusion is a perceptual phenomenon where a two-dimensional planeis manipulated to simulate three-dimensional spatial relationships, making aflat artwork or object look three-dimensional in the human visual system. Inthis paper, we reveal that the machine visual system is also seriously fooledby 3D visual illusions, including monocular and binocular depth estimation. Inorder to explore and analyze the impact of 3D visual illusion on depthestimation, we collect a large dataset containing almost 3k scenes and 200kimages to train and evaluate SOTA monocular and binocular depth estimationmethods. We also propose a 3D visual illusion depth estimation framework thatuses common sense from the vision language model to adaptively fuse depth frombinocular disparity and monocular depth. Experiments show that SOTA monocular,binocular, and multi-view depth estimation approaches are all fooled by various3D visual illusions, while our method achieves SOTA performance.</description><author>Chengtang Yao, Zhidan Liu, Jiaxi Zeng, Lidong Yu, Yuwei Wu, Yunde Jia</author><pubDate>Wed, 22 Oct 2025 16:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.13061v4</guid></item><item><title>GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks</title><link>http://arxiv.org/abs/2503.18129v2</link><description>This paper establishes a benchmark for evaluating tool-calling capabilitiesof large language models (LLMs) on multi-step geospatial tasks relevant tocommercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o,GPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23geospatial functions. Our benchmark comprises tasks in four categories ofincreasing complexity, with both solvable and intentionally unsolvable tasks totest rejection accuracy. We develop a LLM-as-Judge evaluation framework tocompare agent solutions against reference solutions. Results show o4-mini andClaude 3.5 Sonnet achieve the best overall performance, OpenAI's GPT-4.1,GPT-4o and Google's Gemini 2.5 Pro Preview do not fall far behind, but the lasttwo are more efficient in identifying unsolvable tasks. Claude Sonnet 4, dueits preference to provide any solution rather than reject a task, proved to beless accurate. We observe significant differences in token usage, withAnthropic models consuming more tokens than competitors. Common errors includemisunderstanding geometrical relationships, relying on outdated knowledge, andinefficient data manipulation. The resulting benchmark set, evaluationframework, and data generation pipeline are released as open-source resources(available at https://github.com/Solirinai/GeoBenchX), providing one morestandardized method for the ongoing evaluation of LLMs for GeoAI.</description><author>Varvara Krechetova, Denis Kochedykov</author><pubDate>Wed, 22 Oct 2025 16:12:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.18129v2</guid></item><item><title>From Answers to Guidance: A Proactive Dialogue System for Legal Documents</title><link>http://arxiv.org/abs/2510.19723v1</link><description>The accessibility of legal information remains a constant challenge,particularly for laypersons seeking to understand and apply complexinstitutional texts. While the European Union provides open access tolegislation, parliamentary responses, and regulatory documents, these resourcescan be challenging for laypeople to explore. In this paper, we introduceEUDial, a proactive multi-turn dialogue dataset constructed from 204 blogscurated by the Citizens' Enquiries Unit (AskEP) of the European ParliamentaryResearch Service. EUDial contains 880 dialogue turns (averaging 4.3 turns perdialogue), where each dialogue includes initial questions, structured answers,and follow-up questions. Beyond dataset construction, we propose the LexGuideframework that leverages retrieval-augmented generation with hierarchical topicorganization to structure dialogue progression, ensuring both comprehensivecoverage of legal aspects and coherence across conversational turns. Theresults demonstrate that proactive, structured navigation closes the gapbetween the availability of legal information and citizen comprehension,establishing EUDial and LexGuide as practical resources for advancing proactivelegal dialogue systems.</description><author>Ashish Chouhan, Michael Gertz</author><pubDate>Wed, 22 Oct 2025 16:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19723v1</guid></item><item><title>Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction</title><link>http://arxiv.org/abs/2505.20755v4</link><description>In this paper, we unify more than 10 existing one-step diffusion distillationapproaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside atheory-driven framework which we name the \textbf{\emph{Uni-Instruct}}.Uni-Instruct is motivated by our proposed diffusion expansion theory of the$f$-divergence family. Then we introduce key theories that overcome theintractability issue of the original expanded $f$-divergence, resulting in anequivalent yet tractable loss that effectively trains one-step diffusion modelsby minimizing the expanded $f$-divergence family. The novel unificationintroduced by Uni-Instruct not only offers new theoretical contributions thathelp understand existing approaches from a high-level perspective but alsoleads to state-of-the-art one-step diffusion generation performances. On theCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking FrechetInception Distance (FID) values of \textbf{\emph{1.46}} for unconditionalgeneration and \textbf{\emph{1.38}} for conditional generation. On theImageNet-$64\times 64$ generation benchmark, Uni-Instruct achieves a new SoTAone-step generation FID of \textbf{\emph{1.02}}, which outperforms its 79-stepteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).We also apply Uni-Instruct on broader tasks like text-to-3D generation. Fortext-to-3D generation, Uni-Instruct gives decent results, which slightlyoutperforms previous methods, such as SDS and VSD, in terms of both generationquality and diversity. Both the solid theoretical and empirical contributionsof Uni-Instruct will potentially help future studies on one-step diffusiondistillation and knowledge transferring of diffusion models.</description><author>Yifei Wang, Weimin Bai, Colin Zhang, Debing Zhang, Weijian Luo, He Sun</author><pubDate>Wed, 22 Oct 2025 16:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.20755v4</guid></item><item><title>LyTimeT: Towards Robust and Interpretable State-Variable Discovery</title><link>http://arxiv.org/abs/2510.19716v1</link><description>Extracting the true dynamical variables of a system from high-dimensionalvideo is challenging due to distracting visual factors such as backgroundmotion, occlusions, and texture changes. We propose LyTimeT, a two-phaseframework for interpretable variable extraction that learns robust and stablelatent representations of dynamical systems. In Phase 1, LyTimeT employs aspatio-temporal TimeSformer-based autoencoder that uses global attention tofocus on dynamically relevant regions while suppressing nuisance variation,enabling distraction-robust latent state learning and accurate long-horizonvideo prediction. In Phase 2, we probe the learned latent space, select themost physically meaningful dimensions using linear correlation analysis, andrefine the transition dynamics with a Lyapunov-based stability regularizer toenforce contraction and reduce error accumulation during roll-outs. Experimentson five synthetic benchmarks and four real-world dynamical systems, includingchaotic phenomena, show that LyTimeT achieves mutual information and intrinsicdimension estimates closest to ground truth, remains invariant under backgroundperturbations, and delivers the lowest analytical mean squared error amongCNN-based (TIDE) and transformer-only baselines. Our results demonstrate thatcombining spatio-temporal attention with stability constraints yieldspredictive models that are not only accurate but also physically interpretable.</description><author>Kuai Yu, Crystal Su, Xiang Liu, Judah Goldfeder, Mingyuan Shao, Hod Lipson</author><pubDate>Wed, 22 Oct 2025 16:03:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19716v1</guid></item><item><title>Base Models Know How to Reason, Thinking Models Learn When</title><link>http://arxiv.org/abs/2510.07364v3</link><description>Why do thinking language models like DeepSeek R1 outperform their basecounterparts? Despite consistent performance gains, it remains unclear to whatextent thinking models learn entirely new reasoning capabilities or repurposepre-existing base model ones. In this work, we propose a hybrid model where weactivate reasoning mechanisms in base models at the right time to elicitthinking-model-level reasoning chains, implying that thinking models exploitalready existing capabilities. To ground our analysis, we introduce anunsupervised, bottom-up approach for uncovering human-interpretable reasoningbehaviors in thinking models. This approach provides an unbiased method todiscover reasoning behaviors without imposing manual or LLM-derivedassumptions. Across three base and four thinking models, using GSM8K andMATH500, our hybrid model recovers up to 91% of the performance gap to thinkingmodels without any weight updates while steering only 12% of tokens.Concretely, our empirical setup provides a simple, causal way to test theeffectiveness of existing reasoning mechanisms in base models by invoking themdirectly and measuring the resulting task performance. More broadly, theseresults reframe our understanding of how thinking models are trained:pre-training is when models acquire most of their reasoning mechanisms, andpost-training teaches efficient deployment of these mechanisms at the righttime, enabling efficient use of their inference-time compute.</description><author>Constantin Venhoff, Iván Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda</author><pubDate>Wed, 22 Oct 2025 16:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.07364v3</guid></item><item><title>LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits</title><link>http://arxiv.org/abs/2410.01735v3</link><description>Reward Models (RMs) are crucial to aligning large language models (LLMs), butthe degree to which an RM specialized to one task (e.g. writing) generalizes tonew tasks (e.g. math) is often not known a priori, often making using only onefixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMssimultaneously can incur a prohibitively high computational cost and lead toconflicting signals from different RMs that may degrade performance. To addressthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),which frames reward model selection as a multi-armed bandit problem,efficiently and iteratively training LLMs using multiple RMs by selecting themost well-suited RM for each instance. On commonsense and math reasoning tasks,we show that LASeR boosts iterative LLM training, improving the absoluteaverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble ofRM scores while also showing superior efficiency (e.g., a 2x speedup).Moreover, on WildChat (open-ended instruction-following tasks), LASeR leads toa 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending tolong-context generation, LASeR improves by 2.96 F1 points (avg.) onsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RMscore ensemble baseline with best-of-n sampling.</description><author>Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Wed, 22 Oct 2025 16:01:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01735v3</guid></item><item><title>Learning to Learn with Contrastive Meta-Objective</title><link>http://arxiv.org/abs/2410.05975v4</link><description>Meta-learning enables learning systems to adapt quickly to new tasks, similarto humans. Different meta-learning approaches all work under/with themini-batch episodic training framework. Such framework naturally gives theinformation about task identity, which can serve as additional supervision formeta-training to improve generalizability. We propose to exploit task identityas additional supervision in meta-training, inspired by the alignment anddiscrimination ability which is is intrinsic in human's fast learning. This isachieved by contrasting what meta-learners learn, i.e., model representations.The proposed ConML is evaluating and optimizing the contrastive meta-objectiveunder a problem- and learner-agnostic meta-training framework. We demonstratethat ConML integrates seamlessly with existing meta-learners, as well asin-context learning models, and brings significant boost in performance withsmall implementation cost.</description><author>Shiguang Wu, Yaqing Wang, Yatao Bian, Quanming Yao</author><pubDate>Wed, 22 Oct 2025 16:00:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05975v4</guid></item><item><title>SEMPO: Lightweight Foundation Models for Time Series Forecasting</title><link>http://arxiv.org/abs/2510.19710v1</link><description>The recent boom of large pre-trained models witnesses remarkable success indeveloping foundation models (FMs) for time series forecasting. Despiteimpressive performance across diverse downstream forecasting tasks, existingtime series FMs possess massive network architectures and require substantialpre-training on large-scale datasets, which significantly hinders theirdeployment in resource-constrained environments. In response to this growingtension between versatility and affordability, we propose SEMPO, a novellightweight foundation model that requires pretraining on relativelysmall-scale data, yet exhibits strong general time series forecasting.Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctraldecomposition module, that substantially improves the utilization ofpre-training data by modeling not only the high-energy frequency signals butalso the low-energy yet informative frequency signals that are ignored incurrent methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learnsheterogeneous temporal patterns through small dataset-specific prompts andadaptively routes time series tokens to prompt-based experts forparameter-efficient model adaptation across different datasets and domains.Equipped with these modules, SEMPO significantly reduces both pre-training datascale and model size, while achieving strong generalization. Extensiveexperiments on two large-scale benchmarks covering 16 datasets demonstrate thesuperior performance of SEMPO in both zero-shot and few-shot forecastingscenarios compared with state-of-the-art methods. Code and data are availableat https://github.com/mala-lab/SEMPO.</description><author>Hui He, Kun Yi, Yuanchi Ma, Qi Zhang, Zhendong Niu, Guansong Pang</author><pubDate>Wed, 22 Oct 2025 15:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19710v1</guid></item><item><title>Fast Inference via Hierarchical Speculative Decoding</title><link>http://arxiv.org/abs/2510.19705v1</link><description>Transformer language models generate text autoregressively, making inferencelatency proportional to the number of tokens generated. Speculative decodingreduces this latency without sacrificing output quality, by leveraging a smalldraft model to propose tokens that the larger target model verifies inparallel. In practice, however, there may exist a set of potential draftmodels- ranging from faster but less inaccurate, to slower yet more reliable.We introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacksthese draft models into a hierarchy, where each model proposes tokens, and thenext larger model verifies them in a single forward pass, until finally thetarget model verifies tokens. We derive an expression for the expected latencyof any such hierarchy and show that selecting the latency-optimal hierarchy canbe done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over thebest single-draft baseline, demonstrating the practicality of our algorithm inreducing generation latency beyond previous techniques.</description><author>Amir Globerson, Haim Kaplan, Yishay Mansour, Clara Mohri, Tal Schuster</author><pubDate>Wed, 22 Oct 2025 15:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19705v1</guid></item><item><title>RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models</title><link>http://arxiv.org/abs/2510.19698v1</link><description>Large Language Models (LLMs) can propose rules in natural language,sidestepping the need for a predefined predicate space in traditional rulelearning. Yet many LLM-based approaches ignore interactions among rules, andthe opportunity to couple LLMs with probabilistic rule learning for robustinference remains underexplored. We present RLIE, a unified framework thatintegrates LLMs with probabilistic modeling to learn a set of weighted rules.RLIE has four stages: (1) Rule generation, where an LLM proposes and filterscandidates; (2) Logistic regression, which learns probabilistic weights forglobal selection and calibration; (3) Iterative refinement, which updates therule set using prediction errors; and (4) Evaluation, which compares theweighted rule set as a direct classifier with methods that inject rules into anLLM. We evaluate multiple inference strategies on real-world datasets. Applyingrules directly with their learned weights yields superior performance, whereasprompting LLMs with the rules, weights, and logistic-model outputs surprisinglydegrades accuracy. This supports the view that LLMs excel at semanticgeneration and interpretation but are less reliable for precise probabilisticintegration. RLIE clarifies the potential and limitations of LLMs for inductivereasoning and couples them with classic probabilistic rule combination methodsto enable more reliable neuro-symbolic reasoning.</description><author>Yang Yang, Hua XU, Zhangyi Hu, Yutao Yue</author><pubDate>Wed, 22 Oct 2025 15:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19698v1</guid></item><item><title>What Expressivity Theory Misses: Message Passing Complexity for GNNs</title><link>http://arxiv.org/abs/2509.01254v2</link><description>Expressivity theory, characterizing which graphs a GNN can distinguish, hasbecome the predominant framework for analyzing GNNs, with new models strivingfor higher expressivity. However, we argue that this focus is misguided: First,higher expressivity is not necessary for most real-world tasks as these tasksrarely require expressivity beyond the basic WL test. Second, expressivitytheory's binary characterization and idealized assumptions fail to reflectGNNs' practical capabilities. To overcome these limitations, we propose MessagePassing Complexity (MPC): a continuous measure that quantifies the difficultyfor a GNN architecture to solve a given task through message passing. MPCcaptures practical limitations like over-squashing while preserving thetheoretical impossibility results from expressivity theory, effectivelynarrowing the gap between theory and practice. Through extensive validation onfundamental GNN tasks, we show that MPC's theoretical predictions correlatewith empirical performance, successfully explaining architectural successes andfailures. Thereby, MPC advances beyond expressivity theory to provide a morepowerful and nuanced framework for understanding and improving GNNarchitectures.</description><author>Niklas Kemper, Tom Wollschläger, Stephan Günnemann</author><pubDate>Wed, 22 Oct 2025 15:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.01254v2</guid></item><item><title>Explainable Face Presentation Attack Detection via Ensemble-CAM</title><link>http://arxiv.org/abs/2510.19695v1</link><description>Presentation attacks represent a critical security threat where adversariesuse fake biometric data, such as face, fingerprint, or iris images, to gainunauthorized access to protected systems. Various presentation attack detection(PAD) systems have been designed leveraging deep learning (DL) models tomitigate this type of threat. Despite their effectiveness, most of the DLmodels function as black boxes - their decisions are opaque to their users. Thepurpose of explainability techniques is to provide detailed information aboutthe reason behind the behavior or decision of DL models. In particular, visualexplanation is necessary to better understand the decisions or predictions ofDL-based PAD systems and determine the key regions due to which a biometricimage is considered real or fake by the system. In this work, a noveltechnique, Ensemble-CAM, is proposed for providing visual explanations for thedecisions made by deep learning-based face PAD systems. Our goal is to improveDL-based face PAD systems by providing a better understanding of theirbehavior. Our provided visual explanations will enhance the transparency andtrustworthiness of DL-based face PAD systems.</description><author>Rashik Shadman, M G Sarwar Murshed, Faraz Hussain</author><pubDate>Wed, 22 Oct 2025 15:45:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19695v1</guid></item><item><title>Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings</title><link>http://arxiv.org/abs/2510.19694v1</link><description>Prompting is a common approach for leveraging LMs in zero-shot settings.However, the underlying mechanisms that enable LMs to perform diverse taskswithout task-specific supervision remain poorly understood. Studying therelationship between prompting and the quality of internal representations canshed light on how pre-trained embeddings may support in-context task solving.In this empirical study, we conduct a series of probing experiments on promptembeddings, analyzing various combinations of prompt templates for zero-shotclassification. Our findings show that while prompting affects the quality ofrepresentations, these changes do not consistently correlate with the relevanceof the prompts to the target task. This result challenges the assumption thatmore relevant prompts necessarily lead to better representations. We furtheranalyze potential factors that may contribute to this unexpected behavior.</description><author>Cesar Gonzalez-Gutierrez, Dirk Hovy</author><pubDate>Wed, 22 Oct 2025 15:43:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19694v1</guid></item><item><title>Fast sampling and model selection for Bayesian mixture models</title><link>http://arxiv.org/abs/2501.07668v2</link><description>We study Bayesian estimation of mixture models and argue in favor of fittingthe marginal posterior distribution over component assignments directly, ratherthan Gibbs sampling from the joint posterior on components and parameters as iscommonly done. Some previous authors have found the former approach to haveslow mixing, but we show that, implemented correctly, it can achieve excellentperformance. In particular, we describe a new Monte Carlo algorithm forsampling from the marginal posterior of a general integrable mixture that makesuse of rejection-free sampling from the prior over component assignments toachieve excellent mixing times in typical applications, outperforming standardGibbs sampling, in some cases by a wide margin. We demonstrate the approachwith a selection of applications to Gaussian, Poisson, and categorical models.</description><author>M. E. J. Newman</author><pubDate>Wed, 22 Oct 2025 15:43:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07668v2</guid></item><item><title>Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary</title><link>http://arxiv.org/abs/2510.19692v1</link><description>Agentic AI is poised to usher in a seismic paradigm shift in SoftwareEngineering (SE). As technologists rush head-along to make agentic AI areality, SE researchers are driven to establish agentic SE as a research area.While early visions of agentic SE are primarily focused on code-relatedactivities, early empirical evidence calls for a consideration of a range ofsocio-technical concerns to make it work in practice. This paper contributes tothe emerging community vision by: (a) recommending an expansion of its scopebeyond code, toward a 'whole of process' vision, grounding it in SE foundationsand evolution and emerging agentic SE frameworks, (b) proposing a preliminaryset of values and principles to guide efforts, and (c) sharing guidance ondesigning/using well-defined vocabulary for agentic SE. It is hoped that theseideas will encourage community collaborations and steer the SE communitytowards laying strong foundations of agentic SE so its not only inevitable butalso deliberate and desirable in the long run.</description><author>Rashina Hoda</author><pubDate>Wed, 22 Oct 2025 15:39:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19692v1</guid></item><item><title>Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly with Neural Networks</title><link>http://arxiv.org/abs/2502.16763v3</link><description>Neural networks are known for their ability to approximate smooth functions,yet they fail to generalize perfectly to unseen inputs when trained on discreteoperations. Such operations lie at the heart of algorithmic tasks such asarithmetic, which is often used as a test bed for algorithmic execution inneural networks. In this work, we ask: can neural networks learn to executebinary-encoded algorithmic instructions exactly? We use the Neural TangentKernel (NTK) framework to study the training dynamics of two-layer fullyconnected networks in the infinite-width limit and show how a sufficientlylarge ensemble of such models can be trained to execute exactly, with highprobability, four fundamental tasks: binary permutations, binary addition,binary multiplication, and Subtract and Branch if Negative (SBN) instructions.Since SBN is Turing-complete, our framework extends to computable functions. Weshow how this can be efficiently achieved using only logarithmically manytraining data. Our approach relies on two techniques: structuring the trainingdata to isolate bit-level rules, and controlling correlations in the NTK regimeto align model predictions with the target algorithmic executions.</description><author>Artur Back de Luca, George Giapitzakis, Kimon Fountoulakis</author><pubDate>Wed, 22 Oct 2025 15:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.16763v3</guid></item><item><title>3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding</title><link>http://arxiv.org/abs/2510.16780v2</link><description>Masked graph modeling (MGM) is a promising approach for molecularrepresentation learning (MRL).However, extending the success of re-maskdecoding from 2D to 3D MGM is non-trivial, primarily due to two conflictingchallenges: avoiding 2D structure leakage to the decoder, while still providingsufficient 2D context for reconstructing re-masked atoms. To address thesechallenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder withSelective Re-mask Decoding. The core innovation of 3D-GSRD lies in itsSelective Re-mask Decoding(SRD), which re-masks only 3D-relevant informationfrom encoder representations while preserving the 2D graph structures. This SRDis synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)encoder alongside a structure-independent decoder. We analyze that SRD,combined with the structure-independent decoder, enhances the encoder's role inMRL. Extensive experiments show that 3D-GSRD achieves strong downstreamperformance, setting a new state-of-the-art on 7 out of 8 targets in the widelyused MD17 molecular property prediction benchmark. The code is released athttps://github.com/WuChang0124/3D-GSRD.</description><author>Chang Wu, Zhiyuan Liu, Wen Shu, Liang Wang, Yanchen Luo, Wenqiang Lei, Yatao Bian, Junfeng Fang, Xiang Wang</author><pubDate>Wed, 22 Oct 2025 15:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.16780v2</guid></item><item><title>PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs</title><link>http://arxiv.org/abs/2507.05101v2</link><description>Deep learning-based computational methods have achieved promising results inpredicting protein-protein interactions (PPIs). However, existing benchmarkspredominantly focus on isolated pairwise evaluations, overlooking a model'scapability to reconstruct biologically meaningful PPI networks, which iscrucial for biology research. To address this gap, we introduce PRING, thefirst comprehensive benchmark that evaluates protein-protein interactionprediction from a graph-level perspective. PRING curates a high-quality,multi-species PPI network dataset comprising 21,484 proteins and 186,818interactions, with well-designed strategies to address both data redundancy andleakage. Building on this golden-standard dataset, we establish twocomplementary evaluation paradigms: (1) topology-oriented tasks, which assessintra and cross-species PPI network construction, and (2) function-orientedtasks, including protein complex pathway prediction, GO module analysis, andessential protein justification. These evaluations not only reflect the model'scapability to understand the network topology but also facilitate proteinfunction annotation, biological module detection, and even disease mechanismanalysis. Extensive experiments on four representative model categories,consisting of sequence similarity-based, naive sequence-based, protein languagemodel-based, and structure-based approaches, demonstrate that current PPImodels have potential limitations in recovering both structural and functionalproperties of PPI networks, highlighting the gap in supporting real-worldbiological applications. We believe PRING provides a reliable platform to guidethe development of more effective PPI prediction models for the community. Thedataset and source code of PRING are available athttps://github.com/SophieSarceau/PRING.</description><author>Xinzhe Zheng, Hao Du, Fanding Xu, Jinzhe Li, Zhiyuan Liu, Wenkang Wang, Tao Chen, Wanli Ouyang, Stan Z. Li, Yan Lu, Nanqing Dong, Yang Zhang</author><pubDate>Wed, 22 Oct 2025 15:38:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05101v2</guid></item><item><title>Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation</title><link>http://arxiv.org/abs/2510.19689v1</link><description>Industrial and government organizations increasingly depend on data-drivenanalytics for workforce, finance, and regulated decision processes, wheretimeliness, cost efficiency, and compliance are critical. Distributedframeworks such as Spark and Flink remain effective for massive-scale batch orstreaming analytics but introduce coordination complexity and auditingoverheads that misalign with moderate-scale, latency-sensitive inference.Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNetenable interpretable tabular ML, motivating new deployment blueprints forregulated environments. In this paper, we present a production-oriented BigData as a Service (BDaaS) blueprint that integrates a single-node serverlessGPU runtime with TabNet. The design leverages GPU acceleration for throughput,serverless elasticity for cost reduction, and feature-mask interpretability forIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,comparing our approach against Spark and CPU baselines. Our results show thatGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%lower cost per 1K inferences compared to Spark baselines, while compliancemechanisms add only ~5.7 ms latency with p99 &lt; 22 ms. Interpretability remainsstable under peak load, ensuring reliable auditability. Taken together, thesefindings provide a compliance-aware benchmark, a reproducible Helm-packagedblueprint, and a decision framework that demonstrate the practicality ofsecure, interpretable, and cost-efficient serverless GPU analytics forregulated enterprise and government settings.</description><author>Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang</author><pubDate>Wed, 22 Oct 2025 15:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.19689v1</guid></item><item><title>Discretized Gaussian Representation for Tomographic Reconstruction</title><link>http://arxiv.org/abs/2411.04844v4</link><description>Computed Tomography (CT) enables detailed cross-sectional imaging butcontinues to face challenges in balancing reconstruction quality andcomputational efficiency. While deep learning-based methods have significantlyimproved image quality and noise reduction, they typically require large-scaletraining data and intensive computation. Recent advances in scenereconstruction, such as Neural Radiance Fields and 3D Gaussian Splatting, offeralternative perspectives but are not well-suited for direct volumetric CTreconstruction. In this work, we propose Discretized Gaussian Representation(DGR), a novel framework that reconstructs the 3D volume directly using a setof discretized Gaussian functions in an end-to-end manner. To further enhanceefficiency, we introduce Fast Volume Reconstruction, a highly parallelizedtechnique that aggregates Gaussian contributions into the voxel grid withminimal overhead. Extensive experiments on both real-world and syntheticdatasets demonstrate that DGR achieves superior reconstruction quality andruntime performance across various CT reconstruction scenarios. Our code ispublicly available at https://github.com/wskingdom/DGR.</description><author>Shaokai Wu, Yuxiang Lu, Yapan Guo, Wei Ji, Suizhi Huang, Fengyu Yang, Shalayiding Sirejiding, Qichen He, Jing Tong, Yanbiao Ji, Yue Ding, Hongtao Lu</author><pubDate>Wed, 22 Oct 2025 15:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04844v4</guid></item></channel></rss>