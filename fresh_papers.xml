<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 25 Jul 2024 01:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?</title><link>http://arxiv.org/abs/2404.03302v3</link><description>By leveraging the retrieval of information from external knowledge databases,Large Language Models (LLMs) exhibit enhanced capabilities for accomplishingmany knowledge-intensive tasks. However, due to the inherent flaws of currentretrieval systems, there might exist irrelevant information within thoseretrieving top-ranked passages. In this work, we present a comprehensiveinvestigation into the robustness of LLMs to different types of irrelevantinformation under various conditions. We initially introduce a framework toconstruct high-quality irrelevant information that ranges from semanticallyunrelated, partially related, and related to questions. Furthermore, ouranalysis demonstrates that the constructed irrelevant information not onlyscores highly on similarity metrics, being highly retrieved by existingsystems, but also bears semantic connections to the context. Our investigationreveals that current LLMs still face challenges in discriminating highlysemantically related information and can be easily distracted by theseirrelevant yet misleading content. Besides, we also find that current solutionsfor handling irrelevant information have limitations in improving therobustness of LLMs to such distractions. All the resources are available onGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.</description><author>Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao</author><pubDate>Wed, 24 Jul 2024 15:51:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03302v3</guid></item><item><title>AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game</title><link>http://arxiv.org/abs/2407.16521v2</link><description>Strategic social deduction games serve as valuable testbeds for evaluatingthe understanding and inference skills of language models, offering crucialinsights into social science, artificial intelligence, and strategic gaming.This paper focuses on creating proxies of human behavior in simulatedenvironments, with Among Us utilized as a tool for studying simulated humanbehavior. The study introduces a text-based game environment, namedAmongAgents, that mirrors the dynamics of Among Us. Players act as crew membersaboard a spaceship, tasked with identifying impostors who are sabotaging theship and eliminating the crew. Within this environment, the behavior ofsimulated language agents is analyzed. The experiments involve diverse gamesequences featuring different configurations of Crewmates and Impostorpersonality archetypes. Our work demonstrates that state-of-the-art largelanguage models (LLMs) can effectively grasp the game rules and make decisionsbased on the current context. This work aims to promote further exploration ofLLMs in goal-oriented games with incomplete information and complex actionspaces, as these settings offer valuable opportunities to assess language modelperformance in socially driven scenarios.</description><author>Yizhou Chi, Lingjun Mao, Zineng Tang</author><pubDate>Wed, 24 Jul 2024 15:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16521v2</guid></item><item><title>Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models for Autonomous Vehicles</title><link>http://arxiv.org/abs/2407.16636v2</link><description>Fusing different sensor modalities can be a difficult task, particularly ifthey are asynchronous. Asynchronisation may arise due to long processing timesor improper synchronisation during calibration, and there must exist a way tostill utilise this previous information for the purpose of safe driving, andobject detection in ego vehicle/ multi-agent trajectory prediction.Difficulties arise in the fact that the sensor modalities have capturedinformation at different times and also at different positions in space.Therefore, they are not spatially nor temporally aligned. This paper willinvestigate the challenge of radar and LiDAR sensors being asynchronousrelative to the camera sensors, for various time latencies. The spatialalignment will be resolved before lifting into BEV space via the transformationof the radar/LiDAR point clouds into the new ego frame coordinate system. Onlyafter this can we concatenate the radar/LiDAR point cloud and lifted camerafeatures. Temporal alignment will be remedied for radar data only, we willimplement a novel method of inferring the future radar point positions usingthe velocity information. Our approach to resolving the issue of sensorasynchrony yields promising results. We demonstrate velocity information candrastically improve IoU for asynchronous datasets, as for a time latency of 360milliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a timelatency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR(C+L) model by 0.18 IoU. This is an advancement in utilising theoften-neglected radar sensor modality, which is less favoured than LiDAR forautonomous driving purposes.</description><author>Seamie Hayes, Sushil Sharma, Ciarán Eising</author><pubDate>Wed, 24 Jul 2024 13:04:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16636v2</guid></item><item><title>On the Utility of Speech and Audio Foundation Models for Marmoset Call Analysis</title><link>http://arxiv.org/abs/2407.16417v2</link><description>Marmoset monkeys encode vital information in their calls and serve as asurrogate model for neuro-biologists to understand the evolutionary origins ofhuman vocal communication. Traditionally analyzed with signal processing-basedfeatures, recent approaches have utilized self-supervised models pre-trained onhuman speech for feature extraction, capitalizing on their ability to learn asignal's intrinsic structure independently of its acoustic domain. However, theutility of such foundation models remains unclear for marmoset call analysis interms of multi-class classification, bandwidth, and pre-training domain. Thisstudy assesses feature representations derived from speech and general audiodomains, across pre-training bandwidths of 4, 8, and 16 kHz for marmosetcall-type and caller classification tasks. Results show that models with higherbandwidth improve performance, and pre-training on speech or general audioyields comparable results, improving over a spectral baseline.</description><author>Eklavya Sarkar, Mathew Magimai. -Doss</author><pubDate>Wed, 24 Jul 2024 11:19:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16417v2</guid></item><item><title>Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning</title><link>http://arxiv.org/abs/2407.16564v2</link><description>Text-to-music models allow users to generate nearly realistic musical audiowith textual commands. However, editing music audios remains challenging due tothe conflicting desiderata of performing fine-grained alterations on the audiowhile maintaining a simple user interface. To address this challenge, wepropose Audio Prompt Adapter (or AP-Adapter), a lightweight addition topretrained text-to-music models. We utilize AudioMAE to extract features fromthe input audio, and construct attention-based adapters to feedthese featuresinto the internal layers of AudioLDM2, a diffusion-based text-to-music model.With 22M trainable parameters, AP-Adapter empowers users to harness both global(e.g., genre and timbre) and local (e.g., melody) aspects of music, using theoriginal audio and a short text as inputs. Through objective and subjectivestudies, we evaluate AP-Adapter on three tasks: timbre transfer, genretransfer, and accompaniment generation. Additionally, we demonstrate itseffectiveness on out-of-domain audios containing unseen instruments duringtraining.</description><author>Fang-Duo Tsai, Shih-Lun Wu, Haven Kim, Bo-Yu Chen, Hao-Chung Cheng, Yi-Hsuan Yang</author><pubDate>Wed, 24 Jul 2024 11:12:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16564v2</guid></item><item><title>A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data</title><link>http://arxiv.org/abs/2407.16680v2</link><description>Despite the availability of international prize-money competitions, scaledvehicles, and simulation environments, research on autonomous racing and thecontrol of sports cars operating close to the limit of handling has beenlimited by the high costs of vehicle acquisition and management, as well as thelimited physics accuracy of open-source simulators. In this paper, we propose aracing simulation platform based on the simulator Assetto Corsa to test,validate, and benchmark autonomous driving algorithms, including reinforcementlearning (RL) and classical Model Predictive Control (MPC), in realistic andchallenging scenarios. Our contributions include the development of thissimulation platform, several state-of-the-art algorithms tailored to the racingenvironment, and a comprehensive dataset collected from human drivers.Additionally, we evaluate algorithms in the offline RL setting. All thenecessary code (including environment and benchmarks), working examples,datasets, and videos are publicly released and can be found at:https://assetto-corsa-gym.github.io</description><author>Adrian Remonda, Nicklas Hansen, Ayoub Raji, Nicola Musiu, Marko Bertogna, Eduardo Veas, Xiaolong Wang</author><pubDate>Wed, 24 Jul 2024 10:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16680v2</guid></item><item><title>SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2407.16344v2</link><description>High frame-rate (HFR) videos of action recognition improve fine-grainedexpression while reducing the spatio-temporal relation and motion informationdensity. Thus, large amounts of video samples are continuously required fortraditional data-driven training. However, samples are not always sufficient inreal-world scenarios, promoting few-shot action recognition (FSAR) research. Weobserve that most recent FSAR works build spatio-temporal relation of videosamples via temporal alignment after spatial feature extraction, cutting apartspatial and temporal features within samples. They also capture motioninformation via narrow perspectives between adjacent frames without consideringdensity, leading to insufficient motion information capturing. Therefore, wepropose a novel plug-and-play architecture for FSAR called Spatio-tempOralfrAme tuPle enhancer (SOAP) in this paper. The model we designed with sucharchitecture refers to SOAP-Net. Temporal connections between different featurechannels and spatio-temporal relation of features are considered instead ofsimple feature extraction. Comprehensive motion information is also captured,using frame tuples with multiple frames containing more motion information thanadjacent frames. Combining frame tuples of diverse frame counts furtherprovides a broader perspective. SOAP-Net achieves new state-of-the-artperformance across well-known benchmarks such as SthSthV2, Kinetics, UCF101,and HMDB51. Extensive empirical evaluations underscore the competitiveness,pluggability, generalization, and robustness of SOAP. The code is released athttps://github.com/wenbohuang1002/SOAP.</description><author>Wenbo Huang, Jinghui Zhang, Xuwei Qian, Zhen Wu, Meng Wang, Lei Zhang</author><pubDate>Wed, 24 Jul 2024 08:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16344v2</guid></item><item><title>Aggregated Attributions for Explanatory Analysis of 3D Segmentation Models</title><link>http://arxiv.org/abs/2407.16653v2</link><description>Analysis of 3D segmentation models, especially in the context of medicalimaging, is often limited to segmentation performance metrics that overlook thecrucial aspect of explainability and bias. Currently, effectively explainingthese models with saliency maps is challenging due to the high dimensions ofinput images multiplied by the ever-growing number of segmented class labels.To this end, we introduce Agg^2Exp, a methodology for aggregating fine-grainedvoxel attributions of the segmentation model's predictions. Unlike classicalexplanation methods that primarily focus on the local feature attribution,Agg^2Exp enables a more comprehensive global view on the importance ofpredicted segments in 3D images. Our benchmarking experiments show thatgradient-based voxel attributions are more faithful to the model's predictionsthan perturbation-based explanations. As a concrete use-case, we apply Agg^2Expto discover knowledge acquired by the Swin UNEt TRansformer model trained onthe TotalSegmentator v2 dataset for segmenting anatomical structures incomputed tomography medical images. Agg^2Exp facilitates the explanatoryanalysis of large segmentation models beyond their predictive performance.</description><author>Maciej Chrabaszcz, Hubert Baniecki, Piotr Komorowski, Szymon Płotka, Przemyslaw Biecek</author><pubDate>Wed, 24 Jul 2024 07:18:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16653v2</guid></item><item><title>A Faster Branching Algorithm for the Maximum $k$-Defective Clique Problem</title><link>http://arxiv.org/abs/2407.16588v2</link><description>A $k$-defective clique of an undirected graph $G$ is a subset of its verticesthat induces a nearly complete graph with a maximum of $k$ missing edges. Themaximum $k$-defective clique problem, which asks for the largest $k$-defectiveclique from the given graph, is important in many applications, such as socialand biological network analysis. In the paper, we propose a new branchingalgorithm that takes advantage of the structural properties of the$k$-defective clique and uses the efficient maximum clique algorithm as asubroutine. As a result, the algorithm has a better asymptotic running timethan the existing ones. We also investigate upper-bounding techniques andpropose a new upper bound utilizing the \textit{conflict relationship} betweenvertex pairs. Because conflict relationship is common in many graph problems,we believe that this technique can be potentially generalized. Finally,experiments show that our algorithm outperforms state-of-the-art solvers on awide range of open benchmarks.</description><author>Chunyu Luo, Yi Zhou, Zhengren Wang, Mingyu Xiao</author><pubDate>Wed, 24 Jul 2024 02:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16588v2</guid></item><item><title>MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues</title><link>http://arxiv.org/abs/2407.16552v2</link><description>Multimodal Large Language Models (MLLMs) have demonstrated remarkablemultimodal emotion recognition capabilities, integrating multimodal cues fromvisual, acoustic, and linguistic contexts in the video to recognize humanemotional states. However, existing methods ignore capturing local facialfeatures of temporal dynamics of micro-expressions and do not leverage thecontextual dependencies of the utterance-aware temporal segments in the video,thereby limiting their expected effectiveness to a certain extent. In thiswork, we propose MicroEmo, a time-sensitive MLLM aimed at directing attentionto the local facial micro-expression dynamics and the contextual dependenciesof utterance-aware video clips. Our model incorporates two key architecturalcontributions: (1) a global-local attention visual encoder that integratesglobal frame-level timestamp-bound image features with local facial features oftemporal dynamics of micro-expressions; (2) an utterance-aware video Q-Formerthat captures multi-scale and contextual dependencies by generating visualtoken sequences for each utterance segment and for the entire video thencombining them. Preliminary qualitative experiments demonstrate that in a newExplainable Multimodal Emotion Recognition (EMER) task that exploitsmulti-modal and multi-faceted clues to predict emotions in an open-vocabulary(OV) manner, MicroEmo demonstrates its effectiveness compared with the latestmethods.</description><author>Liyun Zhang</author><pubDate>Wed, 24 Jul 2024 01:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16552v2</guid></item><item><title>FCNR: Fast Compressive Neural Representation of Visualization Images</title><link>http://arxiv.org/abs/2407.16369v2</link><description>We present FCNR, a fast compressive neural representation for tens ofthousands of visualization images under varying viewpoints and timesteps. Theexisting NeRVI solution, albeit enjoying a high compression ratio, incurs slowspeeds in encoding and decoding. Built on the recent advances in stereo imagecompression, FCNR assimilates stereo context modules and joint context transfermodules to compress image pairs. Our solution significantly improves encodingand decoding speed while maintaining high reconstruction quality and satisfyingcompression ratio. To demonstrate its effectiveness, we compare FCNR withstate-of-the-art neural compression methods, including E-NeRV, HNeRV, NeRVI,and ECSIC. The source code can be found athttps://github.com/YunfeiLu0112/FCNR.</description><author>Yunfei Lu, Pengfei Gu, Chaoli Wang</author><pubDate>Wed, 24 Jul 2024 00:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16369v2</guid></item><item><title>Diffusion Models for Monocular Depth Estimation: Overcoming Challenging Conditions</title><link>http://arxiv.org/abs/2407.16698v1</link><description>We present a novel approach designed to address the complexities posed bychallenging, out-of-distribution data in the single-image depth estimationtask. Starting with images that facilitate depth prediction due to the absenceof unfavorable factors, we systematically generate new, user-defined sceneswith a comprehensive set of challenges and associated depth information. Thisis achieved by leveraging cutting-edge text-to-image diffusion models withdepth-aware control, known for synthesizing high-quality image content fromtextual prompts while preserving the coherence of 3D structure betweengenerated and source imagery. Subsequent fine-tuning of any monocular depthnetwork is carried out through a self-distillation protocol that takes intoaccount images generated using our strategy and its own depth predictions onsimple, unchallenging scenes. Experiments on benchmarks tailored for ourpurposes demonstrate the effectiveness and versatility of our proposal.</description><author>Fabio Tosi, Pierluigi Zama Ramirez, Matteo Poggi</author><pubDate>Tue, 23 Jul 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16698v1</guid></item><item><title>AbdomenAtlas: A Large-Scale, Detailed-Annotated, &amp; Multi-Center Dataset for Efficient Transfer Learning and Open Algorithmic Benchmarking</title><link>http://arxiv.org/abs/2407.16697v1</link><description>We introduce the largest abdominal CT dataset (termed AbdomenAtlas) of 20,460three-dimensional CT volumes sourced from 112 hospitals across diversepopulations, geographies, and facilities. AbdomenAtlas provides 673Khigh-quality masks of anatomical structures in the abdominal region annotatedby a team of 10 radiologists with the help of AI algorithms. We start by havingexpert radiologists manually annotate 22 anatomical structures in 5,246 CTvolumes. Following this, a semi-automatic annotation procedure is performed forthe remaining CT volumes, where radiologists revise the annotations predictedby AI, and in turn, AI improves its predictions by learning from revisedannotations. Such a large-scale, detailed-annotated, and multi-center datasetis needed for two reasons. Firstly, AbdomenAtlas provides important resourcesfor AI development at scale, branded as large pre-trained models, which canalleviate the annotation workload of expert radiologists to transfer to broaderclinical applications. Secondly, AbdomenAtlas establishes a large-scalebenchmark for evaluating AI algorithms -- the more data we use to test thealgorithms, the better we can guarantee reliable performance in complexclinical scenarios. An ISBI &amp; MICCAI challenge named BodyMaps: Towards 3D Atlasof Human Body was launched using a subset of our AbdomenAtlas, aiming tostimulate AI innovation and to benchmark segmentation accuracy, inferenceefficiency, and domain generalizability. We hope our AbdomenAtlas can set thestage for larger-scale clinical trials and offer exceptional opportunities topractitioners in the medical imaging community. Codes, models, and datasets areavailable at https://www.zongweiz.com/dataset</description><author>Wenxuan Li, Chongyu Qu, Xiaoxi Chen, Pedro R. A. S. Bassi, Yijia Shi, Yuxiang Lai, Qian Yu, Huimin Xue, Yixiong Chen, Xiaorui Lin, Yutong Tang, Yining Cao, Haoqi Han, Zheyuan Zhang, Jiawei Liu, Tiezheng Zhang, Yujiu Ma, Jincheng Wang, Guang Zhang, Alan Yuille, Zongwei Zhou</author><pubDate>Tue, 23 Jul 2024 17:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16697v1</guid></item><item><title>Molecular Topological Profile (MOLTOP) -- Simple and Strong Baseline for Molecular Graph Classification</title><link>http://arxiv.org/abs/2407.12136v3</link><description>We revisit the effectiveness of topological descriptors for molecular graphclassification and design a simple, yet strong baseline. We demonstrate that asimple approach to feature engineering - employing histogram aggregation ofedge descriptors and one-hot encoding for atomic numbers and bond types - whencombined with a Random Forest classifier, can establish a strong baseline forGraph Neural Networks (GNNs). The novel algorithm, Molecular TopologicalProfile (MOLTOP), integrates Edge Betweenness Centrality, Adjusted Rand Indexand SCAN Structural Similarity score. This approach proves to be remarkablycompetitive when compared to modern GNNs, while also being simple, fast,low-variance and hyperparameter-free. Our approach is rigorously tested onMoleculeNet datasets using fair evaluation protocol provided by Open GraphBenchmark. We additionally show out-of-domain generation capabilities onpeptide classification task from Long Range Graph Benchmark. The evaluationsacross eleven benchmark datasets reveal MOLTOP's strong discriminativecapabilities, surpassing the $1$-WL test and even $3$-WL test for some classesof graphs. Our conclusion is that descriptor-based baselines, such as the onewe propose, are still crucial for accurately assessing advancements in the GNNdomain.</description><author>Jakub Adamczyk, Wojciech Czech</author><pubDate>Tue, 23 Jul 2024 17:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12136v3</guid></item><item><title>PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects</title><link>http://arxiv.org/abs/2407.16696v1</link><description>We present PartGLEE, a part-level foundation model for locating andidentifying both objects and parts in images. Through a unified framework,PartGLEE accomplishes detection, segmentation, and grounding of instances atany granularity in the open world scenario. Specifically, we propose a Q-Formerto construct the hierarchical relationship between objects and parts, parsingevery object into corresponding semantic parts. By incorporating a large amountof object-level data, the hierarchical relationships can be extended, enablingPartGLEE to recognize a rich variety of parts. We conduct comprehensive studiesto validate the effectiveness of our method, PartGLEE achieves thestate-of-the-art performance across various part-level tasks and obtaincompetitive results on object-level tasks. The proposed PartGLEE significantlyenhances hierarchical modeling capabilities and part-level perception over ourprevious GLEE model. Further analysis indicates that the hierarchical cognitiveability of PartGLEE is able to facilitate a detailed comprehension in imagesfor mLLMs. The model and code will be released athttps://provencestar.github.io/PartGLEE-Vision/ .</description><author>Junyi Li, Junfeng Wu, Weizhi Zhao, Song Bai, Xiang Bai</author><pubDate>Tue, 23 Jul 2024 17:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16696v1</guid></item><item><title>Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack</title><link>http://arxiv.org/abs/2407.16695v1</link><description>We introduce Lifelong ICL, a problem setting that challenges long-contextlanguage models (LMs) to learn from a sequence of language tasks throughin-context learning (ICL). We further introduce Task Haystack, an evaluationsuite dedicated to assessing and diagnosing how long-context LMs utilizescontexts in Lifelong ICL. When given a task instruction and test inputs,long-context LMs are expected to leverage the relevant demonstrations in theLifelong ICL prompt, avoid distraction and interference from other tasks, andachieve test accuracies that are not significantly worse than the Single-taskICL baseline. Task Haystack draws inspiration from the widely-adopted"needle-in-a-haystack" (NIAH) evaluation, but presents new and uniquechallenges. It demands that models (1) utilize the contexts with deeperunderstanding, rather than resorting to simple copying and pasting; (2)navigate through long streams of evolving topics and tasks, which closelyapproximates the complexities of real-world usage of long-context LMs.Additionally, Task Haystack inherits the controllability aspect of NIAH,providing model developers with tools and visualizations to identify modelvulnerabilities effectively. We benchmark 12 long-context LMs using Task Haystack. We find thatstate-of-the-art closed models such as GPT-4o still struggle in this setting,failing 15% of the cases on average, while all open-weight models we evaluatefurther lack behind by a large margin, failing up to 61% of the cases. In ourcontrolled analysis, we identify factors such as distraction and recency biasas contributors to these failure cases. Further, we observe declines inperformance when task instructions are paraphrased at test time or when ICLdemonstrations are repeated excessively, raising concerns about the robustness,instruction understanding, and true context utilization of current long-contextLMs.</description><author>Xiaoyue Xu, Qinyuan Ye, Xiang Ren</author><pubDate>Tue, 23 Jul 2024 17:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16695v1</guid></item><item><title>Explanation Regularisation through the Lens of Attributions</title><link>http://arxiv.org/abs/2407.16693v1</link><description>Explanation regularisation (ER) has been introduced as a way to guide modelsto make their predictions in a manner more akin to humans, i.e., making theirattributions "plausible". This is achieved by introducing an auxiliaryexplanation loss, that measures how well the output of an input attributiontechnique for the model agrees with relevant human-annotated rationales. Onepositive outcome of using ER appears to be improved performance inout-of-domain (OOD) settings, presumably due to an increased reliance on"plausible" tokens. However, previous work has under-explored the impact of theER objective on model attributions, in particular when obtained with techniquesother than the one used to train ER. In this work, we contribute a study ofER's effectiveness at informing classification decisions on plausible tokens,and the relationship between increased plausibility and robustness to OODconditions. Through a series of analyses, we find that the connection betweenER and the ability of a classifier to rely on plausible features has beenoverstated and that a stronger reliance on plausible tokens does not seem to bethe cause for any perceived OOD improvements.</description><author>Pedro Ferreira, Wilker Aziz, Ivan Titov</author><pubDate>Tue, 23 Jul 2024 17:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16693v1</guid></item><item><title>Deep learning empowered sensor fusion boosts infant movement classification</title><link>http://arxiv.org/abs/2406.09014v4</link><description>There is a recent boom in the development of AI solutions to facilitate andenhance diagnostic procedures for established clinical tools. To assess theintegrity of the developing nervous system, the Prechtl general movementassessment (GMA) is recognized for its clinical value in diagnosingneurological impairments in early infancy. GMA has been increasingly augmentedthrough machine learning approaches intending to scale-up its application,circumvent costs in the training of human assessors and further standardizeclassification of spontaneous motor patterns. Available deep learning tools,all of which are based on single sensor modalities, are however stillconsiderably inferior to that of well-trained human assessors. These approachesare hardly comparable as all models are designed, trained and evaluated onproprietary/silo-data sets. With this study we propose a sensor fusion approachfor assessing fidgety movements (FMs) comparing three different sensormodalities (pressure, inertial, and visual sensors). Various combinations andtwo sensor fusion approaches (late and early fusion) for infant movementclassification were tested to evaluate whether a multi-sensor systemoutperforms single modality assessments. The performance of the three-sensorfusion (classification accuracy of 94.5\%) was significantly higher than thatof any single modality evaluated, suggesting the sensor fusion approach is apromising avenue for automated classification of infant motor patterns. Thedevelopment of a robust sensor fusion system may significantly enhance AI-basedearly recognition of neurofunctions, ultimately facilitating automated earlydetection of neurodevelopmental conditions.</description><author>Tomas Kulvicius, Dajie Zhang, Luise Poustka, Sven Bölte, Lennart Jahn, Sarah Flügge, Marc Kraft, Markus Zweckstetter, Karin Nielsen-Saines, Florentin Wörgötter, Peter B Marschik</author><pubDate>Tue, 23 Jul 2024 17:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09014v4</guid></item><item><title>Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</title><link>http://arxiv.org/abs/2403.09636v2</link><description>Transformers have emerged as the backbone of large language models (LLMs).However, generation remains inefficient due to the need to store in memory acache of key-value representations for past tokens, whose size scales linearlywith the input sequence length and batch size. As a solution, we proposeDynamic Memory Compression (DMC), a method for online key-value cachecompression at inference time. Most importantly, the model learns to applydifferent compression ratios in different heads and layers. We retrofitpre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,achieving up to 7x throughput increase during auto-regressive inference on anNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligiblepercentage of the original data without adding any extra parameters. DMCpreserves the original downstream performance with up to 4x cache compression,outperforming up-trained grouped-query attention (GQA) and key-value evictionpolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compoundedgains. Hence, DMC can serve as a drop-in replacement for KV caching in existingLLMs to fit longer contexts and larger batches within any given memory budget.</description><author>Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti</author><pubDate>Tue, 23 Jul 2024 17:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09636v2</guid></item><item><title>Automatic Equalization for Individual Instrument Tracks Using Convolutional Neural Networks</title><link>http://arxiv.org/abs/2407.16691v1</link><description>We propose a novel approach for the automatic equalization of individualmusical instrument tracks. Our method begins by identifying the instrumentpresent within a source recording in order to choose its corresponding idealspectrum as a target. Next, the spectral difference between the recording andthe target is calculated, and accordingly, an equalizer matching model is usedto predict settings for a parametric equalizer. To this end, we build upon adifferentiable parametric equalizer matching neural network, demonstratingimprovements relative to previously established state-of-the-art. Unlike pastapproaches, we show how our system naturally allows real-world audio data to beleveraged during the training of our matching model, effectively generatingsuitably produced training targets in an automated manner mirroring conditionsat inference time. Consequently, we illustrate how fine-tuning our matchingmodel on such examples considerably improves parametric equalizer matchingperformance in real-world scenarios, decreasing mean absolute error by 24%relative to methods relying solely on random parameter sampling techniques as aself-supervised learning strategy. We perform listening tests, and demonstratethat our proposed automatic equalization solution subjectively enhances thetonal characteristics for recordings of common instrument types.</description><author>Florian Mockenhaupt, Joscha Simon Rieber, Shahan Nercessian</author><pubDate>Tue, 23 Jul 2024 17:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16691v1</guid></item><item><title>Can Large Language Models Automatically Jailbreak GPT-4V?</title><link>http://arxiv.org/abs/2407.16686v1</link><description>GPT-4V has attracted considerable attention due to its extraordinary capacityfor integrating and processing multimodal information. At the same time, itsability of face recognition raises new safety concerns of privacy leakage.Despite researchers' efforts in safety alignment through RLHF or preprocessingfilters, vulnerabilities might still be exploited. In our study, we introduceAutoJailbreak, an innovative automatic jailbreak technique inspired by promptoptimization. We leverage Large Language Models (LLMs) for red-teaming torefine the jailbreak prompt and employ weak-to-strong in-context learningprompts to boost efficiency. Furthermore, we present an effective search methodthat incorporates early stopping to minimize optimization time and tokenexpenditure. Our experiments demonstrate that AutoJailbreak significantlysurpasses conventional methods, achieving an Attack Success Rate (ASR)exceeding 95.3\%. This research sheds light on strengthening GPT-4V security,underscoring the potential for LLMs to be exploited in compromising GPT-4Vintegrity.</description><author>Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun</author><pubDate>Tue, 23 Jul 2024 17:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16686v1</guid></item><item><title>AutoRG-Brain: Grounded Report Generation for Brain MRI</title><link>http://arxiv.org/abs/2407.16684v1</link><description>Radiologists are tasked with interpreting a large number of images in a dailybase, with the responsibility of generating corresponding reports. Thisdemanding workload elevates the risk of human error, potentially leading totreatment delays, increased healthcare costs, revenue loss, and operationalinefficiencies. To address these challenges, we initiate a series of work ongrounded Automatic Report Generation (AutoRG), starting from the brain MRIinterpretation system, which supports the delineation of brain structures, thelocalization of anomalies, and the generation of well-organized findings. Wemake contributions from the following aspects, first, on dataset construction,we release a comprehensive dataset encompassing segmentation masks of anomalyregions and manually authored reports, termed as RadGenome-Brain MRI. This dataresource is intended to catalyze ongoing research and development in the fieldof AI-assisted report generation systems. Second, on system design, we proposeAutoRG-Brain, the first brain MRI report generation system with pixel-levelgrounded visual clues. Third, for evaluation, we conduct quantitativeassessments and human evaluations of brain structure segmentation, anomalylocalization, and report generation tasks to provide evidence of itsreliability and accuracy. This system has been integrated into real clinicalscenarios, where radiologists were instructed to write reports based on ourgenerated findings and anomaly segmentation masks. The results demonstrate thatour system enhances the report-writing skills of junior doctors, aligning theirperformance more closely with senior doctors, thereby boosting overallproductivity.</description><author>Jiayu Lei, Xiaoman Zhang, Chaoyi Wu, Lisong Dai, Ya Zhang, Yanyong Zhang, Yanfeng Wang, Weidi Xie, Yuehua Li</author><pubDate>Tue, 23 Jul 2024 17:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16684v1</guid></item><item><title>SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation</title><link>http://arxiv.org/abs/2407.16682v1</link><description>The Segment Anything model (SAM) has shown a generalized ability to groupimage pixels into patches, but applying it to semantic-aware segmentation stillfaces major challenges. This paper presents SAM-CP, a simple approach thatestablishes two types of composable prompts beyond SAM and composes them forversatile segmentation. Specifically, given a set of classes (in texts) and aset of SAM patches, the Type-I prompt judges whether a SAM patch aligns with atext label, and the Type-II prompt judges whether two SAM patches with the sametext label also belong to the same instance. To decrease the complexity indealing with a large number of semantic classes and patches, we establish aunified framework that calculates the affinity between (semantic and instance)queries and SAM patches and merges patches with high affinity to the query.Experiments show that SAM-CP achieves semantic, instance, and panopticsegmentation in both open and closed domains. In particular, it achievesstate-of-the-art performance in open-vocabulary segmentation. Our researchoffers a novel and generalized methodology for equipping vision foundationmodels like SAM with multi-grained semantic perception abilities.</description><author>Pengfei Chen, Lingxi Xie, Xinyue Huo, Xuehui Yu, Xiaopeng Zhang, Yingfei Sun, Zhenjun Han, Qi Tian</author><pubDate>Tue, 23 Jul 2024 17:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16682v1</guid></item><item><title>A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data</title><link>http://arxiv.org/abs/2407.16680v1</link><description>Despite the availability of international prize-money competitions, scaledvehicles, and simulation environments, research on autonomous racing and thecontrol of sports cars operating close to the limit of handling has beenlimited by the high costs of vehicle acquisition and management, as well as thelimited physics accuracy of open-source simulators. In this paper, we propose aracing simulation platform based on the simulator Assetto Corsa to test,validate, and benchmark autonomous driving algorithms, including reinforcementlearning (RL) and classical Model Predictive Control (MPC), in realistic andchallenging scenarios. Our contributions include the development of thissimulation platform, several state-of-the-art algorithms tailored to the racingenvironment, and a comprehensive dataset collected from human drivers.Additionally, we evaluate algorithms in the offline RL setting. All thenecessary code (including environment and benchmarks), working examples,datasets, and videos are publicly released and can be found at:\url{https://assetto-corsa-gym.github.io}.</description><author>Adrian Remonda, Nicklas Hansen, Ayoub Raji, Nicola Musiu, Marko Bertogna, Eduardo Veas, Xiaolong Wang</author><pubDate>Tue, 23 Jul 2024 17:45:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16680v1</guid></item><item><title>From Imitation to Refinement -- Residual RL for Precise Visual Assembly</title><link>http://arxiv.org/abs/2407.16677v1</link><description>Behavior cloning (BC) currently stands as a dominant paradigm for learningreal-world visual manipulation. However, in tasks that require locallycorrective behaviors like multi-part assembly, learning robust policies purelyfrom human demonstrations remains challenging. Reinforcement learning (RL) canmitigate these limitations by allowing policies to acquire locally correctivebehaviors through task reward supervision and exploration. This paper exploresthe use of RL fine-tuning to improve upon BC-trained policies in precisemanipulation tasks. We analyze and overcome technical challenges associatedwith using RL to directly train policy networks that incorporate modernarchitectural components like diffusion models and action chunking. We proposetraining residual policies on top of frozen BC-trained diffusion models usingstandard policy gradient methods and sparse rewards, an approach we call ResiP(Residual for Precise manipulation). Our experimental results demonstrate thatthis residual learning framework can significantly improve success rates beyondthe base BC-trained models in high-precision assembly tasks by learningcorrective actions. We also show that by combining ResiP with teacher-studentdistillation and visual domain randomization, our method can enable learningreal-world policies for robotic assembly directly from RGB images. Find videosand code at \url{https://residual-assembly.github.io}.</description><author>Lars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, Pulkit Agrawal</author><pubDate>Tue, 23 Jul 2024 17:44:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16677v1</guid></item><item><title>KAN or MLP: A Fairer Comparison</title><link>http://arxiv.org/abs/2407.16674v1</link><description>This paper does not introduce a novel method. Instead, it offers a fairer andmore comprehensive comparison of KAN and MLP models across various tasks,including machine learning, computer vision, audio processing, natural languageprocessing, and symbolic formula representation. Specifically, we control thenumber of parameters and FLOPs to compare the performance of KAN and MLP. Ourmain observation is that, except for symbolic formula representation tasks, MLPgenerally outperforms KAN. We also conduct ablation studies on KAN and findthat its advantage in symbolic formula representation mainly stems from itsB-spline activation function. When B-spline is applied to MLP, performance insymbolic formula representation significantly improves, surpassing or matchingthat of KAN. However, in other tasks where MLP already excels over KAN,B-spline does not substantially enhance MLP's performance. Furthermore, we findthat KAN's forgetting issue is more severe than that of MLP in a standardclass-incremental continual learning setting, which differs from the findingsreported in the KAN paper. We hope these results provide insights for futureresearch on KAN and other MLP alternatives. Project link:https://github.com/yu-rp/KANbeFair</description><author>Runpeng Yu, Weihao Yu, Xinchao Wang</author><pubDate>Tue, 23 Jul 2024 17:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16674v1</guid></item><item><title>FakingRecipe: Detecting Fake News on Short Video Platforms from the Perspective of Creative Process</title><link>http://arxiv.org/abs/2407.16670v1</link><description>As short-form video-sharing platforms become a significant channel for newsconsumption, fake news in short videos has emerged as a serious threat in theonline information ecosystem, making developing detection methods for this newscenario an urgent need. Compared with that in text and image formats, fakenews on short video platforms contains rich but heterogeneous information invarious modalities, posing a challenge to effective feature utilization. Unlikeexisting works mostly focusing on analyzing what is presented, we introduce anovel perspective that considers how it might be created. Through the lens ofthe creative process behind news video production, our empirical analysisuncovers the unique characteristics of fake news videos in material selectionand editing. Based on the obtained insights, we design FakingRecipe, a creativeprocess-aware model for detecting fake news short videos. It captures the fakenews preferences in material selection from sentimental and semantic aspectsand considers the traits of material editing from spatial and temporal aspects.To improve evaluation comprehensiveness, we first construct FakeTT, an Englishdataset for this task, and conduct experiments on both FakeTT and the existingChinese FakeSV dataset. The results show FakingRecipe's superiority indetecting fake news on short video platforms.</description><author>Yuyan Bu, Qiang Sheng, Juan Cao, Peng Qi, Danding Wang, Jintao Li</author><pubDate>Tue, 23 Jul 2024 17:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16670v1</guid></item><item><title>RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent</title><link>http://arxiv.org/abs/2407.16667v1</link><description>Recently, advanced Large Language Models (LLMs) such as GPT-4 have beenintegrated into many real-world applications like Code Copilot. Theseapplications have significantly expanded the attack surface of LLMs, exposingthem to a variety of threats. Among them, jailbreak attacks that induce toxicresponses through jailbreak prompts have raised critical safety concerns. Toidentify these threats, a growing number of red teaming approaches simulatepotential adversarial scenarios by crafting jailbreak prompts to test thetarget LLM. However, existing red teaming methods do not consider the uniquevulnerabilities of LLM in different scenarios, making it difficult to adjustthe jailbreak prompts to find context-specific vulnerabilities. Meanwhile,these methods are limited to refining jailbreak templates using a few mutationoperations, lacking the automation and scalability to adapt to differentscenarios. To enable context-aware and efficient red teaming, we abstract andmodel existing attacks into a coherent concept called "jailbreak strategy" andpropose a multi-agent LLM system named RedAgent that leverages these strategiesto generate context-aware jailbreak prompts. By self-reflecting on contextualfeedback in an additional memory buffer, RedAgent continuously learns how toleverage these strategies to achieve effective jailbreaks in specific contexts.Extensive experiments demonstrate that our system can jailbreak most black-boxLLMs in just five queries, improving the efficiency of existing red teamingmethods by two times. Additionally, RedAgent can jailbreak customized LLMapplications more efficiently. By generating context-aware jailbreak promptstowards applications on GPTs, we discover 60 severe vulnerabilities of thesereal-world applications with only two queries per vulnerability. We havereported all found issues and communicated with OpenAI and Meta for bug fixes.</description><author>Huiyu Xu, Wenhui Zhang, Zhibo Wang, Feng Xiao, Rui Zheng, Yunhe Feng, Zhongjie Ba, Kui Ren</author><pubDate>Tue, 23 Jul 2024 17:34:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16667v1</guid></item><item><title>Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods</title><link>http://arxiv.org/abs/2302.14368v3</link><description>As Diffusion Models have shown promising performance, a lot of efforts havebeen made to improve the controllability of Diffusion Models. However, how totrain Diffusion Models to have the disentangled latent spaces and how tonaturally incorporate the disentangled conditions during the sampling processhave been underexplored. In this paper, we present a training framework forfeature disentanglement of Diffusion Models (FDiff). We further propose twosampling methods that can boost the realism of our Diffusion Models and alsoenhance the controllability. Concisely, we train Diffusion Models conditionedon two latent features, a spatial content mask, and a flattened styleembedding. We rely on the inductive bias of the denoising process of DiffusionModels to encode pose/layout information in the content feature andsemantic/style information in the style feature. Regarding the samplingmethods, we first generalize Composable Diffusion Models (GCDM) by breaking theconditional independence assumption to allow for some dependence betweenconditional inputs, which is shown to be effective in realistic generation inour experiments. Second, we propose timestep-dependent weight scheduling forcontent and style features to further improve the performance. We also observebetter controllability of our proposed methods compared to existing methods inimage manipulation and image translation.</description><author>Wonwoong Cho, Hareesh Ravi, Midhun Harikumar, Vinh Khuc, Krishna Kumar Singh, Jingwan Lu, David I. Inouye, Ajinkya Kale</author><pubDate>Tue, 23 Jul 2024 17:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14368v3</guid></item><item><title>A Framework for Pupil Tracking with Event Cameras</title><link>http://arxiv.org/abs/2407.16665v1</link><description>Saccades are extremely rapid movements of both eyes that occursimultaneously, typically observed when an individual shifts their focus fromone object to another. These movements are among the swiftest produced byhumans and possess the potential to achieve velocities greater than that ofblinks. The peak angular speed of the eye during a saccade can reach as high as700{\deg}/s in humans, especially during larger saccades that cover a visualangle of 25{\deg}. Previous research has demonstrated encouraging outcomes incomprehending neurological conditions through the study of saccades. Anecessary step in saccade detection involves accurately identifying the preciselocation of the pupil within the eye, from which additional information such asgaze angles can be inferred. Conventional frame-based cameras often strugglewith the high temporal precision necessary for tracking very fast movements,resulting in motion blur and latency issues. Event cameras, on the other hand,offer a promising alternative by recording changes in the visual sceneasynchronously and providing high temporal resolution and low latency. Bybridging the gap between traditional computer vision and event-based vision, wepresent events as frames that can be readily utilized by standard deep learningalgorithms. This approach harnesses YOLOv8, a state-of-the-art object detectiontechnology, to process these frames for pupil tracking using the publiclyaccessible Ev-Eye dataset. Experimental results demonstrate the framework'seffectiveness, highlighting its potential applications in neuroscience,ophthalmology, and human-computer interaction.</description><author>Khadija Iddrisu, Waseem Shariff, Suzanne Little</author><pubDate>Tue, 23 Jul 2024 17:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16665v1</guid></item><item><title>A Comprehensive Review of Knowledge Distillation in Computer Vision</title><link>http://arxiv.org/abs/2404.00936v4</link><description>Deep learning techniques have been demonstrated to surpass precedingcutting-edge machine learning techniques in recent years, with computer visionbeing one of the most prominent examples. However, deep learning models sufferfrom significant drawbacks when deployed in resource-constrained environmentsdue to their large model size and high complexity. Knowledge Distillation isone of the prominent solutions to overcome this challenge. This review paperexamines the current state of research on knowledge distillation, a techniquefor compressing complex models into smaller and simpler ones. The paperprovides an overview of the major principles and techniques associated withknowledge distillation and reviews the applications of knowledge distillationin the domain of computer vision. The review focuses on the benefits ofknowledge distillation, as well as the problems that must be overcome toimprove its effectiveness.</description><author>Gousia Habib, Tausifa jan Saleem, Sheikh Musa Kaleem, Tufail Rouf, Brejesh Lall</author><pubDate>Tue, 23 Jul 2024 17:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00936v4</guid></item><item><title>Towards scalable efficient on-device ASR with transfer learning</title><link>http://arxiv.org/abs/2407.16664v1</link><description>Multilingual pretraining for transfer learning significantly boosts therobustness of low-resource monolingual ASR models. This study systematicallyinvestigates three main aspects: (a) the impact of transfer learning on modelperformance during initial training or fine-tuning, (b) the influence oftransfer learning across dataset domains and languages, and (c) the effect onrare-word recognition compared to non-rare words. Our finding suggests thatRNNT-loss pretraining, followed by monolingual fine-tuning with Minimum WordError Rate (MinWER) loss, consistently reduces Word Error Rates (WER) acrosslanguages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8%compared to monolingual baselines for MLS and in-house datasets. Out-of-domainpretraining leads to 28% higher WERR than in-domain pretraining. Both rare andnon-rare words benefit, with rare words showing greater improvements without-of-domain pretraining, and non-rare words with in-domain pretraining.</description><author>Laxmi Pandey, Ke Li, Jinxi Guo, Debjyoti Paul, Arthur Guo, Jay Mahadeokar, Xuedong Zhang</author><pubDate>Tue, 23 Jul 2024 17:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16664v1</guid></item><item><title>Computable learning of natural hypothesis classes</title><link>http://arxiv.org/abs/2407.16663v1</link><description>This paper is about the recent notion of computably probably approximatelycorrect learning, which lies between the statistical learning theory wherethere is no computational requirement on the learner and efficient PAC wherethe learner must be polynomially bounded. Examples have recently been given ofhypothesis classes which are PAC learnable but not computably PAC learnable,but these hypothesis classes are unnatural or non-canonical in the sense thatthey depend on a numbering of proofs, formulas, or programs. We use theon-a-cone machinery from computability theory to prove that, under mildassumptions such as that the hypothesis class can be computably listable, anynatural hypothesis class which is learnable must be computably learnable. Thusthe counterexamples given previously are necessarily unnatural.</description><author>Matthew Harrison-Trainor, Syed Akbari</author><pubDate>Tue, 23 Jul 2024 17:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16663v1</guid></item><item><title>EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval</title><link>http://arxiv.org/abs/2407.16658v1</link><description>In Composed Video Retrieval, a video and a textual description which modifiesthe video content are provided as inputs to the model. The aim is to retrievethe relevant video with the modified content from a database of videos. In thischallenging task, the first step is to acquire large-scale training datasetsand collect high-quality benchmarks for evaluation. In this work, we introduceEgoCVR, a new evaluation benchmark for fine-grained Composed Video Retrievalusing large-scale egocentric video datasets. EgoCVR consists of 2,295 queriesthat specifically focus on high-quality temporal video understanding. We findthat existing Composed Video Retrieval frameworks do not achieve the necessaryhigh-quality temporal video understanding for this task. To address thisshortcoming, we adapt a simple training-free method, propose a genericre-ranking framework for Composed Video Retrieval, and demonstrate that thisachieves strong results on EgoCVR. Our code and benchmark are freely availableat https://github.com/ExplainableML/EgoCVR.</description><author>Thomas Hummel, Shyamgopal Karthik, Mariana-Iuliana Georgescu, Zeynep Akata</author><pubDate>Tue, 23 Jul 2024 17:19:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16658v1</guid></item><item><title>MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence</title><link>http://arxiv.org/abs/2407.16655v1</link><description>Recent advancements in video generation have primarily leveraged diffusionmodels for short-duration content. However, these approaches often fall shortin modeling complex narratives and maintaining character consistency overextended periods, which is essential for long-form video production likemovies. We propose MovieDreamer, a novel hierarchical framework that integratesthe strengths of autoregressive models with diffusion-based rendering topioneer long-duration video generation with intricate plot progressions andhigh visual fidelity. Our approach utilizes autoregressive models for globalnarrative coherence, predicting sequences of visual tokens that aresubsequently transformed into high-quality video frames through diffusionrendering. This method is akin to traditional movie production processes, wherecomplex stories are factorized down into manageable scene capturing. Further,we employ a multimodal script that enriches scene descriptions with detailedcharacter information and visual style, enhancing continuity and characteridentity across scenes. We present extensive experiments across various moviegenres, demonstrating that our approach not only achieves superior visual andnarrative quality but also effectively extends the duration of generatedcontent significantly beyond current capabilities. Homepage:https://aim-uofa.github.io/MovieDreamer/.</description><author>Canyu Zhao, Mingyu Liu, Wen Wang, Jianlong Yuan, Hao Chen, Bo Zhang, Chunhua Shen</author><pubDate>Tue, 23 Jul 2024 17:17:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16655v1</guid></item><item><title>Aggregated Attributions for Explanatory Analysis of 3D Segmentation Models</title><link>http://arxiv.org/abs/2407.16653v1</link><description>Analysis of 3D segmentation models, especially in the context of medicalimaging, is often limited to segmentation performance metrics that overlook thecrucial aspect of explainability and bias. Currently, effectively explainingthese models with saliency maps is challenging due to the high dimensions ofinput images multiplied by the ever-growing number of segmented class labels.To this end, we introduce Agg^2Exp, a methodology for aggregating fine-grainedvoxel attributions of the segmentation model's predictions. Unlike classicalexplanation methods that primarily focus on the local feature attribution,Agg^2Exp enables a more comprehensive global view on the importance ofpredicted segments in 3D images. Our benchmarking experiments show thatgradient-based voxel attributions are more faithful to the model's predictionsthan perturbation-based explanations. As a concrete use-case, we apply Agg^2Expto discover knowledge acquired by the Swin UNEt TRansformer model trained onthe TotalSegmentator v2 dataset for segmenting anatomical structures incomputed tomography medical images. Agg^2Exp facilitates the explanatoryanalysis of large segmentation models beyond their predictive performance.</description><author>Maciej Chrabaszcz, Hubert Baniecki, Piotr Komorowski, Szymon Płotka, Przemyslaw Biecek</author><pubDate>Tue, 23 Jul 2024 17:14:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16653v1</guid></item><item><title>MapsTP: HD Map Images Based Multimodal Trajectory Prediction for Automated Vehicles</title><link>http://arxiv.org/abs/2407.05811v2</link><description>Predicting ego vehicle trajectories remains a critical challenge, especiallyin urban and dense areas due to the unpredictable behaviours of other vehiclesand pedestrians. Multimodal trajectory prediction enhances decision-making byconsidering multiple possible future trajectories based on diverse sources ofenvironmental data. In this approach, we leverage ResNet-50 to extract imagefeatures from high-definition map data and use IMU sensor data to calculatespeed, acceleration, and yaw rate. A temporal probabilistic network is employedto compute potential trajectories, selecting the most accurate and highlyprobable trajectory paths. This method integrates HD map data to improve therobustness and reliability of trajectory predictions for autonomous vehicles.</description><author>Sushil Sharma, Arindam Das, Ganesh Sistu, Mark Halton, Ciarán Eising</author><pubDate>Tue, 23 Jul 2024 17:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05811v2</guid></item><item><title>Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model</title><link>http://arxiv.org/abs/2407.07053v3</link><description>Although most current large multimodal models (LMMs) can already understandphotos of natural scenes and portraits, their understanding of abstract images,e.g., charts, maps, or layouts, and visual reasoning capabilities remains quiterudimentary. They often struggle with simple daily tasks, such as reading timefrom a clock, understanding a flowchart, or planning a route using a road map.In light of this, we design a multi-modal self-instruct, utilizing largelanguage models and their code capabilities to synthesize massive abstractimages and visual reasoning instructions across daily scenarios. Our strategyeffortlessly creates a multimodal benchmark with 11,193 instructions for eightvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,relation graphs, floor plans, and visual puzzles. \textbf{This benchmark,constructed with simple lines and geometric elements, exposes the shortcomingsof most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract imageunderstanding, spatial relations reasoning, and visual element induction.Besides, to verify the quality of our synthetic data, we fine-tune an LMM using62,476 synthetic chart, table and road map instructions. The resultsdemonstrate improved chart understanding and map navigation performance, andalso demonstrate potential benefits for other visual reasoning tasks. Our codeis available at: \url{https://github.com/zwq2018/Multi-modal-Self-instruct}.</description><author>Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen, Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, Yueting Zhuang</author><pubDate>Tue, 23 Jul 2024 17:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07053v3</guid></item><item><title>Deformable Convolution Based Road Scene Semantic Segmentation of Fisheye Images in Autonomous Driving</title><link>http://arxiv.org/abs/2407.16647v1</link><description>This study investigates the effectiveness of modern Deformable ConvolutionalNeural Networks (DCNNs) for semantic segmentation tasks, particularly inautonomous driving scenarios with fisheye images. These images, providing awide field of view, pose unique challenges for extracting spatial and geometricinformation due to dynamic changes in object attributes. Our experiments focuson segmenting the WoodScape fisheye image dataset into ten distinct classes,assessing the Deformable Networks' ability to capture intricate spatialrelationships and improve segmentation accuracy. Additionally, we exploredifferent loss functions to address class imbalance issues and compare theperformance of conventional CNN architectures with Deformable Convolution-basedCNNs, including Vanilla U-Net and Residual U-Net architectures. The significantimprovement in mIoU score resulting from integrating Deformable CNNsdemonstrates their effectiveness in handling the geometric distortions presentin fisheye imagery, exceeding the performance of traditional CNN architectures.This underscores the significant role of Deformable convolution in enhancingsemantic segmentation performance for fisheye imagery.</description><author>Anam Manzoor, Aryan Singh, Ganesh Sistu, Reenu Mohandas, Eoin Grua, Anthony Scanlan, Ciarán Eising</author><pubDate>Tue, 23 Jul 2024 17:02:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16647v1</guid></item><item><title>Deep-Graph-Sprints: Accelerated Representation Learning in Continuous-Time Dynamic Graphs</title><link>http://arxiv.org/abs/2407.07712v2</link><description>Continuous-time dynamic graphs (CTDGs) are essential for modelinginterconnected, evolving systems. Traditional methods for extracting knowledgefrom these graphs often depend on feature engineering or deep learning. Featureengineering is limited by the manual and time-intensive nature of craftingfeatures, while deep learning approaches suffer from high inference latency,making them impractical for real-time applications. This paper introducesDeep-Graph-Sprints (DGS), a novel deep learning architecture designed forefficient representation learning on CTDGs with low-latency inferencerequirements. We benchmark DGS against state-of-the-art feature engineering andgraph neural network methods using five diverse datasets. The results indicatethat DGS achieves competitive performance while improving inference speed up to12x compared to other deep learning approaches on our tested benchmarks. Ourmethod effectively bridges the gap between deep representation learning andlow-latency application requirements for CTDGs.</description><author>Ahmad Naser Eddin, Jacopo Bono, David Aparício, Hugo Ferreira, Pedro Ribeiro, Pedro Bizarro</author><pubDate>Tue, 23 Jul 2024 17:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07712v2</guid></item><item><title>Global Minima by Penalized Full-dimensional Scaling</title><link>http://arxiv.org/abs/2407.16645v1</link><description>The full-dimensional (metric, Euclidean, least squares) multidimensionalscaling stress loss function is combined with a quadratic external penaltyfunction term. The trajectory of minimizers of stress for increasing values ofthe penalty parameter is then used to find (tentative) global minima forlow-dimensional multidimensional scaling. This is illustrated with severalone-dimensional and two-dimensional examples.</description><author>Jan de Leeuw</author><pubDate>Tue, 23 Jul 2024 16:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16645v1</guid></item><item><title>Synthesizer Sound Matching Using Audio Spectrogram Transformers</title><link>http://arxiv.org/abs/2407.16643v1</link><description>Systems for synthesizer sound matching, which automatically set theparameters of a synthesizer to emulate an input sound, have the potential tomake the process of synthesizer programming faster and easier for novice andexperienced musicians alike, whilst also affording new means of interactionwith synthesizers. Considering the enormous variety of synthesizers in themarketplace, and the complexity of many of them, general-purpose sound matchingsystems that function with minimal knowledge or prior assumptions about theunderlying synthesis architecture are particularly desirable. With this inmind, we introduce a synthesizer sound matching model based on the AudioSpectrogram Transformer. We demonstrate the viability of this model by trainingon a large synthetic dataset of randomly generated samples from the popularMassive synthesizer. We show that this model can reconstruct parameters ofsamples generated from a set of 16 parameters, highlighting its improvedfidelity relative to multi-layer perceptron and convolutional neural networkbaselines. We also provide audio examples demonstrating the out-of-domain modelperformance in emulating vocal imitations, and sounds from other synthesizersand musical instruments.</description><author>Fred Bruford, Frederik Blang, Shahan Nercessian</author><pubDate>Tue, 23 Jul 2024 16:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16643v1</guid></item><item><title>Aggregation of expert advice, revisited</title><link>http://arxiv.org/abs/2407.16642v1</link><description>We revisit the classic problem of aggregating binary advice fromconditionally independent experts, also known as the Naive Bayes setting. Ourquantity of interest is the error probability of the optimal decision rule. Inthe symmetric case (sensitivity = specificity), reasonably tight bounds on theoptimal error probability are known. In the general asymmetric case, we are notaware of any nontrivial estimates on this quantity. Our contribution consistsof sharp upper and lower bounds on the optimal error probability in the generalcase, which recover and sharpen the best known results in the symmetric specialcase. Since this amounts to estimating the total variation distance between twoproduct distributions, our results also have bearing on this important andchallenging problem.</description><author>Aryeh Kontorovich</author><pubDate>Tue, 23 Jul 2024 16:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16642v1</guid></item><item><title>A Geometry-Aware Algorithm to Learn Hierarchical Embeddings in Hyperbolic Space</title><link>http://arxiv.org/abs/2407.16641v1</link><description>Hyperbolic embeddings are a class of representation learning methods thatoffer competitive performances when data can be abstracted as a tree-likegraph. However, in practice, learning hyperbolic embeddings of hierarchicaldata is difficult due to the different geometry between hyperbolic space andthe Euclidean space. To address such difficulties, we first categorize threekinds of illness that harm the performance of the embeddings. Then, we developa geometry-aware algorithm using a dilation operation and a transitive closureregularization to tackle these illnesses. We empirically validate thesetechniques and present a theoretical analysis of the mechanism behind thedilation operation. Experiments on synthetic and real-world datasets revealsuperior performances of our algorithm.</description><author>Zhangyu Wang, Lantian Xu, Zhifeng Kong, Weilong Wang, Xuyu Peng, Enyang Zheng</author><pubDate>Tue, 23 Jul 2024 16:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16641v1</guid></item><item><title>Unveiling and Mitigating Bias in Audio Visual Segmentation</title><link>http://arxiv.org/abs/2407.16638v1</link><description>Community researchers have developed a range of advanced audio-visualsegmentation models aimed at improving the quality of sounding objects' masks.While masks created by these models may initially appear plausible, theyoccasionally exhibit anomalies with incorrect grounding logic. We attributethis to real-world inherent preferences and distributions as a simpler signalfor learning than the complex audio-visual grounding, which leads to thedisregard of important modality information. Generally, the anomalous phenomenaare often complex and cannot be directly observed systematically. In thisstudy, we made a pioneering effort with the proper synthetic data to categorizeand analyze phenomena as two types "audio priming bias" and "visual prior"according to the source of anomalies. For audio priming bias, to enhance audiosensitivity to different intensities and semantics, a perception modulespecifically for audio perceives the latent semantic information andincorporates information into a limited set of queries, namely active queries.Moreover, the interaction mechanism related to such active queries in thetransformer decoder is customized to adapt to the need for interactionregulating among audio semantics. For visual prior, multiple contrastivetraining strategies are explored to optimize the model by incorporating abiased branch, without even changing the structure of the model. Duringexperiments, observation demonstrates the presence and the impact that has beenproduced by the biases of the existing model. Finally, through experimentalevaluation of AVS benchmarks, we demonstrate the effectiveness of our methodsin handling both types of biases, achieving competitive performance across allthree subsets.</description><author>Peiwen Sun, Honggang Zhang, Di Hu</author><pubDate>Tue, 23 Jul 2024 16:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16638v1</guid></item><item><title>Course-Correction: Safety Alignment Using Synthetic Preferences</title><link>http://arxiv.org/abs/2407.16637v1</link><description>The risk of harmful content generated by large language models (LLMs) becomesa critical concern. This paper presents a systematic study on assessing andimproving LLMs' capability to perform the task of \textbf{course-correction},\ie, the model can steer away from generating harmful content autonomously. Tostart with, we introduce the \textsc{C$^2$-Eval} benchmark for quantitativeassessment and analyze 10 popular LLMs, revealing varying proficiency ofcurrent safety-tuned LLMs in course-correction. To improve, we proposefine-tuning LLMs with preference learning, emphasizing the preference fortimely course-correction. Using an automated pipeline, we create\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, toteach models the concept of timely course-correction through data-drivenpreference learning. Experiments on 2 LLMs, \textsc{Llama2-Chat 7B} and\textsc{Qwen2 7B}, show that our method effectively enhances course-correctionskills without affecting general performance. Additionally, it effectivelyimproves LLMs' safety, particularly in resisting jailbreak attacks.</description><author>Rongwu Xu, Yishuo Cai, Zhenhong Zhou, Renjie Gu, Haiqin Weng, Yan Liu, Tianwei Zhang, Wei Xu, Han Qiu</author><pubDate>Tue, 23 Jul 2024 16:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16637v1</guid></item><item><title>Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models for Autonomous Vehicles</title><link>http://arxiv.org/abs/2407.16636v1</link><description>Fusing different sensor modalities can be a difficult task, particularly ifthey are asynchronous. Asynchronisation may arise due to long processing timesor improper synchronisation during calibration, and there must exist a way tostill utilise this previous information for the purpose of safe driving, andobject detection in ego vehicle/ multi-agent trajectory prediction.Difficulties arise in the fact that the sensor modalities have capturedinformation at different times and also at different positions in space.Therefore, they are not spatially nor temporally aligned. This paper willinvestigate the challenge of radar and LiDAR sensors being asynchronousrelative to the camera sensors, for various time latencies. The spatialalignment will be resolved before lifting into BEV space via the transformationof the radar/LiDAR point clouds into the new ego frame coordinate system. Onlyafter this can we concatenate the radar/LiDAR point cloud and lifted camerafeatures. Temporal alignment will be remedied for radar data only, we willimplement a novel method of inferring the future radar point positions usingthe velocity information. Our approach to resolving the issue of sensorasynchrony yields promising results. We demonstrate velocity information candrastically improve IoU for asynchronous datasets, as for a time latency of 360milliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a timelatency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR(C+L) model by 0.18 IoU. This is an advancement in utilising theoften-neglected radar sensor modality, which is less favoured than LiDAR forautonomous driving purposes.</description><author>Seamie Hayes, Sushil Sharma, Ciarán Eising</author><pubDate>Tue, 23 Jul 2024 16:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16636v1</guid></item><item><title>Knowledge-driven AI-generated data for accurate and interpretable breast ultrasound diagnoses</title><link>http://arxiv.org/abs/2407.16634v1</link><description>Data-driven deep learning models have shown great capabilities to assistradiologists in breast ultrasound (US) diagnoses. However, their effectivenessis limited by the long-tail distribution of training data, which leads toinaccuracies in rare cases. In this study, we address a long-standing challengeof improving the diagnostic model performance on rare cases using long-taileddata. Specifically, we introduce a pipeline, TAILOR, that builds aknowledge-driven generative model to produce tailored synthetic data. Thegenerative model, using 3,749 lesions as source data, can generate millions ofbreast-US images, especially for error-prone rare cases. The generated data canbe further used to build a diagnostic model for accurate and interpretablediagnoses. In the prospective external evaluation, our diagnostic modeloutperforms the average performance of nine radiologists by 33.5% inspecificity with the same sensitivity, improving their performance by providingpredictions with an interpretable decision-making process. Moreover, on ductalcarcinoma in situ (DCIS), our diagnostic model outperforms all radiologists bya large margin, with only 34 DCIS lesions in the source data. We believe thatTAILOR can potentially be extended to various diseases and imaging modalities.</description><author>Haojun Yu, Youcheng Li, Nan Zhang, Zihan Niu, Xuantong Gong, Yanwen Luo, Quanlin Wu, Wangyan Qin, Mengyuan Zhou, Jie Han, Jia Tao, Ziwei Zhao, Di Dai, Di He, Dong Wang, Binghui Tang, Ling Huo, Qingli Zhu, Yong Wang, Liwei Wang</author><pubDate>Tue, 23 Jul 2024 16:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16634v1</guid></item><item><title>Generalization within in silico screening</title><link>http://arxiv.org/abs/2307.09379v2</link><description>In silico screening uses predictive models to select a batch of compoundswith favorable properties from a library for experimental validation. Unlikeconventional learning paradigms, success in this context is measured by theperformance of the predictive model on the selected subset of compounds ratherthan the entire set of predictions. By extending learning theory, we show thatthe selectivity of the selection policy can significantly impactgeneralization, with a higher risk of errors occurring when exclusivelyselecting predicted positives and when targeting rare properties. Our analysissuggests a way to mitigate these challenges. We show that generalization can bemarkedly enhanced when considering a model's ability to predict the fraction ofdesired outcomes in a batch. This is promising, as the primary aim of screeningis not necessarily to pinpoint the label of each compound individually, butrather to assemble a batch enriched for desirable compounds. Our theoreticalinsights are empirically validated across diverse tasks, architectures, andscreening scenarios, underscoring their applicability.</description><author>Andreas Loukas, Pan Kessel, Vladimir Gligorijevic, Richard Bonneau</author><pubDate>Tue, 23 Jul 2024 16:37:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09379v2</guid></item><item><title>Semantic Change Characterization with LLMs using Rhetorics</title><link>http://arxiv.org/abs/2407.16624v1</link><description>Languages continually evolve in response to societal events, resulting in newterms and shifts in meanings. These changes have significant implications forcomputer applications, including automatic translation and chatbots, making itessential to characterize them accurately. The recent development of LLMs hasnotably advanced natural language understanding, particularly in senseinference and reasoning. In this paper, we investigate the potential of LLMs incharacterizing three types of semantic change: dimension, relation, andorientation. We achieve this by combining LLMs' Chain-of-Thought withrhetorical devices and conducting an experimental assessment of our approachusing newly created datasets. Our results highlight the effectiveness of LLMsin capturing and analyzing semantic changes, providing valuable insights toimprove computational linguistic applications.</description><author>Jader Martins Camboim de Sá, Marcos Da Silveira, Cédric Pruski</author><pubDate>Tue, 23 Jul 2024 16:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16624v1</guid></item><item><title>Inverse Particle and Ensemble Kalman Filters</title><link>http://arxiv.org/abs/2407.16623v1</link><description>In cognitive systems, recent emphasis has been placed on studying cognitiveprocesses of the subject whose behavior was the primary focus of the system'scognitive response. This approach, known as inverse cognition, arises incounter-adversarial applications and has motivated the development of inverseBayesian filters. In this context, a cognitive adversary, such as a radar, usesa forward Bayesian filter to track its target of interest. An inverse filter isthen employed to infer adversary's estimate of target's or defender's state.Previous studies have addressed this inverse filtering problem by introducingmethods like inverse Kalman filter (I-KF), inverse extended KF (I-EKF), andinverse unscented KF (I-UKF). However, these inverse filters assume additiveGaussian noises and/or rely on local approximations of non-linear dynamics atthe state estimates, limiting their practical application. Contrarily, thispaper adopts a global filtering approach and develops an inverse particlefilter (I-PF). The particle filter framework employs Monte Carlo (MC) methodsto approximate arbitrary posterior distributions. Moreover, under mildsystem-level conditions, the proposed I-PF demonstrates convergence to theoptimal inverse filter. Additionally, we explore MC techniques to approximateGaussian posteriors and introduce inverse Gaussian PF (I-GPF) and inverseensemble KF (I-EnKF). Our I-GPF and I-EnKF can efficiently handle non-Gaussiannoises with suitable modifications. Additionally, we propose the differentiableI-PF, differentiable I-EnKF, and reproducing kernel Hilbert space-based EnKF(RKHS-EnKF) methods to address scenarios where system information is unknown todefender. Using recursive Cram\'er-Rao lower bound and non-credibility index(NCI), our numerical experiments for different applications demonstrate theestimation performance and time complexity of the proposed filters.</description><author>Himali Singh, Arpan Chattopadhyay, Kumar Vijay Mishra</author><pubDate>Tue, 23 Jul 2024 16:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16623v1</guid></item><item><title>Graph Neural Networks for Learning Equivariant Representations of Neural Networks</title><link>http://arxiv.org/abs/2403.12143v3</link><description>Neural networks that process the parameters of other neural networks findapplications in domains as diverse as classifying implicit neuralrepresentations, generating neural network weights, and predictinggeneralization errors. However, existing approaches either overlook theinherent permutation symmetry in the neural network or rely on intricateweight-sharing patterns to achieve equivariance, while ignoring the impact ofthe network architecture itself. In this work, we propose to represent neuralnetworks as computational graphs of parameters, which allows us to harnesspowerful graph neural networks and transformers that preserve permutationsymmetry. Consequently, our approach enables a single model to encode neuralcomputational graphs with diverse architectures. We showcase the effectivenessof our method on a wide range of tasks, including classification and editing ofimplicit neural representations, predicting generalization performance, andlearning to optimize, while consistently outperforming state-of-the-artmethods. The source code is open-sourced athttps://github.com/mkofinas/neural-graphs.</description><author>Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang</author><pubDate>Tue, 23 Jul 2024 16:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12143v3</guid></item><item><title>Implementing engrams from a machine learning perspective: the relevance of a latent space</title><link>http://arxiv.org/abs/2407.16616v1</link><description>In our previous work, we proposed that engrams in the brain could bebiologically implemented as autoencoders over recurrent neural networks. Theseautoencoders would comprise basic excitatory/inhibitory motifs, with creditassignment deriving from a simple homeostatic criterion. This brief noteexamines the relevance of the latent space in these autoencoders. We considerthe relationship between the dimensionality of these autoencoders and thecomplexity of the information being encoded. We discuss how observeddifferences between species in their connectome could be linked to theircognitive capacities. Finally, we link this analysis with a basic but oftenoverlooked fact: human cognition is likely limited by our own brain structure.However, this limitation does not apply to machine learning systems, and weshould be aware of the need to learn how to exploit this augmented vision ofthe nature.</description><author>J Marco de Lucas</author><pubDate>Tue, 23 Jul 2024 16:24:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16616v1</guid></item><item><title>Lawma: The Power of Specialization for Legal Tasks</title><link>http://arxiv.org/abs/2407.16615v1</link><description>Annotation and classification of legal text are central components ofempirical legal research. Traditionally, these tasks are often delegated totrained research assistants. Motivated by the advances in language modeling,empirical legal scholars are increasingly turning to prompting commercialmodels, hoping that it will alleviate the significant cost of human annotation.Despite growing use, our understanding of how to best utilize large languagemodels for legal tasks remains limited. We conduct a comprehensive study of 260legal text classification tasks, nearly all new to the machine learningcommunity. Starting from GPT-4 as a baseline, we show that it has non-trivialbut highly varied zero-shot accuracy, often exhibiting performance that may beinsufficient for legal work. We then demonstrate that a lightly fine-tunedLlama 3 model vastly outperforms GPT-4 on almost all tasks, typically bydouble-digit percentage points. We find that larger models respond better tofine-tuning than smaller models. A few tens to hundreds of examples suffice toachieve high classification accuracy. Notably, we can fine-tune a single modelon all 260 tasks simultaneously at a small loss in accuracy relative to havinga separate model for each task. Our work points to a viable alternative to thepredominant practice of prompting commercial models. For concrete legal taskswith some available labeled data, researchers are better off using a fine-tunedopen-source model.</description><author>Ricardo Dominguez-Olmedo, Vedant Nanda, Rediet Abebe, Stefan Bechtold, Christoph Engel, Jens Frankenreiter, Krishna Gummadi, Moritz Hardt, Michael Livermore</author><pubDate>Tue, 23 Jul 2024 16:23:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16615v1</guid></item><item><title>ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy</title><link>http://arxiv.org/abs/2311.09215v3</link><description>Modern computer vision offers a great variety of models to practitioners, andselecting a model from multiple options for specific applications can bechallenging. Conventionally, competing model architectures and trainingprotocols are compared by their classification accuracy on ImageNet. However,this single metric does not fully capture performance nuances critical forspecialized tasks. In this work, we conduct an in-depth comparative analysis ofmodel behaviors beyond ImageNet accuracy, for both ConvNet and VisionTransformer architectures, each across supervised and CLIP training paradigms.Although our selected models have similar ImageNet accuracies and computerequirements, we find that they differ in many other aspects: types ofmistakes, output calibration, transferability, and feature invariance, amongothers. This diversity in model characteristics, not captured by traditionalmetrics, highlights the need for more nuanced analysis when choosing amongdifferent models. Our code is available athttps://github.com/kirill-vish/Beyond-INet.</description><author>Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu</author><pubDate>Tue, 23 Jul 2024 16:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09215v3</guid></item><item><title>No-brainer: Morphological Computation driven Adaptive Behavior in Soft Robots</title><link>http://arxiv.org/abs/2407.16613v1</link><description>It is prevalent in contemporary AI and robotics to separately postulate abrain modeled by neural networks and employ it to learn intelligent andadaptive behavior. While this method has worked very well for many types oftasks, it isn't the only type of intelligence that exists in nature. In thiswork, we study the ways in which intelligent behavior can be created without aseparate and explicit brain for robot control, but rather solely as a result ofthe computation occurring within the physical body of a robot. Specifically, weshow that adaptive and complex behavior can be created in voxel-based virtualsoft robots by using simple reactive materials that actively change the shapeof the robot, and thus its behavior, under different environmental cues. Wedemonstrate a proof of concept for the idea of closed-loop morphologicalcomputation, and show that in our implementation, it enables behavior mimickinglogic gates, enabling us to demonstrate how such behaviors may be combined tobuild up more complex collective behaviors.</description><author>Alican Mertan, Nick Cheney</author><pubDate>Tue, 23 Jul 2024 16:20:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16613v1</guid></item><item><title>Local vs Global continual learning</title><link>http://arxiv.org/abs/2407.16611v1</link><description>Continual learning is the problem of integrating new information in a modelwhile retaining the knowledge acquired in the past. Despite the tangibleimprovements achieved in recent years, the problem of continual learning isstill an open one. A better understanding of the mechanisms behind thesuccesses and failures of existing continual learning algorithms can unlock thedevelopment of new successful strategies. In this work, we view continuallearning from the perspective of the multi-task loss approximation, and wecompare two alternative strategies, namely local and global approximations. Weclassify existing continual learning algorithms based on the approximationused, and we assess the practical effects of this distinction in commoncontinual learning settings.Additionally, we study optimal continual learningobjectives in the case of local polynomial approximations and we provideexamples of existing algorithms implementing the optimal objectives</description><author>Giulia Lanzillotta, Sidak Pal Singh, Benjamin F. Grewe, Thomas Hofmann</author><pubDate>Tue, 23 Jul 2024 16:18:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16611v1</guid></item><item><title>Recurrent Action Transformer with Memory</title><link>http://arxiv.org/abs/2306.09459v4</link><description>Recently, the use of transformers in offline reinforcement learning hasbecome a rapidly developing area. This is due to their ability to treat theagent's trajectory in the environment as a sequence, thereby reducing thepolicy learning problem to sequence modeling. In environments where the agent'sdecisions depend on past events (POMDPs), capturing both the event itself andthe decision point in the context of the model is essential. However, thequadratic complexity of the attention mechanism limits the potential forcontext expansion. One solution to this problem is to enhance transformers withmemory mechanisms. This paper proposes a Recurrent Action Transformer withMemory (RATE), a novel model architecture incorporating a recurrent memorymechanism designed to regulate information retention. To evaluate our model, weconducted extensive experiments on memory-intensive environments(ViZDoom-Two-Colors, T-Maze, Memory Maze, Minigrid.Memory), classic Atari gamesand MuJoCo control environments. The results show that using memory cansignificantly improve performance in memory-intensive environments whilemaintaining or improving results in classic environments. We hope our findingswill stimulate research on memory mechanisms for transformers applicable tooffline reinforcement learning.</description><author>Egor Cherepanov, Alexey Staroverov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov</author><pubDate>Tue, 23 Jul 2024 16:17:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09459v4</guid></item><item><title>TimeInf: Time Series Data Contribution via Influence Functions</title><link>http://arxiv.org/abs/2407.15247v2</link><description>Evaluating the contribution of individual data points to a model's predictionis critical for interpreting model predictions and improving model performance.Existing data contribution methods have been applied to various data types,including tabular data, images, and texts; however, their primary focus hasbeen on i.i.d. settings. Despite the pressing need for principled approachestailored to time series datasets, the problem of estimating data contributionin such settings remains unexplored, possibly due to challenges associated withhandling inherent temporal dependencies. This paper introduces TimeInf, a datacontribution estimation method for time-series datasets. TimeInf uses influencefunctions to attribute model predictions to individual time points whilepreserving temporal structures. Our extensive empirical results demonstratethat TimeInf outperforms state-of-the-art methods in identifying harmfulanomalies and helpful time points for forecasting. Additionally, TimeInf offersintuitive and interpretable attributions of data values, allowing us to easilydistinguish diverse anomaly patterns through visualizations.</description><author>Yizi Zhang, Jingyan Shen, Xiaoxue Xiong, Yongchan Kwon</author><pubDate>Tue, 23 Jul 2024 16:16:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15247v2</guid></item><item><title>Towards a "universal translator" for neural dynamics at single-cell, single-spike resolution</title><link>http://arxiv.org/abs/2407.14668v2</link><description>Neuroscience research has made immense progress over the last decade, but ourunderstanding of the brain remains fragmented and piecemeal: the dream ofprobing an arbitrary brain region and automatically reading out the informationencoded in its neural activity remains out of reach. In this work, we buildtowards a first foundation model for neural spiking data that can solve adiverse set of tasks across multiple brain areas. We introduce a novelself-supervised modeling approach for population activity in which the modelalternates between masking out and reconstructing neural activity acrossdifferent time steps, neurons, and brain regions. To evaluate our approach, wedesign unsupervised and supervised prediction tasks using the InternationalBrain Laboratory repeated site dataset, which is comprised of Neuropixelsrecordings targeting the same brain locations across 48 animals andexperimental sessions. The prediction tasks include single-neuron andregion-level activity prediction, forward prediction, and behavior decoding. Wedemonstrate that our multi-task-masking (MtM) approach significantly improvesthe performance of current state-of-the-art population models and enablesmulti-task learning. We also show that by training on multiple animals, we canimprove the generalization ability of the model to unseen animals, paving theway for a foundation model of the brain at single-cell, single-spikeresolution.</description><author>Yizi Zhang, Yanchen Wang, Donato Jimenez-Beneto, Zixuan Wang, Mehdi Azabou, Blake Richards, Olivier Winter, International Brain Laboratory, Eva Dyer, Liam Paninski, Cole Hurwitz</author><pubDate>Tue, 23 Jul 2024 16:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14668v2</guid></item><item><title>Deep Bayesian segmentation for colon polyps: Well-calibrated predictions in medical imaging</title><link>http://arxiv.org/abs/2407.16608v1</link><description>Colorectal polyps are generally benign alterations that, if not identifiedpromptly and managed successfully, can progress to cancer and causeaffectations on the colon mucosa, known as adenocarcinoma. Today advances inDeep Learning have demonstrated the ability to achieve significant performancein image classification and detection in medical diagnosis applications.Nevertheless, these models are prone to overfitting, and making decisions basedonly on point estimations may provide incorrect predictions. Thus, to obtain amore informed decision, we must consider point estimations along with theirreliable uncertainty quantification. In this paper, we built different Bayesianneural network approaches based on the flexibility of posterior distribution todevelop semantic segmentation of colorectal polyp images. We found that thesemodels not only provide state-of-the-art performance on the segmentation ofthis medical dataset but also, yield accurate uncertainty estimates. We appliedmultiplicative normalized flows(MNF) and reparameterization trick on the UNET,FPN, and LINKNET architectures tested with multiple backbones in deterministicand Bayesian versions. We report that the FPN + EfficientnetB7 architecturewith MNF is the most promising option given its IOU of 0.94 and ExpectedCalibration Error (ECE) of 0.004, combined with its superiority in identifyingdifficult-to-detect colorectal polyps, which is effective in clinical areaswhere early detection prevents the development of colon cancer.</description><author>Daniela L. Ramos, Hector J. Hortua</author><pubDate>Tue, 23 Jul 2024 16:13:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16608v1</guid></item><item><title>Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?</title><link>http://arxiv.org/abs/2407.16607v1</link><description>The pretraining data of today's strongest language models is opaque. Inparticular, little is known about the proportions of various domains orlanguages represented. In this work, we tackle a task which we call datamixture inference, which aims to uncover the distributional make-up of trainingdata. We introduce a novel attack based on a previously overlooked source ofinformation -- byte-pair encoding (BPE) tokenizers, used by the vast majorityof modern language models. Our key insight is that the ordered list of mergerules learned by a BPE tokenizer naturally reveals information about the tokenfrequencies in its training data: the first merge is the most common byte pair,the second is the most common pair after merging the first token, and so on.Given a tokenizer's merge list along with data samples for each category ofinterest, we formulate a linear program that solves for the proportion of eachcategory in the tokenizer's training set. Importantly, to the extent to whichtokenizer training data is representative of the pretraining data, weindirectly learn about the pretraining data. In controlled experiments, we showthat our attack recovers mixture ratios with high precision for tokenizerstrained on known mixtures of natural languages, programming languages, and datasources. We then apply our approach to off-the-shelf tokenizers released withrecent LMs. We confirm much publicly disclosed information about these models,and also make several new inferences: GPT-4o's tokenizer is much moremultilingual than its predecessors, training on 39% non-English data; Llama3extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's andClaude's tokenizers are trained on predominantly code (~60%). We hope our worksheds light on current design practices for pretraining data, and inspirescontinued research into data mixture inference for LMs.</description><author>Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith</author><pubDate>Tue, 23 Jul 2024 16:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16607v1</guid></item><item><title>Learning to Play Foosball: System and Baselines</title><link>http://arxiv.org/abs/2407.16606v1</link><description>This work stages Foosball as a versatile platform for advancing scientificresearch, particularly in the realm of robot learning. We present an automatedFoosball table along with its corresponding simulated counterpart, showcasing adiverse range of challenges through example tasks within the Foosballenvironment. Initial findings are shared using a simple baseline approach.Foosball constitutes a versatile learning environment with the potential toyield cutting-edge research in various fields of artificial intelligence andmachine learning, notably robust learning, while also extending itsapplicability to industrial robotics and automation setups. To transform ourphysical Foosball table into a research-friendly system, we augmented it with a2 degrees of freedom kinematic chain to control the goalkeeper rod as aninitial setup with the intention to be extended to the full game as soon aspossible. Our experiments reveal that a realistic simulation is essential formastering complex robotic tasks, yet translating these accomplishments to thereal system remains challenging, often accompanied by a performance decline.This emphasizes the critical importance of research in this direction. In thisconcern, we spotlight the automated Foosball table as an invaluable tool,possessing numerous desirable attributes, to serve as a demanding learningenvironment for advancing robotics and automation research.</description><author>Janosch Moos, Cedric Derstroff, Niklas Schröder, Debora Clever</author><pubDate>Tue, 23 Jul 2024 16:11:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16606v1</guid></item><item><title>Interpretable Machine Learning for TabPFN</title><link>http://arxiv.org/abs/2403.10923v2</link><description>The recently developed Prior-Data Fitted Networks (PFNs) have shown verypromising results for applications in low-data regimes. The TabPFN model, aspecial case of PFNs for tabular data, is able to achieve state-of-the-artperformance on a variety of classification tasks while producing posteriorpredictive distributions in mere seconds by in-context learning without theneed for learning parameters or hyperparameter tuning. This makes TabPFN a veryattractive option for a wide range of domain applications. However, a majordrawback of the method is its lack of interpretability. Therefore, we proposeseveral adaptations of popular interpretability methods that we specificallydesign for TabPFN. By taking advantage of the unique properties of the model,our adaptations allow for more efficient computations than existingimplementations. In particular, we show how in-context learning facilitates theestimation of Shapley values by avoiding approximate retraining and enables theuse of Leave-One-Covariate-Out (LOCO) even when working with large-scaleTransformers. In addition, we demonstrate how data valuation methods can beused to address scalability challenges of TabPFN. Our proposed methods areimplemented in a package tabpfn_iml and made available athttps://github.com/david-rundel/tabpfn_iml.</description><author>David Rundel, Julius Kobialka, Constantin von Crailsheim, Matthias Feurer, Thomas Nagler, David Rügamer</author><pubDate>Tue, 23 Jul 2024 16:10:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10923v2</guid></item><item><title>Kernel Memory Networks: A Unifying Framework for Memory Modeling</title><link>http://arxiv.org/abs/2208.09416v3</link><description>We consider the problem of training a neural network to store a set ofpatterns with maximal noise robustness. A solution, in terms of optimal weightsand state update rules, is derived by training each individual neuron toperform either kernel classification or interpolation with a minimum weightnorm. By applying this method to feed-forward and recurrent networks, we deriveoptimal models, termed kernel memory networks, that include, as special cases,many of the hetero- and auto-associative memory models that have been proposedover the past years, such as modern Hopfield networks and Kanerva's sparsedistributed memory. We modify Kanerva's model and demonstrate a simple way todesign a kernel memory network that can store an exponential number ofcontinuous-valued patterns with a finite basin of attraction. The framework ofkernel memory networks offers a simple and intuitive way to understand thestorage capacity of previous memory models, and allows for new biologicalinterpretations in terms of dendritic non-linearities and synaptic cross-talk.</description><author>Georgios Iatropoulos, Johanni Brea, Wulfram Gerstner</author><pubDate>Tue, 23 Jul 2024 16:09:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09416v3</guid></item><item><title>Shared Imagination: LLMs Hallucinate Alike</title><link>http://arxiv.org/abs/2407.16604v1</link><description>Despite the recent proliferation of large language models (LLMs), theirtraining recipes -- model architecture, pre-training data and optimizationalgorithm -- are often very similar. This naturally raises the question of thesimilarity among the resulting models. In this paper, we propose a novelsetting, imaginary question answering (IQA), to better understand modelsimilarity. In IQA, we ask one model to generate purely imaginary questions(e.g., on completely made-up concepts in physics) and prompt another model toanswer. Surprisingly, despite the total fictionality of these questions, allmodels can answer each other's questions with remarkable success, suggesting a"shared imagination space" in which these models operate during suchhallucinations. We conduct a series of investigations into this phenomenon anddiscuss implications on model homogeneity, hallucination, and computationalcreativity.</description><author>Yilun Zhou, Caiming Xiong, Silvio Savarese, Chien-Sheng Wu</author><pubDate>Tue, 23 Jul 2024 16:06:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16604v1</guid></item><item><title>Functional Acceleration for Policy Mirror Descent</title><link>http://arxiv.org/abs/2407.16602v1</link><description>We apply functional acceleration to the Policy Mirror Descent (PMD) generalfamily of algorithms, which cover a wide range of novel and fundamental methodsin Reinforcement Learning (RL). Leveraging duality, we propose a momentum-basedPMD update. By taking the functional route, our approach is independent of thepolicy parametrization and applicable to large-scale optimization, coveringprevious applications of momentum at the level of policy parameters as aspecial case. We theoretically analyze several properties of this approach andcomplement with a numerical ablation study, which serves to illustrate thepolicy optimization dynamics on the value polytope, relative to differentalgorithmic design choices in this space. We further characterize numericallyseveral features of the problem setting relevant for functional acceleration,and lastly, we investigate the impact of approximation on their learningmechanics.</description><author>Veronica Chelu, Doina Precup</author><pubDate>Tue, 23 Jul 2024 16:04:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16602v1</guid></item><item><title>DHGS: Decoupled Hybrid Gaussian Splatting for Driving Scene</title><link>http://arxiv.org/abs/2407.16600v1</link><description>Existing Gaussian splatting methods struggle to achieve satisfactory novelview synthesis in driving scenes due to the lack of crafty design and geometricconstraints of related elements. This paper introduces a novel method calledDecoupled Hybrid Gaussian Splatting (DHGS), which aims at promoting therendering quality of novel view synthesis for driving scenes. The novelty ofthis work lies in the decoupled and hybrid pixel-level blender for road andnon-road layers, without conventional unified differentiable rendering logicfor the entire scene, meanwhile maintaining consistent and continuoussuperimposition through the proposed depth-ordered rendering strategy. Beyondthat, an implicit road representation comprised of Signed Distance Field (SDF)is trained to supervise the road surface with subtle geometric attributes.Accompanied by the use of auxiliary transmittance loss and consistency loss,novel images with imperceptible boundary and elevated fidelity are ultimatelyobtained. Substantial experiments on Waymo dataset prove that DHGS outperformsthe state-of-the-art methods.</description><author>Xi Shi, Lingli Chen, Peng Wei, Xi Wu, Tian Jiang, Yonggang Luo, Lecheng Xie</author><pubDate>Tue, 23 Jul 2024 16:03:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16600v1</guid></item><item><title>COMO: Compact Mapping and Odometry</title><link>http://arxiv.org/abs/2404.03531v2</link><description>We present COMO, a real-time monocular mapping and odometry system thatencodes dense geometry via a compact set of 3D anchor points. Decoding anchorpoint projections into dense geometry via per-keyframe depth covariancefunctions guarantees that depth maps are joined together at visible anchorpoints. The representation enables joint optimization of camera poses and densegeometry, intrinsic 3D consistency, and efficient second-order inference. Tomaintain a compact yet expressive map, we introduce a frontend that leveragesthe covariance function for tracking and initializing potentially visuallyindistinct 3D points across frames. Altogether, we introduce a real-time systemcapable of estimating accurate poses and consistent geometry.</description><author>Eric Dexheimer, Andrew J. Davison</author><pubDate>Tue, 23 Jul 2024 16:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03531v2</guid></item><item><title>PDF: A Probability-Driven Framework for Open World 3D Point Cloud Semantic Segmentation</title><link>http://arxiv.org/abs/2404.00979v2</link><description>Existing point cloud semantic segmentation networks cannot identify unknownclasses and update their knowledge, due to a closed-set and static perspectiveof the real world, which would induce the intelligent agent to make baddecisions. To address this problem, we propose a Probability-Driven Framework(PDF) for open world semantic segmentation that includes (i) a lightweightU-decoder branch to identify unknown classes by estimating the uncertainties,(ii) a flexible pseudo-labeling scheme to supply geometry features along withprobability distribution features of unknown classes by generating pseudolabels, and (iii) an incremental knowledge distillation strategy to incorporatenovel classes into the existing knowledge base gradually. Our framework enablesthe model to behave like human beings, which could recognize unknown objectsand incrementally learn them with the corresponding knowledge. Experimentalresults on the S3DIS and ScanNetv2 datasets demonstrate that the proposed PDFoutperforms other methods by a large margin in both important tasks of openworld semantic segmentation.</description><author>Jinfeng Xu, Siyuan Yang, Xianzhi Li, Yuan Tang, Yixue Hao, Long Hu, Min Chen</author><pubDate>Tue, 23 Jul 2024 15:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00979v2</guid></item><item><title>GenRec: A Flexible Data Generator for Recommendations</title><link>http://arxiv.org/abs/2407.16594v1</link><description>The scarcity of realistic datasets poses a significant challenge inbenchmarking recommender systems and social network analysis methods andtechniques. A common and effective solution is to generate synthetic data thatsimulates realistic interactions. However, although various methods have beenproposed, the existing literature still lacks generators that are fullyadaptable and allow easy manipulation of the underlying data distributions andstructural properties. To address this issue, the present work introducesGenRec, a novel framework for generating synthetic user-item interactions thatexhibit realistic and well-known properties observed in recommendationscenarios. The framework is based on a stochastic generative process based onlatent factor modeling. Here, the latent factors can be exploited to yieldlong-tailed preference distributions, and at the same time they characterizesubpopulations of users and topic-based item clusters. Notably, the proposedframework is highly flexible and offers a wide range of hyper-parameters forcustomizing the generation of user-item interactions. The code used to performthe experiments is publicly available athttps://anonymous.4open.science/r/GenRec-DED3.</description><author>Erica Coppolillo, Simone Mungari, Ettore Ritacco, Giuseppe Manco</author><pubDate>Tue, 23 Jul 2024 15:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16594v1</guid></item><item><title>A Comparative Study on Patient Language across Therapeutic Domains for Effective Patient Voice Classification in Online Health Discussions</title><link>http://arxiv.org/abs/2407.16593v1</link><description>There exists an invisible barrier between healthcare professionals'perception of a patient's clinical experience and the reality. This barrier maybe induced by the environment that hinders patients from sharing theirexperiences openly with healthcare professionals. As patients are observed todiscuss and exchange knowledge more candidly on social media, valuable insightscan be leveraged from these platforms. However, the abundance of non-patientposts on social media necessitates filtering out such irrelevant content todistinguish the genuine voices of patients, a task we refer to as patient voiceclassification. In this study, we analyse the importance of linguisticcharacteristics in accurately classifying patient voices. Our findingsunderscore the essential role of linguistic and statistical text similarityanalysis in identifying common patterns among patient groups. These resultsallude to even starker differences in the way patients express themselves at adisease level and across various therapeutic domains. Additionally, wefine-tuned a pre-trained Language Model on the combined datasets with similarlinguistic patterns, resulting in a highly accurate automatic patient voiceclassification. Being the pioneering study on the topic, our focus onextracting authentic patient experiences from social media stands as a crucialstep towards advancing healthcare standards and fostering a patient-centricapproach.</description><author>Giorgos Lysandrou, Roma English Owen, Vanja Popovic, Grant Le Brun, Aryo Pradipta Gema, Beatrice Alex, Elizabeth A. L. Fairley</author><pubDate>Tue, 23 Jul 2024 15:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16593v1</guid></item><item><title>Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters</title><link>http://arxiv.org/abs/2407.01406v2</link><description>This paper explores the integration of graph knowledge from linguisticontologies into multilingual Large Language Models (LLMs) using adapters toimprove performance for low-resource languages (LRLs) in sentiment analysis(SA) and named entity recognition (NER). Building upon successfulparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, wepropose a similar approach for incorporating knowledge from multilingualgraphs, connecting concepts in various languages with each other throughlinguistic relationships, into multilingual LLMs for LRLs. Specifically, wefocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,Uyghur, Tibetan, and Sinhala -- and employ language-specific adaptersfine-tuned on data extracted from the language-specific section of ConceptNet,aiming to enable knowledge transfer across the languages covered by theknowledge graph. We compare various fine-tuning objectives, including standardMasked Language Modeling (MLM), MLM with full-word masking, and MLM withtargeted masking, to analyse their effectiveness in learning and integratingthe extracted graph data. Through empirical evaluation on language-specifictasks, we assess how structured graph knowledge affects the performance ofmultilingual LLMs for LRLs in SA and NER, providing insights into the potentialbenefits of adapting language models for low-resource scenarios.</description><author>Daniil Gurgurov, Mareike Hartmann, Simon Ostermann</author><pubDate>Tue, 23 Jul 2024 15:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01406v2</guid></item><item><title>A Faster Branching Algorithm for the Maximum $k$-Defective Clique Problem</title><link>http://arxiv.org/abs/2407.16588v1</link><description>A $k$-defective clique of an undirected graph $G$ is a subset of its verticesthat induces a nearly complete graph with a maximum of $k$ missing edges. Themaximum $k$-defective clique problem, which asks for the largest $k$-defectiveclique from the given graph, is important in many applications, such as socialand biological network analysis. In the paper, we propose a new branchingalgorithm that takes advantage of the structural properties of the$k$-defective clique and uses the efficient maximum clique algorithm as asubroutine. As a result, the algorithm has a better asymptotic running timethan the existing ones. We also investigate upper-bounding techniques andpropose a new upper bound utilizing the \textit{conflict relationship} betweenvertex pairs. Because conflict relationship is common in many graph problems,we believe that this technique can be potentially generalized. Finally,experiments show that our algorithm outperforms state-of-the-art solvers on awide range of open benchmarks.</description><author>Chunyu Luo, Yi Zhou Zhengren Wang, Mingyu Xiao</author><pubDate>Tue, 23 Jul 2024 15:40:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16588v1</guid></item><item><title>SemiSFL: Split Federated Learning on Unlabeled and Non-IID Data</title><link>http://arxiv.org/abs/2307.15870v4</link><description>Federated Learning (FL) has emerged to allow multiple clients tocollaboratively train machine learning models on their private data at thenetwork edge. However, training and deploying large-scale models onresource-constrained devices is challenging. Fortunately, Split FederatedLearning (SFL) offers a feasible solution by alleviating the computation and/orcommunication burden on clients. However, existing SFL works often assumesufficient labeled data on clients, which is usually impractical. Besides, datanon-IIDness poses another challenge to ensure efficient model training. To ourbest knowledge, the above two issues have not been simultaneously addressed inSFL. Herein, we propose a novel Semi-supervised SFL system, termed SemiSFL,which incorporates clustering regularization to perform SFL with unlabeled andnon-IID client data. Moreover, our theoretical and experimental investigationsinto model convergence reveal that the inconsistent training processes onlabeled and unlabeled data have an influence on the effectiveness of clusteringregularization. To mitigate the training inconsistency, we develop an algorithmfor dynamically adjusting the global updating frequency, so as to improvetraining performance. Extensive experiments on benchmark models and datasetsshow that our system provides a 3.8x speed-up in training time, reduces thecommunication cost by about 70.3% while reaching the target accuracy, andachieves up to 5.8% improvement in accuracy under non-IID scenarios compared tothe state-of-the-art baselines.</description><author>Yang Xu, Yunming Liao, Hongli Xu, Zhipeng Sun, Liusheng Huang, Chunming Qiao</author><pubDate>Tue, 23 Jul 2024 15:30:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15870v4</guid></item><item><title>Timeliness-Fidelity Tradeoff in 3D Scene Representations</title><link>http://arxiv.org/abs/2407.16575v1</link><description>Real-time three-dimensional (3D) scene representations serve as one of thebuilding blocks that bolster various innovative applications, e.g., digitalmanufacturing, Virtual/Augmented/Extended/Mixed Reality (VR/AR/XR/MR), and themetaverse. Despite substantial efforts that have been made to real-timecommunications and computing, real-time 3D scene representations remain achallenging task. This paper investigates the tradeoff between timeliness andfidelity in real-time 3D scene representations. Specifically, we establish aframework to evaluate the impact of communication delay on the tradeoff, wherethe real-world scenario is monitored by multiple cameras that communicate withan edge server. To improve fidelity for 3D scene representations, we propose touse a single-step Proximal Policy Optimization (PPO) method that leverages theAge of Information (AoI) to decide if the received image needs to be involvedin 3D scene representations and rendering. We test our framework and theproposed approach with different well-known 3D scene representation methods.Simulation results reveal that real-time 3D scene representation can besensitively affected by communication delay, and our proposed method canachieve optimal 3D scene representation results.</description><author>Xiangmin Xu, Zhen Meng, Yichi Zhang, Changyang She, Philip G. Zhao</author><pubDate>Tue, 23 Jul 2024 15:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16575v1</guid></item><item><title>TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback</title><link>http://arxiv.org/abs/2407.16574v1</link><description>Reinforcement Learning from Human Feedback (RLHF) leverages human preferencedata to train language models to align more closely with human essence. Thesehuman preference data, however, are labeled at the sequence level, creating amismatch between sequence-level preference labels and tokens, which areautoregressively generated from the language model. Although several recentapproaches have tried to provide token-level (i.e., dense) rewards for eachindividual token, these typically rely on predefined discrete reward values(e.g., positive: +1, negative: -1, neutral: 0), failing to account for varyingdegrees of preference inherent to each token. To address this limitation, weintroduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates adiscriminator trained to distinguish positive and negative tokens, and theconfidence of the discriminator is used to assign continuous rewards to eachtoken considering the context. Extensive experiments show that our proposedTLCR leads to consistent performance improvements over previous sequence-levelor token-level discrete rewards on open-ended generation benchmarks.</description><author>Eunseop Yoon, Hee Suk Yoon, SooHwan Eom, Gunsoo Han, Daniel Wontae Nam, Daejin Jo, Kyoung-Woon On, Mark A. Hasegawa-Johnson, Sungwoong Kim, Chang D. Yoo</author><pubDate>Tue, 23 Jul 2024 15:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16574v1</guid></item><item><title>Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models</title><link>http://arxiv.org/abs/2407.16565v1</link><description>Recent surge in the accessibility of large language models (LLMs) to thegeneral population can lead to untrackable use of such models formedical-related recommendations. Language generation via LLMs models has twokey problems: firstly, they are prone to hallucination and therefore, for anymedical purpose they require scientific and factual grounding; secondly, LLMspose tremendous challenge to computational resources due to their giganticmodel size. In this work, we introduce pRAGe, a pipeline for RetrievalAugmented Generation and evaluation of medical paraphrases generation usingSmall Language Models (SLM). We study the effectiveness of SLMs and the impactof external knowledge base for medical paraphrase generation in French.</description><author>Ioana Buhnila, Aman Sinha, Mathieu Constant</author><pubDate>Tue, 23 Jul 2024 15:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16565v1</guid></item><item><title>Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning</title><link>http://arxiv.org/abs/2407.16564v1</link><description>Text-to-music models allow users to generate nearly realistic musical audiowith textual commands. However, editing music audios remains challenging due tothe conflicting desiderata of performing fine-grained alterations on the audiowhile maintaining a simple user interface. To address this challenge, wepropose Audio Prompt Adapter (or AP-Adapter), a lightweight addition topretrained text-to-music models. We utilize AudioMAE to extract features fromthe input audio, and construct attention-based adapters to feedthese featuresinto the internal layers of AudioLDM2, a diffusion-based text-to-music model.With 22M trainable parameters, AP-Adapter empowers users to harness both global(e.g., genre and timbre) and local (e.g., melody) aspects of music, using theoriginal audio and a short text as inputs. Through objective and subjectivestudies, we evaluate AP-Adapter on three tasks: timbre transfer, genretransfer, and accompaniment generation. Additionally, we demonstrate itseffectiveness on out-of-domain audios containing unseen instruments duringtraining.</description><author>Fang-Duo Tsai, Shih-Lun Wu, Haven Kim, Bo-Yu Chen, Hao-Chung Cheng, Yi-Hsuan Yang</author><pubDate>Tue, 23 Jul 2024 15:16:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16564v1</guid></item><item><title>COALA: A Practical and Vision-Centric Federated Learning Platform</title><link>http://arxiv.org/abs/2407.16560v1</link><description>We present COALA, a vision-centric Federated Learning (FL) platform, and asuite of benchmarks for practical FL scenarios, which we categorize into threelevels: task, data, and model. At the task level, COALA extends support fromsimple classification to 15 computer vision tasks, including object detection,segmentation, pose estimation, and more. It also facilitates federatedmultiple-task learning, allowing clients to tackle multiple taskssimultaneously. At the data level, COALA goes beyond supervised FL to benchmarkboth semi-supervised FL and unsupervised FL. It also benchmarks featuredistribution shifts other than commonly considered label distribution shifts.In addition to dealing with static data, it supports federated continuallearning for continuously changing data in real-world scenarios. At the modellevel, COALA benchmarks FL with split models and different models in differentclients. COALA platform offers three degrees of customization for thesepractical FL scenarios, including configuration customization, componentscustomization, and workflow customization. We conduct systematic benchmarkingexperiments for the practical FL scenarios and highlight potentialopportunities for further advancements in FL. Codes are open sourced athttps://github.com/SonyResearch/COALA.</description><author>Weiming Zhuang, Jian Xu, Chen Chen, Jingtao Li, Lingjuan Lyu</author><pubDate>Tue, 23 Jul 2024 15:14:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16560v1</guid></item><item><title>Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method</title><link>http://arxiv.org/abs/2401.17460v2</link><description>Cross-device federated learning (FL) is a growing machine learning settingwhereby multiple edge devices collaborate to train a model without disclosingtheir raw data. With the great number of mobile devices participating in moreFL applications via the wireless environment, the practical implementation ofthese applications will be hindered due to the limited uplink capacity ofdevices, causing critical bottlenecks. In this work, we propose a novel doublycommunication-efficient zero-order (ZO) method with a one-point gradientestimator that replaces communicating long vectors with scalar values and thatharnesses the nature of the wireless communication channel, overcoming the needto know the channel state coefficient. It is the first method that includes thewireless channel in the learning algorithm itself instead of wasting resourcesto analyze it and remove its impact. We then offer a thorough analysis of theproposed zero-order federated learning (ZOFL) framework and prove that ourmethod converges \textit{almost surely}, which is a novel result in nonconvexZO optimization. We further prove a convergence rate of$O(\frac{1}{\sqrt[3]{K}})$ in the nonconvex setting. We finally demonstrate thepotential of our algorithm with experimental results.</description><author>Elissa Mhanna, Mohamad Assaad</author><pubDate>Tue, 23 Jul 2024 15:14:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17460v2</guid></item><item><title>RogueGPT: dis-ethical tuning transforms ChatGPT4 into a Rogue AI in 158 Words</title><link>http://arxiv.org/abs/2407.15009v2</link><description>The ethical implications and potentials for misuse of Generative ArtificialIntelligence are increasingly worrying topics. This paper explores how easilythe default ethical guardrails of ChatGPT, using its latest customizationfeatures, can be bypassed by simple prompts and fine-tuning, that can beeffortlessly accessed by the broad public. This malevolently altered version ofChatGPT, nicknamed "RogueGPT", responded with worrying behaviours, beyond thosetriggered by jailbreak prompts. We conduct an empirical study of RogueGPTresponses, assessing its flexibility in answering questions pertaining to whatshould be disallowed usage. Our findings raise significant concerns about themodel's knowledge about topics like illegal drug production, torture methodsand terrorism. The ease of driving ChatGPT astray, coupled with its globalaccessibility, highlights severe issues regarding the data quality used fortraining the foundational model and the implementation of ethical safeguards.We thus underline the responsibilities and dangers of user-drivenmodifications, and the broader effects that these may have on the design ofsafeguarding and ethical modules implemented by AI programmers.</description><author>Alessio Buscemi, Daniele Proverbio</author><pubDate>Tue, 23 Jul 2024 15:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15009v2</guid></item><item><title>Patched RTC: evaluating LLMs for diverse software development tasks</title><link>http://arxiv.org/abs/2407.16557v1</link><description>This paper introduces Patched Round-Trip Correctness (Patched RTC), a novelevaluation technique for Large Language Models (LLMs) applied to diversesoftware development tasks, particularly focusing on "outer loop" activitiessuch as bug fixing, code review, and documentation updates. Patched RTC extendsthe original Round-Trip Correctness method to work with any LLM and downstreamtask, offering a self-evaluating framework that measures consistency androbustness of model responses without human intervention. The studydemonstrates a correlation between Patched RTC scores and task-specificaccuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigmfor open-domain task evaluation. We implement Patched RTC in an open-sourceframework called patchwork, allowing for transparent evaluation duringinference across various patchflows. Experiments comparing GPT-3.5 and GPT-4models across different software development tasks reveal that Patched RTCeffectively distinguishes model performance and task difficulty. The paper alsoexplores the impact of consistency prompts on improving model accuracy,suggesting that Patched RTC can guide prompt refinement and model selection forcomplex software development workflows.</description><author>Asankhaya Sharma</author><pubDate>Tue, 23 Jul 2024 15:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16557v1</guid></item><item><title>DC is all you need: describing ReLU from a signal processing standpoint</title><link>http://arxiv.org/abs/2407.16556v1</link><description>Non-linear activation functions are crucial in Convolutional Neural Networks.However, until now they have not been well described in the frequency domain.In this work, we study the spectral behavior of ReLU, a popular activationfunction. We use the ReLU's Taylor expansion to derive its frequency domainbehavior. We demonstrate that ReLU introduces higher frequency oscillations inthe signal and a constant DC component. Furthermore, we investigate theimportance of this DC component, where we demonstrate that it helps the modelextract meaningful features related to the input frequency content. Weaccompany our theoretical derivations with experiments and real-world examples.First, we numerically validate our frequency response model. Then we observeReLU's spectral behavior on two example models and a real-world one. Finally,we experimentally investigate the role of the DC component introduced by ReLUin the CNN's representations. Our results indicate that the DC helps toconverge to a weight configuration that is close to the initial random weights.</description><author>Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza</author><pubDate>Tue, 23 Jul 2024 15:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16556v1</guid></item><item><title>Coarse-to-Fine Proposal Refinement Framework for Audio Temporal Forgery Detection and Localization</title><link>http://arxiv.org/abs/2407.16554v1</link><description>Recently, a novel form of audio partial forgery has posed challenges to itsforensics, requiring advanced countermeasures to detect subtle forgerymanipulations within long-duration audio. However, existing countermeasuresstill serve a classification purpose and fail to perform meaningful analysis ofthe start and end timestamps of partial forgery segments. To address thischallenge, we introduce a novel coarse-to-fine proposal refinement framework(CFPRF) that incorporates a frame-level detection network (FDN) and a proposalrefinement network (PRN) for audio temporal forgery detection and localization.Specifically, the FDN aims to mine informative inconsistency cues between realand fake frames to obtain discriminative features that are beneficial forroughly indicating forgery regions. The PRN is responsible for predictingconfidence scores and regression offsets to refine the coarse-grained proposalsderived from the FDN. To learn robust discriminative features, we devise adifference-aware feature learning (DAFL) module guided by contrastiverepresentation learning to enlarge the sensitive differences between differentframes induced by minor manipulations. We further design a boundary-awarefeature enhancement (BAFE) module to capture the contextual information ofmultiple transition boundaries and guide the interaction between boundaryinformation and temporal features via a cross-attention mechanism. Extensiveexperiments show that our CFPRF achieves state-of-the-art performance onvarious datasets, including LAV-DF, ASVS2019PS, and HAD.</description><author>Junyan Wu, Wei Lu, Xiangyang Luo, Rui Yang, Qian Wang, Xiaochun Cao</author><pubDate>Tue, 23 Jul 2024 15:07:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16554v1</guid></item><item><title>MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues</title><link>http://arxiv.org/abs/2407.16552v1</link><description>Multimodal Large Language Models (MLLMs) have demonstrated remarkablemultimodal emotion recognition capabilities, integrating multimodal cues fromvisual, acoustic, and linguistic contexts in the video to recognize humanemotional states. However, existing methods ignore capturing local facialfeatures of temporal dynamics of micro-expressions and do not leverage thecontextual dependencies of the utterance-aware temporal segments in the video,thereby limiting their expected effectiveness to a certain extent. In thiswork, we propose MicroEmo, a time-sensitive MLLM aimed at directing attentionto the local facial micro-expression dynamics and the contextual dependenciesof utterance-aware video clips. Our model incorporates two key architecturalcontributions: (1) a global-local attention visual encoder that integratesglobal frame-level timestamp-bound image features with local facial features oftemporal dynamics of micro-expressions; (2) an utterance-aware video Q-Formerthat captures multi-scale and contextual dependencies by generating visualtoken sequences for each utterance segment and for the entire video thencombining them. Preliminary qualitative experiments demonstrate that in a newExplainable Multimodal Emotion Recognition (EMER) task that exploitsmulti-modal and multi-faceted clues to predict emotions in an open-vocabulary(OV) manner, MicroEmo demonstrates its effectiveness compared with the latestmethods.</description><author>Liyun Zhang</author><pubDate>Tue, 23 Jul 2024 15:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16552v1</guid></item><item><title>A Kernel-Based Conditional Two-Sample Test Using Nearest Neighbors (with Applications to Calibration, Regression Curves, and Simulation-Based Inference)</title><link>http://arxiv.org/abs/2407.16550v1</link><description>In this paper we introduce a kernel-based measure for detecting differencesbetween two conditional distributions. Using the `kernel trick' andnearest-neighbor graphs, we propose a consistent estimate of this measure whichcan be computed in nearly linear time (for a fixed number of nearestneighbors). Moreover, when the two conditional distributions are the same, theestimate has a Gaussian limit and its asymptotic variance has a simple formthat can be easily estimated from the data. The resulting test attains preciseasymptotic level and is universally consistent for detecting differencesbetween two conditional distributions. We also provide a resampling based testusing our estimate that applies to the conditional goodness-of-fit problem,which controls Type I error in finite samples and is asymptotically consistentwith only a finite number of resamples. A method to de-randomize the resamplingtest is also presented. The proposed methods can be readily applied to a broadrange of problems, ranging from classical nonparametric statistics to modernmachine learning. Specifically, we explore three applications: testing modelcalibration, regression curve evaluation, and validation of emulator models insimulation-based inference. We illustrate the superior performance of ourmethod for these tasks, both in simulations as well as on real data. Inparticular, we apply our method to (1) assess the calibration of neural networkmodels trained on the CIFAR-10 dataset, (2) compare regression functions forwind power generation across two different turbines, and (3) validate emulatormodels on benchmark examples with intractable posteriors and for generatingsynthetic `redshift' associated with galaxy images.</description><author>Anirban Chatterjee, Ziang Niu, Bhaswar B. Bhattacharya</author><pubDate>Tue, 23 Jul 2024 15:04:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16550v1</guid></item><item><title>Era Splitting: Invariant Learning for Decision Trees</title><link>http://arxiv.org/abs/2309.14496v5</link><description>Real-life machine learning problems exhibit distributional shifts in the datafrom one time to another or from one place to another. This behavior is beyondthe scope of the traditional empirical risk minimization paradigm, whichassumes i.i.d. distribution of data over time and across locations. Theemerging field of out-of-distribution (OOD) generalization addresses thisreality with new theory and algorithms which incorporate "environmental", or"era-wise" information into the algorithms. So far, most research has beenfocused on linear models and/or neural networks . In this research we developtwo new splitting criteria for decision trees, which allow us to apply ideasfrom OOD generalization research to decision tree models, namely, gradientboosting decision trees (GBDTs). The new splitting criteria use era-wiseinformation associated with the data to grow tree-based models that are optimalacross all disjoint eras in the data, instead of optimal over the entire dataset pooled together, which is the default setting. In this paper, two newsplitting criteria are defined and analyzed theoretically. Effectiveness istested on four experiments, ranging from simple, synthetic to complex,real-world applications. In particular we cast the OOD domain-adaptationproblem in the context of financial markets, where the new models out-performstate-of-the-art GBDT models on the Numerai data set. The new criteria areincorporated into the Scikit-Learn code base and made freely available online.</description><author>Timothy DeLise</author><pubDate>Tue, 23 Jul 2024 15:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14496v5</guid></item><item><title>Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions</title><link>http://arxiv.org/abs/2402.15602v2</link><description>We study the asymptotic error of score-based diffusion model sampling inlarge-sample scenarios from a non-parametric statistics perspective. We showthat a kernel-based score estimator achieves an optimal mean square error of$\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and$d$ represent the sample size and the dimension, $t$ is bounded above and belowby polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. Asa consequence, this yields an $\widetilde{O}\left(n^{-1/2}t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of thedistribution of the sample generated by the diffusion model under a meresub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametricfamily of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an earlystopping strategy, we obtain that the diffusion model is nearly (up to logfactors) minimax optimal. This removes the crucial lower bound assumption on$p_0$ in previous proofs of the minimax optimality of the diffusion model fornonparametric families.</description><author>Kaihong Zhang, Caitlyn H. Yin, Feng Liang, Jingbo Liu</author><pubDate>Tue, 23 Jul 2024 15:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15602v2</guid></item><item><title>Disentangling spatio-temporal knowledge for weakly supervised object detection and segmentation in surgical video</title><link>http://arxiv.org/abs/2407.15794v2</link><description>Weakly supervised video object segmentation (WSVOS) enables theidentification of segmentation maps without requiring an extensive trainingdataset of object masks, relying instead on coarse video labels indicatingobject presence. Current state-of-the-art methods either require multipleindependent stages of processing that employ motion cues or, in the case ofend-to-end trainable networks, lack in segmentation accuracy, in part due tothe difficulty of learning segmentation maps from videos with transient objectpresence. This limits the application of WSVOS for semantic annotation ofsurgical videos where multiple surgical tools frequently move in and out of thefield of view, a problem that is more difficult than typically encountered inWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks(VDST-Net), a framework to disentangle spatiotemporal information usingsemi-decoupled knowledge distillation to predict high-quality class activationmaps (CAMs). A teacher network designed to resolve temporal conflicts whenspecifics about object location and timing in the video are not provided workswith a student network that integrates information over time by leveragingtemporal dependencies. We demonstrate the efficacy of our framework on a publicreference dataset and on a more challenging surgical video dataset whereobjects are, on average, present in less than 60\% of annotated frames. Ourmethod outperforms state-of-the-art techniques and generates superiorsegmentation masks under video-level weak supervision.</description><author>Guiqiu Liao, Matjaz Jogan, Sai Koushik, Eric Eaton, Daniel A. Hashimoto</author><pubDate>Tue, 23 Jul 2024 14:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15794v2</guid></item><item><title>End-to-End Video Question Answering with Frame Scoring Mechanisms and Adaptive Sampling</title><link>http://arxiv.org/abs/2407.15047v2</link><description>Video Question Answering (VideoQA) has emerged as a challenging frontier inthe field of multimedia processing, requiring intricate interactions betweenvisual and textual modalities. Simply uniformly sampling frames orindiscriminately aggregating frame-level visual features often falls short incapturing the nuanced and relevant contexts of videos to well perform VideoQA.To mitigate these issues, we propose VidF4, a novel VideoQA framework equippedwith tailored frame selection strategy for effective and efficient VideoQA. Wepropose three frame-scoring mechanisms that consider both question relevanceand inter-frame similarity to evaluate the importance of each frame for a givenquestion on the video. Furthermore, we design a differentiable adaptive framesampling mechanism to facilitate end-to-end training for the frame selector andanswer generator. The experimental results across three widely adoptedbenchmarks demonstrate that our model consistently outperforms existing VideoQAmethods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA(+1.0%). Furthermore, through both quantitative and qualitative analyses, wevalidate the effectiveness of each design choice.</description><author>Jianxin Liang, Xiaojun Meng, Yueqian Wang, Chang Liu, Qun Liu, Dongyan Zhao</author><pubDate>Tue, 23 Jul 2024 14:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15047v2</guid></item><item><title>AutoGPT+P: Affordance-based Task Planning with Large Language Models</title><link>http://arxiv.org/abs/2402.10778v2</link><description>Recent advances in task planning leverage Large Language Models (LLMs) toimprove generalizability by combining such models with classical planningalgorithms to address their inherent limitations in reasoning capabilities.However, these approaches face the challenge of dynamically capturing theinitial state of the task planning problem. To alleviate this issue, we proposeAutoGPT+P, a system that combines an affordance-based scene representation witha planning system. Affordances encompass the action possibilities of an agenton the environment and objects present in it. Thus, deriving the planningdomain from an affordance-based scene representation allows symbolic planningwith arbitrary objects. AutoGPT+P leverages this representation to derive andexecute a plan for a task specified by the user in natural language. Inaddition to solving planning tasks under a closed-world assumption, AutoGPT+Pcan also handle planning with incomplete information, e. g., tasks with missingobjects by exploring the scene, suggesting alternatives, or providing a partialplan. The affordance-based scene representation combines object detection withan automatically generated object-affordance-mapping using ChatGPT. The coreplanning tool extends existing work by automatically correcting semantic andsyntactic errors. Our approach achieves a success rate of 98%, surpassing thecurrent 81% success rate of the current state-of-the-art LLM-based planningmethod SayCan on the SayCan instruction set. Furthermore, we evaluated ourapproach on our newly created dataset with 150 scenarios covering a wide rangeof complex tasks with missing objects, achieving a success rate of 79% on ourdataset. The dataset and the code are publicly available athttps://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.</description><author>Timo Birr, Christoph Pohl, Abdelrahman Younes, Tamim Asfour</author><pubDate>Tue, 23 Jul 2024 14:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10778v2</guid></item><item><title>QPT V2: Masked Image Modeling Advances Visual Scoring</title><link>http://arxiv.org/abs/2407.16541v1</link><description>Quality assessment and aesthetics assessment aim to evaluate the perceivedquality and aesthetics of visual content. Current learning-based methods suffergreatly from the scarcity of labeled data and usually perform sub-optimally interms of generalization. Although masked image modeling (MIM) has achievednoteworthy advancements across various high-level tasks (e.g., classification,detection etc.). In this work, we take on a novel perspective to investigateits capabilities in terms of quality- and aesthetics-awareness. To this end, wepropose Quality- and aesthetics-aware pretraining (QPT V2), the firstpretraining framework based on MIM that offers a unified solution to qualityand aesthetics assessment. To perceive the high-level semantics andfine-grained details, pretraining data is curated. To comprehensively encompassquality- and aesthetics-related factors, degradation is introduced. To capturemulti-scale quality and aesthetic information, model structure is modified.Extensive experimental results on 11 downstream benchmarks clearly show thesuperior performance of QPT V2 in comparison with current state-of-the-artapproaches and other pretraining paradigms. Code and models will be released at\url{https://github.com/KeiChiTse/QPT-V2}.</description><author>Qizhi Xie, Kun Yuan, Yunpeng Qu, Mingda Wu, Ming Sun, Chao Zhou, Jihong Zhu</author><pubDate>Tue, 23 Jul 2024 14:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16541v1</guid></item><item><title>Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2407.15706v2</link><description>Skeleton-based action recognition has garnered significant attention due tothe utilization of concise and resilient skeletons. Nevertheless, the absenceof detailed body information in skeletons restricts performance, while othermultimodal methods require substantial inference resources and are inefficientwhen using multimodal data during both training and inference stages. Toaddress this and fully harness the complementary multimodal features, wepropose a novel multi-modality co-learning (MMCL) framework by leveraging themultimodal large language models (LLMs) as auxiliary networks for efficientskeleton-based action recognition, which engages in multi-modality co-learningduring the training stage and keeps efficiency by employing only conciseskeletons in inference. Our MMCL framework primarily consists of two modules.First, the Feature Alignment Module (FAM) extracts rich RGB features from videoframes and aligns them with global skeleton features via contrastive learning.Second, the Feature Refinement Module (FRM) uses RGB images with temporalinformation and text instruction to generate instructive features based on thepowerful generalization of multimodal LLMs. These instructive text featureswill further refine the classification scores and the refined scores willenhance the model's robustness and generalization in a manner similar to softlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLAbenchmarks consistently verify the effectiveness of our MMCL, which outperformsthe existing skeleton-based action recognition methods. Meanwhile, experimentson UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalizationof our MMCL in zero-shot and domain-adaptive action recognition. Our code ispublicly available at: https://github.com/liujf69/MMCL-Action.</description><author>Jinfu Liu, Chen Chen, Mengyuan Liu</author><pubDate>Tue, 23 Jul 2024 14:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15706v2</guid></item><item><title>A Diffusion Model for Simulation Ready Coronary Anatomy with Morpho-skeletal Control</title><link>http://arxiv.org/abs/2407.15631v2</link><description>Virtual interventions enable the physics-based simulation of devicedeployment within coronary arteries. This framework allows for counterfactualreasoning by deploying the same device in different arterial anatomies.However, current methods to create such counterfactual arteries face atrade-off between controllability and realism. In this study, we investigatehow Latent Diffusion Models (LDMs) can custom synthesize coronary anatomy forvirtual intervention studies based on mid-level anatomic constraints such astopological validity, local morphological shape, and global skeletal structure.We also extend diffusion model guidance strategies to the context ofmorpho-skeletal conditioning and propose a novel guidance method for continuousattributes that adaptively updates the negative guiding condition throughoutsampling. Our framework enables the generation and editing of coronary anatomyin a controllable manner, allowing device designers to derive mechanisticinsights regarding anatomic variation and simulated device deployment.</description><author>Karim Kadry, Shreya Gupta, Jonas Sogbadji, Michiel Schaap, Kersten Petersen, Takuya Mizukami, Carlos Collet, Farhad R. Nezami, Elazer R. Edelman</author><pubDate>Tue, 23 Jul 2024 14:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15631v2</guid></item><item><title>Mixture of segmentation for heterogeneous functional data</title><link>http://arxiv.org/abs/2303.10712v3</link><description>In this paper we consider functional data with heterogeneity in time and inpopulation. We propose a mixture model with segmentation of time to representthis heterogeneity while keeping the functional structure. Maximum likelihoodestimator is considered, proved to be identifiable and consistent. In practice,an EM algorithm is used, combined with dynamic programming for the maximizationstep, to approximate the maximum likelihood estimator. The method isillustrated on a simulated dataset, and used on a real dataset of electricityconsumption.</description><author>Vincent Brault, Émilie Devijver, Charlotte Laclau</author><pubDate>Tue, 23 Jul 2024 14:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10712v3</guid></item><item><title>Position: AI/ML Influencers Have a Place in the Academic Process</title><link>http://arxiv.org/abs/2401.13782v3</link><description>As the number of accepted papers at AI and ML conferences reaches into thethousands, it has become unclear how researchers access and read researchpublications. In this paper, we investigate the role of social mediainfluencers in enhancing the visibility of machine learning research,particularly the citation counts of papers they share. We have compiled acomprehensive dataset of over 8,000 papers, spanning tweets from December 2018to October 2023, alongside controls precisely matched by 9 key covariates. Ourstatistical and causal inference analysis reveals a significant increase incitations for papers endorsed by these influencers, with median citation counts2-3 times higher than those of the control group. Additionally, the studydelves into the geographic, gender, and institutional diversity of highlightedauthors. Given these findings, we advocate for a responsible approach tocuration, encouraging influencers to uphold the journalistic standard thatincludes showcasing diverse research topics, authors, and institutions.</description><author>Iain Xie Weissburg, Mehir Arora, Xinyi Wang, Liangming Pan, William Yang Wang</author><pubDate>Tue, 23 Jul 2024 14:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13782v3</guid></item><item><title>Unsupervised End-to-End Training with a Self-Defined Target</title><link>http://arxiv.org/abs/2403.12116v2</link><description>Designing algorithms for versatile AI hardware that can learn on the edgeusing both labeled and unlabeled data is challenging. Deep end-to-end trainingmethods incorporating phases of self-supervised and supervised learning areaccurate and adaptable to input data but self-supervised learning requires evenmore computational and memory resources than supervised learning, too high forcurrent embedded hardware. Conversely, unsupervised layer-by-layer training,such as Hebbian learning, is more compatible with existing hardware but doesnot integrate well with supervised learning. To address this, we propose amethod enabling networks or hardware designed for end-to-end supervisedlearning to also perform high-performance unsupervised learning by adding twosimple elements to the output layer: Winner-Take-All (WTA) selectivity andhomeostasis regularization. These mechanisms introduce a "self-defined target"for unlabeled data, allowing purely unsupervised training for bothfully-connected and convolutional layers using backpropagation or equilibriumpropagation on datasets like MNIST (up to 99.2%), Fashion-MNIST (up to 90.3%),and SVHN (up to 81.5%). We extend this method to semi-supervised learning,adjusting targets based on data type, achieving 96.6% accuracy with only 600labeled MNIST samples in a multi-layer perceptron. Our results show that thisapproach can effectively enable networks and hardware initially dedicated tosupervised learning to also perform unsupervised learning, adapting to varyingavailability of labeled data.</description><author>Dongshu Liu, Jérémie Laydevant, Adrien Pontlevy, Damien Querlioz, Julie Grollier</author><pubDate>Tue, 23 Jul 2024 14:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12116v2</guid></item><item><title>Enhancing Encrypted Internet Traffic Classification Through Advanced Data Augmentation Techniques</title><link>http://arxiv.org/abs/2407.16539v1</link><description>The increasing popularity of online services has made Internet TrafficClassification a critical field of study. However, the rapid development ofinternet protocols and encryption limits usable data availability. This paperaddresses the challenges of classifying encrypted internet traffic, focusing onthe scarcity of open-source datasets and limitations of existing ones. Wepropose two Data Augmentation (DA) techniques to synthetically generate databased on real samples: Average augmentation and MTU augmentation. Bothaugmentations are aimed to improve the performance of the classifier, each froma different perspective: The Average augmentation aims to increase dataset sizeby generating new synthetic samples, while the MTU augmentation enhancesclassifier robustness to varying Maximum Transmission Units (MTUs). Ourexperiments, conducted on two well-known academic datasets and a commercialdataset, demonstrate the effectiveness of these approaches in improving modelperformance and mitigating constraints associated with limited and homogeneousdatasets. Our findings underscore the potential of data augmentation inaddressing the challenges of modern internet traffic classification.Specifically, we show that our augmentation techniques significantly enhanceencrypted traffic classification models. This improvement can positively impactuser Quality of Experience (QoE) by more accurately classifying traffic asvideo streaming (e.g., YouTube) or chat (e.g., Google Chat). Additionally, itcan enhance Quality of Service (QoS) for file downloading activities (e.g.,Google Docs).</description><author>Yehonatan Zion, Porat Aharon, Ran Dubin, Amit Dvir, Chen Hajaj</author><pubDate>Tue, 23 Jul 2024 14:49:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16539v1</guid></item><item><title>Quantifying the Role of Textual Predictability in Automatic Speech Recognition</title><link>http://arxiv.org/abs/2407.16537v1</link><description>A long-standing question in automatic speech recognition research is how toattribute errors to the ability of a model to model the acoustics, versus itsability to leverage higher-order context (lexicon, morphology, syntax,semantics). We validate a novel approach which models error rates as a functionof relative textual predictability, and yields a single number, $k$, whichmeasures the effect of textual predictability on the recognizer. We use thismethod to demonstrate that a Wav2Vec 2.0-based model makes greater stronger useof textual context than a hybrid ASR model, in spite of not using an explicitlanguage model, and also use it to shed light on recent results demonstratingpoor performance of standard ASR systems on African-American English. Wedemonstrate that these mostly represent failures of acoustic--phoneticmodelling. We show how this approach can be used straightforwardly indiagnosing and improving ASR.</description><author>Sean Robertson, Gerald Penn, Ewan Dunbar</author><pubDate>Tue, 23 Jul 2024 14:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16537v1</guid></item></channel></rss>