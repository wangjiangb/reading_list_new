<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 20 May 2024 06:00:52 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Probabilistic transfer learning methodology to expedite high fidelity simulation of reactive flows</title><link>http://arxiv.org/abs/2405.10944v1</link><description>Reduced order models based on the transport of a lower dimensional manifoldrepresentation of the thermochemical state, such as Principal Component (PC)transport and Machine Learning (ML) techniques, have been developed to reducethe computational cost associated with the Direct Numerical Simulations (DNS)of reactive flows. Both PC transport and ML normally require an abundance ofdata to exhibit sufficient predictive accuracy, which might not be availabledue to the prohibitive cost of DNS or experimental data acquisition. Toalleviate such difficulties, similar data from an existing dataset or domain(source domain) can be used to train ML models, potentially resulting inadequate predictions in the domain of interest (target domain). This studypresents a novel probabilistic transfer learning (TL) framework to enhance thetrust in ML models in correctly predicting the thermochemical state in a lowerdimensional manifold and a sparse data setting. The framework uses Bayesianneural networks, and autoencoders, to reduce the dimensionality of the statespace and diffuse the knowledge from the source to the target domain. The newframework is applied to one-dimensional freely-propagating flame solutionsunder different data sparsity scenarios. The results reveal that there is anoptimal amount of knowledge to be transferred, which depends on the amount ofdata available in the target domain and the similarity between the domains. TLcan reduce the reconstruction error by one order of magnitude for cases withlarge sparsity. The new framework required 10 times less data for the targetdomain to reproduce the same error as in the abundant data scenario.Furthermore, comparisons with a state-of-the-art deterministic TL strategy showthat the probabilistic method can require four times less data to achieve thesame reconstruction error.</description><author>Bruno S. Soriano, Ki Sung Jung, Tarek Echekki, Jacqueline H. Chen, Mohammad Khalil</author><pubDate>Fri, 17 May 2024 18:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10944v1</guid></item><item><title>HLSFactory: A Framework Empowering High-Level Synthesis Datasets for Machine Learning and Beyond</title><link>http://arxiv.org/abs/2405.00820v2</link><description>Machine learning (ML) techniques have been applied to high-level synthesis(HLS) flows for quality-of-result (QoR) prediction and design space exploration(DSE). Nevertheless, the scarcity of accessible high-quality HLS datasets andthe complexity of building such datasets present challenges. Existing datasetshave limitations in terms of benchmark coverage, design space enumeration,vendor extensibility, or lack of reproducible and extensible software fordataset construction. Many works also lack user-friendly ways to add moredesigns, limiting wider adoption of such datasets. In response to these challenges, we introduce HLSFactory, a comprehensiveframework designed to facilitate the curation and generation of high-qualityHLS design datasets. HLSFactory has three main stages: 1) a design spaceexpansion stage to elaborate single HLS designs into large design spaces usingvarious optimization directives across multiple vendor tools, 2) a designsynthesis stage to execute HLS and FPGA tool flows concurrently across designs,and 3) a data aggregation stage for extracting standardized data into packageddatasets for ML usage. This tripartite architecture ensures broad design spacecoverage via design space expansion and supports multiple vendor tools. Userscan contribute to each stage with their own HLS designs and synthesis resultsand extend the framework itself with custom frontends and tool flows. We alsoinclude an initial set of built-in designs from common HLS benchmarks curatedopen-source HLS designs. We showcase the versatility and multi-functionality of our framework throughsix case studies: I) Design space sampling; II) Fine-grained parallelismbackend speedup; III) Targeting Intel's HLS flow; IV) Adding new auxiliarydesigns; V) Integrating published HLS data; VI) HLS tool version regressionbenchmarking. Code at https://github.com/sharc-lab/HLSFactory.</description><author>Stefan Abi-Karam, Rishov Sarkar, Allison Seigler, Sean Lowe, Zhigang Wei, Hanqiu Chen, Nanditha Rao, Lizy John, Aman Arora, Cong Hao</author><pubDate>Fri, 17 May 2024 18:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00820v2</guid></item><item><title>DINO as a von Mises-Fisher mixture model</title><link>http://arxiv.org/abs/2405.10939v1</link><description>Self-distillation methods using Siamese networks are popular forself-supervised pre-training. DINO is one such method based on a cross-entropyloss between $K$-dimensional probability vectors, obtained by applying asoftmax function to the dot product between representations and learntprototypes. Given the fact that the learned representations are$L^2$-normalized, we show that DINO and its derivatives, such as iBOT, can beinterpreted as a mixture model of von Mises-Fisher components. With thisinterpretation, DINO assumes equal precision for all components when theprototypes are also $L^2$-normalized. Using this insight we propose DINO-vMF,that adds appropriate normalization constants when computing the clusterassignment probabilities. Unlike DINO, DINO-vMF is stable also for the largerViT-Base model with unnormalized prototypes. We show that the added flexibilityof the mixture model is beneficial in terms of better image representations.The DINO-vMF pre-trained model consistently performs better than DINO on arange of downstream tasks. We obtain similar improvements for iBOT-vMF vs iBOTand thereby show the relevance of our proposed modification also for othermethods derived from DINO.</description><author>Hariprasath Govindarajan, Per Sidén, Jacob Roll, Fredrik Lindsten</author><pubDate>Fri, 17 May 2024 18:49:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10939v1</guid></item><item><title>Observational Scaling Laws and the Predictability of Language Model Performance</title><link>http://arxiv.org/abs/2405.10938v1</link><description>Understanding how language model performance varies with scale is critical tobenchmark and algorithm development. Scaling laws are one approach to buildingthis understanding, but the requirement of training models across manydifferent scales has limited their use. We propose an alternative,observational approach that bypasses model training and instead builds scalinglaws from ~80 publically available models. Building a single scaling law frommultiple model families is challenging due to large variations in theirtraining compute efficiencies and capabilities. However, we show that thesevariations are consistent with a simple, generalized scaling law where languagemodel performance is a function of a low-dimensional capability space, andmodel families only vary in their efficiency in converting training compute tocapabilities. Using this approach, we show the surprising predictability ofcomplex scaling phenomena: we show that several emergent phenomena follow asmooth, sigmoidal behavior and are predictable from small models; we show thatthe agent performance of models such as GPT-4 can be precisely predicted fromsimpler non-agentic benchmarks; and we show how to predict the impact ofpost-training interventions like Chain-of-Thought and Self-Consistency aslanguage model capabilities continue to improve.</description><author>Yangjun Ruan, Chris J. Maddison, Tatsunori Hashimoto</author><pubDate>Fri, 17 May 2024 18:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10938v1</guid></item><item><title>A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers</title><link>http://arxiv.org/abs/2405.10936v1</link><description>The rapid development of Large Language Models (LLMs) demonstrates remarkablemultilingual capabilities in natural language processing, attracting globalattention in both academia and industry. To mitigate potential discriminationand enhance the overall usability and accessibility for diverse language usergroups, it is important for the development of language-fair technology.Despite the breakthroughs of LLMs, the investigation into the multilingualscenario remains insufficient, where a comprehensive survey to summarize recentapproaches, developments, limitations, and potential solutions is desirable. Tothis end, we provide a survey with multiple perspectives on the utilization ofLLMs in the multilingual scenario. We first rethink the transitions betweenprevious and current research on pre-trained language models. Then we introduceseveral perspectives on the multilingualism of LLMs, including training andinference methods, model security, multi-domain with language culture, andusage of datasets. We also discuss the major challenges that arise in theseaspects, along with possible solutions. Besides, we highlight future researchdirections that aim at further enhancing LLMs with multilingualism. The surveyaims to help the research community address multilingual problems and provide acomprehensive understanding of the core concepts, key techniques, and latestdevelopments in multilingual natural language processing based on LLMs.</description><author>Kaiyu Huang, Fengran Mo, Hongliang Li, You Li, Yuanchi Zhang, Weijian Yi, Yulong Mao, Jinchen Liu, Yuzhuang Xu, Jinan Xu, Jian-Yun Nie, Yang Liu</author><pubDate>Fri, 17 May 2024 18:47:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10936v1</guid></item><item><title>Visibility into AI Agents</title><link>http://arxiv.org/abs/2401.13138v6</link><description>Increased delegation of commercial, scientific, governmental, and personalactivities to AI agents -- systems capable of pursuing complex goals withlimited supervision -- may exacerbate existing societal risks and introduce newrisks. Understanding and mitigating these risks involves critically evaluatingexisting governance structures, revising and adapting these structures whereneeded, and ensuring accountability of key stakeholders. Information aboutwhere, why, how, and by whom certain AI agents are used, which we refer to asvisibility, is critical to these objectives. In this paper, we assess threecategories of measures to increase visibility into AI agents: agentidentifiers, real-time monitoring, and activity logging. For each, we outlinepotential implementations that vary in intrusiveness and informativeness. Weanalyze how the measures apply across a spectrum of centralized throughdecentralized deployment contexts, accounting for various actors in the supplychain including hardware and software service providers. Finally, we discussthe implications of our measures for privacy and concentration of power.Further work into understanding the measures and mitigating their negativeimpacts can help to build a foundation for the governance of AI agents.</description><author>Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond, Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt, Lennart Heim, Markus Anderljung</author><pubDate>Fri, 17 May 2024 18:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13138v6</guid></item><item><title>Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification</title><link>http://arxiv.org/abs/2311.10319v6</link><description>Advancements in clinical treatment are increasingly constrained by thelimitations of supervised learning techniques, which depend heavily on largevolumes of annotated data. The annotation process is not only costly but alsodemands substantial time from clinical specialists. Addressing this issue, weintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)pipeline, a novel approach that leverages advancements in self-supervised andsemi-supervised learning. These techniques engage in auxiliary tasks that donot require labeling, thus simplifying the scaling of machine supervisioncompared to fully-supervised methods. Our study benchmarks these techniques onthree distinct medical imaging datasets to evaluate their effectiveness inclassification and segmentation tasks. Notably, we observed that selfsupervised learning significantly surpassed the performance of supervisedmethods in the classification of all evaluated datasets. Remarkably, thesemi-supervised approach demonstrated superior outcomes in segmentation,outperforming fully-supervised methods while using 50% fewer labels across alldatasets. In line with our commitment to contributing to the scientificcommunity, we have made the S4MI code openly accessible, allowing for broaderapplication and further development of these methods.</description><author>Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone</author><pubDate>Fri, 17 May 2024 18:42:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10319v6</guid></item><item><title>Reconstruction of Manipulated Garment with Guided Deformation Prior</title><link>http://arxiv.org/abs/2405.10934v1</link><description>Modeling the shape of garments has received much attention, but most existingapproaches assume the garments to be worn by someone, which constrains therange of shapes they can assume. In this work, we address shape recovery whengarments are being manipulated instead of worn, which gives rise to an evenlarger range of possible shapes. To this end, we leverage the implicit sewingpatterns (ISP) model for garment modeling and extend it by adding adiffusion-based deformation prior to represent these shapes. To recover 3Dgarment shapes from incomplete 3D point clouds acquired when the garment isfolded, we map the points to UV space, in which our priors are learned, toproduce partial UV maps, and then fit the priors to recover complete UV mapsand 2D to 3D mappings. Experimental results demonstrate the superiorreconstruction accuracy of our method compared to previous ones, especiallywhen dealing with large non-rigid deformations arising from the manipulations.</description><author>Ren Li, Corentin Dumery, Zhantao Deng, Pascal Fua</author><pubDate>Fri, 17 May 2024 18:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10934v1</guid></item><item><title>Learning low-degree quantum objects</title><link>http://arxiv.org/abs/2405.10933v1</link><description>We consider the problem of learning low-degree quantum objects up to$\varepsilon$-error in $\ell_2$-distance. We show the following results: $(i)$unknown $n$-qubit degree-$d$ (in the Pauli basis) quantum channels andunitaries can be learned using $O(1/\varepsilon^d)$ queries (independent of$n$), $(ii)$ polynomials $p:\{-1,1\}^n\rightarrow [-1,1]$ arising from$d$-query quantum algorithms can be classically learned from$O((1/\varepsilon)^d\cdot \log n)$ many random examples $(x,p(x))$ (whichimplies learnability even for $d=O(\log n)$), and $(iii)$ degree-$d$polynomials $p:\{-1,1\}^n\to [-1,1]$ can be learned through$O(1/\varepsilon^d)$ queries to a quantum unitary $U_p$ that block-encodes $p$.Our main technical contributions are new Bohnenblust-Hille inequalities forquantum channels and completely bounded~polynomials.</description><author>Srinivasan Arunachalam, Arkopal Dutt, Francisco Escudero Gutiérrez, Carlos Palazuelos</author><pubDate>Fri, 17 May 2024 18:36:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10933v1</guid></item><item><title>Transpose Attack: Stealing Datasets with Bidirectional Training</title><link>http://arxiv.org/abs/2311.07389v2</link><description>Deep neural networks are normally executed in the forward direction. However,in this work, we identify a vulnerability that enables models to be trained inboth directions and on different tasks. Adversaries can exploit this capabilityto hide rogue models within seemingly legitimate models. In addition, in thiswork we show that neural networks can be taught to systematically memorize andretrieve specific samples from datasets. Together, these findings expose anovel method in which adversaries can exfiltrate datasets from protectedlearning environments under the guise of legitimate models. We focus on thedata exfiltration attack and show that modern architectures can be used tosecretly exfiltrate tens of thousands of samples with high fidelity, highenough to compromise data privacy and even train new models. Moreover, tomitigate this threat we propose a novel approach for detecting infected models.</description><author>Guy Amit, Mosh Levy, Yisroel Mirsky</author><pubDate>Fri, 17 May 2024 18:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07389v2</guid></item><item><title>Submodular Information Selection for Hypothesis Testing with Misclassification Penalties</title><link>http://arxiv.org/abs/2405.10930v1</link><description>We consider the problem of selecting an optimal subset of information sourcesfor a hypothesis testing/classification task where the goal is to identify thetrue state of the world from a finite set of hypotheses, based on finiteobservation samples from the sources. In order to characterize the learningperformance, we propose a misclassification penalty framework, which enablesnon-uniform treatment of different misclassification errors. In a centralizedBayesian learning setting, we study two variants of the subset selectionproblem: (i) selecting a minimum cost information set to ensure that themaximum penalty of misclassifying the true hypothesis remains bounded and (ii)selecting an optimal information set under a limited budget to minimize themaximum penalty of misclassifying the true hypothesis. Under mild assumptions,we prove that the objective (or constraints) of these combinatorialoptimization problems are weak (or approximate) submodular, and establishhigh-probability performance guarantees for greedy algorithms. Further, wepropose an alternate metric for information set selection which is based on thetotal penalty of misclassification. We prove that this metric is submodular andestablish near-optimal guarantees for the greedy algorithms for both theinformation set selection problems. Finally, we present numerical simulationsto validate our theoretical results over several randomly generated instances.</description><author>Jayanth Bhargav, Mahsa Ghasemi, Shreyas Sundaram</author><pubDate>Fri, 17 May 2024 18:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10930v1</guid></item><item><title>The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks</title><link>http://arxiv.org/abs/2405.10928v1</link><description>Mechanistic interpretability aims to understand the behavior of neuralnetworks by reverse-engineering their internal computations. However, currentmethods struggle to find clear interpretations of neural network activationsbecause a decomposition of activations into computational features is missing.Individual neurons or model components do not cleanly correspond to distinctfeatures or functions. We present a novel interpretability method that aims toovercome this limitation by transforming the activations of the network into anew basis - the Local Interaction Basis (LIB). LIB aims to identifycomputational features by removing irrelevant activations and interactions. Ourmethod drops irrelevant activation directions and aligns the basis with thesingular vectors of the Jacobian matrix between adjacent layers. It also scalesfeatures based on their importance for downstream computation, producing aninteraction graph that shows all computationally-relevant features andinteractions in a model. We evaluate the effectiveness of LIB on modularaddition and CIFAR-10 models, finding that it identifies morecomputationally-relevant features that interact more sparsely, compared toprincipal component analysis. However, LIB does not yield substantialimprovements in interpretability or interaction sparsity when applied tolanguage models. We conclude that LIB is a promising theory-driven approach foranalyzing neural networks, but in its current form is not applicable to largelanguage models.</description><author>Lucius Bushnaq, Stefan Heimersheim Nicholas Goldowsky-Dill, Dan Braun, Jake Mendel, Kaarel Hänni, Avery Griffin, Jörn Stöhler, Magdalena Wache, Marius Hobbhahn</author><pubDate>Fri, 17 May 2024 18:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10928v1</guid></item><item><title>Using Degeneracy in the Loss Landscape for Mechanistic Interpretability</title><link>http://arxiv.org/abs/2405.10927v1</link><description>Mechanistic Interpretability aims to reverse engineer the algorithmsimplemented by neural networks by studying their weights and activations. Anobstacle to reverse engineering neural networks is that many of the parametersinside a network are not involved in the computation being implemented by thenetwork. These degenerate parameters may obfuscate internal structure. Singularlearning theory teaches us that neural network parameterizations are biasedtowards being more degenerate, and parameterizations with more degeneracy arelikely to generalize further. We identify 3 ways that network parameters can bedegenerate: linear dependence between activations in a layer; linear dependencebetween gradients passed back to a layer; ReLUs which fire on the same subsetof datapoints. We also present a heuristic argument that modular networks arelikely to be more degenerate, and we develop a metric for identifying modulesin a network that is based on this argument. We propose that if we canrepresent a neural network in a way that is invariant to reparameterizationsthat exploit the degeneracies, then this representation is likely to be moreinterpretable, and we provide some evidence that such a representation islikely to have sparser interactions. We introduce the Interaction Basis, atractable technique to obtain a representation that is invariant todegeneracies from linear dependence of activations or Jacobians.</description><author>Lucius Bushnaq, Jake Mendel, Stefan Heimersheim, Dan Braun, Nicholas Goldowsky-Dill, Kaarel Hänni, Cindy Wu, Marius Hobbhahn</author><pubDate>Fri, 17 May 2024 18:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10927v1</guid></item><item><title>Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation</title><link>http://arxiv.org/abs/2402.17614v2</link><description>Few-shot segmentation performance declines substantially when facing imagesfrom a domain different than the training domain, effectively limitingreal-world use cases. To alleviate this, recently cross-domain few-shotsegmentation (CD-FSS) has emerged. Works that address this task mainlyattempted to learn segmentation on a source domain in a manner that generalizesacross domains. Surprisingly, we can outperform these approaches whileeliminating the training stage and removing their main segmentation network. Weshow test-time task-adaption is the key for successful CD-FSS instead.Task-adaption is achieved by appending small networks to the feature pyramid ofa conventionally classification-pretrained backbone. To avoid overfitting tothe few labeled samples in supervised fine-tuning, consistency across augmentedviews of input images serves as guidance while learning the parameters of theattached layers. Despite our self-restriction not to use any images other thanthe few labeled samples at test time, we achieve new state-of-the-artperformance in CD-FSS, evidencing the need to rethink approaches for the task.</description><author>Jonas Herzog</author><pubDate>Fri, 17 May 2024 18:25:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17614v2</guid></item><item><title>High-dimensional multiple imputation (HDMI) for partially observed confounders including natural language processing-derived auxiliary covariates</title><link>http://arxiv.org/abs/2405.10925v1</link><description>Multiple imputation (MI) models can be improved by including auxiliarycovariates (AC), but their performance in high-dimensional data is not wellunderstood. We aimed to develop and compare high-dimensional MI (HDMI)approaches using structured and natural language processing (NLP)-derived AC instudies with partially observed confounders. We conducted a plasmode simulationstudy using data from opioid vs. non-steroidal anti-inflammatory drug (NSAID)initiators (X) with observed serum creatinine labs (Z2) and time-to-acutekidney injury as outcome. We simulated 100 cohorts with a null treatmenteffect, including X, Z2, atrial fibrillation (U), and 13 otherinvestigator-derived confounders (Z1) in the outcome generation. We thenimposed missingness (MZ2) on 50% of Z2 measurements as a function of Z2 and Uand created different HDMI candidate AC using structured and NLP-derivedfeatures. We mimicked scenarios where U was unobserved by omitting it from allAC candidate sets. Using LASSO, we data-adaptively selected HDMI covariatesassociated with Z2 and MZ2 for MI, and with U to include in propensity scoremodels. The treatment effect was estimated following propensity score matchingin MI datasets and we benchmarked HDMI approaches against a baseline imputationand complete case analysis with Z1 only. HDMI using claims data showed thelowest bias (0.072). Combining claims and sentence embeddings led to animprovement in the efficiency displaying the lowest root-mean-squared-error(0.173) and coverage (94%). NLP-derived AC alone did not perform better thanbaseline MI. HDMI approaches may decrease bias in studies with partiallyobserved confounders where missingness depends on unobserved factors.</description><author>Janick Weberpals, Pamela A. Shaw, Kueiyu Joshua Lin, Richard Wyss, Joseph M Plasek, Li Zhou, Kerry Ngan, Thomas DeRamus, Sudha R. Raman, Bradley G. Hammill, Hana Lee, Sengwee Toh, John G. Connolly, Kimberly J. Dandreo, Fang Tian, Wei Liu, Jie Li, José J. Hernández-Muñoz, Sebastian Schneeweiss, Rishi J. Desai</author><pubDate>Fri, 17 May 2024 18:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10925v1</guid></item><item><title>ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2310.05872v2</link><description>In our work, we explore the synergistic capabilities of pre-trainedvision-and-language models (VLMs) and large language models (LLMs) on visualcommonsense reasoning (VCR) problems. We find that VLMs and LLMs-based decisionpipelines are good at different kinds of VCR problems. Pre-trained VLMs exhibitstrong performance for problems involving understanding the literal visualcontent, which we noted as visual commonsense understanding (VCU). For problemswhere the goal is to infer conclusions beyond image content, which we noted asvisual commonsense inference (VCI), VLMs face difficulties, while LLMs, givensufficient visual evidence, can use commonsense to infer the answer well. Weempirically validate this by letting LLMs classify VCR problems into these twocategories and show the significant difference between VLM and LLM with imagecaption decision pipelines on two subproblems. Moreover, we identify achallenge with VLMs' passive perception, which may miss crucial contextinformation, leading to incorrect reasoning by LLMs. Based on these, we suggesta collaborative approach, named ViCor, where pre-trained LLMs serve as problemclassifiers to analyze the problem category, then either use VLMs to answer thequestion directly or actively instruct VLMs to concentrate on and gatherrelevant visual elements to support potential commonsense inferences. Weevaluate our framework on two VCR benchmark datasets and outperform all othermethods that do not require in-domain fine-tuning.</description><author>Kaiwen Zhou, Kwonjoon Lee, Teruhisa Misu, Xin Eric Wang</author><pubDate>Fri, 17 May 2024 18:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05872v2</guid></item><item><title>Boosting Few-Pixel Robustness Verification via Covering Verification Designs</title><link>http://arxiv.org/abs/2405.10924v1</link><description>Proving local robustness is crucial to increase the reliability of neuralnetworks. While many verifiers prove robustness in $L_\infty$ $\epsilon$-balls,very little work deals with robustness verification in $L_0$ $\epsilon$-balls,capturing robustness to few pixel attacks. This verification introduces acombinatorial challenge, because the space of pixels to perturb is discrete andof exponential size. A previous work relies on covering designs to identifysets for defining $L_\infty$ neighborhoods, which if proven robust imply thatthe $L_0$ $\epsilon$-ball is robust. However, the number of neighborhoods toverify remains very high, leading to a high analysis time. We propose coveringverification designs, a combinatorial design that tailors effective butanalysis-incompatible coverings to $L_0$ robustness verification. The challengeis that computing a covering verification design introduces a high time andmemory overhead, which is intensified in our setting, where multiple candidatecoverings are required to identify how to reduce the overall analysis time. Weintroduce CoVerD, an $L_0$ robustness verifier that selects between differentcandidate coverings without constructing them, but by predicting their blocksize distribution. This prediction relies on a theorem providing closed-formexpressions for the mean and variance of this distribution. CoVerD constructsthe chosen covering verification design on-the-fly, while keeping the memoryconsumption minimal and enabling to parallelize the analysis. The experimentalresults show that CoVerD reduces the verification time on average by up to 5.1xcompared to prior work and that it scales to larger $L_0$ $\epsilon$-balls.</description><author>Yuval Shapira, Naor Wiesel, Shahar Shabelman, Dana Drachsler-Cohen</author><pubDate>Fri, 17 May 2024 18:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10924v1</guid></item><item><title>On Computational Modeling of Sleep-Wake Cycle</title><link>http://arxiv.org/abs/2404.05484v2</link><description>Why do mammals need to sleep? Neuroscience treats sleep and wake as defaultand perturbation modes of the brain. It is hypothesized that the brainself-organizes neural activities without environmental inputs. This paperpresents a new computational model of the sleep-wake cycle (SWC) for learningand memory. During the sleep mode, the memory consolidation by thethalamocortical system is abstracted by a disentangling operator that mapscontext-dependent representations (CDR) to context-independent representations(CIR) for generalization. Such a disentangling operator can be mathematicallyformalized by an integral transform that integrates the context variable fromCDR. During the wake mode, the memory formation by the hippocampal-neocorticalsystem is abstracted by an entangling operator from CIR to CDR where thecontext is introduced by physical motion. When designed as inductive bias,entangled CDR linearizes the problem of unsupervised learning for sensorymemory by direct-fit. The concatenation of disentangling and entanglingoperators forms a disentangling-entangling cycle (DEC) as the building blockfor sensorimotor learning. We also discuss the relationship of DEC and SWC tothe perception-action cycle (PAC) for internal model learning and perceptualcontrol theory for the ecological origin of natural languages.</description><author>Xin Li</author><pubDate>Fri, 17 May 2024 18:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05484v2</guid></item><item><title>Identifying the Risks of LM Agents with an LM-Emulated Sandbox</title><link>http://arxiv.org/abs/2309.15817v2</link><description>Recent advances in Language Model (LM) agents and tool use, exemplified byapplications like ChatGPT Plugins, enable a rich set of capabilities but alsoamplify potential risks - such as leaking private data or causing financiallosses. Identifying these risks is labor-intensive, necessitating implementingthe tools, setting up the environment for each test scenario manually, andfinding risky cases. As tools and agents become more complex, the high cost oftesting these agents will make it increasingly difficult to find high-stakes,long-tailed risks. To address these challenges, we introduce ToolEmu: aframework that uses an LM to emulate tool execution and enables the testing ofLM agents against a diverse range of tools and scenarios, without manualinstantiation. Alongside the emulator, we develop an LM-based automatic safetyevaluator that examines agent failures and quantifies associated risks. We testboth the tool emulator and evaluator through human evaluation and find that68.8% of failures identified with ToolEmu would be valid real-world agentfailures. Using our curated initial benchmark consisting of 36 high-stakestools and 144 test cases, we provide a quantitative risk analysis of current LMagents and identify numerous failures with potentially severe outcomes.Notably, even the safest LM agent exhibits such failures 23.9% of the timeaccording to our evaluator, underscoring the need to develop safer LM agentsfor real-world deployment.</description><author>Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, Tatsunori Hashimoto</author><pubDate>Fri, 17 May 2024 18:17:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15817v2</guid></item><item><title>Efficient Deep Learning with Decorrelated Backpropagation</title><link>http://arxiv.org/abs/2405.02385v2</link><description>The backpropagation algorithm remains the dominant and most successful methodfor training deep neural networks (DNNs). At the same time, training DNNs atscale comes at a significant computational cost and therefore a high carbonfootprint. Converging evidence suggests that input decorrelation may speed updeep learning. However, to date, this has not yet translated into substantialimprovements in training efficiency in large-scale DNNs. This is mainly causedby the challenge of enforcing fast and stable network-wide decorrelation. Here,we show for the first time that much more efficient training of very deepneural networks using decorrelated backpropagation is feasible. To achieve thisgoal we made use of a novel algorithm which induces network-wide inputdecorrelation using minimal computational overhead. By combining this algorithmwith careful optimizations, we obtain a more than two-fold speed-up and highertest accuracy compared to backpropagation when training a 18-layer deepresidual network. This demonstrates that decorrelation provides excitingprospects for efficient deep learning at scale.</description><author>Sander Dalm, Joshua Offergeld, Nasir Ahmad, Marcel van Gerven</author><pubDate>Fri, 17 May 2024 18:13:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02385v2</guid></item><item><title>GenToC: Leveraging Partially-Labeled Data for Product Attribute-Value Identification</title><link>http://arxiv.org/abs/2405.10918v1</link><description>In the e-commerce domain, the accurate extraction of attribute-value pairsfrom product listings (e.g., Brand: Apple) is crucial for enhancing search andrecommendation systems. The automation of this extraction process ischallenging due to the vast diversity of product categories and theirrespective attributes, compounded by the lack of extensive, accuratelyannotated training datasets and the demand for low latency to meet thereal-time needs of e-commerce platforms. To address these challenges, weintroduce GenToC, a novel two-stage model for extracting attribute-value pairsfrom product titles. GenToC is designed to train with partially-labeled data,leveraging incomplete attribute-value pairs and obviating the need for a fullyannotated dataset. Moreover, we introduce a bootstrapping method that enablesGenToC to progressively refine and expand its training dataset. Thisenhancement substantially improves the quality of data available for trainingother neural network models that are typically faster but are inherently lesscapable than GenToC in terms of their capacity to handle partially-labeleddata. By supplying an enriched dataset for training, GenToC significantlyadvances the performance of these alternative models, making them more suitablefor real-time deployment. Our results highlight the unique capability of GenToCto learn from a limited set of labeled data and to contribute to the trainingof more efficient models, marking a significant leap forward in the automatedextraction of attribute-value pairs from product titles. GenToC has beensuccessfully integrated into India's largest B2B e-commerce platform,IndiaMART.com, achieving a significant increase of 21.1% in recall over theexisting deployed system while maintaining a high precision of 89.5% in thischallenging task.</description><author>D. Subhalingam, Keshav Kolluru, Mausam, Saurabh Singal</author><pubDate>Fri, 17 May 2024 18:09:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10918v1</guid></item><item><title>Blackbox Adaptation for Medical Image Segmentation</title><link>http://arxiv.org/abs/2405.10913v1</link><description>In recent years, various large foundation models have been proposed for imagesegmentation. There models are often trained on large amounts of datacorresponding to general computer vision tasks. Hence, these models do notperform well on medical data. There have been some attempts in the literatureto perform parameter-efficient finetuning of such foundation models for medicalimage segmentation. However, these approaches assume that all the parameters ofthe model are available for adaptation. But, in many cases, these models arereleased as APIs or blackboxes, with no or limited access to the modelparameters and data. In addition, finetuning methods also require a significantamount of compute, which may not be available for the downstream task. At thesame time, medical data can't be shared with third-party agents for finetuningdue to privacy reasons. To tackle these challenges, we pioneer a blackboxadaptation technique for prompted medical image segmentation, called BAPS. BAPShas two components - (i) An Image-Prompt decoder (IP decoder) module thatgenerates visual prompts given an image and a prompt, and (ii) A Zero OrderOptimization (ZOO) Method, called SPSA-GC that is used to update the IP decoderwithout the need for backpropagating through the foundation model. Thus, ourmethod does not require any knowledge about the foundation model's weights orgradients. We test BAPS on four different modalities and show that our methodcan improve the original model's performance by around 4%.</description><author>Jay N. Paranjape, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel</author><pubDate>Fri, 17 May 2024 18:02:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10913v1</guid></item><item><title>Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models</title><link>http://arxiv.org/abs/2403.11802v3</link><description>While recent research endeavors have focused on developing Large LanguageModels (LLMs) with robust long-context capabilities, due to the lack oflong-context benchmarks, relatively little is known about how well theperformance of long-context LLMs. To address this gap, we propose amulti-evidence, position-aware, and scalable benchmark for evaluatinglong-context LLMs, named Counting-Stars, which evaluates long-context LLMs byusing two tasks: multi-evidence acquisition and multi-evidence reasoning. Basedon the Counting-Stars test, we conduct experiments to evaluate long-contextLLMs (i.e., GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4, and Moonshot-v1).Experimental results demonstrate that Gemini 1.5 Pro achieves the best overallresults, while the performance of GPT-4 Turbo is the most stable across varioustasks. Furthermore, our analysis of these LLMs, which are extended to handlelong-context scenarios, indicates that there is potential for improvement asthe length of the input context and the intricacy of the tasks are increasing.</description><author>Mingyang Song, Mao Zheng, Xuan Luo</author><pubDate>Fri, 17 May 2024 17:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11802v3</guid></item><item><title>AdaptiX -- A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics</title><link>http://arxiv.org/abs/2310.15887v3</link><description>With the ongoing efforts to empower people with mobility impairments and theincrease in technological acceptance by the general public, assistivetechnologies, such as collaborative robotic arms, are gaining popularity. Yet,their widespread success is limited by usability issues, specifically thedisparity between user input and software control along the autonomy continuum.To address this, shared control concepts provide opportunities to combine thetargeted increase of user autonomy with a certain level of computer assistance.This paper presents the free and open-source AdaptiX XR framework fordeveloping and evaluating shared control applications in a high-resolutionsimulation environment. The initial framework consists of a simulated roboticarm with an example scenario in Virtual Reality (VR), multiple standard controlinterfaces, and a specialized recording/replay system. AdaptiX can easily beextended for specific research needs, allowing Human-Robot Interaction (HRI)researchers to rapidly design and test novel interaction methods, interventionstrategies, and multi-modal feedback techniques, without requiring an actualphysical robotic arm during the early phases of ideation, prototyping, andevaluation. Also, a Robot Operating System (ROS) integration enables thecontrolling of a real robotic arm in a PhysicalTwin approach without anysimulation-reality gap. Here, we review the capabilities and limitations ofAdaptiX in detail and present three bodies of research based on the framework.AdaptiX can be accessed at https://adaptix.robot-research.de.</description><author>Max Pascher, Felix Ferdinand Goldau, Kirill Kronhardt, Udo Frese, Jens Gerken</author><pubDate>Fri, 17 May 2024 17:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15887v3</guid></item><item><title>Efficient Line Search Method Based on Regression and Uncertainty Quantification</title><link>http://arxiv.org/abs/2405.10897v1</link><description>Unconstrained optimization problems are typically solved using iterativemethods, which often depend on line search techniques to determine optimal steplengths in each iteration. This paper introduces a novel line search approach.Traditional line search methods, aimed at determining optimal step lengths,often discard valuable data from the search process and focus on refining steplength intervals. This paper proposes a more efficient method using Bayesianoptimization, which utilizes all available data points, i.e., function valuesand gradients, to guide the search towards a potential global minimum. This newapproach more effectively explores the search space, leading to better solutionquality. It is also easy to implement and integrate into existing frameworks.Tested on the challenging CUTEst test set, it demonstrates superior performancecompared to existing state-of-the-art methods, solving more problems tooptimality with equivalent resource usage.</description><author>Sören Laue, Tomislav Prusina</author><pubDate>Fri, 17 May 2024 17:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10897v1</guid></item><item><title>COGNET-MD, an evaluation framework and dataset for Large Language Model benchmarks in the medical domain</title><link>http://arxiv.org/abs/2405.10893v1</link><description>Large Language Models (LLMs) constitute a breakthrough state-of-the-artArtificial Intelligence (AI) technology which is rapidly evolving and promisesto aid in medical diagnosis either by assisting doctors or by simulating adoctor's workflow in more advanced and complex implementations. In thistechnical paper, we outline Cognitive Network Evaluation Toolkit for MedicalDomains (COGNET-MD), which constitutes a novel benchmark for LLM evaluation inthe medical domain. Specifically, we propose a scoring-framework with increaseddifficulty to assess the ability of LLMs in interpreting medical text. Theproposed framework is accompanied with a database of Multiple Choice Quizzes(MCQs). To ensure alignment with current medical trends and enhance safety,usefulness, and applicability, these MCQs have been constructed incollaboration with several associated medical experts in various medicaldomains and are characterized by varying degrees of difficulty. The current(first) version of the database includes the medical domains of Psychiatry,Dentistry, Pulmonology, Dermatology and Endocrinology, but it will becontinuously extended and expanded to include additional medical domains.</description><author>Dimitrios P. Panagoulias, Persephone Papatheodosiou, Anastasios P. Palamidas, Mattheos Sanoudos, Evridiki Tsoureli-Nikita, Maria Virvou, George A. Tsihrintzis</author><pubDate>Fri, 17 May 2024 17:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10893v1</guid></item><item><title>A Versatile Framework for Analyzing Galaxy Image Data by Implanting Human-in-the-loop on a Large Vision Model</title><link>http://arxiv.org/abs/2405.10890v1</link><description>The exponential growth of astronomical datasets provides an unprecedentedopportunity for humans to gain insight into the Universe. However, effectivelyanalyzing this vast amount of data poses a significant challenge. Astronomersare turning to deep learning techniques to address this, but the methods arelimited by their specific training sets, leading to considerable duplicateworkloads too. Hence, as an example to present how to overcome the issue, webuilt a framework for general analysis of galaxy images, based on a largevision model (LVM) plus downstream tasks (DST), including galaxy morphologicalclassification, image restoration, object detection, parameter extraction, andmore. Considering the low signal-to-noise ratio of galaxy images and theimbalanced distribution of galaxy categories, we have incorporated aHuman-in-the-loop (HITL) module into our large vision model, which leverageshuman knowledge to enhance the reliability and interpretability of processinggalaxy images interactively. The proposed framework exhibits notable few-shotlearning capabilities and versatile adaptability to all the abovementionedtasks on galaxy images in the DESI legacy imaging surveys. Expressly, forobject detection, trained by 1000 data points, our DST upon the LVM achieves anaccuracy of 96.7%, while ResNet50 plus Mask R-CNN gives an accuracy of 93.1%;for morphology classification, to obtain AUC ~0.9, LVM plus DST and HITL onlyrequests 1/50 training sets compared to ResNet18. Expectedly, multimodal datacan be integrated similarly, which opens up possibilities for conducting jointanalyses with datasets spanning diverse domains in the era of multi-messageastronomy.</description><author>Mingxiang Fu, Yu Song, Jiameng Lv, Liang Cao, Peng Jia, Nan Li, Xiangru Li, Jifeng Liu, A-Li Luo, Bo Qiu, Shiyin Shen, Liangping Tu, Lili Wang, Shoulin Wei, Haifeng Yang, Zhenping Yi, Zhiqiang Zou</author><pubDate>Fri, 17 May 2024 17:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10890v1</guid></item><item><title>Efficient Learning of Accurate Surrogates for Simulations of Complex Systems</title><link>http://arxiv.org/abs/2207.12855v3</link><description>Machine learning methods are increasingly used to build computationallyinexpensive surrogates for complex physical models. The predictive capabilityof these surrogates suffers when data are noisy, sparse, or time-dependent. Aswe are interested in finding a surrogate that provides valid predictions of anypotential future model evaluations, we introduce an online learning methodempowered by optimizer-driven sampling. The method has two advantages overcurrent approaches. First, it ensures that all turning points on the modelresponse surface are included in the training data. Second, after any new modelevaluations, surrogates are tested and "retrained" (updated) if the "score"drops below a validity threshold. Tests on benchmark functions reveal thatoptimizer-directed sampling generally outperforms traditional sampling methodsin terms of accuracy around local extrema, even when the scoring metric favorsoverall accuracy. We apply our method to simulations of nuclear matter todemonstrate that highly accurate surrogates for the nuclear equation of statecan be reliably auto-generated from expensive calculations using a few modelevaluations.</description><author>A. Diaw, M. McKerns, I. Sagert, L. G. Stanton, M. S. Murillo</author><pubDate>Fri, 17 May 2024 17:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.12855v3</guid></item><item><title>FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth Estimation</title><link>http://arxiv.org/abs/2405.10885v1</link><description>Most existing methods often rely on complex models to predict scene depthwith high accuracy, resulting in slow inference that is not conducive todeployment. To better balance precision and speed, we first designed SmallDepthbased on sparsity. Second, to enhance the feature representation ability ofSmallDepth during training under the condition of equal complexity duringinference, we propose an equivalent transformation module(ETM). Third, toimprove the ability of each layer in the case of a fixed SmallDepth to perceivedifferent context information and improve the robustness of SmallDepth to theleft-right direction and illumination changes, we propose pyramid loss. Fourth,to further improve the accuracy of SmallDepth, we utilized the proposedfunction approximation loss (APX) to transfer knowledge in the pretrainedHQDecv2, obtained by optimizing the previous HQDec to address grid artifacts insome regions, to SmallDepth. Extensive experiments demonstrate that eachproposed component improves the precision of SmallDepth without changing thecomplexity of SmallDepth during inference, and the developed approach achievesstate-of-the-art results on KITTI at an inference speed of more than 500 framesper second and with approximately 2 M parameters. The code and models will bepublicly available at https://github.com/fwucas/FA-Depth.</description><author>Fei Wang, Jun Cheng</author><pubDate>Fri, 17 May 2024 17:22:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10885v1</guid></item><item><title>Application of Artificial Intelligence in Schizophrenia Rehabilitation Management: Systematic Literature Review</title><link>http://arxiv.org/abs/2405.10883v1</link><description>This review aims to systematically assess the current status and prospects ofartificial intelligence (AI) in the rehabilitation management of patients withschizophrenia and their impact on the rehabilitation process. We selected 70studies from 2012 to the present, focusing on application, technologycategories, products, and data types of machine learning, deep learning,reinforcement learning, and other technologies in mental health interventionsand management. The results indicate that AI can be widely used in symptommonitoring, relapse risk prediction, and rehabilitation treatment by analyzingecological momentary assessment, behavioral, and speech data. This reviewfurther explores the potential challenges and future directions of emergingproducts, technologies, and analytical methods based on AI, such as socialmedia analysis, serious games, and large language models in rehabilitation. Insummary, this study systematically reviews the application status of AI inschizophrenia rehabilitation management and provides valuable insights andrecommendations for future research paths.</description><author>Hongyi Yang, Fangyuan Chang, Dian Zhu, Muroi Fumie, Zhao Liu</author><pubDate>Fri, 17 May 2024 17:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10883v1</guid></item><item><title>One registration is worth two segmentations</title><link>http://arxiv.org/abs/2405.10879v1</link><description>The goal of image registration is to establish spatial correspondence betweentwo or more images, traditionally through dense displacement fields (DDFs) orparametric transformations (e.g., rigid, affine, and splines). Rethinking theexisting paradigms of achieving alignment via spatial transformations, weuncover an alternative but more intuitive correspondence representation: a setof corresponding regions-of-interest (ROI) pairs, which we demonstrate to havesufficient representational capability as other correspondence representationmethods.Further, it is neither necessary nor sufficient for these ROIs to holdspecific anatomical or semantic significance. In turn, we formulate imageregistration as searching for the same set of corresponding ROIs from bothmoving and fixed images - in other words, two multi-class segmentation tasks ona pair of images. For a general-purpose and practical implementation, weintegrate the segment anything model (SAM) into our proposed algorithms,resulting in a SAM-enabled registration (SAMReg) that does not require anytraining data, gradient-based fine-tuning or engineered prompts. Weexperimentally show that the proposed SAMReg is capable of segmenting andmatching multiple ROI pairs, which establish sufficiently accuratecorrespondences, in three clinical applications of registering prostate MR,cardiac MR and abdominal CT images. Based on metrics including Dice and targetregistration errors on anatomical structures, the proposed registrationoutperforms both intensity-based iterative algorithms and DDF-predictinglearning-based networks, even yielding competitive performance withweakly-supervised registration which requires fully-segmented training data.</description><author>Shiqi Huang, Tingfa Xu, Ziyi Shen, Shaheer Ullah Saeed, Wen Yan, Dean Barratt, Yipeng Hu</author><pubDate>Fri, 17 May 2024 17:14:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10879v1</guid></item><item><title>Towards gaze-independent c-VEP BCI: A pilot study</title><link>http://arxiv.org/abs/2404.00031v2</link><description>A limitation of brain-computer interface (BCI) spellers is that they requirethe user to be able to move the eyes to fixate on targets. This poses an issuefor users who cannot voluntarily control their eye movements, for instance,people living with late-stage amyotrophic lateral sclerosis (ALS). This pilotstudy makes the first step towards a gaze-independent speller based on thecode-modulated visual evoked potential (c-VEP). Participants were presentedwith two bi-laterally located stimuli, one of which was flashing, and weretasked to attend to one of these stimuli either by directly looking at thestimuli (overt condition) or by using spatial attention, eliminating the needfor eye movement (covert condition). The attended stimuli were decoded fromelectroencephalography (EEG) and classification accuracies of 88% and 100% wereobtained for the covert and overt conditions, respectively. These fundamentalinsights show the promising feasibility of utilizing the c-VEP protocol forgaze-independent BCIs that use covert spatial attention when both stimuli flashsimultaneously.</description><author>S. Narayanan, S. Ahmadi, P. Desain, J. Thielen</author><pubDate>Fri, 17 May 2024 17:12:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00031v2</guid></item><item><title>WEITS: A Wavelet-enhanced residual framework for interpretable time series forecasting</title><link>http://arxiv.org/abs/2405.10877v1</link><description>Time series (TS) forecasting has been an unprecedentedly popular problem inrecent years, with ubiquitous applications in both scientific and businessfields. Various approaches have been introduced to time series analysis,including both statistical approaches and deep neural networks. Although neuralnetwork approaches have illustrated stronger ability of representation thanstatistical methods, they struggle to provide sufficient interpretablility, andcan be too complicated to optimize. In this paper, we present WEITS, afrequency-aware deep learning framework that is highly interpretable andcomputationally efficient. Through multi-level wavelet decomposition, WEITSnovelly infuses frequency analysis into a highly deep learning framework.Combined with a forward-backward residual architecture, it enjoys both highrepresentation capability and statistical interpretability. Extensiveexperiments on real-world datasets have demonstrated competitive performance ofour model, along with its additional advantage of high computation efficiency.Furthermore, WEITS provides a general framework that can always seamlesslyintegrate with state-of-the-art approaches for time series forecast.</description><author>Ziyou Guo, Yan Sun, Tieru Wu</author><pubDate>Fri, 17 May 2024 17:09:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10877v1</guid></item><item><title>Recursively Feasible Shrinking-Horizon MPC in Dynamic Environments with Conformal Prediction Guarantees</title><link>http://arxiv.org/abs/2405.10875v1</link><description>In this paper, we focus on the problem of shrinking-horizon Model PredictiveControl (MPC) in uncertain dynamic environments. We consider controlling adeterministic autonomous system that interacts with uncontrollable stochasticagents during its mission. Employing tools from conformal prediction, existingworks derive high-confidence prediction regions for the unknown agenttrajectories, and integrate these regions in the design of suitable safetyconstraints for MPC. Despite guaranteeing probabilistic safety of theclosed-loop trajectories, these constraints do not ensure feasibility of therespective MPC schemes for the entire duration of the mission. We propose ashrinking-horizon MPC that guarantees recursive feasibility via a gradualrelaxation of the safety constraints as new prediction regions become availableonline. This relaxation enforces the safety constraints to hold over the leastrestrictive prediction region from the set of all available prediction regions.In a comparative case study with the state of the art, we empirically show thatour approach results in tighter prediction regions and verify recursivefeasibility of our MPC scheme.</description><author>Charis Stamouli, Lars Lindemann, George J. Pappas</author><pubDate>Fri, 17 May 2024 17:07:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10875v1</guid></item><item><title>Variational Mode Decomposition-Based Nonstationary Coherent Structure Analysis for Spatiotemporal Data</title><link>http://arxiv.org/abs/2312.12113v2</link><description>The conventional modal analysis techniques face difficulties in handlingnonstationary phenomena, such as transient, nonperiodic, or intermittentphenomena. This paper presents a variational mode decomposition--basednonstationary coherent structure (VMD-NCS) analysis that enables the extractionand analysis of coherent structures in the case of nonstationary phenomena fromhigh-dimensional spatiotemporal data. The VMD-NCS analysis decomposes the inputspatiotemporal data into intrinsic coherent structures (ICSs) that representnonstationary spatiotemporal patterns and exhibit coherence in both spatial andtemporal directions. Unlike many conventional modal analysis techniques, theproposed method accounts for the temporal changes in the spatial distributionwith time. Tthe VMD-NCS analysis was validated based on the transient growthphenomena in the flow around a cylinder. It was confirmed that the temporalchanges in the spatial distribution, depicting the transient growth of vortexshedding where fluctuations arising in the far-wake region gradually approachthe near-wake region, were represented as a single ICS. Furthermore, in theanalysis of the quasi-periodic flow field around a pitching airfoil, thetemporal changes in the spatial distribution and the amplitude of vortexshedding behind the airfoil, influenced by the pitching motion of the airfoil,were captured as a single ICS. The impact of two parameters that control thenumber of ICSs ($K$) and the penalty factor related to the temporal coherence($\alpha$), was investigated. The results revealed that $K$ has a significantimpact on the VMD-NCS analysis results. In the case of a relatively high $K$,the VMD-NCS analysis tends to extract more periodic spatiotemporal patternsresembling the results of dynamic mode decomposition. In the case of a small$K$, it tends to extract more nonstationary spatiotemporal patterns.</description><author>Yuya Ohmichi</author><pubDate>Fri, 17 May 2024 17:03:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12113v2</guid></item><item><title>PREGO: online mistake detection in PRocedural EGOcentric videos</title><link>http://arxiv.org/abs/2404.01933v2</link><description>Promptly identifying procedural errors from egocentric videos in an onlinesetting is highly challenging and valuable for detecting mistakes as soon asthey happen. This capability has a wide range of applications across variousfields, such as manufacturing and healthcare. The nature of procedural mistakesis open-set since novel types of failures might occur, which calls forone-class classifiers trained on correctly executed procedures. However, notechnique can currently detect open-set procedural mistakes online. We proposePREGO, the first online one-class classification model for mistake detection inPRocedural EGOcentric videos. PREGO is based on an online action recognitioncomponent to model the current action, and a symbolic reasoning module topredict the next actions. Mistake detection is performed by comparing therecognized current action with the expected future one. We evaluate PREGO ontwo procedural egocentric video datasets, Assembly101 and Epic-tent, which weadapt for online benchmarking of procedural mistake detection to establishsuitable benchmarks, thus defining the Assembly101-O and Epic-tent-O datasets,respectively.</description><author>Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso</author><pubDate>Fri, 17 May 2024 17:03:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01933v2</guid></item><item><title>BraTS-Path Challenge: Assessing Heterogeneous Histopathologic Brain Tumor Sub-regions</title><link>http://arxiv.org/abs/2405.10871v1</link><description>Glioblastoma is the most common primary adult brain tumor, with a grimprognosis - median survival of 12-18 months following treatment, and 4 monthsotherwise. Glioblastoma is widely infiltrative in the cerebral hemispheres andwell-defined by heterogeneous molecular and micro-environmental histopathologicprofiles, which pose a major obstacle in treatment. Correctly diagnosing thesetumors and assessing their heterogeneity is crucial for choosing the precisetreatment and potentially enhancing patient survival rates. In thegold-standard histopathology-based approach to tumor diagnosis, detectingvarious morpho-pathological features of distinct histology throughout digitizedtissue sections is crucial. Such "features" include the presence of cellulartumor, geographic necrosis, pseudopalisading necrosis, areas abundant inmicrovascular proliferation, infiltration into the cortex, wide extension insubcortical white matter, leptomeningeal infiltration, regions dense withmacrophages, and the presence of perivascular or scattered lymphocytes. Withthese features in mind and building upon the main aim of the BraTS Cluster ofChallenges https://www.synapse.org/brats2024, the goal of the BraTS-Pathchallenge is to provide a systematically prepared comprehensive dataset and abenchmarking environment to develop and fairly compare deep-learning modelscapable of identifying tumor sub-regions of distinct histologic profile. Thesemodels aim to further our understanding of the disease and assist in thediagnosis and grading of conditions in a consistent manner.</description><author>Spyridon Bakas, Siddhesh P. Thakur, Shahriar Faghani, Mana Moassefi, Ujjwal Baid, Verena Chung, Sarthak Pati, Shubham Innani, Bhakti Baheti, Jake Albrecht, Alexandros Karargyris, Hasan Kassem, MacLean P. Nasrallah, Jared T. Ahrendsen, Valeria Barresi, Maria A. Gubbiotti, Giselle Y. López, Calixto-Hope G. Lucas, Michael L. Miller, Lee A. D. Cooper, Jason T. Huse, William R. Bell</author><pubDate>Fri, 17 May 2024 17:02:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10871v1</guid></item><item><title>Attention-Driven Multi-Agent Reinforcement Learning: Enhancing Decisions with Expertise-Informed Tasks</title><link>http://arxiv.org/abs/2404.05840v3</link><description>In this paper, we introduce an alternative approach to enhancing Multi-AgentReinforcement Learning (MARL) through the integration of domain knowledge andattention-based policy mechanisms. Our methodology focuses on the incorporationof domain-specific expertise into the learning process, which simplifies thedevelopment of collaborative behaviors. This approach aims to reduce thecomplexity and learning overhead typically associated with MARL by enablingagents to concentrate on essential aspects of complex tasks, thus optimizingthe learning curve. The utilization of attention mechanisms plays a key role inour model. It allows for the effective processing of dynamic context data andnuanced agent interactions, leading to more refined decision-making. Applied instandard MARL scenarios, such as the Stanford Intelligent Systems Laboratory(SISL) Pursuit and Multi-Particle Environments (MPE) Simple Spread, our methodhas been shown to improve both learning efficiency and the effectiveness ofcollaborative behaviors. The results indicate that our attention-based approachcan be a viable approach for improving the efficiency of MARL training process,integrating domain-specific knowledge at the action level.</description><author>Andre R Kuroswiski, Annie S Wu, Angelo Passaro</author><pubDate>Fri, 17 May 2024 17:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05840v3</guid></item><item><title>Multicenter Privacy-Preserving Model Training for Deep Learning Brain Metastases Autosegmentation</title><link>http://arxiv.org/abs/2405.10870v1</link><description>Objectives: This work aims to explore the impact of multicenter dataheterogeneity on deep learning brain metastases (BM) autosegmentationperformance, and assess the efficacy of an incremental transfer learningtechnique, namely learning without forgetting (LWF), to improve modelgeneralizability without sharing raw data. Materials and methods: A total of six BM datasets from University HospitalErlangen (UKER), University Hospital Zurich (USZ), Stanford, UCSF, NYU andBraTS Challenge 2023 on BM segmentation were used for this evaluation. First,the multicenter performance of a convolutional neural network (DeepMedic) forBM autosegmentation was established for exclusive single-center training andfor training on pooled data, respectively. Subsequently bilateral collaborationwas evaluated, where a UKER pretrained model is shared to another center forfurther training using transfer learning (TL) either with or without LWF. Results: For single-center training, average F1 scores of BM detection rangefrom 0.625 (NYU) to 0.876 (UKER) on respective single-center test data. Mixedmulticenter training notably improves F1 scores at Stanford and NYU, withnegligible improvement at other centers. When the UKER pretrained model isapplied to USZ, LWF achieves a higher average F1 score (0.839) than naive TL(0.570) and single-center training (0.688) on combined UKER and USZ test data.Naive TL improves sensitivity and contouring accuracy, but compromisesprecision. Conversely, LWF demonstrates commendable sensitivity, precision andcontouring accuracy. When applied to Stanford, similar performance wasobserved. Conclusion: Data heterogeneity results in varying performance in BMautosegmentation, posing challenges to model generalizability. LWF is apromising approach to peer-to-peer privacy-preserving model training.</description><author>Yixing Huang, Zahra Khodabakhshi, Ahmed Gomaa, Manuel Schmidt, Rainer Fietkau, Matthias Guckenberger, Nicolaus Andratschke, Christoph Bert, Stephanie Tanadini-Lang, Florian Putz</author><pubDate>Fri, 17 May 2024 17:01:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10870v1</guid></item><item><title>Air Signing and Privacy-Preserving Signature Verification for Digital Documents</title><link>http://arxiv.org/abs/2405.10868v1</link><description>This paper presents a novel approach to the digital signing of electronicdocuments through the use of a camera-based interaction system, single-fingertracking for sign recognition, and multi commands executing hand gestures. Theproposed solution, referred to as "Air Signature," involves writing thesignature in front of the camera, rather than relying on traditional methodssuch as mouse drawing or physically signing on paper and showing it to a webcamera. The goal is to develop a state-of-the-art method for detecting andtracking gestures and objects in real-time. The proposed methods includeapplying existing gesture recognition and object tracking systems, improvingaccuracy through smoothing and line drawing, and maintaining continuity duringfast finger movements. An evaluation of the fingertip detection, sketching, andoverall signing process is performed to assess the effectiveness of theproposed solution. The secondary objective of this research is to develop amodel that can effectively recognize the unique signature of a user. This typeof signature can be verified by neural cores that analyze the movement, speed,and stroke pixels of the signing in real time. The neural cores use machinelearning algorithms to match air signatures to the individual's storedsignatures, providing a secure and efficient method of verification. Ourproposed System does not require sensors or any hardware other than the camera.</description><author>P. Sarveswarasarma, T. Sathulakjan, V. J. V. Godfrey, Thanuja D. Ambegoda</author><pubDate>Fri, 17 May 2024 17:00:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10868v1</guid></item><item><title>Improving face generation quality and prompt following with synthetic captions</title><link>http://arxiv.org/abs/2405.10864v1</link><description>Recent advancements in text-to-image generation using diffusion models havesignificantly improved the quality of generated images and expanded the abilityto depict a wide range of objects. However, ensuring that these models adhereclosely to the text prompts remains a considerable challenge. This issue isparticularly pronounced when trying to generate photorealistic images ofhumans. Without significant prompt engineering efforts models often produceunrealistic images and typically fail to incorporate the full extent of theprompt information. This limitation can be largely attributed to the nature ofcaptions accompanying the images used in training large scale diffusion models,which typically prioritize contextual information over details related to theperson's appearance. In this paper we address this issue by introducing atraining-free pipeline designed to generate accurate appearance descriptionsfrom images of people. We apply this method to create approximately 250,000captions for publicly available face datasets. We then use these syntheticcaptions to fine-tune a text-to-image diffusion model. Our results demonstratethat this approach significantly improves the model's ability to generatehigh-quality, realistic human faces and enhances adherence to the givenprompts, compared to the baseline model. We share our synthetic captions,pretrained checkpoints and training code.</description><author>Michail Tarasiou, Stylianos Moschoglou, Jiankang Deng, Stefanos Zafeiriou</author><pubDate>Fri, 17 May 2024 16:50:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10864v1</guid></item><item><title>Tailoring Vaccine Messaging with Common-Ground Opinions</title><link>http://arxiv.org/abs/2405.10861v1</link><description>One way to personalize chatbot interactions is by establishing common groundwith the intended reader. A domain where establishing mutual understandingcould be particularly impactful is vaccine concerns and misinformation. Vaccineinterventions are forms of messaging which aim to answer concerns expressedabout vaccination. Tailoring responses in this domain is difficult, sinceopinions often have seemingly little ideological overlap. We define the task oftailoring vaccine interventions to a Common-Ground Opinion (CGO). Tailoringresponses to a CGO involves meaningfully improving the answer by relating it toan opinion or belief the reader holds. In this paper we introduce TAILOR-CGO, adataset for evaluating how well responses are tailored to provided CGOs. Webenchmark several major LLMs on this task; finding GPT-4-Turbo performssignificantly better than others. We also build automatic evaluation metrics,including an efficient and accurate BERT model that outperforms finetuned LLMs,investigate how to successfully tailor vaccine messaging to CGOs, and provideactionable recommendations from this investigation. Code and model weights: https://github.com/rickardstureborg/tailor-cgoDataset: https://huggingface.co/datasets/DukeNLP/tailor-cgo</description><author>Rickard Stureborg, Sanxing Chen, Ruoyu Xie, Aayushi Patel, Christopher Li, Chloe Qinyu Zhu, Tingnan Hu, Jun Yang, Bhuwan Dhingra</author><pubDate>Fri, 17 May 2024 16:48:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10861v1</guid></item><item><title>Exploiting Style Latent Flows for Generalizing Deepfake Video Detection</title><link>http://arxiv.org/abs/2403.06592v2</link><description>This paper presents a new approach for the detection of fake videos, based onthe analysis of style latent vectors and their abnormal behavior in temporalchanges in the generated videos. We discovered that the generated facial videossuffer from the temporal distinctiveness in the temporal changes of stylelatent vectors, which are inevitable during the generation of temporally stablevideos with various facial expressions and geometric transformations. Ourframework utilizes the StyleGRU module, trained by contrastive learning, torepresent the dynamic properties of style latent vectors. Additionally, weintroduce a style attention module that integrates StyleGRU-generated featureswith content-based features, enabling the detection of visual and temporalartifacts. We demonstrate our approach across various benchmark scenarios indeepfake detection, showing its superiority in cross-dataset andcross-manipulation scenarios. Through further analysis, we also validate theimportance of using temporal changes of style latent vectors to improve thegenerality of deepfake video detection.</description><author>Jongwook Choi, Taehoon Kim, Yonghyun Jeong, Seungryul Baek, Jongwon Choi</author><pubDate>Fri, 17 May 2024 16:46:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06592v2</guid></item><item><title>ECR-Chain: Advancing Generative Language Models to Better Emotion-Cause Reasoners through Reasoning Chains</title><link>http://arxiv.org/abs/2405.10860v1</link><description>Understanding the process of emotion generation is crucial for analyzing thecauses behind emotions. Causal Emotion Entailment (CEE), anemotion-understanding task, aims to identify the causal utterances in aconversation that stimulate the emotions expressed in a target utterance.However, current works in CEE mainly focus on modeling semantic and emotionalinteractions in conversations, neglecting the exploration of theemotion-generation process. This hinders the models from deeply understandingemotions, restricting their ability to produce explainable predictions. In thiswork, inspired by the emotion generation process of"stimulus-appraisal-emotion" in the cognitive appraisal theory, we introduce astep-by-step reasoning method, Emotion-Cause Reasoning Chain (ECR-Chain), toinfer the stimulus from the target emotional expressions in conversations.Specifically, we first introduce the ECR-Chain to ChatGPT via few-shotprompting, which significantly improves its performance on the CEE task. Wefurther propose an automated construction process to utilize ChatGPT inbuilding an ECR-Chain set, which can enhance the reasoning abilities of smallermodels through supervised training and assist the Vicuna-7B model in achievingstate-of-the-art CEE performance. Moreover, our methods can enable thesegenerative language models to effectively perform emotion-cause reasoning in anexplainable manner. Our code, data and more details are athttps://github.com/hzp3517/ECR-Chain.</description><author>Zhaopei Huang, Jinming Zhao, Qin Jin</author><pubDate>Fri, 17 May 2024 16:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10860v1</guid></item><item><title>REB: Reducing Biases in Representation for Industrial Anomaly Detection</title><link>http://arxiv.org/abs/2308.12577v2</link><description>Existing representation-based methods usually conduct industrial anomalydetection in two stages: obtain feature representations with a pre-trainedmodel and perform distance measures for anomaly detection. Among them,K-nearest neighbor (KNN) retrieval-based anomaly detection methods showpromising results. However, the features are not fully exploited as thesemethods ignore domain bias of pre-trained models and the difference of localdensity in feature space, which limits the detection performance. In thispaper, we propose Reducing Biases (REB) in representation by considering thedomain bias and building a self-supervised learning task for better domainadaption with a defect generation strategy (DefectMaker) that ensures a strongdiversity in the synthetic defects. Additionally, we propose a local-densityKNN (LDKNN) to reduce the local density bias in the feature space and obtaineffective anomaly detection. The proposed REB method achieves a promisingresult of 99.5\% Im.AUROC on the widely used MVTec AD, with smaller backbonenetworks such as Vgg11 and Resnet18. The method also achieves an impressive88.8\% Im.AUROC on the MVTec LOCO AD dataset and a remarkable 96.0\% on theBTAD dataset, outperforming other representation-based approaches. Theseresults indicate the effectiveness and efficiency of REB for practicalindustrial applications. Code:https://github.com/ShuaiLYU/REB.</description><author>Shuai Lyu, Dongmei Mo, Waikeung Wong</author><pubDate>Fri, 17 May 2024 16:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12577v2</guid></item><item><title>Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features</title><link>http://arxiv.org/abs/2402.02969v2</link><description>Understanding the reasons behind the exceptional success of transformersrequires a better analysis of why attention layers are suitable for NLP tasks.In particular, such tasks require predictive models to capture contextualmeaning which often depends on one or few words, even if the sentence is long.Our work studies this key property, dubbed word sensitivity (WS), in theprototypical setting of random features. We show that attention layers enjoyhigh WS, namely, there exists a vector in the space of embeddings that largelyperturbs the random attention features map. The argument critically exploitsthe role of the softmax in the attention layer, highlighting its benefitcompared to other activations (e.g., ReLU). In contrast, the WS of standardrandom features is of order $1/\sqrt{n}$, $n$ being the number of words in thetextual sample, and thus it decays with the length of the context. We thentranslate these results on the word sensitivity into generalization bounds: dueto their low WS, random features provably cannot learn to distinguish betweentwo sentences that differ only in a single word; in contrast, due to their highWS, random attention features have higher generalization capabilities. Wevalidate our theoretical results with experimental evidence over the BERT-Baseword embeddings of the imdb review dataset.</description><author>Simone Bombari, Marco Mondelli</author><pubDate>Fri, 17 May 2024 16:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02969v2</guid></item><item><title>The Future of Large Language Model Pre-training is Federated</title><link>http://arxiv.org/abs/2405.10853v1</link><description>Generative pre-trained large language models (LLMs) have demonstratedimpressive performance over a wide range of tasks, thanks to the unprecedentedamount of data they have been trained on. As established scaling laws indicate,LLMs' future performance improvement depends on the amount of computing anddata sources we can leverage for pre-training. Federated learning (FL) has thepotential to unleash the majority of the planet's data and computationalresources, which are underutilized by the data-center-focused trainingmethodology of current LLM practice. Our work presents a robust, flexible,reproducible FL approach that enables large-scale collaboration acrossinstitutions to train LLMs. This would mobilize more computational and dataresources while matching or potentially exceeding centralized performance. Wefurther show the effectiveness of the federated training scales with model sizeand present our approach for training a billion-scale federated LLM usinglimited resources. This will help data-rich actors to become the protagonistsof LLMs pre-training instead of leaving the stage to compute-rich actors alone.</description><author>Lorenzo Sani, Alex Iacob, Zeyu Cao, Bill Marino, Yan Gao, Tomas Paulik, Wanru Zhao, William F. Shen, Preslav Aleksandrov, Xinchi Qiu, Nicholas D. Lane</author><pubDate>Fri, 17 May 2024 16:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10853v1</guid></item><item><title>KernelSHAP-IQ: Weighted Least-Square Optimization for Shapley Interactions</title><link>http://arxiv.org/abs/2405.10852v1</link><description>The Shapley value (SV) is a prevalent approach of allocating credit tomachine learning (ML) entities to understand black box ML models. Enrichingsuch interpretations with higher-order interactions is inevitable for complexsystems, where the Shapley Interaction Index (SII) is a direct axiomaticextension of the SV. While it is well-known that the SV yields an optimalapproximation of any game via a weighted least square (WLS) objective, anextension of this result to SII has been a long-standing open problem, whicheven led to the proposal of an alternative index. In this work, we characterizehigher-order SII as a solution to a WLS problem, which constructs an optimalapproximation via SII and $k$-Shapley values ($k$-SII). We prove thisrepresentation for the SV and pairwise SII and give empirically validatedconjectures for higher orders. As a result, we propose KernelSHAP-IQ, a directextension of KernelSHAP for SII, and demonstrate state-of-the-art performancefor feature interactions.</description><author>Fabian Fumagalli, Maximilian Muschalik, Patrick Kolpaczki, Eyke Hüllermeier, Barbara Hammer</author><pubDate>Fri, 17 May 2024 16:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10852v1</guid></item><item><title>RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset for Natural Disaster Damage Assessment</title><link>http://arxiv.org/abs/2202.12361v4</link><description>Recent advancements in computer vision and deep learning techniques havefacilitated notable progress in scene understanding, thereby assisting rescueteams in achieving precise damage assessment. In this paper, we presentRescueNet, a meticulously curated high-resolution post-disaster dataset thatincludes detailed classification and semantic segmentation annotations. Thisdataset aims to facilitate comprehensive scene understanding in the aftermathof natural disasters. RescueNet comprises post-disaster images collected afterHurricane Michael, obtained using Unmanned Aerial Vehicles (UAVs) from multipleimpacted regions. The uniqueness of RescueNet lies in its provision ofhigh-resolution post-disaster imagery, accompanied by comprehensive annotationsfor each image. Unlike existing datasets that offer annotations limited tospecific scene elements such as buildings, RescueNet provides pixel-levelannotations for all classes, including buildings, roads, pools, trees, andmore. Furthermore, we evaluate the utility of the dataset by implementingstate-of-the-art segmentation models on RescueNet, demonstrating its value inenhancing existing methodologies for natural disaster damage assessment.</description><author>Maryam Rahnemoonfar, Tashnim Chowdhury, Robin Murphy</author><pubDate>Fri, 17 May 2024 16:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.12361v4</guid></item><item><title>Picking watermarks from noise (PWFN): an improved robust watermarking model against intensive distortions</title><link>http://arxiv.org/abs/2405.05170v2</link><description>Digital watermarking is the process of embedding secret information byaltering images in an undetectable way to the human eye. To increase therobustness of the model, many deep learning-based watermarking methods use theencoder-noise-decoder architecture by adding different noises to the noiselayer. The decoder then extracts the watermarked information from the distortedimage. However, this method can only resist weak noise attacks. To improve therobustness of the decoder against stronger noise, this paper proposes tointroduce a denoise module between the noise layer and the decoder. The moduleaims to reduce noise and recover some of the information lost caused bydistortion. Additionally, the paper introduces the SE module to fuse thewatermarking information pixel-wise and channel dimensions-wise, improving theencoder's efficiency. Experimental results show that our proposed method iscomparable to existing models and outperforms state-of-the-art under differentnoise intensities. In addition, ablation experiments show the superiority ofour proposed module.</description><author>Sijing Xie, Chengxin Zhao, Nan Sun, Wei Li, Hefei Ling</author><pubDate>Fri, 17 May 2024 16:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05170v2</guid></item><item><title>FOLIO: Natural Language Reasoning with First-Order Logic</title><link>http://arxiv.org/abs/2209.00840v2</link><description>Large language models (LLMs) have achieved remarkable performance on avariety of natural language understanding tasks. However, existing benchmarksare inadequate in measuring the complex logical reasoning capabilities of amodel. We present FOLIO, a human-annotated, logically complex and diversedataset for reasoning in natural language (NL), equipped with first-order logic(FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), eachpaired with one of 487 sets of premises used to deductively reason for thevalidity of each conclusion. The logical correctness of the premises andconclusions is ensured by their FOL annotations, which are automaticallyverified by an FOL inference engine. In addition to the main NL reasoning task,NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Ourexperiments on FOLIO systematically evaluate the FOL reasoning ability ofsupervised fine-tuning on medium-sized language models. For both NL reasoningand NL-FOL translation, we benchmark multiple state-of-the-art language models.Our results show that a subset of FOLIO presents a challenge for one of themost capable {Large Language Model (LLM)} publicly available, GPT-4.</description><author>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alex Wardle-Solano, Hannah Szabo, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander R. Fabbri, Wojciech Kryscinski, Semih Yavuz, Ye Liu, Xi Victoria Lin, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Rex Ying, Arman Cohan, Dragomir Radev</author><pubDate>Fri, 17 May 2024 16:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.00840v2</guid></item><item><title>Automated Radiology Report Generation: A Review of Recent Advances</title><link>http://arxiv.org/abs/2405.10842v1</link><description>Increasing demands on medical imaging departments are taking a toll on theradiologist's ability to deliver timely and accurate reports. Recenttechnological advances in artificial intelligence have demonstrated greatpotential for automatic radiology report generation (ARRG), sparking anexplosion of research. This survey paper conducts a methodological review ofcontemporary ARRG approaches by way of (i) assessing datasets based oncharacteristics, such as availability, size, and adoption rate, (ii) examiningdeep learning training methods, such as contrastive learning and reinforcementlearning, (iii) exploring state-of-the-art model architectures, includingvariations of CNN and transformer models, (iv) outlining techniques integratingclinical knowledge through multimodal inputs and knowledge graphs, and (v)scrutinising current model evaluation techniques, including commonly appliedNLP metrics and qualitative clinical reviews. Furthermore, the quantitativeresults of the reviewed models are analysed, where the top performing modelsare examined to seek further insights. Finally, potential new directions arehighlighted, with the adoption of additional datasets from other radiologicalmodalities and improved evaluation methods predicted as important areas offuture development.</description><author>Phillip Sloan, Philip Clatworthy, Edwin Simpson, Majid Mirmehdi</author><pubDate>Fri, 17 May 2024 16:06:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10842v1</guid></item><item><title>Model orthogonalization and Bayesian forecast mixing via Principal Component Analysis</title><link>http://arxiv.org/abs/2405.10839v1</link><description>One can improve predictability in the unknown domain by combining forecastsof imperfect complex computational models using a Bayesian statistical machinelearning framework. In many cases, however, the models used in the mixingprocess are similar. In addition to contaminating the model space, theexistence of such similar, or even redundant, models during the multimodelingprocess can result in misinterpretation of results and deterioration ofpredictive performance. In this work we describe a method based on thePrincipal Component Analysis that eliminates model redundancy. We show that byadding model orthogonalization to the proposed Bayesian Model Combinationframework, one can arrive at better prediction accuracy and reach excellentuncertainty quantification performance.</description><author>Pablo Giuliani, Kyle Godbey, Vojtech Kejzlar, Witold Nazarewicz</author><pubDate>Fri, 17 May 2024 16:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10839v1</guid></item><item><title>CG-HOI: Contact-Guided 3D Human-Object Interaction Generation</title><link>http://arxiv.org/abs/2311.16097v2</link><description>We propose CG-HOI, the first method to address the task of generating dynamic3D human-object interactions (HOIs) from text. We model the motion of bothhuman and object in an interdependent fashion, as semantically rich humanmotion rarely happens in isolation without any interactions. Our key insight isthat explicitly modeling contact between the human body surface and objectgeometry can be used as strong proxy guidance, both during training andinference. Using this guidance to bridge human and object motion enablesgenerating more realistic and physically plausible interaction sequences, wherethe human body and corresponding object move in a coherent manner. Our methodfirst learns to model human motion, object motion, and contact in a jointdiffusion process, inter-correlated through cross-attention. We then leveragethis learned contact for guidance during inference to synthesize realistic andcoherent HOIs. Extensive evaluation shows that our joint contact-basedhuman-object interaction approach generates realistic and physically plausiblesequences, and we show two applications highlighting the capabilities of ourmethod. Conditioned on a given object trajectory, we can generate thecorresponding human motion without re-training, demonstrating stronghuman-object interdependency learning. Our approach is also flexible, and canbe applied to static real-world 3D scene scans.</description><author>Christian Diller, Angela Dai</author><pubDate>Fri, 17 May 2024 16:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16097v2</guid></item><item><title>Anatomically aware dual-hop learning for pulmonary embolism detection in CT pulmonary angiograms</title><link>http://arxiv.org/abs/2303.17593v2</link><description>Pulmonary Embolisms (PE) represent a leading cause of cardiovascular death.While medical imaging, through computed tomographic pulmonary angiography(CTPA), represents the gold standard for PE diagnosis, it is still susceptibleto misdiagnosis or significant diagnosis delays, which may be fatal forcritical cases. Despite the recently demonstrated power of deep learning tobring a significant boost in performance in a wide range of medical imagingtasks, there are still very few published researches on automatic pulmonaryembolism detection. Herein we introduce a deep learning based approach, whichefficiently combines computer vision and deep neural networks for pulmonaryembolism detection in CTPA. Our method features novel improvements along threeorthogonal axes: 1) automatic detection of anatomical structures; 2) anatomicalaware pretraining, and 3) a dual-hop deep neural net for PE detection. Weobtain state-of-the-art results on the publicly available multicenterlarge-scale RSNA dataset.</description><author>Florin Condrea, Saikiran Rapaka, Lucian Itu, Puneet Sharma, Jonathan Sperl, A Mohamed Ali, Marius Leordeanu</author><pubDate>Fri, 17 May 2024 16:00:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17593v2</guid></item><item><title>FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations</title><link>http://arxiv.org/abs/2211.14309v3</link><description>We present a generative approach to forecast long-term future human behaviorin 3D, requiring only weak supervision from readily available 2D human actiondata. This is a fundamental task enabling many downstream applications. Therequired ground-truth data is hard to capture in 3D (mocap suits, expensivesetups) but easy to acquire in 2D (simple RGB cameras). Thus, we design ourmethod to only require 2D RGB data at inference time while being able togenerate 3D human motion sequences. We use a differentiable 2D projectionscheme in an autoregressive manner for weak supervision, and an adversarialloss for 3D regularization. Our method predicts long and complex human behaviorsequences (e.g., cooking, assembly) consisting of multiple sub-actions. Wetackle this in a semantically hierarchical manner, jointly predictinghigh-level coarse action labels together with their low-level fine-grainedrealizations as characteristic 3D human poses. We observe that these two actionrepresentations are coupled in nature, and joint prediction benefits bothaction and pose forecasting. Our experiments demonstrate the complementarynature of joint action and 3D pose prediction: our joint approach outperformseach task treated individually, enables robust longer-term sequence prediction,and improves over alternative approaches to forecast actions and characteristic3D poses.</description><author>Christian Diller, Thomas Funkhouser, Angela Dai</author><pubDate>Fri, 17 May 2024 15:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14309v3</guid></item><item><title>Automatic segmentation of Organs at Risk in Head and Neck cancer patients from CT and MRI scans</title><link>http://arxiv.org/abs/2405.10833v1</link><description>Background and purpose: Deep Learning (DL) has been widely explored forOrgans at Risk (OARs) segmentation; however, most studies have focused on asingle modality, either CT or MRI, not both simultaneously. This study presentsa high-performing DL pipeline for segmentation of 30 OARs from MRI and CT scansof Head and Neck (H&amp;N) cancer patients. Materials and methods: Paired CT and MRI-T1 images from 42 H&amp;N cancerpatients alongside annotation for 30 OARs from the H&amp;N OAR CT &amp; MR segmentationchallenge dataset were used to develop a segmentation pipeline. After croppingirrelevant regions, rigid followed by non-rigid registration of CT and MRIvolumes was performed. Two versions of the CT volume, representing soft tissuesand bone anatomy, were stacked with the MRI volume and used as input to annnU-Net pipeline. Modality Dropout was used during the training to force themodel to learn from the different modalities. Segmentation masks were predictedwith the trained model for an independent set of 14 new patients. The mean DiceScore (DS) and Hausdorff Distance (HD) were calculated for each OAR acrossthese patients to evaluate the pipeline. Results: This resulted in an overall mean DS and HD of 0.777 +- 0.118 and3.455 +- 1.679, respectively, establishing the state-of-the-art (SOTA) for thischallenge at the time of submission. Conclusion: The proposed pipeline achieved the best DS and HD among allparticipants of the H&amp;N OAR CT and MR segmentation challenge and sets a newSOTA for automated segmentation of H&amp;N OARs.</description><author>Sébastien Quetin, Andrew Heschl, Mauricio Murillo, Murali Rohit, Shirin A. Enger, Farhad Maleki</author><pubDate>Fri, 17 May 2024 15:54:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10833v1</guid></item><item><title>Open-Vocabulary Spatio-Temporal Action Detection</title><link>http://arxiv.org/abs/2405.10832v1</link><description>Spatio-temporal action detection (STAD) is an important fine-grained videounderstanding task. Current methods require box and label supervision for allaction classes in advance. However, in real-world applications, it is verylikely to come across new action classes not seen in training because theaction category space is large and hard to enumerate. Also, the cost of dataannotation and model training for new classes is extremely high for traditionalmethods, as we need to perform detailed box annotations and re-train the wholenetwork from scratch. In this paper, we propose a new challenging setting byperforming open-vocabulary STAD to better mimic the situation of actiondetection in an open world. Open-vocabulary spatio-temporal action detection(OV-STAD) requires training a model on a limited set of base classes with boxand label supervision, which is expected to yield good generalizationperformance on novel action classes. For OV-STAD, we build two benchmarks basedon the existing STAD datasets and propose a simple but effective method basedon pretrained video-language models (VLM). To better adapt the holistic VLM forthe fine-grained action detection task, we carefully fine-tune it on thelocalized video region-text pairs. This customized fine-tuning endows the VLMwith better motion understanding, thus contributing to a more accuratealignment between video regions and texts. Local region feature and globalvideo feature fusion before alignment is adopted to further improve the actiondetection performance by providing global context. Our method achieves apromising performance on novel classes.</description><author>Tao Wu, Shuqiu Ge, Jie Qin, Gangshan Wu, Limin Wang</author><pubDate>Fri, 17 May 2024 15:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10832v1</guid></item><item><title>Exploring new territory: Calibration-free decoding for c-VEP BCI</title><link>http://arxiv.org/abs/2403.15521v2</link><description>This study explores two zero-training methods aimed at enhancing theusability of brain-computer interfaces (BCIs) by eliminating the need for acalibration session. We introduce a novel method rooted in the event-relatedpotential (ERP) domain, unsupervised mean maximization (UMM), to the fastcode-modulated visual evoked potential (c-VEP) stimulus protocol. We compareUMM to the state-of-the-art c-VEP zero-training method that uses canonicalcorrelation analysis (CCA). The comparison includes instantaneousclassification and classification with cumulative learning from previouslyclassified trials for both CCA and UMM. Our study shows the effectiveness ofboth methods in navigating the complexities of a c-VEP dataset, highlightingtheir differences and distinct strengths. This research not only providesinsights into the practical implementation of calibration-free BCI methods butalso paves the way for further exploration and refinement. Ultimately, thefusion of CCA and UMM holds promise for enhancing the accessibility andusability of BCI systems across various application domains and a multitude ofstimulus protocols.</description><author>J. Thielen, J. Sosulski, M. Tangermann</author><pubDate>Fri, 17 May 2024 15:48:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15521v2</guid></item><item><title>Analysis of Impulsive Interference in Digital Audio Broadcasting Systems in Electric Vehicles</title><link>http://arxiv.org/abs/2405.10828v1</link><description>Recently, new types of interference in electric vehicles (EVs), such asconverters switching and/or battery chargers, have been found to degrade theperformance of wireless digital transmission systems. Measurements show thatsuch an interference is characterized by impulsive behavior and is widelyvarying in time. This paper uses recorded data from our EV testbed to analyzethe impulsive interference in the digital audio broadcasting band. Moreover, weuse our analysis to obtain a corresponding interference model. In particular,we studied the temporal characteristics of the interference and confirmed thatits amplitude indeed exhibits an impulsive behavior. Our results show thatimpulsive events span successive received signal samples and thus indicate abursty nature. To this end, we performed a data-driven modification of awell-established model for bursty impulsive interference, the Markov-Middletonmodel, to produce synthetic noise realization. We investigate the optimalsymbol detector design based on the proposed model and show significantperformance gains compared to the conventional detector based on the additivewhite Gaussian noise assumption.</description><author>Chin-Hung Chen, Wen-Hung Huang, Boris Karanov, Alex Young, Yan Wu, Wim van Houtum</author><pubDate>Fri, 17 May 2024 15:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10828v1</guid></item><item><title>Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?</title><link>http://arxiv.org/abs/2402.12025v2</link><description>The field of natural language processing (NLP) has recently witnessed atransformative shift with the emergence of foundation models, particularlyLarge Language Models (LLMs) that have revolutionized text-based NLP. Thisparadigm has extended to other modalities, including speech, where researchersare actively exploring the combination of Speech Foundation Models (SFMs) andLLMs into single, unified models capable of addressing multimodal tasks. Amongsuch tasks, this paper focuses on speech-to-text translation (ST). By examiningthe published papers on the topic, we propose a unified view of thearchitectural solutions and training strategies presented so far, highlightingsimilarities and differences among them. Based on this examination, we not onlyorganize the lessons learned but also show how diverse settings and evaluationapproaches hinder the identification of the best-performing solution for eacharchitectural building block and training choice. Lastly, we outlinerecommendations for future works on the topic aimed at better understanding thestrengths and weaknesses of the SFM+LLM solutions for ST.</description><author>Marco Gaido, Sara Papi, Matteo Negri, Luisa Bentivogli</author><pubDate>Fri, 17 May 2024 15:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12025v2</guid></item><item><title>Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities</title><link>http://arxiv.org/abs/2405.10825v1</link><description>Large language models (LLMs) have received considerable attention recentlydue to their outstanding comprehension and reasoning capabilities, leading togreat progress in many fields. The advancement of LLM techniques also offerspromising opportunities to automate many tasks in the telecommunication(telecom) field. After pre-training and fine-tuning, LLMs can perform diversedownstream tasks based on human instructions, paving the way to artificialgeneral intelligence (AGI)-enabled 6G. Given the great potential of LLMtechnologies, this work aims to provide a comprehensive overview of LLM-enabledtelecom networks. In particular, we first present LLM fundamentals, includingmodel architecture, pre-training, fine-tuning, inference and utilization, modelevaluation, and telecom deployment. Then, we introduce LLM-enabled keytechniques and telecom applications in terms of generation, classification,optimization, and prediction problems. Specifically, the LLM-enabled generationapplications include telecom domain knowledge, code, and network configurationgeneration. After that, the LLM-based classification applications involvenetwork security, text, image, and traffic classification problems. Moreover,multiple LLM-enabled optimization techniques are introduced, such as automatedreward function design for reinforcement learning and verbal reinforcementlearning. Furthermore, for LLM-aided prediction problems, we discussedtime-series prediction models and multi-modality prediction problems fortelecom. Finally, we highlight the challenges and identify the futuredirections of LLM-enabled telecom networks.</description><author>Hao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Yili Jin, Can Chen, Haolun Wu, Dun Yuan, Li Jiang, Di Wu, Xue Liu, Charlie Zhang, Xianbin Wang, Jiangchuan Liu</author><pubDate>Fri, 17 May 2024 15:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10825v1</guid></item><item><title>Towards auditory attention decoding with noise-tagging: A pilot study</title><link>http://arxiv.org/abs/2403.15523v2</link><description>Auditory attention decoding (AAD) aims to extract from brain activity theattended speaker amidst candidate speakers, offering promising applications forneuro-steered hearing devices and brain-computer interfacing. This pilot studymakes a first step towards AAD using the noise-tagging stimulus protocol, whichevokes reliable code-modulated evoked potentials, but is minimally explored inthe auditory modality. Participants were sequentially presented with two Dutchspeech stimuli that were amplitude-modulated with a unique binary pseudo-randomnoise-code, effectively tagging these with additional decodable information. Wecompared the decoding of unmodulated audio against audio modulated with variousmodulation depths, and a conventional AAD method against a standard method todecode noise-codes. Our pilot study revealed higher performances for theconventional method with 70 to 100 percent modulation depths compared tounmodulated audio. The noise-code decoder did not further improve theseresults. These fundamental insights highlight the potential of integratingnoise-codes in speech to enhance auditory speaker detection when multiplespeakers are presented simultaneously.</description><author>H. A. Scheppink, S. Ahmadi, P. Desain, M. Tangermann, J. Thielen</author><pubDate>Fri, 17 May 2024 15:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15523v2</guid></item><item><title>Generative modeling through internal high-dimensional chaotic activity</title><link>http://arxiv.org/abs/2405.10822v1</link><description>Generative modeling aims at producing new datapoints whose statisticalproperties resemble the ones in a training dataset. In recent years, there hasbeen a burst of machine learning techniques and settings that can achieve thisgoal with remarkable performances. In most of these settings, one uses thetraining dataset in conjunction with noise, which is added as a source ofstatistical variability and is essential for the generative task. Here, weexplore the idea of using internal chaotic dynamics in high-dimensional chaoticsystems as a way to generate new datapoints from a training dataset. We showthat simple learning rules can achieve this goal within a set of vanillaarchitectures and characterize the quality of the generated datapoints throughstandard accuracy measures.</description><author>Samantha J. Fournier, Pierfrancesco Urbani</author><pubDate>Fri, 17 May 2024 15:43:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10822v1</guid></item><item><title>Exploration and Anti-Exploration with Distributional Random Network Distillation</title><link>http://arxiv.org/abs/2401.09750v3</link><description>Exploration remains a critical issue in deep reinforcement learning for anagent to attain high returns in unknown environments. Although the prevailingexploration Random Network Distillation (RND) algorithm has been demonstratedto be effective in numerous environments, it often needs more discriminativepower in bonus allocation. This paper highlights the "bonus inconsistency"issue within RND, pinpointing its primary limitation. To address this issue, weintroduce the Distributional RND (DRND), a derivative of the RND. DRND enhancesthe exploration process by distilling a distribution of random networks andimplicitly incorporating pseudo counts to improve the precision of bonusallocation. This refinement encourages agents to engage in more extensiveexploration. Our method effectively mitigates the inconsistency issue withoutintroducing significant computational overhead. Both theoretical analysis andexperimental results demonstrate the superiority of our approach over theoriginal RND algorithm. Our method excels in challenging online explorationscenarios and effectively serves as an anti-exploration mechanism in D4RLoffline tasks. Our code is publicly available athttps://github.com/yk7333/DRND.</description><author>Kai Yang, Jian Tao, Jiafei Lyu, Xiu Li</author><pubDate>Fri, 17 May 2024 15:41:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09750v3</guid></item><item><title>CapHuman: Capture Your Moments in Parallel Universes</title><link>http://arxiv.org/abs/2402.00627v3</link><description>We concentrate on a novel human-centric image synthesis task, that is, givenonly one reference facial photograph, it is expected to generate specificindividual images with diverse head positions, poses, facial expressions, andilluminations in different contexts. To accomplish this goal, we argue that ourgenerative model should be capable of the following favorable characteristics:(1) a strong visual and semantic understanding of our world and human societyfor basic object and human image generation. (2) generalizable identitypreservation ability. (3) flexible and fine-grained head control. Recently,large pre-trained text-to-image diffusion models have shown remarkable results,serving as a powerful generative foundation. As a basis, we aim to unleash theabove two capabilities of the pre-trained model. In this work, we present a newframework named CapHuman. We embrace the "encode then learn to align" paradigm,which enables generalizable identity preservation for new individuals withoutcumbersome tuning at inference. CapHuman encodes identity features and thenlearns to align them into the latent space. Moreover, we introduce the 3Dfacial prior to equip our model with control over the human head in a flexibleand 3D-consistent manner. Extensive qualitative and quantitative analysesdemonstrate our CapHuman can produce well-identity-preserved, photo-realistic,and high-fidelity portraits with content-rich representations and various headrenditions, superior to established baselines. Code and checkpoint will bereleased at https://github.com/VamosC/CapHuman.</description><author>Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang</author><pubDate>Fri, 17 May 2024 15:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00627v3</guid></item><item><title>Restless Linear Bandits</title><link>http://arxiv.org/abs/2405.10817v1</link><description>A more general formulation of the linear bandit problem is considered toallow for dependencies over time. Specifically, it is assumed that there existsan unknown $\mathbb{R}^d$-valued stationary $\varphi$-mixing sequence ofparameters $(\theta_t,~t \in \mathbb{N})$ which gives rise to pay-offs. Thisinstance of the problem can be viewed as a generalization of both the classicallinear bandits with iid noise, and the finite-armed restless bandits. In lightof the well-known computational hardness of optimal policies for restlessbandits, an approximation is proposed whose error is shown to be controlled bythe $\varphi$-dependence between consecutive $\theta_t$. An optimisticalgorithm, called LinMix-UCB, is proposed for the case where $\theta_t$ has anexponential mixing rate. The proposed algorithm is shown to incur a sub-linearregret of $\mathcal{O}\left(\sqrt{d n\mathrm{polylog}(n) }\right)$ with respectto an oracle that always plays a multiple of $\mathbb{E}\theta_t$. The mainchallenge in this setting is to ensure that the exploration-exploitationstrategy is robust against long-range dependencies. The proposed method relieson Berbee's coupling lemma to carefully select near-independent samples andconstruct confidence ellipsoids around empirical estimates of$\mathbb{E}\theta_t$.</description><author>Azadeh Khaleghi</author><pubDate>Fri, 17 May 2024 15:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10817v1</guid></item><item><title>A Functional Model Method for Nonconvex Nonsmooth Conditional Stochastic Optimization</title><link>http://arxiv.org/abs/2405.10815v1</link><description>We consider stochastic optimization problems involving an expected value of anonlinear function of a base random vector and a conditional expectation ofanother function depending on the base random vector, a dependent randomvector, and the decision variables. We call such problems conditionalstochastic optimization problems. They arise in many applications, such asuplift modeling, reinforcement learning, and contextual optimization. Wepropose a specialized single time-scale stochastic method for nonconvexconstrained conditional stochastic optimization problems with a Lipschitzsmooth outer function and a generalized differentiable inner function. In themethod, we approximate the inner conditional expectation with a rich parametricmodel whose mean squared error satisfies a stochastic version of a{\L}ojasiewicz condition. The model is used by an inner learning algorithm. Themain feature of our approach is that unbiased stochastic estimates of thedirections used by the method can be generated with one observation from thejoint distribution per iteration, which makes it applicable to real-timelearning. The directions, however, are not gradients or subgradients of anyoverall objective function. We prove the convergence of the method withprobability one, using the method of differential inclusions and a speciallydesigned Lyapunov function, involving a stochastic generalization of theBregman distance. Finally, a numerical illustration demonstrates the viabilityof our approach.</description><author>Andrzej Ruszczyński, Shangzhe Yang</author><pubDate>Fri, 17 May 2024 15:35:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10815v1</guid></item><item><title>Data-Driven Symbol Detection for Intersymbol Interference Channels with Bursty Impulsive Noise</title><link>http://arxiv.org/abs/2405.10814v1</link><description>We developed machine learning approaches for data-driven trellis-based softsymbol detection in coded transmission over intersymbol interference (ISI)channels in presence of bursty impulsive noise (IN), for example encountered inwireless digital broadcasting systems and vehicular communications. Thisenabled us to obtain optimized detectors based on the Bahl-Cocke-Jelinek-Raviv(BCJR) algorithm while circumventing the use of full channel state information(CSI) for computing likelihoods and trellis state transition probabilities.First, we extended the application of the neural network (NN)-aided BCJR,recently proposed for ISI channels with additive white Gaussian noise (AWGN).Although suitable for estimating likelihoods via labeling of transmissionsequences, the BCJR-NN method does not provide a framework for learning thetrellis state transitions. In addition to detection over the joint ISI and INstates we also focused on another scenario where trellis transitions are nottrivial: detection for the ISI channel with AWGN with inaccurate knowledge ofthe channel memory at the receiver. Without access to the accurate statetransition matrix, the BCJR- NN performance significantly degrades in bothsettings. To this end, we devised an alternative approach for data-driven BCJRdetection based on the unsupervised learning of a hidden Markov model (HMM).The BCJR-HMM allowed us to optimize both the likelihood function and the statetransition matrix without labeling. Moreover, we demonstrated the viability ofa hybrid NN and HMM BCJR detection where NN is used for learning thelikelihoods, while the state transitions are optimized via HMM. While reducingthe required prior channel knowledge, the examined data-driven detectors withlearned trellis state transitions achieve bit error rates close to the optimalfull CSI-based BCJR, significantly outperforming detection with inaccurate CSI.</description><author>Boris Karanov, Chin-Hung Chen, Yan Wu, Alex Young, Wim van Houtum</author><pubDate>Fri, 17 May 2024 15:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10814v1</guid></item><item><title>Forecasting with Hyper-Trees</title><link>http://arxiv.org/abs/2405.07836v2</link><description>This paper introduces the concept of Hyper-Trees and offers a new directionin applying tree-based models to time series data. Unlike conventionalapplications of decision trees that forecast time series directly, Hyper-Treesare designed to learn the parameters of a target time series model. Ourframework leverages the gradient-based nature of boosted trees, which allows usto extend the concept of Hyper-Networks to Hyper-Trees and to induce atime-series inductive bias to tree models. By relating the parameters of atarget time series model to features, Hyper-Trees address the issue ofparameter non-stationarity and enable tree-based forecasts to extend beyondtheir training range. With our research, we aim to explore the effectiveness ofHyper-Trees across various forecasting scenarios and to extend the applicationof gradient boosted decision trees outside their conventional use in timeseries modeling.</description><author>Alexander März, Kashif Rasul</author><pubDate>Fri, 17 May 2024 15:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07836v2</guid></item><item><title>ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios</title><link>http://arxiv.org/abs/2405.10808v1</link><description>Active learning is designed to minimize annotation efforts by prioritizinginstances that most enhance learning. However, many active learning strategiesstruggle with a 'cold start' problem, needing substantial initial data to beeffective. This limitation often reduces their utility for pre-trained models,which already perform well in few-shot scenarios. To address this, we introduceActiveLLM, a novel active learning approach that leverages large languagemodels such as GPT-4, Llama 3, and Mistral Large for selecting instances. Wedemonstrate that ActiveLLM significantly enhances the classificationperformance of BERT classifiers in few-shot scenarios, outperforming bothtraditional active learning methods and the few-shot learning method SetFit.Additionally, ActiveLLM can be extended to non-few-shot scenarios, allowing foriterative selections. In this way, ActiveLLM can even help other activelearning strategies to overcome their cold start problem. Our results suggestthat ActiveLLM offers a promising solution for improving model performanceacross various learning setups.</description><author>Markus Bayer, Christian Reuter</author><pubDate>Fri, 17 May 2024 15:23:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10808v1</guid></item><item><title>ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale</title><link>http://arxiv.org/abs/2310.01217v3</link><description>Multi-task learning (MTL) has shown considerable practical benefits,particularly when using language models (LMs). While this is commonly achievedby learning $n$ tasks under a joint optimization procedure, some methods, suchas AdapterFusion, divide the problem into two stages: (i) task learning, whereknowledge specific to a task is encapsulated within sets of parameters (e.g.,adapters), and (ii) transfer, where this already learned knowledge is leveragedfor a target task. This separation of concerns provides numerous benefits(e.g., promoting reusability). However, current two-stage MTL introduces asubstantial number of additional parameters. We address this issue byleveraging the usefulness of linearly scaling the output representations ofsource adapters for transfer learning. We introduce ScaLearn, a simple andhighly parameter-efficient two-stage MTL method that capitalizes on theknowledge of the source tasks by learning a minimal set of scaling parametersthat enable effective transfer to a target task. Our experiments on threebenchmarks (GLUE, SuperGLUE, and HumSet) and two encoder LMs show that ScaLearnconsistently outperforms strong baselines with a small number of transferparameters (~ $0.35$% of those of AdapterFusion). Remarkably, we observe thatScaLearn maintains its strong abilities even when further reducing parameters,achieving competitive results with only $8$ transfer parameters per targettask. Our proposed approach thus demonstrates the power of simple scaling as apromise for more efficient task transfer.</description><author>Markus Frohmann, Carolin Holtermann, Shahed Masoudian, Anne Lauscher, Navid Rekabsaz</author><pubDate>Fri, 17 May 2024 15:23:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01217v3</guid></item><item><title>C(NN)FD -- a deep learning framework for turbomachinery CFD analysis</title><link>http://arxiv.org/abs/2306.05889v2</link><description>Deep Learning methods have seen a wide range of successful applicationsacross different industries. Up until now, applications to physical simulationssuch as CFD (Computational Fluid Dynamics), have been limited to simpletest-cases of minor industrial relevance. This paper demonstrates thedevelopment of a novel deep learning framework for real-time predictions of theimpact of manufacturing and build variations on the overall performance ofaxial compressors in gas turbines, with a focus on tip clearance variations.The associated scatter in efficiency can significantly increase the CO2emissions, thus being of great industrial and environmental relevance. Theproposed C(NN)FD architecture achieves in real-time accuracy comparable to theCFD benchmark. Predicting the flow field and using it to calculate thecorresponding overall performance renders the methodology generalisable, whilefiltering only relevant parts of the CFD solution makes the methodologyscalable to industrial applications.</description><author>Giuseppe Bruni, Sepehr Maleki, Senthil K. Krishnababu</author><pubDate>Fri, 17 May 2024 15:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05889v2</guid></item><item><title>A Large-scale Multi Domain Leukemia Dataset for the White Blood Cells Detection with Morphological Attributes for Explainability</title><link>http://arxiv.org/abs/2405.10803v1</link><description>Earlier diagnosis of Leukemia can save thousands of lives annually. Theprognosis of leukemia is challenging without the morphological information ofWhite Blood Cells (WBC) and relies on the accessibility of expensivemicroscopes and the availability of hematologists to analyze Peripheral BloodSamples (PBS). Deep Learning based methods can be employed to assisthematologists. However, these algorithms require a large amount of labeleddata, which is not readily available. To overcome this limitation, we haveacquired a realistic, generalized, and large dataset. To collect thiscomprehensive dataset for real-world applications, two microscopes from twodifferent cost spectrums (high-cost HCM and low-cost LCM) are used for datasetcapturing at three magnifications (100x, 40x, 10x) through different sensors(high-end camera for HCM, middle-level camera for LCM and mobile-phone camerafor both). The high-sensor camera is 47 times more expensive than themiddle-level camera and HCM is 17 times more expensive than LCM. In thiscollection, using HCM at high resolution (100x), experienced hematologistsannotated 10.3k WBC types (14) and artifacts, having 55k morphological labels(Cell Size, Nuclear Chromatin, Nuclear Shape, etc.) from 2.4k images of severalPBS leukemia patients. Later on, these annotations are transferred to other 2magnifications of HCM, and 3 magnifications of LCM, and on each camera capturedimages. Along with the LeukemiaAttri dataset, we provide baselines overmultiple object detectors and Unsupervised Domain Adaptation (UDA) strategies,along with morphological information-based attribute prediction. The datasetwill be publicly available after publication to facilitate the research in thisdirection.</description><author>Abdul Rehman, Talha Meraj, Aiman Mahmood Minhas, Ayisha Imran, Mohsen Ali, Waqas Sultani</author><pubDate>Fri, 17 May 2024 15:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10803v1</guid></item><item><title>Reduced storage direct tensor ring decomposition for convolutional neural networks compression</title><link>http://arxiv.org/abs/2405.10802v1</link><description>Convolutional neural networks (CNNs) are among the most widely used machinelearning models for computer vision tasks, such as image classification. Toimprove the efficiency of CNNs, many CNNs compressing approaches have beendeveloped. Low-rank methods approximate the original convolutional kernel witha sequence of smaller convolutional kernels, which leads to reduced storage andtime complexities. In this study, we propose a novel low-rank CNNs compressionmethod that is based on reduced storage direct tensor ring decomposition(RSDTR). The proposed method offers a higher circular mode permutationflexibility, and it is characterized by large parameter and FLOPS compressionrates, while preserving a good classification accuracy of the compressednetwork. The experiments, performed on the CIFAR-10 and ImageNet datasets,clearly demonstrate the efficiency of RSDTR in comparison to otherstate-of-the-art CNNs compression approaches.</description><author>Mateusz Gabor, Rafał Zdunek</author><pubDate>Fri, 17 May 2024 15:16:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10802v1</guid></item><item><title>Leveraging SO(3)-steerable convolutions for pose-robust semantic segmentation in 3D medical data</title><link>http://arxiv.org/abs/2303.00351v3</link><description>Convolutional neural networks (CNNs) allow for parameter sharing andtranslational equivariance by using convolutional kernels in their linearlayers. By restricting these kernels to be SO(3)-steerable, CNNs can furtherimprove parameter sharing. These rotationally-equivariant convolutional layershave several advantages over standard convolutional layers, including increasedrobustness to unseen poses, smaller network size, and improved sampleefficiency. Despite this, most segmentation networks used in medical imageanalysis continue to rely on standard convolutional kernels. In this paper, wepresent a new family of segmentation networks that use equivariant voxelconvolutions based on spherical harmonics. These networks are robust to dataposes not seen during training, and do not require rotation-based dataaugmentation during training. In addition, we demonstrate improved segmentationperformance in MRI brain tumor and healthy brain structure segmentation tasks,with enhanced robustness to reduced amounts of training data and improvedparameter efficiency. Code to reproduce our results, and to implement theequivariant segmentation networks for other tasks is available athttp://github.com/SCAN-NRAD/e3nn_Unet</description><author>Ivan Diaz, Mario Geiger, Richard Iain McKinley</author><pubDate>Fri, 17 May 2024 15:16:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00351v3</guid></item><item><title>Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting</title><link>http://arxiv.org/abs/2405.10800v1</link><description>Spatiotemporal time series forecasting plays a key role in a wide range ofreal-world applications. While significant progress has been made in this area,fully capturing and leveraging spatiotemporal heterogeneity remains afundamental challenge. Therefore, we propose a novel Heterogeneity-InformedMeta-Parameter Learning scheme. Specifically, our approach implicitly capturesspatiotemporal heterogeneity through learning spatial and temporal embeddings,which can be viewed as a clustering process. Then, a novel spatiotemporalmeta-parameter learning paradigm is proposed to learn spatiotemporal-specificparameters from meta-parameter pools, which is informed by the capturedheterogeneity. Based on these ideas, we develop a Heterogeneity-InformedSpatiotemporal Meta-Network (HimNet) for spatiotemporal time seriesforecasting. Extensive experiments on five widely-used benchmarks demonstrateour method achieves state-of-the-art performance while exhibiting superiorinterpretability. Our code is available athttps://github.com/XDZhelheim/HimNet.</description><author>Zheng Dong, Renhe Jiang, Haotian Gao, Hangchen Liu, Jinliang Deng, Qingsong Wen, Xuan Song</author><pubDate>Fri, 17 May 2024 15:10:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10800v1</guid></item><item><title>Training Compute Thresholds: Features and Functions in AI Governance</title><link>http://arxiv.org/abs/2405.10799v1</link><description>This paper examines the use of training compute thresholds as a tool forgoverning artificial intelligence (AI) systems. We argue that computethresholds serve as a valuable trigger for further evaluation of AI models,rather than being the sole determinant of the regulation. Key advantages ofcompute thresholds include their correlation with model capabilities and risks,quantifiability, ease of measurement, robustness to circumvention, knowabilitybefore model development and deployment, potential for external verification,and targeted scope. Compute thresholds provide a practical starting point foridentifying potentially high-risk models and can be used as an initial filterin AI governance frameworks alongside other sector-specific regulations andbroader governance measures.</description><author>Lennart Heim</author><pubDate>Fri, 17 May 2024 15:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10799v1</guid></item><item><title>How Spurious Features Are Memorized: Precise Analysis for Random and NTK Features</title><link>http://arxiv.org/abs/2305.12100v3</link><description>Deep learning models are known to overfit and memorize spurious features inthe training dataset. While numerous empirical studies have aimed atunderstanding this phenomenon, a rigorous theoretical framework to quantify itis still missing. In this paper, we consider spurious features that areuncorrelated with the learning task, and we provide a precise characterizationof how they are memorized via two separate terms: (i) the stability of themodel with respect to individual training samples, and (ii) the featurealignment between the spurious feature and the full sample. While the firstterm is well established in learning theory and it is connected to thegeneralization error in classical work, the second one is, to the best of ourknowledge, novel. Our key technical result gives a precise characterization ofthe feature alignment for the two prototypical settings of random features (RF)and neural tangent kernel (NTK) regression. We prove that the memorization ofspurious features weakens as the generalization capability increases and,through the analysis of the feature alignment, we unveil the role of the modeland of its activation function. Numerical experiments show the predictive powerof our theory on standard datasets (MNIST, CIFAR-10).</description><author>Simone Bombari, Marco Mondelli</author><pubDate>Fri, 17 May 2024 15:10:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12100v3</guid></item><item><title>Two-Stage Stance Labeling: User-Hashtag Heuristics with Graph Neural Networks</title><link>http://arxiv.org/abs/2404.10228v2</link><description>The high volume and rapid evolution of content on social media present majorchallenges for studying the stance of social media users. In this work, wedevelop a two stage stance labeling method that utilizes the user-hashtagbipartite graph and the user-user interaction graph. In the first stage, asimple and efficient heuristic for stance labeling uses the user-hashtagbipartite graph to iteratively update the stance association of user andhashtag nodes via a label propagation mechanism. This set of soft labels isthen integrated with the user-user interaction graph to train a graph neuralnetwork (GNN) model using semi-supervised learning. We evaluate this method ontwo large-scale datasets containing tweets related to climate change from June2021 to June 2022 and gun control from January 2022 to January 2023. Ourexperiments demonstrate that enriching text-based embeddings of users withnetwork information from the user interaction graph using our semi-supervisedGNN method outperforms both classifiers trained on user textual embeddings andzero-shot classification using LLMs such as GPT4. We discuss the need forintegrating nuanced understanding from social science with the scalability ofcomputational methods to better understand how polarization on social mediaoccurs for divisive issues such as climate change and gun control.</description><author>Joshua Melton, Shannon Reid, Gabriel Terejanu, Siddharth Krishnan</author><pubDate>Fri, 17 May 2024 15:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10228v2</guid></item><item><title>Differentially private projection-depth-based medians</title><link>http://arxiv.org/abs/2312.07792v2</link><description>We develop $(\epsilon,\delta)$-differentially private projection-depth-basedmedians using the propose-test-release (PTR) and exponential mechanisms. Undergeneral conditions on the input parameters and the population measure, (e.g. wedo not assume any moment bounds), we quantify the probability the test in PTRfails, as well as the cost of privacy via finite sample deviation bounds. Wethen present a new definition of the finite sample breakdown point whichapplies to a mechanism, and present a lower bound on the finite samplebreakdown point of the projection-depth-based median. We demonstrate our mainresults on the canonical projection-depth-based median, as well as onprojection-depth-based medians derived from trimmed estimators. In the Gaussiansetting, we show that the resulting deviation bound matches the known lowerbound for private Gaussian mean estimation. In the Cauchy setting, we show thatthe "outlier error amplification" effect resulting from the heavy tailsoutweighs the cost of privacy. This result is then verified via numericalsimulations. Additionally, we present results on general PTR mechanisms and auniform concentration result on the projected spacings of order statistics,which may be of general interest.</description><author>Kelly Ramsay, Dylan Spicker</author><pubDate>Fri, 17 May 2024 14:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07792v2</guid></item><item><title>Naturalistic Music Decoding from EEG Data via Latent Diffusion Models</title><link>http://arxiv.org/abs/2405.09062v2</link><description>In this article, we explore the potential of using latent diffusion models, afamily of powerful generative models, for the task of reconstructingnaturalistic music from electroencephalogram (EEG) recordings. Unlike simplermusic with limited timbres, such as MIDI-generated tunes or monophonic pieces,the focus here is on intricate music featuring a diverse array of instruments,voices, and effects, rich in harmonics and timbre. This study represents aninitial foray into achieving general music reconstruction of high-quality usingnon-invasive EEG data, employing an end-to-end training approach directly onraw data without the need for manual pre-processing and channel selection. Wetrain our models on the public NMED-T dataset and perform quantitativeevaluation proposing neural embedding-based metrics. We additionally performsong classification based on the generated tracks. Our work contributes to theongoing research in neural decoding and brain-computer interfaces, offeringinsights into the feasibility of using EEG data for complex auditoryinformation reconstruction.</description><author>Emilian Postolache, Natalia Polouliakh, Hiroaki Kitano, Akima Connelly, Emanuele Rodolà, Taketo Akama</author><pubDate>Fri, 17 May 2024 14:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09062v2</guid></item><item><title>Baseline Results for Selected Nonlinear System Identification Benchmarks</title><link>http://arxiv.org/abs/2405.10779v1</link><description>Nonlinear system identification remains an important open challenge acrossresearch and academia. Large numbers of novel approaches are seen publishedeach year, each presenting improvements or extensions to existing methods. Itis natural, therefore, to consider how one might choose between these competingmodels. Benchmark datasets provide one clear way to approach this question.However, to make meaningful inference based on benchmark performance it isimportant to understand how well a new method performs comparatively to resultsavailable with well-established methods. This paper presents a set of tenbaseline techniques and their relative performances on five popular benchmarks.The aim of this contribution is to stimulate thought and discussion regardingobjective comparison of identification methodologies.</description><author>Max D. Champneys, Gerben I. Beintema, Roland Tóth, Maarten Schoukens, Maarten Schoukens, Timothy J. Rogers</author><pubDate>Fri, 17 May 2024 14:40:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10779v1</guid></item><item><title>Multi-modal Stance Detection: New Datasets and Model</title><link>http://arxiv.org/abs/2402.14298v2</link><description>Stance detection is a challenging task that aims to identify public opinionfrom social media platforms with respect to specific targets. Previous work onstance detection largely focused on pure texts. In this paper, we studymulti-modal stance detection for tweets consisting of texts and images, whichare prevalent in today's fast-growing social media platforms where people oftenpost multi-modal messages. To this end, we create five new multi-modal stancedetection datasets of different domains based on Twitter, in which each exampleconsists of a text and an image. In addition, we propose a simple yet effectiveTargeted Multi-modal Prompt Tuning framework (TMPT), where target informationis leveraged to learn multi-modal stance features from textual and visualmodalities. Experimental results on our three benchmark datasets show that theproposed TMPT achieves state-of-the-art performance in multi-modal stancedetection.</description><author>Bin Liang, Ang Li, Jingqian Zhao, Lin Gui, Min Yang, Yue Yu, Kam-Fai Wong, Ruifeng Xu</author><pubDate>Fri, 17 May 2024 14:36:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14298v2</guid></item><item><title>Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment</title><link>http://arxiv.org/abs/2311.04818v5</link><description>Learning from the collective knowledge of data dispersed across privatesources can provide neural networks with enhanced generalization capabilities.Federated learning, a method for collaboratively training a machine learningmodel across remote clients, achieves this by combining client models via theorchestration of a central server. However, current approaches face twocritical limitations: i) they struggle to converge when client domains aresufficiently different, and ii) current aggregation techniques produce anidentical global model for each client. In this work, we address these issuesby reformulating the typical federated learning setup: rather than learning asingle global model, we learn N models each optimized for a common objective.To achieve this, we apply a weighted distance minimization to model parametersshared in a peer-to-peer topology. The resulting framework, Iterative ParameterAlignment, applies naturally to the cross-silo setting, and has the followingproperties: (i) a unique solution for each participant, with the option toglobally converge each model in the federation, and (ii) an optionalearly-stopping mechanism to elicit fairness among peers in collaborativelearning settings. These characteristics jointly provide a flexible newframework for iteratively learning from peer models trained on disparatedatasets. We find that the technique achieves competitive results on a varietyof data partitions compared to state-of-the-art approaches. Further, we showthat the method is robust to divergent domains (i.e. disjoint classes acrosspeers) where existing approaches struggle.</description><author>Matt Gorbett, Hossein Shirazi, Indrakshi Ray</author><pubDate>Fri, 17 May 2024 14:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04818v5</guid></item><item><title>Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems</title><link>http://arxiv.org/abs/2405.06624v2</link><description>Ensuring that AI systems reliably and robustly avoid harmful or dangerousbehaviours is a crucial challenge, especially for AI systems with a high degreeof autonomy and general intelligence, or systems used in safety-criticalcontexts. In this paper, we will introduce and define a family of approaches toAI safety, which we will refer to as guaranteed safe (GS) AI. The core featureof these approaches is that they aim to produce AI systems which are equippedwith high-assurance quantitative safety guarantees. This is achieved by theinterplay of three core components: a world model (which provides amathematical description of how the AI system affects the outside world), asafety specification (which is a mathematical description of what effects areacceptable), and a verifier (which provides an auditable proof certificate thatthe AI satisfies the safety specification relative to the world model). Weoutline a number of approaches for creating each of these three corecomponents, describe the main technical challenges, and suggest a number ofpotential solutions to them. We also argue for the necessity of this approachto AI safety, and for the inadequacy of the main alternative approaches.</description><author>David "davidad" Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, Alessandro Abate, Joe Halpern, Clark Barrett, Ding Zhao, Tan Zhi-Xuan, Jeannette Wing, Joshua Tenenbaum</author><pubDate>Fri, 17 May 2024 14:31:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06624v2</guid></item><item><title>Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks</title><link>http://arxiv.org/abs/2012.02030v3</link><description>Attention mechanisms play a crucial role in the neural revolution of NaturalLanguage Processing (NLP). With the growth of attention-based models, severalpruning techniques have been developed to identify and exploit sparseness,making these models more efficient. Most efforts focus on hard-coding attentionpatterns or pruning attention weights based on training data. We proposeAttention Pruning (AP), a framework that observes attention patterns in a fixeddataset and generates a global sparseness mask. AP saves 90% of attentioncomputation for language modeling and about 50% for machine translation andGLUE tasks, maintaining result quality. Our method reveals importantdistinctions between self- and cross-attention patterns, guiding future NLPresearch. Our framework can reduce both latency and memory requirements for anyattention-based model, aiding in the development of improved models forexisting or new NLP applications. We have demonstrated this with encoder andautoregressive transformer models using Triton GPU kernels and make our codepublicly available at https://github.com/irugina/AP.</description><author>Ileana Rugina, Rumen Dangovski, Li Jing, Preslav Nakov, Marin Soljačić</author><pubDate>Fri, 17 May 2024 14:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.02030v3</guid></item><item><title>What should be observed for optimal reward in POMDPs?</title><link>http://arxiv.org/abs/2405.10768v1</link><description>Partially observable Markov Decision Processes (POMDPs) are a standard modelfor agents making decisions in uncertain environments. Most work on POMDPsfocuses on synthesizing strategies based on the available capabilities.However, system designers can often control an agent's observationcapabilities, e.g. by placing or selecting sensors. This raises the question ofhow one should select an agent's sensors cost-effectively such that it achievesthe desired goals. In this paper, we study the novel optimal observabilityproblem OOP: Given a POMDP M, how should one change M's observationcapabilities within a fixed budget such that its (minimal) expected rewardremains below a given threshold? We show that the problem is undecidable ingeneral and decidable when considering positional strategies only. We presenttwo algorithms for a decidable fragment of the OOP: one based on optimalstrategies of M's underlying Markov decision process and one based on parametersynthesis with SMT. We report promising results for variants of typicalexamples from the POMDP literature.</description><author>Alyzia-Maria Konsta, Alberto Lluch Lafuente, Christoph Matheja</author><pubDate>Fri, 17 May 2024 14:27:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10768v1</guid></item><item><title>Evaluating Saliency Explanations in NLP by Crowdsourcing</title><link>http://arxiv.org/abs/2405.10767v1</link><description>Deep learning models have performed well on many NLP tasks. However, theirinternal mechanisms are typically difficult for humans to understand. Thedevelopment of methods to explain models has become a key issue in thereliability of deep learning models in many important applications. Varioussaliency explanation methods, which give each feature of input a scoreproportional to the contribution of output, have been proposed to determine thepart of the input which a model values most. Despite a considerable body ofwork on the evaluation of saliency methods, whether the results of variousevaluation metrics agree with human cognition remains an open question. In thisstudy, we propose a new human-based method to evaluate saliency methods in NLPby crowdsourcing. We recruited 800 crowd workers and empirically evaluatedseven saliency methods on two datasets with the proposed method. We analyzedthe performance of saliency methods, compared our results with existingautomated evaluation methods, and identified notable differences between NLPand computer vision (CV) fields when using saliency methods. The instance-leveldata of our crowdsourced experiments and the code to reproduce the explanationsare available at https://github.com/xtlu/lreccoling_evaluation.</description><author>Xiaotian Lu, Jiyi Li, Zhen Wan, Xiaofeng Lin, Koh Takeuchi, Hisashi Kashima</author><pubDate>Fri, 17 May 2024 14:27:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10767v1</guid></item><item><title>Research on Credit Risk Early Warning Model of Commercial Banks Based on Neural Network Algorithm</title><link>http://arxiv.org/abs/2405.10762v1</link><description>In the realm of globalized financial markets, commercial banks are confrontedwith an escalating magnitude of credit risk, thereby imposing heightenedrequisites upon the security of bank assets and financial stability. This studyharnesses advanced neural network techniques, notably the Backpropagation (BP)neural network, to pioneer a novel model for preempting credit risk incommercial banks. The discourse initially scrutinizes conventional financialrisk preemptive models, such as ARMA, ARCH, and Logistic regression models,critically analyzing their real-world applications. Subsequently, theexposition elaborates on the construction process of the BP neural networkmodel, encompassing network architecture design, activation function selection,parameter initialization, and objective function construction. Throughcomparative analysis, the superiority of neural network models in preemptingcredit risk in commercial banks is elucidated. The experimental segment selectsspecific bank data, validating the model's predictive accuracy andpracticality. Research findings evince that this model efficaciously enhancesthe foresight and precision of credit risk management.</description><author>Yu Cheng, Qin Yang, Liyang Wang, Ao Xiang, Jingyu Zhang</author><pubDate>Fri, 17 May 2024 14:18:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10762v1</guid></item><item><title>UFORecon: Generalizable Sparse-View Surface Reconstruction from Arbitrary and UnFavOrable Sets</title><link>http://arxiv.org/abs/2403.05086v3</link><description>Generalizable neural implicit surface reconstruction aims to obtain anaccurate underlying geometry given a limited number of multi-view images fromunseen scenes. However, existing methods select only informative and relevantviews using predefined scores for training and testing phases. This constraintrenders the model impractical in real-world scenarios, where the availabilityof favorable combinations cannot always be ensured. We introduce and validate aview-combination score to indicate the effectiveness of the input viewcombination. We observe that previous methods output degenerate solutions underarbitrary and unfavorable sets. Building upon this finding, we proposeUFORecon, a robust view-combination generalizable surface reconstructionframework. To achieve this, we apply cross-view matching transformers to modelinteractions between source images and build correlation frustums to captureglobal correlations. Additionally, we explicitly encode pairwise featuresimilarities as view-consistent priors. Our proposed framework significantlyoutperforms previous methods in terms of view-combination generalizability andalso in the conventional generalizable protocol trained with favorableview-combinations. The code is available athttps://github.com/Youngju-Na/UFORecon.</description><author>Youngju Na, Woo Jae Kim, Kyu Beom Han, Suhyeon Ha, Sung-eui Yoon</author><pubDate>Fri, 17 May 2024 14:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05086v3</guid></item><item><title>Research on Splicing Image Detection Algorithms Based on Natural Image Statistical Characteristics</title><link>http://arxiv.org/abs/2404.16296v3</link><description>With the development and widespread application of digital image processingtechnology, image splicing has become a common method of image manipulation,raising numerous security and legal issues. This paper introduces a newsplicing image detection algorithm based on the statistical characteristics ofnatural images, aimed at improving the accuracy and efficiency of splicingimage detection. By analyzing the limitations of traditional methods, we havedeveloped a detection framework that integrates advanced statistical analysistechniques and machine learning methods. The algorithm has been validated usingmultiple public datasets, showing high accuracy in detecting spliced edges andlocating tampered areas, as well as good robustness. Additionally, we explorethe potential applications and challenges faced by the algorithm in real-worldscenarios. This research not only provides an effective technological means forthe field of image tampering detection but also offers new ideas and methodsfor future related research.</description><author>Ao Xiang, Jingyu Zhang, Qin Yang, Liyang Wang, Yu Cheng</author><pubDate>Fri, 17 May 2024 14:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16296v3</guid></item><item><title>Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective</title><link>http://arxiv.org/abs/2405.10757v1</link><description>Graph Neural Networks (GNNs) have shown remarkable performance in varioustasks. However, recent works reveal that GNNs are vulnerable to backdoorattacks. Generally, backdoor attack poisons the graph by attaching backdoortriggers and the target class label to a set of nodes in the training graph. AGNN trained on the poisoned graph will then be misled to predict test nodesattached with trigger to the target class. Despite their effectiveness, ourempirical analysis shows that triggers generated by existing methods tend to beout-of-distribution (OOD), which significantly differ from the clean data.Hence, these injected triggers can be easily detected and pruned with widelyused outlier detection methods in real-world applications. Therefore, in thispaper, we study a novel problem of unnoticeable graph backdoor attacks within-distribution (ID) triggers. To generate ID triggers, we introduce an OODdetector in conjunction with an adversarial learning strategy to generate theattributes of the triggers within distribution. To ensure a high attack successrate with ID triggers, we introduce novel modules designed to enhance triggermemorization by the victim model trained on poisoned graph. Extensiveexperiments on real-world datasets demonstrate the effectiveness of theproposed method in generating in distribution triggers that can by-pass variousdefense strategies while maintaining a high attack success rate.</description><author>Zhiwei Zhang, Minhua Lin, Enyan Dai, Suhang Wang</author><pubDate>Fri, 17 May 2024 14:09:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10757v1</guid></item><item><title>Stable Phase Retrieval with Mirror Descent</title><link>http://arxiv.org/abs/2405.10754v1</link><description>In this paper, we aim to reconstruct an n-dimensional real vector from mphaseless measurements corrupted by an additive noise. We extend the noiselessframework developed in [15], based on mirror descent (or Bregman gradientdescent), to deal with noisy measurements and prove that the procedure isstable to (small enough) additive noise. In the deterministic case, we showthat mirror descent converges to a critical point of the phase retrievalproblem, and if the algorithm is well initialized and the noise is smallenough, the critical point is near the true vector up to a global sign change.When the measurements are i.i.d Gaussian and the signal-to-noise ratio is largeenough, we provide global convergence guarantees that ensure that with highprobability, mirror descent converges to a global minimizer near the truevector (up to a global sign change), as soon as the number of measurements m islarge enough. The sample complexity bound can be improved if a spectral methodis used to provide a good initial guess. We complement our theoretical studywith several numerical results showing that mirror descent is both acomputationally and statistically efficient scheme to solve the phase retrievalproblem.</description><author>Jean-Jacques Godeme, Jalal Fadili, Claude Amra, Myriam Zerrad</author><pubDate>Fri, 17 May 2024 14:07:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10754v1</guid></item><item><title>Sharpness-Aware Minimization in Genetic Programming</title><link>http://arxiv.org/abs/2405.10267v2</link><description>Sharpness-Aware Minimization (SAM) was recently introduced as aregularization procedure for training deep neural networks. It simultaneouslyminimizes the fitness (or loss) function and the so-called fitness sharpness.The latter serves as a measure of the nonlinear behavior of a solution and doesso by finding solutions that lie in neighborhoods having uniformly similar lossvalues across all fitness cases. In this contribution, we adapt SAM for treeGenetic Programming (TGP) by exploring the semantic neighborhoods of solutionsusing two simple approaches. By capitalizing upon perturbing input and outputof program trees, sharpness can be estimated and used as a second optimizationcriterion during the evolution. To better understand the impact of this variantof SAM on TGP, we collect numerous indicators of the evolutionary process,including generalization ability, complexity, diversity, and a recentlyproposed genotype-phenotype mapping to study the amount of redundancy in trees.The experimental results demonstrate that using any of the two proposed SAMadaptations in TGP allows (i) a significant reduction of tree sizes in thepopulation and (ii) a decrease in redundancy of the trees. When assessed onreal-world benchmarks, the generalization ability of the elite solutions doesnot deteriorate.</description><author>Illya Bakurov, Nathan Haut, Wolfgang Banzhaf</author><pubDate>Fri, 17 May 2024 14:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10267v2</guid></item><item><title>Parameter Identification for Electrochemical Models of Lithium-Ion Batteries Using Bayesian Optimization</title><link>http://arxiv.org/abs/2405.10750v1</link><description>Efficient parameter identification of electrochemical models is crucial foraccurate monitoring and control of lithium-ion cells. This process becomeschallenging when applied to complex models that rely on a considerable numberof interdependent parameters that affect the output response. Gradient-basedand metaheuristic optimization techniques, although previously employed forthis task, are limited by their lack of robustness, high computational costs,and susceptibility to local minima. In this study, Bayesian Optimization isused for tuning the dynamic parameters of an electrochemical equivalent circuitbattery model (E-ECM) for a nickel-manganese-cobalt (NMC)-graphite cell. Theperformance of the Bayesian Optimization is compared with baseline methodsbased on gradient-based and metaheuristic approaches. The robustness of theparameter optimization method is tested by performing verification using anexperimental drive cycle. The results indicate that Bayesian Optimizationoutperforms Gradient Descent and PSO optimization techniques, achievingreductions on average testing loss by 28.8% and 5.8%, respectively. Moreover,Bayesian optimization significantly reduces the variance in testing loss by95.8% and 72.7%, respectively.</description><author>Jianzong Pi, Samuel Filgueira da Silva, Mehmet Fatih Ozkan, Abhishek Gupta, Marcello Canova</author><pubDate>Fri, 17 May 2024 13:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10750v1</guid></item><item><title>Deep Data Consistency: a Fast and Robust Diffusion Model-based Solver for Inverse Problems</title><link>http://arxiv.org/abs/2405.10748v1</link><description>Diffusion models have become a successful approach for solving various imageinverse problems by providing a powerful diffusion prior. Many studies tried tocombine the measurement into diffusion by score function replacement, matrixdecomposition, or optimization algorithms, but it is hard to balance the dataconsistency and realness. The slow sampling speed is also a main obstacle toits wide application. To address the challenges, we propose Deep DataConsistency (DDC) to update the data consistency step with a deep learningmodel when solving inverse problems with diffusion models. By analyzingexisting methods, the variational bound training objective is used to maximizethe conditional posterior and reduce its impact on the diffusion process. Incomparison with state-of-the-art methods in linear and non-linear tasks, DDCdemonstrates its outstanding performance of both similarity and realnessmetrics in generating high-quality solutions with only 5 inference steps in0.77 seconds on average. In addition, the robustness of DDC is well illustratedin the experiments across datasets, with large noise and the capacity to solvemultiple tasks in only one pre-trained model.</description><author>Hanyu Chen, Zhixiu Hao, Liying Xiao</author><pubDate>Fri, 17 May 2024 13:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10748v1</guid></item><item><title>Identifiability of total effects from abstractions of time series causal graphs</title><link>http://arxiv.org/abs/2310.14691v4</link><description>We study the problem of identifiability of the total effect of anintervention from observational time series in the situation, common inpractice, where one only has access to abstractions of the true causal graph.We consider here two abstractions: the extended summary causal graph, whichconflates all lagged causal relations but distinguishes between lagged andinstantaneous relations, and the summary causal graph which does not give anyindication about the lag between causal relations. We show that the totaleffect is always identifiable in extended summary causal graphs and providesufficient conditions for identifiability in summary causal graphs. Wefurthermore provide adjustment sets allowing to estimate the total effectwhenever it is identifiable.</description><author>Charles K. Assaad, Emilie Devijver, Eric Gaussier, Gregor Gössler, Anouar Meynaoui</author><pubDate>Fri, 17 May 2024 13:50:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14691v4</guid></item><item><title>Causality in the Can: Diet Coke's Impact on Fatness</title><link>http://arxiv.org/abs/2405.10746v1</link><description>Artificially sweetened beverages like Diet Coke are often consideredhealthier alternatives, but the debate over their impact on obesity persists.Previous research has predominantly relied on observational data or randomizedcontrolled trials (RCTs), which may not accurately capture the causalrelationship between Diet Coke consumption and obesity. This study uses causalinference methods, employing data from the National Health and NutritionExamination Survey (NHANES) to examine this relationship across diversedemographics. Instead of relying on RCT data, we constructed a causal graph andapplied the back-door criterion with its adjustment formula to estimate the RCTdistributions. We then calculated the counterfactual quantity, the Probabilityof Necessity and Sufficiency (PNS), using both NHANES data and estimated RCTdata. We propose that PNS is the essential metric for assessing the impact ofDiet Coke on obesity. Our results indicate that between 20% to 50% ofindividuals, especially those with poor dietary habits, are more likely to gainweight from Diet Coke. Conversely, in groups like young females with healthierdiets, only a small proportion experience weight gain due to Diet Coke. Thesefindings highlight the influence of individual lifestyle and potential hormonalfactors on the varied effects of Diet Coke, providing a new framework forunderstanding its nutritional impacts on health.</description><author>Yicheng Qi, Ang Li</author><pubDate>Fri, 17 May 2024 13:49:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10746v1</guid></item><item><title>Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings</title><link>http://arxiv.org/abs/2405.10745v1</link><description>Knowledge-intensive tasks pose a significant challenge for Machine Learning(ML) techniques. Commonly adopted methods, such as Large Language Models(LLMs), often exhibit limitations when applied to such tasks. Nevertheless,there have been notable endeavours to mitigate these challenges, with asignificant emphasis on augmenting LLMs through Knowledge Graphs (KGs). WhileKGs provide many advantages for representing knowledge, their development costscan deter extensive research and applications. Addressing this limitation, weintroduce a framework for enriching embeddings of small-scale domain-specificKnowledge Graphs with well-established general-purpose KGs. Adopting ourmethod, a modest domain-specific KG can benefit from a performance boost indownstream tasks when linked to a substantial general-purpose KG. Experimentalevaluations demonstrate a notable enhancement, with up to a 44% increaseobserved in the Hits@10 metric. This relatively unexplored research directioncan catalyze more frequent incorporation of KGs in knowledge-intensive tasks,resulting in more robust, reliable ML implementations, which hallucinates lessthan prevalent LLM solutions. Keywords: knowledge graph, knowledge graph completion, entity alignment,representation learning, machine learning</description><author>Albert Sawczyn, Jakub Binkowski, Piotr Bielak, Tomasz Kajdanowicz</author><pubDate>Fri, 17 May 2024 13:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10745v1</guid></item></channel></rss>