<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 08 Nov 2025 12:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Dark Energy Survey Year 3 results: Simulation-based $w$CDM inference from weak lensing and galaxy clustering maps with deep learning. I. Analysis design</title><link>http://arxiv.org/abs/2511.04681v1</link><description>Data-driven approaches using deep learning are emerging as powerfultechniques to extract non-Gaussian information from cosmological large-scalestructure. This work presents the first simulation-based inference (SBI)pipeline that combines weak lensing and galaxy clustering maps in a realisticDark Energy Survey Year 3 (DES Y3) configuration and serves as preparation fora forthcoming analysis of the survey data. We develop a scalable forward modelbased on the CosmoGridV1 suite of N-body simulations to generate over onemillion self-consistent mock realizations of DES Y3 at the map level.Leveraging this large dataset, we train deep graph convolutional neuralnetworks on the full survey footprint in spherical geometry to learnlow-dimensional features that approximately maximize mutual information withtarget parameters. These learned compressions enable neural density estimationof the implicit likelihood via normalizing flows in a ten-dimensional parameterspace spanning cosmological $w$CDM, intrinsic alignment, and linear galaxy biasparameters, while marginalizing over baryonic, photometric redshift, and shearbias nuisances. To ensure robustness, we extensively validate our inferencepipeline using synthetic observations derived from both systematiccontaminations in our forward model and independent Buzzard galaxy catalogs.Our forecasts yield significant improvements in cosmological parameterconstraints, achieving $2-3\times$ higher figures of merit in the $\Omega_m -S_8$ plane relative to our implementation of baseline two-point statistics andeffectively breaking parameter degeneracies through probe combination. Theseresults demonstrate the potential of SBI analyses powered by deep learning forupcoming Stage-IV wide-field imaging surveys.</description><author>A. Thomsen, J. Bucko, T. Kacprzak, V. Ajani, J. Fluri, A. Refregier, D. Anbajagane, F. J. Castander, A. Ferté, M. Gatti, N. Jeffrey, A. Alarcon, A. Amon, K. Bechtol, M. R. Becker, G. M. Bernstein, A. Campos, A. Carnero Rosell, C. Chang, R. Chen, A. Choi, M. Crocce, C. Davis, J. DeRose, S. Dodelson, C. Doux, K. Eckert, J. Elvin-Poole, S. Everett, P. Fosalba, D. Gruen, I. Harrison, K. Herner, E. M. Huff, M. Jarvis, N. Kuropatkin, P. -F. Leget, N. MacCrann, J. McCullough, J. Myles, A. Navarro-Alsina, S. Pandey, A. Porredon, J. Prat, M. Raveri, M. Rodriguez-Monroy, R. P. Rollins, A. Roodman, E. S. Rykoff, C. Sánchez, L. F. Secco, E. Sheldon, T. Shin, M. A. Troxel, I. Tutusaus, T. N. Varga, N. Weaverdyck, R. H. Wechsler, B. Yanny, B. Yin, Y. Zhang, J. Zuntz, S. Allam, F. Andrade-Oliveira, D. Ba</author><pubDate>Thu, 06 Nov 2025 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04681v1</guid></item><item><title>TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models</title><link>http://arxiv.org/abs/2505.23769v2</link><description>Image-text models excel at image-level tasks but struggle with detailedvisual understanding. While these models provide strong visual-languagealignment, segmentation models like SAM2 offer precise spatial boundaries forobjects. To this end, we propose TextRegion, a simple, effective, andtraining-free framework that combines the strengths of image-text models andSAM2 to generate powerful text-aligned region tokens. These tokens enabledetailed visual understanding while preserving open-vocabulary capabilities.They can be directly applied to various downstream tasks, including open-worldsemantic segmentation, referring expression comprehension, and grounding. Weconduct extensive evaluations and consistently achieve superior or competitiveperformance compared to state-of-the-art training-free methods. Additionally,our framework is compatible with many image-text models, making it highlypractical and easily extensible as stronger models emerge. Code is availableat: https://github.com/avaxiao/TextRegion.</description><author>Yao Xiao, Qiqian Fu, Heyi Tao, Yuqun Wu, Zhen Zhu, Derek Hoiem</author><pubDate>Thu, 06 Nov 2025 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.23769v2</guid></item><item><title>Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping</title><link>http://arxiv.org/abs/2511.04680v1</link><description>Automatic image cropping is a method for maximizing the human-perceivedquality of cropped regions in photographs. Although several works have proposedtechniques for producing singular crops, little work has addressed the problemof producing multiple, distinct crops with aesthetic appeal. In this paper, wemotivate the problem with a discussion on modern social media applications,introduce a dataset of 277 relevant images and human labels, and evaluate theefficacy of several single-crop models with an image partitioning algorithm asa pre-processing step. The dataset is available athttps://github.com/RafeLoya/carousel.</description><author>Rafe Loya, Andrew Hamara, Benjamin Estell, Benjamin Kilpatrick, Andrew C. Freeman</author><pubDate>Thu, 06 Nov 2025 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04680v1</guid></item><item><title>GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction</title><link>http://arxiv.org/abs/2511.04679v1</link><description>Humanoid robots are expected to operate in human-centered environments wheresafe and natural physical interaction is essential. However, most recentreinforcement learning (RL) policies emphasize rigid tracking and suppressexternal forces. Existing impedance-augmented approaches are typicallyrestricted to base or end-effector control and focus on resisting extremeforces rather than enabling compliance. We introduce GentleHumanoid, aframework that integrates impedance control into a whole-body motion trackingpolicy to achieve upper-body compliance. At its core is a unified spring-basedformulation that models both resistive contacts (restoring forces when pressingagainst surfaces) and guiding contacts (pushes or pulls sampled from humanmotion data). This formulation ensures kinematically consistent forces acrossthe shoulder, elbow, and wrist, while exposing the policy to diverseinteraction scenarios. Safety is further supported through task-adjustableforce thresholds. We evaluate our approach in both simulation and on theUnitree G1 humanoid across tasks requiring different levels of compliance,including gentle hugging, sit-to-stand assistance, and safe objectmanipulation. Compared to baselines, our policy consistently reduces peakcontact forces while maintaining task success, resulting in smoother and morenatural interactions. These results highlight a step toward humanoid robotsthat can safely and effectively collaborate with humans and handle objects inreal-world environments.</description><author>Qingzhou Lu, Yao Feng, Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu</author><pubDate>Thu, 06 Nov 2025 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04679v1</guid></item><item><title>Residual Kolmogorov-Arnold Network for Enhanced Deep Learning</title><link>http://arxiv.org/abs/2410.05500v4</link><description>Despite their immense success, deep convolutional neural networks (CNNs) canbe difficult to optimize and costly to train due to hundreds of layers withinthe network depth. Conventional convolutional operations are fundamentallylimited by their linear nature along with fixed activations, where many layersare needed to learn meaningful patterns in data. Because of the sheer size ofthese networks, this approach is simply computationally inefficient, and posesoverfitting or gradient explosion risks, especially in small datasets. As aresult, we introduce a "plug-in" module, called Residual Kolmogorov-ArnoldNetwork (RKAN). Our module is highly compact, so it can be easily added intoany stage (level) of traditional deep networks, where it learns to integratesupportive polynomial feature transformations to existing convolutionalframeworks. RKAN offers consistent improvements over baseline models indifferent vision tasks and widely tested benchmarks, accomplishing cutting-edgeperformance on them.</description><author>Ray Congrui Yu, Sherry Wu, Jiang Gui</author><pubDate>Thu, 06 Nov 2025 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05500v4</guid></item><item><title>Tracking and Understanding Object Transformations</title><link>http://arxiv.org/abs/2511.04678v1</link><description>Real-world objects frequently undergo state transformations. From an applebeing cut into pieces to a butterfly emerging from its cocoon, tracking throughthese changes is important for understanding real-world objects and dynamics.However, existing methods often lose track of the target object aftertransformation, due to significant changes in object appearance. To addressthis limitation, we introduce the task of Track Any State: tracking objectsthrough transformations while detecting and describing state changes,accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, wepresent TubeletGraph, a zero-shot system that recovers missing objects aftertransformation and maps out how object states are evolving over time.TubeletGraph first identifies potentially overlooked tracks, and determineswhether they should be integrated based on semantic and proximity priors. Then,it reasons about the added tracks and generates a state graph describing eachobserved transformation. TubeletGraph achieves state-of-the-art trackingperformance under transformations, while demonstrating deeper understanding ofobject transformations and promising capabilities in temporal grounding andsemantic reasoning for complex object transformations. Code, additionalresults, and the benchmark dataset are available athttps://tubelet-graph.github.io.</description><author>Yihong Sun, Xinyu Yang, Jennifer J. Sun, Bharath Hariharan</author><pubDate>Thu, 06 Nov 2025 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04678v1</guid></item><item><title>InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation</title><link>http://arxiv.org/abs/2511.04675v1</link><description>We introduce InfinityStar, a unified spacetime autoregressive framework forhigh-resolution image and dynamic video synthesis. Building on the recentsuccess of autoregressive modeling in both vision and language, our purelydiscrete approach jointly captures spatial and temporal dependencies within asingle architecture. This unified design naturally supports a variety ofgeneration tasks such as text-to-image, text-to-video, image-to-video, and longinteractive video synthesis via straightforward temporal autoregression.Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,outperforming all autoregressive models by large margins, even surpassing somediffusion competitors like HunyuanVideo. Without extra optimizations, our modelgenerates a 5s, 720p video approximately 10x faster than leadingdiffusion-based methods. To our knowledge, InfinityStar is the first discreteautoregressive video generator capable of producing industrial level 720pvideos. We release all code and models to foster further research in efficient,high-quality video generation.</description><author>Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu, Xing Wang, Yi Jiang, Bingyue Peng, Zehuan Yuan</author><pubDate>Thu, 06 Nov 2025 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04675v1</guid></item><item><title>Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</title><link>http://arxiv.org/abs/2506.15680v2</link><description>Modeling the dynamics of deformable objects is challenging due to theirdiverse physical properties and the difficulty of estimating states fromlimited visual information. We address these challenges with a neural dynamicsframework that combines object particles and spatial grids in a hybridrepresentation. Our particle-grid model captures global shape and motioninformation while predicting dense particle movements, enabling the modeling ofobjects with varied shapes and materials. Particles represent object shapes,while the spatial grid discretizes the 3D space to ensure spatial continuityand enhance learning efficiency. Coupled with Gaussian Splattings for visualrendering, our framework achieves a fully learning-based digital twin ofdeformable objects and generates 3D action-conditioned videos. Throughexperiments, we demonstrate that our model learns the dynamics of diverseobjects -- such as ropes, cloths, stuffed animals, and paper bags -- fromsparse-view RGB-D recordings of robot-object interactions, while alsogeneralizing at the category level to unseen instances. Our approachoutperforms state-of-the-art learning-based and physics-based simulators,particularly in scenarios with limited camera views. Furthermore, we showcasethe utility of our learned models in model-based planning, enablinggoal-conditioned object manipulation across a range of tasks. The project pageis available at https://kywind.github.io/pgnd .</description><author>Kaifeng Zhang, Baoyu Li, Kris Hauser, Yunzhu Li</author><pubDate>Thu, 06 Nov 2025 18:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.15680v2</guid></item><item><title>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences</title><link>http://arxiv.org/abs/2509.16189v2</link><description>When do machine learning systems fail to generalize, and what mechanismscould improve their generalization? Here, we draw inspiration from cognitivescience to argue that one weakness of parametric machine learning systems istheir failure to exhibit latent learning -- learning information that is notrelevant to the task at hand, but that might be useful in a future task. Weshow how this perspective links failures ranging from the reversal curse inlanguage modeling to new findings on agent-based navigation. We then highlighthow cognitive science points to episodic memory as a potential part of thesolution to these issues. Correspondingly, we show that a system with an oracleretrieval mechanism can use learning experiences more flexibly to generalizebetter across many of these challenges. We also identify some of the essentialcomponents for effectively using retrieval, including the importance ofwithin-example in-context learning for acquiring the ability to use informationacross retrieved examples. In summary, our results illustrate one possiblecontributor to the relative data inefficiency of current machine learningsystems compared to natural intelligence, and help to understand how retrievalmethods can complement parametric learning to improve generalization. We closeby discussing some of the links between these findings and prior results incognitive science and neuroscience, and the broader implications.</description><author>Andrew Kyle Lampinen, Martin Engelcke, Yuxuan Li, Arslan Chaudhry, James L. McClelland</author><pubDate>Thu, 06 Nov 2025 18:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.16189v2</guid></item><item><title>X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations</title><link>http://arxiv.org/abs/2511.04671v1</link><description>Human videos can be recorded quickly and at scale, making them an appealingsource of training data for robot learning. However, humans and robots differfundamentally in embodiment, resulting in mismatched action execution. Directkinematic retargeting of human hand motion can therefore produce actions thatare physically infeasible for robots. Despite these low-level differences,human demonstrations provide valuable motion cues about how to manipulate andinteract with objects. Our key idea is to exploit the forward diffusionprocess: as noise is added to actions, low-level execution differences fadewhile high-level task guidance is preserved. We present X-Diffusion, aprincipled framework for training diffusion policies that maximally leverageshuman data without learning dynamically infeasible motions. X-Diffusion firsttrains a classifier to predict whether a noisy action is executed by a human orrobot. Then, a human action is incorporated into policy training only afteradding sufficient noise such that the classifier cannot discern its embodiment.Actions consistent with robot execution supervise fine-grained denoising at lownoise levels, while mismatched human actions provide only coarse guidance athigher noise levels. Our experiments show that naive co-training underexecution mismatches degrades policy performance, while X-Diffusionconsistently improves it. Across five manipulation tasks, X-Diffusion achievesa 16% higher average success rate than the best baseline. The project websiteis available at https://portal-cornell.github.io/X-Diffusion/.</description><author>Maximus A. Pace, Prithwish Dan, Chuanruo Ning, Atiksh Bhardwaj, Audrey Du, Edward W. Duan, Wei-Chiu Ma, Kushal Kedia</author><pubDate>Thu, 06 Nov 2025 18:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04671v1</guid></item><item><title>Cambrian-S: Towards Spatial Supersensing in Video</title><link>http://arxiv.org/abs/2511.04670v1</link><description>We argue that progress in true multimodal intelligence calls for a shift fromreactive, task-driven systems and brute-force long context towards a broaderparadigm of supersensing. We frame spatial supersensing as four stages beyondlinguistic-only understanding: semantic perception (naming what is seen),streaming event cognition (maintaining memory across continuous experiences),implicit 3D spatial cognition (inferring the world behind pixels), andpredictive world modeling (creating internal models that filter and organizeinformation). Current benchmarks largely test only the early stages, offeringnarrow coverage of spatial cognition and rarely challenging models in ways thatrequire true world modeling. To drive progress in spatial supersensing, wepresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatialrecall) and VSC (continual visual spatial counting). These tasks requirearbitrarily long video inputs yet are resistant to brute-force contextexpansion. We then test data scaling limits by curating VSI-590K and trainingCambrian-S, achieving +30% absolute improvement on VSI-Bench withoutsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,indicating that scale alone is insufficient for spatial supersensing. Wepropose predictive sensing as a path forward, presenting a proof-of-concept inwhich a self-supervised next-latent-frame predictor leverages surprise(prediction error) to drive memory and event segmentation. On VSI-SUPER, thisapproach substantially outperforms leading proprietary baselines, showing thatspatial supersensing requires models that not only see but also anticipate,select, and organize experience.</description><author>Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie</author><pubDate>Thu, 06 Nov 2025 18:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04670v1</guid></item><item><title>SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</title><link>http://arxiv.org/abs/2511.04668v1</link><description>Despite impressive high-level video comprehension, multimodal language modelsstruggle with spatial reasoning across time and space. While current spatialtraining approaches rely on real-world video data, obtaining diverse footagewith precise spatial annotations remains a bottleneck. To alleviate thisbottleneck, we present SIMS-V -- a systematic data-generation framework thatleverages the privileged information of 3D simulators to create spatially-richvideo training data for multimodal language models. Using this framework, weinvestigate which properties of simulated data drive effective real-worldtransfer through systematic ablations of question types, mixes, and scales. Weidentify a minimal set of three question categories (metric measurement,perspective-dependent reasoning, and temporal tracking) that prove mosteffective for developing transferable spatial intelligence, outperformingcomprehensive coverage despite using fewer question types. These insightsenable highly efficient training: our 7B-parameter video LLM fine-tuned on just25K simulated examples outperforms the larger 72B baseline and achievescompetitive performance with proprietary models on rigorous real-world spatialreasoning benchmarks. Our approach demonstrates robust generalization,maintaining performance on general video understanding while showingsubstantial improvements on embodied and real-world spatial tasks.</description><author>Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie</author><pubDate>Thu, 06 Nov 2025 18:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04668v1</guid></item><item><title>Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches</title><link>http://arxiv.org/abs/2511.04667v1</link><description>This study evaluates a 40-item mathematics placement examination administeredto 198 students using a multi-method framework combining Classical Test Theory,machine learning, and unsupervised clustering. Classical Test Theory analysisreveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$)while 30\% demonstrate poor discrimination ($D &lt; 0.20$) requiring replacement.Question 6 (Graph Interpretation) emerges as the examination's most powerfuldiscriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVAF-statistic ($F = 4609.1$), and maximum Random Forest feature importance(0.206), accounting for 20.6\% of predictive power. Machine learning algorithmsdemonstrate exceptional performance, with Random Forest and Gradient Boostingachieving 97.5\% and 96.0\% cross-validation accuracy. K-means clusteringidentifies a natural binary competency structure with a boundary at 42.5\%,diverging from the institutional threshold of 55\% and suggesting potentialoverclassification into remedial categories. The two-cluster solution exhibitsexceptional stability (bootstrap ARI = 0.855) with perfect lower-clusterpurity. Convergent evidence across methods supports specific refinements:replace poorly discriminating items, implement a two-stage assessment, andintegrate Random Forest predictions with transparency mechanisms. Thesefindings demonstrate that multi-method integration provides a robust empiricalfoundation for evidence-based mathematics placement optimization.</description><author>Julian D. Allagan, Dasia A. Singleton, Shanae N. Perry, Gabrielle C. Morgan, Essence A. Morgan</author><pubDate>Thu, 06 Nov 2025 18:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04667v1</guid></item><item><title>Forgetting is Everywhere</title><link>http://arxiv.org/abs/2511.04666v1</link><description>A fundamental challenge in developing general learning algorithms is theirtendency to forget past knowledge when adapting to new data. Addressing thisproblem requires a principled understanding of forgetting; yet, despite decadesof study, no unified definition has emerged that provides insights into theunderlying dynamics of learning. We propose an algorithm- and task-agnostictheory that characterises forgetting as a lack of self-consistency in alearner's predictive distribution over future experiences, manifesting as aloss of predictive information. Our theory naturally yields a general measureof an algorithm's propensity to forget. To validate the theory, we design acomprehensive set of experiments that span classification, regression,generative modelling, and reinforcement learning. We empirically demonstratehow forgetting is present across all learning settings and plays a significantrole in determining learning efficiency. Together, these results establish aprincipled understanding of forgetting and lay the foundation for analysing andimproving the information retention capabilities of general learningalgorithms.</description><author>Ben Sanati, Thomas L. Lee, Trevor McInroe, Aidan Scannell, Nikolay Malkin, David Abel, Amos Storkey</author><pubDate>Thu, 06 Nov 2025 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04666v1</guid></item><item><title>Distillation versus Contrastive Learning: How to Train Your Rerankers</title><link>http://arxiv.org/abs/2507.08336v3</link><description>Training effective text rerankers is crucial for information retrieval. Twostrategies are widely used: contrastive learning (optimizing directly onground-truth labels) and knowledge distillation (transferring knowledge from alarger reranker). While both have been studied extensively, a clear comparisonof their effectiveness for training cross-encoder rerankers under practicalconditions is needed. This paper empirically compares these strategies by training rerankers ofdifferent sizes (0.5B, 1.5B, 3B, 7B) and architectures (Transformer, Recurrent)using both methods on the same data, with a strong contrastive learning modelacting as the distillation teacher. Our results show that knowledgedistillation generally yields better in-domain and out-of-domain rankingperformance than contrastive learning when distilling from a more performantteacher model. This finding is consistent across student model sizes andarchitectures. However, distilling from a teacher of the same capacity does notprovide the same advantage, particularly for out-of-domain tasks. Thesefindings offer practical guidance for choosing a training strategy based onavailable teacher models. We recommend using knowledge distillation to trainsmaller rerankers if a larger, more performant teacher is accessible; in itsabsence, contrastive learning remains a robust baseline. Our codeimplementation is made available to facilitate reproducbility.</description><author>Zhichao Xu, Zhiqi Huang, Shengyao Zhuang, Vivek Srikumar</author><pubDate>Thu, 06 Nov 2025 18:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.08336v3</guid></item><item><title>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</title><link>http://arxiv.org/abs/2511.04665v1</link><description>Robotic manipulation policies are advancing rapidly, but their directevaluation in the real world remains costly, time-consuming, and difficult toreproduce, particularly for tasks involving deformable objects. Simulationprovides a scalable and systematic alternative, yet existing simulators oftenfail to capture the coupled visual and physical complexity of soft-bodyinteractions. We present a real-to-sim policy evaluation framework thatconstructs soft-body digital twins from real-world videos and renders robots,objects, and environments with photorealistic fidelity using 3D GaussianSplatting. We validate our approach on representative deformable manipulationtasks, including plush toy packing, rope routing, and T-block pushing,demonstrating that simulated rollouts correlate strongly with real-worldexecution performance and reveal key behavioral patterns of learned policies.Our results suggest that combining physics-informed reconstruction withhigh-quality rendering enables reproducible, scalable, and accurate evaluationof robotic manipulation policies. Website: https://real2sim-eval.github.io/</description><author>Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, Yunzhu Li</author><pubDate>Thu, 06 Nov 2025 18:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04665v1</guid></item><item><title>VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks</title><link>http://arxiv.org/abs/2511.04662v1</link><description>LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), butthey cannot reliably verify their own logic. Even when they reach correctanswers, the underlying reasoning may be flawed, undermining trust inhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, aneuro-symbolic method that extracts and verifies formal logical arguments fromCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-orderlogic and identifies premises that ground the argument in source context,commonsense knowledge, or prior reasoning steps. The symbolic representationenables automated solvers to verify logical validity while the NL premisesallow humans and systems to identify ungrounded or fallacious reasoning steps.Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoTeffectively identifies flawed reasoning, and serves as a strong predictor offinal answer correctness. We also leverage VeriCoT's verification signal for(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) onVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with directpreference optimization (DPO) using verification-based pairwise rewards,further improving reasoning validity and accuracy.</description><author>Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel, Sapana Chaudhary, Benjamin Kiesl-Reiter, Huzefa Rangwala</author><pubDate>Thu, 06 Nov 2025 18:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04662v1</guid></item><item><title>Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions</title><link>http://arxiv.org/abs/2509.08217v2</link><description>For machine learning datasets to accurately represent diverse opinions in apopulation, they must preserve variation in data labels while filtering outspam or low-quality responses. How can we balance annotator reliability andrepresentation? We empirically evaluate how a range of heuristics for annotatorfiltering affect the preservation of variation on subjective tasks. We findthat these methods, designed for contexts in which variation from a singleground-truth label is considered noise, often remove annotators who disagreeinstead of spam annotators, introducing suboptimal tradeoffs between accuracyand label diversity. We find that conservative settings for annotator removal(&lt;5%) are best, after which all tested methods increase the mean absolute errorfrom the true average label. We analyze performance on synthetic spam toobserve that these methods often assume spam annotators are more random thanreal spammers tend to be: most spammers are distributionally indistinguishablefrom real annotators, and the minority that are distinguishable tend to giverelatively fixed answers, not random ones. Thus, tasks requiring thepreservation of variation reverse the intuition of existing spam filteringmethods: spammers tend to be less random than non-spammers, so metrics thatassume variation is spam fare worse. These results highlight the need for spamremoval methods that account for label diversity.</description><author>Eve Fleisig, Matthias Orlikowski, Philipp Cimiano, Dan Klein</author><pubDate>Thu, 06 Nov 2025 18:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.08217v2</guid></item><item><title>CREA: A Collaborative Multi-Agent Framework for Creative Image Editing and Generation</title><link>http://arxiv.org/abs/2504.05306v2</link><description>Creativity in AI imagery remains a fundamental challenge, requiring not onlythe generation of visually compelling content but also the capacity to addnovel, expressive, and artistically rich transformations to images. Unlikeconventional editing tasks that rely on direct prompt-based modifications,creative image editing requires an autonomous, iterative approach that balancesoriginality, coherence, and artistic intent. To address this, we introduceCREA, a novel multi-agent collaborative framework that mimics the humancreative process. Our framework leverages a team of specialized AI agents whodynamically collaborate to conceptualize, generate, critique, and enhanceimages. Through extensive qualitative and quantitative evaluations, wedemonstrate that CREA significantly outperforms state-of-the-art methods indiversity, semantic alignment, and creative transformation. To the best of ourknowledge, this is the first work to introduce the task of creative editing.</description><author>Kavana Venkatesh, Connor Dunlop, Pinar Yanardag</author><pubDate>Thu, 06 Nov 2025 18:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05306v2</guid></item><item><title>Nowcast3D: Reliable precipitation nowcasting via gray-box learning</title><link>http://arxiv.org/abs/2511.04659v1</link><description>Extreme precipitation nowcasting demands high spatiotemporal fidelity andextended lead times, yet existing approaches remain limited. Numerical WeatherPrediction (NWP) and its deep-learning emulations are too slow and coarse forrapidly evolving convection, while extrapolation and purely data-driven modelssuffer from error accumulation and excessive smoothing. Hybrid 2D radar-basedmethods discard crucial vertical information, preventing accuratereconstruction of height-dependent dynamics. We introduce a gray-box, fullythree-dimensional nowcasting framework that directly processes volumetric radarreflectivity and couples physically constrained neural operators withdatadriven learning. The model learns vertically varying 3D advection fieldsunder a conservative advection operator, parameterizes spatially varyingdiffusion, and introduces a Brownian-motion--inspired stochastic term torepresent unresolved motions. A residual branch captures small-scale convectiveinitiation and microphysical variability, while a diffusion-based stochasticmodule estimates uncertainty. The framework achieves more accurate forecasts upto three-hour lead time across precipitation regimes and ranked first in 57\%of cases in a blind evaluation by 160 meteorologists. By restoring full 3Ddynamics with physical consistency, it offers a scalable and robust pathway forskillful and reliable nowcasting of extreme precipitation.</description><author>Huaguan Chen, Wei Han, Haofei Sun, Ning Lin, Xingtao Song, Yunfan Yang, Jie Tian, Yang Liu, Ji-Rong Wen, Xiaoye Zhang, Xueshun Shen, Hao Sun</author><pubDate>Thu, 06 Nov 2025 18:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04659v1</guid></item><item><title>Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts</title><link>http://arxiv.org/abs/2511.04655v1</link><description>Robust benchmarks are crucial for evaluating Multimodal Large Language Models(MLLMs). Yet we find that models can ace many multimodal benchmarks withoutstrong visual understanding, instead exploiting biases, linguistic priors, andsuperficial patterns. This is especially problematic for vision-centricbenchmarks that are meant to require visual inputs. We adopt a diagnosticprinciple for benchmark design: if a benchmark can be gamed, it will be.Designers should therefore try to ``game'' their own benchmarks first, usingdiagnostic and debiasing procedures to systematically identify and mitigatenon-visual biases. Effective diagnosis requires directly ``training on the testset'' -- probing the released test set for its intrinsic, exploitable patterns. We operationalize this standard with two components. First, we diagnosebenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.Our primary diagnostic tool involves fine-tuning a powerful Large LanguageModel via $k$-fold cross-validation on exclusively the non-visual, textualinputs of the test set to reveal shortcut performance and assign each sample abias score $s(x)$. We complement this with a lightweight Random Forest-baseddiagnostic operating on hand-crafted features for fast, interpretable auditing.Second, we debias benchmarks by filtering high-bias samples using an``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to fourbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasivenon-visual biases. As a case study, we apply our full framework to createVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a widervision-blind performance gap than the original.</description><author>Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie</author><pubDate>Thu, 06 Nov 2025 18:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04655v1</guid></item><item><title>Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2511.04654v1</link><description>Chain-of-Thought (CoT) prompting is a key technique for enabling complexreasoning in large language models. However, generating full, fixed-lengthrationales is computationally wasteful, inflating both token usage and latency.We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-freedecoding algorithm that adaptively halts rationale generation. LEASH monitorstwo intrinsic signals: the slope of token-level entropy and the improvement inthe top-logit margin. It terminates the generation once both signals plateau,indicating the model has reached a stable reasoning state. Across fourinstruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reducesaverage token generation by 30--35% and latency by 27%, while incurring a 10p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires noadditional training or supervision, offering a simple and efficient alternativeto CoT decoding.</description><author>Mohammad Atif Quamar, Mohammad Areeb</author><pubDate>Thu, 06 Nov 2025 18:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04654v1</guid></item><item><title>TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning</title><link>http://arxiv.org/abs/2511.04653v1</link><description>Federated learning (FL) offers new opportunities in machine learning,particularly in addressing data privacy concerns. In contrast to conventionalevent-based federated learning, time-triggered federated learning (TT-Fed), asa general form of both asynchronous and synchronous FL, clusters users intodifferent tiers based on fixed time intervals. However, the FL network consistsof a growing number of user devices with limited wireless bandwidth,consequently magnifying issues such as stragglers and communication overhead.In this paper, we introduce adaptive model pruning to wireless TT-Fed systemsand study the problem of jointly optimizing the pruning ratio and bandwidthallocation to minimize the training loss while ensuring minimal learninglatency. To answer this question, we perform convergence analysis on thegradient l_2 norm of the TT-Fed model based on model pruning. Based on theobtained convergence upper bound, a joint optimization problem of pruning ratioand wireless bandwidth is formulated to minimize the model training loss undera given delay threshold. Then, we derive closed-form solutions for wirelessbandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. Thesimulation results show that model pruning could reduce the communication costby 40% while maintaining the model performance at the same level.</description><author>Xinlu Zhang, Yansha Deng, Toktam Mahmoodi</author><pubDate>Thu, 06 Nov 2025 18:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04653v1</guid></item><item><title>Polarization-resolved imaging improves eye tracking</title><link>http://arxiv.org/abs/2511.04652v1</link><description>Polarization-resolved near-infrared imaging adds a useful optical contrastmechanism to eye tracking by measuring the polarization state of lightreflected by ocular tissues in addition to its intensity. In this paper wedemonstrate how this contrast can be used to enable eye tracking. Specifically,we demonstrate that a polarization-enabled eye tracking (PET) system composedof a polarization--filter--array camera paired with a linearly polarizednear-infrared illuminator can reveal trackable features across the sclera andgaze-informative patterns on the cornea, largely absent in intensity-onlyimages. Across a cohort of 346 participants, convolutional neural network basedmachine learning models trained on data from PET reduced the median95th-percentile absolute gaze error by 10--16\% relative to capacity-matchedintensity baselines under nominal conditions and in the presence of eyelidocclusions, eye-relief changes, and pupil-size variation. These results linklight--tissue polarization effects to practical gains in human--computerinteraction and position PET as a simple, robust sensing modality for futurewearable devices.</description><author>Mantas Žurauskas, Tom Bu, Sanaz Alali, Beyza Kalkanli, Derek Shi, Fernando Alamos, Gauresh Pandit, Christopher Mei, Ali Behrooz, Ramin Mirjalili, Dave Stronks, Alexander Fix, Dmitri Model</author><pubDate>Thu, 06 Nov 2025 18:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04652v1</guid></item><item><title>CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation</title><link>http://arxiv.org/abs/2509.07325v2</link><description>The National Comprehensive Cancer Network (NCCN) provides evidence-basedguidelines for cancer treatment. Translating complex patient presentations intoguideline-compliant treatment recommendations is time-intensive, requiresspecialized expertise, and is prone to error. Advances in large language model(LLM) capabilities promise to reduce the time required to generate treatmentrecommendations and improve accuracy. We present an LLM agent-based approach toautomatically generate guideline-concordant treatment trajectories for patientswith non-small cell lung cancer (NSCLC). Our contributions are threefold.First, we construct a novel longitudinal dataset of 121 cases of NSCLC patientsthat includes clinical encounters, diagnostic results, and medical histories,each expertly annotated with the corresponding NCCN guideline trajectories byboard-certified oncologists. Second, we demonstrate that existing LLMs possessdomain-specific knowledge that enables high-quality proxy benchmark generationfor both model development and evaluation, achieving strong correlation(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.Third, we develop a hybrid approach combining expensive human annotations withmodel consistency information to create both the agent framework that predictsthe relevant guidelines for a patient, as well as a meta-classifier thatverifies prediction accuracy with calibrated confidence scores for treatmentrecommendations (AUROC=0.800), a critical capability for communicating theaccuracy of outputs, custom-tailoring tradeoffs in performance, and supportingregulatory compliance. This work establishes a framework for clinically viableLLM-based guideline adherence systems that balance accuracy, interpretability,and regulatory requirements while reducing annotation costs, providing ascalable pathway toward automated clinical decision support.</description><author>Alyssa Unell, Noel C. F. Codella, Sam Preston, Peniel Argaw, Wen-wai Yim, Zelalem Gero, Cliff Wong, Rajesh Jena, Eric Horvitz, Amanda K. Hall, Ruican Rachel Zhong, Jiachen Li, Shrey Jain, Mu Wei, Matthew Lungren, Hoifung Poon</author><pubDate>Thu, 06 Nov 2025 18:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07325v2</guid></item><item><title>Optimal Inference Schedules for Masked Diffusion Models</title><link>http://arxiv.org/abs/2511.04647v1</link><description>A major bottleneck of standard auto-regressive large language models is thattheir inference process is inherently sequential, resulting in very long andcostly inference times. To circumvent this, practitioners proposed a class oflanguage models called diffusion language models, of which the masked diffusionmodel (MDM) is the most successful. The MDM is able to sample tokensout-of-order and, ostensibly, many tokens at once and in parallel. However,there is very limited rigorous understanding of how much parallel samplingthese models can perform without noticeable degradation in their samplingperformance. Prior work of Li and Cai obtained some preliminary bounds, butthese are not tight for many natural classes of distributions. In this work, wegive a new, exact characterization of the expected divergence between the truedistribution and the sampled distribution, for any distribution and anyunmasking schedule for the sampler, showing an elegant connection to the theoryof univariate function approximation. By leveraging this connection, we then attain a number of novel lower andupper bounds for this problem. While the connection to function approximationin principle gives the optimal unmasking schedule for any distribution, we showthat it is in general impossible to compete with it without strong a prioriknowledge of the distribution, even in seemingly benign settings. However, wealso demonstrate new upper bounds and new sampling schedules in terms ofwell-studied information-theoretic properties of the base distribution, namely,its total correlation and dual total correlation, which show that in somenatural settings, one can sample in $O(log n)$ steps without any visible lossin performance, where $n$ is the total sequence length.</description><author>Sitan Chen, Kevin Cong, Jerry Li</author><pubDate>Thu, 06 Nov 2025 18:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04647v1</guid></item><item><title>DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2511.04646v1</link><description>Cooperative multi-agent planning requires agents to make joint decisions withpartial information and limited communication. Coordination at the trajectorylevel often fails, as small deviations in timing or movement cascade intoconflicts. Symbolic planning mitigates this challenge by raising the level ofabstraction and providing a minimal vocabulary of actions that enablesynchronization and collective progress. We present DR. WELL, a decentralizedneurosymbolic framework for cooperative multi-agent planning. Cooperationunfolds through a two-phase negotiation protocol: agents first proposecandidate roles with reasoning and then commit to a joint allocation underconsensus and environment constraints. After commitment, each agentindependently generates and executes a symbolic plan for its role withoutrevealing detailed trajectories. Plans are grounded in execution outcomes via ashared world model that encodes the current state and is updated as agents act.By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoidsbrittle step-level alignment and enables higher-level operations that arereusable, synchronizable, and interpretable. Experiments on cooperativeblock-push tasks show that agents adapt across episodes, with the dynamic worldmodel capturing reusable patterns and improving task completion rates andefficiency. Experiments on cooperative block-push tasks show that our dynamicworld model improves task completion and efficiency through negotiation andself-refinement, trading a time overhead for evolving, more efficientcollaboration strategies.</description><author>Narjes Nourzad, Hanqing Yang, Shiyu Chen, Carlee Joe-Wong</author><pubDate>Thu, 06 Nov 2025 18:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04646v1</guid></item><item><title>When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection</title><link>http://arxiv.org/abs/2511.04643v1</link><description>The proliferation of misinformation necessitates robust yet computationallyefficient fact verification systems. While current state-of-the-art approachesleverage Large Language Models (LLMs) for generating explanatory rationales,these methods face significant computational barriers and hallucination risksin real-world deployments. We present DeReC (Dense Retrieval Classification), alightweight framework that demonstrates how general-purpose text embeddings caneffectively replace autoregressive LLM-based approaches in fact verificationtasks. By combining dense retrieval with specialized classification, our systemachieves better accuracy while being significantly more efficient. DeReCoutperforms explanation-generating LLMs in efficiency, reducing runtime by 95%on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),showcasing its effectiveness across varying dataset sizes. On the RAWFCdataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-artmethod L-Defense (61.20%). Our results demonstrate that carefully engineeredretrieval-based systems can match or exceed LLM performance in specializedtasks while being significantly more practical for real-world deployment.</description><author>Alamgir Munir Qazi, John P. McCrae, Jamal Abdul Nasir</author><pubDate>Thu, 06 Nov 2025 18:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04643v1</guid></item><item><title>Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering</title><link>http://arxiv.org/abs/2511.04499v1</link><description>As Large Language Models (LLMs) become integral to human-centeredapplications, understanding their personality-like behaviors is increasinglyimportant for responsible development and deployment. This paper systematicallyevaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, toassess trait expressions under varying sampling temperatures. We findsignificant differences across four of the five personality dimensions, withNeuroticism and Extraversion susceptible to temperature adjustments. Further,hierarchical clustering reveals distinct model clusters, suggesting thatarchitectural features may predispose certain models toward stable traitprofiles. Taken together, these results offer new insights into the emergenceof personality-like patterns in LLMs and provide a new perspective on modeltuning, selection, and the ethical governance of AI systems. We share the dataand code for this analysis here:https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1</description><author>Christos-Nikolaos Zacharopoulos, Revekka Kyriakoglou</author><pubDate>Thu, 06 Nov 2025 16:20:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04499v1</guid></item><item><title>MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection</title><link>http://arxiv.org/abs/2507.17978v2</link><description>Phishing emails continue to pose a significant threat to cybersecurity byexploiting human vulnerabilities through deceptive content and maliciouspayloads. While Machine Learning (ML) models are effective at detectingphishing threats, their performance largely relies on the quality and diversityof the training data. This paper presents MeAJOR (Merged email Assets fromJoint Open-source Repositories) Corpus, a novel, multi-source phishing emaildataset designed to overcome critical limitations in existing resources. Itintegrates 135894 samples representing a broad number of phishing tactics andlegitimate emails, with a wide spectrum of engineered features. We evaluatedthe dataset's utility for phishing detection research through systematicexperiments with four classification models (RF, XGB, MLP, and CNN) acrossmultiple feature configurations. Results highlight the dataset's effectiveness,achieving 98.34% F1 with XGB. By integrating broad features from multiplecategories, our dataset provides a reusable and consistent resource, whileaddressing common challenges like class imbalance, generalisability andreproducibility.</description><author>Paulo Mendes, Eva Maia, Isabel Praça</author><pubDate>Thu, 06 Nov 2025 16:20:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.17978v2</guid></item><item><title>OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation</title><link>http://arxiv.org/abs/2511.04495v1</link><description>This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task(Alva-Manchego et al., 2025), designed for readability-controlled textsimplification using LLM-prompting-based generation. Based on the analysis ofprompt-based text simplification methods, we discovered an interesting findingthat text simplification performance is highly related to the gap between thesource CEFR (Arase et al., 2022) level and the target CEFR level. Inspired bythis finding, we propose two multi-round simplification methods and generatethem via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-basedLLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.Later improvements with MRS-Joint show that taking the LLM simplifiedcandidates as the starting point could further boost the multi-roundsimplification performance.</description><author>Cuong Huynh, Jie Cao</author><pubDate>Thu, 06 Nov 2025 16:16:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04495v1</guid></item><item><title>LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users</title><link>http://arxiv.org/abs/2406.17737v2</link><description>While state-of-the-art large language models (LLMs) have shown impressiveperformance on many tasks, there has been extensive research on undesirablemodel behavior such as hallucinations and bias. In this work, we investigatehow the quality of LLM responses changes in terms of information accuracy,truthfulness, and refusals depending on three user traits: English proficiency,education level, and country of origin. We present extensive experimentation onthree state-of-the-art LLMs and two different datasets targeting truthfulnessand factuality. Our findings suggest that undesirable behaviors instate-of-the-art LLMs occur disproportionately more for users with lowerEnglish proficiency, of lower education status, and originating from outsidethe US, rendering these models unreliable sources of information towards theirmost vulnerable users.</description><author>Elinor Poole-Dayan, Deb Roy, Jad Kabbara</author><pubDate>Thu, 06 Nov 2025 16:16:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17737v2</guid></item><item><title>Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks</title><link>http://arxiv.org/abs/2511.04494v1</link><description>Neural networks are widely used for image-related tasks but typically demandconsiderable computing power. Once a network has been trained, however, itsmemory- and compute-footprint can be reduced by compression. In this work, wefocus on compression through tensorization and low-rank representations.Whereas classical approaches search for a low-rank approximation by minimizingan isotropic norm such as the Frobenius norm in weight-space, we usedata-informed norms that measure the error in function space. Concretely, weminimize the change in the layer's output distribution, which can be expressedas $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ isthe square root of the covariance matrix of the layer's input and $W$,$\widetilde{W}$ are the original and compressed weights. We propose newalternating least square algorithms for the two most common tensordecompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlikeconventional compression pipelines, which almost always requirepost-compression fine-tuning, our data-informed approach often achievescompetitive accuracy without any fine-tuning. We further show that the samecovariance-based norm can be transferred from one dataset to another with onlya minor accuracy drop, enabling compression even when the original trainingdataset is unavailable. Experiments on several CNN architectures (ResNet-18/50,and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100)confirm the advantages of the proposed method.</description><author>Alper Kalle, Theo Rudkiewicz, Mohamed-Oumar Ouerfelli, Mohamed Tamaazousti</author><pubDate>Thu, 06 Nov 2025 16:15:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04494v1</guid></item><item><title>Optimized Minimal 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2503.16924v2</link><description>3D Gaussian Splatting (3DGS) has emerged as a powerful representation forreal-time, high-performance rendering, enabling a wide range of applications.However, representing 3D scenes with numerous explicit Gaussian primitivesimposes significant storage and memory overhead. Recent studies have shown thathigh-quality rendering can be achieved with a substantially reduced number ofGaussians when represented with high-precision attributes. Nevertheless,existing 3DGS compression methods still rely on a relatively large number ofGaussians, focusing primarily on attribute compression. This is because asmaller set of Gaussians becomes increasingly sensitive to lossy attributecompression, leading to severe quality degradation. Since the number ofGaussians is directly tied to computational costs, it is essential to reducethe number of Gaussians effectively rather than only optimizing storage. Inthis paper, we propose Optimized Minimal Gaussians representation (OMG), whichsignificantly reduces storage while using a minimal number of primitives.First, we determine the distinct Gaussian from the near ones, minimizingredundancy without sacrificing quality. Second, we propose a compact andprecise attribute representation that efficiently captures both continuity andirregularity among primitives. Additionally, we propose a sub-vectorquantization technique for improved irregularity representation, maintainingfast training with a negligible codebook size. Extensive experimentsdemonstrate that OMG reduces storage requirements by nearly 50% compared to theprevious state-of-the-art and enables 600+ FPS rendering while maintaining highrendering quality. Our source code is available athttps://maincold2.github.io/omg/.</description><author>Joo Chan Lee, Jong Hwan Ko, Eunbyung Park</author><pubDate>Thu, 06 Nov 2025 16:12:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.16924v2</guid></item><item><title>RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables</title><link>http://arxiv.org/abs/2511.04491v1</link><description>Existing tabular reasoning benchmarks mostly test models on small, uniformtables, underrepresenting the complexity of real-world data and giving anincomplete view of Large Language Models' (LLMs) reasoning abilities. Realtables are long, heterogeneous, and domain-specific, mixing structured fieldswith free text and requiring multi-hop reasoning across thousands of tokens. Toaddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluatesLLMs jointly across scale, heterogeneity, domain specificity, and reasoningcomplexity. Experiments with open-source and proprietary models show that LLMsstruggle with heterogeneous schemas and complex multi-hop inference, revealingpersistent weaknesses in current architectures and prompting strategies.RUST-BENCH establishes a challenging new testbed for advancing tabularreasoning research.</description><author>Nikhil Abhyankar, Purvi Chaurasia, Sanchit Kabra, Ananya Srivastava, Vivek Gupta, Chandan K. Reddy</author><pubDate>Thu, 06 Nov 2025 16:10:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04491v1</guid></item><item><title>Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training</title><link>http://arxiv.org/abs/2511.04485v1</link><description>Parameter-efficient training, based on low-rank optimization, has become ahighly successful tool for fine-tuning large deep-learning models. However,these methods fail at low-rank pre-training tasks where maintaining thelow-rank structure and the objective remains a challenging task. We propose theQuadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novellow-rank inducing training strategy inspired by the iteratively reweightedleast squares (IRLS) framework. Q3R is based on a quadratic regularizer termwhich majorizes a smoothed log determinant serving as rank surrogate objective.Unlike other low-rank training techniques, Q3R is able to train weight matriceswith prescribed, low target ranks of models that achieve comparable predictiveperformance as dense models, with small computational overhead, while remainingfully compatible with existing architectures. For example, we demonstrated oneexperiment where we are able to truncate $60\%$ and $80\%$ of the parameters ofa ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performancerespectively. The efficacy of Q3R is confirmed on Transformers across bothimage and language tasks, including for low-rank fine-tuning.</description><author>Ipsita Ghosh, Ethan Nguyen, Christian Kümmerle</author><pubDate>Thu, 06 Nov 2025 16:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04485v1</guid></item><item><title>Online Algorithms for Repeated Optimal Stopping: Achieving Both Competitive Ratio and Regret Bounds</title><link>http://arxiv.org/abs/2511.04484v1</link><description>We study the repeated optimal stopping problem, which generalizes theclassical optimal stopping problem with an unknown distribution to a settingwhere the same problem is solved repeatedly over $T$ rounds. In this framework,we aim to design algorithms that guarantee a competitive ratio in each roundwhile also achieving sublinear regret across all rounds. Our primary contribution is a general algorithmic framework that achievesthese objectives simultaneously for a wide array of repeated optimal stoppingproblems. The core idea is to dynamically select an algorithm for each round,choosing between two candidates: (1) an empirically optimal algorithm derivedfrom the history of observations, and (2) a sample-based algorithm with aproven competitive ratio guarantee. Based on this approach, we design analgorithm that performs no worse than the baseline sample-based algorithm inevery round, while ensuring that the total regret is bounded by$\tilde{O}(\sqrt{T})$. We demonstrate the broad applicability of our framework to canonicalproblems, including the prophet inequality, the secretary problem, and theirvariants under adversarial, random, and i.i.d. input models. For example, forthe repeated prophet inequality problem, our method achieves a$1/2$-competitive ratio from the second round on and an $\tilde{O}(\sqrt{T})$regret. Furthermore, we establish a regret lower bound of $\Omega(\sqrt{T})$even in the i.i.d. model, confirming that our algorithm's performance is almostoptimal with respect to the number of rounds.</description><author>Tsubasa Harada, Yasushi Kawase, Hanna Sumita</author><pubDate>Thu, 06 Nov 2025 16:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04484v1</guid></item><item><title>JaneEye: A 12-nm 2K-FPS 18.9-$μ$J/Frame Event-based Eye Tracking Accelerator</title><link>http://arxiv.org/abs/2510.01213v2</link><description>Eye tracking has become a key technology for gaze-based interactions inExtended Reality (XR). However, conventional frame-based eye-tracking systemsoften fall short of XR's stringent requirements for high accuracy, low latency,and energy efficiency. Event cameras present a compelling alternative, offeringultra-high temporal resolution and low power consumption. In this paper, wepresent JaneEye, an energy-efficient event-based eye-tracking hardwareaccelerator designed specifically for wearable devices, leveraging sparse,high-temporal-resolution event data. We introduce an ultra-lightweight neuralnetwork architecture featuring a novel ConvJANET layer, which simplifies thetraditional ConvLSTM by retaining only the forget gate, thereby halvingcomputational complexity without sacrificing temporal modeling capability. Ourproposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. Tofurther enhance hardware efficiency, we employ custom linear approximations ofactivation functions (hardsigmoid and hardtanh) and fixed-point quantization.Through software-hardware co-design, our 12-nm ASIC implementation operates at400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 FramesPer Second (FPS)) at an energy efficiency of 18.9 $\mu$J/frame. JaneEye sets anew benchmark in low-power, high-performance eye-tracking solutions suitablefor integration into next-generation XR wearables.</description><author>Tao Han, Ang Li, Qinyu Chen, Chang Gao</author><pubDate>Thu, 06 Nov 2025 16:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.01213v2</guid></item><item><title>Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis</title><link>http://arxiv.org/abs/2511.04481v1</link><description>Web agents, like OpenAI's Operator and Google's Project Mariner, are powerfulagentic systems pushing the boundaries of Large Language Models (LLM). They canautonomously interact with the internet at the user's behest, such asnavigating websites, filling search masks, and comparing price lists. Thoughweb agent research is thriving, induced sustainability issues remain largelyunexplored. To highlight the urgency of this issue, we provide an initialexploration of the energy and $CO_2$ cost associated with web agents from botha theoretical -via estimation- and an empirical perspective -by benchmarking.Our results show how different philosophies in web agent creation can severelyimpact the associated expended energy, and that more energy consumed does notnecessarily equate to better results. We highlight a lack of transparencyregarding disclosing model parameters and processes used for some web agents asa limiting factor when estimating energy consumption. Our work contributestowards a change in thinking of how we evaluate web agents, advocating fordedicated metrics measuring energy consumption in benchmarks.</description><author>Lars Krupp, Daniel Geißler, Vishal Banwari, Paul Lukowicz, Jakob Karolus</author><pubDate>Thu, 06 Nov 2025 15:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04481v1</guid></item><item><title>LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge</title><link>http://arxiv.org/abs/2511.01409v2</link><description>Evaluating large language models (LLMs) on question answering often relies onstatic benchmarks that reward memorization and understate the role ofretrieval, failing to capture the dynamic nature of world knowledge. We presentLiveSearchBench, an automated pipeline for constructing retrieval-dependentbenchmarks from recent knowledge updates. Our method computes deltas betweensuccessive Wikidata snapshots, filters candidate triples for quality, andsynthesizes natural-language questions at three levels of reasoning difficulty,each guaranteed to admit a unique, verifiable answer through SPARQL validation.The pipeline is fully automated, scalable across time, and minimizes humanintervention, enabling continual regeneration of temporally groundedbenchmarks. Experiments show a pronounced performance drop when models confrontfacts that post-date pretraining, with the gap most salient on multi-hopqueries. Retrieval augmented methods and larger, instruction-tuned modelsprovide partial gains but fail to close this recency gap. By design,LiveSearchBench shifts evaluation from static memorization toward tasks thatrequire up-to-date retrieval and reasoning, offering a foundation forsystematic, long-term assessment of LLMs under evolving knowledge.</description><author>Heng Zhou, Ao Yu, Yuchen Fan, Jianing Shi, Li Kang, Hejia Geng, Yongting Zhang, Yutao Fan, Yuhao Wu, Tiancheng He, Yiran Qin, Lei Bai, Zhenfei Yin</author><pubDate>Thu, 06 Nov 2025 15:57:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.01409v2</guid></item><item><title>ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai</title><link>http://arxiv.org/abs/2511.04479v1</link><description>We present ThaiOCRBench, the first comprehensive benchmark for evaluatingvision-language models (VLMs) on Thai text-rich visual understanding tasks.Despite recent progress in multimodal modeling, existing benchmarkspredominantly focus on high-resource languages, leaving Thai underrepresented,especially in tasks requiring document structure understanding. ThaiOCRBenchaddresses this gap by offering a diverse, human-annotated dataset comprising2,808 samples across 13 task categories. We evaluate a wide range ofstate-of-the-art VLMs in a zero-shot setting, spanning both proprietary andopen-source systems. Results show a significant performance gap, withproprietary models (e.g., Gemini 2.5 Pro) outperforming open-sourcecounterparts. Notably, fine-grained text recognition and handwritten contentextraction exhibit the steepest performance drops among open-source models.Through detailed error analysis, we identify key challenges such as languagebias, structural mismatch, and hallucinated content. ThaiOCRBench provides astandardized framework for assessing VLMs in low-resource, script-complexsettings, and provides actionable insights for improving Thai-language documentunderstanding.</description><author>Surapon Nonesung, Teetouch Jaknamon, Sirinya Chaiophat, Natapong Nitarach, Chanakan Wittayasakpan, Warit Sirichotedumrong, Adisai Na-Thalang, Kunat Pipatanakul</author><pubDate>Thu, 06 Nov 2025 15:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04479v1</guid></item><item><title>Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop Refinement of LLM Judges</title><link>http://arxiv.org/abs/2511.04478v1</link><description>The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, butits effectiveness is often limited by the scarcity of diverse, representativedata for refining criteria. We present a tool that integrates synthetic datageneration into the LLM-as-a-judge workflow, empowering users to createtailored and challenging test cases with configurable domains, personas,lengths, and desired outcomes, including borderline cases. The tool alsosupports AI-assisted inline editing of existing test cases. To enhancetransparency and interpretability, it reveals the prompts and explanationsbehind each generation. In a user study (N=24), 83% of participants preferredthe tool over manually creating or selecting test cases, as it allowed them torapidly generate diverse synthetic data without additional workload. Thegenerated synthetic data proved as effective as hand-crafted data for bothrefining evaluation criteria and aligning with human preferences. Thesefindings highlight synthetic data as a promising alternative, particularly incontexts where efficiency and scalability are critical.</description><author>Hyo Jin Do, Zahra Ashktorab, Jasmina Gajcin, Erik Miehling, Martín Santillán Cooper, Qian Pan, Elizabeth M. Daly, Werner Geyer</author><pubDate>Thu, 06 Nov 2025 15:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04478v1</guid></item><item><title>What Are They Talking About? A Benchmark of Knowledge-Grounded Discussion Summarization</title><link>http://arxiv.org/abs/2505.12474v3</link><description>Traditional dialogue summarization primarily focuses on dialogue content,assuming it comprises adequate information for a clear summary. However, thisassumption often fails for discussions grounded in shared background, whereparticipants frequently omit context and use implicit references. This resultsin summaries that are confusing to readers unfamiliar with the background. Toaddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),a novel task that produces a supplementary background summary for context and aclear opinion summary with clarified references. To facilitate research, weconstruct the first KGDS benchmark, featuring news-discussion pairs andexpert-created multi-granularity gold annotations for evaluating sub-summaries.We also propose a novel hierarchical evaluation framework with fine-grained andinterpretable metrics. Our extensive evaluation of 12 advanced large languagemodels (LLMs) reveals that KGDS remains a significant challenge. The modelsfrequently miss key facts and retain irrelevant ones in backgroundsummarization, and often fail to resolve implicit references in opinion summaryintegration.</description><author>Weixiao Zhou, Junnan Zhu, Gengyao Li, Xianfu Cheng, Xinnian Liang, Feifei Zhai, Zhoujun Li</author><pubDate>Thu, 06 Nov 2025 15:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.12474v3</guid></item><item><title>Probabilistic Textual Time Series Depression Detection</title><link>http://arxiv.org/abs/2511.04476v1</link><description>Accurate and interpretable predictions of depression severity are essentialfor clinical decision support, yet existing models often lack uncertaintyestimates and temporal modeling. We propose PTTSD, a Probabilistic Textual TimeSeries Depression Detection framework that predicts PHQ-8 scores fromutterance-level clinical interviews while modeling uncertainty over time. PTTSDincludes sequence-to-sequence and sequence-to-one variants, both combiningbidirectional LSTMs, self-attention, and residual connections with Gaussian orStudent-t output heads trained via negative log-likelihood. Evaluated on E-DAICand DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-onlysystems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibratedprediction intervals. Ablations confirm the value of attention andprobabilistic modeling, while comparisons with MentalBERT establish generality.A three-part calibration analysis and qualitative case studies furtherhighlight the interpretability and clinical relevance of uncertainty-awareforecasting.</description><author>Fabian Schmidt, Seyedehmoniba Ravan, Vladimir Vlassov</author><pubDate>Thu, 06 Nov 2025 15:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04476v1</guid></item><item><title>Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability</title><link>http://arxiv.org/abs/2511.04474v1</link><description>Landslides cause severe damage to lives, infrastructure, and the environment,making accurate and timely mapping essential for disaster preparedness andresponse. However, conventional deep learning models often struggle whenapplied across different sensors, regions, or under conditions of limitedtraining data. To address these challenges, we present a three-axis analyticalframework of sensor, label, and domain for adapting geospatial foundationmodels (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through aseries of experiments, we show that it consistently outperforms task-specificCNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and otherGeoFMs (TerraMind, SatMAE). The model, built on global pretraining,self-supervision, and adaptable fine-tuning, proved resilient to spectralvariation, maintained accuracy under label scarcity, and generalized morereliably across diverse datasets and geographic settings. Alongside thesestrengths, we also highlight remaining challenges such as computational costand the limited availability of reusable AI-ready training data for landslideresearch. Overall, our study positions GeoFMs as a step toward more robust andscalable approaches for landslide risk reduction and environmental monitoring.</description><author>Wenwen Li, Sizhe Wang, Hyunho Lee, Chenyan Lu, Sujit Roy, Rahul Ramachandran, Chia-Yu Hsu</author><pubDate>Thu, 06 Nov 2025 15:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04474v1</guid></item><item><title>Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards</title><link>http://arxiv.org/abs/2505.04847v2</link><description>Retrieval-augmented generation (RAG) aims to reduce hallucinations bygrounding responses in external context, yet large language models (LLMs) stillfrequently introduce unsupported information or contradictions even whenprovided with relevant context. This paper presents two complementary effortsat Vectara to measure and benchmark LLM faithfulness in RAG. First, we describeour original hallucination leaderboard, which has tracked hallucination ratesfor LLMs since 2023 using our HHEM hallucination detection model. Motivated bylimitations observed in current hallucination detection methods, we introduceFaithJudge, an LLM-as-a-judge framework that leverages a pool of diversehuman-annotated hallucination examples to substantially improve the automatedhallucination evaluation of LLMs. We introduce an enhanced hallucinationleaderboard centered on FaithJudge that benchmarks LLMs on RAG faithfulness insummarization, question-answering, and data-to-text generation tasks.FaithJudge enables a more reliable benchmarking of LLM hallucinations in RAGand supports the development of more trustworthy generative AI systems:https://github.com/vectara/FaithJudge.</description><author>Manveer Singh Tamber, Forrest Sheng Bao, Chenyu Xu, Ge Luo, Suleman Kazi, Minseok Bae, Miaoran Li, Ofer Mendelevitch, Renyi Qu, Jimmy Lin</author><pubDate>Thu, 06 Nov 2025 15:46:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.04847v2</guid></item><item><title>Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs</title><link>http://arxiv.org/abs/2511.04473v1</link><description>Retrieval of information from graph-structured knowledge bases represents apromising direction for improving the factuality of LLMs. While varioussolutions have been proposed, a comparison of methods is difficult due to thelack of challenging QA datasets with ground-truth targets for graph retrieval.We present SynthKGQA, a framework for generating high-quality syntheticKnowledge Graph Question Answering datasets from any Knowledge Graph, providingthe full set of ground-truth facts in the KG to reason over each question. Weshow how, in addition to enabling more informative benchmarking of KGretrievers, the data produced with SynthKGQA also allows us to train bettermodels. We apply SynthKGQA to Wikidata to generate GTSQA, a new datasetdesigned to test zero-shot generalization abilities of KG retrievers withrespect to unseen graph structures and relation types, and benchmark popularsolutions for KG-augmented LLMs on it.</description><author>Alberto Cattaneo, Carlo Luschi, Daniel Justus</author><pubDate>Thu, 06 Nov 2025 15:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04473v1</guid></item><item><title>Towards Causal Market Simulators</title><link>http://arxiv.org/abs/2511.04469v1</link><description>Market generators using deep generative models have shown promise forsynthetic financial data generation, but existing approaches lack causalreasoning capabilities essential for counterfactual analysis and riskassessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) thatcombines variational autoencoders with structural causal models to generatecounterfactual financial time series while preserving both temporaldependencies and causal relationships. Our approach enforces causal constraintsthrough directed acyclic graphs in the decoder architecture and employs thecausal Wasserstein distance for training. We validate our method on syntheticautoregressive models inspired by the Ornstein-Uhlenbeck process, demonstratingsuperior performance in counterfactual probability estimation with L1 distancesas low as 0.03-0.10 compared to ground truth. The model enables financialstress testing, scenario analysis, and enhanced backtesting by generatingplausible counterfactual market trajectories that respect underlying causalmechanisms.</description><author>Dennis Thumm, Luis Ontaneda Mijares</author><pubDate>Thu, 06 Nov 2025 15:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04469v1</guid></item><item><title>UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment</title><link>http://arxiv.org/abs/2506.01802v2</link><description>Learning an animatable and clothed human avatar model with vivid dynamics andphotorealistic appearance from multi-view videos is an important foundationalresearch problem in computer graphics and vision. Fueled by recent advances inimplicit representations, the quality of the animatable avatars has achieved anunprecedented level by attaching the implicit representation to drivable humantemplate meshes. However, they usually fail to preserve the highest level ofdetail, particularly apparent when the virtual camera is zoomed in and whenrendering at 4K resolution and higher. We argue that this limitation stems frominaccurate surface tracking, specifically, depth misalignment and surface driftbetween character geometry and the ground truth surface, which forces thedetailed appearance model to compensate for geometric errors. To address this,we propose a latent deformation model and supervising the 3D deformation of theanimatable character using guidance from foundational 2D video point trackers,which offer improved robustness to shading and surface variations, and are lessprone to local minima than differentiable rendering. To mitigate the drift overtime and lack of 3D awareness of 2D point trackers, we introduce a cascadedtraining strategy that generates consistent 3D point tracks by anchoring pointtracks to the rendered avatar, which ultimately supervises our avatar at thevertex and texel level. To validate the effectiveness of our approach, weintroduce a novel dataset comprising five multi-view video sequences, each over10 minutes in duration, captured using 40 calibrated 6K-resolution cameras,featuring subjects dressed in clothing with challenging texture patterns andwrinkle deformations. Our approach demonstrates significantly improvedperformance in rendering quality and geometric accuracy over the prior state ofthe art.</description><author>Heming Zhu, Guoxing Sun, Christian Theobalt, Marc Habermann</author><pubDate>Thu, 06 Nov 2025 15:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.01802v2</guid></item><item><title>Fraud-Proof Revenue Division on Subscription Platforms</title><link>http://arxiv.org/abs/2511.04465v1</link><description>We study a model of subscription-based platforms where users pay a fixed feefor unlimited access to content, and creators receive a share of the revenue.Existing approaches to detecting fraud predominantly rely on machine learningmethods, engaging in an ongoing arms race with bad actors. We explore revenuedivision mechanisms that inherently disincentivize manipulation. We formalizethree types of manipulation-resistance axioms and examine which existing rulessatisfy these. We show that a mechanism widely used by streaming platforms, notonly fails to prevent fraud, but also makes detecting manipulationcomputationally intractable. We also introduce a novel rule, ScaledUserProp,that satisfies all three manipulation-resistance axioms. Finally, experimentswith both real-world and synthetic streaming data support ScaledUserProp as afairer alternative compared to existing rules.</description><author>Abheek Ghosh, Tzeh Yuan Neoh, Nicholas Teh, Giannis Tyrovolas</author><pubDate>Thu, 06 Nov 2025 15:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04465v1</guid></item><item><title>Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context</title><link>http://arxiv.org/abs/2511.04464v1</link><description>Traditional vehicle routing systems efficiently optimize singular metricslike time or distance, and when considering multiple metrics, they need moreprocesses to optimize . However, they lack the capability to interpret andintegrate the complex, semantic, and dynamic contexts of human drivers, such asmulti-step tasks, situational constraints, or urgent needs. This paperintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), ahybrid agentic assistant designed to augment classical pathfinding algorithmswith contextual reasoning. Our approach employs a Large Language Model (LLM)agent that operates on a candidate set of routes generated by a multi-objective(time, CO2) Dijkstra algorithm. The agent evaluates these options againstuser-provided tasks, preferences, and avoidance rules by leveraging apre-processed geospatial cache of urban Points of Interest (POIs). In abenchmark of realistic urban scenarios, PAVe successfully used complex userintent into appropriate route modifications, achieving over 88% accuracy in itsinitial route selections with a local model. We conclude that combiningclassical routing algorithms with an LLM-based semantic reasoning layer is arobust and effective approach for creating personalized, adaptive, and scalablesolutions for urban mobility optimization.</description><author>Carnot Braun, Rafael O. Jarczewski, Gabriel U. Talasso, Leandro A. Villas, Allan M. de Souza</author><pubDate>Thu, 06 Nov 2025 15:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04464v1</guid></item><item><title>Diffusion &amp; Adversarial Schrödinger Bridges via Iterative Proportional Markovian Fitting</title><link>http://arxiv.org/abs/2410.02601v4</link><description>The Iterative Markovian Fitting (IMF) procedure, which iteratively projectsonto the space of Markov processes and the reciprocal class, successfullysolves the Schr\"odinger Bridge (SB) problem. However, an efficient practicalimplementation requires a heuristic modification -- alternating between fittingforward and backward time diffusion at each iteration. This modification iscrucial for stabilizing training and achieving reliable results in applicationssuch as unpaired domain translation. Our work reveals a close connectionbetween the modified version of IMF and the Iterative Proportional Fitting(IPF) procedure -- a foundational method for the SB problem, also known asSinkhorn's algorithm. Specifically, we demonstrate that the heuristicmodification of the IMF effectively integrates both IMF and IPF procedures. Werefer to this combined approach as the Iterative Proportional Markovian Fitting(IPMF) procedure. Through theoretical and empirical analysis, we establish theconvergence of the IPMF procedure under various settings, contributing todeveloping a unified framework for solving SB problems. Moreover, from apractical standpoint, the IPMF procedure enables a flexible trade-off betweenimage similarity and generation quality, offering a new mechanism for tailoringmodels to specific tasks.</description><author>Sergei Kholkin, Grigoriy Ksenofontov, David Li, Nikita Kornilov, Nikita Gushchin, Alexandra Suvorikova, Alexey Kroshnin, Evgeny Burnaev, Alexander Korotin</author><pubDate>Thu, 06 Nov 2025 15:32:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02601v4</guid></item><item><title>Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition</title><link>http://arxiv.org/abs/2511.04461v1</link><description>In this study, we present and validate an ensemble-based Hankel Dynamic ModeDecomposition with control (HDMDc) for uncertainty-aware seakeeping predictionsof a high-speed catamaran, namely the Delft 372 model. Experimentalmeasurements (time histories) of wave elevation at the longitudinal center ofgravity, heave, pitch, notional flight-deck velocity, notional bridgeacceleration, and total resistance were collected from irregular wave basintests on a 1:33.3 scale replica of the Delft 372 model under sea state 5conditions at Fr = 0.425, and organized into training, validation, and testsets. The HDMDc algorithm constructs an equation-free linear reduced-ordermodel of the seakeeping vessel by augmenting states and inputs with theirtime-lagged copies to capture nonlinear and memory effects. Two ensemblingstrategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparametersconsidered stochastic variables with prior distribution to produce posteriormean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), whichaggregates multiple model obtained over data subsets, are compared in providingseakeeping prediction and uncertainty quantification. The FHDMDc approach isfound to improve the accuracy of the predictions compared to the deterministiccounterpart, also providing robust uncertainty estimation; whereas theapplication of BHDMDc to the present test case is not found beneficial incomparison to the deterministic model. FHDMDc-derived probability densityfunctions for the motions closely match both experimental data and URANSresults, demonstrating reliable and computationally efficient seakeepingprediction for design and operational support.</description><author>Giorgio Palma, Andrea Serani, Matteo Diez</author><pubDate>Thu, 06 Nov 2025 15:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04461v1</guid></item><item><title>V-Thinker: Interactive Thinking with Images</title><link>http://arxiv.org/abs/2511.04460v1</link><description>Empowering Large Multimodal Models (LMMs) to deeply integrate imageinteraction with long-horizon reasoning capabilities remains a long-standingchallenge in this field. Recent advances in vision-centric reasoning explore apromising "Thinking with Images" paradigm for LMMs, marking a shift fromimage-assisted reasoning to image-interactive thinking. While this milestoneenables models to focus on fine-grained image regions, progress remainsconstrained by limited visual tool spaces and task-specific workflow designs.To bridge this gap, we present V-Thinker, a general-purpose multimodalreasoning assistant that enables interactive, vision-centric thinking throughend-to-end reinforcement learning. V-Thinker comprises two key components: (1)a Data Evolution Flywheel that automatically synthesizes, evolves, and verifiesinteractive reasoning datasets across three dimensions-diversity, quality, anddifficulty; and (2) a Visual Progressive Training Curriculum that first alignsperception via point-level supervision, then integrates interactive reasoningthrough a two-stage reinforcement learning framework. Furthermore, we introduceVTBench, an expert-verified benchmark targeting vision-centric interactivereasoning tasks. Extensive experiments demonstrate that V-Thinker consistentlyoutperforms strong LMM-based baselines in both general and interactivereasoning scenarios, providing valuable insights for advancingimage-interactive reasoning applications.</description><author>Runqi Qiao, Qiuna Tan, Minghan Yang, Guanting Dong, Peiqing Yang, Shiqiang Lang, Enhui Wan, Xiaowan Wang, Yida Xu, Lan Yang, Chong Sun, Chen Li, Honggang Zhang</author><pubDate>Thu, 06 Nov 2025 15:32:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04460v1</guid></item><item><title>HoliSafe: Holistic Safety Benchmarking and Modeling for Vision-Language Model</title><link>http://arxiv.org/abs/2506.04704v4</link><description>Despite emerging efforts to enhance the safety of Vision-Language Models(VLMs), current approaches face two main shortcomings. 1) Existingsafety-tuning datasets and benchmarks only partially consider how image-textinteractions can yield harmful content, often overlooking contextually unsafeoutcomes from seemingly benign pairs. This narrow coverage leaves VLMsvulnerable to jailbreak attacks in unseen configurations. 2) Prior methods relyprimarily on data-centric tuning, with limited architectural innovations tointrinsically strengthen safety. We address these gaps by introducing aholistic safety dataset and benchmark, \textbf{HoliSafe}, that spans all fivesafe/unsafe image-text combinations, providing a more robust basis for bothtraining and evaluation (HoliSafe-Bench). We further propose a novel modularframework for enhancing VLM safety with a visual guard module (VGM) designed toassess the harmfulness of input images for VLMs. This module endows VLMs with adual functionality: they not only learn to generate safer responses but canalso provide an interpretable harmfulness classification to justify theirrefusal decisions. A significant advantage of this approach is its modularity;the VGM is designed as a plug-in component, allowing for seamless integrationwith diverse pre-trained VLMs across various scales. Experiments show thatSafe-VLM with VGM, trained on our HoliSafe, achieves state-of-the-art safetyperformance across multiple VLM benchmarks. Additionally, the HoliSafe-Benchitself reveals critical vulnerabilities in existing VLM models. We hope thatHoliSafe and VGM will spur further research into robust and interpretable VLMsafety, expanding future avenues for multimodal alignment.</description><author>Youngwan Lee, Kangsan Kim, Kwanyong Park, Ilcahe Jung, Soojin Jang, Seanie Lee, Yong-Ju Lee, Sung Ju Hwang</author><pubDate>Thu, 06 Nov 2025 15:28:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.04704v4</guid></item><item><title>Federated Stochastic Minimax Optimization under Heavy-Tailed Noises</title><link>http://arxiv.org/abs/2511.04456v1</link><description>Heavy-tailed noise has attracted growing attention in nonconvex stochasticoptimization, as numerous empirical studies suggest it offers a more realisticassumption than standard bounded variance assumption. In this work, weinvestigate nonconvex-PL minimax optimization under heavy-tailed gradient noisein federated learning. We propose two novel algorithms: Fed-NSGDA-M, whichintegrates normalized gradients, and FedMuon-DA, which leverages the Muonoptimizer for local updates. Both algorithms are designed to effectivelyaddress heavy-tailed noise in federated minimax optimization, under a mildercondition. We theoretically establish that both algorithms achieve aconvergence rate of $O({1}/{(TNp)^{\frac{s-1}{2s}}})$. To the best of ourknowledge, these are the first federated minimax optimization algorithms withrigorous theoretical guarantees under heavy-tailed noise. Extensive experimentsfurther validate their effectiveness.</description><author>Xinwen Zhang, Hongchang Gao</author><pubDate>Thu, 06 Nov 2025 15:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04456v1</guid></item><item><title>Fitting Reinforcement Learning Model to Behavioral Data under Bandits</title><link>http://arxiv.org/abs/2511.04454v1</link><description>We consider the problem of fitting a reinforcement learning (RL) model tosome given behavioral data under a multi-armed bandit environment. These modelshave received much attention in recent years for characterizing human andanimal decision making behavior. We provide a generic mathematical optimizationproblem formulation for the fitting problem of a wide range of RL models thatappear frequently in scientific research applications, followed by a detailedtheoretical analysis of its convexity properties. Based on the theoreticalresults, we introduce a novel solution method for the fitting problem of RLmodels based on convex relaxation and optimization. Our method is thenevaluated in several simulated bandit environments to compare with somebenchmark methods that appear in the literature. Numerical results indicatethat our method achieves comparable performance to the state-of-the-art, whilesignificantly reducing computation time. We also provide an open-source Pythonpackage for our proposed method to empower researchers to apply it in theanalysis of their datasets directly, without prior knowledge of convexoptimization.</description><author>Hao Zhu, Jasper Hoffmann, Baohe Zhang, Joschka Boedecker</author><pubDate>Thu, 06 Nov 2025 15:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04454v1</guid></item><item><title>Evaluating LLM-Contaminated Crowdsourcing Data Without Ground Truth</title><link>http://arxiv.org/abs/2506.06991v2</link><description>The recent success of generative AI highlights the crucial role ofhigh-quality human feedback in building trustworthy AI systems. However, theincreasing use of large language models (LLMs) by crowdsourcing workers poses asignificant challenge: datasets intended to reflect human input may becompromised by LLM-generated responses. Existing LLM detection approaches oftenrely on high-dimensional training data such as text, making them unsuitable forannotation tasks like multiple-choice labeling. In this work, we investigatethe potential of peer prediction -- a mechanism that evaluates the informationwithin workers' responses without using ground truth -- to mitigateLLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Ourapproach quantifies the correlations between worker answers while conditioningon (a subset of) LLM-generated labels available to the requester. Building onprior research, we propose a training-free scoring mechanism with theoreticalguarantees under a crowdsourcing model that accounts for LLM collusion. Weestablish conditions under which our method is effective and empiricallydemonstrate its robustness in detecting low-effort cheating on real-worldcrowdsourcing datasets.</description><author>Yichi Zhang, Jinlong Pang, Zhaowei Zhu, Yang Liu</author><pubDate>Thu, 06 Nov 2025 15:24:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.06991v2</guid></item><item><title>Regularized least squares learning with heavy-tailed noise is minimax optimal</title><link>http://arxiv.org/abs/2505.14214v3</link><description>This paper examines the performance of ridge regression in reproducing kernelHilbert spaces in the presence of noise that exhibits a finite number of highermoments. We establish excess risk bounds consisting of subgaussian andpolynomial terms based on the well known integral operator framework. Thedominant subgaussian component allows to achieve convergence rates that havepreviously only been derived under subexponential noise - a prevalentassumption in related work from the last two decades. These rates are optimalunder standard eigenvalue decay conditions, demonstrating the asymptoticrobustness of regularized least squares against heavy-tailed noise. Ourderivations are based on a Fuk-Nagaev inequality for Hilbert-space valuedrandom variables.</description><author>Mattes Mollenhauer, Nicole Mücke, Dimitri Meunier, Arthur Gretton</author><pubDate>Thu, 06 Nov 2025 15:23:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14214v3</guid></item><item><title>Deep Dictionary-Free Method for Identifying Linear Model of Nonlinear System with Input Delay</title><link>http://arxiv.org/abs/2511.04451v1</link><description>Nonlinear dynamical systems with input delays pose significant challenges forprediction, estimation, and control due to their inherent complexity and theimpact of delays on system behavior. Traditional linear control techniquesoften fail in these contexts, necessitating innovative approaches. This paperintroduces a novel approach to approximate the Koopman operator using anLSTM-enhanced Deep Koopman model, enabling linear representations of nonlinearsystems with time delays. By incorporating Long Short-Term Memory (LSTM)layers, the proposed framework captures historical dependencies and efficientlyencodes time-delayed system dynamics into a latent space. Unlike traditionalextended Dynamic Mode Decomposition (eDMD) approaches that rely on predefineddictionaries, the LSTM-enhanced Deep Koopman model is dictionary-free, whichmitigates the problems with the underlying dynamics being known andincorporated into the dictionary. Quantitative comparisons with extended eDMDon a simulated system demonstrate highly significant performance gains inprediction accuracy in cases where the true nonlinear dynamics are unknown andachieve comparable results to eDMD with known dynamics of a system.</description><author>Patrik Valábek, Marek Wadinger, Michal Kvasnica, Martin Klaučo</author><pubDate>Thu, 06 Nov 2025 15:22:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04451v1</guid></item><item><title>Solving Convex Partition Visual Jigsaw Puzzles</title><link>http://arxiv.org/abs/2511.04450v1</link><description>Jigsaw puzzle solving requires the rearrangement of unordered pieces intotheir original pose in order to reconstruct a coherent whole, often an image,and is known to be an intractable problem. While the possible impact ofautomatic puzzle solvers can be disruptive in various application domains, mostof the literature has focused on developing solvers for square jigsaw puzzles,severely limiting their practical use. In this work, we significantly expandthe types of puzzles handled computationally, focusing on what is known asConvex Partitions, a major subset of polygonal puzzles whose pieces are convex.We utilize both geometrical and pictorial compatibilities, introduce a greedysolver, and report several performance measures next to the first benchmarkdataset of such puzzles.</description><author>Yaniv Ohayon, Ofir Itzhak Shahar, Ohad Ben-Shahar</author><pubDate>Thu, 06 Nov 2025 15:22:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04450v1</guid></item><item><title>ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting</title><link>http://arxiv.org/abs/2511.04445v1</link><description>Time series forecasting is essential across domains from finance to supplychain management. This paper introduces ForecastGAN, a novel decompositionbased adversarial framework addressing limitations in existing approaches formulti-horizon predictions. Although transformer models excel in long-termforecasting, they often underperform in short-term scenarios and typicallyignore categorical features. ForecastGAN operates through three integratedmodules: a Decomposition Module that extracts seasonality and trend components;a Model Selection Module that identifies optimal neural network configurationsbased on forecasting horizon; and an Adversarial Training Module that enhancesprediction robustness through Conditional Generative Adversarial Networktraining. Unlike conventional approaches, ForecastGAN effectively integratesboth numerical and categorical features. We validate our framework on elevenbenchmark multivariate time series datasets that span various forecastinghorizons. The results show that ForecastGAN consistently outperformsstate-of-the-art transformer models for short-term forecasting while remainingcompetitive for long-term horizons. This research establishes a moregeneralizable approach to time series forecasting that adapts to specificcontexts while maintaining strong performance across diverse datacharacteristics without extensive hyperparameter tuning.</description><author>Syeda Sitara Wishal Fatima, Afshin Rahimi</author><pubDate>Thu, 06 Nov 2025 15:19:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04445v1</guid></item><item><title>SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors</title><link>http://arxiv.org/abs/2509.15827v2</link><description>Accurate day-ahead forecasts of solar irradiance are required for thelarge-scale integration of solar photovoltaic (PV) systems into the power grid.However, current forecasting solutions lack the temporal and spatial resolutionrequired by system operators. In this paper, we introduce SolarCrossFormer, anovel deep learning model for day-ahead irradiance forecasting, that combinessatellite images and time series from a ground-based network of meteorologicalstations. SolarCrossFormer uses novel graph neural networks to exploit theinter- and intra-modal correlations of the input data and improve the accuracyand resolution of the forecasts. It generates probabilistic forecasts for anylocation in Switzerland with a 15-minute resolution for horizons up to 24 hoursahead. One of the key advantages of SolarCrossFormer its robustness in reallife operations. It can incorporate new time-series data without retraining themodel and, additionally, it can produce forecasts for locations without inputdata by using only their coordinates. Experimental results over a dataset ofone year and 127 locations across Switzerland show that SolarCrossFormer yielda normalized mean absolute error of 6.1 % over the forecasting horizon. Theresults are competitive with those achieved by a commercial numerical weatherprediction service.</description><author>Baptiste Schubnel, Jelena Simeunović, Corentin Tissier, Pierre-Jean Alet, Rafael E. Carrillo</author><pubDate>Thu, 06 Nov 2025 15:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.15827v2</guid></item><item><title>SLAM&amp;Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM</title><link>http://arxiv.org/abs/2504.13713v3</link><description>Models and methods originally developed for Novel View Synthesis and SceneRendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, areincreasingly being adopted as representations in Simultaneous Localization andMapping (SLAM). However, existing datasets fail to include the specificchallenges of both fields, such as sequential operations and, in many settings,multi-modality in SLAM or generalization across viewpoints and illuminationconditions in neural rendering. Additionally, the data are often collectedusing sensors which are handheld or mounted on drones or mobile robots, whichcomplicates the accurate reproduction of sensor motions. To bridge these gaps,we introduce SLAM&amp;Render, a novel dataset designed to benchmark methods in theintersection between SLAM, Novel View Rendering and Gaussian Splatting.Recorded with a robot manipulator, it uniquely includes 40 sequences withtime-synchronized RGB-D images, IMU readings, robot kinematic data, andground-truth pose streams. By releasing robot kinematic data, the dataset alsoenables the assessment of recent integrations of SLAM paradigms within roboticapplications. The dataset features five setups with consumer and industrialobjects under four controlled lighting conditions, each with separate trainingand test trajectories. All sequences are static with different levels of objectrearrangements and occlusions. Our experimental results, obtained with severalbaselines from the literature, validate SLAM&amp;Render as a relevant benchmark forthis emerging research area.</description><author>Samuel Cerezo, Gaetano Meli, Tomás Berriel Martins, Kirill Safronov, Javier Civera</author><pubDate>Thu, 06 Nov 2025 15:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.13713v3</guid></item><item><title>The Peril of Preference: Why GRPO fails on Ordinal Rewards</title><link>http://arxiv.org/abs/2511.04439v1</link><description>Group-relative Policy Optimization's (GRPO) simplicity makes it highlydesirable for adapting LLMs to become experts at specific tasks. But thissimplicity also makes it ill-specified as we seek to enhance RL training withricher, non-binary feedback. When using ordinal rewards to give partial credit,GRPO's simplicity starts to hurt, as its group-average baseline often assigns apositive advantage to failed trajectories and reinforces incorrect behavior. We introduce Correctness Relative Policy Optimization (CoRPO), a newformulation that solves this flaw. CoRPO uses an adaptive baseline thatenforces a minimum quality threshold, ensuring failed solutions are neverpositively reinforced. Once the policy consistently meets this threshold, thebaseline automatically transitions to a relative preference mode, pushing themodel to find optimal solutions rather than just "acceptable" ones. Weempirically validate CoRPO on a code verification task, where it demonstratesmore stable convergence and better out-of-domain generalization. This work represents a critical step in our broader research program toenable LLMs to learn genuinely new capabilities through reinforcement learning.We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback- progressing from binary to ordinal rewards in this work, and onward todenser, per-step supervision.</description><author>Anisha Garg, Ganesh Venkatesh</author><pubDate>Thu, 06 Nov 2025 15:12:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04439v1</guid></item><item><title>Deep Koopman Economic Model Predictive Control of a Pasteurisation Unit</title><link>http://arxiv.org/abs/2511.04437v1</link><description>This paper presents a deep Koopman-based Economic Model Predictive Control(EMPC) for efficient operation of a laboratory-scale pasteurization unit (PU).The method uses Koopman operator theory to transform the complex, nonlinearsystem dynamics into a linear representation, enabling the application ofconvex optimization while representing the complex PU accurately. The deepKoopman model utilizes neural networks to learn the linear dynamics fromexperimental data, achieving a 45% improvement in open-loop prediction accuracyover conventional N4SID subspace identification. Both analyzed models wereemployed in the EMPC formulation that includes interpretable economic costs,such as energy consumption, material losses due to inadequate pasteurization,and actuator wear. The feasibility of EMPC is ensured using slack variables.The deep Koopman EMPC and N4SID EMPC are numerically validated on a nonlinearmodel of multivariable PU under external disturbance. The disturbances includefeed pump fail-to-close scenario and the introduction of a cold batch to bepastuerized. These results demonstrate that the deep Koopmand EMPC achieves a32% reduction in total economic cost compared to the N4SID baseline. Thisimprovement is mainly due to the reductions in material losses and energyconsumption. Furthermore, the steady-state operation via Koopman-based EMPCrequires 10.2% less electrical energy. The results highlight the practicaladvantages of integrating deep Koopman representations with economicoptimization to achieve resource-efficient control of thermal-intensive plants.</description><author>Patrik Valábek, Michaela Horváthová, Martin Klaučo</author><pubDate>Thu, 06 Nov 2025 15:08:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04437v1</guid></item><item><title>If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs</title><link>http://arxiv.org/abs/2511.04432v1</link><description>In this study, we experiment with the ability of LLMs to do temporalreasoning. Using a Norwegian book from 1940 containing trivia questions, weprompt the LLMs to answer the questions as if it were 1940. We also pose thequestions in both English and Norwegian. Correct answers are often presented assentences, and grading is done by means of LLM-as-judge, with sampled checks bya native speaker. Prompting in English consistently gave better results than inNorwegian, an unexpected result. In contrast, using larger LLMs improvedresults. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,and also the largest available LLM especially crafted for Norwegian.</description><author>Lars Bungum, Charles Yijia Huang, Abeer Kashar</author><pubDate>Thu, 06 Nov 2025 15:06:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04432v1</guid></item><item><title>Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development</title><link>http://arxiv.org/abs/2511.04427v1</link><description>Large language models (LLMs) have demonstrated the promise to revolutionizethe field of software engineering. Among other things, LLM agents are rapidlygaining momentum in their application to software development, withpractitioners claiming a multifold productivity increase after adoption. Yet,empirical evidence is lacking around these claims. In this paper, we estimatethe causal effect of adopting a widely popular LLM agent assistant, namelyCursor, on development velocity and software quality. The estimation is enabledby a state-of-the-art difference-in-differences design comparingCursor-adopting GitHub projects with a matched control group of similar GitHubprojects that do not use Cursor. We find that the adoption of Cursor leads to asignificant, large, but transient increase in project-level developmentvelocity, along with a significant and persistent increase in static analysiswarnings and code complexity. Further panel generalized method of momentsestimation reveals that the increase in static analysis warnings and codecomplexity acts as a major factor causing long-term velocity slowdown. Ourstudy carries implications for software engineering practitioners, LLM agentassistant designers, and researchers.</description><author>Hao He, Courtney Miller, Shyam Agarwal, Christian Kästner, Bogdan Vasilescu</author><pubDate>Thu, 06 Nov 2025 15:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04427v1</guid></item><item><title>HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats</title><link>http://arxiv.org/abs/2511.04426v1</link><description>Analyzing octopuses in their natural habitats is challenging due to theircamouflage capability, rapid changes in skin texture and color, non-rigid bodydeformations, and frequent occlusions, all of which are compounded by variableunderwater lighting and turbidity. Addressing the lack of large-scale annotateddatasets, this paper introduces HideAndSeg, a novel, minimally supervisedAI-based tool for segmenting videos of octopuses. It establishes a quantitativebaseline for this task. HideAndSeg integrates SAM2 with a custom-trainedYOLOv11 object detector. First, the user provides point coordinates to generatethe initial segmentation masks with SAM2. These masks serve as training datafor the YOLO model. After that, our approach fully automates the pipeline byproviding a bounding box prompt to SAM2, eliminating the need for furthermanual intervention. We introduce two unsupervised metrics - temporalconsistency $DICE_t$ and new component count $NC_t$ - to quantitativelyevaluate segmentation quality and guide mask refinement in the absence ofground-truth data, i.e., real-world information that serves to train, validate,and test AI models. Results show that HideAndSeg achieves satisfactoryperformance, reducing segmentation noise compared to the manually promptedapproach. Our method can re-identify and segment the octopus even after periodsof complete occlusion in natural environments, a scenario in which the manuallyprompted model fails. By reducing the need for manual analysis in real-worldscenarios, this work provides a practical tool that paves the way for moreefficient behavioral studies of wild cephalopods.</description><author>Alan de Aguiar, Michaella Pereira Andrade, Charles Morphy D. Santos, João Paulo Gois</author><pubDate>Thu, 06 Nov 2025 14:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04426v1</guid></item><item><title>On the Equivalence of Regression and Classification</title><link>http://arxiv.org/abs/2511.04422v1</link><description>A formal link between regression and classification has been tenuous. Eventhough the margin maximization term $\|w\|$ is used in support vectorregression, it has at best been justified as a regularizer. We show that aregression problem with $M$ samples lying on a hyperplane has a one-to-oneequivalence with a linearly separable classification task with $2M$ samples. Weshow that margin maximization on the equivalent classification task leads to adifferent regression formulation than traditionally used. Using theequivalence, we demonstrate a ``regressability'' measure, that can be used toestimate the difficulty of regressing a dataset, without needing to first learna model for it. We use the equivalence to train neural networks to learn alinearizing map, that transforms input variables into a space where a linearregressor is adequate.</description><author>Jayadeva, Naman Dwivedi, Hari Krishnan, N. M. Anoop Krishnan</author><pubDate>Thu, 06 Nov 2025 14:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04422v1</guid></item><item><title>Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning</title><link>http://arxiv.org/abs/2506.06694v5</link><description>Foundation models have revolutionized fields such as natural languageprocessing and computer vision by enabling general-purpose learning acrossdiverse tasks and datasets. However, building analogous models for humanmobility remains challenging due to the privacy-sensitive nature of mobilitydata and the resulting data silos across institutions. To bridge this gap, wepropose MoveGCL, a scalable and privacy-preserving framework for trainingmobility foundation models via generative continual learning. Without sharingraw data, MoveGCL enables decentralized and progressive model evolution byreplaying synthetic trajectories generated from a frozen teacher model, andreinforces knowledge retention through a tailored distillation strategy thatmitigates catastrophic forgetting. To address the heterogeneity of mobilitypatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with amobility-aware expert routing mechanism, and employs a layer-wise progressiveadaptation strategy to stabilize continual updates. Experiments on sixreal-world urban datasets demonstrate that MoveGCL achieves performancecomparable to joint training and significantly outperforms federated learningbaselines, while offering strong privacy protection. MoveGCL marks a crucialstep toward unlocking foundation models for mobility, offering a practicalblueprint for open, scalable, and privacy-preserving model development in theera of foundation models. To facilitate reproducibility and future research, wehave released the code and models athttps://github.com/tsinghua-fib-lab/MoveGCL.</description><author>Yuan Yuan, Yukun Liu, Chonghua Han, Jie Feng, Yong Li</author><pubDate>Thu, 06 Nov 2025 14:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.06694v5</guid></item><item><title>The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity</title><link>http://arxiv.org/abs/2511.04418v1</link><description>Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) iscritical for trustworthy deployment. While real-world language is inherentlyambiguous, reflecting aleatoric uncertainty, existing UQ methods are typicallybenchmarked against tasks with no ambiguity. In this work, we demonstrate thatwhile current uncertainty estimators perform well under the restrictiveassumption of no ambiguity, they degrade to close-to-random performance onambiguous data. To this end, we introduce MAQA* and AmbigQA*, the firstambiguous question-answering (QA) datasets equipped with ground-truth answerdistributions estimated from factual co-occurrence. We find this performancedeterioration to be consistent across different estimation paradigms: using thepredictive distribution itself, internal representations throughout the model,and an ensemble of models. We show that this phenomenon can be theoreticallyexplained, revealing that predictive-distribution and ensemble-based estimatorsare fundamentally limited under ambiguity. Overall, our study reveals a keyshortcoming of current UQ methods for LLMs and motivates a rethinking ofcurrent modeling paradigms.</description><author>Tim Tomov, Dominik Fuchsgruber, Tom Wollschläger, Stephan Günnemann</author><pubDate>Thu, 06 Nov 2025 14:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04418v1</guid></item><item><title>QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm Design</title><link>http://arxiv.org/abs/2410.07961v2</link><description>Quantum computing is an emerging field recognized for the significant speedupit offers over classical computing through quantum algorithms. However,designing and implementing quantum algorithms pose challenges due to thecomplex nature of quantum mechanics and the necessity for precise control overquantum states. Despite the significant advancements in AI, there has been alack of datasets specifically tailored for this purpose. In this work, weintroduce QCircuitBench, the first benchmark dataset designed to evaluate AI'scapability in designing and implementing quantum algorithms using quantumprogramming languages. Unlike using AI for writing traditional codes, this taskis fundamentally more complicated due to highly flexible design space. Our keycontributions include: 1. A general framework which formulates the key featuresof quantum algorithm design for Large Language Models. 2. Implementations forquantum algorithms from basic primitives to advanced applications, spanning 3task suites, 25 algorithms, and 120,290 data points. 3. Automatic validationand verification functions, allowing for iterative evaluation and interactivereasoning without human inspection. 4. Promising potential as a trainingdataset through preliminary fine-tuning results. We observed severalinteresting experimental phenomena: LLMs tend to exhibit consistent errorpatterns, and fine-tuning does not always outperform few-shot learning. In all,QCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithmdesign, and it reveals limitations of LLMs in this domain.</description><author>Rui Yang, Ziruo Wang, Yuntian Gu, Tianyi Chen, Yitao Liang, Tongyang Li</author><pubDate>Thu, 06 Nov 2025 14:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07961v2</guid></item><item><title>Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning</title><link>http://arxiv.org/abs/2511.04406v1</link><description>Data quality and its effective selection are fundamental to improving theperformance of machine translation models, serving as cornerstones forachieving robust and reliable translation systems. This paper presents a dataselection methodology specifically designed for fine-tuning machine translationsystems, which leverages the synergy between a learner model and a pre-trainedreference model to enhance overall training effectiveness. By defining alearnability score, our approach systematically evaluates the utility of datapoints for training, ensuring that only the most relevant and impactfulexamples contribute to the fine-tuning process. Furthermore, our method employsa batch selection strategy which considers interdependencies among data points,optimizing the efficiency of the training process while maintaining a focus ondata relevance. Experiments on English to Persian and several other languagepairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate thatour method can achieve up to a fivefold improvement in data efficiency comparedto an iid baseline. Experimental results indicate that our approach improvescomputational efficiency by 24 when utilizing cached embeddings, as it requiresfewer training data points. Additionally, it enhances generalization, resultingin superior translation performance compared to random selection method.</description><author>Mohammad Amin Ghanizadeh, Mohammad Javad Dousti</author><pubDate>Thu, 06 Nov 2025 14:33:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04406v1</guid></item><item><title>Multimodal Cancer Modeling in the Age of Foundation Model Embeddings</title><link>http://arxiv.org/abs/2505.07683v3</link><description>The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as alarge-scale reference dataset in cancer through its harmonized genomics,clinical, and imaging data. Numerous prior studies have developed bespoke deeplearning models over TCGA for tasks such as cancer survival prediction. Amodern paradigm in biomedical deep learning is the development of foundationmodels (FMs) to derive feature embeddings agnostic to a specific modeling task.Biomedical text especially has seen growing development of FMs. While TCGAcontains free-text data as pathology reports, these have been historicallyunderutilized. Here, we investigate the ability to train classical machinelearning models over multimodal, zero-shot FM embeddings of cancer data. Wedemonstrate the ease and additive effect of multimodal fusion, outperformingunimodal models. Further, we show the benefit of including pathology reporttext and rigorously evaluate the effect of model-based text summarization andhallucination. Overall, we propose an embedding-centric approach to multimodalcancer modeling.</description><author>Steven Song, Morgan Borjigin-Wang, Irene Madejski, Robert L. Grossman</author><pubDate>Thu, 06 Nov 2025 14:32:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.07683v3</guid></item><item><title>Online Bayesian Experimental Design for Partially Observed Dynamical Systems</title><link>http://arxiv.org/abs/2511.04403v1</link><description>Bayesian experimental design (BED) provides a principled framework foroptimizing data collection, but existing approaches do not apply to crucialreal-world settings such as dynamical systems with partial observability, whereonly noisy and incomplete observations are available. These systems arenaturally modeled as state-space models (SSMs), where latent states mediate thelink between parameters and data, making the likelihood -- and thusinformation-theoretic objectives like the expected information gain (EIG) --intractable. In addition, the dynamical nature of the system requires onlinealgorithms that update posterior distributions and select designs sequentiallyin a computationally efficient manner. We address these challenges by derivingnew estimators of the EIG and its gradient that explicitly marginalize latentstates, enabling scalable stochastic optimization in nonlinear SSMs. Ourapproach leverages nested particle filters (NPFs) for efficient onlineinference with convergence guarantees. Applications to realistic models, suchas the susceptible-infected-recovered (SIR) and a moving source location task,show that our framework successfully handles both partial observability andonline computation.</description><author>Sara Pérez-Vieites, Sahel Iqbal, Simo Särkkä, Dominik Baumann</author><pubDate>Thu, 06 Nov 2025 14:29:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04403v1</guid></item><item><title>Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness</title><link>http://arxiv.org/abs/2511.04401v1</link><description>Deep learning models achieve strong performance across various domains butoften rely on spurious correlations, making them vulnerable to distributionshifts. This issue is particularly severe in subpopulation shift scenarios,where models struggle in underrepresented groups. While existing methods havemade progress in mitigating this issue, their performance gains are stillconstrained. They lack a rigorous theoretical framework connecting theembedding space representations with worst-group error. To address thislimitation, we propose Spurious Correlation-Aware Embedding Regularization forWorst-Group Robustness (SCER), a novel approach that directly regularizesfeature representations to suppress spurious cues. We show theoretically thatworst-group error is influenced by how strongly the classifier relies onspurious versus core directions, identified from differences in group-wise meanembeddings across domains and classes. By imposing theoretical constraints atthe embedding level, SCER encourages models to focus on core features whilereducing sensitivity to spurious patterns. Through systematic evaluation onmultiple vision and language, we show that SCER outperforms priorstate-of-the-art studies in worst-group accuracy. Our code is available at\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.</description><author>Subeen Park, Joowang Kim, Hakyung Lee, Sunjae Yoo, Kyungwoo Song</author><pubDate>Thu, 06 Nov 2025 14:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04401v1</guid></item><item><title>DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns</title><link>http://arxiv.org/abs/2507.20343v5</link><description>We present DYNARTmo, a dynamic articulatory model designed to visualizespeech articulation processes in a two-dimensional midsagittal plane. The modelbuilds upon the UK-DYNAMO framework and integrates principles of articulatoryunderspecification, segmental and gestural control, and coarticulation.DYNARTmo simulates six key articulators based on ten continuous and sixdiscrete control parameters, allowing for the generation of both vocalic andconsonantal articulatory configurations. The current implementation is embeddedin a web-based application (SpeechArticulationTrainer) that includes sagittal,glottal, and palatal views, making it suitable for use in phonetics educationand speech therapy. While this paper focuses on the static modeling aspects,future work will address dynamic movement generation and integration witharticulatory-acoustic modules.</description><author>Bernd J. Kröger</author><pubDate>Thu, 06 Nov 2025 14:27:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.20343v5</guid></item><item><title>DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale</title><link>http://arxiv.org/abs/2511.04394v1</link><description>DORAEMON is an open-source PyTorch library that unifies visual objectmodeling and representation learning across diverse scales. A singleYAML-driven workflow covers classification, retrieval and metric learning; morethan 1000 pretrained backbones are exposed through a timm-compatible interface,together with modular losses, augmentations and distributed-training utilities.Reproducible recipes match or exceed reference results on ImageNet-1K,MS-Celeb-1M and Stanford online products, while one-command export to ONNX orHuggingFace bridges research and deployment. By consolidating datasets, models,and training techniques into one platform, DORAEMON offers a scalablefoundation for rapid experimentation in visual recognition and representationlearning, enabling efficient transfer of research advances to real-worldapplications. The repository is available at https://github.com/wuji3/DORAEMON.</description><author>Ke Du, Yimin Peng, Chao Gao, Fan Zhou, Siqiao Xue</author><pubDate>Thu, 06 Nov 2025 14:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04394v1</guid></item><item><title>Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach</title><link>http://arxiv.org/abs/2511.04393v1</link><description>Large language models (LLMs) are increasingly deployed as "agents" fordecision-making (DM) in interactive and dynamic environments. Yet, since theywere not originally designed for DM, recent studies show that LLMs can struggleeven in basic online DM problems, failing to achieve low regret or an effectiveexploration-exploitation tradeoff. To address this, we introduce IterativeRegret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedurethat repeatedly distills low-regret decision trajectories back into the basemodel. At each iteration, the model rolls out multiple decision trajectories,selects the k-lowest regret ones, and fine-tunes itself on them. Unlike priormethods that (a) distill action sequences from known DM algorithms or (b) relyon manually crafted chain-of-thought templates, our approach leverages theregret metric to elicit the model's own DM ability and reasoning rationales.This reliance on model-generated reasoning avoids rigid output engineering andprovides more flexible, natural-language training signals. Empirical resultsshow that Iterative RMFT improves LLMs' DM performance across diverse models -from Transformers with numerical input/output, to open-weight LLMs, andadvanced closed-weight models like GPT-4o mini. Its flexibility in output andreasoning formats enables generalization across tasks with varying horizons,action spaces, reward processes, and natural-language contexts. Finally, weprovide theoretical insight showing that a single-layer Transformer under thisparadigm can act as a no-regret learner in a simplified setting. Overall,Iterative RMFT offers a principled and general post-training framework forenhancing LLMs' decision-making capabilities.</description><author>Chanwoo Park, Ziyang Chen, Asuman Ozdaglar, Kaiqing Zhang</author><pubDate>Thu, 06 Nov 2025 14:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04393v1</guid></item><item><title>BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems</title><link>http://arxiv.org/abs/2511.04388v1</link><description>Depth estimation is one of the key technologies for realizing 3D perceptionin unmanned systems. Monocular depth estimation has been widely researchedbecause of its low-cost advantage, but the existing methods face the challengesof poor depth estimation performance and blurred object boundaries on embeddedsystems. In this paper, we propose a novel monocular depth estimation model,BoRe-Depth, which contains only 8.7M parameters. It can accurately estimatedepth maps on embedded systems and significantly improves boundary quality.Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) whichadaptively fuses depth features to enhance boundary detail representation.Secondly, we integrate semantic knowledge into the encoder to improve theobject recognition and boundary perception capabilities. Finally, BoRe-Depth isdeployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. Wedemonstrate that the proposed model significantly outperforms previouslightweight models on multiple challenging datasets, and we provide detailedablation studies for the proposed methods. The code is available athttps://github.com/liangxiansheng093/BoRe-Depth.</description><author>Chang Liu, Juan Li, Sheng Zhang, Chang Liu, Jie Li, Xu Zhang</author><pubDate>Thu, 06 Nov 2025 14:17:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04388v1</guid></item><item><title>Universal Fourier Neural Operators for periodic homogenization problems in linear elasticity</title><link>http://arxiv.org/abs/2507.12233v3</link><description>Solving cell problems in homogenization is hard, and available deep-learningframeworks fail to match the speed and generality of traditional computationalframeworks. More to the point, it is generally unclear what to expect ofmachine-learning approaches, let alone single out which approaches arepromising. In the work at hand, we advocate Fourier Neural Operators (FNOs) formicromechanics, empowering them by insights from computational micromechanicsmethods based on the fast Fourier transform (FFT). We construct an FNOsurrogate mimicking the basic scheme foundational for FFT-based methods andshow that the resulting operator predicts solutions to cell problems witharbitrary stiffness distribution only subject to a material-contrast constraintup to a desired accuracy. In particular, there are no restrictions on thematerial symmetry like isotropy, on the number of phases and on the geometry ofthe interfaces between materials. Also, the provided fidelity is sharp anduniform, providing explicit guarantees leveraging our physical empowerment ofFNOs. To show the desired universal approximation property, we construct an FNOexplicitly that requires no training to begin with. Still, the obtained neuraloperator complies with the same memory requirements as the basic scheme andcomes with runtimes proportional to classical FFT solvers. In particular,large-scale problems with more than 100 million voxels are readily handled. Thegoal of this work is to underline the potential of FNOs for solvingmicromechanical problems, linking FFT-based methods to FNOs. This connection isexpected to provide a fruitful exchange between both worlds.</description><author>Binh Huy Nguyen, Matti Schneider</author><pubDate>Thu, 06 Nov 2025 14:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.12233v3</guid></item><item><title>Text2VectorSQL: Towards a Unified Interface for Vector Search and SQL Queries</title><link>http://arxiv.org/abs/2506.23071v2</link><description>The proliferation of unstructured data poses a fundamental challenge totraditional database interfaces. While Text-to-SQL has democratized access tostructured data, it remains incapable of interpreting semantic or multi-modalqueries. Concurrently, vector search has emerged as the de facto standard forquerying unstructured data, but its integration with SQL-termed VectorSQL-stillrelies on manual query crafting and lacks standardized evaluationmethodologies, creating a significant gap between its potential and practicalapplication. To bridge this fundamental gap, we introduce and formalize Text2VectorSQL, anovel task to establish a unified natural language interface for seamlesslyquerying both structured and unstructured data. To catalyze research in thisnew domain, we present a comprehensive foundational ecosystem, including: (1) Ascalable and robust pipeline for synthesizing high-quality Text-to-VectorSQLtraining data. (2) VectorSQLBench, the first large-scale, multi-facetedbenchmark for this task, encompassing 12 distinct combinations across threedatabase backends (SQLite, PostgreSQL, ClickHouse) and four data sources (BIRD,Spider, arXiv, Wikipedia). (3) Several novel evaluation metrics designed formore nuanced performance analysis. Extensive experiments not only confirmstrong baseline performance with our trained models, but also reveal the recalldegradation challenge: the integration of SQL filters with vector search canlead to more pronounced result omissions than in conventional filtered vectorsearch. By defining the core task, delivering the essential data and evaluationinfrastructure, and identifying key research challenges, our work lays theessential groundwork to build the next generation of unified and intelligentdata interfaces. Our repository is available athttps://github.com/OpenDCAI/Text2VectorSQL.</description><author>Zhengren Wang, Dongwen Yao, Bozhou Li, Dongsheng Ma, Bo Li, Zhiyu Li, Feiyu Xiong, Bin Cui, Linpeng Tang, Wentao Zhang</author><pubDate>Thu, 06 Nov 2025 14:14:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.23071v2</guid></item><item><title>Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA</title><link>http://arxiv.org/abs/2511.04384v1</link><description>We present a multi-task framework for the MediaEval Medico 2025 challenge,leveraging a LoRA-tuned Florence-2 model for simultaneous visual questionanswering (VQA), explanation generation, and visual grounding. The proposedsystem integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answerlearning, (2) a synthetically enriched explanation dataset offering structuredmedical reasoning, and (3) text-to-region pairs linking visual features withsegmentation masks. This multi-task setup enables the model to jointly learnvisual grounding, reasoning, and interpretation, producing responses that areboth accurate and interpretable. Extensive evaluation demonstrates that ourapproach substantially improves over single-task baselines in both answeraccuracy and visual localization, highlighting the effectiveness of groundedmulti-task learning for medical VQA applications.</description><author>Itbaan Safwan, Muhammad Annas Shaikh, Muhammad Haaris, Ramail Khan, Muhammad Atif Tahir</author><pubDate>Thu, 06 Nov 2025 14:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04384v1</guid></item><item><title>Hemorica: A Comprehensive CT Scan Dataset for Automated Brain Hemorrhage Classification, Segmentation, and Detection</title><link>http://arxiv.org/abs/2509.22993v2</link><description>Timely diagnosis of Intracranial hemorrhage (ICH) on Computed Tomography (CT)scans remains a clinical priority, yet the development of robust ArtificialIntelligence (AI) solutions is still hindered by fragmented public data. Toclose this gap, we introduce Hemorica, a publicly available collection of 372head CT examinations acquired between 2012 and 2024. Each scan has beenexhaustively annotated for five ICH subtypes-epidural (EPH), subdural (SDH),subarachnoid (SAH), intraparenchymal (IPH), and intraventricular (IVH)-yieldingpatient-wise and slice-wise classification labels, subtype-specific boundingboxes, two-dimensional pixel masks and three-dimensional voxel masks. Adouble-reading workflow, preceded by a pilot consensus phase and supported byneurosurgeon adjudication, maintained low inter-rater variability.Comprehensive statistical analysis confirms the clinical realism of thedataset. To establish reference baselines, standard convolutional andtransformer architectures were fine-tuned for binary slice classification andhemorrhage segmentation. With only minimal fine-tuning, lightweight models suchas MobileViT-XS achieved an F1 score of 87.8% in binary classification, whereasa U-Net with a DenseNet161 encoder reached a Dice score of 85.5% for binarylesion segmentation that validate both the quality of the annotations and thesufficiency of the sample size. Hemorica therefore offers a unified,fine-grained benchmark that supports multi-task and curriculum learning,facilitates transfer to larger but weakly labelled cohorts, and facilitates theprocess of designing an AI-based assistant for ICH detection and quantificationsystems.</description><author>Kasra Davoodi, Mohammad Hoseyni, Javad Khoramdel, Reza Barati, Reihaneh Mortazavi, Amirhossein Nikoofard, Mahdi Aliyari-Shoorehdeli, Jaber Hatam Parikhan</author><pubDate>Thu, 06 Nov 2025 14:05:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22993v2</guid></item><item><title>MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers</title><link>http://arxiv.org/abs/2511.04376v1</link><description>Music editing has emerged as an important and practical area of artificialintelligence, with applications ranging from video game and film musicproduction to personalizing existing tracks according to user preferences.However, existing models face significant limitations, such as being restrictedto editing synthesized music generated by their own models, requiring highlyprecise prompts, or necessitating task-specific retraining, thus lacking truezero-shot capability. Leveraging recent advances in rectified flow anddiffusion transformers, we introduce MusRec, the first zero-shot text-to-musicediting model capable of performing diverse editing tasks on real-world musicefficiently and effectively. Experimental results demonstrate that our approachoutperforms existing methods in preserving musical content, structuralconsistency, and editing fidelity, establishing a strong foundation forcontrollable music editing in real-world scenarios.</description><author>Ali Boudaghi, Hadi Zare</author><pubDate>Thu, 06 Nov 2025 14:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04376v1</guid></item><item><title>Will Large Language Models Transform Clinical Prediction?</title><link>http://arxiv.org/abs/2505.18246v2</link><description>Objective: Large language models (LLMs) are attracting increasing interest inhealthcare. This commentary evaluates the potential of LLMs to improve clinicalprediction models (CPMs) for diagnostic and prognostic tasks, with a focus ontheir ability to process longitudinal electronic health record (EHR) data. Findings: LLMs show promise in handling multimodal and longitudinal EHR dataand can support multi-outcome predictions for diverse health conditions.However, methodological, validation, infrastructural, and regulatory chal-lenges remain. These include inadequate methods for time-to-event modelling,poor calibration of predictions, limited external validation, and biasaffecting underrepresented groups. High infrastructure costs and the absence ofclear regulatory frameworks further prevent adoption. Implications: Further work and interdisciplinary collaboration are needed tosupport equitable and effective integra- tion into the clinical prediction.Developing temporally aware, fair, and explainable models should be a priorityfocus for transforming clinical prediction workflow.</description><author>Yusuf Yildiz, Goran Nenadic, Meghna Jani, David A. Jenkins</author><pubDate>Thu, 06 Nov 2025 13:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.18246v2</guid></item><item><title>Causal Regime Detection in Energy Markets With Augmented Time Series Structural Causal Models</title><link>http://arxiv.org/abs/2511.04361v1</link><description>Energy markets exhibit complex causal relationships between weather patterns,generation technologies, and price formation, with regime changes occurringcontinuously rather than at discrete break points. Current approaches modelelectricity prices without explicit causal interpretation or counterfactualreasoning capabilities. We introduce Augmented Time Series Causal Models(ATSCM) for energy markets, extending counterfactual reasoning frameworks tomultivariate temporal data with learned causal structure. Our approach modelsenergy systems through interpretable factors (weather, generation mix, demandpatterns), rich grid dynamics, and observable market variables. We integrateneural causal discovery to learn time-varying causal graphs without requiringground truth DAGs. Applied to real-world electricity price data, ATSCM enablesnovel counterfactual queries such as "What would prices be under differentrenewable generation scenarios?".</description><author>Dennis Thumm</author><pubDate>Thu, 06 Nov 2025 13:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04361v1</guid></item><item><title>A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory</title><link>http://arxiv.org/abs/2507.01110v3</link><description>Gaussian Splatting has emerged as a high-performance technique for novel viewsynthesis, enabling real-time rendering and high-quality reconstruction ofsmall scenes. However, scaling to larger environments has so far relied onpartitioning the scene into chunks -- a strategy that introduces artifacts atchunk boundaries, complicates training across varying scales, and is poorlysuited to unstructured scenarios such as city-scale flyovers combined withstreet-level views. Moreover, rendering remains fundamentally limited by GPUmemory, as all visible chunks must reside in VRAM simultaneously. We introduceA LoD of Gaussians, a framework for training and rendering ultra-large-scaleGaussian scenes on a single consumer-grade GPU -- without partitioning. Ourmethod stores the full scene out-of-core (e.g., in CPU memory) and trains aLevel-of-Detail (LoD) representation directly, dynamically streaming only therelevant Gaussians. A hybrid data structure combining Gaussian hierarchies withSequential Point Trees enables efficient, view-dependent LoD selection, while alightweight caching and view scheduling system exploits temporal coherence tosupport real-time streaming and rendering. Together, these innovations enableseamless multi-scale reconstruction and interactive visualization of complexscenes -- from broad aerial views to fine-grained ground-level details.</description><author>Felix Windisch, Thomas Köhler, Lukas Radl, Michael Steiner, Dieter Schmalstieg, Markus Steinberger</author><pubDate>Thu, 06 Nov 2025 13:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.01110v3</guid></item><item><title>GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies</title><link>http://arxiv.org/abs/2511.04357v1</link><description>Deploying autonomous robots that can learn new skills from demonstrations isan important challenge of modern robotics. Existing solutions often applyend-to-end imitation learning with Vision-Language Action (VLA) models orsymbolic approaches with Action Model Learning (AML). On the one hand, currentVLA models are limited by the lack of high-level symbolic planning, whichhinders their abilities in long-horizon tasks. On the other hand, symbolicapproaches in AML lack generalization and scalability perspectives. In thispaper we present a new neuro-symbolic approach, GraSP-VLA, a framework thatuses a Continuous Scene Graph representation to generate a symbolicrepresentation of human demonstrations. This representation is used to generatenew planning domains during inference and serves as an orchestrator forlow-level VLA policies, scaling up the number of actions that can be reproducedin a row. Our results show that GraSP-VLA is effective for modeling symbolicrepresentations on the task of automatic planning domain generation fromobservations. In addition, results on real-world experiments show the potentialof our Continuous Scene Graph representation to orchestrate low-level VLApolicies in long-horizon tasks.</description><author>Maëlic Neau, Zoe Falomir, Paulo E. Santos, Anne-Gwenn Bosser, Cédric Buche</author><pubDate>Thu, 06 Nov 2025 13:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04357v1</guid></item><item><title>Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks</title><link>http://arxiv.org/abs/2511.04355v1</link><description>Large Language Models (LLMs) have achieved remarkable success in codegeneration, and the race to improve their performance has become a centralfocus of AI research. Benchmarks and leaderboards are increasingly popular,offering quantitative rankings of LLMs. However, they provide limited insightinto the tasks that LLMs consistently fail to solve - information that iscrucial for understanding current limitations and guiding the development ofmore capable models. To address this gap, we examined code generation tasksacross four popular benchmarks, identifying those that major LLMs are mostlikely to fail. To understand the causes of these failures, we investigatedwhether the static complexity of solution code contributes to them, followed bya systematic inspection of 114 tasks that LLMs consistently struggled with. Ouranalysis revealed four recurring patterns of weaknesses in LLMs, as well ascommon complications within benchmark tasks that most often lead to failure.</description><author>Amir Molzam Sharifloo, Maedeh Heydari, Parsa Kazerooni, Daniel Maninger, Mira Mezini</author><pubDate>Thu, 06 Nov 2025 13:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04355v1</guid></item><item><title>ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models</title><link>http://arxiv.org/abs/2509.24239v2</link><description>Recent large language models (LLMs) have shown strong reasoning capabilities.However, a critical question remains: do these models possess genuine reasoningskills particularly complex strategic reasoning or are they primarily excellingat sophisticated pattern recognition within their training data? To addressthis question, this paper presents a chess testbed, ChessArena, to evaluate thestrategic reasoning capabilities of LLMs. Chess requires complex strategicreasoning capabilities including long-term planning, strict rule comprehension,and multi-turn conversation memorization. Specifically, ChessArena is acompetitive framework where LLMs play against each other, under four differentplay modes. The testbed is equipped with a ranking algorithm and a leaderboard.The testbed can also evaluate fine-grained capabilities including basicunderstanding, move selection, and puzzle solving. Over 13 LLMs with differentmodes are evaluated in ChessArena, playing over 800 games. The results revealsignificant shortcomings in current LLMs: no model can beat Maia-1100 (a chessengine at human amateur level), while some even failed to defeat a randomplayer that selects moves arbitrarily. We also present a strong baseline to thetestbed: our fine-tuned Qwen3-8B substantially improved performance,approaching much larger state-of-the-art reasoning models.</description><author>Jincheng Liu, Sijun He, Jingjing Wu, Xiangsen Wang, Yang Chen, Zhaoqi Kuang, Siqi Bao, Yuan Yao</author><pubDate>Thu, 06 Nov 2025 13:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.24239v2</guid></item><item><title>A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications</title><link>http://arxiv.org/abs/2511.04349v1</link><description>Background In analytical chemistry, spatial information about materials iscommonly captured through imaging techniques, such as traditional color camerasor with advanced hyperspectral cameras and microscopes. However, efficientlyextracting and analyzing this spatial information for exploratory andpredictive purposes remains a challenge, especially when using traditionalchemometric methods. Recent advances in deep learning and artificialintelligence have significantly enhanced image processing capabilities,enabling the extraction of multiscale deep features that are otherwisechallenging to capture with conventional image processing techniques. Despitethe wide availability of open-source deep learning models, adoption inanalytical chemistry remains limited because of the absence of structured,step-by-step guidance for implementing these models. Results This tutorial aims to bridge this gap by providing a step-by-stepguide for applying deep learning approaches to extract spatial information fromimaging data and integrating it with other data sources, such as spectralinformation. Importantly, the focus of this work is not on training deeplearning models for image processing but on using existing open source modelsto extract deep features from imaging data. Significance The tutorial provides MATLAB code tutorial demonstrations,showcasing the processing of imaging data from various imaging modalitiescommonly encountered in analytical chemistry. Readers must run the tutorialsteps on their own datasets using the codes presented in this tutorial.</description><author>Puneet Mishra, Martijntje Vollebregt, Yizhou Ma, Maria Font-i-Furnols</author><pubDate>Thu, 06 Nov 2025 13:29:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04349v1</guid></item><item><title>Small Singular Values Matter: A Random Matrix Analysis of Transformer Models</title><link>http://arxiv.org/abs/2410.17770v3</link><description>This work analyzes singular-value spectra of weight matrices in pretrainedtransformer models to understand how information is stored at both ends of thespectrum. Using Random Matrix Theory (RMT) as a zero information hypothesis, weassociate agreement with RMT as evidence of randomness and deviations asevidence for learning. Surprisingly, we observe pronounced departures from RMTnot only among the largest singular values -- the usual outliers -- but alsoamong the smallest ones. A comparison of the associated singular vectors withthe eigenvectors of the activation covariance matrices shows that there isconsiderable overlap wherever RMT is violated. Thus, significant directions inthe data are captured by small singular values and their vectors as well as bythe large ones. We confirm this empirically: zeroing out the singular valuesthat deviate from RMT raises language-model perplexity far more than removingvalues from the bulk, and after fine-tuning the smallest decile can be thethird most influential part of the spectrum. To explain how vectors linked tosmall singular values can carry more information than those linked to largervalues, we propose a linear random-matrix model. Our findings highlight theoverlooked importance of the low end of the spectrum and provide theoreticaland practical guidance for SVD-based pruning and compression of large languagemodels.</description><author>Max Staats, Matthias Thamm, Bernd Rosenow</author><pubDate>Thu, 06 Nov 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17770v3</guid></item><item><title>Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection</title><link>http://arxiv.org/abs/2511.04347v1</link><description>Accurate 3D object detection is essential for automated vehicles to navigatesafely in complex real-world environments. Bird's Eye View (BEV)representations, which project multi-sensor data into a top-down spatialformat, have emerged as a powerful approach for robust perception. AlthoughBEV-based fusion architectures have demonstrated strong performance throughmultimodal integration, the effects of sensor occlusions, caused byenvironmental conditions such as fog, haze, or physical obstructions, on 3Ddetection accuracy remain underexplored. In this work, we investigate theimpact of occlusions on both camera and Light Detection and Ranging (LiDAR)outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.Detection performance is measured using mean Average Precision (mAP) and thenuScenes Detection Score (NDS). Our results show that moderate cameraocclusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection isbased only on the camera. On the other hand, LiDAR sharply drops in performanceonly under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),with a severe impact on long-range detection. In fused settings, the effectdepends on which sensor is occluded: occluding the camera leads to a minor 4.1%drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the taskof 3D object detection. Our results highlight the need for future research intoocclusion-aware evaluation methods and improved sensor fusion techniques thatcan maintain detection accuracy in the presence of partial sensor failure ordegradation due to adverse environmental conditions.</description><author>Sanjay Kumar, Tim Brophy, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising</author><pubDate>Thu, 06 Nov 2025 13:25:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04347v1</guid></item><item><title>Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset</title><link>http://arxiv.org/abs/2511.04344v1</link><description>This paper presents a comprehensive evaluation of nine convolutional neuralnetwork architectures for binary classification of horses and motorcycles inthe VOC 2008 dataset. We address the significant class imbalance problem byimplementing minority-class augmentation techniques. Our experiments comparemodern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, andVision Transformer across multiple performance metrics. Results demonstratesubstantial performance variations, with ConvNeXt-Tiny achieving the highestAverage Precision (AP) of 95.53% for horse detection and 89.12% for motorcycledetection. We observe that data augmentation significantly improves minorityclass detection, particularly benefiting deeper architectures. This studyprovides insights into architecture selection for imbalanced binaryclassification tasks and quantifies the impact of data augmentation strategiesin mitigating class imbalance issues in object detection.</description><author>Muhammad Annas Shaikh, Hamza Zaman, Arbaz Asif</author><pubDate>Thu, 06 Nov 2025 13:24:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04344v1</guid></item><item><title>Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning</title><link>http://arxiv.org/abs/2511.04341v1</link><description>Test-time reasoning architectures such as those following the Generate-Verifyparadigm -- where a model iteratively refines or verifies its own generatedoutputs -- prioritise generation and verification but exclude the monitoringprocesses that determine when and how reasoning should begin. This omission maycontribute to the prefix dominance trap, in which models commit early tosuboptimal reasoning paths and seldom recover, yielding roughly 20% accuracyloss. We address this architectural gap by formalising Flavell's and Nelson andNarens' metacognitive theories into computational specifications, proposing theMonitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verifyparadigm by adding explicit monitoring that captures metacognitive experiences(from difficulty assessments to confidence judgements) before generation beginsand refines future monitoring through verification feedback. Though we presentno empirical validation, this work provides the first systematic computationaltranslation of foundational metacognitive theories, offering a principledvocabulary for understanding reasoning system failures and suggesting specificarchitectural interventions for future test-time reasoning designs.</description><author>Nick Oh, Fernand Gobet</author><pubDate>Thu, 06 Nov 2025 13:22:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04341v1</guid></item><item><title>CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding</title><link>http://arxiv.org/abs/2412.07236v6</link><description>Electroencephalography (EEG) is a non-invasive technique to measure andrecord brain electrical activity, widely used in various BCI and healthcareapplications. Early EEG decoding methods rely on supervised learning, limitedby specific tasks and datasets, hindering model performance andgeneralizability. With the success of large language models, there is a growingbody of studies focusing on EEG foundation models. However, these studies stillleave challenges: Firstly, most of existing EEG foundation models employ fullEEG modeling strategy. It models the spatial and temporal dependencies betweenall EEG patches together, but ignores that the spatial and temporaldependencies are heterogeneous due to the unique structural characteristics ofEEG signals. Secondly, existing EEG foundation models have limitedgeneralizability on a wide range of downstream BCI tasks due to varying formatsof EEG data, making it challenging to adapt to. To address these challenges, wepropose a novel foundation model called CBraMod. Specifically, we devise acriss-cross transformer as the backbone to thoroughly leverage the structuralcharacteristics of EEG signals, which can model spatial and temporaldependencies separately through two parallel attention mechanisms. And weutilize an asymmetric conditional positional encoding scheme which can encodepositional information of EEG patches and be easily adapted to the EEG withdiverse formats. CBraMod is pre-trained on a very large corpus of EEG throughpatch-based masked EEG reconstruction. We evaluate CBraMod on up to 10downstream BCI tasks (12 public datasets). CBraMod achieves thestate-of-the-art performance across the wide range of tasks, proving its strongcapability and generalizability. The source code is publicly available athttps://github.com/wjq-learning/CBraMod.</description><author>Jiquan Wang, Sha Zhao, Zhiling Luo, Yangxuan Zhou, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan</author><pubDate>Thu, 06 Nov 2025 13:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07236v6</guid></item><item><title>How do Transformers Learn Implicit Reasoning?</title><link>http://arxiv.org/abs/2505.23653v2</link><description>Recent work suggests that large language models (LLMs) can perform multi-hopreasoning implicitly -- producing correct answers without explicitlyverbalizing intermediate steps -- but the underlying mechanisms remain poorlyunderstood. In this paper, we study how such implicit reasoning emerges bytraining transformers from scratch in a controlled symbolic environment. Ouranalysis reveals a three-stage developmental trajectory: early memorization,followed by in-distribution generalization, and eventually cross-distributiongeneralization. We find that training with atomic triples is not necessary butaccelerates learning, and that second-hop generalization relies on query-levelexposure to specific compositional structures. To interpret these behaviors, weintroduce two diagnostic tools: cross-query semantic patching, which identifiessemantically reusable intermediate representations, and a cosine-basedrepresentational lens, which reveals that successful reasoning correlates withthe cosine-base clustering in hidden space. This clustering phenomenon in turnprovides a coherent explanation for the behavioral dynamics observed acrosstraining, linking representational structure to reasoning capability. Thesefindings provide new insights into the interpretability of implicit multi-hopreasoning in LLMs, helping to clarify how complex reasoning processes unfoldinternally and offering pathways to enhance the transparency of such models.</description><author>Jiaran Ye, Zijun Yao, Zhidian Huang, Liangming Pan, Jinxin Liu, Yushi Bai, Amy Xin, Weichuan Liu, Xiaoyin Che, Lei Hou, Juanzi Li</author><pubDate>Thu, 06 Nov 2025 13:18:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.23653v2</guid></item><item><title>Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography</title><link>http://arxiv.org/abs/2511.04334v1</link><description>The accurate delineation of tumours in radiological images like ComputedTomography is a very specialised and time-consuming task, and currently abottleneck preventing quantitative analyses to be performed routinely in theclinical setting. For this reason, developing methods for the automatedsegmentation of tumours in medical imaging is of the utmost importance and hasdriven significant efforts in recent years. However, challenges regarding theimpracticality of 3D scans, given the large amount of voxels to be analysed,usually requires the downsampling of such images or using patches thereof whenapplying traditional convolutional neural networks. To overcome this problem,in this paper we propose a new methodology that uses, divided into two stages,voxel sparsification and submanifold sparse convolutional networks. This methodallows segmentations to be performed with high-resolution inputs and a native3D model architecture, obtaining state-of-the-art accuracies whilesignificantly reducing the computational resources needed in terms of GPUmemory and time. We studied the deployment of this methodology in the contextof Computed Tomography images of renal cancer patients from the KiTS23challenge, and our method achieved results competitive with the challengewinners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%for tumours + cysts, and 80.3% for tumours alone. Crucially, our method alsooffers significant computational improvements, achieving up to a 60% reductionin inference time and up to a 75\% reduction in VRAM usage compared to anequivalent dense architecture, across both CPU and various GPU cards tested.</description><author>Saúl Alonso-Monsalve, Leigh H. Whitehead, Adam Aurisano, Lorena Escudero Sanchez</author><pubDate>Thu, 06 Nov 2025 13:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04334v1</guid></item></channel></rss>