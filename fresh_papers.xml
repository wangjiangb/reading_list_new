<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 12 May 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The Un-Kidnappable Robot: Acoustic Localization of Sneaking People</title><link>http://arxiv.org/abs/2310.03743v2</link><description>How easy is it to sneak up on a robot? We examine whether we can detectpeople using only the incidental sounds they produce as they move, even whenthey try to be quiet. We collect a robotic dataset of high-quality 4-channelaudio paired with 360 degree RGB data of people moving in different indoorsettings. We train models that predict if there is a moving person nearby andtheir location using only audio. We implement our method on a robot, allowingit to track a single person moving quietly with only passive audio sensing. Fordemonstration videos, see our project page:https://sites.google.com/view/unkidnappable-robot</description><author>Mengyu Yang, Patrick Grady, Samarth Brahmbhatt, Arun Balajee Vasudevan, Charles C. Kemp, James Hays</author><pubDate>Thu, 09 May 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03743v2</guid></item><item><title>A Universal Growth Rate for Learning with Smooth Surrogate Losses</title><link>http://arxiv.org/abs/2405.05968v1</link><description>This paper presents a comprehensive analysis of the growth rate of$H$-consistency bounds (and excess error bounds) for various surrogate lossesused in classification. We prove a square-root growth rate near zero for smoothmargin-based surrogate losses in binary classification, providing both upperand lower bounds under mild assumptions. This result also translates to excesserror bounds. Our lower bound requires weaker conditions than those in previouswork for excess error bounds, and our upper bound is entirely novel. Moreover,we extend this analysis to multi-class classification with a series of novelresults, demonstrating a universal square-root growth rate for smooth comp-sumand constrained losses, covering common choices for training neural networks inmulti-class classification. Given this universal rate, we turn to the questionof choosing among different surrogate losses. We first examine how$H$-consistency bounds vary across surrogates based on the number of classes.Next, ignoring constants and focusing on behavior near zero, we identifyminimizability gaps as the key differentiating factor in these bounds. Thus, wethoroughly analyze these gaps, to guide surrogate loss selection, covering:comparisons across different comp-sum losses, conditions where gaps becomezero, and general conditions leading to small gaps. Additionally, wedemonstrate the key role of minimizability gaps in comparing excess errorbounds and $H$-consistency bounds.</description><author>Anqi Mao, Mehryar Mohri, Yutao Zhong</author><pubDate>Thu, 09 May 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05968v1</guid></item><item><title>Distilling Diffusion Models into Conditional GANs</title><link>http://arxiv.org/abs/2405.05967v1</link><description>We propose a method to distill a complex multistep diffusion model into asingle-step conditional GAN student model, dramatically accelerating inference,while preserving image quality. Our approach interprets diffusion distillationas a paired image-to-image translation task, using noise-to-image pairs of thediffusion model's ODE trajectory. For efficient regression loss computation, wepropose E-LatentLPIPS, a perceptual loss operating directly in diffusionmodel's latent space, utilizing an ensemble of augmentations. Furthermore, weadapt a diffusion model to construct a multi-scale discriminator with a textalignment loss to build an effective conditional GAN-based formulation.E-LatentLPIPS converges more efficiently than many existing distillationmethods, even accounting for dataset construction costs. We demonstrate thatour one-step generator outperforms cutting-edge one-step diffusion distillationmodels - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark.</description><author>Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, Taesung Park</author><pubDate>Thu, 09 May 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05967v1</guid></item><item><title>Natural Language Processing RELIES on Linguistics</title><link>http://arxiv.org/abs/2405.05966v1</link><description>Large Language Models (LLMs) have become capable of generating highly fluenttext in certain languages, without modules specially designed to capturegrammar or semantic coherence. What does this mean for the future of linguisticexpertise in NLP? We highlight several aspects in which NLP (still) relies onlinguistics, or where linguistic thinking can illuminate new directions. Weargue our case around the acronym $RELIES$ that encapsulates six major facetswhere linguistics contributes to NLP: $R$esources, $E$valuation, $L$ow-resourcesettings, $I$nterpretability, $E$xplanation, and the $S$tudy of language. Thislist is not exhaustive, nor is linguistics the main point of reference forevery effort under these themes; but at a macro level, these facets highlightthe enduring importance of studying machine systems vis-a-vis systems of humanlanguage.</description><author>Juri Opitz, Shira Wein, Nathan Schneider</author><pubDate>Thu, 09 May 2024 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05966v1</guid></item><item><title>Age Aware Scheduling for Differentially-Private Federated Learning</title><link>http://arxiv.org/abs/2405.05962v1</link><description>This paper explores differentially-private federated learning (FL) acrosstime-varying databases, delving into a nuanced three-way tradeoff involvingage, accuracy, and differential privacy (DP). Emphasizing the potentialadvantages of scheduling, we propose an optimization problem aimed at meetingDP requirements while minimizing the loss difference between the aggregatedmodel and the model obtained without DP constraints. To harness the benefits ofscheduling, we introduce an age-dependent upper bound on the loss, leading tothe development of an age-aware scheduling design. Simulation resultsunderscore the superior performance of our proposed scheme compared to FL withclassic DP, which does not consider scheduling as a design factor. Thisresearch contributes insights into the interplay of age, accuracy, and DP infederated learning, with practical implications for scheduling strategies.</description><author>Kuan-Yu Lin, Hsuan-Yin Lin, Yu-Pin Hsu, Yu-Chih Huang</author><pubDate>Thu, 09 May 2024 18:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05962v1</guid></item><item><title>Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask</title><link>http://arxiv.org/abs/2405.05959v1</link><description>Time Series Representation Learning (TSRL) focuses on generating informativerepresentations for various Time Series (TS) modeling tasks. TraditionalSelf-Supervised Learning (SSL) methods in TSRL fall into four main categories:reconstructive, adversarial, contrastive, and predictive, each with a commonchallenge of sensitivity to noise and intricate data nuances. Recently,diffusion-based methods have shown advanced generative capabilities. However,they primarily target specific application scenarios like imputation andforecasting, leaving a gap in leveraging diffusion models for generic TSRL. Ourwork, Time Series Diffusion Embedding (TSDE), bridges this gap as the firstdiffusion-based SSL TSRL approach. TSDE segments TS data into observed andmasked parts using an Imputation-Interpolation-Forecasting (IIF) mask. Itapplies a trainable embedding function, featuring dual-orthogonal Transformerencoders with a crossover mechanism, to the observed part. We train a reversediffusion process conditioned on the embeddings, designed to predict noiseadded to the masked part. Extensive experiments demonstrate TSDE's superiorityin imputation, interpolation, forecasting, anomaly detection, classification,and clustering. We also conduct an ablation study, present embeddingvisualizations, and compare inference speed, further substantiating TSDE'sefficiency and validity in learning representations of TS data.</description><author>Zineb Senane, Lele Cao, Valentin Leonhard Buchner, Yusuke Tashiro, Lei You, Pawel Herman, Mats Nordahl, Ruibo Tu, Vilhelm von Ehrenheim</author><pubDate>Thu, 09 May 2024 18:55:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05959v1</guid></item><item><title>OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning</title><link>http://arxiv.org/abs/2405.05957v1</link><description>Large Language Models (LLMs) have played an important role in many fields dueto their powerful capabilities.However, their massive number of parametersleads to high deployment requirements and incurs significant inference costs,which impedes their practical applications. Training smaller models is aneffective way to address this problem. Therefore, we introduce OpenBA-V2, a3.4B model derived from multi-stage compression and continual pre-training fromthe original 15B OpenBA model. OpenBA-V2 utilizes more data, more flexibletraining objectives, and techniques such as layer pruning, neural pruning, andvocabulary pruning to achieve a compression rate of 77.3\% with minimalperformance loss. OpenBA-V2 demonstrates competitive performance compared toother open-source models of similar size, achieving results close to or on parwith the 15B OpenBA model in downstream tasks such as common sense reasoningand Named Entity Recognition (NER). OpenBA-V2 illustrates that LLMs can becompressed into smaller ones with minimal performance loss by employingadvanced training objectives and data strategies, which may help deploy LLMs inresource-limited scenarios.</description><author>Dan Qiao, Yi Su, Pinzheng Wang, Jing Ye, Wenjing Xie, Yuechi Zhou, Yuyang Ding, Zecheng Tang, Jikai Wang, Yixin Ji, Yue Wang, Pei Guo, Zechen Sun, Zikang Zhang, Juntao Li, Pingfu Chao, Wenliang Chen, Guohong Fu, Guodong Zhou, Qiaoming Zhu, Min Zhang</author><pubDate>Thu, 09 May 2024 18:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05957v1</guid></item><item><title>Probing Multimodal LLMs as World Models for Driving</title><link>http://arxiv.org/abs/2405.05956v1</link><description>We provide a sober look at the application of Multimodal Large LanguageModels (MLLMs) within the domain of autonomous driving and challenge/verifysome common assumptions, focusing on their ability to reason and interpretdynamic driving scenarios through sequences of images/frames in a closed-loopcontrol environment. Despite the significant advancements in MLLMs like GPT-4V,their performance in complex, dynamic driving environments remains largelyuntested and presents a wide area of exploration. We conduct a comprehensiveexperimental study to evaluate the capability of various MLLMs as world modelsfor driving from the perspective of a fixed in-car camera. Our findings revealthat, while these models proficiently interpret individual images, theystruggle significantly with synthesizing coherent narratives or logicalsequences across frames depicting dynamic behavior. The experiments demonstrateconsiderable inaccuracies in predicting (i) basic vehicle dynamics(forward/backward, acceleration/deceleration, turning right or left), (ii)interactions with other road actors (e.g., identifying speeding cars or heavytraffic), (iii) trajectory planning, and (iv) open-set dynamic scene reasoning,suggesting biases in the models' training data. To enable this experimentalstudy we introduce a specialized simulator, DriveSim, designed to generatediverse driving scenarios, providing a platform for evaluating MLLMs in therealms of driving. Additionally, we contribute the full open-source code and anew dataset, "Eval-LLM-Drive", for evaluating MLLMs in driving. Our resultshighlight a critical gap in the current capabilities of state-of-the-art MLLMs,underscoring the need for enhanced foundation models to improve theirapplicability in real-world dynamic environments.</description><author>Shiva Sreeram, Tsun-Hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus</author><pubDate>Thu, 09 May 2024 18:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05956v1</guid></item><item><title>Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning</title><link>http://arxiv.org/abs/2405.05955v1</link><description>The emergence of large language models (LLMs) has opened up unprecedentedpossibilities for automating complex tasks that are often comparable to humanperformance. Despite their capabilities, LLMs still encounter difficulties incompleting tasks that require high levels of accuracy and complexity due totheir inherent limitations in handling multifaceted problems single-handedly.This paper introduces "Smurfs", a cutting-edge multi-agent framework designedto revolutionize the application of LLMs. By transforming a conventional LLMinto a synergistic multi-agent ensemble, Smurfs enhances task decomposition andexecution without necessitating extra training. This is achieved throughinnovative prompting strategies that allocate distinct roles within the model,thereby facilitating collaboration among specialized agents. The frameworkgives access to external tools to efficiently solve complex tasks. Ourempirical investigation, featuring the mistral-7b-instruct model as a casestudy, showcases Smurfs' superior capability in intricate tool utilizationscenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 andI3 benchmark with a remarkable 84.4% win rate, surpassing the highest recordedperformance of a GPT-4 model at 73.5%. Furthermore, through comprehensiveablation studies, we dissect the contribution of the core components of themulti-agent framework to its overall efficacy. This not only verifies theeffectiveness of the framework, but also sets a route for future exploration ofmulti-agent LLM systems.</description><author>Junzhi Chen, Juhao Liang, Benyou Wang</author><pubDate>Thu, 09 May 2024 18:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05955v1</guid></item><item><title>Frame Interpolation with Consecutive Brownian Bridge Diffusion</title><link>http://arxiv.org/abs/2405.05953v1</link><description>Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as adiffusion-based conditional image generation problem, synthesizing theintermediate frame given a random noise and neighboring frames. Due to therelatively high resolution of videos, Latent Diffusion Models (LDMs) areemployed as the conditional generation model, where the autoencoder compressesimages into latent representations for diffusion and then reconstructs imagesfrom these latent representations. Such a formulation poses a crucialchallenge: VFI expects that the output is deterministically equal to the groundtruth intermediate frame, but LDMs randomly generate a diverse set of differentimages when the model runs multiple times. The reason for the diversegeneration is that the cumulative variance (variance accumulated at each stepof generation) of generated latent representations in LDMs is large. This makesthe sampling trajectory random, resulting in diverse rather than deterministicgenerations. To address this problem, we propose our unique solution: FrameInterpolation with Consecutive Brownian Bridge Diffusion. Specifically, wepropose consecutive Brownian Bridge diffusion that takes a deterministicinitial value as input, resulting in a much smaller cumulative variance ofgenerated latent representations. Our experiments suggest that our method canimprove together with the improvement of the autoencoder and achievestate-of-the-art performance in VFI, leaving strong potential for furtherenhancement.</description><author>Zonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen</author><pubDate>Thu, 09 May 2024 18:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05953v1</guid></item><item><title>Federated Combinatorial Multi-Agent Multi-Armed Bandits</title><link>http://arxiv.org/abs/2405.05950v1</link><description>This paper introduces a federated learning framework tailored for onlinecombinatorial optimization with bandit feedback. In this setting, agents selectsubsets of arms, observe noisy rewards for these subsets without accessingindividual arm information, and can cooperate and share information at specificintervals. Our framework transforms any offline resilient single-agent$(\alpha-\epsilon)$-approximation algorithm, having a complexity of$\tilde{\mathcal{O}}(\frac{\psi}{\epsilon^\beta})$, where the logarithm isomitted, for some function $\psi$ and constant $\beta$, into an onlinemulti-agent algorithm with $m$ communicating agents and an $\alpha$-regret ofno more than $\tilde{\mathcal{O}}(m^{-\frac{1}{3+\beta}} \psi^\frac{1}{3+\beta}T^\frac{2+\beta}{3+\beta})$. This approach not only eliminates the $\epsilon$approximation error but also ensures sublinear growth with respect to the timehorizon $T$ and demonstrates a linear speedup with an increasing number ofcommunicating agents. Additionally, the algorithm is notablycommunication-efficient, requiring only a sublinear number of communicationrounds, quantified as $\tilde{\mathcal{O}}\left(\psiT^\frac{\beta}{\beta+1}\right)$. Furthermore, the framework has beensuccessfully applied to online stochastic submodular maximization using variousoffline algorithms, yielding the first results for both single-agent andmulti-agent settings and recovering specialized single-agent theoreticalguarantees. We empirically validate our approach to a stochastic datasummarization problem, illustrating the effectiveness of the proposedframework, even in single-agent scenarios.</description><author>Fares Fourati, Mohamed-Slim Alouini, Vaneet Aggarwal</author><pubDate>Thu, 09 May 2024 18:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05950v1</guid></item><item><title>CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts</title><link>http://arxiv.org/abs/2405.05949v1</link><description>Recent advancements in Multimodal Large Language Models (LLMs) have focusedprimarily on scaling by increasing text-image pair data and enhancing LLMs toimprove performance on multimodal tasks. However, these scaling approaches arecomputationally expensive and overlook the significance of improving modelcapabilities from the vision side. Inspired by the successful applications ofMixture-of-Experts (MoE) in LLMs, which improves model scalability duringtraining while keeping inference costs similar to those of smaller models, wepropose CuMo. CuMo incorporates Co-upcycled Top-K sparsely-gatedMixture-of-experts blocks into both the vision encoder and the MLP connector,thereby enhancing the multimodal LLMs with minimal additional activatedparameters during inference. CuMo first pre-trains the MLP blocks and theninitializes each expert in the MoE block from the pre-trained MLP block duringthe visual instruction tuning stage. Auxiliary losses are used to ensure abalanced loading of experts. CuMo outperforms state-of-the-art multimodal LLMsacross various VQA and visual-instruction-following benchmarks using modelswithin each model size group, all while training exclusively on open-sourceddatasets. The code and model weights for CuMo are open-sourced athttps://github.com/SHI-Labs/CuMo.</description><author>Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, Longyin Wen</author><pubDate>Thu, 09 May 2024 18:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05949v1</guid></item><item><title>An Embodied Generalist Agent in 3D World</title><link>http://arxiv.org/abs/2311.12871v3</link><description>Leveraging massive knowledge from large language models (LLMs), recentmachine learning models show notable successes in general-purpose task solvingin diverse domains such as computer vision and robotics. However, severalsignificant challenges remain: (i) most of these models rely on 2D images yetexhibit a limited capacity for 3D input; (ii) these models rarely explore thetasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoningand acting. We argue these limitations significantly hinder current models fromperforming real-world tasks and approaching general intelligence. To this end,we introduce LEO, an embodied multi-modal generalist agent that excels inperceiving, grounding, reasoning, planning, and acting in the 3D world. LEO istrained with a unified task interface, model architecture, and objective in twostages: (i) 3D vision-language (VL) alignment and (ii) 3Dvision-language-action (VLA) instruction tuning. We collect large-scaledatasets comprising diverse object-level and scene-level tasks, which requireconsiderable understanding of and interaction with the 3D world. Moreover, wemeticulously design an LLM-assisted pipeline to produce high-quality 3D VLdata. Through extensive experiments, we demonstrate LEO's remarkableproficiency across a wide spectrum of tasks, including 3D captioning, questionanswering, embodied reasoning, navigation and manipulation. Our ablativestudies and scaling analyses further provide valuable insights for developingfuture embodied generalist agents. Code and data are available on project page.</description><author>Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang</author><pubDate>Thu, 09 May 2024 18:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12871v3</guid></item><item><title>Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers</title><link>http://arxiv.org/abs/2405.05945v1</link><description>Sora unveils the potential of scaling Diffusion Transformer for generatingphotorealistic images and videos at arbitrary resolutions, aspect ratios, anddurations, yet it still lacks sufficient implementation details. In thistechnical report, we introduce the Lumina-T2X family - a series of Flow-basedLarge Diffusion Transformers (Flag-DiT) equipped with zero-initializedattention, as a unified framework designed to transform noise into images,videos, multi-view 3D objects, and audio clips conditioned on textinstructions. By tokenizing the latent spatial-temporal space and incorporatinglearnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2Xseamlessly unifies the representations of different modalities across variousspatial-temporal resolutions. This unified approach enables training within asingle framework for different modalities and allows for flexible generation ofmultimodal data at any resolution, aspect ratio, and length during inference.Advanced techniques like RoPE, RMSNorm, and flow matching enhance thestability, flexibility, and scalability of Flag-DiT, enabling models ofLumina-T2X to scale up to 7 billion parameters and extend the context window to128K tokens. This is particularly beneficial for creating ultra-high-definitionimages with our Lumina-T2I model and long 720p videos with our Lumina-T2Vmodel. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT,requires only 35% of the training computational costs of a600-million-parameter naive DiT. Our further comprehensive analysis underscoresLumina-T2X's preliminary capability in resolution extrapolation,high-resolution editing, generating consistent 3D views, and synthesizingvideos with seamless transitions. We expect that the open-sourcing ofLumina-T2X will further foster creativity, transparency, and diversity in thegenerative AI community.</description><author>Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, Renrui Zhang, Junlin Xi, Wenqi Shao, Zhengkai Jiang, Tianshuo Yang, Weicai Ye, He Tong, Jingwen He, Yu Qiao, Hongsheng Li</author><pubDate>Thu, 09 May 2024 18:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05945v1</guid></item><item><title>MRISegmentator-Abdomen: A Fully Automated Multi-Organ and Structure Segmentation Tool for T1-weighted Abdominal MRI</title><link>http://arxiv.org/abs/2405.05944v1</link><description>Background: Segmentation of organs and structures in abdominal MRI is usefulfor many clinical applications, such as disease diagnosis and radiotherapy.Current approaches have focused on delineating a limited set of abdominalstructures (13 types). To date, there is no publicly available abdominal MRIdataset with voxel-level annotations of multiple organs and structures.Consequently, a segmentation tool for multi-structure segmentation is alsounavailable. Methods: We curated a T1-weighted abdominal MRI dataset consistingof 195 patients who underwent imaging at National Institutes of Health (NIH)Clinical Center. The dataset comprises of axial pre-contrast T1, arterial,venous, and delayed phases for each patient, thereby amounting to a total of780 series (69,248 2D slices). Each series contains voxel-level annotations of62 abdominal organs and structures. A 3D nnUNet model, dubbed asMRISegmentator-Abdomen (MRISegmentator in short), was trained on this dataset,and evaluation was conducted on an internal test set and two large externaldatasets: AMOS22 and Duke Liver. The predicted segmentations were comparedagainst the ground-truth using the Dice Similarity Coefficient (DSC) andNormalized Surface Distance (NSD). Findings: MRISegmentator achieved an averageDSC of 0.861$\pm$0.170 and a NSD of 0.924$\pm$0.163 in the internal test set.On the AMOS22 dataset, MRISegmentator attained an average DSC of0.829$\pm$0.133 and a NSD of 0.908$\pm$0.067. For the Duke Liver dataset, anaverage DSC of 0.933$\pm$0.015 and a NSD of 0.929$\pm$0.021 was obtained.Interpretation: The proposed MRISegmentator provides automatic, accurate, androbust segmentations of 62 organs and structures in T1-weighted abdominal MRIsequences. The tool has the potential to accelerate research on variousclinical topics, such as abnormality detection, radiotherapy, diseaseclassification among others.</description><author>Yan Zhuang, Tejas Sudharshan Mathai, Pritam Mukherjee, Brandon Khoury, Boah Kim, Benjamin Hou, Nusrat Rabbee, Ronald M. Summers</author><pubDate>Thu, 09 May 2024 18:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05944v1</guid></item><item><title>Evaluating Real-World Robot Manipulation Policies in Simulation</title><link>http://arxiv.org/abs/2405.05941v1</link><description>The field of robotics has made significant advances towards generalist robotmanipulation policies. However, real-world evaluation of such policies is notscalable and faces reproducibility challenges, which are likely to worsen aspolicies broaden the spectrum of tasks they can perform. We identify controland visual disparities between real and simulated environments as keychallenges for reliable simulated evaluation and propose approaches formitigating these gaps without needing to craft full-fidelity digital twins ofreal-world environments. We then employ these approaches to create SIMPLER, acollection of simulated environments for manipulation policy evaluation oncommon real robot setups. Through paired sim-and-real evaluations ofmanipulation policies, we demonstrate strong correlation between policyperformance in SIMPLER environments and in the real world. Additionally, wefind that SIMPLER evaluations accurately reflect real-world policy behaviormodes such as sensitivity to various distribution shifts. We open-source allSIMPLER environments along with our workflow for creating new environments athttps://simpler-env.github.io to facilitate research on general-purposemanipulation policies and simulated evaluation frameworks.</description><author>Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, Ted Xiao</author><pubDate>Thu, 09 May 2024 18:30:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05941v1</guid></item><item><title>DOLOMITES: Domain-Specific Long-Form Methodical Tasks</title><link>http://arxiv.org/abs/2405.05938v1</link><description>Experts in various fields routinely perform methodical writing tasks to plan,organize, and report their work. From a clinician writing a differentialdiagnosis for a patient, to a teacher writing a lesson plan for students, thesetasks are pervasive, requiring to methodically generate structured long-formoutput for a given input. We develop a typology of methodical tasks structuredin the form of a task objective, procedure, input, and output, and introduceDoLoMiTes, a novel benchmark with specifications for 519 such tasks elicitedfrom hundreds of experts from across 25 fields. Our benchmark further containsspecific instantiations of methodical tasks with concrete input and outputexamples (1,857 in total) which we obtain by collecting expert revisions of upto 10 model-generated examples of each task. We use these examples to evaluatecontemporary language models highlighting that automating methodical tasks is achallenging long-form generation problem, as it requires performing complexinferences, while drawing upon the given context as well as domain knowledge.</description><author>Chaitanya Malaviya, Priyanka Agrawal, Kuzman Ganchev, Pranesh Srinivasan, Fantine Huot, Jonathan Berant, Mark Yatskar, Dipanjan Das, Mirella Lapata, Chris Alberti</author><pubDate>Thu, 09 May 2024 18:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05938v1</guid></item><item><title>Large Human Language Models: A Need and the Challenges</title><link>http://arxiv.org/abs/2312.07751v3</link><description>As research in human-centered NLP advances, there is a growing recognition ofthe importance of incorporating human and social factors into NLP models. Atthe same time, our NLP systems have become heavily reliant on LLMs, most ofwhich do not model authors. To build NLP systems that can truly understandhuman language, we must better integrate human contexts into LLMs. This bringsto the fore a range of design considerations and challenges in terms of whathuman aspects to capture, how to represent them, and what modeling strategiesto pursue. To address these, we advocate for three positions toward creatinglarge human language models (LHLMs) using concepts from psychological andbehavioral sciences: First, LM training should include the human context.Second, LHLMs should recognize that people are more than their group(s). Third,LHLMs should be able to account for the dynamic and temporally-dependent natureof the human context. We refer to relevant advances and present open challengesthat need to be addressed and their possible solutions in realizing thesegoals.</description><author>Nikita Soni, H. Andrew Schwartz, Jo√£o Sedoc, Niranjan Balasubramanian</author><pubDate>Thu, 09 May 2024 18:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07751v3</guid></item><item><title>Inference for Regression with Variables Generated from Unstructured Data</title><link>http://arxiv.org/abs/2402.15585v3</link><description>The leading strategy for analyzing unstructured data uses two steps. First,latent variables of economic interest are estimated with an upstreaminformation retrieval model. Second, the estimates are treated as "data" in adownstream econometric model. We establish theoretical arguments for why thistwo-step strategy leads to biased inference in empirically plausible settings.More constructively, we propose a one-step strategy for valid inference thatuses the upstream and downstream models jointly. The one-step strategy (i)substantially reduces bias in simulations; (ii) has quantitatively importanteffects in a leading application using CEO time-use data; and (iii) can bereadily adapted by applied researchers.</description><author>Laura Battaglia, Timothy Christensen, Stephen Hansen, Szymon Sacher</author><pubDate>Thu, 09 May 2024 18:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15585v3</guid></item><item><title>Theoretical Guarantees of Data Augmented Last Layer Retraining Methods</title><link>http://arxiv.org/abs/2405.05934v1</link><description>Ensuring fair predictions across many distinct subpopulations in the trainingdata can be prohibitive for large models. Recently, simple linear last layerretraining strategies, in combination with data augmentation methods such asupweighting, downsampling and mixup, have been shown to achievestate-of-the-art performance for worst-group accuracy, which quantifiesaccuracy for the least prevalent subpopulation. For linear last layerretraining and the abovementioned augmentations, we present the optimalworst-group accuracy when modeling the distribution of the latentrepresentations (input to the last layer) as Gaussian for each subpopulation.We evaluate and verify our results for both synthetic and large publiclyavailable datasets.</description><author>Monica Welfert, Nathan Stromberg, Lalitha Sankar</author><pubDate>Thu, 09 May 2024 18:16:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05934v1</guid></item><item><title>Trustworthy AI-Generative Content in Intelligent 6G Network: Adversarial, Privacy, and Fairness</title><link>http://arxiv.org/abs/2405.05930v1</link><description>AI-generated content (AIGC) models, represented by large language models(LLM), have brought revolutionary changes to the content generation fields. Thehigh-speed and extensive 6G technology is an ideal platform for providingpowerful AIGC mobile service applications, while future 6G mobile networks alsoneed to support intelligent and personalized mobile generation services.However, the significant ethical and security issues of current AIGC models,such as adversarial attacks, privacy, and fairness, greatly affect thecredibility of 6G intelligent networks, especially in ensuring secure, private,and fair AIGC applications. In this paper, we propose TrustGAIN, a novelparadigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scaleAIGC services in future 6G networks. We first discuss the adversarial attacksand privacy threats faced by AIGC systems in 6G networks, as well as thecorresponding protection issues. Subsequently, we emphasize the importance ofensuring the unbiasedness and fairness of the mobile generative service infuture intelligent networks. In particular, we conduct a use case todemonstrate that TrustGAIN can effectively guide the resistance againstmalicious or generated false information. We believe that TrustGAIN is anecessary paradigm for intelligent and trustworthy 6G networks to support AIGCservices, ensuring the security, privacy, and fairness of AIGC networkservices.</description><author>Siyuan Li, Xi Lin, Yaju Liu, Jianhua Li</author><pubDate>Thu, 09 May 2024 18:16:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05930v1</guid></item><item><title>FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting</title><link>http://arxiv.org/abs/2405.05925v1</link><description>Ensemble weather forecasting is essential for weather predictions andmitigating the impacts of extreme weather events. Constructing an ensembleprediction system (EPS) based on conventional numerical weather prediction(NWP) models is highly computationally expensive. Machine learning (ML) modelshave emerged as valuable tools for deterministic weather forecasts, providingforecasts with significantly reduced computational requirements and evensurpassing the forecast performance of traditional NWP models. However,challenges arise when applying ML models to ensemble forecasting. Recent MLmodels, such as GenCast and SEEDS model, rely on the ERA5 Ensemble of DataAssimilations (EDA) or two operational NWP ensemble members for forecastgeneration. The spatial resolution of 1{\deg} or 2{\deg} in these models isoften considered too coarse for many applications. To overcome theselimitations, we introduce FuXi-ENS, an advanced ML model designed to deliver6-hourly global ensemble weather forecasts up to 15 days. This model runs at asignificantly improved spatial resolution of 0.25{\deg}, incorporating 5upper-air atmospheric variables at 13 pressure levels, along with 13 surfacevariables. By leveraging the inherent probabilistic nature of VariationalAutoEncoder (VAE), FuXi-ENS optimizes a loss function that combines thecontinuous ranked probability score (CRPS) and the KL divergence between thepredicted and target distribution. This innovative approach represents anadvancement over the traditional use of L1 loss combined with the KL loss instandard VAE models when VAE for ensemble weather forecasts. Evaluation resultsdemonstrate that FuXi-ENS outperforms ensemble forecasts from the EuropeanCentre for Medium-Range Weather Forecasts (ECMWF), a world leading NWP model,on 98.1% of 360 variable and forecast lead time combinations on CRPS.</description><author>Xiaohui Zhong, Lei Chen, Hao Li, Jie Feng, Bo Lu</author><pubDate>Thu, 09 May 2024 18:15:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05925v1</guid></item><item><title>Diag2Diag: Multi modal super resolution for physics discovery with application to fusion</title><link>http://arxiv.org/abs/2405.05908v1</link><description>This paper introduces a groundbreaking multi-modal neural network modeldesigned for resolution enhancement, which innovatively leveragesinter-diagnostic correlations within a system. Traditional approaches haveprimarily focused on uni-modal enhancement strategies, such as pixel-basedimage enhancement or heuristic signal interpolation. In contrast, our modelemploys a novel methodology by harnessing the diagnostic relationships withinthe physics of fusion plasma. Initially, we establish the correlation amongdiagnostics within the tokamak. Subsequently, we utilize these correlations tosubstantially enhance the temporal resolution of the Thomson Scatteringdiagnostic, which assesses plasma density and temperature. By increasing itsresolution from conventional 200Hz to 500kHz, we facilitate a new level ofinsight into plasma behavior, previously attainable only throughcomputationally intensive simulations. This enhancement goes beyond simpleinterpolation, offering novel perspectives on the underlying physical phenomenagoverning plasma dynamics.</description><author>Azarakhsh Jalalvand, Max Curie, SangKyeun Kim, Peter Steiner, Jaemin Seo, Qiming Hu, Andrew Oakleigh Nelson, Egemen Kolemen</author><pubDate>Thu, 09 May 2024 18:06:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05908v1</guid></item><item><title>Deep Multi-Task Learning for Malware Image Classification</title><link>http://arxiv.org/abs/2405.05906v1</link><description>Malicious software is a pernicious global problem. A novel multi-tasklearning framework is proposed in this paper for malware image classificationfor accurate and fast malware detection. We generate bitmap (BMP) and (PNG)images from malware features, which we feed to a deep learning classifier. Ourstate-of-the-art multi-task learning approach has been tested on a new dataset,for which we have collected approximately 100,000 benign and malicious PE, APK,Mach-o, and ELF examples. Experiments with seven tasks tested with 4 activationfunctions, ReLU, LeakyReLU, PReLU, and ELU separately demonstrate that PReLUgives the highest accuracy of more than 99.87% on all tasks. Our model caneffectively detect a variety of obfuscation methods like packing, encryption,and instruction overlapping, strengthing the beneficial claims of our model, inaddition to achieving the state-of-art methods in terms of accuracy.</description><author>Ahmed Bensaoud, Jugal Kalita</author><pubDate>Thu, 09 May 2024 18:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05906v1</guid></item><item><title>Truthful Aggregation of LLMs with an Application to Online Advertising</title><link>http://arxiv.org/abs/2405.05905v1</link><description>We address the challenge of aggregating the preferences of multiple agentsover LLM-generated replies to user queries, where agents might modify orexaggerate their preferences. New agents may participate for each new query,making fine-tuning LLMs on these preferences impractical. To overcome thesechallenges, we propose an auction mechanism that operates without fine-tuningor access to model weights. This mechanism is designed to provably converge tothe ouput of the optimally fine-tuned LLM as computational resources areincreased. The mechanism can also incorporate contextual information about theagents when avaiable, which significantly accelerates its convergence. Awell-designed payment rule ensures that truthful reporting is the optimalstrategy for all agents, while also promoting an equity property by aligningeach agent's utility with her contribution to social welfare - an essentialfeature for the mechanism's long-term viability. While our approach can beapplied whenever monetary transactions are permissible, our flagshipapplication is in online advertising. In this context, advertisers try to steerLLM-generated responses towards their brand interests, while the platform aimsto maximize advertiser value and ensure user satisfaction. Experimental resultsconfirm that our mechanism not only converges efficiently to the optimallyfine-tuned LLM but also significantly boosts advertiser value and platformrevenue, all with minimal computational overhead.</description><author>Ermis Soumalias, Michael J. Curry, Sven Seuken</author><pubDate>Thu, 09 May 2024 18:01:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05905v1</guid></item><item><title>Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</title><link>http://arxiv.org/abs/2405.05904v1</link><description>When large language models are aligned via supervised fine-tuning, they mayencounter new factual information that was not acquired through pre-training.It is often conjectured that this can teach the model the behavior ofhallucinating factually incorrect responses, as the model is trained togenerate facts that are not grounded in its pre-existing knowledge. In thiswork, we study the impact of such exposure to new knowledge on the capabilityof the fine-tuned model to utilize its pre-existing knowledge. To this end, wedesign a controlled setup, focused on closed-book QA, where we vary theproportion of the fine-tuning examples that introduce new knowledge. Wedemonstrate that large language models struggle to acquire new factualknowledge through fine-tuning, as fine-tuning examples that introduce newknowledge are learned significantly slower than those consistent with themodel's knowledge. However, we also find that as the examples with newknowledge are eventually learned, they linearly increase the model's tendencyto hallucinate. Taken together, our results highlight the risk in introducingnew factual knowledge through fine-tuning, and support the view that largelanguage models mostly acquire factual knowledge through pre-training, whereasfine-tuning teaches them to use it more efficiently.</description><author>Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig</author><pubDate>Thu, 09 May 2024 18:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05904v1</guid></item><item><title>A Comprehensive Survey of Masked Faces: Recognition, Detection, and Unmasking</title><link>http://arxiv.org/abs/2405.05900v1</link><description>Masked face recognition (MFR) has emerged as a critical domain in biometricidentification, especially by the global COVID-19 pandemic, which introducedwidespread face masks. This survey paper presents a comprehensive analysis ofthe challenges and advancements in recognising and detecting individuals withmasked faces, which has seen innovative shifts due to the necessity of adaptingto new societal norms. Advanced through deep learning techniques, MFR, alongwith Face Mask Recognition (FMR) and Face Unmasking (FU), represent significantareas of focus. These methods address unique challenges posed by obscuredfacial features, from fully to partially covered faces. Our comprehensivereview delves into the various deep learning-based methodologies developed forMFR, FMR, and FU, highlighting their distinctive challenges and the solutionsproposed to overcome them. Additionally, we explore benchmark datasets andevaluation metrics specifically tailored for assessing performance in MFRresearch. The survey also discusses the substantial obstacles still facingresearchers in this field and proposes future directions for the ongoingdevelopment of more robust and effective masked face recognition systems. Thispaper serves as an invaluable resource for researchers and practitioners,offering insights into the evolving landscape of face recognition technologiesin the face of global health crises and beyond.</description><author>Mohamed Mahmoud, Mahmoud SalahEldin Kasem, Hyun-Soo Kang</author><pubDate>Thu, 09 May 2024 17:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05900v1</guid></item><item><title>Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons</title><link>http://arxiv.org/abs/2405.05894v1</link><description>LLM-as-a-judge approaches are a practical and effective way of assessing arange of text tasks, aligning with human judgements especially when applied ina comparative assessment fashion. However, when using pairwise comparisons torank a set of candidates the computational costs scale quadratically with thenumber of candidates, which can have practical limitations. This paperintroduces a Product of Expert (PoE) framework for efficient LLM ComparativeAssessment. Here individual comparisons are considered experts that provideinformation on a pair's score difference. The PoE framework combines theinformation from these experts to yield an expression that can be maximizedwith respect to the underlying set of candidates, and is highly flexible whereany form of expert can be assumed. When Gaussian experts are used one canderive simple closed-form solutions for the optimal candidate ranking, as wellas expressions for selecting which comparisons should be made to maximize theprobability of this ranking. Our approach enables efficient comparativeassessment, where by using only a small subset of the possible comparisons, onecan generate score predictions that correlate as well to human judgements asthe predictions when all comparisons are used. We evaluate the approach onmultiple NLG tasks and demonstrate that our framework can yield considerablecomputational savings when performing pairwise comparative assessment. When Nis large, with as few as 2% of comparisons the PoE solution can achieve similarperformance to when all comparisons are used.</description><author>Adian Liusie, Vatsal Raina, Yassir Fathullah, Mark Gales</author><pubDate>Thu, 09 May 2024 17:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05894v1</guid></item><item><title>Safe Exploration Using Bayesian World Models and Log-Barrier Optimization</title><link>http://arxiv.org/abs/2405.05890v1</link><description>A major challenge in deploying reinforcement learning in online tasks isensuring that safety is maintained throughout the learning process. In thiswork, we propose CERL, a new method for solving constrained Markov decisionprocesses while keeping the policy safe during learning. Our method leveragesBayesian world models and suggests policies that are pessimistic w.r.t. themodel's epistemic uncertainty. This makes CERL robust towards modelinaccuracies and leads to safe exploration during learning. In our experiments,we demonstrate that CERL outperforms the current state-of-the-art in terms ofsafety and optimality in solving CMDPs from image observations.</description><author>Yarden As, Bhavya Sukhija, Andreas Krause</author><pubDate>Thu, 09 May 2024 17:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05890v1</guid></item><item><title>ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems</title><link>http://arxiv.org/abs/2308.04588v2</link><description>Recently, uncertainty-aware deep learning methods for multiclass labelingproblems have been developed that provide calibrated class predictionprobabilities and out-of-distribution (OOD) indicators, letting machinelearning (ML) consumers and engineers gauge a model's confidence in itspredictions. However, this extra neural network prediction information ischallenging to scalably convey visually for arbitrary data sources undermultiple uncertainty contexts. To address these challenges, we presentScatterUQ, an interactive system that provides targeted visualizations to allowusers to better understand model performance in context-driven uncertaintysettings. ScatterUQ leverages recent advances in distance-aware neuralnetworks, together with dimensionality reduction techniques, to constructrobust, 2-D scatter plots explaining why a model predicts a test example to be(1) in-distribution and of a particular class, (2) in-distribution but unsureof the class, and (3) out-of-distribution. ML consumers and engineers canvisually compare the salient features of test samples with training examplesthrough the use of a ``hover callback'' to understand model uncertaintyperformance and decide follow up courses of action. We demonstrate theeffectiveness of ScatterUQ to explain model uncertainty for a multiclass imageclassification on a distance-aware neural network trained on Fashion-MNIST andtested on Fashion-MNIST (in distribution) and MNIST digits (out ofdistribution), as well as a deep learning model for a cyber dataset. Wequantitatively evaluate dimensionality reduction techniques to optimize ourcontextually driven UQ visualizations. Our results indicate that the ScatterUQsystem should scale to arbitrary, multiclass datasets. Our code is available athttps://github.com/mit-ll-responsible-ai/equine-webapp</description><author>Harry Li, Steven Jorgensen, John Holodnak, Allan Wollaber</author><pubDate>Thu, 09 May 2024 17:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04588v2</guid></item><item><title>Exploiting Autoencoder's Weakness to Generate Pseudo Anomalies</title><link>http://arxiv.org/abs/2405.05886v1</link><description>Due to the rare occurrence of anomalous events, a typical approach to anomalydetection is to train an autoencoder (AE) with normal data only so that itlearns the patterns or representations of the normal training data. At testtime, the trained AE is expected to well reconstruct normal but to poorlyreconstruct anomalous data. However, contrary to the expectation, anomalousdata is often well reconstructed as well. In order to further separate thereconstruction quality between normal and anomalous data, we propose creatingpseudo anomalies from learned adaptive noise by exploiting the aforementionedweakness of AE, i.e., reconstructing anomalies too well. The generated noise isadded to the normal data to create pseudo anomalies. Extensive experiments onPed2, Avenue, ShanghaiTech, CIFAR-10, and KDDCUP datasets demonstrate theeffectiveness and generic applicability of our approach in improving thediscriminative capability of AEs for anomaly detection.</description><author>Marcella Astrid, Muhammad Zaigham Zaheer, Djamila Aouada, Seung-Ik Lee</author><pubDate>Thu, 09 May 2024 17:22:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05886v1</guid></item><item><title>Fusing Models with Complementary Expertise</title><link>http://arxiv.org/abs/2310.01542v2</link><description>Training AI models that generalize across tasks and domains has long beenamong the open problems driving AI research. The emergence of Foundation Modelsmade it easier to obtain expert models for a given task, but the heterogeneityof data that may be encountered at test time often means that any single expertis insufficient. We consider the Fusion of Experts (FoE) problem of fusingoutputs of expert models with complementary knowledge of the data distributionand formulate it as an instance of supervised learning. Our method isapplicable to both discriminative and generative tasks and leads to significantperformance improvements in image and text classification, text summarization,multiple-choice QA, and automatic evaluation of generated text. We also extendour method to the "frugal" setting where it is desired to reduce the number ofexpert model evaluations at test time. Our implementation is publicly availableat https://github.com/hwang595/FoE-ICLR2024.</description><author>Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric Xing, Mikhail Yurochkin</author><pubDate>Thu, 09 May 2024 17:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01542v2</guid></item><item><title>Composable Part-Based Manipulation</title><link>http://arxiv.org/abs/2405.05876v1</link><description>In this paper, we propose composable part-based manipulation (CPM), a novelapproach that leverages object-part decomposition and part-part correspondencesto improve learning and generalization of robotic manipulation skills. Byconsidering the functional correspondences between object parts, weconceptualize functional actions, such as pouring and constrained placing, ascombinations of different correspondence constraints. CPM comprises acollection of composable diffusion models, where each model captures adifferent inter-object correspondence. These diffusion models can generateparameters for manipulation skills based on the specific object parts.Leveraging part-based correspondences coupled with the task decomposition intodistinct constraints enables strong generalization to novel objects and objectcategories. We validate our approach in both simulated and real-worldscenarios, demonstrating its effectiveness in achieving robust and generalizedmanipulation capabilities.</description><author>Weiyu Liu, Jiayuan Mao, Joy Hsu, Tucker Hermans, Animesh Garg, Jiajun Wu</author><pubDate>Thu, 09 May 2024 17:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05876v1</guid></item><item><title>Designed Dithering Sign Activation for Binary Neural Networks</title><link>http://arxiv.org/abs/2405.02220v2</link><description>Binary Neural Networks emerged as a cost-effective and energy-efficientsolution for computer vision tasks by binarizing either network weights oractivations. However, common binary activations, such as the Sign activationfunction, abruptly binarize the values with a single threshold, losingfine-grained details in the feature outputs. This work proposes an activationthat applies multiple thresholds following dithering principles, shifting theSign activation function for each pixel according to a spatially periodicthreshold kernel. Unlike literature methods, the shifting is defined jointlyfor a set of adjacent pixels, taking advantage of spatial correlations.Experiments over the classification task demonstrate the effectiveness of thedesigned dithering Sign activation function as an alternative activation forbinary neural networks, without increasing the computational cost. Further,DeSign balances the preservation of details with the efficiency of binaryoperations.</description><author>Brayan Monroy, Juan Estupi√±an, Tatiana Gelvez-Barrera, Jorge Bacca, Henry Arguello</author><pubDate>Thu, 09 May 2024 17:02:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02220v2</guid></item><item><title>Selecting the Most Conflicting Pair of Candidates</title><link>http://arxiv.org/abs/2405.05870v1</link><description>We study committee elections from a perspective of finding the mostconflicting candidates, that is, candidates that imply the largest amount ofconflict, as per voter preferences. By proposing basic axioms to capture thisobjective, we show that none of the prominent multiwinner voting rules meetthem. Consequently, we design committee voting rules compliant with ourdesiderata, introducing conflictual voting rules. A subsequent deepenedanalysis sheds more light on how they operate. Our investigation identifiesvarious aspects of conflict, for which we come up with relevant axioms andquantitative measures, which may be of independent interest. We support ourtheoretical study with experiments on both real-life and synthetic data.</description><author>Th√©o Delemazure, ≈Åukasz Janeczko, Andrzej Kaczmarczyk, Stanis≈Çaw Szufa</author><pubDate>Thu, 09 May 2024 17:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05870v1</guid></item><item><title>Faster Linear Systems and Matrix Norm Approximation via Multi-level Sketched Preconditioning</title><link>http://arxiv.org/abs/2405.05865v1</link><description>We present a new class of preconditioned iterative methods for solving linearsystems of the form $Ax = b$. Our methods are based on constructing a low-rankNystr\"om approximation to $A$ using sparse random sketching. Thisapproximation is used to construct a preconditioner, which itself is invertedquickly using additional levels of random sketching and preconditioning. Weprove that the convergence of our methods depends on a natural averagecondition number of $A$, which improves as the rank of the Nystr\"omapproximation increases. Concretely, this allows us to obtain faster runtimesfor a number of fundamental linear algebraic problems: 1. We show how to solve any $n\times n$ linear system that iswell-conditioned except for $k$ outlying large singular values in$\tilde{O}(n^{2.065} + k^\omega)$ time, improving on a recent result of[Derezi\'nski, Yang, STOC 2024] for all $k \gtrsim n^{0.78}$. 2. We give the first $\tilde{O}(n^2 + {d_\lambda}^{\omega}$) time algorithmfor solving a regularized linear system $(A + \lambda I)x = b$, where $A$ ispositive semidefinite with effective dimension $d_\lambda$. This problem arisesin applications like Gaussian process regression. 3. We give faster algorithms for approximating Schatten $p$-norms and othermatrix norms. For example, for the Schatten 1 (nuclear) norm, we give analgorithm that runs in $\tilde{O}(n^{2.11})$ time, improving on an$\tilde{O}(n^{2.18})$ method of [Musco et al., ITCS 2018]. Interestingly, previous state-of-the-art algorithms for most of the problemsabove relied on stochastic iterative methods, like stochastic coordinate andgradient descent. Our work takes a completely different approach, insteadleveraging tools from matrix sketching.</description><author>Micha≈Ç Derezi≈Ñski, Christopher Musco, Jiaming Yang</author><pubDate>Thu, 09 May 2024 16:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05865v1</guid></item><item><title>ExACT: An End-to-End Autonomous Excavator System Using Action Chunking With Transformers</title><link>http://arxiv.org/abs/2405.05861v1</link><description>Excavators are crucial for diverse tasks such as construction and mining,while autonomous excavator systems enhance safety and efficiency, address laborshortages, and improve human working conditions. Different from the existingmodularized approaches, this paper introduces ExACT, an end-to-end autonomousexcavator system that processes raw LiDAR, camera data, and joint positions tocontrol excavator valves directly. Utilizing the Action Chunking withTransformers (ACT) architecture, ExACT employs imitation learning to takeobservations from multi-modal sensors as inputs and generate actionablesequences. In our experiment, we build a simulator based on the capturedreal-world data to model the relations between excavator valve states and jointvelocities. With a few human-operated demonstration data trajectories, ExACTdemonstrates the capability of completing different excavation tasks, includingreaching, digging and dumping through imitation learning in validations withthe simulator. To the best of our knowledge, ExACT represents the firstinstance towards building an end-to-end autonomous excavator system viaimitation learning methods with a minimal set of human demonstrations. Thevideo about this work can be accessed at https://youtu.be/NmzR_Rf-aEk.</description><author>Liangliang Chen, Shiyu Jin, Haoyu Wang, Liangjun Zhang</author><pubDate>Thu, 09 May 2024 16:48:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05861v1</guid></item><item><title>The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing Human Labels</title><link>http://arxiv.org/abs/2405.05860v1</link><description>Longstanding data labeling practices in machine learning involve collectingand aggregating labels from multiple annotators. But what should we do whenannotators disagree? Though annotator disagreement has long been seen as aproblem to minimize, new perspectivist approaches challenge this assumption bytreating disagreement as a valuable source of information. In this positionpaper, we examine practices and assumptions surrounding the causes ofdisagreement--some challenged by perspectivist approaches, and some that remainto be addressed--as well as practical and normative challenges for workoperating under these assumptions. We conclude with recommendations for thedata labeling pipeline and avenues for future research engaging withsubjectivity and disagreement.</description><author>Eve Fleisig, Su Lin Blodgett, Dan Klein, Zeerak Talat</author><pubDate>Thu, 09 May 2024 16:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05860v1</guid></item><item><title>Free-Moving Object Reconstruction and Pose Estimation with Virtual Camera</title><link>http://arxiv.org/abs/2405.05858v1</link><description>We propose an approach for reconstructing free-moving object from a monocularRGB video. Most existing methods either assume scene prior, hand pose prior,object category pose prior, or rely on local optimization with multiplesequence segments. We propose a method that allows free interaction with theobject in front of a moving camera without relying on any prior, and optimizesthe sequence globally without any segments. We progressively optimize theobject shape and pose simultaneously based on an implicit neuralrepresentation. A key aspect of our method is a virtual camera system thatreduces the search space of the optimization significantly. We evaluate ourmethod on the standard HO3D dataset and a collection of egocentric RGBsequences captured with a head-mounted device. We demonstrate that our approachoutperforms most methods significantly, and is on par with recent techniquesthat assume prior information.</description><author>Haixin Shi, Yinlin Hu, Daniel Koguciuk, Juan-Ting Lin, Mathieu Salzmann, David Ferstl</author><pubDate>Thu, 09 May 2024 16:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05858v1</guid></item><item><title>Compressed Bayesian Federated Learning for Reliable Passive Radio Sensing in Industrial IoT</title><link>http://arxiv.org/abs/2405.05855v1</link><description>Bayesian Federated Learning (FL) has been recently introduced to providewell-calibrated Machine Learning (ML) models quantifying the uncertainty oftheir predictions. Despite their advantages compared to frequentist FL setups,Bayesian FL tools implemented over decentralized networks are subject to highcommunication costs due to the iterated exchange of local posteriordistributions among cooperating devices. Therefore, this paper proposes acommunication-efficient decentralized Bayesian FL policy to reduce thecommunication overhead without sacrificing final learning accuracy andcalibration. The proposed method integrates compression policies and allowsdevices to perform multiple optimization steps before sending the localposterior distributions. We integrate the developed tool in an IndustrialInternet of Things (IIoT) use case where collaborating nodes equipped withautonomous radar sensors are tasked to reliably localize human operators in aworkplace shared with robots. Numerical results show that the developedapproach obtains highly accurate yet well-calibrated ML models compatible withthe ones provided by conventional (uncompressed) Bayesian FL tools whilesubstantially decreasing the communication overhead (i.e., up to 99%).Furthermore, the proposed approach is advantageous when compared withstate-of-the-art compressed frequentist FL setups in terms of calibration,especially when the statistical distribution of the testing dataset changes.</description><author>Luca Barbieri, Stefano Savazzi, Monica Nicoli</author><pubDate>Thu, 09 May 2024 16:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05855v1</guid></item><item><title>Robust and Explainable Fine-Grained Visual Classification with Transfer Learning: A Dual-Carriageway Framework</title><link>http://arxiv.org/abs/2405.05853v1</link><description>In the realm of practical fine-grained visual classification applicationsrooted in deep learning, a common scenario involves training a model using apre-existing dataset. Subsequently, a new dataset becomes available, promptingthe desire to make a pivotal decision for achieving enhanced and leveragedinference performance on both sides: Should one opt to train datasets fromscratch or fine-tune the model trained on the initial dataset using the newlyreleased dataset? The existing literature reveals a lack of methods tosystematically determine the optimal training strategy, necessitatingexplainability. To this end, we present an automatic best-suit trainingsolution searching framework, the Dual-Carriageway Framework (DCF), to fillthis gap. DCF benefits from the design of a dual-direction search (startingfrom the pre-existing or the newly released dataset) where five differenttraining settings are enforced. In addition, DCF is not only capable offiguring out the optimal training strategy with the capability of avoidingoverfitting but also yields built-in quantitative and visual explanationsderived from the actual input and weights of the trained model. We validatedDCF's effectiveness through experiments with three convolutional neuralnetworks (ResNet18, ResNet34 and Inception-v3) on two temporally continuedcommercial product datasets. Results showed fine-tuning pathways outperformedtraining-from-scratch ones by up to 2.13% and 1.23% on the pre-existing and newdatasets, respectively, in terms of mean accuracy. Furthermore, DCF identifiedreflection padding as the superior padding method, enhancing testing accuracyby 3.72% on average. This framework stands out for its potential to guide thedevelopment of robust and explainable AI solutions in fine-grained visualclassification tasks.</description><author>Zheming Zuo, Joseph Smith, Jonathan Stonehouse, Boguslaw Obara</author><pubDate>Thu, 09 May 2024 16:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05853v1</guid></item><item><title>Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control</title><link>http://arxiv.org/abs/2405.05852v1</link><description>Embodied AI agents require a fine-grained understanding of the physical worldmediated through visual and language inputs. Such capabilities are difficult tolearn solely from task-specific data. This has led to the emergence ofpre-trained vision-language models as a tool for transferring representationslearned from internet-scale data to downstream tasks and new domains. However,commonly used contrastively trained representations such as in CLIP have beenshown to fail at enabling embodied agents to gain a sufficiently fine-grainedscene understanding -- a capability vital for control. To address thisshortcoming, we consider representations from pre-trained text-to-imagediffusion models, which are explicitly optimized to generate images from textprompts and as such, contain text-conditioned representations that reflecthighly fine-grained visuo-spatial information. Using pre-trained text-to-imagediffusion models, we construct Stable Control Representations which allowlearning downstream control policies that generalize to complex, open-endedenvironments. We show that policies learned using Stable ControlRepresentations are competitive with state-of-the-art representation learningapproaches across a broad range of simulated control settings, encompassingchallenging manipulation and navigation tasks. Most notably, we show thatStable Control Representations enable learning policies that exhibitstate-of-the-art performance on OVMM, a difficult open-vocabulary navigationbenchmark.</description><author>Gunshi Gupta, Karmesh Yadav, Yarin Gal, Dhruv Batra, Zsolt Kira, Cong Lu, Tim G. J. Rudner</author><pubDate>Thu, 09 May 2024 16:39:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05852v1</guid></item><item><title>The Transformation Logics</title><link>http://arxiv.org/abs/2304.09639v2</link><description>We introduce a new family of temporal logics designed to finely balance thetrade-off between expressivity and complexity. Their key feature is thepossibility of defining operators of a new kind that we call transformationoperators. Some of them subsume existing temporal operators, while others areentirely novel. Of particular interest are transformation operators based onsemigroups. They enable logics to harness the richness of semigroup theory, andwe show them to yield logics capable of creating hierarchies of increasingexpressivity and complexity which are non-trivial to characterise in existinglogics. The result is a genuinely novel and yet unexplored landscape oftemporal logics, each of them with the potential of matching the trade-offbetween expressivity and complexity required by specific applications.</description><author>Alessandro Ronca</author><pubDate>Thu, 09 May 2024 16:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09639v2</guid></item><item><title>Learned feature representations are biased by complexity, learning order, position, and more</title><link>http://arxiv.org/abs/2405.05847v1</link><description>Representation learning, and interpreting learned representations, are keyareas of focus in machine learning and neuroscience. Both fields generally userepresentations as a means to understand or improve a system's computations. Inthis work, however, we explore surprising dissociations between representationand computation that may pose challenges for such efforts. We create datasetsin which we attempt to match the computational role that different featuresplay, while manipulating other properties of the features or the data. We trainvarious deep learning architectures to compute these multiple abstract featuresabout their inputs. We find that their learned feature representations aresystematically biased towards representing some features more strongly thanothers, depending upon extraneous properties such as feature complexity, theorder in which features are learned, and the distribution of features over theinputs. For example, features that are simpler to compute or learned first tendto be represented more strongly and densely than features that are more complexor learned later, even if all features are learned equally well. We alsoexplore how these biases are affected by architectures, optimizers, andtraining regimes (e.g., in transformers, features decoded earlier in the outputsequence also tend to be represented more strongly). Our results help tocharacterize the inductive biases of gradient-based representation learning.These results also highlight a key challenge for interpretability $-$ or forcomparing the representations of models and brains $-$ disentangling extraneousbiases from the computationally important aspects of a system's internalrepresentations.</description><author>Andrew Kyle Lampinen, Stephanie C. Y. Chan, Katherine Hermann</author><pubDate>Thu, 09 May 2024 16:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05847v1</guid></item><item><title>Deep Diversity-Enhanced Feature Representation of Hyperspectral Images</title><link>http://arxiv.org/abs/2301.06132v3</link><description>In this paper, we study the problem of efficiently and effectively embeddingthe high-dimensional spatio-spectral information of hyperspectral (HS) images,guided by feature diversity. Specifically, based on the theoretical formulationthat feature diversity is correlated with the rank of the unfolded kernelmatrix, we rectify 3D convolution by modifying its topology to enhance the rankupper-bound. This modification yields a rank-enhanced spatial-spectralsymmetrical convolution set (ReS$^3$-ConvSet), which not only learns diverseand powerful feature representations but also saves network parameters.Additionally, we also propose a novel diversity-aware regularization (DA-Reg)term that directly acts on the feature maps to maximize independence amongelements. To demonstrate the superiority of the proposed ReS$^3$-ConvSet andDA-Reg, we apply them to various HS image processing and analysis tasks,including denoising, spatial super-resolution, and classification. Extensiveexperiments show that the proposed approaches outperform state-of-the-artmethods both quantitatively and qualitatively to a significant extent. The codeis publicly available at https://github.com/jinnh/ReSSS-ConvSet.</description><author>Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, Deyu Meng</author><pubDate>Thu, 09 May 2024 16:33:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06132v3</guid></item><item><title>Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model</title><link>http://arxiv.org/abs/2210.04017v3</link><description>End-to-end autonomous driving provides a feasible way to automaticallymaximize overall driving system performance by directly mapping the raw pixelsfrom a front-facing camera to control signals. Recent advanced methodsconstruct a latent world model to map the high dimensional observations intocompact latent space. However, the latent states embedded by the world modelproposed in previous works may contain a large amount of task-irrelevantinformation, resulting in low sampling efficiency and poor robustness to inputperturbations. Meanwhile, the training data distribution is usually unbalanced,and the learned policy is challenging to cope with the corner cases during thedriving process. To solve the above challenges, we present a SEMantic Maskedrecurrent world model (SEM2), which introduces a semantic filter to extract keydriving-relevant features and make decisions via the filtered features, and istrained with a multi-source data sampler, which aggregates common data andmultiple corner case data in a single batch, to balance the data distribution.Extensive experiments on CARLA show our method outperforms the state-of-the-artapproaches in terms of sample efficiency and robustness to input permutations.</description><author>Zeyu Gao, Yao Mu, Chen Chen, Jingliang Duan, Shengbo Eben Li, Ping Luo, Yanfeng Lu</author><pubDate>Thu, 09 May 2024 16:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.04017v3</guid></item><item><title>Could It Be Generated? Towards Practical Analysis of Memorization in Text-To-Image Diffusion Models</title><link>http://arxiv.org/abs/2405.05846v1</link><description>The past few years have witnessed substantial advancement in text-guidedimage generation powered by diffusion models. However, it was shown thattext-to-image diffusion models are vulnerable to training image memorization,raising concerns on copyright infringement and privacy invasion. In this work,we perform practical analysis of memorization in text-to-image diffusionmodels. Targeting a set of images to protect, we conduct quantitive analysis onthem without need to collect any prompts. Specifically, we first formallydefine the memorization of image and identify three necessary conditions ofmemorization, respectively similarity, existence and probability. We thenreveal the correlation between the model's prediction error and imagereplication. Based on the correlation, we propose to utilize inversiontechniques to verify the safety of target images against memorization andmeasure the extent to which they are memorized. Model developers can utilizeour analysis method to discover memorized images or reliably claim safetyagainst memorization. Extensive experiments on the Stable Diffusion, a popularopen-source text-to-image diffusion model, demonstrate the effectiveness of ouranalysis method.</description><author>Zhe Ma, Xuhong Zhang, Qingming Li, Tianyu Du, Wenzhi Chen, Zonghui Wang, Shouling Ji</author><pubDate>Thu, 09 May 2024 16:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05846v1</guid></item><item><title>Large Language Model Agent as a Mechanical Designer</title><link>http://arxiv.org/abs/2404.17525v2</link><description>Conventional mechanical design paradigms rely on experts systematicallyrefining concepts through experience-guided modification and FEA to meetspecific requirements. However, this approach can be time-consuming and heavilydependent on prior knowledge and experience. While numerous machine learningmodels have been developed to streamline this intensive and expert-driveniterative process, these methods typically demand extensive training data andconsiderable computational resources. Furthermore, methods based on deeplearning are usually restricted to the specific domains and tasks for whichthey were trained, limiting their applicability across different tasks. Thiscreates a trade-off between the efficiency of automation and the demand forresources. In this study, we present a novel approach that integratespre-trained LLMs with a FEM module. The FEM module evaluates each design andprovides essential feedback, guiding the LLMs to continuously learn, plan,generate, and optimize designs without the need for domain-specific training.We demonstrate the effectiveness of our proposed framework in managing theiterative optimization of truss structures, showcasing its capability to reasonabout and refine designs according to structured feedback and criteria. Ourresults reveal that these LLM-based agents can successfully generate trussdesigns that comply with natural language specifications with a success rate ofup to 90%, which varies according to the applied constraints. By employingprompt-based optimization techniques we show that LLM based agents exhibitoptimization behavior when provided with solution-score pairs to iterativelyrefine designs to meet specifications. This ability of LLM agents to produceviable designs and optimize them based on their inherent reasoning capabilitieshighlights their potential to develop and implement effective design strategiesautonomously.</description><author>Yayati Jadhav, Amir Barati Farimani</author><pubDate>Thu, 09 May 2024 16:31:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17525v2</guid></item><item><title>PiRD: Physics-informed Residual Diffusion for Flow Field Reconstruction</title><link>http://arxiv.org/abs/2404.08412v2</link><description>The use of machine learning in fluid dynamics is becoming more common toexpedite the computation when solving forward and inverse problems of partialdifferential equations. Yet, a notable challenge with existing convolutionalneural network (CNN)-based methods for data fidelity enhancement is theirreliance on specific low-fidelity data patterns and distributions during thetraining phase. In addition, the CNN-based method essentially treats the flowreconstruction task as a computer vision task that prioritizes the element-wiseprecision which lacks a physical and mathematical explanation. This dependencecan dramatically affect the models' effectiveness in real-world scenarios,especially when the low-fidelity input deviates from the training data orcontains noise not accounted for during training. The introduction of diffusionmodels in this context shows promise for improving performance andgeneralizability. Unlike direct mapping from a specific low-fidelity to ahigh-fidelity distribution, diffusion models learn to transition from anylow-fidelity distribution towards a high-fidelity one. Our proposed model -Physics-informed Residual Diffusion, demonstrates the capability to elevate thequality of data from both standard low-fidelity inputs, to low-fidelity inputswith injected Gaussian noise, and randomly collected samples. By integratingphysics-based insights into the objective function, it further refines theaccuracy and the fidelity of the inferred high-quality data. Experimentalresults have shown that our approach can effectively reconstruct high-qualityoutcomes for two-dimensional turbulent flows from a range of low-fidelity inputconditions without requiring retraining.</description><author>Siming Shan, Pengkai Wang, Song Chen, Jiaxu Liu, Chao Xu, Shengze Cai</author><pubDate>Thu, 09 May 2024 16:30:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08412v2</guid></item><item><title>Self-Supervised Pre-training with Symmetric Superimposition Modeling for Scene Text Recognition</title><link>http://arxiv.org/abs/2405.05841v1</link><description>In text recognition, self-supervised pre-training emerges as a good solutionto reduce dependence on expansive annotated real data. Previous studiesprimarily focus on local visual representation by leveraging mask imagemodeling or sequence contrastive learning. However, they omit modeling thelinguistic information in text images, which is crucial for recognizing text.To simultaneously capture local character features and linguistic informationin visual space, we propose Symmetric Superimposition Modeling (SSM). Theobjective of SSM is to reconstruct the direction-specific pixel and featuresignals from the symmetrically superimposed input. Specifically, we add theoriginal image with its inverted views to create the symmetrically superimposedinputs. At the pixel level, we reconstruct the original and inverted images tocapture character shapes and texture-level linguistic context. At the featurelevel, we reconstruct the feature of the same original image and inverted imagewith different augmentations to model the semantic-level linguistic context andthe local character discrimination. In our design, we disrupt the charactershape and linguistic rules. Consequently, the dual-level reconstructionfacilitates understanding character shapes and linguistic information from theperspective of visual texture and feature semantics. Experiments on varioustext recognition benchmarks demonstrate the effectiveness and generality ofSSM, with 4.1% average performance gains and 86.6% new state-of-the-art averageword accuracy on Union14M benchmarks.</description><author>Zuan Gao, Yuxin Wang, Yadong Qu, Boqiang Zhang, Zixiao Wang, Jianjun Xu, Hongtao Xie</author><pubDate>Thu, 09 May 2024 16:23:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05841v1</guid></item><item><title>Informed Decision-Making through Advancements in Open Set Recognition and Unknown Sample Detection</title><link>http://arxiv.org/abs/2405.05836v1</link><description>Machine learning-based techniques open up many opportunities and improvementsto derive deeper and more practical insights from data that can help businessesmake informed decisions. However, the majority of these techniques focus on theconventional closed-set scenario, in which the label spaces for the trainingand test sets are identical. Open set recognition (OSR) aims to bringclassification tasks in a situation that is more like reality, which focuses onclassifying the known classes as well as handling unknown classes effectively.In such an open-set problem the gathered samples in the training set cannotencompass all the classes and the system needs to identify unknown samples attest time. On the other hand, building an accurate and comprehensive model in areal dynamic environment presents a number of obstacles, because it isprohibitively expensive to train for every possible example of unknown items,and the model may fail when tested in testbeds. This study provides analgorithm exploring a new representation of feature space to improveclassification in OSR tasks. The efficacy and efficiency of business processesand decision-making can be improved by integrating OSR, which offers moreprecise and insightful predictions of outcomes. We demonstrate the performanceof the proposed method on three established datasets. The results indicate thatthe proposed model outperforms the baseline methods in accuracy and F1-score.</description><author>Atefeh Mahdavi, Marco Carvalho</author><pubDate>Thu, 09 May 2024 16:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05836v1</guid></item><item><title>Creative Beam Search: LLM-as-a-Judge For Improving Response Generation</title><link>http://arxiv.org/abs/2405.00099v2</link><description>Large language models are revolutionizing several areas, including artificialcreativity. However, the process of generation in machines profoundly divergesfrom that observed in humans. In particular, machine generation ischaracterized by a lack of intentionality and an underlying creative process.We propose a method called Creative Beam Search that uses Diverse Beam Searchand LLM-as-a-Judge to perform response generation and response validation. Theresults of a qualitative experiment show how our approach can provide betteroutput than standard sampling techniques. We also show that the responsevalidation step is a necessary complement to the response generation step.</description><author>Giorgio Franceschelli, Mirco Musolesi</author><pubDate>Thu, 09 May 2024 16:14:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00099v2</guid></item><item><title>Mask-TS Net: Mask Temperature Scaling Uncertainty Calibration for Polyp Segmentation</title><link>http://arxiv.org/abs/2405.05830v1</link><description>Lots of popular calibration methods in medical images focus onclassification, but there are few comparable studies on semantic segmentation.In polyp segmentation of medical images, we find most diseased area occupiesonly a small portion of the entire image, resulting in previous models beingnot well-calibrated for lesion regions but well-calibrated for background,despite their seemingly better Expected Calibration Error (ECE) scores overall.Therefore, we proposed four-branches calibration network with Mask-Loss andMask-TS strategies to more focus on the scaling of logits within potentiallesion regions, which serves to mitigate the influence of backgroundinterference. In the experiments, we compare the existing calibration methodswith the proposed Mask Temperature Scaling (Mask-TS). The results indicate thatthe proposed calibration network outperforms other methods both qualitativelyand quantitatively.</description><author>Yudian Zhang, Chenhao Xu, Kaiye Xu, Haijiang Zhu</author><pubDate>Thu, 09 May 2024 16:04:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05830v1</guid></item><item><title>MAD-ICP: It Is All About Matching Data -- Robust and Informed LiDAR Odometry</title><link>http://arxiv.org/abs/2405.05828v1</link><description>LiDAR odometry is the task of estimating the ego-motion of the sensor fromsequential laser scans. This problem has been addressed by the community formore than two decades, and many effective solutions are available nowadays.Most of these systems implicitly rely on assumptions about the operatingenvironment, the sensor used, and motion pattern. When these assumptions areviolated, several well-known systems tend to perform poorly. This paperpresents a LiDAR odometry system that can overcome these limitations andoperate well under different operating conditions while achieving performancecomparable with domain-specific methods. Our algorithm follows the well-knownICP paradigm that leverages a PCA-based kd-tree implementation that is used toextract structural information about the clouds being registered and to computethe minimization metric for the alignment. The drift is bound by managing thelocal map based on the estimated uncertainty of the tracked pose. To benefitthe community, we release an open-source C++ anytime real-time implementation.</description><author>Simone Ferrari, Luca Di Giammarino, Leonardo Brizi, Giorgio Grisetti</author><pubDate>Thu, 09 May 2024 16:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05828v1</guid></item><item><title>Multimodal Multi-loss Fusion Network for Sentiment Analysis</title><link>http://arxiv.org/abs/2308.00264v3</link><description>This paper investigates the optimal selection and fusion of feature encodersacross multiple modalities and combines these in one neural network to improvesentiment detection. We compare different fusion methods and examine the impactof multi-loss training within the multi-modality fusion network, identifyingsurprisingly important findings relating to subnet performance. We have alsofound that integrating context significantly enhances model performance. Ourbest model achieves state-of-the-art performance for three datasets (CMU-MOSI,CMU-MOSEI and CH-SIMS). These results suggest a roadmap toward an optimizedfeature selection and fusion approach for enhancing sentiment detection inneural networks.</description><author>Zehui Wu, Ziwei Gong, Jaywon Koo, Julia Hirschberg</author><pubDate>Thu, 09 May 2024 16:01:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00264v3</guid></item><item><title>Fine-grained Analysis and Faster Algorithms for Iteratively Solving Linear Systems</title><link>http://arxiv.org/abs/2405.05818v1</link><description>While effective in practice, iterative methods for solving large systems oflinear equations can be significantly affected by problem-dependent conditionnumber quantities. This makes characterizing their time complexity challenging,particularly when we wish to make comparisons between deterministic andstochastic methods, that may or may not rely on preconditioning and/or fastmatrix multiplication. In this work, we consider a fine-grained notion ofcomplexity for iterative linear solvers which we call the spectral tailcondition number, $\kappa_\ell$, defined as the ratio between the $\ell$thlargest and the smallest singular value of the matrix representing the system. Concretely, we prove the following main algorithmic result: Given an $n\timesn$ matrix $A$ and a vector $b$, we can find $\tilde{x}$ such that$\|A\tilde{x}-b\|\leq\epsilon\|b\|$ in time $\tilde{O}(\kappa_\ell\cdot n^2\log1/\epsilon)$ for any $\ell = O(n^{\frac1{\omega-1}})=O(n^{0.729})$, where$\omega \approx 2.372$ is the current fast matrix multiplication exponent. Thisguarantee is achieved by Sketch-and-Project with Nesterov's acceleration. Someof the implications of our result, and of the use of $\kappa_\ell$, includedirect improvement over a fine-grained analysis of the Conjugate Gradientmethod, suggesting a stronger separation between deterministic and stochasticiterative solvers; and relating the complexity of iterative solvers to theongoing algorithmic advances in fast matrix multiplication, since the bound on$\ell$ improves with $\omega$. Our main technical contributions are new sharp characterizations for thefirst and second moments of the random projection matrix that commonly arisesin sketching algorithms, building on a combination of techniques fromcombinatorial sampling via determinantal point processes and Gaussianuniversality results from random matrix theory.</description><author>Micha≈Ç Derezi≈Ñski, Daniel LeJeune, Deanna Needell, Elizaveta Rebrova</author><pubDate>Thu, 09 May 2024 15:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05818v1</guid></item><item><title>MSDiff: Multi-Scale Diffusion Model for Ultra-Sparse View CT Reconstruction</title><link>http://arxiv.org/abs/2405.05814v1</link><description>Computed Tomography (CT) technology reduces radiation haz-ards to the humanbody through sparse sampling, but fewer sampling angles pose challenges forimage reconstruction. Score-based generative models are widely used insparse-view CT re-construction, performance diminishes significantly with asharp reduction in projection angles. Therefore, we propose an ultra-sparseview CT reconstruction method utilizing multi-scale dif-fusion models (MSDiff),designed to concentrate on the global distribution of information andfacilitate the reconstruction of sparse views with local image characteristics.Specifically, the proposed model ingeniously integrates information from bothcomprehensive sampling and selectively sparse sampling tech-niques. Throughprecise adjustments in diffusion model, it is capable of extracting diversenoise distribution, furthering the understanding of the overall structure ofimages, and aiding the fully sampled model in recovering image information moreeffec-tively. By leveraging the inherent correlations within the projec-tiondata, we have designed an equidistant mask, enabling the model to focus itsattention more effectively. Experimental re-sults demonstrated that themulti-scale model approach signifi-cantly improved the quality of imagereconstruction under ultra-sparse angles, with good generalization acrossvarious datasets.</description><author>Pinhuang Tan, Mengxiao Geng, Jingya Lu, Liu Shi, Bin Huang, Qiegen Liu</author><pubDate>Thu, 09 May 2024 15:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05814v1</guid></item><item><title>Parallel Cross Strip Attention Network for Single Image Dehazing</title><link>http://arxiv.org/abs/2405.05811v1</link><description>The objective of single image dehazing is to restore hazy images and produceclear, high-quality visuals. Traditional convolutional models struggle withlong-range dependencies due to their limited receptive field size. WhileTransformers excel at capturing such dependencies, their quadraticcomputational complexity in relation to feature map resolution makes them lesssuitable for pixel-to-pixel dense prediction tasks. Moreover, fixed kernels ortokens in most models do not adapt well to varying blur sizes, resulting insuboptimal dehazing performance. In this study, we introduce a novel dehazingnetwork based on Parallel Stripe Cross Attention (PCSA) with a multi-scalestrategy. PCSA efficiently integrates long-range dependencies by simultaneouslycapturing horizontal and vertical relationships, allowing each pixel to capturecontextual cues from an expanded spatial domain. To handle different sizes andshapes of blurs flexibly, We employs a channel-wise design with varyingconvolutional kernel sizes and strip lengths in each PCSA to capture contextinformation at different scales.Additionally, we incorporate a softmax-basedadaptive weighting mechanism within PCSA to prioritize and leverage morecritical features.</description><author>Lihan Tong, Yun Liu, Tian Ye, Weijia Li, Liyuan Chen, Erkang Chen</author><pubDate>Thu, 09 May 2024 15:50:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05811v1</guid></item><item><title>Aequitas Flow: Streamlining Fair ML Experimentation</title><link>http://arxiv.org/abs/2405.05809v1</link><description>Aequitas Flow is an open-source framework for end-to-end Fair MachineLearning (ML) experimentation in Python. This package fills the existingintegration gaps in other Fair ML packages of complete and accessibleexperimentation. It provides a pipeline for fairness-aware model training,hyperparameter optimization, and evaluation, enabling rapid and simpleexperiments and result analysis. Aimed at ML practitioners and researchers, theframework offers implementations of methods, datasets, metrics, and standardinterfaces for these components to improve extensibility. By facilitating thedevelopment of fair ML practices, Aequitas Flow seeks to enhance the adoptionof these concepts in AI technologies.</description><author>S√©rgio Jesus, Pedro Saleiro, In√™s Oliveira e Silva, Beatriz M. Jorge, Rita P. Ribeiro, Jo√£o Gama, Pedro Bizarro, Rayid Ghani</author><pubDate>Thu, 09 May 2024 15:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05809v1</guid></item><item><title>Fast and Controllable Post-training Sparsity: Learning Optimal Sparsity Allocation with Global Constraint in Minutes</title><link>http://arxiv.org/abs/2405.05808v1</link><description>Neural network sparsity has attracted many research interests due to itssimilarity to biological schemes and high energy efficiency. However, existingmethods depend on long-time training or fine-tuning, which prevents large-scaleapplications. Recently, some works focusing on post-training sparsity (PTS)have emerged. They get rid of the high training cost but usually suffer fromdistinct accuracy degradation due to neglect of the reasonable sparsity rate ateach layer. Previous methods for finding sparsity rates mainly focus on thetraining-aware scenario, which usually fails to converge stably under the PTSsetting with limited data and much less training cost. In this paper, wepropose a fast and controllable post-training sparsity (FCPTS) framework. Byincorporating a differentiable bridge function and a controllable optimizationobjective, our method allows for rapid and accurate sparsity allocationlearning in minutes, with the added assurance of convergence to a predeterminedglobal sparsity rate. Equipped with these techniques, we can surpass thestate-of-the-art methods by a large margin, e.g., over 30\% improvement forResNet-50 on ImageNet under the sparsity rate of 80\%. Our plug-and-play codeand supplementary materials are open-sourced athttps://github.com/ModelTC/FCPTS.</description><author>Ruihao Gong, Yang Yong, Zining Wang, Jinyang Guo, Xiuying Wei, Yuqing Ma, Xianglong Liu</author><pubDate>Thu, 09 May 2024 15:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05808v1</guid></item><item><title>Position: Leverage Foundational Models for Black-Box Optimization</title><link>http://arxiv.org/abs/2405.03547v2</link><description>Undeniably, Large Language Models (LLMs) have stirred an extraordinary waveof innovation in the machine learning research domain, resulting in substantialimpact across diverse fields such as reinforcement learning, robotics, andcomputer vision. Their incorporation has been rapid and transformative, markinga significant paradigm shift in the field of machine learning research.However, the field of experimental design, grounded on black-box optimization,has been much less affected by such a paradigm shift, even though integratingLLMs with optimization presents a unique landscape ripe for exploration. Inthis position paper, we frame the field of black-box optimization aroundsequence-based foundation models and organize their relationship with previousliterature. We discuss the most promising ways foundational language models canrevolutionize optimization, which include harnessing the vast wealth ofinformation encapsulated in free-form text to enrich task comprehension,utilizing highly flexible sequence models such as Transformers to engineersuperior optimization strategies, and enhancing performance prediction overpreviously unseen search spaces.</description><author>Xingyou Song, Yingtao Tian, Robert Tjarko Lange, Chansoo Lee, Yujin Tang, Yutian Chen</author><pubDate>Thu, 09 May 2024 15:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03547v2</guid></item><item><title>MasterWeaver: Taming Editability and Identity for Personalized Text-to-Image Generation</title><link>http://arxiv.org/abs/2405.05806v1</link><description>Text-to-image (T2I) diffusion models have shown significant success inpersonalized text-to-image generation, which aims to generate novel images withhuman identities indicated by the reference images. Despite promising identityfidelity has been achieved by several tuning-free methods, they usually sufferfrom overfitting issues. The learned identity tends to entangle with irrelevantinformation, resulting in unsatisfied text controllability, especially onfaces. In this work, we present MasterWeaver, a test-time tuning-free methoddesigned to generate personalized images with both faithful identity fidelityand flexible editability. Specifically, MasterWeaver adopts an encoder toextract identity features and steers the image generation through additionalintroduced cross attention. To improve editability while maintaining identityfidelity, we propose an editing direction loss for training, which aligns theediting directions of our MasterWeaver with those of the original T2I model.Additionally, a face-augmented dataset is constructed to facilitatedisentangled identity learning, and further improve the editability. Extensiveexperiments demonstrate that our MasterWeaver can not only generatepersonalized images with faithful identity, but also exhibit superiority intext controllability. Our code will be publicly available athttps://github.com/csyxwei/MasterWeaver.</description><author>Yuxiang Wei, Zhilong Ji, Jinfeng Bai, Hongzhi Zhang, Lei Zhang, Wangmeng Zuo</author><pubDate>Thu, 09 May 2024 15:42:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05806v1</guid></item><item><title>ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast</title><link>http://arxiv.org/abs/2402.01295v3</link><description>Data-driven weather forecast based on machine learning (ML) has experiencedrapid development and demonstrated superior performance in the globalmedium-range forecast compared to traditional physics-based dynamical models.However, most of these ML models struggle with accurately predicting extremeweather, which is closely related to the extreme value prediction. Throughmathematical analysis, we prove that the use of symmetric losses, such as theMean Squared Error (MSE), leads to biased predictions and underestimation ofextreme values. To address this issue, we introduce Exloss, a novel lossfunction that performs asymmetric optimization and highlights extreme values toobtain accurate extreme weather forecast. Furthermore, we introduce atraining-free extreme value enhancement strategy named ExEnsemble, whichincreases the variance of pixel values and improves the forecast robustness.Combined with an advanced global weather forecast model, extensive experimentsshow that our solution can achieve state-of-the-art performance in extremeweather prediction, while maintaining the overall forecast accuracy comparableto the top medium-range forecast models.</description><author>Wanghan Xu, Kang Chen, Tao Han, Hao Chen, Wanli Ouyang, Lei Bai</author><pubDate>Thu, 09 May 2024 15:40:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01295v3</guid></item><item><title>Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference</title><link>http://arxiv.org/abs/2405.05803v1</link><description>Multimodal large language models (MLLMs) demand considerable computations forinference due to the extensive parameters and the additional input tokensneeded for visual information representation. Herein, we introduce VisualTokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapidinference. Our approach is inspired by two intriguing phenomena we haveobserved: (1) the attention sink phenomenon that is prevalent in LLMs alsopersists in MLLMs, suggesting that initial tokens and nearest tokens receivethe majority of attention, while middle vision tokens garner minimal attentionin deep layers; (2) the presence of information migration, which implies thatvisual information is transferred to subsequent text tokens within the firstfew layers of MLLMs. As per our findings, we conclude that vision tokens arenot necessary in the deep layers of MLLMs. Thus, we strategically withdraw themat a certain layer, enabling only text tokens to engage in subsequent layers.To pinpoint the ideal layer for vision tokens withdrawal, we initially analyzea limited set of tiny datasets and choose the first layer that meets theKullback-Leibler divergence criterion. Our VTW approach can cut computationaloverhead by over 40\% across diverse multimodal tasks while maintainingperformance. Our code is released at https://github.com/lzhxmu/VTW.</description><author>Zhihang Lin, Mingbao Lin, Luxi Lin, Rongrong Ji</author><pubDate>Thu, 09 May 2024 15:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05803v1</guid></item><item><title>Deploying Graph Neural Networks in Wireless Networks: A Link Stability Viewpoint</title><link>http://arxiv.org/abs/2405.05802v1</link><description>As an emerging artificial intelligence technology, graph neural networks(GNNs) have exhibited promising performance across a wide range ofgraph-related applications. However, information exchanges among neighbor nodesin GNN pose new challenges in the resource-constrained scenario, especially inwireless systems. In practical wireless systems, the communication links amongnodes are usually unreliable due to wireless fading and receiver noise,consequently resulting in performance degradation of GNNs. To improve thelearning performance of GNNs, we aim to maximize the number of long-termaverage (LTA) communication links by the optimized power control under energyconsumption constraints. Using the Lyapunov optimization method, we firsttransform the intractable long-term problem into a deterministic problem ineach time slot by converting the long-term energy constraints into theobjective function. In spite of this non-convex combinatorial optimizationproblem, we address this problem via equivalently solving a sequence of convexfeasibility problems together with a greedy based solver. Simulation resultsdemonstrate the superiority of our proposed scheme over the baselines.</description><author>Jun Li, Weiwei Zhang, Kang Wei, Guangji Chen, Long Shi, Wen Chen</author><pubDate>Thu, 09 May 2024 15:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05802v1</guid></item><item><title>Learning Extrinsic Dexterity with Parameterized Manipulation Primitives</title><link>http://arxiv.org/abs/2310.17785v3</link><description>Many practically relevant robot grasping problems feature a target object forwhich all grasps are occluded, e.g., by the environment. Single-shot graspplanning invariably fails in such scenarios. Instead, it is necessary to firstmanipulate the object into a configuration that affords a grasp. We solve thisproblem by learning a sequence of actions that utilize the environment tochange the object's pose. Concretely, we employ hierarchical reinforcementlearning to combine a sequence of learned parameterized manipulationprimitives. By learning the low-level manipulation policies, our approach cancontrol the object's state through exploiting interactions between the object,the gripper, and the environment. Designing such a complex behavioranalytically would be infeasible under uncontrolled conditions, as an analyticapproach requires accurate physical modeling of the interaction and contactdynamics. In contrast, we learn a hierarchical policy model that operatesdirectly on depth perception data, without the need for object detection, poseestimation, or manual design of controllers. We evaluate our approach onpicking box-shaped objects of various weight, shape, and friction propertiesfrom a constrained table-top workspace. Our method transfers to a real robotand is able to successfully complete the object picking task in 98\% ofexperimental trials. Supplementary information and videos can be found athttps://shihminyang.github.io/ED-PMP/.</description><author>Shih-Min Yang, Martin Magnusson, Johannes A. Stork, Todor Stoyanov</author><pubDate>Thu, 09 May 2024 15:35:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17785v3</guid></item><item><title>DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian Representation</title><link>http://arxiv.org/abs/2405.05800v1</link><description>User-friendly 3D object editing is a challenging task that has attractedsignificant attention recently. The limitations of direct 3D object editingwithout 2D prior knowledge have prompted increased attention towards utilizing2D generative models for 3D editing. While existing methods like InstructNeRF-to-NeRF offer a solution, they often lack user-friendliness, particularlydue to semantic guided editing. In the realm of 3D representation, 3D GaussianSplatting emerges as a promising approach for its efficiency and naturalexplicit property, facilitating precise editing tasks. Building upon theseinsights, we propose DragGaussian, a 3D object drag-editing framework based on3D Gaussian Splatting, leveraging diffusion models for interactive imageediting with open-vocabulary input. This framework enables users to performdrag-based editing on pre-trained 3D Gaussian object models, producing modified2D images through multi-view consistent editing. Our contributions include theintroduction of a new task, the development of DragGaussian for interactivepoint-based 3D editing, and comprehensive validation of its effectivenessthrough qualitative and quantitative experiments.</description><author>Sitian Shen, Jing Xu, Yuheng Yuan, Xingyi Yang, Qiuhong Shen, Xinchao Wang</author><pubDate>Thu, 09 May 2024 15:34:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05800v1</guid></item><item><title>Adaptability and Homeostasis in the Game of Life interacting with the evolved Cellular Automata</title><link>http://arxiv.org/abs/2405.05797v1</link><description>In this paper we study the emergence of homeostasis in a two-layer system ofthe Game of Life, in which the Game of Life in the first layer couples withanother system of cellular automata in the second layer. Homeostasis is definedhere as a space-time dynamic that regulates the number of cells in state-1 inthe Game of Life layer. A genetic algorithm is used to evolve the rules of thesecond layer to control the pattern of the Game of Life. We discovered thatthere are two antagonistic attractors that control the numbers of cells instate-1 in the first layer. The homeostasis sustained by these attractors arecompared with the homeostatic dynamics observed in Daisy World.</description><author>Keisuke Suzuki, Takashi Ikegami</author><pubDate>Thu, 09 May 2024 15:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05797v1</guid></item><item><title>Enhancing Suicide Risk Detection on Social Media through Semi-Supervised Deep Label Smoothing</title><link>http://arxiv.org/abs/2405.05795v1</link><description>Suicide is a prominent issue in society. Unfortunately, many people at riskfor suicide do not receive the support required. Barriers to people receivingsupport include social stigma and lack of access to mental health care. Withthe popularity of social media, people have turned to online forums, such asReddit to express their feelings and seek support. This provides theopportunity to support people with the aid of artificial intelligence. Socialmedia posts can be classified, using text classification, to help connectpeople with professional help. However, these systems fail to account for theinherent uncertainty in classifying mental health conditions. Unlike otherareas of healthcare, mental health conditions have no objective measurements ofdisease often relying on expert opinion. Thus when formulating deep learningproblems involving mental health, using hard, binary labels does not accuratelyrepresent the true nature of the data. In these settings, where human expertsmay disagree, fuzzy or soft labels may be more appropriate. The current workintroduces a novel label smoothing method which we use to capture anyuncertainty within the data. We test our approach on a five-label multi-classclassification problem. We show, our semi-supervised deep label smoothingmethod improves classification accuracy above the existing state of the art.Where existing research reports an accuracy of 43\% on the Reddit C-SSRSdataset, using empirical experiments to evaluate our novel label smoothingmethod, we improve upon this existing benchmark to 52\%. These improvements inmodel performance have the potential to better support those experiencingmental distress. Future work should explore the use of probabilistic methods inboth natural language processing and quantifying contributions of bothepistemic and aleatoric uncertainty in noisy datasets.</description><author>Matthew Squires, Xiaohui Tao, Soman Elangovan, U Rajendra Acharya, Raj Gururajan, Haoran Xie, Xujuan Zhou</author><pubDate>Thu, 09 May 2024 15:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05795v1</guid></item><item><title>Sequential Amodal Segmentation via Cumulative Occlusion Learning</title><link>http://arxiv.org/abs/2405.05791v1</link><description>To fully understand the 3D context of a single image, a visual system must beable to segment both the visible and occluded regions of objects, whilediscerning their occlusion order. Ideally, the system should be able to handleany object and not be restricted to segmenting a limited set of object classes,especially in robotic applications. Addressing this need, we introduce adiffusion model with cumulative occlusion learning designed for sequentialamodal segmentation of objects with uncertain categories. This modeliteratively refines the prediction using the cumulative mask strategy duringdiffusion, effectively capturing the uncertainty of invisible regions andadeptly reproducing the complex distribution of shapes and occlusion orders ofoccluded objects. It is akin to the human capability for amodal perception,i.e., to decipher the spatial ordering among objects and accurately predictcomplete contours for occluded objects in densely layered visual scenes.Experimental results across three amodal datasets show that our methodoutperforms established baselines.</description><author>Jiayang Ao, Qiuhong Ke, Krista A. Ehinger</author><pubDate>Thu, 09 May 2024 15:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05791v1</guid></item><item><title>RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation</title><link>http://arxiv.org/abs/2405.05792v1</link><description>Mapping is crucial for spatial reasoning, planning and robot navigation.Existing approaches range from metric, which require precise geometry-basedoptimization, to purely topological, where image-as-node based graphs lackexplicit object-level reasoning and interconnectivity. In this paper, wepropose a novel topological representation of an environment based on "imagesegments", which are semantically meaningful and open-vocabulary queryable,conferring several advantages over previous works based on pixel-levelfeatures. Unlike 3D scene graphs, we create a purely topological graph withsegments as nodes, where edges are formed by a) associating segment-leveldescriptors between pairs of consecutive images and b) connecting neighboringsegments within an image using their pixel centroids. This unveils a"continuous sense of a place", defined by inter-image persistence of segmentsalong with their intra-image neighbours. It further enables us to represent andupdate segment-level descriptors through neighborhood aggregation using graphconvolution layers, which improves robot localization based on segment-levelretrieval. Using real-world data, we show how our proposed map representationcan be used to i) generate navigation plans in the form of "hops over segments"and ii) search for target objects using natural language queries describingspatial relations of objects. Furthermore, we quantitatively analyze dataassociation at the segment level, which underpins inter-image connectivityduring mapping and segment-level localization when revisiting the same place.Finally, we show preliminary trials on segment-level `hopping' based zero-shotreal-world navigation. Project page with supplementary details:oravus.github.io/RoboHop/</description><author>Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko S√ºnderhauf, Feras Dayoub, Ian Reid</author><pubDate>Thu, 09 May 2024 15:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05792v1</guid></item><item><title>A Robust eLORETA Technique for Localization of Brain Sources in the Presence of Forward Model Uncertainties</title><link>http://arxiv.org/abs/2405.05790v1</link><description>In this paper, we present a robust version of the well-known exactlow-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA,to localize brain sources in the presence of different forward modeluncertainties. Methods: We first assume that the true lead field matrix is atransformation of the existing lead field matrix distorted by uncertainties andpropose an iterative approach to estimate this transformation accurately. Majorsources of the forward model uncertainties, including differences in geometry,conductivity, and source space resolution between the real and simulated headmodels, and misaligned electrode positions, are then simulated to test theproposed method. Results: ReLORETA and eLORETA are applied to simulated focalsources in different regions of the brain and the presence of various noiselevels as well as real data from a patient with focal epilepsy. The resultsshow that ReLORETA is considerably more robust and accurate than eLORETA in allcases. Conclusion: Having successfully dealt with the forward modeluncertainties, ReLORETA proved to be a promising method for real-world clinicalapplications. Significance: eLORETA is one of the localization techniques thatcould be used to study brain activity for medical applications such asdetermining the epileptogenic zone in patients with medically refractoryepilepsy. However, the major limitation of eLORETA is sensitivity to theuncertainties in the forward model. Since this problem can substantiallyundermine its performance in real-world applications where the exact lead fieldmatrix is unknown, developing a more robust method capable of dealing withthese uncertainties is of significant interest.</description><author>A. Noroozi, M. Ravan, B. Razavi, R. S. Fisher, Y. Law, M. S. Hasan</author><pubDate>Thu, 09 May 2024 15:15:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05790v1</guid></item><item><title>You Only Cache Once: Decoder-Decoder Architectures for Language Models</title><link>http://arxiv.org/abs/2405.05254v2</link><description>We introduce a decoder-decoder architecture, YOCO, for large language models,which only caches key-value pairs once. It consists of two components, i.e., across-decoder stacked upon a self-decoder. The self-decoder efficiently encodesglobal key-value (KV) caches that are reused by the cross-decoder viacross-attention. The overall model behaves like a decoder-only Transformer,although YOCO only caches once. The design substantially reduces GPU memorydemands, yet retains global attention capability. Additionally, the computationflow enables prefilling to early exit without changing the final output,thereby significantly speeding up the prefill stage. Experimental resultsdemonstrate that YOCO achieves favorable performance compared to Transformer invarious settings of scaling up model size and number of training tokens. Wealso extend YOCO to 1M context length with near-perfect needle retrievalaccuracy. The profiling results show that YOCO improves inference memory,prefill latency, and throughput by orders of magnitude across context lengthsand model sizes. Code is available at https://aka.ms/YOCO.</description><author>Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, Furu Wei</author><pubDate>Thu, 09 May 2024 15:12:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05254v2</guid></item><item><title>DRSI-Net: Dual-Residual Spatial Interaction Network for Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2402.16640v2</link><description>Multi-person pose estimation (MPPE), which aims to locate the key points forall persons in the frames, is an active research branch of computer vision.Variable human poses and complex scenes make MPPE dependent on local detailsand global structures; their absence may cause key point feature misalignment.In this case, high-order spatial interactions that can effectively link thelocal and global information of features are particularly important. However,most methods do not include spatial interactions. A few methods have low-orderspatial interactions, but achieving a good balance between accuracy andcomplexity is challenging. To address the above problems, a dual-residualspatial interaction network (DRSI-Net) for MPPE with high accuracy and lowcomplexity is proposed herein. Compared to other methods, DRSI-Net recursivelyperforms residual spatial information interactions on the neighbouring featuresso that more useful spatial information can be retained and more similaritiescan be obtained between shallow and deep extracted features. The channel andspatial dual attention mechanism introduced in the multi-scale feature fusionalso helps the network to adaptively focus on features relevant to the targetkey points and further refine the generated poses. Simultaneously, byoptimising the interactive channel dimensions and dividing the gradient flow,the spatial interaction module is designed to be lightweight, thus reducing thecomplexity of the network. According to the experimental results on the COCOdataset, the proposed DRSI-Net outperforms other state-of-the-art methods inaccuracy and complexity.</description><author>Shang Wu, Bin Wang</author><pubDate>Thu, 09 May 2024 15:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16640v2</guid></item><item><title>Autonomous Robotic Ultrasound System for Liver Follow-up Diagnosis: Pilot Phantom Study</title><link>http://arxiv.org/abs/2405.05787v1</link><description>The paper introduces a novel autonomous robot ultrasound (US) systemtargeting liver follow-up scans for outpatients in local communities. Given acomputed tomography (CT) image with specific target regions of interest, theproposed system carries out the autonomous follow-up scan in three steps: (i)initial robot contact to surface, (ii) coordinate mapping between CT image androbot, and (iii) target US scan. Utilizing 3D US-CT registration and deeplearning-based segmentation networks, we can achieve precise imaging of 3Dhepatic veins, facilitating accurate coordinate mapping between CT and therobot. This enables the automatic localization of follow-up targets within theCT image, allowing the robot to navigate precisely to the target's surface.Evaluation of the ultrasound phantom confirms the quality of the US-CTregistration and shows the robot reliably locates the targets in repeatedtrials. The proposed framework holds the potential to significantly reduce timeand costs for healthcare providers, clinicians, and follow-up patients, therebyaddressing the increasing healthcare burden associated with chronic disease inlocal communities.</description><author>Tianpeng Zhang, Sekeun Kim, Jerome Charton, Haitong Ma, Kyungsang Kim, Na Li, Quanzheng Li</author><pubDate>Thu, 09 May 2024 15:11:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05787v1</guid></item><item><title>FusionTransNet for Smart Urban Mobility: Spatiotemporal Traffic Forecasting Through Multimodal Network Integration</title><link>http://arxiv.org/abs/2405.05786v1</link><description>This study develops FusionTransNet, a framework designed forOrigin-Destination (OD) flow predictions within smart and multimodal urbantransportation systems. Urban transportation complexity arises from thespatiotemporal interactions among various traffic modes. Motivated by analyzingmultimodal data from Shenzhen, a framework that can dissect complicatedspatiotemporal interactions between these modes, from the microscopic locallevel to the macroscopic city-wide perspective, is essential. The frameworkcontains three core components: the Intra-modal Learning Module, theInter-modal Learning Module, and the Prediction Decoder. The Intra-modalLearning Module is designed to analyze spatial dependencies within individualtransportation modes, facilitating a granular understanding of single-modespatiotemporal dynamics. The Inter-modal Learning Module extends this analysis,integrating data across different modes to uncover cross-modalinterdependencies, by breaking down the interactions at both local and globalscales. Finally, the Prediction Decoder synthesizes insights from the precedingmodules to generate accurate OD flow predictions, translating complexmultimodal interactions into forecasts. Empirical evaluations conducted inmetropolitan contexts, including Shenzhen and New York, demonstrateFusionTransNet's superior predictive accuracy compared to existingstate-of-the-art methods. The implication of this study extends beyond urbantransportation, as the method for transferring information across differentspatiotemporal graphs at both local and global scales can be instrumental inother spatial systems, such as supply chain logistics and epidemics spreading.</description><author>Binwu Wang, Yan Leng, Guang Wang, Yang Wang</author><pubDate>Thu, 09 May 2024 15:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05786v1</guid></item><item><title>Gland Segmentation Via Dual Encoders and Boundary-Enhanced Attention</title><link>http://arxiv.org/abs/2401.15990v2</link><description>Accurate and automated gland segmentation on pathological images can assistpathologists in diagnosing the malignancy of colorectal adenocarcinoma.However, due to various gland shapes, severe deformation of malignant glands,and overlapping adhesions between glands. Gland segmentation has always beenvery challenging. To address these problems, we propose a DEA model. This modelconsists of two branches: the backbone encoding and decoding network and thelocal semantic extraction network. The backbone encoding and decoding networkextracts advanced Semantic features, uses the proposed feature decoder torestore feature space information, and then enhances the boundary features ofthe gland through boundary enhancement attention. The local semantic extractionnetwork uses the pre-trained DeepLabv3+ as a Local semantic-guided encoder torealize the extraction of edge features. Experimental results on two publicdatasets, GlaS and CRAG, confirm that the performance of our method is betterthan other gland segmentation methods.</description><author>Huadeng Wang, Jiejiang Yu, Bingbing Li, Xipeng Pan, Zhenbing Liu, Rushi Lan, Xiaonan Luo</author><pubDate>Thu, 09 May 2024 15:05:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15990v2</guid></item><item><title>Link Stealing Attacks Against Inductive Graph Neural Networks</title><link>http://arxiv.org/abs/2405.05784v1</link><description>A graph neural network (GNN) is a type of neural network that is specificallydesigned to process graph-structured data. Typically, GNNs can be implementedin two settings, including the transductive setting and the inductive setting.In the transductive setting, the trained model can only predict the labels ofnodes that were observed at the training time. In the inductive setting, thetrained model can be generalized to new nodes/graphs. Due to its flexibility,the inductive setting is the most popular GNN setting at the moment. Previouswork has shown that transductive GNNs are vulnerable to a series of privacyattacks. However, a comprehensive privacy analysis of inductive GNN models isstill missing. This paper fills the gap by conducting a systematic privacyanalysis of inductive GNNs through the lens of link stealing attacks, one ofthe most popular attacks that are specifically designed for GNNs. We proposetwo types of link stealing attacks, i.e., posterior-only attacks and combinedattacks. We define threat models of the posterior-only attacks with respect tonode topology and the combined attacks by considering combinations ofposteriors, node attributes, and graph features. Extensive evaluation on sixreal-world datasets demonstrates that inductive GNNs leak rich information thatenables link stealing attacks with advantageous properties. Even attacks withno knowledge about graph structures can be effective. We also show that ourattacks are robust to different node similarities and different graph features.As a counterpart, we investigate two possible defenses and discover they areineffective against our attacks, which calls for more effective defenses.</description><author>Yixin Wu, Xinlei He, Pascal Berrang, Mathias Humbert, Michael Backes, Neil Zhenqiang Gong, Yang Zhang</author><pubDate>Thu, 09 May 2024 15:03:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05784v1</guid></item><item><title>Neural Network Learning of Black-Scholes Equation for Option Pricing</title><link>http://arxiv.org/abs/2405.05780v1</link><description>One of the most discussed problems in the financial world is stock optionpricing. The Black-Scholes Equation is a Parabolic Partial DifferentialEquation which provides an option pricing model. The present work proposes anapproach based on Neural Networks to solve the Black-Scholes Equations.Real-world data from the stock options market were used as the initial boundaryto solve the Black-Scholes Equation. In particular, times series of calloptions prices of Brazilian companies Petrobras and Vale were employed. Theresults indicate that the network can learn to solve the Black-Sholes Equationfor a specific real-world stock options time series. The experimental resultsshowed that the Neural network option pricing based on the Black-SholesEquation solution can reach an option pricing forecasting more accurate thanthe traditional Black-Sholes analytical solutions. The experimental resultsmaking it possible to use this methodology to make short-term call option priceforecasts in options markets.</description><author>Daniel de Souza Santos, Tiago Alessandro Espinola Ferreira</author><pubDate>Thu, 09 May 2024 14:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05780v1</guid></item><item><title>SlimPajama-DC: Understanding Data Combinations for LLM Training</title><link>http://arxiv.org/abs/2309.10818v3</link><description>This paper aims to understand the impacts of various data combinations (e.g.,web text, Wikipedia, GitHub, books) on the pretraining of large language modelsusing SlimPajama. SlimPajama is a rigorously deduplicated, multi-sourcedataset, which has been refined and further deduplicated to 627B tokens fromthe extensive 1.2T token RedPajama dataset contributed by Together. We havetermed our research as SlimPajama-DC, an empirical analysis designed to uncoverfundamental characteristics and best practices associated with employingSlimPajama in the training of large language models. During our research withSlimPajama, two pivotal observations emerged: (1) Global deduplication vs.local deduplication. We analyze and discuss how global (across differentsources of datasets) and local (within the single source of dataset)deduplications affect the performance of trained models. (2) Proportions ofhighly-deduplicated multi-source datasets in the combination. To study this, weconstruct six configurations on SlimPajama dataset and train individual onesusing 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configurationoutperforms the 1.3B model trained on RedPajama using the same number oftraining tokens by a significant margin. All our 1.3B models are trained onCerebras 16$\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixedprecision. We further extend our discoveries (such as increasing data diversityis crucial after global deduplication) on a 7B model with large batch-sizetraining. Our SlimPajama-DC models are available at:https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DCdatasets are available at:https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.</description><author>Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, Eric Xing</author><pubDate>Thu, 09 May 2024 14:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10818v3</guid></item><item><title>Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the S√°mi Language</title><link>http://arxiv.org/abs/2405.05777v1</link><description>S\'ami, an indigenous language group comprising multiple languages, facesdigital marginalization due to the limited availability of data andsophisticated language models designed for its linguistic intricacies. Thiswork focuses on increasing technological participation for the S\'ami language.We draw the attention of the ML community towards the language modeling problemof Ultra Low Resource (ULR) languages. ULR languages are those for which theamount of available textual resources is very low, and the speaker count forthem is also very low. ULRLs are also not supported by mainstream LargeLanguage Models (LLMs) like ChatGPT, due to which gathering artificial trainingdata for them becomes even more challenging. Mainstream AI foundational modeldevelopment has given less attention to this category of languages. Generally,these languages have very few speakers, making it hard to find them. However,it is important to develop foundational models for these ULR languages topromote inclusion and the tangible abilities and impact of LLMs. To this end,we have compiled the available S\'ami language resources from the web to createa clean dataset for training language models. In order to study the behavior ofmodern LLM models with ULR languages (S\'ami), we have experimented withdifferent kinds of LLMs, mainly at the order of $\sim$ seven billionparameters. We have also explored the effect of multilingual LLM training forULRLs. We found that the decoder-only models under a sequential multilingualtraining scenario perform better than joint multilingual training, whereasmultilingual training with high semantic overlap, in general, performs betterthan training from scratch.This is the first study on the S\'ami language foradapting non-statistical language models that use the latest developments inthe field of natural language processing (NLP).</description><author>Ronny Paul, Himanshu Buckchash, Shantipriya Parida, Dilip K. Prasad</author><pubDate>Thu, 09 May 2024 14:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05777v1</guid></item><item><title>Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions</title><link>http://arxiv.org/abs/2405.05776v1</link><description>Human communication is based on a variety of inferences that we draw fromsentences, often going beyond what is literally said. While there is wideagreement on the basic distinction between entailment, implicature, andpresupposition, the status of many inferences remains controversial. In thispaper, we focus on three inferences of plain and embedded disjunctions, andcompare them with regular scalar implicatures. We investigate this comparisonfrom the novel perspective of the predictions of state-of-the-art largelanguage models, using the same experimental paradigms as recent studiesinvestigating the same inferences with humans. The results of our bestperforming models mostly align with those of humans, both in the largedifferences we find between those inferences and implicatures, as well as infine-grained distinctions among different aspects of those inferences.</description><author>Polina Tsvilodub, Paul Marty, Sonia Ramotowska, Jacopo Romoli, Michael Franke</author><pubDate>Thu, 09 May 2024 14:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05776v1</guid></item><item><title>Exploring Text-Guided Single Image Editing for Remote Sensing Images</title><link>http://arxiv.org/abs/2405.05769v1</link><description>Artificial Intelligence Generative Content (AIGC) technologies havesignificantly influenced the remote sensing domain, particularly in the realmof image generation. However, remote sensing image editing, an equally vitalresearch area, has not garnered sufficient attention. Different fromtext-guided editing in natural images, which relies on extensive text-imagepaired data for semantic correlation, the application scenarios of remotesensing image editing are often extreme, such as forest on fire, so it isdifficult to obtain sufficient paired samples. At the same time, the lack ofremote sensing semantics and the ambiguity of text also restrict the furtherapplication of image editing in remote sensing field. To solve above problems,this letter proposes a diffusion based method to fulfill stable andcontrollable remote sensing image editing with text guidance. Our method avoidsthe use of a large number of paired image, and can achieve good image editingresults using only a single image. The quantitative evaluation system includingCLIP score and subjective evaluation metrics shows that our method has betterediting effect on remote sensing images than the existing image editing model.</description><author>Fangzhou Han, Lingyu Si, Hongwei Dong, Lamei Zhang, Hao Chen, Bo Du</author><pubDate>Thu, 09 May 2024 14:45:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05769v1</guid></item><item><title>FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic Gaussian Splatting</title><link>http://arxiv.org/abs/2405.05768v1</link><description>Text-driven 3D indoor scene generation holds broad applications, ranging fromgaming and smart homes to AR/VR applications. Fast and high-fidelity scenegeneration is paramount for ensuring user-friendly experiences. However,existing methods are characterized by lengthy generation processes ornecessitate the intricate manual specification of motion parameters, whichintroduces inconvenience for users. Furthermore, these methods often rely onnarrow-field viewpoint iterative generations, compromising global consistencyand overall scene quality. To address these issues, we propose FastScene, aframework for fast and higher-quality 3D scene generation, while maintainingthe scene consistency. Specifically, given a text prompt, we generate apanorama and estimate its depth, since the panorama encompasses informationabout the entire scene and exhibits explicit geometric constraints. To obtainhigh-quality novel views, we introduce the Coarse View Synthesis (CVS) andProgressive Novel View Inpainting (PNVI) strategies, ensuring both sceneconsistency and view quality. Subsequently, we utilize Multi-View Projection(MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) forscene reconstruction. Comprehensive experiments demonstrate FastScene surpassesother methods in both generation speed and quality with better sceneconsistency. Notably, guided only by a text prompt, FastScene can generate a 3Dscene within a mere 15 minutes, which is at least one hour faster thanstate-of-the-art methods, making it a paradigm for user-friendly scenegeneration.</description><author>Yikun Ma, Dandan Zhan, Zhi Jin</author><pubDate>Thu, 09 May 2024 14:44:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05768v1</guid></item><item><title>Large Language Model-Aided Evolutionary Search for Constrained Multiobjective Optimization</title><link>http://arxiv.org/abs/2405.05767v1</link><description>Evolutionary algorithms excel in solving complex optimization problems,especially those with multiple objectives. However, their stochastic nature cansometimes hinder rapid convergence to the global optima, particularly inscenarios involving constraints. In this study, we employ a large languagemodel (LLM) to enhance evolutionary search for solving constrainedmulti-objective optimization problems. Our aim is to speed up the convergenceof the evolutionary population. To achieve this, we finetune the LLM throughtailored prompt engineering, integrating information concerning both objectivevalues and constraint violations of solutions. This process enables the LLM tograsp the relationship between well-performing and poorly performing solutionsbased on the provided input data. Solution's quality is assessed based on theirconstraint violations and objective-based performance. By leveraging therefined LLM, it can be used as a search operator to generate superior-qualitysolutions. Experimental evaluations across various test benchmarks illustratethat LLM-aided evolutionary search can significantly accelerate thepopulation's convergence speed and stands out competitively againstcutting-edge evolutionary algorithms.</description><author>Zeyi Wang, Songbai Liu, Jianyong Chen, Kay Chen Tan</author><pubDate>Thu, 09 May 2024 14:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05767v1</guid></item><item><title>To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems</title><link>http://arxiv.org/abs/2405.05766v1</link><description>The increasing reliance on Deep Learning models, combined with their inherentlack of transparency, has spurred the development of a novel field of studyknown as eXplainable AI (XAI) methods. These methods seek to enhance the trustof end-users in automated systems by providing insights into the rationalebehind their decisions. This paper presents a novel approach for measuring usertrust in XAI systems, allowing their refinement. Our proposed metric combinesboth performance metrics and trust indicators from an objective perspective. Tovalidate this novel methodology, we conducted a case study in a realisticmedical scenario: the usage of XAI system for the detection of pneumonia fromx-ray images.</description><author>Miquel Mir√≥-Nicolau, Gabriel Moy√†-Alcover, Antoni Jaume-i-Cap√≥, Manuel Gonz√°lez-Hidalgo, Maria Gemma Sempere Campello, Juan Antonio Palmer Sancho</author><pubDate>Thu, 09 May 2024 14:42:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05766v1</guid></item><item><title>DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion Models</title><link>http://arxiv.org/abs/2405.05763v1</link><description>Detail features of magnetic resonance images play a cru-cial role in accuratemedical diagnosis and treatment, as they capture subtle changes that posechallenges for doc-tors when performing precise judgments. However, the widelyutilized naive diffusion model has limitations, as it fails to accuratelycapture more intricate details. To en-hance the quality of MRI reconstruction,we propose a comprehensive detail-preserving reconstruction method usingmultiple diffusion models to extract structure and detail features in k-spacedomain instead of image do-main. Moreover, virtual binary modal masks areutilized to refine the range of values in k-space data through highly adaptivecenter windows, which allows the model to focus its attention more efficiently.Last but not least, an inverted pyramid structure is employed, where thetop-down image information gradually decreases, ena-bling a cascaderepresentation. The framework effective-ly represents multi-scale sampled data,taking into ac-count the sparsity of the inverted pyramid architecture, andutilizes cascade training data distribution to repre-sent multi-scale data.Through a step-by-step refinement approach, the method refines theapproximation of de-tails. Finally, the proposed method was evaluated bycon-ducting experiments on clinical and public datasets. The resultsdemonstrate that the proposed method outper-forms other methods.</description><author>Mengxiao Geng, Jiahao Zhu, Xiaolin Zhu, Qiqing Liu, Dong Liang, Qiegen Liu</author><pubDate>Thu, 09 May 2024 14:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05763v1</guid></item><item><title>Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media</title><link>http://arxiv.org/abs/2405.05760v1</link><description>The purpose of semantic location prediction is to extract relevant semanticlocation information from multimodal social media posts, offering a morecontextual understanding of daily activities compared to GPS coordinates.However, this task becomes challenging due to the presence of noise andirrelevant information in "text-image" pairs. Existing methods suffer frominsufficient feature representations and fail to consider the comprehensiveintegration of similarity at different granularities, making it difficult tofilter out noise and irrelevant information. To address these challenges, wepropose a Similarity-Guided Multimodal Fusion Transformer (SG-MFT) forpredicting social users' semantic locations. First, we utilize a pre-trainedlarge-scale vision-language model to extract high-quality featurerepresentations from social media posts. Then, we introduce a Similarity-GuidedInteraction Module (SIM) to alleviate modality heterogeneity and noiseinterference by incorporating coarse-grained and fine-grained similarityguidance for modality interactions. Specifically, we propose a novelsimilarity-aware feature interpolation attention mechanism at the coarse level,leveraging modality-wise similarity to mitigate heterogeneity and reduce noisewithin each modality. Meanwhile, we employ a similarity-aware feed-forwardblock at the fine level, utilizing element-wise similarity to further mitigatethe impact of modality heterogeneity. Building upon pre-processed features withminimal noise and modal interference, we propose a Similarity-aware FeatureFusion Module (SFM) to fuse two modalities with cross-attention mechanism.Comprehensive experimental results demonstrate the superior performance of ourproposed method in handling modality imbalance while maintaining efficientfusion effectiveness.</description><author>Zhizhen Zhang, Ning Wang, Haojie Li, Zhihui Wang</author><pubDate>Thu, 09 May 2024 14:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05760v1</guid></item><item><title>DGMamba: Domain Generalization via Generalized State Space Model</title><link>http://arxiv.org/abs/2404.07794v2</link><description>Domain generalization~(DG) aims at solving distribution shift problems invarious scenes. Existing approaches are based on Convolution Neural Networks(CNNs) or Vision Transformers (ViTs), which suffer from limited receptivefields or quadratic complexities issues. Mamba, as an emerging state spacemodel (SSM), possesses superior linear complexity and global receptive fields.Despite this, it can hardly be applied to DG to address distribution shifts,due to the hidden state issues and inappropriate scan mechanisms. In thispaper, we propose a novel framework for DG, named DGMamba, that excels instrong generalizability toward unseen domains and meanwhile has the advantagesof global receptive fields, and efficient linear complexity. Our DGMambacompromises two core components: Hidden State Suppressing~(HSS) andSemantic-aware Patch refining~(SPR). In particular, HSS is introduced tomitigate the influence of hidden states associated with domain-specificfeatures during output prediction. SPR strives to encourage the model toconcentrate more on objects rather than context, consisting of two designs:Prior-Free Scanning~(PFS), and Domain Context Interchange~(DCI). Concretely,PFS aims to shuffle the non-semantic patches within images, creating moreflexible and effective sequences from images, and DCI is designed to regularizeMamba with the combination of mismatched non-semantic and semantic informationby fusing patches among domains. Extensive experiments on four commonly used DGbenchmarks demonstrate that the proposed DGMamba achieves remarkably superiorresults to state-of-the-art models. The code will be made publicly available.</description><author>Shaocong Long, Qianyu Zhou, Xiangtai Li, Xuequan Lu, Chenhao Ying, Yuan Luo, Lizhuang Ma, Shuicheng Yan</author><pubDate>Thu, 09 May 2024 14:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07794v2</guid></item><item><title>Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma</title><link>http://arxiv.org/abs/2405.05758v1</link><description>Qualitative analysis is a challenging, yet crucial aspect of advancingresearch in the field of Human-Computer Interaction (HCI). Recent studies showthat large language models (LLMs) can perform qualitative coding withinexisting schemes, but their potential for collaborative human-LLM discovery andnew insight generation in qualitative analysis is still underexplored. Tobridge this gap and advance qualitative analysis by harnessing the power ofLLMs, we propose CHALET, a novel methodology that leverages the human-LLMcollaboration paradigm to facilitate conceptualization and empower qualitativeresearch. The CHALET approach involves LLM-supported data collection,performing both human and LLM deductive coding to identify disagreements, andperforming collaborative inductive coding on these disagreement cases to derivenew conceptual insights. We validated the effectiveness of CHALET through itsapplication to the attribution model of mental-illness stigma, uncoveringimplicit stigmatization themes on cognitive, emotional and behavioraldimensions. We discuss the implications for future research, methodology, andthe transdisciplinary opportunities CHALET presents for the HCI community andbeyond.</description><author>Han Meng, Yitian Yang, Yunan Li, Jungup Lee, Yi-Chieh Lee</author><pubDate>Thu, 09 May 2024 14:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05758v1</guid></item><item><title>CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks</title><link>http://arxiv.org/abs/2405.05755v1</link><description>In recent years, convolutional neural networks (CNNs) with channel-wisefeature refining mechanisms have brought noticeable benefits to modellingchannel dependencies. However, current attention paradigms fail to infer anoptimal channel descriptor capable of simultaneously exploiting statistical andspatial relationships among feature maps. In this paper, to overcome thisshortcoming, we present a novel channel-wise spatially autocorrelated (CSA)attention mechanism. Inspired by geographical analysis, the proposed CSAexploits the spatial relationships between channels of feature maps to producean effective channel descriptor. To the best of our knowledge, this is the first time that the concept of geographical spatial analysis is utilized in deepCNNs. The proposed CSA imposes negligible learning parameters and lightcomputational overhead to the deep model, making it a powerful yet efficientattention module of choice. We validate the effectiveness of the proposed CSAnetworks (CSA-Nets) through extensive experiments and analysis on ImageNet, andMS COCO benchmark datasets for image classification, object detection, andinstance segmentation. The experimental results demonstrate that CSA-Nets areable to consistently achieve competitive performance and superiorgeneralization than several state-of-the-art attention-based CNNs overdifferent benchmark tasks and datasets.</description><author>Nick, Nikzad, Yongsheng Gao, Jun Zhou</author><pubDate>Thu, 09 May 2024 14:21:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05755v1</guid></item><item><title>Behavioural Rehearsing Illuminates Scientific Problems of Organised Complexity</title><link>http://arxiv.org/abs/2401.09851v3</link><description>As artificial intelligence becomes increasingly prevalent in scientificresearch, data-driven methodologies appear to overshadow traditional methods inresolving scientific problems. In this Perspective, we revisit a classicclassification of scientific problems and rethink the evolution of scientificparadigms from the standpoint of data, algorithms, and computational power. Weobserve that the strengths of new paradigms have expanded the range ofresolvable scientific problems, but the continued advancement of data,algorithms, and computational power is unlikely to bring a new paradigm. Totackle unresolved problems of organised complexity in more intricate systems,we argue that the integration of paradigms is a promising approach.Consequently, we propose behavioural rehearsing, checking what will happen insuch systems through multiple times of simulation. One of the methodologies torealise it, sophisticated behavioural simulation (SBS), represents a higherlevel of paradigms integration based on foundational models to simulate complexsocial systems involving sophisticated human strategies and behaviours. SBSextends beyond the capabilities of traditional agent-based modelling simulation(ABMS), and therefore, makes behavioural rehearsing a potential solution toproblems of organised complexity in complex human systems.</description><author>Cheng Wang, Chuwen Wang, Wang Zhang, Shirong Zeng, Yu Zhao, Ronghui Ning, Changjun Jiang</author><pubDate>Thu, 09 May 2024 14:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09851v3</guid></item><item><title>A Multi-Level Superoptimizer for Tensor Programs</title><link>http://arxiv.org/abs/2405.05751v1</link><description>We introduce Mirage, the first multi-level superoptimizer for tensorprograms. A key idea in Mirage is $\mu$Graphs, a uniform representation oftensor programs at the kernel, thread block, and thread levels of the GPUcompute hierarchy. $\mu$Graphs enable Mirage to discover novel optimizationsthat combine algebraic transformations, schedule transformations, andgeneration of new custom kernels. To navigate the large search space, Mirageintroduces a pruning technique based on abstraction that significantly reducesthe search space and provides a certain optimality guarantee. To ensure thatthe optimized $\mu$Graph is equivalent to the input program, Mirage introducesa probabilistic equivalence verification procedure with strong theoreticalguarantees. Our evaluation shows that Mirage outperforms existing approaches byup to 3.5$\times$ even for DNNs that are widely used and heavily optimized.Mirage is publicly available at https://github.com/mirage-project/mirage.</description><author>Mengdi Wu, Xinhao Cheng, Oded Padon, Zhihao Jia</author><pubDate>Thu, 09 May 2024 14:15:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05751v1</guid></item><item><title>NeRFFaceSpeech: One-shot Audio-diven 3D Talking Head Synthesis via Generative Prior</title><link>http://arxiv.org/abs/2405.05749v1</link><description>Audio-driven talking head generation is advancing from 2D to 3D content.Notably, Neural Radiance Field (NeRF) is in the spotlight as a means tosynthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-basedapproach typically requires a large number of paired audio-visual data for eachidentity, thereby limiting the scalability of the method. Although there havebeen attempts to generate audio-driven 3D talking head animations with a singleimage, the results are often unsatisfactory due to insufficient information onobscured regions in the image. In this paper, we mainly focus on addressing theoverlooked aspect of 3D consistency in the one-shot, audio-driven domain, wherefacial animations are synthesized primarily in front-facing perspectives. Wepropose a novel method, NeRFFaceSpeech, which enables to produce high-quality3D-aware talking head. Using prior knowledge of generative models combined withNeRF, our method can craft a 3D-consistent facial feature space correspondingto a single image. Our spatial synchronization method employs audio-correlatedvertex dynamics of a parametric face model to transform static image featuresinto dynamic visuals through ray deformation, ensuring realistic 3D facialmotion. Moreover, we introduce LipaintNet that can replenish the lackinginformation in the inner-mouth area, which can not be obtained from a givensingle image. The network is trained in a self-supervised manner by utilizingthe generative capabilities without additional data. The comprehensiveexperiments demonstrate the superiority of our method in generatingaudio-driven talking heads from a single image with enhanced 3D consistencycompared to previous approaches. In addition, we introduce a quantitative wayof measuring the robustness of a model against pose changes for the first time,which has been possible only qualitatively.</description><author>Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</author><pubDate>Thu, 09 May 2024 14:14:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05749v1</guid></item><item><title>Learning to Slice Wi-Fi Networks: A State-Augmented Primal-Dual Approach</title><link>http://arxiv.org/abs/2405.05748v1</link><description>Network slicing is a key feature in 5G/NG cellular networks that createscustomized slices for different service types with various quality-of-service(QoS) requirements, which can achieve service differentiation and guaranteeservice-level agreement (SLA) for each service type. In Wi-Fi networks, thereis limited prior work on slicing, and a potential solution is based on amulti-tenant architecture on a single access point (AP) that dedicatesdifferent channels to different slices. In this paper, we define a flexible,constrained learning framework to enable slicing in Wi-Fi networks subject toQoS requirements. We specifically propose an unsupervised learning-basednetwork slicing method that leverages a state-augmented primal-dual algorithm,where a neural network policy is trained offline to optimize a Lagrangianfunction and the dual variable dynamics are updated online in the executionphase. We show that state augmentation is crucial for generating slicingdecisions that meet the ergodic QoS requirements.</description><author>Yiƒüit Berkay Uslu, Roya Doostnejad, Alejandro Ribeiro, Navid NaderiAlizadeh</author><pubDate>Thu, 09 May 2024 14:13:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05748v1</guid></item><item><title>Efficient Pretraining Model based on Multi-Scale Local Visual Field Feature Reconstruction for PCB CT Image Element Segmentation</title><link>http://arxiv.org/abs/2405.05745v1</link><description>Element segmentation is a key step in nondestructive testing of PrintedCircuit Boards (PCB) based on Computed Tomography (CT) technology. In recentyears, the rapid development of self-supervised pretraining technology canobtain general image features without labeled samples, and then use a smallamount of labeled samples to solve downstream tasks, which has a good potentialin PCB element segmentation. At present, Masked Image Modeling (MIM)pretraining model has been initially applied in PCB CT image elementsegmentation. However, due to the small and regular size of PCB elements suchas vias, wires, and pads, the global visual field has redundancy for a singleelement reconstruction, which may damage the performance of the model. Based onthis issue, we propose an efficient pretraining model based on multi-scalelocal visual field feature reconstruction for PCB CT image element segmentation(EMLR-seg). In this model, the teacher-guided MIM pretraining model isintroduced into PCB CT image element segmentation for the first time, and amulti-scale local visual field extraction (MVE) module is proposed to reduceredundancy by focusing on local visual fields. At the same time, a simple4-Transformer-blocks decoder is used. Experiments show that EMLR-seg canachieve 88.6% mIoU on the PCB CT image dataset we proposed, which exceeds 1.2%by the baseline model, and the training time is reduced by 29.6 hours, areduction of 17.4% under the same experimental condition, which reflects theadvantage of EMLR-seg in terms of performance and efficiency.</description><author>Chen Chen, Kai Qiao, Jie Yang, Jian Chen, Bin Yan</author><pubDate>Thu, 09 May 2024 14:10:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05745v1</guid></item><item><title>Accuracy vs Memory Advantage in the Quantum Simulation of Stochastic Processes</title><link>http://arxiv.org/abs/2312.13473v2</link><description>Many inference scenarios rely on extracting relevant information from knowndata in order to make future predictions. When the underlying stochasticprocess satisfies certain assumptions, there is a direct mapping between itsexact classical and quantum simulators, with the latter asymptotically usingless memory. Here we focus on studying whether such quantum advantage persistswhen those assumptions are not satisfied, and the model is doomed to haveimperfect accuracy. By studying the trade-off between accuracy and memoryrequirements, we show that quantum models can reach the same accuracy with lessmemory, or alternatively, better accuracy with the same memory. Finally, wediscuss the implications of this result for learning tasks.</description><author>Leonardo Banchi</author><pubDate>Thu, 09 May 2024 14:06:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13473v2</guid></item><item><title>How Quality Affects Deep Neural Networks in Fine-Grained Image Classification</title><link>http://arxiv.org/abs/2405.05742v1</link><description>In this paper, we propose a No-Reference Image Quality Assessment (NRIQA)guided cut-off point selection (CPS) strategy to enhance the performance of afine-grained classification system. Scores given by existing NRIQA methods onthe same image may vary and not be as independent of natural imageaugmentations as expected, which weakens their connection and explainability tofine-grained image classification. Taking the three most commonly adopted imageaugmentation configurations -- cropping, rotating, and blurring -- as the entrypoint, we formulate a two-step mechanism for selecting the most discriminativesubset from a given image dataset by considering both the confidence of modelpredictions and the density distribution of image qualities over several NRIQAmethods. Concretely, the cut-off points yielded by those methods are aggregatedvia majority voting to inform the process of image subset selection. Theefficacy and efficiency of such a mechanism have been confirmed by comparingthe models being trained on high-quality images against a combination of high-and low-quality ones, with a range of 0.7% to 4.2% improvement on a commercialproduct dataset in terms of mean accuracy through four deep neural classifiers.The robustness of the mechanism has been proven by the observations that allthe selected high-quality images can work jointly with 70% low-quality imageswith 1.3% of classification precision sacrificed when using ResNet34 in anablation study.</description><author>Joseph Smith, Zheming Zuo, Jonathan Stonehouse, Boguslaw Obara</author><pubDate>Thu, 09 May 2024 13:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05742v1</guid></item><item><title>Can large language models understand uncommon meanings of common words?</title><link>http://arxiv.org/abs/2405.05741v1</link><description>Large language models (LLMs) like ChatGPT have shown significant advancementsacross diverse natural language understanding (NLU) tasks, includingintelligent dialogue and autonomous agents. Yet, lacking widely acknowledgedtesting mechanisms, answering `whether LLMs are stochastic parrots or genuinelycomprehend the world' remains unclear, fostering numerous studies and sparkingheated debates. Prevailing research mainly focuses on surface-level NLU,neglecting fine-grained explorations. However, such explorations are crucialfor understanding their unique comprehension mechanisms, aligning with humancognition, and finally enhancing LLMs' general NLU capacities. To address thisgap, our study delves into LLMs' nuanced semantic comprehension capabilities,particularly regarding common words with uncommon meanings. The idea stems fromfoundational principles of human communication within psychology, whichunderscore accurate shared understandings of word semantics. Specifically, thispaper presents the innovative construction of a Lexical Semantic Comprehension(LeSC) dataset with novel evaluation metrics, the first benchmark encompassingboth fine-grained and cross-lingual dimensions. Introducing models of bothopen-source and closed-source, varied scales and architectures, our extensiveempirical experiments demonstrate the inferior performance of existing modelsin this basic lexical-meaning understanding task. Notably, even thestate-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%and 22.3%, respectively. Additionally, multiple advanced prompting techniquesand retrieval-augmented generation are also introduced to help alleviate thistrouble, yet limitations persist. By highlighting the above criticalshortcomings, this research motivates further investigation and offers novelinsights for developing more intelligent LLMs.</description><author>Jinyang Wu, Feihu Che, Xinxin Zheng, Shuai Zhang, Ruihan Jin, Shuai Nie, Pengpeng Shao, Jianhua Tao</author><pubDate>Thu, 09 May 2024 13:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05741v1</guid></item><item><title>Optimal Baseline Corrections for Off-Policy Contextual Bandits</title><link>http://arxiv.org/abs/2405.05736v1</link><description>The off-policy learning paradigm allows for recommender systems and generalranking applications to be framed as decision-making problems, where we aim tolearn decision policies that optimize an unbiased offline estimate of an onlinereward metric. With unbiasedness comes potentially high variance, and prevalentmethods exist to reduce estimation variance. These methods typically make useof control variates, either additive (i.e., baseline corrections or doublyrobust methods) or multiplicative (i.e., self-normalisation). Our work unifiesthese approaches by proposing a single framework built on their equivalence inlearning scenarios. The foundation of our framework is the derivation of anequivalent baseline correction for all of the existing control variates.Consequently, our framework enables us to characterize the variance-optimalunbiased estimator and provide a closed-form solution for it. This optimalestimator brings significantly improved performance in both evaluation andlearning, and minimizes data requirements. Empirical observations corroborateour theoretical findings.</description><author>Shashank Gupta, Olivier Jeunen, Harrie Oosterhuis, Maarten de Rijke</author><pubDate>Thu, 09 May 2024 13:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05736v1</guid></item></channel></rss>