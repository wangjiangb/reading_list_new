<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 19 Sep 2024 13:00:16 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Gender Representation and Bias in Indian Civil Service Mock Interviews</title><link>http://arxiv.org/abs/2409.12194v1</link><description>This paper makes three key contributions. First, via a substantial corpus of51,278 interview questions sourced from 888 YouTube videos of mock interviewsof Indian civil service candidates, we demonstrate stark gender bias in thebroad nature of questions asked to male and female candidates. Second, ourexperiments with large language models show a strong presence of gender bias inexplanations provided by the LLMs on the gender inference task. Finally, wepresent a novel dataset of 51,278 interview questions that can inform futuresocial science studies.</description><author>Somonnoy Banerjee, Sujan Dutta, Soumyajit Datta, Ashiqur R. KhudaBukhsh</author><pubDate>Wed, 18 Sep 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12194v1</guid></item><item><title>Vista3D: Unravel the 3D Darkside of a Single Image</title><link>http://arxiv.org/abs/2409.12193v1</link><description>We embark on the age-old quest: unveiling the hidden dimensions of objectsfrom mere glimpses of their visible parts. To address this, we present Vista3D,a framework that realizes swift and consistent 3D generation within a mere 5minutes. At the heart of Vista3D lies a two-phase approach: the coarse phaseand the fine phase. In the coarse phase, we rapidly generate initial geometrywith Gaussian Splatting from a single image. In the fine phase, we extract aSigned Distance Function (SDF) directly from learned Gaussian Splatting,optimizing it with a differentiable isosurface representation. Furthermore, itelevates the quality of generation by using a disentangled representation withtwo independent implicit functions to capture both visible and obscured aspectsof objects. Additionally, it harmonizes gradients from 2D diffusion prior with3D-aware diffusion priors by angular diffusion prior composition. Throughextensive evaluation, we demonstrate that Vista3D effectively sustains abalance between the consistency and diversity of the generated 3D objects.Demos and code will be available at https://github.com/florinshen/Vista3D.</description><author>Qiuhong Shen, Xingyi Yang, Michael Bi Mi, Xinchao Wang</author><pubDate>Wed, 18 Sep 2024 17:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12193v1</guid></item><item><title>DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</title><link>http://arxiv.org/abs/2409.12192v1</link><description>Imitation learning has proven to be a powerful tool for training complexvisuomotor policies. However, current methods often require hundreds tothousands of expert demonstrations to handle high-dimensional visualobservations. A key reason for this poor data efficiency is that visualrepresentations are predominantly either pretrained on out-of-domain data ortrained directly through a behavior cloning objective. In this work, we presentDynaMo, a new in-domain, self-supervised method for learning visualrepresentations. Given a set of expert demonstrations, we jointly learn alatent inverse dynamics model and a forward dynamics model over a sequence ofimage embeddings, predicting the next frame in latent space, withoutaugmentations, contrastive sampling, or access to ground truth actions.Importantly, DynaMo does not require any out-of-domain data such as Internetdatasets or cross-embodied datasets. On a suite of six simulated and realenvironments, we show that representations learned with DynaMo significantlyimprove downstream imitation learning performance over prior self-supervisedlearning objectives, and pretrained representations. Gains from using DynaMohold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,and nearest neighbors. Finally, we ablate over key components of DynaMo andmeasure its impact on downstream policy performance. Robot videos are bestviewed at https://dynamo-ssl.github.io</description><author>Zichen Jeff Cui, Hengkai Pan, Aadhithya Iyer, Siddhant Haldar, Lerrel Pinto</author><pubDate>Wed, 18 Sep 2024 17:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12192v1</guid></item><item><title>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</title><link>http://arxiv.org/abs/2409.12191v1</link><description>We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VLmodels that redefines the conventional predetermined-resolution approach invisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,which enables the model to dynamically process images of varying resolutionsinto different numbers of visual tokens. This approach allows the model togenerate more efficient and accurate visual representations, closely aligningwith human perceptual processes. The model also integrates Multimodal RotaryPosition Embedding (M-RoPE), facilitating the effective fusion of positionalinformation across text, images, and videos. We employ a unified paradigm forprocessing both images and videos, enhancing the model's visual perceptioncapabilities. To explore the potential of large multimodal models, Qwen2-VLinvestigates the scaling laws for large vision-language models (LVLMs). Byscaling both the model size-with versions at 2B, 8B, and 72B parameters-and theamount of training data, the Qwen2-VL Series achieves highly competitiveperformance. Notably, the Qwen2-VL-72B model achieves results comparable toleading models such as GPT-4o and Claude3.5-Sonnet across various multimodalbenchmarks, outperforming other generalist models. Code is available at\url{https://github.com/QwenLM/Qwen2-VL}.</description><author>Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin</author><pubDate>Wed, 18 Sep 2024 17:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12191v1</guid></item><item><title>Bundle Adjustment in the Eager Mode</title><link>http://arxiv.org/abs/2409.12190v1</link><description>Bundle adjustment (BA) is a critical technique in various roboticapplications, such as simultaneous localization and mapping (SLAM), augmentedreality (AR), and photogrammetry. BA optimizes parameters such as camera posesand 3D landmarks to align them with observations. With the growing importanceof deep learning in perception systems, there is an increasing need tointegrate BA with deep learning frameworks for enhanced reliability andperformance. However, widely-used C++-based BA frameworks, such as GTSAM,g$^2$o, and Ceres, lack native integration with modern deep learning librarieslike PyTorch. This limitation affects their flexibility, adaptability, ease ofdebugging, and overall implementation efficiency. To address this gap, weintroduce an eager-mode BA framework seamlessly integrated with PyPose,providing PyTorch-compatible interfaces with high efficiency. Our approachincludes GPU-accelerated, differentiable, and sparse operations designed for2nd-order optimization, Lie group and Lie algebra operations, and linearsolvers. Our eager-mode BA on GPU demonstrates substantial runtime efficiency,achieving an average speedup of 18.5$\times$, 22$\times$, and 23$\times$compared to GTSAM, g$^2$o, and Ceres, respectively.</description><author>Zitong Zhan, Huan Xu, Zihang Fang, Xinpeng Wei, Yaoyu Hu, Chen Wang</author><pubDate>Wed, 18 Sep 2024 17:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12190v1</guid></item><item><title>Massively Multi-Person 3D Human Motion Forecasting with Scene Context</title><link>http://arxiv.org/abs/2409.12189v1</link><description>Forecasting long-term 3D human motion is challenging: the stochasticity ofhuman behavior makes it hard to generate realistic human motion from the inputsequence alone. Information on the scene environment and the motion of nearbypeople can greatly aid the generation process. We propose a scene-aware socialtransformer model (SAST) to forecast long-term (10s) human motion motion.Unlike previous models, our approach can model interactions between both widelyvarying numbers of people and objects in a scene. We combine a temporalconvolutional encoder-decoder architecture with a Transformer-based bottleneckthat allows us to efficiently combine motion and scene information. We modelthe conditional motion distribution using denoising diffusion models. Webenchmark our approach on the Humans in Kitchens dataset, which contains 1 to16 persons and 29 to 50 objects that are visible simultaneously. Our modeloutperforms other approaches in terms of realism and diversity on differentmetrics and in a user study. Code is available athttps://github.com/felixbmuller/SAST.</description><author>Felix B Mueller, Julian Tanke, Juergen Gall</author><pubDate>Wed, 18 Sep 2024 17:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12189v1</guid></item><item><title>Qwen2.5-Coder Technical Report</title><link>http://arxiv.org/abs/2409.12186v1</link><description>In this report, we introduce the Qwen2.5-Coder series, a significant upgradefrom its predecessor, CodeQwen1.5. This series includes two models:Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model,Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrainedon a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coderdemonstrates impressive code generation capabilities while retaining generalversatility. The model has been evaluated on a wide range of code-relatedtasks, achieving state-of-the-art (SOTA) performance across more than 10benchmarks, including code generation, completion, reasoning, and repair,consistently outperforming larger models of the same model size. We believethat the release of the Qwen2.5-Coder series will not only push the boundariesof research in code intelligence but also, through its permissive licensing,encourage broader adoption by developers in real-world applications.</description><author>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin</author><pubDate>Wed, 18 Sep 2024 17:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12186v1</guid></item><item><title>LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba</title><link>http://arxiv.org/abs/2408.02615v2</link><description>Recent Transformer-based diffusion models have shown remarkable performance,largely attributed to the ability of the self-attention mechanism to accuratelycapture both global and local contexts by computing all-pair interactions amonginput tokens. However, their quadratic complexity poses significantcomputational challenges for long-sequence inputs. Conversely, a recent statespace model called Mamba offers linear complexity by compressing a filteredglobal context into a hidden state. Despite its efficiency, compressioninevitably leads to information loss of fine-grained local dependencies amongtokens, which are crucial for effective visual generative modeling. Motivatedby these observations, we introduce Local Attentional Mamba (LaMamba) blocksthat combine the strengths of self-attention and Mamba, capturing both globalcontexts and local details with linear complexity. Leveraging the efficientU-Net architecture, our model exhibits exceptional scalability and surpassesthe performance of DiT across various model scales on ImageNet at 256x256resolution, all while utilizing substantially fewer GFLOPs and a comparablenumber of parameters. Compared to state-of-the-art diffusion models on ImageNet256x256 and 512x512, our largest model presents notable advantages, such as areduction of up to 62% GFLOPs compared to DiT-XL/2, while achieving superiorperformance with comparable or fewer parameters.</description><author>Yunxiang Fu, Chaoqi Chen, Yizhou Yu</author><pubDate>Wed, 18 Sep 2024 17:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02615v2</guid></item><item><title>To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning</title><link>http://arxiv.org/abs/2409.12183v1</link><description>Chain-of-thought (CoT) via prompting is the de facto method for elicitingreasoning capabilities from large language models (LLMs). But for what kinds oftasks is this extra ``thinking'' really helpful? To analyze this, we conducteda quantitative meta-analysis covering over 100 papers using CoT and ran our ownevaluations of 20 datasets across 14 models. Our results show that CoT givesstrong performance benefits primarily on tasks involving math or logic, withmuch smaller gains on other types of tasks. On MMLU, directly generating theanswer without CoT leads to almost identical accuracy as CoT unless thequestion or model's response contains an equals sign, indicating symbolicoperations and reasoning. Following this finding, we analyze the behavior ofCoT on these problems by separating planning and execution and comparingagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolicexecution, but it underperforms relative to using a symbolic solver. Ourresults indicate that CoT can be applied selectively, maintaining performancewhile saving inference costs. Furthermore, they suggest a need to move beyondprompt-based CoT to new paradigms that better leverage intermediate computationacross the whole range of LLM applications.</description><author>Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett</author><pubDate>Wed, 18 Sep 2024 17:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12183v1</guid></item><item><title>A Controlled Study on Long Context Extension and Generalization in LLMs</title><link>http://arxiv.org/abs/2409.12181v1</link><description>Broad textual understanding and in-context learning require language modelsthat utilize full document contexts. Due to the implementation challengesassociated with directly training long-context models, many methods have beenproposed for extending models to handle long contexts. However, owing todifferences in data and model classes, it has been challenging to compare theseapproaches, leading to uncertainty as to how to evaluate long-contextperformance and whether it differs from standard evaluation. We implement acontrolled protocol for extension methods with a standardized evaluation,utilizing consistent base models and extension data. Our study yields severalinsights into long-context behavior. First, we reaffirm the critical role ofperplexity as a general-purpose performance indicator even in longer-contexttasks. Second, we find that current approximate attention methodssystematically underperform across long-context tasks. Finally, we confirm thatexact fine-tuning based methods are generally effective within the range oftheir extension, whereas extrapolation remains challenging. All codebases,models, and checkpoints will be made available open-source, promotingtransparency and facilitating further research in this critical area of AIdevelopment.</description><author>Yi Lu, Jing Nathan Yan, Songlin Yang, Justin T. Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, Alexander M. Rush</author><pubDate>Wed, 18 Sep 2024 17:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12181v1</guid></item><item><title>Finetuning Language Models to Emit Linguistic Expressions of Uncertainty</title><link>http://arxiv.org/abs/2409.12180v1</link><description>Large language models (LLMs) are increasingly employed in information-seekingand decision-making tasks. Despite their broad utility, LLMs tend to generateinformation that conflicts with real-world facts, and their persuasive stylecan make these inaccuracies appear confident and convincing. As a result,end-users struggle to consistently align the confidence expressed by LLMs withthe accuracy of their predictions, often leading to either blind trust in alloutputs or a complete disregard for their reliability. In this work, we exploresupervised finetuning on uncertainty-augmented predictions as a method todevelop models that produce linguistic expressions of uncertainty.Specifically, we measure the calibration of pre-trained models and thenfine-tune language models to generate calibrated linguistic expressions ofuncertainty. Through experiments on various question-answering datasets, wedemonstrate that LLMs are well-calibrated in assessing their predictions, andsupervised finetuning based on the model's own confidence leads towell-calibrated expressions of uncertainty, particularly for single-claimanswers.</description><author>Arslan Chaudhry, Sridhar Thiagarajan, Dilan Gorur</author><pubDate>Wed, 18 Sep 2024 17:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12180v1</guid></item><item><title>AnySkin: Plug-and-play Skin Sensing for Robotic Touch</title><link>http://arxiv.org/abs/2409.08276v2</link><description>While tactile sensing is widely accepted as an important and useful sensingmodality, its use pales in comparison to other sensory modalities like visionand proprioception. AnySkin addresses the critical challenges that impede theuse of tactile sensing -- versatility, replaceability, and data reusability.Building on the simplistic design of ReSkin, and decoupling the sensingelectronics from the sensing interface, AnySkin simplifies integration makingit as straightforward as putting on a phone case and connecting a charger.Furthermore, AnySkin is the first uncalibrated tactile-sensor withcross-instance generalizability of learned manipulation policies. To summarize,this work makes three key contributions: first, we introduce a streamlinedfabrication process and a design tool for creating an adhesive-free, durableand easily replaceable magnetic tactile sensor; second, we characterize slipdetection and policy learning with the AnySkin sensor; and third, wedemonstrate zero-shot generalization of models trained on one instance ofAnySkin to new instances, and compare it with popular existing tactilesolutions like DIGIT and ReSkin.https://any-skin.github.io/</description><author>Raunaq Bhirangi, Venkatesh Pattabiraman, Enes Erciyes, Yifeng Cao, Tess Hellebrekers, Lerrel Pinto</author><pubDate>Wed, 18 Sep 2024 17:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08276v2</guid></item><item><title>Computational Dynamical Systems</title><link>http://arxiv.org/abs/2409.12179v1</link><description>We study the computational complexity theory of smooth, finite-dimensionaldynamical systems. Building off of previous work, we give definitions for whatit means for a smooth dynamical system to simulate a Turing machine. We thenshow that 'chaotic' dynamical systems (more precisely, Axiom A systems) and'integrable' dynamical systems (more generally, measure-preserving systems)cannot robustly simulate universal Turing machines, although such machines canbe robustly simulated by other kinds of dynamical systems. Subsequently, weshow that any Turing machine that can be encoded into a structurally stableone-dimensional dynamical system must have a decidable halting problem, andmoreover an explicit time complexity bound in instances where it does halt.More broadly, our work elucidates what it means for one 'machine' to simulateanother, and emphasizes the necessity of defining low-complexity 'encoders' and'decoders' to translate between the dynamics of the simulation and the systembeing simulated. We highlight how the notion of a computational dynamicalsystem leads to questions at the intersection of computational complexitytheory, dynamical systems theory, and real algebraic geometry.</description><author>Jordan Cotler, Semon Rezchikov</author><pubDate>Wed, 18 Sep 2024 17:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12179v1</guid></item><item><title>UKAN: Unbound Kolmogorov-Arnold Network Accompanied with Accelerated Library</title><link>http://arxiv.org/abs/2408.11200v2</link><description>In this work, we present a GPU-accelerated library for the underlyingcomponents of Kolmogorov-Arnold Networks (KANs), along with an algorithm toeliminate bounded grids in KANs. The GPU-accelerated library reduces thecomputational complexity of Basis Spline (B-spline) evaluation by a factor of$\mathcal{O}$(grid size) compared to existing codes, enabling batch computationfor large-scale learning. To overcome the limitations of traditional KANs, weintroduce Unbounded KANs (UKANs), which eliminate the need for a bounded gridand a fixed number of B-spline coefficients. To do so, we replace the KANparameters (B-spline coefficients) with a coefficient generator (CG) model. Theinputs to the CG model are designed based on the idea of an infinite symmetricgrid extending from negative infinity to positive infinity. The positionalencoding of grid group, a sequential collection of B-spline grid indexes, isfed into the CG model, and coefficients are consumed by the efficientimplementation (matrix representations) of B-spline functions to generateoutputs. We perform several experiments on regression, classification, andgenerative tasks, which are promising. In particular, UKAN does not requiredata normalization or a bounded domain for evaluation. Additionally, ourbenchmarking results indicate the superior memory and computational efficiencyof our library compared to existing codes.</description><author>Alireza Moradzadeh, Lukasz Wawrzyniak, Miles Macklin, Saee G. Paliwal</author><pubDate>Wed, 18 Sep 2024 17:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11200v2</guid></item><item><title>Machine Learning Approaches for Diagnostics and Prognostics of Industrial Systems Using Open Source Data from PHM Data Challenges: A Review</title><link>http://arxiv.org/abs/2312.16810v3</link><description>In the field of Prognostics and Health Management (PHM), recent years havewitnessed a significant surge in the application of machine learning (ML).Despite this growth, the field grapples with a lack of unified guidelines andsystematic approaches for effectively implementing these ML techniques andcomprehensive analysis regarding industrial open-source data across variedscenarios. To address these gaps, this paper provides a comprehensive review ofML approaches for diagnostics and prognostics of industrial systems usingopen-source datasets from PHM Data Challenge Competitions held between 2018 and2023 by PHM Society and IEEE Reliability Society and summarizes a unified MLframework. This review systematically categorizes and scrutinizes the problems,challenges, methodologies, and advancements demonstrated in these competitions,highlighting the evolving role of both conventional machine learning and deeplearning in tackling complex industrial tasks related to detection, diagnosis,assessment, and prognosis. Moreover, this paper delves into the commonchallenges in PHM data challenge competitions by emphasizing data-related andmodel-related issues and evaluating the limitations of these competitions. Thepotential solutions to address these challenges are also summarized. Finally,we identify key themes and potential directions for future research, providingopportunities and prospects for next-generation ML-PHM development in PHMdomain.</description><author>Hanqi Su, Jay Lee</author><pubDate>Wed, 18 Sep 2024 17:45:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16810v3</guid></item><item><title>Characterizing Dynamical Stability of Stochastic Gradient Descent in Overparameterized Learning</title><link>http://arxiv.org/abs/2407.20209v2</link><description>For overparameterized optimization tasks, such as the ones found in modernmachine learning, global minima are generally not unique. In order tounderstand generalization in these settings, it is vital to study to whichminimum an optimization algorithm converges. The possibility of having minimathat are unstable under the dynamics imposed by the optimization algorithmlimits the potential minima that the algorithm can find. In this paper, wecharacterize the global minima that are dynamically stable/unstable for bothdeterministic and stochastic gradient descent (SGD). In particular, weintroduce a characteristic Lyapunov exponent which depends on the localdynamics around a global minimum and rigorously prove that the sign of thisLyapunov exponent determines whether SGD can accumulate at the respectiveglobal minimum.</description><author>Dennis Chemnitz, Maximilian Engel</author><pubDate>Wed, 18 Sep 2024 17:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20209v2</guid></item><item><title>NovAScore: A New Automated Metric for Evaluating Document Level Novelty</title><link>http://arxiv.org/abs/2409.09249v2</link><description>The rapid expansion of online content has intensified the issue ofinformation redundancy, underscoring the need for solutions that can identifygenuinely new information. Despite this challenge, the research community hasseen a decline in focus on novelty detection, particularly with the rise oflarge language models (LLMs). Additionally, previous approaches have reliedheavily on human annotation, which is time-consuming, costly, and particularlychallenging when annotators must compare a target document against a vastnumber of historical documents. In this work, we introduce NovAScore (NoveltyEvaluation in Atomicity Score), an automated metric for evaluatingdocument-level novelty. NovAScore aggregates the novelty and salience scores ofatomic information, providing high interpretability and a detailed analysis ofa document's novelty. With its dynamic weight adjustment scheme, NovAScoreoffers enhanced flexibility and an additional dimension to assess both thenovelty level and the importance of information within a document. Ourexperiments show that NovAScore strongly correlates with human judgments ofnovelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0dataset and a 0.920 Pearson correlation on an internal human-annotated dataset.</description><author>Lin Ai, Ziwei Gong, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Ahmad Emami, Julia Hirschberg</author><pubDate>Wed, 18 Sep 2024 17:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09249v2</guid></item><item><title>You Only Read Once (YORO): Learning to Internalize Database Knowledge for Text-to-SQL</title><link>http://arxiv.org/abs/2409.12172v1</link><description>While significant progress has been made on the text-to-SQL task, recentsolutions repeatedly encode the same database schema for every question,resulting in unnecessary high inference cost and often overlooking crucialdatabase knowledge. To address these issues, we propose You Only Read Once(YORO), a novel paradigm that directly internalizes database knowledge into theparametric knowledge of a text-to-SQL model during training and eliminates theneed for schema encoding during inference. YORO significantly reduces the inputtoken length by 66%-98%. Despite its shorter inputs, our empirical resultsdemonstrate YORO's competitive performances with traditional systems on threebenchmarks as well as its significant outperformance on large databases.Furthermore, YORO excels in handling questions with challenging valueretrievals such as abbreviation.</description><author>Hideo Kobayashi, Wuwei Lan, Peng Shi, Shuaichen Chang, Jiang Guo, Henghui Zhu, Zhiguo Wang, Patrick Ng</author><pubDate>Wed, 18 Sep 2024 17:38:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12172v1</guid></item><item><title>multiPI-TransBTS: A Multi-Path Learning Framework for Brain Tumor Image Segmentation Based on Multi-Physical Information</title><link>http://arxiv.org/abs/2409.12167v1</link><description>Brain Tumor Segmentation (BraTS) plays a critical role in clinical diagnosis,treatment planning, and monitoring the progression of brain tumors. However,due to the variability in tumor appearance, size, and intensity acrossdifferent MRI modalities, automated segmentation remains a challenging task. Inthis study, we propose a novel Transformer-based framework, multiPI-TransBTS,which integrates multi-physical information to enhance segmentation accuracy.The model leverages spatial information, semantic information, and multi-modalimaging data, addressing the inherent heterogeneity in brain tumorcharacteristics. The multiPI-TransBTS framework consists of an encoder, anAdaptive Feature Fusion (AFF) module, and a multi-source, multi-scale featuredecoder. The encoder incorporates a multi-branch architecture to separatelyextract modality-specific features from different MRI sequences. The AFF modulefuses information from multiple sources using channel-wise and element-wiseattention, ensuring effective feature recalibration. The decoder combines bothcommon and task-specific features through a Task-Specific Feature Introduction(TSFI) strategy, producing accurate segmentation outputs for Whole Tumor (WT),Tumor Core (TC), and Enhancing Tumor (ET) regions. Comprehensive evaluations onthe BraTS2019 and BraTS2020 datasets demonstrate the superiority ofmultiPI-TransBTS over the state-of-the-art methods. The model consistentlyachieves better Dice coefficients, Hausdorff distances, and Sensitivity scores,highlighting its effectiveness in addressing the BraTS challenges. Our resultsalso indicate the need for further exploration of the balance between precisionand recall in the ET segmentation task. The proposed framework represents asignificant advancement in BraTS, with potential implications for improvingclinical outcomes for brain tumor patients.</description><author>Hongjun Zhu, Jiaohang Huang, Kuo Chen, Xuehui Ying, Ying Qian</author><pubDate>Wed, 18 Sep 2024 17:35:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12167v1</guid></item><item><title>ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework</title><link>http://arxiv.org/abs/2409.10289v2</link><description>Empathetic response generation necessitates the integration of emotional andintentional dynamics to foster meaningful interactions. Existing researcheither neglects the intricate interplay between emotion and intent, leading tosuboptimal controllability of empathy, or resorts to large language models(LLMs), which incur significant computational overhead. In this paper, weintroduce ReflectDiffu, a lightweight and comprehensive framework forempathetic response generation. This framework incorporates emotion contagionto augment emotional expressiveness and employs an emotion-reasoning mask topinpoint critical emotional elements. Additionally, it integrates intentmimicry within reinforcement learning for refinement during diffusion. Byharnessing an intent twice reflect the mechanism ofExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotionaldecision-making into precise intent actions, thereby addressing empatheticresponse misalignments stemming from emotional misrecognition. Throughreflection, the framework maps emotional states to intents, markedly enhancingboth response empathy and flexibility. Comprehensive experiments reveal thatReflectDiffu outperforms existing models regarding relevance, controllability,and informativeness, achieving state-of-the-art results in both automatic andhuman evaluations.</description><author>Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem</author><pubDate>Wed, 18 Sep 2024 17:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10289v2</guid></item><item><title>TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for Dynamic UAV-based Scenes</title><link>http://arxiv.org/abs/2405.02762v2</link><description>In this paper, we present a new approach to bridge the domain gap betweensynthetic and real-world data for unmanned aerial vehicle (UAV)-basedperception. Our formulation is designed for dynamic scenes, consisting of smallmoving objects or human actions. We propose an extension of K-Planes NeuralRadiance Field (NeRF), wherein our algorithm stores a set of tiered featurevectors. The tiered feature vectors are generated to effectively modelconceptual information about a scene as well as an image decoder thattransforms output feature maps into RGB images. Our technique leverages theinformation amongst both static and dynamic objects within a scene and is ableto capture salient scene attributes of high altitude videos. We evaluate itsperformance on challenging datasets, including Okutama Action and UG2, andobserve considerable improvement in accuracy over state of the art neuralrendering methods.</description><author>Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon</author><pubDate>Wed, 18 Sep 2024 17:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02762v2</guid></item><item><title>Precise Forecasting of Sky Images Using Spatial Warping</title><link>http://arxiv.org/abs/2409.12162v1</link><description>The intermittency of solar power, due to occlusion from cloud cover, is oneof the key factors inhibiting its widespread use in both commercial andresidential settings. Hence, real-time forecasting of solar irradiance forgrid-connected photovoltaic systems is necessary to schedule and allocateresources across the grid. Ground-based imagers that capture wide field-of-viewimages of the sky are commonly used to monitor cloud movement around aparticular site in an effort to forecast solar irradiance. However, these wideFOV imagers capture a distorted image of sky image, where regions near thehorizon are heavily compressed. This hinders the ability to precisely predictcloud motion near the horizon which especially affects prediction over longertime horizons. In this work, we combat the aforementioned constraint byintroducing a deep learning method to predict a future sky image frame withhigher resolution than previous methods. Our main contribution is to derive anoptimal warping method to counter the adverse affects of clouds at the horizon,and learn a framework for future sky image prediction which better determinescloud evolution for longer time horizons.</description><author>Leron Julian, Aswin C. Sankaranarayanan</author><pubDate>Wed, 18 Sep 2024 17:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12162v1</guid></item><item><title>JEAN: Joint Expression and Audio-guided NeRF-based Talking Face Generation</title><link>http://arxiv.org/abs/2409.12156v1</link><description>We introduce a novel method for joint expression and audio-guided talkingface generation. Recent approaches either struggle to preserve the speakeridentity or fail to produce faithful facial expressions. To address thesechallenges, we propose a NeRF-based network. Since we train our network onmonocular videos without any ground truth, it is essential to learndisentangled representations for audio and expression. We first learn audiofeatures in a self-supervised manner, given utterances from multiple subjects.By incorporating a contrastive learning technique, we ensure that the learnedaudio features are aligned to the lip motion and disentangled from the musclemotion of the rest of the face. We then devise a transformer-based architecturethat learns expression features, capturing long-range facial expressions anddisentangling them from the speech-specific mouth movements. Throughquantitative and qualitative evaluation, we demonstrate that our method cansynthesize high-fidelity talking face videos, achieving state-of-the-art facialexpression transfer along with lip synchronization to unseen audio.</description><author>Sai Tanmay Reddy Chakkera, Aggelina Chatziagapi, Dimitris Samaras</author><pubDate>Wed, 18 Sep 2024 17:18:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12156v1</guid></item><item><title>Autopet III challenge: Incorporating anatomical knowledge into nnUNet for lesion segmentation in PET/CT</title><link>http://arxiv.org/abs/2409.12155v1</link><description>Lesion segmentation in PET/CT imaging is essential for precise tumorcharacterization, which supports personalized treatment planning and enhancesdiagnostic precision in oncology. However, accurate manual segmentation oflesions is time-consuming and prone to inter-observer variability. Given therising demand and clinical use of PET/CT, automated segmentation methods,particularly deep-learning-based approaches, have become increasingly morerelevant. The autoPET III Challenge focuses on advancing automated segmentationof tumor lesions in PET/CT images in a multitracer multicenter setting,addressing the clinical need for quantitative, robust, and generalizablesolutions. Building on previous challenges, the third iteration of the autoPETchallenge introduces a more diverse dataset featuring two different tracers(FDG and PSMA) from two clinical centers. To this extent, we developed aclassifier that identifies the tracer of the given PET/CT based on the MaximumIntensity Projection of the PET scan. We trained two individualnnUNet-ensembles for each tracer where anatomical labels are included as amulti-label task to enhance the model's performance. Our final submissionachieves cross-validation Dice scores of 76.90% and 61.33% for the publiclyavailable FDG and PSMA datasets, respectively. The code is available athttps://github.com/hakal104/autoPETIII/ .</description><author>Hamza Kalisch, Fabian HÃ¶rst, Ken Herrmann, Jens Kleesiek, Constantin Seibold</author><pubDate>Wed, 18 Sep 2024 17:16:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12155v1</guid></item><item><title>Abductive explanations of classifiers under constraints: Complexity and properties</title><link>http://arxiv.org/abs/2409.12154v1</link><description>Abductive explanations (AXp's) are widely used for understanding decisions ofclassifiers. Existing definitions are suitable when features are independent.However, we show that ignoring constraints when they exist between features maylead to an explosion in the number of redundant or superfluous AXp's. Wepropose three new types of explanations that take into account constraints andthat can be generated from the whole feature space or from a sample (such as adataset). They are based on a key notion of coverage of an explanation, the setof instances it explains. We show that coverage is powerful enough to discardredundant and superfluous AXp's. For each type, we analyse the complexity offinding an explanation and investigate its formal properties. The final resultis a catalogue of different forms of AXp's with different complexities anddifferent formal guarantees.</description><author>Martin Cooper, Leila Amgoud</author><pubDate>Wed, 18 Sep 2024 17:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12154v1</guid></item><item><title>Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference</title><link>http://arxiv.org/abs/2409.12150v1</link><description>Personalized outfit recommendation remains a complex challenge, demandingboth fashion compatibility understanding and trend awareness. This paperpresents a novel framework that harnesses the expressive power of largelanguage models (LLMs) for this task, mitigating their "black box" and staticnature through fine-tuning and direct feedback integration. We bridge the itemvisual-textual gap in items descriptions by employing image captioning with aMultimodal Large Language Model (MLLM). This enables the LLM to extract styleand color characteristics from human-curated fashion images, forming the basisfor personalized recommendations. The LLM is efficiently fine-tuned on theopen-source Polyvore dataset of curated fashion images, optimizing its abilityto recommend stylish outfits. A direct preference mechanism using negativeexamples is employed to enhance the LLM's decision-making process. This createsa self-enhancing AI feedback loop that continuously refines recommendations inline with seasonal fashion trends. Our framework is evaluated on the Polyvoredataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,and complementary item retrieval. These evaluations underline the framework'sability to generate stylish, trend-aligned outfit suggestions, continuouslyimproving through direct feedback. The evaluation results demonstrated that ourproposed framework significantly outperforms the base LLM, creating morecohesive outfits. The improved performance in these tasks underscores theproposed framework's potential to enhance the shopping experience with accuratesuggestions, proving its effectiveness over the vanilla LLM based outfitgeneration.</description><author>Najmeh Forouzandehmehr, Nima Farrokhsiar, Ramin Giahi, Evren Korpeoglu, Kannan Achan</author><pubDate>Wed, 18 Sep 2024 17:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12150v1</guid></item><item><title>MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning</title><link>http://arxiv.org/abs/2409.12147v1</link><description>Large Language Models' (LLM) reasoning can be improved using test-timeaggregation strategies, i.e., generating multiple samples and voting amonggenerated samples. While these improve performance, they often reach asaturation point. Refinement offers an alternative by using LLM-generatedfeedback to improve solution quality. However, refinement introduces 3 keychallenges: (1) Excessive refinement: Uniformly refining all instances canover-correct and reduce the overall performance. (2) Inability to localize andaddress errors: LLMs have a limited ability to self-correct and struggle toidentify and correct their own mistakes. (3) Insufficient refinement: Decidinghow many iterations of refinement are needed is non-trivial, and stopping toosoon could leave errors unaddressed. To tackle these issues, we proposeMAgICoRe, which avoids excessive refinement by categorizing problem difficultyas easy or hard, solving easy problems with coarse-grained aggregation and hardones with fine-grained and iterative multi-agent refinement. To improve errorlocalization, we incorporate external step-wise reward model (RM) scores.Moreover, to ensure effective refinement, we employ a multi-agent loop withthree agents: Solver, Reviewer (which generates targeted feedback based onstep-wise RM scores), and the Refiner (which incorporates feedback). To ensuresufficient refinement, we re-evaluate updated solutions, iteratively initiatingfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5and show its effectiveness across 5 math datasets. Even one iteration ofMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by4.0% while using less than half the samples. Unlike iterative refinement withbaselines, MAgICoRe continues to improve with more iterations. Finally, ourablations highlight the importance of MAgICoRe's RMs and multi-agentcommunication.</description><author>Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Wed, 18 Sep 2024 17:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12147v1</guid></item><item><title>Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications</title><link>http://arxiv.org/abs/2401.17434v3</link><description>Hackathons have emerged as pivotal platforms in the software industry,driving both innovation and skill development for organizations and studentsalike. These events enable companies to quickly prototype new ideas whileoffering students practical, hands-on learning experiences. Over time,hackathons have transitioned from purely competitive events to valuableeducational tools, integrating theory with real-world problem-solving throughcollaboration between academia and industry. The infusion of artificialintelligence (AI) and machine learning is now reshaping hackathons, providingenhanced learning opportunities while also introducing ethical challenges. Thisstudy explores the influence of generative AI on students' technologicalchoices, focusing on a case study from the 2023 University of Iowa Hackathon.The findings offer insights into AI's role in these events, its educationalimpact, and propose strategies for integrating such technologies in futurehackathons, ensuring a balance between innovation, ethics, and educationalvalue.</description><author>Ramteja Sajja, Carlos Erazo Ramirez, Zhouyayan Li, Bekir Z. Demiray, Yusuf Sermet, Ibrahim Demir</author><pubDate>Wed, 18 Sep 2024 17:07:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17434v3</guid></item><item><title>Integrating AI and Learning Analytics for Data-Driven Pedagogical Decisions and Personalized Interventions in Education</title><link>http://arxiv.org/abs/2312.09548v2</link><description>This research study explores the conceptualization, development, anddeployment of an innovative learning analytics tool, leveraging OpenAI's GPT-4model to quantify student engagement, map learning progression, and evaluatediverse instructional strategies within an educational context. By analyzingcritical data points such as students' stress levels, curiosity, confusion,agitation, topic preferences, and study methods, the tool provides acomprehensive view of the learning environment. It also employs Bloom'staxonomy to assess cognitive development based on student inquiries. Inaddition to technical evaluation through synthetic data, feedback from a surveyof teaching faculty at the University of Iowa was collected to gauge perceivedbenefits and challenges. Faculty recognized the tool's potential to enhanceinstructional decision-making through real-time insights but expressed concernsabout data security and the accuracy of AI-generated insights. The studyoutlines the design, implementation, and evaluation of the tool, highlightingits contributions to educational outcomes, practical integration withinlearning management systems, and future refinements needed to address privacyand accuracy concerns. This research underscores AI's role in shapingpersonalized, data-driven education.</description><author>Ramteja Sajja, Yusuf Sermet, David Cwiertny, Ibrahim Demir</author><pubDate>Wed, 18 Sep 2024 17:05:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09548v2</guid></item><item><title>MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion</title><link>http://arxiv.org/abs/2409.12140v1</link><description>We introduce MoRAG, a novel multi-part fusion based retrieval-augmentedgeneration strategy for text-based human motion generation. The method enhancesmotion diffusion models by leveraging additional knowledge obtained through animproved motion retrieval process. By effectively prompting large languagemodels (LLMs), we address spelling errors and rephrasing issues in motionretrieval. Our approach utilizes a multi-part retrieval strategy to improve thegeneralizability of motion retrieval across the language space. We creatediverse samples through the spatial composition of the retrieved motions.Furthermore, by utilizing low-level, part-specific motion information, we canconstruct motion samples for unseen text descriptions. Our experimentsdemonstrate that our framework can serve as a plug-and-play module, improvingthe performance of motion diffusion models. Code, pretrained models and samplevideos will be made available at: https://motion-rag.github.io/</description><author>Kalakonda Sai Shashank, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla</author><pubDate>Wed, 18 Sep 2024 17:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12140v1</guid></item><item><title>Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models</title><link>http://arxiv.org/abs/2409.12139v1</link><description>With the advent of the big data and large language model era, zero-shotpersonalized rapid customization has emerged as a significant trend. In thisreport, we introduce Takin AudioLLM, a series of techniques and models, mainlyincluding Takin TTS, Takin VC, and Takin Morphing, specifically designed foraudiobook production. These models are capable of zero-shot speech production,generating high-quality speech that is nearly indistinguishable from real humanspeech and facilitating individuals to customize the speech content accordingto their own needs. Specifically, we first introduce Takin TTS, a neural codeclanguage model that builds upon an enhanced neural speech codec and amulti-task training framework, capable of generating high-fidelity naturalspeech in a zero-shot way. For Takin VC, we advocate an effective content andtimbre joint modeling approach to improve the speaker similarity, whileadvocating for a conditional flow matching based decoder to further enhance itsnaturalness and expressiveness. Last, we propose the Takin Morphing system withhighly decoupled and advanced timbre and prosody modeling approaches, whichenables individuals to customize speech production with their preferred timbreand prosody in a precise and controllable manner. Extensive experimentsvalidate the effectiveness and robustness of our Takin AudioLLM series models.For detailed demos, please refer to https://takinaudiollm.github.io.</description><author>EverestAI, :, Sijin Chen, Yuan Feng, Laipeng He, Tianwei He, Wendi He, Yanni Hu, Bin Lin, Yiting Lin, Pengfei Tan, Chengwei Tian, Chen Wang, Zhicheng Wang, Ruoye Xie, Jingjing Yin, Jianhao Ye, Jixun Yao, Quanlei Yan, Yuguang Yang</author><pubDate>Wed, 18 Sep 2024 17:03:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12139v1</guid></item><item><title>Estimating the number of reachable positions in Minishogi</title><link>http://arxiv.org/abs/2409.00129v2</link><description>To investigate the feasibility of strongly solving Minishogi (Gogo Shogi), itis necessary to know the number of its reachable positions from the initialposition. However, there currently remains a significant gap between the lowerand upper bounds of the value, since checking the legality of a Minishogiposition is difficult. In this paper, the authors estimate the number ofreachable positions by generating candidate positions using uniform randomsampling and measuring the proportion of those reachable by a series of legalmoves from the initial position. The experimental results reveal that thenumber of reachable Minishogi positions is approximately $2.38\times 10^{18}$.</description><author>Sotaro Ishii, Tetsuro Tanaka</author><pubDate>Wed, 18 Sep 2024 17:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00129v2</guid></item><item><title>GRIN: GRadient-INformed MoE</title><link>http://arxiv.org/abs/2409.12136v1</link><description>Mixture-of-Experts (MoE) models scale more effectively than dense models dueto sparse computation through expert routing, selectively activating only asmall subset of expert modules. However, sparse computation challengestraditional training practices, as discrete expert routing hinders standardbackpropagation and thus gradient-based optimization, which are the cornerstoneof deep learning. To better pursue the scaling power of MoE, we introduce GRIN(GRadient-INformed MoE training), which incorporates sparse gradient estimationfor expert routing and configures model parallelism to avoid token dropping.Applying GRIN to autoregressive language modeling, we develop a top-216$\times$3.8B MoE model. Our model, with only 6.6B activated parameters,outperforms a 7B dense model and matches the performance of a 14B dense modeltrained on the same data. Extensive evaluations across diverse tasksdemonstrate the potential of GRIN to significantly enhance MoE efficacy,achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.</description><author>Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, Weizhu Chen</author><pubDate>Wed, 18 Sep 2024 17:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12136v1</guid></item><item><title>Almost Sure Convergence of Linear Temporal Difference Learning with Arbitrary Features</title><link>http://arxiv.org/abs/2409.12135v1</link><description>Temporal difference (TD) learning with linear function approximation,abbreviated as linear TD, is a classic and powerful prediction algorithm inreinforcement learning. While it is well understood that linear TD convergesalmost surely to a unique point, this convergence traditionally requires theassumption that the features used by the approximator are linearly independent.However, this linear independence assumption does not hold in many practicalscenarios. This work is the first to establish the almost sure convergence oflinear TD without requiring linearly independent features. In fact, we do notmake any assumptions on the features. We prove that the approximated valuefunction converges to a unique point and the weight iterates converge to a set.We also establish a notion of local stability of the weight iterates.Importantly, we do not need to introduce any other additional assumptions anddo not need to make any modification to the linear TD algorithm. Key to ouranalysis is a novel characterization of bounded invariant sets of the mean ODEof linear TD.</description><author>Jiuqi Wang, Shangtong Zhang</author><pubDate>Wed, 18 Sep 2024 16:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12135v1</guid></item><item><title>BERT-VBD: Vietnamese Multi-Document Summarization Framework</title><link>http://arxiv.org/abs/2409.12134v1</link><description>In tackling the challenge of Multi-Document Summarization (MDS), numerousmethods have been proposed, spanning both extractive and abstractivesummarization techniques. However, each approach has its own limitations,making it less effective to rely solely on either one. An emerging andpromising strategy involves a synergistic fusion of extractive and abstractivesummarization methods. Despite the plethora of studies in this domain, researchon the combined methodology remains scarce, particularly in the context ofVietnamese language processing. This paper presents a novel Vietnamese MDSframework leveraging a two-component pipeline architecture that integratesextractive and abstractive techniques. The first component employs anextractive approach to identify key sentences within each document. This isachieved by a modification of the pre-trained BERT network, which derivessemantically meaningful phrase embeddings using siamese and triplet networkstructures. The second component utilizes the VBD-LLaMA2-7B-50b model forabstractive summarization, ultimately generating the final summary document.Our proposed framework demonstrates a positive performance, attaining ROUGE-2scores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-artbaselines.</description><author>Tuan-Cuong Vuong, Trang Mai Xuan, Thien Van Luong</author><pubDate>Wed, 18 Sep 2024 16:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12134v1</guid></item><item><title>Linguini: A benchmark for language-agnostic linguistic reasoning</title><link>http://arxiv.org/abs/2409.12126v1</link><description>We propose a new benchmark to measure a language model's linguistic reasoningskills without relying on pre-existing language-specific knowledge. The testcovers 894 questions grouped in 160 problems across 75 (mostly) extremelylow-resource languages, extracted from the International Linguistic Olympiadcorpus. To attain high accuracy on this benchmark, models don't need previousknowledge of the tested language, as all the information needed to solve thelinguistic puzzle is presented in the context. We find that, while all analyzedmodels rank below 25% accuracy, there is a significant gap between open andclosed models, with the best-performing proprietary model at 24.05% and thebest-performing open model at 8.84%.</description><author>Eduardo SÃ¡nchez, Belen Alastruey, Christophe Ropers, Pontus Stenetorp, Mikel Artetxe, Marta R. Costa-jussÃ </author><pubDate>Wed, 18 Sep 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12126v1</guid></item><item><title>Probabilistically Robust Watermarking of Neural Networks</title><link>http://arxiv.org/abs/2401.08261v2</link><description>As deep learning (DL) models are widely and effectively used in MachineLearning as a Service (MLaaS) platforms, there is a rapidly growing interest inDL watermarking techniques that can be used to confirm the ownership of aparticular model. Unfortunately, these methods usually produce watermarkssusceptible to model stealing attacks. In our research, we introduce a noveltrigger set-based watermarking approach that demonstrates resilience againstfunctionality stealing attacks, particularly those involving extraction anddistillation. Our approach does not require additional model training and canbe applied to any model architecture. The key idea of our method is to computethe trigger set, which is transferable between the source model and the set ofproxy models with a high probability. In our experimental study, we show thatif the probability of the set being transferable is reasonably high, it can beeffectively used for ownership verification of the stolen model. We evaluateour method on multiple benchmarks and show that our approach outperformscurrent state-of-the-art watermarking techniques in all considered experimentalsetups.</description><author>Mikhail Pautov, Nikita Bogdanov, Stanislav Pyatkin, Oleg Rogov, Ivan Oseledets</author><pubDate>Wed, 18 Sep 2024 16:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08261v2</guid></item><item><title>Optimal Visual Search with Highly Heuristic Decision Rules</title><link>http://arxiv.org/abs/2409.12124v1</link><description>Visual search is a fundamental natural task for humans and other animals. Weinvestigated the decision processes humans use when searching briefly presenteddisplays having well-separated potential target-object locations. Performancewas compared with the Bayesian-optimal decision process under the assumptionthat the information from the different potential target locations isstatistically independent. Surprisingly, humans performed slightly better thanoptimal, despite humans' substantial loss of sensitivity in the fovea, and theimplausibility of the human brain replicating the optimal computations. We showthat three factors can quantitatively explain these seemingly paradoxicalresults. Most importantly, simple and fixed heuristic decision rules reach nearoptimal search performance. Secondly, foveal neglect primarily affects only thecentral potential target location. Finally, spatially correlated neural noisecauses search performance to exceed that predicted for independent noise. Thesefindings have far-reaching implications for understanding visual search tasksand other identification tasks in humans and other animals.</description><author>Anqi Zhang, Wilson S. Geisler</author><pubDate>Wed, 18 Sep 2024 16:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12124v1</guid></item><item><title>Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement</title><link>http://arxiv.org/abs/2409.12122v1</link><description>In this report, we present a series of math-specific large language models:Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of theQwen2.5 series lies in integrating the philosophy of self-improvementthroughout the entire pipeline, from pre-training and post-training toinference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilizedto generate large-scale, high-quality mathematical data. (2) In thepost-training phase, we develop a reward model (RM) by conducting massivesampling from Qwen2-Math-Instruct. This RM is then applied to the iterativeevolution of data in supervised fine-tuning (SFT). With a stronger SFT model,it's possible to iteratively train and update the RM, which in turn guides thenext round of SFT data iteration. On the final SFT model, we employ theultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.(3) Furthermore, during the inference stage, the RM is used to guide sampling,optimizing the model's performance. Qwen2.5-Math-Instruct supports both Chinese and English, and possess advancedmathematical reasoning capabilities, including Chain-of-Thought (CoT) andTool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematicsdatasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, andAIME24, covering a range of difficulties from grade school level to mathcompetition problems.</description><author>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang</author><pubDate>Wed, 18 Sep 2024 16:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12122v1</guid></item><item><title>Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference</title><link>http://arxiv.org/abs/2409.12117v1</link><description>Large language models (LLMs) have significantly advanced audio processingthrough audio codecs that convert audio into discrete tokens, enabling theapplication of language modeling techniques to audio data. However, audiocodecs often operate at high frame rates, resulting in slow training andinference, especially for autoregressive models. To address this challenge, wepresent the Low Frame-rate Speech Codec (LFSC): a neural audio codec thatleverages finite scalar quantization and adversarial training with large speechlanguage models to achieve high-quality audio compression with a 1.89 kbpsbitrate and 21.5 frames per second. We demonstrate that our novel codec canmake the inference of LLM-based text-to-speech models around three times fasterwhile improving intelligibility and producing quality comparable to previousmodels.</description><author>Edresson Casanova, Ryan Langman, Paarth Neekhara, Shehzeen Hussain, Jason Li, Subhankar Ghosh, Ante JukiÄ, Sang-gil Lee</author><pubDate>Wed, 18 Sep 2024 16:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12117v1</guid></item><item><title>Stronger Baseline Models -- A Key Requirement for Aligning Machine Learning Research with Clinical Utility</title><link>http://arxiv.org/abs/2409.12116v1</link><description>Machine Learning (ML) research has increased substantially in recent years,due to the success of predictive modeling across diverse application domains.However, well-known barriers exist when attempting to deploy ML models inhigh-stakes, clinical settings, including lack of model transparency (or theinability to audit the inference process), large training data requirementswith siloed data sources, and complicated metrics for measuring model utility.In this work, we show empirically that including stronger baseline models inhealthcare ML evaluations has important downstream effects that aidpractitioners in addressing these challenges. Through a series of case studies,we find that the common practice of omitting baselines or comparing against aweak baseline model (e.g. a linear model with no optimization) obscures thevalue of ML methods proposed in the research literature. Using these insights,we propose some best practices that will enable practitioners to moreeffectively study and deploy ML models in clinical settings.</description><author>Nathan Wolfrath, Joel Wolfrath, Hengrui Hu, Anjishnu Banerjee, Anai N. Kothari</author><pubDate>Wed, 18 Sep 2024 16:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12116v1</guid></item><item><title>Pareto Data Framework: Steps Towards Resource-Efficient Decision Making Using Minimum Viable Data (MVD)</title><link>http://arxiv.org/abs/2409.12112v1</link><description>This paper introduces the Pareto Data Framework, an approach for identifyingand selecting the Minimum Viable Data (MVD) required for enabling machinelearning applications on constrained platforms such as embedded systems, mobiledevices, and Internet of Things (IoT) devices. We demonstrate that strategicdata reduction can maintain high performance while significantly reducingbandwidth, energy, computation, and storage costs. The framework identifiesMinimum Viable Data (MVD) to optimize efficiency across resource-constrainedenvironments without sacrificing performance. It addresses common inefficientpractices in an IoT application such as overprovisioning of sensors andoverprecision, and oversampling of signals, proposing scalable solutions foroptimal sensor selection, signal extraction and transmission, and datarepresentation. An experimental methodology demonstrates effective acousticdata characterization after downsampling, quantization, and truncation tosimulate reduced-fidelity sensors and network and storage constraints; resultsshows that performance can be maintained up to 95\% with sample rates reducedby 75\% and bit depths and clip length reduced by 50\% which translates intosubstantial cost and resource reduction. These findings have implications onthe design and development of constrained systems. The paper also discussesbroader implications of the framework, including the potential to democratizeadvanced AI technologies across IoT applications and sectors such asagriculture, transportation, and manufacturing to improve access and multiplythe benefits of data-driven insights.</description><author>Tashfain Ahmed, Josh Siegel</author><pubDate>Wed, 18 Sep 2024 16:31:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12112v1</guid></item><item><title>Applications of Knowledge Distillation in Remote Sensing: A Survey</title><link>http://arxiv.org/abs/2409.12111v1</link><description>With the ever-growing complexity of models in the field of remote sensing(RS), there is an increasing demand for solutions that balance model accuracywith computational efficiency. Knowledge distillation (KD) has emerged as apowerful tool to meet this need, enabling the transfer of knowledge from large,complex models to smaller, more efficient ones without significant loss inperformance. This review article provides an extensive examination of KD andits innovative applications in RS. KD, a technique developed to transferknowledge from a complex, often cumbersome model (teacher) to a more compactand efficient model (student), has seen significant evolution and applicationacross various domains. Initially, we introduce the fundamental concepts andhistorical progression of KD methods. The advantages of employing KD arehighlighted, particularly in terms of model compression, enhanced computationalefficiency, and improved performance, which are pivotal for practicaldeployments in RS scenarios. The article provides a comprehensive taxonomy ofKD techniques, where each category is critically analyzed to demonstrate thebreadth and depth of the alternative options, and illustrates specific casestudies that showcase the practical implementation of KD methods in RS tasks,such as instance segmentation and object detection. Further, the reviewdiscusses the challenges and limitations of KD in RS, including practicalconstraints and prospective future directions, providing a comprehensiveoverview for researchers and practitioners in the field of RS. Through thisorganization, the paper not only elucidates the current state of research in KDbut also sets the stage for future research opportunities, thereby contributingsignificantly to both academic research and real-world applications.</description><author>Yassine Himeur, Nour Aburaed, Omar Elharrouss, Iraklis Varlamis, Shadi Atalla, Wathiq Mansoor, Hussain Al Ahmad</author><pubDate>Wed, 18 Sep 2024 16:30:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12111v1</guid></item><item><title>Model-free quantification of completeness, uncertainties, and outliers in atomistic machine learning using information theory</title><link>http://arxiv.org/abs/2404.12367v2</link><description>An accurate description of information is relevant for a range of problems inatomistic machine learning (ML), such as crafting training sets, performinguncertainty quantification (UQ), or extracting physical insights from largedatasets. However, atomistic ML often relies on unsupervised learning or modelpredictions to analyze information contents from simulation or training data.Here, we introduce a theoretical framework that provides a rigorous, model-freetool to quantify information contents in atomistic simulations. We demonstratethat the information entropy of a distribution of atom-centered environmentsexplains known heuristics in ML potential developments, from training set sizesto dataset optimality. Using this tool, we propose a model-free UQ method thatreliably predicts epistemic uncertainty and detects out-of-distributionsamples, including rare events in systems such as nucleation. This methodprovides a general tool for data-driven atomistic modeling and combines effortsin ML, simulations, and physical explainability.</description><author>Daniel Schwalbe-Koda, Sebastien Hamel, Babak Sadigh, Fei Zhou, Vincenzo Lordi</author><pubDate>Wed, 18 Sep 2024 16:30:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12367v2</guid></item><item><title>SPRMamba: Surgical Phase Recognition for Endoscopic Submucosal Dissection with Mamba</title><link>http://arxiv.org/abs/2409.12108v1</link><description>Endoscopic Submucosal Dissection (ESD) is a minimally invasive procedureinitially designed for the treatment of early gastric cancer but is now widelyused for various gastrointestinal lesions. Computer-assisted Surgery systemshave played a crucial role in improving the precision and safety of ESDprocedures, however, their effectiveness is limited by the accurate recognitionof surgical phases. The intricate nature of ESD, with different lesioncharacteristics and tissue structures, presents challenges for real-timesurgical phase recognition algorithms. Existing surgical phase recognitionalgorithms struggle to efficiently capture temporal contexts in video-basedscenarios, leading to insufficient performance. To address these issues, wepropose SPRMamba, a novel Mamba-based framework for ESD surgical phaserecognition. SPRMamba leverages the strengths of Mamba for long-term temporalmodeling while introducing the Scaled Residual TranMamba block to enhance thecapture of fine-grained details, overcoming the limitations of traditionaltemporal models like Temporal Convolutional Networks and Transformers.Moreover, a Temporal Sample Strategy is introduced to accelerate theprocessing, which is essential for real-time phase recognition in clinicalsettings. Extensive testing on the ESD385 dataset and the cholecystectomyCholec80 dataset demonstrates that SPRMamba surpasses existing state-of-the-artmethods and exhibits greater robustness across various surgical phaserecognition tasks.</description><author>Xiangning Zhang, Jinnan Chen, Qingwei Zhang, Chengfeng Zhou, Zhengjie Zhang, Xiaobo Li, Dahong Qian</author><pubDate>Wed, 18 Sep 2024 16:26:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12108v1</guid></item><item><title>Measuring Human and AI Values based on Generative Psychometrics with Large Language Models</title><link>http://arxiv.org/abs/2409.12106v1</link><description>Human values and their measurement are long-standing interdisciplinaryinquiry. Recent advances in AI have sparked renewed interest in this area, withlarge language models (LLMs) emerging as both tools and subjects of valuemeasurement. This work introduces Generative Psychometrics for Values (GPV), anLLM-based, data-driven value measurement paradigm, theoretically grounded intext-revealed selective perceptions. We begin by fine-tuning an LLM foraccurate perception-level value measurement and verifying the capability ofLLMs to parse texts into perceptions, forming the core of the GPV pipeline.Applying GPV to human-authored blogs, we demonstrate its stability, validity,and superiority over prior psychological tools. Then, extending GPV to LLMvalue measurement, we advance the current art with 1) a psychometricmethodology that measures LLM values based on their scalable and free-formoutputs, enabling context-specific measurement; 2) a comparative analysis ofmeasurement paradigms, indicating response biases of prior methods; and 3) anattempt to bridge LLM values and their safety, revealing the predictive powerof different value systems and the impacts of various values on LLM safety.Through interdisciplinary efforts, we aim to leverage AI for next-generationpsychometrics and psychometrics for value-aligned AI.</description><author>Haoran Ye, Yuhang Xie, Yuanyi Ren, Hanjun Fang, Xin Zhang, Guojie Song</author><pubDate>Wed, 18 Sep 2024 16:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12106v1</guid></item><item><title>FedLF: Adaptive Logit Adjustment and Feature Optimization in Federated Long-Tailed Learning</title><link>http://arxiv.org/abs/2409.12105v1</link><description>Federated learning offers a paradigm to the challenge of preserving privacyin distributed machine learning. However, datasets distributed across eachclient in the real world are inevitably heterogeneous, and if the datasets canbe globally aggregated, they tend to be long-tailed distributed, which greatlyaffects the performance of the model. The traditional approach to federatedlearning primarily addresses the heterogeneity of data among clients, yet itfails to address the phenomenon of class-wise bias in global long-tailed data.This results in the trained model focusing on the head classes while neglectingthe equally important tail classes. Consequently, it is essential to develop amethodology that considers classes holistically. To address the above problems,we propose a new method FedLF, which introduces three modifications in thelocal training phase: adaptive logit adjustment, continuous class centredoptimization, and feature decorrelation. We compare seven state-of-the-artmethods with varying degrees of data heterogeneity and long-taileddistribution. Extensive experiments on benchmark datasets CIFAR-10-LT andCIFAR-100-LT demonstrate that our approach effectively mitigates the problem ofmodel performance degradation due to data heterogeneity and long-taileddistribution. our code is available at https://github.com/18sym/FedLF.</description><author>Xiuhua Lu, Peng Li, Xuefeng Jiang</author><pubDate>Wed, 18 Sep 2024 16:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12105v1</guid></item><item><title>Contextual Breach: Assessing the Robustness of Transformer-based QA Models</title><link>http://arxiv.org/abs/2409.10997v2</link><description>Contextual question-answering models are susceptible to adversarialperturbations to input context, commonly observed in real-world scenarios.These adversarial noises are designed to degrade the performance of the modelby distorting the textual input. We introduce a unique dataset thatincorporates seven distinct types of adversarial noise into the context, eachapplied at five different intensity levels on the SQuAD dataset. To quantifythe robustness, we utilize robustness metrics providing a standardized measurefor assessing model performance across varying noise types and levels.Experiments on transformer-based question-answering models reveal robustnessvulnerabilities and important insights into the model's performance inrealistic textual input.</description><author>Asir Saadat, Nahian Ibn Asad, Md Farhan Ishmam</author><pubDate>Wed, 18 Sep 2024 16:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10997v2</guid></item><item><title>Symmetry-Enriched Learning: A Category-Theoretic Framework for Robust Machine Learning Models</title><link>http://arxiv.org/abs/2409.12100v1</link><description>This manuscript presents a novel framework that integrates higher-ordersymmetries and category theory into machine learning. We introduce newmathematical constructs, including hyper-symmetry categories and functorialrepresentations, to model complex transformations within learning algorithms.Our contributions include the design of symmetry-enriched learning models, thedevelopment of advanced optimization techniques leveraging categoricalsymmetries, and the theoretical analysis of their implications for modelrobustness, generalization, and convergence. Through rigorous proofs andpractical applications, we demonstrate that incorporating higher-dimensionalcategorical structures enhances both the theoretical foundations and practicalcapabilities of modern machine learning algorithms, opening new directions forresearch and innovation.</description><author>Ronald Katende</author><pubDate>Wed, 18 Sep 2024 16:20:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12100v1</guid></item><item><title>Brain-Streams: fMRI-to-Image Reconstruction with Multi-modal Guidance</title><link>http://arxiv.org/abs/2409.12099v1</link><description>Understanding how humans process visual information is one of the crucialsteps for unraveling the underlying mechanism of brain activity. Recently, thiscuriosity has motivated the fMRI-to-image reconstruction task; given the fMRIdata from visual stimuli, it aims to reconstruct the corresponding visualstimuli. Surprisingly, leveraging powerful generative models such as the LatentDiffusion Model (LDM) has shown promising results in reconstructing complexvisual stimuli such as high-resolution natural images from vision datasets.Despite the impressive structural fidelity of these reconstructions, they oftenlack details of small objects, ambiguous shapes, and semantic nuances.Consequently, the incorporation of additional semantic knowledge, beyond merevisuals, becomes imperative. In light of this, we exploit how modern LDMseffectively incorporate multi-modal guidance (text guidance, visual guidance,and image layout) for structurally and semantically plausible imagegenerations. Specifically, inspired by the two-streams hypothesis suggestingthat perceptual and semantic information are processed in different brainregions, our framework, Brain-Streams, maps fMRI signals from these brainregions to appropriate embeddings. That is, by extracting textual guidance fromsemantic information regions and visual guidance from perceptual informationregions, Brain-Streams provides accurate multi-modal guidance to LDMs. Wevalidate the reconstruction ability of Brain-Streams both quantitatively andqualitatively on a real fMRI dataset comprising natural image stimuli and fMRIdata.</description><author>Jaehoon Joo, Taejin Jeong, Seongjae Hwang</author><pubDate>Wed, 18 Sep 2024 16:19:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12099v1</guid></item><item><title>VideoClusterNet: Self-Supervised and Adaptive Face Clustering For Videos</title><link>http://arxiv.org/abs/2407.12214v2</link><description>With the rise of digital media content production, the need for analyzingmovies and TV series episodes to locate the main cast of characters preciselyis gaining importance.Specifically, Video Face Clustering aims to grouptogether detected video face tracks with common facial identities. This problemis very challenging due to the large range of pose, expression, appearance, andlighting variations of a given face across video frames. Generic pre-trainedFace Identification (ID) models fail to adapt well to the video productiondomain, given its high dynamic range content and also unique cinematic style.Furthermore, traditional clustering algorithms depend on hyperparametersrequiring individual tuning across datasets. In this paper, we present a novelvideo face clustering approach that learns to adapt a generic face ID model tonew video face tracks in a fully self-supervised fashion. We also propose aparameter-free clustering algorithm that is capable of automatically adaptingto the finetuned model's embedding space for any input video. Due to the lackof comprehensive movie face clustering benchmarks, we also present afirst-of-kind movie dataset: MovieFaceCluster. Our dataset is handpicked byfilm industry professionals and contains extremely challenging face IDscenarios. Experiments show our method's effectiveness in handling difficultmainstream movie scenes on our benchmark dataset and state-of-the-artperformance on traditional TV series datasets.</description><author>Devesh Walawalkar, Pablo Garrido</author><pubDate>Wed, 18 Sep 2024 16:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12214v2</guid></item><item><title>Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval</title><link>http://arxiv.org/abs/2409.12097v1</link><description>Finding the perfect match between a job proposal and a set of freelancers isnot an easy task to perform at scale, especially in multiple languages. In thispaper, we propose a novel neural retriever architecture that tackles thisproblem in a multilingual setting. Our method encodes project descriptions andfreelancer profiles by leveraging pre-trained multilingual language models. Thelatter are used as backbone for a custom transformer architecture that aims tokeep the structure of the profiles and project. This model is trained with acontrastive loss on historical data. Thanks to several experiments, we showthat this approach effectively captures skill matching similarity andfacilitates efficient matching, outperforming traditional methods.</description><author>Warren Jouanneau, Marc Palyart, Emma Jouffroy</author><pubDate>Wed, 18 Sep 2024 16:15:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12097v1</guid></item><item><title>Computationally efficient reductions between some statistical models</title><link>http://arxiv.org/abs/2402.07717v2</link><description>We study the problem of approximately transforming a sample from a sourcestatistical model to a sample from a target statistical model without knowingthe parameters of the source model, and construct several computationallyefficient such reductions between canonical statistical experiments. Inparticular, we provide computationally efficient procedures that approximatelyreduce uniform, Erlang, and Laplace location models to general target families.We illustrate our methodology by establishing nonasymptotic reductions betweensome canonical high-dimensional problems, spanning mixtures of experts, phaseretrieval, and signal denoising. Notably, the reductions arestructure-preserving and can accommodate missing data. We also point to apossible application in transforming one differentially private mechanism toanother.</description><author>Mengqi Lou, Guy Bresler, Ashwin Pananjady</author><pubDate>Wed, 18 Sep 2024 16:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07717v2</guid></item><item><title>High-Resolution Maps of Left Atrial Displacements and Strains Estimated with 3D Cine MRI using Online Learning Neural Networks</title><link>http://arxiv.org/abs/2312.09387v2</link><description>The functional analysis of the left atrium (LA) is important for evaluatingcardiac health and understanding diseases like atrial fibrillation. Cine MRI isideally placed for the detailed 3D characterization of LA motion anddeformation but is lacking appropriate acquisition and analysis tools. Here, wepropose tools for the Analysis for Left Atrial Displacements and DeformatIonsusing online learning neural Networks (Aladdin) and present a technicalfeasibility study on how Aladdin can characterize 3D LA function globally andregionally. Aladdin includes an online segmentation and image registrationnetwork, and a strain calculation pipeline tailored to the LA. We create mapsof LA Displacement Vector Field (DVF) magnitude and LA principal strain valuesfrom images of 10 healthy volunteers and 8 patients with cardiovascular disease(CVD), of which 2 had large left ventricular ejection fraction (LVEF)impairment. We additionally create an atlas of these biomarkers using the datafrom the healthy volunteers. Results showed that Aladdin can accurately trackthe LA wall across the cardiac cycle and characterize its motion anddeformation. Global LA function markers assessed with Aladdin agree well withestimates from 2D Cine MRI. A more marked active contraction phase was observedin the healthy cohort, while the CVD LVEF group showed overall reduced LAfunction. Aladdin is uniquely able to identify LA regions with abnormaldeformation metrics that may indicate focal pathology. We expect Aladdin tohave important clinical applications as it can non-invasively characterizeatrial pathophysiology. All source code and data are available at:https://github.com/cgalaz01/aladdin_cmr_la.</description><author>Christoforos Galazis, Samuel Shepperd, Emma Brouwer, Sandro QueirÃ³s, Ebraham Alskaf, Mustafa Anjari, Amedeo Chiribiri, Jack Lee, Anil A. Bharath, Marta Varela</author><pubDate>Wed, 18 Sep 2024 16:11:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09387v2</guid></item><item><title>EHRFL: Federated Learning Framework for Institution-Specific Model Construction using Electronic Health Records</title><link>http://arxiv.org/abs/2404.13318v2</link><description>The increasing volume of electronic health records (EHRs) across healthcareinstitutions presents the opportunity to enhance model accuracy and robustnessin clinical prediction tasks. Federated learning enables training on data frommultiple institutions while preserving patient privacy and complying toregulatory constraints. However, most federated learning research focuses onconstructing a global model for multiple clients, overlooking the practicalneed for institution-specific models. In this work, we introduce EHRFL, afederated learning framework using EHRs designed to develop a model tailored toa single healthcare institution. Our framework addresses two key challenges:(1) enabling federated learning across institutions with heterogeneous EHRsystems using text-based EHR modeling, and (2) reducing the costs associatedwith federated learning by selecting suitable participating clients usingaveraged patient embeddings, which enables optimizing the number ofparticipants without compromising model performance for the institution. Ourexperiment results on multiple open-source EHR datasets demonstrate theeffectiveness of EHRFL in addressing the two challenges, establishing it as apractical solution for institution-specific model development in federatedlearning.</description><author>Jiyoun Kim, Junu Kim, Kyunghoon Hur, Edward Choi</author><pubDate>Wed, 18 Sep 2024 16:09:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13318v2</guid></item><item><title>Improving Ontology Requirements Engineering with OntoChat and Participatory Prompting</title><link>http://arxiv.org/abs/2408.15256v3</link><description>Past ontology requirements engineering (ORE) has primarily relied on manualmethods, such as interviews and collaborative forums, to gather userrequirements from domain experts, especially in large projects. CurrentOntoChat offers a framework for ORE that utilises large language models (LLMs)to streamline the process through four key functions: user story creation,competency question (CQ) extraction, CQ filtration and analysis, and ontologytesting support. In OntoChat, users are expected to prompt the chatbot togenerate user stories. However, preliminary evaluations revealed that theystruggle to do this effectively. To address this issue, we experimented with aresearch method called participatory prompting, which involvesresearcher-mediated interactions to help users without deep knowledge of LLMsuse the chatbot more effectively. This participatory prompting user studyproduces pre-defined prompt templates based on user queries, focusing oncreating and refining personas, goals, scenarios, sample data, and dataresources for user stories. These refined user stories will subsequently beconverted into CQs.</description><author>Yihang Zhao, Bohui Zhang, Xi Hu, Shuyin Ouyang, Jongmo Kim, Nitisha Jain, Jacopo de Berardinis, Albert MeroÃ±o-PeÃ±uela, Elena Simperl</author><pubDate>Wed, 18 Sep 2024 16:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15256v3</guid></item><item><title>IMRL: Integrating Visual, Physical, Temporal, and Geometric Representations for Enhanced Food Acquisition</title><link>http://arxiv.org/abs/2409.12092v1</link><description>Robotic assistive feeding holds significant promise for improving the qualityof life for individuals with eating disabilities. However, acquiring diversefood items under varying conditions and generalizing to unseen food presentsunique challenges. Existing methods that rely on surface-level geometricinformation (e.g., bounding box and pose) derived from visual cues (e.g.,color, shape, and texture) often lacks adaptability and robustness, especiallywhen foods share similar physical properties but differ in visual appearance.We employ imitation learning (IL) to learn a policy for food acquisition.Existing methods employ IL or Reinforcement Learning (RL) to learn a policybased on off-the-shelf image encoders such as ResNet-50. However, suchrepresentations are not robust and struggle to generalize across diverseacquisition scenarios. To address these limitations, we propose a novelapproach, IMRL (Integrated Multi-Dimensional Representation Learning), whichintegrates visual, physical, temporal, and geometric representations to enhancethe robustness and generalizability of IL for food acquisition. Our approachcaptures food types and physical properties (e.g., solid, semi-solid, granular,liquid, and mixture), models temporal dynamics of acquisition actions, andintroduces geometric information to determine optimal scooping points andassess bowl fullness. IMRL enables IL to adaptively adjust scooping strategiesbased on context, improving the robot's capability to handle diverse foodacquisition scenarios. Experiments on a real robot demonstrate our approach'srobustness and adaptability across various foods and bowl configurations,including zero-shot generalization to unseen settings. Our approach achievesimprovement up to $35\%$ in success rate compared with the best-performingbaseline.</description><author>Rui Liu, Zahiruddin Mahammad, Amisha Bhaskar, Pratap Tokekar</author><pubDate>Wed, 18 Sep 2024 16:09:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12092v1</guid></item><item><title>The Impact of Element Ordering on LM Agent Performance</title><link>http://arxiv.org/abs/2409.12089v1</link><description>There has been a surge of interest in language model agents that can navigatevirtual environments such as the web or desktop. To navigate such environments,agents benefit from information on the various elements (e.g., buttons, text,or images) present. It remains unclear which element attributes have thegreatest impact on agent performance, especially in environments that onlyprovide a graphical representation (i.e., pixels). Here we find that theordering in which elements are presented to the language model is surprisinglyimpactful--randomizing element ordering in a webpage degrades agent performancecomparably to removing all visible text from an agent's state representation.While a webpage provides a hierarchical ordering of elements, there is no suchordering when parsing elements directly from pixels. Moreover, as tasks becomemore challenging and models more sophisticated, our experiments suggest thatthe impact of ordering increases. Finding an effective ordering is non-trivial.We investigate the impact of various element ordering methods in web anddesktop environments. We find that dimensionality reduction provides a viableordering for pixel-only environments. We train a UI element detection model toderive elements from pixels and apply our findings to an agentbenchmark--OmniACT--where we only have access to pixels. Our method completesmore than two times as many tasks on average relative to the previousstate-of-the-art.</description><author>Wayne Chi, Ameet Talwalkar, Chris Donahue</author><pubDate>Wed, 18 Sep 2024 16:04:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12089v1</guid></item><item><title>Towards Interpretable End-Stage Renal Disease (ESRD) Prediction: Utilizing Administrative Claims Data with Explainable AI Techniques</title><link>http://arxiv.org/abs/2409.12087v1</link><description>This study explores the potential of utilizing administrative claims data,combined with advanced machine learning and deep learning techniques, topredict the progression of Chronic Kidney Disease (CKD) to End-Stage RenalDisease (ESRD). We analyze a comprehensive, 10-year dataset provided by a majorhealth insurance organization to develop prediction models for multipleobservation windows using traditional machine learning methods such as RandomForest and XGBoost as well as deep learning approaches such as Long Short-TermMemory (LSTM) networks. Our findings demonstrate that the LSTM model,particularly with a 24-month observation window, exhibits superior performancein predicting ESRD progression, outperforming existing models in theliterature. We further apply SHapley Additive exPlanations (SHAP) analysis toenhance interpretability, providing insights into the impact of individualfeatures on predictions at the individual patient level. This study underscoresthe value of leveraging administrative claims data for CKD management andpredicting ESRD progression.</description><author>Yubo Li, Saba Al-Sayouri, Rema Padman</author><pubDate>Wed, 18 Sep 2024 16:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12087v1</guid></item><item><title>Calibration Error for Decision Making</title><link>http://arxiv.org/abs/2404.13503v3</link><description>Calibration allows predictions to be reliably interpreted as probabilities bydecision makers. We propose a decision-theoretic calibration error, theCalibration Decision Loss (CDL), defined as the maximum improvement in decisionpayoff obtained by calibrating the predictions, where the maximum is over allpayoff-bounded decision tasks. Vanishing CDL guarantees the payoff loss frommiscalibration vanishes simultaneously for all downstream decision tasks. Weshow separations between CDL and existing calibration error metrics, includingthe most well-studied metric Expected Calibration Error (ECE). Our maintechnical contribution is a new efficient algorithm for online calibration thatachieves near-optimal $O(\frac{\log T}{\sqrt{T}})$ expected CDL, bypassing the$\Omega(T^{-0.472})$ lower bound for ECE by Qiao and Valiant (2021).</description><author>Lunjia Hu, Yifan Wu</author><pubDate>Wed, 18 Sep 2024 15:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13503v3</guid></item><item><title>Denoising diffusion models for high-resolution microscopy image restoration</title><link>http://arxiv.org/abs/2409.12078v1</link><description>Advances in microscopy imaging enable researchers to visualize structures atthe nanoscale level thereby unraveling intricate details of biologicalorganization. However, challenges such as image noise, photobleaching offluorophores, and low tolerability of biological samples to high light dosesremain, restricting temporal resolutions and experiment durations. Reducedlaser doses enable longer measurements at the cost of lower resolution andincreased noise, which hinders accurate downstream analyses. Here we train adenoising diffusion probabilistic model (DDPM) to predict high-resolutionimages by conditioning the model on low-resolution information. Additionally,the probabilistic aspect of the DDPM allows for repeated generation of imagesthat tend to further increase the signal-to-noise ratio. We show that our modelachieves a performance that is better or similar to the previouslybest-performing methods, across four highly diverse datasets. Importantly,while any of the previous methods show competitive performance for some, butnot all datasets, our method consistently achieves high performance across allfour data sets, suggesting high generalizability.</description><author>Pamela Osuna-Vargas, Maren H. Wehrheim, Lucas Zinz, Johanna Rahm, Ashwin Balakrishnan, Alexandra Kaminer, Mike Heilemann, Matthias Kaschube</author><pubDate>Wed, 18 Sep 2024 15:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12078v1</guid></item><item><title>Unsupervised Domain Adaptation Via Data Pruning</title><link>http://arxiv.org/abs/2409.12076v1</link><description>The removal of carefully-selected examples from training data has recentlyemerged as an effective way of improving the robustness of machine learningmodels. However, the best way to select these examples remains an openquestion. In this paper, we consider the problem from the perspective ofunsupervised domain adaptation (UDA). We propose AdaPrune, a method for UDAwhereby training examples are removed to attempt to align the trainingdistribution to that of the target data. By adopting the maximum meandiscrepancy (MMD) as the criterion for alignment, the problem can be neatlyformulated and solved as an integer quadratic program. We evaluate our approachon a real-world domain shift task of bioacoustic event detection. As a methodfor UDA, we show that AdaPrune outperforms related techniques, and iscomplementary to other UDA algorithms such as CORAL. Our analysis of therelationship between the MMD and model accuracy, along with t-SNE plots,validate the proposed method as a principled and well-founded way of performingdata pruning.</description><author>Andrea Napoli, Paul White</author><pubDate>Wed, 18 Sep 2024 15:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12076v1</guid></item><item><title>Visualizing Temporal Topic Embeddings with a Compass</title><link>http://arxiv.org/abs/2409.10649v2</link><description>Dynamic topic modeling is useful at discovering the development and change inlatent topics over time. However, present methodology relies on algorithms thatseparate document and word representations. This prevents the creation of ameaningful embedding space where changes in word usage and documents can bedirectly analyzed in a temporal context. This paper proposes an expansion ofthe compass-aligned temporal Word2Vec methodology into dynamic topic modeling.Such a method allows for the direct comparison of word and document embeddingsacross time in dynamic topics. This enables the creation of visualizations thatincorporate temporal word embeddings within the context of documents into topicvisualizations. In experiments against the current state-of-the-art, ourproposed method demonstrates overall competitive performance in topic relevancyand diversity across temporal datasets of varying size. Simultaneously, itprovides insightful visualizations focused on temporal word embeddings whilemaintaining the insights provided by global topic evolution, advancing ourunderstanding of how topics evolve over time.</description><author>Daniel Palamarchuk, Lemara Williams, Brian Mayer, Thomas Danielson, Rebecca Faust, Larry Deschaine, Chris North</author><pubDate>Wed, 18 Sep 2024 15:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10649v2</guid></item><item><title>Online Refractive Camera Model Calibration in Visual Inertial Odometry</title><link>http://arxiv.org/abs/2409.12074v1</link><description>This paper presents a general refractive camera model and onlineco-estimation of odometry and the refractive index of unknown media. Thisenables operation in diverse and varying refractive fluids, given only thecamera calibration in air. The refractive index is estimated online as a statevariable of a monocular visual-inertial odometry framework in an iterativeformulation using the proposed camera model. The method was verified on datacollected using an underwater robot traversing inside a pool. The evaluationsdemonstrate convergence to the ideal refractive index for water despitesignificant perturbations in the initialization. Simultaneously, the approachenables on-par visual-inertial odometry performance in refractive media withoutprior knowledge of the refractive index or requirement of medium-specificcamera calibration.</description><author>Mohit Singh, Kostas Alexis</author><pubDate>Wed, 18 Sep 2024 15:48:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12074v1</guid></item><item><title>PAD-FT: A Lightweight Defense for Backdoor Attacks via Data Purification and Fine-Tuning</title><link>http://arxiv.org/abs/2409.12072v1</link><description>Backdoor attacks pose a significant threat to deep neural networks,particularly as recent advancements have led to increasingly subtleimplantation, making the defense more challenging. Existing defense mechanismstypically rely on an additional clean dataset as a standard reference andinvolve retraining an auxiliary model or fine-tuning the entire victim model.However, these approaches are often computationally expensive and not alwaysfeasible in practical applications. In this paper, we propose a novel andlightweight defense mechanism, termed PAD-FT, that does not require anadditional clean dataset and fine-tunes only a very small part of the model todisinfect the victim model. To achieve this, our approach first introduces asimple data purification process to identify and select the most-likely cleandata from the poisoned training dataset. The self-purified clean dataset isthen used for activation clipping and fine-tuning only the last classificationlayer of the victim model. By integrating data purification, activationclipping, and classifier fine-tuning, our mechanism PAD-FT demonstratessuperior effectiveness across multiple backdoor attack methods and datasets, asconfirmed through extensive experimental evaluation.</description><author>Yukai Xu, Yujie Gu, Kouichi Sakurai</author><pubDate>Wed, 18 Sep 2024 15:47:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12072v1</guid></item><item><title>Adaptive Step Sizes for Preconditioned Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2311.16956v2</link><description>This paper proposes a novel approach to adaptive step sizes in stochasticgradient descent (SGD) by utilizing quantities that we have identified asnumerically traceable -- the Lipschitz constant for gradients and a concept ofthe local variance in search directions. Our findings yield a nearlyhyperparameter-free algorithm for stochastic optimization, which has provableconvergence properties and exhibits truly problem adaptive behavior onclassical image classification tasks. Our framework is set in a general Hilbertspace and thus enables the potential inclusion of a preconditioner through thechoice of the inner product.</description><author>Frederik KÃ¶hne, Leonie Kreis, Anton Schiela, Roland Herzog</author><pubDate>Wed, 18 Sep 2024 15:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16956v2</guid></item><item><title>Fitting Multilevel Factor Models</title><link>http://arxiv.org/abs/2409.12067v1</link><description>We examine a special case of the multilevel factor model, with covariancegiven by multilevel low rank (MLR) matrix~\cite{parshakova2023factor}. Wedevelop a novel, fast implementation of the expectation-maximization (EM)algorithm, tailored for multilevel factor models, to maximize the likelihood ofthe observed data. This method accommodates any hierarchical structure andmaintains linear time and storage complexities per iteration. This is achievedthrough a new efficient technique for computing the inverse of the positivedefinite MLR matrix. We show that the inverse of an invertible PSD MLR matrixis also an MLR matrix with the same sparsity in factors, and we use therecursive Sherman-Morrison-Woodbury matrix identity to obtain the factors ofthe inverse. Additionally, we present an algorithm that computes the Choleskyfactorization of an expanded matrix with linear time and space complexities,yielding the covariance matrix as its Schur complement. This paper isaccompanied by an open-source package that implements the proposed methods.</description><author>Tetiana Parshakova, Trevor Hastie, Stephen Boyd</author><pubDate>Wed, 18 Sep 2024 15:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12067v1</guid></item><item><title>A geometric view on probabilistically robust learning</title><link>http://arxiv.org/abs/2305.18779v2</link><description>Although deep neural networks have achieved super-human performance on manyclassification tasks, they often exhibit a worrying lack of robustness towardsadversarially generated examples. Thus, considerable effort has been investedinto reformulating standard Risk Minimization (RM) into an adversarially robustframework. Recently, attention has shifted towards approaches which interpolatebetween the robustness offered by adversarial training and the higher cleanaccuracy and faster training times of RM. In this paper, we take a fresh andgeometric view on one such method -- Probabilistically Robust Learning (PRL).We propose a mathematical framework for understanding PRL, which allows us toidentify geometric pathologies in its original formulation and to introduce afamily of probabilistic nonlocal perimeter functionals to rectify them. Weprove existence of solutions to the original and modified problems using novelrelaxation methods and also study properties, as well as local limits, of theintroduced perimeters. We also clarify, through a suitable $\Gamma$-convergenceanalysis, the way in which the original and modified PRL models interpolatebetween risk minimization and adversarial training.</description><author>Leon Bungert, NicolÃ¡s GarcÃ­a Trillos, Matt Jacobs, Daniel McKenzie, ÄorÄe NikoliÄ, Qingsong Wang</author><pubDate>Wed, 18 Sep 2024 15:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18779v2</guid></item><item><title>Generalized Robot Learning Framework</title><link>http://arxiv.org/abs/2409.12061v1</link><description>Imitation based robot learning has recently gained significant attention inthe robotics field due to its theoretical potential for transferability andgeneralizability. However, it remains notoriously costly, both in terms ofhardware and data collection, and deploying it in real-world environmentsdemands meticulous setup of robots and precise experimental conditions. In thispaper, we present a low-cost robot learning framework that is both easilyreproducible and transferable to various robots and environments. Wedemonstrate that deployable imitation learning can be successfully applied evento industrial-grade robots, not just expensive collaborative robotic arms.Furthermore, our results show that multi-task robot learning is achievable withsimple network architectures and fewer demonstrations than previously thoughtnecessary. As the current evaluating method is almost subjective when it comesto real-world manipulation tasks, we propose Voting Positive Rate (VPR) - anovel evaluation strategy that provides a more objective assessment ofperformance. We conduct an extensive comparison of success rates across variousself-designed tasks to validate our approach. To foster collaboration andsupport the robot learning community, we have open-sourced all relevantdatasets and model checkpoints, available at huggingface.co/ZhiChengAI.</description><author>Jiahuan Yan, Zhouyang Hong, Yu Zhao, Yu Tian, Yunxin Liu, Travis Davies, Luhui Hu</author><pubDate>Wed, 18 Sep 2024 15:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12061v1</guid></item><item><title>PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase Detection Models</title><link>http://arxiv.org/abs/2409.12060v1</link><description>The task of determining whether two texts are paraphrases has long been achallenge in NLP. However, the prevailing notion of paraphrase is often quitesimplistic, offering only a limited view of the vast spectrum of paraphrasephenomena. Indeed, we find that evaluating models in a paraphrase dataset canleave uncertainty about their true semantic understanding. To alleviate this,we release paraphrasus, a benchmark designed for multi-dimensional assessmentof paraphrase detection models and finer model selection. We find thatparaphrase detection models under a fine-grained evaluation lens exhibittrade-offs that cannot be captured through a single classification dataset.</description><author>Andrianos Michail, Simon Clematide, Juri Opitz</author><pubDate>Wed, 18 Sep 2024 15:33:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12060v1</guid></item><item><title>Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking</title><link>http://arxiv.org/abs/2409.12059v1</link><description>Large Language Model can reasonably understand and generate human expressionsbut may lack of thorough thinking and reasoning mechanisms. Recently there havebeen several studies which enhance the thinking ability of language models butmost of them are not data-driven or training-based. In this paper, we aremotivated by the cognitive mechanism in the natural world, and design a novelmodel architecture called TaS which allows it to first consider the thoughtsand then express the response based upon the query. We design several pipelinesto annotate or generate the thought contents from prompt-response samples, thenadd language heads in a middle layer which behaves as the thinking layer. Wetrain the language model by the thoughts-augmented data and successfully letthe thinking layer automatically generate reasonable thoughts and finallyoutput more reasonable responses. Both qualitative examples and quantitativeresults validate the effectiveness and performance of TaS. Our code isavailable at https://anonymous.4open.science/r/TadE.</description><author>Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji</author><pubDate>Wed, 18 Sep 2024 15:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12059v1</guid></item><item><title>Cartan moving frames and the data manifolds</title><link>http://arxiv.org/abs/2409.12057v1</link><description>The purpose of this paper is to employ the language of Cartan moving framesto study the geometry of the data manifolds and its Riemannian structure, viathe data information metric and its curvature at data points. Using thisframework and through experiments, explanations on the response of a neuralnetwork are given by pointing out the output classes that are easily reachablefrom a given input. This emphasizes how the proposed mathematical relationshipbetween the output of the network and the geometry of its inputs can beexploited as an explainable artificial intelligence tool.</description><author>Eliot Tron, Rita Fioresi, Nicolas Couellan, StÃ©phane Puechmorel</author><pubDate>Wed, 18 Sep 2024 15:31:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12057v1</guid></item><item><title>PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning</title><link>http://arxiv.org/abs/2305.19472v3</link><description>Procedural planning, which entails decomposing a high-level goal into asequence of temporally ordered steps, is an important yet intricate task formachines. It involves integrating common-sense knowledge to reason aboutcomplex and often contextualized situations, e.g. ``scheduling a doctor'sappointment without a phone''. While current approaches show encouragingresults using large language models (LLMs), they are hindered by drawbacks suchas costly API calls and reproducibility issues. In this paper, we advocateplanning using smaller language models. We present PlaSma, a novel two-prongedapproach to endow small language models with procedural knowledge and(constrained) language planning capabilities. More concretely, we developsymbolic procedural knowledge distillation to enhance the commonsense knowledgein small language models and an inference-time algorithm to facilitate morestructured and accurate reasoning. In addition, we introduce a new relatedtask, Replanning, that requires a revision of a plan to cope with a constrainedsituation. In both the planning and replanning settings, we show thatorders-of-magnitude smaller models (770M-11B parameters) can compete and oftensurpass their larger teacher models' capabilities. Finally, we showcasesuccessful application of PlaSma in an embodied environment, VirtualHome.</description><author>Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D. Hwang, Xiang Lorraine Li, Hirona J. Arai, Soumya Sanyal, Keisuke Sakaguchi, Xiang Ren, Yejin Choi</author><pubDate>Wed, 18 Sep 2024 15:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19472v3</guid></item><item><title>Extended Deep Submodular Functions</title><link>http://arxiv.org/abs/2409.12053v1</link><description>We introduce a novel category of set functions called Extended DeepSubmodular functions (EDSFs), which are neural network-representable. EDSFsserve as an extension of Deep Submodular Functions (DSFs), inheriting crucialproperties from DSFs while addressing innate limitations. It is known that DSFscan represent a limiting subset of submodular functions. In contrast, throughan analysis of polymatroid properties, we establish that EDSFs possess thecapability to represent all monotone submodular functions, a notableenhancement compared to DSFs. Furthermore, our findings demonstrate that EDSFscan represent any monotone set function, indicating the family of EDSFs isequivalent to the family of all monotone set functions. Additionally, we provethat EDSFs maintain the concavity inherent in DSFs when the components of theinput vector are non-negative real numbers-an essential feature in certaincombinatorial optimization problems. Through extensive experiments, weillustrate that EDSFs exhibit significantly lower empirical generalizationerror than DSFs in the learning of coverage functions. This suggests that EDSFspresent a promising advancement in the representation and learning of setfunctions with improved generalization capabilities.</description><author>Seyed Mohammad Hosseini, Arash Jamshid, Seyed Mahdi Noormousavi, Mahdi Jafari Siavoshani, Naeimeh Omidvar</author><pubDate>Wed, 18 Sep 2024 15:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12053v1</guid></item><item><title>A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models</title><link>http://arxiv.org/abs/2408.14496v3</link><description>Recent advances in deep learning have completely transformed the domain ofcomputational pathology (CPath). More specifically, it has altered thediagnostic workflow of pathologists by integrating foundation models (FMs) andvision-language models (VLMs) in their assessment and decision-making process.The limitations of existing deep learning approaches in CPath can be overcomeby FMs through learning a representation space that can be adapted to a widevariety of downstream tasks without explicit supervision. Deploying VLMs allowpathology reports written in natural language be used as rich semanticinformation sources to improve existing models as well as generate predictionsin natural language form. In this survey, a holistic and systematic overview ofrecent innovations in FMs and VLMs in CPath is presented. Furthermore, thetools, datasets and training schemes for these models are summarized inaddition to categorizing them into distinct groups. This extensive surveyhighlights the current trends in CPath and its possible revolution through theuse of FMs and VLMs in the future.</description><author>Dibaloke Chanda, Milan Aryal, Nasim Yahya Soltani, Masoud Ganji</author><pubDate>Wed, 18 Sep 2024 15:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14496v3</guid></item><item><title>Comparison of Two Augmentation Methods in Improving Detection Accuracy of Hemarthrosis</title><link>http://arxiv.org/abs/2409.05225v2</link><description>With the increase of computing power, machine learning models in medicalimaging have been introduced to help in rending medical diagnosis andinspection, like hemophilia, a rare disorder in which blood cannot clotnormally. Often, one of the bottlenecks of detecting hemophilia is the lack ofdata available to train the algorithm to increase the accuracy. As a possiblesolution, this research investigated whether introducing augmented data by datasynthesis or traditional augmentation techniques can improve model accuracy,helping to diagnose the diseases. To tackle this research, features ofultrasound images were extracted by the pre-trained VGG-16, and similaritieswere compared by cosine similarity measure based on extracted features indifferent distributions among real images, synthetic images, and augmentationimages (Real vs. Real, Syn vs. Syn, Real vs. Different Batches of Syn, Real vs.Augmentation Techniques). Model testing performance was investigated usingEffientNet-B4 to recognize "blood" images with two augmentation methods. Inaddition, a gradient-weighted class activation mapping (Grad-CAM) visualizationwas used to interpret the unexpected results like loss of accuracy. Syntheticand real images do not show high similarity, with a mean similarity score of0.4737. Synthetic batch 1 dataset and images by horizontal flip are moresimilar to the original images. Classic augmentation techniques and datasynthesis can improve model accuracy, and data by traditional augmentationtechniques have a better performance than synthetic data. In addition, theGrad-CAM heatmap figured out the loss of accuracy is due to a shift in thedomain. Overall, this research found that two augmentation methods, datasynthesis and traditional augmentation techniques, both can improve accuracy toa certain extent to help to diagnose rare diseases.</description><author>Qianyu Fan</author><pubDate>Wed, 18 Sep 2024 15:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05225v2</guid></item><item><title>Operational Wind Speed Forecasts for Chile's Electric Power Sector Using a Hybrid ML Model</title><link>http://arxiv.org/abs/2409.09263v3</link><description>As Chile's electric power sector advances toward a future powered byrenewable energy, accurate forecasting of renewable generation is essential formanaging grid operations. The integration of renewable energy sources isparticularly challenging due to the operational difficulties of managing theirpower generation, which is highly variable compared to fossil fuel sources,delaying the availability of clean energy. To mitigate this, we quantify theimpact of increasing intermittent generation from wind and solar on thermalpower plants in Chile and introduce a hybrid wind speed forecasting methodologywhich combines two custom ML models for Chile. The first model is based onTiDE, an MLP-based ML model for short-term forecasts, and the second is basedon a graph neural network, GraphCast, for medium-term forecasts up to 10 days.Our hybrid approach outperforms the most accurate operational deterministicsystems by 4-21% for short-term forecasts and 5-23% for medium-term forecastsand can directly lower the impact of wind generation on thermal ramping,curtailment, and system-level emissions in Chile.</description><author>Dhruv Suri, Praneet Dutta, Flora Xue, Ines Azevedo, Ravi Jain</author><pubDate>Wed, 18 Sep 2024 15:17:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09263v3</guid></item><item><title>Using Large Language Models to Generate Clinical Trial Tables and Figures</title><link>http://arxiv.org/abs/2409.12046v1</link><description>Tables, figures, and listings (TFLs) are essential tools for summarizingclinical trial data. Creation of TFLs for reporting activities is often atime-consuming task encountered routinely during the execution of clinicaltrials. This study explored the use of large language models (LLMs) to automatethe generation of TFLs through prompt engineering and few-shot transferlearning. Using public clinical trial data in ADaM format, our resultsdemonstrated that LLMs can efficiently generate TFLs with prompt instructions,showcasing their potential in this domain. Furthermore, we developed aconservational agent named Clinical Trial TFL Generation Agent: An app thatmatches user queries to predefined prompts that produce customized programs togenerate specific predefined TFLs.</description><author>Yumeng Yang, Peter Krusche, Kristyn Pantoja, Cheng Shi, Ethan Ludmir, Kirk Roberts, Gen Zhu</author><pubDate>Wed, 18 Sep 2024 15:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12046v1</guid></item><item><title>Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning</title><link>http://arxiv.org/abs/2409.12045v1</link><description>Safety is one of the key issues preventing the deployment of reinforcementlearning techniques in real-world robots. While most approaches in the SafeReinforcement Learning area do not require prior knowledge of constraints androbot kinematics and rely solely on data, it is often difficult to deploy themin complex real-world settings. Instead, model-based approaches thatincorporate prior knowledge of the constraints and dynamics into the learningframework have proven capable of deploying the learning algorithm directly onthe real robot. Unfortunately, while an approximated model of the robotdynamics is often available, the safety constraints are task-specific and hardto obtain: they may be too complicated to encode analytically, too expensive tocompute, or it may be difficult to envision a priori the long-term safetyrequirements. In this paper, we bridge this gap by extending the safeexploration method, ATACOM, with learnable constraints, with a particular focuson ensuring long-term safety and handling of uncertainty. Our approach iscompetitive or superior to state-of-the-art methods in final performance whilemaintaining safer behavior during training.</description><author>Jonas GÃ¼nster, Puze Liu, Jan Peters, Davide Tateo</author><pubDate>Wed, 18 Sep 2024 15:08:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12045v1</guid></item><item><title>Understanding the Effects of the Baidu-ULTR Logging Policy on Two-Tower Models</title><link>http://arxiv.org/abs/2409.12043v1</link><description>Despite the popularity of the two-tower model for unbiased learning to rank(ULTR) tasks, recent work suggests that it suffers from a major limitation thatcould lead to its collapse in industry applications: the problem of loggingpolicy confounding. Several potential solutions have even been proposed;however, the evaluation of these methods was mostly conducted usingsemi-synthetic simulation experiments. This paper bridges the gap betweentheory and practice by investigating the confounding problem on the largestreal-world dataset, Baidu-ULTR. Our main contributions are threefold: 1) weshow that the conditions for the confounding problem are given on Baidu-ULTR,2) the confounding problem bears no significant effect on the two-tower model,and 3) we point to a potential mismatch between expert annotations, the goldenstandard in ULTR, and user click behavior.</description><author>Morris de Haan, Philipp Hager</author><pubDate>Wed, 18 Sep 2024 15:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12043v1</guid></item><item><title>ASR Benchmarking: Need for a More Representative Conversational Dataset</title><link>http://arxiv.org/abs/2409.12042v1</link><description>Automatic Speech Recognition (ASR) systems have achieved remarkableperformance on widely used benchmarks such as LibriSpeech and Fleurs. However,these benchmarks do not adequately reflect the complexities of real-worldconversational environments, where speech is often unstructured and containsdisfluencies such as pauses, interruptions, and diverse accents. In this study,we introduce a multilingual conversational dataset, derived from TalkBank,consisting of unstructured phone conversation between adults. Our results showa significant performance drop across various state-of-the-art ASR models whentested in conversational settings. Furthermore, we observe a correlationbetween Word Error Rate and the presence of speech disfluencies, highlightingthe critical need for more realistic, conversational ASR benchmarks.</description><author>Gaurav Maheshwari, Dmitry Ivanov, ThÃ©o Johannet, Kevin El Haddad</author><pubDate>Wed, 18 Sep 2024 15:03:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12042v1</guid></item><item><title>SFDA-rPPG: Source-Free Domain Adaptive Remote Physiological Measurement with Spatio-Temporal Consistency</title><link>http://arxiv.org/abs/2409.12040v1</link><description>Remote Photoplethysmography (rPPG) is a non-contact method that uses facialvideo to predict changes in blood volume, enabling physiological metricsmeasurement. Traditional rPPG models often struggle with poor generalizationcapacity in unseen domains. Current solutions to this problem is to improve itsgeneralization in the target domain through Domain Generalization (DG) orDomain Adaptation (DA). However, both traditional methods require access toboth source domain data and target domain data, which cannot be implemented inscenarios with limited access to source data, and another issue is the privacyof accessing source domain data. In this paper, we propose the firstSource-free Domain Adaptation benchmark for rPPG measurement (SFDA-rPPG), whichovercomes these limitations by enabling effective domain adaptation withoutaccess to source domain data. Our framework incorporates a Three-BranchSpatio-Temporal Consistency Network (TSTC-Net) to enhance feature consistencyacross domains. Furthermore, we propose a new rPPG distribution alignment lossbased on the Frequency-domain Wasserstein Distance (FWD), which leveragesoptimal transport to align power spectrum distributions across domainseffectively and further enforces the alignment of the three branches. Extensivecross-domain experiments and ablation studies demonstrate the effectiveness ofour proposed method in source-free domain adaptation settings. Our findingshighlight the significant contribution of the proposed FWD loss fordistributional alignment, providing a valuable reference for future researchand applications. The source code is available athttps://github.com/XieYiping66/SFDA-rPPG</description><author>Yiping Xie, Zitong Yu, Bingjie Wu, Weicheng Xie, Linlin Shen</author><pubDate>Wed, 18 Sep 2024 14:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12040v1</guid></item><item><title>A Fisher-Rao gradient flow for entropic mean-field min-max games</title><link>http://arxiv.org/abs/2405.15834v2</link><description>Gradient flows play a substantial role in addressing many machine learningproblems. We examine the convergence in continuous-time of a\textit{Fisher-Rao} (Mean-Field Birth-Death) gradient flow in the context ofsolving convex-concave min-max games with entropy regularization. We proposeappropriate Lyapunov functions to demonstrate convergence with explicit ratesto the unique mixed Nash equilibrium.</description><author>Razvan-Andrei Lascu, Mateusz B. Majka, Åukasz Szpruch</author><pubDate>Wed, 18 Sep 2024 14:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15834v2</guid></item><item><title>A Unified Framework for Neural Computation and Learning Over Time</title><link>http://arxiv.org/abs/2409.12038v1</link><description>This paper proposes Hamiltonian Learning, a novel unified framework forlearning with neural networks "over time", i.e., from a possibly infinitestream of data, in an online manner, without having access to futureinformation. Existing works focus on the simplified setting in which the streamhas a known finite length or is segmented into smaller sequences, leveragingwell-established learning strategies from statistical machine learning. In thispaper, the problem of learning over time is rethought from scratch, leveragingtools from optimal control theory, which yield a unifying view of the temporaldynamics of neural computations and learning. Hamiltonian Learning is based ondifferential equations that: (i) can be integrated without the need of externalsoftware solvers; (ii) generalize the well-established notion of gradient-basedlearning in feed-forward and recurrent networks; (iii) open to novelperspectives. The proposed framework is showcased by experimentally proving howit can recover gradient-based learning, comparing it to out-of-the boxoptimizers, and describing how it is flexible enough to switch from fully-localto partially/non-local computational schemes, possibly distributed overmultiple devices, and BackPropagation without storing activations. HamiltonianLearning is easy to implement and can help researches approach in a principledand innovative manner the problem of learning over time.</description><author>Stefano Melacci, Alessandro Betti, Michele Casoni, Tommaso Guidi, Matteo Tiezzi, Marco Gori</author><pubDate>Wed, 18 Sep 2024 14:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12038v1</guid></item><item><title>Efficient 3D Instance Mapping and Localization with Neural Fields</title><link>http://arxiv.org/abs/2403.19797v3</link><description>We tackle the problem of learning an implicit scene representation for 3Dinstance segmentation from a sequence of posed RGB images. Towards this, weintroduce 3DIML, a novel framework that efficiently learns a neural label fieldwhich can render 3D instance segmentation masks from novel viewpoints. Opposedto prior art that optimizes a neural field in a self-supervised manner,requiring complicated training procedures and loss function design, 3DIMLleverages a two-phase process. The first phase, InstanceMap, takes as input 2Dsegmentation masks of the image sequence generated by a frontend instancesegmentation model, and associates corresponding masks across images to 3Dlabels. These almost 3D-consistent pseudolabel masks are then used in thesecond phase, InstanceLift, to supervise the training of a neural label field,which interpolates regions missed by InstanceMap and resolves ambiguities.Additionally, we introduce InstanceLoc, which enables near realtimelocalization of instance masks given a trained neural label field. We evaluate3DIML on sequences from the Replica and ScanNet datasets and demonstrate itseffectiveness under mild assumptions for the image sequences. We achieve alarge practical speedup over existing implicit scene representation methodswith comparable quality, showcasing its potential to facilitate faster and moreeffective 3D scene understanding.</description><author>George Tang, Krishna Murthy Jatavallabhula, Antonio Torralba</author><pubDate>Wed, 18 Sep 2024 14:56:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19797v3</guid></item><item><title>Multi-Sensor Deep Learning for Glacier Mapping</title><link>http://arxiv.org/abs/2409.12034v1</link><description>The more than 200,000 glaciers outside the ice sheets play a crucial role inour society by influencing sea-level rise, water resource management, naturalhazards, biodiversity, and tourism. However, only a fraction of these glaciersbenefit from consistent and detailed in-situ observations that allow forassessing their status and changes over time. This limitation can, in part, beovercome by relying on satellite-based Earth Observation techniques.Satellite-based glacier mapping applications have historically mainly relied onmanual and semi-automatic detection methods, while recently, a fast and notabletransition to deep learning techniques has started. This chapter reviews how combining multi-sensor remote sensing data and deeplearning allows us to better delineate (i.e. map) glaciers and detect theirtemporal changes. We explain how relying on deep learning multi-sensorframeworks to map glaciers benefits from the extensive availability of regionaland global glacier inventories. We also analyse the rationale behind glaciermapping, the benefits of deep learning methodologies, and the inherentchallenges in integrating multi-sensor earth observation data with deeplearning algorithms. While our review aims to provide a broad overview of glacier mapping efforts,we highlight a few setups where deep learning multi-sensor remote sensingapplications have a considerable potential added value. This includesapplications for debris-covered and rock glaciers that are visually difficultto distinguish from surroundings and for calving glaciers that are in contactwith the ocean. These specific cases are illustrated through a series of visualimageries, highlighting some significant advantages and challenges whendetecting glacier changes, including dealing with seasonal snow cover, changingdebris coverage, and distinguishing glacier fronts from the surrounding seaice.</description><author>CodruÅ£-Andrei Diaconu, Konrad Heidler, Jonathan L. Bamber, Harry Zekollari</author><pubDate>Wed, 18 Sep 2024 14:51:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12034v1</guid></item><item><title>NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change Detection in Heterogeneous Remote Sensing Images</title><link>http://arxiv.org/abs/2303.17448v2</link><description>Change detection (CD) in heterogeneous remote sensing images has been widelyused for disaster monitoring and land-use management. In the past decade, theheterogeneous CD problem has significantly benefited from the development ofdeep neural networks (DNNs). However, the purely data-driven DNNs perform likea black box where the lack of interpretability limits the trustworthiness andcontrollability of DNNs in most practical CD applications. As a powerfulknowledge-driven tool, copula theory performs well in modeling relationshipsamong random variables. To enhance the interpretability of existing neuralnetworks for CD, we propose a knowledge-data-driven heterogeneous CD methodbased on a copula-guided neural network, named NN-Copula-CD. In ourNN-Copula-CD, the mathematical characteristics of copula are employed as theloss functions to supervise a neural network to learn the dependence betweenbi-temporal heterogeneous superpixel pairs, and then the changed regions areidentified via binary classification based on the degrees of dependence of allthe superpixel pairs in the bi-temporal images. We conduct in-depth experimentson three datasets with heterogeneous images, including optical, syntheticaperture radar, multispectral, and near-infrared images, where bothquantitative and visual results demonstrate both the effectiveness andinterpretability of our proposed NN-Copula-CD method.</description><author>Weiming Li, Xueqian Wang, Gang Li, Baocheng Geng, Pramod K. Varshney</author><pubDate>Wed, 18 Sep 2024 14:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17448v2</guid></item><item><title>Topological Deep Learning with State-Space Models: A Mamba Approach for Simplicial Complexes</title><link>http://arxiv.org/abs/2409.12033v1</link><description>Graph Neural Networks based on the message-passing (MP) mechanism are adominant approach for handling graph-structured data. However, they areinherently limited to modeling only pairwise interactions, making it difficultto explicitly capture the complexity of systems with $n$-body relations. Toaddress this, topological deep learning has emerged as a promising field forstudying and modeling higher-order interactions using various topologicaldomains, such as simplicial and cellular complexes. While these new domainsprovide powerful representations, they introduce new challenges, such aseffectively modeling the interactions among higher-order structures throughhigher-order MP. Meanwhile, structured state-space sequence models have provento be effective for sequence modeling and have recently been adapted for graphdata by encoding the neighborhood of a node as a sequence, thereby avoiding theMP mechanism. In this work, we propose a novel architecture designed to operatewith simplicial complexes, utilizing the Mamba state-space model as itsbackbone. Our approach generates sequences for the nodes based on theneighboring cells, enabling direct communication between all higher-orderstructures, regardless of their rank. We extensively validate our model,demonstrating that it achieves competitive performance compared tostate-of-the-art models developed for simplicial complexes.</description><author>Marco Montagna, Simone Scardapane, Lev Telyatnikov</author><pubDate>Wed, 18 Sep 2024 14:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12033v1</guid></item><item><title>PhysMamba: Efficient Remote Physiological Measurement with SlowFast Temporal Difference Mamba</title><link>http://arxiv.org/abs/2409.12031v1</link><description>Facial-video based Remote photoplethysmography (rPPG) aims at measuringphysiological signals and monitoring heart activity without any contact,showing significant potential in various applications. Previous deep learningbased rPPG measurement are primarily based on CNNs and Transformers. However,the limited receptive fields of CNNs restrict their ability to capturelong-range spatio-temporal dependencies, while Transformers also struggle withmodeling long video sequences with high complexity. Recently, the state spacemodels (SSMs) represented by Mamba are known for their impressive performanceon capturing long-range dependencies from long sequences. In this paper, wepropose the PhysMamba, a Mamba-based framework, to efficiently representlong-range physiological dependencies from facial videos. Specifically, weintroduce the Temporal Difference Mamba block to first enhance local dynamicdifferences and further model the long-range spatio-temporal context. Moreover,a dual-stream SlowFast architecture is utilized to fuse the multi-scaletemporal features. Extensive experiments are conducted on three benchmarkdatasets to demonstrate the superiority and efficiency of PhysMamba. The codesare available at https://github.com/Chaoqi31/PhysMamba</description><author>Chaoqi Luo, Yiping Xie, Zitong Yu</author><pubDate>Wed, 18 Sep 2024 14:48:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12031v1</guid></item><item><title>Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models</title><link>http://arxiv.org/abs/2405.14496v2</link><description>Learning the unique directed acyclic graph corresponding to an unknown causalmodel is a challenging task. Methods based on functional causal models canidentify a unique graph, but either suffer from the curse of dimensionality orimpose strong parametric assumptions. To address these challenges, we propose anovel hybrid approach for global causal discovery in observational data thatleverages local causal substructures. We first present a topological sortingalgorithm that leverages ancestral relationships in linear structural equationmodels to establish a compact top-down hierarchical ordering, encoding morecausal information than linear orderings produced by existing methods. Wedemonstrate that this approach generalizes to nonlinear settings with arbitrarynoise. We then introduce a nonparametric constraint-based algorithm that prunesspurious edges by searching for local conditioning sets, achieving greateraccuracy than current methods. We provide theoretical guarantees forcorrectness and worst-case polynomial time complexities, with empiricalvalidation on synthetic data.</description><author>Sujai Hiremath, Jacqueline R. M. A. Maasch, Mengxiao Gao, Promit Ghosal, Kyra Gan</author><pubDate>Wed, 18 Sep 2024 14:43:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14496v2</guid></item><item><title>The Ethics of AI Value Chains</title><link>http://arxiv.org/abs/2307.16787v3</link><description>Researchers, practitioners, and policymakers with an interest in AI ethicsneed more integrative approaches for studying and intervening in AI systemsacross many contexts and scales of activity. This paper presents AI valuechains as an integrative concept that satisfies that need. To more clearlytheorize AI value chains and conceptually distinguish them from supply chains,we review theories of value chains and AI value chains from the strategicmanagement, service science, economic geography, industry, government, andapplied research literature. We then conduct an integrative review of a sampleof 67 sources that cover the ethical concerns implicated in AI value chains.Building upon the findings of our integrative review, we recommend three futuredirections that researchers, practitioners, and policymakers can take toadvance more ethical practices across AI value chains. We urge AI ethicsresearchers and practitioners to move toward value chain perspectives thatsituate actors in context, account for the many types of resources involved inco-creating AI systems, and integrate a wider range of ethical concerns acrosscontexts and scales.</description><author>Blair Attard-Frost, David Gray Widder</author><pubDate>Wed, 18 Sep 2024 14:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16787v3</guid></item><item><title>On Vision Transformers for Classification Tasks in Side-Scan Sonar Imagery</title><link>http://arxiv.org/abs/2409.12026v1</link><description>Side-scan sonar (SSS) imagery presents unique challenges in theclassification of man-made objects on the seafloor due to the complex andvaried underwater environments. Historically, experts have manually interpretedSSS images, relying on conventional machine learning techniques withhand-crafted features. While Convolutional Neural Networks (CNNs) significantlyadvanced automated classification in this domain, they often fall short whendealing with diverse seafloor textures, such as rocky or ripple sand bottoms,where false positive rates may increase. Recently, Vision Transformers (ViTs)have shown potential in addressing these limitations by utilizing aself-attention mechanism to capture global information in image patches,offering more flexibility in processing spatial hierarchies. This paperrigorously compares the performance of ViT models alongside commonly used CNNarchitectures, such as ResNet and ConvNext, for binary classification tasks inSSS imagery. The dataset encompasses diverse geographical seafloor types and isbalanced between the presence and absence of man-made objects. ViT-based modelsexhibit superior classification performance across f1-score, precision, recall,and accuracy metrics, although at the cost of greater computational resources.CNNs, with their inductive biases, demonstrate better computational efficiency,making them suitable for deployment in resource-constrained environments likeunderwater vehicles. Future research directions include exploringself-supervised learning for ViTs and multi-modal fusion to further enhanceperformance in challenging underwater environments.</description><author>BW Sheffield, Jeffrey Ellen, Ben Whitmore</author><pubDate>Wed, 18 Sep 2024 14:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12026v1</guid></item><item><title>Mitigating Urban-Rural Disparities in Contrastive Representation Learning with Satellite Imagery</title><link>http://arxiv.org/abs/2211.08672v3</link><description>Satellite imagery is being leveraged for many societally critical tasksacross climate, economics, and public health. Yet, because of heterogeneity inlandscapes (e.g. how a road looks in different places), models can showdisparate performance across geographic areas. Given the important potential ofdisparities in algorithmic systems used in societal contexts, here we considerthe risk of urban-rural disparities in identification of land-cover features.This is via semantic segmentation (a common computer vision task in which imageregions are labelled according to what is being shown) which uses pre-trainedimage representations generated via contrastive self-supervised learning. Wepropose fair dense representation with contrastive learning (FairDCL) as amethod for de-biasing the multi-level latent space of convolution neuralnetwork models. The method improves feature identification by removing spuriousmodel representations which are disparately distributed across urban and ruralareas, and is achieved in an unsupervised way by contrastive pre-training. Theobtained image representation mitigates downstream urban-rural predictiondisparities and outperforms state-of-the-art baselines on real-world satelliteimages. Embedding space evaluation and ablation studies further demonstrateFairDCL's robustness. As generalizability and robustness in geographic imageryis a nascent topic, our work motivates researchers to consider metrics beyondaverage accuracy in such applications.</description><author>Miao Zhang, Rumi Chunara</author><pubDate>Wed, 18 Sep 2024 14:35:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08672v3</guid></item><item><title>LEMON: Localized Editing with Mesh Optimization and Neural Shaders</title><link>http://arxiv.org/abs/2409.12024v1</link><description>In practical use cases, polygonal mesh editing can be faster than generatingnew ones, but it can still be challenging and time-consuming for users.Existing solutions for this problem tend to focus on a single task, eithergeometry or novel view synthesis, which often leads to disjointed resultsbetween the mesh and view. In this work, we propose LEMON, a mesh editingpipeline that combines neural deferred shading with localized meshoptimization. Our approach begins by identifying the most important vertices inthe mesh for editing, utilizing a segmentation model to focus on these keyregions. Given multi-view images of an object, we optimize a neural shader anda polygonal mesh while extracting the normal map and the rendered image fromeach view. By using these outputs as conditioning data, we edit the inputimages with a text-to-image diffusion model and iteratively update our datasetwhile deforming the mesh. This process results in a polygonal mesh that isedited according to the given text instruction, preserving the geometriccharacteristics of the initial mesh while focusing on the most significantareas. We evaluate our pipeline using the DTU dataset, demonstrating that itgenerates finely-edited meshes more rapidly than the current state-of-the-artmethods. We include our code and additional results in the supplementarymaterial.</description><author>Furkan Mert Algan, Umut Yazgan, Driton Salihu, Cem Eteke, Eckehard Steinbach</author><pubDate>Wed, 18 Sep 2024 14:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12024v1</guid></item><item><title>Promise and Peril of Collaborative Code Generation Models: Balancing Effectiveness and Memorization</title><link>http://arxiv.org/abs/2409.12020v1</link><description>In the rapidly evolving field of machine learning, training models withdatasets from various locations and organizations presents significantchallenges due to privacy and legal concerns. The exploration of effectivecollaborative training settings capable of leveraging valuable knowledge fromdistributed and isolated datasets is increasingly crucial. This studyinvestigates key factors that impact the effectiveness of collaborativetraining methods in code next-token prediction, as well as the correctness andutility of the generated code, demonstrating the promise of such methods.Additionally, we evaluate the memorization of different participant trainingdata across various collaborative training settings, including centralized,federated, and incremental training, highlighting their potential risks inleaking data. Our findings indicate that the size and diversity of codedatasets are pivotal factors influencing the success of collaboratively trainedcode models. We show that federated learning achieves competitive performancecompared to centralized training while offering better data protection, asevidenced by lower memorization ratios in the generated code. However,federated learning can still produce verbatim code snippets from hiddentraining data, potentially violating privacy or copyright. Our study furtherexplores effectiveness and memorization patterns in incremental learning,emphasizing the sequence in which individual participant datasets areintroduced. We also identify cross-organizational clones as a prevalentchallenge in both centralized and federated learning scenarios. Our findingshighlight the persistent risk of data leakage during inference, even whentraining data remains unseen. We conclude with recommendations forpractitioners and researchers to optimize multisource datasets, propellingcross-organizational collaboration forward.</description><author>Zhi Chen, Lingxiao Jiang</author><pubDate>Wed, 18 Sep 2024 14:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12020v1</guid></item><item><title>Computational Imaging for Long-Term Prediction of Solar Irradiance</title><link>http://arxiv.org/abs/2409.12016v1</link><description>The occlusion of the sun by clouds is one of the primary sources ofuncertainties in solar power generation, and is a factor that affects thewide-spread use of solar power as a primary energy source. Real-timeforecasting of cloud movement and, as a result, solar irradiance is necessaryto schedule and allocate energy across grid-connected photovoltaic systems.Previous works monitored cloud movement using wide-angle field of view imageryof the sky. However, such images have poor resolution for clouds that appearnear the horizon, which reduces their effectiveness for long term prediction ofsolar occlusion. Specifically, to be able to predict occlusion of the sun overlong time periods, clouds that are near the horizon need to be detected, andtheir velocities estimated precisely. To enable such a system, we design anddeploy a catadioptric system that delivers wide-angle imagery with uniformspatial resolution of the sky over its field of view. To enable prediction overa longer time horizon, we design an algorithm that uses carefully selectedspatio-temporal slices of the imagery using estimated wind direction andvelocity as inputs. Using ray-tracing simulations as well as a real testbeddeployed outdoors, we show that the system is capable of predicting solarocclusion as well as irradiance for tens of minutes in the future, which is anorder of magnitude improvement over prior work.</description><author>Leron Julian, Haejoon Lee, Soummya Kar, Aswin C. Sankaranarayanan</author><pubDate>Wed, 18 Sep 2024 14:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12016v1</guid></item><item><title>All-in-one foundational models learning across quantum chemical levels</title><link>http://arxiv.org/abs/2409.12015v1</link><description>Machine learning (ML) potentials typically target a single quantum chemical(QC) level while the ML models developed for multi-fidelity learning have notbeen shown to provide scalable solutions for foundational models. Here weintroduce the all-in-one (AIO) ANI model architecture based on multimodallearning which can learn an arbitrary number of QC levels. Our all-in-onelearning approach offers a more general and easier-to-use alternative totransfer learning. We use it to train the AIO-ANI-UIP foundational model withthe generalization capability comparable to semi-empirical GFN2-xTB and DFTwith a double-zeta basis set for organic molecules. We show that the AIO-ANImodel can learn across different QC levels ranging from semi-empirical todensity functional theory to coupled cluster. We also use AIO models to designthe foundational model {\Delta}-AIO-ANI based on {\Delta}-learning withincreased accuracy and robustness compared to AIO-ANI-UIP. The code and thefoundational models are available at https://github.com/dralgroup/aio-ani; theywill be integrated into the universal and updatable AI-enhanced QM (UAIQM)library and made available in the MLatom package so that they can be usedonline at the XACS cloud computing platform (seehttps://github.com/dralgroup/mlatom for updates).</description><author>Yuxinxin Chen, Pavlo O. Dral</author><pubDate>Wed, 18 Sep 2024 14:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12015v1</guid></item><item><title>BRDF-NeRF: Neural Radiance Fields with Optical Satellite Images and BRDF Modelling</title><link>http://arxiv.org/abs/2409.12014v1</link><description>Understanding the anisotropic reflectance of complex Earth surfaces fromsatellite imagery is crucial for numerous applications. Neural radiance fields(NeRF) have become popular as a machine learning technique capable of deducingthe bidirectional reflectance distribution function (BRDF) of a scene frommultiple images. However, prior research has largely concentrated on applyingNeRF to close-range imagery, estimating basic Microfacet BRDF models, whichfall short for many Earth surfaces. Moreover, high-quality NeRFs generallyrequire several images captured simultaneously, a rare occurrence in satelliteimaging. To address these limitations, we propose BRDF-NeRF, developed toexplicitly estimate the Rahman-Pinty-Verstraete (RPV) model, a semi-empiricalBRDF model commonly employed in remote sensing. We assess our approach usingtwo datasets: (1) Djibouti, captured in a single epoch at varying viewingangles with a fixed Sun position, and (2) Lanzhou, captured over multipleepochs with different viewing angles and Sun positions. Our results, based ononly three to four satellite images for training, demonstrate that BRDF-NeRFcan effectively synthesize novel views from directions far removed from thetraining data and produce high-quality digital surface models (DSMs).</description><author>Lulin Zhang, Ewelina Rupnik, Tri Dung Nguyen, StÃ©phane Jacquemoud, Yann Klinger</author><pubDate>Wed, 18 Sep 2024 14:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12014v1</guid></item><item><title>Checklist to Define the Identification of TP, FP, and FN Object Detections in Automated Driving</title><link>http://arxiv.org/abs/2308.07106v2</link><description>The object perception of automated driving systems must pass quality androbustness tests before a safe deployment. Such tests typically identify truepositive (TP), false-positive (FP), and false-negative (FN) detections andaggregate them to metrics. Since the literature seems to be lacking acomprehensive way to define the identification of TPs/FPs/FNs, this paperprovides a checklist of relevant functional aspects and implementation details.Besides labeling policies of the test set, we cover areas of vision, occlusionhandling, safety-relevant areas, matching criteria, temporal and probabilisticissues, and further aspects. Even though the checklist cannot be fullyformalized, it can help practitioners minimize the ambiguity of their tests,which, in turn, makes statements on object perception more reliable andcomparable.</description><author>Michael Hoss</author><pubDate>Wed, 18 Sep 2024 14:27:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07106v2</guid></item><item><title>Measuring Dimensions of Self-Presentation in Twitter Bios and their Links to Misinformation Sharing</title><link>http://arxiv.org/abs/2305.09548v4</link><description>Social media platforms provide users with a profile description field,commonly known as a ``bio," where they can present themselves to the world. Agrowing literature shows that text in these bios can improve our understandingof online self-presentation and behavior, but existing work relies exclusivelyon keyword-based approaches to do so. We here propose and evaluate a suite of\hl{simple, effective, and theoretically motivated} approaches to embed bios inspaces that capture salient dimensions of social meaning, such as age andpartisanship. We \hl{evaluate our methods on four tasks, showing that thestrongest one out-performs several practical baselines.} We then show theutility of our method in helping understand associations betweenself-presentation and the sharing of URLs from low-quality news sites onTwitter\hl{, with a particular focus on explore the interactions between ageand partisanship, and exploring the effects of self-presentations ofreligiosity}. Our work provides new tools to help computational socialscientists make use of information in bios, and provides new insights into howmisinformation sharing may be perceived on Twitter.</description><author>Navid Madani, Rabiraj Bandyopadhyay, Briony Swire-Thompson, Michael Miller Yoder, Kenneth Joseph</author><pubDate>Wed, 18 Sep 2024 14:26:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09548v4</guid></item></channel></rss>