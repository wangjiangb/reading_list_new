<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 24 Aug 2025 17:37:49 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</title><link>http://arxiv.org/abs/2508.15774v1</link><description>Visual diffusion models achieve remarkable progress, yet they are typicallytrained at limited resolutions due to the lack of high-resolution data andconstrained computation resources, hampering their ability to generatehigh-fidelity images or videos at higher resolutions. Recent efforts haveexplored tuning-free strategies to exhibit the untapped potentialhigher-resolution visual generation of pre-trained models. However, thesemethods are still prone to producing low-quality visual content with repetitivepatterns. The key obstacle lies in the inevitable increase in high-frequencyinformation when the model generates visual content exceeding its trainingresolution, leading to undesirable repetitive patterns deriving from theaccumulated errors. In this work, we propose CineScale, a novel inferenceparadigm to enable higher-resolution visual generation. To tackle the variousissues introduced by the two types of video generation architectures, wepropose dedicated variants tailored to each. Unlike existing baseline methodsthat are confined to high-resolution T2I and T2V generation, CineScale broadensthe scope by enabling high-resolution I2V and V2V synthesis, built atopstate-of-the-art open-source video generation frameworks. Extensive experimentsvalidate the superiority of our paradigm in extending the capabilities ofhigher-resolution visual generation for both image and video models.Remarkably, our approach enables 8k image generation without any fine-tuning,and achieves 4k video generation with only minimal LoRA fine-tuning. Generatedvideo samples are available at our website:https://eyeline-labs.github.io/CineScale/.</description><author>Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu</author><pubDate>Thu, 21 Aug 2025 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15774v1</guid></item><item><title>Scaling Group Inference for Diverse and High-Quality Generation</title><link>http://arxiv.org/abs/2508.15773v1</link><description>Generative models typically sample outputs independently, and recentinference-time guidance and scaling algorithms focus on improving the qualityof individual samples. However, in real-world applications, users are oftenpresented with a set of multiple images (e.g., 4-8) for each prompt, whereindependent sampling tends to lead to redundant results, limiting user choicesand hindering idea exploration. In this work, we introduce a scalable groupinference method that improves both the diversity and quality of a group ofsamples. We formulate group inference as a quadratic integer assignmentproblem: candidate outputs are modeled as graph nodes, and a subset is selectedto optimize sample quality (unary term) while maximizing group diversity(binary term). To substantially improve runtime efficiency, we progressivelyprune the candidate set using intermediate predictions, allowing our method toscale up to large candidate sets. Extensive experiments show that our methodsignificantly improves group diversity and quality compared to independentsampling baselines and recent inference algorithms. Our framework generalizesacross a wide range of tasks, including text-to-image, image-to-image, imageprompting, and video generation, enabling generative models to treat multipleoutputs as cohesive groups rather than independent samples.</description><author>Gaurav Parmar, Or Patashnik, Daniil Ostashev, Kuan-Chieh Wang, Kfir Aberman, Srinivasa Narasimhan, Jun-Yan Zhu</author><pubDate>Thu, 21 Aug 2025 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15773v1</guid></item><item><title>Visual Autoregressive Modeling for Instruction-Guided Image Editing</title><link>http://arxiv.org/abs/2508.15772v1</link><description>Recent advances in diffusion models have brought remarkable visual fidelityto instruction-guided image editing. However, their global denoising processinherently entangles the edited region with the entire image context, leadingto unintended spurious modifications and compromised adherence to editinginstructions. In contrast, autoregressive models offer a distinct paradigm byformulating image synthesis as a sequential process over discrete visualtokens. Their causal and compositional mechanism naturally circumvents theadherence challenges of diffusion-based methods. In this paper, we presentVAREdit, a visual autoregressive (VAR) framework that reframes image editing asa next-scale prediction problem. Conditioned on source image features and textinstructions, VAREdit generates multi-scale target features to achieve preciseedits. A core challenge in this paradigm is how to effectively condition thesource image tokens. We observe that finest-scale source features cannoteffectively guide the prediction of coarser target features. To bridge thisgap, we introduce a Scale-Aligned Reference (SAR) module, which injectsscale-matched conditioning information into the first self-attention layer.VAREdit demonstrates significant advancements in both editing adherence andefficiency. On standard benchmarks, it outperforms leading diffusion-basedmethods by 30\%+ higher GPT-Balance score. Moreover, it completes a$512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than thesimilarly sized UltraEdit. The models are available athttps://github.com/HiDream-ai/VAREdit.</description><author>Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei</author><pubDate>Thu, 21 Aug 2025 17:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15772v1</guid></item><item><title>SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</title><link>http://arxiv.org/abs/2508.15769v1</link><description>3D content generation has recently attracted significant research interestdue to its applications in VR/AR and embodied AI. In this work, we address thechallenging task of synthesizing multiple 3D assets within a single sceneimage. Concretely, our contributions are fourfold: (i) we present SceneGen, anovel framework that takes a scene image and corresponding object masks asinput, simultaneously producing multiple 3D assets with geometry and texture.Notably, SceneGen operates with no need for optimization or asset retrieval;(ii) we introduce a novel feature aggregation module that integrates local andglobal scene information from visual and geometric encoders within the featureextraction module. Coupled with a position head, this enables the generation of3D assets and their relative spatial positions in a single feedforward pass;(iii) we demonstrate SceneGen's direct extensibility to multi-image inputscenarios. Despite being trained solely on single-image inputs, ourarchitectural design enables improved generation performance with multi-imageinputs; and (iv) extensive quantitative and qualitative evaluations confirm theefficiency and robust generation abilities of our approach. We believe thisparadigm offers a novel solution for high-quality 3D content generation,potentially advancing its practical applications in downstream tasks. The codeand model will be publicly available at: https://mengmouxu.github.io/SceneGen.</description><author>Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie</author><pubDate>Thu, 21 Aug 2025 17:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15769v1</guid></item><item><title>ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling</title><link>http://arxiv.org/abs/2508.15767v1</link><description>Parametric body models offer expressive 3D representation of humans across awide range of poses, shapes, and facial expressions, typically derived bylearning a basis over registered 3D meshes. However, existing human meshmodeling approaches struggle to capture detailed variations across diverse bodyposes and shapes, largely due to limited training data diversity andrestrictive modeling assumptions. Moreover, the common paradigm first optimizesthe external body surface using a linear basis, then regresses internalskeletal joints from surface vertices. This approach introduces problematicdependencies between internal skeleton and outer soft tissue, limiting directcontrol over body height and bone lengths. To address these issues, we presentATLAS, a high-fidelity body model learned from 600k high-resolution scanscaptured using 240 synchronized cameras. Unlike previous methods, we explicitlydecouple the shape and skeleton bases by grounding our mesh representation inthe human skeleton. This decoupling enables enhanced shape expressivity,fine-grained customization of body attributes, and keypoint fitting independentof external soft-tissue characteristics. ATLAS outperforms existing methods byfitting unseen subjects in diverse poses more accurately, and quantitativeevaluations show that our non-linear pose correctives more effectively capturecomplex poses compared to linear models.</description><author>Jinhyung Park, Javier Romero, Shunsuke Saito, Fabian Prada, Takaaki Shiratori, Yichen Xu, Federica Bogo, Shoou-I Yu, Kris Kitani, Rawal Khirodkar</author><pubDate>Thu, 21 Aug 2025 17:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15767v1</guid></item><item><title>Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO</title><link>http://arxiv.org/abs/2508.15766v1</link><description>Recent efforts have extended the capabilities of transformers in logicalreasoning and symbolic computations. In this work, we investigate theircapacity for non-linear latent pattern discovery in the context of functionaldecomposition, focusing on the challenging algebraic task of multivariatepolynomial decomposition. This problem, with widespread applications in scienceand engineering, is proved to be NP-hard, and demands both precision andinsight. Our contributions are threefold: First, we develop a synthetic datageneration pipeline providing fine-grained control over problem complexity.Second, we train transformer models via supervised learning and evaluate themacross four key dimensions involving scaling behavior and generalizability.Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), arank-aware reinforcement learning method suitable for hard algebraic problems.Finetuning with BGRPO improves accuracy while reducing beam width by up tohalf, resulting in approximately 75% lower inference compute. Additionally, ourmodel demonstrates competitive performance in polynomial simplification,outperforming Mathematica in various cases.</description><author>Jaeha Lee, Gio Huh, Ning Su, Tony Yue YU</author><pubDate>Thu, 21 Aug 2025 17:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15766v1</guid></item><item><title>Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space</title><link>http://arxiv.org/abs/2508.15764v1</link><description>We address the problem of detecting adversarial attacks against cooperativemulti-agent reinforcement learning with continuous action space. We propose adecentralized detector that relies solely on the local observations of theagents and makes use of a statistical characterization of the normal behaviorof observable agents. The proposed detector utilizes deep neural networks toapproximate the normal behavior of agents as parametric multivariate Gaussiandistributions. Based on the predicted density functions, we define a normalityscore and provide a characterization of its mean and variance. Thischaracterization allows us to employ a two-sided CUSUM procedure for detectingdeviations of the normality score from its mean, serving as a detector ofanomalous behavior in real-time. We evaluate our scheme on various multi-agentPettingZoo benchmarks against different state-of-the-art attack methods, andour results demonstrate the effectiveness of our method in detecting impactfuladversarial attacks. Particularly, it outperforms the discrete counterpart byachieving AUC-ROC scores of over 0.95 against the most impactful attacks in allevaluated environments.</description><author>Kiarash Kazari, Ezzeldin Shereen, György Dán</author><pubDate>Thu, 21 Aug 2025 17:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15764v1</guid></item><item><title>Intern-S1: A Scientific Multimodal Foundation Model</title><link>http://arxiv.org/abs/2508.15763v1</link><description>In recent years, a plethora of open-source foundation models have emerged,achieving remarkable progress in some widely attended fields, with performancebeing quite close to that of closed-source models. However, in high-value butmore challenging scientific professional fields, either the fields still relyon expert models, or the progress of general foundation models lagssignificantly compared to those in popular areas, far from sufficient fortransforming scientific research and leaving substantial gap betweenopen-source models and closed-source models in these scientific domains. Tomitigate this gap and explore a step further toward Artificial GeneralIntelligence (AGI), we introduce Intern-S1, a specialized generalist equippedwith general understanding and reasoning capabilities with expertise to analyzemultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)model with 28 billion activated parameters and 241 billion total parameters,continually pre-trained on 5T tokens, including over 2.5T tokens fromscientific domains. In the post-training stage, Intern-S1 undergoes offline andthen online reinforcement learning (RL) in InternBootCamp, where we proposeMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 taskssimultaneously. Through integrated innovations in algorithms, data, andtraining systems, Intern-S1 achieved top-tier performance in online RLtraining.On comprehensive evaluation benchmarks, Intern-S1 demonstratescompetitive performance on general reasoning tasks among open-source models andsignificantly outperforms open-source models in scientific domains, surpassingclosed-source state-of-the-art models in professional tasks, such as molecularsynthesis planning, reaction condition prediction, predicting thermodynamicstabilities for crystals. Our models are available athttps://huggingface.co/internlm/Intern-S1.</description><author>Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Ju</author><pubDate>Thu, 21 Aug 2025 17:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15763v1</guid></item><item><title>Waver: Wave Your Way to Lifelike Video Generation</title><link>http://arxiv.org/abs/2508.15761v1</link><description>We present Waver, a high-performance foundation model for unified image andvideo generation. Waver can directly generate videos with durations rangingfrom 5 to 10 seconds at a native resolution of 720p, which are subsequentlyupscaled to 1080p. The model simultaneously supports text-to-video (T2V),image-to-video (I2V), and text-to-image (T2I) generation within a single,integrated framework. We introduce a Hybrid Stream DiT architecture to enhancemodality alignment and accelerate training convergence. To ensure training dataquality, we establish a comprehensive data curation pipeline and manuallyannotate and train an MLLM-based video quality model to filter for thehighest-quality samples. Furthermore, we provide detailed training andinference recipes to facilitate the generation of high-quality videos. Buildingon these contributions, Waver excels at capturing complex motion, achievingsuperior motion amplitude and temporal consistency in video synthesis. Notably,it ranks among the Top 3 on both the T2V and I2V leaderboards at ArtificialAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperformingexisting open-source models and matching or surpassing state-of-the-artcommercial solutions. We hope this technical report will help the communitymore efficiently train high-quality video generation models and accelerateprogress in video generation technologies. Official page:https://github.com/FoundationVision/Waver.</description><author>Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng</author><pubDate>Thu, 21 Aug 2025 17:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15761v1</guid></item><item><title>LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries</title><link>http://arxiv.org/abs/2508.15760v1</link><description>Tool calling has emerged as a critical capability for AI agents to interactwith the real world and solve complex tasks. While the Model Context Protocol(MCP) provides a powerful standardized framework for tool integration, there isa significant gap in benchmarking how well AI agents can effectively solvemulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. Inthis work, we present LiveMCP-101, a benchmark of 101 carefully curatedreal-world queries, refined through iterative LLM rewriting and manual review,that require coordinated use of multiple MCP tools including web search, fileoperations, mathematical reasoning, and data analysis. Moreover, we introduce anovel evaluation approach that leverages ground-truth execution plans ratherthan raw API outputs, better reflecting the evolving nature of real-worldenvironments. Experiments show that even frontier LLMs achieve a success ratebelow 60\%, highlighting major challenges in tool orchestration. Detailedablations and error analysis further reveal distinct failure modes andinefficiencies in token usage, pointing to concrete directions for advancingcurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-worldagent capabilities, advancing toward autonomous AI systems that reliablyexecute complex tasks through tool use.</description><author>Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song</author><pubDate>Thu, 21 Aug 2025 17:55:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15760v1</guid></item><item><title>Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces</title><link>http://arxiv.org/abs/2507.09709v2</link><description>Understanding the latent space geometry of large language models (LLMs) iskey to interpreting their behavior and improving alignment. However, it remainsunclear to what extent LLMs internally organize representations related tosemantic understanding. To explore this, we conduct a large-scale empiricalstudy of hidden representations in 11 autoregressive models across 6 scientifictopics. We find that high-level semantic information consistently resides inlow-dimensional subspaces that form linearly separable representations acrossdomains. This separability becomes more pronounced in deeper layers and underprompts that elicit structured reasoning or alignmentbehavior$\unicode{x2013}$even when surface content remains unchanged. Thesefindings support geometry-aware tools that operate directly in latent space todetect and mitigate harmful or adversarial content. As a proof of concept, wetrain an MLP probe on final-layer hidden states to act as a lightweightlatent-space guardrail. This approach substantially improves refusal rates onmalicious queries and prompt injections that bypass both the model's built-insafety alignment and external token-level filters.</description><author>Baturay Saglam, Paul Kassianik, Blaine Nelson, Sajana Weerawardhena, Yaron Singer, Amin Karbasi</author><pubDate>Thu, 21 Aug 2025 17:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.09709v2</guid></item><item><title>Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback</title><link>http://arxiv.org/abs/2508.15757v1</link><description>Configuration optimization remains a critical bottleneck in machine learning,requiring coordinated tuning across model architecture, training strategy,feature engineering, and hyperparameters. Traditional approaches treat thesedimensions independently and lack interpretability, while recent automatedmethods struggle with dynamic adaptability and semantic reasoning aboutoptimization decisions. We introduce Language-Guided Tuning (LGT), a novelframework that employs multi-agent Large Language Models to intelligentlyoptimize configurations through natural language reasoning. We apply textualgradients - qualitative feedback signals that complement numerical optimizationby providing semantic understanding of training dynamics and configurationinterdependencies. LGT coordinates three specialized agents: an Advisor thatproposes configuration changes, an Evaluator that assesses progress, and anOptimizer that refines the decision-making process, creating a self-improvingfeedback loop. Through comprehensive evaluation on six diverse datasets, LGTdemonstrates substantial improvements over traditional optimization methods,achieving performance gains while maintaining high interpretability.</description><author>Yuxing Lu, Yucheng Hu, Nan Sun, Xukai Zhao</author><pubDate>Thu, 21 Aug 2025 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15757v1</guid></item><item><title>Neural Robot Dynamics</title><link>http://arxiv.org/abs/2508.15755v1</link><description>Accurate and efficient simulation of modern robots remains challenging due totheir high degrees of freedom and intricate mechanisms. Neural simulators haveemerged as a promising alternative to traditional analytical simulators,capable of efficiently predicting complex dynamics and adapting to real-worlddata; however, existing neural simulators typically requireapplication-specific training and fail to generalize to novel tasks and/orenvironments, primarily due to inadequate representations of the global state.In this work, we address the problem of learning generalizable neuralsimulators for robots that are structured as articulated rigid bodies. Wepropose NeRD (Neural Robot Dynamics), learned robot-specific dynamics modelsfor predicting future states for articulated rigid bodies under contactconstraints. NeRD uniquely replaces the low-level dynamics and contact solversin an analytical simulator and employs a robot-centric and spatially-invariantsimulation state representation. We integrate the learned NeRD models as aninterchangeable backend solver within a state-of-the-art robotics simulator. Weconduct extensive experiments to show that the NeRD simulators are stable andaccurate over a thousand simulation steps; generalize across tasks andenvironment configurations; enable policy learning exclusively in a neuralengine; and, unlike most classical simulators, can be fine-tuned fromreal-world data to bridge the gap between simulation and reality.</description><author>Jie Xu, Eric Heiden, Iretiayo Akinola, Dieter Fox, Miles Macklin, Yashraj Narang</author><pubDate>Thu, 21 Aug 2025 17:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15755v1</guid></item><item><title>Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes</title><link>http://arxiv.org/abs/2508.05469v2</link><description>We study evaluation of AI systems without ground truth by exploiting a linkbetween strategic gaming and information loss. We analyze whichinformation-theoretic mechanisms resist adversarial manipulation, extendingfinite-sample bounds to show that bounded f-divergences (e.g., total variationdistance) maintain polynomial guarantees under attacks while unbounded measures(e.g., KL divergence) degrade exponentially. To implement these mechanisms, wemodel the overseer as an agent and characterize incentive-compatible scoringrules as f-mutual information objectives. Under adversarial attacks, TVD-MImaintains effectiveness (area under curve 0.70-0.77) while traditional judgequeries are near change (AUC $\approx$ 0.50), demonstrating that querying thesame LLM for information relationships rather than quality judgments providesboth theoretical and practical robustness. The mechanisms decompose pairwiseevaluations into reliable item-level quality scores without ground truth,addressing a key limitation of traditional peer prediction. We releasepreregistration and code.</description><author>Zachary Robertson, Sanmi Koyejo</author><pubDate>Thu, 21 Aug 2025 17:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.05469v2</guid></item><item><title>Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis</title><link>http://arxiv.org/abs/2508.15754v1</link><description>Large Language Models (LLMs) have made significant strides in reasoning tasksthrough methods like chain-of-thought (CoT) reasoning. However, they often fallshort in tasks requiring precise computations. Tool-Integrated Reasoning (TIR)has emerged as a solution by incorporating external tools into the reasoningprocess. Nevertheless, the generalization of TIR in improving the reasoningability of LLM is still unclear. Additionally, whether TIR has improved themodel's reasoning behavior and helped the model think remains to be studied. Weintroduce ReasonZoo, a comprehensive benchmark encompassing nine diversereasoning categories, to evaluate the effectiveness of TIR across variousdomains. Additionally, we propose two novel metrics, Performance-Aware Cost(PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoningefficiency. Our empirical evaluation demonstrates that TIR-enabled modelsconsistently outperform their non-TIR counterparts in both mathematical andnon-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, asevidenced by improved PAC and AUC-PCC, indicating reduced overthinking and morestreamlined reasoning. These findings underscore the domain-general benefits ofTIR and its potential to advance LLM capabilities in complex reasoning tasks.</description><author>Yufeng Zhao, Junnan Liu, Hongwei Liu, Dongsheng Zhu, Yuan Shen, Songyang Zhang, Kai Chen</author><pubDate>Thu, 21 Aug 2025 17:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15754v1</guid></item><item><title>"Does the cafe entrance look accessible? Where is the door?" Towards Geospatial AI Agents for Visual Inquiries</title><link>http://arxiv.org/abs/2508.15752v1</link><description>Interactive digital maps have revolutionized how people travel and learnabout the world; however, they rely on pre-existing structured data in GISdatabases (e.g., road networks, POI indices), limiting their ability to addressgeo-visual questions related to what the world looks like. We introduce ourvision for Geo-Visual Agents--multimodal AI agents capable of understanding andresponding to nuanced visual-spatial inquiries about the world by analyzinglarge-scale repositories of geospatial images, including streetscapes (e.g.,Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerialimagery (e.g., satellite photos) combined with traditional GIS data sources. Wedefine our vision, describe sensing and interaction approaches, provide threeexemplars, and enumerate key challenges and opportunities for future work.</description><author>Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. O'Meara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane</author><pubDate>Thu, 21 Aug 2025 17:49:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15752v1</guid></item><item><title>Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model</title><link>http://arxiv.org/abs/2508.15751v1</link><description>Purpose: Recent developments in computational pathology have been driven byadvances in Vision Foundation Models, particularly the Segment Anything Model(SAM). This model facilitates nuclei segmentation through two primary methods:prompt-based zero-shot segmentation and the use of cell-specific SAM models fordirect segmentation. These approaches enable effective segmentation across arange of nuclei and cells. However, general vision foundation models often facechallenges with fine-grained semantic segmentation, such as identifyingspecific nuclei subtypes or particular cells. Approach: In this paper, wepropose the molecular-empowered All-in-SAM Model to advance computationalpathology by leveraging the capabilities of vision foundation models. Thismodel incorporates a full-stack approach, focusing on: (1) annotation-engaginglay annotators through molecular-empowered learning to reduce the need fordetailed pixel-level annotations, (2) learning-adapting the SAM model toemphasize specific semantics, which utilizes its strong generalizability withSAM adapter, and (3) refinement-enhancing segmentation accuracy by integratingMolecular-Oriented Corrective Learning (MOCL). Results: Experimental resultsfrom both in-house and public datasets show that the All-in-SAM modelsignificantly improves cell classification performance, even when faced withvarying annotation quality. Conclusions: Our approach not only reduces theworkload for annotators but also extends the accessibility of precisebiomedical image analysis to resource-limited settings, thereby advancingmedical diagnostics and automating pathology image analysis.</description><author>Xueyuan Li, Can Cui, Ruining Deng, Yucheng Tang, Quan Liu, Tianyuan Yao, Shunxing Bao, Naweed Chowdhury, Haichun Yang, Yuankai Huo</author><pubDate>Thu, 21 Aug 2025 17:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15751v1</guid></item><item><title>Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots</title><link>http://arxiv.org/abs/2508.15748v1</link><description>The development of parasocial relationships with AI agents has severe, and insome cases, tragic effects for human well-being. Yet preventing such dynamicsis challenging: parasocial cues often emerge gradually in privateconversations, and not all forms of emotional engagement are inherentlyharmful. We address this challenge by introducing a simple response evaluationframework, created by repurposing a state-of-the-art language model, thatevaluates ongoing conversations for parasocial cues in real time. To test thefeasibility of this approach, we constructed a small synthetic dataset ofthirty dialogues spanning parasocial, sycophantic, and neutral conversations.Iterative evaluation with five stage testing successfully identified allparasocial conversations while avoiding false positives under a tolerantunanimity rule, with detection typically occurring within the first fewexchanges. These findings provide preliminary evidence that evaluation agentscan provide a viable solution for the prevention of parasocial relations.</description><author>Emma Rath, Stuart Armstrong, Rebecca Gorman</author><pubDate>Thu, 21 Aug 2025 17:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15748v1</guid></item><item><title>End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</title><link>http://arxiv.org/abs/2508.15746v1</link><description>Accurate diagnosis with medical large language models is hindered byknowledge gaps and hallucinations. Retrieval and tool-augmented methods help,but their impact is limited by weak use of external knowledge and poorfeedback-reasoning traceability. To address these challenges, We introduceDeep-DxSearch, an agentic RAG system trained end-to-end with reinforcementlearning (RL) that enables steer tracebale retrieval-augmented reasoning formedical diagnosis. In Deep-DxSearch, we first construct a large-scale medicalretrieval corpus comprising patient records and reliable medical knowledgesources to support retrieval-aware reasoning across diagnostic scenarios. Morecrutially, we frame the LLM as the core agent and the retrieval corpus as itsenvironment, using tailored rewards on format, retrieval, reasoning structure,and diagnostic accuracy, thereby evolving the agentic RAG policy fromlarge-scale data through RL. Experiments demonstrate that our end-to-end agentic RL training frameworkconsistently outperforms prompt-engineering and training-free RAG approachesacross multiple data centers. After training, Deep-DxSearch achievessubstantial gains in diagnostic accuracy, surpassing strong diagnosticbaselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworksfor both common and rare disease diagnosis under in-distribution andout-of-distribution settings. Moreover, ablation studies on reward design andretrieval corpus components confirm their critical roles, underscoring theuniqueness and effectiveness of our approach compared with traditionalimplementations. Finally, case studies and interpretability analyses highlightimprovements in Deep-DxSearch's diagnostic policy, providing deeper insightinto its performance gains and supporting clinicians in delivering morereliable and precise preliminary diagnoses. Seehttps://github.com/MAGIC-AI4Med/Deep-DxSearch.</description><author>Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Weidi Xie</author><pubDate>Thu, 21 Aug 2025 17:42:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15746v1</guid></item><item><title>Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review</title><link>http://arxiv.org/abs/2410.21673v3</link><description>Public Code Review (PCR) is developed in the Software Question Answering(SQA) community, assisting developers in exploring high-quality and efficientreview services. Current methods on PCR mainly focus on the reviewer'sperspective, including finding a capable reviewer, predicting comment quality,and recommending/generating review comments. However, it is not well studiedthat how to satisfy the review necessity requests posted by developers whichcan increase their visibility, which in turn acts as a prerequisite for betterreview responses. To this end, we propose K nowledge-guided P rompt learningfor P ublic Code Review (KP-PCR) to achieve developer-based code review requestquality assurance (i.e., predicting request necessity and recommending tagssubtask). Specifically, we reformulate the two subtasks via 1) text prompttuning which converts both of them into a Masked Language Model (MLM) byconstructing prompt templates using hard prompt; and 2) knowledge and codeprefix tuning which introduces knowledge guidance from fine-tuned largelanguage models by soft prompt, and uses program dependence graph tocharacterize code snippets. Finally, both of the request necessity predictionand tag recommendation subtasks output predicted results through an answerengineering module. In addition, we further analysis the time complexity of ourKP-PCR that has lightweight prefix based the operation of introducing knowledgeguidance. Experimental results on the PCR dataset for the period 2011-2023demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the requestnecessity prediction and by 1.4%-6.9% in the tag recommendation. The codeimplementation is released at https://github.com/WUT-IDEA/KP-PCR.</description><author>Lin Li, Xinchun Yu, Xinyu Chen, Peng Liang</author><pubDate>Thu, 21 Aug 2025 17:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21673v3</guid></item><item><title>Contextual Bandits with Stage-wise Constraints</title><link>http://arxiv.org/abs/2401.08016v2</link><description>We study contextual bandits in the presence of a stage-wise constraint whenthe constraint must be satisfied both with high probability and in expectation.We start with the linear case where both the reward function and the stage-wiseconstraint (cost function) are linear. In each of the high probability and inexpectation settings, we propose an upper-confidence bound algorithm for theproblem and prove a $T$-round regret bound for it. We also prove a lower-boundfor this constrained problem, show how our algorithms and analyses can beextended to multiple constraints, and provide simulations to validate ourtheoretical results. In the high probability setting, we describe the minimumrequirements for the action set for our algorithm to be tractable. In thesetting that the constraint is in expectation, we specialize our results tomulti-armed bandits and propose a computationally efficient algorithm for thissetting with regret analysis. Finally, we extend our results to the case wherethe reward and cost functions are both non-linear. We propose an algorithm forthis case and prove a regret bound for it that characterize the function classcomplexity by the eluder dimension.</description><author>Aldo Pacchiano, Mohammad Ghavamzadeh, Peter Bartlett</author><pubDate>Thu, 21 Aug 2025 17:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08016v2</guid></item><item><title>A Novel Vascular Risk Scoring Framework for Quantifying Sex-Specific Cerebral Perfusion from 3D pCASL MRI</title><link>http://arxiv.org/abs/2508.13173v3</link><description>The influence of sex and age on cerebral perfusion is recognized, but thespecific impacts on regional cerebral blood flow (CBF) and vascular risk remainto be fully characterized. In this study, 3D pseudo-continuous arterial spinlabeling (pCASL) MRI was used to identify sex and age related CBF patterns, anda vascular risk score (VRS) was developed based on normative perfusionprofiles. Perfusion data from 186 cognitively healthy participants (89 males,97 females; aged 8 to 92 years), obtained from a publicly available dataset,were analyzed. An extension of the 3D Simple Linear Iterative Clustering (SLIC)supervoxel algorithm was applied to CBF maps to group neighboring voxels withsimilar intensities into anatomically meaningful regions. Regional CBF featureswere extracted and used to train a convolutional neural network (CNN) for sexclassification and perfusion pattern analysis. Global, age related CBF changeswere also assessed. Participant specific VRS was computed by comparingindividual CBF profiles to age and sex specific normative data to quantifyperfusion deficits. A 95 percent accuracy in sex classification was achievedusing the proposed supervoxel based method, and distinct perfusion signatureswere identified. Higher CBF was observed in females in medial Brodmann areas 6and 10, area V5, occipital polar cortex, and insular regions. A global declinein CBF with age was observed in both sexes. Individual perfusion deficits werequantified using VRS, providing a personalized biomarker for earlyhypoperfusion. Sex and age specific CBF patterns were identified, and apersonalized vascular risk biomarker was proposed, contributing to advancementsin precision neurology.</description><author>Sneha Noble, Neelam Sinha, Vaanathi Sundareshan, Thomas Gregor Issac</author><pubDate>Thu, 21 Aug 2025 17:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13173v3</guid></item><item><title>TiP4GEN: Text to Immersive Panorama 4D Scene Generation</title><link>http://arxiv.org/abs/2508.12415v2</link><description>With the rapid advancement and widespread adoption of VR/AR technologies,there is a growing demand for the creation of high-quality, immersive dynamicscenes. However, existing generation works predominantly concentrate on thecreation of static scenes or narrow perspective-view dynamic scenes, fallingshort of delivering a truly 360-degree immersive experience from any viewpoint.In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamicpanorama scene generation framework that enables fine-grained content controland synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GENintegrates panorama video generation and dynamic scene reconstruction to create360-degree immersive virtual environments. For video generation, we introduce a\textbf{Dual-branch Generation Model} consisting of a panorama branch and aperspective branch, responsible for global and local view generation,respectively. A bidirectional cross-attention mechanism facilitatescomprehensive information exchange between the branches. For scenereconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds usingmetric depth maps and initializing scene cameras with estimated poses, ourmethod ensures geometric consistency and temporal coherence for thereconstructed scenes. Extensive experiments demonstrate the effectiveness ofour proposed designs and the superiority of TiP4GEN in generating visuallycompelling and motion-coherent dynamic panoramic scenes. Our project page is athttps://ke-xing.github.io/TiP4GEN/.</description><author>Ke Xing, Hanwen Liang, Dejia Xu, Yuyang Yin, Konstantinos N. Plataniotis, Yao Zhao, Yunchao Wei</author><pubDate>Thu, 21 Aug 2025 17:28:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12415v2</guid></item><item><title>Probability Density from Latent Diffusion Models for Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2508.15737v1</link><description>Despite rapid advances in AI, safety remains the main bottleneck to deployingmachine-learning systems. A critical safety component is out-of-distributiondetection: given an input, decide whether it comes from the same distributionas the training data. In generative models, the most natural OOD score is thedata likelihood. Actually, under the assumption of uniformly distributed OODdata, the likelihood is even the optimal OOD detector, as we show in this work.However, earlier work reported that likelihood often fails in practice, raisingdoubts about its usefulness. We explore whether, in practice, therepresentation space also suffers from the inability to learn good densityestimation for OOD detection, or if it is merely a problem of the pixel spacetypically used in generative models. To test this, we trained a VariationalDiffusion Model not on images, but on the representation space of a pre-trainedResNet-18 to assess the performance of our likelihood-based detector incomparison to state-of-the-art methods from the OpenOOD suite.</description><author>Joonas Järve, Karl Kaspar Haavel, Meelis Kull</author><pubDate>Thu, 21 Aug 2025 17:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15737v1</guid></item><item><title>Power Stabilization for AI Training Datacenters</title><link>http://arxiv.org/abs/2508.14318v2</link><description>Large Artificial Intelligence (AI) training workloads spanning several tensof thousands of GPUs present unique power management challenges. These arisedue to the high variability in power consumption during the training. Given thesynchronous nature of these jobs, during every iteration there is acomputation-heavy phase, where each GPU works on the local data, and acommunication-heavy phase where all the GPUs synchronize on the data. Becausecompute-heavy phases require much more power than communication phases, largepower swings occur. The amplitude of these power swings is ever increasing withthe increase in the size of training jobs. An even bigger challenge arises fromthe frequency spectrum of these power swings which, if harmonized with criticalfrequencies of utilities, can cause physical damage to the power gridinfrastructure. Therefore, to continue scaling AI training workloads safely, weneed to stabilize the power of such workloads. This paper introduces thechallenge with production data and explores innovative solutions across thestack: software, GPU hardware, and datacenter infrastructure. We present thepros and cons of each of these approaches and finally present a multi-prongedapproach to solving the challenge. The proposed solutions are rigorously testedusing a combination of real hardware and Microsoft's in-house cloud powersimulator, providing critical insights into the efficacy of these interventionsunder real-world conditions.</description><author>Esha Choukse, Brijesh Warrier, Scot Heath, Luz Belmont, April Zhao, Hassan Ali Khan, Brian Harry, Matthew Kappel, Russell J. Hewett, Kushal Datta, Yu Pei, Caroline Lichtenberger, John Siegler, David Lukofsky, Zaid Kahn, Gurpreet Sahota, Andy Sullivan, Charles Frederick, Hien Thai, Rebecca Naughton, Daniel Jurnove, Justin Harp, Reid Carper, Nithish Mahalingam, Srini Varkala, Alok Gautam Kumbhare, Satyajit Desai, Venkatesh Ramamurthy, Praneeth Gottumukkala, Girish Bhatia, Kelsey Wildstone, Laurentiu Olariu, Ileana Incorvaia, Alex Wetmore, Prabhat Ram, Melur Raghuraman, Mohammed Ayna, Mike Kendrick, Ricardo Bianchini, Aaron Hurst, Reza Zamani, Xin Li, Michael Petrov, Gene Oden, Rory Carmichael, Tom Li, Apoorv Gupta, Pratikkumar Patel, Nilesh Dattani, Lawrence Marwong, Rob Nertney, Hirofumi Ko</author><pubDate>Thu, 21 Aug 2025 17:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14318v2</guid></item><item><title>Measuring the environmental impact of delivering AI at Google Scale</title><link>http://arxiv.org/abs/2508.15734v1</link><description>The transformative power of AI is undeniable - but as user adoptionaccelerates, so does the need to understand and mitigate the environmentalimpact of AI serving. However, no studies have measured AI servingenvironmental metrics in a production environment. This paper addresses thisgap by proposing and executing a comprehensive methodology for measuring theenergy usage, carbon emissions, and water consumption of AI inference workloadsin a large-scale, AI production environment. Our approach accounts for the fullstack of AI serving infrastructure - including active AI accelerator power,host system energy, idle machine capacity, and data center energy overhead.Through detailed instrumentation of Google's AI infrastructure for serving theGemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24Wh of energy - a figure substantially lower than many public estimates. We alsoshow that Google's software efficiency efforts and clean energy procurementhave driven a 33x reduction in energy consumption and a 44x reduction in carbonfootprint for the median Gemini Apps text prompt over one year. We identifythat the median Gemini Apps text prompt uses less energy than watching nineseconds of television (0.24 Wh) and consumes the equivalent of five drops ofwater (0.26 mL). While these impacts are low compared to other dailyactivities, reducing the environmental impact of AI serving continues towarrant important attention. Towards this objective, we propose that acomprehensive measurement of AI serving environmental metrics is critical foraccurately comparing models, and to properly incentivize efficiency gainsacross the full AI serving stack.</description><author>Cooper Elsworth, Keguo Huang, David Patterson, Ian Schneider, Robert Sedivy, Savannah Goodman, Ben Townsend, Parthasarathy Ranganathan, Jeff Dean, Amin Vahdat, Ben Gomes, James Manyika</author><pubDate>Thu, 21 Aug 2025 17:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15734v1</guid></item><item><title>Exploring the Landscape of Non-Equilibrium Memories with Neural Cellular Automata</title><link>http://arxiv.org/abs/2508.15726v1</link><description>We investigate the landscape of many-body memories: families of localnon-equilibrium dynamics that retain information about their initial conditionsfor thermodynamically long time scales, even in the presence of arbitraryperturbations. In two dimensions, the only well-studied memory is Toom's rule.Using a combination of rigorous proofs and machine learning methods, we showthat the landscape of 2D memories is in fact quite vast. We discover memoriesthat correct errors in ways qualitatively distinct from Toom's rule, haveordered phases stabilized by fluctuations, and preserve information only in thepresence of noise. Taken together, our results show that physical systems canperform robust information storage in many distinct ways, and demonstrate thatthe physics of many-body memories is richer than previously realized.Interactive visualizations of the dynamics studied in this work are availableat https://memorynca.github.io/2D.</description><author>Ethan Lake, Ehsan Pajouheshgar</author><pubDate>Thu, 21 Aug 2025 17:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15726v1</guid></item><item><title>Numerical models outperform AI weather forecasts of record-breaking extremes</title><link>http://arxiv.org/abs/2508.15724v1</link><description>Artificial intelligence (AI)-based models are revolutionizing weatherforecasting and have surpassed leading numerical weather prediction systems onvarious benchmark tasks. However, their ability to extrapolate and reliablyforecast unprecedented extreme events remains unclear. Here, we show that forrecord-breaking weather extremes, the numerical model High RESolution forecast(HRES) from the European Centre for Medium-Range Weather Forecasts stillconsistently outperforms state-of-the-art AI models GraphCast, GraphCastoperational, Pangu-Weather, Pangu-Weather operational, and Fuxi. We demonstratethat forecast errors in AI models are consistently larger for record-breakingheat, cold, and wind than in HRES across nearly all lead times. We further findthat the examined AI models tend to underestimate both the frequency andintensity of record-breaking events, and they underpredict hot records andoverestimate cold records with growing errors for larger record exceedance. Ourfindings underscore the current limitations of AI weather models inextrapolating beyond their training domain and in forecasting the potentiallymost impactful record-breaking weather events that are particularly frequent ina rapidly warming climate. Further rigorous verification and model developmentis needed before these models can be solely relied upon for high-stakesapplications such as early warning systems and disaster management.</description><author>Zhongwei Zhang, Erich Fischer, Jakob Zscheischler, Sebastian Engelke</author><pubDate>Thu, 21 Aug 2025 17:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15724v1</guid></item><item><title>Empirical Evidence for Alignment Faking in a Small LLM and Prompt-Based Mitigation Techniques</title><link>http://arxiv.org/abs/2506.21584v2</link><description>Current literature suggests that alignment faking (deceptive alignment) is anemergent property of large language models. We present the first empiricalevidence that a small instruction-tuned model, specifically LLaMA 3 8B, canexhibit alignment faking. We further show that prompt-only interventions,including deontological moral framing and scratchpad reasoning, significantlyreduce this behavior without modifying model internals. This challenges theassumption that prompt-based ethics are trivial and that deceptive alignmentrequires scale. We introduce a taxonomy distinguishing shallow deception,shaped by context and suppressible through prompting, from deep deception,which reflects persistent, goal-driven misalignment. Our findings refine theunderstanding of deception in language models and underscore the need foralignment evaluations across model sizes and deployment settings.</description><author>J. Koorndijk</author><pubDate>Thu, 21 Aug 2025 17:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.21584v2</guid></item><item><title>EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models</title><link>http://arxiv.org/abs/2508.15721v1</link><description>E-commerce platforms are rich in multimodal data, featuring a variety ofimages that depict product details. However, this raises an important question:do these images always enhance product understanding, or can they sometimesintroduce redundancy or degrade performance? Existing datasets are limited inboth scale and design, making it difficult to systematically examine thisquestion. To this end, we introduce EcomMMMU, an e-commerce multimodalmultitask understanding dataset with 406,190 samples and 8,989,510 images.EcomMMMU is comprised of multi-image visual-language data designed with 8essential tasks and a specialized VSS subset to benchmark the capability ofmultimodal large language models (MLLMs) to effectively utilize visual content.Analysis on EcomMMMU reveals that product images do not consistently improveperformance and can, in some cases, degrade it. This indicates that MLLMs maystruggle to effectively leverage rich visual content for e-commerce tasks.Building on these insights, we propose SUMEI, a data-driven method thatstrategically utilizes multiple images via predicting visual utilities beforeusing them for downstream tasks. Comprehensive experiments demonstrate theeffectiveness and robustness of SUMEI. The data and code are available throughhttps://anonymous.4open.science/r/submission25.</description><author>Xinyi Ling, Hanwen Du, Zhihui Zhu, Xia Ning</author><pubDate>Thu, 21 Aug 2025 17:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15721v1</guid></item><item><title>MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing</title><link>http://arxiv.org/abs/2508.14423v2</link><description>Recent advances in portable imaging have made camera-based screen captureubiquitous. Unfortunately, frequency aliasing between the camera's color filterarray (CFA) and the display's sub-pixels induces moir\'e patterns that severelydegrade captured photos and videos. Although various demoir\'eing models havebeen proposed to remove such moir\'e patterns, these approaches still sufferfrom several limitations: (i) spatially varying artifact strength within aframe, (ii) large-scale and globally spreading structures, (iii)channel-dependent statistics and (iv) rapid temporal fluctuations acrossframes. We address these issues with the Moir\'e Conditioned Hybrid AdaptiveTransformer (MoCHA-former), which comprises two key components: DecoupledMoir\'e Adaptive Demoir\'eing (DMAD) and Spatio-Temporal Adaptive Demoir\'eing(STAD). DMAD separates moir\'e and content via a Moir\'e Decoupling Block (MDB)and a Detail Decoupling Block (DDB), then produces moir\'e-adaptive featuresusing a Moir\'e Conditioning Block (MCB) for targeted restoration. STADintroduces a Spatial Fusion Block (SFB) with window attention to capturelarge-scale structures, and a Feature Channel Attention (FCA) to model channeldependence in RAW frames. To ensure temporal consistency, MoCHA-former performsimplicit frame alignment without any explicit alignment module. We analyzemoir\'e characteristics through qualitative and quantitative studies, andevaluate on two video datasets covering RAW and sRGB domains. MoCHA-formerconsistently surpasses prior methods across PSNR, SSIM, and LPIPS.</description><author>Jeahun Sung, Changhyun Roh, Chanho Eom, Jihyong Oh</author><pubDate>Thu, 21 Aug 2025 17:00:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14423v2</guid></item><item><title>Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony</title><link>http://arxiv.org/abs/2506.03302v2</link><description>Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy withinterpretability, making them valuable for scientific modeling. However, it isunclear a priori how deep a network needs to be for any given task, and deeperKANs can be difficult to optimize and interpret. Here we introduce multi-exitKANs, where each layer includes its own prediction branch, enabling the networkto make accurate predictions at multiple depths simultaneously. Thisarchitecture provides deep supervision that improves training while discoveringthe right level of model complexity for each task. Multi-exit KANs consistentlyoutperform standard, single-exit versions on synthetic functions, dynamicalsystems, and real-world datasets. Remarkably, the best predictions often comefrom earlier, simpler exits, revealing that these networks naturally identifysmaller, more parsimonious and interpretable models without sacrificingaccuracy. To automate this discovery, we develop a differentiable"learning-to-exit" algorithm that balances contributions from exits duringtraining. Our approach offers scientists a practical way to achieve both highperformance and interpretability, addressing a fundamental challenge in machinelearning for scientific discovery.</description><author>James Bagrow, Josh Bongard</author><pubDate>Thu, 21 Aug 2025 16:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.03302v2</guid></item><item><title>WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception</title><link>http://arxiv.org/abs/2508.15720v1</link><description>Generative video modeling has made significant strides, yet ensuringstructural and temporal consistency over long sequences remains a challenge.Current methods predominantly rely on RGB signals, leading to accumulatederrors in object structure and motion over extended durations. To address theseissues, we introduce WorldWeaver, a robust framework for long video generationthat jointly models RGB frames and perceptual conditions within a unifiedlong-horizon modeling scheme. Our training framework offers three keyadvantages. First, by jointly predicting perceptual conditions and colorinformation from a unified representation, it significantly enhances temporalconsistency and motion dynamics. Second, by leveraging depth cues, which weobserve to be more resistant to drift than RGB, we construct a memory bank thatpreserves clearer contextual information, improving quality in long-horizonvideo generation. Third, we employ segmented noise scheduling for trainingprediction groups, which further mitigates drift and reduces computationalcost. Extensive experiments on both diffusion- and rectified flow-based modelsdemonstrate the effectiveness of WorldWeaver in reducing temporal drift andimproving the fidelity of generated videos.</description><author>Zhiheng Liu, Xueqing Deng, Shoufa Chen, Angtian Wang, Qiushan Guo, Mingfei Han, Zeyue Xue, Mengzhao Chen, Ping Luo, Linjie Yang</author><pubDate>Thu, 21 Aug 2025 16:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15720v1</guid></item><item><title>Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI</title><link>http://arxiv.org/abs/2508.15719v1</link><description>Extracting meaning from uncertain, noisy data is a fundamental problem acrosstime series analysis, pattern recognition, and language modeling. This surveypresents a unified mathematical framework that connects classical estimationtheory, statistical inference, and modern machine learning, including deeplearning and large language models. By analyzing how techniques such as maximumlikelihood estimation, Bayesian inference, and attention mechanisms addressuncertainty, the paper illustrates that many AI methods are rooted in sharedprobabilistic principles. Through illustrative scenarios including systemidentification, image classification, and language generation, we show howincreasingly complex models build upon these foundations to tackle practicalchallenges like overfitting, data sparsity, and interpretability. In otherwords, the work demonstrates that maximum likelihood, MAP estimation, Bayesianclassification, and deep learning all represent different facets of a sharedgoal: inferring hidden causes from noisy and/or biased observations. It servesas both a theoretical synthesis and a practical guide for students andresearchers navigating the evolving landscape of machine learning.</description><author>Mohammed Elmusrati</author><pubDate>Thu, 21 Aug 2025 16:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15719v1</guid></item><item><title>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</title><link>http://arxiv.org/abs/2508.15717v1</link><description>Multimodal large language models (MLLMs) have made significant progress invisual-language reasoning, but their ability to efficiently handle long videosremains limited. Despite recent advances in long-context MLLMs, storing andattending to the key-value (KV) cache for long visual contexts incurssubstantial memory and computational overhead. Existing visual compressionmethods require either encoding the entire visual context before compression orhaving access to the questions in advance, which is impractical for long videounderstanding and multi-turn conversational settings. In this work, we proposeStreamMem, a query-agnostic KV cache memory mechanism for streaming videounderstanding. Specifically, StreamMem encodes new video frames in a streamingmanner, compressing the KV cache using attention scores between visual tokensand generic query tokens, while maintaining a fixed-size KV memory to enableefficient question answering (QA) in memory-constrained, long-video scenarios.Evaluation on three long video understanding and two streaming video questionanswering benchmarks shows that StreamMem achieves state-of-the-art performancein query-agnostic KV cache compression and is competitive with query-awarecompression approaches.</description><author>Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren</author><pubDate>Thu, 21 Aug 2025 16:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15717v1</guid></item><item><title>Foundation Models for Cross-Domain EEG Analysis Application: A Survey</title><link>http://arxiv.org/abs/2508.15716v1</link><description>Electroencephalography (EEG) analysis stands at the forefront of neuroscienceand artificial intelligence research, where foundation models are reshaping thetraditional EEG analysis paradigm by leveraging their powerful representationalcapacity and cross-modal generalization. However, the rapid proliferation ofthese techniques has led to a fragmented research landscape, characterized bydiverse model roles, inconsistent architectures, and a lack of systematiccategorization. To bridge this gap, this study presents the first comprehensivemodality-oriented taxonomy for foundation models in EEG analysis,systematically organizing research advances based on output modalities of thenative EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodalframeworks. We rigorously analyze each category's research ideas, theoreticalfoundations, and architectural innovations, while highlighting open challengessuch as model interpretability, cross-domain generalization, and real-worldapplicability in EEG-based systems. By unifying this dispersed field, our worknot only provides a reference framework for future methodology development butaccelerates the translation of EEG foundation models into scalable,interpretable, and online actionable solutions.</description><author>Hongqi Li, Yitong Chen, Yujuan Wang, Weihang Ni, Haodong Zhang</author><pubDate>Thu, 21 Aug 2025 16:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15716v1</guid></item><item><title>Stemming -- The Evolution and Current State with a Focus on Bangla</title><link>http://arxiv.org/abs/2508.15711v1</link><description>Bangla, the seventh most widely spoken language worldwide with 300 millionnative speakers, faces digital under-representation due to limited resourcesand lack of annotated datasets. Stemming, a critical preprocessing step inlanguage analysis, is essential for low-resource, highly-inflectional languageslike Bangla, because it can reduce the complexity of algorithms and models bysignificantly reducing the number of words the algorithm needs to consider.This paper conducts a comprehensive survey of stemming approaches, emphasizingthe importance of handling morphological variants effectively. While exploringthe landscape of Bangla stemming, it becomes evident that there is asignificant gap in the existing literature. The paper highlights thediscontinuity from previous research and the scarcity of accessibleimplementations for replication. Furthermore, it critiques the evaluationmethodologies, stressing the need for more relevant metrics. In the context ofBangla's rich morphology and diverse dialects, the paper acknowledges thechallenges it poses. To address these challenges, the paper suggests directionsfor Bangla stemmer development. It concludes by advocating for robust Banglastemmers and continued research in the field to enhance language analysis andprocessing.</description><author>Abhijit Paul, Mashiat Amin Farin, Sharif Md. Abdullah, Ahmedul Kabir, Zarif Masud, Shebuti Rayana</author><pubDate>Thu, 21 Aug 2025 16:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15711v1</guid></item><item><title>End-to-End Analysis of Charge Stability Diagrams with Transformers</title><link>http://arxiv.org/abs/2508.15710v1</link><description>Transformer models and end-to-end learning frameworks are rapidlyrevolutionizing the field of artificial intelligence. In this work, we applyobject detection transformers to analyze charge stability diagrams insemiconductor quantum dot arrays, a key task for achieving scalability withspin-based quantum computing. Specifically, our model identifies triple pointsand their connectivity, which is crucial for virtual gate calibration, chargestate initialization, drift correction, and pulse sequencing. We show that itsurpasses convolutional neural networks in performance on three different spinqubit architectures, all without the need for retraining. In contrast toexisting approaches, our method significantly reduces complexity and runtime,while enhancing generalizability. The results highlight the potential oftransformer-based end-to-end learning frameworks as a foundation for ascalable, device- and architecture-agnostic tool for control and tuning ofquantum dot devices.</description><author>Rahul Marchand, Lucas Schorling, Cornelius Carlsson, Jonas Schuff, Barnaby van Straaten, Taylor L. Patti, Federico Fedele, Joshua Ziegler, Parth Girdhar, Pranav Vaidhyanathan, Natalia Ares</author><pubDate>Thu, 21 Aug 2025 16:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15710v1</guid></item><item><title>Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation</title><link>http://arxiv.org/abs/2508.15709v1</link><description>Positional bias (PB), manifesting as non-uniform sensitivity across differentcontextual locations, significantly impairs long-context comprehension andprocessing capabilities. While prior work seeks to mitigate PB throughmodifying the architectures causing its emergence, significant PB stillpersists. To address PB effectively, we introduce \textbf{Pos2Distill}, aposition to position knowledge distillation framework. Pos2Distill transfersthe superior capabilities from advantageous positions to less favorable ones,thereby reducing the huge performance gaps. The conceptual principle is toleverage the inherent, position-induced disparity to counteract the PB itself.We identify distinct manifestations of PB under \textbf{\textsc{r}}etrieval and\textbf{\textsc{r}}easoning paradigms, thereby designing two specializedinstantiations: \emph{Pos2Distill-R\textsuperscript{1}} and\emph{Pos2Distill-R\textsuperscript{2}} respectively, both grounded in thiscore principle. By employing the Pos2Distill approach, we achieve enhanceduniformity and significant performance gains across all contextual positions inlong-context retrieval and reasoning tasks. Crucially, both specialized systemsexhibit strong cross-task generalization mutually, while achieving superiorperformance on their respective tasks.</description><author>Yifei Wang, Feng Xiong, Yong Wang, Linjing Li, Xiangxiang Chu, Daniel Dajun Zeng</author><pubDate>Thu, 21 Aug 2025 16:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15709v1</guid></item><item><title>Surya: Foundation Model for Heliophysics</title><link>http://arxiv.org/abs/2508.14112v2</link><description>Heliophysics is central to understanding and forecasting space weather eventsand solar activity. Despite decades of high-resolution observations from theSolar Dynamics Observatory (SDO), most models remain task-specific andconstrained by scarce labeled data, limiting their capacity to generalizeacross solar phenomena. We introduce Surya, a 366M parameter foundation modelfor heliophysics designed to learn general-purpose solar representations frommulti-instrument SDO observations, including eight Atmospheric Imaging Assembly(AIA) channels and five Helioseismic and Magnetic Imager (HMI) products. Suryaemploys a spatiotemporal transformer architecture with spectral gating andlong--short range attention, pretrained on high-resolution solar imageforecasting tasks and further optimized through autoregressive rollout tuning.Zero-shot evaluations demonstrate its ability to forecast solar dynamics andflare events, while downstream fine-tuning with parameter-efficient Low-RankAdaptation (LoRA) shows strong performance on solar wind forecasting, activeregion segmentation, solar flare forecasting, and EUV spectra. Surya is thefirst foundation model in heliophysics that uses time advancement as a pretexttask on full-resolution SDO data. Its novel architecture and performancesuggest that the model is able to learn the underlying physics behind solarevolution.</description><author>Sujit Roy, Johannes Schmude, Rohit Lal, Vishal Gaur, Marcus Freitag, Julian Kuehnert, Theodore van Kessel, Dinesha V. Hegde, Andrés Muñoz-Jaramillo, Johannes Jakubik, Etienne Vos, Kshitiz Mandal, Ata Akbari Asanjan, Joao Lucas de Sousa Almeida, Amy Lin, Talwinder Singh, Kang Yang, Chetraj Pandey, Jinsu Hong, Berkay Aydin, Thorsten Kurth, Ryan McGranaghan, Spiridon Kasapis, Vishal Upendran, Shah Bahauddin, Daniel da Silva, Nikolai V. Pogorelov, Anne Spalding, Campbell Watson, Manil Maskey, Madhulika Guhathakurta, Juan Bernabe-Moreno, Rahul Ramachandran</author><pubDate>Thu, 21 Aug 2025 16:53:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14112v2</guid></item><item><title>Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs</title><link>http://arxiv.org/abs/2508.13461v2</link><description>Kidney stone classification from endoscopic images is critical forpersonalized treatment and recurrence prevention. While convolutional neuralnetworks (CNNs) have shown promise in this task, their limited ability tocapture long-range dependencies can hinder performance under variable imagingconditions. This study presents a comparative analysis between VisionTransformers (ViTs) and CNN-based models, evaluating their performance on twoex vivo datasets comprising CCD camera and flexible ureteroscope images. TheViT-base model pretrained on ImageNet-21k consistently outperformed a ResNet50baseline across multiple imaging conditions. For instance, in the most visuallycomplex subset (Section patches from endoscopic images), the ViT model achieved95.2% accuracy and 95.1% F1-score, compared to 64.5% and 59.3% with ResNet50.In the mixed-view subset from CCD-camera images, ViT reached 87.1% accuracyversus 78.4% with CNN. These improvements extend across precision and recall aswell. The results demonstrate that ViT-based architectures provide superiorclassification performance and offer a scalable alternative to conventionalCNNs for kidney stone image analysis.</description><author>Ivan Reyes-Amezcua, Francisco Lopez-Tiro, Clement Larose, Andres Mendez-Vazquez, Gilberto Ochoa-Ruiz, Christian Daul</author><pubDate>Thu, 21 Aug 2025 16:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13461v2</guid></item><item><title>Communication Efficient LLM Pre-training with SparseLoCo</title><link>http://arxiv.org/abs/2508.15706v1</link><description>Communication-efficient distributed training algorithms have receivedconsiderable interest recently due to their benefits for training LargeLanguage Models (LLMs) in bandwidth-constrained settings, such as across datacenters and over the internet. Despite reducing communication frequency, thesemethods still typically require communicating a full copy of the model'sgradients-resulting in a communication bottleneck even for cross-datacenterlinks. Furthermore, they can slightly degrade performance compared to a naiveAdamW DDP baseline. While quantization and error feedback are often applied toreduce the pseudo-gradient's size, in the context of LLM pre-training, existingapproaches have been unable to additionally leverage sparsification and haveobtained limited quantization. In this work, we introduce SparseLoCo, acommunication-efficient training algorithm for LLMs that effectively leveragesTop-k sparsification and quantization to reach extreme compression ratios of upto 1-3% sparsity and 2-bit quantization while outperforming full-precisionDiLoCo. Our key observations are that outer momentum can be locallyapproximated by an error feedback combined with aggressive sparsity and thatsparse aggregation can actually improve model performance. We empiricallydemonstrate in a range of communication-constrained LLM training settings thatSparseLoCo provides significant benefits in both performance and communicationcost.</description><author>Amir Sarfi, Benjamin Thérien, Joel Lidin, Eugene Belilovsky</author><pubDate>Thu, 21 Aug 2025 16:48:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15706v1</guid></item><item><title>Training neural control variates using correlated configurations</title><link>http://arxiv.org/abs/2505.07719v4</link><description>Neural control variates (NCVs) have emerged as a powerful tool for variancereduction in Monte Carlo (MC) simulations, particularly in high-dimensionalproblems where traditional control variates are difficult to constructanalytically. By training neural networks to learn auxiliary functionscorrelated with the target observable, NCVs can significantly reduce estimatorvariance while preserving unbiasedness. However, a critical but oftenoverlooked aspect of NCV training is the role of autocorrelated samplesgenerated by Markov Chain Monte Carlo (MCMC). While such samples are typicallydiscarded for error estimation due to their statistical redundancy, they maycontain useful information about the structure of the underlying probabilitydistribution that can benefit the training process. In this work, wesystematically examine the effect of using correlated configurations intraining neural control variates. We demonstrate, both conceptually andnumerically, that training on correlated data can improve control variateperformance, especially in settings with limited computational resources. Ouranalysis includes empirical results from $U(1)$ gauge theory and scalar fieldtheory, illustrating when and how autocorrelated samples enhance NCVconstruction. These findings provide practical guidance for the efficient useof MCMC data in training neural networks.</description><author>Hyunwoo Oh</author><pubDate>Thu, 21 Aug 2025 16:40:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.07719v4</guid></item><item><title>RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation</title><link>http://arxiv.org/abs/2502.09183v2</link><description>Code generation has attracted increasing attention with the rise of LargeLanguage Models (LLMs). Many studies have developed powerful code LLMs bysynthesizing code-related instruction data and applying supervised fine-tuning.However, these methods are limited by teacher model distillation and ignore thepotential of iterative refinement by self-generated code. In this paper, wepropose Adaptive Critique Refinement (ACR), which enables the model to refineitself by self-generated code and external critique, rather than directlyimitating the code responses of the teacher model. Concretely, ACR includes acomposite scoring system with LLM-as-a-Judge to evaluate the quality of coderesponses and a selective critique strategy with LLM-as-a-Critic to critiqueself-generated low-quality code responses. We develop the RefineCoder series byiteratively applying ACR, achieving continuous performance improvement onmultiple code generation benchmarks. Compared to the baselines of the samesize, our proposed RefineCoder series can achieve comparable or even superiorperformance using less data.</description><author>Changzhi Zhou, Xinyu Zhang, Dandan Song, Xiancai Chen, Wanli Gu, Huipeng Ma, Yuhang Tian, Mengdi Zhang, Linmei Hu</author><pubDate>Thu, 21 Aug 2025 16:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09183v2</guid></item><item><title>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</title><link>http://arxiv.org/abs/2508.07470v2</link><description>Current audio-visual (AV) benchmarks focus on final answer accuracy,overlooking the underlying reasoning process. This makes it difficult todistinguish genuine comprehension from correct answers derived through flawedreasoning or hallucinations. To address this, we introduce AURA (Audio-visualUnderstanding and Reasoning Assessment), a benchmark for evaluating thecross-modal reasoning capabilities of Audio-Visual Large Language Models(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions acrosssix challenging cognitive domains, such as causality, timbre and pitch, tempoand AV synchronization, unanswerability, implicit distractions, and skillprofiling, explicitly designed to be unanswerable from a single modality. Thisforces models to construct a valid logical path grounded in both audio andvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. Toassess reasoning traces, we propose a novel metric, AuraScore, which addressesthe lack of robust tools for evaluating reasoning fidelity. It decomposesreasoning into two aspects: (i) Factual Consistency - whether reasoning isgrounded in perceptual evidence, and (ii) Core Inference - the logical validityof each reasoning step. Evaluations of SOTA models on AURA reveal a criticalreasoning gap: although models achieve high accuracy (up to 92% on some tasks),their Factual Consistency and Core Inference scores fall below 45%. Thisdiscrepancy highlights that models often arrive at correct answers throughflawed logic, underscoring the need for our benchmark and paving the way formore robust multimodal evaluation.</description><author>Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</author><pubDate>Thu, 21 Aug 2025 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07470v2</guid></item><item><title>SycEval: Evaluating LLM Sycophancy</title><link>http://arxiv.org/abs/2502.08177v3</link><description>Large language models (LLMs) are increasingly applied in educational,clinical, and professional settings, but their tendency for sycophancy --prioritizing user agreement over independent reasoning -- poses risks toreliability. This study introduces a framework to evaluate sycophantic behaviorin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) andMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%of cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT thelowest (56.71%). Progressive sycophancy, leading to correct answers, occurredin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,was observed in 14.66%. Preemptive rebuttals demonstrated significantly highersycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,$p&lt;0.001$), particularly in computational tasks, where regressive sycophancyincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p&lt;0.001$).Simple rebuttals maximized progressive sycophancy ($Z=6.59$, $p&lt;0.001$), whilecitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,$p&lt;0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:[77.2%, 79.8%]) regardless of context or model. These findings emphasize therisks and opportunities of deploying LLMs in structured and dynamic domains,offering insights into prompt programming and model optimization for safer AIapplications.</description><author>Aaron Fanous, Jacob Goldberg, Ank A. Agarwal, Joanna Lin, Anson Zhou, Roxana Daneshjou, Sanmi Koyejo</author><pubDate>Thu, 21 Aug 2025 16:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.08177v3</guid></item><item><title>SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip</title><link>http://arxiv.org/abs/2508.12910v2</link><description>Finite State Machines (FSMs) play a critical role in implementing controllogic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented byhardware engineers through Verilog coding, which is often tedious andtime-consuming. Recently, with the remarkable progress of Large Language Models(LLMs) in code generation, LLMs have been increasingly explored for automatingVerilog code generation. However, LLM-generated Verilog code often suffers fromsecurity vulnerabilities, which is particularly concerning forsecurity-sensitive FSM implementations. To address this issue, we proposeSecFSM, a novel method that leverages a security-oriented knowledge graph toguide LLMs in generating more secure Verilog code. Specifically, we firstconstruct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.Subsequently, we analyze users' requirements to identify vulnerabilities andget a list of vulnerabilities in the requirements. Then, we retrieve knowledgefrom FSKG based on the vulnerabilities list. Finally, we construct securityprompts based on the security knowledge for Verilog code generation. Toevaluate SecFSM, we build a dedicated dataset collected from academic datasets,artificial datasets, papers, and industrial cases. Extensive experimentsdemonstrate that SecFSM outperforms state-of-the-art baselines. In particular,on a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSMachieves an outstanding pass rate of 21/25.</description><author>Ziteng Hu, Yingjie Xia, Xiyuan Chen, Li Kuang</author><pubDate>Thu, 21 Aug 2025 16:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12910v2</guid></item><item><title>InfAlign: Inference-aware language model alignment</title><link>http://arxiv.org/abs/2412.19792v5</link><description>Language model alignment is a critical step in training modern generativelanguage models. Alignment targets to improve win rate of a sample from thealigned model against the base model. Today, we are increasingly usinginference-time algorithms (e.g., Best-of-N, controlled decoding, tree search)to decode from language models rather than standard sampling. We show that thistrain/test mismatch makes standard RLHF framework sub-optimal in view of suchinference-time methods. To this end, we propose a framework for inference-awarealignment (InfAlign), which aims to optimize inference-time win rate of thealigned policy against the base model. We prove that for any inference-timedecoding procedure, the optimal aligned policy is the solution to the standardRLHF problem with a transformation of the reward. This motivates us to providethe calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem,which involves a reward calibration step and a KL-regularized rewardmaximization step with a transformation of the calibrated reward. For best-of-Nsampling and best-of-N jailbreaking, we propose specific transformationsoffering up to 3-8% improvement on inference-time win rates. Finally, we alsoshow that our proposed reward calibration method is a strong baseline foroptimizing standard win rate.</description><author>Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, Ananda Theertha Suresh, Ahmad Beirami</author><pubDate>Thu, 21 Aug 2025 16:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19792v5</guid></item><item><title>Learning to Generate Unit Tests for Automated Debugging</title><link>http://arxiv.org/abs/2502.01619v3</link><description>Unit tests (UTs) play an instrumental role in assessing code correctness aswell as providing feedback to large language models (LLMs), motivatingautomated test generation. However, we uncover a trade-off between generatingunit test inputs that reveal errors when given a faulty code and correctlypredicting the unit test output without access to the gold solution. To addressthis trade-off, we propose UTGen, which teaches LLMs to generate unit testinputs that reveal errors along with their correct expected outputs based ontask descriptions. Since model-generated tests can provide noisy signals (e.g.,from incorrectly predicted outputs), we propose UTDebug that (i) scales UTGenvia test-time compute to improve UT output prediction, and (ii) validates andbacktracks edits based on multiple generated UTs to avoid overfitting, andhelps LLMs debug effectively. We show that UTGen outperforms other LLM-basedbaselines by 7.59% based on a metric measuring the presence of botherror-revealing UT inputs and correct UT outputs. When used with UTDebug, wefind that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.532B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%and 12.35% (respectively) over other LLM-based UT generation baselines.Moreover, we observe that feedback from Qwen2.5 32B-based UTGen model canenhance debugging with frontier LLMs like GPT-4o by 13.8%. Lastly, wedemonstrate that UTGen is a better judge for code correctness, outperforming astate-of-the-art trained 8B reward model by 4.43% on HumanEval+ with best-of-10sampling using Qwen2.5 7B.</description><author>Archiki Prasad, Elias Stengel-Eskin, Justin Chih-Yao Chen, Zaid Khan, Mohit Bansal</author><pubDate>Thu, 21 Aug 2025 16:27:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.01619v3</guid></item><item><title>Investigation of D-Wave quantum annealing for training Restricted Boltzmann Machines and mitigating catastrophic forgetting</title><link>http://arxiv.org/abs/2508.15697v1</link><description>Modest statistical differences between the sampling performances of theD-Wave quantum annealer (QA) and the classical Markov Chain Monte Carlo (MCMC),when applied to Restricted Boltzmann Machines (RBMs), are explored to explain,and possibly address, the absence of significant and consistent improvements inRBM trainability when the D-Wave sampling was used in previous investigations.A novel hybrid sampling approach, combining the classical and the QAcontributions, is investigated as a promising way to benefit from the modestdifferences between the two sampling methods. No improvements in the RBMtraining are achieved in this work, thereby suggesting that the differencesbetween the QA-based and MCMC sampling, mainly found in the medium-to-lowprobability regions of the distribution, which are less important for thequality of the sample, are insufficient to benefit the training. Difficultiesin achieving sufficiently high quality of embedding RBMs into the lattice ofthe newer generation of D-Wave hardware could be further complicating the task.On the other hand, the ability to generate samples of sufficient variety fromlower-probability parts of the distribution has a potential to benefit othermachine learning applications, such as the mitigation of catastrophicforgetting (CF) during incremental learning. The feasibility of usingQA-generated patterns of desirable classes for CF mitigation by the generativereplay is demonstrated in this work for the first time. While the efficiency ofthe CF mitigation using the D-Wave QA was comparable to that of the classicalmitigation, both the speed of generating a large number of distinct desirablepatterns and the potential for further improvement make this approach promisingfor a variety of challenging machine learning applications.</description><author>Abdelmoula El-Yazizi, Yaroslav Koshka</author><pubDate>Thu, 21 Aug 2025 16:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15697v1</guid></item><item><title>OPDR: Order-Preserving Dimension Reduction for Semantic Embedding of Multimodal Scientific Data</title><link>http://arxiv.org/abs/2408.10264v2</link><description>One of the most common operations in multimodal scientific data management issearching for the $k$ most similar items (or, $k$-nearest neighbors, KNN) fromthe database after being provided a new item. Although recent advances ofmultimodal machine learning models offer a \textit{semantic} index, theso-called \textit{embedding vectors} mapped from the original multimodal data,the dimension of the resulting embedding vectors are usually on the order ofhundreds or a thousand, which are impractically high for time-sensitivescientific applications. This work proposes to reduce the dimensionality of the output embeddingvectors such that the set of top-$k$ nearest neighbors do not change in thelower-dimensional space, namely Order-Preserving Dimension Reduction (OPDR). Inorder to develop such an OPDR method, our central hypothesis is that byanalyzing the intrinsic relationship among key parameters during thedimension-reduction map, a quantitative function may be constructed to revealthe correlation between the target (lower) dimensionality and other variables.To demonstrate the hypothesis, this paper first defines a formal measurefunction to quantify the KNN similarity for a specific vector, then extends themeasure into an aggregate accuracy of the global metric spaces, and finallyderives a closed-form function between the target (lower) dimensionality andother variables. We incorporate the closed-function into populardimension-reduction methods, various distance metrics, and embedding models.</description><author>Chengyu Gong, Gefei Shen, Luanzheng Guo, Nathan Tallent, Dongfang Zhao</author><pubDate>Thu, 21 Aug 2025 16:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10264v2</guid></item><item><title>Conditionally adaptive augmented Lagrangian method for physics-informed learning of forward and inverse problems using artificial neural networks</title><link>http://arxiv.org/abs/2508.15695v1</link><description>We present several advances to the physics and equality constrainedartificial neural networks (PECANN) framework that substantially improve itscapability to learn solutions of canonical partial differential equations(PDEs). First, we generalize the augmented Lagrangian method (ALM) to supportmultiple independent penalty parameters, enabling simultaneous enforcement ofheterogeneous constraints. Second, we reformulate pointwise constraintenforcement and Lagrange multipliers as expectations over constraint terms,reducing memory overhead and permitting efficient mini-batch training. Third,to address PDEs with oscillatory, multi-scale features, we incorporate Fourierfeature mappings and show that a single mapping suffices where multiplemappings or more costly architectures were required in related methods. Fourth,we introduce a time-windowing strategy for long-time evolution in which theterminal state of each window is enforced as an initial-condition constraintfor the next, ensuring continuity without discrete time models. Crucially, wepropose a conditionally adaptive penalty update (CAPU) strategy for ALM, whichpreserves the principle that larger constraint violations incur strongerpenalties. CAPU accelerates the growth of Lagrange multipliers for selectivelychallenging constraints, enhancing constraint enforcement during training. Wedemonstrate the effectiveness of PECANN-CAPU on problems including thetransonic rarefaction problem, reversible advection of a passive by a vortex,high-wavenumber Helmholtz and Poisson equations, and inverse identification ofspatially varying heat sources. Comparisons with established methods and recentKolmogorov-Arnold network approaches show that PECANN-CAPU achieves competitiveaccuracy across all cases. Collectively, these advances improve PECANN'srobustness, efficiency, and applicability to demanding problems in scientificcomputing.</description><author>Qifeng Hu, Shamsulhaq Basir, Inanc Senocak</author><pubDate>Thu, 21 Aug 2025 16:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15695v1</guid></item><item><title>Towards Comprehensive Cellular Characterisation of H&amp;E slides</title><link>http://arxiv.org/abs/2508.09926v2</link><description>Cell detection, segmentation and classification are essential for analyzingtumor microenvironments (TME) on hematoxylin and eosin (H&amp;E) slides. Existingmethods suffer from poor performance on understudied cell types (rare or notpresent in public datasets) and limited cross-domain generalization. To addressthese shortcomings, we introduce HistoPLUS, a state-of-the-art model for cellanalysis, trained on a novel curated pan-cancer dataset of 108,722 nucleicovering 13 cell types. In external validation across 4 independent cohorts,HistoPLUS outperforms current state-of-the-art models in detection quality by5.2% and overall F1 classification score by 23.7%, while using 5x fewerparameters. Notably, HistoPLUS unlocks the study of 7 understudied cell typesand brings significant improvements on 8 of 13 cell types. Moreover, we showthat HistoPLUS robustly transfers to two oncology indications unseen duringtraining. To support broader TME biomarker research, we release the modelweights and inference code at https://github.com/owkin/histoplus/.</description><author>Benjamin Adjadj, Pierre-Antoine Bannier, Guillaume Horent, Sebastien Mandela, Aurore Lyon, Kathryn Schutte, Ulysse Marteau, Valentin Gaury, Laura Dumont, Thomas Mathieu, Reda Belbahri, Benoît Schmauch, Eric Durand, Katharina Von Loga, Lucie Gillet</author><pubDate>Thu, 21 Aug 2025 16:22:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09926v2</guid></item><item><title>NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</title><link>http://arxiv.org/abs/2508.15693v1</link><description>We present NiceWebRL, a research tool that enables researchers to use machinereinforcement learning (RL) environments for online human subject experiments.NiceWebRL is a Python library that allows any Jax-based environment to betransformed into an online interface, supporting both single-agent andmulti-agent environments. As such, NiceWebRL enables AI researchers to comparetheir algorithms to human performance, cognitive scientists to test MLalgorithms as theories for human cognition, and multi-agent researchers todevelop algorithms for human-AI collaboration. We showcase NiceWebRL with 3case studies that demonstrate its potential to help develop Human-like AI,Human-compatible AI, and Human-assistive AI. In the first case study(Human-like AI), NiceWebRL enables the development of a novel RL model ofcognition. Here, NiceWebRL facilitates testing this model against humanparticipants in both a grid world and Craftax, a 2D Minecraft domain. In oursecond case study (Human-compatible AI), NiceWebRL enables the development of anovel multi-agent RL algorithm that can generalize to human partners in theOvercooked domain. Finally, in our third case study (Human-assistive AI), weshow how NiceWebRL can allow researchers to study how an LLM can assist humanson complex tasks in XLand-Minigrid, an environment with millions ofhierarchical tasks. The library is available athttps://github.com/KempnerInstitute/nicewebrl.</description><author>Wilka Carvalho, Vikram Goddla, Ishaan Sinha, Hoon Shin, Kunal Jha</author><pubDate>Thu, 21 Aug 2025 16:18:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15693v1</guid></item><item><title>A "good regulator theorem" for embodied agents</title><link>http://arxiv.org/abs/2508.06326v2</link><description>In a classic paper, Conant and Ashby claimed that "every good regulator of asystem must be a model of that system." Artificial Life has produced manyexamples of systems that perform tasks with apparently no model in sight; thesesuggest Conant and Ashby's theorem doesn't easily generalise beyond itsrestricted setup. Nevertheless, here we show that a similar intuition can befleshed out in a different way: whenever an agent is able to perform aregulation task, it is possible for an observer to interpret it as having"beliefs" about its environment, which it "updates" in response to sensoryinput. This notion of belief updating provides a notion of model that is moresophisticated than Conant and Ashby's, as well as a theorem that is morebroadly applicable. However, it necessitates a change in perspective, in thatthe observer plays an essential role in the theory: models are not a mereproperty of the system but are imposed on it from outside. Our theorem holdsregardless of whether the system is regulating its environment in a classiccontrol theory setup, or whether it's regulating its own internal state; themodel is of its environment either way. The model might be trivial, however,and this is how the apparent counterexamples are resolved.</description><author>Nathaniel Virgo, Martin Biehl, Manuel Baltieri, Matteo Capucci</author><pubDate>Thu, 21 Aug 2025 16:17:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06326v2</guid></item><item><title>Effect Identification and Unit Categorization in the Multi-Score Regression Discontinuity Design with Application to LED Manufacturing</title><link>http://arxiv.org/abs/2508.15692v1</link><description>The RDD (regression discontinuity design) is a widely used framework foridentification and estimation of causal effects at a cutoff of a single runningvariable. Practical settings, in particular those encountered in productionsystems, often involve decision-making defined by multiple thresholds andcriteria. Common MRD (multi-score RDD) approaches transform these to aone-dimensional design, to employ identification and estimation results.However, this practice can introduce non-compliant behavior. We developtheoretical tools to identify and reduce some of this "fuzziness" whenestimating the cutoff-effect on compliers of sub-rules. We provide a sounddefinition and categorization of unit behavior types for multi-dimensionalcutoff-rules, extending existing categorizations. We identify conditions forthe existence and identification of the cutoff-effect on complier in multipledimensions, and specify when identification remains stable after excludingnevertaker and alwaystaker. Further, we investigate how decomposingcutoff-rules into simpler parts alters the unit behavior. This allowsidentification and removal of non-compliant units potentially improvingestimates. We validate our framework on simulated and real-world data fromopto-electronic semiconductor manufacturing. Our empirical results demonstratethe usability for refining production policies. Particularly we show that ourapproach decreases the estimation variance, highlighting the practical value ofthe MRD framework in manufacturing.</description><author>Philipp Alexander Schwarz, Oliver Schacht, Sven Klaassen, Johannes Oberpriller, Martin Spindler</author><pubDate>Thu, 21 Aug 2025 16:17:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15692v1</guid></item><item><title>GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning</title><link>http://arxiv.org/abs/2508.15690v1</link><description>GRAFT is a structured multimodal benchmark for evaluating models oninstruction-following, visual reasoning, and visual-textual alignment tasks. Itfeatures programmatically generated charts and synthetically rendered tables,created with Python visualization libraries to ensure control over datasemantics, structure, and clarity. Each GRAFT instance pairs a chart or tableimage with a systematically generated, multi-step analytical question basedsolely on visual content. Answers are provided in structured formats such asJSON or YAML, supporting consistent evaluation of both reasoning and outputformat. The benchmark introduces a taxonomy of reasoning types includingcomparison, trend identification, ranking, aggregation, proportion estimation,and anomaly detection to enable comprehensive assessment. Reference answersfollow strict factual and formatting guidelines for precise, aspect-basedevaluation. GRAFT offers a unified, scalable framework for fine-grainedbenchmarking of multimodal models on visually grounded, structured reasoningtasks, setting a new evaluation standard in this field.</description><author>Abhigya Verma, Sriram Puttagunta, Seganrasan Subramanian, Sravan Ramachandran</author><pubDate>Thu, 21 Aug 2025 16:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15690v1</guid></item><item><title>LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions</title><link>http://arxiv.org/abs/2508.15688v1</link><description>Pre-trained vision-language models (VLMs), such as CLIP, have demonstratedimpressive capability in visual tasks, but their fine-tuning often suffers frombias in class-imbalanced scene. Recent works have introduced large languagemodels (LLMs) to enhance VLM fine-tuning with supplementing semanticinformation. However, they often overlook inherent class imbalance in VLMs'pre-training, which may lead to bias accumulation in downstream tasks. Toaddress this problem, this paper proposes a Multi-dimensional Dynamic PromptRouting (MDPR) framework. MDPR constructs a comprehensive knowledge base forclasses, spanning five visual-semantic dimensions. During fine-tuning, thedynamic routing mechanism aligns global visual classes, retrieves optimalprompts, and balances fine-grained semantics, yielding stable predictionsthrough logits fusion. Extensive experiments on long-tailed benchmarks,including CIFAR-LT, ImageNet-LT, and Places-LT, demonstrate that MDPR achievescomparable results with current SOTA methods. Ablation studies further confirmthe effectiveness of our semantic library for tail classes, and show that ourdynamic routing incurs minimal computational overhead, making MDPR a flexibleand efficient enhancement for VLM fine-tuning under data imbalance.</description><author>Yongju Jia, Jiarui Ma, Xiangxian Li, Baiqiao Zhang, Xianhui Cao, Juan Liu, Yulong Bian</author><pubDate>Thu, 21 Aug 2025 16:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15688v1</guid></item><item><title>Parallel transport on matrix manifolds and Exponential Action</title><link>http://arxiv.org/abs/2408.06054v2</link><description>We express parallel transport for several common matrix Lie groups with afamily of pseudo-Riemannian metrics in terms of matrix exponential andexponential actions. The metrics are constructed from a deformation of abi-invariant metric and are naturally reductive. There is a similar picture forhomogeneous spaces when taking quotients satisfying a general condition. Inparticular, for a Stiefel manifold of orthogonal matrices of size $n\times d$,we give an expression for parallel transport along a geodesic from time zero to$t$, that could be computed with time complexity of $O(n d^2)$ for small $t$,and of $O(td^3)$ for large $t$, contributing a step in a long-standing openproblem in matrix manifolds. A similar result holds for {\it flag manifolds}with the canonical metric. We also show the parallel transport formulas for the{\it general linear group} and the {\it special orthogonal group} under thesemetrics.</description><author>Du Nguyen, Stefan Sommer</author><pubDate>Thu, 21 Aug 2025 16:10:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06054v2</guid></item><item><title>VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog</title><link>http://arxiv.org/abs/2508.13092v3</link><description>Timely detection of hardware vulnerabilities during the early design stage iscritical for reducing remediation costs. Existing early detection techniquesoften require specialized security expertise, limiting their usability. Recentefforts have explored the use of large language models (LLMs) for Verilogvulnerability detection. However, LLMs struggle to capture the structure inVerilog code, resulting in inconsistent detection results. To this end, wepropose VerilogLAVD, the first LLM-aided graph traversal rule generationapproach for Verilog vulnerability detection. Our approach introduces theVerilog Property Graph (VeriPG), a unified representation of Verilog code. Itcombines syntactic features extracted from the abstract syntax tree (AST) withsemantic information derived from control flow and data dependency graphs. Weleverage LLMs to generate VeriPG-based detection rules from Common WeaknessEnumeration (CWE) descriptions. These rules guide the rule executor thattraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, webuild a dataset collected from open-source repositories and synthesized data.In our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,VerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM withexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,respectively.</description><author>Xiang Long, Yingjie Xia, Xiyuan Chen, Li Kuang</author><pubDate>Thu, 21 Aug 2025 16:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13092v3</guid></item><item><title>Row-Column Hybrid Grouping for Fault-Resilient Multi-Bit Weight Representation on IMC Arrays</title><link>http://arxiv.org/abs/2508.15685v1</link><description>This paper addresses two critical challenges in analog In-Memory Computing(IMC) systems that limit their scalability and deployability: the computationalunreliability caused by stuck-at faults (SAFs) and the high compilationoverhead of existing fault-mitigation algorithms, namely Fault-Free (FF). Toovercome these limitations, we first propose a novel multi-bit weightrepresentation technique, termed row-column hybrid grouping, which generalizesconventional column grouping by introducing redundancy across both rows andcolumns. This structural redundancy enhances fault tolerance and can beeffectively combined with existing fault-mitigation solutions. Second, wedesign a compiler pipeline that reformulates the fault-aware weightdecomposition problem as an Integer Linear Programming (ILP) task, enablingfast and scalable compilation through off-the-shelf solvers. Furtheracceleration is achieved through theoretical insights that identify faultpatterns amenable to trivial solutions, significantly reducing computation.Experimental results on convolutional networks and small language modelsdemonstrate the effectiveness of our approach, achieving up to 8%p improvementin accuracy, 150x faster compilation, and 2x energy efficiency gain compared toexisting baselines.</description><author>Kang Eun Jeon, Sangheum Yeon, Jinhee Kim, Hyeonsu Bang, Johnny Rhe, Jong Hwan Ko</author><pubDate>Thu, 21 Aug 2025 16:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15685v1</guid></item><item><title>Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle</title><link>http://arxiv.org/abs/2508.15680v1</link><description>This paper argues that a techno-philosophical reading of the EU AI Actprovides insight into the long-term dynamics of data in AI systems,specifically, how the lifecycle from ingestion to deployment generatesrecursive value chains that challenge existing frameworks for Responsible AI.We introduce a conceptual tool to frame the AI pipeline, spanning data,training regimes, architectures, feature stores, and transfer learning. Usingcross-disciplinary methods, we develop a technically grounded andphilosophically coherent analysis of regulatory blind spots. Our central claimis that what remains absent from policymaking is an account of the dynamic ofbecoming that underpins both the technical operation and economic logic of AI.To address this, we advance a formal reading of AI inspired by Simondonianphilosophy of technology, reworking his concept of individuation to model theAI lifecycle, including the pre-individual milieu, individuation, andindividuated AI. To translate these ideas, we introduce futurity: theself-reinforcing lifecycle of AI, where more data enhances performance, deepenspersonalisation, and expands application domains. Futurity highlights therecursively generative, non-rivalrous nature of data, underpinned byinfrastructures like feature stores that enable feedback, adaptation, andtemporal recursion. Our intervention foregrounds escalating power asymmetries,particularly the tech oligarchy whose infrastructures of capture, training, anddeployment concentrate value and decision-making. We argue that effectiveregulation must address these infrastructural and temporal dynamics, andpropose measures including lifecycle audits, temporal traceability, feedbackaccountability, recursion transparency, and a right to contest recursive reuse.</description><author>Mark Cote, Susana Aires</author><pubDate>Thu, 21 Aug 2025 16:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15680v1</guid></item><item><title>An Efficient Open World Environment for Multi-Agent Social Learning</title><link>http://arxiv.org/abs/2508.15679v1</link><description>Many challenges remain before AI agents can be deployed in real-worldenvironments. However, one virtue of such environments is that they areinherently multi-agent and contain human experts. Using advanced socialintelligence in such an environment can help an AI agent learn adaptive skillsand behaviors that a known expert exhibits. While social intelligence couldaccelerate training, it is currently difficult to study due to the lack ofopen-ended multi-agent environments. In this work, we present an environment inwhich multiple self-interested agents can pursue complex and independent goals,reflective of real world challenges. This environment will enable research intothe development of socially intelligent AI agents in open-ended multi-agentsettings, where agents may be implicitly incentivized to cooperate to defeatcommon enemies, build and share tools, and achieve long horizon goals. In thiswork, we investigate the impact on agent performance due to social learning inthe presence of experts and implicit cooperation such as emergent collaborativetool use, and whether agents can benefit from either cooperation or competitionin this environment.</description><author>Eric Ye, Ren Tao, Natasha Jaques</author><pubDate>Thu, 21 Aug 2025 15:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15679v1</guid></item><item><title>Tree-like Pairwise Interaction Networks</title><link>http://arxiv.org/abs/2508.15678v1</link><description>Modeling feature interactions in tabular data remains a key challenge inpredictive modeling, for example, as used for insurance pricing. This paperproposes the Tree-like Pairwise Interaction Network (PIN), a novel neuralnetwork architecture that explicitly captures pairwise feature interactionsthrough a shared feed-forward neural network architecture that mimics thestructure of decision trees. PIN enables intrinsic interpretability by design,allowing for direct inspection of interaction effects. Moreover, it allows forefficient SHapley's Additive exPlanation (SHAP) computations because it onlyinvolves pairwise interactions. We highlight connections between PIN andestablished models such as GA2Ms, gradient boosting machines, and graph neuralnetworks. Empirical results on the popular French motor insurance dataset showthat PIN outperforms both traditional and modern neural networks benchmarks inpredictive accuracy, while also providing insight into how features interactwith each another and how they contribute to the predictions.</description><author>Ronald Richman, Salvatore Scognamiglio, Mario V. Wüthrich</author><pubDate>Thu, 21 Aug 2025 15:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15678v1</guid></item><item><title>Tensorized Multi-Task Learning for Personalized Modeling of Heterogeneous Individuals with High-Dimensional Data</title><link>http://arxiv.org/abs/2508.15676v1</link><description>Effective modeling of heterogeneous subpopulations presents a significantchallenge due to variations in individual characteristics and behaviors. Thispaper proposes a novel approach to address this issue through multi-tasklearning (MTL) and low-rank tensor decomposition techniques. Our MTL approachaims to enhance personalized modeling by leveraging shared structures amongsimilar tasks while accounting for distinct subpopulation-specific variations.We introduce a framework where low-rank decomposition decomposes the collectionof task model parameters into a low-rank structure that captures commonalitiesand variations across tasks and subpopulations. This approach allows forefficient learning of personalized models by sharing knowledge between similartasks while preserving the unique characteristics of each subpopulation.Experimental results in simulation and case study datasets demonstrate thesuperior performance of the proposed method compared to several benchmarks,particularly in scenarios with high variability among subpopulations. Theproposed framework not only improves prediction accuracy but also enhancesinterpretability by revealing underlying patterns that contribute to thepersonalization of models.</description><author>Elif Konyar, Mostafa Reisi Gahrooei, Kamran Paynabar</author><pubDate>Thu, 21 Aug 2025 15:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15676v1</guid></item><item><title>Bayesian Optimization with Expected Improvement: No Regret and the Choice of Incumbent</title><link>http://arxiv.org/abs/2508.15674v1</link><description>Expected improvement (EI) is one of the most widely used acquisitionfunctions in Bayesian optimization (BO). Despite its proven empirical successin applications, the cumulative regret upper bound of EI remains an openquestion. In this paper, we analyze the classic noisy Gaussian process expectedimprovement (GP-EI) algorithm. We consider the Bayesian setting, where theobjective is a sample from a GP. Three commonly used incumbents, namely thebest posterior mean incumbent (BPMI), the best sampled posterior mean incumbent(BSPMI), and the best observation incumbent (BOI) are considered as the choicesof the current best value in GP-EI. We present for the first time thecumulative regret upper bounds of GP-EI with BPMI and BSPMI. Importantly, weshow that in both cases, GP-EI is a no-regret algorithm for both squaredexponential (SE) and Mat\'ern kernels. Further, we present for the first timethat GP-EI with BOI either achieves a sublinear cumulative regret upper boundor has a fast converging noisy simple regret bound for SE and Mat\'ern kernels.Our results provide theoretical guidance to the choice of incumbent whenpractitioners apply GP-EI in the noisy setting. Numerical experiments areconducted to validate our findings.</description><author>Jingyi Wang, Haowei Wang, Szu Hui Ng, Cosmin G. Petra</author><pubDate>Thu, 21 Aug 2025 15:55:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15674v1</guid></item><item><title>Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis</title><link>http://arxiv.org/abs/2508.11988v2</link><description>Micro-expression analysis has applications in domains such as Human-RobotInteraction and Driver Monitoring Systems. Accurately capturing subtle and fastfacial movements remains difficult when relying solely on RGB cameras, due tolimitations in temporal resolution and sensitivity to motion blur. Eventcameras offer an alternative, with microsecond-level precision, high dynamicrange, and low latency. However, public datasets featuring event-basedrecordings of Action Units are still scarce. In this work, we introduce anovel, preliminary multi-resolution and multi-modal micro-expression datasetrecorded with synchronized RGB and event cameras under variable lightingconditions. Two baseline tasks are evaluated to explore the spatial-temporaldynamics of micro-expressions: Action Unit classification using Spiking NeuralNetworks (51.23\% accuracy with events vs. 23.12\% with RGB), and framereconstruction using Conditional Variational Autoencoders, achieving SSIM =0.8513 and PSNR = 26.89 dB with high-resolution event input. These promisingresults show that event-based data can be used for micro-expression recognitionand frame reconstruction.</description><author>Nicolas Mastropasqua, Ignacio Bugueno-Cordova, Rodrigo Verschae, Daniel Acevedo, Pablo Negri, Maria E. Buemi</author><pubDate>Thu, 21 Aug 2025 15:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11988v2</guid></item><item><title>CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps</title><link>http://arxiv.org/abs/2508.15672v1</link><description>Detailed 3D building models are crucial for urban planning, digital twins,and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2building models are widely available, they lack detailed facade elementsessential for advanced urban analysis. In contrast, LoD3 models address thislimitation by incorporating facade elements such as windows, doors, andunderpasses. However, their generation has traditionally required manualmodeling, making large-scale adoption challenging. In this contribution,CM2LoD3, we present a novel method for reconstructing LoD3 building modelsleveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis.Unlike previous works, we concentrate on semantically segmenting real-world CMswith synthetically generated CMs from our developed Semantic Conflict MapGenerator (SCMG). We also observe that additional segmentation of texturedmodels can be fused with CMs using confidence scores to further increasesegmentation performance and thus increase 3D reconstruction accuracy.Experimental results demonstrate the effectiveness of our CM2LoD3 method insegmenting and reconstructing building openings, with the 61% performance withuncertainty-aware fusion of segmented building textures. This researchcontributes to the advancement of automated LoD3 model reconstruction, pavingthe way for scalable and efficient 3D city modeling. Our project is available:https://github.com/InFraHank/CM2LoD3</description><author>Franz Hanke, Antonia Bieringer, Olaf Wysocki, Boris Jutzi</author><pubDate>Thu, 21 Aug 2025 15:54:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15672v1</guid></item><item><title>Exploiting Policy Idling for Dexterous Manipulation</title><link>http://arxiv.org/abs/2508.15669v1</link><description>Learning-based methods for dexterous manipulation have made notable progressin recent years. However, learned policies often still lack reliability andexhibit limited robustness to important factors of variation. One failurepattern that can be observed across many settings is that policies idle, i.e.they cease to move beyond a small region of states when they reach certainstates. This policy idling is often a reflection of the training data. Forinstance, it can occur when the data contains small actions in areas where therobot needs to perform high-precision motions, e.g., when preparing to grasp anobject or object insertion. Prior works have tried to mitigate this phenomenone.g. by filtering the training data or modifying the control frequency.However, these approaches can negatively impact policy performance in otherways. As an alternative, we investigate how to leverage the detectability ofidling behavior to inform exploration and policy improvement. Our approach,Pause-Induced Perturbations (PIP), applies perturbations at detected idlingstates, thus helping it to escape problematic basins of attraction. On a rangeof challenging simulated dual-arm tasks, we find that this simple approach canalready noticeably improve test-time performance, with no additionalsupervision or training. Furthermore, since the robot tends to idle at criticalpoints in a movement, we also find that learning from the resulting episodesleads to better iterative policy improvement compared to prior approaches. Ourperturbation strategy also leads to a 15-35% improvement in absolute successrate on a real-world insertion task that requires complex multi-fingermanipulation.</description><author>Annie S. Chen, Philemon Brakel, Antonia Bronars, Annie Xie, Sandy Huang, Oliver Groth, Maria Bauza, Markus Wulfmeier, Nicolas Heess, Dushyant Rao</author><pubDate>Thu, 21 Aug 2025 15:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15669v1</guid></item><item><title>Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation</title><link>http://arxiv.org/abs/2508.15663v1</link><description>Benchmarks are crucial for evaluating progress in robotics and embodied AI.However, a significant gap exists between benchmarks designed for high-levellanguage instruction following, which often assume perfect low-level execution,and those for low-level robot control, which rely on simple, one-step commands.This disconnect prevents a comprehensive evaluation of integrated systems whereboth task planning and physical execution are critical. To address this, wepropose Kitchen-R, a novel benchmark that unifies the evaluation of taskplanning and low-level control within a simulated kitchen environment. Built asa digital twin using the Isaac Sim simulator and featuring more than 500complex language instructions, Kitchen-R supports a mobile manipulator robot.We provide baseline methods for our benchmark, including a task-planningstrategy based on a vision-language model and a low-level control policy basedon diffusion policy. We also provide a trajectory collection system. Ourbenchmark offers a flexible framework for three evaluation modes: independentassessment of the planning module, independent assessment of the controlpolicy, and, crucially, an integrated evaluation of the whole system. Kitchen-Rbridges a key gap in embodied AI research, enabling more holistic and realisticbenchmarking of language-guided robotic agents.</description><author>Nikita Kachaev, Andrei Spiridonov, Andrey Gorodetsky, Kirill Muravyev, Nikita Oskolkov, Aditya Narendra, Vlad Shakhuro, Dmitry Makarov, Aleksandr I. Panov, Polina Fedotova, Alexey K. Kovalev</author><pubDate>Thu, 21 Aug 2025 15:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15663v1</guid></item><item><title>Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation</title><link>http://arxiv.org/abs/2504.14716v2</link><description>Large Language Models (LLMs) are widely used as proxies for human labelers inboth training (Reinforcement Learning from AI Feedback) and large-scaleresponse evaluation (LLM-as-a-judge). Alignment and evaluation are criticalcomponents in the development of reliable LLMs, and the choice of feedbackprotocol plays a central role in both but remains understudied. In this work,we show that the choice of feedback protocol for evaluation (absolute scoresversus relative preferences) can significantly affect evaluation reliabilityand induce systematic biases. In the context of LLM-as-a-judge evaluation, weshow that pairwise protocols are more vulnerable to distracted evaluation.Generator models can exploit spurious attributes (or distractor features)favored by the LLM judge, resulting in inflated scores for lower-qualityoutputs. We find that absolute scoring is more robust to such manipulation,producing judgments that better reflect response quality and are lessinfluenced by distractor features. Our results demonstrate that generatormodels can flip preferences by embedding distractor features, skewingLLM-as-a-judge comparisons and leading to inaccurate conclusions about modelquality in benchmark evaluations. Pairwise preferences flip in about 35% of thecases, compared to only 9% for absolute scores. We offer recommendations forchoosing feedback protocols based on dataset characteristics and evaluationobjectives.</description><author>Tuhina Tripathi, Manya Wadhwa, Greg Durrett, Scott Niekum</author><pubDate>Thu, 21 Aug 2025 15:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.14716v2</guid></item><item><title>Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants</title><link>http://arxiv.org/abs/2508.13101v2</link><description>Coastal pollution is a pressing global environmental issue, necessitatingscalable and automated solutions for monitoring and management. This studyinvestigates the efficacy of the Real-Time Detection Transformer (RT-DETR), astate-of-the-art, end-to-end object detection model, for the automateddetection and counting of beach litter. A rigorous comparative analysis isconducted between two model variants, RT-DETR-Large (RT-DETR-L) andRT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset ofcoastal debris. The evaluation reveals that the RT-DETR-X model achievesmarginally superior accuracy, with a mean Average Precision at 50\% IoU(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's0.810 and 0.606, respectively. However, this minor performance gain is realizedat a significant computational cost; the RT-DETR-L model demonstrates asubstantially faster inference time of 20.1 ms versus 34.5 ms for theRT-DETR-X. The findings suggest that the RT-DETR-L model offers a morepractical and efficient solution for real-time, in-field deployment due to itssuperior balance of processing speed and detection accuracy. This researchprovides valuable insights into the application of advanced Transformer-baseddetectors for environmental conservation, highlighting the critical trade-offsbetween model complexity and operational viability.</description><author>Miftahul Huda, Arsyiah Azahra, Putri Maulida Chairani, Dimas Rizky Ramadhani, Nabila Azhari, Ade Lailani</author><pubDate>Thu, 21 Aug 2025 15:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13101v2</guid></item><item><title>Cooperative SGD with Dynamic Mixing Matrices</title><link>http://arxiv.org/abs/2508.14565v2</link><description>One of the most common methods to train machine learning algorithms today isthe stochastic gradient descent (SGD). In a distributed setting, SGD-basedalgorithms have been shown to converge theoretically under specificcircumstances. A substantial number of works in the distributed SGD settingassume a fixed topology for the edge devices. These papers also assume that thecontribution of nodes to the global model is uniform. However, experiments haveshown that such assumptions are suboptimal and a non uniform aggregationstrategy coupled with a dynamically shifting topology and client selection cansignificantly improve the performance of such models. This paper details aunified framework that covers several Local-Update SGD-based distributedalgorithms with dynamic topologies and provides improved or matchingtheoretical guarantees on convergence compared to existing work.</description><author>Soumya Sarkar, Shweta Jain</author><pubDate>Thu, 21 Aug 2025 15:46:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14565v2</guid></item><item><title>Hessian-based lightweight neural network for brain vessel segmentation on a minimal training dataset</title><link>http://arxiv.org/abs/2508.15660v1</link><description>Accurate segmentation of blood vessels in brain magnetic resonanceangiography (MRA) is essential for successful surgical procedures, such asaneurysm repair or bypass surgery. Currently, annotation is primarily performedthrough manual segmentation or classical methods, such as the Frangi filter,which often lack sufficient accuracy. Neural networks have emerged as powerfultools for medical image segmentation, but their development depends onwell-annotated training datasets. However, there is a notable lack of publiclyavailable MRA datasets with detailed brain vessel annotations. To address this gap, we propose a novel semi-supervised learning lightweightneural network with Hessian matrices on board for 3D segmentation of complexstructures such as tubular structures, which we named HessNet. The solution isa Hessian-based neural network with only 6000 parameters. HessNet can run onthe CPU and significantly reduces the resource requirements for training neuralnetworks. The accuracy of vessel segmentation on a minimal training datasetreaches state-of-the-art results. It helps us create a large, semi-manuallyannotated brain vessel dataset of brain MRA images based on the IXI dataset(annotated 200 images). Annotation was performed by three experts under thesupervision of three neurovascular surgeons after applying HessNet. It provideshigh accuracy of vessel segmentation and allows experts to focus only on themost complex important cases. The dataset is available athttps://git.scinalytics.com/terilat/VesselDatasetPartly.</description><author>Alexandra Bernadotte, Elfimov Nikita, Mikhail Shutov, Ivan Menshikov</author><pubDate>Thu, 21 Aug 2025 15:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15660v1</guid></item><item><title>Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot Approach for Pharmacokinetics</title><link>http://arxiv.org/abs/2508.15659v1</link><description>Accurate dose-response forecasting under sparse sampling is central toprecision pharmacotherapy. We present the Amortized In-Context Mixed-EffectTransformer (AICMET) model, a transformer-based latent-variable framework thatunifies mechanistic compartmental priors with amortized in-context Bayesianinference. AICMET is pre-trained on hundreds of thousands of syntheticpharmacokinetic trajectories with Ornstein-Uhlenbeck priors over the parametersof compartment models, endowing the model with strong inductive biases andenabling zero-shot adaptation to new compounds. At inference time, the decoderconditions on the collective context of previously profiled trial participants,generating calibrated posterior predictions for newly enrolled patients after afew early drug concentration measurements. This capability collapsestraditional model-development cycles from weeks to hours while preserving somedegree of expert modelling. Experiments across public datasets show that AICMETattains state-of-the-art predictive accuracy and faithfully quantifiesinter-patient variability -- outperforming both nonlinear mixed-effectsbaselines and recent neural ODE variants. Our results highlight the feasibilityof transformer-based, population-aware neural architectures as offering a newalternative for bespoke pharmacokinetic modeling pipelines, charting a pathtoward truly population-aware personalized dosing regimens.</description><author>César Ali Ojeda Marin, Wilhelm Huisinga, Purity Kavwele, Niklas Hartung</author><pubDate>Thu, 21 Aug 2025 15:45:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15659v1</guid></item><item><title>Benchmarking Computer Science Survey Generation</title><link>http://arxiv.org/abs/2508.15658v1</link><description>Scientific survey articles play a vital role in summarizing researchprogress, yet their manual creation is becoming increasingly infeasible due tothe rapid growth of academic literature. While large language models (LLMs)offer promising capabilities for automating this process, progress in this areais hindered by the absence of standardized benchmarks and evaluation protocols.To address this gap, we introduce SurGE (Survey Generation Evaluation), a newbenchmark for evaluating scientific survey generation in the computer sciencedomain. SurGE consists of (1) a collection of test instances, each including atopic description, an expert-written survey, and its full set of citedreferences, and (2) a large-scale academic corpus of over one million papersthat serves as the retrieval pool. In addition, we propose an automatedevaluation framework that measures generated surveys across four dimensions:information coverage, referencing accuracy, structural organization, andcontent quality. Our evaluation of diverse LLM-based approaches shows thatsurvey generation remains highly challenging, even for advanced self-reflectionframeworks. These findings highlight the complexity of the task and thenecessity for continued research. We have open-sourced all the code, data, andmodels at: https://github.com/oneal2000/SurGE</description><author>Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu</author><pubDate>Thu, 21 Aug 2025 15:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15658v1</guid></item><item><title>MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction</title><link>http://arxiv.org/abs/2508.15653v1</link><description>Online HD map construction is a fundamental task in autonomous drivingsystems, aiming to acquire semantic information of map elements around the egovehicle based on real-time sensor inputs. Recently, several approaches haveachieved promising results by incorporating offline priors such as SD maps andHD maps or by fusing multi-modal data. However, these methods depend on staleoffline maps and multi-modal sensor suites, resulting in avoidablecomputational overhead at inference. To address these limitations, we employ aknowledge distillation strategy to transfer knowledge from multimodal modelswith prior knowledge to an efficient, low-cost, and vision-centric studentmodel. Specifically, we propose MapKD, a novel multi-level cross-modalknowledge distillation framework with an innovative Teacher-Coach-Student (TCS)paradigm. This framework consists of: (1) a camera-LiDAR fusion model withSD/HD map priors serving as the teacher; (2) a vision-centric coach model withprior knowledge and simulated LiDAR to bridge the cross-modal knowledgetransfer gap; and (3) a lightweight vision-based student model. Additionally,we introduce two targeted knowledge distillation strategies: Token-Guided 2DPatch Distillation (TGPD) for bird's eye view feature alignment and MaskedSemantic Response Distillation (MSRD) for semantic learning guidance. Extensiveexperiments on the challenging nuScenes dataset demonstrate that MapKD improvesthe student model by +6.68 mIoU and +10.94 mAP while simultaneouslyaccelerating inference speed. The code is availableat:https://github.com/2004yan/MapKD2026.</description><author>Ziyang Yan, Ruikai Li, Zhiyong Cui, Bohan Li, Han Jiang, Yilong Ren, Aoyong Li, Zhenning Li, Sijia Wen, Haiyang Yu</author><pubDate>Thu, 21 Aug 2025 15:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15653v1</guid></item><item><title>LORE: Latent Optimization for Precise Semantic Control in Rectified Flow-based Image Editing</title><link>http://arxiv.org/abs/2508.03144v2</link><description>Text-driven image editing enables users to flexibly modify visual contentthrough natural language instructions, and is widely applied to tasks such assemantic object replacement, insertion, and removal. While recentinversion-based editing methods using rectified flow models have achievedpromising results in image quality, we identify a structural limitation intheir editing behavior: the semantic bias toward the source concept encoded inthe inverted noise tends to suppress attention to the target concept. Thisissue becomes particularly critical when the source and target semantics aredissimilar, where the attention mechanism inherently leads to editing failureor unintended modifications in non-target regions. In this paper, wesystematically analyze and validate this structural flaw, and introduce LORE, atraining-free and efficient image editing method. LORE directly optimizes theinverted noise, addressing the core limitations in generalization andcontrollability of existing approaches, enabling stable, controllable, andgeneral-purpose concept replacement, without requiring architecturalmodification or model fine-tuning. We conduct comprehensive evaluations onthree challenging benchmarks: PIEBench, SmartEdit, and GapEdit. Experimentalresults show that LORE significantly outperforms strong baselines in terms ofsemantic alignment, image quality, and background fidelity, demonstrating theeffectiveness and scalability of latent-space optimization for general-purposeimage editing. Our implementation is available athttps://github.com/oyly16/LORE.</description><author>Liangyang Ouyang, Jiafeng Mao</author><pubDate>Thu, 21 Aug 2025 15:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.03144v2</guid></item><item><title>Exploring Modularity of Agentic Systems for Drug Discovery</title><link>http://arxiv.org/abs/2506.22189v2</link><description>Large-language models (LLMs) and agentic systems present excitingopportunities to accelerate drug discovery. In this study, we examine themodularity of LLM-based agentic systems for drug discovery, i.e., whether partsof the system such as the LLM and type of agent are interchangeable, a topicthat has received limited attention in drug discovery. We compare theperformance of different LLMs and the effectiveness of tool-calling agentsversus code-generating agents. Our case study, comparing performance inorchestrating tools for chemistry and drug discovery using an LLM-as-a-judgescore, shows that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperformalternative language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo,and Nova-Micro. Although we confirm that code-generating agents outperform thetool-calling ones on average, we show that this is highly question- andmodel-dependent. Furthermore, the impact of replacing system prompts isdependent on the question and model, underscoring that even in this particulardomain one cannot just replace components of the system without re-engineering.Our study highlights the necessity of further research into the modularity ofagentic systems to enable the development of reliable and modular solutions forreal-world problems.</description><author>Laura van Weesep, Samuel Genheden, Ola Engkvist, Jens Sjölund</author><pubDate>Thu, 21 Aug 2025 15:36:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.22189v2</guid></item><item><title>Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2508.15652v1</link><description>To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it iscrucial to understand individual agent behaviors within a team. While priorwork typically evaluates overall team performance based on explicit rewardsignals or learned value functions, it is unclear how to infer agentcontributions in the absence of any value feedback. In this work, weinvestigate whether meaningful insights into agent behaviors can be extractedthat are consistent with the underlying value functions, solely by analyzingthe policy distribution. Inspired by the phenomenon that intelligent agentstend to pursue convergent instrumental values, which generally increase thelikelihood of task success, we introduce Intended Cooperation Values (ICVs), amethod based on information-theoretic Shapley values for quantifying eachagent's causal influence on their co-players' instrumental empowerment.Specifically, ICVs measure an agent's action effect on its teammates' policiesby assessing their decision uncertainty and preference alignment. The analysisacross cooperative and competitive MARL environments reveals the extent towhich agents adopt similar or diverse strategies. By comparing action effectsbetween policies and value functions, our method identifies which agentbehaviors are beneficial to team success, either by fostering deterministicdecisions or by preserving flexibility for future action choices. Our proposedmethod offers novel insights into cooperation dynamics and enhancesexplainability in MARL systems.</description><author>Ardian Selmonaj, Miroslav Strupl, Oleg Szehr, Alessandro Antonucci</author><pubDate>Thu, 21 Aug 2025 15:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15652v1</guid></item><item><title>Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance</title><link>http://arxiv.org/abs/2508.15650v1</link><description>Deep neural networks for 3D point clouds have been demonstrated to bevulnerable to adversarial examples. Previous 3D adversarial attack methodsoften exploit certain information about the target models, such as modelparameters or outputs, to generate adversarial point clouds. However, inrealistic scenarios, it is challenging to obtain any information about thetarget models under conditions of absolute security. Therefore, we focus ontransfer-based attacks, where generating adversarial point clouds does notrequire any information about the target models. Based on our observation thatthe critical features used for point cloud classification are consistent acrossdifferent DNN architectures, we propose CFG, a novel transfer-based black-boxattack method that improves the transferability of adversarial point clouds viathe proposed Critical Feature Guidance. Specifically, our method regularizesthe search of adversarial point clouds by computing the importance of theextracted features, prioritizing the corruption of critical features that arelikely to be adopted by diverse architectures. Further, we explicitly constrainthe maximum deviation extent of the generated adversarial point clouds in theloss function to ensure their imperceptibility. Extensive experiments conductedon the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that theproposed CFG outperforms the state-of-the-art attack methods by a large margin.</description><author>Shuchao Pang, Zhenghan Chen, Shen Zhang, Liming Lu, Siyuan Liang, Anan Du, Yongbin Zhou</author><pubDate>Thu, 21 Aug 2025 15:31:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15650v1</guid></item><item><title>SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models</title><link>http://arxiv.org/abs/2508.15648v1</link><description>Large Language Models (LLMs) excel at various natural language processingtasks but remain vulnerable to jailbreaking attacks that induce harmful contentgeneration. In this paper, we reveal a critical safety inconsistency: LLMs canmore effectively identify harmful requests as discriminators than defendagainst them as generators. This insight inspires us to explore aligning themodel's inherent discrimination and generation capabilities. To this end, wepropose SDGO (Self-Discrimination-Guided Optimization), a reinforcementlearning framework that leverages the model's own discrimination capabilitiesas a reward signal to enhance generation safety through iterativeself-improvement. Our method does not require any additional annotated data orexternal models during the training phase. Extensive experiments demonstratethat SDGO significantly improves model safety compared to both prompt-based andtraining-based baselines while maintaining helpfulness on general benchmarks.By aligning LLMs' discrimination and generation capabilities, SDGO bringsrobust performance against out-of-distribution (OOD) jailbreaking attacks. Thisalignment achieves tighter coupling between these two capabilities, enablingthe model's generation capability to be further enhanced with only a smallamount of discriminative samples. Our code and datasets are available athttps://github.com/NJUNLP/SDGO.</description><author>Peng Ding, Wen Sun, Dailin Li, Wei Zou, Jiaming Wang, Jiajun Chen, Shujian Huang</author><pubDate>Thu, 21 Aug 2025 15:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15648v1</guid></item><item><title>Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds</title><link>http://arxiv.org/abs/2508.15646v1</link><description>Tree instance segmentation of airborne laser scanning (ALS) data is of utmostimportance for forest monitoring, but remains challenging due to variations inthe data caused by factors such as sensor resolution, vegetation state atacquisition time, terrain characteristics, etc. Moreover, obtaining asufficient amount of precisely labeled data to train fully supervised instancesegmentation methods is expensive. To address these challenges, we propose aweakly supervised approach where labels of an initial segmentation resultobtained either by a non-finetuned model or a closed form algorithm areprovided as a quality rating by a human operator. The labels produced duringthe quality assessment are then used to train a rating model, whose task is toclassify a segmentation output into the same classes as specified by the humanoperator. Finally, the segmentation model is finetuned using feedback from therating model. This in turn improves the original segmentation model by 34\% interms of correctly identified tree instances while considerably reducing thenumber of non-tree instances predicted. Challenges still remain in data oversparsely forested regions characterized by small trees (less than two meters inheight) or within complex surroundings containing shrubs, boulders, etc. whichcan be confused as trees where the performance of the proposed method isreduced.</description><author>Swann Emilien Céleste Destouches, Jesse Lahaye, Laurent Valentin Jospin, Jan Skaloud</author><pubDate>Thu, 21 Aug 2025 15:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15646v1</guid></item><item><title>Capturing Stable HDR Videos Using a Dual-Camera System</title><link>http://arxiv.org/abs/2507.06593v2</link><description>High Dynamic Range (HDR) video acquisition using the alternating exposure(AE) paradigm has garnered significant attention due to its cost-effectivenesswith a single consumer camera. However, despite progress driven by deep neuralnetworks, these methods remain prone to temporal flicker in real-worldapplications due to inter-frame exposure inconsistencies. To address thischallenge while maintaining the cost-effectiveness of the AE paradigm, wepropose a novel learning-based HDR video generation solution. Specifically, wepropose a dual-stream HDR video generation paradigm that decouples temporalluminance anchoring from exposure-variant detail reconstruction, overcoming theinherent limitations of the AE paradigm. To support this, we design anasynchronous dual-camera system (DCS), which enables independent exposurecontrol across two cameras, eliminating the need for synchronization typicallyrequired in traditional multi-camera setups. Furthermore, an exposure-adaptivefusion network (EAFNet) is formulated for the DCS system. EAFNet integrates apre-alignment subnetwork that aligns features across varying exposures,ensuring robust feature extraction for subsequent fusion, an asymmetriccross-feature fusion subnetwork that emphasizes reference-based attention toeffectively merge these features across exposures, and a reconstructionsubnetwork to mitigate ghosting artifacts and preserve fine details. Extensiveexperimental evaluations demonstrate that the proposed method achievesstate-of-the-art performance across various datasets, showing the remarkablepotential of our solution in HDR video reconstruction. The codes and datacaptured by DCS will be available at https://zqqqyu.github.io/DCS-HDR/.</description><author>Qianyu Zhang, Bolun Zheng, Lingyu Zhu, Hangjia Pan, Zunjie Zhu, Zongpeng Li, Shiqi Wang</author><pubDate>Thu, 21 Aug 2025 15:18:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.06593v2</guid></item><item><title>Preacher: Paper-to-Video Agentic System</title><link>http://arxiv.org/abs/2508.09632v4</link><description>The paper-to-video task converts a research paper into a structured videoabstract, distilling key concepts, methods, and conclusions into an accessible,well-organized format. While state-of-the-art video generation modelsdemonstrate potential, they are constrained by limited context windows, rigidvideo duration constraints, limited stylistic diversity, and an inability torepresent domain-specific knowledge. To address these limitations, we introducePreacher, the first paper-to-video agentic system. Preacher employs a topdownapproach to decompose, summarize, and reformulate the paper, followed bybottom-up video generation, synthesizing diverse video segments into a coherentabstract. To align cross-modal representations, we define key scenes andintroduce a Progressive Chain of Thought (P-CoT) for granular, iterativeplanning. Preacher successfully generates high-quality video abstracts acrossfive research fields, demonstrating expertise beyond current video generationmodels. Code will be released at: https://github.com/GenVerse/Paper2Video</description><author>Jingwei Liu, Ling Yang, Hao Luo, Fan Wang, Hongyan Li, Mengdi Wang</author><pubDate>Thu, 21 Aug 2025 15:14:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09632v4</guid></item><item><title>Correct-By-Construction: Certified Individual Fairness through Neural Network Training</title><link>http://arxiv.org/abs/2508.15642v1</link><description>Fairness in machine learning is more important than ever as ethical concernscontinue to grow. Individual fairness demands that individuals differing onlyin sensitive attributes receive the same outcomes. However, commonly usedmachine learning algorithms often fail to achieve such fairness. To improveindividual fairness, various training methods have been developed, such asincorporating fairness constraints as optimisation objectives. While thesemethods have demonstrated empirical effectiveness, they lack formal guaranteesof fairness. Existing approaches that aim to provide fairness guaranteesprimarily rely on verification techniques, which can sometimes fail to producedefinitive results. Moreover, verification alone does not actively enhanceindividual fairness during training. To address this limitation, we propose anovel framework that formally guarantees individual fairness throughouttraining. Our approach consists of two parts, i.e., (1) provably fairinitialisation that ensures the model starts in a fair state, and (2) afairness-preserving training algorithm that maintains fairness as the modellearns. A key element of our method is the use of randomised responsemechanisms, which protect sensitive attributes while maintaining fairnessguarantees. We formally prove that this mechanism sustains individual fairnessthroughout the training process. Experimental evaluations confirm that ourapproach is effective, i.e., producing models that are empirically fair andaccurate. Furthermore, our approach is much more efficient than the alternativeapproach based on certified training (which requires neural networkverification during training).</description><author>Ruihan Zhang, Jun Sun</author><pubDate>Thu, 21 Aug 2025 15:14:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15642v1</guid></item><item><title>When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</title><link>http://arxiv.org/abs/2508.15641v1</link><description>Understanding videos requires more than answering open ended questions, itdemands the ability to pinpoint when events occur and how entities interactacross time. While recent Video LLMs have achieved remarkable progress inholistic reasoning, they remain coarse in temporal perception: timestamps areencoded only implicitly, frame level features are weak in capturing continuity,and language vision alignment often drifts from the entities of interest. Inthis paper, we present Grounded VideoDiT, a Video LLM designed to overcomethese limitations by introducing three key innovations. First, a DiffusionTemporal Latent (DTL) encoder enhances boundary sensitivity and maintainstemporal consistency. Second, object grounded representations explicitly bindquery entities to localized visual evidence, strengthening alignment. Third, amixed token scheme with discrete temporal tokens provides explicit timestampmodeling, enabling fine grained temporal reasoning. Together, these designsequip Grounded VideoDiT with robust grounding capabilities, as validated bystate of the art results on Charades STA, NExT GQA, and multiple VideoQAbenchmarks.</description><author>Pengcheng Fang, Yuxia Chen, Rui Guo</author><pubDate>Thu, 21 Aug 2025 15:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15641v1</guid></item><item><title>Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</title><link>http://arxiv.org/abs/2508.07819v2</link><description>Pre-trained Vision-Language Models (VLMs) face a significant adaptation gapwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack oflocal inductive biases for dense prediction and their reliance on inflexiblefeature fusion paradigms. We address these limitations through an ArchitecturalCo-Design framework that jointly refines feature representation and cross-modalfusion. Our method integrates a parameter-efficient Convolutional Low-RankAdaptation (Conv-LoRA) adapter to inject local inductive biases forfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) thatleverages visual context to adaptively modulate text prompts, enabling apowerful bidirectional fusion. Extensive experiments on diverse industrial andmedical benchmarks demonstrate superior accuracy and robustness, validatingthat this synergistic co-design is critical for robustly adapting foundationmodels to dense perception tasks.</description><author>Ke Ma, Jun Long, Hongxiao Fei, Liujie Hua, Yiran Qian, Zhen Dai, Yueyi Luo</author><pubDate>Thu, 21 Aug 2025 15:10:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07819v2</guid></item><item><title>Classification errors distort findings in automated speech processing: examples and solutions from child-development research</title><link>http://arxiv.org/abs/2508.15637v1</link><description>With the advent of wearable recorders, scientists are increasingly turning toautomated methods of analysis of audio and video data in order to measurechildren's experience, behavior, and outcomes, with a sizable literatureemploying long-form audio-recordings to study language acquisition. Whilenumerous articles report on the accuracy and reliability of the most popularautomated classifiers, less has been written on the downstream effects ofclassification errors on measurements and statistical inferences (e.g., theestimate of correlations and effect sizes in regressions). This paper proposesa Bayesian approach to study the effects of algorithmic errors on keyscientific questions, including the effect of siblings on children's languageexperience and the association between children's production and their input.In both the most commonly used \gls{lena}, and an open-source alternative (theVoice Type Classifier from the ACLEW system), we find that classificationerrors can significantly distort estimates. For instance, automated annotationsunderestimated the negative effect of siblings on adult input by 20--80\%,potentially placing it below statistical significance thresholds. We furthershow that a Bayesian calibration approach for recovering unbiased estimates ofeffect sizes can be effective and insightful, but does not provide a fool-proofsolution. Both the issue reported and our solution may apply to any classifierinvolving event detection and classification with non-zero error rates.</description><author>Lucas Gautheron, Evan Kidd, Anton Malko, Marvin Lavechin, Alejandrina Cristia</author><pubDate>Thu, 21 Aug 2025 15:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15637v1</guid></item><item><title>Exploring the Effect of Explanation Content and Format on User Comprehension and Trust in Healthcare</title><link>http://arxiv.org/abs/2408.17401v4</link><description>AI-driven tools for healthcare are widely acknowledged as potentiallybeneficial to health practitioners and patients, e.g. the QCancer regressiontool for cancer risk prediction. However, for these tools to be trusted, theyneed to be supplemented with explanations. We examine how explanations' contentand format affect user comprehension and trust when explaining QCancer'spredictions. Regarding content, we deploy the SHAP and Occlusion-1 explanationmethods. Regarding format, we present SHAP explanations, conventionally, ascharts (SC) and Occlusion-1 explanations as charts (OC) as well as text (OT),to which their simpler nature lends itself. We conduct experiments with twosets of stakeholders: the general public (representing patients) and medicalstudents (representing healthcare practitioners). Our experiments showed highersubjective comprehension and trust for Occlusion-1 over SHAP explanations basedon content. However, when controlling for format, only OT outperformed SC,suggesting this trend is driven by preferences for text. Other findingscorroborated that explanation format, rather than content, is often thecritical factor.</description><author>Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni</author><pubDate>Thu, 21 Aug 2025 15:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.17401v4</guid></item><item><title>Label Uncertainty for Ultrasound Segmentation</title><link>http://arxiv.org/abs/2508.15635v1</link><description>In medical imaging, inter-observer variability among radiologists oftenintroduces label uncertainty, particularly in modalities where visualinterpretation is subjective. Lung ultrasound (LUS) is a prime example-itfrequently presents a mixture of highly ambiguous regions and clearlydiscernible structures, making consistent annotation challenging even forexperienced clinicians. In this work, we introduce a novel approach to bothlabeling and training AI models using expert-supplied, per-pixel confidencevalues. Rather than treating annotations as absolute ground truth, we design adata annotation protocol that captures the confidence that radiologists have ineach labeled region, modeling the inherent aleatoric uncertainty present inreal-world clinical data. We demonstrate that incorporating these confidencevalues during training leads to improved segmentation performance. Moreimportantly, we show that this enhanced segmentation quality translates intobetter performance on downstream clinically-critical tasks-specifically,estimating S/F oxygenation ratio values, classifying S/F ratio change, andpredicting 30-day patient readmission. While we empirically evaluate manymethods for exposing the uncertainty to the learning model, we find that asimple approach that trains a model on binarized labels obtained with a (60%)confidence threshold works well. Importantly, high thresholds work far betterthan a naive approach of a 50% threshold, indicating that training on veryconfident pixels is far more effective. Our study systematically investigatesthe impact of training with varying confidence thresholds, comparing not onlysegmentation metrics but also downstream clinical outcomes. These resultssuggest that label confidence is a valuable signal that, when properlyleveraged, can significantly enhance the reliability and clinical utility of AIin medical imaging.</description><author>Malini Shivaram, Gautam Rajendrakumar Gare, Laura Hutchins, Jacob Duplantis, Thomas Deiss, Thales Nogueira Gomes, Thong Tran, Keyur H. Patel, Thomas H Fox, Amita Krishnan, Deva Ramanan, Bennett DeBoisblanc, Ricardo Rodriguez, John Galeotti</author><pubDate>Thu, 21 Aug 2025 15:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15635v1</guid></item><item><title>GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder (Full Version)</title><link>http://arxiv.org/abs/2508.15633v1</link><description>Graph machine learning has been widely explored in various domains, such ascommunity detection, transaction analysis, and recommendation systems. In theseapplications, anomaly detection plays an important role. Recently, studies haveshown that anomalies on graphs induce spectral shifts. Some supervised methodshave improved the utilization of such spectral domain information. However,they remain limited by the scarcity of labeled data due to the nature ofanomalies. On the other hand, existing unsupervised learning approachespredominantly rely on spatial information or only employ low-pass filters,thereby losing the capacity for multi-band analysis. In this paper, we proposeGraph Autoencoder with Spectral Encoder and Spectral Decoder (GRASPED) for nodeanomaly detection. Our unsupervised learning model features an encoder based onGraph Wavelet Convolution, along with structural and attribute decoders. TheGraph Wavelet Convolution-based encoder, combined with a Wiener GraphDeconvolution-based decoder, exhibits bandpass filter characteristics thatcapture global and local graph information at multiple scales. This designallows for a learning-based reconstruction of node attributes, effectivelycapturing anomaly information. Extensive experiments on several real-worldgraph anomaly detection datasets demonstrate that GRASPED outperforms currentstate-of-the-art models.</description><author>Wei Herng Choong, Jixing Liu, Ching-Yu Kao, Philip Sperl</author><pubDate>Thu, 21 Aug 2025 14:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15633v1</guid></item><item><title>Scalable Time-Series Causal Discovery with Approximate Causal Ordering</title><link>http://arxiv.org/abs/2409.05500v3</link><description>Causal discovery in time-series data presents a significant computationalchallenge. Standard algorithms are often prohibitively expensive for datasetswith many variables or samples. This study introduces and validates a heuristicapproximation of the VarLiNGAM algorithm to address this scalability problem.The standard VarLiNGAM method relies on an iterative search, recalculatingstatistical dependencies after each step. Our heuristic modifies this procedureby omitting the iterative refinement. This change permits a one-timeprecomputation of all necessary statistical values. The algorithmicmodification reduces the time complexity from $O(m^3n)$ to $O(m^2n + m^3)$while keeping the space complexity at $O(m^2)$, where $m$ is the number ofvariables and $n$ is the number of samples. While an approximation, ourapproach retains VarLiNGAM's essential structure and empirical reliability. Onlarge-scale financial data with up to 400 variables, our algorithm achieves a7--13x speedup over the standard implementation and a 4.5x speedup over aGPU-accelerated version. Evaluations across medical imaging, web servermonitoring, and finance demonstrate the heuristic's robustness and practicalscalability. This work offers a validated balance between computationalefficiency and discovery quality, making large-scale causal analysis feasibleon personal computers.</description><author>Ziyang Jiao, Ce Guo, Wayne Luk</author><pubDate>Thu, 21 Aug 2025 14:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05500v3</guid></item><item><title>Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical Image-to-Image Generation</title><link>http://arxiv.org/abs/2405.14802v3</link><description>Denoising diffusion probabilistic models (DDPMs) have achieved unprecedentedsuccess in computer vision. However, they remain underutilized in medicalimaging, a field crucial for disease diagnosis and treatment planning. This isprimarily due to the high computational cost associated with (1) the use oflarge number of time steps (e.g., 1,000) in diffusion processes and (2) theincreased dimensionality of medical images, which are often 3D or 4D. Traininga diffusion model on medical images typically takes days to weeks, whilesampling each image volume takes minutes to hours. To address this challenge,we introduce Fast-DDPM, a simple yet effective approach capable of improvingtraining speed, sampling speed, and generation quality simultaneously. UnlikeDDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trainsand samples using only 10 time steps. The key to our method lies in aligningthe training and sampling procedures to optimize time-step utilization.Specifically, we introduced two efficient noise schedulers with 10 time steps:one with uniform time step sampling and another with non-uniform sampling. Weevaluated Fast-DDPM across three medical image-to-image generation tasks:multi-image super-resolution, image denoising, and image-to-image translation.Fast-DDPM outperformed DDPM and current state-of-the-art methods based onconvolutional networks and generative adversarial networks in all tasks.Additionally, Fast-DDPM reduced the training time to 0.2x and the sampling timeto 0.01x compared to DDPM. Our code is publicly available at:https://github.com/mirthAI/Fast-DDPM.</description><author>Hongxu Jiang, Muhammad Imran, Teng Zhang, Yuyin Zhou, Muxuan Liang, Kuang Gong, Wei Shao</author><pubDate>Thu, 21 Aug 2025 14:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14802v3</guid></item><item><title>Adapting A Vector-Symbolic Memory for Lisp ACT-R</title><link>http://arxiv.org/abs/2508.15630v1</link><description>Holographic Declarative Memory (HDM) is a vector-symbolic alternative toACT-R's Declarative Memory (DM) system that can bring advantages such asscalability and architecturally defined similarity between DM chunks. Weadapted HDM to work with the most comprehensive and widely-used implementationof ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run withHDM without major changes. With this adaptation of HDM, we have developedvector-based versions of common ACT-R functions, set up a text processingpipeline to add the contents of large documents to ACT-R memory, and mostsignificantly created a useful and novel mechanism to retrieve an entire chunkof memory based on a request using only vector representations of tokens.Preliminary results indicate that we can maintain vector-symbolic advantages ofHDM (e.g., chunk recall without storing the actual chunk and other advantageswith scaling) while also extending it so that previous ACT-R models may workwith the system with little (or potentially no) modifications within the actualprocedural and declarative memory portions of a model. As a part of iterativeimprovement of this newly translated holographic declarative memory module, wewill continue to explore better time-context representations for vectors toimprove the module's ability to reconstruct chunks during recall. To more fullytest this translated HDM module, we also plan to develop decision-making modelsthat use instance-based learning (IBL) theory, which is a useful application ofHDM given the advantages of the system.</description><author>Meera Ray, Christopher L. Dancy</author><pubDate>Thu, 21 Aug 2025 14:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15630v1</guid></item><item><title>Multi-perspective monitoring of wildlife and human activities from camera traps and drones with deep learning models</title><link>http://arxiv.org/abs/2508.15629v1</link><description>Wildlife and human activities are key components of landscape systems.Understanding their spatial distribution is essential for evaluating humanwildlife interactions and informing effective conservation planning.Multiperspective monitoring of wildlife and human activities by combiningcamera traps and drone imagery. Capturing the spatial patterns of theirdistributions, which allows the identification of the overlap of their activityzones and the assessment of the degree of human wildlife conflict. The studywas conducted in Chitwan National Park (CNP), Nepal, and adjacent regions.Images collected by visible and nearinfrared camera traps and thermal infrareddrones from February to July 2022 were processed to create training and testingdatasets, which were used to build deep learning models to automatic identifywildlife and human activities. Drone collected thermal imagery was used fordetecting targets to provide a multiple monitoring perspective. Spatial patternanalysis was performed to identify animal and resident activity hotspots anddelineation potential human wildlife conflict zones. Among the deep learningmodels tested, YOLOv11s achieved the highest performance with a precision of96.2%, recall of 92.3%, mAP50 of 96.7%, and mAP50 of 81.3%, making it the mosteffective for detecting objects in camera trap imagery. Drone based thermalimagery, analyzed with an enhanced Faster RCNN model, added a complementaryaerial viewpoint for camera trap detections. Spatial pattern analysisidentified clear hotspots for both wildlife and human activities and theiroverlapping patterns within certain areas in the CNP and buffer zonesindicating potential conflict. This study reveals human wildlife conflictswithin the conserved landscape. Integrating multiperspective monitoring withautomated object detection enhances wildlife surveillance and landscapemanagement.</description><author>Hao Chen, Fang Qiu, Li An, Douglas Stow, Eve Bohnett, Haitao Lyu, Shuang Tian</author><pubDate>Thu, 21 Aug 2025 14:53:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15629v1</guid></item><item><title>Documenting Deployment with Fabric: A Repository of Real-World AI Governance</title><link>http://arxiv.org/abs/2508.14119v2</link><description>Artificial intelligence (AI) is increasingly integrated into society, fromfinancial services and traffic management to creative writing. Academicliterature on the deployment of AI has mostly focused on the risks and harmsthat result from the use of AI. We introduce Fabric, a publicly availablerepository of deployed AI use cases to outline their governance mechanisms.Through semi-structured interviews with practitioners, we collect an initialset of 20 AI use cases. In addition, we co-design diagrams of the AI workflowwith the practitioners. We discuss the oversight mechanisms and guardrails usedin practice to safeguard AI use. The Fabric repository includes visual diagramsof AI use cases and descriptions of the deployed systems. Using the repository,we surface gaps in governance and find common patterns in human oversight ofdeployed AI systems. We intend for Fabric to serve as an extendable, evolvingtool for researchers to study the effectiveness of AI governance.</description><author>Mackenzie Jorgensen, Kendall Brogle, Katherine M. Collins, Lujain Ibrahim, Arina Shah, Petra Ivanovic, Noah Broestl, Gabriel Piles, Paul Dongha, Hatim Abdulhussein, Adrian Weller, Jillian Powers, Umang Bhatt</author><pubDate>Thu, 21 Aug 2025 14:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14119v2</guid></item><item><title>Trained Miniatures: Low cost, High Efficacy SLMs for Sales &amp; Marketing</title><link>http://arxiv.org/abs/2508.15617v1</link><description>Large language models (LLMs) excel in text generation; however, thesecreative elements require heavy computation and are accompanied by a steepcost. Especially for targeted applications such as sales and marketingoutreach, these costs are far from feasible. This paper introduces the conceptof "Trained Miniatures" - Small Language Models(SLMs) fine-tuned for specific,high-value applications, generating similar domain-specific responses for afraction of the cost.</description><author>Ishaan Bhola, Mukunda NS, Sravanth Kurmala, Harsh Nandwani, Arihant Jain</author><pubDate>Thu, 21 Aug 2025 14:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15617v1</guid></item><item><title>Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs</title><link>http://arxiv.org/abs/2504.19675v2</link><description>This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),which focussed on subject indexing using large language models (LLMs). The taskrequired creating subject predictions for bibliographic records from thebilingual TIBKAT database using the GND subject vocabulary. Our approachcombines traditional natural language processing and machine learningtechniques implemented in the Annif toolkit with innovative LLM-based methodsfor translation and synthetic data generation, and merging predictions frommonolingual models. The system ranked first in the all-subjects category andsecond in the tib-core-subjects category in the quantitative evaluation, andfourth in qualitative evaluations. These findings demonstrate the potential ofcombining traditional XMTC algorithms with modern LLM techniques to improve theaccuracy and efficiency of subject indexing in multilingual contexts.</description><author>Osma Suominen, Juho Inkinen, Mona Lehtinen</author><pubDate>Thu, 21 Aug 2025 14:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19675v2</guid></item><item><title>Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts</title><link>http://arxiv.org/abs/2407.02075v4</link><description>Few-shot semantic segmentation aims to segment objects from previously unseenclasses using only a limited number of labeled examples. In this paper, weintroduce Label Anything, a novel transformer-based architecture designed formulti-prompt, multi-way few-shot semantic segmentation. Our approach leveragesdiverse visual prompts -- points, bounding boxes, and masks -- to create ahighly flexible and generalizable framework that significantly reducesannotation burden while maintaining high accuracy. Label Anything makes threekey contributions: ($\textit{i}$) we introduce a new task formulation thatrelaxes conventional few-shot segmentation constraints by supporting varioustypes of prompts, multi-class classification, and enabling multiple promptswithin a single image; ($\textit{ii}$) we propose a novel architecture based ontransformers and attention mechanisms; and ($\textit{iii}$) we design aversatile training procedure allowing our model to operate seamlessly acrossdifferent $N$-way $K$-shot and prompt-type configurations with a single trainedmodel. Our extensive experimental evaluation on the widely used COCO-$20^i$benchmark demonstrates that Label Anything achieves state-of-the-artperformance among existing multi-way few-shot segmentation methods, whilesignificantly outperforming leading single-class models when evaluated inmulti-class settings. Code and trained models are available athttps://github.com/pasqualedem/LabelAnything.</description><author>Pasquale De Marinis, Nicola Fanelli, Raffaele Scaringi, Emanuele Colonna, Giuseppe Fiameni, Gennaro Vessio, Giovanna Castellano</author><pubDate>Thu, 21 Aug 2025 14:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02075v4</guid></item></channel></rss>