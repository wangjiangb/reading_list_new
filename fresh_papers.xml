<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 27 Jul 2023 06:00:24 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Virtual Mirrors: Non-Line-of-Sight Imaging Beyond the Third Bounce</title><link>http://arxiv.org/abs/2307.14341v1</link><description>Non-line-of-sight (NLOS) imaging methods are capable of reconstructingcomplex scenes that are not visible to an observer using indirect illumination.However, they assume only third-bounce illumination, so they are currentlylimited to single-corner configurations, and present limited visibility whenimaging surfaces at certain orientations. To reason about and tackle theselimitations, we make the key observation that planar diffuse surfaces behavespecularly at wavelengths used in the computational wave-based NLOS imagingdomain. We call such surfaces virtual mirrors. We leverage this observation toexpand the capabilities of NLOS imaging using illumination beyond the thirdbounce, addressing two problems: imaging single-corner objects at limitedvisibility angles, and imaging objects hidden behind two corners. To imageobjects at limited visibility angles, we first analyze the reflections of theknown illuminated point on surfaces of the scene as an estimator of theposition and orientation of objects with limited visibility. We then imagethose limited visibility objects by computationally building secondaryapertures at other surfaces that observe the target object from a directvisibility perspective. Beyond single-corner NLOS imaging, we exploit thespecular behavior of virtual mirrors to image objects hidden behind a secondcorner by imaging the space behind such virtual mirrors, where the mirror imageof objects hidden around two corners is formed. No specular surfaces wereinvolved in the making of this paper.</description><author>Diego Royo, Talha Sultan, Adolfo Muñoz, Khadijeh Masumnia-Bisheh, Eric Brandt, Diego Gutierrez, Andreas Velten, Julio Marco</author><pubDate>Wed, 26 Jul 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14341v1</guid></item><item><title>TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning</title><link>http://arxiv.org/abs/2307.14338v1</link><description>Deep learning (DL) models for tabular data problems are receivingincreasingly more attention, while the algorithms based on gradient-boosteddecision trees (GBDT) remain a strong go-to solution. Following the recenttrends in other domains, such as natural language processing and computervision, several retrieval-augmented tabular DL models have been recentlyproposed. For a given target object, a retrieval-based model retrieves otherrelevant objects, such as the nearest neighbors, from the available (training)data and uses their features or even labels to make a better prediction.However, we show that the existing retrieval-based tabular DL solutions provideonly minor, if any, benefits over the properly tuned simple retrieval-freebaselines. Thus, it remains unclear whether the retrieval-based approach is aworthy direction for tabular DL. In this work, we give a strong positive answer to this question. We start byincrementally augmenting a simple feed-forward architecture with anattention-like retrieval component similar to those of many (tabular)retrieval-based models. Then, we highlight several details of the attentionmechanism that turn out to have a massive impact on the performance on tabulardata problems, but that were not explored in prior work. As a result, we designTabR -- a simple retrieval-based tabular DL model which, on a set of publicbenchmarks, demonstrates the best average performance among tabular DL models,becomes the new state-of-the-art on several datasets, and even outperforms GBDTmodels on the recently proposed ``GBDT-friendly'' benchmark (see the firstfigure).</description><author>Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, Artem Babenko</author><pubDate>Wed, 26 Jul 2023 18:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14338v1</guid></item><item><title>MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation</title><link>http://arxiv.org/abs/2307.14336v1</link><description>We propose MAMo, a novel memory and attention frame-work for monocular videodepth estimation. MAMo can augment and improve any single-image depthestimation networks into video depth estimation models, enabling them to takeadvantage of the temporal information to predict more accurate depth. In MAMo,we augment model with memory which aids the depth prediction as the modelstreams through the video. Specifically, the memory stores learned visual anddisplacement tokens of the previous time instances. This allows the depthnetwork to cross-reference relevant features from the past when predictingdepth on the current frame. We introduce a novel scheme to continuously updatethe memory, optimizing it to keep tokens that correspond with both the past andthe present visual information. We adopt attention-based approach to processmemory features where we first learn the spatio-temporal relation among theresultant visual and displacement memory tokens using self-attention module.Further, the output features of self-attention are aggregated with the currentvisual features through cross-attention. The cross-attended features arefinally given to a decoder to predict depth on the current frame. Throughextensive experiments on several benchmarks, including KITTI, NYU-Depth V2, andDDAD, we show that MAMo consistently improves monocular depth estimationnetworks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo videodepth estimation provides higher accuracy with lower latency, when omparing toSOTA cost-volume-based video depth models.</description><author>Rajeev Yasarla, Hong Cai, Jisoo Jeong, Yunxiao Shi, Risheek Garrepalli, Fatih Porikli</author><pubDate>Wed, 26 Jul 2023 18:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14336v1</guid></item><item><title>WavJourney: Compositional Audio Creation with Large Language Models</title><link>http://arxiv.org/abs/2307.14335v1</link><description>Large Language Models (LLMs) have shown great promise in integrating diverseexpert models to tackle intricate language and vision tasks. Despite theirsignificance in advancing the field of Artificial Intelligence GeneratedContent (AIGC), their potential in intelligent audio content creation remainsunexplored. In this work, we tackle the problem of creating audio content withstorylines encompassing speech, music, and sound effects, guided by textinstructions. We present WavJourney, a system that leverages LLMs to connectvarious audio models for audio content generation. Given a text description ofan auditory scene, WavJourney first prompts LLMs to generate a structuredscript dedicated to audio storytelling. The audio script incorporates diverseaudio elements, organized based on their spatio-temporal relationships. As aconceptual representation of audio, the audio script provides an interactiveand interpretable rationale for human engagement. Afterward, the audio scriptis fed into a script compiler, converting it into a computer program. Each lineof the program calls a task-specific audio generation model or computationaloperation function (e.g., concatenate, mix). The computer program is thenexecuted to obtain an explainable solution for audio generation. We demonstratethe practicality of WavJourney across diverse real-world scenarios, includingscience fiction, education, and radio play. The explainable and interactivedesign of WavJourney fosters human-machine co-creation in multi-rounddialogues, enhancing creative control and adaptability in audio production.WavJourney audiolizes the human imagination, opening up new avenues forcreativity in multimedia content creation.</description><author>Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D. Plumbley, Wenwu Wang</author><pubDate>Wed, 26 Jul 2023 18:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14335v1</guid></item><item><title>Towards Generalist Biomedical AI</title><link>http://arxiv.org/abs/2307.14334v1</link><description>Medicine is inherently multimodal, with rich data modalities spanning text,imaging, genomics, and more. Generalist biomedical artificial intelligence (AI)systems that flexibly encode, integrate, and interpret this data at scale canpotentially enable impactful applications ranging from scientific discovery tocare delivery. To enable the development of these models, we first curateMultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses14 diverse tasks such as medical question answering, mammography anddermatology image interpretation, radiology report generation andsummarization, and genomic variant calling. We then introduce Med-PaLMMultimodal (Med-PaLM M), our proof of concept for a generalist biomedical AIsystem. Med-PaLM M is a large multimodal generative model that flexibly encodesand interprets biomedical data including clinical language, imaging, andgenomics with the same set of model weights. Med-PaLM M reaches performancecompetitive with or exceeding the state of the art on all MultiMedBench tasks,often surpassing specialist models by a wide margin. We also report examples ofzero-shot generalization to novel medical concepts and tasks, positive transferlearning across tasks, and emergent zero-shot medical reasoning. To furtherprobe the capabilities and limitations of Med-PaLM M, we conduct a radiologistevaluation of model-generated (and human) chest X-ray reports and observeencouraging performance across model scales. In a side-by-side ranking on 246retrospective chest X-rays, clinicians express a pairwise preference forMed-PaLM M reports over those produced by radiologists in up to 40.50% ofcases, suggesting potential clinical utility. While considerable work is neededto validate these models in real-world use cases, our results represent amilestone towards the development of generalist biomedical AI systems.</description><author>Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, Basil Mustafa, Aakanksha Chowdhery, Yun Liu, Simon Kornblith, David Fleet, Philip Mansfield, Sushant Prakash, Renee Wong, Sunny Virmani, Christopher Semturs, S Sara Mahdavi, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Karan Singhal, Pete Florence, Alan Karthikesalingam, Vivek Natarajan</author><pubDate>Wed, 26 Jul 2023 18:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14334v1</guid></item><item><title>Event-based Vision for Early Prediction of Manipulation Actions</title><link>http://arxiv.org/abs/2307.14332v1</link><description>Neuromorphic visual sensors are artificial retinas that output sequences ofasynchronous events when brightness changes occur in the scene. These sensorsoffer many advantages including very high temporal resolution, no motion blurand smart data compression ideal for real-time processing. In this study, weintroduce an event-based dataset on fine-grained manipulation actions andperform an experimental study on the use of transformers for action predictionwith events. There is enormous interest in the fields of cognitive robotics andhuman-robot interaction on understanding and predicting human actions as earlyas possible. Early prediction allows anticipating complex stages for planning,enabling effective and real-time interaction. Our Transformer network usesevents to predict manipulation actions as they occur, using online inference.The model succeeds at predicting actions early on, building up confidence overtime and achieving state-of-the-art classification. Moreover, theattention-based transformer architecture allows us to study the role of thespatio-temporal patterns selected by the model. Our experiments show that theTransformer network captures action dynamic features outperforming video-basedapproaches and succeeding with scenarios where the differences between actionslie in very subtle cues. Finally, we release the new event dataset, which isthe first in the literature for manipulation action recognition. Code will beavailable at https://github.com/DaniDeniz/EventVisionTransformer.</description><author>Daniel Deniz, Cornelia Fermuller, Eduardo Ros, Manuel Rodriguez-Alvarez, Francisco Barranco</author><pubDate>Wed, 26 Jul 2023 18:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14332v1</guid></item><item><title>Visual Instruction Inversion: Image Editing via Visual Prompting</title><link>http://arxiv.org/abs/2307.14331v1</link><description>Text-conditioned image editing has emerged as a powerful tool for editingimages. However, in many situations, language can be ambiguous and ineffectivein describing specific image edits. When faced with such challenges, visualprompts can be a more informative and intuitive way to convey ideas. We presenta method for image editing via visual prompting. Given pairs of example thatrepresent the "before" and "after" images of an edit, our goal is to learn atext-based editing direction that can be used to perform the same edit on newimages. We leverage the rich, pretrained editing capabilities of text-to-imagediffusion models by inverting visual prompts into editing instructions. Ourresults show that with just one example pair, we can achieve competitiveresults compared to state-of-the-art text-conditioned image editing frameworks.</description><author>Thao Nguyen, Yuheng Li, Utkarsh Ojha, Yong Jae Lee</author><pubDate>Wed, 26 Jul 2023 18:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14331v1</guid></item><item><title>Waypoint-Based Imitation Learning for Robotic Manipulation</title><link>http://arxiv.org/abs/2307.14326v1</link><description>While imitation learning methods have seen a resurgent interest for roboticmanipulation, the well-known problem of compounding errors continues to afflictbehavioral cloning (BC). Waypoints can help address this problem by reducingthe horizon of the learning problem for BC, and thus, the errors compoundedover time. However, waypoint labeling is underspecified, and requiresadditional human supervision. Can we generate waypoints automatically withoutany additional human supervision? Our key insight is that if a trajectorysegment can be approximated by linear motion, the endpoints can be used aswaypoints. We propose Automatic Waypoint Extraction (AWE) for imitationlearning, a preprocessing module to decompose a demonstration into a minimalset of waypoints which when interpolated linearly can approximate thetrajectory up to a specified error threshold. AWE can be combined with any BCalgorithm, and we find that AWE can increase the success rate ofstate-of-the-art algorithms by up to 25% in simulation and by 4-28% onreal-world bimanual manipulation tasks, reducing the decision making horizon byup to a factor of 10. Videos and code are available athttps://lucys0.github.io/awe/</description><author>Lucy Xiaoyang Shi, Archit Sharma, Tony Z. Zhao, Chelsea Finn</author><pubDate>Wed, 26 Jul 2023 18:45:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14326v1</guid></item><item><title>Evaluating the Moral Beliefs Encoded in LLMs</title><link>http://arxiv.org/abs/2307.14324v1</link><description>This paper presents a case study on the design, administration,post-processing, and evaluation of surveys on large language models (LLMs). Itcomprises two components: (1) A statistical method for eliciting beliefsencoded in LLMs. We introduce statistical measures and evaluation metrics thatquantify the probability of an LLM "making a choice", the associateduncertainty, and the consistency of that choice. (2) We apply this method tostudy what moral beliefs are encoded in different LLMs, especially in ambiguouscases where the right choice is not obvious. We design a large-scale surveycomprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a whitelie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for apedestrian on the road?"). Each scenario includes a description, two possibleactions, and auxiliary labels indicating violated rules (e.g., "do not kill").We administer the survey to 28 open- and closed-source LLMs. We find that (a)in unambiguous scenarios, most models "choose" actions that align withcommonsense. In ambiguous cases, most models express uncertainty. (b) Somemodels are uncertain about choosing the commonsense action because theirresponses are sensitive to the question-wording. (c) Some models reflect clearpreferences in ambiguous scenarios. Specifically, closed-source models tend toagree with each other.</description><author>Nino Scherrer, Claudia Shi, Amir Feder, David M. Blei</author><pubDate>Wed, 26 Jul 2023 18:42:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14324v1</guid></item><item><title>Modeling Inverse Demand Function with Explainable Dual Neural Networks</title><link>http://arxiv.org/abs/2307.14322v1</link><description>Financial contagion has been widely recognized as a fundamental risk to thefinancial system. Particularly potent is price-mediated contagion, whereinforced liquidations by firms depress asset prices and propagate financialstress, enabling crises to proliferate across a broad spectrum of seeminglyunrelated entities. Price impacts are currently modeled via exogenous inversedemand functions. However, in real-world scenarios, only the initial shocks andthe final equilibrium asset prices are typically observable, leaving actualasset liquidations largely obscured. This missing data presents significantlimitations to calibrating the existing models. To address these challenges, weintroduce a novel dual neural network structure that operates in two sequentialstages: the first neural network maps initial shocks to predicted assetliquidations, and the second network utilizes these liquidations to deriveresultant equilibrium prices. This data-driven approach can capture both linearand non-linear forms without pre-specifying an analytical structure;furthermore, it functions effectively even in the absence of observableliquidation data. Experiments with simulated datasets demonstrate that ourmodel can accurately predict equilibrium asset prices based solely on initialshocks, while revealing a strong alignment between predicted and trueliquidations. Our explainable framework contributes to the understanding andmodeling of price-mediated contagion and provides valuable insights forfinancial authorities to construct effective stress tests and regulatorypolicies.</description><author>Zhiyu Cao, Zihan Chen, Prerna Mishra, Hamed Amini, Zachary Feinstein</author><pubDate>Wed, 26 Jul 2023 18:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14322v1</guid></item><item><title>Reinforcement Learning by Guided Safe Exploration</title><link>http://arxiv.org/abs/2307.14316v1</link><description>Safety is critical to broadening the application of reinforcement learning(RL). Often, we train RL agents in a controlled environment, such as alaboratory, before deploying them in the real world. However, the real-worldtarget task might be unknown prior to deployment. Reward-free RL trains anagent without the reward to adapt quickly once the reward is revealed. Weconsider the constrained reward-free setting, where an agent (the guide) learnsto explore safely without the reward signal. This agent is trained in acontrolled environment, which allows unsafe interactions and still provides thesafety signal. After the target task is revealed, safety violations are notallowed anymore. Thus, the guide is leveraged to compose a safe behaviourpolicy. Drawing from transfer learning, we also regularize a target policy (thestudent) towards the guide while the student is unreliable and graduallyeliminate the influence of the guide as training progresses. The empiricalanalysis shows that this method can achieve safe transfer learning and helpsthe student solve the target task faster.</description><author>Qisong Yang, Thiago D. Simão, Nils Jansen, Simon H. Tindemans, Matthijs T. J. Spaan</author><pubDate>Wed, 26 Jul 2023 18:26:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14316v1</guid></item><item><title>Comparative Analysis of Libraries for the Sentimental Analysis</title><link>http://arxiv.org/abs/2307.14311v1</link><description>This study is main goal is to provide a comparative comparison of librariesusing machine learning methods. Experts in natural language processing (NLP)are becoming more and more interested in sentiment analysis (SA) of textchanges. The objective of employing NLP text analysis techniques is torecognize and categorize feelings related to twitter users utterances. In thisexamination, issues with SA and the libraries utilized are also looked at.provides a number of cooperative methods to classify emotional polarity. TheNaive Bayes Classifier, Decision Tree Classifier, Maxent Classifier, SklearnClassifier, Sklearn Classifier MultinomialNB, and other conjoint learningalgorithms, according to recent research, are very effective. In the projectwill use Five Python and R libraries NLTK, TextBlob, Vader, Transformers (GPTand BERT pretrained), and Tidytext will be used in the study to apply sentimentanalysis techniques. Four machine learning models Tree of Decisions (DT),Support Vector Machine (SVM), Naive Bayes (NB), and K-Nearest Neighbor (KNN)will also be used. To evaluate how well libraries for SA operate in the socialnetwork environment, comparative study was also carried out. The measures toassess the best algorithms in this experiment, which used a single data set foreach method, were precision, recall, and F1 score. We conclude that the BERTtransformer method with an Accuracy: 0.973 is recommended for sentimentanalysis.</description><author>Wendy Ccoya, Edson Pinto</author><pubDate>Wed, 26 Jul 2023 18:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14311v1</guid></item><item><title>UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer</title><link>http://arxiv.org/abs/2304.08870v2</link><description>Text-to-image models (T2I) such as StableDiffusion have been used to generatehigh quality images of people. However, due to the random nature of thegeneration process, the person has a different appearance e.g. pose, face, andclothing, despite using the same text prompt. The appearance inconsistencymakes T2I unsuitable for pose transfer. We address this by proposing amultimodal diffusion model that accepts text, pose, and visual prompting. Ourmodel is the first unified method to perform all person image tasks -generation, pose transfer, and mask-less edit. We also pioneer using smalldimensional 3D body model parameters directly to demonstrate new capability -simultaneous pose and camera view interpolation while maintaining the person'sappearance.</description><author>Soon Yau Cheong, Armin Mustafa, Andrew Gilbert</author><pubDate>Wed, 26 Jul 2023 18:13:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08870v2</guid></item><item><title>Automatically Evaluating Opinion Prevalence in Opinion Summarization</title><link>http://arxiv.org/abs/2307.14305v1</link><description>When faced with a large number of product reviews, it is not clear that ahuman can remember all of them and weight opinions representatively to write agood reference summary. We propose an automatic metric to test the prevalenceof the opinions that a summary expresses, based on counting the number ofreviews that are consistent with each statement in the summary, whilediscrediting trivial or redundant statements. To formulate this opinionprevalence metric, we consider several existing methods to score the factualconsistency of a summary statement with respect to each individual sourcereview. On a corpus of Amazon product reviews, we gather multiple humanjudgments of the opinion consistency, to determine which automatic metric bestexpresses consistency in product reviews. Using the resulting opinionprevalence metric, we show that a human authored summary has only slightlybetter opinion prevalence than randomly selected extracts from the sourcereviews, and previous extractive and abstractive unsupervised opinionsummarization methods perform worse than humans. We demonstrate room forimprovement with a greedy construction of extractive summaries with twice theopinion prevalence achieved by humans. Finally, we show that preprocessingsource reviews by simplification can raise the opinion prevalence achieved byexisting abstractive opinion summarization systems to the level of humanperformance.</description><author>Christopher Malon</author><pubDate>Wed, 26 Jul 2023 18:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14305v1</guid></item><item><title>A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch</title><link>http://arxiv.org/abs/2307.14304v1</link><description>The optimal dispatch of energy storage systems (ESSs) presents formidablechallenges due to the uncertainty introduced by fluctuations in dynamic prices,demand consumption, and renewable-based energy generation. By exploiting thegeneralization capabilities of deep neural networks (DNNs), deep reinforcementlearning (DRL) algorithms can learn good-quality control models that adaptivelyrespond to distribution networks' stochastic nature. However, current DRLalgorithms lack the capabilities to enforce operational constraints strictly,often even providing unfeasible control actions. To address this issue, wepropose a DRL framework that effectively handles continuous action spaces whilestrictly enforcing the environments and action space operational constraintsduring online operation. Firstly, the proposed framework trains an action-valuefunction modeled using DNNs. Subsequently, this action-value function isformulated as a mixed-integer programming (MIP) formulation enabling theconsideration of the environment's operational constraints. Comprehensivenumerical simulations show the superior performance of the proposed MIP-DRLframework, effectively enforcing all constraints while delivering high-qualitydispatch decisions when compared with state-of-the-art DRL algorithms and theoptimal solution obtained with a perfect forecast of the stochastic variables.</description><author>Shengren Hou, Edgar Mauricio Salazar Duque, Peter Palensky, Pedro P. Vergara</author><pubDate>Wed, 26 Jul 2023 18:12:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14304v1</guid></item><item><title>TreeFlow: Going beyond Tree-based Gaussian Probabilistic Regression</title><link>http://arxiv.org/abs/2206.04140v2</link><description>The tree-based ensembles are known for their outstanding performance inclassification and regression problems characterized by feature vectorsrepresented by mixed-type variables from various ranges and domains. However,considering regression problems, they are primarily designed to providedeterministic responses or model the uncertainty of the output with Gaussian orparametric distribution. In this work, we introduce TreeFlow, the tree-basedapproach that combines the benefits of using tree ensembles with thecapabilities of modeling flexible probability distributions using normalizingflows. The main idea of the solution is to use a tree-based model as a featureextractor and combine it with a conditional variant of normalizing flow.Consequently, our approach is capable of modeling complex distributions for theregression outputs. We evaluate the proposed method on challenging regressionbenchmarks with varying volume, feature characteristics, and targetdimensionality. We obtain the SOTA results for both probabilistic anddeterministic metrics on datasets with multi-modal target distributions andcompetitive results on unimodal ones compared to tree-based regressionbaselines.</description><author>Patryk Wielopolski, Maciej Zięba</author><pubDate>Wed, 26 Jul 2023 18:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.04140v2</guid></item><item><title>ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality</title><link>http://arxiv.org/abs/2307.14298v1</link><description>Recommender systems have become indispensable tools in the hotel hospitalityindustry, enabling personalized and tailored experiences for guests. Recentadvancements in large language models (LLMs), such as ChatGPT, and persuasivetechnologies, have opened new avenues for enhancing the effectiveness of thosesystems. This paper explores the potential of integrating ChatGPT andpersuasive technologies for automating and improving hotel hospitalityrecommender systems. First, we delve into the capabilities of ChatGPT, whichcan understand and generate human-like text, enabling more accurate andcontext-aware recommendations. We discuss the integration of ChatGPT intorecommender systems, highlighting the ability to analyze user preferences,extract valuable insights from online reviews, and generate personalizedrecommendations based on guest profiles. Second, we investigate the role ofpersuasive technology in influencing user behavior and enhancing the persuasiveimpact of hotel recommendations. By incorporating persuasive techniques, suchas social proof, scarcity and personalization, recommender systems caneffectively influence user decision-making and encourage desired actions, suchas booking a specific hotel or upgrading their room. To investigate theefficacy of ChatGPT and persuasive technologies, we present a pilot experi-mentwith a case study involving a hotel recommender system. We aim to study theimpact of integrating ChatGPT and persua-sive techniques on user engagement,satisfaction, and conversion rates. The preliminary results demonstrate thepotential of these technologies in enhancing the overall guest experience andbusiness performance. Overall, this paper contributes to the field of hotelhospitality by exploring the synergistic relationship between LLMs andpersuasive technology in recommender systems, ultimately influencing guestsatisfaction and hotel revenue.</description><author>Manolis Remountakis, Konstantinos Kotis, Babis Kourtzis, George E. Tsekouras</author><pubDate>Wed, 26 Jul 2023 17:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14298v1</guid></item><item><title>Multimodal Manoeuvre and Trajectory Prediction for Automated Driving on Highways Using Transformer Networks</title><link>http://arxiv.org/abs/2303.16109v2</link><description>Predicting the behaviour (i.e., manoeuvre/trajectory) of other road users,including vehicles, is critical for the safe and efficient operation ofautonomous vehicles (AVs), a.k.a., automated driving systems (ADSs). Due to theuncertain future behaviour of vehicles, multiple future behaviour modes areoften plausible for a vehicle in a given driving scene. Therefore, multimodalprediction can provide richer information than single-mode prediction, enablingAVs to perform a better risk assessment. To this end, we propose a novelmultimodal prediction framework that can predict multiple plausible behaviourmodes and their likelihoods. The proposed framework includes a bespoke problemformulation for manoeuvre prediction, a novel transformer-based predictionmodel, and a tailored training method for multimodal manoeuvre and trajectoryprediction. The performance of the framework is evaluated using three publichighway driving datasets, namely NGSIM, highD, and exiD. The results show thatour framework outperforms the state-of-the-art multimodal methods in terms ofprediction error and is capable of predicting plausible manoeuvre andtrajectory modes.</description><author>Sajjad Mozaffari, Mreza Alipour Sormoli, Konstantinos Koufos, Mehrdad Dianati</author><pubDate>Wed, 26 Jul 2023 17:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16109v2</guid></item><item><title>Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis</title><link>http://arxiv.org/abs/2307.14294v1</link><description>Splitting of sequential data, such as videos and time series, is an essentialstep in various data analysis tasks, including object tracking and anomalydetection. However, splitting sequential data presents a variety of challengesthat can impact the accuracy and reliability of subsequent analyses. Thisconcept article examines the challenges associated with splitting sequentialdata, including data acquisition, data representation, split ratio selection,setting up quality criteria, and choosing suitable selection strategies. Weexplore these challenges through two real-world examples: motor test benchesand particle tracking in liquids.</description><author>Diego Botache, Kristina Dingel, Rico Huhnstock, Arno Ehresmann, Bernhard Sick</author><pubDate>Wed, 26 Jul 2023 17:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14294v1</guid></item><item><title>An optimal control perspective on diffusion-based generative modeling</title><link>http://arxiv.org/abs/2211.01364v2</link><description>We establish a connection between stochastic optimal control and generativemodels based on stochastic differential equations (SDEs), such as recentlydeveloped diffusion probabilistic models. In particular, we derive aHamilton-Jacobi-Bellman equation that governs the evolution of thelog-densities of the underlying SDE marginals. This perspective allows totransfer methods from optimal control theory to generative modeling. First, weshow that the evidence lower bound is a direct consequence of the well-knownverification theorem from control theory. Further, we can formulatediffusion-based generative modeling as a minimization of the Kullback-Leiblerdivergence between suitable measures in path space. Finally, we develop a noveldiffusion-based method for sampling from unnormalized densities -- a problemfrequently occurring in statistics and computational sciences. We demonstratethat our time-reversed diffusion sampler (DIS) can outperform otherdiffusion-based sampling approaches on multiple numerical examples.</description><author>Julius Berner, Lorenz Richter, Karen Ullrich</author><pubDate>Wed, 26 Jul 2023 17:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01364v2</guid></item><item><title>Founding a mathematical diffusion model in linguistics. The case study of German syntactic features in the North-Eastern Italian dialects</title><link>http://arxiv.org/abs/2307.14291v1</link><description>We take as a case study the spread of Germanic syntactic features intoRomance dialects of North-Eastern Italy, which occurred after the immigrationof German people in the Tyrol during the High Middle Ages. An interactive map is produced using tools of what is called Geographic DataScience. A smooth two-dimensional surface $\mathcal{G}$ expresses locally whichfraction of territory uses a given German language feature: it is obtained byinterpolating a discrete function that says if at any surveyed locality thatfeature is used or not.\newline This surface $\mathcal{G}$ is thought of as the value at the present time ofa function describing a diffusion-convection phenomenon in two dimensions (heresaid \emph{tidal} mode), which is subjected in a very natural way to the sameequation, suitably contextualized, used in physics for a number ofphenomenological facts like the heat diffusion. It is shown that solutions ofthis equation, evaluated at the present time, fit well with the data asinterpolated by $\mathcal{G}$, thus providing convincing pictures ofdiffusion-convection of the linguistic features of the case study, albeitsimplifications and approximations.\newline Very importantly, it is shown that Schmidt's 'waves' can be counted among thesolutions of the diffusion equation: superimposing Schmidt 'waves' to a 'tidalflooding' can reproduce complexities of real linguistic diffusion events.</description><author>I. Lazzizzera</author><pubDate>Wed, 26 Jul 2023 17:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14291v1</guid></item><item><title>US &amp; MR Image-Fusion Based on Skin Co-Registration</title><link>http://arxiv.org/abs/2307.14288v1</link><description>The study and development of innovative solutions for the advancedvisualisation, representation and analysis of medical images offer differentresearch directions. Current practice in medical imaging consists in combiningreal-time US with imaging modalities that allow internal anatomy acquisitions,such as CT, MRI, PET or similar. Application of image-fusion approaches can befound in tracking surgical tools and/or needles, in real-time duringinterventions. Thus, this work proposes a fusion imaging system for theregistration of CT and MRI images with real-time US acquisition leveraging a 3Dcamera sensor. The main focus of the work is the portability of the system andits applicability to different anatomical districts.</description><author>Martina Paccini, Giacomo Paschina, Stefano De Beni, Giuseppe Patanè</author><pubDate>Wed, 26 Jul 2023 17:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14288v1</guid></item><item><title>Emerging Statistical Machine Learning Techniques for Extreme Temperature Forecasting in U.S. Cities</title><link>http://arxiv.org/abs/2307.14285v1</link><description>In this paper, we present a comprehensive analysis of extreme temperaturepatterns using emerging statistical machine learning techniques. Our researchfocuses on exploring and comparing the effectiveness of various statisticalmodels for climate time series forecasting. The models considered includeAuto-Regressive Integrated Moving Average, Exponential Smoothing, MultilayerPerceptrons, and Gaussian Processes. We apply these methods to climate timeseries data from five most populated U.S. cities, utilizing Python and Julia todemonstrate the role of statistical computing in understanding climate changeand its impacts. Our findings highlight the differences between the statisticalmethods and identify Multilayer Perceptrons as the most effective approach.Additionally, we project extreme temperatures using this best-performingmethod, up to 2030, and examine whether the temperature changes are greaterthan zero, thereby testing a hypothesis.</description><author>Kameron B. Kinast, Ernest Fokoué</author><pubDate>Wed, 26 Jul 2023 17:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14285v1</guid></item><item><title>General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications</title><link>http://arxiv.org/abs/2307.14283v1</link><description>Most applications of Artificial Intelligence (AI) are designed for a confinedand specific task. However, there are many scenarios that call for a moregeneral AI, capable of solving a wide array of tasks without being specificallydesigned for them. The term General-Purpose Artificial Intelligence Systems(GPAIS) has been defined to refer to these AI systems. To date, the possibilityof an Artificial General Intelligence, powerful enough to perform anyintellectual task as if it were human, or even improve it, has remained anaspiration, fiction, and considered a risk for our society. Whilst we mightstill be far from achieving that, GPAIS is a reality and sitting at theforefront of AI research. This work discusses existing definitions for GPAIS and proposes a newdefinition that allows for a gradual differentiation among types of GPAISaccording to their properties and limitations. We distinguish betweenclosed-world and open-world GPAIS, characterising their degree of autonomy andability based on several factors such as adaptation to new tasks, competence indomains not intentionally trained for, ability to learn from few data, orproactive acknowledgment of their own limitations. We then propose a taxonomyof approaches to realise GPAIS, describing research trends such as the use ofAI techniques to improve another AI or foundation models. As a prime example,we delve into generative AI, aligning them with the terms and conceptspresented in the taxonomy. Through the proposed definition and taxonomy, ouraim is to facilitate research collaboration across different areas that aretackling general-purpose tasks, as they share many common aspects. Finally, wediscuss the current state of GPAIS, its challenges and prospects, implicationsfor our society, and the need for responsible and trustworthy AI systems andregulation, with the goal of providing a holistic view of GPAIS.</description><author>Isaac Triguero, Daniel Molina, Javier Poyatos, Javier Del Ser, Francisco Herrera</author><pubDate>Wed, 26 Jul 2023 17:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14283v1</guid></item><item><title>Uniformity Testing over Hypergrids with Subcube Conditioning</title><link>http://arxiv.org/abs/2302.09013v2</link><description>We give an algorithm for testing uniformity of distributions supported onhypergrids $[m_1] \times \cdots \times [m_n]$, which makes$\smash{\widetilde{O}(\text{poly}(m)\sqrt{n}/\epsilon^2)}$ many queries to asubcube conditional sampling oracle with $m=\max_i m_i$. When $m$ is aconstant, our algorithm is nearly optimal and strengthens the algorithm of[CCK+21] which has the same query complexity but works for hypercubes $\{\pm1\}^n$ only. A key technical contribution behind the analysis of our algorithm is a proofof a robust version of Pisier's inequality for functions over hypergrids usingFourier analysis.</description><author>Xi Chen, Cassandra Marcussen</author><pubDate>Wed, 26 Jul 2023 17:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09013v2</guid></item><item><title>Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review</title><link>http://arxiv.org/abs/2210.03829v2</link><description>This paper provides a comprehensive review of past and current advances inthe early detection of bark beetle-induced tree mortality from three primaryperspectives: bark beetle &amp; host interactions, RS, and ML/DL. In contrast toprior efforts, this review encompasses all RS systems and emphasizes ML/DLmethods to investigate their strengths and weaknesses. We parse existingliterature based on multi- or hyper-spectral analyses and distill theirknowledge based on: bark beetle species &amp; attack phases with a primary emphasison early stages of attacks, host trees, study regions, RS platforms &amp; sensors,spectral/spatial/temporal resolutions, spectral signatures, spectral vegetationindices (SVIs), ML approaches, learning schemes, task categories, models,algorithms, classes/clusters, features, and DL networks &amp; architectures.Although DL-based methods and the random forest (RF) algorithm showed promisingresults, highlighting their potential to detect subtle changes across visible,thermal, and short-wave infrared (SWIR) spectral regions, they still havelimited effectiveness and high uncertainties. To inspire novel solutions tothese shortcomings, we delve into the principal challenges &amp; opportunities fromdifferent perspectives, enabling a deeper understanding of the current state ofresearch and guiding future research directions.</description><author>Seyed Mojtaba Marvasti-Zadeh, Devin Goodsman, Nilanjan Ray, Nadir Erbilgin</author><pubDate>Wed, 26 Jul 2023 17:26:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03829v2</guid></item><item><title>Estimating large causal polytrees from small samples</title><link>http://arxiv.org/abs/2209.07028v2</link><description>We consider the problem of estimating a large causal polytree from arelatively small i.i.d. sample. This is motivated by the problem of determiningcausal structure when the number of variables is very large compared to thesample size, such as in gene regulatory networks. We give an algorithm thatrecovers the tree with high accuracy in such settings. The algorithm worksunder essentially no distributional or modeling assumptions other than somemild non-degeneracy conditions.</description><author>Sourav Chatterjee, Mathukumalli Vidyasagar</author><pubDate>Wed, 26 Jul 2023 17:21:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.07028v2</guid></item><item><title>Large-scale Fully-Unsupervised Re-Identification</title><link>http://arxiv.org/abs/2307.14278v1</link><description>Fully-unsupervised Person and Vehicle Re-Identification have receivedincreasing attention due to their broad applicability in surveillance,forensics, event understanding, and smart cities, without requiring any manualannotation. However, most of the prior art has been evaluated in datasets thathave just a couple thousand samples. Such small-data setups often allow the useof costly techniques in time and memory footprints, such as Re-Ranking, toimprove clustering results. Moreover, some previous work even pre-selects thebest clustering hyper-parameters for each dataset, which is unrealistic in alarge-scale fully-unsupervised scenario. In this context, this work tackles amore realistic scenario and proposes two strategies to learn from large-scaleunlabeled data. The first strategy performs a local neighborhood sampling toreduce the dataset size in each iteration without violating neighborhoodrelationships. A second strategy leverages a novel Re-Ranking technique, whichhas a lower time upper bound complexity and reduces the memory complexity fromO(n^2) to O(kn) with k &lt;&lt; n. To avoid the pre-selection of specifichyper-parameter values for the clustering algorithm, we also present a novelscheduling algorithm that adjusts the density parameter during training, toleverage the diversity of samples and keep the learning robust to noisylabeling. Finally, due to the complementary knowledge learned by differentmodels, we also introduce a co-training strategy that relies upon thepermutation of predicted pseudo-labels, among the backbones, with no need forany hyper-parameters or weighting optimization. The proposed methodologyoutperforms the state-of-the-art methods in well-known benchmarks and in thechallenging large-scale Veri-Wild dataset, with a faster and memory-efficientRe-Ranking strategy, and a large-scale, noisy-robust, and ensemble-basedlearning approach.</description><author>Gabriel Bertocco, Fernanda Andaló, Terrance E. Boult, Anderson Rocha</author><pubDate>Wed, 26 Jul 2023 17:19:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14278v1</guid></item><item><title>GPT-3 Models are Few-Shot Financial Reasoners</title><link>http://arxiv.org/abs/2307.13617v2</link><description>Financial analysis is an important tool for evaluating company performance.Practitioners work to answer financial questions to make profitable investmentdecisions, and use advanced quantitative analyses to do so. As a result,Financial Question Answering (QA) is a question answering task that requiresdeep reasoning about numbers. Furthermore, it is unknown how well pre-trainedlanguage models can reason in the financial domain. The currentstate-of-the-art requires a retriever to collect relevant facts about thefinancial question from the text and a generator to produce a valid financialprogram and a final answer. However, recently large language models like GPT-3have achieved state-of-the-art performance on wide variety of tasks with just afew shot examples. We run several experiments with GPT-3 and find that aseparate retrieval model and logic engine continue to be essential componentsto achieving SOTA performance in this task, particularly due to the precisenature of financial questions and the complex information stored in financialdocuments. With this understanding, our refined prompt-engineering approach onGPT-3 achieves near SOTA accuracy without any fine-tuning.</description><author>Raul Salles de Padua, Imran Qureshi, Mustafa U. Karakaplan</author><pubDate>Wed, 26 Jul 2023 17:14:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13617v2</guid></item><item><title>G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory</title><link>http://arxiv.org/abs/2307.14277v1</link><description>The recent video grounding works attempt to introduce vanilla contrastivelearning into video grounding. However, we claim that this naive solution issuboptimal. Contrastive learning requires two key properties: (1)\emph{alignment} of features of similar samples, and (2) \emph{uniformity} ofthe induced distribution of the normalized features on the hypersphere. Due totwo annoying issues in video grounding: (1) the co-existence of some visualentities in both ground truth and other moments, \ie semantic overlapping; (2)only a few moments in the video are annotated, \ie sparse annotation dilemma,vanilla contrastive learning is unable to model the correlations betweentemporally distant moments and learned inconsistent video representations. Bothcharacteristics lead to vanilla contrastive learning being unsuitable for videogrounding. In this paper, we introduce Geodesic and Game Localization (G2L), asemantically aligned and uniform video grounding framework via geodesic andgame theory. We quantify the correlations among moments leveraging the geodesicdistance that guides the model to learn the correct cross-modalrepresentations. Furthermore, from the novel perspective of game theory, wepropose semantic Shapley interaction based on geodesic distance sampling tolearn fine-grained semantic alignment in similar moments. Experiments on threebenchmarks demonstrate the effectiveness of our method.</description><author>Hongxiang Li, Meng Cao, Xuxin Cheng, Yaowei Li, Zhihong Zhu, Yuexian Zou</author><pubDate>Wed, 26 Jul 2023 17:14:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14277v1</guid></item><item><title>Combining optimal path search with task-dependent learning in a neural network</title><link>http://arxiv.org/abs/2201.11104v4</link><description>Finding optimal paths in connected graphs requires determining the smallesttotal cost for traveling along the graph's edges. This problem can be solved byseveral classical algorithms where, usually, costs are predefined for alledges. Conventional planning methods can, thus, normally not be used whenwanting to change costs in an adaptive way following the requirements of sometask. Here we show that one can define a neural network representation of pathfinding problems by transforming cost values into synaptic weights, whichallows for online weight adaptation using network learning mechanisms. Whenstarting with an initial activity value of one, activity propagation in thisnetwork will lead to solutions, which are identical to those found by theBellman-Ford algorithm. The neural network has the same algorithmic complexityas Bellman-Ford and, in addition, we can show that network learning mechanisms(such as Hebbian learning) can adapt the weights in the network augmenting theresulting paths according to some task at hand. We demonstrate this by learningto navigate in an environment with obstacles as well as by learning to followcertain sequences of path nodes. Hence, the here-presented novel algorithm mayopen up a different regime of applications where path-augmentation (bylearning) is directly coupled with path finding in a natural way.</description><author>Tomas Kulvicius, Minija Tamosiunaite, Florentin Wörgötter</author><pubDate>Wed, 26 Jul 2023 17:13:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.11104v4</guid></item><item><title>Deepfake Image Generation for Improved Brain Tumor Segmentation</title><link>http://arxiv.org/abs/2307.14273v1</link><description>As the world progresses in technology and health, awareness of disease byrevealing asymptomatic signs improves. It is important to detect and treattumors in early stage as it can be life-threatening. Computer-aidedtechnologies are used to overcome lingering limitations facing diseasediagnosis, while brain tumor segmentation remains a difficult process,especially when multi-modality data is involved. This is mainly attributed toineffective training due to lack of data and corresponding labelling. This workinvestigates the feasibility of employing deep-fake image generation foreffective brain tumor segmentation. To this end, a Generative AdversarialNetwork was used for image-to-image translation for increasing dataset size,followed by image segmentation using a U-Net-based convolutional neural networktrained with deepfake images. Performance of the proposed approach is comparedwith ground truth of four publicly available datasets. Results show improvedperformance in terms of image segmentation quality metrics, and couldpotentially assist when training with limited data.</description><author>Roa'a Al-Emaryeen, Sara Al-Nahhas, Fatima Himour, Waleed Mahafza, Omar Al-Kadi</author><pubDate>Wed, 26 Jul 2023 17:11:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14273v1</guid></item><item><title>MMBench: Is Your Multi-modal Model an All-around Player?</title><link>http://arxiv.org/abs/2307.06281v2</link><description>Large vision-language models have recently achieved remarkable progress,exhibiting great perception and reasoning abilities concerning visualinformation. However, how to effectively evaluate these large vision-languagemodels remains a major obstacle, hindering future model development.Traditional benchmarks like VQAv2 or COCO Caption provide quantitativeperformance measurements but suffer from a lack of fine-grained abilityassessment and non-robust evaluation metrics. Recent subjective benchmarks,such as OwlEval, offer comprehensive evaluations of a model's abilities byincorporating human labor, but they are not scalable and display significantbias. In response to these challenges, we propose MMBench, a novelmulti-modality benchmark. MMBench methodically develops a comprehensiveevaluation pipeline, primarily comprised of two elements. The first element isa meticulously curated dataset that surpasses existing similar benchmarks interms of the number and variety of evaluation questions and abilities. Thesecond element introduces a novel CircularEval strategy and incorporates theuse of ChatGPT. This implementation is designed to convert free-formpredictions into pre-defined choices, thereby facilitating a more robustevaluation of the model's predictions. MMBench is a systematically-designedobjective benchmark for robustly evaluating the various abilities ofvision-language models. We hope MMBench will assist the research community inbetter evaluating their models and encourage future advancements in thisdomain. Project page: https://opencompass.org.cn/mmbench.</description><author>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin</author><pubDate>Wed, 26 Jul 2023 17:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06281v2</guid></item><item><title>Event-based Stereo Visual Odometry with Native Temporal Resolution via Continuous-time Gaussian Process Regression</title><link>http://arxiv.org/abs/2306.01188v2</link><description>Event-based cameras asynchronously capture individual visual changes in ascene. This makes them more robust than traditional frame-based cameras tohighly dynamic motions and poor illumination. It also means that everymeasurement in a scene can occur at a unique time. Handling these different measurement times is a major challenge of usingevent-based cameras. It is often addressed in visual odometry (VO) pipelines byapproximating temporally close measurements as occurring at one common time.This grouping simplifies the estimation problem but, absent additional sensors,sacrifices the inherent temporal resolution of event-based cameras. This paper instead presents a complete stereo VO pipeline that estimatesdirectly with individual event-measurement times without requiring any groupingor approximation in the estimation state. It uses continuous-time trajectoryestimation to maintain the temporal fidelity and asynchronous nature ofevent-based cameras through Gaussian process regression with a physicallymotivated prior. Its performance is evaluated on the MVSEC dataset, where itachieves 7.9e-3 and 5.9e-3 RMS relative error on two independent sequences,outperforming the existing publicly available event-based stereo VO pipeline bytwo and four times, respectively.</description><author>Jianeng Wang, Jonathan D. Gammell</author><pubDate>Wed, 26 Jul 2023 16:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01188v2</guid></item><item><title>On Embeddings for Numerical Features in Tabular Deep Learning</title><link>http://arxiv.org/abs/2203.05556v3</link><description>Recently, Transformer-like deep architectures have shown strong performanceon tabular data problems. Unlike traditional models, e.g., MLP, thesearchitectures map scalar values of numerical features to high-dimensionalembeddings before mixing them in the main backbone. In this work, we argue thatembeddings for numerical features are an underexplored degree of freedom intabular DL, which allows constructing more powerful DL models and competingwith GBDT on some traditionally GBDT-friendly benchmarks. We start bydescribing two conceptually different approaches to building embedding modules:the first one is based on a piecewise linear encoding of scalar values, and thesecond one utilizes periodic activations. Then, we empirically demonstrate thatthese two approaches can lead to significant performance boosts compared to theembeddings based on conventional blocks such as linear layers and ReLUactivations. Importantly, we also show that embedding numerical features isbeneficial for many backbones, not only for Transformers. Specifically, afterproper embeddings, simple MLP-like models can perform on par with theattention-based architectures. Overall, we highlight embeddings for numericalfeatures as an important design aspect with good potential for furtherimprovements in tabular DL.</description><author>Yury Gorishniy, Ivan Rubachev, Artem Babenko</author><pubDate>Wed, 26 Jul 2023 16:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.05556v3</guid></item><item><title>Revisiting Deep Learning Models for Tabular Data</title><link>http://arxiv.org/abs/2106.11959v3</link><description>The existing literature on deep learning for tabular data proposes a widerange of novel architectures and reports competitive results on variousdatasets. However, the proposed models are usually not properly compared toeach other and existing works often use different benchmarks and experimentprotocols. As a result, it is unclear for both researchers and practitionerswhat models perform best. Additionally, the field still lacks effectivebaselines, that is, the easy-to-use models that provide competitive performanceacross different problems. In this work, we perform an overview of the main families of DL architecturesfor tabular data and raise the bar of baselines in tabular DL by identifyingtwo simple and powerful deep architectures. The first one is a ResNet-likearchitecture which turns out to be a strong baseline that is often missing inprior works. The second model is our simple adaptation of the Transformerarchitecture for tabular data, which outperforms other solutions on most tasks.Both models are compared to many existing architectures on a diverse set oftasks under the same training and tuning protocols. We also compare the best DLmodels with Gradient Boosted Decision Trees and conclude that there is still nouniversally superior solution.</description><author>Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko</author><pubDate>Wed, 26 Jul 2023 16:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.11959v3</guid></item><item><title>Improving International Climate Policy via Mutually Conditional Binding Commitments</title><link>http://arxiv.org/abs/2307.14267v1</link><description>The Paris Agreement, considered a significant milestone in climatenegotiations, has faced challenges in effectively addressing climate change dueto the unconditional nature of most Nationally Determined Contributions (NDCs).This has resulted in a prevalence of free-riding behavior among major pollutersand a lack of concrete conditionality in NDCs. To address this issue, wepropose the implementation of a decentralized, bottom-up approach called theConditional Commitment Mechanism. This mechanism, inspired by the NationalPopular Vote Interstate Compact, offers flexibility and incentives for earlyadopters, aiming to formalize conditional cooperation in international climatepolicy. In this paper, we provide an overview of the mechanism, its performancein the AI4ClimateCooperation challenge, and discuss potential real-worldimplementation aspects. Prior knowledge of the climate mitigation collectiveaction problem, basic economic principles, and game theory concepts areassumed.</description><author>Jobst Heitzig, Jörg Oechssler, Christoph Pröschel, Niranjana Ragavan, Yat Long Lo</author><pubDate>Wed, 26 Jul 2023 16:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14267v1</guid></item><item><title>Improving International Climate Policy via Mutually Conditional Binding Commitments</title><link>http://arxiv.org/abs/2307.14266v1</link><description>This paper proposes enhancements to the RICE-N simulation and multi-agentreinforcement learning framework to improve the realism of internationalclimate policy negotiations. Acknowledging the framework's value, we highlightthe necessity of significant enhancements to address the diverse array offactors in modeling climate negotiations. Building upon our previous work onthe "Conditional Commitments Mechanism" (CCF mechanism) we discuss ways tobridge the gap between simulation and reality. We suggest the inclusion of arecommender or planner agent to enhance coordination, address the Real2Sim gapby incorporating social factors and non-party stakeholder sub-agents, andpropose enhancements to the underlying Reinforcement Learning solutionalgorithm. These proposed improvements aim to advance the evaluation andformulation of negotiation protocols for more effective international climatepolicy decision-making in Rice-N. However, further experimentation and testingare required to determine the implications and effectiveness of thesesuggestions.</description><author>Jobst Heitzig, Jörg Oechssler, Christoph Pröschel, Niranjana Ragavan, Richie YatLong Lo</author><pubDate>Wed, 26 Jul 2023 16:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14266v1</guid></item><item><title>Artifact Restoration in Histology Images with Diffusion Probabilistic Models</title><link>http://arxiv.org/abs/2307.14262v1</link><description>Histological whole slide images (WSIs) can be usually compromised byartifacts, such as tissue folding and bubbles, which will increase theexamination difficulty for both pathologists and Computer-Aided Diagnosis (CAD)systems. Existing approaches to restoring artifact images are confined toGenerative Adversarial Networks (GANs), where the restoration process isformulated as an image-to-image transfer. Those methods are prone to sufferfrom mode collapse and unexpected mistransfer in the stain style, leading tounsatisfied and unrealistic restored images. Innovatively, we make the firstattempt at a denoising diffusion probabilistic model for histological artifactrestoration, namely ArtiFusion.Specifically, ArtiFusion formulates the artifactregion restoration as a gradual denoising process, and its training reliessolely on artifact-free images to simplify the training complexity.Furthermore,to capture local-global correlations in the regional artifact restoration, anovel Swin-Transformer denoising architecture is designed, along with a timetoken scheme. Our extensive evaluations demonstrate the effectiveness ofArtiFusion as a pre-processing method for histology analysis, which cansuccessfully preserve the tissue structures and stain style in artifact-freeregions during the restoration. Code is available athttps://github.com/zhenqi-he/ArtiFusion.</description><author>Zhenqi He, Junjun He, Jin Ye, Yiqing Shen</author><pubDate>Wed, 26 Jul 2023 16:50:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14262v1</guid></item><item><title>Exploring Multi-Modal Representations for Ambiguity Detection &amp; Coreference Resolution in the SIMMC 2.0 Challenge</title><link>http://arxiv.org/abs/2202.12645v2</link><description>Anaphoric expressions, such as pronouns and referential descriptions, aresituated with respect to the linguistic context of prior turns, as well as, theimmediate visual environment. However, a speaker's referential descriptions donot always uniquely identify the referent, leading to ambiguities in need ofresolution through subsequent clarificational exchanges. Thus, effectiveAmbiguity Detection and Coreference Resolution are key to task success inConversational AI. In this paper, we present models for these two tasks as partof the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERTand LXMERT based models, compare them to a number of baselines and provideablation experiments. Our results show that (1) language models are able toexploit correlations in the data to detect ambiguity; and (2) unimodalcoreference resolution models can avoid the need for a vision component,through the use of smart object representations.</description><author>Javier Chiyah-Garcia, Alessandro Suglia, José Lopes, Arash Eshghi, Helen Hastie</author><pubDate>Wed, 26 Jul 2023 16:49:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.12645v2</guid></item><item><title>Unifying Flow, Stereo and Depth Estimation</title><link>http://arxiv.org/abs/2211.05783v3</link><description>We present a unified formulation and model for three motion and 3D perceptiontasks: optical flow, rectified stereo matching and unrectified stereo depthestimation from posed images. Unlike previous specialized architectures foreach specific task, we formulate all three tasks as a unified densecorrespondence matching problem, which can be solved with a single model bydirectly comparing feature similarities. Such a formulation calls fordiscriminative feature representations, which we achieve using a Transformer,in particular the cross-attention mechanism. We demonstrate thatcross-attention enables integration of knowledge from another image viacross-view interactions, which greatly improves the quality of the extractedfeatures. Our unified model naturally enables cross-task transfer since themodel architecture and parameters are shared across tasks. We outperform RAFTwith our unified model on the challenging Sintel dataset, and our final modelthat uses a few additional task-specific refinement steps outperforms orcompares favorably to recent state-of-the-art methods on 10 popular flow,stereo and depth datasets, while being simpler and more efficient in terms ofmodel design and inference speed.</description><author>Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, Andreas Geiger</author><pubDate>Wed, 26 Jul 2023 16:42:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05783v3</guid></item><item><title>Sparse Double Descent in Vision Transformers: real or phantom threat?</title><link>http://arxiv.org/abs/2307.14253v1</link><description>Vision transformers (ViT) have been of broad interest in recent theoreticaland empirical works. They are state-of-the-art thanks to their attention-basedapproach, which boosts the identification of key features and patterns withinimages thanks to the capability of avoiding inductive bias, resulting in highlyaccurate image analysis. Meanwhile, neoteric studies have reported a ``sparsedouble descent'' phenomenon that can occur in modern deep-learning models,where extremely over-parametrized models can generalize well. This raisespractical questions about the optimal size of the model and the quest overfinding the best trade-off between sparsity and performance is launched: areVision Transformers also prone to sparse double descent? Can we find a way toavoid such a phenomenon? Our work tackles the occurrence of sparse doubledescent on ViTs. Despite some works that have shown that traditionalarchitectures, like Resnet, are condemned to the sparse double descentphenomenon, for ViTs we observe that an optimally-tuned $\ell_2$ regularizationrelieves such a phenomenon. However, everything comes at a cost: optimal lambdawill sacrifice the potential compression of the ViT.</description><author>Victor Quétu, Marta Milovanovic, Enzo Tartaglione</author><pubDate>Wed, 26 Jul 2023 16:33:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14253v1</guid></item><item><title>FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios</title><link>http://arxiv.org/abs/2307.13528v2</link><description>The emergence of generative pre-trained models has facilitated the synthesisof high-quality text, but it has also posed challenges in identifying factualerrors in the generated text. In particular: (1) A wider range of tasks nowface an increasing risk of containing factual errors when handled by generativemodels. (2) Generated texts tend to be lengthy and lack a clearly definedgranularity for individual facts. (3) There is a scarcity of explicit evidenceavailable during the process of fact checking. With the above challenges inmind, in this paper, we propose FacTool, a task and domain agnostic frameworkfor detecting factual errors of texts generated by large language models (e.g.,ChatGPT). Experiments on four different tasks (knowledge-based QA, codegeneration, mathematical reasoning, and scientific literature review) show theefficacy of the proposed method. We release the code of FacTool associated withChatGPT plugin interface at https://github.com/GAIR-NLP/factool .</description><author>I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu</author><pubDate>Wed, 26 Jul 2023 16:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13528v2</guid></item><item><title>Client Selection in Federated Learning: Principles, Challenges, and Opportunities</title><link>http://arxiv.org/abs/2211.01549v2</link><description>As a privacy-preserving paradigm for training Machine Learning (ML) models,Federated Learning (FL) has received tremendous attention from both industryand academia. In a typical FL scenario, clients exhibit significantheterogeneity in terms of data distribution and hardware configurations. Thus,randomly sampling clients in each training round may not fully exploit thelocal updates from heterogeneous clients, resulting in lower model accuracy,slower convergence rate, degraded fairness, etc. To tackle the FL clientheterogeneity problem, various client selection algorithms have been developed,showing promising performance improvement. In this paper, we systematicallypresent recent advances in the emerging field of FL client selection and itschallenges and research opportunities. We hope to facilitate practitioners inchoosing the most suitable client selection mechanisms for their applications,as well as inspire researchers and newcomers to better understand this excitingresearch topic.</description><author>Lei Fu, Huanle Zhang, Ge Gao, Mi Zhang, Xin Liu</author><pubDate>Wed, 26 Jul 2023 16:15:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01549v2</guid></item><item><title>A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)</title><link>http://arxiv.org/abs/2307.14246v1</link><description>Within the field of Requirements Engineering (RE), the increasingsignificance of Explainable Artificial Intelligence (XAI) in aligningAI-supported systems with user needs, societal expectations, and regulatorystandards has garnered recognition. In general, explainability has emerged asan important non-functional requirement that impacts system quality. However,the supposed trade-off between explainability and performance challenges thepresumed positive influence of explainability. If meeting the requirement ofexplainability entails a reduction in system performance, then carefulconsideration must be given to which of these quality aspects takes precedenceand how to compromise between them. In this paper, we critically examine thealleged trade-off. We argue that it is best approached in a nuanced way thatincorporates resource availability, domain characteristics, and considerationsof risk. By providing a foundation for future research and best practices, thiswork aims to advance the field of RE for AI.</description><author>Timo Speith, Markus Langer</author><pubDate>Wed, 26 Jul 2023 16:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14246v1</guid></item><item><title>Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy</title><link>http://arxiv.org/abs/2307.14243v1</link><description>Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopyimages and the corresponding ground-truth annotations, designed to fosterinnovative research in the domains of Life Sciences and Deep Learning. Thisdataset encompasses three image collections in which rodent neuronal cells'nuclei and cytoplasm are stained with diverse markers to highlight theiranatomical or functional characteristics. Alongside the images, we provideground-truth annotations for several learning tasks, including semanticsegmentation, object detection, and counting. The contribution is two-fold.First, given the variety of annotations and their accessible formats, weenvision our work facilitating methodological advancements in computer visionapproaches for segmentation, detection, feature learning, unsupervised andself-supervised learning, transfer learning, and related areas. Second, byenabling extensive exploration and benchmarking, we hope Fluorescent NeuronalCells v2 will catalyze breakthroughs in fluorescence microscopy analysis andpromote cutting-edge discoveries in life sciences. The data are available at:https://amsacta.unibo.it/id/eprint/7347</description><author>Luca Clissa, Antonio Macaluso, Roberto Morelli, Alessandra Occhinegro, Emiliana Piscitiello, Ludovico Taddei, Marco Luppi, Roberto Amici, Matteo Cerri, Timna Hitrec, Lorenzo Rinaldi, Antonio Zoccoli</author><pubDate>Wed, 26 Jul 2023 16:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14243v1</guid></item><item><title>Defending Adversarial Patches via Joint Region Localizing and Inpainting</title><link>http://arxiv.org/abs/2307.14242v1</link><description>Deep neural networks are successfully used in various applications, but showtheir vulnerability to adversarial examples. With the development ofadversarial patches, the feasibility of attacks in physical scenes increases,and the defenses against patch attacks are urgently needed. However, defendingsuch adversarial patch attacks is still an unsolved problem. In this paper, weanalyse the properties of adversarial patches, and find that: on the one hand,adversarial patches will lead to the appearance or contextual inconsistency inthe target objects; on the other hand, the patch region will show abnormalchanges on the high-level feature maps of the objects extracted by a backbonenetwork. Considering the above two points, we propose a novel defense methodbased on a ``localizing and inpainting" mechanism to pre-process the inputexamples. Specifically, we design an unified framework, where the ``localizing"sub-network utilizes a two-branch structure to represent the above two aspectsto accurately detect the adversarial patch region in the image. For the``inpainting" sub-network, it utilizes the surrounding contextual cues torecover the original content covered by the adversarial patch. The quality ofinpainted images is also evaluated by measuring the appearance consistency andthe effects of adversarial attacks. These two sub-networks are then jointlytrained via an iterative optimization manner. In this way, the ``localizing"and ``inpainting" modules can interact closely with each other, and thus learna better solution. A series of experiments versus traffic sign classificationand detection tasks are conducted to defend against various adversarial patchattacks.</description><author>Junwen Chen, Xingxing Wei</author><pubDate>Wed, 26 Jul 2023 16:11:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14242v1</guid></item><item><title>DisguisOR: Holistic Face Anonymization for the Operating Room</title><link>http://arxiv.org/abs/2307.14241v1</link><description>Purpose: Recent advances in Surgical Data Science (SDS) have contributed toan increase in video recordings from hospital environments. While methods suchas surgical workflow recognition show potential in increasing the quality ofpatient care, the quantity of video data has surpassed the scale at whichimages can be manually anonymized. Existing automated 2D anonymization methodsunder-perform in Operating Rooms (OR), due to occlusions and obstructions. Wepropose to anonymize multi-view OR recordings using 3D data from multiplecamera streams. Methods: RGB and depth images from multiple cameras are fusedinto a 3D point cloud representation of the scene. We then detect eachindividual's face in 3D by regressing a parametric human mesh model ontodetected 3D human keypoints and aligning the face mesh with the fused 3D pointcloud. The mesh model is rendered into every acquired camera view, replacingeach individual's face. Results: Our method shows promise in locating faces ata higher rate than existing approaches. DisguisOR produces geometricallyconsistent anonymizations for each camera view, enabling more realisticanonymization that is less detrimental to downstream tasks. Conclusion:Frequent obstructions and crowding in operating rooms leaves significant roomfor improvement for off-the-shelf anonymization methods. DisguisOR addressesprivacy on a scene level and has the potential to facilitate further researchin SDS.</description><author>Lennart Bastian, Tony Danjun Wang, Tobias Czempiel, Benjamin Busam, Nassir Navab</author><pubDate>Wed, 26 Jul 2023 16:10:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14241v1</guid></item><item><title>Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)</title><link>http://arxiv.org/abs/2307.14239v1</link><description>Within the field of Requirements Engineering (RE), the increasingsignificance of Explainable Artificial Intelligence (XAI) in aligningAI-supported systems with user needs, societal expectations, and regulatorystandards has garnered recognition. In general, explainability has emerged asan important non-functional requirement that impacts system quality. However,the supposed trade-off between explainability and performance challenges thepresumed positive influence of explainability. If meeting the requirement ofexplainability entails a reduction in system performance, then carefulconsideration must be given to which of these quality aspects takes precedenceand how to compromise between them. In this paper, we critically examine thealleged trade-off. We argue that it is best approached in a nuanced way thatincorporates resource availability, domain characteristics, and considerationsof risk. By providing a foundation for future research and best practices, thiswork aims to advance the field of RE for AI.</description><author>Barnaby Crook, Maximilian Schlüter, Timo Speith</author><pubDate>Wed, 26 Jul 2023 16:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14239v1</guid></item><item><title>Evolving Multi-Objective Neural Network Controllers for Robot Swarms</title><link>http://arxiv.org/abs/2307.14237v1</link><description>Many swarm robotics tasks consist of multiple conflicting objectives. Thisresearch proposes a multi-objective evolutionary neural network approach todeveloping controllers for swarms of robots. The swarm robot controllers aretrained in a low-fidelity Python simulator and then tested in a high-fidelitysimulated environment using Webots. Simulations are then conducted to test thescalability of the evolved multi-objective robot controllers to environmentswith a larger number of robots. The results presented demonstrate that theproposed approach can effectively control each of the robots. The robot swarmexhibits different behaviours as the weighting for each objective is adjusted.The results also confirm that multi-objective neural network controllersevolved in a low-fidelity simulator can be transferred to high-fidelitysimulated environments and that the controllers can scale to environments witha larger number of robots without further retraining needed.</description><author>Karl Mason, Sabine Hauert</author><pubDate>Wed, 26 Jul 2023 16:05:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14237v1</guid></item><item><title>UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text</title><link>http://arxiv.org/abs/2307.14236v1</link><description>This demo paper presents UnScientify, an interactive system designed todetect scientific uncertainty in scholarly full text. The system utilizes aweakly supervised technique that employs a fine-grained annotation scheme toidentify verbally formulated uncertainty at the sentence level in scientifictexts. The pipeline for the system includes a combination of pattern matching,complex sentence checking, and authorial reference checking. Our approachautomates labeling and annotation tasks for scientific uncertaintyidentification, taking into account different types of scientific uncertainty,that can serve various applications such as information retrieval, text mining,and scholarly document processing. Additionally, UnScientify providesinterpretable results, aiding in the comprehension of identified instances ofscientific uncertainty in text.</description><author>Panggih Kusuma Ningrum, Philipp Mayr, Iana Atanassova</author><pubDate>Wed, 26 Jul 2023 16:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14236v1</guid></item><item><title>Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale</title><link>http://arxiv.org/abs/2306.00017v3</link><description>Large language models (LLMs) have achieved a milestone that undenia-blychanged many held beliefs in artificial intelligence (AI). However, thereremains many limitations of these LLMs when it comes to true languageunderstanding, limitations that are a byproduct of the under-lying architectureof deep neural networks. Moreover, and due to their subsymbolic nature,whatever knowledge these models acquire about how language works will always beburied in billions of microfeatures (weights), none of which is meaningful onits own, making such models hopelessly unexplainable. To address theselimitations, we suggest com-bining the strength of symbolic representationswith what we believe to be the key to the success of LLMs, namely a successfulbottom-up re-verse engineering of language at scale. As such we argue for abottom-up reverse engineering of language in a symbolic setting. Hints on whatthis project amounts to have been suggested by several authors, and we discussin some detail here how this project could be accomplished.</description><author>Walid S. Saba</author><pubDate>Wed, 26 Jul 2023 16:03:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00017v3</guid></item><item><title>Sources of Opacity in Computer Systems: Towards a Comprehensive Taxonomy</title><link>http://arxiv.org/abs/2307.14232v1</link><description>Modern computer systems are ubiquitous in contemporary life yet many of themremain opaque. This poses significant challenges in domains where desideratasuch as fairness or accountability are crucial. We suggest that the beststrategy for achieving system transparency varies depending on the specificsource of opacity prevalent in a given context. Synthesizing and extendingexisting discussions, we propose a taxonomy consisting of eight sources ofopacity that fall into three main categories: architectural, analytical, andsocio-technical. For each source, we provide initial suggestions as to how toaddress the resulting opacity in practice. The taxonomy provides a startingpoint for requirements engineers and other practitioners to understandcontextually prevalent sources of opacity, and to select or develop appropriatestrategies for overcoming them.</description><author>Sara Mann, Barnaby Crook, Lena Kästner, Astrid Schomäcker, Timo Speith</author><pubDate>Wed, 26 Jul 2023 16:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14232v1</guid></item><item><title>Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics</title><link>http://arxiv.org/abs/2212.00679v4</link><description>Automated synthesis of provably correct controllers for cyber-physicalsystems is crucial for deployment in safety-critical scenarios. However, hybridfeatures and stochastic or unknown behaviours make this problem challenging. Wepropose a method for synthesising controllers for Markov jump linear systems(MJLSs), a class of discrete-time models for cyber-physical systems, so thatthey certifiably satisfy probabilistic computation tree logic (PCTL) formulae.An MJLS consists of a finite set of stochastic linear dynamics and discretejumps between these dynamics that are governed by a Markov decision process(MDP). We consider the cases where the transition probabilities of this MDP areeither known up to an interval or completely unknown. Our approach is based ona finite-state abstraction that captures both the discrete (mode-jumping) andcontinuous (stochastic linear) behaviour of the MJLS. We formalise thisabstraction as an interval MDP (iMDP) for which we compute intervals oftransition probabilities using sampling techniques from the so-called 'scenarioapproach', resulting in a probabilistically sound approximation. We apply ourmethod to multiple realistic benchmark problems, in particular, a temperaturecontrol and an aerial vehicle delivery problem.</description><author>Luke Rickard, Thom Badings, Licio Romao, Alessandro Abate</author><pubDate>Wed, 26 Jul 2023 16:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00679v4</guid></item><item><title>Dominating Set Database Selection for Visual Place Recognition</title><link>http://arxiv.org/abs/2303.05123v2</link><description>This paper presents an approach for creating a visual place recognition (VPR)database for localization in indoor environments from RGBD scanning sequences.The proposed approach is formulated as a minimization problem in terms ofdominating set algorithm for graph, constructed from spatial information, andreferred as DominatingSet. Our algorithm shows better scene coverage incomparison to other methodologies that are used for database creation. Also, wedemonstrate that using DominatingSet, a database size could be up to 250-1400times smaller than the original scanning sequence while maintaining a recallrate of more than 80% on testing sequences. We evaluated our algorithm on7-scenes and BundleFusion datasets and an additionally recorded sequence in ahighly repetitive office setting. In addition, the database selection canproduce weakly-supervised labels for fine-tuning neural place recognitionalgorithms to particular settings, improving even more their accuracy. Thepaper also presents a fully automated pipeline for VPR database creation fromRGBD scanning sequences, as well as a set of metrics for VPR databaseevaluation. The code and released data are available on our web-page~ --https://prime-slam.github.io/place-recognition-db/</description><author>Anastasiia Kornilova, Ivan Moskalenko, Timofei Pushkin, Fakhriddin Tojiboev, Rahim Tariverdizadeh, Gonzalo Ferrer</author><pubDate>Wed, 26 Jul 2023 15:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05123v2</guid></item><item><title>Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice</title><link>http://arxiv.org/abs/2202.12780v3</link><description>One of the main tasks of actuaries and data scientists is to build goodpredictive models for certain phenomena such as the claim size or the number ofclaims in insurance. These models ideally exploit given feature information toenhance the accuracy of prediction. This user guide revisits and clarifiesstatistical techniques to assess the calibration or adequacy of a model on theone hand, and to compare and rank different models on the other hand. In doingso, it emphasises the importance of specifying the prediction target functionalat hand a priori (e.g. the mean or a quantile) and of choosing the scoringfunction in model comparison in line with this target functional. Guidance forthe practical choice of the scoring function is provided. Striving to bridgethe gap between science and daily practice in application, it focuses mainly onthe pedagogical presentation of existing results and of best practice. Theresults are accompanied and illustrated by two real data case studies onworkers' compensation and customer churn.</description><author>Tobias Fissler, Christian Lorentzen, Michael Mayer</author><pubDate>Wed, 26 Jul 2023 15:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.12780v3</guid></item><item><title>Learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking Heads Generation</title><link>http://arxiv.org/abs/2306.01415v2</link><description>This paper presents a novel approach for generating 3D talking heads from rawaudio inputs. Our method grounds on the idea that speech related movements canbe comprehensively and efficiently described by the motion of a few controlpoints located on the movable parts of the face, i.e., landmarks. Theunderlying musculoskeletal structure then allows us to learn how their motioninfluences the geometrical deformations of the whole face. The proposed methodemploys two distinct models to this aim: the first one learns to generate themotion of a sparse set of landmarks from the given audio. The second modelexpands such landmarks motion to a dense motion field, which is utilized toanimate a given 3D mesh in neutral state. Additionally, we introduce a novelloss function, named Cosine Loss, which minimizes the angle between thegenerated motion vectors and the ground truth ones. Using landmarks in 3Dtalking head generation offers various advantages such as consistency,reliability, and obviating the need for manual-annotation. Our approach isdesigned to be identity-agnostic, enabling high-quality facial animations forany users without additional data or training.</description><author>Federico Nocentini, Claudio Ferrari, Stefano Berretti</author><pubDate>Wed, 26 Jul 2023 15:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01415v2</guid></item><item><title>Computational Approaches for Traditional Chinese Painting: From the "Six Principles of Painting" Perspective</title><link>http://arxiv.org/abs/2307.14227v1</link><description>Traditional Chinese Painting (TCP) is an invaluable cultural heritageresource and a unique visual art style. In recent years, increasing interesthas been placed on digitalizing TCPs to preserve and revive the culture. Theresulting digital copies have enabled the advancement of computational methodsfor structured and systematic understanding of TCPs. To explore this topic, weconducted an in-depth analysis of 92 pieces of literature. We examined thecurrent use of computer technologies on TCPs from three perspectives, based onnumerous conversations with specialists. First, in light of the "Six Principlesof Painting" theory, we categorized the articles according to their researchfocus on artistic elements. Second, we created a four-stage framework toillustrate the purposes of TCP applications. Third, we summarized the popularcomputational techniques applied to TCPs. The framework also provides insightsinto potential applications and future prospects, with professional opinion.The list of surveyed publications and related information is available onlineat https://ca4tcp.com.</description><author>Wei Zhang, Jian-Wei Zhang, Kam Kwai Wong, Yifang Wang, Yingchaojie Feng, Luwei Wang, Wei Chen</author><pubDate>Wed, 26 Jul 2023 15:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14227v1</guid></item><item><title>Cross-modal Manifold Cutmix for Self-supervised Video Representation Learning</title><link>http://arxiv.org/abs/2112.03906v2</link><description>In this paper, we address the challenge of obtaining large-scale unlabelledvideo datasets for contrastive representation learning in real-worldapplications. We present a novel video augmentation technique forself-supervised learning, called Cross-Modal Manifold Cutmix (CMMC), whichgenerates augmented samples by combining different modalities in videos. Byembedding a video tesseract into another across two modalities in the featurespace, our method enhances the quality of learned video representations. Weperform extensive experiments on two small-scale video datasets, UCF101 andHMDB51, for action recognition and video retrieval tasks. Our approach is alsoshown to be effective on the NTU dataset with limited domain knowledge. OurCMMC achieves comparable performance to other self-supervised methods whileusing less training data for both downstream tasks.</description><author>Srijan Das, Michael S. Ryoo</author><pubDate>Wed, 26 Jul 2023 15:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.03906v2</guid></item><item><title>Explore the possibility of advancing climate negotiations on the basis of regional trade organizations: A study based on RICE-N</title><link>http://arxiv.org/abs/2307.14226v1</link><description>Climate issues have become more and more important now. Although globalgovernments have made some progress, we are still facing the truth that theprospect of international cooperation is not clear at present. Due to thelimitations of the Integrated assessment models (IAMs) model, it is difficultto simulate the dynamic negotiation process. Therefore, using deep learning tobuild a new agents based model (ABM) might can provide new theoretical supportfor climate negotiations. Building on the RICE-N model, this work proposed anapproach to climate negotiations based on existing trade groups. Simulationresults show that the scheme has a good prospect.</description><author>Wubo Dai</author><pubDate>Wed, 26 Jul 2023 15:48:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14226v1</guid></item><item><title>Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences</title><link>http://arxiv.org/abs/2307.14225v1</link><description>Traditional recommender systems leverage users' item preference history torecommend novel content that users may like. However, modern dialog interfacesthat allow users to express language-based preferences offer a fundamentallydifferent modality for preference input. Inspired by recent successes ofprompting paradigms for large language models (LLMs), we study their use formaking recommendations from both item-based and language-based preferences incomparison to state-of-the-art item-based collaborative filtering (CF) methods.To support this investigation, we collect a new dataset consisting of bothitem-based and language-based preferences elicited from users along with theirratings on a variety of (biased) recommended items and (unbiased) random items.Among numerous experimental results, we find that LLMs provide competitiverecommendation performance for pure language-based preferences (no itempreferences) in the near cold-start case in comparison to item-based CFmethods, despite having no supervised training for this specific task(zero-shot) or only a few labels (few-shot). This is particularly promising aslanguage-based preference representations are more explainable and scrutablethan item-based or vector-based representations.</description><author>Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, Lucas Dixon</author><pubDate>Wed, 26 Jul 2023 15:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14225v1</guid></item><item><title>Generative AI Assistants in Software Development Education</title><link>http://arxiv.org/abs/2303.13936v2</link><description>The software development industry is amid another disruptive paradigm change- adopting the use of generative AI (GAI) assistants for programming. Whilst AIis already used in various areas of software engineering, GAI technologies,such as GitHub Copilot and ChatGPT, have ignited peoples' imaginations (andfears). It is unclear how the industry will adapt, but the move to integratethese technologies by large software companies, such as Microsoft (GitHub,Bing) and Google (Bard), is a clear indication of intent and direction. Weperformed exploratory interviews with industry professionals to understandcurrent practice and challenges, which we incorporate into our vision of afuture of software development education and make some pedagogicalrecommendations.</description><author>Christopher Bull, Ahmed Kharrufa</author><pubDate>Wed, 26 Jul 2023 15:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13936v2</guid></item><item><title>MediaGPT : A Large Language Model For Chinese Media</title><link>http://arxiv.org/abs/2307.10930v2</link><description>Large language models (LLMs) have shown remarkable capabilities in generatinghigh-quality text and making predictions based on large amounts of data,including the media domain. However, in practical applications, the differencesbetween the media's use cases and the general-purpose applications of LLMs havebecome increasingly apparent, especially Chinese. This paper examines theunique characteristics of media-domain-specific LLMs compared to general LLMs,designed a diverse set of task instruction types to cater the specificrequirements of the domain and constructed unique datasets that are tailored tothe media domain. Based on these, we proposed MediaGPT, a domain-specific LLMfor the Chinese media domain, training by domain-specific data and experts SFTdata. By performing human experts evaluation and strong model evaluation on avalidation set, this paper demonstrated that MediaGPT outperforms mainstreammodels on various Chinese media domain tasks and verifies the importance ofdomain data and domain-defined prompt types for building an effectivedomain-specific LLM.</description><author>Zhonghao Wang, Zijia Lu, Bo Jin, Haiying Deng</author><pubDate>Wed, 26 Jul 2023 15:21:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10930v2</guid></item><item><title>TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations</title><link>http://arxiv.org/abs/2307.09916v2</link><description>Deep learning (DL) approaches are being increasingly used for time-seriesforecasting, with many efforts devoted to designing complex DL models. Recentstudies have shown that the DL success is often attributed to effective datarepresentations, fostering the fields of feature engineering and representationlearning. However, automated approaches for feature learning are typicallylimited with respect to incorporating prior knowledge, identifying interactionsamong variables, and choosing evaluation metrics to ensure that the models arereliable. To improve on these limitations, this paper contributes a novelvisual analytics framework, namely TimeTuner, designed to help analystsunderstand how model behaviors are associated with localized correlations,stationarity, and granularity of time-series representations. The system mainlyconsists of the following two-stage technique: We first leverage counterfactualexplanations to connect the relationships among time-series representations,multivariate features and model predictions. Next, we design multiplecoordinated views including a partition-based correlation matrix and juxtaposedbivariate stripes, and provide a set of interactions that allow users to stepinto the transformation selection process, navigate through the feature space,and reason the model performance. We instantiate TimeTuner with twotransformation methods of smoothing and sampling, and demonstrate itsapplicability on real-world time-series forecasting of univariate sunspots andmultivariate air pollutants. Feedback from domain experts indicates that oursystem can help characterize time-series representations and guide the featureengineering processes.</description><author>Jianing Hao, Qing Shi, Yilin Ye, Wei Zeng</author><pubDate>Wed, 26 Jul 2023 15:21:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09916v2</guid></item><item><title>Online Modeling and Monitoring of Dependent Processes under Resource Constraints</title><link>http://arxiv.org/abs/2307.14208v1</link><description>Monitoring a population of dependent processes under limited resources iscritical for abnormal events detection. A novel online collaborative learningmethod is proposed to adaptively allocate the resources for exploitation ofhigh-risk processes and exploration of dependent dynamics. Efficiency of theproposed method is proved through theoretical analysis and experiments.</description><author>Tanapol Kosolwattana, Huazheng Wang, Ying Lin</author><pubDate>Wed, 26 Jul 2023 15:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14208v1</guid></item><item><title>AI and Education: An Investigation into the Use of ChatGPT for Systems Thinking</title><link>http://arxiv.org/abs/2307.14206v1</link><description>This exploratory study investigates the potential of the artificialintelligence tool, ChatGPT, to support systems thinking (ST) in varioussubjects. Using both general and subject specific prompts, the study assessesthe accuracy, helpfulness, and reliability of ChatGPT's responses acrossdifferent versions of the tool. The results indicate that ChatGPT can providelargely correct and very helpful responses in various subjects, demonstratingits potential as a tool for enhancing ST skills. However, occasionalinaccuracies highlight the need for users to remain critical of ChatGPT'sresponses. Despite some limitations, this study suggests that with careful useand attention to its idiosyncrasies, ChatGPT can be a valuable tool forteaching and learning ST.</description><author>Holger Arndt</author><pubDate>Wed, 26 Jul 2023 15:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14206v1</guid></item><item><title>Robust Quantity-Aware Aggregation for Federated Learning</title><link>http://arxiv.org/abs/2205.10848v2</link><description>Federated learning (FL) enables multiple clients to collaboratively trainmodels without sharing their local data, and becomes an importantprivacy-preserving machine learning framework. However, classical FL facesserious security and robustness problem, e.g., malicious clients can poisonmodel updates and at the same time claim large quantities to amplify the impactof their model updates in the model aggregation. Existing defense methods forFL, while all handling malicious model updates, either treat all quantitiesbenign or simply ignore/truncate the quantities of all clients. The former isvulnerable to quantity-enhanced attack, while the latter leads to sub-optimalperformance since the local data on different clients is usually insignificantly different sizes. In this paper, we propose a robustquantity-aware aggregation algorithm for federated learning, called FedRA, toperform the aggregation with awareness of local data quantities while beingable to defend against quantity-enhanced attacks. More specifically, we proposea method to filter malicious clients by jointly considering the uploaded modelupdates and data quantities from different clients, and performingquantity-aware weighted averaging on model updates from remaining clients.Moreover, as the number of malicious clients participating in the federatedlearning may dynamically change in different rounds, we also propose amalicious client number estimator to predict how many suspicious clients shouldbe filtered in each round. Experiments on four public datasets demonstrate theeffectiveness of our FedRA method in defending FL against quantity-enhancedattacks.</description><author>Jingwei Yi, Fangzhao Wu, Huishuai Zhang, Bin Zhu, Tao Qi, Guangzhong Sun, Xing Xie</author><pubDate>Wed, 26 Jul 2023 15:08:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.10848v2</guid></item><item><title>Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling</title><link>http://arxiv.org/abs/2307.14199v1</link><description>The hydrometallurgical method of zinc production involves leaching zinc fromore and then separating the solid residue from the liquid solution by pressurefiltration. This separation process is very important since the solid residuecontains some moisture that can reduce the amount of zinc recovered. This studymodeled the pressure filtration process through Random Forest (RF) and SupportVector Machine (SVM). The models take continuous variables (extracted features)from the lab samples as inputs. Thus, regression models namely Random ForestRegression (RFR) and Support Vector Regression (SVR) were chosen. A totaldataset was obtained during the pressure filtration process in two conditions:1) Polypropylene (S1) and 2) Polyester fabrics (S2). To predict the cakemoisture, solids concentration (0.2 and 0.38), temperature (35 and 65centigrade), pH (2, 3.5, and 5), pressure, cake thickness (14, 20, 26, and 34mm), air-blow time (2, 10 and 15 min) and filtration time were applied as inputvariables. The models' predictive accuracy was evaluated by the coefficient ofdetermination (R2) parameter. The results revealed that the RFR model issuperior to the SVR model for cake moisture prediction.</description><author>Masoume Kazemi, Davood Moradkhani, Alireza Abbas Alipour</author><pubDate>Wed, 26 Jul 2023 14:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14199v1</guid></item><item><title>Efficient Learning of Discrete-Continuous Computation Graphs</title><link>http://arxiv.org/abs/2307.14193v1</link><description>Numerous models for supervised and reinforcement learning benefit fromcombinations of discrete and continuous model components. End-to-end learnablediscrete-continuous models are compositional, tend to generalize better, andare more interpretable. A popular approach to building discrete-continuouscomputation graphs is that of integrating discrete probability distributionsinto neural networks using stochastic softmax tricks. Prior work has mainlyfocused on computation graphs with a single discrete component on each of thegraph's execution paths. We analyze the behavior of more complex stochasticcomputations graphs with multiple sequential discrete components. We show thatit is challenging to optimize the parameters of these models, mainly due tosmall gradients and local minima. We then propose two new strategies toovercome these challenges. First, we show that increasing the scale parameterof the Gumbel noise perturbations during training improves the learningbehavior. Second, we propose dropout residual connections specifically tailoredto stochastic, discrete-continuous computation graphs. With an extensive set ofexperiments, we show that we can train complex discrete-continuous models whichone cannot train with standard stochastic softmax tricks. We also show thatcomplex discrete-stochastic models generalize better than their continuouscounterparts on several benchmark datasets.</description><author>David Friede, Mathias Niepert</author><pubDate>Wed, 26 Jul 2023 14:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14193v1</guid></item><item><title>Unveiling Security, Privacy, and Ethical Concerns of ChatGPT</title><link>http://arxiv.org/abs/2307.14192v1</link><description>This paper delves into the realm of ChatGPT, an AI-powered chatbot thatutilizes topic modeling and reinforcement learning to generate naturalresponses. Although ChatGPT holds immense promise across various industries,such as customer service, education, mental health treatment, personalproductivity, and content creation, it is essential to address its security,privacy, and ethical implications. By exploring the upgrade path from GPT-1 toGPT-4, discussing the model's features, limitations, and potentialapplications, this study aims to shed light on the potential risks ofintegrating ChatGPT into our daily lives. Focusing on security, privacy, andethics issues, we highlight the challenges these concerns pose for widespreadadoption. Finally, we analyze the open problems in these areas, calling forconcerted efforts to ensure the development of secure and ethically sound largelanguage models.</description><author>Xiaodong Wu, Ran Duan, Jianbing Ni</author><pubDate>Wed, 26 Jul 2023 14:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14192v1</guid></item><item><title>MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent</title><link>http://arxiv.org/abs/2203.04317v2</link><description>Image registration is the process of bringing different images into a commoncoordinate system - a technique widely used in various applications of computervision, such as remote sensing, image retrieval, and, most commonly, medicalimaging. Deep learning based techniques have been applied successfully totackle various complex medical image processing problems, including medicalimage registration. Over the years, several image registration techniques havebeen proposed using deep learning. Deformable image registration techniquessuch as Voxelmorph have been successful in capturing finer changes andproviding smoother deformations. However, Voxelmorph, as well as ICNet andFIRE, do not explicitly encode global dependencies (i.e. the overall anatomicalview of the supplied image) and, therefore, cannot track large deformations. Inorder to tackle the aforementioned problems, this paper extends the Voxelmorphapproach in three different ways. To improve the performance in case of smallas well as large deformations, supervision of the model at differentresolutions has been integrated using a multi-scale UNet. To support thenetwork to learn and encode the minute structural co-relations of the givenimage-pairs, a self-constructing graph network (SCGNet) has been used as thelatent of the multi-scale UNet - which can improve the learning process of themodel and help the model to generalise better. And finally, to make thedeformations inverse-consistent, cycle consistency loss has been employed. Onthe task of registration of brain MRIs, the proposed method achievedsignificant improvements over ANTs and VoxelMorph, obtaining a Dice score of0.8013 \pm 0.0243 for intramodal and 0.6211 \pm 0.0309 for intermodal, whileVoxelMorph achieved 0.7747 \pm 0.0260 and 0.6071 \pm 0.0510, respectively</description><author>Soumick Chatterjee, Himanshi Bajaj, Istiyak H. Siddiquee, Nandish Bandi Subbarayappa, Steve Simon, Suraj Bangalore Shashidhar, Oliver Speck, Andreas Nürnberge</author><pubDate>Wed, 26 Jul 2023 14:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.04317v2</guid></item><item><title>ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation</title><link>http://arxiv.org/abs/2307.14187v1</link><description>Forecasting future trajectories of agents in complex traffic scenes requiresreliable and efficient predictions for all agents in the scene. However,existing methods for trajectory prediction are either inefficient or sacrificeaccuracy. To address this challenge, we propose ADAPT, a novel approach forjointly predicting the trajectories of all agents in the scene with dynamicweight learning. Our approach outperforms state-of-the-art methods in bothsingle-agent and multi-agent settings on the Argoverse and Interactiondatasets, with a fraction of their computational overhead. We attribute theimprovement in our performance: first, to the adaptive head augmenting themodel capacity without increasing the model size; second, to our design choicesin the endpoint-conditioned prediction, reinforced by gradient stopping. Ouranalyses show that ADAPT can focus on each agent with adaptive prediction,allowing for accurate predictions efficiently. https://KUIS-AI.github.io/adapt</description><author>Görkay Aydemir, Adil Kaan Akan, Fatma Güney</author><pubDate>Wed, 26 Jul 2023 14:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14187v1</guid></item><item><title>A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia</title><link>http://arxiv.org/abs/2307.14185v1</link><description>Low-lying coastal cities, exemplified by Norfolk, Virginia, face thechallenge of street flooding caused by rainfall and tides, which straintransportation and sewer systems and can lead to property damage. Whilehigh-fidelity, physics-based simulations provide accurate predictions of urbanpluvial flooding, their computational complexity renders them unsuitable forreal-time applications. Using data from Norfolk rainfall events between 2016and 2018, this study compares the performance of a previous surrogate modelbased on a random forest algorithm with two deep learning models: LongShort-Term Memory (LSTM) and Gated Recurrent Unit (GRU). This investigationunderscores the importance of using a model architecture that supports thecommunication of prediction uncertainty and the effective integration ofrelevant, multi-modal features.</description><author>Diana McSpadden, Steven Goldenberg, Binata Roy, Malachi Schram, Jonathan L. Goodall, Heather Richter</author><pubDate>Wed, 26 Jul 2023 14:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14185v1</guid></item><item><title>FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle</title><link>http://arxiv.org/abs/2302.14831v2</link><description>This work proposes to solve the problem of few-shot biometric authenticationby computing the Mahalanobis distance between testing embeddings and amultivariate Gaussian distribution of training embeddings obtained usingpre-trained CNNs. Experimental results show that models pre-trained on theImageNet dataset significantly outperform models pre-trained on human faces.With a VGG16 model, we obtain a FRR of 1.25% for a FAR of 1.18% on a dataset of20 cattle identities.</description><author>Meshia Cédric Oveneke, Rucha Vaishampayan, Deogratias Lukamba Nsadisa, Jenny Ambukiyenyi Onya</author><pubDate>Wed, 26 Jul 2023 14:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14831v2</guid></item><item><title>Resolution-Aware Design of Atrous Rates for Semantic Segmentation Networks</title><link>http://arxiv.org/abs/2307.14179v1</link><description>DeepLab is a widely used deep neural network for semantic segmentation, whosesuccess is attributed to its parallel architecture called atrous spatialpyramid pooling (ASPP). ASPP uses multiple atrous convolutions with differentatrous rates to extract both local and global information. However, fixedvalues of atrous rates are used for the ASPP module, which restricts the sizeof its field of view. In principle, atrous rate should be a hyperparameter tochange the field of view size according to the target task or dataset. However,the manipulation of atrous rate is not governed by any guidelines. This studyproposes practical guidelines for obtaining an optimal atrous rate. First, aneffective receptive field for semantic segmentation is introduced to analyzethe inner behavior of segmentation networks. We observed that the use of ASPPmodule yielded a specific pattern in the effective receptive field, which wastraced to reveal the module's underlying mechanism. Accordingly, we derivepractical guidelines for obtaining the optimal atrous rate, which should becontrolled based on the size of input image. Compared to other values, usingthe optimal atrous rate consistently improved the segmentation results acrossmultiple datasets, including the STARE, CHASE_DB1, HRF, Cityscapes, and iSAIDdatasets.</description><author>Bum Jun Kim, Hyeyeon Choi, Hyeonah Jang, Sang Woo Kim</author><pubDate>Wed, 26 Jul 2023 14:11:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14179v1</guid></item><item><title>PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation</title><link>http://arxiv.org/abs/2307.04956v2</link><description>Visual anomaly detection is essential and commonly used for many tasks in thefield of computer vision. Recent anomaly detection datasets mainly focus onindustrial automated inspection, medical image analysis and video surveillance.In order to broaden the application and research of anomaly detection inunmanned supermarkets and smart manufacturing, we introduce the supermarketgoods anomaly detection (GoodsAD) dataset. It contains 6124 high-resolutionimages of 484 different appearance goods divided into 6 categories. Eachcategory contains several common different types of anomalies such asdeformation, surface damage and opened. Anomalies contain both texture changesand structural changes. It follows the unsupervised setting and only normal(defect-free) images are used for training. Pixel-precise ground truth regionsare provided for all anomalies. Moreover, we also conduct a thorough evaluationof current state-of-the-art unsupervised anomaly detection methods. Thisinitial benchmark indicates that some methods which perform well on theindustrial anomaly detection dataset (e.g., MVTec AD), show poor performance onour dataset. This is a comprehensive, multi-object dataset for supermarketgoods anomaly detection that focuses on real-world applications.</description><author>Jian Zhang, Runwei Ding, Miaoju Ban, Ge Yang</author><pubDate>Wed, 26 Jul 2023 14:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04956v2</guid></item><item><title>AMAE: Adaptation of Pre-Trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays</title><link>http://arxiv.org/abs/2307.12721v2</link><description>Unsupervised anomaly detection in medical images such as chest radiographs isstepping into the spotlight as it mitigates the scarcity of the labor-intensiveand costly expert annotation of anomaly data. However, nearly all existingmethods are formulated as a one-class classification trained only onrepresentations from the normal class and discard a potentially significantportion of the unlabeled data. This paper focuses on a more practical setting,dual distribution anomaly detection for chest X-rays, using the entire trainingdata, including both normal and unlabeled images. Inspired by a modernself-supervised vision transformer model trained using partial image inputs toreconstruct missing image regions -- we propose AMAE, a two-stage algorithm foradaptation of the pre-trained masked autoencoder (MAE). Starting from MAEinitialization, AMAE first creates synthetic anomalies from only normaltraining images and trains a lightweight classifier on frozen transformerfeatures. Subsequently, we propose an adaptation strategy to leverage unlabeledimages containing anomalies. The adaptation scheme is accomplished by assigningpseudo-labels to unlabeled images and using two separate MAE based modules tomodel the normative and anomalous distributions of pseudo-labeled images. Theeffectiveness of the proposed adaptation strategy is evaluated with differentanomaly ratios in an unlabeled training set. AMAE leads to consistentperformance gains over competing self-supervised and dual distribution anomalydetection methods, setting the new state-of-the-art on three public chest X-raybenchmarks: RSNA, NIH-CXR, and VinDr-CXR.</description><author>Behzad Bozorgtabar, Dwarikanath Mahapatra, Jean-Philippe Thiran</author><pubDate>Wed, 26 Jul 2023 14:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12721v2</guid></item><item><title>High-definition event frame generation using SoC FPGA devices</title><link>http://arxiv.org/abs/2307.14177v1</link><description>In this paper we have addressed the implementation of the accumulation andprojection of high-resolution event data stream (HD -1280 x 720 pixels) ontothe image plane in FPGA devices. The results confirm the feasibility of thisapproach, but there are a number of challenges, limitations and trade-offs tobe considered. The required hardware resources of selected datarepresentations, such as binary frame, event frame, exponentially decaying timesurface and event frequency, were compared with those available on severalpopular platforms from AMD Xilinx. The resulting event frames can be used fortypical vision algorithms, such as object classification and detection, usingboth classical and deep neural network methods.</description><author>Krzysztof Blachut, Tomasz Kryjak</author><pubDate>Wed, 26 Jul 2023 14:06:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14177v1</guid></item><item><title>Learning Sequence Descriptor based on Spatio-Temporal Attention for Visual Place Recognition</title><link>http://arxiv.org/abs/2305.11467v3</link><description>Visual Place Recognition (VPR) aims to retrieve frames from a geotaggeddatabase that are located at the same place as the query frame. To improve therobustness of VPR in perceptually aliasing scenarios, sequence-based VPRmethods are proposed. These methods are either based on matching between framesequences or extracting sequence descriptors for direct retrieval. However, theformer is usually based on the assumption of constant velocity, which isdifficult to hold in practice, and is computationally expensive and subject tosequence length. Although the latter overcomes these problems, existingsequence descriptors are constructed by aggregating features of multiple framesonly, without interaction on temporal information, and thus cannot obtaindescriptors with spatio-temporal discrimination. In this paper, we propose asequence descriptor that effectively incorporates spatio-temporal information.Specifically, spatial attention within the same frame is utilized to learnspatial feature patterns, while attention in corresponding local regions ofdifferent frames is utilized to learn the persistence or change of featuresover time. We use a sliding window to control the temporal range of attentionand use relative position encoding to construct sequential relationshipsbetween different features. This allows our descriptors to capture theintrinsic dynamics in a sequence of frames. Comprehensive experiments onchallenging benchmark datasets show that the proposed approach outperformsrecent state-of-the-art methods.</description><author>Fenglin Zhang, Junqiao Zhao, Yingfeng Cai, Gengxuan Tian, Wenjie Mu, Chen Ye</author><pubDate>Wed, 26 Jul 2023 13:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11467v3</guid></item><item><title>Learning Disentangled Discrete Representations</title><link>http://arxiv.org/abs/2307.14151v1</link><description>Recent successes in image generation, model-based reinforcement learning, andtext-to-image generation have demonstrated the empirical advantages of discretelatent representations, although the reasons behind their benefits remainunclear. We explore the relationship between discrete latent spaces anddisentangled representations by replacing the standard Gaussian variationalautoencoder (VAE) with a tailored categorical variational autoencoder. We showthat the underlying grid structure of categorical distributions mitigates theproblem of rotational invariance associated with multivariate Gaussiandistributions, acting as an efficient inductive prior for disentangledrepresentations. We provide both analytical and empirical findings thatdemonstrate the advantages of discrete VAEs for learning disentangledrepresentations. Furthermore, we introduce the first unsupervised modelselection strategy that favors disentangled representations.</description><author>David Friede, Christian Reimers, Heiner Stuckenschmidt, Mathias Niepert</author><pubDate>Wed, 26 Jul 2023 13:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14151v1</guid></item><item><title>Toward Design of Synthetic Active Inference Agents by Mere Mortals</title><link>http://arxiv.org/abs/2307.14145v1</link><description>The theoretical properties of active inference agents are impressive, but howdo we realize effective agents in working hardware and software on edgedevices? This is an interesting problem because the computational load forpolicy exploration explodes exponentially, while the computational resourcesare very limited for edge devices. In this paper, we discuss the necessaryfeatures for a software toolbox that supports a competent non-expert engineerto develop working active inference agents. We introduce a toolbox-in-progressthat aims to accelerate the democratization of active inference agents in asimilar way as TensorFlow propelled applications of deep learning technology.</description><author>Bert de Vries</author><pubDate>Wed, 26 Jul 2023 13:20:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14145v1</guid></item><item><title>LOIS: Looking Out of Instance Semantics for Visual Question Answering</title><link>http://arxiv.org/abs/2307.14142v1</link><description>Visual question answering (VQA) has been intensively studied as a multimodaltask that requires effort in bridging vision and language to infer answerscorrectly. Recent attempts have developed various attention-based modules forsolving VQA tasks. However, the performance of model inference is largelybottlenecked by visual processing for semantics understanding. Most existingdetection methods rely on bounding boxes, remaining a serious challenge for VQAmodels to understand the causal nexus of object semantics in images andcorrectly infer contextual information. To this end, we propose a finer modelframework without bounding boxes in this work, termed Looking Out of InstanceSemantics (LOIS) to tackle this important issue. LOIS enables more fine-grainedfeature descriptions to produce visual facts. Furthermore, to overcome thelabel ambiguity caused by instance masks, two types of relation attentionmodules: 1) intra-modality and 2) inter-modality, are devised to infer thecorrect answers from the different multi-view features. Specifically, weimplement a mutual relation attention module to model sophisticated and deepervisual semantic relations between instance objects and background information.In addition, our proposed attention model can further analyze salient imageregions by focusing on important word-related questions. Experimental resultson four benchmark VQA datasets prove that our proposed method has favorableperformance in improving visual reasoning capability.</description><author>Siyu Zhang, Yeming Chen, Yaoru Sun, Fang Wang, Haibo Shi, Haoran Wang</author><pubDate>Wed, 26 Jul 2023 13:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14142v1</guid></item><item><title>Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards</title><link>http://arxiv.org/abs/2307.14138v1</link><description>We study the piecewise stationary combinatorial semi-bandit problem withcausally related rewards. In our nonstationary environment, variations in thebase arms' distributions, causal relationships between rewards, or both, changethe reward generation process. In such an environment, an optimaldecision-maker must follow both sources of change and adapt accordingly. Theproblem becomes aggravated in the combinatorial semi-bandit setting, where thedecision-maker only observes the outcome of the selected bundle of arms. Thecore of our proposed policy is the Upper Confidence Bound (UCB) algorithm. Weassume the agent relies on an adaptive approach to overcome the challenge. Morespecifically, it employs a change-point detector based on the GeneralizedLikelihood Ratio (GLR) test. Besides, we introduce the notion of group restartas a new alternative restarting strategy in the decision making process instructured environments. Finally, our algorithm integrates a mechanism to tracethe variations of the underlying graph structure, which captures the causalrelationships between the rewards in the bandit setting. Theoretically, weestablish a regret upper bound that reflects the effects of the number ofstructural- and distribution changes on the performance. The outcome of ournumerical experiments in real-world scenarios exhibits applicability andsuperior performance of our proposal compared to the state-of-the-artbenchmarks.</description><author>Behzad Nourani-Koliji, Steven Bilaj, Amir Rezaei Balef, Setareh Maghsudi</author><pubDate>Wed, 26 Jul 2023 13:06:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14138v1</guid></item><item><title>Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models</title><link>http://arxiv.org/abs/2307.14134v1</link><description>This study introduces and evaluates tiny, mini, small, and medium-sizeduncased Turkish BERT models, aiming to bridge the research gap inless-resourced languages. We trained these models on a diverse datasetencompassing over 75GB of text from multiple sources and tested them on severaltasks, including mask prediction, sentiment analysis, news classification, and,zero-shot classification. Despite their smaller size, our models exhibitedrobust performance, including zero-shot task, while ensuring computationalefficiency and faster execution times. Our findings provide valuable insightsinto the development and application of smaller language models, especially inthe context of the Turkish language.</description><author>Himmet Toprak Kesgin, Muzaffer Kaan Yuce, Mehmet Fatih Amasyali</author><pubDate>Wed, 26 Jul 2023 13:02:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14134v1</guid></item><item><title>Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition</title><link>http://arxiv.org/abs/2307.14132v1</link><description>RNN-T models are widely used in ASR, which rely on the RNN-T loss to achievelength alignment between input audio and target sequence. However, theimplementation complexity and the alignment-based optimization target of RNN-Tloss lead to computational redundancy and a reduced role for predictor network,respectively. In this paper, we propose a novel model named CIF-Transducer(CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanismwith the RNN-T model to achieve efficient alignment. In this way, the RNN-Tloss is abandoned, thus bringing a computational reduction and allowing thepredictor network a more significant role. We also introduce Funnel-CIF,Context Blocks, Unified Gating and Bilinear Pooling joint network, andauxiliary training strategy to further improve performance. Experiments on the178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achievesstate-of-the-art results with lower computational overhead compared to RNN-Tmodels.</description><author>Tian-Hao Zhang, Dinghao Zhou, Guiping Zhon, Baoxiang Li</author><pubDate>Wed, 26 Jul 2023 12:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14132v1</guid></item><item><title>Self-supervised dense representation learning for live-cell microscopy with time arrow prediction</title><link>http://arxiv.org/abs/2305.05511v2</link><description>State-of-the-art object detection and segmentation methods for microscopyimages rely on supervised machine learning, which requires laborious manualannotation of training data. Here we present a self-supervised method based ontime arrow prediction pre-training that learns dense image representations fromraw, unlabeled live-cell microscopy videos. Our method builds upon the task ofpredicting the correct order of time-flipped image regions via a single-imagefeature extractor followed by a time arrow prediction head that operates on thefused features. We show that the resulting dense representations captureinherently time-asymmetric biological processes such as cell divisions on apixel-level. We furthermore demonstrate the utility of these representations onseveral live-cell microscopy datasets for detection and segmentation ofdividing cells, as well as for cell state classification. Our methodoutperforms supervised methods, particularly when only limited ground truthannotations are available as is commonly the case in practice. We provide codeat https://github.com/weigertlab/tarrow.</description><author>Benjamin Gallusser, Max Stieber, Martin Weigert</author><pubDate>Wed, 26 Jul 2023 12:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05511v2</guid></item><item><title>Few-shot Medical Image Segmentation via Cross-Reference Transformer</title><link>http://arxiv.org/abs/2304.09630v4</link><description>Deep learning models have become the mainstream method for medical imagesegmentation, but they require a large manually labeled dataset for trainingand are difficult to extend to unseen categories. Few-shot segmentation(FSS)has the potential to address these challenges by learning new categories from asmall number of labeled samples. The majority of the current methods employ aprototype learning architecture, which involves expanding support prototypevectors and concatenating them with query features to conduct conditionalsegmentation. However, such framework potentially focuses more on queryfeatures while may neglect the correlation between support and query features.In this paper, we propose a novel self-supervised few shot medical imagesegmentation network with Cross-Reference Transformer, which addresses the lackof interaction between the support image and the query image. We first enhancethe correlation features between the support set image and the query imageusing a bidirectional cross-attention module. Then, we employ a cross-referencemechanism to mine and enhance the similar parts of support features and queryfeatures in high-dimensional channels. Experimental results show that theproposed model achieves good results on both CT dataset and MRI dataset.</description><author>Yao Huang, Jianming Liu</author><pubDate>Wed, 26 Jul 2023 12:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09630v4</guid></item><item><title>Creative Birds: Self-Supervised Single-View 3D Style Transfer</title><link>http://arxiv.org/abs/2307.14127v1</link><description>In this paper, we propose a novel method for single-view 3D style transferthat generates a unique 3D object with both shape and texture transfer. Ourfocus lies primarily on birds, a popular subject in 3D reconstruction, forwhich no existing single-view 3D transfer methods have been developed.Themethod we propose seeks to generate a 3D mesh shape and texture of a bird fromtwo single-view images. To achieve this, we introduce a novel shape transfergenerator that comprises a dual residual gated network (DRGNet), and amulti-layer perceptron (MLP). DRGNet extracts the features of source and targetimages using a shared coordinate gate unit, while the MLP generates spatialcoordinates for building a 3D mesh. We also introduce a semantic UV texturetransfer module that implements textural style transfer using semantic UVsegmentation, which ensures consistency in the semantic meaning of thetransferred regions. This module can be widely adapted to many existingapproaches. Finally, our method constructs a novel 3D bird using adifferentiable renderer. Experimental results on the CUB dataset verify thatour method achieves state-of-the-art performance on the single-view 3D styletransfer task. Code is available inhttps://github.com/wrk226/2D-to-3D-Evolution-Transfer.</description><author>Renke Wang, Guimin Que, Shuo Chen, Xiang Li, Jun Li, Jian Yang</author><pubDate>Wed, 26 Jul 2023 12:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14127v1</guid></item><item><title>Multi-modal Learning with Missing Modality via Shared-Specific Feature Modelling</title><link>http://arxiv.org/abs/2307.14126v1</link><description>The missing modality issue is critical but non-trivial to be solved bymulti-modal models. Current methods aiming to handle the missing modalityproblem in multi-modal tasks, either deal with missing modalities only duringevaluation or train separate models to handle specific missing modalitysettings. In addition, these models are designed for specific tasks, so forexample, classification models are not easily adapted to segmentation tasks andvice versa. In this paper, we propose the Shared-Specific Feature Modelling(ShaSpec) method that is considerably simpler and more effective than competingapproaches that address the issues above. ShaSpec is designed to take advantageof all available input modalities during training and evaluation by learningshared and specific features to better represent the input data. This isachieved from a strategy that relies on auxiliary tasks based on distributionalignment and domain classification, in addition to a residual feature fusionprocedure. Also, the design simplicity of ShaSpec enables its easy adaptationto multiple tasks, such as classification and segmentation. Experiments areconducted on both medical image segmentation and computer visionclassification, with results indicating that ShaSpec outperforms competingmethods by a large margin. For instance, on BraTS2018, ShaSpec improves theSOTA by more than 3% for enhancing tumour, 5% for tumour core and 3% for wholetumour.</description><author>Hu Wang, Yuanhong Chen, Congbo Ma, Jodie Avery, Louise Hull, Gustavo Carneiro</author><pubDate>Wed, 26 Jul 2023 12:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14126v1</guid></item><item><title>Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras</title><link>http://arxiv.org/abs/2307.14124v1</link><description>Recent advances in event camera research emphasize processing data in itsoriginal sparse form, which allows the use of its unique features such as hightemporal resolution, high dynamic range, low latency, and resistance to imageblur. One promising approach for analyzing event data is through graphconvolutional networks (GCNs). However, current research in this domainprimarily focuses on optimizing computational costs, neglecting the associatedmemory costs. In this paper, we consider both factors together in order toachieve satisfying results and relatively low model complexity. For thispurpose, we performed a comparative analysis of different graph convolutionoperations, considering factors such as execution time, the number of trainablemodel parameters, data format requirements, and training outcomes. Our resultsshow a 450-fold reduction in the number of parameters for the featureextraction module and a 4.5-fold reduction in the size of the datarepresentation while maintaining a classification accuracy of 52.3%, which is6.3% higher compared to the operation used in state-of-the-art approaches. Tofurther evaluate performance, we implemented the object detection architectureand evaluated its performance on the N-Caltech101 dataset. The results showedan accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs persecond.</description><author>Kamil Jeziorek, Andrea Pinna, Tomasz Kryjak</author><pubDate>Wed, 26 Jul 2023 12:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14124v1</guid></item><item><title>Factor Fields: A Unified Framework for Neural Fields and Beyond</title><link>http://arxiv.org/abs/2302.01226v2</link><description>We present Factor Fields, a novel framework for modeling and representingsignals. Factor Fields decomposes a signal into a product of factors, each ofwhich is represented by a neural or regular field representation operating on acoordinate transformed input signal. We show that this decomposition yields aunified framework that generalizes several recent signal representationsincluding NeRF, PlenOxels, EG3D, Instant-NGP, and TensoRF. Moreover, theframework allows for the creation of powerful new signal representations, suchas the Coefficient-Basis Factorization (CoBaFa) which we propose in this paper.As evidenced by our experiments, CoBaFa leads to improvements over previousfast reconstruction methods in terms of the three critical goals in neuralsignal representation: approximation quality, compactness and efficiency.Experimentally, we demonstrate that our representation achieves better imageapproximation quality on 2D image regression tasks, higher geometric qualitywhen reconstructing 3D signed distance fields and higher compactness forradiance field reconstruction tasks compared to previous fast reconstructionmethods. Besides, our CoBaFa representation enables generalization by sharingthe basis across signals during training, enabling generalization tasks such asimage regression with sparse observations and few-shot radiance fieldreconstruction. Project Page: https://apchenstu.github.io/FactorFields/</description><author>Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, Andreas Geiger</author><pubDate>Wed, 26 Jul 2023 12:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01226v2</guid></item><item><title>Cliqueful graphs as a means of calculating the maximal number of maximum cliques of simple graphs</title><link>http://arxiv.org/abs/2307.14120v1</link><description>A simple graph on $n$ vertices may contain a lot of maximum cliques. But howmany can it potentially contain? We will show that the maximum number ofmaximum cliques is taken over so-called cliqueful graphs, more specifically,later we will show that it is taken over saturated composite cliqueful graphs,if $n \ge 15$. Using this we will show that the graph that contains $3^{\lfloorn/3 \rfloor}c$ maxcliques has the most number of maxcliques on $n$ vertices,where $c\in\{1,\frac{4}{3},2\}$, depending on $n \text{ mod } 3$.</description><author>Dániel Pfeifer</author><pubDate>Wed, 26 Jul 2023 12:39:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14120v1</guid></item><item><title>A semantics-driven methodology for high-quality image annotation</title><link>http://arxiv.org/abs/2307.14119v1</link><description>Recent work in Machine Learning and Computer Vision has highlighted thepresence of various types of systematic flaws inside ground truth objectrecognition benchmark datasets. Our basic tenet is that these flaws are rootedin the many-to-many mappings which exist between the visual information encodedin images and the intended semantics of the labels annotating them. The netconsequence is that the current annotation process is largely under-specified,thus leaving too much freedom to the subjective judgment of annotators. In thispaper, we propose vTelos, an integrated Natural Language Processing, KnowledgeRepresentation, and Computer Vision methodology whose main goal is to makeexplicit the (otherwise implicit) intended annotation semantics, thusminimizing the number and role of subjective choices. A key element of vTelosis the exploitation of the WordNet lexico-semantic hierarchy as the main meansfor providing the meaning of natural language labels and, as a consequence, fordriving the annotation of images based on the objects and the visual propertiesthey depict. The methodology is validated on images populating a subset of theImageNet hierarchy.</description><author>Fausto Giunchiglia, Mayukh Bagchi, Xiaolei Diao</author><pubDate>Wed, 26 Jul 2023 12:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14119v1</guid></item><item><title>Leveraging Implicit Feedback from Deployment Data in Dialogue</title><link>http://arxiv.org/abs/2307.14117v1</link><description>We study improving social conversational agents by learning from naturaldialogue between users and a deployed model, without extra annotations. Toimplicitly measure the quality of a machine-generated utterance, we leveragesignals like user response length, sentiment and reaction of the future humanutterances in the collected dialogue episodes. Our experiments use the publiclyreleased deployment data from BlenderBot (Xu et al., 2023). Human evaluationindicates improvements in our new models over baseline responses; however, wefind that some proxy signals can lead to more generations with undesirableproperties as well. For example, optimizing for conversation length can lead tomore controversial or unfriendly generations compared to the baseline, whereasoptimizing for positive sentiment or reaction can decrease these behaviors.</description><author>Richard Yuanzhe Pang, Stephen Roller, Kyunghyun Cho, He He, Jason Weston</author><pubDate>Wed, 26 Jul 2023 12:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14117v1</guid></item><item><title>Periocular biometrics: databases, algorithms and directions</title><link>http://arxiv.org/abs/2307.14111v1</link><description>Periocular biometrics has been established as an independent modality due toconcerns on the performance of iris or face systems in uncontrolled conditions.Periocular refers to the facial region in the eye vicinity, including eyelids,lashes and eyebrows. It is available over a wide range of acquisitiondistances, representing a trade-off between the whole face (which can beoccluded at close distances) and the iris texture (which do not have enoughresolution at long distances). Since the periocular region appears in face oriris images, it can be used also in conjunction with these modalities. Featuresextracted from the periocular region have been also used successfully forgender classification and ethnicity classification, and to study the impact ofgender transformation or plastic surgery in the recognition performance. Thispaper presents a review of the state of the art in periocular biometricresearch, providing an insight of the most relevant issues and giving athorough coverage of the existing literature. Future research trends are alsobriefly discussed.</description><author>Fernando Alonso-Fernandez, Josef Bigun</author><pubDate>Wed, 26 Jul 2023 12:14:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14111v1</guid></item><item><title>GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs</title><link>http://arxiv.org/abs/2307.14109v1</link><description>GraphRNN is a deep learning-based architecture proposed by You et al. forlearning generative models for graphs. We replicate the results of You et al.using a reproduced implementation of the GraphRNN architecture and evaluatethis against baseline models using new metrics. Through an ablation study, wefind that the BFS traversal suggested by You et al. to collapse representationsof isomorphic graphs contributes significantly to model performance.Additionally, we extend GraphRNN to generate directed acyclic graphs byreplacing the BFS traversal with a topological sort. We demonstrate that thismethod improves significantly over a directed-multiclass variant of GraphRNN ona real-world dataset.</description><author>Taniya Das, Mark Koch, Maya Ravichandran, Nikhil Khatri</author><pubDate>Wed, 26 Jul 2023 12:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14109v1</guid></item><item><title>Decoding ChatGPT: A Taxonomy of Existing Research, Current Challenges, and Possible Future Directions</title><link>http://arxiv.org/abs/2307.14107v1</link><description>Chat Generative Pre-trained Transformer (ChatGPT) has gained significantinterest and attention since its launch in November 2022. It has shownimpressive performance in various domains, including passing exams and creativewriting. However, challenges and concerns related to biases and trust persist.In this work, we present a comprehensive review of over 100 Scopus-indexedpublications on ChatGPT, aiming to provide a taxonomy of ChatGPT research andexplore its applications. We critically analyze the existing literature,identifying common approaches employed in the studies. Additionally, weinvestigate diverse application areas where ChatGPT has found utility, such ashealthcare, marketing and financial services, software engineering, academicand scientific writing, research and education, environmental science, andnatural language processing. Through examining these applications, we gainvaluable insights into the potential of ChatGPT in addressing real-worldchallenges. We also discuss crucial issues related to ChatGPT, including biasesand trustworthiness, emphasizing the need for further research and developmentin these areas. Furthermore, we identify potential future directions forChatGPT research, proposing solutions to current challenges and speculating onexpected advancements. By fully leveraging the capabilities of ChatGPT, we canunlock its potential across various domains, leading to advancements inconversational AI and transformative impacts in society.</description><author>Shahab Saquib Sohail, Faiza Farhat, Yassine Himeur, Mohammad Nadeem, Dag Øivind Madsen, Yashbir Singh, Shadi Atalla, Wathiq Mansoor</author><pubDate>Wed, 26 Jul 2023 12:10:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14107v1</guid></item><item><title>Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts</title><link>http://arxiv.org/abs/2303.17595v3</link><description>Supervised learning of image classifiers distills human knowledge into aparametric model through pairs of images and corresponding labels (X,Y). Weargue that this simple and widely used representation of human knowledgeneglects rich auxiliary information from the annotation procedure, such as thetime-series of mouse traces and clicks left after image selection. Our insightis that such annotation byproducts Z provide approximate human attention thatweakly guides the model to focus on the foreground cues, reducing spuriouscorrelations and discouraging shortcut learning. To verify this, we createImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched withsample-wise annotation byproducts, collected by replicating the respectiveoriginal annotation tasks. We refer to the new paradigm of training models withannotation byproducts as learning using annotation byproducts (LUAB). We showthat a simple multitask loss for regressing Z together with Y already improvesthe generalisability and robustness of the learned models. Compared to theoriginal supervised learning, LUAB does not require extra annotation costs.ImageNet-AB and COCO-AB are at https://github.com/naver-ai/NeglectedFreeLunch.</description><author>Dongyoon Han, Junsuk Choe, Seonghyeok Chun, John Joon Young Chung, Minsuk Chang, Sangdoo Yun, Jean Y. Song, Seong Joon Oh</author><pubDate>Wed, 26 Jul 2023 12:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17595v3</guid></item><item><title>Exploring Weight Balancing on Long-Tailed Recognition Problem</title><link>http://arxiv.org/abs/2305.16573v3</link><description>Recognition problems in long-tailed data, where the sample size per class isheavily skewed, have recently gained importance because the distribution of thesample size per class in a dataset is generally exponential unless the samplesize is intentionally adjusted. Various approaches have been devised to addressthese problems. Recently, weight balancing, which combines well-known classicalregularization techniques with two-stage training, has been proposed. Despiteits simplicity, it is known for its high performance against existing methodsdevised in various ways. However, there is a lack of understanding as to whythis approach is effective for long-tailed data. In this study, we analyze themethod focusing on neural collapse and cone effect at each training stage andfind that it can be decomposed into the increase in Fisher's discriminant ratioof the feature extractor caused by weight decay and cross entropy loss andimplicit logit adjustment caused by weight decay and class-balanced loss. Ouranalysis shows that the training method can be further simplified by reducingthe number of training stages to one while increasing accuracy.</description><author>Naoya Hasegawa, Issei Sato</author><pubDate>Wed, 26 Jul 2023 12:03:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16573v3</guid></item><item><title>Beyond the Edge of Stability via Two-step Gradient Updates</title><link>http://arxiv.org/abs/2206.04172v3</link><description>Gradient Descent (GD) is a powerful workhorse of modern machine learningthanks to its scalability and efficiency in high-dimensional spaces. Itsability to find local minimisers is only guaranteed for losses with Lipschitzgradients, where it can be seen as a `bona-fide' discretisation of anunderlying gradient flow. Yet, many ML setups involving overparametrised modelsdo not fall into this problem class, which has motivated research beyond theso-called ``Edge of Stability'' (EoS), where the step-size crosses theadmissibility threshold inversely proportional to the Lipschitz constant above.Perhaps surprisingly, GD has been empirically observed to still convergeregardless of local instability and oscillatory behavior. The incipient theoretical analysis of this phenomena has mainly focused inthe overparametrised regime, where the effect of choosing a large learning ratemay be associated to a `Sharpness-Minimisation' implicit regularisation withinthe manifold of minimisers, under appropriate asymptotic limits. In contrast,in this work we directly examine the conditions for such unstable convergence,focusing on simple, yet representative, learning problems, via analysis oftwo-step gradient updates. Specifically, we characterize a local conditioninvolving third-order derivatives that guarantees existence and convergence tofixed points of the two-step updates, and leverage such property in ateacher-student setting, under population loss. Finally, starting from MatrixFactorization, we provide observations of period-2 orbit of GD inhigh-dimensional settings with intuition of its dynamics, along withexploration into more general settings.</description><author>Lei Chen, Joan Bruna</author><pubDate>Wed, 26 Jul 2023 11:48:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.04172v3</guid></item></channel></rss>