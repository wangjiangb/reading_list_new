<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 06 Jul 2023 14:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LongNet: Scaling Transformers to 1,000,000,000 Tokens</title><link>http://arxiv.org/abs/2307.02486v1</link><description>Scaling sequence length has become a critical demand in the era of largelanguage models. However, existing methods struggle with either computationalcomplexity or model expressivity, rendering the maximum sequence lengthrestricted. In this work, we introduce LongNet, a Transformer variant that canscale sequence length to more than 1 billion tokens, without sacrificing theperformance on shorter sequences. Specifically, we propose dilated attention,which expands the attentive field exponentially as the distance grows. LongNethas significant advantages: 1) it has a linear computation complexity and alogarithm dependency between tokens; 2) it can be served as a distributedtrainer for extremely long sequences; 3) its dilated attention is a drop-inreplacement for standard attention, which can be seamlessly integrated with theexisting Transformer-based optimization. Experiments results demonstrate thatLongNet yields strong performance on both long-sequence modeling and generallanguage tasks. Our work opens up new possibilities for modeling very longsequences, e.g., treating a whole corpus or even the entire Internet as asequence.</description><author>Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Furu Wei</author><pubDate>Wed, 05 Jul 2023 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02486v1</guid></item><item><title>Compositionality as Lexical Symmetry</title><link>http://arxiv.org/abs/2201.12926v2</link><description>In tasks like semantic parsing, instruction following, and questionanswering, standard deep networks fail to generalize compositionally from smalldatasets. Many existing approaches overcome this limitation with modelarchitectures that enforce a compositional process of sentence interpretation.In this paper, we present a domain-general and model-agnostic formulation ofcompositionality as a constraint on symmetries of data distributions ratherthan models. Informally, we prove that whenever a task can be solved by acompositional model, there is a corresponding data augmentation scheme -- aprocedure for transforming examples into other well formed examples -- thatimparts compositional inductive bias on any model trained to solve the sametask. We describe a procedure called LEXSYM that discovers thesetransformations automatically, then applies them to training data for ordinaryneural sequence models. Unlike existing compositional data augmentationprocedures, LEXSYM can be deployed agnostically across text, structured data,and even images. It matches or surpasses state-of-the-art, task-specific modelson COGS semantic parsing, SCAN and ALCHEMY instruction following, andCLEVR-COGENT visual question answering datasets.</description><author>Ekin Akyürek, Jacob Andreas</author><pubDate>Wed, 05 Jul 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.12926v2</guid></item><item><title>Building Cooperative Embodied Agents Modularly with Large Language Models</title><link>http://arxiv.org/abs/2307.02485v1</link><description>Large Language Models (LLMs) have demonstrated impressive planning abilitiesin single-agent embodied tasks across various domains. However, their capacityfor planning and communication in multi-agent cooperation remains unclear, eventhough these are crucial skills for intelligent embodied agents. In this paper,we present a novel framework that utilizes LLMs for multi-agent cooperation andtests it in various embodied environments. Our framework enables embodiedagents to plan, communicate, and cooperate with other embodied agents or humansto accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs,such as GPT-4, can surpass strong planning-based methods and exhibit emergenteffective communication using our framework without requiring fine-tuning orfew-shot prompting. We also discover that LLM-based agents that communicate innatural language can earn more trust and cooperate more effectively withhumans. Our research underscores the potential of LLMs for embodied AI and laysthe foundation for future research in multi-agent cooperation. Videos can befound on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.</description><author>Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan</author><pubDate>Wed, 05 Jul 2023 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02485v1</guid></item><item><title>Elastic Decision Transformer</title><link>http://arxiv.org/abs/2307.02484v1</link><description>This paper introduces Elastic Decision Transformer (EDT), a significantadvancement over the existing Decision Transformer (DT) and its variants.Although DT purports to generate an optimal trajectory, empirical evidencesuggests it struggles with trajectory stitching, a process involving thegeneration of an optimal or near-optimal trajectory from the best parts of aset of sub-optimal trajectories. The proposed EDT differentiates itself byfacilitating trajectory stitching during action inference at test time,achieved by adjusting the history length maintained in DT. Further, the EDToptimizes the trajectory by retaining a longer history when the previoustrajectory is optimal and a shorter one when it is sub-optimal, enabling it to"stitch" with a more optimal trajectory. Extensive experimentation demonstratesEDT's ability to bridge the performance gap between DT-based and QLearning-based approaches. In particular, the EDT outperforms Q Learning-basedmethods in a multi-task regime on the D4RL locomotion benchmark and Atarigames. Videos are available at: https://kristery.github.io/edt/</description><author>Yueh-Hua Wu, Xiaolong Wang, Masashi Hamaya</author><pubDate>Wed, 05 Jul 2023 18:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02484v1</guid></item><item><title>Jailbroken: How Does LLM Safety Training Fail?</title><link>http://arxiv.org/abs/2307.02483v1</link><description>Large language models trained for safety and harmlessness remain susceptibleto adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks onearly releases of ChatGPT that elicit undesired behavior. Going beyondrecognition of the issue, we investigate why such attacks succeed and how theycan be created. We hypothesize two failure modes of safety training: competingobjectives and mismatched generalization. Competing objectives arise when amodel's capabilities and safety goals conflict, while mismatched generalizationoccurs when safety training fails to generalize to a domain for whichcapabilities exist. We use these failure modes to guide jailbreak design andthen evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic'sClaude v1.3, against both existing and newly designed attacks. We find thatvulnerabilities persist despite the extensive red-teaming and safety-trainingefforts behind these models. Notably, new attacks utilizing our failure modessucceed on every prompt in a collection of unsafe requests from the models'red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Ouranalysis emphasizes the need for safety-capability parity -- that safetymechanisms should be as sophisticated as the underlying model -- and arguesagainst the idea that scaling alone can resolve these safety failure modes.</description><author>Alexander Wei, Nika Haghtalab, Jacob Steinhardt</author><pubDate>Wed, 05 Jul 2023 18:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02483v1</guid></item><item><title>A Dataset of Inertial Measurement Units for Handwritten English Alphabets</title><link>http://arxiv.org/abs/2307.02480v1</link><description>This paper presents an end-to-end methodology for collecting datasets torecognize handwritten English alphabets by utilizing Inertial Measurement Units(IMUs) and leveraging the diversity present in the Indian writing style. TheIMUs are utilized to capture the dynamic movement patterns associated withhandwriting, enabling more accurate recognition of alphabets. The Indiancontext introduces various challenges due to the heterogeneity in writingstyles across different regions and languages. By leveraging this diversity,the collected dataset and the collection system aim to achieve higherrecognition accuracy. Some preliminary experimental results demonstrate theeffectiveness of the dataset in accurately recognizing handwritten Englishalphabet in the Indian context. This research can be extended and contributesto the field of pattern recognition and offers valuable insights for developingimproved systems for handwriting recognition, particularly in diverselinguistic and cultural contexts.</description><author>Hari Prabhat Gupta, Rahul Mishra</author><pubDate>Wed, 05 Jul 2023 18:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02480v1</guid></item><item><title>STEdge: Self-training Edge Detection with Multi-layer Teaching and Regularization</title><link>http://arxiv.org/abs/2201.05121v2</link><description>Learning-based edge detection has hereunto been strongly supervised withpixel-wise annotations which are tedious to obtain manually. We study theproblem of self-training edge detection, leveraging the untapped wealth oflarge-scale unlabeled image datasets. We design a self-supervised frameworkwith multi-layer regularization and self-teaching. In particular, we impose aconsistency regularization which enforces the outputs from each of the multiplelayers to be consistent for the input image and its perturbed counterpart. Weadopt L0-smoothing as the 'perturbation' to encourage edge prediction lying onsalient boundaries following the cluster assumption in self-supervisedlearning. Meanwhile, the network is trained with multi-layer supervision bypseudo labels which are initialized with Canny edges and then iterativelyrefined by the network as the training proceeds. The regularization andself-teaching together attain a good balance of precision and recall, leadingto a significant performance boost over supervised methods, with lightweightrefinement on the target dataset. Furthermore, our method demonstrates strongcross-dataset generality. For example, it attains 4.8% improvement for ODS and5.8% for OIS when tested on the unseen BIPED dataset, compared to thestate-of-the-art methods.</description><author>Yunfan Ye, Renjiao Yi, Zhiping Cai, Kai Xu</author><pubDate>Wed, 05 Jul 2023 18:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.05121v2</guid></item><item><title>Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions</title><link>http://arxiv.org/abs/2307.02478v1</link><description>In this paper, we study linear regression applied to data structured on amanifold. We assume that the data manifold is smooth and is embedded in aEuclidean space, and our objective is to reveal the impact of the datamanifold's extrinsic geometry on the regression. Specifically, we analyze theimpact of the manifold's curvatures (or higher order nonlinearity in theparameterization when the curvatures are locally zero) on the uniqueness of theregression solution. Our findings suggest that the corresponding linearregression does not have a unique solution when the embedded submanifold isflat in some dimensions. Otherwise, the manifold's curvature (or higher ordernonlinearity in the embedding) may contribute significantly, particularly inthe solution associated with the normal directions of the manifold. Ourfindings thus reveal the role of data manifold geometry in ensuring thestability of regression models for out-of-distribution inferences.</description><author>Liangchen Liu, Juncai He, Richard Tsai</author><pubDate>Wed, 05 Jul 2023 18:51:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02478v1</guid></item><item><title>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks</title><link>http://arxiv.org/abs/2307.02477v1</link><description>The impressive performance of recent language models across a wide range oftasks suggests that they possess a degree of abstract reasoning skills. Arethese skills general and transferable, or specialized to specific tasks seenduring pretraining? To disentangle these effects, we propose an evaluationframework based on "counterfactual" task variants that deviate from the defaultassumptions underlying standard tasks. Across a suite of 11 tasks, we observenontrivial performance on the counterfactual variants, but nevertheless findthat performance substantially and consistently degrades compared to thedefault conditions. This suggests that while current LMs may possess abstracttask-solving skills to a degree, they often also rely on narrow,non-transferable procedures for task-solving. These results motivate a morecareful interpretation of language model performance that teases apart theseaspects of behavior.</description><author>Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim</author><pubDate>Wed, 05 Jul 2023 18:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02477v1</guid></item><item><title>Natural Language Deduction with Incomplete Information</title><link>http://arxiv.org/abs/2307.02472v1</link><description>A growing body of work studies how to answer a question or verify a claim bygenerating a natural language "proof": a chain of deductive inferences yieldingthe answer based on a set of premises. However, these methods can only makesound deductions when they follow from evidence that is given. We propose a newsystem that can handle the underspecified setting where not all premises arestated at the outset; that is, additional assumptions need to be materializedto prove a claim. By using a natural language generation model to abductivelyinfer a premise given another premise and a conclusion, we can impute missingpieces of evidence needed for the conclusion to be true. Our system searchesover two fringes in a bidirectional fashion, interleaving deductive(forward-chaining) and abductive (backward-chaining) generation steps. Wesample multiple possible outputs for each step to achieve coverage of thesearch space, at the same time ensuring correctness by filtering low-qualitygenerations with a round-trip validation procedure. Results on a modifiedversion of the EntailmentBank dataset and a new dataset called Everyday Norms:Why Not? show that abductive generation with validation can recover premisesacross in- and out-of-domain settings</description><author>Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett</author><pubDate>Wed, 05 Jul 2023 18:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02472v1</guid></item><item><title>What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?</title><link>http://arxiv.org/abs/2307.02469v1</link><description>Recent advancements in Large Language Models (LLMs) such as GPT4 havedisplayed exceptional multi-modal capabilities in following open-endedinstructions given images. However, the performance of these models heavilyrelies on design choices such as network structures, training data, andtraining strategies, and these choices have not been extensively discussed inthe literature, making it difficult to quantify progress in this field. Toaddress this issue, this paper presents a systematic and comprehensive study,quantitatively and qualitatively, on training such models. We implement over 20variants with controlled settings. Concretely, for network structures, wecompare different LLM backbones and model designs. For training data, weinvestigate the impact of data and sampling strategies. For instructions, weexplore the influence of diversified prompts on the instruction-followingability of the trained models. For benchmarks, we contribute the first, to ourbest knowledge, comprehensive evaluation set including both image and videotasks through crowd-sourcing. Based on our findings, we present Lynx, whichperforms the most accurate multi-modal understanding while keeping the bestmulti-modal generation ability compared to existing open-sourced GPT4-stylemodels.</description><author>Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, Tao Kong</author><pubDate>Wed, 05 Jul 2023 18:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02469v1</guid></item><item><title>Large-scale Detection of Marine Debris in Coastal Areas with Sentinel-2</title><link>http://arxiv.org/abs/2307.02465v1</link><description>Detecting and quantifying marine pollution and macro-plastics is anincreasingly pressing ecological issue that directly impacts ecology and humanhealth. Efforts to quantify marine pollution are often conducted with sparseand expensive beach surveys, which are difficult to conduct on a large scale.Here, remote sensing can provide reliable estimates of plastic pollution byregularly monitoring and detecting marine debris in coastal areas.Medium-resolution satellite data of coastal areas is readily available and canbe leveraged to detect aggregations of marine debris containing plastic litter.In this work, we present a detector for marine debris built on a deepsegmentation model that outputs a probability for marine debris at the pixellevel. We train this detector with a combination of annotated datasets ofmarine debris and evaluate it on specifically selected test sites where it ishighly probable that plastic pollution is present in the detected marinedebris. We demonstrate quantitatively and qualitatively that a deep learningmodel trained on this dataset issued from multiple sources outperforms existingdetection models trained on previous datasets by a large margin. Ourexperiments show, consistent with the principles of data-centric AI, that thisperformance is due to our particular dataset design with extensive sampling ofnegative examples and label refinements rather than depending on the particulardeep learning model. We hope to accelerate advances in the large-scaleautomated detection of marine debris, which is a step towards quantifying andmonitoring marine litter with remote sensing at global scales, and release themodel weights and training source code underhttps://github.com/marccoru/marinedebrisdetector</description><author>Marc Rußwurm, Sushen Jilla Venkatesa, Devis Tuia</author><pubDate>Wed, 05 Jul 2023 18:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02465v1</guid></item><item><title>AxonCallosumEM Dataset: Axon Semantic Segmentation of Whole Corpus Callosum cross section from EM Images</title><link>http://arxiv.org/abs/2307.02464v1</link><description>The electron microscope (EM) remains the predominant technique forelucidating intricate details of the animal nervous system at the nanometerscale. However, accurately reconstructing the complex morphology of axons andmyelin sheaths poses a significant challenge. Furthermore, the absence ofpublicly available, large-scale EM datasets encompassing complete crosssections of the corpus callosum, with dense ground truth segmentation for axonsand myelin sheaths, hinders the advancement and evaluation of holistic corpuscallosum reconstructions. To surmount these obstacles, we introduce theAxonCallosumEM dataset, comprising a 1.83 times 5.76mm EM image captured fromthe corpus callosum of the Rett Syndrome (RTT) mouse model, which entailextensive axon bundles. We meticulously proofread over 600,000 patches at aresolution of 1024 times 1024, thus providing a comprehensive ground truth formyelinated axons and myelin sheaths. Additionally, we extensively annotatedthree distinct regions within the dataset for the purposes of training,testing, and validation. Utilizing this dataset, we develop a fine-tuningmethodology that adapts Segment Anything Model (SAM) to EM images segmentationtasks, called EM-SAM, enabling outperforms other state-of-the-art methods.Furthermore, we present the evaluation results of EM-SAM as a baseline.</description><author>Ao Cheng, Guoqiang Zhao, Lirong Wang, Ruobing Zhang</author><pubDate>Wed, 05 Jul 2023 18:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02464v1</guid></item><item><title>Expert-Agnostic Ultrasound Image Quality Assessment using Deep Variational Clustering</title><link>http://arxiv.org/abs/2307.02462v1</link><description>Ultrasound imaging is a commonly used modality for several diagnostic andtherapeutic procedures. However, the diagnosis by ultrasound relies heavily onthe quality of images assessed manually by sonographers, which diminishes theobjectivity of the diagnosis and makes it operator-dependent. The supervisedlearning-based methods for automated quality assessment require manuallyannotated datasets, which are highly labour-intensive to acquire. Theseultrasound images are low in quality and suffer from noisy annotations causedby inter-observer perceptual variations, which hampers learning efficiency. Wepropose an UnSupervised UltraSound image Quality assessment Network, US2QNet,that eliminates the burden and uncertainty of manual annotations. US2QNet usesthe variational autoencoder embedded with the three modules, pre-processing,clustering and post-processing, to jointly enhance, extract, cluster andvisualize the quality feature representation of ultrasound images. Thepre-processing module uses filtering of images to point the network's attentiontowards salient quality features, rather than getting distracted by noise.Post-processing is proposed for visualizing the clusters of featurerepresentations in 2D space. We validated the proposed framework for qualityassessment of the urinary bladder ultrasound images. The proposed frameworkachieved 78% accuracy and superior performance to state-of-the-art clusteringmethods.</description><author>Deepak Raina, Dimitrios Ntentia, SH Chandrashekhara, Richard Voyles, Subir Kumar Saha</author><pubDate>Wed, 05 Jul 2023 18:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02462v1</guid></item><item><title>Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources</title><link>http://arxiv.org/abs/2307.02460v1</link><description>Traditionally, data selection has been studied in settings where all samplesfrom prospective sources are fully revealed to a machine learning developer.However, in practical data exchange scenarios, data providers often reveal onlya limited subset of samples before an acquisition decision is made. Recently,there have been efforts to fit scaling laws that predict model performance atany size and data source composition using the limited available samples.However, these scaling functions are black-box, computationally expensive tofit, highly susceptible to overfitting, or/and difficult to optimize for dataselection. This paper proposes a framework called &lt;projektor&gt;, which predictsmodel performance and supports data selection decisions based on partialsamples of prospective data sources. Our approach distinguishes itself fromexisting work by introducing a novel *two-stage* performance inference process.In the first stage, we leverage the Optimal Transport distance to predict themodel's performance for any data mixture ratio within the range of discloseddata sizes. In the second stage, we extrapolate the performance to largerundisclosed data sizes based on a novel parameter-free mapping techniqueinspired by neural scaling laws. We further derive an efficient gradient-basedmethod to select data sources based on the projected model performance.Evaluation over a diverse range of applications demonstrates that &lt;projektor&gt;significantly improves existing performance scaling approaches in terms of boththe accuracy of performance inference and the computation costs associated withconstructing the performance predictor. Also, &lt;projektor&gt; outperforms by a widemargin in data selection effectiveness compared to a range of otheroff-the-shelf solutions.</description><author>Feiyang Kang, Hoang Anh Just, Anit Kumar Sahu, Ruoxi Jia</author><pubDate>Wed, 05 Jul 2023 18:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02460v1</guid></item><item><title>Gaussian Database Alignment and Gaussian Planted Matching</title><link>http://arxiv.org/abs/2307.02459v1</link><description>Database alignment is a variant of the graph alignment problem: Given a pairof anonymized databases containing separate yet correlated features for a setof users, the problem is to identify the correspondence between the featuresand align the anonymized user sets based on correlation alone. This closelyrelates to planted matching, where given a bigraph with random weights, thegoal is to identify the underlying matching that generated the given weights.We study an instance of the database alignment problem with multivariateGaussian features and derive results that apply both for database alignment andfor planted matching, demonstrating the connection between them. Theperformance thresholds for database alignment converge to that for plantedmatching when the dimensionality of the database features is \(\omega(\logn)\), where \(n\) is the size of the alignment, and no individual feature istoo strong. The maximum likelihood algorithms for both planted matching anddatabase alignment take the form of a linear program and we study relaxationsto better understand the significance of various constraints under variousconditions and present achievability and converse bounds. Our results show thatthe almost-exact alignment threshold for the relaxed algorithms coincide withthat of maximum likelihood, while there is a gap between the exact alignmentthresholds. Our analysis and results extend to the unbalanced case where oneuser set is not fully covered by the alignment.</description><author>Osman Emre Dai, Daniel Cullina, Negar Kiyavash</author><pubDate>Wed, 05 Jul 2023 18:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02459v1</guid></item><item><title>DeSRA: Detect and Delete the Artifacts of GAN-based Real-World Super-Resolution Models</title><link>http://arxiv.org/abs/2307.02457v1</link><description>Image super-resolution (SR) with generative adversarial networks (GAN) hasachieved great success in restoring realistic details. However, it is notoriousthat GAN-based SR models will inevitably produce unpleasant and undesirableartifacts, especially in practical scenarios. Previous works typically suppressartifacts with an extra loss penalty in the training phase. They only work forin-distribution artifact types generated during training. When applied inreal-world scenarios, we observe that those improved methods still generateobviously annoying artifacts during inference. In this paper, we analyze thecause and characteristics of the GAN artifacts produced in unseen test datawithout ground-truths. We then develop a novel method, namely, DeSRA, to Detectand then Delete those SR Artifacts in practice. Specifically, we propose tomeasure a relative local variance distance from MSE-SR results and GAN-SRresults, and locate the problematic areas based on the above distance andsemantic-aware thresholds. After detecting the artifact regions, we develop afinetune procedure to improve GAN-based SR models with a few samples, so thatthey can deal with similar types of artifacts in more unseen real data.Equipped with our DeSRA, we can successfully eliminate artifacts from inferenceand improve the ability of SR models to be applied in real-world scenarios. Thecode will be available at https://github.com/TencentARC/DeSRA.</description><author>Liangbin Xie, Xintao Wang, Xiangyu Chen, Gen Li, Ying Shan, Jiantao Zhou, Chao Dong</author><pubDate>Wed, 05 Jul 2023 18:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02457v1</guid></item><item><title>Transgressing the boundaries: towards a rigorous understanding of deep learning and its (non-)robustness</title><link>http://arxiv.org/abs/2307.02454v1</link><description>The recent advances in machine learning in various fields of applications canbe largely attributed to the rise of deep learning (DL) methods andarchitectures. Despite being a key technology behind autonomous cars, imageprocessing, speech recognition, etc., a notorious problem remains the lack oftheoretical understanding of DL and related interpretability and (adversarial)robustness issues. Understanding the specifics of DL, as compared to, say,other forms of nonlinear regression methods or statistical learning, isinteresting from a mathematical perspective, but at the same time it is ofcrucial importance in practice: treating neural networks as mere black boxesmight be sufficient in certain cases, but many applications require waterproofperformance guarantees and a deeper understanding of what could go wrong andwhy it could go wrong. It is probably fair to say that, despite beingmathematically well founded as a method to approximate complicated functions,DL is mostly still more like modern alchemy that is firmly in the hands ofengineers and computer scientists. Nevertheless, it is evident that certainspecifics of DL that could explain its success in applications demandssystematic mathematical approaches. In this work, we review robustness issuesof DL and particularly bridge concerns and attempts from approximation theoryto statistical learning theory. Further, we review Bayesian Deep Learning as ameans for uncertainty quantification and rigorous explainability.</description><author>Carsten Hartmann, Lorenz Richter</author><pubDate>Wed, 05 Jul 2023 18:27:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02454v1</guid></item><item><title>Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models</title><link>http://arxiv.org/abs/2305.14705v2</link><description>Sparse Mixture-of-Experts (MoE) is a neural architecture design that can beutilized to add learnable parameters to Large Language Models (LLMs) withoutincreasing inference cost. Instruction tuning is a technique for training LLMsto follow instructions. We advocate combining these two approaches, as we findthat MoE models benefit more from instruction tuning than dense models. Inparticular, we conduct empirical studies across three experimental setups: (i)Direct finetuning on individual downstream tasks devoid of instruction tuning;(ii) Instructiontuning followed by in-context few-shot or zero-shotgeneralization on downstream tasks; and (iii) Instruction tuning supplementedby further finetuning on individual downstream tasks. In the first scenario,MoE models overall underperform dense models of identical computationalcapacity. This narrative, however, dramatically changes with the introductionof instruction tuning (second and third scenario), used independently or inconjunction with task-specific finetuning. Our most powerful model,FLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmarktasks, while using only a third of the FLOPs. The advancements embodiedbyFLAN-MOE inspire a reevaluation of the design principles of large-scale,high-performance language models in the framework of task-agnostic learning.</description><author>Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, Denny Zhou</author><pubDate>Wed, 05 Jul 2023 18:24:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14705v2</guid></item><item><title>LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion</title><link>http://arxiv.org/abs/2307.02452v1</link><description>Wireless capsule endoscopy (WCE) is a painless and non-invasive diagnostictool for gastrointestinal (GI) diseases. However, due to GI anatomicalconstraints and hardware manufacturing limitations, WCE vision signals maysuffer from insufficient illumination, leading to a complicated screening andexamination procedure. Deep learning-based low-light image enhancement (LLIE)in the medical field gradually attracts researchers. Given the exuberantdevelopment of the denoising diffusion probabilistic model (DDPM) in computervision, we introduce a WCE LLIE framework based on the multi-scaleconvolutional neural network (CNN) and reverse diffusion process. Themulti-scale design allows models to preserve high-resolution representation andcontext information from low-resolution, while the curved wavelet attention(CWA) block is proposed for high-frequency and local feature learning.Furthermore, we combine the reverse diffusion procedure to further optimize theshallow output and generate the most realistic image. The proposed method iscompared with ten state-of-the-art (SOTA) LLIE methods and significantlyoutperforms quantitatively and qualitatively. The superior performance on GIdisease segmentation further demonstrates the clinical potential of ourproposed model. Our code is publicly accessible.</description><author>Long Bai, Tong Chen, Yanan Wu, An Wang, Mobarakol Islam, Hongliang Ren</author><pubDate>Wed, 05 Jul 2023 18:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02452v1</guid></item><item><title>An Exploratory Literature Study on Sharing and Energy Use of Language Models for Source Code</title><link>http://arxiv.org/abs/2307.02443v1</link><description>Large language models trained on source code can support a variety ofsoftware development tasks, such as code recommendation and program repair.Large amounts of data for training such models benefit the models' performance.However, the size of the data and models results in long training times andhigh energy consumption. While publishing source code allows for replicability,users need to repeat the expensive training process if models are not shared.The main goal of the study is to investigate if publications that trainedlanguage models for software engineering (SE) tasks share source code andtrained artifacts. The second goal is to analyze the transparency on trainingenergy usage. We perform a snowballing-based literature search to findpublications on language models for source code, and analyze their reusabilityfrom a sustainability standpoint. From 494 unique publications, we identified 293 relevant publications thatuse language models to address code-related tasks. Among them, 27% (79 out of293) make artifacts available for reuse. This can be in the form of tools orIDE plugins designed for specific tasks or task-agnostic models that can befine-tuned for a variety of downstream tasks. Moreover, we collect insights onthe hardware used for model training, as well as training time, which togetherdetermine the energy consumption of the development process. We find that thereare deficiencies in the sharing of information and artifacts for currentstudies on source code models for software engineering tasks, with 40% of thesurveyed papers not sharing source code or trained artifacts. We recommend thesharing of source code as well as trained artifacts, to enable sustainablereproducibility. Moreover, comprehensive information on training times andhardware configurations should be shared for transparency on a model's carbonfootprint.</description><author>Max Hort, Anastasiia Grishina, Leon Moonen</author><pubDate>Wed, 05 Jul 2023 18:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02443v1</guid></item><item><title>Phase Unwrapping of Color Doppler Echocardiography using Deep Learning</title><link>http://arxiv.org/abs/2306.13695v2</link><description>Color Doppler echocardiography is a widely used non-invasive imaging modalitythat provides real-time information about the intracardiac blood flow. In anapical long-axis view of the left ventricle, color Doppler is subject to phasewrapping, or aliasing, especially during cardiac filling and ejection. Whensetting up quantitative methods based on color Doppler, it is necessary tocorrect this wrapping artifact. We developed an unfolded primal-dual network tounwrap (dealias) color Doppler echocardiographic images and compared itseffectiveness against two state-of-the-art segmentation approaches based onnnU-Net and transformer models. We trained and evaluated the performance ofeach method on an in-house dataset and found that the nnU-Net-based methodprovided the best dealiased results, followed by the primal-dual approach andthe transformer-based technique. Noteworthy, the primal-dual network, which hadsignificantly fewer trainable parameters, performed competitively with respectto the other two methods, demonstrating the high potential of deep unfoldingmethods. Our results suggest that deep learning-based methods can effectivelyremove aliasing artifacts in color Doppler echocardiographic images,outperforming DeAN, a state-of-the-art semi-automatic technique. Overall, ourresults show that deep learning-based methods have the potential to effectivelypreprocess color Doppler images for downstream quantitative analysis.</description><author>Hang Jung Ling, Olivier Bernard, Nicolas Ducros, Damien Garcia</author><pubDate>Wed, 05 Jul 2023 18:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13695v2</guid></item><item><title>Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes</title><link>http://arxiv.org/abs/2305.02301v2</link><description>Deploying large language models (LLMs) is challenging because they are memoryinefficient and compute-intensive for practical applications. In reaction,researchers train smaller task-specific models by either finetuning with humanlabels or distilling using LLM-generated labels. However, finetuning anddistillation require large amounts of training data to achieve comparableperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that(a) trains smaller models that outperform LLMs, and (b) achieves so byleveraging less training data needed by finetuning or distillation. Our methodextracts LLM rationales as additional supervision for training small modelswithin a multi-task framework. We present three findings across 4 NLPbenchmarks: First, compared to both finetuning and distillation, our mechanismachieves better performance with much fewer labeled/unlabeled trainingexamples. Second, compared to few-shot prompted LLMs, we achieve betterperformance using substantially smaller model sizes. Third, we reduce both themodel size and the amount of data required to outperform LLMs; our finetuned770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%of available data on a benchmark, whereas standard finetuning the same T5 modelstruggles to match even by using 100% of the dataset. We release the code at:https://github.com/google-research/distilling-step-by-step .</description><author>Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister</author><pubDate>Wed, 05 Jul 2023 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02301v2</guid></item><item><title>Exploring Continual Learning for Code Generation Models</title><link>http://arxiv.org/abs/2307.02435v1</link><description>Large-scale code generation models such as Codex and CodeT5 have achievedimpressive performance. However, libraries are upgraded or deprecated veryfrequently and re-training large-scale language models is computationallyexpensive. Therefore, Continual Learning (CL) is an important aspect thatremains underexplored in the code domain. In this paper, we introduce abenchmark called CodeTask-CL that covers a wide range of tasks, including codegeneration, translation, summarization, and refinement, with different inputand output programming languages. Next, on our CodeTask-CL benchmark, wecompare popular CL techniques from NLP and Vision domains. We find thateffective methods like Prompt Pooling (PP) suffer from catastrophic forgettingdue to the unstable training of the prompt selection mechanism caused by starkdistribution shifts in coding tasks. We address this issue with our proposedmethod, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes trainingby enforcing constraints on the prompt selection mechanism and leads to a21.54% improvement over Prompt Pooling. Along with the benchmark, we establisha training pipeline that can be used for CL on code models, which we believecan motivate further development of CL methods for code models. Our code isavailable at https://github.com/amazon-science/codetaskcl-pptf</description><author>Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Xiaofei Ma, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal, Bing Xiang</author><pubDate>Wed, 05 Jul 2023 17:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02435v1</guid></item><item><title>Exclusive Supermask Subnetwork Training for Continual Learning</title><link>http://arxiv.org/abs/2210.10209v2</link><description>Continual Learning (CL) methods focus on accumulating knowledge over timewhile avoiding catastrophic forgetting. Recently, Wortsman et al. (2020)proposed a CL method, SupSup, which uses a randomly initialized, fixed basenetwork (model) and finds a supermask for each new task that selectively keepsor removes each weight to produce a subnetwork. They prevent forgetting as thenetwork weights are not being updated. Although there is no forgetting, theperformance of SupSup is sub-optimal because fixed weights restrict itsrepresentational power. Furthermore, there is no accumulation or transfer ofknowledge inside the model when new tasks are learned. Hence, we proposeExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive andnon-overlapping subnetwork weight training. This avoids conflicting updates tothe shared weights by subsequent tasks to improve performance while stillpreventing forgetting. Furthermore, we propose a novel KNN-based KnowledgeTransfer (KKT) module that utilizes previously acquired knowledge to learn newtasks better and faster. We demonstrate that ExSSNeT outperforms strongprevious methods on both NLP and Vision domains while preventing forgetting.Moreover, ExSSNeT is particularly advantageous for sparse masks that activate2-10% of the model parameters, resulting in an average improvement of 8.3% overSupSup. Furthermore, ExSSNeT scales to a large number of tasks (100). Our codeis available at https://github.com/prateeky2806/exessnet.</description><author>Prateek Yadav, Mohit Bansal</author><pubDate>Wed, 05 Jul 2023 17:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10209v2</guid></item><item><title>A probabilistic, data-driven closure model for RANS simulations with aleatoric, model uncertainty</title><link>http://arxiv.org/abs/2307.02432v1</link><description>We propose a data-driven, closure model for Reynolds-averaged Navier-Stokes(RANS) simulations that incorporates aleatoric, model uncertainty. The proposedclosure consists of two parts. A parametric one, which utilizes previouslyproposed, neural-network-based tensor basis functions dependent on the rate ofstrain and rotation tensor invariants. This is complemented by latent, randomvariables which account for aleatoric model errors. A fully Bayesianformulation is proposed, combined with a sparsity-inducing prior in order toidentify regions in the problem domain where the parametric closure isinsufficient and where stochastic corrections to the Reynolds stress tensor areneeded. Training is performed using sparse, indirect data, such as meanvelocities and pressures, in contrast to the majority of alternatives thatrequire direct Reynolds stress data. For inference and learning, a StochasticVariational Inference scheme is employed, which is based on Monte Carloestimates of the pertinent objective in conjunction with the reparametrizationtrick. This necessitates derivatives of the output of the RANS solver, forwhich we developed an adjoint-based formulation. In this manner, the parametricsensitivities from the differentiable solver can be combined with the built-in,automatic differentiation capability of the neural network library in order toenable an end-to-end differentiable framework. We demonstrate the capability ofthe proposed model to produce accurate, probabilistic, predictive estimates forall flow quantities, even in regions where model errors are present, on aseparated flow in the backward-facing step benchmark problem.</description><author>Atul Agrawal, Phaedon-Stelios Koutsourelakis</author><pubDate>Wed, 05 Jul 2023 17:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02432v1</guid></item><item><title>Base Layer Efficiency in Scalable Human-Machine Coding</title><link>http://arxiv.org/abs/2307.02430v1</link><description>A basic premise in scalable human-machine coding is that the base layer isintended for automated machine analysis and is therefore more compressible thanthe same content would be for human viewing. Use cases for such coding includevideo surveillance and traffic monitoring, where the majority of the contentwill never be seen by humans. Therefore, base layer efficiency is of paramountimportance because the system would most frequently operate at the base-layerrate. In this paper, we analyze the coding efficiency of the base layer in astate-of-the-art scalable human-machine image codec, and show that it can beimproved. In particular, we demonstrate that gains of 20-40% in BD-Ratecompared to the currently best results on object detection and instancesegmentation are possible.</description><author>Yalda Foroutan, Alon Harell, Anderson de Andrade, Ivan V. Bajić</author><pubDate>Wed, 05 Jul 2023 17:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02430v1</guid></item><item><title>FOCUS: Object-Centric World Models for Robotics Manipulation</title><link>http://arxiv.org/abs/2307.02427v1</link><description>Understanding the world in terms of objects and the possible interplays withthem is an important cognition ability, especially in robotics manipulation,where many tasks require robot-object interactions. However, learning such astructured world model, which specifically captures entities and relationships,remains a challenging and underexplored problem. To address this, we proposeFOCUS, a model-based agent that learns an object-centric world model. Thanks toa novel exploration bonus that stems from the object-centric representation,FOCUS can be deployed on robotics manipulation tasks to explore objectinteractions more easily. Evaluating our approach on manipulation tasks acrossdifferent settings, we show that object-centric world models allow the agent tosolve tasks more efficiently and enable consistent exploration of robot-objectinteractions. Using a Franka Emika robot arm, we also showcase how FOCUS couldbe adopted in real-world settings.</description><author>Stefano Ferraro, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt</author><pubDate>Wed, 05 Jul 2023 17:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02427v1</guid></item><item><title>DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models</title><link>http://arxiv.org/abs/2307.02421v1</link><description>Despite the ability of existing large-scale text-to-image (T2I) models togenerate high-quality images from detailed textual descriptions, they oftenlack the ability to precisely edit the generated or real images. In this paper,we propose a novel image editing method, DragonDiffusion, enabling Drag-stylemanipulation on Diffusion models. Specifically, we construct classifierguidance based on the strong correspondence of intermediate features in thediffusion model. It can transform the editing signals into gradients viafeature correspondence loss to modify the intermediate representation of thediffusion model. Based on this guidance strategy, we also build a multi-scaleguidance to consider both semantic and geometric alignment. Moreover, across-branch self-attention is added to maintain the consistency between theoriginal image and the editing result. Our method, through an efficient design,achieves various editing modes for the generated or real images, such as objectmoving, object resizing, object appearance replacement, and content dragging.It is worth noting that all editing and content preservation signals come fromthe image itself, and the model does not require fine-tuning or additionalmodules. Our source code will be available athttps://github.com/MC-E/DragonDiffusion.</description><author>Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang</author><pubDate>Wed, 05 Jul 2023 17:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02421v1</guid></item><item><title>Deep Learning Hydrodynamic Forecasting for Flooded Region Assessment in Near-Real-Time (DL Hydro-FRAN)</title><link>http://arxiv.org/abs/2305.12052v2</link><description>Hydrodynamic flood modeling improves hydrologic and hydraulic prediction ofstorm events. However, the computationally intensive numerical solutionsrequired for high-resolution hydrodynamics have historically prevented theirimplementation in near-real-time flood forecasting. This study examines whetherseveral Deep Neural Network (DNN) architectures are suitable for optimizinghydrodynamic flood models. Several pluvial flooding events were simulated in alow-relief high-resolution urban environment using a 2D HEC-RAS hydrodynamicmodel. These simulations were assembled into a training set for the DNNs, whichwere then used to forecast flooding depths and velocities. The DNNs' forecastswere compared to the hydrodynamic flood models, and showed good agreement, witha median RMSE of around 2 mm for cell flooding depths in the study area. TheDNNs also improved forecast computation time significantly, with the DNNsproviding forecasts between 34.2 and 72.4 times faster than conventionalhydrodynamic models. The study area showed little change between HEC-RAS' FullMomentum Equations and Diffusion Equations, however, important numericalstability considerations were discovered that impact equation selection and DNNarchitecture configuration. Overall, the results from this study show that DNNscan greatly optimize hydrodynamic flood modeling, and enable near-real-timehydrodynamic flood forecasting.</description><author>Francisco Haces-Garcia, Natalya Maslennikova, Craig L Glennie, Hanadi S Rifai, Vedhus Hoskere, Nima Ekhtari</author><pubDate>Wed, 05 Jul 2023 17:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12052v2</guid></item><item><title>In-Context Learning for Attention Scheme: from Single Softmax Regression to Multiple Softmax Regression via a Tensor Trick</title><link>http://arxiv.org/abs/2307.02419v1</link><description>Large language models (LLMs) have brought significant and transformativechanges in human society. These models have demonstrated remarkablecapabilities in natural language understanding and generation, leading tovarious advancements and impacts across several domains. We consider the in-context learning under two formulation for attentionrelated regression in this work. Given matrices $A_1 \in \mathbb{R}^{n \timesd}$, and $A_2 \in \mathbb{R}^{n \times d}$ and $B \in \mathbb{R}^{n \times n}$,the purpose is to solve some certain optimization problems: Normalized version$\min_{X} \| D(X)^{-1} \exp(A_1 X A_2^\top) - B \|_F^2$ and Rescaled version$\| \exp(A_1 X A_2^\top) - D(X) \cdot B \|_F^2$. Here $D(X) := \mathrm{diag}(\exp(A_1 X A_2^\top) {\bf 1}_n )$. Our regression problem shares similarities with previous studies onsoftmax-related regression. Prior research has extensively investigatedregression techniques related to softmax regression: Normalized version $\|\langle \exp(Ax) , {\bf 1}_n \rangle^{-1} \exp(Ax) - b \|_2^2$ and Resscaledversion $\| \exp(Ax) - \langle \exp(Ax), {\bf 1}_n \rangle b \|_2^2 $ In contrast to previous approaches, we adopt a vectorization technique toaddress the regression problem in matrix formulation. This approach expands thedimension from $d$ to $d^2$, resembling the formulation of the regressionproblem mentioned earlier. Upon completing the lipschitz analysis of our regression function, we havederived our main result concerning in-context learning.</description><author>Yeqi Gao, Zhao Song, Shenghao Xie</author><pubDate>Wed, 05 Jul 2023 17:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02419v1</guid></item><item><title>$ν^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows</title><link>http://arxiv.org/abs/2307.02405v1</link><description>In this work we introduce $\nu^2$-Flows, an extension of the $\nu$-Flowsmethod to final states containing multiple neutrinos. The architecture cannatively scale for all combinations of object types and multiplicities in thefinal state for any desired neutrino multiplicities. In $t\bar{t}$ dileptonevents, the momenta of both neutrinos and correlations between them arereconstructed more accurately than when using the most popular standardanalytical techniques, and solutions are found for all events. Inference timeis significantly faster than competing methods, and can be reduced further byevaluating in parallel on graphics processing units. We apply $\nu^2$-Flows to$t\bar{t}$ dilepton events and show that the per-bin uncertainties in unfoldeddistributions is much closer to the limit of performance set by perfectneutrino reconstruction than standard techniques. For the chosen doubledifferential observables $\nu^2$-Flows results in improved statisticalprecision for each bin by a factor of 1.5 to 2 in comparison to the NeutrinoWeighting method and up to a factor of four in comparison to the Ellipseapproach.</description><author>John Andrew Raine, Matthew Leigh, Knut Zoch, Tobias Golling</author><pubDate>Wed, 05 Jul 2023 17:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02405v1</guid></item><item><title>Cross-Shape Attention for Part Segmentation of 3D Point Clouds</title><link>http://arxiv.org/abs/2003.09053v6</link><description>We present a deep learning method that propagates point-wise featurerepresentations across shapes within a collection for the purpose of 3D shapesegmentation. We propose a cross-shape attention mechanism to enableinteractions between a shape's point-wise features and those of other shapes.The mechanism assesses both the degree of interaction between points and alsomediates feature propagation across shapes, improving the accuracy andconsistency of the resulting point-wise feature representations for shapesegmentation. Our method also proposes a shape retrieval measure to selectsuitable shapes for cross-shape attention operations for each test shape. Ourexperiments demonstrate that our approach yields state-of-the-art results inthe popular PartNet dataset.</description><author>Marios Loizou, Siddhant Garg, Dmitry Petrov, Melinos Averkiou, Evangelos Kalogerakis</author><pubDate>Wed, 05 Jul 2023 17:26:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2003.09053v6</guid></item><item><title>Unbalanced Optimal Transport: A Unified Framework for Object Detection</title><link>http://arxiv.org/abs/2307.02402v1</link><description>During training, supervised object detection tries to correctly match thepredicted bounding boxes and associated classification scores to the groundtruth. This is essential to determine which predictions are to be pushedtowards which solutions, or to be discarded. Popular matching strategiesinclude matching to the closest ground truth box (mostly used in combinationwith anchors), or matching via the Hungarian algorithm (mostly used inanchor-free methods). Each of these strategies comes with its own properties,underlying losses, and heuristics. We show how Unbalanced Optimal Transportunifies these different approaches and opens a whole continuum of methods inbetween. This allows for a finer selection of the desired properties.Experimentally, we show that training an object detection model with UnbalancedOptimal Transport is able to reach the state-of-the-art both in terms ofAverage Precision and Average Recall as well as to provide a faster initialconvergence. The approach is well suited for GPU implementation, which provesto be an advantage for large-scale models.</description><author>Henri De Plaen, Pierre-François De Plaen, Johan A. K. Suykens, Marc Proesmans, Tinne Tuytelaars, Luc Van Gool</author><pubDate>Wed, 05 Jul 2023 17:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02402v1</guid></item><item><title>Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection</title><link>http://arxiv.org/abs/2306.17431v2</link><description>Detecting the salient objects in a remote sensing image has wide applicationsfor the interdisciplinary research. Many existing deep learning methods havebeen proposed for Salient Object Detection (SOD) in remote sensing images andget remarkable results. However, the recent adversarial attack examples,generated by changing a few pixel values on the original remote sensing image,could result in a collapse for the well-trained deep learning based SOD model.Different with existing methods adding perturbation to original images, wepropose to jointly tune adversarial exposure and additive perturbation forattack and constrain image close to cloudy image as Adversarial Cloud. Cloud isnatural and common in remote sensing images, however, camouflaging cloud basedadversarial attack and defense for remote sensing images are not well studiedbefore. Furthermore, we design DefenseNet as a learn-able pre-processing to theadversarial cloudy images so as to preserve the performance of the deeplearning based remote sensing SOD model, without tuning the already deployeddeep SOD model. By considering both regular and generalized adversarialexamples, the proposed DefenseNet can defend the proposed Adversarial Cloud inwhite-box setting and other attack methods in black-box setting. Experimentalresults on a synthesized benchmark from the public remote sensing SOD dataset(EORSSD) show the promising defense against adversarial cloud attacks.</description><author>Huiming Sun, Lan Fu, Jinlong Li, Qing Guo, Zibo Meng, Tianyun Zhang, Yuewei Lin, Hongkai Yu</author><pubDate>Wed, 05 Jul 2023 17:15:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17431v2</guid></item><item><title>A Versatile Hub Model For Efficient Information Propagation And Feature Selection</title><link>http://arxiv.org/abs/2307.02398v1</link><description>Hub structure, characterized by a few highly interconnected nodes surroundedby a larger number of nodes with fewer connections, is a prominent topologicalfeature of biological brains, contributing to efficient information transferand cognitive processing across various species. In this paper, a mathematicalmodel of hub structure is presented. The proposed method is versatile and canbe broadly applied to both computational neuroscience and Recurrent NeuralNetworks (RNNs) research. We employ the Echo State Network (ESN) as a means toinvestigate the mechanistic underpinnings of hub structures. Our findingsdemonstrate a substantial enhancement in performance upon incorporating the hubstructure. Through comprehensive mechanistic analyses, we show that the hubstructure improves model performance by facilitating efficient informationprocessing and better feature extractions.</description><author>Zhaoze Wang, Junsong Wang</author><pubDate>Wed, 05 Jul 2023 17:14:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02398v1</guid></item><item><title>Lightweight Vision Transformer with Cross Feature Attention</title><link>http://arxiv.org/abs/2207.07268v2</link><description>Recent advances in vision transformers (ViTs) have achieved great performancein visual recognition tasks. Convolutional neural networks (CNNs) exploitspatial inductive bias to learn visual representations, but these networks arespatially local. ViTs can learn global representations with theirself-attention mechanism, but they are usually heavy-weight and unsuitable formobile devices. In this paper, we propose cross feature attention (XFA) tobring down computation cost for transformers, and combine efficient mobile CNNsto form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which canserve as a general-purpose backbone to learn both global and localrepresentation. Experimental results show that XFormer outperforms numerous CNNand ViT-based models across different tasks and datasets. On ImageNet1Kdataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters,which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT(ViT-based) for similar number of parameters. Our model also performs well whentransferring to object detection and semantic segmentation tasks. On MS COCOdataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 -&gt; 33.2 AP) in YOLOv3framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, withonly a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3,surpassing state-of-the-art lightweight segmentation networks.</description><author>Youpeng Zhao, Huadong Tang, Yingying Jiang, Yong A, Qiang Wu</author><pubDate>Wed, 05 Jul 2023 17:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.07268v2</guid></item><item><title>Won't Get Fooled Again: Answering Questions with False Premises</title><link>http://arxiv.org/abs/2307.02394v1</link><description>Pre-trained language models (PLMs) have shown unprecedented potential invarious fields, especially as the backbones for question-answering (QA)systems. However, they tend to be easily deceived by tricky questions such as"How many eyes does the sun have?". Such frailties of PLMs often allude to thelack of knowledge within them. In this paper, we find that the PLMs alreadypossess the knowledge required to rebut such questions, and the key is how toactivate the knowledge. To systematize this observation, we investigate thePLMs' responses to one kind of tricky questions, i.e., the false premisesquestions (FPQs). We annotate a FalseQA dataset containing 2365 human-writtenFPQs, with the corresponding explanations for the false premises and therevised true premise questions. Using FalseQA, we discover that PLMs arecapable of discriminating FPQs by fine-tuning on moderate numbers (e.g., 256)of examples. PLMs also generate reasonable explanations for the false premise,which serve as rebuttals. Further replaying a few general questions duringtraining allows PLMs to excel on FPQs and general questions simultaneously. Ourwork suggests that once the rebuttal ability is stimulated, knowledge insidethe PLMs can be effectively utilized to handle FPQs, which incentivizes theresearch on PLM-based QA systems.</description><author>Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, Maosong Sun</author><pubDate>Wed, 05 Jul 2023 17:09:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02394v1</guid></item><item><title>Learning Models of Adversarial Agent Behavior under Partial Observability</title><link>http://arxiv.org/abs/2306.11168v2</link><description>The need for opponent modeling and tracking arises in several real-worldscenarios, such as professional sports, video game design, and drug-traffickinginterdiction. In this work, we present Graph based Adversarial Modeling withMutal Information (GrAMMI) for modeling the behavior of an adversarial opponentagent. GrAMMI is a novel graph neural network (GNN) based approach that usesmutual information maximization as an auxiliary objective to predict thecurrent and future states of an adversarial opponent with partialobservability. To evaluate GrAMMI, we design two large-scale, pursuit-evasiondomains inspired by real-world scenarios, where a team of heterogeneous agentsis tasked with tracking and interdicting a single adversarial agent, and theadversarial agent must evade detection while achieving its own objectives. Withthe mutual information formulation, GrAMMI outperforms all baselines in bothdomains and achieves 31.68% higher log-likelihood on average for futureadversarial state predictions across both domains.</description><author>Sean Ye, Manisha Natarajan, Zixuan Wu, Rohan Paleja, Letian Chen, Matthew C. Gombolay</author><pubDate>Wed, 05 Jul 2023 17:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11168v2</guid></item><item><title>RADiff: Controllable Diffusion Models for Radio Astronomical Maps Generation</title><link>http://arxiv.org/abs/2307.02392v1</link><description>Along with the nearing completion of the Square Kilometre Array (SKA), comesan increasing demand for accurate and reliable automated solutions to extractvaluable information from the vast amount of data it will allow acquiring.Automated source finding is a particularly important task in this context, asit enables the detection and classification of astronomical objects.Deep-learning-based object detection and semantic segmentation models haveproven to be suitable for this purpose. However, training such deep networksrequires a high volume of labeled data, which is not trivial to obtain in thecontext of radio astronomy. Since data needs to be manually labeled by experts,this process is not scalable to large dataset sizes, limiting the possibilitiesof leveraging deep networks to address several tasks. In this work, we proposeRADiff, a generative approach based on conditional diffusion models trainedover an annotated radio dataset to generate synthetic images, containing radiosources of different morphologies, to augment existing datasets and reduce theproblems caused by class imbalances. We also show that it is possible togenerate fully-synthetic image-annotation pairs to automatically augment anyannotated dataset. We evaluate the effectiveness of this approach by training asemantic segmentation model on a real dataset augmented in two ways: 1) usingsynthetic images obtained from real masks, and 2) generating images fromsynthetic semantic masks. We show an improvement in performance when applyingaugmentation, gaining up to 18% in performance when using real masks and 4%when augmenting with synthetic masks. Finally, we employ this model to generatelarge-scale radio maps with the objective of simulating Data Challenges.</description><author>Renato Sortino, Thomas Cecconello, Andrea DeMarco, Giuseppe Fiameni, Andrea Pilzer, Andrew M. Hopkins, Daniel Magro, Simone Riggi, Eva Sciacca, Adriano Ingallinera, Cristobal Bordiu, Filomena Bufano, Concetto Spampinato</author><pubDate>Wed, 05 Jul 2023 17:04:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02392v1</guid></item><item><title>Causal Discovery with Language Models as Imperfect Experts</title><link>http://arxiv.org/abs/2307.02390v1</link><description>Understanding the causal relationships that underlie a system is afundamental prerequisite to accurate decision-making. In this work, we explorehow expert knowledge can be used to improve the data-driven identification ofcausal graphs, beyond Markov equivalence classes. In doing so, we consider asetting where we can query an expert about the orientation of causalrelationships between variables, but where the expert may provide erroneousinformation. We propose strategies for amending such expert knowledge based onconsistency properties, e.g., acyclicity and conditional independencies in theequivalence class. We then report a case study, on real data, where a largelanguage model is used as an imperfect expert.</description><author>Stephanie Long, Alexandre Piché, Valentina Zantedeschi, Tibor Schuster, Alexandre Drouin</author><pubDate>Wed, 05 Jul 2023 17:01:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02390v1</guid></item><item><title>Synthetic Data for Model Selection</title><link>http://arxiv.org/abs/2105.00717v2</link><description>Recent breakthroughs in synthetic data generation approaches made it possibleto produce highly photorealistic images which are hardly distinguishable fromreal ones. Furthermore, synthetic generation pipelines have the potential togenerate an unlimited number of images. The combination of high photorealismand scale turn synthetic data into a promising candidate for improving variousmachine learning (ML) pipelines. Thus far, a large body of research in thisfield has focused on using synthetic images for training, by augmenting andenlarging training data. In contrast to using synthetic data for training, inthis work we explore whether synthetic data can be beneficial for modelselection. Considering the task of image classification, we demonstrate thatwhen data is scarce, synthetic data can be used to replace the held outvalidation set, thus allowing to train on a larger dataset. We also introduce anovel method to calibrate the synthetic error estimation to fit that of thereal domain. We show that such calibration significantly improves theusefulness of synthetic data for model selection.</description><author>Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Matan Fintz, Gerard Medioni</author><pubDate>Wed, 05 Jul 2023 16:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2105.00717v2</guid></item><item><title>Multi-Task Learning with Summary Statistics</title><link>http://arxiv.org/abs/2307.02388v1</link><description>Multi-task learning has emerged as a powerful machine learning paradigm forintegrating data from multiple sources, leveraging similarities between tasksto improve overall model performance. However, the application of multi-tasklearning to real-world settings is hindered by data-sharing constraints,especially in healthcare settings. To address this challenge, we propose aflexible multi-task learning framework utilizing summary statistics fromvarious sources. Additionally, we present an adaptive parameter selectionapproach based on a variant of Lepski's method, allowing for data-driven tuningparameter selection when only summary statistics are available. Our systematicnon-asymptotic analysis characterizes the performance of the proposed methodsunder various regimes of the sample complexity and overlap. We demonstrate ourtheoretical findings and the performance of the method through extensivesimulations. This work offers a more flexible tool for training related modelsacross various domains, with practical implications in genetic risk predictionand many other fields.</description><author>Parker Knight, Rui Duan</author><pubDate>Wed, 05 Jul 2023 16:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02388v1</guid></item><item><title>Exploring Local Norms in Exp-concave Statistical Learning</title><link>http://arxiv.org/abs/2302.10726v2</link><description>We consider the problem of stochastic convex optimization with exp-concavelosses using Empirical Risk Minimization in a convex class. Answering aquestion raised in several prior works, we provide a $O( d / n + \log( 1 /\delta) / n )$ excess risk bound valid for a wide class of bounded exp-concavelosses, where $d$ is the dimension of the convex reference set, $n$ is thesample size, and $\delta$ is the confidence level. Our result is based on aunified geometric assumption on the gradient of losses and the notion of localnorms.</description><author>Nikita Puchkin, Nikita Zhivotovskiy</author><pubDate>Wed, 05 Jul 2023 16:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10726v2</guid></item><item><title>Machine learning at the mesoscale: a computation-dissipation bottleneck</title><link>http://arxiv.org/abs/2307.02379v1</link><description>The cost of information processing in physical systems calls for a trade-offbetween performance and energetic expenditure. Here we formulate and study acomputation-dissipation bottleneck in mesoscopic systems used as input-outputdevices. Using both real datasets and synthetic tasks, we show hownon-equilibrium leads to enhanced performance. Our framework sheds light on acrucial compromise between information compression, input-output computationand dynamic irreversibility induced by non-reciprocal interactions.</description><author>Alessandro Ingrosso, Emanuele Panizon</author><pubDate>Wed, 05 Jul 2023 16:46:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02379v1</guid></item><item><title>Continuum Limits of Ollivier's Ricci Curvature on data clouds: pointwise consistency and global lower bounds</title><link>http://arxiv.org/abs/2307.02378v1</link><description>Let $\mathcal{M} \subseteq \mathbb{R}^d$ denote a low-dimensional manifoldand let $\mathcal{X}= \{ x_1, \dots, x_n \}$ be a collection of pointsuniformly sampled from $\mathcal{M}$. We study the relationship between thecurvature of a random geometric graph built from $\mathcal{X}$ and thecurvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier'sdiscrete Ricci curvature. We prove pointwise, non-asymptotic consistencyresults and also show that if $\mathcal{M}$ has Ricci curvature bounded frombelow by a positive constant, then the random geometric graph will inherit thisglobal structural property with high probability. We discuss applications ofthe global discrete curvature bounds to contraction properties of heat kernelson graphs, as well as implications for manifold learning from data clouds. Inparticular, we show that the consistency results allow for characterizing theintrinsic curvature of a manifold from extrinsic curvature.</description><author>Nicolas Garcia Trillos, Melanie Weber</author><pubDate>Wed, 05 Jul 2023 16:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02378v1</guid></item><item><title>Causal Dependence Plots</title><link>http://arxiv.org/abs/2303.04209v2</link><description>Explaining artificial intelligence or machine learning models is increasinglyimportant. To use such data-driven systems wisely we must understand how theyinteract with the world, including how they depend causally on data inputs. Inthis work we develop Causal Dependence Plots (CDPs) to visualize how onevariable--an outcome--depends on changes in another variable--apredictor--$\textit{along with any consequent causal changes in other predictorvariables}$. Crucially, CDPs differ from standard methods based on holdingother predictors constant or assuming they are independent. CDPs make use of anauxiliary causal model because causal conclusions require causal assumptions.With simulations and real data experiments, we show CDPs can be combined in amodular way with methods for causal learning or sensitivity analysis. Sincepeople often think causally about input-output dependence, CDPs can be powerfultools in the xAI or interpretable machine learning toolkit and contribute toapplications like scientific machine learning and algorithmic fairness.</description><author>Joshua R. Loftus, Lucius E. J. Bynum, Sakina Hansen</author><pubDate>Wed, 05 Jul 2023 16:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04209v2</guid></item><item><title>Distance Preserving Machine Learning for Uncertainty Aware Accelerator Capacitance Predictions</title><link>http://arxiv.org/abs/2307.02367v1</link><description>Providing accurate uncertainty estimations is essential for producingreliable machine learning models, especially in safety-critical applicationssuch as accelerator systems. Gaussian process models are generally regarded asthe gold standard method for this task, but they can struggle with large,high-dimensional datasets. Combining deep neural networks with Gaussian processapproximation techniques have shown promising results, but dimensionalityreduction through standard deep neural network layers is not guaranteed tomaintain the distance information necessary for Gaussian process models. Webuild on previous work by comparing the use of the singular value decompositionagainst a spectral-normalized dense layer as a feature extractor for a deepneural Gaussian process approximation model and apply it to a capacitanceprediction problem for the High Voltage Converter Modulators in the Oak RidgeSpallation Neutron Source. Our model shows improved distance preservation andpredicts in-distribution capacitance values with less than 1% error.</description><author>Steven Goldenberg, Malachi Schram, Kishansingh Rajput, Thomas Britton, Chris Pappas, Dan Lu, Jared Walden, Majdi I. Radaideh, Sarah Cousineau, Sudarshan Harave</author><pubDate>Wed, 05 Jul 2023 16:32:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02367v1</guid></item><item><title>TransRUPNet for Improved Out-of-Distribution Generalization in Polyp Segmentation</title><link>http://arxiv.org/abs/2306.02176v2</link><description>Out-of-distribution (OOD) generalization is a critical challenge in deeplearning. It is specifically important when the test samples are drawn from adifferent distribution than the training data. We develop a novel real-timedeep learning based architecture, TransRUPNet that is based on a Transformerand residual upsampling network for colorectal polyp segmentation to improveOOD generalization. The proposed architecture, TransRUPNet, is anencoder-decoder network that consists of three encoder blocks, three decoderblocks, and some additional upsampling blocks at the end of the network. Withthe image size of $256\times256$, the proposed method achieves an excellentreal-time operation speed of \textbf{47.07} frames per second with an averagemean dice coefficient score of 0.7786 and mean Intersection over Union of0.7210 on the out-of-distribution polyp datasets. The results on the publiclyavailable PolypGen dataset (OOD dataset in our case) suggest that TransRUPNetcan give real-time feedback while retaining high accuracy for in-distributiondataset. Furthermore, we demonstrate the generalizability of the proposedmethod by showing that it significantly improves performance on OOD datasetscompared to the existing methods.</description><author>Debesh Jha, Nikhil Kumar Tomar, Debayan Bhattacharya, Ulas Bagci</author><pubDate>Wed, 05 Jul 2023 16:29:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02176v2</guid></item><item><title>Broadcasted Residual Learning for Efficient Keyword Spotting</title><link>http://arxiv.org/abs/2106.04140v4</link><description>Keyword spotting is an important research field because it plays a key rolein device wake-up and user interaction on smart devices. However, it ischallenging to minimize errors while operating efficiently in devices withlimited resources such as mobile phones. We present a broadcasted residuallearning method to achieve high accuracy with small model size andcomputational load. Our method configures most of the residual functions as 1Dtemporal convolution while still allows 2D convolution together using abroadcasted-residual connection that expands temporal output tofrequency-temporal dimension. This residual mapping enables the network toeffectively represent useful audio features with much less computation thanconventional convolutional neural networks. We also propose a novel networkarchitecture, Broadcasting-residual network (BC-ResNet), based on broadcastedresidual learning and describe how to scale up the model according to thetarget device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%top-1 accuracy on Google speech command datasets v1 and v2, respectively, andconsistently outperform previous approaches, using fewer computations andparameters. Code is available athttps://github.com/Qualcomm-AI-research/bcresnet.</description><author>Byeonggeun Kim, Simyung Chang, Jinkyu Lee, Dooyong Sung</author><pubDate>Wed, 05 Jul 2023 16:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.04140v4</guid></item><item><title>To be or not to be: a translation reception study of a literary text translated into Dutch and Catalan using machine translation</title><link>http://arxiv.org/abs/2307.02358v1</link><description>This article presents the results of a study involving the reception of afictional story by Kurt Vonnegut translated from English into Catalan and Dutchin three conditions: machine-translated (MT), post-edited (PE) and translatedfrom scratch (HT). 223 participants were recruited who rated the readingconditions using three scales: Narrative Engagement, Enjoyment and TranslationReception. The results show that HT presented a higher engagement, enjoymentand translation reception in Catalan if compared to PE and MT. However, theDutch readers show higher scores in PE than in both HT and MT, and the highestengagement and enjoyments scores are reported when reading the original Englishversion. We hypothesize that when reading a fictional story in translation, notonly the condition and the quality of the translations is key to understand itsreception, but also the participants reading patterns, reading language, and,perhaps language status in their own societies.</description><author>Ana Guerberof Arenas, Antonio Toral</author><pubDate>Wed, 05 Jul 2023 16:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02358v1</guid></item><item><title>Decentralized Data Governance as Part of a Data Mesh Platform: Concepts and Approaches</title><link>http://arxiv.org/abs/2307.02357v1</link><description>Data mesh is a socio-technical approach to decentralized analytics datamanagement. To manage this decentralization efficiently, data mesh relies onautomation provided by a self-service data infrastructure platform. A keyaspect of this platform is to enable decentralized data governance. Becausedata mesh is a young approach, there is a lack of coherence in how data meshconcepts are interpreted in the industry, and almost no work on how a data meshplatform facilitates governance. This paper presents a conceptual model of keydata mesh concepts and discusses different approaches to drive governancethrough platform means. The insights presented are drawn from concreteexperiences of implementing a fully-functional data mesh platform that can beused as a reference on how to approach data mesh platform development.</description><author>Arif Wider, Sumedha Verma, Atif Akhtar</author><pubDate>Wed, 05 Jul 2023 16:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02357v1</guid></item><item><title>Differentiable Point-Based Radiance Fields for Efficient View Synthesis</title><link>http://arxiv.org/abs/2205.14330v4</link><description>We propose a differentiable rendering algorithm for efficient novel viewsynthesis. By departing from volume-based representations in favor of a learnedpoint representation, we improve on existing methods more than an order ofmagnitude in memory and runtime, both in training and inference. The methodbegins with a uniformly-sampled random point cloud and learns per-pointposition and view-dependent appearance, using a differentiable splat-basedrenderer to evolve the model to match a set of input images. Our method is upto 300x faster than NeRF in both training and inference, with only a marginalsacrifice in quality, while using less than 10~MB of memory for a static scene.For dynamic scenes, our method trains two orders of magnitude faster thanSTNeRF and renders at near interactive rate, while maintaining high imagequality and temporal coherence even without imposing any temporal-coherencyregularizers.</description><author>Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, Felix Heide</author><pubDate>Wed, 05 Jul 2023 16:17:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.14330v4</guid></item><item><title>Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality</title><link>http://arxiv.org/abs/2307.02347v1</link><description>Diffusion models recently have been successfully applied for the visualsynthesis of strikingly realistic appearing images. This raises strong concernsabout their potential for malicious purposes. In this paper, we propose usingthe lightweight multi Local Intrinsic Dimensionality (multiLID), which has beenoriginally developed in context of the detection of adversarial examples, forthe automatic detection of synthetic images and the identification of theaccording generator networks. In contrast to many existing detectionapproaches, which often only work for GAN-generated images, the proposed methodprovides close to perfect detection results in many realistic use cases.Extensive experiments on known and newly created datasets demonstrate thatmultiLID exhibits superiority in diffusion detection and model identification.Since the empirical evaluations of recent publications on the detection ofgenerated images is often too focused on the "LSUN-Bedroom" dataset, we furtherestablish a comprehensive benchmark for the detection of diffusion-generatedimages, including samples from several diffusion models with different imagesizes to evaluate the performance of their multiLID. Code for our experiments is provided athttps://github.com/deepfake-study/deepfake_multiLID.</description><author>Peter Lorenz, Ricard Durall, Janis Keuper</author><pubDate>Wed, 05 Jul 2023 16:03:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02347v1</guid></item><item><title>LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning</title><link>http://arxiv.org/abs/2307.02345v1</link><description>Currently, research on Reinforcement learning (RL) can be broadly classifiedinto two categories: online RL and offline RL. Both in online and offline RL,the primary focus of research on the Bellman error lies in the optimizationtechniques and performance improvement, rather than exploring the inherentstructural properties of the Bellman error, such as distributioncharacteristics. In this study, we analyze the distribution of the Bellmanapproximation error in both online and offline settings. We find that in theonline environment, the Bellman error follows a Logistic distribution, while inthe offline environment, the Bellman error follows a constrained Logisticdistribution, where the constrained distribution is dependent on the priorpolicy in the offline data set. Based on this finding, we have improved theMSELoss which is based on the assumption that the Bellman errors follow anormal distribution, and we utilized the Logistic maximum likelihood functionto construct $\rm LLoss$ as an alternative loss function. In addition, weobserved that the rewards in the offline data set should follow a specificdistribution, which would facilitate the achievement of offline objectives. Inour numerical experiments, we performed controlled variable corrections on theloss functions of two variants of Soft-Actor-Critic in both online and offlineenvironments. The results confirmed our hypothesis regarding the online andoffline settings, we also found that the variance of LLoss is smaller thanMSELoss. Our research provides valuable insights for further investigationsbased on the distribution of Bellman errors.</description><author>Outongyi Lv, Bingxin Zhou, Yu Guang Wang</author><pubDate>Wed, 05 Jul 2023 16:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02345v1</guid></item><item><title>MuLMS-AZ: An Argumentative Zoning Dataset for the Materials Science Domain</title><link>http://arxiv.org/abs/2307.02340v1</link><description>Scientific publications follow conventionalized rhetorical structures.Classifying the Argumentative Zone (AZ), e.g., identifying whether a sentencestates a Motivation, a Result or Background information, has been proposed toimprove processing of scholarly documents. In this work, we adapt and extendthis idea to the domain of materials science research. We present and release anew dataset of 50 manually annotated research articles. The dataset spans sevensub-topics and is annotated with a materials-science focused multi-labelannotation scheme for AZ. We detail corpus statistics and demonstrate highinter-annotator agreement. Our computational experiments show that usingdomain-specific pre-trained transformer-based text encoders is key to highclassification performance. We also find that AZ categories from existingdatasets in other domains are transferable to varying degrees.</description><author>Timo Pierre Schrader, Teresa Bürkle, Sophie Henning, Sherry Tan, Matteo Finco, Stefan Grünewald, Maira Indrikova, Felix Hildebrand, Annemarie Friedrich</author><pubDate>Wed, 05 Jul 2023 15:55:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02340v1</guid></item><item><title>SAITS: Self-Attention-based Imputation for Time Series</title><link>http://arxiv.org/abs/2202.08516v5</link><description>Missing data in time series is a pervasive problem that puts obstacles in theway of advanced analysis. A popular solution is imputation, where thefundamental challenge is to determine what values should be filled in. Thispaper proposes SAITS, a novel method based on the self-attention mechanism formissing value imputation in multivariate time series. Trained by ajoint-optimization approach, SAITS learns missing values from a weightedcombination of two diagonally-masked self-attention (DMSA) blocks. DMSAexplicitly captures both the temporal dependencies and feature correlationsbetween time steps, which improves imputation accuracy and training speed.Meanwhile, the weighted-combination design enables SAITS to dynamically assignweights to the learned representations from two DMSA blocks according to theattention map and the missingness information. Extensive experimentsquantitatively and qualitatively demonstrate that SAITS outperforms thestate-of-the-art methods on the time-series imputation task efficiently andreveal SAITS' potential to improve the learning performance of patternrecognition models on incomplete time-series data from the real world. The codeis open source on GitHub at https://github.com/WenjieDu/SAITS.</description><author>Wenjie Du, David Cote, Yan Liu</author><pubDate>Wed, 05 Jul 2023 15:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.08516v5</guid></item><item><title>GAFAR: Graph-Attention Feature-Augmentation for Registration A Fast and Light-weight Point Set Registration Algorithm</title><link>http://arxiv.org/abs/2307.02339v1</link><description>Rigid registration of point clouds is a fundamental problem in computervision with many applications from 3D scene reconstruction to geometry captureand robotics. If a suitable initial registration is available, conventionalmethods like ICP and its many variants can provide adequate solutions. Inabsence of a suitable initialization and in the presence of a high outlier rateor in the case of small overlap though the task of rigid registration stillpresents great challenges. The advent of deep learning in computer vision hasbrought new drive to research on this topic, since it provides the possibilityto learn expressive feature-representations and provide one-shot estimatesinstead of depending on time-consuming iterations of conventional robustmethods. Yet, the rotation and permutation invariant nature of point cloudsposes its own challenges to deep learning, resulting in loss of performance andlow generalization capability due to sensitivity to outliers andcharacteristics of 3D scans not present during network training. In this work,we present a novel fast and light-weight network architecture using theattention mechanism to augment point descriptors at inference time to optimallysuit the registration task of the specific point clouds it is presented with.Employing a fully-connected graph both within and between point clouds lets thenetwork reason about the importance and reliability of points for registration,making our approach robust to outliers, low overlap and unseen data. We testthe performance of our registration algorithm on different registration andgeneralization tasks and provide information on runtime and resourceconsumption. The code and trained weights are available athttps://github.com/mordecaimalignatius/GAFAR/.</description><author>Ludwig Mohr, Ismail Geles, Friedrich Fraundorfer</author><pubDate>Wed, 05 Jul 2023 15:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02339v1</guid></item><item><title>PlantDet: A benchmark for Plant Detection in the Three-Rivers-Source Region</title><link>http://arxiv.org/abs/2304.04963v2</link><description>The Three-River-Source region is a highly significant natural reserve inChina that harbors a plethora of untamed botanical resources. To meet thepractical requirements of botanical research and intelligent plant management,we construct a large-scale dataset for Plant detection in theThree-River-Source region (PTRS). This dataset comprises 6965 high-resolutionimages of 2160*3840 pixels, captured by diverse sensors and platforms, andfeaturing objects of varying shapes and sizes. Subsequently, a team ofbotanical image interpretation experts annotated these images with 21 commonlyoccurring object categories. The fully annotated PTRS images contain 122, 300instances of plant leaves, each labeled by a horizontal rectangle. The PTRSpresents us with challenges such as dense occlusion, varying leaf resolutions,and high feature similarity among plants, prompting us to develop a novelobject detection network named PlantDet. This network employs a window-basedefficient self-attention module (ST block) to generate robust featurerepresentation at multiple scales, improving the detection efficiency for smalland densely-occluded objects. Our experimental results validate the efficacy ofour proposed plant detection benchmark, with a precision of 88.1%, a meanaverage precision (mAP) of 77.6%, and a higher recall compared to the baseline.Additionally, our method effectively overcomes the issue of missing smallobjects. We intend to share our data and code with interested parties toadvance further research in this field.</description><author>Huanhuan Li, Xuechao Zou, Yu-an Zhang, Jiangcai Zhaba, Guomei Li, Lamao Yongga</author><pubDate>Wed, 05 Jul 2023 15:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04963v2</guid></item><item><title>FAM: Relative Flatness Aware Minimization</title><link>http://arxiv.org/abs/2307.02337v1</link><description>Flatness of the loss curve around a model at hand has been shown toempirically correlate with its generalization ability. Optimizing for flatnesshas been proposed as early as 1994 by Hochreiter and Schmidthuber, and wasfollowed by more recent successful sharpness-aware optimization techniques.Their widespread adoption in practice, though, is dubious because of the lackof theoretically grounded connection between flatness and generalization, inparticular in light of the reparameterization curse - certainreparameterizations of a neural network change most flatness measures but donot change generalization. Recent theoretical work suggests that a particularrelative flatness measure can be connected to generalization and solves thereparameterization curse. In this paper, we derive a regularizer based on thisrelative flatness that is easy to compute, fast, efficient, and works witharbitrary loss functions. It requires computing the Hessian only of a singlelayer of the network, which makes it applicable to large neural networks, andwith it avoids an expensive mapping of the loss surface in the vicinity of themodel. In an extensive empirical evaluation we show that this relative flatnessaware minimization (FAM) improves generalization in a multitude of applicationsand models, both in finetuning and standard training. We make the codeavailable at github.</description><author>Linara Adilova, Amr Abourayya, Jianning Li, Amin Dada, Henning Petzka, Jan Egger, Jens Kleesiek, Michael Kamp</author><pubDate>Wed, 05 Jul 2023 15:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02337v1</guid></item><item><title>GPT-FL: Generative Pre-trained Model-Assisted Federated Learning</title><link>http://arxiv.org/abs/2306.02210v2</link><description>In this work, we propose GPT-FL, a generative pre-trained model-assistedfederated learning (FL) framework. At its core, GPT-FL leverages generativepre-trained models to generate diversified synthetic data. These generated dataare used to train a downstream model on the server, which is then fine-tunedwith private client data under the standard FL framework. We show that GPT-FLconsistently outperforms state-of-the-art FL methods in terms of model testaccuracy, communication efficiency, and client sampling efficiency. Throughcomprehensive ablation analysis, we discover that the downstream modelgenerated by synthetic data plays a crucial role in controlling the directionof gradient diversity during FL training, which enhances convergence speed andcontributes to the notable accuracy boost observed with GPT-FL. Also,regardless of whether the target data falls within or outside the domain of thepre-trained generative model, GPT-FL consistently achieves significantperformance gains, surpassing the results obtained by models trained solelywith FL or synthetic data.</description><author>Tuo Zhang, Tiantian Feng, Samiul Alam, Dimitrios Dimitriadis, Mi Zhang, Shrikanth S. Narayanan, Salman Avestimehr</author><pubDate>Wed, 05 Jul 2023 15:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02210v2</guid></item><item><title>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</title><link>http://arxiv.org/abs/2307.02334v1</link><description>Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging(MRI) images from partial measurement is essential to medical imaging research.Benefiting from the diverse and complementary information of multi-contrast MRimages in different imaging modalities, multi-contrast Super-Resolution (SR)reconstruction is promising to yield SR images with higher quality. In themedical scenario, to fully visualize the lesion, radiologists are accustomed tozooming the MR images at arbitrary scales rather than using a fixed scale, asused by most MRI SR methods. In addition, existing multi-contrast MRI SRmethods often require a fixed resolution for the reference image, which makesacquiring reference images difficult and imposes limitations on arbitrary scaleSR tasks. To address these issues, we proposed an implicit neuralrepresentations based dual-arbitrary multi-contrast MRI super-resolutionmethod, called Dual-ArbNet. First, we decouple the resolution of the target andreference images by a feature encoder, enabling the network to input target andreference images at arbitrary scales. Then, an implicit fusion decoder fusesthe multi-contrast features and uses an Implicit Decoding Function~(IDF) toobtain the final MRI SR results. Furthermore, we introduce a curriculumlearning strategy to train our network, which improves the generalization andperformance of our Dual-ArbNet. Extensive experiments in two public MRIdatasets demonstrate that our method outperforms state-of-the-art approachesunder different scale factors and has great potential in clinical practice.</description><author>Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, Yapeng Tian</author><pubDate>Wed, 05 Jul 2023 15:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02334v1</guid></item><item><title>Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements</title><link>http://arxiv.org/abs/2307.02329v1</link><description>The advent of novel 5G services and applications with binding latencyrequirements and guaranteed Quality of Service (QoS) hastened the need toincorporate autonomous and proactive decision-making in network managementprocedures. The objective of our study is to provide a thorough analysis ofpredictive latency within 5G networks by utilizing real-world network data thatis accessible to mobile network operators (MNOs). In particular, (i) we presentan analytical formulation of the user-plane latency as a Hypoexponentialdistribution, which is validated by means of a comparative analysis withempirical measurements, and (ii) we conduct experimental results ofprobabilistic regression, anomaly detection, and predictive forecastingleveraging on emerging domains in Machine Learning (ML), such as BayesianLearning (BL) and Machine Learning on Graphs (GML). We test our predictiveframework using data gathered from scenarios of vehicular mobility, dense-urbantraffic, and social gathering events. Our results provide valuable insightsinto the efficacy of predictive algorithms in practical applications.</description><author>Marco Skocaj, Francesca Conserva, Nicol Sarcone Grande, Andrea Orsi, Davide Micheli, Giorgio Ghinamo, Simone Bizzarri, Roberto Verdone</author><pubDate>Wed, 05 Jul 2023 15:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02329v1</guid></item><item><title>A Quantitative Functional Central Limit Theorem for Shallow Neural Networks</title><link>http://arxiv.org/abs/2306.16932v2</link><description>We prove a Quantitative Functional Central Limit Theorem for one-hidden-layerneural networks with generic activation function. The rates of convergence thatwe establish depend heavily on the smoothness of the activation function, andthey range from logarithmic in non-differentiable cases such as the Relu to$\sqrt{n}$ for very regular activations. Our main tools are functional versionsof the Stein-Malliavin approach; in particular, we exploit heavily aquantitative functional central limit theorem which has been recentlyestablished by Bourguin and Campese (2020).</description><author>Valentina Cammarota, Domenico Marinucci, Michele Salvi, Stefano Vigogna</author><pubDate>Wed, 05 Jul 2023 15:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16932v2</guid></item><item><title>MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers</title><link>http://arxiv.org/abs/2307.02321v1</link><description>The input tokens to Vision Transformers carry little semantic meaning as theyare defined as regular equal-sized patches of the input image, regardless ofits content. However, processing uniform background areas of an image shouldnot necessitate as much compute as dense, cluttered areas. To address thisissue, we propose a dynamic mixed-scale tokenization scheme for ViT, MSViT. Ourmethod introduces a conditional gating mechanism that selects the optimal tokenscale for every image region, such that the number of tokens is dynamicallydetermined per input. The proposed gating module is lightweight, agnostic tothe choice of transformer backbone, and trained within a few epochs (e.g., 20epochs on ImageNet) with little training overhead. In addition, to enhance theconditional behavior of the gate during training, we introduce a novelgeneralization of the batch-shaping loss. We show that our gating module isable to learn meaningful semantics despite operating locally at the coarsepatch-level. We validate MSViT on the tasks of classification and segmentationwhere it leads to improved accuracy-complexity trade-off.</description><author>Jakob Drachmann Havtorn, Amelie Royer, Tijmen Blankevoort, Babak Ehteshami Bejnordi</author><pubDate>Wed, 05 Jul 2023 15:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02321v1</guid></item><item><title>Algorithms, Incentives, and Democracy</title><link>http://arxiv.org/abs/2307.02319v1</link><description>Classification algorithms are increasingly used in areas such as housing,credit, and law enforcement in order to make decisions affecting peoples'lives. These algorithms can change individual behavior deliberately (a fraudprediction algorithm deterring fraud) or inadvertently (content sortingalgorithms spreading misinformation), and they are increasingly facing publicscrutiny and regulation. Some of these regulations, like the elimination ofcash bail in some states, have focused on \textit{lowering the stakes ofcertain classifications}. In this paper we characterize how optimalclassification by an algorithm designer can affect the distribution of behaviorin a population -- sometimes in surprising ways. We then look at the effect ofdemocratizing the rewards and punishments, or stakes, to algorithmicclassification to consider how a society can potentially stem (or facilitate!)predatory classification. Our results speak to questions of algorithmicfairness in settings where behavior and algorithms are interdependent, andwhere typical measures of fairness focusing on statistical accuracy acrossgroups may not be appropriate.</description><author>Elizabeth Maggie Penn, John W. Patty</author><pubDate>Wed, 05 Jul 2023 15:22:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02319v1</guid></item><item><title>Deep Contract Design via Discontinuous Piecewise Affine Neural Networks</title><link>http://arxiv.org/abs/2307.02318v1</link><description>Contract design involves a principal who establishes contractual agreementsabout payments for outcomes that arise from the actions of an agent. In thispaper, we initiate the study of deep learning for the automated design ofoptimal contracts. We formulate this as an offline learning problem, where adeep network is used to represent the principal's expected utility as afunction of the design of a contract. We introduce a novel representation: theDiscontinuous ReLU (DeLU) network, which models the principal's utility as adiscontinuous piecewise affine function where each piece corresponds to theagent taking a particular action. DeLU networks implicitly learn closed-formexpressions for the incentive compatibility constraints of the agent and theutility maximization objective of the principal, and support parallel inferenceon each piece through linear programming or interior-point methods that solvefor optimal contracts. We provide empirical results that demonstrate success inapproximating the principal's utility function with a small number of trainingsamples and scaling to find approximately optimal contracts on problems with alarge number of actions and outcomes.</description><author>Tonghan Wang, Paul Dütting, Dmitry Ivanov, Inbal Talgam-Cohen, David C. Parkes</author><pubDate>Wed, 05 Jul 2023 15:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02318v1</guid></item><item><title>The Curse of Passive Data Collection in Batch Reinforcement Learning</title><link>http://arxiv.org/abs/2106.09973v3</link><description>In high stake applications, active experimentation may be considered toorisky and thus data are often collected passively. While in simple cases, suchas in bandits, passive and active data collection are similarly effective, theprice of passive sampling can be much higher when collecting data from a systemwith controlled states. The main focus of the current paper is thecharacterization of this price. For example, when learning in episodic finitestate-action Markov decision processes (MDPs) with $\mathrm{S}$ states and$\mathrm{A}$ actions, we show that even with the best (but passively chosen)logging policy, $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$episodes are necessary (and sufficient) to obtain an $\epsilon$-optimal policy,where $H$ is the length of episodes. Note that this shows that the samplecomplexity blows up exponentially compared to the case of active datacollection, a result which is not unexpected, but, as far as we know, have notbeen published beforehand and perhaps the form of the exact expression is alittle surprising. We also extend these results in various directions, such asother criteria or learning in the presence of function approximation, withsimilar conclusions. A remarkable feature of our result is the sharpcharacterization of the exponent that appears, which is critical forunderstanding what makes passive learning hard.</description><author>Chenjun Xiao, Ilbin Lee, Bo Dai, Dale Schuurmans, Csaba Szepesvari</author><pubDate>Wed, 05 Jul 2023 15:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.09973v3</guid></item><item><title>Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social Media</title><link>http://arxiv.org/abs/2307.02313v1</link><description>In this work, we present the contribution of the BLUE team in the eRisk Labtask on searching for symptoms of depression. The task consists of retrievingand ranking Reddit social media sentences that convey symptoms of depressionfrom the BDI-II questionnaire. Given that synthetic data provided by LLMs havebeen proven to be a reliable method for augmenting data and fine-tuningdownstream models, we chose to generate synthetic data using ChatGPT for eachof the symptoms of the BDI-II questionnaire. We designed a prompt such that thegenerated data contains more richness and semantic diversity than the BDI-IIresponses for each question and, at the same time, contains emotional andanecdotal experiences that are specific to the more intimate way of sharingexperiences on Reddit. We perform semantic search and rank the sentences'relevance to the BDI-II symptoms by cosine similarity. We used twostate-of-the-art transformer-based models for embedding the social media posts,the original and generated responses of the BDI-II, MentalRoBERTa and a variantof MPNet. Our results show that an approach using for sentence embeddings amodel that is designed for semantic search outperforms the model pre-trained onmental health data. Furthermore, the generated synthetic data were proved toospecific for this task, the approach simply relying on the BDI-II responses hadthe best performance.</description><author>Ana-Maria Bucur</author><pubDate>Wed, 05 Jul 2023 15:15:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02313v1</guid></item><item><title>Multi-Scale Prototypical Transformer for Whole Slide Image Classification</title><link>http://arxiv.org/abs/2307.02308v1</link><description>Whole slide image (WSI) classification is an essential task in computationalpathology. Despite the recent advances in multiple instance learning (MIL) forWSI classification, accurate classification of WSIs remains challenging due tothe extreme imbalance between the positive and negative instances in bags, andthe complicated pre-processing to fuse multi-scale information of WSI. To thisend, we propose a novel multi-scale prototypical Transformer (MSPT) for WSIclassification, which includes a prototypical Transformer (PT) module and amulti-scale feature fusion module (MFFM). The PT is developed to reduceredundant instances in bags by integrating prototypical learning into theTransformer architecture. It substitutes all instances with cluster prototypes,which are then re-calibrated through the self-attention mechanism of theTrans-former. Thereafter, an MFFM is proposed to fuse the clustered prototypesof different scales, which employs MLP-Mixer to enhance the informationcommunication between prototypes. The experimental results on two public WSIdatasets demonstrate that the proposed MSPT outperforms all the comparedalgorithms, suggesting its potential applications.</description><author>Saisai Ding, Jun Wang, Juncheng Li, Jun Shi</author><pubDate>Wed, 05 Jul 2023 15:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02308v1</guid></item><item><title>The Potential of Visual ChatGPT For Remote Sensing</title><link>http://arxiv.org/abs/2304.13009v2</link><description>Recent advancements in Natural Language Processing (NLP), particularly inLarge Language Models (LLMs), associated with deep learning-based computervision techniques, have shown substantial potential for automating a variety oftasks. One notable model is Visual ChatGPT, which combines ChatGPT's LLMcapabilities with visual computation to enable effective image analysis. Themodel's ability to process images based on textual inputs can revolutionizediverse fields. However, its application in the remote sensing domain remainsunexplored. This is the first paper to examine the potential of Visual ChatGPT,a cutting-edge LLM founded on the GPT architecture, to tackle the aspects ofimage processing related to the remote sensing domain. Among its currentcapabilities, Visual ChatGPT can generate textual descriptions of images,perform canny edge and straight line detection, and conduct image segmentation.These offer valuable insights into image content and facilitate theinterpretation and extraction of information. By exploring the applicability ofthese techniques within publicly available datasets of satellite images, wedemonstrate the current model's limitations in dealing with remote sensingimages, highlighting its challenges and future prospects. Although still inearly development, we believe that the combination of LLMs and visual modelsholds a significant potential to transform remote sensing image processing,creating accessible and practical application opportunities in the field.</description><author>Lucas Prado Osco, Eduardo Lopes de Lemos, Wesley Nunes Gonçalves, Ana Paula Marques Ramos, José Marcato Junior</author><pubDate>Wed, 05 Jul 2023 15:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13009v2</guid></item><item><title>Sumformer: Universal Approximation for Efficient Transformers</title><link>http://arxiv.org/abs/2307.02301v1</link><description>Natural language processing (NLP) made an impressive jump with theintroduction of Transformers. ChatGPT is one of the most famous examples,changing the perception of the possibilities of AI even outside the researchcommunity. However, besides the impressive performance, the quadratic time andspace complexity of Transformers with respect to sequence length posesignificant limitations for handling long sequences. While efficientTransformer architectures like Linformer and Performer with linear complexityhave emerged as promising solutions, their theoretical understanding remainslimited. In this paper, we introduce Sumformer, a novel and simple architecturecapable of universally approximating equivariant sequence-to-sequencefunctions. We use Sumformer to give the first universal approximation resultsfor Linformer and Performer. Moreover, we derive a new proof for Transformers,showing that just one attention layer is sufficient for universalapproximation.</description><author>Silas Alberti, Niclas Dern, Laura Thesing, Gitta Kutyniok</author><pubDate>Wed, 05 Jul 2023 14:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02301v1</guid></item><item><title>Improving Address Matching using Siamese Transformer Networks</title><link>http://arxiv.org/abs/2307.02300v1</link><description>Matching addresses is a critical task for companies and post offices involvedin the processing and delivery of packages. The ramifications of incorrectlydelivering a package to the wrong recipient are numerous, ranging from harm tothe company's reputation to economic and environmental costs. This researchintroduces a deep learning-based model designed to increase the efficiency ofaddress matching for Portuguese addresses. The model comprises two parts: (i) abi-encoder, which is fine-tuned to create meaningful embeddings of Portuguesepostal addresses, utilized to retrieve the top 10 likely matches of theun-normalized target address from a normalized database, and (ii) across-encoder, which is fine-tuned to accurately rerank the 10 addressesobtained by the bi-encoder. The model has been tested on a real-case scenarioof Portuguese addresses and exhibits a high degree of accuracy, exceeding 95%at the door level. When utilized with GPU computations, the inference speed isabout 4.5 times quicker than other traditional approaches such as BM25. Animplementation of this system in a real-world scenario would substantiallyincrease the effectiveness of the distribution process. Such an implementationis currently under investigation.</description><author>André V. Duarte, Arlindo L. Oliveira</author><pubDate>Wed, 05 Jul 2023 14:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02300v1</guid></item><item><title>Statistical Comparisons of Classifiers by Generalized Stochastic Dominance</title><link>http://arxiv.org/abs/2209.01857v2</link><description>Although being a crucial question for the development of machine learningalgorithms, there is still no consensus on how to compare classifiers overmultiple data sets with respect to several criteria. Every comparison frameworkis confronted with (at least) three fundamental challenges: the multiplicity ofquality criteria, the multiplicity of data sets and the randomness of theselection of data sets. In this paper, we add a fresh view to the vivid debateby adopting recent developments in decision theory. Based on so-calledpreference systems, our framework ranks classifiers by a generalized concept ofstochastic dominance, which powerfully circumvents the cumbersome, and ofteneven self-contradictory, reliance on aggregates. Moreover, we show thatgeneralized stochastic dominance can be operationalized by solvingeasy-to-handle linear programs and moreover statistically tested employing anadapted two-sample observation-randomization test. This yields indeed apowerful framework for the statistical comparison of classifiers over multipledata sets with respect to multiple quality criteria simultaneously. Weillustrate and investigate our framework in a simulation study and with a setof standard benchmark data sets.</description><author>Christoph Jansen, Malte Nalenz, Georg Schollmeyer, Thomas Augustin</author><pubDate>Wed, 05 Jul 2023 14:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01857v2</guid></item><item><title>Neural Mixed Effects for Nonlinear Personalized Predictions</title><link>http://arxiv.org/abs/2306.08149v2</link><description>Personalized prediction is a machine learning approach that predicts aperson's future observations based on their past labeled observations and istypically used for sequential tasks, e.g., to predict daily mood ratings. Whenmaking personalized predictions, a model can combine two types of trends: (a)trends shared across people, i.e., person-generic trends, such as being happieron weekends, and (b) unique trends for each person, i.e., person-specifictrends, such as a stressful weekly meeting. Mixed effect models are popularstatistical models to study both trends by combining person-generic andperson-specific parameters. Though linear mixed effect models are gainingpopularity in machine learning by integrating them with neural networks, theseintegrations are currently limited to linear person-specific parameters: rulingout nonlinear person-specific trends. In this paper, we propose Neural MixedEffect (NME) models to optimize nonlinear person-specific parameters anywherein a neural network in a scalable manner. NME combines the efficiency of neuralnetwork optimization with nonlinear mixed effects modeling. Empirically, weobserve that NME improves performance across six unimodal and multimodaldatasets, including a smartphone dataset to predict daily mood and amother-adolescent dataset to predict affective state sequences where half themothers experience at least moderate symptoms of depression. Furthermore, weevaluate NME for two model architectures, including for neural conditionalrandom fields (CRF) to predict affective state sequences where the CRF learnsnonlinear person-specific temporal transitions between affective states.Analysis of these person-specific transitions on the mother-adolescent datasetshows interpretable trends related to the mother's depression symptoms.</description><author>Torsten Wörtwein, Nicholas Allen, Lisa B. Sheeber, Randy P. Auerbach, Jeffrey F. Cohn, Louis-Philippe Morency</author><pubDate>Wed, 05 Jul 2023 14:54:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08149v2</guid></item><item><title>Deep Subspace Encoders for Nonlinear System Identification</title><link>http://arxiv.org/abs/2210.14816v2</link><description>Using Artificial Neural Networks (ANN) for nonlinear system identificationhas proven to be a promising approach, but despite of all recent researchefforts, many practical and theoretical problems still remain open.Specifically, noise handling and models, issues of consistency and reliableestimation under minimisation of the prediction error are the most severeproblems. The latter comes with numerous practical challenges such as explosionof the computational cost in terms of the number of data samples and theoccurrence of instabilities during optimization. In this paper, we aim toovercome these issues by proposing a method which uses a truncated predictionloss and a subspace encoder for state estimation. The truncated prediction lossis computed by selecting multiple truncated subsections from the time seriesand computing the average prediction loss. To obtain a computationallyefficient estimation method that minimizes the truncated prediction loss, asubspace encoder represented by an artificial neural network is introduced.This encoder aims to approximate the state reconstructability map of theestimated model to provide an initial state for each truncated subsection givenpast inputs and outputs. By theoretical analysis, we show that, under mildconditions, the proposed method is locally consistent, increases optimizationstability, and achieves increased data efficiency by allowing for overlapbetween the subsections. Lastly, we provide practical insights and userguidelines employing a numerical example and state-of-the-art benchmarkresults.</description><author>Gerben I. Beintema, Maarten Schoukens, Roland Tóth</author><pubDate>Wed, 05 Jul 2023 14:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.14816v2</guid></item><item><title>Meta-Learning Adversarial Bandit Algorithms</title><link>http://arxiv.org/abs/2307.02295v1</link><description>We study online meta-learning with bandit feedback, with the goal ofimproving performance across multiple tasks if they are similar according tosome natural similarity measure. As the first to target the adversarialonline-within-online partial-information setting, we design meta-algorithmsthat combine outer learners to simultaneously tune the initialization and otherhyperparameters of an inner learner for two important cases: multi-armedbandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learnersinitialize and set hyperparameters of the Tsallis-entropy generalization ofExp3, with the task-averaged regret improving if the entropy of theoptima-in-hindsight is small. For BLO, we learn to initialize and tune onlinemirror descent (OMD) with self-concordant barrier regularizers, showing thattask-averaged regret varies directly with an action space-dependent measurethey induce. Our guarantees rely on proving that unregularizedfollow-the-leader combined with two levels of low-dimensional hyperparametertuning is enough to learn a sequence of affine functions of non-Lipschitz andsometimes non-convex Bregman divergences bounding the regret of OMD.</description><author>Mikhail Khodak, Ilya Osadchiy, Keegan Harris, Maria-Florina Balcan, Kfir Y. Levy, Ron Meir, Zhiwei Steven Wu</author><pubDate>Wed, 05 Jul 2023 14:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02295v1</guid></item><item><title>Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition</title><link>http://arxiv.org/abs/2203.14092v2</link><description>The physical and textural attributes of objects have been widely studied forrecognition, detection and segmentation tasks in computer vision.~A number ofdatasets, such as large scale ImageNet, have been proposed for feature learningusing data hungry deep neural networks and for hand-crafted feature extraction.To intelligently interact with objects, robots and intelligent machines needthe ability to infer beyond the traditional physical/textural attributes, andunderstand/learn visual cues, called visual affordances, for affordancerecognition, detection and segmentation. To date there is no publicly availablelarge dataset for visual affordance understanding and learning. In this paper,we introduce a large scale multi-view RGBD visual affordance learning dataset,a benchmark of 47210 RGBD images from 37 object categories, annotated with 15visual affordance categories. To the best of our knowledge, this is the firstever and the largest multi-view RGBD visual affordance learning dataset. Webenchmark the proposed dataset for affordance segmentation and recognitiontasks using popular Vision Transformer and Convolutional Neural Networks.Several state-of-the-art deep learning networks are evaluated each foraffordance recognition and segmentation tasks. Our experimental resultsshowcase the challenging nature of the dataset and present definite prospectsfor new and robust affordance learning algorithms. The dataset is publiclyavailable at https://sites.google.com/view/afaqshah/dataset.</description><author>Zeyad Khalifa, Syed Afaq Ali Shah</author><pubDate>Wed, 05 Jul 2023 14:48:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.14092v2</guid></item><item><title>Focusing on what to decode and what to train: Efficient Training with HOI Split Decoders and Specific Target Guided DeNoising</title><link>http://arxiv.org/abs/2307.02291v1</link><description>Recent one-stage transformer-based methods achieve notable gains in theHuman-object Interaction Detection (HOI) task by leveraging the detection ofDETR. However, the current methods redirect the detection target of the objectdecoder, and the box target is not explicitly separated from the queryembeddings, which leads to long and hard training. Furthermore, matching thepredicted HOI instances with the ground-truth is more challenging than objectdetection, simply adapting training strategies from the object detection makesthe training more difficult. To clear the ambiguity between human and objectdetection and share the prediction burden, we propose a novel one-stageframework (SOV), which consists of a subject decoder, an object decoder, and averb decoder. Moreover, we propose a novel Specific Target Guided (STG)DeNoising strategy, which leverages learnable object and verb label embeddingsto guide the training and accelerates the training convergence. In addition,for the inference part, the label-specific information is directly fed into thedecoders by initializing the query embeddings from the learnable labelembeddings. Without additional features or prior language knowledge, our method(SOV-STG) achieves higher accuracy than the state-of-the-art method inone-third of training epochs. The code is available at\url{https://github.com/cjw2021/SOV-STG}.</description><author>Junwen Chen, Yingcheng Wang, Keiji Yanai</author><pubDate>Wed, 05 Jul 2023 14:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02291v1</guid></item><item><title>Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard</title><link>http://arxiv.org/abs/2307.02288v1</link><description>This paper presents a performance comparison of three large language models(LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard, on theVNHSGE English dataset. The results show that BingChat is better than ChatGPTand Bard. Therefore, BingChat and Bard can replace ChatGPT while ChatGPT is notyet officially available in Vietnam. The results also indicate that ChatGPT,Bing Chat, and Bard outperform Vietnamese students in English languageproficiency. The findings of this study contribute to the understanding of thepotential of LLMs in English language education. The remarkable performance ofChatGPT, Bing Chat, and Bard demonstrates their potential as effective toolsfor teaching and learning English at the high school level.</description><author>Xuan-Quy Dao</author><pubDate>Wed, 05 Jul 2023 14:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02288v1</guid></item><item><title>Minimizing Dynamic Regret on Geodesic Metric Spaces</title><link>http://arxiv.org/abs/2302.08652v2</link><description>In this paper, we consider the sequential decision problem where the goal isto minimize the general dynamic regret on a complete Riemannian manifold. Thetask of offline optimization on such a domain, also known as a geodesic metricspace, has recently received significant attention. The online setting hasreceived significantly less attention, and it has remained an open questionwhether the body of results that hold in the Euclidean setting can betransplanted into the land of Riemannian manifolds where new challenges (e.g.,curvature) come into play. In this paper, we show how to get optimistic regretbound on manifolds with non-positive curvature whenever improper learning isallowed and propose an array of adaptive no-regret algorithms. To the best ofour knowledge, this is the first work that considers general dynamic regret anddevelops "optimistic" online learning algorithms which can be employed ongeodesic metric spaces.</description><author>Zihao Hu, Guanghui Wang, Jacob Abernethy</author><pubDate>Wed, 05 Jul 2023 14:40:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08652v2</guid></item><item><title>Absorbing Phase Transitions in Artificial Deep Neural Networks</title><link>http://arxiv.org/abs/2307.02284v1</link><description>Theoretical understanding of the behavior of infinitely-wide neural networkshas been rapidly developed for various architectures due to the celebratedmean-field theory. However, there is a lack of a clear, intuitive framework forextending our understanding to finite networks that are of more practical andrealistic importance. In the present contribution, we demonstrate that thebehavior of properly initialized neural networks can be understood in terms ofuniversal critical phenomena in absorbing phase transitions. More specifically,we study the order-to-chaos transition in the fully-connected feedforwardneural networks and the convolutional ones to show that (i) there is awell-defined transition from the ordered state to the chaotics state even forthe finite networks, and (ii) difference in architecture is reflected in thatof the universality class of the transition. Remarkably, the finite-sizescaling can also be successfully applied, indicating that intuitivephenomenological argument could lead us to semi-quantitative description of thesignal propagation dynamics.</description><author>Keiichi Tamai, Tsuyoshi Okubo, Truong Vinh Truong Duy, Naotake Natori, Synge Todo</author><pubDate>Wed, 05 Jul 2023 14:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02284v1</guid></item><item><title>Dense Distinct Query for End-to-End Object Detection</title><link>http://arxiv.org/abs/2303.12776v2</link><description>One-to-one label assignment in object detection has successfully obviated theneed for non-maximum suppression (NMS) as postprocessing and makes the pipelineend-to-end. However, it triggers a new dilemma as the widely used sparsequeries cannot guarantee a high recall, while dense queries inevitably bringmore similar queries and encounter optimization difficulties. As both sparseand dense queries are problematic, then what are the expected queries inend-to-end object detection? This paper shows that the solution should be DenseDistinct Queries (DDQ). Concretely, we first lay dense queries like traditionaldetectors and then select distinct ones for one-to-one assignments. DDQ blendsthe advantages of traditional and recent end-to-end detectors and significantlyimproves the performance of various detectors including FCN, R-CNN, and DETRs.Most impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12epochs using a ResNet-50 backbone, outperforming all existing detectors in thesame setting. DDQ also shares the benefit of end-to-end detectors in crowdedscenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchersto consider the complementarity between traditional methods and end-to-enddetectors. The source code can be found at\url{https://github.com/jshilong/DDQ}.</description><author>Shilong Zhang, Xinjiang Wang, Jiaqi Wang, Jiangmiao Pang, Chengqi Lyu, Wenwei Zhang, Ping Luo, Kai Chen</author><pubDate>Wed, 05 Jul 2023 14:36:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12776v2</guid></item><item><title>Interactive Image Segmentation with Cross-Modality Vision Transformers</title><link>http://arxiv.org/abs/2307.02280v1</link><description>Interactive image segmentation aims to segment the target from the backgroundwith the manual guidance, which takes as input multimodal data such as images,clicks, scribbles, and bounding boxes. Recently, vision transformers haveachieved a great success in several downstream visual tasks, and a few effortshave been made to bring this powerful architecture to interactive segmentationtask. However, the previous works neglect the relations between two modalitiesand directly mock the way of processing purely visual information withself-attentions. In this paper, we propose a simple yet effective network forclick-based interactive segmentation with cross-modality vision transformers.Cross-modality transformers exploits mutual information to better guide thelearning process. The experiments on several benchmarks show that the proposedmethod achieves superior performance in comparison to the previousstate-of-the-art models. The stability of our method in term of avoidingfailure cases shows its potential to be a practical annotation tool. The codeand pretrained models will be released underhttps://github.com/lik1996/iCMFormer.</description><author>Kun Li, George Vosselman, Michael Ying Yang</author><pubDate>Wed, 05 Jul 2023 14:29:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02280v1</guid></item><item><title>From NeurODEs to AutoencODEs: a mean-field control framework for width-varying Neural Networks</title><link>http://arxiv.org/abs/2307.02279v1</link><description>In our work, we build upon the established connection between Residual NeuralNetworks (ResNets) and continuous-time control systems known as NeurODEs. Byconstruction, NeurODEs have been limited to constant-width layers, making themunsuitable for modeling deep learning architectures with width-varying layers.In this paper, we propose a continuous-time Autoencoder, which we callAutoencODE, and we extend to this case the mean-field control framework alreadydeveloped for usual NeurODEs. In this setting, we tackle the case of lowTikhonov regularization, resulting in potentially non-convex cost landscapes.While the global results obtained for high Tikhonov regularization may not holdglobally, we show that many of them can be recovered in regions where the lossfunction is locally convex. Inspired by our theoretical findings, we develop atraining method tailored to this specific type of Autoencoders with residualconnections, and we validate our approach through numerical experimentsconducted on various examples.</description><author>Cristina Cipriani, Massimo Fornasier, Alessandro Scagliotti</author><pubDate>Wed, 05 Jul 2023 14:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02279v1</guid></item><item><title>AMIGO: Sparse Multi-Modal Graph Transformer with Shared-Context Processing for Representation Learning of Giga-pixel Images</title><link>http://arxiv.org/abs/2303.00865v2</link><description>Processing giga-pixel whole slide histopathology images (WSI) is acomputationally expensive task. Multiple instance learning (MIL) has become theconventional approach to process WSIs, in which these images are split intosmaller patches for further processing. However, MIL-based techniques ignoreexplicit information about the individual cells within a patch. In this paper,by defining the novel concept of shared-context processing, we designed amulti-modal Graph Transformer (AMIGO) that uses the celluar graph within thetissue to provide a single representation for a patient while taking advantageof the hierarchical structure of the tissue, enabling a dynamic focus betweencell-level and tissue-level information. We benchmarked the performance of ourmodel against multiple state-of-the-art methods in survival prediction andshowed that ours can significantly outperform all of them includinghierarchical Vision Transformer (ViT). More importantly, we show that our modelis strongly robust to missing information to an extent that it can achieve thesame performance with as low as 20% of the data. Finally, in two differentcancer datasets, we demonstrated that our model was able to stratify thepatients into low-risk and high-risk groups while other state-of-the-artmethods failed to achieve this goal. We also publish a large dataset ofimmunohistochemistry images (InUIT) containing 1,600 tissue microarray (TMA)cores from 188 patients along with their survival information, making it one ofthe largest publicly available datasets in this context.</description><author>Ramin Nakhli, Puria Azadi Moghadam, Haoyang Mi, Hossein Farahani, Alexander Baras, Blake Gilks, Ali Bashashati</author><pubDate>Wed, 05 Jul 2023 14:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00865v2</guid></item><item><title>First-Explore, then Exploit: Meta-Learning Intelligent Exploration</title><link>http://arxiv.org/abs/2307.02276v1</link><description>Standard reinforcement learning (RL) agents never intelligently explore likea human (i.e. by taking into account complex domain priors and previousexplorations). Even the most basic intelligent exploration strategies such asexhaustive search are only inefficiently or poorly approximated by approachessuch as novelty search or intrinsic motivation, let alone more complicatedstrategies like learning new skills, climbing stairs, opening doors, orconducting experiments. This lack of intelligent exploration limits sampleefficiency and prevents solving hard exploration domains. We argue a corebarrier prohibiting many RL approaches from learning intelligent exploration isthat the methods attempt to explore and exploit simultaneously, which harmsboth exploration and exploitation as the goals often conflict. We propose anovel meta-RL framework (First-Explore) with two policies: one policy learns toonly explore and one policy learns to only exploit. Once trained, we can thenexplore with the explore policy, for as long as desired, and then exploit basedon all the information gained during exploration. This approach avoids theconflict of trying to do both exploration and exploitation at once. Wedemonstrate that First-Explore can learn intelligent exploration strategiessuch as exhaustive search and more, and that it outperforms dominant standardRL and meta-RL approaches on domains where exploration requires sacrificingreward. First-Explore is a significant step towards creating meta-RL algorithmscapable of learning human-level exploration which is essential to solvechallenging unseen hard-exploration domains.</description><author>Ben Norman, Jeff Clune</author><pubDate>Wed, 05 Jul 2023 14:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02276v1</guid></item><item><title>Convolutions Through the Lens of Tensor Networks</title><link>http://arxiv.org/abs/2307.02275v1</link><description>Despite their simple intuition, convolutions are more tedious to analyze thandense layers, which complicates the generalization of theoretical andalgorithmic ideas. We provide a new perspective onto convolutions throughtensor networks (TNs) which allow reasoning about the underlying tensormultiplications by drawing diagrams, and manipulating them to perform functiontransformations, sub-tensor access, and fusion. We demonstrate this expressivepower by deriving the diagrams of various autodiff operations and popularapproximations of second-order information with full hyper-parameter support,batching, channel groups, and generalization to arbitrary convolutiondimensions. Further, we provide convolution-specific transformations based onthe connectivity pattern which allow to re-wire and simplify diagrams beforeevaluation. Finally, we probe computational performance, relying on establishedmachinery for efficient TN contraction. Our TN implementation speeds up arecently-proposed KFAC variant up to 4.5x and enables new hardware-efficienttensor dropout for approximate backpropagation.</description><author>Felix Dangel</author><pubDate>Wed, 05 Jul 2023 14:19:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02275v1</guid></item><item><title>Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression</title><link>http://arxiv.org/abs/2307.02273v1</link><description>Recently, the performance of neural image compression (NIC) has steadilyimproved thanks to the last line of study, reaching or outperformingstate-of-the-art conventional codecs. Despite significant progress, current NICmethods still rely on ConvNet-based entropy coding, limited in modelinglong-range dependencies due to their local connectivity and the increasingnumber of architectural biases and priors, resulting in complex underperformingmodels with high decoding latency. Motivated by the efficiency investigation ofthe Tranformer-based transform coding framework, namely SwinT-ChARM, we proposeto enhance the latter, as first, with a more straightforward yet effectiveTranformer-based channel-wise auto-regressive prior model, resulting in anabsolute image compression transformer (ICT). Through the proposed ICT, we cancapture both global and local contexts from the latent representations andbetter parameterize the distribution of the quantized latents. Further, weleverage a learnable scaling module with a sandwich ConvNeXt-basedpre-/post-processor to accurately extract more compact latent codes whilereconstructing higher-quality images. Extensive experimental results onbenchmark datasets showed that the proposed framework significantly improvesthe trade-off between coding efficiency and decoder complexity over theversatile video coding (VVC) reference encoder (VTM-18.0) and the neural codecSwinT-ChARM. Moreover, we provide model scaling studies to verify thecomputational efficiency of our approach and conduct several objective andsubjective analyses to bring to the fore the performance gap between theadaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.</description><author>Ahmed Ghorbel, Wassim Hamidouche, Luce Morin</author><pubDate>Wed, 05 Jul 2023 14:17:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02273v1</guid></item><item><title>SVDM: Single-View Diffusion Model for Pseudo-Stereo 3D Object Detection</title><link>http://arxiv.org/abs/2307.02270v1</link><description>One of the key problems in 3D object detection is to reduce the accuracy gapbetween methods based on LiDAR sensors and those based on monocular cameras. Arecently proposed framework for monocular 3D detection based on Pseudo-Stereohas received considerable attention in the community. However, so far these twoproblems are discovered in existing practices, including (1) monocular depthestimation and Pseudo-Stereo detector must be trained separately, (2) Difficultto be compatible with different stereo detectors and (3) the overallcalculation is large, which affects the reasoning speed. In this work, wepropose an end-to-end, efficient pseudo-stereo 3D detection framework byintroducing a Single-View Diffusion Model (SVDM) that uses a few iterations togradually deliver right informative pixels to the left image. SVDM allows theentire pseudo-stereo 3D detection pipeline to be trained end-to-end and canbenefit from the training of stereo detectors. Afterwards, we further explorethe application of SVDM in depth-free stereo 3D detection, and the finalframework is compatible with most stereo detectors. Among multiple benchmarkson the KITTI dataset, we achieve new state-of-the-art performance.</description><author>Yuguang Shi</author><pubDate>Wed, 05 Jul 2023 14:10:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02270v1</guid></item><item><title>Age Prediction Performance Varies Across Deep, Superficial, and Cerebellar White Matter Connections</title><link>http://arxiv.org/abs/2211.07398v2</link><description>The brain's white matter (WM) undergoes developmental and degenerativeprocesses during the human lifespan. To investigate the relationship between WManatomical regions and age, we study diffusion magnetic resonance imagingtractography that is finely parcellated into fiber clusters in the deep,superficial, and cerebellar WM. We propose a deep-learning-based age predictionmodel that leverages large convolutional kernels and inverted bottlenecks. Weimprove performance using novel discrete multi-faceted mix data augmentationand a novel prior-knowledge-based loss function that encourages age predictionsin the expected range. We study a dataset of 965 healthy young adults (22-37years) derived from the Human Connectome Project (HCP). Experimental resultsdemonstrate that the proposed model achieves a mean absolute error of 2.59years and outperforms compared methods. We find that the deep WM is the mostinformative for age prediction in this cohort, while the superficial WM is theleast informative. Overall, the most predictive WM tracts are thethalamo-frontal tract from the deep WM and the intracerebellar input andPurkinje tract from the cerebellar WM.</description><author>Yuxiang Wei, Tengfei Xue, Yogesh Rathi, Nikos Makris, Fan Zhang, Lauren J. O'Donnell</author><pubDate>Wed, 05 Jul 2023 14:08:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07398v2</guid></item><item><title>SpaceNLI: Evaluating the Consistency of Predicting Inferences in Space</title><link>http://arxiv.org/abs/2307.02269v1</link><description>While many natural language inference (NLI) datasets target certain semanticphenomena, e.g., negation, tense &amp; aspect, monotonicity, and presupposition, tothe best of our knowledge, there is no NLI dataset that involves diverse typesof spatial expressions and reasoning. We fill this gap by semi-automaticallycreating an NLI dataset for spatial reasoning, called SpaceNLI. The datasamples are automatically generated from a curated set of reasoning patterns,where the patterns are annotated with inference labels by experts. We testseveral SOTA NLI systems on SpaceNLI to gauge the complexity of the dataset andthe system's capacity for spatial reasoning. Moreover, we introduce a PatternAccuracy and argue that it is a more reliable and stricter measure than theaccuracy for evaluating a system's performance on pattern-based generated datasamples. Based on the evaluation results we find that the systems obtainmoderate results on the spatial NLI problems but lack consistency per inferencepattern. The results also reveal that non-projective spatial inferences(especially due to the "between" preposition) are the most challenging ones.</description><author>Lasha Abzianidze, Joost Zwarts, Yoad Winter</author><pubDate>Wed, 05 Jul 2023 14:08:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02269v1</guid></item><item><title>Dynamical Isometry based Rigorous Fair Neural Architecture Search</title><link>http://arxiv.org/abs/2307.02263v1</link><description>Recently, the weight-sharing technique has significantly speeded up thetraining and evaluation procedure of neural architecture search. However, mostexisting weight-sharing strategies are solely based on experience orobservation, which makes the searching results lack interpretability andrationality. In addition, due to the negligence of fairness, current methodsare prone to make misjudgments in module evaluation. To address these problems,we propose a novel neural architecture search algorithm based on dynamicalisometry. We use the fix point analysis method in the mean field theory toanalyze the dynamics behavior in the steady state random neural network, andhow dynamic isometry guarantees the fairness of weight-sharing based NAS.Meanwhile, we prove that our module selection strategy is rigorous fair byestimating the generalization error of all modules with well-conditionedJacobian. Extensive experiments show that, with the same size, the architecturesearched by the proposed method can achieve state-of-the-art top-1 validationaccuracy on ImageNet classification. In addition, we demonstrate that ourmethod is able to achieve better and more stable training performance withoutloss of generality.</description><author>Jianxiang Luo, Junyi Hu, Tianji Pang, Weihao Huang, Chuang Liu</author><pubDate>Wed, 05 Jul 2023 14:01:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02263v1</guid></item><item><title>Analyzing Different Expert-Opined Strategies to Enhance the Effect on the Goal of a Multi-Attribute Decision-Making System Using a Concept of Effort Propagation and Application in Enhancement of High School Students' Performance</title><link>http://arxiv.org/abs/2307.02254v1</link><description>In many real-world multi-attribute decision-making (MADM) problems, miningthe inter-relationships and possible hierarchical structures among the factorsare considered to be one of the primary tasks. But, besides that, one majortask is to determine an optimal strategy to work on the factors to enhance theeffect on the goal attribute. This paper proposes two such strategies, namelyparallel and hierarchical effort assignment, and propagation strategies. Theconcept of effort propagation through a strategy is formally defined anddescribed in the paper. Both the parallel and hierarchical strategies aredivided into sub-strategies based on whether the assignment of efforts to thefactors is uniform or depends upon some appropriate heuristics related to thefactors in the system. The adapted and discussed heuristics are the relativesignificance and effort propagability of the factors. The strategies areanalyzed for a real-life case study regarding Indian high school administrativefactors that play an important role in enhancing students' performance. Totaleffort propagation of around 7%-15% to the goal is seen across the proposedstrategies given a total of 1 unit of effort to the directly accessible factorsof the system. A comparative analysis is adapted to determine the optimalstrategy among the proposed ones to enhance student performance mosteffectively. The highest effort propagation achieved in the work isapproximately 14.4348%. The analysis in the paper establishes the necessity ofresearch towards the direction of effort propagation analysis in case ofdecision-making problems.</description><author>Suvojit Dhara, Adrijit Goswami</author><pubDate>Wed, 05 Jul 2023 13:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02254v1</guid></item><item><title>Multivariate Time Series Classification: A Deep Learning Approach</title><link>http://arxiv.org/abs/2307.02253v1</link><description>This paper investigates different methods and various neural networkarchitectures applicable in the time series classification domain. The data isobtained from a fleet of gas sensors that measure and track quantities such asoxygen and sound. With the help of this data, we can detect events such asoccupancy in a specific environment. At first, we analyze the time series datato understand the effect of different parameters, such as the sequence length,when training our models. These models employ Fully Convolutional Networks(FCN) and Long Short-Term Memory (LSTM) for supervised learning and RecurrentAutoencoders for semisupervised learning. Throughout this study, we spot thedifferences between these methods based on metrics such as precision and recallidentifying which technique best suits this problem.</description><author>Mohamed Abouelnaga, Julien Vitay, Aida Farahani</author><pubDate>Wed, 05 Jul 2023 13:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02253v1</guid></item><item><title>RanPAC: Random Projections and Pre-trained Models for Continual Learning</title><link>http://arxiv.org/abs/2307.02251v1</link><description>Continual learning (CL) aims to incrementally learn different tasks (such asclassification) in a non-stationary data stream without forgetting old ones.Most CL works focus on tackling catastrophic forgetting under alearning-from-scratch paradigm. However, with the increasing prominence offoundation models, pre-trained models equipped with informative representationshave become available for various downstream requirements. Several CL methodsbased on pre-trained models have been explored, either utilizing pre-extractedfeatures directly (which makes bridging distribution gaps challenging) orincorporating adaptors (which may be subject to forgetting). In this paper, wepropose a concise and effective approach for CL with pre-trained models. Giventhat forgetting occurs during parameter updating, we contemplate an alternativeapproach that exploits training-free random projectors and class-prototypeaccumulation, which thus bypasses the issue. Specifically, we inject a frozenRandom Projection layer with nonlinear activation between the pre-trainedmodel's feature representations and output head, which captures interactionsbetween features with expanded dimensionality, providing enhanced linearseparability for class-prototype-based CL. We also demonstrate the importanceof decorrelating the class-prototypes to reduce the distribution disparity whenusing pre-trained representations. These techniques prove to be effective andcircumvent the problem of forgetting for both class- and domain-incrementalcontinual learning. Compared to previous methods applied to pre-trainedViT-B/16 models, we reduce final error rates by between 10\% and 62\% on sevenclass-incremental benchmark datasets, despite not using any rehearsal memory.We conclude that the full potential of pre-trained models for simple,effective, and fast continual learning has not hitherto been fully tapped.</description><author>Mark D. McDonnell, Dong Gong, Amin Parveneh, Ehsan Abbasnejad, Anton van den Hengel</author><pubDate>Wed, 05 Jul 2023 13:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02251v1</guid></item><item><title>Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need</title><link>http://arxiv.org/abs/2307.02249v1</link><description>Weakly supervised whole slide image classification is usually formulated as amultiple instance learning (MIL) problem, where each slide is treated as a bag,and the patches cut out of it are treated as instances. Existing methods eithertrain an instance classifier through pseudo-labeling or aggregate instancefeatures into a bag feature through attention mechanisms and then train a bagclassifier, where the attention scores can be used for instance-levelclassification. However, the pseudo instance labels constructed by the formerusually contain a lot of noise, and the attention scores constructed by thelatter are not accurate enough, both of which affect their performance. In thispaper, we propose an instance-level MIL framework based on contrastive learningand prototype learning to effectively accomplish both instance classificationand bag classification tasks. To this end, we propose an instance-level weaklysupervised contrastive learning algorithm for the first time under the MILsetting to effectively learn instance feature representation. We also proposean accurate pseudo label generation method through prototype learning. We thendevelop a joint training strategy for weakly supervised contrastive learning,prototype learning, and instance classifier training. Extensive experiments andvisualizations on four datasets demonstrate the powerful performance of ourmethod. Codes will be available.</description><author>Linhao Qu, Yingfan Ma, Xiaoyuan Luo, Manning Wang, Zhijian Song</author><pubDate>Wed, 05 Jul 2023 13:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02249v1</guid></item><item><title>SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption</title><link>http://arxiv.org/abs/2307.00677v2</link><description>Density-based clustering could be the most popular clustering algorithm sinceit can identify clusters of arbitrary shape as long as different (high-density)clusters are separated by low-density regions. However, the requirement of theseparateness of clusters by low-density regions is not trivial since ahigh-density region might have different structures which should be clusteredinto different groups. Such a situation demonstrates the main flaw of allprevious density-based clustering algorithms we have known--structures in ahigh-density cluster could not be detected. Therefore, this paper aims toprovide a density-based clustering scheme that not only has the abilityprevious ones have but could also detect structures in a high-density regionnot separated by low-density ones. The algorithm employs secondary directeddifferential, hierarchy, normalized density, as well as the self-adaptioncoefficient, and thus is called Structure Detecting Cluster by HierarchicalSecondary Directed Differential with Normalized Density and Self-Adaption,dubbed by SDC-HSDD-NDSA for short. To illustrate its effectiveness, we run thealgorithm in several data sets. The results verify its validity in structuredetection, robustness over noises, as well as independence of granularities,and demonstrate that it could outperform previous ones. The Python code of thepaper could be found on https://github.com/Hao-B-Shu/SDC-HSDD-NDSA.</description><author>Hao Shu</author><pubDate>Wed, 05 Jul 2023 13:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00677v2</guid></item><item><title>S3C: Self-Supervised Stochastic Classifiers for Few-Shot Class-Incremental Learning</title><link>http://arxiv.org/abs/2307.02246v1</link><description>Few-shot class-incremental learning (FSCIL) aims to learn progressively aboutnew classes with very few labeled samples, without forgetting the knowledge ofalready learnt classes. FSCIL suffers from two major challenges: (i)over-fitting on the new classes due to limited amount of data, (ii)catastrophically forgetting about the old classes due to unavailability of datafrom these classes in the incremental stages. In this work, we propose aself-supervised stochastic classifier (S3C) to counter both these challenges inFSCIL. The stochasticity of the classifier weights (or class prototypes) notonly mitigates the adverse effect of absence of large number of samples of thenew classes, but also the absence of samples from previously learnt classesduring the incremental steps. This is complemented by the self-supervisioncomponent, which helps to learn features from the base classes which generalizewell to unseen classes that are encountered in future, thus reducingcatastrophic forgetting. Extensive evaluation on three benchmark datasets usingmultiple evaluation metrics show the effectiveness of the proposed framework.We also experiment on two additional realistic scenarios of FSCIL, namely wherethe number of annotated data available for each of the new classes can bedifferent, and also where the number of base classes is much lesser, and showthat the proposed S3C performs significantly better than the state-of-the-artfor all these challenging scenarios.</description><author>Jayateja Kalla, Soma Biswas</author><pubDate>Wed, 05 Jul 2023 13:41:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02246v1</guid></item><item><title>Set Learning for Accurate and Calibrated Models</title><link>http://arxiv.org/abs/2307.02245v1</link><description>Model overconfidence and poor calibration are common in machine learning anddifficult to account for when applying standard empirical risk minimization. Inthis work, we propose a novel method to alleviate these problems that we callodd-$k$-out learning (OKO), which minimizes the cross-entropy error for setsrather than for single examples. This naturally allows the model to capturecorrelations across data examples and achieves both better accuracy andcalibration, especially in limited training data and class-imbalanced regimes.Perhaps surprisingly, OKO often yields better calibration even when trainingwith hard labels and dropping any additional calibration parameter tuning, suchas temperature scaling. We provide theoretical justification, establishing thatOKO naturally yields better calibration, and provide extensive experimentalanalyses that corroborate our theoretical findings. We emphasize that OKO is ageneral framework that can be easily adapted to many settings and the trainedmodel can be applied to single examples at inference time, without introducingsignificant run-time overhead or architecture changes.</description><author>Lukas Muttenthaler, Robert A. Vandermeulen, Qiuyi, Zhang, Thomas Unterthiner, Klaus-Robert Müller</author><pubDate>Wed, 05 Jul 2023 13:39:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02245v1</guid></item></channel></rss>