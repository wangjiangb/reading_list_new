<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 06 Jan 2024 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Anatomy-aware and acquisition-agnostic joint registration with SynthMorph</title><link>http://arxiv.org/abs/2301.11329v2</link><description>Affine image registration is a cornerstone of medical-image analysis. Whileclassical algorithms can achieve excellent accuracy, they solve atime-consuming optimization for every image pair. Deep-learning (DL) methodslearn a function that maps an image pair to an output transform. Evaluating thefunction is fast, but capturing large transforms can be challenging, andnetworks tend to struggle if a test-image characteristic shifts from thetraining domain, such as resolution. Most affine methods are agnostic toanatomy, meaning the registration will be inaccurate if algorithms consider allstructures in the image. We address these shortcomings with SynthMorph, an easy-to-use DL tool forjoint affine-deformable registration of any brain image without preprocessing,right off the MRI scanner. First, we leverage a strategy to train networks withwildly varying images synthesized from label maps, yielding robust performanceacross acquisition specifics unseen at training. Second, we optimize thespatial overlap of select anatomical labels. This enables networks todistinguish anatomy of interest from irrelevant structures, removing the needfor preprocessing that excludes content which would impinge on anatomy-specificregistration. Third, we combine the affine model with a deformable hypernetworkthat lets users choose the optimal deformation-field regularity for theirspecific data, at registration time, in a fraction of the time required byclassical methods. We rigorously analyze how competing architectures learn affine transforms andcompare state-of-the-art registration tools across an extremely diverse set ofneuroimaging data, aiming to truly capture the behavior of methods in the realworld. SynthMorph demonstrates consistent and improved accuracy. It isavailable at https://w3id.org/synthmorph, as a single complete end-to-endsolution for registration of brain MRI.</description><author>Malte Hoffmann, Andrew Hoopes, Douglas N. Greve, Bruce Fischl, Adrian V. Dalca</author><pubDate>Thu, 04 Jan 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11329v2</guid></item><item><title>Learning to Prompt with Text Only Supervision for Vision-Language Models</title><link>http://arxiv.org/abs/2401.02418v1</link><description>Foundational vision-language models such as CLIP are becoming a new paradigmin vision, due to their excellent generalization abilities. However, adaptingthese models for downstream tasks while maintaining their generalizationremains a challenge. In literature, one branch of methods adapts CLIP bylearning prompts using visual information. While effective, most of these worksrequire labeled data which is not practical, and often struggle to generalizetowards new datasets due to over-fitting on the source data. An alternativeapproach resorts to training-free methods by generating class descriptions fromlarge language models (LLMs) and perform prompt ensembling. However, thesemethods often generate class specific prompts that cannot be transferred toother classes, which incur higher costs by generating LLM descriptions for eachclass separately. In this work, we propose to combine the strengths of theseboth streams of methods by learning prompts using only text data derived fromLLMs. As supervised training of prompts is not trivial due to absence ofimages, we develop a training approach that allows prompts to extract richcontextual knowledge from LLM data. Moreover, with LLM contextual data mappedwithin the learned prompts, it enables zero-shot transfer of prompts to newclasses and datasets potentially cutting the LLM prompt engineering cost. Tothe best of our knowledge, this is the first work that learns generalizedprompts using text only data. We perform extensive evaluations on 4 benchmarkswhere our method improves over prior ensembling works while being competitiveto those utilizing labeled images. Our code and pre-trained models areavailable at https://github.com/muzairkhattak/ProText.</description><author>Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, Luc Van Gool, Federico Tombari</author><pubDate>Thu, 04 Jan 2024 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02418v1</guid></item><item><title>Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition</title><link>http://arxiv.org/abs/2401.02417v1</link><description>While word error rates of automatic speech recognition (ASR) systems haveconsistently fallen, natural language understanding (NLU) applications built ontop of ASR systems still attribute significant numbers of failures tolow-quality speech recognition results. Existing assistant systems collectlarge numbers of these unsuccessful interactions, but these systems usuallyfail to learn from these interactions, even in an offline fashion. In thiswork, we introduce CLC: Contrastive Learning for Conversations, a family ofmethods for contrastive fine-tuning of models in a self-supervised fashion,making use of easily detectable artifacts in unsuccessful conversations withassistants. We demonstrate that our CLC family of approaches can improve theperformance of ASR models on OD3, a new public large-scale semi-syntheticmeta-dataset of audio task-oriented dialogues, by up to 19.2%. These gainstransfer to real-world systems as well, where we show that CLC can help toimprove performance by up to 6.7% over baselines. We make OD3 publiclyavailable at https://github.com/amazon-science/amazon-od3 .</description><author>David M. Chan, Shalini Ghosh, Hitesh Tulsiani, Ariya Rastrow, Björn Hoffmeister</author><pubDate>Thu, 04 Jan 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02417v1</guid></item><item><title>ODIN: A Single Model for 2D and 3D Perception</title><link>http://arxiv.org/abs/2401.02416v1</link><description>State-of-the-art models on contemporary 3D perception benchmarks like ScanNetconsume and label dataset-provided 3D point clouds, obtained through postprocessing of sensed multiview RGB-D images. They are typically trainedin-domain, forego large-scale 2D pre-training and outperform alternatives thatfeaturize the posed RGB-D multiview images instead. The gap in performancebetween methods that consume posed images versus post-processed 3D point cloudshas fueled the belief that 2D and 3D perception require distinct modelarchitectures. In this paper, we challenge this view and propose ODIN(Omni-Dimensional INstance segmentation), a model that can segment and labelboth 2D RGB images and 3D point clouds, using a transformer architecture thatalternates between 2D within-view and 3D cross-view information fusion. Ourmodel differentiates 2D and 3D feature operations through the positionalencodings of the tokens involved, which capture pixel coordinates for 2D patchtokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-artperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentationbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. Itoutperforms all previous works by a wide margin when the sensed 3D point cloudis used in place of the point cloud sampled from 3D mesh. When used as the 3Dperception engine in an instructable embodied agent architecture, it sets a newstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code andcheckpoints can be found at the project website: https://odin-seg.github.io.</description><author>Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki</author><pubDate>Thu, 04 Jan 2024 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02416v1</guid></item><item><title>LLaMA Pro: Progressive LLaMA with Block Expansion</title><link>http://arxiv.org/abs/2401.02415v1</link><description>Humans generally acquire new skills without compromising the old; however,the opposite holds for Large Language Models (LLMs), e.g., from LLaMA toCodeLLaMA. To this end, we propose a new post-pretraining method for LLMs withan expansion of Transformer blocks. We tune the expanded blocks using only newcorpus, efficiently and effectively improving the model's knowledge withoutcatastrophic forgetting. In this paper, we experiment on the corpus of code andmath, yielding LLaMA Pro-8.3B, a versatile foundation model initialized fromLLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Proand its instruction-following counterpart (LLaMA Pro-Instruct) achieve advancedperformance among various benchmarks, demonstrating superiority over existingopen models in the LLaMA family and the immense potential of reasoning andaddressing diverse tasks as an intelligent agent. Our findings provide valuableinsights into integrating natural and programming languages, laying a solidfoundation for developing advanced language agents that operate effectively invarious environments.</description><author>Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, Ying Shan</author><pubDate>Thu, 04 Jan 2024 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02415v1</guid></item><item><title>Controlling Moments with Kernel Stein Discrepancies</title><link>http://arxiv.org/abs/2211.05408v2</link><description>Kernel Stein discrepancies (KSDs) measure the quality of a distributionalapproximation and can be computed even when the target density has anintractable normalizing constant. Notable applications include the diagnosis ofapproximate MCMC samplers and goodness-of-fit tests for unnormalizedstatistical models. The present work analyzes the convergence controlproperties of KSDs. We first show that standard KSDs used for weak convergencecontrol fail to control moment convergence. To address this limitation, we nextprovide sufficient conditions under which alternative diffusion KSDs controlboth moment and weak convergence. As an immediate consequence we develop, foreach $q &gt; 0$, the first KSDs known to exactly characterize $q$-Wassersteinconvergence.</description><author>Heishiro Kanagawa, Alessandro Barp, Arthur Gretton, Lester Mackey</author><pubDate>Thu, 04 Jan 2024 18:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05408v2</guid></item><item><title>Bring Metric Functions into Diffusion Models</title><link>http://arxiv.org/abs/2401.02414v1</link><description>We introduce a Cascaded Diffusion Model (Cas-DM) that improves a DenoisingDiffusion Probabilistic Model (DDPM) by effectively incorporating additionalmetric functions in training. Metric functions such as the LPIPS loss have beenproven highly effective in consistency models derived from the score matching.However, for the diffusion counterparts, the methodology and efficacy of addingextra metric functions remain unclear. One major challenge is the mismatchbetween the noise predicted by a DDPM at each step and the desired clean imagethat the metric function works well on. To address this problem, we proposeCas-DM, a network architecture that cascades two network modules to effectivelyapply metric functions to the diffusion model training. The first module,similar to a standard DDPM, learns to predict the added noise and is unaffectedby the metric function. The second cascaded module learns to predict the cleanimage, thereby facilitating the metric function computation. Experiment resultsshow that the proposed diffusion model backbone enables the effective use ofthe LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) onvarious established benchmarks.</description><author>Jie An, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Zicheng Liu, Lijuan Wang, Jiebo Luo</author><pubDate>Thu, 04 Jan 2024 18:55:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02414v1</guid></item><item><title>Simulation-Based Inference with Quantile Regression</title><link>http://arxiv.org/abs/2401.02413v1</link><description>We present Neural Quantile Estimation (NQE), a novel Simulation-BasedInference (SBI) method based on conditional quantile regression. NQEautoregressively learns individual one dimensional quantiles for each posteriordimension, conditioned on the data and previous posterior dimensions. Posteriorsamples are obtained by interpolating the predicted quantiles using monotoniccubic Hermite spline, with specific treatment for the tail behavior andmulti-modal distributions. We introduce an alternative definition for theBayesian credible region using the local Cumulative Density Function (CDF),offering substantially faster evaluation than the traditional Highest PosteriorDensity Region (HPDR). In case of limited simulation budget and/or known modelmisspecification, a post-processing broadening step can be integrated into NQEto ensure the unbiasedness of the posterior estimation with negligibleadditional computational cost. We demonstrate that the proposed NQE methodachieves state-of-the-art performance on a variety of benchmark problems.</description><author>He Jia</author><pubDate>Thu, 04 Jan 2024 18:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02413v1</guid></item><item><title>LLM Augmented LLMs: Expanding Capabilities through Composition</title><link>http://arxiv.org/abs/2401.02412v1</link><description>Foundational models with billions of parameters which have been trained onlarge corpora of data have demonstrated non-trivial skills in a variety ofdomains. However, due to their monolithic structure, it is challenging andexpensive to augment them or impart new skills. On the other hand, due to theiradaptation abilities, several new instances of these models are being trainedtowards new domains and tasks. In this work, we study the problem of efficientand practical composition of existing foundation models with more specificmodels to enable newer capabilities. To this end, we propose CALM --Composition to Augment Language Models -- which introduces cross-attentionbetween models to compose their representations and enable new capabilities.Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'existing LLMs along with a few additional parameters and data, (ii) Existingmodel weights are kept intact, and hence preserves existing capabilities, and(iii) Applies to diverse domains and settings. We illustrate that augmentingPaLM2-S with a smaller model trained on low-resource languages results in anabsolute improvement of up to 13\% on tasks like translation into English andarithmetic reasoning for low-resource languages. Similarly, when PaLM2-S isaugmented with a code-specific model, we see a relative improvement of 40\%over the base model for code generation and explanation tasks -- on-par withfully fine-tuned counterparts.</description><author>Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar</author><pubDate>Thu, 04 Jan 2024 18:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02412v1</guid></item><item><title>What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs</title><link>http://arxiv.org/abs/2401.02411v1</link><description>3D-aware Generative Adversarial Networks (GANs) have shown remarkableprogress in learning to generate multi-view-consistent images and 3D geometriesof scenes from collections of 2D images via neural volume rendering. Yet, thesignificant memory and computational costs of dense sampling in volumerendering have forced 3D GANs to adopt patch-based training or employlow-resolution rendering with post-processing 2D super resolution, whichsacrifices multiview consistency and the quality of resolved geometry.Consequently, 3D GANs have not yet been able to fully resolve the rich 3Dgeometry present in 2D images. In this work, we propose techniques to scaleneural volume rendering to the much higher resolution of native 2D images,thereby resolving fine-grained 3D geometry with unprecedented detail. Ourapproach employs learning-based samplers for accelerating neural rendering for3D GAN training using up to 5 times fewer depth samples. This enables us toexplicitly "render every pixel" of the full-resolution image during trainingand inference without post-processing superresolution in 2D. Together with ourstrategy to learn high-quality surface geometry, our method synthesizeshigh-resolution 3D geometry and strictly view-consistent images whilemaintaining image quality on par with baselines relying on post-processingsuper resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQand AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3DGANs.</description><author>Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano</author><pubDate>Thu, 04 Jan 2024 18:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02411v1</guid></item><item><title>Evaluating Language-Model Agents on Realistic Autonomous Tasks</title><link>http://arxiv.org/abs/2312.11671v2</link><description>In this report, we explore the ability of language model agents to acquireresources, create copies of themselves, and adapt to novel challenges theyencounter in the wild. We refer to this cluster of capabilities as "autonomousreplication and adaptation" or ARA. We believe that systems capable of ARAcould have wide-reaching and hard-to-anticipate consequences, and thatmeasuring and forecasting ARA may be useful for informing measures aroundsecurity, monitoring, and alignment. Additionally, once a system is capable ofARA, placing bounds on a system's capabilities may become significantly moredifficult. We construct four simple example agents that combine language models withtools that allow them to take actions in the world. We then evaluate theseagents on 12 tasks relevant to ARA. We find that these language model agentscan only complete the easiest tasks from this list, although they make someprogress on the more challenging tasks. Unfortunately, these evaluations arenot adequate to rule out the possibility that near-future agents will becapable of ARA. In particular, we do not think that these evaluations providegood assurance that the ``next generation'' of language models (e.g. 100xeffective compute scaleup on existing models) will not yield agents capable ofARA, unless intermediate evaluations are performed during pretraining.Relatedly, we expect that fine-tuning of the existing models could producesubstantially more competent agents, even if the fine-tuning is not directlytargeted at ARA.</description><author>Megan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R. Lin, Hjalmar Wijk, Joel Burget, Aaron Ho, Elizabeth Barnes, Paul Christiano</author><pubDate>Thu, 04 Jan 2024 18:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11671v2</guid></item><item><title>Sliced gradient-enhanced Kriging for high-dimensional function approximation</title><link>http://arxiv.org/abs/2204.03562v3</link><description>Gradient-enhanced Kriging (GE-Kriging) is a well-established surrogatemodelling technique for approximating expensive computational models. However,it tends to get impractical for high-dimensional problems due to the size ofthe inherent correlation matrix and the associated high-dimensionalhyper-parameter tuning problem. To address these issues, a new method, calledsliced GE-Kriging (SGE-Kriging), is developed in this paper for reducing boththe size of the correlation matrix and the number of hyper-parameters. We firstsplit the training sample set into multiple slices, and invoke Bayes' theoremto approximate the full likelihood function via a sliced likelihood function,in which multiple small correlation matrices are utilized to describe thecorrelation of the sample set rather than one large one. Then, we replace theoriginal high-dimensional hyper-parameter tuning problem with a low-dimensionalcounterpart by learning the relationship between the hyper-parameters and thederivative-based global sensitivity indices. The performance of SGE-Kriging isfinally validated by means of numerical experiments with several benchmarks anda high-dimensional aerodynamic modeling problem. The results show that theSGE-Kriging model features an accuracy and robustness that is comparable to thestandard one but comes at much less training costs. The benefits are mostevident for high-dimensional problems with tens of variables.</description><author>Kai Cheng, Ralf Zimmermann</author><pubDate>Thu, 04 Jan 2024 18:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.03562v3</guid></item><item><title>Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2401.02403v1</link><description>Accurately predicting the temperature field in metal additive manufacturing(AM) processes is critical to preventing overheating, adjusting processparameters, and ensuring process stability. While physics-based computationalmodels offer precision, they are often time-consuming and unsuitable forreal-time predictions and online control in iterative design scenarios.Conversely, machine learning models rely heavily on high-quality datasets,which can be costly and challenging to obtain within the metal AM domain. Ourwork addresses this by introducing a physics-informed neural network frameworkspecifically designed for temperature field prediction in metal AM. Thisframework incorporates a physics-informed input, physics-informed lossfunction, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture.Utilizing real-time temperature data from the process, our model predicts 2Dtemperature fields for future timestamps across diverse geometries, depositionpatterns, and process parameters. We validate the proposed framework in twoscenarios: full-field temperature prediction for a thin wall and 2D temperaturefield prediction for cylinder and cubic parts, demonstrating errors below 3%and 1%, respectively. Our proposed framework exhibits the flexibility to beapplied across diverse scenarios with varying process parameters, geometries,and deposition patterns.</description><author>Pouyan Sajadi, Mostafa Rahmani Dehaghani, Yifan Tang, G. Gary Wang</author><pubDate>Thu, 04 Jan 2024 18:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02403v1</guid></item><item><title>3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation</title><link>http://arxiv.org/abs/2401.02402v1</link><description>3D panoptic segmentation is a challenging perception task, which aims topredict both semantic and instance annotations for 3D points in a scene.Although prior 3D panoptic segmentation approaches have achieved greatperformance on closed-set benchmarks, generalizing to novel categories remainsan open problem. For unseen object categories, 2D open-vocabulary segmentationhas achieved promising results that solely rely on frozen CLIP backbones andensembling multiple classification outputs. However, we find that simplyextending these 2D models to 3D does not achieve good performance due to poorper-mask classification quality on novel categories. In this paper, we proposethe first method to tackle 3D open-vocabulary panoptic segmentation. Our modeltakes advantage of the fusion between learnable LiDAR features and dense frozenvision CLIP features, using a single classification head to make predictionsfor both base and novel classes. To further improve the classificationperformance on novel classes and leverage the CLIP model, we propose two novelloss functions: object-level distillation loss and voxel-level distillationloss. Our experiments on the nuScenes and SemanticKITTI datasets show that ourmethod outperforms strong baselines by a large margin.</description><author>Zihao Xiao, Longlong Jing, Shangxuan Wu, Alex Zihao Zhu, Jingwei Ji, Chiyu Max Jiang, Wei-Chih Hung, Thomas Funkhouser, Weicheng Kuo, Anelia Angelova, Yin Zhou, Shiwei Sheng</author><pubDate>Thu, 04 Jan 2024 18:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02402v1</guid></item><item><title>Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays</title><link>http://arxiv.org/abs/2302.13991v3</link><description>Performance degradation due to distribution discrepancy is a longstandingchallenge in intelligent imaging, particularly for chest X-rays (CXRs). Recentstudies have demonstrated that CNNs are biased toward styles (e.g.,uninformative textures) rather than content (e.g., shape), in stark contrast tothe human vision system. Radiologists tend to learn visual cues from CXRs andthus perform well across multiple domains. Motivated by this, we employ thenovel on-the-fly style randomization modules at both image (SRM-IL) and feature(SRM-FL) levels to create rich style perturbed features while keeping thecontent intact for robust cross-domain performance. Previous methods simulateunseen domains by constructing new styles via interpolation or swapping stylesfrom existing data, limiting them to available source domains during training.However, SRM-IL samples the style statistics from the possible value range of aCXR image instead of the training data to achieve more diversifiedaugmentations. Moreover, we utilize pixel-wise learnable parameters in theSRM-FL compared to pre-defined channel-wise mean and standard deviations asstyle embeddings for capturing more representative style features.Additionally, we leverage consistency regularizations on global semanticfeatures and predictive distributions from with and without style-perturbedversions of the same CXR to tweak the model's sensitivity toward contentmarkers for accurate predictions. Our proposed method, trained on CheXpert andMIMIC-CXR datasets, achieves 77.32$\pm$0.35, 88.38$\pm$0.19, 82.63$\pm$0.13AUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIHchest X-ray14, respectively, compared to 75.56$\pm$0.80, 87.57$\pm$0.46,82.07$\pm$0.19 from state-of-the-art models on five-fold cross-validation withstatistically significant results in thoracic disease classification.</description><author>Mohammad Zunaed, Md. Aynal Haque, Taufiq Hasan</author><pubDate>Thu, 04 Jan 2024 18:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13991v3</guid></item><item><title>Learning the 3D Fauna of the Web</title><link>http://arxiv.org/abs/2401.02400v1</link><description>Learning 3D models of all animals on the Earth requires massively scaling upexisting solutions. With this ultimate goal in mind, we develop 3D-Fauna, anapproach that learns a pan-category deformable 3D animal model for more than100 animal species jointly. One crucial bottleneck of modeling animals is thelimited availability of training data, which we overcome by simply learningfrom 2D Internet images. We show that prior category-specific attempts fail togeneralize to rare species with limited training images. We address thischallenge by introducing the Semantic Bank of Skinned Models (SBSM), whichautomatically discovers a small set of base animal shapes by combininggeometric inductive priors with semantic knowledge implicitly captured by anoff-the-shelf self-supervised feature extractor. To train such a model, we alsocontribute a new large-scale dataset of diverse animal species. At inferencetime, given a single image of any quadruped animal, our model reconstructs anarticulated 3D mesh in a feed-forward fashion within seconds.</description><author>Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, Jiajun Wu</author><pubDate>Thu, 04 Jan 2024 18:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02400v1</guid></item><item><title>Generating synthetic data for neural operators</title><link>http://arxiv.org/abs/2401.02398v1</link><description>Numerous developments in the recent literature show the promising potentialof deep learning in obtaining numerical solutions to partial differentialequations (PDEs) beyond the reach of current numerical solvers. However,data-driven neural operators all suffer from the same problem: the data neededto train a network depends on classical numerical solvers such as finitedifference or finite element, among others. In this paper, we propose a newapproach to generating synthetic functional training data that does not requiresolving a PDE numerically. The way we do this is simple: we draw a large number$N$ of independent and identically distributed `random functions' $u_j$ fromthe underlying solution space (e.g., $H_0^1(\Omega)$) in which we know thesolution lies according to classical theory. We then plug each such randomcandidate solution into the equation and get a corresponding right-hand sidefunction $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ assupervised training data for learning the underlying inverse problem $f\rightarrow u$. This `backwards' approach to generating training data onlyrequires derivative computations, in contrast to standard `forward' approaches,which require a numerical PDE solver, enabling us to generate a large number ofsuch data points quickly and efficiently. While the idea is simple, we hopethat this method will expand the potential for developing neural PDE solversthat do not depend on classical numerical solvers.</description><author>Erisa Hasani, Rachel A. Ward</author><pubDate>Thu, 04 Jan 2024 18:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02398v1</guid></item><item><title>One Shot Learning as Instruction Data Prospector for Large Language Models</title><link>http://arxiv.org/abs/2312.10302v3</link><description>Aligning large language models(LLMs) with human is a critical step ineffectively utilizing their pre-trained capabilities across a wide array oflanguage tasks. Current instruction tuning practices often rely on expandingdataset size without a clear strategy for ensuring data quality, which caninadvertently introduce noise and degrade model performance. To address thischallenge, we introduce Nuggets, a novel and efficient methodology that employsone shot learning to select high-quality instruction data from expansivedatasets. Nuggets assesses the potential of individual instruction examples toact as effective one shot examples, thereby identifying those that cansignificantly enhance diverse task performance. Nuggets utilizes a scoringsystem based on the impact of candidate examples on the perplexity of a diverseanchor set, facilitating the selection of the most beneficial data forinstruction tuning. Through rigorous testing on two benchmarks, includingMT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top1% of Nuggets-curated examples substantially outperforms conventional methodsthat use the full dataset. These findings advocate for a data selectionparadigm that prioritizes quality, offering a more efficient pathway to alignLLMs with humans.</description><author>Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, Yongbin Li</author><pubDate>Thu, 04 Jan 2024 18:00:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10302v3</guid></item><item><title>UpFusion: Novel View Diffusion from Unposed Sparse View Observations</title><link>http://arxiv.org/abs/2312.06661v2</link><description>We propose UpFusion, a system that can perform novel view synthesis and infer3D representations for an object given a sparse set of reference images withoutcorresponding pose information. Current sparse-view 3D inference methodstypically rely on camera poses to geometrically aggregate information frominput views, but are not robust in-the-wild when such information isunavailable/inaccurate. In contrast, UpFusion sidesteps this requirement bylearning to implicitly leverage the available images as context in aconditional generative model for synthesizing novel views. We incorporate twocomplementary forms of conditioning into diffusion models for leveraging theinput views: a) via inferring query-view aligned features using a scene-leveltransformer, b) via intermediate attentional layers that can directly observethe input image tokens. We show that this mechanism allows generatinghigh-fidelity novel views while improving the synthesis quality givenadditional (unposed) images. We evaluate our approach on the Co3Dv2 and GoogleScanned Objects datasets and demonstrate the benefits of our method overpose-reliant sparse-view methods as well as single-view methods that cannotleverage additional views. Finally, we also show that our learned model cangeneralize beyond the training categories and even allow reconstruction fromself-captured images of generic objects in-the-wild.</description><author>Bharath Raj Nagoor Kani, Hsin-Ying Lee, Sergey Tulyakov, Shubham Tulsiani</author><pubDate>Thu, 04 Jan 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06661v2</guid></item><item><title>TinyLlama: An Open-Source Small Language Model</title><link>http://arxiv.org/abs/2401.02385v1</link><description>We present TinyLlama, a compact 1.1B language model pretrained on around 1trillion tokens for approximately 3 epochs. Building on the architecture andtokenizer of Llama 2, TinyLlama leverages various advances contributed by theopen-source community (e.g., FlashAttention), achieving better computationalefficiency. Despite its relatively small size, TinyLlama demonstratesremarkable performance in a series of downstream tasks. It significantlyoutperforms existing open-source language models with comparable sizes. Ourmodel checkpoints and code are publicly available on GitHub athttps://github.com/jzhang38/TinyLlama.</description><author>Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu</author><pubDate>Thu, 04 Jan 2024 17:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02385v1</guid></item><item><title>ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning</title><link>http://arxiv.org/abs/2401.02384v1</link><description>Charts play a vital role in data visualization, understanding data patterns,and informed decision-making. However, their unique combination of graphicalelements (e.g., bars, lines) and textual components (e.g., labels, legends)poses challenges for general-purpose multimodal models. While vision-languagemodels trained on chart data excel in comprehension, they struggle withgeneralization and require task-specific fine-tuning. To address thesechallenges, we propose ChartAssistant, a chart-based vision-language model foruniversal chart comprehension and reasoning. ChartAssistant leverages ChartSFT,a comprehensive dataset covering diverse chart-related tasks with basic andspecialized chart types. It undergoes a two-stage training process, startingwith pre-training on chart-to-table parsing to align chart and text, followedby multitask instruction-following fine-tuning. This approach enablesChartAssistant to achieve competitive performance across various chart taskswithout task-specific fine-tuning. Experimental results demonstrate significantperformance gains over the state-of-the-art UniChart method, outperformingOpenAI's GPT-4V(ision) on real-world chart data. The code and data areavailable at https://github.com/OpenGVLab/ChartAst.</description><author>Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, Ping Luo</author><pubDate>Thu, 04 Jan 2024 17:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02384v1</guid></item><item><title>Survey of 3D Human Body Pose and Shape Estimation Methods for Contemporary Dance Applications</title><link>http://arxiv.org/abs/2401.02383v1</link><description>3D human body shape and pose estimation from RGB images is a challengingproblem with potential applications in augmented/virtual reality, healthcareand fitness technology and virtual retail. Recent solutions have focused onthree types of inputs: i) single images, ii) multi-view images and iii) videos.In this study, we surveyed and compared 3D body shape and pose estimationmethods for contemporary dance and performing arts, with a special focus onhuman body pose and dressing, camera viewpoint, illumination conditions andbackground conditions. We demonstrated that multi-frame methods, such as PHALP,provide better results than single-frame method for pose estimation whendancers are performing contemporary dances.</description><author>Darshan Venkatrayappa, Alain Tremeau, Damien Muselet, Philippe Colantoni</author><pubDate>Thu, 04 Jan 2024 17:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02383v1</guid></item><item><title>Generalized Quadratic Embeddings for Nonlinear Dynamics using Deep Learning</title><link>http://arxiv.org/abs/2211.00357v2</link><description>The engineering design process often relies on mathematical modeling that candescribe the underlying dynamic behavior. In this work, we present adata-driven methodology for modeling the dynamics of nonlinear systems. Tosimplify this task, we aim to identify a coordinate transformation that allowsus to represent the dynamics of nonlinear systems using a common, simple modelstructure. The advantage of a common simple model is that customized designtools developed for it can be applied to study a large variety of nonlinearsystems. The simplest common model -- one can think of -- is linear, but linearsystems often fall short in accurately capturing the complex dynamics ofnonlinear systems. In this work, we propose using quadratic systems as thecommon structure, inspired by the lifting principle. According to thisprinciple, smooth nonlinear systems can be expressed as quadratic systems insuitable coordinates without approximation errors. However, finding thesecoordinates solely from data is challenging. Here, we leverage deep learning toidentify such lifted coordinates using only data, enabling a quadraticdynamical system to describe the system's dynamics. Additionally, we discussthe asymptotic stability of these quadratic dynamical systems. We illustratethe approach using data collected from various numerical examples,demonstrating its superior performance with the existing well-known techniques.</description><author>Pawan Goyal, Peter Benner</author><pubDate>Thu, 04 Jan 2024 17:51:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.00357v2</guid></item><item><title>Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation</title><link>http://arxiv.org/abs/2401.01078v3</link><description>Poetry generation has been a challenging task in the field of NaturalLanguage Processing, as it requires the model to understand the nuances oflanguage, sentiment, and style. In this paper, we propose using Large LanguageModels to generate Vietnamese poems of various genres from natural languageprompts, thereby facilitating an intuitive process with enhanced contentcontrol. Our most efficacious model, the GPT-3 Babbage variant, achieves acustom evaluation score of 0.8, specifically tailored to the "luc bat" genre ofVietnamese poetry. Furthermore, we also explore the idea of paraphrasing poemsinto normal text prompts and yield a relatively high score of 0.781 in the "lucbat" genre. This experiment presents the potential for cross-Languagepoem-to-poem translation with translated poems as the inputs while concurrentlymaintaining complete control over the generated content.</description><author>Triet Minh Huynh, Quan Le Bao</author><pubDate>Thu, 04 Jan 2024 17:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01078v3</guid></item><item><title>SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval</title><link>http://arxiv.org/abs/2401.02369v1</link><description>Clinician must write a lengthy summary each time a patient is discharged fromthe hospital. This task is time-consuming due to the sheer number of uniqueclinical concepts covered in the admission. Identifying and covering saliententities is vital for the summary to be clinically useful. We fine-tuneopen-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\b{eta}) on the task andfind that they generate incomplete and unfaithful summaries. To increase entitycoverage, we train a smaller, encoder-only model to predict salient entities,which are treated as content-plans to guide the LLM. To encourage the LLM tofocus on specific mentions in the source notes, we propose SPEER:Sentence-level Planning via Embedded Entity Retrieval. Specifically, we markeach salient entity span with special "{{ }}" boundary tags and instruct theLLM to retrieve marked spans before generating each sentence. Sentence-levelplanning acts as a form of state tracking in that the model is explicitlyrecording the entities it uses. We fine-tune Mistral and Zephyr variants on alarge-scale, diverse dataset of ~167k in-patient hospital admissions andevaluate on 3 datasets. SPEER shows gains in both coverage and faithfulnessmetrics over non-guided and guided baselines.</description><author>Griffin Adams, Jason Zucker, Noémie Elhadad</author><pubDate>Thu, 04 Jan 2024 17:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02369v1</guid></item><item><title>A Generalizable Physics-informed Learning Framework for Risk Probability Estimation</title><link>http://arxiv.org/abs/2305.06432v2</link><description>Accurate estimates of long-term risk probabilities and their gradients arecritical for many stochastic safe control methods. However, computing such riskprobabilities in real-time and in unseen or changing environments ischallenging. Monte Carlo (MC) methods cannot accurately evaluate theprobabilities and their gradients as an infinitesimal devisor can amplify thesampling noise. In this paper, we develop an efficient method to evaluate theprobabilities of long-term risk and their gradients. The proposed methodexploits the fact that long-term risk probability satisfies certain partialdifferential equations (PDEs), which characterize the neighboring relationsbetween the probabilities, to integrate MC methods and physics-informed neuralnetworks. We provide theoretical guarantees of the estimation error givencertain choices of training configurations. Numerical results show the proposedmethod has better sample efficiency, generalizes well to unseen regions, andcan adapt to systems with changing parameters. The proposed method can alsoaccurately estimate the gradients of risk probabilities, which enables first-and second-order techniques on risk probabilities to be used for learning andcontrol.</description><author>Zhuoyuan Wang, Yorie Nakahira</author><pubDate>Thu, 04 Jan 2024 17:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06432v2</guid></item><item><title>Integration of physics-informed operator learning and finite element method for parametric learning of partial differential equations</title><link>http://arxiv.org/abs/2401.02363v1</link><description>We present a method that employs physics-informed deep learning techniquesfor parametrically solving partial differential equations. The focus is on thesteady-state heat equations within heterogeneous solids exhibiting significantphase contrast. Similar equations manifest in diverse applications likechemical diffusion, electrostatics, and Darcy flow. The neural network aims toestablish the link between the complex thermal conductivity profiles andtemperature distributions, as well as heat flux components within themicrostructure, under fixed boundary conditions. A distinctive aspect is ourindependence from classical solvers like finite element methods for data. Anoteworthy contribution lies in our novel approach to defining the lossfunction, based on the discretized weak form of the governing equation. Thisnot only reduces the required order of derivatives but also eliminates the needfor automatic differentiation in the construction of loss terms, acceptingpotential numerical errors from the chosen discretization method. As a result,the loss function in this work is an algebraic equation that significantlyenhances training efficiency. We benchmark our methodology against the standardfinite element method, demonstrating accurate yet faster predictions using thetrained neural network for temperature and flux profiles. We also show higheraccuracy by using the proposed method compared to purely data-driven approachesfor unforeseen scenarios.</description><author>Shahed Rezaei, Ahmad Moeineddin, Michael Kaliske, Markus Apel</author><pubDate>Thu, 04 Jan 2024 17:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02363v1</guid></item><item><title>An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</title><link>http://arxiv.org/abs/2401.02361v1</link><description>Grounding-DINO is a state-of-the-art open-set detection model that tacklesmultiple vision tasks including Open-Vocabulary Detection (OVD), PhraseGrounding (PG), and Referring Expression Comprehension (REC). Its effectivenesshas led to its widespread adoption as a mainstream architecture for variousdownstream applications. However, despite its significance, the originalGrounding-DINO model lacks comprehensive public technical details due to theunavailability of its training code. To bridge this gap, we presentMM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,which is built with the MMDetection toolbox. It adopts abundant vision datasetsfor pre-training and various detection and grounding datasets for fine-tuning.We give a comprehensive analysis of each reported result and detailed settingsfor reproduction. The extensive experiments on the benchmarks mentioneddemonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tinybaseline. We release all our models to the research community. Codes andtrained models are released athttps://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino.</description><author>Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang</author><pubDate>Thu, 04 Jan 2024 17:00:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02361v1</guid></item><item><title>A novel method to enhance pneumonia detection via a model-level ensembling of CNN and vision transformer</title><link>http://arxiv.org/abs/2401.02358v1</link><description>Pneumonia remains a leading cause of morbidity and mortality worldwide. ChestX-ray (CXR) imaging is a fundamental diagnostic tool, but traditional analysisrelies on time-intensive expert evaluation. Recently, deep learning has shownimmense potential for automating pneumonia detection from CXRs. This paperexplores applying neural networks to improve CXR-based pneumonia diagnosis. Wedeveloped a novel model fusing Convolution Neural networks (CNN) and VisionTransformer networks via model-level ensembling. Our fusion architecturecombines a ResNet34 variant and a Multi-Axis Vision Transformer small model.Both base models are initialized with ImageNet pre-trained weights. The outputlayers are removed, and features are combined using a flattening layer beforefinal classification. Experiments used the Kaggle pediatric pneumonia datasetcontaining 1,341 normal and 3,875 pneumonia CXR images. We compared our modelagainst standalone ResNet34, Vision Transformer, and Swin Transformer Tinybaseline models using identical training procedures. Extensive dataaugmentation, Adam optimization, learning rate warmup, and decay were employed.The fusion model achieved a state-of-the-art accuracy of 94.87%, surpassing thebaselines. We also attained excellent sensitivity, specificity, kappa score,and positive predictive value. Confusion matrix analysis confirms fewermisclassifications. The ResNet34 and Vision Transformer combination enablesjointly learning robust features from CNNs and Transformer paradigms. Thismodel-level ensemble technique effectively integrates their complementarystrengths for enhanced pneumonia classification.</description><author>Sandeep Angara, Nishith Reddy Mannuru, Aashrith Mannuru, Sharath Thirunagaru</author><pubDate>Thu, 04 Jan 2024 16:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02358v1</guid></item><item><title>Fit-NGP: Fitting Object Models to Neural Graphics Primitives</title><link>http://arxiv.org/abs/2401.02357v1</link><description>Accurate 3D object pose estimation is key to enabling many roboticapplications that involve challenging object interactions. In this work, weshow that the density field created by a state-of-the-art efficient radiancefield reconstruction method is suitable for highly accurate and robust poseestimation for objects with known 3D models, even when they are very small andwith challenging reflective surfaces. We present a fully automatic object poseestimation system based on a robot arm with a single wrist-mounted camera,which can scan a scene from scratch, detect and estimate the 6-Degrees ofFreedom (DoF) poses of multiple objects within a couple of minutes ofoperation. Small objects such as bolts and nuts are estimated with accuracy onorder of 1mm.</description><author>Marwan Taher, Ignacio Alzugaray, Andrew J. Davison</author><pubDate>Thu, 04 Jan 2024 16:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02357v1</guid></item><item><title>Audiovisual Masked Autoencoders</title><link>http://arxiv.org/abs/2212.05922v3</link><description>Can we leverage the audiovisual information already present in video toimprove self-supervised representation learning? To answer this question, westudy various pretraining architectures and objectives within the maskedautoencoding framework, motivated by the success of similar methods in naturallanguage and image understanding. We show that we can achieve significantimprovements on audiovisual downstream classification tasks, surpassing thestate-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage ouraudiovisual pretraining scheme for multiple unimodal downstream tasks using asingle audiovisual pretrained model. We additionally demonstrate thetransferability of our representations, achieving state-of-the-art audiovisualresults on Epic Kitchens without pretraining specifically for this dataset.</description><author>Mariana-Iuliana Georgescu, Eduardo Fonseca, Radu Tudor Ionescu, Mario Lucic, Cordelia Schmid, Anurag Arnab</author><pubDate>Thu, 04 Jan 2024 16:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05922v3</guid></item><item><title>Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences</title><link>http://arxiv.org/abs/2401.01641v2</link><description>Machine learning models underpin many modern financial systems for use casessuch as fraud detection and churn prediction. Most are based on supervisedlearning with hand-engineered features, which relies heavily on theavailability of labelled data. Large self-supervised generative models haveshown tremendous success in natural language processing and computer vision,yet so far they haven't been adapted to multivariate time series of financialtransactions. In this paper, we present a generative pretraining method thatcan be used to obtain contextualised embeddings of financial transactions.Benchmarks on public datasets demonstrate that it outperforms state-of-the-artself-supervised methods on a range of downstream tasks. We additionally performlarge-scale pretraining of an embedding model using a corpus of data from 180issuing banks containing 5.1 billion transactions and apply it to the cardfraud detection problem on hold-out datasets. The embedding model significantlyimproves value detection rate at high precision thresholds and transfers wellto out-of-domain distributions.</description><author>Piotr Skalski, David Sutton, Stuart Burrell, Iker Perez, Jason Wong</author><pubDate>Thu, 04 Jan 2024 16:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01641v2</guid></item><item><title>A Survey Analyzing Generalization in Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2401.02349v1</link><description>Reinforcement learning research obtained significant success and attentionwith the utilization of deep neural networks to solve problems in highdimensional state or action spaces. While deep reinforcement learning policiesare currently being deployed in many different fields from medical applicationsto self driving vehicles, there are still ongoing questions the field is tryingto answer on the generalization capabilities of deep reinforcement learningpolicies. In this paper, we will outline the fundamental reasons why deepreinforcement learning policies encounter overfitting problems that limit theirrobustness and generalization capabilities. Furthermore, we will formalize andunify the diverse solution approaches to increase generalization, and overcomeoverfitting in state-action value functions. We believe our study can provide acompact systematic unified analysis for the current advancements in deepreinforcement learning, and help to construct robust deep neural policies withimproved generalization abilities.</description><author>Ezgi Korkmaz</author><pubDate>Thu, 04 Jan 2024 16:45:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02349v1</guid></item><item><title>Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training</title><link>http://arxiv.org/abs/2401.02347v1</link><description>Image captioning aims at generating descriptive and meaningful textualdescriptions of images, enabling a broad range of vision-language applications.Prior works have demonstrated that harnessing the power of Contrastive ImageLanguage Pre-training (CLIP) offers a promising approach to achieving zero-shotcaptioning, eliminating the need for expensive caption annotations. However,the widely observed modality gap in the latent space of CLIP harms theperformance of zero-shot captioning by breaking the alignment between pairedimage-text features. To address this issue, we conduct an analysis on the CLIPlatent space which leads to two findings. Firstly, we observe that the CLIP'svisual feature of image subregions can achieve closer proximity to the pairedcaption due to the inherent information loss in text descriptions. In addition,we show that the modality gap between a paired image-text can be empiricallymodeled as a zero-mean Gaussian distribution. Motivated by the findings, wepropose a novel zero-shot image captioning framework with text-only training toreduce the modality gap. In particular, we introduce a subregion featureaggregation to leverage local region information, which produces a compactvisual representation for matching text representation. Moreover, weincorporate a noise injection and CLIP reranking strategy to boost captioningperformance. We also extend our framework to build a zero-shot VQA pipeline,demonstrating its generality. Through extensive experiments on commoncaptioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show thatour method achieves remarkable performance improvements. Code is available athttps://github.com/Artanic30/MacCap.</description><author>Longtian Qiu, Shan Ning, Xuming He</author><pubDate>Thu, 04 Jan 2024 16:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02347v1</guid></item><item><title>Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition</title><link>http://arxiv.org/abs/2401.02344v1</link><description>Although deep learning-based algorithms have demonstrated excellentperformance in automated emotion recognition via electroencephalogram (EEG)signals, variations across brain signal patterns of individuals can diminishthe model's effectiveness when applied across different subjects. Whiletransfer learning techniques have exhibited promising outcomes, they stillencounter challenges related to inadequate feature representations and mayoverlook the fact that source subjects themselves can possess distinctcharacteristics. In this work, we propose a multi-source domain adaptationapproach with a transformer-based feature generator (MSDA-TF) designed toleverage information from multiple sources. The proposed feature generatorretains convolutional layers to capture shallow spatial, temporal, and spectralEEG data representations, while self-attention mechanisms extract globaldependencies within these features. During the adaptation process, we group thesource subjects based on correlation values and aim to align the moments of thetarget subject with each source as well as within the sources. MSDA-TF isvalidated on the SEED dataset and is shown to yield promising results.</description><author>Shadi Sartipi, Mujdat Cetin</author><pubDate>Thu, 04 Jan 2024 16:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02344v1</guid></item><item><title>Evasive Hardware Trojan through Adversarial Power Trace</title><link>http://arxiv.org/abs/2401.02342v1</link><description>The globalization of the Integrated Circuit (IC) supply chain, driven bytime-to-market and cost considerations, has made ICs vulnerable to hardwareTrojans (HTs). Against this threat, a promising approach is to use MachineLearning (ML)-based side-channel analysis, which has the advantage of being anon-intrusive method, along with efficiently detecting HTs under goldenchip-free settings. In this paper, we question the trustworthiness of ML-basedHT detection via side-channel analysis. We introduce a HT obfuscation (HTO)approach to allow HTs to bypass this detection method. Rather thantheoretically misleading the model by simulated adversarial traces, a keyaspect of our approach is the design and implementation of adversarial noise aspart of the circuitry, alongside the HT. We detail HTO methodologies for ASICsand FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly,we found that HTO can be implemented with only a single transistor for ASICdesigns to generate adversarial power traces that can fool the defense with100% efficiency. We also efficiently implemented our approach on a Spartan 6Xilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii)ring-oscillator-based design. Additionally, we assess the efficiency ofcountermeasures like spectral domain analysis, and we show that an adaptiveattacker can still design evasive HTOs by constraining the design with aspectral noise budget. In addition, while adversarial training (AT) offershigher protection against evasive HTs, AT models suffer from a considerableutility loss, potentially rendering them unsuitable for such securityapplication. We believe this research represents a significant step inunderstanding and exploiting ML vulnerabilities in a hardware security context,and we make all resources and designs openly available online:https://dev.d18uu4lqwhbmka.amplifyapp.com</description><author>Behnam Omidi, Khaled N. Khasawneh, Ihsen Alouani</author><pubDate>Thu, 04 Jan 2024 16:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02342v1</guid></item><item><title>Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It</title><link>http://arxiv.org/abs/2312.15228v2</link><description>Fake news detection models are critical to countering disinformation but canbe manipulated through adversarial attacks. In this position paper, we analyzehow an attacker can compromise the performance of an online learning detectoron specific news content without being able to manipulate the original targetnews. In some contexts, such as social networks, where the attacker cannotexert complete control over all the information, this scenario can indeed bequite plausible. Therefore, we show how an attacker could potentially introducepoisoning data into the training data to manipulate the behavior of an onlinelearning method. Our initial findings reveal varying susceptibility of logisticregression models based on complexity and attack type.</description><author>Federico Siciliano, Luca Maiano, Lorenzo Papa, Federica Baccini, Irene Amerini, Fabrizio Silvestri</author><pubDate>Thu, 04 Jan 2024 16:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15228v2</guid></item><item><title>Linguistic Profiling of Deepfakes: An Open Database for Next-Generation Deepfake Detection</title><link>http://arxiv.org/abs/2401.02335v1</link><description>The emergence of text-to-image generative models has revolutionized the fieldof deepfakes, enabling the creation of realistic and convincing visual contentdirectly from textual descriptions. However, this advancement presentsconsiderably greater challenges in detecting the authenticity of such content.Existing deepfake detection datasets and methods often fall short ineffectively capturing the extensive range of emerging deepfakes and offeringsatisfactory explanatory information for detection. To address the significantissue, this paper introduces a deepfake database (DFLIP-3K) for the developmentof convincing and explainable deepfake detection. It encompasses about 300Kdiverse deepfake samples from approximately 3K generative models, which boaststhe largest number of deepfake models in the literature. Moreover, it collectsaround 190K linguistic footprints of these deepfakes. The two distinguishedfeatures enable DFLIP-3K to develop a benchmark that promotes progress inlinguistic profiling of deepfakes, which includes three sub-tasks namelydeepfake detection, model identification, and prompt prediction. The deepfakemodel and prompt are two essential components of each deepfake, and thusdissecting them linguistically allows for an invaluable exploration oftrustworthy and interpretable evidence in deepfake detection, which we believeis the key for the next-generation deepfake detection. Furthermore, DFLIP-3K isenvisioned as an open database that fosters transparency and encouragescollaborative efforts to further enhance its growth. Our extensive experimentson the developed benchmark verify that our DFLIP-3K database is capable ofserving as a standardized resource for evaluating and comparinglinguistic-based deepfake detection, identification, and prompt predictiontechniques.</description><author>Yabin Wang, Zhiwu Huang, Zhiheng Ma, Xiaopeng Hong</author><pubDate>Thu, 04 Jan 2024 16:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02335v1</guid></item><item><title>Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models</title><link>http://arxiv.org/abs/2401.02333v1</link><description>The conventional use of the Retrieval-Augmented Generation (RAG) architecturehas proven effective for retrieving information from diverse documents.However, challenges arise in handling complex table queries, especially withinPDF documents containing intricate tabular structures.This research introducesan innovative approach to enhance the accuracy of complex table queries inRAG-based systems. Our methodology involves storing PDFs in the retrievaldatabase and extracting tabular content separately. The extracted tablesundergo a process of context enrichment, concatenating headers withcorresponding values. To ensure a comprehensive understanding of the enricheddata, we employ a fine-tuned version of the Llama-2-chat language model forsummarisation within the RAG architecture. Furthermore, we augment the tabulardata with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.This enriched data is then fed into the retrieval database alongside otherPDFs. Our approach aims to significantly improve the precision of complex tablequeries, offering a promising solution to a longstanding challenge ininformation retrieval.</description><author>Uday Allu, Biddwan Ahmed, Vishesh Tripathi</author><pubDate>Thu, 04 Jan 2024 16:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02333v1</guid></item><item><title>LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model</title><link>http://arxiv.org/abs/2401.02330v1</link><description>In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficientmulti-modal assistant that harnesses the power of the recently advanced smalllanguage model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks anotable advancement in the realm of compact multi-modal models. It demonstratesthat even smaller language models, with as few as 2.7B parameters, caneffectively engage in intricate dialogues that integrate both textual andvisual elements, provided they are trained with high-quality corpora. Our modeldelivers commendable performance on publicly available benchmarks thatencompass visual comprehension, reasoning, and knowledge-based perception.Beyond its remarkable performance in multi-modal dialogue tasks, our modelopens new avenues for applications in time-sensitive environments and systemsthat require real-time interaction, such as embodied agents. It highlights thepotential of smaller language models to achieve sophisticated levels ofunderstanding and interaction, while maintaining greater resourceefficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.</description><author>Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang</author><pubDate>Thu, 04 Jan 2024 16:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02330v1</guid></item><item><title>Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning</title><link>http://arxiv.org/abs/2401.02329v1</link><description>Data heterogeneity, characterized by disparities in local data distributionacross clients, poses a significant challenge in federated learning.Substantial efforts have been devoted to addressing the heterogeneity in locallabel distribution. As minority classes suffer from worse accuracy due tooverfitting on local imbalanced data, prior methods often incorporateclass-balanced learning techniques during local training. Despite the improvedmean accuracy across all classes, we observe that empty classes-referring tocategories absent from a client's data distribution-are still not wellrecognized. This paper introduces FedED, a novel approach in heterogeneousfederated learning that integrates both empty-class distillation and logitsuppression simultaneously. Specifically, empty-class distillation leveragesknowledge distillation during local training on each client to retain essentialinformation related to empty classes from the global model. Moreover, logitsuppression directly penalizes network logits for non-label classes,effectively addressing misclassifications in minority classes that may bebiased toward majority classes. Extensive experiments validate the efficacy ofFedED, surpassing previous state-of-the-art methods across diverse datasetswith varying degrees of label distribution shift.</description><author>Kuangpu Guo, Yuhe Ding, Jian Liang, Ran He, Zilei Wang, Tieniu Tan</author><pubDate>Thu, 04 Jan 2024 16:06:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02329v1</guid></item><item><title>ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation</title><link>http://arxiv.org/abs/2401.02326v1</link><description>In the realm of artificial intelligence, the emergence of foundation models,backed by high computing capabilities and extensive data, has beenrevolutionary. Segment Anything Model (SAM), built on the Vision Transformer(ViT) model with millions of parameters and vast training dataset SA-1B, excelsin various segmentation scenarios relying on its significance of semanticinformation and generalization ability. Such achievement of visual foundationmodel stimulates continuous researches on specific downstream tasks in computervision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt thehigh-performing SAM for landcover classification on space-borne SyntheticAperture Radar (SAR) images. The proposed CWSAM freezes most of SAM'sparameters and incorporates lightweight adapters for parameter efficientfine-tuning, and a classwise mask decoder is designed to achieve semanticsegmentation task. This adapt-tuning method allows for efficient landcoverclassification of SAR images, balancing the accuracy with computational demand.In addition, the task specific input module injects low frequency informationof SAR images by MLP-based layers to improve the model performance. Compared toconventional state-of-the-art semantic segmentation algorithms by extensiveexperiments, CWSAM showcases enhanced performance with fewer computingresources, highlighting the potential of leveraging foundational models likeSAM for specific downstream tasks in the SAR domain. The source code isavailable at: https://github.com/xypu98/CWSAM.</description><author>Xinyang Pu, Hecheng Jia, Linghao Zheng, Feng Wang, Feng Xu</author><pubDate>Thu, 04 Jan 2024 15:54:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02326v1</guid></item><item><title>A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning</title><link>http://arxiv.org/abs/2401.02325v1</link><description>Distributional Reinforcement Learning (RL) estimates return distributionmainly by learning quantile values via minimizing the quantile Huber lossfunction, entailing a threshold parameter often selected heuristically or viahyperparameter search, which may not generalize well and can be suboptimal.This paper introduces a generalized quantile Huber loss function derived fromWasserstein distance (WD) calculation between Gaussian distributions, capturingnoise in predicted (current) and target (Bellman-updated) quantile values.Compared to the classical quantile Huber loss, this innovative loss functionenhances robustness against outliers. Notably, the classical Huber lossfunction can be seen as an approximation of our proposed loss, enablingparameter adjustment by approximating the amount of noise in the data duringthe learning process. Empirical tests on Atari games, a common application indistributional RL, and a recent hedging strategy using distributional RL,validate the effectiveness of our proposed loss function and its potential forparameter adjustments in distributional RL.</description><author>Parvin Malekzadeh, Konstantinos N. Plataniotis, Zissis Poulos, Zeyu Wang</author><pubDate>Thu, 04 Jan 2024 15:51:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02325v1</guid></item><item><title>From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion</title><link>http://arxiv.org/abs/2204.03842v3</link><description>While weakly supervised multi-view face reconstruction (MVR) is garneringincreased attention, one critical issue still remains open: how to effectivelyfuse multiple image information to reconstruct high-precision 3D models. Inthis regard, we propose a novel model called Deep Fusion MVR (DF-MVR) anddesign a multi-view encoding to single decoding framework with skipconnections, able to extract, integrate, and compensate deep features withattention from multi-view images. Furthermore, we adopt the involution kernelto enrich deep fusion features with channel features. In addition, we developthe face parse network to learn, identify, and emphasize the critical commonface area within multi-view images. Experiments on Pixel-Face and Bosphorusdatasets indicate the superiority of our model. Without 3D annotation, DF-MVRachieves 5.2% and 3.0% RMSE improvement over the existing weakly supervisedMVRs respectively on Pixel-Face and Bosphorus dataset. Code will be availablepublicly at https://github.com/weiguangzhao/DF_MVR.</description><author>Weiguang Zhao, Chaolong Yang, Jianan Ye, Rui Zhang, Yuyao Yan, Xi Yang, Bin Dong, Amir Hussain, Kaizhu Huang</author><pubDate>Thu, 04 Jan 2024 15:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.03842v3</guid></item><item><title>Multi-Agent Context Learning Strategy for Interference-Aware Beam Allocation in mmWave Vehicular Communications</title><link>http://arxiv.org/abs/2401.02323v1</link><description>Millimeter wave (mmWave) has been recognized as one of key technologies for5G and beyond networks due to its potential to enhance channel bandwidth andnetwork capacity. The use of mmWave for various applications includingvehicular communications has been extensively discussed. However, applyingmmWave to vehicular communications faces challenges of high mobility nodes andnarrow coverage along the mmWave beams. Due to high mobility in dense networks,overlapping beams can cause strong interference which leads to performancedegradation. As a remedy, beam switching capability in mmWave can be utilized.Then, frequent beam switching and cell change become inevitable to manageinterference, which increase computational and signalling complexity. In orderto deal with the complexity in interference control, we develop a new strategycalled Multi-Agent Context Learning (MACOL), which utilizes Contextual Banditto manage interference while allocating mmWave beams to serve vehicles in thenetwork. Our approach demonstrates that by leveraging knowledge of neighbouringbeam status, the machine learning agent can identify and avoid potentialinterfering transmissions to other ongoing transmissions. Furthermore, we showthat even under heavy traffic loads, our proposed MACOL strategy is able tomaintain low interference levels at around 10%.</description><author>Abdulkadir Kose, Haeyoung Lee, Chuan Heng Foh, Mohammad Shojafar</author><pubDate>Thu, 04 Jan 2024 15:43:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02323v1</guid></item><item><title>BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model</title><link>http://arxiv.org/abs/2401.02317v1</link><description>In this paper, we address the challenge of image resolution variation for theSegment Anything Model (SAM). SAM, known for its zero-shot generalizability,exhibits a performance degradation when faced with datasets with varying imagesizes. Previous approaches tend to resize the image to a fixed size or adoptstructure modifications, hindering the preservation of SAM's rich priorknowledge. Besides, such task-specific tuning necessitates a completeretraining of the model, which is cost-expensive and unacceptable fordeployment in the downstream tasks. In this paper, we reformulate this issue asa length extrapolation problem, where token sequence length varies whilemaintaining a consistent patch size for images of different sizes. To this end,we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM'sadaptability to varying image resolutions while eliminating the need forstructure modifications. Firstly, we introduce a new scaling factor to ensureconsistent magnitude in the attention layer's dot product values when the tokensequence length changes. Secondly, we present a bias-mode attention mask thatallows each token to prioritize neighboring information, mitigating the impactof untrained distant information. Our BA-SAM demonstrates efficacy in twoscenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability tosignificantly mitigate performance degradation in the zero-shot setting andachieve state-of-the-art performance with minimal fine-tuning. Furthermore, wepropose a generalized model and benchmark, showcasing BA-SAM's generalizabilityacross all four datasets simultaneously.</description><author>Yiran Song, Qianyu Zhou, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma</author><pubDate>Thu, 04 Jan 2024 15:34:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02317v1</guid></item><item><title>SuperEdge: Towards a Generalization Model for Self-Supervised Edge Detection</title><link>http://arxiv.org/abs/2401.02313v1</link><description>Edge detection is a fundamental technique in various computer vision tasks.Edges are indeed effectively delineated by pixel discontinuity and can offerreliable structural information even in textureless areas. State-of-the-artheavily relies on pixel-wise annotations, which are labor-intensive and subjectto inconsistencies when acquired manually. In this work, we propose a novelself-supervised approach for edge detection that employs a multi-level,multi-homography technique to transfer annotations from synthetic to real-worlddatasets. To fully leverage the generated edge annotations, we developedSuperEdge, a streamlined yet efficient model capable of concurrently extractingedges at pixel-level and object-level granularity. Thanks to self-supervisedtraining, our method eliminates the dependency on manual annotated edge labels,thereby enhancing its generalizability across diverse datasets. Comparativeevaluations reveal that SuperEdge advances edge detection, demonstratingimprovements of 4.9% in ODS and 3.3% in OIS over the existing STEdge method onBIPEDv2.</description><author>Leng Kai, Zhang Zhijie, Liu Jie, Zed Boukhers, Sui Wei, Cong Yang, Li Zhijun</author><pubDate>Thu, 04 Jan 2024 15:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02313v1</guid></item><item><title>The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)</title><link>http://arxiv.org/abs/2305.17033v4</link><description>Pediatric tumors of the central nervous system are the most common cause ofcancer-related death in children. The five-year survival rate for high-gradegliomas in children is less than 20\%. Due to their rarity, the diagnosis ofthese entities is often delayed, their treatment is mainly based on historictreatment concepts, and clinical trials require multi-institutionalcollaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is alandmark community benchmark event with a successful history of 12 years ofresource creation for the segmentation and analysis of adult glioma. Here wepresent the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, whichrepresents the first BraTS challenge focused on pediatric brain tumors withdata acquired across multiple international consortia dedicated to pediatricneuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses onbenchmarking the development of volumentric segmentation algorithms forpediatric brain glioma through standardized quantitative performance evaluationmetrics utilized across the BraTS 2023 cluster of challenges. Models gainingknowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) trainingdata will be evaluated on separate validation and unseen test mpMRI dataofhigh-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023challenge brings together clinicians and AI/imaging scientists to lead tofaster development of automated segmentation techniques that could benefitclinical trials, and ultimately the care of children with brain tumors.</description><author>Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan Haldar, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole, Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen, Blaise V Jones, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier, Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez, Jeffrey D Rudie, Ibraheem Salman Shaikh, Lubdha M. Shah, Nakul Sheth, Russel</author><pubDate>Thu, 04 Jan 2024 15:10:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17033v4</guid></item><item><title>Perceptual Musical Features for Interpretable Audio Tagging</title><link>http://arxiv.org/abs/2312.11234v2</link><description>In the age of music streaming platforms, the task of automatically taggingmusic audio has garnered significant attention, driving researchers to devisemethods aimed at enhancing performance metrics on standard datasets. Mostrecent approaches rely on deep neural networks, which, despite their impressiveperformance, possess opacity, making it challenging to elucidate their outputfor a given input. While the issue of interpretability has been emphasized inother fields like medicine, it has not received attention in music-relatedtasks. In this study, we explored the relevance of interpretability in thecontext of automatic music tagging. We constructed a workflow that incorporatesthree different information extraction techniques: a) leveraging symbolicknowledge, b) utilizing auxiliary deep neural networks, and c) employing signalprocessing to extract perceptual features from audio files. These features weresubsequently used to train an interpretable machine-learning model for tagprediction. We conducted experiments on two datasets, namely the MTG-Jamendodataset and the GTZAN dataset. Our method surpassed the performance of baselinemodels in both tasks and, in certain instances, demonstrated competitivenesswith the current state-of-the-art. We conclude that there are use cases wherethe deterioration in performance is outweighed by the value ofinterpretability.</description><author>Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Giorgos Stamou</author><pubDate>Thu, 04 Jan 2024 15:09:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11234v2</guid></item><item><title>On Model Compression for Neural Networks: Framework, Algorithm, and Convergence Guarantee</title><link>http://arxiv.org/abs/2303.06815v2</link><description>Model compression is a crucial part of deploying neural networks (NNs),especially when the memory and storage of computing devices are limited in manyapplications. This paper focuses on two model compression techniques: low-rankapproximation and weight pruning in neural networks, which are very popularnowadays. However, training NN with low-rank approximation and weight pruningalways suffers significant accuracy loss and convergence issues. In this paper,a holistic framework is proposed for model compression from a novel perspectiveof nonconvex optimization by designing an appropriate objective function. Then,we introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve thenonconvex optimization. One advantage of our algorithm is that an efficientiteration scheme can be derived with closed-form, which is gradient-free.Therefore, our algorithm will not suffer from vanishing/exploding gradientproblems. Furthermore, with the Kurdyka-{\L}ojasiewicz (K{\L}) property of ourobjective function, we show that our algorithm globally converges to a criticalpoint at the rate of O(1/k), where k denotes the number of iterations. Lastly,extensive experiments with tensor train decomposition and weight pruningdemonstrate the efficiency and superior performance of the proposed framework.Our code implementation is available at https://github.com/ChenyangLi-97/NN-BCD</description><author>Chenyang Li, Jihoon Chung, Biao Cai, Haimin Wang, Xianlian Zhou, Bo Shen</author><pubDate>Thu, 04 Jan 2024 15:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06815v2</guid></item><item><title>Learning to Generate Training Datasets for Robust Semantic Segmentation</title><link>http://arxiv.org/abs/2308.02535v3</link><description>Semantic segmentation methods have advanced significantly. Still, theirrobustness to real-world perturbations and object types not seen duringtraining remains a challenge, particularly in safety-critical applications. Wepropose a novel approach to improve the robustness of semantic segmentationtechniques by leveraging the synergy between label-to-image generators andimage-to-label segmentation models. Specifically, we design Robusta, a novelrobust conditional generative adversarial network to generate realistic andplausible perturbed images that can be used to train reliable segmentationmodels. We conduct in-depth studies of the proposed generative model, assessthe performance and robustness of the downstream segmentation network, anddemonstrate that our approach can significantly enhance the robustness in theface of real-world perturbations, distribution shifts, and out-of-distributionsamples. Our results suggest that this approach could be valuable insafety-critical applications, where the reliability of perception modules suchas semantic segmentation is of utmost importance and comes with a limitedcomputational budget in inference. We release our code athttps://github.com/ENSTA-U2IS/robusta.</description><author>Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Shihao Zhang, Andrei Bursuc, Angela Yao, Gianni Franchi</author><pubDate>Thu, 04 Jan 2024 15:06:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02535v3</guid></item><item><title>A Survey and Benchmark of Automatic Surface Reconstruction from Point Clouds</title><link>http://arxiv.org/abs/2301.13656v2</link><description>We present a comprehensive survey and benchmark of both traditional andlearning-based methods for surface reconstruction from point clouds. This taskis particularly challenging for real-world acquisitions due to factors likenoise, outliers, non-uniform sampling, and missing data. Traditional approachesoften simplify the problem by imposing handcrafted priors on either the inputpoint clouds or the resulting surface, a process that can necessitate tedioushyperparameter tuning. Conversely, deep learning models have the capability todirectly learn the properties of input point clouds and desired surfaces fromdata. We study the influence of these handcrafted and learned priors on theprecision and robustness of surface reconstruction techniques. We evaluatevarious time-tested and contemporary methods in a standardized manner. Whenboth trained and evaluated on point clouds with identical characteristics, thelearning-based models consistently produce superior surfaces compared to theirtraditional counterparts$\unicode{x2013}$even in scenarios involving novelshape categories. However, traditional methods demonstrate greater resilienceto the diverse array of point cloud anomalies commonly found in real-world 3Dacquisitions. For the benefit of the research community, we make our code anddatasets available, inviting further enhancements to learning-based surfacereconstruction. This can be accessed athttps://github.com/raphaelsulzer/dsr-benchmark .</description><author>Raphael Sulzer, Renaud Marlet, Bruno Vallet, Loic Landrieu</author><pubDate>Thu, 04 Jan 2024 14:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13656v2</guid></item><item><title>TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection</title><link>http://arxiv.org/abs/2401.02309v1</link><description>Video moment retrieval (MR) and highlight detection (HD) based on naturallanguage queries are two highly related tasks, which aim to obtain relevantmoments within videos and highlight scores of each video clip. Recently,several methods have been devoted to building DETR-based networks to solve bothMR and HD jointly. These methods simply add two separate task heads aftermulti-modal feature extraction and feature interaction, achieving goodperformance. Nevertheless, these approaches underutilize the reciprocalrelationship between two tasks. In this paper, we propose a task-reciprocaltransformer based on DETR (TR-DETR) that focuses on exploring the inherentreciprocity between MR and HD. Specifically, a local-global multi-modalalignment module is first built to align features from diverse modalities intoa shared latent space. Subsequently, a visual feature refinement is designed toeliminate query-irrelevant information from visual features for modalinteraction. Finally, a task cooperation module is constructed to refine theretrieval pipeline and the highlight score prediction process by utilizing thereciprocity between MR and HD. Comprehensive experiments on QVHighlights,Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existingstate-of-the-art methods. Codes are available at\url{https://github.com/mingyao1120/TR-DETR}.</description><author>Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie</author><pubDate>Thu, 04 Jan 2024 14:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02309v1</guid></item><item><title>Approximating the Shapley Value without Marginal Contributions</title><link>http://arxiv.org/abs/2302.00736v4</link><description>The Shapley value, which is arguably the most popular approach for assigninga meaningful contribution value to players in a cooperative game, has recentlybeen used intensively in explainable artificial intelligence. Itsmeaningfulness is due to axiomatic properties that only the Shapley valuesatisfies, which, however, comes at the expense of an exact computation growingexponentially with the number of agents. Accordingly, a number of works aredevoted to the efficient approximation of the Shapley value, most of themrevolve around the notion of an agent's marginal contribution. In this paper,we propose with SVARM and Stratified SVARM two parameter-free anddomain-independent approximation algorithms based on a representation of theShapley value detached from the notion of marginal contribution. We proveunmatched theoretical guarantees regarding their approximation quality andprovide empirical results including synthetic games as well as commonexplainability use cases comparing ourselves with state-of-the-art methods.</description><author>Patrick Kolpaczki, Viktor Bengs, Maximilian Muschalik, Eyke Hüllermeier</author><pubDate>Thu, 04 Jan 2024 14:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00736v4</guid></item><item><title>Robust Physics Informed Neural Networks</title><link>http://arxiv.org/abs/2401.02300v1</link><description>We introduce a Robust version of the Physics-Informed Neural Networks(RPINNs) to approximate the Partial Differential Equations (PDEs) solution.Standard Physics Informed Neural Networks (PINN) takes into account thegoverning physical laws described by PDE during the learning process. Thenetwork is trained on a data set that consists of randomly selected points inthe physical domain and its boundary. PINNs have been successfully applied tosolve various problems described by PDEs with boundary conditions. The lossfunction in traditional PINNs is based on the strong residuals of the PDEs.This loss function in PINNs is generally not robust with respect to the trueerror. The loss function in PINNs can be far from the true error, which makesthe training process more difficult. In particular, we do not know if thetraining process has already converged to the solution with the requiredaccuracy. This is especially true if we do not know the exact solution, so wecannot estimate the true error during the training. This paper introduces adifferent way of defining the loss function. It incorporates the residual andthe inverse of the Gram matrix, computed using the energy norm. We test ourRPINN algorithm on two Laplace problems and one advection-diffusion problem intwo spatial dimensions. We conclude that RPINN is a robust method. The proposedloss coincides well with the true error of the solution, as measured in theenergy norm. Thus, we know if our training process goes well, and we know whento stop the training to obtain the neural network approximation of the solutionof the PDE with the true error of required accuracy.</description><author>Marcin Łoś, Maciej Paszyński</author><pubDate>Thu, 04 Jan 2024 14:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02300v1</guid></item><item><title>Are LLMs Robust for Spoken Dialogues?</title><link>http://arxiv.org/abs/2401.02297v1</link><description>Large Pre-Trained Language Models have demonstrated state-of-the-artperformance in different downstream tasks, including dialogue state trackingand end-to-end response generation. Nevertheless, most of the publiclyavailable datasets and benchmarks on task-oriented dialogues focus on writtenconversations. Consequently, the robustness of the developed models to spokeninteractions is unknown. In this work, we have evaluated the performance ofLLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to thelack of proper spoken dialogue datasets, we have automatically transcribed adevelopment set of spoken dialogues with a state-of-the-art ASR engine. We havecharacterized the ASR-error types and their distributions and simulated theseerrors in a large dataset of dialogues. We report the intrinsic (perplexity)and extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 modelsin two subtasks of response generation and dialogue state tracking,respectively. The results show that LLMs are not robust to spoken noise bydefault, however, fine-tuning/training such models on a proper dataset ofspoken TODs can result in a more robust performance.</description><author>Seyed Mahed Mousavi, Gabriel Roccabruna, Simone Alghisi, Massimo Rizzoli, Mirco Ravanelli, Giuseppe Riccardi</author><pubDate>Thu, 04 Jan 2024 14:36:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02297v1</guid></item><item><title>Training Single-Layer Morphological Perceptron Using Convex-Concave Programming</title><link>http://arxiv.org/abs/2401.02296v1</link><description>This paper concerns the training of a single-layer morphological perceptronusing disciplined convex-concave programming (DCCP). We introduce an algorithmreferred to as K-DDCCP, which combines the existing single-layer morphologicalperceptron (SLMP) model proposed by Ritter and Urcid with the weighteddisciplined convex-concave programming (WDCCP) algorithm by Charisopoulos andMaragos. The proposed training algorithm leverages the disciplinedconvex-concave procedure (DCCP) and formulates a non-convex optimizationproblem for binary classification. To tackle this problem, the constraints areexpressed as differences of convex functions, enabling the application of theDCCP package. The experimental results confirm the effectiveness of the K-DDCCPalgorithm in solving binary classification problems. Overall, this workcontributes to the field of morphological neural networks by proposing analgorithm that extends the capabilities of the SLMP model.</description><author>Iara Cunha, Marcos Eduardo Valle</author><pubDate>Thu, 04 Jan 2024 14:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02296v1</guid></item><item><title>GridFormer: Point-Grid Transformer for Surface Reconstruction</title><link>http://arxiv.org/abs/2401.02292v1</link><description>Implicit neural networks have emerged as a crucial technology in 3D surfacereconstruction. To reconstruct continuous surfaces from discrete point clouds,encoding the input points into regular grid features (plane or volume) has beencommonly employed in existing approaches. However, these methods typically usethe grid as an index for uniformly scattering point features. Compared with theirregular point features, the regular grid features may sacrifice somereconstruction details but improve efficiency. To take full advantage of thesetwo types of features, we introduce a novel and high-efficiency attentionmechanism between the grid and point features named Point-Grid Transformer(GridFormer). This mechanism treats the grid as a transfer point connecting thespace and point cloud. Our method maximizes the spatial expressiveness of gridfeatures and maintains computational efficiency. Furthermore, optimizingpredictions over the entire space could potentially result in blurredboundaries. To address this issue, we further propose a boundary optimizationstrategy incorporating margin binary cross-entropy loss and boundary sampling.This approach enables us to achieve a more precise representation of the objectstructure. Our experiments validate that our method is effective andoutperforms the state-of-the-art approaches under widely used benchmarks byproducing more precise geometry reconstructions. The code is available athttps://github.com/list17/GridFormer.</description><author>Shengtao Li, Ge Gao, Yudong Liu, Yu-Shen Liu, Ming Gu</author><pubDate>Thu, 04 Jan 2024 14:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02292v1</guid></item><item><title>HawkRover: An Autonomous mmWave Vehicular Communication Testbed with Multi-sensor Fusion and Deep Learning</title><link>http://arxiv.org/abs/2401.01822v2</link><description>Connected and automated vehicles (CAVs) have become a transformativetechnology that can change our daily life. Currently, millimeter-wave (mmWave)bands are identified as the promising CAV connectivity solution. While it canprovide high data rate, their realization faces many challenges such as highattenuation during mmWave signal propagation and mobility management. Existingsolution has to initiate pilot signal to measure channel information, thenapply signal processing to calculate the best narrow beam towards the receiverend to guarantee sufficient signal power. This process takes significantoverhead and time, hence not suitable for vehicles. In this study, we proposean autonomous and low-cost testbed to collect extensive co-located mmWavesignal and other sensors data such as LiDAR (Light Detection and Ranging),cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWavevehicular communications. Intuitively, these sensors can build a 3D map aroundthe vehicle and signal propagation path can be estimated, eliminating iterativethe process via pilot signals. This multimodal data fusion, together with AI,is expected to bring significant advances in ``connected'' research.</description><author>Ethan Zhu, Haijian Sun, Mingyue Ji</author><pubDate>Thu, 04 Jan 2024 14:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01822v2</guid></item><item><title>Uncertainty in GNN Learning Evaluations: A Comparison Between Measures for Quantifying Randomness in GNN Community Detection</title><link>http://arxiv.org/abs/2312.09015v2</link><description>(1) The enhanced capability of Graph Neural Networks (GNNs) in unsupervisedcommunity detection of clustered nodes is attributed to their capacity toencode both the connectivity and feature information spaces of graphs. Theidentification of latent communities holds practical significance in variousdomains, from social networks to genomics. Current real-world performancebenchmarks are perplexing due to the multitude of decisions influencing GNNevaluations for this task. (2) Three metrics are compared to assess theconsistency of algorithm rankings in the presence of randomness. Theconsistency and quality of performance between the results under ahyperparameter optimisation with the default hyperparameters is evaluated. (3)The results compare hyperparameter optimisation with default hyperparameters,revealing a significant performance loss when neglecting hyperparameterinvestigation. A comparison of metrics indicates that ties in ranks cansubstantially alter the quantification of randomness. (4) Ensuring adherence tothe same evaluation criteria may result in notable differences in the reportedperformance of methods for this task. The $W$ Randomness coefficient, based onthe Wasserstein distance, is identified as providing the most robust assessmentof randomness.</description><author>William Leeney, Ryan McConville</author><pubDate>Thu, 04 Jan 2024 14:23:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09015v2</guid></item><item><title>Path-based Explanation for Knowledge Graph Completion</title><link>http://arxiv.org/abs/2401.02290v1</link><description>Graph Neural Networks (GNNs) have achieved great success in Knowledge GraphCompletion (KGC) by modelling how entities and relations interact in recentyears. However, the explanation of the predicted facts has not caught thenecessary attention. Proper explanations for the results of GNN-based KGCmodels increase model transparency and help researchers develop more reliablemodels. Existing practices for explaining KGC tasks rely oninstance/subgraph-based approaches, while in some scenarios, paths can providemore user-friendly and interpretable explanations. Nonetheless, the methods forgenerating path-based explanations for KGs have not been well-explored. Toaddress this gap, we propose Power-Link, the first path-based KGC explainerthat explores GNN-based models. We design a novel simplified graph-poweringtechnique, which enables the generation of path-based explanations with a fullyparallelisable and memory-efficient training scheme. We further introduce threenew metrics for quantitative evaluation of the explanations, together with aqualitative human evaluation. Extensive experiments demonstrate that Power-Linkoutperforms the SOTA baselines in interpretability, efficiency, andscalability.</description><author>Heng Chang, Jiangnan Ye, Alejo Lopez Avila, Jinhua Du, Jia Li</author><pubDate>Thu, 04 Jan 2024 14:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02290v1</guid></item><item><title>SGFormer: Simplifying and Empowering Transformers for Large-Graph Representations</title><link>http://arxiv.org/abs/2306.10759v4</link><description>Learning representations on large-sized graphs is a long-standing challengedue to the inter-dependence nature involved in massive data points.Transformers, as an emerging class of foundation encoders for graph-structureddata, have shown promising performance on small graphs due to its globalattention capable of capturing all-pair influence beyond neighboring nodes.Even so, existing approaches tend to inherit the spirit of Transformers inlanguage and vision tasks, and embrace complicated models by stacking deepmulti-head attentions. In this paper, we critically demonstrate that even usinga one-layer attention can bring up surprisingly competitive performance acrossnode property prediction benchmarks where node numbers range fromthousand-level to billion-level. This encourages us to rethink the designphilosophy for Transformers on large graphs, where the global attention is acomputation overhead hindering the scalability. We frame the proposed scheme asSimplified Graph Transformers (SGFormer), which is empowered by a simpleattention model that can efficiently propagate information among arbitrarynodes in one layer. SGFormer requires none of positional encodings,feature/graph pre-processing or augmented loss. Empirically, SGFormersuccessfully scales to the web-scale graph ogbn-papers100M and yields up to141x inference acceleration over SOTA Transformers on medium-sized graphs.Beyond current results, we believe the proposed methodology alone enlightens anew technical path of independent interest for building Transformers on largegraphs.</description><author>Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang, Yatao Bian, Junchi Yan</author><pubDate>Thu, 04 Jan 2024 14:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10759v4</guid></item><item><title>Learning Discretized Neural Networks under Ricci Flow</title><link>http://arxiv.org/abs/2302.03390v4</link><description>In this paper, we study Discretized Neural Networks (DNNs) composed oflow-precision weights and activations, which suffer from either infinite orzero gradients due to the non-differentiable discrete function during training.Most training-based DNNs in such scenarios employ the standard Straight-ThroughEstimator (STE) to approximate the gradient w.r.t. discrete values. However,the use of STE introduces the problem of gradient mismatch, arising fromperturbations in the approximated gradient. To address this problem, this paperreveals that this mismatch can be interpreted as a metric perturbation in aRiemannian manifold, viewed through the lens of duality theory. Building oninformation geometry, we construct the Linearly Nearly Euclidean (LNE) manifoldfor DNNs, providing a background for addressing perturbations. By introducing apartial differential equation on metrics, i.e., the Ricci flow, we establishthe dynamical stability and convergence of the LNE metric with the $L^2$-normperturbation. In contrast to previous perturbation theories with convergencerates in fractional powers, the metric perturbation under the Ricci flowexhibits exponential decay in the LNE manifold. Experimental results acrossvarious datasets demonstrate that our method achieves superior and more stableperformance for DNNs compared to other representative training-based methods.</description><author>Jun Chen, Hanwen Chen, Mengmeng Wang, Guang Dai, Ivor W. Tsang, Yong Liu</author><pubDate>Thu, 04 Jan 2024 14:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03390v4</guid></item><item><title>Smoothing Methods for Automatic Differentiation Across Conditional Branches</title><link>http://arxiv.org/abs/2310.03585v2</link><description>Programs involving discontinuities introduced by control flow constructs suchas conditional branches pose challenges to mathematical optimization methodsthat assume a degree of smoothness in the objective function's responsesurface. Smooth interpretation (SI) is a form of abstract interpretation thatapproximates the convolution of a program's output with a Gaussian kernel, thussmoothing its output in a principled manner. Here, we combine SI with automaticdifferentiation (AD) to efficiently compute gradients of smoothed programs. Incontrast to AD across a regular program execution, these gradients also capturethe effects of alternative control flow paths. The combination of SI with ADenables the direct gradient-based parameter synthesis for branching programs,allowing for instance the calibration of simulation models or their combinationwith neural network models in machine learning pipelines. We detail the effectsof the approximations made for tractability in SI and propose a novel MonteCarlo estimator that avoids the underlying assumptions by estimating thesmoothed programs' gradients through a combination of AD and sampling. UsingDiscoGrad, our tool for automatically translating simple C++ programs to asmooth differentiable form, we perform an extensive evaluation. We compare thecombination of SI with AD and our Monte Carlo estimator to existinggradient-free and stochastic methods on four non-trivial and originallydiscontinuous problems ranging from classical simulation-based optimization toneural network-driven control. While the optimization progress with theSI-based estimator depends on the complexity of the program's control flow, ourMonte Carlo estimator is competitive in all problems, exhibiting the fastestconvergence by a substantial margin in our highest-dimensional problem.</description><author>Justin N. Kreikemeyer, Philipp Andelfinger</author><pubDate>Thu, 04 Jan 2024 14:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03585v2</guid></item><item><title>Distillation-based fabric anomaly detection</title><link>http://arxiv.org/abs/2401.02287v1</link><description>Unsupervised texture anomaly detection has been a concerning topic in a vastamount of industrial processes. Patterned textures inspection, particularly inthe context of fabric defect detection, is indeed a widely encountered usecase. This task involves handling a diverse spectrum of colors and textiletypes, encompassing a wide range of fabrics. Given the extensive variability incolors, textures, and defect types, fabric defect detection poses a complex andchallenging problem in the field of patterned textures inspection. In thisarticle, we propose a knowledge distillation-based approach tailoredspecifically for addressing the challenge of unsupervised anomaly detection intextures resembling fabrics. Our method aims to redefine the recentlyintroduced reverse distillation approach, which advocates for anencoder-decoder design to mitigate classifier bias and to prevent the studentfrom reconstructing anomalies. In this study, we present a new reversedistillation technique for the specific task of fabric defect detection. Ourapproach involves a meticulous design selection that strategically highlightshigh-level features. To demonstrate the capabilities of our approach both interms of performance and inference speed, we conducted a series of experimentson multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongsideconducting experiments on a dataset acquired from a textile manufacturingfacility. The main contributions of this paper are the following: a robusttexture anomaly detector utilizing a reverse knowledge-distillation techniquesuitable for both anomaly detection and domain generalization and a noveldataset encompassing a diverse range of fabrics and defects.</description><author>Simon Thomine, Hichem Snoussi</author><pubDate>Thu, 04 Jan 2024 14:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02287v1</guid></item><item><title>DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace</title><link>http://arxiv.org/abs/2401.02283v1</link><description>Software development in the aerospace domain requires adhering to strict,high-quality standards. While there exist regulatory guidelines for commercialsoftware in this domain (e.g., ARP-4754 and DO-178), these do not apply tosoftware with deep neural network (DNN) components. Consequently, it is unclearhow to allow aerospace systems to benefit from the deep learning revolution.Our work here seeks to address this challenge with a novel, output-centricapproach for DNN certification. Our method employs statistical verificationtechniques, and has the key advantage of being able to flag specific inputs forwhich the DNN's output may be unreliable - so that they may be later inspectedby a human expert. To achieve this, our method conducts a statistical analysisof the DNN's predictions for other, nearby inputs, in order to detectinconsistencies. This is in contrast to existing techniques, which typicallyattempt to certify the entire DNN, as opposed to individual outputs. Our methoduses the DNN as a black-box, and makes no assumptions about its topology. Wehope that this work constitutes another step towards integrating DNNs insafety-critical applications - especially in the aerospace domain, where highstandards of quality and reliability are crucial.</description><author>Guy Katz, Natan Levy, Idan Refaeli, Raz Yerushalmi</author><pubDate>Thu, 04 Jan 2024 14:01:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02283v1</guid></item><item><title>Dynamically Masked Discriminator for Generative Adversarial Networks</title><link>http://arxiv.org/abs/2306.07716v3</link><description>Training Generative Adversarial Networks (GANs) remains a challengingproblem. The discriminator trains the generator by learning the distribution ofreal/generated data. However, the distribution of generated data changesthroughout the training process, which is difficult for the discriminator tolearn. In this paper, we propose a novel method for GANs from the viewpoint ofonline continual learning. We observe that the discriminator model, trained onhistorically generated data, often slows down its adaptation to the changes inthe new arrival generated data, which accordingly decreases the quality ofgenerated results. By treating the generated data in training as a stream, wepropose to detect whether the discriminator slows down the learning of newknowledge in generated data. Therefore, we can explicitly enforce thediscriminator to learn new knowledge fast. Particularly, we propose a newdiscriminator, which automatically detects its retardation and then dynamicallymasks its features, such that the discriminator can adaptively learn thetemporally-vary distribution of generated data. Experimental results show ourmethod outperforms the state-of-the-art approaches.</description><author>Wentian Zhang, Haozhe Liu, Bing Li, Jinheng Xie, Yawen Huang, Yuexiang Li, Yefeng Zheng, Bernard Ghanem</author><pubDate>Thu, 04 Jan 2024 13:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07716v3</guid></item><item><title>PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DOF Object Pose Dataset Generation</title><link>http://arxiv.org/abs/2401.02281v1</link><description>We introduce Physically Enhanced Gaussian Splatting Simulation System(PEGASUS) for 6DOF object pose dataset generation, a versatile datasetgenerator based on 3D Gaussian Splatting. Environment and objectrepresentations can be easily obtained using commodity cameras to reconstructwith Gaussian Splatting. PEGASUS allows the composition of new scenes bymerging the respective underlying Gaussian Splatting point cloud of anenvironment with one or multiple objects. Leveraging a physics engine enablesthe simulation of natural object placement within a scene through interactionbetween meshes extracted for the objects and the environment. Consequently, anextensive amount of new scenes - static or dynamic - can be created bycombining different environments and objects. By rendering scenes from variousperspectives, diverse data points such as RGB images, depth maps, semanticmasks, and 6DoF object poses can be extracted. Our study demonstrates thattraining on data generated by PEGASUS enables pose estimation networks tosuccessfully transfer from synthetic data to real-world data. Moreover, weintroduce the Ramen dataset, comprising 30 Japanese cup noodle items. Thisdataset includes spherical scans that captures images from both objecthemisphere and the Gaussian Splatting reconstruction, making them compatiblewith PEGASUS.</description><author>Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae</author><pubDate>Thu, 04 Jan 2024 13:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02281v1</guid></item><item><title>Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case</title><link>http://arxiv.org/abs/2401.02278v1</link><description>The enormous demand for seafood products has led to exploitation of marineresources and near-extinction of some species. In particular, overfishing isone the main issues in sustainable marine development. In alignment with theprotection of marine resources and sustainable fishing, this study proposes toadvance fish classification techniques that support identifying protected fishspecies using state-of-the-art machine learning. We use a custom modificationof the MobileNet model to design a lightweight classifier called M-MobileNetthat is capable of running on limited hardware. As part of the study, wecompiled a labeled dataset of 37,462 images of fish found in the waters of theIndonesian archipelago. The proposed model is trained on the dataset toclassify images of the captured fish into their species and giverecommendations on whether they are consumable or not. Our modified MobileNetmodel uses only 50\% of the top layer parameters with about 42% GTX 860Mutility and achieves up to 97% accuracy in fish classification and determiningits consumability. Given the limited computing capacity available on manyfishing vessels, the proposed model provides a practical solution to on-sitefish classification. In addition, synchronized implementation of the proposedmodel on multiple vessels can supply valuable information about the movementand location of different species of fish.</description><author>Febrian Kurniawan, Gandeva Bayu Satrya, Firuz Kamalov</author><pubDate>Thu, 04 Jan 2024 13:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02278v1</guid></item><item><title>Universal Approximation Theorem for Vector- and Hypercomplex-Valued Neural Networks</title><link>http://arxiv.org/abs/2401.02277v1</link><description>The universal approximation theorem states that a neural network with onehidden layer can approximate continuous functions on compact sets with anydesired precision. This theorem supports using neural networks for variousapplications, including regression and classification tasks. Furthermore, it isvalid for real-valued neural networks and some hypercomplex-valued neuralnetworks such as complex-, quaternion-, tessarine-, and Clifford-valued neuralnetworks. However, hypercomplex-valued neural networks are a type ofvector-valued neural network defined on an algebra with additional algebraic orgeometric properties. This paper extends the universal approximation theoremfor a wide range of vector-valued neural networks, includinghypercomplex-valued models as particular instances. Precisely, we introduce theconcept of non-degenerate algebra and state the universal approximation theoremfor neural networks defined on such algebras.</description><author>Marcos Eduardo Valle, Wington L. Vital, Guilherme Vieira</author><pubDate>Thu, 04 Jan 2024 13:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02277v1</guid></item><item><title>ShapeAug: Occlusion Augmentation for Event Camera Data</title><link>http://arxiv.org/abs/2401.02274v1</link><description>Recently, Dynamic Vision Sensors (DVSs) sparked a lot of interest due totheir inherent advantages over conventional RGB cameras. These advantagesinclude a low latency, a high dynamic range and a low energy consumption.Nevertheless, the processing of DVS data using Deep Learning (DL) methodsremains a challenge, particularly since the availability of event training datais still limited. This leads to a need for event data augmentation techniquesin order to improve accuracy as well as to avoid over-fitting on the trainingdata. Another challenge especially in real world automotive applications isocclusion, meaning one object is hindering the view onto the object behind it.In this paper, we present a novel event data augmentation approach, whichaddresses this problem by introducing synthetic events for randomly movingobjects in a scene. We test our method on multiple DVS classification datasets,resulting in an relative improvement of up to 6.5 % in top1-accuracy. Moreover,we apply our augmentation technique on the real world Gen1 Automotive EventDataset for object detection, where we especially improve the detection ofpedestrians by up to 5 %.</description><author>Katharina Bendig, René Schuster, Didier Stricker</author><pubDate>Thu, 04 Jan 2024 13:49:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02274v1</guid></item><item><title>Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle Perspective</title><link>http://arxiv.org/abs/2302.09457v2</link><description>Adversarial machine learning (AML) studies the adversarial phenomenon ofmachine learning, which may make inconsistent or unexpected predictions withhumans. Some paradigms have been recently developed to explore this adversarialphenomenon occurring at different stages of a machine learning system, such asbackdoor attack occurring at the pre-training, in-training and inference stage;weight attack occurring at the post-training, deployment and inference stage;adversarial attack occurring at the inference stage. However, although theseadversarial paradigms share a common goal, their developments are almostindependent, and there is still no big picture of AML. In this work, we aim toprovide a unified perspective to the AML community to systematically review theoverall progress of this field. We firstly provide a general definition aboutAML, and then propose a unified mathematical framework to covering existingattack paradigms. According to the proposed unified framework, we build a fulltaxonomy to systematically categorize and review existing representativemethods for each paradigm. Besides, using this unified framework, it is easy tofigure out the connections and differences among different attack paradigms,which may inspire future researchers to develop more advanced attack paradigms.Finally, to facilitate the viewing of the built taxonomy and the relatedliterature in adversarial machine learning, we further provide a website, \ie,\url{http://adversarial-ml.com}, where the taxonomies and literature will becontinuously updated.</description><author>Baoyuan Wu, Zihao Zhu, Li Liu, Qingshan Liu, Zhaofeng He, Siwei Lyu</author><pubDate>Thu, 04 Jan 2024 13:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09457v2</guid></item><item><title>How do media talk about the Covid-19 pandemic? Metaphorical thematic clustering in Italian online newspapers</title><link>http://arxiv.org/abs/2204.02106v2</link><description>The contribution presents a study on figurative language of the first monthsof the COVID-19 crisis in Italian online newspapers. Particularly, we contrasttopics and metaphorical language used by journalists in the first and secondphase of the government response to the pandemic in Spring 2020. The analysisis conducted on a journalistic corpus collected between February 24th and June3rd, 2020. The analysis is performed using both quantitative and qualitativeapproaches, combining Structural Topic Modelling (Roberts et al. 2016),Conceptual Metaphor Theory (Lakoff &amp; Johnson, 1980), and qualitative-corpusbased metaphor analysis (Charteris-Black, 2004). We find a significant shift intopics discussed across Phase 1 and Phase 2, and interesting overlaps intopic-specific metaphors. Using qualitative corpus analysis, we present a morein-depth case study discussing metaphorical collocations of the topics ofEconomy and Society</description><author>Lucia Busso, Ottavia Tordini</author><pubDate>Thu, 04 Jan 2024 13:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.02106v2</guid></item><item><title>Few-shot Adaptation of Multi-modal Foundation Models: A Survey</title><link>http://arxiv.org/abs/2401.01736v2</link><description>Multi-modal (vision-language) models, such as CLIP, are replacing traditionalsupervised pre-training models (e.g., ImageNet-based pre-training) as the newgeneration of visual foundation models. These models with robust and alignedsemantic representations learned from billions of internet image-text pairs andcan be applied to various downstream tasks in a zero-shot manner. However, insome fine-grained domains like medical imaging and remote sensing, theperformance of multi-modal foundation models often leaves much to be desired.Consequently, many researchers have begun to explore few-shot adaptationmethods for these models, gradually deriving three main technical approaches:1) prompt-based methods, 2) adapter-based methods, and 3) externalknowledge-based methods. Nevertheless, this rapidly developing field hasproduced numerous results without a comprehensive survey to systematicallyorganize the research progress. Therefore, in this survey, we introduce andanalyze the research advancements in few-shot adaptation methods formulti-modal models, summarizing commonly used datasets and experimental setups,and comparing the results of different methods. In addition, due to the lack ofreliable theoretical support for existing methods, we derive the few-shotadaptation generalization error bound for multi-modal models. The theoremreveals that the generalization error of multi-modal foundation models isconstrained by three factors: domain gap, model capacity, and sample size.Based on this, we propose three possible solutions from the following aspects:1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptiveknowledge utilization.</description><author>Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai, Xiaocong Zhou, Delong Chen</author><pubDate>Thu, 04 Jan 2024 13:24:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01736v2</guid></item><item><title>Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation</title><link>http://arxiv.org/abs/2401.02258v1</link><description>Missingness is ubiquitous in multivariate time series and poses an obstacleto reliable downstream analysis. Although recurrent network imputation achievedthe SOTA, existing models do not scale to deep architectures that canpotentially alleviate issues arising in complex data. Moreover, imputationcarries the risk of biased estimations of the ground truth. Yet, confidence inthe imputed values is always unmeasured or computed post hoc from model output.We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimatesmissing values and their associated uncertainty in heterogeneous multivariatetime series. By jointly representing feature-wise correlations and temporaldynamics, we adopt a self attention mechanism, along with an effective residualcomponent, to achieve a deep recurrent neural network with good imputationperformance and stable convergence. We also leverage self-supervised metriclearning to boost performance by optimizing sample similarity. Finally, wetransform DEARI into a Bayesian neural network through a novel Bayesianmarginalization strategy to produce stochastic DEARI, which outperforms itsdeterministic equivalent. Experiments show that DEARI surpasses the SOTA indiverse imputation tasks using real-world datasets, namely air quality control,healthcare and traffic.</description><author>Linglong Qian, Zina Ibrahim, Richard Dobson</author><pubDate>Thu, 04 Jan 2024 13:21:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02258v1</guid></item><item><title>STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning</title><link>http://arxiv.org/abs/2304.07520v2</link><description>Centralized Training with Decentralized Execution (CTDE) has been proven tobe an effective paradigm in cooperative multi-agent reinforcement learning(MARL). One of the major challenges is credit assignment, which aims to creditagents by their contributions. While prior studies have shown great success,their methods typically fail to work in episodic reinforcement learningscenarios where global rewards are revealed only at the end of the episode.They lack the functionality to model complicated relations of the delayedglobal reward in the temporal dimension and suffer from inefficiencies. Totackle this, we introduce Spatial-Temporal Attention with Shapley (STAS), anovel method that learns credit assignment in both temporal and spatialdimensions. It first decomposes the global return back to each time step, thenutilizes the Shapley Value to redistribute the individual payoff from thedecomposed global reward. To mitigate the computational complexity of theShapley Value, we introduce an approximation of marginal contribution andutilize Monte Carlo sampling to estimate it. We evaluate our method on an Alice&amp; Bob example and MPE environments across different scenarios. Our resultsdemonstrate that our method effectively assigns spatial-temporal credit,outperforming all state-of-the-art baselines.</description><author>Sirui Chen, Zhaowei Zhang, Yaodong Yang, Yali Du</author><pubDate>Thu, 04 Jan 2024 13:18:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07520v2</guid></item><item><title>Knowledge Enhanced Conditional Imputation for Healthcare Time-series</title><link>http://arxiv.org/abs/2312.16713v2</link><description>This study presents a novel approach to addressing the challenge of missingdata in multivariate time series, with a particular focus on the complexitiesof healthcare data. Our Conditional Self-Attention Imputation (CSAI) model,grounded in a transformer-based framework, introduces a conditional hiddenstate initialization tailored to the intricacies of medical time series data.This methodology diverges from traditional imputation techniques byspecifically targeting the imbalance in missing data distribution, a crucialaspect often overlooked in healthcare datasets. By integrating advancedknowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts tothe distinct patterns of missing data in Electronic Health Records (EHRs).</description><author>Linglong Qian, Zina Ibrahim, Hugh Logan Ellis, Ao Zhang, Yuezhou Zhang, Tao Wang, Richard Dobson</author><pubDate>Thu, 04 Jan 2024 13:17:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16713v2</guid></item><item><title>Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain Dialogue Systems</title><link>http://arxiv.org/abs/2401.02256v1</link><description>Open-domain dialogue systems have started to engage in continuousconversations with humans. Those dialogue systems are required to be adjustedto the human interlocutor and evaluated in terms of their perspective. However,it is questionable whether the current automatic evaluation methods canapproximate the interlocutor's judgments. In this study, we analyzed andexamined what features are needed in an automatic response evaluator from theinterlocutor's perspective. The first experiment on the Hazumi dataset revealedthat interlocutor awareness plays a critical role in making automatic responseevaluation correlate with the interlocutor's judgments. The second experimentusing massive conversations on X (formerly Twitter) confirmed that dialoguecontinuity prediction can train an interlocutor-aware response evaluatorwithout human feedback while revealing the difficulty in evaluating generatedresponses compared to human responses.</description><author>Yuma Tsuta, Naoki Yoshinaga, Shoetsu Sato, Masashi Toyoda</author><pubDate>Thu, 04 Jan 2024 13:15:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02256v1</guid></item><item><title>Balancing Continual Learning and Fine-tuning for Human Activity Recognition</title><link>http://arxiv.org/abs/2401.02255v1</link><description>Wearable-based Human Activity Recognition (HAR) is a key task inhuman-centric machine learning due to its fundamental understanding of humanbehaviours. Due to the dynamic nature of human behaviours, continual learningpromises HAR systems that are tailored to users' needs. However, because of thedifficulty in collecting labelled data with wearable sensors, existingapproaches that focus on supervised continual learning have limitedapplicability, while unsupervised continual learning methods only handlerepresentation learning while delaying classifier training to a later stage.This work explores the adoption and adaptation of CaSSLe, a continualself-supervised learning model, and Kaizen, a semi-supervised continuallearning model that balances representation learning and down-streamclassification, for the task of wearable-based HAR. These schemes re-purposecontrastive learning for knowledge retention and, Kaizen combines that withself-training in a unified scheme that can leverage unlabelled and labelleddata for continual learning. In addition to comparing state-of-the-artself-supervised continual learning schemes, we further investigated theimportance of different loss terms and explored the trade-off between knowledgeretention and learning from new tasks. In particular, our extensive evaluationdemonstrated that the use of a weighting factor that reflects the ratio betweenlearned and new classes achieves the best overall trade-off in continuallearning.</description><author>Chi Ian Tang, Lorena Qendro, Dimitris Spathis, Fahim Kawsar, Akhil Mathur, Cecilia Mascolo</author><pubDate>Thu, 04 Jan 2024 13:11:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02255v1</guid></item><item><title>L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages</title><link>http://arxiv.org/abs/2401.02254v1</link><description>In this work, we introduce L3Cube-IndicNews, a multilingual textclassification corpus aimed at curating a high-quality dataset for Indianregional languages, with a specific focus on news headlines and articles. Wehave centered our work on 10 prominent Indic languages, including Hindi,Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, andPunjabi. Each of these news datasets comprises 10 or more classes of newsarticles. L3Cube-IndicNews offers 3 distinct datasets tailored to handledifferent document lengths that are classified as: Short HeadlinesClassification (SHC) dataset containing the news headline and news category,Long Document Classification (LDC) dataset containing the whole news articleand the news category, and Long Paragraph Classification (LPC) containingsub-articles of the news and the news category. We maintain consistent labelingacross all 3 datasets for in-depth length-based analysis. We evaluate each ofthese Indic language datasets using 4 different models including monolingualBERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. Thisresearch contributes significantly to expanding the pool of available textclassification datasets and also makes it possible to develop topicclassification models for Indian regional languages. This also serves as anexcellent resource for cross-lingual analysis owing to the high overlap oflabels among languages. The datasets and models are shared publicly athttps://github.com/l3cube-pune/indic-nlp</description><author>Aishwarya Mirashi, Srushti Sonavane, Purva Lingayat, Tejas Padhiyar, Raviraj Joshi</author><pubDate>Thu, 04 Jan 2024 13:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02254v1</guid></item><item><title>Policy-regularized Offline Multi-objective Reinforcement Learning</title><link>http://arxiv.org/abs/2401.02244v1</link><description>In this paper, we aim to utilize only offline trajectory data to train apolicy for multi-objective RL. We extend the offline policy-regularized method,a widely-adopted approach for single-objective offline RL problems, into themulti-objective setting in order to achieve the above goal. However, suchmethods face a new challenge in offline MORL settings, namely thepreference-inconsistent demonstration problem. We propose two solutions to thisproblem: 1) filtering out preference-inconsistent demonstrations viaapproximating behavior preferences, and 2) adopting regularization techniqueswith high policy expressiveness. Moreover, we integrate thepreference-conditioned scalarized update method into policy-regularized offlineRL, in order to simultaneously learn a set of policies using a single policynetwork, thus reducing the computational cost induced by the training of alarge number of individual policies for various preferences. Finally, weintroduce Regularization Weight Adaptation to dynamically determine appropriateregularization weights for arbitrary target preferences during deployment.Empirical results on various multi-objective datasets demonstrate thecapability of our approach in solving offline MORL problems.</description><author>Qian Lin, Chao Yu, Zongkai Liu, Zifan Wu</author><pubDate>Thu, 04 Jan 2024 12:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02244v1</guid></item><item><title>Slot-guided Volumetric Object Radiance Fields</title><link>http://arxiv.org/abs/2401.02241v1</link><description>We present a novel framework for 3D object-centric representation learning.Our approach effectively decomposes complex scenes into individual objects froma single image in an unsupervised fashion. This method, called slot-guidedVolumetric Object Radiance Fields (sVORF), composes volumetric object radiancefields with object slots as a guidance to implement unsupervised 3D scenedecomposition. Specifically, sVORF obtains object slots from a single image viaa transformer module, maps these slots to volumetric object radiance fieldswith a hypernetwork and composes object radiance fields with the guidance ofobject slots at a 3D location. Moreover, sVORF significantly reduces memoryrequirement due to small-sized pixel rendering during training. We demonstratethe effectiveness of our approach by showing top results in scene decompositionand generation tasks of complex synthetic datasets (e.g., Room-Diverse).Furthermore, we also confirm the potential of sVORF to segment objects inreal-world scenes (e.g., the LLFF dataset). We hope our approach can providepreliminary understanding of the physical world and help ease future researchin 3D object-centric representation learning.</description><author>Di Qi, Tong Yang, Xiangyu Zhang</author><pubDate>Thu, 04 Jan 2024 12:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02241v1</guid></item><item><title>U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for Time Series Forecasting</title><link>http://arxiv.org/abs/2401.02236v1</link><description>Time series forecasting is a crucial task in various domains. Caused byfactors such as trends, seasonality, or irregular fluctuations, time seriesoften exhibits non-stationary. It obstructs stable feature propagation throughdeep layers, disrupts feature distributions, and complicates learning datadistribution changes. As a result, many existing models struggle to capture theunderlying patterns, leading to degraded forecasting performance. In thisstudy, we tackle the challenge of non-stationarity in time series forecastingwith our proposed framework called U-Mixer. By combining Unet and Mixer,U-Mixer effectively captures local temporal dependencies between differentpatches and channels separately to avoid the influence of distributionvariations among channels, and merge low- and high-levels features to obtaincomprehensive data representations. The key contribution is a novelstationarity correction method, explicitly restoring data distribution byconstraining the difference in stationarity between the data before and aftermodel processing to restore the non-stationarity information, while ensuringthe temporal dependencies are preserved. Through extensive experiments onvarious real-world time series datasets, U-Mixer demonstrates its effectivenessand robustness, and achieves 14.5\% and 7.7\% improvements overstate-of-the-art (SOTA) methods.</description><author>Xiang Ma, Xuemei Li, Lexin Fang, Tianlong Zhao, Caiming Zhang</author><pubDate>Thu, 04 Jan 2024 12:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02236v1</guid></item><item><title>Trajectory-Oriented Policy Optimization with Sparse Rewards</title><link>http://arxiv.org/abs/2401.02225v1</link><description>Deep reinforcement learning (DRL) remains challenging in tasks with sparserewards. These sparse rewards often only indicate whether the task is partiallyor fully completed, meaning that many exploration actions must be performedbefore the agent obtains useful feedback. Hence, most existing DRL algorithmsfail to learn feasible policies within a reasonable time frame. To overcomethis problem, we develop an approach that exploits offline demonstrationtrajectories for faster and more efficient online RL in sparse reward settings.Our key insight is that by regarding offline demonstration trajectories asguidance, instead of imitating them, our method learns a policy whosestate-action visitation marginal distribution matches that of offlinedemonstrations. Specifically, we introduce a novel trajectory distance based onmaximum mean discrepancy (MMD) and formulate policy optimization as adistance-constrained optimization problem. Then, we show that thisdistance-constrained optimization problem can be reduced into a policy-gradientalgorithm with shaped rewards learned from offline demonstrations. The proposedalgorithm is evaluated on extensive discrete and continuous control tasks withsparse and deceptive rewards. The experimental results indicate that ourproposed algorithm is significantly better than the baseline methods regardingdiverse exploration and learning the optimal policy.</description><author>Guojian Wang, Faguo Wu, Xiao Zhang</author><pubDate>Thu, 04 Jan 2024 12:21:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02225v1</guid></item><item><title>Expressive Speech-driven Facial Animation with controllable emotions</title><link>http://arxiv.org/abs/2301.02008v2</link><description>It is in high demand to generate facial animation with high realism, but itremains a challenging task. Existing approaches of speech-driven facialanimation can produce satisfactory mouth movement and lip synchronization, butshow weakness in dramatic emotional expressions and flexibility in emotioncontrol. This paper presents a novel deep learning-based approach forexpressive facial animation generation from speech that can exhibitwide-spectrum facial expressions with controllable emotion type and intensity.We propose an emotion controller module to learn the relationship between theemotion variations (e.g., types and intensity) and the corresponding facialexpression parameters. It enables emotion-controllable facial animation, wherethe target expression can be continuously adjusted as desired. The qualitativeand quantitative evaluations show that the animation generated by our method isrich in facial emotional expressiveness while retaining accurate lip movement,outperforming other state-of-the-art methods.</description><author>Yutong Chen, Junhong Zhao, Wei-Qiang Zhang</author><pubDate>Thu, 04 Jan 2024 12:20:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.02008v2</guid></item><item><title>Covert Channel Attack to Federated Learning Systems</title><link>http://arxiv.org/abs/2104.10561v2</link><description>Federated learning (FL) goes beyond traditional, centralized machine learningby distributing model training among a large collection of edge clients. Theseclients cooperatively train a global, e.g., cloud-hosted, model withoutdisclosing their local, private training data. The global model is then sharedamong all the participants which use it for local predictions. In this paper,we put forward a novel attacker model aiming at turning FL systems into covertchannels to implement a stealth communication infrastructure. The mainintuition is that, during federated training, a malicious sender can poison theglobal model by submitting purposely crafted examples. Although the effect ofthe model poisoning is negligible to other participants, and does not alter theoverall model performance, it can be observed by a malicious receiver and usedto transmit a single bit.</description><author>Gabriele Costa, Fabio Pinelli, Simone Soderi, Gabriele Tolomei</author><pubDate>Thu, 04 Jan 2024 11:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.10561v2</guid></item><item><title>Dynamic programming by polymorphic semiring algebraic shortcut fusion</title><link>http://arxiv.org/abs/2107.01752v5</link><description>Dynamic programming (DP) is an algorithmic design paradigm for the efficient,exact solution of otherwise intractable, combinatorial problems. However, DPalgorithm design is often presented in an ad-hoc manner. It is sometimesdifficult to justify algorithm correctness. To address this issue, this paperpresents a rigorous algebraic formalism for systematically deriving DPalgorithms, based on semiring polymorphism. We start with a specification,construct an algorithm to compute the required solution which is self-evidentlycorrect because it exhaustively generates and evaluates all possible solutionsmeeting the specification. We then derive, through the use of shortcut fusion,an implementation of this algorithm which is both efficient and correct. Wealso demonstrate how, with the use of semiring lifting, the specification canbe augmented with combinatorial constraints, showing how these constraints canbe fused with the algorithm. We furthermore demonstrate how existing DPalgorithms for a given combinatorial problem can be abstracted from theiroriginal context and re-purposed. This approach can be applied to the full scope of combinatorial problemsexpressible in terms of semirings. This includes, for example: optimalprobability and Viterbi decoding, probabilistic marginalization, logicalinference, fuzzy sets, differentiable softmax, relational and provenancequeries. The approach, building on ideas from the existing literature onconstructive algorithmics, exploits generic properties of polymorphicfunctions, tupling and formal sums and algebraic simplifications arising fromconstraint algebras. We demonstrate the effectiveness of this formalism forsome example applications arising in signal processing, bioinformatics andreliability engineering. Python software implementing these algorithms can bedownloaded from: http://www.maxlittle.net/software/dppolyalg.zip.</description><author>Max A. Little, Xi He, Ugur Kayas</author><pubDate>Thu, 04 Jan 2024 11:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.01752v5</guid></item><item><title>A First Runtime Analysis of the NSGA-II on a Multimodal Problem</title><link>http://arxiv.org/abs/2204.13750v5</link><description>Very recently, the first mathematical runtime analyses of the multi-objectiveevolutionary optimizer NSGA-II have been conducted. We continue this line ofresearch with a first runtime analysis of this algorithm on a benchmark problemconsisting of two multimodal objectives. We prove that if the population size$N$ is at least four times the size of the Pareto front, then the NSGA-II withfour different ways to select parents and bit-wise mutation optimizes theOneJumpZeroJump benchmark with jump size~$2 \le k \le n/4$ in time $O(N n^k)$.When using fast mutation, a recently proposed heavy-tailed mutation operator,this guarantee improves by a factor of $k^{\Omega(k)}$. Overall, this workshows that the NSGA-II copes with the local optima of the OneJumpZeroJumpproblem at least as well as the global SEMO algorithm.</description><author>Benjamin Doerr, Zhongdi Qu</author><pubDate>Thu, 04 Jan 2024 11:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.13750v5</guid></item><item><title>Towards Fully Decoupled End-to-End Person Search</title><link>http://arxiv.org/abs/2309.04967v2</link><description>End-to-end person search aims to jointly detect and re-identify a targetperson in raw scene images with a unified model. The detection task unifies allpersons while the re-id task discriminates different identities, resulting inconflict optimal objectives. Existing works proposed to decouple end-to-endperson search to alleviate such conflict. Yet these methods are stillsub-optimal on one or two of the sub-tasks due to their partially decoupledmodels, which limits the overall person search performance. In this paper, wepropose to fully decouple person search towards optimal person search. Atask-incremental person search network is proposed to incrementally constructan end-to-end model for the detection and re-id sub-task, which decouples themodel architecture for the two sub-tasks. The proposed task-incremental networkallows task-incremental training for the two conflicting tasks. This enablesindependent learning for different objectives thus fully decoupled the modelfor persons earch. Comprehensive experimental evaluations demonstrate theeffectiveness of the proposed fully decoupled models for end-to-end personsearch.</description><author>Pengcheng Zhang, Xiao Bai, Jin Zheng, Xin Ning</author><pubDate>Thu, 04 Jan 2024 11:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04967v2</guid></item><item><title>Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph</title><link>http://arxiv.org/abs/2401.02212v1</link><description>Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph byattaching the time scope. Existing temporal knowledge graph question answering(TKGQA) models solely approach simple questions, owing to the prior assumptionthat each question only contains a single temporal fact with explicit/implicittemporal constraints. Hence, they perform poorly on questions which ownmultiple temporal facts. In this paper, we propose \textbf{\underline{J}}oint\textbf{\underline{M}}ulti \textbf{\underline{F}}acts\textbf{\underline{R}}easoning \textbf{\underline{N}}etwork (JMFRN), to jointlyreasoning multiple temporal facts for accurately answering \emph{complex}temporal questions. Specifically, JMFRN first retrieves question-relatedtemporal facts from TKG for each entity of the given complex question. Forjoint reasoning, we design two different attention (\ie entity-aware andtime-aware) modules, which are suitable for universal settings, to aggregateentities and timestamps information of retrieved facts. Moreover, to filterincorrect type answers, we introduce an additional answer type discriminationtask. Extensive experiments demonstrate our proposed method significantlyoutperforms the state-of-art on the well-known complex temporal questionbenchmark TimeQuestions.</description><author>Rikui Huang, Wei Wei, Xiaoye Qu, Wenfeng Xie, Xianling Mao, Dangyang Chen</author><pubDate>Thu, 04 Jan 2024 11:34:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02212v1</guid></item><item><title>DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models</title><link>http://arxiv.org/abs/2401.02208v1</link><description>We present DIALIGHT, a toolkit for developing and evaluating multilingualTask-Oriented Dialogue (ToD) systems which facilitates systematic evaluationsand comparisons between ToD systems using fine-tuning of Pretrained LanguageModels (PLMs) and those utilising the zero-shot and in-context learningcapabilities of Large Language Models (LLMs). In addition to automaticevaluation, this toolkit features (i) a secure, user-friendly web interface forfine-grained human evaluation at both local utterance level and global dialoguelevel, and (ii) a microservice-based backend, improving efficiency andscalability. Our evaluations reveal that while PLM fine-tuning leads to higheraccuracy and coherence, LLM-based systems excel in producing diverse andlikeable responses. However, we also identify significant challenges of LLMs inadherence to task-specific instructions and generating outputs in multiplelanguages, highlighting areas for future research. We hope this open-sourcedtoolkit will serve as a valuable resource for researchers aiming to develop andproperly evaluate multilingual ToD systems and will lower, currently stillhigh, entry barriers in the field.</description><author>Songbo Hu, Xiaobin Wang, Zhangdie Yuan, Anna Korhonen, Ivan Vulić</author><pubDate>Thu, 04 Jan 2024 11:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02208v1</guid></item><item><title>Robust bilinear factor analysis based on the matrix-variate $t$ distribution</title><link>http://arxiv.org/abs/2401.02203v1</link><description>Factor Analysis based on multivariate $t$ distribution ($t$fa) is a usefulrobust tool for extracting common factors on heavy-tailed or contaminated data.However, $t$fa is only applicable to vector data. When $t$fa is applied tomatrix data, it is common to first vectorize the matrix observations. Thisintroduces two challenges for $t$fa: (i) the inherent matrix structure of thedata is broken, and (ii) robustness may be lost, as vectorized matrix datatypically results in a high data dimension, which could easily lead to thebreakdown of $t$fa. To address these issues, starting from the intrinsic matrixstructure of matrix data, a novel robust factor analysis model, namely bilinearfactor analysis built on the matrix-variate $t$ distribution ($t$bfa), isproposed in this paper. The novelty is that it is capable to simultaneouslyextract common factors for both row and column variables of interest onheavy-tailed or contaminated matrix data. Two efficient algorithms for maximumlikelihood estimation of $t$bfa are developed. Closed-form expression for theFisher information matrix to calculate the accuracy of parameter estimates arederived. Empirical studies are conducted to understand the proposed $t$bfamodel and compare with related competitors. The results demonstrate thesuperiority and practicality of $t$bfa. Importantly, $t$bfa exhibits asignificantly higher breakdown point than $t$fa, making it more suitable formatrix data.</description><author>Xuan Ma, Jianhua Zhao, Changchun Shang, Fen Jiang, Philip L. H. Yu</author><pubDate>Thu, 04 Jan 2024 11:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02203v1</guid></item><item><title>Let There Be Sound: Reconstructing High Quality Speech from Silent Videos</title><link>http://arxiv.org/abs/2308.15256v2</link><description>The goal of this work is to reconstruct high quality speech from lip motionsalone, a task also known as lip-to-speech. A key challenge of lip-to-speechsystems is the one-to-many mapping caused by (1) the existence of homophenesand (2) multiple speech variations, resulting in a mispronounced andover-smoothed speech. In this paper, we propose a novel lip-to-speech systemthat significantly improves the generation quality by alleviating theone-to-many mapping problem from multiple perspectives. Specifically, weincorporate (1) self-supervised speech representations to disambiguatehomophenes, and (2) acoustic variance information to model diverse speechstyles. Additionally, to better solve the aforementioned problem, we employ aflow based post-net which captures and refines the details of the generatedspeech. We perform extensive experiments on two datasets, and demonstrate thatour method achieves the generation quality close to that of real humanutterance, outperforming existing methods in terms of speech naturalness andintelligibility by a large margin. Synthesised samples are available at ourdemo page: https://mm.kaist.ac.kr/projects/LTBS.</description><author>Ji-Hoon Kim, Jaehun Kim, Joon Son Chung</author><pubDate>Thu, 04 Jan 2024 11:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15256v2</guid></item><item><title>LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System</title><link>http://arxiv.org/abs/2401.02199v1</link><description>As the horizon of intelligent transportation expands with the evolution ofAutomated Driving Systems (ADS), ensuring paramount safety becomes moreimperative than ever. Traditional risk assessment methodologies, primarilycrafted for human-driven vehicles, grapple to adequately adapt to themultifaceted, evolving environments of ADS. This paper introduces a frameworkfor real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency ofArtificial Neural Networks (ANNs). Our proposed solution transcends these limitations, drawing upon ANNs, acornerstone of deep learning, to meticulously analyze and categorize riskdimensions using real-time On-board Sensor (OBS) data. This learning-centricapproach not only elevates the ADS's situational awareness but also enrichesits understanding of immediate operational contexts. By dissecting OBS data,the system is empowered to pinpoint its current risk profile, thereby enhancingsafety prospects for onboard passengers and the broader traffic ecosystem. Through this framework, we chart a direction in risk assessment, bridging theconventional voids and enhancing the proficiency of ADS. By utilizing ANNs, ourmethodology offers a perspective, allowing ADS to adeptly navigate and react topotential risk factors, ensuring safer and more informed autonomous journeys.</description><author>Anil Ranjitbhai Patel, Peter Liggesmeyer</author><pubDate>Thu, 04 Jan 2024 11:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02199v1</guid></item><item><title>Nodule detection and generation on chest X-rays: NODE21 Challenge</title><link>http://arxiv.org/abs/2401.02192v1</link><description>Pulmonary nodules may be an early manifestation of lung cancer, the leadingcause of cancer-related deaths among both men and women. Numerous studies haveestablished that deep learning methods can yield high-performance levels in thedetection of lung nodules in chest X-rays. However, the lack of gold-standardpublic datasets slows down the progression of the research and preventsbenchmarking of methods for this task. To address this, we organized a publicresearch challenge, NODE21, aimed at the detection and generation of lungnodules in chest X-rays. While the detection track assesses state-of-the-artnodule detection systems, the generation track determines the utility of nodulegeneration algorithms to augment training data and hence improve theperformance of the detection systems. This paper summarizes the results of theNODE21 challenge and performs extensive additional experiments to examine theimpact of the synthetically generated nodule training images on the detectionalgorithm performance.</description><author>Ecem Sogancioglu, Bram van Ginneken, Finn Behrendt, Marcel Bengs, Alexander Schlaefer, Miron Radu, Di Xu, Ke Sheng, Fabien Scalzo, Eric Marcus, Samuele Papa, Jonas Teuwen, Ernst Th. Scholten, Steven Schalekamp, Nils Hendrix, Colin Jacobs, Ward Hendrix, Clara I Sánchez, Keelin Murphy</author><pubDate>Thu, 04 Jan 2024 10:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02192v1</guid></item><item><title>Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</title><link>http://arxiv.org/abs/2307.05300v3</link><description>Human intelligence thrives on cognitive synergy, where collaboration amongdifferent minds yield superior outcomes compared to isolated individuals. Inthis work, we propose Solo Performance Prompting (SPP), which transforms asingle LLM into a cognitive synergist by engaging in multi-turnself-collaboration with multiple personas. A cognitive synergist is anintelligent agent that collaboratively combines multiple minds' strengths andknowledge to enhance problem-solving in complex tasks. By dynamicallyidentifying and simulating different personas based on task inputs, SPPunleashes the potential of cognitive synergy in LLMs. Our in-depth analysisshows that assigning multiple fine-grained personas in LLMs improvesproblem-solving abilities compared to using a single or fixed number ofpersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,Codenames Collaborative, and Logic Grid Puzzle, encompassing bothknowledge-intensive and reasoning-intensive types. Unlike previous works, suchas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,experimental results demonstrate that SPP effectively reduces factualhallucination, and maintains strong reasoning capabilities. Additionally,comparative experiments show that cognitive synergy only emerges in GPT-4 anddoes not appear in less capable models, such as GPT-3.5-turbo andLlama2-13b-chat, which draws an interesting analogy to human development. Code,data, and prompts can be found at:https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.</description><author>Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji</author><pubDate>Thu, 04 Jan 2024 10:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05300v3</guid></item><item><title>Provably Powerful Graph Neural Networks for Directed Multigraphs</title><link>http://arxiv.org/abs/2306.11586v3</link><description>This paper analyses a set of simple adaptations that transform standardmessage-passing Graph Neural Networks (GNN) into provably powerful directedmultigraph neural networks. The adaptations include multigraph port numbering,ego IDs, and reverse message passing. We prove that the combination of thesetheoretically enables the detection of any directed subgraph pattern. Tovalidate the effectiveness of our proposed adaptations in practice, we conductexperiments on synthetic subgraph detection tasks, which demonstrateoutstanding performance with almost perfect results. Moreover, we apply ourproposed adaptations to two financial crime analysis tasks. We observe dramaticimprovements in detecting money laundering transactions, improving theminority-class F1 score of a standard message-passing GNN by up to 30%, andclosely matching or outperforming tree-based and GNN baselines. Similarlyimpressive results are observed on a real-world phishing detection dataset,boosting three standard GNNs' F1 scores by around 15% and outperforming allbaselines.</description><author>Béni Egressy, Luc von Niederhäusern, Jovan Blanusa, Erik Altman, Roger Wattenhofer, Kubilay Atasu</author><pubDate>Thu, 04 Jan 2024 10:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11586v3</guid></item><item><title>Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench</title><link>http://arxiv.org/abs/2308.03656v3</link><description>Evaluating Large Language Models' (LLMs) anthropomorphic capabilities hasbecome increasingly important in contemporary discourse. Utilizing the emotionappraisal theory from psychology, we propose to evaluate the empathy ability ofLLMs, i.e., how their feelings change when presented with specific situations.After a careful and comprehensive survey, we collect a dataset containing over400 situations that have proven effective in eliciting the eight emotionscentral to our study. Categorizing the situations into 36 factors, we conduct ahuman evaluation involving more than 1,200 subjects worldwide. With the humanevaluation results as references, our evaluation includes five LLMs, coveringboth commercial and open-source models, including variations in model sizes,featuring the latest iterations, such as GPT-4 and LLaMA-2. We find that,despite several misalignments, LLMs can generally respond appropriately tocertain situations. Nevertheless, they fall short in alignment with theemotional behaviors of human beings and cannot establish connections betweensimilar situations. Our collected dataset of situations, the human evaluationresults, and the code of our testing framework, dubbed EmotionBench, is madeopenly accessible via https://github.com/CUHK-ARISE/EmotionBench. We aspire tocontribute to the advancement of LLMs regarding better alignment with theemotional behaviors of human beings, thereby enhancing their utility andapplicability as intelligent assistants.</description><author>Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu</author><pubDate>Thu, 04 Jan 2024 10:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03656v3</guid></item><item><title>Location Aware Modular Biencoder for Tourism Question Answering</title><link>http://arxiv.org/abs/2401.02187v1</link><description>Answering real-world tourism questions that seek Point-of-Interest (POI)recommendations is challenging, as it requires both spatial and non-spatialreasoning, over a large candidate pool. The traditional method of encoding eachpair of question and POI becomes inefficient when the number of candidatesincreases, making it infeasible for real-world applications. To overcome this,we propose treating the QA task as a dense vector retrieval problem, where weencode questions and POIs separately and retrieve the most relevant POIs for aquestion by utilizing embedding space similarity. We use pretrained languagemodels (PLMs) to encode textual information, and train a location encoder tocapture spatial information of POIs. Experiments on a real-world tourism QAdataset demonstrate that our approach is effective, efficient, and outperformsprevious methods across all metrics. Enabled by the dense retrievalarchitecture, we further build a global evaluation baseline, expanding thesearch space by 20 times compared to previous work. We also explore severalfactors that impact on the model's performance through follow-up experiments.Our code and model are publicly available at https://github.com/haonan-li/LAMB.</description><author>Haonan Li, Martin Tomko, Timothy Baldwin</author><pubDate>Thu, 04 Jan 2024 10:39:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02187v1</guid></item><item><title>FairGridSearch: A Framework to Compare Fairness-Enhancing Models</title><link>http://arxiv.org/abs/2401.02183v1</link><description>Machine learning models are increasingly used in critical decision-makingapplications. However, these models are susceptible to replicating or evenamplifying bias present in real-world data. While there are various biasmitigation methods and base estimators in the literature, selecting the optimalmodel for a specific application remains challenging. This paper focuses on binary classification and proposes FairGridSearch, anovel framework for comparing fairness-enhancing models. FairGridSearch enablesexperimentation with different model parameter combinations and recommends thebest one. The study applies FairGridSearch to three popular datasets (Adult,COMPAS, and German Credit) and analyzes the impacts of metric selection, baseestimator choice, and classification threshold on model fairness. The results highlight the significance of selecting appropriate accuracy andfairness metrics for model evaluation. Additionally, different base estimatorsand classification threshold values affect the effectiveness of bias mitigationmethods and fairness stability respectively, but the effects are not consistentacross all datasets. Based on these findings, future research on fairness inmachine learning should consider a broader range of factors when building fairmodels, going beyond bias mitigation methods alone.</description><author>Shih-Chi Ma, Tatiana Ermakova, Benjamin Fabian</author><pubDate>Thu, 04 Jan 2024 10:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02183v1</guid></item></channel></rss>