<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 27 Aug 2025 13:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using a Retrieval-Augmented Model Selection Framework</title><link>http://arxiv.org/abs/2508.14940v2</link><description>Accurate lung cancer risk prediction remains challenging due to substantialvariability across patient populations and clinical settings -- no single modelperforms best for all cohorts. To address this, we propose a personalized lungcancer risk prediction agent that dynamically selects the most appropriatemodel for each patient by combining cohort-specific knowledge with modernretrieval and reasoning techniques. Given a patient's CT scan and structuredmetadata -- including demographic, clinical, and nodule-level features -- theagent first performs cohort retrieval using FAISS-based similarity searchacross nine diverse real-world cohorts to identify the most relevant patientpopulation from a multi-institutional database. Second, a Large Language Model(LLM) is prompted with the retrieved cohort and its associated performancemetrics to recommend the optimal prediction algorithm from a pool of eightrepresentative models, including classical linear risk models (e.g., Mayo,Brock), temporally-aware models (e.g., TD-VIT, DLSTM), and multi-modal computervision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agentpipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic,cohort-aware risk prediction personalized to each patient's profile. Buildingon this architecture, the agent supports flexible and cohort-driven modelselection across diverse clinical populations, offering a practical path towardindividualized risk assessment in real-world lung cancer screening.</description><author>Chongyu Qu, Allen J. Luna, Thomas Z. Li, Junchao Zhu, Junlin Guo, Juming Xiong, Kim L. Sandler, Bennett A. Landman, Yuankai Huo</author><pubDate>Tue, 26 Aug 2025 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14940v2</guid></item><item><title>VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space</title><link>http://arxiv.org/abs/2508.19247v1</link><description>3D local editing of specified regions is crucial for game industry and robotinteraction. Recent methods typically edit rendered multi-view images and thenreconstruct 3D models, but they face challenges in precisely preservingunedited regions and overall coherence. Inspired by structured 3D generativemodels, we propose VoxHammer, a novel training-free approach that performsprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammerfirst predicts its inversion trajectory and obtains its inverted latents andkey-value tokens at each timestep. Subsequently, in the denoising and editingphase, we replace the denoising features of preserved regions with thecorresponding inverted latents and cached key-value tokens. By retaining thesecontextual features, this approach ensures consistent reconstruction ofpreserved areas and coherent integration of edited parts. To evaluate theconsistency of preserved regions, we constructed Edit3D-Bench, ahuman-annotated dataset comprising hundreds of samples, each with carefullylabeled 3D editing regions. Experiments demonstrate that VoxHammersignificantly outperforms existing methods in terms of both 3D consistency ofpreserved regions and overall quality. Our method holds promise forsynthesizing high-quality edited paired data, thereby laying the datafoundation for in-context 3D generation. See our project page athttps://huanngzh.github.io/VoxHammer-Page/.</description><author>Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng</author><pubDate>Tue, 26 Aug 2025 17:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19247v1</guid></item><item><title>Style4D-Bench: A Benchmark Suite for 4D Stylization</title><link>http://arxiv.org/abs/2508.19243v1</link><description>We introduce Style4D-Bench, the first benchmark suite specifically designedfor 4D stylization, with the goal of standardizing evaluation and facilitatingprogress in this emerging area. Style4D-Bench comprises: 1) a comprehensiveevaluation protocol measuring spatial fidelity, temporal coherence, andmulti-view consistency through both perceptual and quantitative metrics, 2) astrong baseline that make an initial attempt for 4D stylization, and 3) acurated collection of high-resolution dynamic 4D scenes with diverse motionsand complex backgrounds. To establish a strong baseline, we present Style4D, anovel framework built upon 4D Gaussian Splatting. It consists of three keycomponents: a basic 4DGS scene representation to capture reliable geometry, aStyle Gaussian Representation that leverages lightweight per-Gaussian MLPs fortemporally and spatially aware appearance control, and a HolisticGeometry-Preserved Style Transfer module designed to enhance spatio-temporalconsistency via contrastive coherence learning and structural contentpreservation. Extensive experiments on Style4D-Bench demonstrate that Style4Dachieves state-of-the-art performance in 4D stylization, producing fine-grainedstylistic details with stable temporal dynamics and consistent multi-viewrendering. We expect Style4D-Bench to become a valuable resource forbenchmarking and advancing research in stylized rendering of dynamic 3D scenes.Project page: https://becky-catherine.github.io/Style4D . Code:https://github.com/Becky-catherine/Style4D-Bench .</description><author>Beiqi Chen, Shuai Shao, Haitang Feng, Jianhuang Lai, Jianlou Si, Guangcong Wang</author><pubDate>Tue, 26 Aug 2025 17:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19243v1</guid></item><item><title>Articulate3D: Zero-Shot Text-Driven 3D Object Posing</title><link>http://arxiv.org/abs/2508.19244v1</link><description>We propose a training-free method, Articulate3D, to pose a 3D asset throughlanguage control. Despite advances in vision and language models, this taskremains surprisingly challenging. To achieve this goal, we decompose theproblem into two steps. We modify a powerful image-generator to create targetimages conditioned on the input image and a text instruction. We then align themesh to the target images through a multi-view pose optimisation step. Indetail, we introduce a self-attention rewiring mechanism (RSActrl) thatdecouples the source structure from pose within an image generative model,allowing it to maintain a consistent structure across varying poses. Weobserved that differentiable rendering is an unreliable signal for articulationoptimisation; instead, we use keypoints to establish correspondences betweeninput and target images. The effectiveness of Articulate3D is demonstratedacross a diverse range of 3D objects and free-form text prompts, successfullymanipulating poses while maintaining the original identity of the mesh.Quantitative evaluations and a comparative user study, in which our method waspreferred over 85\% of the time, confirm its superiority over existingapproaches. Project page:https://odeb1.github.io/articulate3d_page_deb/</description><author>Oishi Deb, Anjun Hu, Ashkan Khakzar, Philip Torr, Christian Rupprecht</author><pubDate>Tue, 26 Aug 2025 17:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19244v1</guid></item><item><title>Autoregressive Universal Video Segmentation Model</title><link>http://arxiv.org/abs/2508.19242v1</link><description>Recent video foundation models such as SAM2 excel at prompted videosegmentation by treating masks as a general-purpose primitive. However, manyreal-world settings require unprompted segmentation that aims to detect andtrack all objects in a video without external cues, leaving today's landscapefragmented across task-specific models and pipelines. We recast streaming videosegmentation as sequential mask prediction, analogous to language modeling, andintroduce the Autoregressive Universal Segmentation Model (AUSM), a singlearchitecture that unifies both prompted and unprompted video segmentation.Built on recent state-space models, AUSM maintains a fixed-size spatial stateand scales to video streams of arbitrary length. Furthermore, all components ofAUSM are designed for parallel training across frames, yielding substantialspeedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS2018 &amp; 2019, MOSE, YouTube-VIS 2019 &amp; 2021, and OVIS) AUSM outperforms prioruniversal streaming video segmentation methods and achieves up to 2.5x fastertraining on 16-frame sequences.</description><author>Miran Heo, Sukjun Hwang, Min-Hung Chen, Yu-Chiang Frank Wang, Albert Gu, Seon Joo Kim, Ryo Hachiuma</author><pubDate>Tue, 26 Aug 2025 17:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19242v1</guid></item><item><title>Model Context Protocols in Adaptive Transport Systems: A Survey</title><link>http://arxiv.org/abs/2508.19239v1</link><description>The rapid expansion of interconnected devices, autonomous systems, and AIapplications has created severe fragmentation in adaptive transport systems,where diverse protocols and context sources remain isolated. This surveyprovides the first systematic investigation of the Model Context Protocol (MCP)as a unifying paradigm, highlighting its ability to bridge protocol-leveladaptation with context-aware decision making. Analyzing establishedliterature, we show that existing efforts have implicitly converged towardMCP-like architectures, signaling a natural evolution from fragmented solutionsto standardized integration frameworks. We propose a five-category taxonomycovering adaptive mechanisms, context-aware frameworks, unification models,integration strategies, and MCP-enabled architectures. Our findings revealthree key insights: traditional transport protocols have reached the limits ofisolated adaptation, MCP's client-server and JSON-RPC structure enablessemantic interoperability, and AI-driven transport demands integrationparadigms uniquely suited to MCP. Finally, we present a research roadmappositioning MCP as a foundation for next-generation adaptive, context-aware,and intelligent transport infrastructures.</description><author>Gaurab Chhetri, Shriyank Somvanshi, Md Monzurul Islam, Shamyo Brotee, Mahmuda Sultana Mimi, Dipti Koirala, Biplov Pandey, Subasish Das</author><pubDate>Tue, 26 Aug 2025 17:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19239v1</guid></item><item><title>MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation</title><link>http://arxiv.org/abs/2508.19236v1</link><description>Temporal context is essential for robotic manipulation because such tasks areinherently non-Markovian, yet mainstream VLA models typically overlook it andstruggle with long-horizon, temporally dependent tasks. Cognitive sciencesuggests that humans rely on working memory to buffer short-livedrepresentations for immediate control, while the hippocampal system preservesverbatim episodic details and semantic gist of past experience for long-termmemory. Inspired by these mechanisms, we propose MemoryVLA, aCognition-Memory-Action framework for long-horizon robotic manipulation. Apretrained VLM encodes the observation into perceptual and cognitive tokensthat form working memory, while a Perceptual-Cognitive Memory Bank storeslow-level details and high-level semantics consolidated from it. Working memoryretrieves decision-relevant entries from the bank, adaptively fuses them withcurrent tokens, and updates the bank by merging redundancies. Using thesetokens, a memory-conditioned diffusion action expert yields temporally awareaction sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasksacross three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, itachieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperformingstate-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain onBridge. On 12 real-world tasks spanning general skills and long-horizontemporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizontasks showing a +26 improvement over state-of-the-art baseline. Project Page:https://shihao1895.github.io/MemoryVLA</description><author>Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, Gao Huang</author><pubDate>Tue, 26 Aug 2025 17:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19236v1</guid></item><item><title>Emergent time-keeping mechanisms in a deep reinforcement learning agent performing an interval timing task</title><link>http://arxiv.org/abs/2508.15784v2</link><description>Drawing parallels between Deep Artificial Neural Networks (DNNs) andbiological systems can aid in understanding complex biological mechanisms thatare difficult to disentangle. Temporal processing, an extensively researchedtopic, is one such example that lacks a coherent understanding of itsunderlying mechanisms. In this study, we investigate temporal processing in aDeep Reinforcement Learning (DRL) agent performing an interval timing task andexplore potential biological counterparts to its emergent behavior. The agentwas successfully trained to perform a duration production task, which involvedmarking successive occurrences of a target interval while viewing a videosequence. Analysis of the agent's internal states revealed oscillatory neuralactivations, a ubiquitous pattern in biological systems. Interestingly, theagent's actions were predominantly influenced by neurons exhibiting theseoscillations with high amplitudes and frequencies corresponding to the targetinterval. Parallels are drawn between the agent's time-keeping strategy and theStriatal Beat Frequency (SBF) model, a biologically plausible model of intervaltiming. Furthermore, the agent maintained its oscillatory representations andtask performance when tested on different video sequences (including a blankvideo). Thus, once learned, the agent internalized its time-keeping mechanismand showed minimal reliance on its environment to perform the timing task. Ahypothesis about the resemblance between this emergent behavior and certainaspects of the evolution of biological processes like circadian rhythms, hasbeen discussed. This study aims to contribute to recent research efforts ofutilizing DNNs to understand biological systems, with a particular emphasis ontemporal processing.</description><author>Amrapali Pednekar, Alvaro Garrido, Pieter Simoens, Yara Khaluf</author><pubDate>Tue, 26 Aug 2025 17:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15784v2</guid></item><item><title>Automated Feature Tracking for Real-Time Kinematic Analysis and Shape Estimation of Carbon Nanotube Growth</title><link>http://arxiv.org/abs/2508.19232v1</link><description>Carbon nanotubes (CNTs) are critical building blocks in nanotechnology, yetthe characterization of their dynamic growth is limited by the experimentalchallenges in nanoscale motion measurement using scanning electron microscopy(SEM) imaging. Existing ex situ methods offer only static analysis, while insitu techniques often require manual initialization and lack continuousper-particle trajectory decomposition. We present Visual Feature Tracking(VFTrack) an in-situ real-time particle tracking framework that automaticallydetects and tracks individual CNT particles in SEM image sequences. VFTrackintegrates handcrafted or deep feature detectors and matchers within a particletracking framework to enable kinematic analysis of CNT micropillar growth. Asystematic using 13,540 manually annotated trajectories identifies the ALIKEDdetector with LightGlue matcher as an optimal combination (F1-score of 0.78,$\alpha$-score of 0.89). VFTrack motion vectors decomposed into axial growth,lateral drift, and oscillations, facilitate the calculation of heterogeneousregional growth rates and the reconstruction of evolving CNT pillarmorphologies. This work enables advancement in automated nano-materialcharacterization, bridging the gap between physics-based models andexperimental observation to enable real-time optimization of CNT synthesis.</description><author>Kaveh Safavigerdini, Ramakrishna Surya, Jaired Collins, Prasad Calyam, Filiz Bunyak, Matthew R. Maschmann, Kannappan Palaniappan</author><pubDate>Tue, 26 Aug 2025 17:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19232v1</guid></item><item><title>Distribution free M-estimation</title><link>http://arxiv.org/abs/2505.22807v4</link><description>The basic question of delineating those statistical problems that aresolvable without making any assumptions on the underlying data distribution haslong animated statistics and learning theory. This paper characterizes when aconvex M-estimation or stochastic optimization problem is solvable in such anassumption-free setting, providing a precise dividing line between solvable andunsolvable problems. The conditions we identify show, perhaps surprisingly,that Lipschitz continuity of the loss being minimized is not necessary fordistribution free minimization, and they are also distinct from classicalcharacterizations of learnability in machine learning.</description><author>Felipe Areces, John C. Duchi</author><pubDate>Tue, 26 Aug 2025 17:47:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.22807v4</guid></item><item><title>StepWiser: Stepwise Generative Judges for Wiser Reasoning</title><link>http://arxiv.org/abs/2508.19229v1</link><description>As models increasingly leverage multi-step reasoning strategies to solvecomplex problems, supervising the logical validity of these intermediate stepshas become a critical research challenge. Process reward models address this byproviding step-by-step feedback, but current approaches have two majordrawbacks: they typically function as classifiers without providingexplanations, and their reliance on supervised fine-tuning with static datasetslimits generalization. Inspired by recent advances, we reframe stepwise rewardmodeling from a classification task to a reasoning task itself. We thus proposea generative judge that reasons about the policy model's reasoning steps (i.e.,meta-reasons), outputting thinking tokens before delivering a final verdict.Our model, StepWiser, is trained by reinforcement learning using relativeoutcomes of rollouts. We show it provides (i) better judgment accuracy onintermediate steps than existing methods; (ii) can be used to improve thepolicy model at training time; and (iii) improves inference-time search.</description><author>Wei Xiong, Wenting Zhao, Weizhe Yuan, Olga Golovneva, Tong Zhang, Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Tue, 26 Aug 2025 17:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19229v1</guid></item><item><title>Predicting the Order of Upcoming Tokens Improves Language Modeling</title><link>http://arxiv.org/abs/2508.19228v1</link><description>Multi-Token Prediction (MTP) has been proposed as an auxiliary objective toimprove next-token prediction (NTP) in language model training but showsinconsistent improvements, underperforming in standard NLP benchmarks. We arguethat MTP's exact future token prediction is too difficult as an auxiliary loss.Instead, we propose Token Order Prediction (TOP), which trains models to orderupcoming tokens by their proximity using a learning-to-rank loss. TOP requiresonly a single additional unembedding layer compared to MTP's multipletransformer layers. We pretrain models of 340M, 1.8B, and 7B parameters usingNTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks showthat TOP overall outperforms both NTP and MTP even at scale. Our code isavailable at https://github.com/zaydzuhri/token-order-prediction</description><author>Zayd M. K. Zuhri, Erland Hilman Fuadi, Alham Fikri Aji</author><pubDate>Tue, 26 Aug 2025 17:43:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19228v1</guid></item><item><title>Generative Interfaces for Language Models</title><link>http://arxiv.org/abs/2508.19227v1</link><description>Large language models (LLMs) are increasingly seen as assistants, copilots,and consultants, capable of supporting a wide range of tasks through naturalconversation. However, most systems remain constrained by a linearrequest-response format that often makes interactions inefficient inmulti-turn, information-dense, and exploratory tasks. To address theselimitations, we propose Generative Interfaces for Language Models, a paradigmin which LLMs respond to user queries by proactively generating user interfaces(UIs) that enable more adaptive and interactive engagement. Our frameworkleverages structured interface-specific representations and iterativerefinements to translate user queries into task-specific UIs. For systematicevaluation, we introduce a multidimensional assessment framework that comparesgenerative interfaces with traditional chat-based ones across diverse tasks,interaction patterns, and query types, capturing functional, interactive, andemotional aspects of user experience. Results show that generative interfacesconsistently outperform conversational ones, with humans preferring them inover 70% of cases. These findings clarify when and why users favor generativeinterfaces, paving the way for future advancements in human-AI interaction.</description><author>Jiaqi Chen, Yanzhe Zhang, Yutong Zhang, Yijia Shao, Diyi Yang</author><pubDate>Tue, 26 Aug 2025 17:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19227v1</guid></item><item><title>Evaluating the Evaluators: Are readability metrics good measures of readability?</title><link>http://arxiv.org/abs/2508.19221v1</link><description>Plain Language Summarization (PLS) aims to distill complex documents intoaccessible summaries for non-expert audiences. In this paper, we conduct athorough survey of PLS literature, and identify that the current standardpractice for readability evaluation is to use traditional readability metrics,such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility inother fields, these metrics have not been compared to human readabilityjudgments in PLS. We evaluate 8 readability metrics and show that mostcorrelate poorly with human judgments, including the most popular metric, FKGL.We then show that Language Models (LMs) are better judges of readability, withthe best-performing model achieving a Pearson correlation of 0.56 with humanjudgments. Extending our analysis to PLS datasets, which contain summariesaimed at non-expert audiences, we find that LMs better capture deeper measuresof readability, such as required background knowledge, and lead to differentconclusions than the traditional metrics. Based on these findings, we offerrecommendations for best practices in the evaluation of plain languagesummaries. We release our analysis code and survey data.</description><author>Isabel Cachola, Daniel Khashabi, Mark Dredze</author><pubDate>Tue, 26 Aug 2025 17:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19221v1</guid></item><item><title>The Subset Sum Matching Problem</title><link>http://arxiv.org/abs/2508.19218v1</link><description>This paper presents a new combinatorial optimisation task, the Subset SumMatching Problem (SSMP), which is an abstraction of common financialapplications such as trades reconciliation. We present three algorithms, twosuboptimal and one optimal, to solve this problem. We also generate a benchmarkto cover different instances of SSMP varying in complexity, and carry out anexperimental evaluation to assess the performance of the approaches.</description><author>Yufei Wu, Manuel R. Torres, Parisa Zehtabi, Alberto Pozanco Lancho, Michael Cashmore, Daniel Borrajo, Manuela Veloso</author><pubDate>Tue, 26 Aug 2025 17:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19218v1</guid></item><item><title>Local Learning Rules for Out-of-Equilibrium Physical Generative Models</title><link>http://arxiv.org/abs/2506.19136v2</link><description>We show that the out-of-equilibrium driving protocol of score-basedgenerative models (SGMs) can be learned via local learning rules. The gradientwith respect to the parameters of the driving protocol is computed directlyfrom force measurements or from observed system dynamics. As a demonstration,we implement an SGM in a network of driven, nonlinear, overdamped oscillatorscoupled to a thermal bath. We first apply it to the problem of sampling from amixture of two Gaussians in 2D. Finally, we train a 12x12 oscillator network onthe MNIST dataset to generate images of handwritten digits 0 and 1.</description><author>Cyrill Bösch, Geoffrey Roeder, Marc Serra-Garcia, Ryan P. Adams</author><pubDate>Tue, 26 Aug 2025 17:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.19136v2</guid></item><item><title>From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification</title><link>http://arxiv.org/abs/2411.14252v2</link><description>In conversational AI systems, a critical challenge in training effectivemulti-turn intent classification models lies in the generation of large-scale,domain-specific, multilingual dialogue datasets. In this paper, we introduceChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)with Large Language Models (LLMs) to generate intent-driven, context-awaredialogues through self-play. Our method first extracts domain-specific intenttransition patterns from real-world e-commerce chat logs, which guide themodeling of turn-level dynamics and intent sequences. LLMs are then employed toparameterize the emission probabilities of HMMs, enabling the generation ofnatural, coherent utterances aligned with predicted intents and dialoguecontext. We further propose MINT-CL, a multi-task contrastive learningframework for multi-turn intent classification, which improves performancewhile reducing dependence on large-scale annotated datasets. Empirical resultsdemonstrate that our approach outperforms competitive baselines in bothdialogue generation quality and classification accuracy, particularly inmultilingual settings. To facilitate future research, we release MINT-E, acomprehensive, multilingual, intent-aware multi-turn dialogue corpus derivedfrom the e-commerce domain. The reproduced source code and dataset areavailable at https://github.com/junhua/chain-of-intent.</description><author>Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim</author><pubDate>Tue, 26 Aug 2025 17:22:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14252v2</guid></item><item><title>Interpolating Speaker Identities in Embedding Space for Data Expansion</title><link>http://arxiv.org/abs/2508.19210v1</link><description>The success of deep learning-based speaker verification systems is largelyattributed to access to large-scale and diverse speaker identity data. However,collecting data from more identities is expensive, challenging, and oftenlimited by privacy concerns. To address this limitation, we propose INSIDE(Interpolating Speaker Identities in Embedding Space), a novel data expansionmethod that synthesizes new speaker identities by interpolating betweenexisting speaker embeddings. Specifically, we select pairs of nearby speakerembeddings from a pretrained speaker embedding space and compute intermediateembeddings using spherical linear interpolation. These interpolated embeddingsare then fed to a text-to-speech system to generate corresponding speechwaveforms. The resulting data is combined with the original dataset to traindownstream models. Experiments show that models trained with INSIDE-expandeddata outperform those trained only on real data, achieving 3.06\% to 5.24\%relative improvements. While INSIDE is primarily designed for speakerverification, we also validate its effectiveness on gender classification,where it yields a 13.44\% relative improvement. Moreover, INSIDE is compatiblewith other augmentation techniques and can serve as a flexible, scalableaddition to existing training pipelines.</description><author>Tianchi Liu, Ruijie Tao, Qiongqiong Wang, Yidi Jiang, Hardik B. Sailor, Ke Zhang, Jingru Lin, Haizhou Li</author><pubDate>Tue, 26 Aug 2025 17:15:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19210v1</guid></item><item><title>OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation</title><link>http://arxiv.org/abs/2508.19209v1</link><description>Existing video avatar models can produce fluid human animations, yet theystruggle to move beyond mere physical likeness to capture a character'sauthentic essence. Their motions typically synchronize with low-level cues likeaudio rhythm, lacking a deeper semantic understanding of emotion, intent, orcontext. To bridge this gap, \textbf{we propose a framework designed togenerate character animations that are not only physically plausible but alsosemantically coherent and expressive.} Our model, \textbf{OmniHuman-1.5}, isbuilt upon two key technical contributions. First, we leverage Multimodal LargeLanguage Models to synthesize a structured textual representation of conditionsthat provides high-level semantic guidance. This guidance steers our motiongenerator beyond simplistic rhythmic synchronization, enabling the productionof actions that are contextually and emotionally resonant. Second, to ensurethe effective fusion of these multimodal inputs and mitigate inter-modalityconflicts, we introduce a specialized Multimodal DiT architecture with a novelPseudo Last Frame design. The synergy of these components allows our model toaccurately interpret the joint semantics of audio, images, and text, therebygenerating motions that are deeply coherent with the character, scene, andlinguistic content. Extensive experiments demonstrate that our model achievesleading performance across a comprehensive set of metrics, including lip-syncaccuracy, video quality, motion naturalness and semantic consistency withtextual prompts. Furthermore, our approach shows remarkable extensibility tocomplex scenarios, such as those involving multi-person and non-human subjects.Homepage: \href{https://omnihuman-lab.github.io/v1_5/}</description><author>Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, Mingyuan Gao</author><pubDate>Tue, 26 Aug 2025 17:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19209v1</guid></item><item><title>Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots</title><link>http://arxiv.org/abs/2508.15748v3</link><description>The development of parasocial relationships with AI agents has severe, and insome cases, tragic effects for human well-being. Yet preventing such dynamicsis challenging: parasocial cues often emerge gradually in privateconversations, and not all forms of emotional engagement are inherentlyharmful. We address this challenge by introducing a simple response evaluationframework, created by repurposing a state-of-the-art language model, thatevaluates ongoing conversations for parasocial cues in real time. To test thefeasibility of this approach, we constructed a small synthetic dataset ofthirty dialogues spanning parasocial, sycophantic, and neutral conversations.Iterative evaluation with five stage testing successfully identified allparasocial conversations while avoiding false positives under a tolerantunanimity rule, with detection typically occurring within the first fewexchanges. These findings provide preliminary evidence that evaluation agentscan provide a viable solution for the prevention of parasocial relations.</description><author>Emma Rath, Stuart Armstrong, Rebecca Gorman</author><pubDate>Tue, 26 Aug 2025 17:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15748v3</guid></item><item><title>Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment</title><link>http://arxiv.org/abs/2508.16839v2</link><description>Clinical workflows are fragmented as a patchwork of scripts and task-specificnetworks that often handle triage, task selection, and model deployment. Thesepipelines are rarely streamlined for data science pipeline, reducing efficiencyand raising operational costs. Workflows also lack data-driven modelidentification (from imaging/tabular inputs) and standardized delivery of modeloutputs. In response, we present a practical, healthcare-first framework thatuses a single vision-language model (VLM) in two complementary roles. First(Solution 1), the VLM acts as an aware model-card matcher that routes anincoming image to the appropriate specialist model via a three-stage workflow(modality -&gt; primary abnormality -&gt; model-card id). Checks are provided by (i)stagewise prompts that allow early exit via None/Normal/Other and (ii) astagewise answer selector that arbitrates between the top-2 candidates at eachstage, reducing the chance of an incorrect selection and aligning the workflowwith clinical risk tolerance. Second (Solution 2), we fine-tune the VLM onspecialty-specific datasets ensuring a single model covers multiple downstreamtasks within each specialty, maintaining performance while simplifyingdeployment. Across gastroenterology, hematology, ophthalmology, and pathology,our single-model deployment matches or approaches specialized baselines. Compared with pipelines composed of many task-specific agents, this approachshows that one VLM can both decide and do. It may reduce effort by datascientists, shorten monitoring, increase the transparency of model selection(with per-stage justifications), and lower integration overhead.</description><author>Shayan Vassef, Soorya Ram Shimegekar, Abhay Goyal, Koustuv Saha, Pi Zonooz, Navin Kumar</author><pubDate>Tue, 26 Aug 2025 17:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16839v2</guid></item><item><title>Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications</title><link>http://arxiv.org/abs/2502.13358v3</link><description>Large Language Models (LLMs) have significantly advanced natural languageprocessing, demonstrating strong capabilities in tasks such as text generation,summarization, and reasoning. Recently, their potential for automating precisetext editing tasks across specialized domains, such as programming code, LaTeX,and structured database languages, has gained attention. However, currentstate-of-the-art LLMs still struggle with executing precise, instruction-drivenedits, particularly when structural accuracy and strict adherence to domainconventions are required. To address these challenges, we introduceInstrEditBench, an automated benchmark dataset comprising over 30,000structured editing tasks spanning diverse domains, including Wikipediaarticles, LaTeX documents, source code, and database languages. Using thisbenchmark, we develop FineEdit, a specialized editing model explicitly trainedfor accurate, context-aware text modifications. Experimental evaluationsdemonstrate that FineEdit outperforms state-of-the-art models, achievingimprovements of approximately 10\% over Gemini models on single-turn edits, upto 30\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance byover 40\% on direct editing tasks. FineEdit also effectively generalizes torealistic multi-turn editing scenarios, highlighting its practicalapplicability. To facilitate further research and reproducibility, we releaseFineEdit at https://github.com/StuRinDQB/FineEdit} andhttps://huggingface.co/datasets/YimingZeng/FineEdit_bench.</description><author>Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu</author><pubDate>Tue, 26 Aug 2025 17:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.13358v3</guid></item><item><title>VibeVoice Technical Report</title><link>http://arxiv.org/abs/2508.19205v1</link><description>This report presents VibeVoice, a novel model designed to synthesizelong-form speech with multiple speakers by employing next-token diffusion,which is a unified method for modeling continuous data by autoregressivelygenerating latent vectors via diffusion. To enable this, we introduce a novelcontinuous speech tokenizer that, when compared to the popular Encodec model,improves data compression by 80 times while maintaining comparable performance.The tokenizer effectively preserves audio fidelity while significantly boostingcomputational efficiency for processing long sequences. Thus, VibeVoice cansynthesize long-form speech for up to 90 minutes (in a 64K context windowlength) with a maximum of 4 speakers, capturing the authentic conversational``vibe'' and surpassing open-source and proprietary dialogue models.</description><author>Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, Shaohan Huang, Yan Xia, Furu Wei</author><pubDate>Tue, 26 Aug 2025 17:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19205v1</guid></item><item><title>LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding</title><link>http://arxiv.org/abs/2508.19204v1</link><description>Large-scale scene data is essential for training and testing in robotlearning. Neural reconstruction methods have promised the capability ofreconstructing large physically-grounded outdoor scenes from captured sensordata. However, these methods have baked-in static environments and only allowfor limited scene control -- they are functionally constrained in scene andtrajectory diversity by the captures from which they are reconstructed. Incontrast, generating driving data with recent image or video diffusion modelsoffers control, however, at the cost of geometry grounding and causality. Inthis work, we aim to bridge this gap and present a method that directlygenerates large-scale 3D driving scenes with accurate geometry, allowing forcausal novel view synthesis with object permanence and explicit 3D geometryestimation. The proposed method combines the generation of a proxy geometry andenvironment representation with score distillation from learned 2D imagepriors. We find that this approach allows for high controllability, enablingthe prompt-guided geometry and high-fidelity texture and structure that can beconditioned on map layouts -- producing realistic and geometrically consistent3D generations of complex driving scenes.</description><author>Julian Ost, Andrea Ramazzina, Amogh Joshi, Maximilian Bömer, Mario Bijelic, Felix Heide</author><pubDate>Tue, 26 Aug 2025 17:04:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19204v1</guid></item><item><title>Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning</title><link>http://arxiv.org/abs/2508.19202v1</link><description>Scientific problem solving poses unique challenges for LLMs, requiring bothdeep domain knowledge and the ability to apply such knowledge through complexreasoning. While automated scientific reasoners hold great promise forassisting human scientists, there is currently no widely adopted holisticbenchmark for evaluating scientific reasoning, and few approachessystematically disentangle the distinct roles of knowledge and reasoning inthese tasks. To address these gaps, we introduce SciReas, a diverse suite ofexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, aselective subset that requires more complex reasoning. Our holistic evaluationsurfaces insights about scientific reasoning performance that remain hiddenwhen relying on individual benchmarks alone. We then propose KRUX, a probingframework for studying the distinct roles of reasoning and knowledge inscientific tasks. Combining the two, we conduct an in-depth analysis thatyields several key findings: (1) Retrieving task-relevant knowledge from modelparameters is a critical bottleneck for LLMs in scientific reasoning; (2)Reasoning models consistently benefit from external knowledge added in-contexton top of the reasoning enhancement; (3) Enhancing verbalized reasoningimproves LLMs' ability to surface task-relevant knowledge. Finally, we conducta lightweight analysis, comparing our science-focused data composition withconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baselinefor scientific reasoning.</description><author>Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan</author><pubDate>Tue, 26 Aug 2025 17:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19202v1</guid></item><item><title>Understanding Tool-Integrated Reasoning</title><link>http://arxiv.org/abs/2508.19201v1</link><description>We study why Tool-Integrated Reasoning (TIR) makes Large Language Models(LLMs) more capable. While LLMs integrated with tools like Python codeinterpreters show great promise, a principled theory explaining why thisparadigm is effective has been missing. This work provides the first formalproof that TIR fundamentally expands an LLM's capabilities. We demonstrate thattools enable a strict expansion of the model's empirical and feasible support,breaking the capability ceiling of pure-text models by unlockingproblem-solving strategies that are otherwise impossible or intractablyverbose. To guide model behavior without compromising training stability andperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), anovel algorithm that directly modifies the advantage function to guide thepolicy behavior. We conduct comprehensive experiments on challengingmathematical benchmarks, leveraging a Python interpreter as the external tool.Our results show that the TIR model decisively outperforms its pure-textcounterpart on the pass@k metric. Crucially, this advantage is not confined tocomputationally-intensive problems but extends to those requiring significantabstract insight. We further identify the emergent cognitive patterns thatillustrate how models learn to think with tools. Finally, we report improvedtool usage behavior with early code invocation and much more interactive turnswith ASPO. Overall, our work provides the first principled explanation forTIR's success, shifting the focus from the mere fact that tools work to why andhow they enable more powerful reasoning.</description><author>Heng Lin, Zhongwen Xu</author><pubDate>Tue, 26 Aug 2025 17:03:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19201v1</guid></item><item><title>The Ramon Llull's Thinking Machine for Automated Ideation</title><link>http://arxiv.org/abs/2508.19200v1</link><description>This paper revisits Ramon Llull's Ars combinatoria - a medieval framework forgenerating knowledge through symbolic recombination - as a conceptualfoundation for building a modern Llull's thinking machine for researchideation. Our approach defines three compositional axes: Theme (e.g.,efficiency, adaptivity), Domain (e.g., question answering, machinetranslation), and Method (e.g., adversarial training, linear attention). Theseelements represent high-level abstractions common in scientific work -motivations, problem settings, and technical approaches - and serve as buildingblocks for LLM-driven exploration. We mine elements from human experts orconference papers and show that prompting LLMs with curated combinationsproduces research ideas that are diverse, relevant, and grounded in currentliterature. This modern thinking machine offers a lightweight, interpretabletool for augmenting scientific creativity and suggests a path towardcollaborative ideation between humans and AI.</description><author>Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu</author><pubDate>Tue, 26 Aug 2025 17:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19200v1</guid></item><item><title>Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation</title><link>http://arxiv.org/abs/2508.19199v1</link><description>Efficient planning in high-dimensional spaces, such as those involvingdeformable objects, requires computationally tractable yet sufficientlyexpressive dynamics models. This paper introduces a method that automaticallygenerates task-specific, spatially adaptive dynamics models by learning whichregions of the object require high-resolution modeling to achieve good taskperformance for a given planning query. Task performance depends on the complexinterplay between the dynamics model, world dynamics, control, and taskrequirements. Our proposed diffusion-based model generator predicts per-regionmodel resolutions based on start and goal pointclouds that define the planningquery. To efficiently collect the data for learning this mapping, a two-stageprocess optimizes resolution using predictive dynamics as a prior beforedirectly optimizing using closed-loop performance. On a tree-manipulation task,our method doubles planning speed with only a small decrease in taskperformance over using a full-resolution model. This approach informs a pathtowards using previous planning and control data to generate computationallyefficient yet sufficiently expressive dynamics models for new tasks.</description><author>Alex LaGrassa, Zixuan Huang, Dmitry Berenson, Oliver Kroemer</author><pubDate>Tue, 26 Aug 2025 17:03:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19199v1</guid></item><item><title>Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates</title><link>http://arxiv.org/abs/2507.09166v2</link><description>The coarse spatial resolution of gridded climate models, such as generalcirculation models, limits their direct use in projecting socially relevantvariables like extreme precipitation. Most downscaling methods estimate theconditional distributions of extremes by generating large ensembles,complicating the assessment of robustness under distributional shifts, such asthose induced by climate change. To better understand and potentially improverobustness, we propose super-resolving the parameters of the target variable'sprobability distribution directly using analytically tractable mappings. Withina perfect-model framework over Switzerland, we demonstrate that vectorgeneralized linear and additive models can super-resolve the generalizedextreme value distribution of summer hourly precipitation extremes from coarseprecipitation fields and topography. We introduce the notion of a "robustnessgap", defined as the difference in predictive error between present-trained andfuture-trained models, and use it to diagnose how model structure affects thegeneralization of each quantile to a pseudo-global warming scenario. Byevaluating multiple model configurations, we also identify an upper limit onthe super-resolution factor based on the spatial auto- and cross-correlation ofprecipitation and elevation, beyond which coarse precipitation loses predictivevalue. Our framework is broadly applicable to variables governed by parametricdistributions and offers a model-agnostic diagnostic for understanding when andwhy empirical downscaling generalizes to climate change and extremes.</description><author>Louise Largeau, Erwan Koch, David Leutwyler, Gregoire Mariethoz, Valerie Chavez-Demoulin, Tom Beucler</author><pubDate>Tue, 26 Aug 2025 17:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.09166v2</guid></item><item><title>Prompt-based Dynamic Token Pruning for Efficient Segmentation of Medical Images</title><link>http://arxiv.org/abs/2506.16369v2</link><description>The high computational demands of Vision Transformers (ViTs) in processing alarge number of tokens often constrain their practical application in analyzingmedical images. This research proposes a Prompt-driven Adaptive Token ({\itPrATo}) pruning method to selectively reduce the processing of irrelevanttokens in the segmentation pipeline. The prompt-based spatial prior helps torank the tokens according to their relevance. Tokens with low-relevance scoresare down-weighted, ensuring that only the relevant ones are propagated forprocessing across subsequent stages. This data-driven pruning strategy improvessegmentation accuracy and inference speed by allocating computational resourcesto essential regions. The proposed framework is integrated with severalstate-of-the-art models to facilitate the elimination of irrelevant tokens,thereby enhancing computational efficiency while preserving segmentationaccuracy. The experimental results show a reduction of $\sim$ 35-55% tokens;thus reducing the computational costs relative to baselines. Cost-effectivemedical image processing, using our framework, facilitates real-time diagnosisby expanding its applicability in resource-constrained environments.</description><author>Pallabi Dutta, Anubhab Maity, Sushmita Mitra</author><pubDate>Tue, 26 Aug 2025 17:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.16369v2</guid></item><item><title>Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels</title><link>http://arxiv.org/abs/2508.17437v2</link><description>Inferring the physical properties of 3D scenes from visual information is acritical yet challenging task for creating interactive and realistic virtualworlds. While humans intuitively grasp material characteristics such aselasticity or stiffness, existing methods often rely on slow, per-sceneoptimization, limiting their generalizability and application. To address thisproblem, we introduce PIXIE, a novel method that trains a generalizable neuralnetwork to predict physical properties across multiple scenes from 3D visualfeatures purely using supervised losses. Once trained, our feed-forward networkcan perform fast inference of plausible material fields, which coupled with alearned static scene representation like Gaussian Splatting enables realisticphysics simulation under external forces. To facilitate this research, we alsocollected PIXIEVERSE, one of the largest known datasets of paired 3D assets andphysic material annotations. Extensive evaluations demonstrate that PIXIE isabout 1.46-4.39x better and orders of magnitude faster than test-timeoptimization methods. By leveraging pretrained visual features like CLIP, ourmethod can also zero-shot generalize to real-world scenes despite only everbeen trained on synthetic data. https://pixie-3d.github.io/</description><author>Long Le, Ryan Lucas, Chen Wang, Chuhao Chen, Dinesh Jayaraman, Eric Eaton, Lingjie Liu</author><pubDate>Tue, 26 Aug 2025 16:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17437v2</guid></item><item><title>All-in-One Slider for Attribute Manipulation in Diffusion Models</title><link>http://arxiv.org/abs/2508.19195v1</link><description>Text-to-image (T2I) diffusion models have made significant strides ingenerating high-quality images. However, progressively manipulating certainattributes of generated images to meet the desired user expectations remainschallenging, particularly for content with rich details, such as human faces.Some studies have attempted to address this by training slider modules.However, they follow a One-for-One manner, where an independent slider istrained for each attribute, requiring additional training whenever a newattribute is introduced. This not only results in parameter redundancyaccumulated by sliders but also restricts the flexibility of practicalapplications and the scalability of attribute manipulation. To address thisissue, we introduce the All-in-One Slider, a lightweight module that decomposesthe text embedding space into sparse, semantically meaningful attributedirections. Once trained, it functions as a general-purpose slider, enablinginterpretable and fine-grained continuous control over various attributes.Moreover, by recombining the learned directions, the All-in-One Slider supportszero-shot manipulation of unseen attributes (e.g., races and celebrities) andthe composition of multiple attributes. Extensive experiments demonstrate thatour method enables accurate and scalable attribute manipulation, achievingnotable improvements compared to previous methods. Furthermore, our method canbe extended to integrate with the inversion framework to perform attributemanipulation on real images, broadening its applicability to various real-worldscenarios. The code and trained model will be released at:https://github.com/ywxsuperstar/KSAE-FaceSteer.</description><author>Weixin Ye, Hongguang Zhu, Wei Wang, Yahui Liu, Mengyu Wang</author><pubDate>Tue, 26 Aug 2025 16:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19195v1</guid></item><item><title>Emotions as Ambiguity-aware Ordinal Representations</title><link>http://arxiv.org/abs/2508.19193v1</link><description>Emotions are inherently ambiguous and dynamic phenomena, yet existingcontinuous emotion recognition approaches either ignore their ambiguity ortreat ambiguity as an independent and static variable over time. Motivated bythis gap in the literature, in this paper we introduce \emph{ambiguity-awareordinal} emotion representations, a novel framework that captures both theambiguity present in emotion annotation and the inherent temporal dynamics ofemotional traces. Specifically, we propose approaches that model emotionambiguity through its rate of change. We evaluate our framework on twoaffective corpora -- RECOLA and GameVibe -- testing our proposed approaches onboth bounded (arousal, valence) and unbounded (engagement) continuous traces.Our results demonstrate that ordinal representations outperform conventionalambiguity-aware models on unbounded labels, achieving the highest ConcordanceCorrelation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,highlighting their effectiveness in modeling the traces' dynamics. For boundedtraces, ordinal representations excel in SDA, revealing their superior abilityto capture relative changes of annotated emotion traces.</description><author>Jingyao Wu, Matthew Barthet, David Melhart, Georgios N. Yannakakis</author><pubDate>Tue, 26 Aug 2025 16:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19193v1</guid></item><item><title>FastMesh:Efficient Artistic Mesh Generation via Component Decoupling</title><link>http://arxiv.org/abs/2508.19188v1</link><description>Recent mesh generation approaches typically tokenize triangle meshes intosequences of tokens and train autoregressive models to generate these tokenssequentially. Despite substantial progress, such token sequences inevitablyreuse vertices multiple times to fully represent manifold meshes, as eachvertex is shared by multiple faces. This redundancy leads to excessively longtoken sequences and inefficient generation processes. In this paper, we proposean efficient framework that generates artistic meshes by treating vertices andfaces separately, significantly reducing redundancy. We employ anautoregressive model solely for vertex generation, decreasing the token countto approximately 23\% of that required by the most compact existing tokenizer.Next, we leverage a bidirectional transformer to complete the mesh in a singlestep by capturing inter-vertex relationships and constructing the adjacencymatrix that defines the mesh faces. To further improve the generation quality,we introduce a fidelity enhancer to refine vertex positioning into more naturalarrangements and propose a post-processing framework to remove undesirable edgeconnections. Experimental results show that our method achieves more than8$\times$ faster speed on mesh generation compared to state-of-the-artapproaches, while producing higher mesh quality.</description><author>Jeonghwan Kim, Yushi Lan, Armando Fortes, Yongwei Chen, Xingang Pan</author><pubDate>Tue, 26 Aug 2025 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19188v1</guid></item><item><title>Real-Time Model Checking for Closed-Loop Robot Reactive Planning</title><link>http://arxiv.org/abs/2508.19186v1</link><description>We present a new application of model checking which achieves real-timemulti-step planning and obstacle avoidance on a real autonomous robot. We havedeveloped a small, purpose-built model checking algorithm which generates plansin situ based on "core" knowledge and attention as found in biological agents.This is achieved in real-time using no pre-computed data on a low-powereddevice. Our approach is based on chaining temporary control systems which arespawned to counteract disturbances in the local environment that disrupt anautonomous agent from its preferred action (or resting state). A noveldiscretization of 2D LiDAR data sensitive to bounded variations in the localenvironment is used. Multi-step planning using model checking by forwarddepth-first search is applied to cul-de-sac and playground scenarios. Bothempirical results and informal proofs of two fundamental properties of ourapproach demonstrate that model checking can be used to create efficientmulti-step plans for local obstacle avoidance, improving on the performance ofa reactive agent which can only plan one step. Our approach is an instructionalcase study for the development of safe, reliable and explainable planning inthe context of autonomous vehicles.</description><author>Christopher Chandler, Bernd Porr, Giulia Lafratta, Alice Miller</author><pubDate>Tue, 26 Aug 2025 16:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19186v1</guid></item><item><title>mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2505.24073v2</link><description>Large Vision-Language Models (LVLMs) have made remarkable strides inmultimodal tasks such as visual question answering, visual grounding, andcomplex reasoning. However, they remain limited by static training data,susceptibility to hallucinations, and inability to verify claims againstup-to-date, external evidence, compromising their performance in dynamicreal-world applications. Retrieval-Augmented Generation (RAG) offers apractical solution to mitigate these challenges by allowing the LVLMs to accesslarge-scale knowledge databases via retrieval mechanisms, thereby groundingmodel outputs in factual, contextually relevant information. Here in thispaper, we conduct the first systematic dissection of the multimodal RAGpipeline for LVLMs, explicitly investigating (1) the retrieval phase: on themodality configurations and retrieval strategies, (2) the re-ranking stage: onstrategies to mitigate positional biases and improve the relevance of retrievedevidence, and (3) the generation phase: we further investigate how to bestintegrate retrieved candidates into the final generation process. Finally, weextend to explore a unified agentic framework that integrates re-ranking andgeneration through self-reflection, enabling LVLMs to select relevant evidenceand suppress irrelevant context dynamically. Our full-stack exploration of RAGfor LVLMs yields substantial insights, resulting in an average performanceboost of 5% without any fine-tuning.</description><author>Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Suofei Feng, Ryan Rossi, Zhengzhong Tu</author><pubDate>Tue, 26 Aug 2025 16:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24073v2</guid></item><item><title>Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</title><link>http://arxiv.org/abs/2508.17630v2</link><description>We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neuralnetwork that integrates variational quantum circuits into the attentionmechanism. At its core, QGAT employs strongly entangling quantum circuits withamplitude-encoded node features to enable expressive nonlinear interactions.Distinct from classical multi-head attention that separately computes eachhead, QGAT leverages a single quantum circuit to simultaneously generatemultiple attention coefficients. This quantum parallelism facilitates parametersharing across heads, substantially reducing computational overhead and modelcomplexity. Classical projection weights and quantum circuit parameters areoptimized jointly in an end-to-end manner, ensuring flexible adaptation tolearning tasks. Empirical results demonstrate QGAT's effectiveness in capturingcomplex structural dependencies and improved generalization in inductivescenarios, highlighting its potential for scalable quantum-enhanced learningacross domains such as chemistry, biology, and network analysis. Furthermore,experiments confirm that quantum embedding enhances robustness against featureand structural noise, suggesting advantages in handling real-world noisy data.The modularity of QGAT also ensures straightforward integration into existingarchitectures, allowing it to easily augment classical attention-based models.</description><author>An Ning, Tai Yue Li, Nan Yow Chen</author><pubDate>Tue, 26 Aug 2025 16:42:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17630v2</guid></item><item><title>Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness</title><link>http://arxiv.org/abs/2508.19183v1</link><description>In safety-critical deep learning applications, robustness measures theability of neural models that handle imperceptible perturbations in input data,which may lead to potential safety hazards. Existing pre-deployment robustnessassessment methods typically suffer from significant trade-offs betweencomputational cost and measurement precision, limiting their practical utility.To address these limitations, this paper conducts a comprehensive comparativeanalysis of existing robustness definitions and associated assessmentmethodologies. We propose tower robustness to evaluate robustness, which is anovel, practical metric based on hypothesis testing to quantitatively evaluateprobabilistic robustness, enabling more rigorous and efficient pre-deploymentassessments. Our extensive comparative evaluation illustrates the advantagesand applicability of our proposed approach, thereby advancing the systematicunderstanding and enhancement of model robustness in safety-critical deeplearning applications.</description><author>Wenchuan Mu, Kwan Hui Lim</author><pubDate>Tue, 26 Aug 2025 16:41:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19183v1</guid></item><item><title>TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use</title><link>http://arxiv.org/abs/2412.15495v2</link><description>Large language models (LLMs) achieve remarkable advancements by leveragingtools to interact with environments, a critical step toward generalized AI.However, the standard supervised fine-tuning (SFT) approach, which relies onlarge-scale datasets, often overlooks task-specific characteristics in tooluse, leading to performance bottlenecks. To address this issue, we analyzethree existing LLMs and uncover key insights: training data can inadvertentlyimpede tool-use behavior, token importance is distributed unevenly, and errorsin tool calls fall into a small set of categories. Building on these findings,we propose~\emph{TL-Training}, a task-feature-based framework that mitigatesthe effects of suboptimal training data, dynamically adjusts token weights toprioritize key tokens during SFT, and incorporates a robust reward mechanismtailored to error categories, optimized through proximal policy optimization.We validate TL-Training by training CodeLLaMA-2-7B and evaluating it on fouropen-source test sets. Our results demonstrate that the LLM trained by ourmethod matches or surpasses both open- and closed-source LLMs in tool-useperformance using only 1,217 training data points. Additionally, our methodenhances robustness in noisy environments and improves general taskperformance, offering a scalable and efficient paradigm for tool-use trainingin LLMs. Code and data are available athttps://github.com/Junjie-Ye/TL-Training.</description><author>Junjie Ye, Yilong Wu, Sixian Li, Yuming Yang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan, Zhengyin Du</author><pubDate>Tue, 26 Aug 2025 16:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15495v2</guid></item><item><title>SoccerNet 2025 Challenges Results</title><link>http://arxiv.org/abs/2508.19182v1</link><description>The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNetopen benchmarking effort, dedicated to advancing computer vision research infootball video understanding. This year's challenges span four vision-basedtasks: (1) Team Ball Action Spotting, focused on detecting ball-related actionsin football broadcasts and assigning actions to teams; (2) Monocular DepthEstimation, targeting the recovery of scene geometry from single-camerabroadcast clips through relative depth estimation for each pixel; (3)Multi-View Foul Recognition, requiring the analysis of multiple synchronizedcamera views to classify fouls and their severity; and (4) Game StateReconstruction, aimed at localizing and identifying all players from abroadcast video to reconstruct the game state on a 2D top-view of the field.Across all tasks, participants were provided with large-scale annotateddatasets, unified evaluation protocols, and strong baselines as startingpoints. This report presents the results of each challenge, highlights thetop-performing solutions, and provides insights into the progress made by thecommunity. The SoccerNet Challenges continue to serve as a driving force forreproducible, open research at the intersection of computer vision, artificialintelligence, and sports. Detailed information about the tasks, challenges, andleaderboards can be found at https://www.soccer-net.org, with baselines anddevelopment kits available at https://github.com/SoccerNet.</description><author>Silvio Giancola, Anthony Cioppa, Marc Gutiérrez-Pérez, Jan Held, Carlos Hinojosa, Victor Joos, Arnaud Leduc, Floriane Magera, Karen Sanchez, Vladimir Somers, Artur Xarles, Antonio Agudo, Alexandre Alahi, Olivier Barnich, Albert Clapés, Christophe De Vleeschouwer, Sergio Escalera, Bernard Ghanem, Thomas B. Moeslund, Marc Van Droogenbroeck, Tomoki Abe, Saad Alotaibi, Faisal Altawijri, Steven Araujo, Xiang Bai, Xiaoyang Bi, Jiawang Cao, Vanyi Chao, Kamil Czarnogórski, Fabian Deuser, Mingyang Du, Tianrui Feng, Patrick Frenzel, Mirco Fuchs, Jorge García, Konrad Habel, Takaya Hashiguchi, Sadao Hirose, Xinting Hu, Yewon Hwang, Ririko Inoue, Riku Itsuji, Kazuto Iwai, Hongwei Ji, Yangguang Ji, Licheng Jiao, Yuto Kageyama, Yuta Kamikawa, Yuuki Kanasugi, Hyungjung Kim, Jinwook Kim, Takuya Kurihara, B</author><pubDate>Tue, 26 Aug 2025 16:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19182v1</guid></item><item><title>Consistent Opponent Modeling of Static Opponents in Imperfect-Information Games</title><link>http://arxiv.org/abs/2508.17671v2</link><description>The goal of agents in multi-agent environments is to maximize total rewardagainst the opposing agents that are encountered. Following a game-theoreticsolution concept, such as Nash equilibrium, may obtain a strong performance insome settings; however, such approaches fail to capitalize on historical andobserved data from repeated interactions against our opponents. Opponentmodeling algorithms integrate machine learning techniques to exploit suboptimalopponents utilizing available data; however, the effectiveness of suchapproaches in imperfect-information games to date is quite limited. We showthat existing opponent modeling approaches fail to satisfy a simple desirableproperty even against static opponents drawn from a known prior distribution;namely, they do not guarantee that the model approaches the opponent's truestrategy even in the limit as the number of game iterations approachesinfinity. We develop a new algorithm that is able to achieve this property andruns efficiently by solving a convex minimization problem based on thesequence-form game representation using projected gradient descent. Thealgorithm is guaranteed to efficiently converge to the opponent's true strategygiven observations from gameplay and possibly additional historical data if itis available.</description><author>Sam Ganzfried</author><pubDate>Tue, 26 Aug 2025 16:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17671v2</guid></item><item><title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2505.04594v5</link><description>Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description><author>Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu</author><pubDate>Tue, 26 Aug 2025 16:31:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.04594v5</guid></item><item><title>Image Coding for Machines via Feature-Preserving Rate-Distortion Optimization</title><link>http://arxiv.org/abs/2504.02216v2</link><description>Many images and videos are primarily processed by computer vision algorithms,involving only occasional human inspection. When this content requirescompression before processing, e.g., in distributed applications, codingmethods must optimize for both visual quality and downstream task performance.We first show theoretically that an approach to reduce the effect ofcompression for a given task loss is to perform rate-distortion optimization(RDO) using the distance between features, obtained from the original and thedecoded images, as a distortion metric. However, optimizing directly such arate-distortion objective is computationally impractical because it requiresiteratively encoding and decoding the entire image-plus feature evaluation-foreach possible coding configuration. We address this problem by simplifying theRDO formulation to make the distortion term computable using block-basedencoders. We first apply Taylor's expansion to the feature extractor, recastingthe feature distance as a quadratic metric involving the Jacobian matrix of theneural network. Then, we replace the linearized metric with a block-wiseapproximation, which we call input-dependent squared error (IDSE). To make themetric computable, we approximate IDSE using sketches of the Jacobian. Theresulting loss can be evaluated block-wise in the transform domain and combinedwith the sum of squared errors (SSE) to address both visual quality andcomputer vision performance. Simulations with AVC and HEVC across multiplefeature extractors and downstream networks show up to 17 % bit-rate savings forthe same task accuracy compared to RDO based on SSE, with no decoder complexityoverhead and a small (7.86 %) encoder complexity increase.</description><author>Samuel Fernández-Menduiña, Eduardo Pavez, Antonio Ortega</author><pubDate>Tue, 26 Aug 2025 16:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.02216v2</guid></item><item><title>Leveraging Evolutionary Surrogate-Assisted Prescription in Multi-Objective Chlorination Control Systems</title><link>http://arxiv.org/abs/2508.19173v1</link><description>This short, written report introduces the idea of EvolutionarySurrogate-Assisted Prescription (ESP) and presents preliminary results on itspotential use in training real-world agents as a part of the 1st AI forDrinking Water Chlorination Challenge at IJCAI-2025. This work was done by ateam from Project Resilience, an organization interested in bridging AI toreal-world problems.</description><author>Rivaaj Monsia, Olivier Francon, Daniel Young, Risto Miikkulainen</author><pubDate>Tue, 26 Aug 2025 16:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19173v1</guid></item><item><title>From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity</title><link>http://arxiv.org/abs/2508.19172v1</link><description>Autonomous skill discovery aims to enable robots to acquire diverse behaviorswithout explicit supervision. Learning such behaviors directly on physicalhardware remains challenging due to safety and data efficiency constraints.Existing methods, including Quality-Diversity Actor-Critic (QDAC), requiremanually defined skill spaces and carefully tuned heuristics, limitingreal-world applicability. We propose Unsupervised Real-world Skill Acquisition(URSA), an extension of QDAC that enables robots to autonomously discover andmaster diverse, high-performing skills directly in the real world. Wedemonstrate that URSA successfully discovers diverse locomotion skills on aUnitree A1 quadruped in both simulation and the real world. Our approachsupports both heuristic-driven skill discovery and fully unsupervised settings.We also show that the learned skill repertoire can be reused for downstreamtasks such as real-world damage adaptation, where URSA outperforms allbaselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.Our results establish a new framework for real-world robot learning thatenables continuous skill discovery with limited human intervention,representing a significant step toward more autonomous and adaptable roboticsystems. Demonstration videos are available athttp://adaptive-intelligent-robotics.github.io/URSA .</description><author>Luca Grillotti, Lisa Coiffard, Oscar Pang, Maxence Faldor, Antoine Cully</author><pubDate>Tue, 26 Aug 2025 16:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19172v1</guid></item><item><title>Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions</title><link>http://arxiv.org/abs/2508.19167v1</link><description>Vision Transformers have demonstrated remarkable success in computer visiontasks, yet their reliance on learnable one-dimensional positional embeddingsfundamentally disrupts the inherent two-dimensional spatial structure of imagesthrough patch flattening procedures. Traditional positional encoding approacheslack geometric constraints and fail to establish monotonic correspondencebetween Euclidean spatial distances and sequential index distances, therebylimiting the model's capacity to leverage spatial proximity priors effectively.We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), amathematically principled approach that directly addresses two-dimensionalcoordinates through natural complex domain representation, where the doublyperiodic properties of elliptic functions align remarkably with translationalinvariance patterns commonly observed in visual data. Our method exploits thenon-linear geometric nature of elliptic functions to encode spatial distancerelationships naturally, while the algebraic addition formula enables directderivation of relative positional information between arbitrary patch pairsfrom their absolute encodings. Comprehensive experiments demonstrate thatWEF-PE achieves superior performance across diverse scenarios, including63.78\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture,93.28\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements onVTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decayproperty through rigorous mathematical proof, while attention visualizationreveals enhanced geometric inductive bias and more coherent semantic focuscompared to conventional approaches.The source code implementing the methodsdescribed in this paper is publicly available on GitHub.</description><author>Zhihang Xin, Xitong Hu, Rui Wang</author><pubDate>Tue, 26 Aug 2025 16:14:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19167v1</guid></item><item><title>Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding</title><link>http://arxiv.org/abs/2508.19165v1</link><description>Monocular 3D visual grounding is a novel task that aims to locate 3D objectsin RGB images using text descriptions with explicit geometry information.Despite the inclusion of geometry details in the text, we observe that the textembeddings are sensitive to the magnitude of numerical values but largelyignore the associated measurement units. For example, simply equidistantmapping the length with unit "meter" to "decimeters" or "centimeters" leads tosevere performance degradation, even though the physical length remainsequivalent. This observation signifies the weak 3D comprehension of pre-trainedlanguage model, which generates misguiding text features to hinder 3Dperception. Therefore, we propose to enhance the 3D perception of model on textembeddings and geometry features with two simple and effective methods.Firstly, we introduce a pre-processing method named 3D-text Enhancement (3DTE),which enhances the comprehension of mapping relationships between differentunits by augmenting the diversity of distance descriptors in text queries.Next, we propose a Text-Guided Geometry Enhancement (TGE) module to furtherenhance the 3D-text information by projecting the basic text features intogeometrically consistent space. These 3D-enhanced text features are thenleveraged to precisely guide the attention of geometry features. We evaluatethe proposed method through extensive comparisons and ablation studies on theMono3DRefer dataset. Experimental results demonstrate substantial improvementsover previous methods, achieving new state-of-the-art results with a notableaccuracy gain of 11.94\% in the "Far" scenario. Our code will be made publiclyavailable.</description><author>Yuzhen Li, Min Liu, Yuan Bian, Xueping Wang, Zhaoyang Li, Gen Li, Yaonan Wang</author><pubDate>Tue, 26 Aug 2025 16:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19165v1</guid></item><item><title>MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation</title><link>http://arxiv.org/abs/2508.19163v1</link><description>Despite the growing use of large language models (LLMs) in clinical dialoguesystems, existing evaluations focus on task completion or fluency, offeringlittle insight into the behavioral and risk management requirements essentialfor safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTionfRamework for safe Interactions and conteXtual clinical conversationalevaluation), a structured, extensible framework for safety-oriented evaluationof clinical dialogue agents. MATRIX integrates three components: (1) a safety-aligned taxonomy of clinicalscenarios, expected system behaviors and failure modes derived throughstructured safety engineering methods; (2) BehvJudge, an LLM-based evaluatorfor detecting safety-relevant dialogue failures, validated against expertclinician annotations; and (3) PatBot, a simulated patient agent capable ofproducing diverse, scenario-conditioned responses, evaluated for realism andbehavioral fidelity with human factors expertise, and a patient-preferencestudy. Across three experiments, we show that MATRIX enables systematic, scalablesafety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazarddetection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blindedassessment of 240 dialogues. We also conducted one of the first realismanalyses of LLM-based patient simulation, showing that PatBot reliablysimulates realistic patient behavior in quantitative and qualitativeevaluations. Using MATRIX, we demonstrate its effectiveness in benchmarkingfive LLM agents across 2,100 simulated dialogues spanning 14 hazard scenariosand 10 clinical domains. MATRIX is the first framework to unify structured safety engineering withscalable, validated conversational AI evaluation, enabling regulator-alignedsafety auditing. We release all evaluation tools, prompts, structuredscenarios, and datasets.</description><author>Ernest Lim, Yajie Vera He, Jared Joselowitz, Kate Preston, Mohita Chowdhury, Louis Williams, Aisling Higham, Katrina Mason, Mariane Melo, Tom Lawton, Yan Jia, Ibrahim Habli</author><pubDate>Tue, 26 Aug 2025 16:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19163v1</guid></item><item><title>Few-Shot Connectivity-Aware Text Line Segmentation in Historical Documents</title><link>http://arxiv.org/abs/2508.19162v1</link><description>A foundational task for the digital analysis of documents is text linesegmentation. However, automating this process with deep learning models ischallenging because it requires large, annotated datasets that are oftenunavailable for historical documents. Additionally, the annotation process is alabor- and cost-intensive task that requires expert knowledge, which makesfew-shot learning a promising direction for reducing data requirements. In thiswork, we demonstrate that small and simple architectures, coupled with atopology-aware loss function, are more accurate and data-efficient than morecomplex alternatives. We pair a lightweight UNet++ with a connectivity-awareloss, initially developed for neuron morphology, which explicitly penalizesstructural errors like line fragmentation and unintended line merges. Toincrease our limited data, we train on small patches extracted from a merethree annotated pages per manuscript. Our methodology significantly improvesupon the current state-of-the-art on the U-DIADS-TL dataset, with a 200%increase in Recognition Accuracy and a 75% increase in Line Intersection overUnion. Our method also achieves an F-Measure score on par with or evenexceeding that of the competition winner of the DIVA-HisDB baseline detectiontask, all while requiring only three annotated pages, exemplifying the efficacyof our approach. Our implementation is publicly available at:https://github.com/RafaelSterzinger/acpr_few_shot_hist.</description><author>Rafael Sterzinger, Tingyu Lin, Robert Sablatnig</author><pubDate>Tue, 26 Aug 2025 16:11:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19162v1</guid></item><item><title>RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration</title><link>http://arxiv.org/abs/2508.19154v1</link><description>We present the RAW domain diffusion model (RDDM), an end-to-end diffusionmodel that restores photo-realistic images directly from the sensor RAW data.While recent sRGB-domain diffusion methods achieve impressive results, they arecaught in a dilemma between high fidelity and realistic generation. As thesemodels process lossy sRGB inputs and neglect the accessibility of the sensorRAW images in many scenarios, e.g., in image and video capturing in edgedevices, resulting in sub-optimal performance. RDDM bypasses this limitation bydirectly restoring images in the RAW domain, replacing the conventionaltwo-stage image signal processing (ISP) + IR pipeline. However, a simpleadaptation of pre-trained diffusion models to the RAW domain confronts theout-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE(RVAE) learning optimal latent representations, (2) a differentiable Post ToneProcessing (PTP) module enabling joint RAW and sRGB space optimization. Tocompensate for the deficiency in the dataset, we develop a scalable degradationpipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets forlarge-scale training. Furthermore, we devise a configurable multi-bayer (CMB)LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensiveexperiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusionmethods, yielding higher fidelity results with fewer artifacts.</description><author>Yan Chen, Yi Wen, Wei Li, Junchao Liu, Yong Guo, Jie Hu, Xinghao Chen</author><pubDate>Tue, 26 Aug 2025 16:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19154v1</guid></item><item><title>MicroMIL: Graph-Based Multiple Instance Learning for Context-Aware Diagnosis with Microscopic Images</title><link>http://arxiv.org/abs/2407.21604v4</link><description>Cancer diagnosis has greatly benefited from the integration of whole-slideimages (WSIs) with multiple instance learning (MIL), enabling high-resolutionanalysis of tissue morphology. Graph-based MIL (GNN-MIL) approaches haveemerged as powerful solutions for capturing contextual information in WSIs,thereby improving diagnostic accuracy. However, WSIs require significantcomputational and infrastructural resources, limiting accessibility inresource-constrained settings. Conventional light microscopes offer acost-effective alternative, but applying GNN-MIL to such data is challengingdue to extensive redundant images and missing spatial coordinates, which hindercontextual learning. To address these issues, we introduce MicroMIL, the firstweakly-supervised MIL framework specifically designed for images acquired fromconventional light microscopes. MicroMIL leverages a representative imageextractor (RIE) that employs deep cluster embedding (DCE) and hardGumbel-Softmax to dynamically reduce redundancy and select representativeimages. These images serve as graph nodes, with edges computed via cosinesimilarity, eliminating the need for spatial coordinates while preservingcontextual information. Extensive experiments on a real-world colon cancerdataset and the BreakHis dataset demonstrate that MicroMIL achievesstate-of-the-art performance, improving both diagnostic accuracy and robustnessto redundancy. The code is available athttps://github.com/kimjongwoo-cell/MicroMIL</description><author>Jongwoo Kim, Bryan Wong, Huazhu Fu, Willmer Rafell Quiñones, Youngsin Ko, Mun Yong Yi</author><pubDate>Tue, 26 Aug 2025 16:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21604v4</guid></item><item><title>Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games</title><link>http://arxiv.org/abs/2508.19152v1</link><description>Contemporary artificial intelligence (AI) development largely centers onrational decision-making, valued for its measurability and suitability forobjective evaluation. Yet in real-world contexts, an intelligent agent'sdecisions are shaped not only by logic but also by deeper influences such asbeliefs, values, and preferences. The diversity of human decision-making stylesemerges from these differences, highlighting that "style" is an essential butoften overlooked dimension of intelligence. This dissertation introduces playstyle as an alternative lens for observingand analyzing the decision-making behavior of intelligent agents, and examinesits foundational meaning and historical context from a philosophicalperspective. By analyzing how beliefs and values drive intentions and actions,we construct a two-tier framework for style formation: the external interactionloop with the environment and the internal cognitive loop of deliberation. Onthis basis, we formalize style-related characteristics and propose measurableindicators such as style capacity, style popularity, and evolutionary dynamics. The study focuses on three core research directions: (1) Defining andmeasuring playstyle, proposing a general playstyle metric based on discretizedstate spaces, and extending it to quantify strategic diversity and competitivebalance; (2) Expressing and generating playstyle, exploring how reinforcementlearning and imitation learning can be used to train agents exhibiting specificstylistic tendencies, and introducing a novel approach for human-like stylelearning and modeling; and (3) Practical applications, analyzing the potentialof these techniques in domains such as game design and interactiveentertainment. Finally, the dissertation outlines future extensions, including the role ofstyle as a core element in building artificial general intelligence (AGI).</description><author>Chiu-Chou Lin</author><pubDate>Tue, 26 Aug 2025 16:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19152v1</guid></item><item><title>Generative Artificial Intelligence-Supported Pentesting: A Comparison between Claude Opus, GPT-4, and Copilot</title><link>http://arxiv.org/abs/2501.06963v2</link><description>The advent of Generative Artificial Intelligence (GenAI) has brought asignificant change to our society. GenAI can be applied across numerous fields,with particular relevance in cybersecurity. Among the various areas ofapplication, its use in penetration testing (pentesting) or ethical hackingprocesses is of special interest. In this paper, we have analyzed the potentialof leading generic-purpose GenAI tools-Claude Opus, GPT-4 from ChatGPT, andCopilot-in augmenting the penetration testing process as defined by thePenetration Testing Execution Standard (PTES). Our analysis involved evaluatingeach tool across all PTES phases within a controlled virtualized environment.The findings reveal that, while these tools cannot fully automate thepentesting process, they provide substantial support by enhancing efficiencyand effectiveness in specific tasks. Notably, all tools demonstrated utility;however, Claude Opus consistently outperformed the others in our experimentalscenarios.</description><author>Antonio López Martínez, Alejandro Cano, Antonio Ruiz-Martínez</author><pubDate>Tue, 26 Aug 2025 16:03:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06963v2</guid></item><item><title>Saddle Hierarchy in Dense Associative Memory</title><link>http://arxiv.org/abs/2508.19151v1</link><description>Dense associative memory (DAM) models have been attracting renewed attentionsince they were shown to be robust to adversarial examples and closely relatedto state-of-the-art machine learning paradigms, such as the attentionmechanisms in transformers and generative diffusion models. We study a DAMbuilt upon a three-layer Boltzmann machine with Potts hidden units, whichrepresent data clusters and classes. Through a statistical mechanics analysis,we derive saddle-point equations that characterize both the stationary pointsof DAMs trained on real data and the fixed points of DAMs trained on syntheticdata within a teacher-student framework. Based on these results, we propose anovel regularization scheme that makes training significantly more stable.Moreover, we show empirically that our DAM learns interpretable solutions toboth supervised and unsupervised classification problems. Pushing ourtheoretical analysis further, we find that the weights learned by relativelysmall DAMs correspond to unstable saddle points in larger DAMs. We implement anetwork-growing algorithm that leverages this saddle-point hierarchy todrastically reduce the computational cost of training dense associative memory.</description><author>Robin Thériault, Daniele Tantari</author><pubDate>Tue, 26 Aug 2025 16:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19151v1</guid></item><item><title>Leveraging Multi-facet Paths for Heterogeneous Graph Representation Learning</title><link>http://arxiv.org/abs/2407.20648v3</link><description>Recent advancements in graph neural networks (GNNs) and heterogeneous GNNs(HGNNs) have advanced node embeddings and relationship learning for varioustasks. However, existing methods often rely on domain-specific predefinedmeta-paths, which are coarse-grained and focus solely on aspects like nodetype, limiting their ability to capture complex interactions. We introduceMF2Vec, a model that uses multi-faceted (fine-grained) paths instead ofpredefined meta-paths. MF2Vec extracts paths via random walks and generatesmulti-faceted vectors, ignoring predefined schemas. This method learns diverseaspects of nodes and their relationships, constructs a homogeneous network, andcreates node embeddings for classification, link prediction, and clustering.Extensive experiments show that MF2Vec outperforms existing methods, offering amore flexible and comprehensive framework for analyzing complex networks. Thecode is available at https://anonymous.4open.science/r/MF2Vec-6ABC.</description><author>Jongwoo Kim, Seongyeub Chu, Hyeongmin Park, Bryan Wong, Keejun Han, Mun Yong Yi</author><pubDate>Tue, 26 Aug 2025 16:02:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20648v3</guid></item><item><title>Uncertainty-Resilient Active Intention Recognition for Robotic Assistants</title><link>http://arxiv.org/abs/2508.19150v1</link><description>Purposeful behavior in robotic assistants requires the integration ofmultiple components and technological advances. Often, the problem is reducedto recognizing explicit prompts, which limits autonomy, or is oversimplifiedthrough assumptions such as near-perfect information. We argue that a criticalgap remains unaddressed -- specifically, the challenge of reasoning about theuncertain outcomes and perception errors inherent to human intentionrecognition. In response, we present a framework designed to be resilient touncertainty and sensor noise, integrating real-time sensor data with acombination of planners. Centered around an intention-recognition POMDP, ourapproach addresses cooperative planning and acting under uncertainty. Ourintegrated framework has been successfully tested on a physical robot withpromising results.</description><author>Juan Carlos Saborío, Marc Vinci, Oscar Lima, Sebastian Stock, Lennart Niecksch, Martin Günther, Alexander Sung, Joachim Hertzberg, Martin Atzmüller</author><pubDate>Tue, 26 Aug 2025 16:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19150v1</guid></item><item><title>Algorithmic Collective Action with Multiple Collectives</title><link>http://arxiv.org/abs/2508.19149v1</link><description>As learning systems increasingly influence everyday decisions, user-sidesteering via Algorithmic Collective Action (ACA)-coordinated changes to shareddata-offers a complement to regulator-side policy and firm-side model design.Although real-world actions have been traditionally decentralized andfragmented into multiple collectives despite sharing overarchingobjectives-with each collective differing in size, strategy, and actionablegoals, most of the ACA literature focused on single collective settings. Inthis work, we present the first theoretical framework for ACA with multiplecollectives acting on the same system. In particular, we focus on collectiveaction in classification, studying how multiple collectives can plant signals,i.e., bias a classifier to learn an association between an altered version ofthe features and a chosen, possibly overlapping, set of target classes. Weprovide quantitative results about the role and the interplay of collectives'sizes and their alignment of goals. Our framework, by also complementingprevious empirical results, opens a path for a holistic treatment of ACA withmultiple collectives.</description><author>Claudio Battiloro, Pietro Greiner, Bret Nestor, Oumaima Amezgar, Francesca Dominici</author><pubDate>Tue, 26 Aug 2025 16:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19149v1</guid></item><item><title>Echoes of the past: A unified perspective on fading memory and echo states</title><link>http://arxiv.org/abs/2508.19145v1</link><description>Recurrent neural networks (RNNs) have become increasingly popular ininformation processing tasks involving time series and temporal data. Afundamental property of RNNs is their ability to create reliable input/outputresponses, often linked to how the network handles its memory of theinformation it processed. Various notions have been proposed to conceptualizethe behavior of memory in RNNs, including steady states, echo states, stateforgetting, input forgetting, and fading memory. Although these notions areoften used interchangeably, their precise relationships remain unclear. Thiswork aims to unify these notions in a common language, derive new implicationsand equivalences between them, and provide alternative proofs to some existingresults. By clarifying the relationships between these concepts, this researchcontributes to a deeper understanding of RNNs and their temporal informationprocessing capabilities.</description><author>Juan-Pablo Ortega, Florian Rossmannek</author><pubDate>Tue, 26 Aug 2025 15:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19145v1</guid></item><item><title>GeNet: A Multimodal LLM-Based Co-Pilot for Network Topology and Configuration</title><link>http://arxiv.org/abs/2407.08249v2</link><description>Communication network engineering in enterprise environments is traditionallya complex, time-consuming, and error-prone manual process. Most research onnetwork engineering automation has concentrated on configuration synthesis,often overlooking changes in the physical network topology. This paperintroduces GeNet, a multimodal co-pilot for enterprise network engineers. GeNetis a novel framework that leverages a large language model (LLM) to streamlinenetwork design workflows. It uses visual and textual modalities to interpretand update network topologies and device configurations based on user intents.GeNet was evaluated on enterprise network scenarios adapted from Ciscocertification exercises. Our results demonstrate GeNet's ability to interpretnetwork topology images accurately, potentially reducing network engineers'efforts and accelerating network design processes in enterprise environments.Furthermore, we show the importance of precise topology understanding whenhandling intents that require modifications to the network's topology.</description><author>Beni Ifland, Elad Duani, Rubin Krief, Miro Ohana, Aviram Zilberman, Andres Murillo, Ofir Manor, Ortal Lavi, Hikichi Kenji, Asaf Shabtai, Yuval Elovici, Rami Puzis</author><pubDate>Tue, 26 Aug 2025 15:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08249v2</guid></item><item><title>A Bag of Tricks for Efficient Implicit Neural Point Clouds</title><link>http://arxiv.org/abs/2508.19140v1</link><description>Implicit Neural Point Cloud (INPC) is a recent hybrid representation thatcombines the expressiveness of neural fields with the efficiency of point-basedrendering, achieving state-of-the-art image quality in novel view synthesis.However, as with other high-quality approaches that query neural networksduring rendering, the practical usability of INPC is limited by comparativelyslow rendering. In this work, we present a collection of optimizations thatsignificantly improve both the training and inference performance of INPCwithout sacrificing visual fidelity. The most significant modifications are animproved rasterizer implementation, more effective sampling techniques, and theincorporation of pre-training for the convolutional neural network used forhole-filling. Furthermore, we demonstrate that points can be modeled as smallGaussians during inference to further improve quality in extrapolated, e.g.,close-up views of the scene. We design our implementations to be broadlyapplicable beyond INPC and systematically evaluate each modification in aseries of experiments. Our optimized INPC pipeline achieves up to 25% fastertraining, 2x faster rendering, and 20% reduced VRAM usage paired with slightimage quality improvements.</description><author>Florian Hahlbohm, Linus Franke, Leon Overkämping, Paula Wespe, Susana Castillo, Martin Eisemann, Marcus Magnor</author><pubDate>Tue, 26 Aug 2025 15:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19140v1</guid></item><item><title>Generative Data Augmentation for Object Point Cloud Segmentation</title><link>http://arxiv.org/abs/2505.17783v2</link><description>Data augmentation is widely used to train deep learning models to addressdata scarcity. However, traditional data augmentation (TDA) typically relies onsimple geometric transformation, such as random rotation and rescaling,resulting in minimal data diversity enrichment and limited model performanceimprovement. State-of-the-art generative models for 3D shape generation rely onthe denoising diffusion probabilistic models and manage to generate realisticnovel point clouds for 3D content creation and manipulation. Nevertheless, thegenerated 3D shapes lack associated point-wise semantic labels, restrictingtheir usage in enlarging the training data for point cloud segmentation tasks.To bridge the gap between data augmentation techniques and the advanceddiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to apart-aware generative model that can generate high-quality point cloudsconditioned on given segmentation masks. Leveraging the novel generative model,we introduce a 3-step generative data augmentation (GDA) pipeline for pointcloud segmentation training. Our GDA approach requires only a small amount oflabeled samples but enriches the training data with generated variants andpseudo-labeled samples, which are validated by a novel diffusion-basedpseudo-label filtering method. Extensive experiments on two large-scalesynthetic datasets and a real-world medical dataset demonstrate that our GDAmethod outperforms TDA approach and related semi-supervised and self-supervisedmethods.</description><author>Dekai Zhu, Stefan Gavranovic, Flavien Boussuge, Benjamin Busam, Slobodan Ilic</author><pubDate>Tue, 26 Aug 2025 15:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.17783v2</guid></item><item><title>ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context</title><link>http://arxiv.org/abs/2407.06866v3</link><description>While the biases of language models in production are extensively documented,the biases of their guardrails have been neglected. This paper studies howcontextual information about the user influences the likelihood of an LLM torefuse to execute a request. By generating user biographies that offerideological and demographic information, we find a number of biases inguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personasare more likely to trigger a refusal guardrail when requesting censored orillegal information. Guardrails are also sycophantic, refusing to comply withrequests for a political position the user is likely to disagree with. We findthat certain identity groups and seemingly innocuous information, e.g., sportsfandom, can elicit changes in guardrail sensitivity similar to directstatements of political ideology. For each demographic category and even forAmerican football team fandom, we find that ChatGPT appears to infer a likelypolitical ideology and modify guardrail behavior accordingly.</description><author>Victoria R. Li, Yida Chen, Naomi Saphra</author><pubDate>Tue, 26 Aug 2025 15:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06866v3</guid></item><item><title>Active Query Selection for Crowd-Based Reinforcement Learning</title><link>http://arxiv.org/abs/2508.19132v1</link><description>Preference-based reinforcement learning has gained prominence as a strategyfor training agents in environments where the reward signal is difficult tospecify or misaligned with human intent. However, its effectiveness is oftenlimited by the high cost and low availability of reliable human input,especially in domains where expert feedback is scarce or errors are costly. Toaddress this, we propose a novel framework that combines two complementarystrategies: probabilistic crowd modelling to handle noisy, multi-annotatorfeedback, and active learning to prioritize feedback on the most informativeagent actions. We extend the Advise algorithm to support multiple trainers,estimate their reliability online, and incorporate entropy-based queryselection to guide feedback requests. We evaluate our approach in a set ofenvironments that span both synthetic and real-world-inspired settings,including 2D games (Taxi, Pacman, Frozen Lake) and a blood glucose control taskfor Type 1 Diabetes using the clinically approved UVA/Padova simulator. Ourpreliminary results demonstrate that agents trained with feedback on uncertaintrajectories exhibit faster learning in most tasks, and we outperform thebaselines for the blood glucose control task.</description><author>Jonathan Erskine, Taku Yamagata, Raúl Santos-Rodríguez</author><pubDate>Tue, 26 Aug 2025 15:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19132v1</guid></item><item><title>Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions</title><link>http://arxiv.org/abs/2507.06029v2</link><description>Explainable AI (XAI) methods often struggle to generate clear, interpretableoutputs for users without domain expertise. We introduce Feature-GuidedNeighbor Selection (FGNS), a post hoc method that enhances interpretability byselecting class-representative examples using both local and global featureimportance. In a user study (N = 98) evaluating Kannada script classifications,FGNS significantly improved non-experts' ability to identify model errors whilemaintaining appropriate agreement with correct predictions. Participants madefaster and more accurate decisions compared to those given traditional k-NNexplanations. Quantitative analysis shows that FGNS selects neighbors thatbetter reflect class characteristics rather than merely minimizingfeature-space distance, leading to more consistent selection and tighterclustering around class prototypes. These results support FGNS as a step towardmore human-aligned model assessment, although further work is needed to addressthe gap between explanation quality and perceived trust.</description><author>Courtney Ford, Mark T. Keane</author><pubDate>Tue, 26 Aug 2025 15:33:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.06029v2</guid></item><item><title>ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments</title><link>http://arxiv.org/abs/2508.19131v1</link><description>The advancement of robotics and autonomous navigation systems hinges on theability to accurately predict terrain traversability. Traditional methods forgenerating datasets to train these prediction models often involve puttingrobots into potentially hazardous environments, posing risks to equipment andsafety. To solve this problem, we present ZeST, a novel approach leveragingvisual reasoning capabilities of Large Language Models (LLMs) to create atraversability map in real-time without exposing robots to danger. Our approachnot only performs zero-shot traversability and mitigates the risks associatedwith real-world data collection but also accelerates the development ofadvanced navigation systems, offering a cost-effective and scalable solution.To support our findings, we present navigation results, in both controlledindoor and unstructured outdoor environments. As shown in the experiments, ourmethod provides safer navigation when compared to other state-of-the-artmethods, constantly reaching the final goal.</description><author>Shreya Gummadi, Mateus V. Gasparino, Gianluca Capezzuto, Marcelo Becker, Girish Chowdhary</author><pubDate>Tue, 26 Aug 2025 15:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19131v1</guid></item><item><title>A Survey on Data Selection for LLM Instruction Tuning</title><link>http://arxiv.org/abs/2402.05123v3</link><description>Instruction tuning is a vital step of training large language models (LLMs),so how to enhance the effect of instruction tuning has received increasedattention. Existing works indicate that the quality of the dataset is morecrucial than the quantity during instruction tuning of LLMs. Therefore,recently a lot of studies focus on exploring the methods of selectinghigh-quality subset from instruction datasets, aiming to reduce training costsand enhance the instruction-following capabilities of LLMs. This paper presentsa comprehensive survey on data selection for LLM instruction tuning. Firstly,we introduce the wildly used instruction datasets. Then, we propose a newtaxonomy of the data selection methods and provide a detailed introduction ofrecent advances, and the evaluation strategies and results of data selectionmethods are also elaborated in detail. Finally, we emphasize the openchallenges and present new frontiers of this task.</description><author>Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu</author><pubDate>Tue, 26 Aug 2025 15:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05123v3</guid></item><item><title>An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal, and Deterministic Approach</title><link>http://arxiv.org/abs/2505.00039v4</link><description>Retrieval-Augmented Generation (RAG) systems in the legal domain face acritical challenge: standard, flat-text retrieval is blind to the hierarchical,diachronic, and causal structure of law, leading to anachronistic andunreliable answers. This paper introduces an ontology-driven Graph RAGframework designed to overcome these limitations. We ground our knowledge graphin a formal, LRMoo-inspired model that distinguishes abstract legal Works fromtheir versioned Expressions. We model temporal states as efficient aggregationsthat reuse the versioned expressions (CTVs) of unchanged components, and wereify legislative events as first-class Action nodes to make causality explicitand queryable. This structured backbone enables a unified, planner-guided querystrategy that applies explicit policies to deterministically resolve complexrequests for (i) point-in-time retrieval, (ii) hierarchical impact analysis,and (iii) auditable provenance reconstruction. Through a case study on theBrazilian Constitution, we demonstrate how this approach provides a verifiable,temporally-correct substrate for LLMs, enabling higher-order analyticalcapabilities while drastically reducing the risk of factual errors. The resultis a practical framework for building more trustworthy and explainable legal AIsystems.</description><author>Hudson de Martim</author><pubDate>Tue, 26 Aug 2025 15:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.00039v4</guid></item><item><title>Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis</title><link>http://arxiv.org/abs/2406.12719v4</link><description>Large Language Models (LLMs), already shown to ace various unstructured textcomprehension tasks, have also remarkably been shown to tackle table(structured) comprehension tasks without specific training. Building on earlierstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),model scale, instruction tuning, and domain bias affect Tabular QA (TQA)robustness by testing LLMs, under diverse augmentations and perturbations, ondiverse domains: Wikipedia-based $\textbf{WTQ}$, financial $\textbf{TAT-QA}$,and scientific $\textbf{SCITAB}$. Although instruction tuning and larger, newerLLMs deliver stronger, more robust TQA performance, data contamination andreliability issues, especially on $\textbf{WTQ}$, remain unresolved. Through anin-depth attention analysis, we reveal a strong correlation betweenperturbation-induced shifts in attention dispersion and the drops inperformance, with sensitivity peaking in the model's middle layers. Wehighlight the need for improved interpretable methodologies to develop morereliable LLMs for table comprehension. Through an in-depth attention analysis,we reveal a strong correlation between perturbation-induced shifts in attentiondispersion and performance drops, with sensitivity peaking in the model'smiddle layers. Based on these findings, we argue for the development ofstructure-aware self-attention mechanisms and domain-adaptive processingtechniques to improve the transparency, generalization, and real-worldreliability of LLMs on tabular data.</description><author>Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao</author><pubDate>Tue, 26 Aug 2025 15:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12719v4</guid></item><item><title>StagFormer: Time Staggering Transformer Decoding for RunningLayers In Parallel</title><link>http://arxiv.org/abs/2501.15665v2</link><description>Decoding in a Transformer based language model is inherently sequential as atoken's embedding needs to pass through all the layers in the network beforethe generation of the next token can begin. In this work, we propose a newarchitecture StagFormer (Staggered Transformer), which staggers execution alongthe sequence axis and thereby enables parallelizing the decoding process alongthe depth of the model. We achieve this by breaking the dependency of the tokenrepresentation at time step $i$ in layer $l$ upon the representations of tokensuntil time step $i$ from layer $l-1$. Instead, we stagger the execution andonly allow a dependency on token representations until time step $i-1$. Thelater sections of the Transformer still get access to the "rich"representations from the prior section but only from those token positionswhich are one time step behind. StagFormer allows for different sections of themodel to be executed in parallel yielding a potential speedup in decoding whilebeing quality neutral in our simulations. We also explore many naturalextensions of this idea. We present how weight-sharing across the differentsections being staggered can be more practical in settings with limited memory.We explore the efficacy of using a bounded window attention to pass informationfrom one section to another which helps drive further latency gains for someapplications. We also explore the scalability of the staggering idea over morethan 2 sections of the Transformer. Finally, we show how one can approximate arecurrent model during inference using weight-sharing. This variant can lead tosubstantial gains in quality for short generations while being neutral in itslatency impact.</description><author>Dylan Cutler, Arun Kandoor, Nishanth Dikkala, Nikunj Saunshi, Xin Wang, Rina Panigrahy</author><pubDate>Tue, 26 Aug 2025 15:21:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.15665v2</guid></item><item><title>Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding</title><link>http://arxiv.org/abs/2304.03907v6</link><description>This paper proposes an approach, Spectral Dynamics Embedding Control (SDEC),to optimal control for nonlinear stochastic systems. This method reveals aninfinite-dimensional feature representation induced by the system's nonlinearstochastic dynamics, enabling a linear representation of the state-action valuefunction. For practical implementation, this representation is approximatedusing finite-dimensional truncations, specifically via two prominent kernelapproximation methods: random feature truncation and Nystrom approximation. Tocharacterize the effectiveness of these approximations, we provide an in-depththeoretical analysis to characterize the approximation error arising from thefinite-dimension truncation and statistical error due to finite-sampleapproximation in both policy evaluation and policy optimization. Empirically,our algorithm performs favorably against existing stochastic control algorithmson several benchmark problems.</description><author>Zhaolin Ren, Tongzheng Ren, Haitong Ma, Na Li, Bo Dai</author><pubDate>Tue, 26 Aug 2025 15:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03907v6</guid></item><item><title>MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and Improved GRU</title><link>http://arxiv.org/abs/2410.20679v3</link><description>As financial markets grow increasingly complex in the big data era, accuratestock prediction has become more critical. Traditional time series models, suchas GRUs, have been widely used but often struggle to capture the intricatenonlinear dynamics of markets, particularly in the flexible selection andeffective utilization of key historical information. Recently, methods likeGraph Neural Networks and Reinforcement Learning have shown promise in stockprediction but require high data quality and quantity, and they tend to exhibitinstability when dealing with data sparsity and noise. Moreover, the trainingand inference processes for these models are typically complex andcomputationally expensive, limiting their broad deployment in practicalapplications. Existing approaches also generally struggle to captureunobservable latent market states effectively, such as market sentiment andexpectations, microstructural factors, and participant behavior patterns,leading to an inadequate understanding of market dynamics and subsequentlyimpact prediction accuracy. To address these challenges, this paper proposes astock prediction model, MCI-GRU, based on a multi-head cross-attentionmechanism and an improved GRU. First, we enhance the GRU model by replacing thereset gate with an attention mechanism, thereby increasing the model'sflexibility in selecting and utilizing historical information. Second, wedesign a multi-head cross-attention mechanism for learning unobservable latentmarket state representations, which are further enriched through interactionswith both temporal features and cross-sectional features. Finally, extensiveexperiments on four main stock markets show that the proposed methodoutperforms SOTA techniques across multiple metrics. Additionally, itssuccessful application in real-world fund management operations confirms itseffectiveness and practicality.</description><author>Peng Zhu, Yuante Li, Yifan Hu, Sheng Xiang, Qinyuan Liu, Dawei Cheng, Yuqi Liang</author><pubDate>Tue, 26 Aug 2025 15:18:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.20679v3</guid></item><item><title>SecureV2X: An Efficient and Privacy-Preserving System for Vehicle-to-Everything (V2X) Applications</title><link>http://arxiv.org/abs/2508.19115v1</link><description>Autonomous driving and V2X technologies have developed rapidly in the pastdecade, leading to improved safety and efficiency in modern transportation.These systems interact with extensive networks of vehicles, roadsideinfrastructure, and cloud resources to support their machine learningcapabilities. However, the widespread use of machine learning in V2X systemsraises issues over the privacy of the data involved. This is particularlyconcerning for smart-transit and driver safety applications which canimplicitly reveal user locations or explicitly disclose medical data such asEEG signals. To resolve these issues, we propose SecureV2X, a scalable,multi-agent system for secure neural network inferences deployed between theserver and each vehicle. Under this setting, we study two multi-agent V2Xapplications: secure drowsiness detection, and secure red-light violationdetection. Our system achieves strong performance relative to baselines, andscales efficiently to support a large number of secure computation interactionssimultaneously. For instance, SecureV2X is $9.4 \times$ faster, requires$143\times$ fewer computational rounds, and involves $16.6\times$ lesscommunication on drowsiness detection compared to other secure systems.Moreover, it achieves a runtime nearly $100\times$ faster than state-of-the-artbenchmarks in object detection tasks for red light violation detection.</description><author>Joshua Lee, Ali Arastehfard, Weiran Liu, Xuegang Ban, Yuan Hong</author><pubDate>Tue, 26 Aug 2025 15:17:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19115v1</guid></item><item><title>Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning</title><link>http://arxiv.org/abs/2508.19113v1</link><description>Large reasoning models (LRMs) have demonstrated strong performance incomplex, multi-step reasoning tasks. Existing methods enhance LRMs bysequentially integrating external knowledge retrieval; models iterativelygenerate queries, retrieve external information, and progressively reason overthis information. However, purely sequential querying increases inferencelatency and context length, diminishing coherence and potentially reducingaccuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep SearchQA), a synthetic dataset automatically generated from Natural Questions,explicitly designed to train LRMs to distinguish parallelizable from sequentialqueries. HDS-QA comprises hybrid-hop questions that combine parallelizableindependent subqueries (executable simultaneously) and sequentially dependentsubqueries (requiring step-by-step resolution), along with syntheticreasoning-querying-retrieval paths involving parallel queries. We fine-tune anLRM using HDS-QA, naming the model HybridDeepSearcher, which outperformsstate-of-the-art baselines across multiple benchmarks, notably achieving +15.9and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, bothrequiring comprehensive and exhaustive search. Experimental results highlighttwo key advantages: HybridDeepSearcher reaches comparable accuracy with fewersearch turns, significantly reducing inference latency, and it effectivelyscales as more turns are permitted. These results demonstrate the efficiency,scalability, and effectiveness of explicitly training LRMs to leverage hybridparallel and sequential querying.</description><author>Dayoon Ko, Jihyuk Kim, Haeju Park, Sohyeon Kim, Dahyun Lee, Yongrae Jo, Gunhee Kim, Moontae Lee, Kyungjae Lee</author><pubDate>Tue, 26 Aug 2025 15:15:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19113v1</guid></item><item><title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title><link>http://arxiv.org/abs/2508.19112v1</link><description>Accurate detection and segmentation of cancerous lesions from computedtomography (CT) scans is essential for automated treatment planning and cancertreatment response assessment. Transformer-based models with self-supervisedpretraining can produce reliably accurate segmentation from in-distribution(ID) data but degrade when applied to out-of-distribution (OOD) datasets. Weaddress this challenge with RF-Deep, a random forest classifier that utilizesdeep features from a pretrained transformer encoder of the segmentation modelto detect OOD scans and enhance segmentation reliability. The segmentationmodel comprises a Swin Transformer encoder, pretrained with masked imagemodeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous andnon-cancerous conditions, with a convolution decoder, trained to segment lungcancers in 317 3D scans. Independent testing was performed on 603 3D CT publicdatasets that included one ID dataset and four OOD datasets comprising chestCTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidneycancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs,consistently outperforming established OOD approaches. The RF-Deep classifierprovides a simple and effective approach to enhance reliability of cancersegmentation in ID and OOD scenarios.</description><author>Aneesh Rangnekar, Harini Veeraraghavan</author><pubDate>Tue, 26 Aug 2025 15:14:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19112v1</guid></item><item><title>Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs</title><link>http://arxiv.org/abs/2508.19111v1</link><description>Large vision-language models (LVLMs) demonstrate strong visual questionanswering (VQA) capabilities but are shown to hallucinate. A reliable modelshould perceive its knowledge boundaries-knowing what it knows and what it doesnot. This paper investigates LVLMs' perception of their knowledge boundaries byevaluating three types of confidence signals: probabilistic confidence, answerconsistency-based confidence, and verbalized confidence. Experiments on threeLVLMs across three VQA datasets show that, although LVLMs possess a reasonableperception level, there is substantial room for improvement. Among the threeconfidences, probabilistic and consistency-based signals are more reliableindicators, while verbalized confidence often leads to overconfidence. Toenhance LVLMs' perception, we adapt several established confidence calibrationmethods from Large Language Models (LLMs) and propose three effective methods.Additionally, we compare LVLMs with their LLM counterparts, finding thatjointly processing visual and textual inputs decreases question-answeringperformance but reduces confidence, resulting in an improved perception levelcompared to LLMs.</description><author>Zhikai Ding, Shiyu Ni, Keping Bi</author><pubDate>Tue, 26 Aug 2025 15:14:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19111v1</guid></item><item><title>MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model</title><link>http://arxiv.org/abs/2507.22675v2</link><description>Recently, large foundation models trained on vast datasets have demonstratedexceptional capabilities in feature extraction and general featurerepresentation. The ongoing advancements in deep learning-driven large modelshave shown great promise in accelerating unsupervised change detection methods,thereby enhancing the practical applicability of change detection technologies.Building on this progress, this paper introduces MergeSAM, an innovativeunsupervised change detection method for high-resolution remote sensingimagery, based on the Segment Anything Model (SAM). Two novel strategies,MaskMatching and MaskSplitting, are designed to address real-world complexitiessuch as object splitting, merging, and other intricate changes. The proposedmethod fully leverages SAM's object segmentation capabilities to constructmultitemporal masks that capture complex changes, embedding the spatialstructure of land cover into the change detection process.</description><author>Meiqi Hu, Lingzhi Lu, Chengxi Han, Xiaoping Liu</author><pubDate>Tue, 26 Aug 2025 15:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22675v2</guid></item><item><title>Egocentric Human-Object Interaction Detection: A New Benchmark and Method</title><link>http://arxiv.org/abs/2506.14189v2</link><description>Egocentric human-object interaction (Ego-HOI) detection is crucial forintelligent agents to understand and assist human activities from afirst-person perspective. However, progress has been hindered by the lack ofbenchmarks and methods tailored to egocentric challenges such as severehand-object occlusion. In this paper, we introduce the real-world Ego-HOIdetection task and the accompanying Ego-HOIBench, a new dataset with over 27Kegocentric images and explicit, fine-grained hand-verb-object tripletannotations across 123 categories. Ego-HOIBench covers diverse daily scenarios,object types, and both single- and two-hand interactions, offering acomprehensive testbed for Ego-HOI research. Benchmarking existing third-personHOI detectors on Ego-HOIBench reveals significant performance gaps,highlighting the need for egocentric-specific solutions. To this end, wepropose Hand Geometry and Interactivity Refinement (HGIR), a lightweight,plug-and-play scheme that leverages hand pose and geometric cues to enhanceinteraction representations. Specifically, HGIR explicitly extracts global handgeometric features from the estimated hand pose proposals, and further refinesinteraction features through pose-interaction attention, enabling the model tofocus on subtle hand-object relationship differences even under severeocclusion. HGIR significantly improves Ego-HOI detection performance acrossmultiple baselines, achieving new state-of-the-art results on Ego-HOIBench. Ourdataset and method establish a solid foundation for future research inegocentric vision and human-object interaction understanding. Project page:https://dengkunyuan.github.io/EgoHOIBench/</description><author>Kunyuan Deng, Yi Wang, Lap-Pui Chau</author><pubDate>Tue, 26 Aug 2025 15:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.14189v2</guid></item><item><title>Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models</title><link>http://arxiv.org/abs/2410.19195v2</link><description>In-context learning (ICL) performance is highly sensitive to prompt design,yet the impact of class label options (e.g. lexicon or order) in zero-shotclassification remains underexplored. This study proposes LOADS (Label setOptimization via Activation Distribution kurtosiS), a post-hoc method forselecting optimal label sets in zero-shot ICL with large language models(LLMs). LOADS is built upon the observations in our empirical analysis, thefirst to systematically examine how label option design (i.e., lexical choice,order, and elaboration) impacts classification performance. This analysis showsthat the lexical choice of the labels in the prompt (such as agree vs. supportin stance classification) plays an important role in both model performance andmodel's sensitivity to the label order. A further investigation demonstratesthat optimal label words tend to activate fewer outlier neurons in LLMs'feed-forward networks. LOADS then leverages kurtosis to measure the neuronactivation distribution for label selection, requiring only a single forwardpass without gradient propagation or labelled data. The LOADS-selected labelwords consistently demonstrate effectiveness for zero-shot ICL acrossclassification tasks, datasets, models and languages, achieving maximumperformance gain from 0.54 to 0.76 compared to the conventional approach ofusing original dataset label words.</description><author>Yue Li, Zhixue Zhao, Carolina Scarton</author><pubDate>Tue, 26 Aug 2025 15:09:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19195v2</guid></item><item><title>A Consolidated Volatility Prediction with Back Propagation Neural Network and Genetic Algorithm</title><link>http://arxiv.org/abs/2412.07223v7</link><description>This paper provides a unique approach with AI algorithms to predict emergingstock markets volatility. Traditionally, stock volatility is derived fromhistorical volatility,Monte Carlo simulation and implied volatility as well. Inthis paper, the writer designs a consolidated model with back-propagationneural network and genetic algorithm to predict future volatility of emergingstock markets and found that the results are quite accurate with low errors.</description><author>Zong Ke, Jingyu Xu, Zizhou Zhang, Yu Cheng, Wenjun Wu</author><pubDate>Tue, 26 Aug 2025 15:09:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07223v7</guid></item><item><title>Learning in Repeated Multi-Objective Stackelberg Games with Payoff Manipulation</title><link>http://arxiv.org/abs/2508.14705v2</link><description>We study payoff manipulation in repeated multi-objective Stackelberg games,where a leader may strategically influence a follower's deterministic bestresponse, e.g., by offering a share of their own payoff. We assume that thefollower's utility function, representing preferences over multiple objectives,is unknown but linear, and its weight parameter must be inferred throughinteraction. This introduces a sequential decision-making challenge for theleader, who must balance preference elicitation with immediate utilitymaximisation. We formalise this problem and propose manipulation policies basedon expected utility (EU) and long-term expected utility (longEU), which guidethe leader in selecting actions and offering incentives that trade offshort-term gains with long-term impact. We prove that under infinite repeatedinteractions, longEU converges to the optimal manipulation. Empirical resultsacross benchmark environments demonstrate that our approach improves cumulativeleader utility while promoting mutually beneficial outcomes, all withoutrequiring explicit negotiation or prior knowledge of the follower's utilityfunction.</description><author>Phurinut Srisawad, Juergen Branke, Long Tran-Thanh</author><pubDate>Tue, 26 Aug 2025 15:07:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14705v2</guid></item><item><title>Ego-Foresight: Self-supervised Learning of Agent-Aware Representations for Improved RL</title><link>http://arxiv.org/abs/2407.01570v3</link><description>Despite the significant advancements in Deep Reinforcement Learning (RL)observed in the last decade, the amount of training experience necessary tolearn effective policies remains one of the primary concerns both in simulatedand real environments. Looking to solve this issue, previous work has shownthat improved training efficiency can be achieved by separately modeling agentand environment, but usually requiring a supervisory agent mask. In contrast to RL, humans can perfect a new skill from a small number oftrials and in most cases do so without a supervisory signal, makingneuroscientific studies of human development a valuable source of inspirationfor RL. In particular, we explore the idea of motor prediction, which statesthat humans develop an internal model of themselves and of the consequencesthat their motor commands have on the immediate sensory inputs. Our insight isthat the movement of the agent provides a cue that allows the duality betweenagent and environment to be learned. To instantiate this idea, we present Ego-Foresight, a self-supervised methodfor disentangling agent and environment based on motion and prediction. Ourmain finding is self-supervised agent-awareness by visuomotor prediction of theagent improves sample-efficiency and performance of the underlying RLalgorithm. To test our approach, we first study its ability to visually predict agentmovement irrespective of the environment, in simulated and real-world roboticdata. Then, we integrate Ego-Foresight with a model-free RL algorithm to solvesimulated robotic tasks, showing that self-supervised agent-awareness canimprove sample-efficiency and performance in RL.</description><author>Manuel Serra Nunes, Atabak Dehban, Yiannis Demiris, José Santos-Victor</author><pubDate>Tue, 26 Aug 2025 15:06:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01570v3</guid></item><item><title>Composition and Alignment of Diffusion Models using Constrained Learning</title><link>http://arxiv.org/abs/2508.19104v1</link><description>Diffusion models have become prevalent in generative modeling due to theirability to sample from complex distributions. To improve the quality ofgenerated samples and their compliance with user requirements, two commonlyused methods are: (i) Alignment, which involves fine-tuning a diffusion modelto align it with a reward; and (ii) Composition, which combines severalpre-trained diffusion models, each emphasizing a desirable attribute in thegenerated outputs. However, trade-offs often arise when optimizing for multiplerewards or combining multiple models, as they can often represent competingproperties. Existing methods cannot guarantee that the resulting modelfaithfully generates samples with all the desired properties. To address thisgap, we propose a constrained optimization framework that unifies alignment andcomposition of diffusion models by enforcing that the aligned model satisfiesreward constraints and/or remains close to (potentially multiple) pre-trainedmodels. We provide a theoretical characterization of the solutions to theconstrained alignment and composition problems and develop a Lagrangian-basedprimal-dual training algorithm to approximate these solutions. Empirically, wedemonstrate the effectiveness and merits of our proposed approach in imagegeneration, applying it to alignment and composition, and show that our alignedor composed model satisfies constraints effectively, and improves on theequally-weighted approach. Our implementation can be found athttps://github.com/shervinkhalafi/constrained_comp_align.</description><author>Shervin Khalafi, Ignacio Hounie, Dongsheng Ding, Alejandro Ribeiro</author><pubDate>Tue, 26 Aug 2025 15:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19104v1</guid></item><item><title>Safe Reinforcement Learning in Black-Box Environments via Adaptive Shielding</title><link>http://arxiv.org/abs/2405.18180v3</link><description>Empowering safe exploration of reinforcement learning (RL) agents duringtraining is a critical challenge towards their deployment in many real-worldscenarios. When prior knowledge of the domain or task is unavailable, trainingRL agents in unknown, black-box environments presents an even greater safetyrisk. We introduce ADVICE (Adaptive Shielding with a Contrastive Autoencoder),a novel post-shielding technique that distinguishes safe and unsafe features ofstate-action pairs during training, and uses this knowledge to protect the RLagent from executing actions that yield likely hazardous outcomes. Ourcomprehensive experimental evaluation against state-of-the-art safe RLexploration techniques shows that ADVICE significantly reduces safetyviolations (approx 50%) during training, with a competitive outcome rewardcompared to other techniques.</description><author>Daniel Bethell, Simos Gerasimou, Radu Calinescu, Calum Imrie</author><pubDate>Tue, 26 Aug 2025 15:01:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18180v3</guid></item><item><title>Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic</title><link>http://arxiv.org/abs/2508.19099v1</link><description>Quantitative Discourse Analysis has seen growing adoption with the rise ofLarge Language Models and computational tools. However, reliance on black boxsoftware such as MAXQDA and NVivo risks undermining methodological transparencyand alignment with research goals. This paper presents a hybrid, transparentframework for QDA that combines lexical and semantic methods to enabletriangulation, reproducibility, and interpretability. Drawing from a case studyin historical political discourse, we demonstrate how custom Python pipelinesusing NLTK, spaCy, and Sentence Transformers allow fine-grained control overpreprocessing, lemmatisation, and embedding generation. We further detail ouriterative BERTopic modelling process, incorporating UMAP dimensionalityreduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimisedthrough parameter tuning and multiple runs to enhance topic coherence andcoverage. By juxtaposing precise lexical searches with context-aware semanticclustering, we argue for a multi-layered approach that mitigates thelimitations of either method in isolation. Our workflow underscores theimportance of code-level transparency, researcher agency, and methodologicaltriangulation in computational discourse studies. Code and supplementarymaterials are available via GitHub.</description><author>Thomas Compton</author><pubDate>Tue, 26 Aug 2025 15:00:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19099v1</guid></item><item><title>Reasoning LLMs in the Medical Domain: A Literature Survey</title><link>http://arxiv.org/abs/2508.19097v1</link><description>The emergence of advanced reasoning capabilities in Large Language Models(LLMs) marks a transformative development in healthcare applications. Beyondmerely expanding functional capabilities, these reasoning mechanisms enhancedecision transparency and explainability-critical requirements in medicalcontexts. This survey examines the transformation of medical LLMs from basicinformation retrieval tools to sophisticated clinical reasoning systems capableof supporting complex healthcare decisions. We provide a thorough analysis ofthe enabling technological foundations, with a particular focus on specializedprompting techniques like Chain-of-Thought and recent breakthroughs inReinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluatespurpose-built medical frameworks while also examining emerging paradigms suchas multi-agent collaborative systems and innovative prompting architectures.The survey critically assesses current evaluation methodologies for medicalvalidation and addresses persistent challenges in field interpretationlimitations, bias mitigation strategies, patient safety frameworks, andintegration of multimodal clinical data. Through this survey, we seek toestablish a roadmap for developing reliable LLMs that can serve as effectivepartners in clinical practice and medical research.</description><author>Armin Berger, Sarthak Khanna, David Berghaus, Rafet Sifa</author><pubDate>Tue, 26 Aug 2025 14:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19097v1</guid></item><item><title>Trustworthy Agents for Electronic Health Records through Confidence Estimation</title><link>http://arxiv.org/abs/2508.19096v1</link><description>Large language models (LLMs) show promise for extracting information fromElectronic Health Records (EHR) and supporting clinical decisions. However,deployment in clinical settings faces challenges due to hallucination risks. Wepropose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metricquantifying the accuracy-reliability trade-off at varying confidencethresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporatingstepwise confidence estimation for clinical question answering. Experiments onMIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines understrict reliability constraints, achieving improvements of 44.23%p and 25.34%pat HCAcc@70% while baseline methods fail at these thresholds. These resultshighlight limitations of traditional accuracy metrics in evaluating healthcareAI agents. Our work contributes to developing trustworthy clinical agents thatdeliver accurate information or transparently express uncertainty whenconfidence is low.</description><author>Yongwoo Song, Minbyul Jeong, Mujeen Sung</author><pubDate>Tue, 26 Aug 2025 14:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19096v1</guid></item><item><title>VibES: Induced Vibration for Persistent Event-Based Sensing</title><link>http://arxiv.org/abs/2508.19094v1</link><description>Event cameras are a bio-inspired class of sensors that asynchronously measureper-pixel intensity changes. Under fixed illumination conditions in static orlow-motion scenes, rigidly mounted event cameras are unable to generate anyevents, becoming unsuitable for most computer vision tasks. To address thislimitation, recent work has investigated motion-induced event stimulation thatoften requires complex hardware or additional optical components. In contrast,we introduce a lightweight approach to sustain persistent event generation byemploying a simple rotating unbalanced mass to induce periodic vibrationalmotion. This is combined with a motion-compensation pipeline that removes theinjected motion and yields clean, motion-corrected events for downstreamperception tasks. We demonstrate our approach with a hardware prototype andevaluate it on real-world captured datasets. Our method reliably recoversmotion parameters and improves both image reconstruction and edge detectionover event-based sensing without motion induction.</description><author>Vincenzo Polizzi, Stephen Yang, Quentin Clark, Jonathan Kelly, Igor Gilitschenski, David B. Lindell</author><pubDate>Tue, 26 Aug 2025 14:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19094v1</guid></item><item><title>Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index</title><link>http://arxiv.org/abs/2508.19093v1</link><description>This research presents a Retrieval-Augmented Generation (RAG) framework forart provenance studies, focusing on the Getty Provenance Index. Provenanceresearch establishes the ownership history of artworks, which is essential forverifying authenticity, supporting restitution and legal claims, andunderstanding the cultural and historical context of art objects. The processis complicated by fragmented, multilingual archival data that hinders efficientretrieval. Current search portals require precise metadata, limitingexploratory searches. Our method enables natural-language and multilingualsearches through semantic retrieval and contextual summarization, reducingdependence on metadata structures. We assess RAG's capability to retrieve andsummarize auction records using a 10,000-record sample from the GettyProvenance Index - German Sales. The results show this approach provides ascalable solution for navigating art market archives, offering a practical toolfor historians and cultural heritage professionals conducting historicallysensitive research.</description><author>Mathew Henrickson</author><pubDate>Tue, 26 Aug 2025 14:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19093v1</guid></item><item><title>KNN and K-means in Gini Prametric Spaces</title><link>http://arxiv.org/abs/2501.18028v3</link><description>This paper introduces enhancements to the K-means and K-nearest neighbors(KNN) algorithms based on the concept of Gini prametric spaces, instead oftraditional metric spaces. Unlike standard distance metrics, Gini prametricsincorporate both value-based and rank-based measures, offering robustness tonoise and outliers. The main contributions include: (1) a Gini prametric thatcaptures rank information alongside value distances; (2) a Gini K-meansalgorithm that is provably convergent and resilient to noisy data; and (3) aGini KNN method that performs competitively with state-of-the-art approacheslike Hassanat's distance in noisy environments. Experimental evaluations on 16UCI datasets demonstrate the superior performance and efficiency of theGini-based algorithms in clustering and classification tasks. This work opensnew directions for rank-based prametrics in machine learning and statisticalanalysis.</description><author>Cassandra Mussard, Arthur Charpentier, Stéphane Mussard</author><pubDate>Tue, 26 Aug 2025 14:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18028v3</guid></item><item><title>It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs</title><link>http://arxiv.org/abs/2508.19089v1</link><description>Extremely low-resource languages, especially those written in rare scripts,as shown in Figure 1, remain largely unsupported by large language models(LLMs). This is due in part to compounding factors such as the lack of trainingdata. This paper delivers the first comprehensive analysis of whether LLMs canacquire such languages purely via in-context learning (ICL), with or withoutauxiliary alignment signals, and how these methods compare toparameter-efficient fine-tuning (PEFT). We systematically evaluate 20under-represented languages across three state-of-the-art multilingual LLMs.Our findings highlight the limitation of PEFT when both language and its scriptare extremely under-represented by the LLM. In contrast, zero-shot ICL withlanguage alignment is impressively effective on extremely low-resourcelanguages, while few-shot ICL or PEFT is more beneficial for languagesrelatively better represented by LLMs. For LLM practitioners working onextremely low-resource languages, we summarise guidelines grounded by ourresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuninga multilingual model on languages of unseen scripts.</description><author>Yue Li, Zhixue Zhao, Carolina Scarton</author><pubDate>Tue, 26 Aug 2025 14:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19089v1</guid></item><item><title>Steerable Scene Generation with Post Training and Inference-Time Search</title><link>http://arxiv.org/abs/2505.04831v2</link><description>Training robots in simulation requires diverse 3D scenes that reflect thespecific challenges of downstream tasks. However, scenes that satisfy stricttask requirements, such as high-clutter environments with plausible spatialarrangement, are rare and costly to curate manually. Instead, we generatelarge-scale scene data using procedural models that approximate realisticenvironments for robotic manipulation, and adapt it to task-specific goals. Wedo this by training a unified diffusion-based generative model that predictswhich objects to place from a fixed asset library, along with their SE(3)poses. This model serves as a flexible scene prior that can be adapted usingreinforcement learning-based post training, conditional generation, orinference-time search, steering generation toward downstream objectives evenwhen they differ from the original data distribution. Our method enablesgoal-directed scene synthesis that respects physical feasibility and scalesacross scene types. We introduce a novel MCTS-based inference-time searchstrategy for diffusion models, enforce feasibility via projection andsimulation, and release a dataset of over 44 million SE(3) scenes spanning fivediverse environments. Website with videos, code, data, and model weights:https://steerable-scene-generation.github.io/</description><author>Nicholas Pfaff, Hongkai Dai, Sergey Zakharov, Shun Iwase, Russ Tedrake</author><pubDate>Tue, 26 Aug 2025 14:49:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.04831v2</guid></item><item><title>APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration</title><link>http://arxiv.org/abs/2508.19087v1</link><description>Large language models (LLMs) have revolutionized AI applications, yet theirenormous computational demands severely limit deployment and real-timeperformance. Quantization methods can help reduce computational costs, however,attaining the extreme efficiency associated with ultra-low-bit quantized LLMsat arbitrary precision presents challenges on GPUs. This is primarily due tothe limited support for GPU Tensor Cores, inefficient memory management, andinflexible kernel optimizations. To tackle these challenges, we propose acomprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.Firstly, we introduce a novel data format, bipolar-INT, which allows forefficient and lossless conversion with signed INT, while also being moreconducive to parallel computation. We also develop a matrix multiplication(MatMul) method allowing for arbitrary precision by dismantling andreassembling matrices at the bit level. This method provides flexible precisionand optimizes the utilization of GPU Tensor Cores. In addition, we propose amemory management system focused on data recovery, which strategically employsfast shared memory to substantially increase kernel execution speed and reducememory access latency. Finally, we develop a kernel mapping method thatdynamically selects the optimal configurable hyperparameters of kernels forvarying matrix sizes, enabling optimal performance across different LLMarchitectures and precision settings. In LLM inference, APT-LLM achieves up toa 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedupover NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedupover CUTLASS integer baselines.</description><author>Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang</author><pubDate>Tue, 26 Aug 2025 14:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19087v1</guid></item><item><title>Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration</title><link>http://arxiv.org/abs/2508.03337v4</link><description>The practical application of Multimodal Large Language Models (MLLMs) toVideo Question Answering (Video-QA) is severely hindered by the high token costof processing numerous video frames. While increasing the number of sampledframes is a common strategy, we observe a "less is more" phenomenon whereexcessive frames can paradoxically degrade performance due to context dilution.Concurrently, state-of-the-art keyframe selection methods, while effective,still yield significant temporal redundancy, which we term 'visual echoes'. Toaddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novelpost-processing method that intelligently prunes the selected keyframes. AFPemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 andCLIP feature space to identify and merge these echoes into singlerepresentatives. To compensate for information loss, we then introduce alightweight, text-based semantic graph that provides critical context withminimal token overhead. Conducting extensive experiments on the LongVideoBenchand VideoMME benchmarks across multiple leading MLLMs, our full approachdemonstrates a drastic reduction in required frames by up to 86.9% and totalinput tokens by up to 83.2%. Crucially, by providing a concise, high-qualityset of frames, our method not only enhances efficiency but often improvesaccuracy over baselines that use more frames. The code will be released uponpublication.</description><author>Shaoguang Wang, Ziyang Chen, Yijie Xu, Weiyu Guo, Hui Xiong</author><pubDate>Tue, 26 Aug 2025 14:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.03337v4</guid></item><item><title>"Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues</title><link>http://arxiv.org/abs/2508.19077v1</link><description>In a doctor-patient dialogue, the primary objective of physicians is todiagnose patients and propose a treatment plan. Medical doctors guide theseconversations through targeted questioning to efficiently gather theinformation required to provide the best possible outcomes for patients. To thebest of our knowledge, this is the first work that studies physician intenttrajectories in doctor-patient dialogues. We use the `Ambient ClinicalIntelligence Benchmark' (Aci-bench) dataset for our study. We collaborate withmedical professionals to develop a fine-grained taxonomy of physician intentsbased on the SOAP framework (Subjective, Objective, Assessment, and Plan). Wethen conduct a large-scale annotation effort to label over 5000 doctor-patientturns with the help of a large number of medical experts recruited usingProlific, a popular crowd-sourcing platform. This large labeled dataset is animportant resource contribution that we use for benchmarking thestate-of-the-art generative and encoder models for medical intentclassification tasks. Our findings show that our models understand the generalstructure of medical dialogues with high accuracy, but often fail to identifytransitions between SOAP categories. We also report for the first time commontrajectories in medical dialogue structures that provide valuable insights fordesigning `differential diagnosis' systems. Finally, we extensively study theimpact of intent filtering for medical dialogue summarization and observe asignificant boost in performance. We make the codes and data, includingannotation guidelines, publicly available athttps://github.com/DATEXIS/medical-intent-classification.</description><author>Tom Röhr, Soumyadeep Roy, Fares Al Mohamad, Jens-Michalis Papaioannou, Wolfgang Nejdl, Felix Gers, Alexander Löser</author><pubDate>Tue, 26 Aug 2025 14:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19077v1</guid></item><item><title>HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance</title><link>http://arxiv.org/abs/2508.19076v1</link><description>Large language model (LLM)-based agents have demonstrated remarkablecapabilities in decision-making tasks, but struggle significantly with complex,long-horizon planning scenarios. This arises from their lack of macroscopicguidance, causing disorientation and failures in complex tasks, as well asinsufficient continuous oversight during execution, rendering them unresponsiveto environmental changes and prone to deviations. To tackle these challenges,we introduce HiPlan, a hierarchical planning framework that provides adaptiveglobal-local guidance to boost LLM-based agents'decision-making. HiPlandecomposes complex tasks into milestone action guides for general direction andstep-wise hints for detailed actions. During the offline phase, we construct amilestone library from expert demonstrations, enabling structured experiencereuse by retrieving semantically similar tasks and milestones. In the executionphase, trajectory segments from past milestones are dynamically adapted togenerate step-wise hints that align current observations with the milestoneobjectives, bridging gaps and correcting deviations. Extensive experimentsacross two challenging benchmarks demonstrate that HiPlan substantiallyoutperforms strong baselines, and ablation studies validate the complementarybenefits of its hierarchical components.</description><author>Ziyue Li, Yuan Chang, Gaihong Yu, Xiaoqiu Le</author><pubDate>Tue, 26 Aug 2025 14:37:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19076v1</guid></item><item><title>Universal Dynamics with Globally Controlled Analog Quantum Simulators</title><link>http://arxiv.org/abs/2508.19075v1</link><description>Analog quantum simulators with global control fields have emerged as powerfulplatforms for exploring complex quantum phenomena. Recent breakthroughs, suchas the coherent control of thousands of atoms, highlight the growing potentialfor quantum applications at scale. Despite these advances, a fundamentaltheoretical question remains unresolved: to what extent can such systemsrealize universal quantum dynamics under global control? Here we establish anecessary and sufficient condition for universal quantum computation using onlyglobal pulse control, proving that a broad class of analog quantum simulatorsis, in fact, universal. We further extend this framework to fermionic andbosonic systems, including modern platforms such as ultracold atoms in opticalsuperlattices. Crucially, to connect the theoretical possibility withexperimental reality, we introduce a new control technique into the experiment- direct quantum optimal control. This method enables the synthesis of complexeffective Hamiltonians and allows us to incorporate realistic hardwareconstraints. To show its practical power, we experimentally engineer three-bodyinteractions outside the blockade regime and demonstrate topological dynamicson a Rydberg atom array. Using the new control framework, we overcome keyexperimental challenges, including hardware limitations and atom positionfluctuations in the non-blockade regime, by identifying smooth, short-durationpulses that achieve high-fidelity dynamics. Experimental measurements revealdynamical signatures of symmetry-protected-topological edge modes, confirmingboth the expressivity and feasibility of our approach. Our work opens a newavenue for quantum simulation beyond native hardware Hamiltonians, enabling theengineering of effective multi-body interactions and advancing the frontier ofquantum information processing with globally-controlled analog platforms.</description><author>Hong-Ye Hu, Abigail McClain Gomez, Liyuan Chen, Aaron Trowbridge, Andy J. Goldschmidt, Zachary Manchester, Frederic T. Chong, Arthur Jaffe, Susanne F. Yelin</author><pubDate>Tue, 26 Aug 2025 14:36:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19075v1</guid></item><item><title>Consensus in Motion: A Case of Dynamic Rationality of Sequential Learning in Probability Aggregation</title><link>http://arxiv.org/abs/2504.14624v2</link><description>We propose a framework for probability aggregation based on propositionalprobability logic. Unlike conventional judgment aggregation, which focuses onstatic rationality, our model addresses dynamic rationality by ensuring thatcollective beliefs update consistently with new information. We show that anyconsensus-compatible and independent aggregation rule on a non-nested agenda isnecessarily linear. Furthermore, we provide sufficient conditions for a fairlearning process, where individuals initially agree on a specified subset ofpropositions known as the common ground, and new information is restricted tothis shared foundation. This guarantees that updating individual judgments viaBayesian conditioning-whether performed before or after aggregation-yields thesame collective belief. A distinctive feature of our framework is its treatmentof sequential decision-making, which allows new information to be incorporatedprogressively through multiple stages while maintaining the established commonground. We illustrate our findings with a running example in a politicalscenario concerning healthcare and immigration policies.</description><author>Polina Gordienko, Christoph Jansen, Thomas Augustin, Martin Rechenauer</author><pubDate>Tue, 26 Aug 2025 14:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.14624v2</guid></item><item><title>SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?</title><link>http://arxiv.org/abs/2503.06029v2</link><description>Large Language Models (LLMs) have become integral to daily life, especiallyadvancing as intelligent assistants through on-device deployment onsmartphones. However, existing LLM evaluation benchmarks predominantly focus onobjective tasks like mathematics and coding in English, which do notnecessarily reflect the practical use cases of on-device LLMs in real-worldmobile scenarios, especially for Chinese users. To address these gaps, weintroduce SmartBench, the first benchmark designed to evaluate the capabilitiesof on-device LLMs in Chinese mobile contexts. We analyze functionalitiesprovided by representative smartphone manufacturers and divide them into fivecategories: text summarization, text Q&amp;A, information extraction, contentcreation, and notification management, further detailed into 20 specific tasks.For each task, we construct high-quality datasets comprising 50 to 200question-answer pairs that reflect everyday mobile interactions, and we developautomated evaluation criteria tailored for these tasks. We conductcomprehensive evaluations of on-device LLMs and MLLMs using SmartBench and alsoassess their performance after quantized deployment on real smartphone NPUs.Our contributions provide a standardized framework for evaluating on-deviceLLMs in Chinese, promoting further development and optimization in thiscritical area. Code and data will be available athttps://github.com/vivo-ai-lab/SmartBench.</description><author>Xudong Lu, Haohao Gao, Renshou Wu, Shuai Ren, Xiaoxin Chen, Hongsheng Li, Fangyuan Li</author><pubDate>Tue, 26 Aug 2025 14:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.06029v2</guid></item><item><title>An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees</title><link>http://arxiv.org/abs/2508.19074v1</link><description>The Large Language Models (LLM) are increasingly being deployed in roboticsto generate robot control programs for specific user tasks, enabling embodiedintelligence. Existing methods primarily focus on LLM training and promptdesign that utilize LLMs to generate executable programs directly from usertasks in natural language. However, due to the inconsistency of the LLMs andthe high complexity of the tasks, such best-effort approaches often lead totremendous programming errors in the generated code, which significantlyundermines the effectiveness especially when the light-weight LLMs are applied.This paper introduces a natural-robotic language translation framework that (i)provides correctness verification for generated control programs and (ii)enhances the performance of LLMs in program generation via feedback-basedfine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) isproposed to abstract away from the intricate details of the control programs,bridging the natural language tasks with the underlying robot skills. Then, theRSL compiler and debugger are constructed to verify RSL programs generated bythe LLM and provide error feedback to the LLM for refining the outputs untilbeing verified by the compiler. This provides correctness guarantees for theLLM-generated programs before being offloaded to the robots for execution,significantly enhancing the effectiveness of LLM-powered robotic applications.Experiments demonstrate NRTrans outperforms the existing method under a rangeof LLMs and tasks, and achieves a high success rate for light-weight LLMs.</description><author>ZhenDong Chen, ZhanShang Nie, ShiXing Wan, JunYi Li, YongTian Cheng, Shuai Zhao</author><pubDate>Tue, 26 Aug 2025 14:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19074v1</guid></item><item><title>Deep vectorised operators for pulsatile hemodynamics estimation in coronary arteries from a steady-state prior</title><link>http://arxiv.org/abs/2410.11920v2</link><description>Cardiovascular hemodynamic fields provide valuable medical decision markersfor coronary artery disease. Computational fluid dynamics (CFD) is the goldstandard for accurate, non-invasive evaluation of these quantities in silico.In this work, we propose a time-efficient surrogate model, powered by machinelearning, for the estimation of pulsatile hemodynamics based on steady-statepriors. We introduce deep vectorised operators, a modelling framework fordiscretisation-independent learning on infinite-dimensional function spaces.The underlying neural architecture is a neural field conditioned on hemodynamicboundary conditions. Importantly, we show how relaxing the requirement ofpoint-wise action to permutation-equivariance leads to a family of models thatcan be parametrised by message passing and self-attention layers. We evaluateour approach on a dataset of 74 stenotic coronary arteries extracted fromcoronary computed tomography angiography (CCTA) with patient-specific pulsatileCFD simulations as ground truth. We show that our model produces accurateestimates of the pulsatile velocity and pressure (approximation disparity 0.368$\pm$ 0.079) while being agnostic ($p &lt; 0.05$ in a one-way ANOVA test) tore-sampling of the source domain, i.e. discretisation-independent. This showsthat deep vectorised operators are a powerful modelling tool for cardiovascularhemodynamics estimation in coronary arteries and beyond.</description><author>Julian Suk, Guido Nannini, Patryk Rygiel, Christoph Brune, Gianluca Pontone, Alberto Redaelli, Jelmer M. Wolterink</author><pubDate>Tue, 26 Aug 2025 14:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11920v2</guid></item></channel></rss>