<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 16 Jan 2025 13:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion</title><link>http://arxiv.org/abs/2501.09019v1</link><description>The first-in-first-out (FIFO) video diffusion, built on a pre-trainedtext-to-video model, has recently emerged as an effective approach fortuning-free long video generation. This technique maintains a queue of videoframes with progressively increasing noise, continuously producing clean framesat the queue's head while Gaussian noise is enqueued at the tail. However,FIFO-Diffusion often struggles to keep long-range temporal consistency in thegenerated videos due to the lack of correspondence modeling across frames. Inthis paper, we propose Ouroboros-Diffusion, a novel video denoising frameworkdesigned to enhance structural and content (subject) consistency, enabling thegeneration of consistent videos of arbitrary length. Specifically, we introducea new latent sampling technique at the queue tail to improve structuralconsistency, ensuring perceptually smooth transitions among frames. To enhancesubject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA)mechanism, which aligns subjects across frames within short segments to achievebetter visual coherence. Furthermore, we introduce self-recurrent guidance.This technique leverages information from all previous cleaner frames at thefront of the queue to guide the denoising of noisier frames at the end,fostering rich and contextual global information interaction. Extensiveexperiments of long video generation on the VBench benchmark demonstrate thesuperiority of our Ouroboros-Diffusion, particularly in terms of subjectconsistency, motion smoothness, and temporal consistency.</description><author>Jingyuan Chen, Fuchen Long, Jie An, Zhaofan Qiu, Ting Yao, Jiebo Luo, Tao Mei</author><pubDate>Wed, 15 Jan 2025 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09019v1</guid></item><item><title>T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation</title><link>http://arxiv.org/abs/2407.14505v2</link><description>Text-to-video (T2V) generative models have advanced significantly, yet theirability to compose different objects, attributes, actions, and motions into avideo remains unexplored. Previous text-to-video benchmarks also neglect thisimportant ability for evaluation. In this work, we conduct the first systematicstudy on compositional text-to-video generation. We propose T2V-CompBench, thefirst benchmark tailored for compositional text-to-video generation.T2V-CompBench encompasses diverse aspects of compositionality, includingconsistent attribute binding, dynamic attribute binding, spatial relationships,motion binding, action binding, object interactions, and generative numeracy.We further carefully design evaluation metrics of multimodal large languagemodel (MLLM)-based, detection-based, and tracking-based metrics, which canbetter reflect the compositional text-to-video generation quality of sevenproposed categories with 1400 text prompts. The effectiveness of the proposedmetrics is verified by correlation with human evaluations. We also benchmarkvarious text-to-video generative models and conduct in-depth analysis acrossdifferent models and various compositional categories. We find thatcompositional text-to-video generation is highly challenging for currentmodels, and we hope our attempt could shed light on future research in thisdirection.</description><author>Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, Xihui Liu</author><pubDate>Wed, 15 Jan 2025 18:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14505v2</guid></item><item><title>How Do Generative Models Draw a Software Engineer? A Case Study on Stable Diffusion Bias</title><link>http://arxiv.org/abs/2501.09014v1</link><description>Generative models are nowadays widely used to generate graphical content usedfor multiple purposes, e.g. web, art, advertisement. However, it has been shownthat the images generated by these models could reinforce societal biasesalready existing in specific contexts. In this paper, we focus on understandingif this is the case when one generates images related to various softwareengineering tasks. In fact, the Software Engineering (SE) community is notimmune from gender and ethnicity disparities, which could be amplified by theuse of these models. Hence, if used without consciousness, artificiallygenerated images could reinforce these biases in the SE domain. Specifically,we perform an extensive empirical evaluation of the gender and ethnicity biasexposed by three versions of the Stable Diffusion (SD) model (a very popularopen-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks. Weobtain 6,720 images by feeding each model with two sets of prompts describingdifferent software-related tasks: one set includes the Software Engineerkeyword, and one set does not include any specification of the personperforming the task. Next, we evaluate the gender and ethnicity disparities inthe generated images. Results show how all models are significantly biasedtowards male figures when representing software engineers. On the contrary,while SD 2 and SD XL are strongly biased towards White figures, SD 3 isslightly more biased towards Asian figures. Nevertheless, all modelssignificantly under-represent Black and Arab figures, regardless of the promptstyle used. The results of our analysis highlight severe concerns aboutadopting those models to generate content for SE tasks and open the field forfuture research on bias mitigation in this context.</description><author>Tosin Fadahunsi, Giordano d'Aloisio, Antinisca Di Marco, Federica Sarro</author><pubDate>Wed, 15 Jan 2025 18:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09014v1</guid></item><item><title>Multimodal LLMs Can Reason about Aesthetics in Zero-Shot</title><link>http://arxiv.org/abs/2501.09012v1</link><description>We present the first study on how Multimodal LLMs' (MLLMs) reasoning abilityshall be elicited to evaluate the aesthetics of artworks. To facilitate thisinvestigation, we construct MM-StyleBench, a novel high-quality dataset forbenchmarking artistic stylization. We then develop a principled method forhuman preference modeling and perform a systematic correlation analysis betweenMLLMs' responses and human preference. Our experiments reveal an inherenthallucination issue of MLLMs in art evaluation, associated with responsesubjectivity. ArtCoT is proposed, demonstrating that art-specific taskdecomposition and the use of concrete language boost MLLMs' reasoning abilityfor aesthetics. Our findings offer valuable insights into MLLMs for art and canbenefit a wide range of downstream applications, such as style transfer andartistic image generation. Code available athttps://github.com/songrise/MLLM4Art.</description><author>Ruixiang Jiang, Changwen Chen</author><pubDate>Wed, 15 Jan 2025 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09012v1</guid></item><item><title>Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians</title><link>http://arxiv.org/abs/2501.09009v1</link><description>The foundation model (FM) paradigm is transforming Machine Learning ForceFields (MLFFs), leveraging general-purpose representations and scalabletraining to perform a variety of computational chemistry tasks. Although MLFFFMs have begun to close the accuracy gap relative to first-principles methods,there is still a strong need for faster inference speed. Additionally, whileresearch is increasingly focused on general-purpose models which transferacross chemical space, practitioners typically only study a small subset ofsystems at a given time. This underscores the need for fast, specialized MLFFsrelevant to specific downstream applications, which preserve test-time physicalsoundness while maintaining train-time scalability. In this work, we introducea method for transferring general-purpose representations from MLFF foundationmodels to smaller, faster MLFFs specialized to specific regions of chemicalspace. We formulate our approach as a knowledge distillation procedure, wherethe smaller "student" MLFF is trained to match the Hessians of the energypredictions of the "teacher" foundation model. Our specialized MLFFs can be upto 20 $\times$ faster than the original foundation model, while retaining, andin some cases exceeding, its performance and that of undistilled models. Wealso show that distilling from a teacher model with a direct forceparameterization into a student model trained with conservative forces (i.e.,computed as derivatives of the potential energy) successfully leverages therepresentations from the large-scale teacher for improved accuracy, whilemaintaining energy conservation during test-time molecular dynamicssimulations. More broadly, our work suggests a new paradigm for MLFFdevelopment, in which foundation models are released along with smaller,specialized simulation "engines" for common chemical subsets.</description><author>Ishan Amin, Sanjeev Raja, Aditi Krishnapriyan</author><pubDate>Wed, 15 Jan 2025 18:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09009v1</guid></item><item><title>SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and Segmentation Mask Generation</title><link>http://arxiv.org/abs/2501.09008v1</link><description>Acquiring and annotating surgical data is often resource-intensive, ethicalconstraining, and requiring significant expert involvement. While generative AImodels like text-to-image can alleviate data scarcity, incorporating spatialannotations, such as segmentation masks, is crucial for precision-drivensurgical applications, simulation, and education. This study introduces both anovel task and method, SimGen, for Simultaneous Image and Mask Generation.SimGen is a diffusion model based on the DDPM framework and Residual U-Net,designed to jointly generate high-fidelity surgical images and theircorresponding segmentation masks. The model leverages cross-correlation priorsto capture dependencies between continuous image and discrete maskdistributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed toenhance class separability and uniformity in the RGB space of the masks. SimGendelivers high-fidelity images and accurate segmentation masks, outperformingbaselines across six public datasets assessed on image and semantic inceptiondistance metrics. Ablation study shows that the CFL improves mask quality andspatial separation. Downstream experiments suggest generated image-mask pairsare usable if regulations limit human data release for research. This workoffers a cost-effective solution for generating paired surgical images andcomplex labels, advancing surgical AI development by reducing the need forexpensive manual annotations.</description><author>Aditya Bhat, Rupak Bose, Chinedu Innocent Nwoye, Nicolas Padoy</author><pubDate>Wed, 15 Jan 2025 18:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09008v1</guid></item><item><title>AI-RAN: Transforming RAN with AI-driven Computing Infrastructure</title><link>http://arxiv.org/abs/2501.09007v1</link><description>The radio access network (RAN) landscape is undergoing a transformative shiftfrom traditional, communication-centric infrastructures towards convergedcompute-communication platforms. This article introduces AI-RAN whichintegrates both RAN and artificial intelligence (AI) workloads on the sameinfrastructure. By doing so, AI-RAN not only meets the performance demands offuture networks but also improves asset utilization. We begin by examining howRANs have evolved beyond mobile broadband towards AI-RAN and articulatingmanifestations of AI-RAN into three forms: AI-for-RAN, AI-on-RAN, andAI-and-RAN. Next, we identify the key requirements and enablers for theconvergence of communication and computing in AI-RAN. We then provide areference architecture for advancing AI-RAN from concept to practice. Toillustrate the practical potential of AI-RAN, we present a proof-of-conceptthat concurrently processes RAN and AI workloads utilizing NVIDIA Grace-HopperGH200 servers. Finally, we conclude the article by outlining future workdirections to guide further developments of AI-RAN.</description><author>Lopamudra Kundu, Xingqin Lin, Rajesh Gadiyar, Jean-Francois Lacasse, Shuvo Chowdhury</author><pubDate>Wed, 15 Jan 2025 18:47:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09007v1</guid></item><item><title>DeblurDiNAT: A Compact Model with Exceptional Generalization and Visual Fidelity on Unseen Domains</title><link>http://arxiv.org/abs/2403.13163v5</link><description>Recent deblurring networks have effectively restored clear images from theblurred ones. However, they often struggle with generalization to unknowndomains. Moreover, these models typically focus on distortion metrics such asPSNR and SSIM, neglecting the critical aspect of metrics aligned with humanperception. To address these limitations, we propose DeblurDiNAT, a deblurringTransformer based on Dilated Neighborhood Attention. First, DeblurDiNAT employsan alternating dilation factor paradigm to capture both local and globalblurred patterns, enhancing generalization and perceptual clarity. Second, alocal cross-channel learner aids the Transformer block to understand theshort-range relationships between adjacent channels. Additionally, we present alinear feed-forward network with a simple while effective design. Finally, adual-stage feature fusion module is introduced as an alternative to theexisting approach, which efficiently process multi-scale visual informationacross network levels. Compared to state-of-the-art models, our compactDeblurDiNAT demonstrates superior generalization capabilities and achievesremarkable performance in perceptual metrics, while maintaining a favorablemodel size.</description><author>Hanzhou Liu, Binghan Li, Chengkai Liu, Mi Lu</author><pubDate>Wed, 15 Jan 2025 18:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13163v5</guid></item><item><title>Improving Stability Estimates in Adversarial Explainable AI through Alternate Search Methods</title><link>http://arxiv.org/abs/2501.09006v1</link><description>Advances in the effectiveness of machine learning models have come at thecost of enormous complexity resulting in a poor understanding of how theyfunction. Local surrogate methods have been used to approximate the workings ofthese complex models, but recent work has revealed their vulnerability toadversarial attacks where the explanation produced is appreciably differentwhile the meaning and structure of the complex model's output remains similar.This prior work has focused on the existence of these weaknesses but not ontheir magnitude. Here we explore using an alternate search method with the goalof finding minimum viable perturbations, the fewest perturbations necessary toachieve a fixed similarity value between the original and altered text'sexplanation. Intuitively, a method that requires fewer perturbations to exposea given level of instability is inferior to one which requires more. Thisnuance allows for superior comparisons of the stability of explainabilitymethods.</description><author>Christopher Burger, Charles Walter</author><pubDate>Wed, 15 Jan 2025 18:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09006v1</guid></item><item><title>Delay Sensitive Hierarchical Federated Learning with Stochastic Local Updates</title><link>http://arxiv.org/abs/2302.04851v2</link><description>The impact of local averaging on the performance of federated learning (FL)systems is studied in the presence of communication delay between the clientsand the parameter server. To minimize the effect of delay, clients are assignedinto different groups, each having its own local parameter server (LPS) thataggregates its clients' models. The groups' models are then aggregated at aglobal parameter server (GPS) that only communicates with the LPSs. Suchsetting is known as hierarchical FL (HFL). Unlike most works in the literature,the number of local and global communication rounds in our work is randomlydetermined by the (different) delays experienced by each group of clients.Specifically, the number of local averaging rounds is tied to a wall-clock timeperiod coined the sync time $S$, after which the LPSs synchronize their modelsby sharing them with the GPS. Such sync time $S$ is then reapplied until aglobal wall-clock time is exhausted. First, an upper bound on the deviation between the updated model at each LPSwith respect to that available at the GPS is derived. This is then used as atool to derive the convergence analysis of our proposed delay-sensitive HFLalgorithm, first at each LPS individually, and then at the GPS. Our theoreticalconvergence bound showcases the effects of the whole system's parameters,including the number of groups, the number of clients per group, and the valueof $S$. Our results show that the value of $S$ should be carefully chosen,especially since it implicitly governs how the delay statistics affect theperformance of HFL in situations where training time is restricted.</description><author>Abdulmoneam Ali, Ahmed Arafa</author><pubDate>Wed, 15 Jan 2025 18:45:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04851v2</guid></item><item><title>Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails</title><link>http://arxiv.org/abs/2501.09004v1</link><description>As Large Language Models (LLMs) and generative AI become increasinglywidespread, concerns about content safety have grown in parallel. Currently,there is a clear lack of high-quality, human-annotated datasets that addressthe full spectrum of LLM-related safety risks and are usable for commercialapplications. To bridge this gap, we propose a comprehensive and adaptabletaxonomy for categorizing safety risks, structured into 12 top-level hazardcategories with an extension to 9 fine-grained subcategories. This taxonomy isdesigned to meet the diverse requirements of downstream users, offering moregranular and flexible tools for managing various risk types. Using a hybriddata generation pipeline that combines human annotations with a multi-LLM"jury" system to assess the safety of responses, we obtain Aegis 2.0, acarefully curated collection of 34,248 samples of human-LLM interactions,annotated according to our proposed taxonomy. To validate its effectiveness, wedemonstrate that several lightweight models, trained using parameter-efficienttechniques on Aegis 2.0, achieve performance competitive with leading safetymodels fully fine-tuned on much larger, non-commercial datasets. In addition,we introduce a novel training blend that combines safety with topic followingdata.This approach enhances the adaptability of guard models, enabling them togeneralize to new risk categories defined during inference. We plan toopen-source Aegis 2.0 data and models to the research community to aid in thesafety guardrailing of LLMs.</description><author>Shaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian Rebedea, Jibin Rajan Varghese, Christopher Parisien</author><pubDate>Wed, 15 Jan 2025 18:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09004v1</guid></item><item><title>Vision Foundation Models for Computed Tomography</title><link>http://arxiv.org/abs/2501.09001v1</link><description>Foundation models (FMs) have shown transformative potential in radiology byperforming diverse, complex tasks across imaging modalities. Here, we developedCT-FM, a large-scale 3D image-based pre-trained model designed explicitly forvarious radiological tasks. CT-FM was pre-trained using 148,000 computedtomography (CT) scans from the Imaging Data Commons through label-agnosticcontrastive learning. We evaluated CT-FM across four categories of tasks,namely, whole-body and tumor segmentation, head CT triage, medical imageretrieval, and semantic understanding, showing superior performance againststate-of-the-art models. Beyond quantitative success, CT-FM demonstrated theability to cluster regions anatomically and identify similar anatomical andstructural concepts across scans. Furthermore, it remained robust acrosstest-retest settings and indicated reasonable salient regions attached to itsembeddings. This study demonstrates the value of large-scale medical imagingfoundation models and by open-sourcing the model weights, code, and data, aimsto support more adaptable, reliable, and interpretable AI solutions inradiology.</description><author>Suraj Pai, Ibrahim Hadzic, Dennis Bontempi, Keno Bressem, Benjamin H. Kann, Andriy Fedorov, Raymond H. Mak, Hugo J. W. L. Aerts</author><pubDate>Wed, 15 Jan 2025 18:30:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09001v1</guid></item><item><title>Reward Machines for Deep RL in Noisy and Uncertain Environments</title><link>http://arxiv.org/abs/2406.00120v4</link><description>Reward Machines provide an automaton-inspired structure for specifyinginstructions, safety constraints, and other temporally extended reward-worthybehaviour. By exposing the underlying structure of a reward function, theyenable the decomposition of an RL task, leading to impressive gains in sampleefficiency. Although Reward Machines and similar formal specifications have arich history of application towards sequential decision-making problems, theycritically rely on a ground-truth interpretation of the domain-specificvocabulary that forms the building blocks of the reward function--suchground-truth interpretations are elusive in the real world due in part topartial observability and noisy sensing. In this work, we explore the use ofReward Machines for Deep RL in noisy and uncertain environments. Wecharacterize this problem as a POMDP and propose a suite of RL algorithms thatexploit task structure under uncertain interpretation of the domain-specificvocabulary. Through theory and experiments, we expose pitfalls in naiveapproaches to this problem while simultaneously demonstrating how taskstructure can be successfully leveraged under noisy interpretations of thevocabulary.</description><author>Andrew C. Li, Zizhao Chen, Toryn Q. Klassen, Pashootan Vaezipoor, Rodrigo Toro Icarte, Sheila A. McIlraith</author><pubDate>Wed, 15 Jan 2025 18:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00120v4</guid></item><item><title>Click-Calib: A Robust Extrinsic Calibration Method for Surround-View Systems</title><link>http://arxiv.org/abs/2501.01557v2</link><description>Surround-View System (SVS) is an essential component in Advanced DriverAssistance System (ADAS) and requires precise calibrations. However,conventional offline extrinsic calibration methods are cumbersome andtime-consuming as they rely heavily on physical patterns. Additionally, thesemethods primarily focus on short-range areas surrounding the vehicle, resultingin lower calibration quality in more distant zones. To address theselimitations, we propose Click-Calib, a pattern-free approach for offline SVSextrinsic calibration. Without requiring any special setup, the user only needsto click a few keypoints on the ground in natural scenes. Unlike other offlinecalibration approaches, Click-Calib optimizes camera poses over a wide range byminimizing reprojection distance errors of keypoints, thereby achievingaccurate calibrations at both short and long distances. Furthermore,Click-Calib supports both single-frame and multiple-frame modes, with thelatter offering even better results. Evaluations on our in-house dataset andthe public WoodScape dataset demonstrate its superior accuracy and robustnesscompared to baseline methods. Code is available athttps://github.com/lwangvaleo/click_calib.</description><author>Lihao Wang</author><pubDate>Wed, 15 Jan 2025 18:29:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01557v2</guid></item><item><title>A General Framework for Inference-time Scaling and Steering of Diffusion Models</title><link>http://arxiv.org/abs/2501.06848v2</link><description>Diffusion models produce impressive results in modalities ranging from imagesand video to protein design and text. However, generating samples withuser-specified properties remains a challenge. Recent research proposesfine-tuning models to maximize rewards that capture desired properties, butthese methods require expensive training and are prone to mode collapse. Inthis work, we propose Feynman Kac (FK) steering, an inference-time frameworkfor steering diffusion models with reward functions. FK steering works bysampling a system of multiple interacting diffusion processes, calledparticles, and resampling particles at intermediate steps based on scorescomputed using functions called potentials. Potentials are defined usingrewards for intermediate states and are selected such that a high valueindicates that the particle will yield a high-reward sample. We explore variouschoices of potentials, intermediate rewards, and samplers. We evaluate FKsteering on text-to-image and text diffusion models. For steering text-to-imagemodels with a human preference reward, we find that FK steering a 0.8Bparameter model outperforms a 2.6B parameter fine-tuned model on promptfidelity, with faster sampling and no training. For steering text diffusionmodels with rewards for text quality and specific text attributes, we find thatFK steering generates lower perplexity, more linguistically acceptable outputsand enables gradient-free control of attributes like toxicity. Our resultsdemonstrate that inference-time scaling and steering of diffusion models, evenwith off-the-shelf rewards, can provide significant sample quality gains andcontrollability benefits. Code is available athttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .</description><author>Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath</author><pubDate>Wed, 15 Jan 2025 18:28:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06848v2</guid></item><item><title>CrystalGRW: Generative Modeling of Crystal Structures with Targeted Properties via Geodesic Random Walks</title><link>http://arxiv.org/abs/2501.08998v1</link><description>Determining whether a candidate crystalline material is thermodynamicallystable depends on identifying its true ground-state structure, a centralchallenge in computational materials science. We introduce CrystalGRW, adiffusion-based generative model on Riemannian manifolds that proposes novelcrystal configurations and can predict stable phases validated by densityfunctional theory. The crystal properties, such as fractional coordinates,atomic types, and lattice matrices, are represented on suitable Riemannianmanifolds, ensuring that new predictions generated through the diffusionprocess preserve the periodicity of crystal structures. We incorporate anequivariant graph neural network to also account for rotational andtranslational symmetries during the generation process. CrystalGRW demonstratesthe ability to generate realistic crystal structures that are close to theirground states with accuracy comparable to existing models, while also enablingconditional control, such as specifying a desired crystallographic point group.These features help accelerate materials discovery and inverse design byoffering stable, symmetry-consistent crystal candidates for experimentalvalidation.</description><author>Krit Tangsongcharoen, Teerachote Pakornchote, Chayanon Atthapak, Natthaphon Choomphon-anomakhun, Annop Ektarawong, Bj√∂rn Alling, Christopher Sutton, Thiti Bovornratanaraks, Thiparat Chotibut</author><pubDate>Wed, 15 Jan 2025 18:26:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08998v1</guid></item><item><title>VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science</title><link>http://arxiv.org/abs/2501.08995v1</link><description>Data scarcity in pharmaceutical research has led to reliance onlabour-intensive trial and error approaches for development rather than datadriven methods. While Machine Learning offers a solution, existing datasets areoften small and noisy, limiting their utility. To address this, we developed aVariationally Encoded Conditional Tabular Generative Adversarial Network (VECTGAN), a novel generative model specifically designed for augmenting small,noisy datasets. We introduce a pipeline where data is augmented beforeregression model development and demonstrate that this consistently andsignificantly improves performance over other state of the art tabulargenerative models. We apply this pipeline across six pharmaceutical datasets,and highlight its real-world applicability by developing novel polymers withmedically desirable mucoadhesive properties, which we made and experimentallycharacterised. Additionally, we pre-train the model on the ChEMBL database ofdrug-like molecules, leveraging knowledge distillation to enhance itsgeneralisability, making it readily available for use on pharmaceuticaldatasets containing small molecules, which is an extremely commonpharmaceutical task. We demonstrate the power of synthetic data forregularising small tabular datasets, highlighting its potential to becomestandard practice in pharmaceutical model development, and make our method,including VECT GAN pretrained on ChEMBL available as a pip package.</description><author>Youssef Abdalla, Marrisa Taub, Eleanor Hilton, Priya Akkaraju, Alexander Milanovic, Mine Orlu, Abdul W. Basit, Michael T Cook, Tapabrata Chakraborty, David Shorthouse</author><pubDate>Wed, 15 Jan 2025 18:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08995v1</guid></item><item><title>RepVideo: Rethinking Cross-Layer Representation for Video Generation</title><link>http://arxiv.org/abs/2501.08994v1</link><description>Video generation has achieved remarkable progress with the introduction ofdiffusion models, which have significantly improved the quality of generatedvideos. However, recent research has primarily focused on scaling up modeltraining, while offering limited insights into the direct impact ofrepresentations on the video generation process. In this paper, we initiallyinvestigate the characteristics of features in intermediate layers, findingsubstantial variations in attention maps across different layers. Thesevariations lead to unstable semantic representations and contribute tocumulative differences between features, which ultimately reduce the similaritybetween adjacent frames and negatively affect temporal coherence. To addressthis, we propose RepVideo, an enhanced representation framework fortext-to-video diffusion models. By accumulating features from neighboringlayers to form enriched representations, this approach captures more stablesemantic information. These enhanced representations are then used as inputs tothe attention mechanism, thereby improving semantic expressiveness whileensuring feature consistency across adjacent frames. Extensive experimentsdemonstrate that our RepVideo not only significantly enhances the ability togenerate accurate spatial appearances, such as capturing complex spatialrelationships between multiple objects, but also improves temporal consistencyin video generation.</description><author>Chenyang Si, Weichen Fan, Zhengyao Lv, Ziqi Huang, Yu Qiao, Ziwei Liu</author><pubDate>Wed, 15 Jan 2025 18:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08994v1</guid></item><item><title>Consistency of Responses and Continuations Generated by Large Language Models on Social Media</title><link>http://arxiv.org/abs/2501.08102v2</link><description>Large Language Models (LLMs) demonstrate remarkable capabilities in textgeneration, yet their emotional consistency and semantic coherence in socialmedia contexts remain insufficiently understood. This study investigates howLLMs handle emotional content and maintain semantic relationships throughcontinuation and response tasks using two open-source models: Gemma and Llama.By analyzing climate change discussions from Twitter and Reddit, we examineemotional transitions, intensity patterns, and semantic similarity betweenhuman-authored and LLM-generated content. Our findings reveal that while bothmodels maintain high semantic coherence, they exhibit distinct emotionalpatterns: Gemma shows a tendency toward negative emotion amplification,particularly anger, while maintaining certain positive emotions like optimism.Llama demonstrates superior emotional preservation across a broader spectrum ofaffects. Both models systematically generate responses with attenuatedemotional intensity compared to human-authored content and show a bias towardpositive emotions in response tasks. Additionally, both models maintain strongsemantic similarity with original texts, though performance varies betweencontinuation and response tasks. These findings provide insights into LLMs'emotional and semantic processing capabilities, with implications for theirdeployment in social media contexts and human-AI interaction design.</description><author>Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu</author><pubDate>Wed, 15 Jan 2025 18:10:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08102v2</guid></item><item><title>Optimal Federated Learning for Functional Mean Estimation under Heterogeneous Privacy Constraints</title><link>http://arxiv.org/abs/2412.18992v2</link><description>Federated learning (FL) is a distributed machine learning technique designedto preserve data privacy and security, and it has gained significant importancedue to its broad range of applications. This paper addresses the problem ofoptimal functional mean estimation from discretely sampled data in a federatedsetting. We consider a heterogeneous framework where the number of individuals,measurements per individual, and privacy parameters vary across one or moreservers, under both common and independent design settings. In the commondesign setting, the same design points are measured for each individual,whereas in the independent design, each individual has their own randomcollection of design points. Within this framework, we establish minimax upperand lower bounds for the estimation error of the underlying mean function,highlighting the nuanced differences between common and independent designsunder distributed privacy constraints. We propose algorithms that achieve the optimal trade-off between privacy andaccuracy and provide optimality results that quantify the fundamental limits ofprivate functional mean estimation across diverse distributed settings. Theseresults characterize the cost of privacy and offer practical insights into thepotential for privacy-preserving statistical analysis in federatedenvironments.</description><author>Tony Cai, Abhinav Chakraborty, Lasse Vuursteen</author><pubDate>Wed, 15 Jan 2025 18:07:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18992v2</guid></item><item><title>DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video Diffusion Models</title><link>http://arxiv.org/abs/2501.08333v1</link><description>Understanding the ability of humans to use objects is crucial for AI toimprove daily life. Existing studies for learning such ability focus onhuman-object patterns (e.g., contact, spatial relation, orientation) in staticsituations, and learning Human-Object Interaction (HOI) patterns over time(i.e., movement of human and object) is relatively less explored. In thispaper, we introduce a novel type of affordance named Dynamic Affordance. For agiven input 3D object mesh, we learn dynamic affordance which models thedistribution of both (1) human motion and (2) human-guided object pose duringinteractions. As a core idea, we present a method to learn the 3D dynamicaffordance from synthetically generated 2D videos, leveraging a pre-trainedvideo diffusion model. Specifically, we propose a pipeline that first generates2D HOI videos from the 3D object and then lifts them into 3D to generate 4D HOIsamples. Once we generate diverse 4D HOI samples on various target objects, wetrain our DAViD, where we present a method based on the Low-Rank Adaptation(LoRA) module for pre-trained human motion diffusion model (MDM) and an objectpose diffusion model with human pose guidance. Our motion diffusion model isextended for multi-object interactions, demonstrating the advantage of ourpipeline with LoRA for combining the concepts of object usage. Throughextensive experiments, we demonstrate our DAViD outperforms the baselines ingenerating human motion with HOIs.</description><author>Hyeonwoo Kim, Sangwon Beak, Hanbyul Joo</author><pubDate>Tue, 14 Jan 2025 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08333v1</guid></item><item><title>MangaNinja: Line Art Colorization with Precise Reference Following</title><link>http://arxiv.org/abs/2501.08332v1</link><description>Derived from diffusion models, MangaNinjia specializes in the task ofreference-guided line art colorization. We incorporate two thoughtful designsto ensure precise character detail transcription, including a patch shufflingmodule to facilitate correspondence learning between the reference color imageand the target line art, and a point-driven control scheme to enablefine-grained color matching. Experiments on a self-collected benchmarkdemonstrate the superiority of our model over current solutions in terms ofprecise colorization. We further showcase the potential of the proposedinteractive point control in handling challenging cases, cross-charactercolorization, multi-reference harmonization, beyond the reach of existingalgorithms.</description><author>Zhiheng Liu, Ka Leong Cheng, Xi Chen, Jie Xiao, Hao Ouyang, Kai Zhu, Yu Liu, Yujun Shen, Qifeng Chen, Ping Luo</author><pubDate>Tue, 14 Jan 2025 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08332v1</guid></item><item><title>Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</title><link>http://arxiv.org/abs/2501.08331v1</link><description>Generative modeling aims to transform random noise into structured outputs.In this work, we enhance video diffusion models by allowing motion control viastructured latent noise sampling. This is achieved by just a change in data: wepre-process training videos to yield structured noise. Consequently, our methodis agnostic to diffusion model design, requiring no changes to modelarchitectures or training pipelines. Specifically, we propose a novel noisewarping algorithm, fast enough to run in real time, that replaces randomtemporal Gaussianity with correlated warped noise derived from optical flowfields, while preserving the spatial Gaussianity. The efficiency of ouralgorithm enables us to fine-tune modern video diffusion base models usingwarped noise with minimal overhead, and provide a one-stop solution for a widerange of user-friendly motion control: local object motion control, globalcamera movement control, and motion transfer. The harmonization betweentemporal coherence and spatial Gaussianity in our warped noise leads toeffective motion control while maintaining per-frame pixel quality. Extensiveexperiments and user studies demonstrate the advantages of our method, makingit a robust and scalable approach for controlling motion in video diffusionmodels. Video results are available on our webpage:https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow/; sourcecode and model checkpoints are available on GitHub:https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.</description><author>Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu</author><pubDate>Tue, 14 Jan 2025 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08331v1</guid></item><item><title>Gradient Equilibrium in Online Learning: Theory and Applications</title><link>http://arxiv.org/abs/2501.08330v1</link><description>We present a new perspective on online learning that we refer to as gradientequilibrium: a sequence of iterates achieves gradient equilibrium if theaverage of gradients of losses along the sequence converges to zero. Ingeneral, this condition is not implied by nor implies sublinear regret. Itturns out that gradient equilibrium is achievable by standard online learningmethods such as gradient descent and mirror descent with constant step sizes(rather than decaying step sizes, as is usually required for no regret).Further, as we show through examples, gradient equilibrium translates into aninterpretable and meaningful property in online prediction problems spanningregression, classification, quantile estimation, and others. Notably, we showthat the gradient equilibrium framework can be used to develop a debiasingscheme for black-box predictions under arbitrary distribution shift, based onsimple post hoc online descent updates. We also show that post hoc gradientupdates can be used to calibrate predicted quantiles under distribution shift,and that the framework leads to unbiased Elo scores for pairwise preferenceprediction.</description><author>Anastasios N. Angelopoulos, Michael I. Jordan, Ryan J. Tibshirani</author><pubDate>Tue, 14 Jan 2025 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08330v1</guid></item><item><title>Predicting 4D Hand Trajectory from Monocular Videos</title><link>http://arxiv.org/abs/2501.08329v1</link><description>We present HaPTIC, an approach that infers coherent 4D hand trajectories frommonocular videos. Current video-based hand pose reconstruction methodsprimarily focus on improving frame-wise 3D pose using adjacent frames ratherthan studying consistent 4D hand trajectories in space. Despite the additionaltemporal cues, they generally underperform compared to image-based methods dueto the scarcity of annotated video data. To address these issues, we repurposea state-of-the-art image-based transformer to take in multiple frames anddirectly predict a coherent trajectory. We introduce two types of lightweightattention layers: cross-view self-attention to fuse temporal information, andglobal cross-attention to bring in larger spatial context. Our method infers 4Dhand trajectories similar to the ground truth while maintaining strong 2Dreprojection alignment. We apply the method to both egocentric and allocentricvideos. It significantly outperforms existing methods in global trajectoryaccuracy while being comparable to the state-of-the-art in single-image poseestimation. Project website: https://judyye.github.io/haptic-www</description><author>Yufei Ye, Yao Feng, Omid Taheri, Haiwen Feng, Shubham Tulsiani, Michael J. Black</author><pubDate>Tue, 14 Jan 2025 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08329v1</guid></item><item><title>PokerBench: Training Large Language Models to become Professional Poker Players</title><link>http://arxiv.org/abs/2501.08328v1</link><description>We introduce PokerBench - a benchmark for evaluating the poker-playingabilities of large language models (LLMs). As LLMs excel in traditional NLPtasks, their application to complex, strategic games like poker poses a newchallenge. Poker, an incomplete information game, demands a multitude of skillssuch as mathematics, reasoning, planning, strategy, and a deep understanding ofgame theory and human psychology. This makes Poker the ideal next frontier forlarge language models. PokerBench consists of a comprehensive compilation of11,000 most important scenarios, split between pre-flop and post-flop play,developed in collaboration with trained poker players. We evaluate prominentmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,finding that all state-of-the-art LLMs underperform in playing optimal poker.However, after fine-tuning, these models show marked improvements. We validatePokerBench by having models with different scores compete with each other,demonstrating that higher scores on PokerBench lead to higher win rates inactual poker games. Through gameplay between our fine-tuned model and GPT-4, wealso identify limitations of simple supervised fine-tuning for learning optimalplaying strategy, suggesting the need for more advanced methodologies foreffectively training language models to excel in games. PokerBench thuspresents a unique benchmark for a quick and reliable evaluation of thepoker-playing ability of LLMs as well as a comprehensive benchmark to study theprogress of LLMs in complex game-playing scenarios. The dataset and code willbe made available at: \url{https://github.com/pokerllm/pokerbench}.</description><author>Richard Zhuang, Akshat Gupta, Richard Yang, Aniket Rahane, Zhengyu Li, Gopala Anumanchipalli</author><pubDate>Tue, 14 Jan 2025 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08328v1</guid></item><item><title>Multigenre AI-powered Story Composition</title><link>http://arxiv.org/abs/2405.06685v2</link><description>This paper shows how to construct genre patterns, whose purpose is to guideinteractive story composition in a way that enforces thematic consistency. Tostart the discussion we argue, based on previous seminal works, for theexistence of five fundamental genres, namely comedy, romance - in the sense ofepic plots, flourishing since the twelfth century -, tragedy, satire, andmystery. To construct the patterns, a simple two-phase process is employed:first retrieving examples that match our genre characterizations, and thenapplying a form of most specific generalization to the groups of examples inorder to find their commonalities. In both phases, AI agents are instrumental,with our PatternTeller prototype being called to operate the story compositionprocess, offering the opportunity to generate stories from a given premise ofthe user, to be developed under the guidance of the chosen pattern and tryingto accommodate the user's suggestions along the composition stages.</description><author>Edirlei Soares de Lima, Margot M. E. Neggers, Antonio L. Furtado</author><pubDate>Tue, 14 Jan 2025 18:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06685v2</guid></item><item><title>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks</title><link>http://arxiv.org/abs/2501.08326v1</link><description>We present Omni-RGPT, a multimodal large language model designed tofacilitate region-level comprehension for both images and videos. To achieveconsistent region representation across spatio-temporal dimensions, weintroduce Token Mark, a set of tokens highlighting the target regions withinthe visual feature space. These tokens are directly embedded into spatialregions using region prompts (e.g., boxes or masks) and simultaneouslyincorporated into the text prompt to specify the target, establishing a directconnection between visual and text tokens. To further support robust videounderstanding without requiring tracklets, we introduce an auxiliary task thatguides Token Mark by leveraging the consistency of the tokens, enabling stableregion interpretation across the video. Additionally, we introduce alarge-scale region-level video instruction dataset (RegVID-300k). Omni-RGPTachieves state-of-the-art results on image and video-based commonsensereasoning benchmarks while showing strong performance in captioning andreferring expression comprehension tasks.</description><author>Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-Chiang Frank Wang, Ryo Hachiuma</author><pubDate>Tue, 14 Jan 2025 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08326v1</guid></item><item><title>GameFactory: Creating New Games with Generative Interactive Videos</title><link>http://arxiv.org/abs/2501.08325v1</link><description>Generative game engines have the potential to revolutionize game developmentby autonomously creating new content and reducing manual workload. However,existing video-based game generation methods fail to address the criticalchallenge of scene generalization, limiting their applicability to existinggames with fixed styles and scenes. In this paper, we present GameFactory, aframework focused on exploring scene generalization in game video generation.To enable the creation of entirely new and diverse games, we leveragepre-trained video diffusion models trained on open-domain video data. To bridgethe domain gap between open-domain priors and small-scale game dataset, wepropose a multi-phase training strategy that decouples game style learning fromaction control, preserving open-domain generalization while achieving actioncontrollability. Using Minecraft as our data source, we release GF-Minecraft, ahigh-quality and diversity action-annotated video dataset for research.Furthermore, we extend our framework to enable autoregressiveaction-controllable game video generation, allowing the production ofunlimited-length interactive game videos. Experimental results demonstrate thatGameFactory effectively generates open-domain, diverse, and action-controllablegame videos, representing a significant step forward in AI-driven gamegeneration. Our dataset and project page are publicly available at\url{https://vvictoryuki.github.io/gamefactory/}.</description><author>Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu</author><pubDate>Tue, 14 Jan 2025 18:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08325v1</guid></item><item><title>ADAM-1: AI and Bioinformatics for Alzheimer's Detection and Microbiome-Clinical Data Integrations</title><link>http://arxiv.org/abs/2501.08324v1</link><description>The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agentlarge language model (LLM) framework designed to integrate and analyzemulti-modal data, including microbiome profiles, clinical datasets, andexternal knowledge bases, to enhance the understanding and detection ofAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)techniques along with its multi-agent architecture, ADAM-1 synthesizes insightsfrom diverse data sources and contextualizes findings using literature-drivenevidence. Comparative evaluation against XGBoost revealed similar mean F1scores but significantly reduced variance for ADAM-1, highlighting itsrobustness and consistency, particularly in small laboratory datasets. Whilecurrently tailored for binary classification tasks, future iterations aim toincorporate additional data modalities, such as neuroimaging and biomarkers, tobroaden the scalability and applicability for Alzheimer's research anddiagnostics.</description><author>Ziyuan Huang, Vishaldeep Kaur Sekhon, Ouyang Guo, Mark Newman, Roozbeh Sadeghian, Maria L. Vaida, Cynthia Jo, Doyle Ward, Vanni Bucci, John P. Haran</author><pubDate>Tue, 14 Jan 2025 18:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08324v1</guid></item><item><title>Exploring Robustness of Multilingual LLMs on Real-World Noisy Data</title><link>http://arxiv.org/abs/2501.08322v1</link><description>Large Language Models (LLMs) are trained on Web data that might containspelling errors made by humans. But do they become robust to similar real-worldnoise? In this paper, we investigate the effect of real-world spelling mistakeson the performance of 9 language models, with parameters ranging from 0.2B to13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), NameEntity Recognition (NER), and Intent Classification (IC). We perform ourexperiments on 6 different languages and build a dictionary of real-world noisefor them using the Wikipedia edit history. We show that the performance gap ofthe studied models on the clean and noisy test data averaged across all thedatasets and languages ranges from 2.3 to 4.3 absolute percentage points. Inaddition, mT5 models, in general, show more robustness compared to BLOOM,Falcon, and BERT-like models. In particular, mT5 (13B), was the most robust onaverage overall, across the 3 tasks, and in 4 of the 6 languages.</description><author>Amirhossein Aliakbarzadeh, Lucie Flek, Akbar Karimi</author><pubDate>Tue, 14 Jan 2025 18:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08322v1</guid></item><item><title>Enhancing Automated Interpretability with Output-Centric Feature Descriptions</title><link>http://arxiv.org/abs/2501.08319v1</link><description>Automated interpretability pipelines generate natural language descriptionsfor the concepts represented by features in large language models (LLMs), suchas plants or the first word in a sentence. These descriptions are derived usinginputs that activate the feature, which may be a dimension or a direction inthe model's representation space. However, identifying activating inputs iscostly, and the mechanistic role of a feature in model behavior is determinedboth by how inputs cause a feature to activate and by how feature activationaffects outputs. Using steering evaluations, we reveal that current pipelinesprovide descriptions that fail to capture the causal effect of the feature onoutputs. To fix this, we propose efficient, output-centric methods forautomatically generating feature descriptions. These methods use the tokensweighted higher after feature stimulation or the highest weight tokens afterapplying the vocabulary "unembedding" head directly to the feature. Ouroutput-centric descriptions better capture the causal effect of a feature onmodel outputs than input-centric descriptions, but combining the two leads tothe best performance on both input and output evaluations. Lastly, we show thatoutput-centric descriptions can be used to find inputs that activate featurespreviously thought to be "dead".</description><author>Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva</author><pubDate>Tue, 14 Jan 2025 18:53:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08319v1</guid></item><item><title>A Similarity Measure Between Functions with Applications to Statistical Learning and Optimization</title><link>http://arxiv.org/abs/2501.08317v1</link><description>In this note, we present a novel measure of similarity between two functions.It quantifies how the sub-optimality gaps of two functions convert to eachother, and unifies several existing notions of functional similarity. We showthat it has convenient operation rules, and illustrate its use in empiricalrisk minimization and non-stationary online optimization.</description><author>Chengpiao Huang, Kaizheng Wang</author><pubDate>Tue, 14 Jan 2025 18:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08317v1</guid></item><item><title>Diffusion Adversarial Post-Training for One-Step Video Generation</title><link>http://arxiv.org/abs/2501.08316v1</link><description>The diffusion models are widely used for image and video generation, buttheir iterative generation process is slow and expansive. While existingdistillation approaches have demonstrated the potential for one-step generationin the image domain, they still suffer from significant quality degradation. Inthis work, we propose Adversarial Post-Training (APT) against real datafollowing diffusion pre-training for one-step video generation. To improve thetraining stability and quality, we introduce several improvements to the modelarchitecture and training procedures, along with an approximated R1regularization objective. Empirically, our experiments show that ouradversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,24fps videos in real time using a single forward evaluation step. Additionally,our model is capable of generating 1024px images in a single step, achievingquality comparable to state-of-the-art methods.</description><author>Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang</author><pubDate>Tue, 14 Jan 2025 18:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08316v1</guid></item><item><title>Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation</title><link>http://arxiv.org/abs/2412.07169v3</link><description>Accurate uncertainty estimation is crucial for deploying neural networks inrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is awidely used technique for approximating predictive uncertainty by performingstochastic forward passes with dropout during inference. However, using staticdropout rates across all layers and inputs can lead to suboptimal uncertaintyestimates, as it fails to adapt to the varying characteristics of individualinputs and network layers. Existing approaches optimize dropout rates duringtraining using labeled data, resulting in fixed inference-time parameters thatcannot adjust to new data distributions, compromising uncertainty estimates inMonte Carlo simulations. In this paper, we propose Rate-In, an algorithm that dynamically adjustsdropout rates during inference by quantifying the information loss induced bydropout in each layer's feature maps. By treating dropout as controlled noiseinjection and leveraging information-theoretic principles, Rate-In adaptsdropout rates per layer and per input instance without requiring ground truthlabels. By quantifying the functional information loss in feature maps, weadaptively tune dropout rates to maintain perceptual quality across diversemedical imaging tasks and architectural configurations. Our extensive empiricalstudy on synthetic data and real-world medical imaging tasks demonstrates thatRate-In improves calibration and sharpens uncertainty estimates compared tofixed or heuristic dropout rates without compromising predictive performance.Rate-In offers a practical, unsupervised, inference-time approach to optimizingdropout for more reliable predictive uncertainty estimation in criticalapplications.</description><author>Tal Zeevi, Ravid Shwartz-Ziv, Yann LeCun, Lawrence H. Staib, John A. Onofrey</author><pubDate>Tue, 14 Jan 2025 18:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07169v3</guid></item><item><title>MiniMax-01: Scaling Foundation Models with Lightning Attention</title><link>http://arxiv.org/abs/2501.08313v1</link><description>We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,which are comparable to top-tier models while offering superior capabilities inprocessing longer contexts. The core lies in lightning attention and itsefficient scaling. To maximize computational capacity, we integrate it withMixture of Experts (MoE), creating a model with 32 experts and 456 billiontotal parameters, of which 45.9 billion are activated for each token. Wedevelop an optimized parallel strategy and highly efficientcomputation-communication overlap techniques for MoE and lightning attention.This approach enables us to conduct efficient training and inference on modelswith hundreds of billions of parameters across contexts spanning millions oftokens. The context window of MiniMax-Text-01 can reach up to 1 million tokensduring training and extrapolate to 4 million tokens during inference at anaffordable cost. Our vision-language model, MiniMax-VL-01 is built throughcontinued training with 512 billion vision-language tokens. Experiments on bothstandard and in-house benchmarks show that our models match the performance ofstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32times longer context window. We publicly release MiniMax-01 athttps://github.com/MiniMax-AI.</description><author>MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong</author><pubDate>Tue, 14 Jan 2025 18:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08313v1</guid></item><item><title>Everybody Likes to Sleep: A Computer-Assisted Comparison of Object Naming Data from 30 Languages</title><link>http://arxiv.org/abs/2501.08312v1</link><description>Object naming - the act of identifying an object with a word or a phrase - isa fundamental skill in interpersonal communication, relevant to manydisciplines, such as psycholinguistics, cognitive linguistics, or language andvision research. Object naming datasets, which consist of concept lists withpicture pairings, are used to gain insights into how humans access and selectnames for objects in their surroundings and to study the cognitive processesinvolved in converting visual stimuli into semantic concepts. Unfortunately,object naming datasets often lack transparency and have a highly idiosyncraticstructure. Our study tries to make current object naming data transparent andcomparable by using a multilingual, computer-assisted approach that linksindividual items of object naming lists to unified concepts. Our current samplelinks 17 object naming datasets that cover 30 languages from 10 differentlanguage families. We illustrate how the comparative dataset can be explored bysearching for concepts that recur across the majority of datasets and comparingthe conceptual spaces of covered object naming datasets with classical basicvocabulary lists from historical linguistics and linguistic typology. Ourfindings can serve as a basis for enhancing cross-linguistic object namingresearch and as a guideline for future studies dealing with object namingtasks.</description><author>Al≈æbƒõta Kuƒçerov√°, Johann-Mattis List</author><pubDate>Tue, 14 Jan 2025 18:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08312v1</guid></item><item><title>Path Loss Prediction Using Machine Learning with Extended Features</title><link>http://arxiv.org/abs/2501.08306v1</link><description>Wireless communications rely on path loss modeling, which is most effectivewhen it includes the physical details of the propagation environment. Acquiringthis data has historically been challenging, but geographic information systemdata is becoming increasingly available with higher resolution and accuracy.Access to such details enables propagation models to more accurately predictcoverage and minimize interference in wireless deployments. Machinelearning-based modeling can significantly support this effort, withfeature-based approaches allowing for accurate, efficient, and scalablepropagation modeling. Building on previous work, we introduce an extended setof features that improves prediction accuracy while, most importantly,maintaining model generalization across a broad range of environments.</description><author>Jonathan Ethier, Mathieu Chateauvert, Ryan G. Dempsey, Alexis Bose</author><pubDate>Tue, 14 Jan 2025 18:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08306v1</guid></item><item><title>Benchmarking Graph Representations and Graph Neural Networks for Multivariate Time Series Classification</title><link>http://arxiv.org/abs/2501.08305v1</link><description>Multivariate Time Series Classification (MTSC) enables the analysis ifcomplex temporal data, and thus serves as a cornerstone in various real-worldapplications, ranging from healthcare to finance. Since the relationship amongvariables in MTS usually contain crucial cues, a large number of graph-basedMTSC approaches have been proposed, as the graph topology and edges canexplicitly represent relationships among variables (channels), where not onlyvarious MTS graph representation learning strategies but also different GraphNeural Networks (GNNs) have been explored. Despite such progresses, there is nocomprehensive study that fairly benchmarks and investigates the performances ofexisting widely-used graph representation learning strategies/GNN classifiersin the application of different MTSC tasks. In this paper, we present the firstbenchmark which systematically investigates the effectiveness of thewidely-used three node feature definition strategies, four edge featurelearning strategies and five GNN architecture, resulting in 60 differentvariants for graph-based MTSC. These variants are developed and evaluated witha standardized data pipeline and training/validation/testing strategy on 26widely-used suspensor MTSC datasets. Our experiments highlight that nodefeatures significantly influence MTSC performance, while the visualization ofedge features illustrates why adaptive edge learning outperforms other edgefeature learning methods. The code of the proposed benchmark is publiclyavailable at\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.</description><author>Wennuo Yang, Shiling Wu, Yuzhi Zhou, Weicheng Xie, Linlin Shen, Siyang Song</author><pubDate>Tue, 14 Jan 2025 18:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08305v1</guid></item><item><title>Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers</title><link>http://arxiv.org/abs/2501.08303v1</link><description>Semantic future prediction is important for autonomous systems navigatingdynamic environments. This paper introduces FUTURIST, a method for multimodalfuture semantic prediction that uses a unified and efficient visual sequencetransformer architecture. Our approach incorporates a multimodal masked visualmodeling objective and a novel masking mechanism designed for multimodaltraining. This allows the model to effectively integrate visible informationfrom various modalities, improving prediction accuracy. Additionally, wepropose a VAE-free hierarchical tokenization process, which reducescomputational complexity, streamlines the training pipeline, and enablesend-to-end training with high-resolution, multimodal inputs. We validateFUTURIST on the Cityscapes dataset, demonstrating state-of-the-art performancein future semantic segmentation for both short- and mid-term forecasting. Weprovide the implementation code at https://github.com/Sta8is/FUTURIST .</description><author>Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis</author><pubDate>Tue, 14 Jan 2025 18:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08303v1</guid></item><item><title>Polynomial Threshold Functions of Bounded Tree-Width: Some Explainability and Complexity Aspects</title><link>http://arxiv.org/abs/2501.08297v1</link><description>The tree-width of a multivariate polynomial is the tree-width of thehypergraph with hyperedges corresponding to its terms. Multivariate polynomialsof bounded tree-width have been studied by Makowsky and Meer as a new sparsitycondition that allows for polynomial solvability of problems which areintractable in general. We consider a variation on this theme for Booleanvariables. A representation of a Boolean function as the sign of a polynomialis called a polynomial threshold representation. We discuss Boolean functionsrepresentable as polynomial threshold functions of bounded tree-width andpresent two applications to Bayesian network classifiers, a probabilisticgraphical model. Both applications are in Explainable Artificial Intelligence(XAI), the research area dealing with the black-box nature of many recentmachine learning models. We also give a separation result between therepresentational power of positive and general polynomial threshold functions.</description><author>Karine Chubarian, Johnny Joyce, Gyorgy Turan</author><pubDate>Tue, 14 Jan 2025 18:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08297v1</guid></item><item><title>A Survey on Pedophile Attribution Techniques for Online Platforms</title><link>http://arxiv.org/abs/2501.08296v1</link><description>Reliance on anonymity in social media has increased its popularity on theseplatforms among all ages. The availability of public Wi-Fi networks hasfacilitated a vast variety of online content, including social mediaapplications. Although anonymity and ease of access can be a convenient meansof communication for their users, it is difficult to manage and protect itsvulnerable users against sexual predators. Using an automated identificationsystem that can attribute predators to their text would make the solution moreattainable. In this survey, we provide a review of the methods of pedophileattribution used in social media platforms. We examine the effect of the sizeof the suspect set and the length of the text on the task of attribution.Moreover, we review the most-used datasets, features, classification techniquesand performance measures for attributing sexual predators. We found that fewstudies have proposed tools to mitigate the risk of online sexual predators,but none of them can provide suspect attribution. Finally, we list several openresearch problems.</description><author>Hiba Fallatah, Ching Suen, Olga Ormandjieva</author><pubDate>Tue, 14 Jan 2025 18:25:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08296v1</guid></item><item><title>LayerAnimate: Layer-specific Control for Animation</title><link>http://arxiv.org/abs/2501.08295v1</link><description>Animated video separates foreground and background elements into layers, withdistinct processes for sketching, refining, coloring, and in-betweening.Existing video generation methods typically treat animation as a monolithicdata domain, lacking fine-grained control over individual layers. In thispaper, we introduce LayerAnimate, a novel architectural approach that enhancesfine-grained control over individual animation layers within a video diffusionmodel, allowing users to independently manipulate foreground and backgroundelements in distinct layers. To address the challenge of limited layer-specificdata, we propose a data curation pipeline that features automated elementsegmentation, motion-state hierarchical merging, and motion coherencerefinement. Through quantitative and qualitative comparisons, and user study,we demonstrate that LayerAnimate outperforms current methods in terms ofanimation quality, control precision, and usability, making it an ideal toolfor both professional animators and amateur enthusiasts. This framework opensup new possibilities for layer-specific animation applications and creativeflexibility. Our code is available at https://layeranimate.github.io.</description><author>Yuxue Yang, Lue Fan, Zuzen Lin, Feng Wang, Zhaoxiang Zhang</author><pubDate>Tue, 14 Jan 2025 18:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08295v1</guid></item><item><title>Gaussian Eigen Models for Human Heads</title><link>http://arxiv.org/abs/2407.04545v2</link><description>Current personalized neural head avatars face a trade-off: lightweight modelslack detail and realism, while high-quality, animatable avatars requiresignificant computational resources, making them unsuitable for commoditydevices. To address this gap, we introduce Gaussian Eigen Models (GEM), whichprovide high-quality, lightweight, and easily controllable head avatars. GEMutilizes 3D Gaussian primitives for representing the appearance combined withGaussian splatting for rendering. Building on the success of mesh-based 3Dmorphable face models (3DMM), we define GEM as an ensemble of linear eigenbasesfor representing the head appearance of a specific subject. In particular, weconstruct linear bases to represent the position, scale, rotation, and opacityof the 3D Gaussians. This allows us to efficiently generate Gaussian primitivesof a specific head shape by a linear combination of the basis vectors, onlyrequiring a low-dimensional parameter vector that contains the respectivecoefficients. We propose to construct these linear bases (GEM) by distillinghigh-quality compute-intense CNN-based Gaussian avatar models that can generateexpression-dependent appearance changes like wrinkles. These high-qualitymodels are trained on multi-view videos of a subject and are distilled using aseries of principal component analyses. Once we have obtained the bases thatrepresent the animatable appearance space of a specific human, we learn aregressor that takes a single RGB image as input and predicts thelow-dimensional parameter vector that corresponds to the shown facialexpression. In a series of experiments, we compare GEM's self-reenactment andcross-person reenactment results to state-of-the-art 3D avatar methods,demonstrating GEM's higher visual quality and better generalization to newexpressions.</description><author>Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies</author><pubDate>Tue, 14 Jan 2025 18:20:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04545v2</guid></item><item><title>HALoGEN: Fantastic LLM Hallucinations and Where to Find Them</title><link>http://arxiv.org/abs/2501.08292v1</link><description>Despite their impressive ability to generate high-quality and fluent text,generative large language models (LLMs) also produce hallucinations: statementsthat are misaligned with established world knowledge or provided input context.However, measuring hallucination can be challenging, as having humans verifymodel generations on-the-fly is both expensive and time-consuming. In thiswork, we release HALoGEN, a comprehensive hallucination benchmark consistingof: (1) 10,923 prompts for generative models spanning nine domains includingprogramming, scientific attribution, and summarization, and (2) automatichigh-precision verifiers for each use case that decompose LLM generations intoatomic units, and verify each unit against a high-quality knowledge source. Weuse this framework to evaluate ~150,000 generations from 14 language models,finding that even the best-performing models are riddled with hallucinations(sometimes up to 86% of generated atomic facts depending on the domain). Wefurther define a novel error classification for LLM hallucinations based onwhether they likely stem from incorrect recollection of training data (Type Aerrors), or incorrect knowledge in training data (Type B errors), or arefabrication (Type C errors). We hope our framework provides a foundation toenable the principled study of why generative models hallucinate, and advancesthe development of trustworthy large language models.</description><author>Abhilasha Ravichander, Shrusti Ghela, David Wadden, Yejin Choi</author><pubDate>Tue, 14 Jan 2025 18:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08292v1</guid></item><item><title>Avoiding subtraction and division of stochastic signals using normalizing flows: NFdeconvolve</title><link>http://arxiv.org/abs/2501.08288v1</link><description>Across the scientific realm, we find ourselves subtracting or dividingstochastic signals. For instance, consider a stochastic realization, $x$,generated from the addition or multiplication of two stochastic signals $a$ and$b$, namely $x=a+b$ or $x = ab$. For the $x=a+b$ example, $a$ can befluorescence background and $b$ the signal of interest whose statistics are tobe learned from the measured $x$. Similarly, when writing $x=ab$, $a$ can bethought of as the illumination intensity and $b$ the density of fluorescentmolecules of interest. Yet dividing or subtracting stochastic signals amplifiesnoise, and we ask instead whether, using the statistics of $a$ and themeasurement of $x$ as input, we can recover the statistics of $b$. Here, weshow how normalizing flows can generate an approximation of the probabilitydistribution over $b$, thereby avoiding subtraction or division altogether.This method is implemented in our software package, NFdeconvolve, available onGitHub with a tutorial linked in the main text.</description><author>Pedro Pessoa, Max Schweiger, Lance W. Q. Xu, Tristan Manha, Ayush Saurabh, Julian Antolin Camarena, Steve Press√©</author><pubDate>Tue, 14 Jan 2025 18:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08288v1</guid></item><item><title>A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems using Disparity Maps</title><link>http://arxiv.org/abs/2410.24031v2</link><description>Face recognition technologies are increasingly used in various applications,yet they are vulnerable to face spoofing attacks. These spoofing attacks ofteninvolve unique 3D structures, such as printed papers or mobile device screens.Although stereo-depth cameras can detect such attacks effectively, theirhigh-cost limits their widespread adoption. Conversely, two-sensor systemswithout extrinsic calibration offer a cost-effective alternative but are unableto calculate depth using stereo techniques. In this work, we propose a methodto overcome this challenge by leveraging facial attributes to derive disparityinformation and estimate relative depth for anti-spoofing purposes, usingnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coinedDisparity Model, that incorporates created disparity maps as a third modalityalongside the two original sensor modalities. We demonstrate the effectivenessof the Disparity Model in countering various spoof attacks using acomprehensive dataset collected from the Intel RealSense ID Solution F455. Ourmethod outperformed existing methods in the literature, achieving an EqualError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a FalsePositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than theerrors of the best comparison method, respectively. Additionally, we introducea model ensemble that addresses 3D spoof attacks as well, achieving an EER of2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides astate-of-the-art solution for the challenging task of anti-spoofing innon-calibrated systems that lack depth information.</description><author>Ariel Larey, Eyal Rond, Omer Achrack</author><pubDate>Tue, 14 Jan 2025 18:03:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24031v2</guid></item><item><title>VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes</title><link>http://arxiv.org/abs/2501.08286v1</link><description>VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM frameworkdesigned for large scenes. The framework comprises four main components: VIOFront End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIOFront End, RGB frames are processed through dense bundle adjustment anduncertainty estimation to extract scene geometry and poses. Based on thisoutput, the mapping module incrementally constructs and maintains a 2D Gaussianmap. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,Score Manager, and Pose Refinement, which collectively improve mapping speedand localization accuracy. This enables the SLAM system to handle large-scaleurban environments with up to 50 million Gaussian ellipsoids. To ensure globalconsistency in large-scale scenes, we design a Loop Closure module, whichinnovatively leverages the Novel View Synthesis (NVS) capabilities of GaussianSplatting for loop closure detection and correction of the Gaussian map.Additionally, we propose a Dynamic Eraser to address the inevitable presence ofdynamic objects in real-world outdoor scenes. Extensive evaluations in indoorand outdoor environments demonstrate that our approach achieves localizationperformance on par with Visual-Inertial Odometry while surpassing recentGS/NeRF SLAM methods. It also significantly outperforms all existing methods interms of mapping and rendering quality. Furthermore, we developed a mobile appand verified that our framework can generate high-quality Gaussian maps in realtime using only a smartphone camera and a low-frequency IMU sensor. To the bestof our knowledge, VINGS-Mono is the first monocular Gaussian SLAM methodcapable of operating in outdoor environments and supporting kilometer-scalelarge scenes.</description><author>Ke Wu, Zicheng Zhang, Muer Tie, Ziqing Ai, Zhongxue Gan, Wenchao Ding</author><pubDate>Tue, 14 Jan 2025 18:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08286v1</guid></item><item><title>Can Bayesian Neural Networks Explicitly Model Input Uncertainty?</title><link>http://arxiv.org/abs/2501.08285v1</link><description>Inputs to machine learning models can have associated noise or uncertainties,but they are often ignored and not modelled. It is unknown if Bayesian NeuralNetworks and their approximations are able to consider uncertainty in theirinputs. In this paper we build a two input Bayesian Neural Network (mean andstandard deviation) and evaluate its capabilities for input uncertaintyestimation across different methods like Ensembles, MC-Dropout, and Flipout.Our results indicate that only some uncertainty estimation methods forapproximate Bayesian NNs can model input uncertainty, in particular Ensemblesand Flipout.</description><author>Matias Valdenegro-Toro, Marco Zullich</author><pubDate>Tue, 14 Jan 2025 18:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08285v1</guid></item><item><title>AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages</title><link>http://arxiv.org/abs/2501.08284v1</link><description>Hate speech and abusive language are global phenomena that needsocio-cultural background knowledge to be understood, identified, andmoderated. However, in many regions of the Global South, there have beenseveral documented occurrences of (1) absence of moderation and (2) censorshipdue to the reliance on keyword spotting out of context. Further, high-profileindividuals have frequently been at the center of the moderation process, whilelarge and targeted hate speech campaigns against minorities have beenoverlooked. These limitations are mainly due to the lack of high-quality datain the local languages and the failure to include local communities in thecollection, annotation, and moderation processes. To address this issue, wepresent AfriHate: a multilingual collection of hate speech and abusive languagedatasets in 15 African languages. Each instance in AfriHate is annotated bynative speakers familiar with the local culture. We report the challengesrelated to the construction of the datasets and present various classificationbaseline results with and without using LLMs. The datasets, individualannotations, and hate speech and offensive language lexicons are available onhttps://github.com/AfriHate/AfriHate</description><author>Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Saminu Mohammad Aliyu, Nelson Odhiambo Onyango, Lilian D. A. Wanzare, Samuel Rutunda, Lukman Jibril Aliyu, Esubalew Alemneh, Oumaima Hourrane, Hagos Tesfahun Gebremichael, Elyas Abdi Ismail, Meriem Beloucif, Ebrahim Chekol Jibril, Andiswa Bukula, Rooweither Mabuya, Salomey Osei, Abigail Oppong, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw, Chiamaka Ijeoma Chukwuneke, Paul R√∂ttger, Seid Muhie Yimam, Nedjma Ousidhoum</author><pubDate>Tue, 14 Jan 2025 18:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08284v1</guid></item><item><title>LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding</title><link>http://arxiv.org/abs/2501.08282v1</link><description>Recent advancements in multimodal large language models (MLLMs) have shownpromising results, yet existing approaches struggle to effectively handle bothtemporal and spatial localization simultaneously. This challenge stems from twokey issues: first, incorporating spatial-temporal localization introduces avast number of coordinate combinations, complicating the alignment oflinguistic and visual coordinate representations; second, encoding fine-grainedtemporal and spatial information during video feature compression is inherentlydifficult. To address these issues, we propose LLaVA-ST, a MLLM forfine-grained spatial-temporal multimodal understanding. In LLaVA-ST, we proposeLanguage-Aligned Positional Embedding, which embeds the textual coordinatespecial token into the visual space, simplifying the alignment of fine-grainedspatial-temporal correspondences. Additionally, we design the Spatial-TemporalPacker, which decouples the feature compression of temporal and spatialresolutions into two distinct point-to-region attention processing streams.Furthermore, we propose ST-Align dataset with 4.3M training samples forfine-grained spatial-temporal multimodal understanding. With ST-align, wepresent a progressive training pipeline that aligns the visual and textualfeature through sequential coarse-to-fine stages.Additionally, we introduce anST-Align benchmark to evaluate spatial-temporal interleaved fine-grainedunderstanding tasks, which include Spatial-Temporal Video Grounding (STVG) ,Event Localization and Captioning (ELC) and Spatial Video Grounding (SVG).LLaVA-ST achieves outstanding performance on 11 benchmarks requiringfine-grained temporal, spatial, or spatial-temporal interleaving multimodalunderstanding. Our code, data and benchmark will be released at Our code, dataand benchmark will be released at https://github.com/appletea233/LLaVA-ST .</description><author>Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, Si Liu</author><pubDate>Tue, 14 Jan 2025 17:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08282v1</guid></item><item><title>Decoding Interpretable Logic Rules from Neural Networks</title><link>http://arxiv.org/abs/2501.08281v1</link><description>As deep neural networks continue to excel across various domains, theirblack-box nature has raised concerns about transparency and trust. Inparticular, interpretability has become increasingly essential for applicationsthat demand high safety and knowledge rigor, such as drug discovery, autonomousdriving, and genomics. However, progress in understanding even the simplestdeep neural networks - such as fully connected networks - has been limited,despite their role as foundational elements in state-of-the-art models likeResNet and Transformer. In this paper, we address this challenge by introducingNeuroLogic, a novel approach for decoding interpretable logic rules from neuralnetworks. NeuroLogic leverages neural activation patterns to capture themodel's critical decision-making processes, translating them into logical rulesrepresented by hidden predicates. Thanks to its flexible design in thegrounding phase, NeuroLogic can be adapted to a wide range of neural networks.For simple fully connected neural networks, hidden predicates can be groundedin certain split patterns of original input features to derivedecision-tree-like rules. For large, complex vision neural networks, NeuroLogicgrounds hidden predicates into high-level visual concepts that areunderstandable to humans. Our empirical study demonstrates that NeuroLogic canextract global and interpretable rules from state-of-the-art models such asResNet, a task at which existing work struggles. We believe NeuroLogic can helppave the way for understanding the black-box nature of neural networks.</description><author>Chuqin Geng, Xiaojie Xu, Zhaoyue Wang, Ziyu Zhao, Xujie Si</author><pubDate>Tue, 14 Jan 2025 17:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08281v1</guid></item><item><title>SmartEraser: Remove Anything from Images using Masked-Region Guidance</title><link>http://arxiv.org/abs/2501.08279v1</link><description>Object removal has so far been dominated by the mask-and-inpaint paradigm,where the masked region is excluded from the input, leaving models relying onunmasked areas to inpaint the missing region. However, this approach lackscontextual information for the masked area, often resulting in unstableperformance. In this work, we introduce SmartEraser, built with a new removingparadigm called Masked-Region Guidance. This paradigm retains the masked regionin the input, using it as guidance for the removal process. It offers severaldistinct advantages: (a) it guides the model to accurately identify the objectto be removed, preventing its regeneration in the output; (b) since the usermask often extends beyond the object itself, it aids in preserving thesurrounding context in the final result. Leveraging this new paradigm, wepresent Syn4Removal, a large-scale object removal dataset, where instancesegmentation data is used to copy and paste objects onto images as removaltargets, with the original images serving as ground truths. Experimentalresults demonstrate that SmartEraser significantly outperforms existingmethods, achieving superior performance in object removal, especially incomplex scenes with intricate compositions.</description><author>Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, Houqiang Li</author><pubDate>Tue, 14 Jan 2025 17:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08279v1</guid></item><item><title>Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax</title><link>http://arxiv.org/abs/2410.06993v2</link><description>Deep InfoMax (DIM) is a well-established method for self-supervisedrepresentation learning (SSRL) based on maximization of the mutual informationbetween the input and the output of a deep neural network encoder. Despite theDIM and contrastive SSRL in general being well-explored, the task of learningrepresentations conforming to a specific distribution (i.e., distributionmatching, DM) is still under-addressed. Motivated by the importance of DM toseveral downstream tasks (including generative modeling, disentanglement,outliers detection and other), we enhance DIM to enable automatic matching oflearned representations to a selected prior distribution. To achieve this, wepropose injecting an independent noise into the normalized outputs of theencoder, while keeping the same InfoMax training objective. We show that suchmodification allows for learning uniformly and normally distributedrepresentations, as well as representations of other absolutely continuousdistributions. Our approach is tested on various downstream tasks. The resultsindicate a moderate trade-off between the performance on the downstream tasksand quality of DM.</description><author>Ivan Butakov, Alexander Semenenko, Alexander Tolmachev, Andrey Gladkov, Marina Munkhoeva, Alexey Frolov</author><pubDate>Tue, 14 Jan 2025 17:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06993v2</guid></item><item><title>Exploring Robustness of LLMs to Sociodemographically-Conditioned Paraphrasing</title><link>http://arxiv.org/abs/2501.08276v1</link><description>Large Language Models (LLMs) have shown impressive performance in various NLPtasks. However, there are concerns about their reliability in different domainsof linguistic variations. Many works have proposed robustness evaluationmeasures for local adversarial attacks, but we need globally robust modelsunbiased to different language styles. We take a broader approach to explore awider range of variations across sociodemographic dimensions to performstructured reliability tests on the reasoning capacity of language models. Weextend the SocialIQA dataset to create diverse paraphrased sets conditioned onsociodemographic styles. The assessment aims to provide a deeper understandingof LLMs in (a) their capability of generating demographic paraphrases withengineered prompts and (b) their reasoning capabilities in real-world, complexlanguage scenarios. We also explore measures such as perplexity,explainability, and ATOMIC performance of paraphrases for fine-grainedreliability analysis of LLMs on these sets. We find that demographic-specificparaphrasing significantly impacts the performance of language models,indicating that the subtleties of language variations remain a significantchallenge. The code and dataset will be made available for reproducibility andfuture research.</description><author>Pulkit Arora, Akbar Karimi, Lucie Flek</author><pubDate>Tue, 14 Jan 2025 17:50:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08276v1</guid></item><item><title>RMem: Restricted Memory Banks Improve Video Object Segmentation</title><link>http://arxiv.org/abs/2406.08476v2</link><description>With recent video object segmentation (VOS) benchmarks evolving tochallenging scenarios, we revisit a simple but overlooked strategy: restrictingthe size of memory banks. This diverges from the prevalent practice ofexpanding memory banks to accommodate extensive historical information. Ourspecially designed "memory deciphering" study offers a pivotal insightunderpinning such a strategy: expanding memory banks, while seeminglybeneficial, actually increases the difficulty for VOS modules to decoderelevant features due to the confusion from redundant information. Byrestricting memory banks to a limited number of essential frames, we achieve anotable improvement in VOS accuracy. This process balances the importance andfreshness of frames to maintain an informative memory bank within a boundedcapacity. Additionally, restricted memory banks reduce the training-inferencediscrepancy in memory lengths compared with continuous expansion. This fostersnew opportunities in temporal reasoning and enables us to introduce thepreviously overlooked "temporal positional embedding." Finally, our insightsare embodied in "RMem" ("R" for restricted), a simple yet effective VOSmodification that excels at challenging VOS scenarios and establishes new stateof the art for object state changes (on the VOST dataset) and long videos (onthe Long Videos dataset). Our code and demo are available athttps://restricted-memory.github.io/.</description><author>Junbao Zhou, Ziqi Pang, Yu-Xiong Wang</author><pubDate>Tue, 14 Jan 2025 17:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08476v2</guid></item><item><title>Comparative Analysis of Efficient Adapter-Based Fine-Tuning of State-of-the-Art Transformer Models</title><link>http://arxiv.org/abs/2501.08271v1</link><description>In this work, we investigate the efficacy of various adapter architectures onsupervised binary classification tasks from the SuperGLUE benchmark as well asa supervised multi-class news category classification task from Kaggle.Specifically, we compare classification performance and time complexity ofthree transformer models, namely DistilBERT, ELECTRA, and BART, usingconventional fine-tuning as well as nine state-of-the-art (SoTA) adapterarchitectures. Our analysis reveals performance differences across adapterarchitectures, highlighting their ability to achieve comparable or betterperformance relative to fine-tuning at a fraction of the training time. Similarresults are observed on the new classification task, further supporting ourfindings and demonstrating adapters as efficient and flexible alternatives tofine-tuning. This study provides valuable insights and guidelines for selectingand implementing adapters in diverse natural language processing (NLP)applications.</description><author>Saad Mashkoor Siddiqui, Mohammad Ali Sheikh, Muhammad Aleem, Kajol R Singh</author><pubDate>Tue, 14 Jan 2025 17:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08271v1</guid></item><item><title>FaVoR: Features via Voxel Rendering for Camera Relocalization</title><link>http://arxiv.org/abs/2409.07571v3</link><description>Camera relocalization methods range from dense image alignment to directcamera pose regression from a query image. Among these, sparse feature matchingstands out as an efficient, versatile, and generally lightweight approach withnumerous applications. However, feature-based methods often struggle withsignificant viewpoint and appearance changes, leading to matching failures andinaccurate pose estimates. To overcome this limitation, we propose a novelapproach that leverages a globally sparse yet locally dense 3D representationof 2D features. By tracking and triangulating landmarks over a sequence offrames, we construct a sparse voxel map optimized to render image patchdescriptors observed during tracking. Given an initial pose estimate, we firstsynthesize descriptors from the voxels using volumetric rendering and thenperform feature matching to estimate the camera pose. This methodology enablesthe generation of descriptors for unseen views, enhancing robustness to viewchanges. We extensively evaluate our method on the 7-Scenes and CambridgeLandmarks datasets. Our results show that our method significantly outperformsexisting state-of-the-art feature representation techniques in indoorenvironments, achieving up to a 39% improvement in median translation error.Additionally, our approach yields comparable results to other methods foroutdoor scenarios while maintaining lower memory and computational costs.</description><author>Vincenzo Polizzi, Marco Cannici, Davide Scaramuzza, Jonathan Kelly</author><pubDate>Tue, 14 Jan 2025 17:33:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07571v3</guid></item><item><title>Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation</title><link>http://arxiv.org/abs/2501.06693v2</link><description>Sim-to-real gap has long posed a significant challenge for robot learning insimulation, preventing the deployment of learned models in the real world.Previous work has primarily focused on domain randomization and systemidentification to mitigate this gap. However, these methods are often limitedby the inherent constraints of the simulation and graphics engines. In thiswork, we propose Vid2Sim, a novel framework that effectively bridges thesim2real gap through a scalable and cost-efficient real2sim pipeline for neural3D scene reconstruction and simulation. Given a monocular video as input,Vid2Sim can generate photorealistic and physically interactable 3D simulationenvironments to enable the reinforcement learning of visual navigation agentsin complex urban environments. Extensive experiments demonstrate that Vid2Simsignificantly improves the performance of urban navigation in the digital twinsand real world by 31.2% and 68.3% in success rate compared with agents trainedwith prior simulation methods.</description><author>Ziyang Xie, Zhizheng Liu, Zhenghao Peng, Wayne Wu, Bolei Zhou</author><pubDate>Tue, 14 Jan 2025 17:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06693v2</guid></item><item><title>AI Driven Water Segmentation with deep learning models for Enhanced Flood Monitoring</title><link>http://arxiv.org/abs/2501.08266v1</link><description>Flooding is a major natural hazard causing significant fatalities andeconomic losses annually, with increasing frequency due to climate change.Rapid and accurate flood detection and monitoring are crucial for mitigatingthese impacts. This study compares the performance of three deep learningmodels UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid inflood detection, utilizing images from drones, in field observations, andsocial media. This study involves creating a new dataset that augmentswellknown benchmark datasets with flood-specific images, enhancing therobustness of the models. The UNet, ResNet, and DeepLab v3 architectures aretested to determine their effectiveness in various environmental conditions andgeographical locations, and the strengths and limitations of each model arealso discussed here, providing insights into their applicability in differentscenarios by predicting image segmentation masks. This fully automated approachallows these models to isolate flooded areas in images, significantly reducingprocessing time compared to traditional semi-automated methods. The outcome ofthis study is to predict segmented masks for each image effected by a flooddisaster and the validation accuracy of these models. This methodologyfacilitates timely and continuous flood monitoring, providing vital data foremergency response teams to reduce loss of life and economic damages. It offersa significant reduction in the time required to generate flood maps, cuttingdown the manual processing time. Additionally, we present avenues for futureresearch, including the integration of multimodal data sources and thedevelopment of robust deep learning architectures tailored specifically forflood detection tasks. Overall, our work contributes to the advancement offlood management strategies through innovative use of deep learningtechnologies.</description><author>Sanjida Afrin Mou, Tasfia Noor Chowdhury, Adib Ibn Mannan, Sadia Nourin Mim, Lubana Tarannum, Tasrin Noman, Jamal Uddin Ahamed</author><pubDate>Tue, 14 Jan 2025 17:26:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08266v1</guid></item><item><title>Multiplayer Federated Learning: Reaching Equilibrium with Less Communication</title><link>http://arxiv.org/abs/2501.08263v1</link><description>Traditional Federated Learning (FL) approaches assume collaborative clientswith aligned objectives working towards a shared global model. However, in manyreal-world scenarios, clients act as rational players with individualobjectives and strategic behaviors, a concept that existing FL frameworks arenot equipped to adequately address. To bridge this gap, we introduceMultiplayer Federated Learning (MpFL), a novel framework that models theclients in the FL environment as players in a game-theoretic context, aiming toreach an equilibrium. In this scenario, each player tries to optimize their ownutility function, which may not align with the collective goal. Within MpFL, wepropose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithmin which each player/client performs local updates independently andperiodically communicates with other players. We theoretically analyzePEARL-SGD and prove that it reaches a neighborhood of equilibrium with lesscommunication in the stochastic setup compared to its non-local counterpart.Finally, we verify our theoretical findings through numerical experiments.</description><author>TaeHo Yoon, Sayantan Choudhury, Nicolas Loizou</author><pubDate>Tue, 14 Jan 2025 17:23:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08263v1</guid></item><item><title>CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation</title><link>http://arxiv.org/abs/2410.02748v3</link><description>Existing automatic prompt engineering methods are typically designed fordiscriminative tasks, where new task prompts are iteratively refined withlimited feedback from a single metric reflecting a single aspect. However,these approaches are suboptimal for generative tasks, which require morenuanced guidance beyond a single numeric metric to improve the prompt andoptimize multiple aspects of the generated text. To address these challenges,we propose a novel multi-aspect Critique-Suggestion-guided automatic PromptOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion moduleas its core component. This module spontaneously discovers aspects, andcompares generated and reference texts across these aspects, providing specificsuggestions for prompt modification. These clear critiques and actionablesuggestions guide a receptive optimizer module to make more substantialchanges, exploring a broader and more effective search space. To furtherimprove CriSPO with multi-metric optimization, we introduce an Automatic SuffixTuning (AST) extension to enhance the performance of task prompts acrossmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4summarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE scoreimprovement on summarization and substantial improvement of various metrics onQA. Code available at https://github.com/amazon-science/crispo</description><author>Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff</author><pubDate>Tue, 14 Jan 2025 17:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02748v3</guid></item><item><title>FDPP: Fine-tune Diffusion Policy with Human Preference</title><link>http://arxiv.org/abs/2501.08259v1</link><description>Imitation learning from human demonstrations enables robots to performcomplex manipulation tasks and has recently witnessed huge success. However,these techniques often struggle to adapt behavior to new preferences or changesin the environment. To address these limitations, we propose Fine-tuningDiffusion Policy with Human Preference (FDPP). FDPP learns a reward functionthrough preference-based learning. This reward is then used to fine-tune thepre-trained policy with reinforcement learning (RL), resulting in alignment ofpre-trained policy with new human preferences while still solving the originaltask. Our experiments across various robotic tasks and preferences demonstratethat FDPP effectively customizes policy behavior without compromisingperformance. Additionally, we show that incorporating Kullback-Leibler (KL)regularization during fine-tuning prevents over-fitting and helps maintain thecompetencies of the initial policy.</description><author>Yuxin Chen, Devesh K. Jha, Masayoshi Tomizuka, Diego Romeres</author><pubDate>Tue, 14 Jan 2025 17:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08259v1</guid></item><item><title>Towards an End-to-End (E2E) Adversarial Learning and Application in the Physical World</title><link>http://arxiv.org/abs/2501.08258v1</link><description>The traditional learning process of patch-based adversarial attacks,conducted in the digital domain and then applied in the physical domain (e.g.,via printed stickers), may suffer from reduced performance due to adversarialpatches' limited transferability from the digital domain to the physicaldomain. Given that previous studies have considered using projectors to applyadversarial attacks, we raise the following question: can adversarial learning(i.e., patch generation) be performed entirely in the physical domain with aprojector? In this work, we propose the Physical-domain Adversarial PatchLearning Augmentation (PAPLA) framework, a novel end-to-end (E2E) frameworkthat converts adversarial learning from the digital domain to the physicaldomain using a projector. We evaluate PAPLA across multiple scenarios,including controlled laboratory settings and realistic outdoor environments,demonstrating its ability to ensure attack success compared to conventionaldigital learning-physical application (DL-PA) methods. We also analyze theimpact of environmental factors, such as projection surface color, projectorstrength, ambient light, distance, and angle of the target object relative tothe camera, on the effectiveness of projected patches. Finally, we demonstratethe feasibility of the attack against a parked car and a stop sign in areal-world outdoor environment. Our results show that under specificconditions, E2E adversarial learning in the physical domain eliminates thetransferability issue and ensures evasion by object detectors. Finally, weprovide insights into the challenges and opportunities of applying adversariallearning in the physical domain and explain where such an approach is moreeffective than using a sticker.</description><author>Dudi Biton, Jacob Shams, Koda Satoru, Asaf Shabtai, Yuval Elovici, Ben Nassi</author><pubDate>Tue, 14 Jan 2025 17:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08258v1</guid></item><item><title>Automated Detection and Analysis of Minor Deformations in Flat Walls Due to Railway Vibrations Using LiDAR and Machine Learning</title><link>http://arxiv.org/abs/2501.06457v2</link><description>This study introduces an advanced methodology for automatically identifyingminor deformations in flat walls caused by vibrations from nearby railwaytracks. It leverages high-density Terrestrial Laser Scanner (TLS) LiDAR surveysand AI/ML techniques to collect and analyze data. The scan data is processedinto a detailed point cloud, which is segmented to distinguish ground points,trees, buildings, and other objects. The analysis focuses on identifyingsections along flat walls and estimating their deformations relative to theground orientation. Findings from the study, conducted at the RGIPT campus, reveal significantdeformations in walls close to the railway corridor, with the highestdeformations ranging from 7 to 8 cm and an average of 3 to 4 cm. In contrast,walls further from the corridor show negligible deformations. The developedautomated process for feature extraction and deformation monitoringdemonstrates potential for structural health monitoring. By integrating LiDARdata with machine learning, the methodology provides an efficient system foridentifying and analyzing structural deformations, highlighting the importanceof continuous monitoring for ensuring structural integrity and public safety inurban infrastructure. This approach represents a substantial advancement inautomated feature extraction and deformation analysis, contributing to moreeffective management of urban infrastructure.</description><author>Surjo Dey, Ankit Sharma, Hritu Raj, Susham Biswas</author><pubDate>Tue, 14 Jan 2025 16:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06457v2</guid></item><item><title>Language-Agnostic Modeling of Source Reliability on Wikipedia</title><link>http://arxiv.org/abs/2410.18803v2</link><description>Over the last few years, content verification through reliable sources hasbecome a fundamental need to combat disinformation. Here, we present alanguage-agnostic model designed to assess the reliability of sources acrossmultiple language editions of Wikipedia. Utilizing editorial activity data, themodel evaluates source reliability within different articles of varyingcontroversiality such as Climate Change, COVID-19, History, Media, and Biologytopics. Crafting features that express domain usage across articles, the modeleffectively predicts source reliability, achieving an F1 Macro score ofapproximately 0.80 for English and other high-resource languages. Formid-resource languages, we achieve 0.65 while the performance of low-resourcelanguages varies; in all cases, the time the domain remains present in thearticles (which we dub as permanence) is one of the most predictive features.We highlight the challenge of maintaining consistent model performance acrosslanguages of varying resource levels and demonstrate that adapting models fromhigher-resource languages can improve performance. This work contributes notonly to Wikipedia's efforts in ensuring content verifiability but in ensuringreliability across diverse user-generated content in various languagecommunities.</description><author>Jacopo D'Ignazi, Andreas Kaltenbrunner, Yelena Mejova, Michele Tizzani, Kyriaki Kalimeri, Mariano Beir√≥, Pablo Arag√≥n</author><pubDate>Tue, 14 Jan 2025 16:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18803v2</guid></item><item><title>Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models</title><link>http://arxiv.org/abs/2410.10733v4</link><description>We present Deep Compression Autoencoder (DC-AE), a new family of autoencodermodels for accelerating high-resolution diffusion models. Existing autoencodermodels have demonstrated impressive results at a moderate spatial compressionratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy forhigh spatial compression ratios (e.g., 64x). We address this challenge byintroducing two key techniques: (1) Residual Autoencoding, where we design ourmodels to learn residuals based on the space-to-channel transformed features toalleviate the optimization difficulty of high spatial-compression autoencoders;(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phasestraining strategy for mitigating the generalization penalty of highspatial-compression autoencoders. With these designs, we improve theautoencoder's spatial compression ratio up to 128 while maintaining thereconstruction quality. Applying our DC-AE to latent diffusion models, weachieve significant speedup without accuracy drop. For example, on ImageNet512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedupon H100 GPU for UViT-H while achieving a better FID, compared with the widelyused SD-VAE-f8 autoencoder. Our code is available athttps://github.com/mit-han-lab/efficientvit.</description><author>Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han</author><pubDate>Tue, 14 Jan 2025 16:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10733v4</guid></item><item><title>Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination</title><link>http://arxiv.org/abs/2409.12746v2</link><description>In this article we present UNED-ACCESS 2024, a bilingual dataset thatconsists of 1003 multiple-choice questions of university entrance level examsin Spanish and English. Questions are originally formulated in Spanish andtranslated manually into English, and have not ever been publicly released. Aselection of current open-source and proprietary models are evaluated in auniform zero-shot experimental setting both on the UNED-ACCESS 2024 dataset andon an equivalent subset of MMLU questions. Results show that (i) reasoningquestions are challenging for models, (ii) smaller models perform worse thanlarger models and degrade faster in Spanish than in English and (iii) theperformance gap between languages is negligible for the best models and growsup to 37% for smaller models. Model ranking on UNED-ACCESS 2024 is almostidentical in English and Spanish, and has also a high correlation (0.98Pearson) with ranking on MMLU, suggesting that a small dataset is sufficientlydiverse and representative to measure performance by discipline.</description><author>Eva S√°nchez Salido, Roser Morante, Julio Gonzalo, Guillermo Marco, Jorge Carrillo-de-Albornoz, Laura Plaza, Enrique Amig√≥, Andr√©s Fern√°ndez, Alejandro Benito-Santos, Adri√°n Ghajari Espinosa, Victor Fresno</author><pubDate>Tue, 14 Jan 2025 16:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12746v2</guid></item><item><title>Scaling White-Box Transformers for Vision</title><link>http://arxiv.org/abs/2405.20299v4</link><description>CRATE, a white-box transformer architecture designed to learn compressed andsparse representations, offers an intriguing alternative to standard visiontransformers (ViTs) due to its inherent mathematical interpretability. Despiteextensive investigations into the scaling behaviors of language and visiontransformers, the scalability of CRATE remains an open question which thispaper aims to address. Specifically, we propose CRATE-$\alpha$, featuringstrategic yet minimal modifications to the sparse coding block in the CRATEarchitecture design, and a light training recipe designed to improve thescalability of CRATE. Through extensive experiments, we demonstrate thatCRATE-$\alpha$ can effectively scale with larger model sizes and datasets. Forexample, our CRATE-$\alpha$-B substantially outperforms the prior best CRATE-Bmodel accuracy on ImageNet classification by 3.7%, achieving an accuracy of83.2%. Meanwhile, when scaling further, our CRATE-$\alpha$-L obtains anImageNet classification accuracy of 85.1%. More notably, these modelperformance improvements are achieved while preserving, and potentially evenenhancing the interpretability of learned CRATE models, as we demonstratethrough showing that the learned token representations of increasingly largertrained CRATE-$\alpha$ models yield increasingly higher-quality unsupervisedobject segmentation of images. The project page ishttps://rayjryang.github.io/CRATE-alpha/.</description><author>Jinrui Yang, Xianhang Li, Druv Pai, Yuyin Zhou, Yi Ma, Yaodong Yu, Cihang Xie</author><pubDate>Tue, 14 Jan 2025 16:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20299v4</guid></item><item><title>Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models</title><link>http://arxiv.org/abs/2501.08248v1</link><description>Recent advancements in long-context language models (LCLMs) promise totransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. Withtheir expanded context windows, LCLMs can process entire knowledge bases andperform retrieval and reasoning directly -- a capability we define asIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks likeLOFT often overestimate LCLM performance by providing overly simplifiedcontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMsin more realistic scenarios by including confounding passages retrieved withstrong retrievers. We then propose three methods to enhance LCLM performance:(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, whichuses attention heads to filter and de-noise long contexts during decoding, and(3) joint retrieval head training alongside the generation head. Our evaluationof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains withour best approach applied to Mistral-7B: +17 and +15 points by Exact Match onLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervisedfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasksdespite being a much smaller model.</description><author>Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han</author><pubDate>Tue, 14 Jan 2025 16:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08248v1</guid></item><item><title>Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful Behaviors with Proximity Constraints</title><link>http://arxiv.org/abs/2501.08246v1</link><description>Recent work has proposed automated red-teaming methods for testing thevulnerabilities of a given target large language model (LLM). These methods usered-teaming LLMs to uncover inputs that induce harmful behavior in a targetLLM. In this paper, we study red-teaming strategies that enable a targetedsecurity assessment. We propose an optimization framework for red-teaming withproximity constraints, where the discovered prompts must be similar toreference prompts from a given dataset. This dataset serves as a template forthe discovered prompts, anchoring the search for test-cases to specific topics,writing styles, or types of harmful behavior. We show that establishedauto-regressive model architectures do not perform well in this setting. Wetherefore introduce a black-box red-teaming method inspired by text-diffusionmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies thereference prompt by perturbing it in the embedding space, directly controllingthe amount of change introduced. We systematically evaluate our method bycomparing its effectiveness with established methods based on model fine-tuningand zero- and few-shot prompting. Our results show that DART is significantlymore effective at discovering harmful inputs in close proximity to thereference prompt.</description><author>Jonathan N√∂ther, Adish Singla, Goran Radanoviƒá</author><pubDate>Tue, 14 Jan 2025 16:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08246v1</guid></item><item><title>Continual Deep Active Learning for Medical Imaging: Replay-Base Architecture for Context Adaptation</title><link>http://arxiv.org/abs/2501.08245v1</link><description>Deep Learning for medical imaging faces challenges in adapting andgeneralizing to new contexts. Additionally, it often lacks sufficient labeleddata for specific tasks requiring significant annotation effort. ContinualLearning (CL) tackles adaptability and generalizability by enabling lifelonglearning from a data stream while mitigating forgetting of previously learnedknowledge. Active Learning (AL) reduces the number of required annotations foreffective training. This work explores both approaches (CAL) to develop a novelframework for robust medical image analysis. Based on the automatic recognitionof shifts in image characteristics, Replay-Base Architecture for ContextAdaptation (RBACA) employs a CL rehearsal method to continually learn fromdiverse contexts, and an AL component to select the most informative instancesfor annotation. A novel approach to evaluate CAL methods is established using adefined metric denominated IL-Score, which allows for the simultaneousassessment of transfer learning, forgetting, and final model performance. Weshow that RBACA works in domain and class-incremental learning scenarios, byassessing its IL-Score on the segmentation and diagnosis of cardiac images. Theresults show that RBACA outperforms a baseline framework without CAL, and astate-of-the-art CAL method across various memory sizes and annotation budgets.Our code is available in https://github.com/RuiDaniel/RBACA .</description><author>Rui Daniel, M. Rita Verdelho, Catarina Barata, Carlos Santiago</author><pubDate>Tue, 14 Jan 2025 16:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08245v1</guid></item><item><title>Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps</title><link>http://arxiv.org/abs/2501.08243v1</link><description>Cloud Operations (CloudOps) is a rapidly growing field focused on theautomated management and optimization of cloud infrastructure which isessential for organizations navigating increasingly complex cloud environments.MontyCloud Inc. is one of the major companies in the CloudOps domain thatleverages autonomous bots to manage cloud compliance, security, and continuousoperations. To make the platform more accessible and effective to thecustomers, we leveraged the use of GenAI. Developing a GenAI-based solution for autonomous CloudOps for the existingMontyCloud system presented us with various challenges such as i) diverse datasources; ii) orchestration of multiple processes; and iii) handling complexworkflows to automate routine tasks. To this end, we developed MOYA, amulti-agent framework that leverages GenAI and balances autonomy with thenecessary human control. This framework integrates various internal andexternal systems and is optimized for factors like task orchestration,security, and error mitigation while producing accurate, reliable, and relevantinsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of ourmulti-agent system with the help of practitioners as well as using automatedchecks demonstrate enhanced accuracy, responsiveness, and effectiveness overnon-agentic approaches across complex workflows.</description><author>Kannan Parthasarathy, Karthik Vaidhyanathan, Rudra Dhar, Venkat Krishnamachari, Basil Muhammed, Adyansh Kakran, Sreemaee Akshathala, Shrikara Arun, Sumant Dubey, Mohan Veerubhotla, Amey Karan</author><pubDate>Tue, 14 Jan 2025 16:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08243v1</guid></item><item><title>A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images using Choquet Integral and Differential Evolution Optimization</title><link>http://arxiv.org/abs/2501.08241v1</link><description>The COVID-19 pandemic has profoundly impacted billions globally. Itchallenges public health and healthcare systems due to its rapid spread andsevere respiratory effects. An effective strategy to mitigate the COVID-19pandemic involves integrating testing to identify infected individuals. WhileRT-PCR is considered the gold standard for diagnosing COVID-19, it has somelimitations such as the risk of false negatives. To address this problem, thispaper introduces a novel Deep Learning Diagnosis System that integratespre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemblelearning framework to achieve precise identification of COVID-19 cases fromChest X-ray (CXR) images. We combine feature vectors from the final hiddenlayers of pre-trained DCNNs using the Choquet integral to capture interactionsbetween different DCNNs that a linear approach cannot. We employedSugeno-$\lambda$ measure theory to derive fuzzy measures for subsets ofnetworks to enable aggregation. We utilized Differential Evolution to estimatefuzzy densities. We developed a TensorFlow-based layer for Choquet operation tofacilitate efficient aggregation, due to the intricacies involved inaggregating feature vectors. Experimental results on the COVIDx dataset showthat our ensemble model achieved 98\% accuracy in three-class classificationand 99.50\% in binary classification, outperforming its components-DenseNet-201(97\% for three-class, 98.75\% for binary), Inception-v3 (96.25\% forthree-class, 98.50\% for binary), and Xception (94.50\% for three-class, 98\%for binary)-and surpassing many previous methods.</description><author>Amir Reza Takhsha, Maryam Rastgarpour, Mozhgan Naderi</author><pubDate>Tue, 14 Jan 2025 16:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08241v1</guid></item><item><title>Privacy-Preserving Model and Preprocessing Verification for Machine Learning</title><link>http://arxiv.org/abs/2501.08236v1</link><description>This paper presents a framework for privacy-preserving verification ofmachine learning models, focusing on models trained on sensitive data.Integrating Local Differential Privacy (LDP) with model explanations from LIMEand SHAP, our framework enables robust verification without compromisingindividual privacy. It addresses two key tasks: binary classification, toverify if a target model was trained correctly by applying the appropriatepreprocessing steps, and multi-class classification, to identify specificpreprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,and Student Record-demonstrate that while the ML-based approach is particularlyeffective in binary tasks, the threshold-based method performs comparably inmulti-class tasks. Results indicate that although verification accuracy variesacross datasets and noise levels, the framework provides effective detection ofpreprocessing errors, strong privacy guarantees, and practical applicabilityfor safeguarding sensitive data.</description><author>Wenbiao Li, Anisa Halimi, Xiaoqian Jiang, Jaideep Vaidya, Erman Ayday</author><pubDate>Tue, 14 Jan 2025 16:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08236v1</guid></item><item><title>Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2501.08234v1</link><description>This paper addresses a critical challenge in the high-speed passenger railwayindustry: designing effective dynamic pricing strategies in the context ofcompeting and cooperating operators. To address this, a multi-agentreinforcement learning (MARL) framework based on a non-zero-sum Markov game isproposed, incorporating random utility models to capture passenger decisionmaking. Unlike prior studies in areas such as energy, airlines, and mobilenetworks, dynamic pricing for railway systems using deep reinforcement learninghas received limited attention. A key contribution of this paper is aparametrisable and versatile reinforcement learning simulator designed to modela variety of railway network configurations and demand patterns while enablingrealistic, microscopic modelling of user behaviour, called RailPricing-RL. Thisenvironment supports the proposed MARL framework, which models heterogeneousagents competing to maximise individual profits while fostering cooperativebehaviour to synchronise connecting services. Experimental results validate theframework, demonstrating how user preferences affect MARL performance and howpricing policies influence passenger choices, utility, and overall systemdynamics. This study provides a foundation for advancing dynamic pricingstrategies in railway systems, aligning profitability with system-wideefficiency, and supporting future research on optimising pricing policies.</description><author>Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David Mu√±oz-Valero, Giovanni Montana, Luis Jimenez-Linares</author><pubDate>Tue, 14 Jan 2025 16:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08234v1</guid></item><item><title>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</title><link>http://arxiv.org/abs/2405.14831v3</link><description>In order to thrive in hostile and ever-changing natural environments,mammalian brains evolved to store large amounts of knowledge about the worldand continually integrate new information while avoiding catastrophicforgetting. Despite the impressive accomplishments, large language models(LLMs), even with retrieval-augmented generation (RAG), still struggle toefficiently and effectively integrate a large amount of new experiences afterpre-training. In this work, we introduce HippoRAG, a novel retrieval frameworkinspired by the hippocampal indexing theory of human long-term memory to enabledeeper and more efficient knowledge integration over new experiences. HippoRAGsynergistically orchestrates LLMs, knowledge graphs, and the PersonalizedPageRank algorithm to mimic the different roles of neocortex and hippocampus inhuman memory. We compare HippoRAG with existing RAG methods on multi-hopquestion answering and show that our method outperforms the state-of-the-artmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achievescomparable or better performance than iterative retrieval like IRCoT whilebeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG intoIRCoT brings further substantial gains. Finally, we show that our method cantackle new types of scenarios that are out of reach of existing methods. Codeand data are available at https://github.com/OSU-NLP-Group/HippoRAG.</description><author>Bernal Jim√©nez Guti√©rrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su</author><pubDate>Tue, 14 Jan 2025 16:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14831v3</guid></item><item><title>A Comprehensive Survey of Foundation Models in Medicine</title><link>http://arxiv.org/abs/2406.10729v2</link><description>Foundation models (FMs) are large-scale deep learning models that aredeveloped using large datasets and self-supervised learning methods. Thesemodels serve as a base for different downstream tasks, including healthcare.FMs have been adopted with great success across various domains withinhealthcare. Existing healthcare-based surveys have not yet included all ofthese domains. Therefore, we provide a detailed survey of FMs in healthcare. Wefocus on the history, learning strategies, flagship models, applications, andchallenges of FMs. We explore how FMs such as the BERT and GPT families arereshaping various healthcare domains, including clinical large language models,medical image analysis, and omics. Furthermore, we provide a detailed taxonomyof healthcare applications facilitated by FMs, such as clinical NLP, medicalcomputer vision, graph learning, and other biology-related tasks. Despite thepromising opportunities FMs provide, they also have several associatedchallenges, which are explained in detail. We also outline open research issuesand potential lessons learned to provide researchers and practitioners withinsights into the capabilities of FMs in healthcare to advance their deploymentand mitigate associated risks.</description><author>Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang</author><pubDate>Tue, 14 Jan 2025 16:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10729v2</guid></item><item><title>Text-guided Image Restoration and Semantic Enhancement for Text-to-Image Person Retrieval</title><link>http://arxiv.org/abs/2307.09059v3</link><description>The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specificperson images according to the given textual descriptions. A primary challengein this task is bridging the substantial representational gap between visualand textual modalities. The prevailing methods map texts and images intounified embedding space for matching, while the intricate semanticcorrespondences between texts and images are still not effectively constructed.To address this issue, we propose a novel TIPR framework to build fine-grainedinteractions and alignment between person images and the corresponding texts.Specifically, via fine-tuning the Contrastive Language-Image Pre-training(CLIP) model, a visual-textual dual encoder is firstly constructed, topreliminarily align the image and text features. Secondly, a Text-guided ImageRestoration (TIR) auxiliary task is proposed to map abstract textual entitiesto specific image regions, improving the alignment between local textual andvisual embeddings. Additionally, a cross-modal triplet loss is presented tohandle hard samples, and further enhance the model's discriminability for minordifferences. Moreover, a pruning-based text data augmentation approach isproposed to enhance focus on essential elements in descriptions, therebyavoiding excessive model attention to less significant information. Theexperimental results show our proposed method outperforms state-of-the-artmethods on three popular benchmark datasets, and the code will be made publiclyavailable at https://github.com/Delong-liu-bupt/SEN.</description><author>Delong Liu, Haiwen Li, Zhicheng Zhao, Yuan Dong, Nikolaos V. Boulgouris</author><pubDate>Tue, 14 Jan 2025 16:11:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09059v3</guid></item><item><title>Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth Models</title><link>http://arxiv.org/abs/2501.08226v1</link><description>Glioblastoma, a highly aggressive brain tumor, poses major challenges due toits poor prognosis and high morbidity rates. Partial differentialequation-based models offer promising potential to enhance therapeutic outcomesby simulating patient-specific tumor behavior for improved radiotherapyplanning. However, model calibration remains a bottleneck due to the highcomputational demands of optimization methods like Monte Carlo sampling andevolutionary algorithms. To address this, we recently introduced an approachleveraging a neural forward solver with gradient-based optimization tosignificantly reduce calibration time. This approach requires a highly accurateand fully differentiable forward model. We investigate multiple architectures,including (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the bestoverall results, excelling in both tumor outline matching and voxel-levelprediction of tumor cell concentration. It halved the MSE relative to thebaseline model and achieved the highest Dice score across all tumor cellconcentration thresholds. Our study demonstrates significant enhancement inforward solver performance and outlines important future research directions.</description><author>Zeineb Haouari, Jonas Weidner, Ivan Ezhov, Aswathi Varma, Daniel Rueckert, Bjoern Menze, Benedikt Wiestler</author><pubDate>Tue, 14 Jan 2025 16:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08226v1</guid></item><item><title>FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors</title><link>http://arxiv.org/abs/2501.08225v1</link><description>Interactive image editing allows users to modify images through visualinteraction operations such as drawing, clicking, and dragging. Existingmethods construct such supervision signals from videos, as they capture howobjects change with various physical interactions. However, these models areusually built upon text-to-image diffusion models, so necessitate (i) massivetraining samples and (ii) an additional reference encoder to learn real-worlddynamics and visual consistency. In this paper, we reformulate this task as animage-to-video generation problem, so that inherit powerful video diffusionpriors to reduce training costs and ensure temporal consistency. Specifically,we introduce FramePainter as an efficient instantiation of this formulation.Initialized with Stable Video Diffusion, it only uses a lightweight sparsecontrol encoder to inject editing signals. Considering the limitations oftemporal attention in handling large motion between two frames, we furtherpropose matching attention to enlarge the receptive field while encouragingdense correspondence between edited and source image tokens. We highlight theeffectiveness and efficiency of FramePainter across various of editing signals:it domainantly outperforms previous state-of-the-art methods with far lesstraining data, achieving highly seamless and coherent editing of images, \eg,automatically adjust the reflection of the cup. Moreover, FramePainter alsoexhibits exceptional generalization in scenarios not present in real-worldvideos, \eg, transform the clownfish into shark-like shape. Our code will beavailable at https://github.com/YBYBZhang/FramePainter.</description><author>Yabo Zhang, Xinpeng Zhou, Yihan Zeng, Hang Xu, Hui Li, Wangmeng Zuo</author><pubDate>Tue, 14 Jan 2025 16:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08225v1</guid></item><item><title>Pareto Set Learning for Multi-Objective Reinforcement Learning</title><link>http://arxiv.org/abs/2501.06773v2</link><description>Multi-objective decision-making problems have emerged in numerous real-worldscenarios, such as video games, navigation and robotics. Considering the clearadvantages of Reinforcement Learning (RL) in optimizing decision-makingprocesses, researchers have delved into the development of Multi-Objective RL(MORL) methods for solving multi-objective decision problems. However, previousmethods either cannot obtain the entire Pareto front, or employ only a singlepolicy network for all the preferences over multiple objectives, which may notproduce personalized solutions for each preference. To address theselimitations, we propose a novel decomposition-based framework for MORL, ParetoSet Learning for MORL (PSL-MORL), that harnesses the generation capability ofhypernetwork to produce the parameters of the policy network for eachdecomposition weight, generating relatively distinct policies for variousscalarized subproblems with high efficiency. PSL-MORL is a general framework,which is compatible for any RL algorithm. The theoretical result guarantees thesuperiority of the model capacity of PSL-MORL and the optimality of theobtained policy network. Through extensive experiments on diverse benchmarks,we demonstrate the effectiveness of PSL-MORL in achieving dense coverage of thePareto front, significantly outperforming state-of-the-art MORL methods in thehypervolume and sparsity indicators.</description><author>Erlong Liu, Yu-Chang Wu, Xiaobin Huang, Chengrui Gao, Ren-Jian Wang, Ke Xue, Chao Qian</author><pubDate>Tue, 14 Jan 2025 16:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06773v2</guid></item><item><title>Big Batch Bayesian Active Learning by Considering Predictive Probabilities</title><link>http://arxiv.org/abs/2501.08223v1</link><description>We observe that BatchBALD, a popular acquisition function for batch Bayesianactive learning for classification, can conflate epistemic and aleatoricuncertainty, leading to suboptimal performance. Motivated by this observation,we propose to focus on the predictive probabilities, which only exhibitepistemic uncertainty. The result is an acquisition function that not onlyperforms better, but is also faster to evaluate, allowing for larger batchesthan before.</description><author>Sebastian W. Ober, Samuel Power, Tom Diethe, Henry B. Moss</author><pubDate>Tue, 14 Jan 2025 16:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08223v1</guid></item><item><title>Optimization of Link Configuration for Satellite Communication Using Reinforcement Learning</title><link>http://arxiv.org/abs/2501.08220v1</link><description>Satellite communication is a key technology in our modern connected world.With increasingly complex hardware, one challenge is to efficiently configurelinks (connections) on a satellite transponder. Planning an optimal linkconfiguration is extremely complex and depends on many parameters and metrics.The optimal use of the limited resources, bandwidth and power of thetransponder is crucial. Such an optimization problem can be approximated usingmetaheuristic methods such as simulated annealing, but recent research resultsalso show that reinforcement learning can achieve comparable or even betterperformance in optimization methods. However, there have not yet been anystudies on link configuration on satellite transponders. In order to close thisresearch gap, a transponder environment was developed as part of this work. Forthis environment, the performance of the reinforcement learning algorithm PPOwas compared with the metaheuristic simulated annealing in two experiments. Theresults show that Simulated Annealing delivers better results for this staticproblem than the PPO algorithm, however, the research in turn also underlinesthe potential of reinforcement learning for optimization problems.</description><author>Tobias Rohe, Michael K√∂lle, Jan Matheis, R√ºdiger H√∂pfl, Leo S√ºnkel, Claudia Linnhoff-Popien</author><pubDate>Tue, 14 Jan 2025 16:04:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08220v1</guid></item><item><title>Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings</title><link>http://arxiv.org/abs/2501.08219v1</link><description>Large language models (LLMs) have shown significant improvements in manynatural language processing (NLP) tasks, accelerating their rapid adoptionacross many industries. These models are resource-intensive, requiringextensive computational resources both during training and inference, leadingto increased energy consumption and negative environmental impact. As theiradoption accelerates, the sustainability of LLMs has become a critical issue,necessitating strategies to optimize their runtime efficiency withoutcompromising performance. Hence, it is imperative to identify the parametersthat significantly influence the performance and energy efficiency of LLMs. Tothat end, in this work, we investigate the effect of important parameters onthe performance and energy efficiency of LLMs during inference and examinetheir trade-offs. First, we analyze how different types of models with varying numbers ofparameters and architectures perform on tasks like text generation, questionanswering, and summarization by benchmarking LLMs such as Falcon-7B,Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we studyinput and output sequence characteristics such as sequence length concerningenergy consumption, performance, and throughput. Finally, we explore the impactof hardware-based power-saving techniques, i.e., Dynamic Voltage FrequencyScaling (DVFS), on the models' latency and energy efficiency. Our extensivebenchmarking and statistical analysis reveal many interesting findings,uncovering how specific optimizations can reduce energy consumption whilemaintaining throughput and accuracy. This study provides actionable insightsfor researchers and practitioners to design energy-efficient LLM inferencesystems.</description><author>Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic</author><pubDate>Tue, 14 Jan 2025 16:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08219v1</guid></item><item><title>Logic Augmented Generation</title><link>http://arxiv.org/abs/2411.14012v2</link><description>Semantic Knowledge Graphs (SKG) face challenges with scalability,flexibility, contextual understanding, and handling unstructured or ambiguousinformation. However, they offer formal and structured knowledge enablinghighly interpretable and reliable results by means of reasoning and querying.Large Language Models (LLMs) overcome those limitations making them suitable inopen-ended tasks and unstructured environments. Nevertheless, LLMs are neitherinterpretable nor reliable. To solve the dichotomy between LLMs and SKGs weenvision Logic Augmented Generation (LAG) that combines the benefits of the twoworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generatepotentially infinite relations and tacit knowledge on-demand. SKGs are key forinjecting a discrete heuristic dimension with clear logical and factualboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,medical diagnostics and climate projections. Understanding the properties andlimitations of LAG, which are still mostly unknown, is of utmost importance forenabling a variety of tasks involving tacit knowledge in order to provideinterpretable and effective results.</description><author>Aldo Gangemi, Andrea Giovanni Nuzzolese</author><pubDate>Tue, 14 Jan 2025 15:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14012v2</guid></item><item><title>ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems</title><link>http://arxiv.org/abs/2501.08208v1</link><description>Large Language Models (LLMs) have shown impressive potential in clinicalquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging asa leading approach for ensuring the factual accuracy of model responses.However, current automated RAG metrics perform poorly in clinical andconversational use cases. Using clinical human evaluations of responses isexpensive, unscalable, and not conducive to the continuous iterativedevelopment of RAG systems. To address these challenges, we introduce ASTRID -an Automated and Scalable TRIaD for evaluating clinical QA systems leveragingRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, isdesigned to better capture the faithfulness of a model's response to theknowledge base without penalising conversational elements. To validate ourtriad, we curate a dataset of over 200 real-world patient questions posed to anLLM-based QA agent during surgical follow-up for cataract surgery - the highestvolume operation in the world - augmented with clinician-selected questions foremergency, clinical, and non-clinical out-of-domain scenarios. We demonstratethat CF can predict human ratings of faithfulness better than existingdefinitions for conversational use cases. Furthermore, we show that evaluationusing our triad consisting of CF, RA, and CR exhibits alignment with clinicianassessment for inappropriate, harmful, or unhelpful responses. Finally, usingnine different LLMs, we demonstrate that the three metrics can closely agreewith human evaluations, highlighting the potential of these metrics for use inLLM-driven automated evaluation pipelines. We also publish the prompts anddatasets for these experiments, providing valuable resources for furtherresearch and development.</description><author>Mohita Chowdhury, Yajie Vera He, Aisling Higham, Ernest Lim</author><pubDate>Tue, 14 Jan 2025 15:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08208v1</guid></item><item><title>Modeling Feature Maps for Quantum Machine Learning</title><link>http://arxiv.org/abs/2501.08205v1</link><description>Quantum Machine Learning (QML) offers significant potential for complex taskslike genome sequence classification, but quantum noise on NoisyIntermediate-Scale Quantum (NISQ) devices poses practical challenges. Thisstudy systematically evaluates how various quantum noise models includingdephasing, amplitude damping, depolarizing, thermal noise, bit-flip, andphase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and featuremapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Resultsindicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN aremore sensitive, particularly to depolarizing and amplitude-damping noise. ThePauliFeatureMap is especially vulnerable, highlighting difficulties inmaintaining accurate classification under noisy conditions. These findingsunderscore the critical importance of feature map selection and noisemitigation strategies in optimizing QML for genomic classification, withpromising implications for personalized medicine.</description><author>Navneet Singh, Shiva Raj Pokhrel</author><pubDate>Tue, 14 Jan 2025 15:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08205v1</guid></item><item><title>Sharp Matrix Empirical Bernstein Inequalities</title><link>http://arxiv.org/abs/2411.09516v3</link><description>We present two sharp empirical Bernstein inequalities for symmetric randommatrices with bounded eigenvalues. By sharp, we mean that both inequalitiesadapt to the unknown variance in a tight manner: the deviation captured by thefirst-order $1/\sqrt{n}$ term asymptotically matches the matrix Bernsteininequality exactly, including constants, the latter requiring knowledge of thevariance. Our first inequality holds for the sample mean of independentmatrices, and our second inequality holds for a mean estimator under martingaledependence at stopping times.</description><author>Hongjian Wang, Aaditya Ramdas</author><pubDate>Tue, 14 Jan 2025 15:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09516v3</guid></item><item><title>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</title><link>http://arxiv.org/abs/2501.08203v1</link><description>While Large Language Models (LLMs) have shown impressive capabilities in mathproblem-solving tasks, their robustness to noisy inputs is not well-studied. Inthis work, we propose ArithmAttack to examine how robust the LLMs are when theyencounter noisy prompts that contain extra noise in the form of punctuationmarks. While being easy to implement, ArithmAttack does not cause anyinformation loss since words are not added or deleted from the context. Weevaluate the robustness of seven LLMs, including LLama3, Mistral, andMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest thatall the studied models show vulnerability to such noise, with more noiseleading to poorer performances.</description><author>Zain Ul Abedin, Shahzeb Qamar, Lucie Flek, Akbar Karimi</author><pubDate>Tue, 14 Jan 2025 15:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08203v1</guid></item><item><title>Data-driven system identification using quadratic embeddings of nonlinear dynamics</title><link>http://arxiv.org/abs/2501.08202v1</link><description>We propose a novel data-driven method called QENDy (Quadratic Embedding ofNonlinear Dynamics) that not only allows us to learn quadratic representationsof highly nonlinear dynamical systems, but also to identify the governingequations. The approach is based on an embedding of the system into ahigher-dimensional feature space in which the dynamics become quadratic. Justlike SINDy (Sparse Identification of Nonlinear Dynamics), our method requirestrajectory data, time derivatives for the training data points, which can alsobe estimated using finite difference approximations, and a set of preselectedbasis functions, called dictionary. We illustrate the efficacy and accuracy ofQENDy with the aid of various benchmark problems and compare its performancewith SINDy and a deep learning method for identifying quadratic embeddings.Furthermore, we analyze the convergence of QENDy and SINDy in the infinite datalimit, highlight their similarities and main differences, and compare thequadratic embedding with linearization techniques based on the Koopmanoperator.</description><author>Stefan Klus, Joel-Pascal N'Konzi</author><pubDate>Tue, 14 Jan 2025 15:37:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08202v1</guid></item><item><title>Globally Convergent Variational Inference</title><link>http://arxiv.org/abs/2501.08201v1</link><description>In variational inference (VI), an approximation of the posterior distributionis selected from a family of distributions through numerical optimization. Withthe most common variational objective function, known as the evidence lowerbound (ELBO), only convergence to a local optimum can be guaranteed. In thiswork, we instead establish the global convergence of a particular VI method.This VI method, which may be considered an instance of neural posteriorestimation (NPE), minimizes an expectation of the inclusive (forward) KLdivergence to fit a variational distribution that is parameterized by a neuralnetwork. Our convergence result relies on the neural tangent kernel (NTK) tocharacterize the gradient dynamics that arise from considering the variationalobjective in function space. In the asymptotic regime of a fixed,positive-definite neural tangent kernel, we establish conditions under whichthe variational objective admits a unique solution in a reproducing kernelHilbert space (RKHS). Then, we show that the gradient descent dynamics infunction space converge to this unique function. In ablation studies andpractical problems, we demonstrate that our results explain the behavior of NPEin non-asymptotic finite-neuron settings, and show that NPE outperformsELBO-based optimization, which often converges to shallow local optima.</description><author>Declan McNamara, Jackson Loper, Jeffrey Regier</author><pubDate>Tue, 14 Jan 2025 15:36:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08201v1</guid></item><item><title>Relaxed Rotational Equivariance via $G$-Biases in Vision</title><link>http://arxiv.org/abs/2408.12454v3</link><description>Group Equivariant Convolution (GConv) can capture rotational equivariancefrom original data. It assumes uniform and strict rotational equivarianceacross all features as the transformations under the specific group. However,the presentation or distribution of real-world data rarely conforms to strictrotational equivariance, commonly referred to as Rotational Symmetry-Breaking(RSB) in the system or dataset, making GConv unable to adapt effectively tothis phenomenon. Motivated by this, we propose a simple but highly effectivemethod to address this problem, which utilizes a set of learnable biases called$G$-Biases under the group order to break strict group constraints and thenachieve a Relaxed Rotational Equivariant Convolution (RREConv). To validate theefficiency of RREConv, we conduct extensive ablation experiments on thediscrete rotational group $\mathcal{C}_n$. Experiments demonstrate that theproposed RREConv-based methods achieve excellent performance compared toexisting GConv-based methods in both classification and 2D object detectiontasks on the natural image datasets.</description><author>Zhiqiang Wu, Yingjie Liu, Licheng Sun, Jian Yang, Hanlin Dong, Shing-Ho J. Lin, Xuan Tang, Jinpeng Mi, Bo Jin, Xian Wei</author><pubDate>Tue, 14 Jan 2025 15:35:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12454v3</guid></item><item><title>Personalized LLM Response Generation with Parameterized Memory Injection</title><link>http://arxiv.org/abs/2404.03565v3</link><description>Large Language Models (LLMs) have exhibited remarkable proficiency incomprehending and generating natural language. On the other hand, personalizedLLM response generation holds the potential to offer substantial benefits forindividuals in critical areas such as medical. Existing research has exploredmemory-augmented methods to prompt the LLM with pre-stored user-specificknowledge for personalized response generation in terms of new queries. Wecontend that such paradigm is unable to perceive fine-granularity information.In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approachusing parameter-efficient fine-tuning (PEFT) and along with a BayesianOptimisation searching strategy to achieve \textbf{L}LM\textbf{P}ersonalization(\textbf{MiLP}).</description><author>Kai Zhang, Yejin Kim, Xiaozhong Liu</author><pubDate>Tue, 14 Jan 2025 15:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03565v3</guid></item><item><title>CWEval: Outcome-driven Evaluation on Functionality and Security of LLM Code Generation</title><link>http://arxiv.org/abs/2501.08200v1</link><description>Large Language Models (LLMs) have significantly aided developers bygenerating or assisting in code writing, enhancing productivity across varioustasks. While identifying incorrect code is often straightforward, detectingvulnerabilities in functionally correct code is more challenging, especiallyfor developers with limited security knowledge, which poses considerablesecurity risks of using LLM-generated code and underscores the need for robustevaluation benchmarks that assess both functional correctness and security.Current benchmarks like CyberSecEval and SecurityEval attempt to solve it butare hindered by unclear and impractical specifications, failing to assess bothfunctionality and security accurately. To tackle these deficiencies, weintroduce CWEval, a novel outcome-driven evaluation framework designed toenhance the evaluation of secure code generation by LLMs. This framework notonly assesses code functionality but also its security simultaneously withhigh-quality task specifications and outcome-driven test oracles which provideshigh accuracy. Coupled with CWEval-bench, a multilingual, security-criticalcoding benchmark, CWEval provides a rigorous empirical security evaluation onLLM-generated code, overcoming previous benchmarks' shortcomings. Through ourevaluations, CWEval reveals a notable portion of functional but insecure codeproduced by LLMs, and shows a serious inaccuracy of previous evaluations,ultimately contributing significantly to the field of secure code generation.We open-source our artifact at: https://github.com/Co1lin/CWEval .</description><author>Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, Baishakhi Ray</author><pubDate>Tue, 14 Jan 2025 15:27:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08200v1</guid></item><item><title>EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition</title><link>http://arxiv.org/abs/2501.08199v1</link><description>Facial expressions play a crucial role in human communication serving as apowerful and impactful means to express a wide range of emotions. Withadvancements in artificial intelligence and computer vision, deep neuralnetworks have emerged as effective tools for facial emotion recognition. Inthis paper, we propose EmoNeXt, a novel deep learning framework for facialexpression recognition based on an adapted ConvNeXt architecture network. Weintegrate a Spatial Transformer Network (STN) to focus on feature-rich regionsof the face and Squeeze-and-Excitation blocks to capture channel-wisedependencies. Moreover, we introduce a self-attention regularization term,encouraging the model to generate compact feature vectors. We demonstrate thesuperiority of our model over existing state-of-the-art deep learning models onthe FER2013 dataset regarding emotion classification accuracy.</description><author>Yassine El Boudouri, Amine Bohi</author><pubDate>Tue, 14 Jan 2025 15:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08199v1</guid></item><item><title>OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training</title><link>http://arxiv.org/abs/2501.08197v1</link><description>Large language models (LLMs) have demonstrated remarkable capabilities, buttheir success heavily relies on the quality of pretraining corpora. For ChineseLLMs, the scarcity of high-quality Chinese datasets presents a significantchallenge, often limiting their performance. To address this issue, we proposethe OpenCSG Chinese Corpus, a series of high-quality datasets specificallydesigned for LLM pretraining, post-training, and fine-tuning. This corpusincludes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, andSmoltalk-chinese, each with distinct characteristics: Fineweb-edu datasetsfocus on filtered, high-quality content derived from diverse Chinese websources; Cosmopedia-chinese provides synthetic, textbook-style data forknowledge-intensive training; and Smoltalk-chinese emphasizes stylistic anddiverse chat-format data. The OpenCSG Chinese Corpus is characterized by itshigh-quality text, diverse coverage across domains, and scalable, reproducibledata curation processes. Additionally, we conducted extensive experimentalanalyses, including evaluations on smaller parameter models, which demonstratedsignificant performance improvements in tasks such as C-Eval, showcasing theeffectiveness of the corpus for training Chinese LLMs.</description><author>Yijiong Yu, Ziyun Dai, Zekun Wang, Wei Wang, Ran Chen, Ji Pei</author><pubDate>Tue, 14 Jan 2025 15:22:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08197v1</guid></item><item><title>KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model</title><link>http://arxiv.org/abs/2501.01028v3</link><description>As retrieval-augmented generation prevails in large language models,embedding models are becoming increasingly crucial. Despite the growing numberof general embedding models, prior work often overlooks the critical role oftraining data quality. In this work, we introduce KaLM-Embedding, a generalmultilingual embedding model that leverages a large quantity of cleaner, morediverse, and domain-specific training data. Our model has been trained with keytechniques proven to enhance performance: (1) persona-based synthetic data tocreate diversified examples distilled from LLMs, (2) ranking consistencyfiltering to remove less informative samples, and (3) semi-homogeneous taskbatch sampling to improve training efficacy. Departing from traditionalBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,facilitating the adaptation of auto-regressive language models for generalembedding tasks. Extensive evaluations of the MTEB benchmark across multiplelanguages show that our model outperforms others of comparable size, setting anew standard for multilingual embedding models with &lt;1B parameters.</description><author>Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, Min Zhang</author><pubDate>Tue, 14 Jan 2025 15:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01028v3</guid></item><item><title>Self-supervised Deep Hyperspectral Inpainting with the Plug and Play and Deep Image Prior Models</title><link>http://arxiv.org/abs/2501.08195v1</link><description>Hyperspectral images are typically composed of hundreds of narrow andcontiguous spectral bands, each containing information regarding the materialcomposition of the imaged scene. However, these images can be affected byvarious sources of noise, distortions, or data loss, which can significantlydegrade their quality and usefulness. This paper introduces a convergentguaranteed algorithm, LRS-PnP-DIP(1-Lip), which successfully addresses theinstability issue of DHP that has been reported before. The proposed algorithmextends the successful joint low-rank and sparse model to further exploit theunderlying data structures beyond the conventional and sometimes restrictiveunions of subspace models. A stability analysis guarantees the convergence ofthe proposed algorithm under mild assumptions , which is crucial for itsapplication in real-world scenarios. Extensive experiments demonstrate that theproposed solution consistently delivers visually and quantitatively superiorinpainting results, establishing state-of-the-art performance.</description><author>Shuo Li, Mehrdad Yaghoobi</author><pubDate>Tue, 14 Jan 2025 15:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08195v1</guid></item><item><title>Modeling Quantum Machine Learning for Genomic Data Analysis</title><link>http://arxiv.org/abs/2501.08193v1</link><description>Quantum Machine Learning (QML) continues to evolve, unlocking newopportunities for diverse applications. In this study, we investigate andevaluate the applicability of QML models for binary classification of genomesequence data by employing various feature mapping techniques. We present anopen-source, independent Qiskit-based implementation to conduct experiments ona benchmark genomic dataset. Our simulations reveal that the interplay betweenfeature mapping techniques and QML algorithms significantly influencesperformance. Notably, the Pegasos Quantum Support Vector Classifier(Pegasos-QSVC) exhibits high sensitivity, particularly excelling in recallmetrics, while Quantum Neural Networks (QNN) achieve the highest trainingaccuracy across all feature maps. However, the pronounced variability inclassifier performance, dependent on feature mapping, highlights the risk ofoverfitting to localized output distributions in certain scenarios. This workunderscores the transformative potential of QML for genomic data classificationwhile emphasizing the need for continued advancements to enhance the robustnessand accuracy of these methodologies.</description><author>Navneet Singh, Shiva Raj Pokhrel</author><pubDate>Tue, 14 Jan 2025 15:14:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08193v1</guid></item></channel></rss>