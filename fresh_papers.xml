<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 24 Apr 2024 06:00:25 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</title><link>http://arxiv.org/abs/2404.15276v1</link><description>Existing Transformers for monocular 3D human shape and pose estimationtypically have a quadratic computation and memory complexity with respect tothe feature length, which hinders the exploitation of fine-grained informationin high-resolution features that is beneficial for accurate reconstruction. Inthis work, we propose an SMPL-based Transformer framework (SMPLer) to addressthis issue. SMPLer incorporates two key ingredients: a decoupled attentionoperation and an SMPL-based target representation, which allow effectiveutilization of high-resolution features in the Transformer. In addition, basedon these two designs, we also introduce several novel modules including amulti-scale attention and a joint-aware attention to further boost thereconstruction performance. Extensive experiments demonstrate the effectivenessof SMPLer against existing 3D human shape and pose estimation methods bothquantitatively and qualitatively. Notably, the proposed algorithm achieves anMPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer bymore than 10% with fewer than one-third of the parameters. Code and pretrainedmodels are available at https://github.com/xuxy09/SMPLer.</description><author>Xiangyu Xu, Lijuan Liu, Shuicheng Yan</author><pubDate>Tue, 23 Apr 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15276v1</guid></item><item><title>ID-Animator: Zero-Shot Identity-Preserving Human Video Generation</title><link>http://arxiv.org/abs/2404.15275v1</link><description>Generating high fidelity human video with specified identities has attractedsignificant attention in the content generation community. However, existingtechniques struggle to strike a balance between training efficiency andidentity preservation, either requiring tedious case-by-case finetuning orusually missing the identity details in video generation process. In thisstudy, we present ID-Animator, a zero-shot human-video generation approach thatcan perform personalized video generation given single reference facial imagewithout further training. ID-Animator inherits existing diffusion-based videogeneration backbones with a face adapter to encode the ID-relevant embeddingsfrom learnable facial latent queries. To facilitate the extraction of identityinformation in video generation, we introduce an ID-oriented datasetconstruction pipeline, which incorporates decoupled human attribute and actioncaptioning technique from a constructed facial image pool. Based on thispipeline, a random face reference training method is further devised toprecisely capture the ID-relevant embeddings from reference images, thusimproving the fidelity and generalization capacity of our model for ID-specificvideo generation. Extensive experiments demonstrate the superiority ofID-Animator to generate personalized human videos over previous models.Moreover, our method is highly compatible with popular pre-trained T2V modelslike animatediff and various community backbone models, showing highextendability in real-world applications for video generation where identitypreservation is highly desired. Our codes and checkpoints will be released athttps://github.com/ID-Animator/ID-Animator.</description><author>Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, Jie Zhang</author><pubDate>Tue, 23 Apr 2024 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15275v1</guid></item><item><title>Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance</title><link>http://arxiv.org/abs/2312.07530v2</link><description>Weakly supervised 3D object detection aims to learn a 3D detector with lowerannotation cost, e.g., 2D labels. Unlike prior work which still relies on fewaccurate 3D annotations, we propose a framework to study how to leverageconstraints between 2D and 3D domains without requiring any 3D labels.Specifically, we employ visual data from three perspectives to establishconnections between 2D and 3D domains. First, we design a feature-levelconstraint to align LiDAR and image features based on object-aware regions.Second, the output-level constraint is developed to enforce the overlap between2D and projected 3D box estimations. Finally, the training-level constraint isutilized by producing accurate and consistent 3D pseudo-labels that align withthe visual data. We conduct extensive experiments on the KITTI dataset tovalidate the effectiveness of the proposed three constraints. Without using any3D labels, our method achieves favorable performance against state-of-the-artapproaches and is competitive with the method that uses 500-frame 3Dannotations. Code and models will be made publicly available athttps://github.com/kuanchihhuang/VG-W3D.</description><author>Kuan-Chih Huang, Yi-Hsuan Tsai, Ming-Hsuan Yang</author><pubDate>Tue, 23 Apr 2024 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07530v2</guid></item><item><title>Metric-guided Image Reconstruction Bounds via Conformal Prediction</title><link>http://arxiv.org/abs/2404.15274v1</link><description>Recent advancements in machine learning have led to novel imaging systems andalgorithms that address ill-posed problems. Assessing their trustworthiness andunderstanding how to deploy them safely at test time remains an important andopen problem. We propose a method that leverages conformal prediction toretrieve upper/lower bounds and statistical inliers/outliers of reconstructionsbased on the prediction intervals of downstream metrics. We apply our method tosparse-view CT for downstream radiotherapy planning and show 1) thatmetric-guided bounds have valid coverage for downstream metrics whileconventional pixel-wise bounds do not and 2) anatomical differences ofupper/lower bounds between metric-guided and pixel-wise methods. Our work pavesthe way for more meaningful reconstruction bounds. Code available athttps://github.com/matthewyccheung/conformal-metric</description><author>Matt Y Cheung, Tucker J Netherton, Laurence E Court, Ashok Veeraraghavan, Guha Balakrishnan</author><pubDate>Tue, 23 Apr 2024 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15274v1</guid></item><item><title>Estimation Network Design framework for efficient distributed optimization</title><link>http://arxiv.org/abs/2404.15273v1</link><description>Distributed decision problems features a group of agents that can onlycommunicate over a peer-to-peer network, without a central memory. Inapplications such as network control and data ranking, each agent is onlyaffected by a small portion of the decision vector: this sparsity is typicallyignored in distributed algorithms, while it could be leveraged to improveefficiency and scalability. To address this issue, our recent paper introducesEstimation Network Design (END), a graph theoretical language for the analysisand design of distributed iterations. END algorithms can be tuned to exploitthe sparsity of specific problem instances, reducing communication overhead andminimizing redundancy, yet without requiring case-by-case convergence analysis.In this paper, we showcase the flexility of END in the context of distributedoptimization. In particular, we study the sparsity-aware version of manyestablished methods, including ADMM, AugDGM and Push-Sum DGD. Simulations on anestimation problem in sensor networks demonstrate that END algorithms can boostconvergence speed and greatly reduce the communication and memory cost.</description><author>Mattia Bianchi, Sergio Grammatico</author><pubDate>Tue, 23 Apr 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15273v1</guid></item><item><title>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios</title><link>http://arxiv.org/abs/2404.15272v1</link><description>Medical Vision-Language Pretraining (Med-VLP) establishes a connectionbetween visual content from medical images and the relevant textualdescriptions. Existing Med-VLP methods primarily focus on 2D images depicting asingle body part, notably chest X-rays. In this paper, we extend the scope ofMed-VLP to encompass 3D images, specifically targeting full-body scenarios, byusing a multimodal dataset of CT images and reports. Compared with the 2Dcounterpart, 3D VLP is required to effectively capture essential semantics fromsignificantly sparser representation in 3D imaging. In this paper, we introduceCT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel methodthat constructs organ-level image-text pairs to enhance multimodal contrastivelearning, aligning grounded visual features with precise diagnostic text.Additionally, we developed an abnormality dictionary to augment contrastivelearning with diverse negative samples. Our method, trained on a multimodal CTdataset comprising 44,011 organ-level vision-text pairs from 17,702 patientsacross 104 organs, demonstrates it can identify organs and abnormalities in azero-shot manner using natural languages. The performance of CT-GLIP isvalidated on a separate test set of 1,130 patients, focusing on the 16 mostfrequent abnormalities across 7 organs. The experimental results show ourmodel's superior performance over the standard CLIP framework across zero-shotand fine-tuning scenarios, using both CNN and ViT architectures.</description><author>Jingyang Lin, Yingda Xia, Jianpeng Zhang, Ke Yan, Le Lu, Jiebo Luo, Ling Zhang</author><pubDate>Tue, 23 Apr 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15272v1</guid></item><item><title>Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models</title><link>http://arxiv.org/abs/2404.15271v1</link><description>Recent advancements in instruction-following models have made userinteractions with models more user-friendly and efficient, broadening theirapplicability. In graphic design, non-professional users often struggle tocreate visually appealing layouts due to limited skills and resources. In thiswork, we introduce a novel multimodal instruction-following framework forlayout planning, allowing users to easily arrange visual elements into tailoredlayouts by specifying canvas size and design purpose, such as for book covers,posters, brochures, or menus. We developed three layout reasoning tasks totrain the model in understanding and executing layout instructions. Experimentson two benchmarks show that our method not only simplifies the design processfor non-professionals but also surpasses the performance of few-shot GPT-4Vmodels, with mIoU higher by 12% on Crello. This progress highlights thepotential of multimodal instruction-following models to automate and simplifythe design process, providing an approachable solution for a wide range ofdesign tasks on visually-rich documents.</description><author>Wanrong Zhu, Jennifer Healey, Ruiyi Zhang, William Yang Wang, Tong Sun</author><pubDate>Tue, 23 Apr 2024 18:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15271v1</guid></item><item><title>Aligning LLM Agents by Learning Latent Preference from User Edits</title><link>http://arxiv.org/abs/2404.15269v1</link><description>We study interactive learning of language agents based on user edits made tothe agent's output. In a typical setting such as writing assistants, the userinteracts with a language agent to generate a response given a context, and mayoptionally edit the agent response to personalize it based on their latentpreference, in addition to improving the correctness. The edit feedback isnaturally generated, making it a suitable candidate for improving the agent'salignment with the user's preference, and for reducing the cost of user editsover time. We propose a learning framework, PRELUDE that infers a descriptionof the user's latent preference based on historic edit data and using it todefine a prompt policy that drives future response generation. This avoidsfine-tuning the agent, which is costly, challenging to scale with the number ofusers, and may even degrade its performance on other tasks. Furthermore,learning descriptive preference improves interpretability, allowing the user toview and modify the learned preference. However, user preference can be complexand vary based on context, making it challenging to learn. To address this, wepropose a simple yet effective algorithm named CIPHER that leverages a largelanguage model (LLM) to infer the user preference for a given context based onuser edits. In the future, CIPHER retrieves inferred preferences from thek-closest contexts in the history, and forms an aggregate preference forresponse generation. We introduce two interactive environments -- summarizationand email writing, for evaluation using a GPT-4 simulated user. We compare withalgorithms that directly retrieve user edits but do not learn descriptivepreference, and algorithms that learn context-agnostic preference. On bothtasks, CIPHER achieves the lowest edit distance cost and learns preferencesthat show significant similarity to the ground truth preferences</description><author>Ge Gao, Alexey Taymanov, Eduardo Salinas, Paul Mineiro, Dipendra Misra</author><pubDate>Tue, 23 Apr 2024 18:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15269v1</guid></item><item><title>Neuro-Inspired Hierarchical Multimodal Learning</title><link>http://arxiv.org/abs/2309.15877v3</link><description>Integrating and processing information from various sources or modalities arecritical for obtaining a comprehensive and accurate perception of the realworld. Drawing inspiration from neuroscience, we develop theInformation-Theoretic Hierarchical Perception (ITHP) model, which utilizes theconcept of information bottleneck. Distinct from most traditional fusion modelsthat aim to incorporate all modalities as input, our model designates the primemodality as input, while the remaining modalities act as detectors in theinformation pathway. Our proposed perception model focuses on constructing aneffective and compact information flow by achieving a balance between theminimization of mutual information between the latent state and the input modalstate, and the maximization of mutual information between the latent states andthe remaining modal states. This approach leads to compact latent staterepresentations that retain relevant information while minimizing redundancy,thereby substantially enhancing the performance of downstream tasks.Experimental evaluations on both the MUStARD and CMU-MOSI datasets demonstratethat our model consistently distills crucial information in multimodal learningscenarios, outperforming state-of-the-art benchmarks.</description><author>Xiongye Xiao, Gengshuo Liu, Gaurav Gupta, Defu Cao, Shixuan Li, Yaxing Li, Tianqing Fang, Mingxi Cheng, Paul Bogdan</author><pubDate>Tue, 23 Apr 2024 18:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15877v3</guid></item><item><title>From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation</title><link>http://arxiv.org/abs/2404.15267v1</link><description>Recent advancements in controllable human image generation have led tozero-shot generation using structural signals (e.g., pose, depth) or facialappearance. Yet, generating human images conditioned on multiple parts of humanappearance remains challenging. Addressing this, we introduce Parts2Whole, anovel framework designed for generating customized portraits from multiplereference images, including pose images and various aspects of humanappearance. To achieve this, we first develop a semantic-aware appearanceencoder to retain details of different human parts, which processes each imagebased on its textual label to a series of multi-scale feature maps rather thanone image token, preserving the image dimension. Second, our framework supportsmulti-image conditioned generation through a shared self-attention mechanismthat operates across reference and target features during the diffusionprocess. We enhance the vanilla attention mechanism by incorporating maskinformation from the reference human images, allowing for the precise selectionof any part. Extensive experiments demonstrate the superiority of our approachover existing alternatives, offering advanced capabilities for multi-partcontrollable human image customization. See our project page athttps://huanngzh.github.io/Parts2Whole/.</description><author>Zehuan Huang, Hongxing Fan, Lipeng Wang, Lu Sheng</author><pubDate>Tue, 23 Apr 2024 18:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15267v1</guid></item><item><title>VideoXum: Cross-modal Visual and Textural Summarization of Videos</title><link>http://arxiv.org/abs/2303.12060v3</link><description>Video summarization aims to distill the most important information from asource video to produce either an abridged clip or a textual narrative.Traditionally, different methods have been proposed depending on whether theoutput is a video or text, thus ignoring the correlation between the twosemantically related tasks of visual summarization and textual summarization.We propose a new joint video and text summarization task. The goal is togenerate both a shortened video clip along with the corresponding textualsummary from a long video, collectively referred to as a cross-modal summary.The generated shortened video clip and text narratives should be semanticallywell aligned. To this end, we first build a large-scale human-annotated dataset-- VideoXum (X refers to different modalities). The dataset is reannotatedbased on ActivityNet. After we filter out the videos that do not meet thelength requirements, 14,001 long videos remain in our new dataset. Each videoin our reannotated dataset has human-annotated video summaries and thecorresponding narrative summaries. We then design a novel end-to-end model --VTSUM-BILP to address the challenges of our proposed task. Moreover, we proposea new metric called VT-CLIPScore to help evaluate the semantic consistency ofcross-modality summary. The proposed model achieves promising performance onthis new task and establishes a benchmark for future research.</description><author>Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho, Jiebo Luo</author><pubDate>Tue, 23 Apr 2024 18:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12060v3</guid></item><item><title>TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting</title><link>http://arxiv.org/abs/2404.15264v1</link><description>Radiance fields have demonstrated impressive performance in synthesizinglifelike 3D talking heads. However, due to the difficulty in fitting steepappearance changes, the prevailing paradigm that presents facial motions bydirectly modifying point appearance may lead to distortions in dynamic regions.To tackle this challenge, we introduce TalkingGaussian, a deformation-basedradiance fields framework for high-fidelity talking head synthesis. Leveragingthe point-based Gaussian Splatting, facial motions can be represented in ourmethod by applying smooth and continuous deformations to persistent Gaussianprimitives, without requiring to learn the difficult appearance change likeprevious methods. Due to this simplification, precise facial motions can besynthesized while keeping a highly intact facial feature. Under such adeformation paradigm, we further identify a face-mouth motion inconsistencythat would affect the learning of detailed speaking motions. To address thisconflict, we decompose the model into two branches separately for the face andinside mouth areas, therefore simplifying the learning tasks to helpreconstruct more accurate motion and structure of the mouth region. Extensiveexperiments demonstrate that our method renders high-quality lip-synchronizedtalking head videos, with better facial fidelity and higher efficiency comparedwith previous methods.</description><author>Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</author><pubDate>Tue, 23 Apr 2024 18:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15264v1</guid></item><item><title>Multi-Session SLAM with Differentiable Wide-Baseline Pose Optimization</title><link>http://arxiv.org/abs/2404.15263v1</link><description>We introduce a new system for Multi-Session SLAM, which tracks camera motionacross multiple disjoint videos under a single global reference. Our approachcouples the prediction of optical flow with solver layers to estimate camerapose. The backbone is trained end-to-end using a novel differentiable solverfor wide-baseline two-view pose. The full system can connect disjointsequences, perform visual odometry, and global optimization. Compared toexisting approaches, our design is accurate and robust to catastrophicfailures. Code is available at github.com/princeton-vl/MultiSlam_DiffPose</description><author>Lahav Lipson, Jia Deng</author><pubDate>Tue, 23 Apr 2024 18:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15263v1</guid></item><item><title>All You Need is Resistance: On the Equivalence of Effective Resistance and Certain Optimal Transport Problems on Graphs</title><link>http://arxiv.org/abs/2404.15261v1</link><description>The fields of effective resistance and optimal transport on graphs are filledwith rich connections to combinatorics, geometry, machine learning, and beyond.In this article we put forth a bold claim: that the two fields should beunderstood as one and the same, up to a choice of $p$. We make this claimprecise by introducing the parameterized family of $p$-Beckmann distances forprobability measures on graphs and relate them sharply to certain Wassersteindistances. Then, we break open a suite of results including explicitconnections to optimal stopping times and random walks on graphs, graph Sobolevspaces, and a Benamou-Brenier type formula for $2$-Beckmann distance. Wefurther explore empirical implications in the world of unsupervised learningfor graph data and propose further study of the usage of these metrics whereWasserstein distance may produce computational bottlenecks.</description><author>Sawyer Robertson, Zhengchao Wan, Alexander Cloninger</author><pubDate>Tue, 23 Apr 2024 18:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15261v1</guid></item><item><title>FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent</title><link>http://arxiv.org/abs/2404.15259v1</link><description>This paper introduces FlowMap, an end-to-end differentiable method thatsolves for precise camera poses, camera intrinsics, and per-frame dense depthof a video sequence. Our method performs per-video gradient-descentminimization of a simple least-squares objective that compares the optical flowinduced by depth, intrinsics, and poses against correspondences obtained viaoff-the-shelf optical flow and point tracking. Alongside the use of pointtracks to encourage long-term geometric consistency, we introducedifferentiable re-parameterizations of depth, intrinsics, and pose that areamenable to first-order optimization. We empirically show that cameraparameters and dense depth recovered by our method enable photo-realistic novelview synthesis on 360-degree trajectories using Gaussian Splatting. Our methodnot only far outperforms prior gradient-descent based bundle adjustmentmethods, but surprisingly performs on par with COLMAP, the state-of-the-art SfMmethod, on the downstream task of 360-degree novel view synthesis (even thoughour method is purely gradient-descent based, fully differentiable, and presentsa complete departure from conventional SfM).</description><author>Cameron Smith, David Charatan, Ayush Tewari, Vincent Sitzmann</author><pubDate>Tue, 23 Apr 2024 18:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15259v1</guid></item><item><title>Score matching for sub-Riemannian bridge sampling</title><link>http://arxiv.org/abs/2404.15258v1</link><description>Simulation of conditioned diffusion processes is an essential tool ininference for stochastic processes, data imputation, generative modelling, andgeometric statistics. Whilst simulating diffusion bridge processes is alreadydifficult on Euclidean spaces, when considering diffusion processes onRiemannian manifolds the geometry brings in further complications. In evenhigher generality, advancing from Riemannian to sub-Riemannian geometriesintroduces hypoellipticity, and the possibility of finding appropriate explicitapproximations for the score of the diffusion process is removed. We handlethese challenges and construct a method for bridge simulation on sub-Riemannianmanifolds by demonstrating how recent progress in machine learning can bemodified to allow for training of score approximators on sub-Riemannianmanifolds. Since gradients dependent on the horizontal distribution, wegeneralise the usual notion of denoising loss to work with non-holonomic framesusing a stochastic Taylor expansion, and we demonstrate the resulting schemeboth explicitly on the Heisenberg group and more generally using adaptedcoordinates. We perform numerical experiments exemplifying samples from thebridge process on the Heisenberg group and the concentration of this processfor small time.</description><author>Erlend Grong, Karen Habermann, Stefan Sommer</author><pubDate>Tue, 23 Apr 2024 18:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15258v1</guid></item><item><title>On-the-fly Definition Augmentation of LLMs for Biomedical NER</title><link>http://arxiv.org/abs/2404.00152v2</link><description>Despite their general capabilities, LLMs still struggle on biomedical NERtasks, which are difficult due to the presence of specialized terminology andlack of training data. In this work we set out to improve LLM performance onbiomedical NER in limited data settings via a new knowledge augmentationapproach which incorporates definitions of relevant concepts on-the-fly. Duringthis process, to provide a test bed for knowledge augmentation, we perform acomprehensive exploration of prompting strategies. Our experiments show thatdefinition augmentation is useful for both open source and closed LLMs. Forexample, it leads to a relative improvement of 15\% (on average) in GPT-4performance (F1) across all (six) of our test datasets. We conduct extensiveablations and analyses to demonstrate that our performance improvements stemfrom adding relevant definitional knowledge. We find that careful promptingstrategies also improve LLM performance, allowing them to outperform fine-tunedlanguage models in few-shot settings. To facilitate future research in thisdirection, we release our code at https://github.com/allenai/beacon.</description><author>Monica Munnangi, Sergey Feldman, Byron C Wallace, Silvio Amir, Tom Hope, Aakanksha Naik</author><pubDate>Tue, 23 Apr 2024 18:43:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00152v2</guid></item><item><title>TOP-Nav: Legged Navigation Integrating Terrain, Obstacle and Proprioception Estimation</title><link>http://arxiv.org/abs/2404.15256v1</link><description>Legged navigation is typically examined within open-world, off-road, andchallenging environments. In these scenarios, estimating external disturbancesrequires a complex synthesis of multi-modal information. This underlines amajor limitation in existing works that primarily focus on avoiding obstacles.In this work, we propose TOP-Nav, a novel legged navigation framework thatintegrates a comprehensive path planner with Terrain awareness, Obstacleavoidance and close-loop Proprioception. TOP-Nav underscores the synergiesbetween vision and proprioception in both path and motion planning. Within thepath planner, we present and integrate a terrain estimator that enables therobot to select waypoints on terrains with higher traversability whileeffectively avoiding obstacles. In the motion planning level, we not onlyimplement a locomotion controller to track the navigation commands, but alsoconstruct a proprioception advisor to provide motion evaluations for the pathplanner. Based on the close-loop motion feedback, we make online correctionsfor the vision-based terrain and obstacle estimations. Consequently, TOP-Navachieves open-world navigation that the robot can handle terrains ordisturbances beyond the distribution of prior knowledge and overcomesconstraints imposed by visual conditions. Building upon extensive experimentsconducted in both simulation and real-world environments, TOP-Nav demonstratessuperior performance in open-world navigation compared to existing methods.</description><author>Junli Ren, Yikai Liu, Yingru Dai, Guijin Wang</author><pubDate>Tue, 23 Apr 2024 18:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15256v1</guid></item><item><title>How to use and interpret activation patching</title><link>http://arxiv.org/abs/2404.15255v1</link><description>Activation patching is a popular mechanistic interpretability technique, buthas many subtleties regarding how it is applied and how one may interpret theresults. We provide a summary of advice and best practices, based on ourexperience using this technique in practice. We include an overview of thedifferent ways to apply activation patching and a discussion on how tointerpret the results. We focus on what evidence patching experiments provideabout circuits, and on the choice of metric and associated pitfalls.</description><author>Stefan Heimersheim, Neel Nanda</author><pubDate>Tue, 23 Apr 2024 18:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15255v1</guid></item><item><title>UniMERNet: A Universal Network for Real-World Mathematical Expression Recognition</title><link>http://arxiv.org/abs/2404.15254v1</link><description>This paper presents the UniMER dataset to provide the first study onMathematical Expression Recognition (MER) towards complex real-world scenarios.The UniMER dataset consists of a large-scale training set UniMER-1M offering anunprecedented scale and diversity with one million training instances and ameticulously designed test set UniMER-Test that reflects a diverse range offormula distributions prevalent in real-world scenarios. Therefore, the UniMERdataset enables the training of a robust and high-accuracy MER model andcomprehensive evaluation of model performance. Moreover, we introduce theUniversal Mathematical Expression Recognition Network (UniMERNet), aninnovative framework designed to enhance MER in practical scenarios. UniMERNetincorporates a Length-Aware Module to process formulas of varied lengthsefficiently, thereby enabling the model to handle complex mathematicalexpressions with greater accuracy. In addition, UniMERNet employs our UniMER-1Mdata and image augmentation techniques to improve the model's robustness underdifferent noise conditions. Our extensive experiments demonstrate thatUniMERNet outperforms existing MER models, setting a new benchmark in variousscenarios and ensuring superior recognition quality in real-world applications.The dataset and model are available athttps://github.com/opendatalab/UniMERNet.</description><author>Bin Wang, Zhuangcheng Gu, Chao Xu, Bo Zhang, Botian Shi, Conghui He</author><pubDate>Tue, 23 Apr 2024 18:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15254v1</guid></item><item><title>GIST: Gibbs self-tuning for locally adaptive Hamiltonian Monte Carlo</title><link>http://arxiv.org/abs/2404.15253v1</link><description>We present a novel and flexible framework for localized tuning of HamiltonianMonte Carlo samplers by sampling the algorithm's tuning parametersconditionally based on the position and momentum at each step. For adaptivelysampling path lengths, we show that randomized Hamiltonian Monte Carlo, theNo-U-Turn Sampler, and the Apogee-to-Apogee Path Sampler all fit within thisunified framework as special cases. The framework is illustrated with a simplealternative to the No-U-Turn Sampler for locally adapting path lengths.</description><author>Nawaf Bou-Rabee, Bob Carpenter, Milo Marsden</author><pubDate>Tue, 23 Apr 2024 18:39:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15253v1</guid></item><item><title>Source-free Domain Adaptation for Video Object Detection Under Adverse Image Conditions</title><link>http://arxiv.org/abs/2404.15252v1</link><description>When deploying pre-trained video object detectors in real-world scenarios,the domain gap between training and testing data caused by adverse imageconditions often leads to performance degradation. Addressing this issuebecomes particularly challenging when only the pre-trained model and degradedvideos are available. Although various source-free domain adaptation (SFDA)methods have been proposed for single-frame object detectors, SFDA for videoobject detection (VOD) remains unexplored. Moreover, most unsupervised domainadaptation works for object detection rely on two-stage detectors, while SFDAfor one-stage detectors, which are more vulnerable to fine-tuning, is not welladdressed in the literature. In this paper, we propose Spatial-TemporalAlternate Refinement with Mean Teacher (STAR-MT), a simple yet effective SFDAmethod for VOD. Specifically, we aim to improve the performance of theone-stage VOD method, YOLOV, under adverse image conditions, including noise,air turbulence, and haze. Extensive experiments on the ImageNetVOD dataset andits degraded versions demonstrate that our method consistently improves videoobject detection performance in challenging imaging conditions, showcasing itspotential for real-world applications.</description><author>Xingguang Zhang, Chih-Hsien Chou</author><pubDate>Tue, 23 Apr 2024 18:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15252v1</guid></item><item><title>CLEANing Cygnus A deep and fast with R2D2</title><link>http://arxiv.org/abs/2309.03291v3</link><description>A novel deep learning paradigm for synthesis imaging by radio interferometryin astronomy was recently proposed, dubbed "Residual-to-Residual DNN series forhigh-Dynamic range imaging" (R2D2). In this work, we start by shedding light onR2D2's algorithmic structure, interpreting it as a learned version of CLEANwith minor cycles substituted with a deep neural network (DNN) whose trainingis iteration-specific. We then proceed with R2D2's first demonstration on realdata, for monochromatic intensity imaging of the radio galaxy Cygnus A from Sband observations with the Very Large Array (VLA). We show that the modelingpower of R2D2's learning approach enables delivering high-precision imaging,superseding the resolution of CLEAN, and matching the precision of modernoptimization and plug-and-play algorithms, respectively uSARA and AIRI.Requiring few major-cycle iterations only, R2D2 provides a much fasterreconstruction than uSARA and AIRI, known to be highly iterative, and is atleast as fast as CLEAN.</description><author>Arwa Dabbech, Amir Aghabiglou, Chung San Chu, Yves Wiaux</author><pubDate>Tue, 23 Apr 2024 18:32:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03291v3</guid></item><item><title>XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts</title><link>http://arxiv.org/abs/2404.15247v1</link><description>We introduce XFT, a simple yet powerful training scheme, by simply mergingupcycled Mixture-of-Experts (MoE) to unleash the performance limit ofinstruction-tuned code Large Language Models (LLMs). While vanilla sparseupcycling fails to improve instruction tuning, XFT introduces a shared expertmechanism with a novel routing weight normalization strategy into sparseupcycling, which significantly boosts instruction tuning. After fine-tuning theupcycled MoE model, XFT introduces a learnable model merging mechanism tocompile the upcycled MoE model back to a dense model, achieving upcycledMoE-level performance with only dense-model compute. By applying XFT to a 1.3Bmodel, we create a new state-of-the-art tiny code LLM (&lt;3B) with 67.1 and 64.6pass@1 on HumanEval and HumanEval+ respectively. With the same data and modelarchitecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+,along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, andDS-1000, demonstrating its generalizability. XFT is fully orthogonal toexisting techniques such as Evol-Instruct and OSS-Instruct, opening a newdimension for improving code instruction tuning. Codes are available athttps://github.com/ise-uiuc/xft .</description><author>Yifeng Ding, Jiawei Liu, Yuxiang Wei, Terry Yue Zhuo, Lingming Zhang</author><pubDate>Tue, 23 Apr 2024 18:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15247v1</guid></item><item><title>StructLM: Towards Building Generalist Models for Structured Knowledge Grounding</title><link>http://arxiv.org/abs/2402.16671v5</link><description>Structured data sources, such as tables, graphs, and databases, areubiquitous knowledge sources. Despite the demonstrated capabilities of largelanguage models (LLMs) on plain text, their proficiency in interpreting andutilizing structured data remains limited. Our investigation reveals a notabledeficiency in LLMs' ability to process structured data, e.g., ChatGPT lagsbehind state-of-the-art (SoTA) model by an average of 35%. To augment theStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed acomprehensive instruction tuning dataset comprising 1.1 million examples.Utilizing this dataset, we train a series of models, referred to as StructLM,based on the Mistral and the CodeLlama model family, ranging from 7B to 34Bparameters. Our StructLM series surpasses task-specific models on 16 out of 18evaluated datasets and establishes new SoTA performance on 8 SKG tasks.Furthermore, StructLM demonstrates strong generalization across 6 novelheld-out SKG tasks, outperforming TableLlama by an average of 35\% and Flan-UL220B by an average of 10\%. Contrary to expectations, we observe that scalingmodel size offers marginal benefits, with StructLM-34B showing only slightimprovements over StructLM-7B. This suggests that structured knowledgegrounding is still a challenging task and requires more innovative design topush to a new level.</description><author>Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen</author><pubDate>Tue, 23 Apr 2024 18:29:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16671v5</guid></item><item><title>Mining Invariance from Nonlinear Multi-Environment Data: Binary Classification</title><link>http://arxiv.org/abs/2404.15245v1</link><description>Making predictions in an unseen environment given data from multiple trainingenvironments is a challenging task. We approach this problem from an invarianceperspective, focusing on binary classification to shed light on generalnonlinear data generation mechanisms. We identify a unique form of invariancethat exists solely in a binary setting that allows us to train models invariantover environments. We provide sufficient conditions for such invariance andshow it is robust even when environmental conditions vary greatly. Ourformulation admits a causal interpretation, allowing us to compare it withvarious frameworks. Finally, we propose a heuristic prediction method andconduct experiments using real and synthetic datasets.</description><author>Austin Goddard, Kang Du, Yu Xiang</author><pubDate>Tue, 23 Apr 2024 18:26:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15245v1</guid></item><item><title>Efficient Transformer Encoders for Mask2Former-style models</title><link>http://arxiv.org/abs/2404.15244v1</link><description>Vision transformer based models bring significant improvements for imagesegmentation tasks. Although these architectures offer powerful capabilitiesirrespective of specific segmentation tasks, their use of computationalresources can be taxing on deployed devices. One way to overcome this challengeis by adapting the computation level to the specific needs of the input imagerather than the current one-size-fits-all approach. To this end, we introduceECO-M2F or EffiCient TransfOrmer Encoders for Mask2Former-style models. Notingthat the encoder module of M2F-style models incur high resource-intensivecomputations, ECO-M2F provides a strategy to self-select the number of hiddenlayers in the encoder, conditioned on the input image. To enable thisself-selection ability for providing a balance between performance andcomputational efficiency, we present a three step recipe. The first step is totrain the parent architecture to enable early exiting from the encoder. Thesecond step is to create an derived dataset of the ideal number of encoderlayers required for each training example. The third step is to use theaforementioned derived dataset to train a gating network that predicts thenumber of encoder layers to be used, conditioned on the input image.Additionally, to change the computational-accuracy tradeoff, only steps two andthree need to be repeated which significantly reduces retraining time.Experiments on the public datasets show that the proposed approach reducesexpected encoder computational cost while maintaining performance, adapts tovarious user compute resources, is flexible in architecture configurations, andcan be extended beyond the segmentation task to object detection.</description><author>Manyi Yao, Abhishek Aich, Yumin Suh, Amit Roy-Chowdhury, Christian Shelton, Manmohan Chandraker</author><pubDate>Tue, 23 Apr 2024 18:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15244v1</guid></item><item><title>A Hybrid Kernel-Free Boundary Integral Method with Operator Learning for Solving Parametric Partial Differential Equations In Complex Domains</title><link>http://arxiv.org/abs/2404.15242v1</link><description>The Kernel-Free Boundary Integral (KFBI) method presents an iterativesolution to boundary integral equations arising from elliptic partialdifferential equations (PDEs). This method effectively addresses elliptic PDEson irregular domains, including the modified Helmholtz, Stokes, and elasticityequations. The rapid evolution of neural networks and deep learning hasinvigorated the exploration of numerical PDEs. An increasing interest isobserved in deep learning approaches that seamlessly integrate mathematicalprinciples for investigating numerical PDEs. We propose a hybrid KFBI method,integrating the foundational principles of the KFBI method with thecapabilities of deep learning. This approach, within the framework of theboundary integral method, designs a network to approximate the solutionoperator for the corresponding integral equations by mapping the parameters,inhomogeneous terms and boundary information of PDEs to the boundary densityfunctions, which can be regarded as the solution of the integral equations. Themodels are trained using data generated by the Cartesian grid-based KFBIalgorithm, exhibiting robust generalization capabilities. It accuratelypredicts density functions across diverse boundary conditions and parameterswithin the same class of equations. Experimental results demonstrate that thetrained model can directly infer the boundary density function withsatisfactory precision, obviating the need for iterative steps in solvingboundary integral equations. Furthermore, applying the inference results of themodel as initial values for iterations is also reasonable; this approach canretain the inherent second-order accuracy of the KFBI method while acceleratingthe traditional KFBI approach by reducing about 50% iterations.</description><author>Shuo Ling, Liwei Tan, Wenjun Ying</author><pubDate>Tue, 23 Apr 2024 18:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15242v1</guid></item><item><title>Beyond Text: Utilizing Vocal Cues to Improve Decision Making in LLMs for Robot Navigation Tasks</title><link>http://arxiv.org/abs/2402.03494v2</link><description>While LLMs excel in processing text in these human conversations, theystruggle with the nuances of verbal instructions in scenarios like socialnavigation, where ambiguity and uncertainty can erode trust in robotic andother AI systems. We can address this shortcoming by moving beyond text andadditionally focusing on the paralinguistic features of these audio responses.These features are the aspects of spoken communication that do not involve theliteral wording (lexical content) but convey meaning and nuance through howsomething is said. We present \emph{Beyond Text}; an approach that improves LLMdecision-making by integrating audio transcription along with a subsection ofthese features, which focus on the affect and more relevant in human-robotconversations.This approach not only achieves a 70.26\% winning rate,outperforming existing LLMs by 22.16\% to 48.30\% (gemini-1.5-pro and gpt-3.5respectively), but also enhances robustness against token manipulationadversarial attacks, highlighted by a 22.44\% less decrease ratio than thetext-only language model in winning rate. ``\textit{Beyond Text}'' marks anadvancement in social robot navigation and broader Human-Robot interactions,seamlessly integrating text-based guidance with human-audio-informed languagemodels.</description><author>Xingpeng Sun, Haoming Meng, Souradip Chakraborty, Amrit Singh Bedi, Aniket Bera</author><pubDate>Tue, 23 Apr 2024 18:20:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03494v2</guid></item><item><title>CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies</title><link>http://arxiv.org/abs/2404.15238v1</link><description>To enhance language models' cultural awareness, we design a generalizablepipeline to construct cultural knowledge bases from different onlinecommunities on a massive scale. With the pipeline, we construct CultureBank, aknowledge base built upon users' self-narratives with 12K cultural descriptorssourced from TikTok and 11K from Reddit. Unlike previous cultural knowledgeresources, CultureBank contains diverse views on cultural descriptors to allowflexible interpretation of cultural knowledge, and contextualized culturalscenarios to help grounded evaluation. With CultureBank, we evaluate differentLLMs' cultural awareness, and identify areas for improvement. We also fine-tunea language model on CultureBank: experiments show that it achieves betterperformances on two downstream cultural tasks in a zero-shot setting. Finally,we offer recommendations based on our findings for future culturally awarelanguage technologies. The project page is https://culturebank.github.io . Thecode and model is at https://github.com/SALT-NLP/CultureBank . The releasedCultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .</description><author>Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Chunhua yu, Raya Horesh, Rog√©rio Abreu de Paula, Diyi Yang</author><pubDate>Tue, 23 Apr 2024 18:16:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15238v1</guid></item><item><title>The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning</title><link>http://arxiv.org/abs/2403.03218v3</link><description>The White House Executive Order on Artificial Intelligence highlights therisks of large language models (LLMs) empowering malicious actors in developingbiological, cyber, and chemical weapons. To measure these risks of malicioususe, government institutions and major AI labs are developing evaluations forhazardous capabilities in LLMs. However, current evaluations are private,preventing further research into mitigating risk. Furthermore, they focus ononly a few, highly specific pathways for malicious use. To fill these gaps, wepublicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, adataset of 4,157 multiple-choice questions that serve as a proxy measurement ofhazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDPwas developed by a consortium of academics and technical consultants, and wasstringently filtered to eliminate sensitive information prior to publicrelease. WMDP serves two roles: first, as an evaluation for hazardous knowledgein LLMs, and second, as a benchmark for unlearning methods to remove suchhazardous knowledge. To guide progress on unlearning, we develop CUT, astate-of-the-art unlearning method based on controlling model representations.CUT reduces model performance on WMDP while maintaining general capabilities inareas such as biology and computer science, suggesting that unlearning may be aconcrete path towards reducing malicious use from LLMs. We release ourbenchmark and code publicly at https://wmdp.ai</description><author>Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Sam Marks, Oam Patel, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili</author><pubDate>Tue, 23 Apr 2024 18:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03218v3</guid></item><item><title>Massively Annotated Datasets for Assessment of Synthetic and Real Data in Face Recognition</title><link>http://arxiv.org/abs/2404.15234v1</link><description>Face recognition applications have grown in parallel with the size ofdatasets, complexity of deep learning models and computational power. However,while deep learning models evolve to become more capable and computationalpower keeps increasing, the datasets available are being retracted and removedfrom public access. Privacy and ethical concerns are relevant topics withinthese domains. Through generative artificial intelligence, researchers have putefforts into the development of completely synthetic datasets that can be usedto train face recognition systems. Nonetheless, the recent advances have notbeen sufficient to achieve performance comparable to the state-of-the-artmodels trained on real data. To study the drift between the performance ofmodels trained on real and synthetic datasets, we leverage a massive attributeclassifier (MAC) to create annotations for four datasets: two real and twosynthetic. From these annotations, we conduct studies on the distribution ofeach attribute within all four datasets. Additionally, we further inspect thedifferences between real and synthetic datasets on the attribute set. Whencomparing through the Kullback-Leibler divergence we have found differencesbetween real and synthetic samples. Interestingly enough, we have verified thatwhile real samples suffice to explain the synthetic distribution, the oppositecould not be further from being true.</description><author>Pedro C. Neto, Rafael M. Mamede, Carolina Albuquerque, Tiago Gon√ßalves, Ana F. Sequeira</author><pubDate>Tue, 23 Apr 2024 18:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15234v1</guid></item><item><title>Direct Zernike Coefficient Prediction from Point Spread Functions and Extended Images using Deep Learning</title><link>http://arxiv.org/abs/2404.15231v1</link><description>Optical imaging quality can be severely degraded by system and sample inducedaberrations. Existing adaptive optics systems typically rely on iterativesearch algorithm to correct for aberrations and improve images. This studydemonstrates the application of convolutional neural networks to characterisethe optical aberration by directly predicting the Zernike coefficients from twoto three phase-diverse optical images. We evaluated our network on 600,000simulated Point Spread Function (PSF) datasets randomly generated within therange of -1 to 1 radians using the first 25 Zernike coefficients. The resultsshow that using only three phase-diverse images captured above, below and atthe focal plane with an amplitude of 1 achieves a low RMSE of 0.10 radians onthe simulated PSF dataset. Furthermore, this approach directly predicts Zernikemodes simulated extended 2D samples, while maintaining a comparable RMSE of0.15 radians. We demonstrate that this approach is effective using only asingle prediction step, or can be iterated a small number of times. This simpleand straightforward technique provides rapid and accurate method for predictingthe aberration correction using three or less phase-diverse images, paving theway for evaluation on real-world dataset.</description><author>Yong En Kok, Alexander Bentley, Andrew Parkes, Amanda J. Wright, Michael G. Somekh, Michael Pound</author><pubDate>Tue, 23 Apr 2024 18:03:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15231v1</guid></item><item><title>MPIrigen: MPI Code Generation through Domain-Specific Language Models</title><link>http://arxiv.org/abs/2402.09126v2</link><description>The imperative need to scale computation across numerous nodes highlights thesignificance of efficient parallel computing, particularly in the realm ofMessage Passing Interface (MPI) integration. The challenging parallelprogramming task of generating MPI-based parallel programs has remainedunexplored. This study first investigates the performance of state-of-the-artlanguage models in generating MPI-based parallel programs. Findings reveal thatwidely used models such as GPT-3.5 and PolyCoder (specialized multi-lingualcode models) exhibit notable performance degradation, when generating MPI-basedprograms compared to general-purpose programs. In contrast, domain-specificmodels such as MonoCoder, which are pretrained on MPI-related programminglanguages of C and C++, outperform larger models. Subsequently, we introduce adedicated downstream task of MPI-based program generation by fine-tuningMonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We proposean innovative preprocessing for completion only after observing the whole code,thus enabling better completion with a wider context. Comparative analysisagainst GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluationmethod, demonstrates that MPIrigen excels in generating accurate MPI functionsup to 0.8 accuracy in location and function predictions, and with more than 0.9accuracy for argument predictions. The success of this tailored solutionunderscores the importance of domain-specific fine-tuning in optimizinglanguage models for parallel computing code generation, paving the way for anew generation of automatic parallelization tools. The sources of this work areavailable at our GitHub MPIrigen repository:https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen</description><author>Nadav Schneider, Niranjan Hasabnis, Vy A. Vo, Tal Kadosh, Neva Krien, Mihai CapotƒÉ, Guy Tamir, Ted Willke, Nesreen Ahmed, Yuval Pinter, Timothy Mattson, Gal Oren</author><pubDate>Tue, 23 Apr 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09126v2</guid></item><item><title>Re-Thinking Inverse Graphics With Large Language Models</title><link>http://arxiv.org/abs/2404.15228v1</link><description>Inverse graphics -- the task of inverting an image into physical variablesthat, when rendered, enable reproduction of the observed scene -- is afundamental challenge in computer vision and graphics. Disentangling an imageinto its constituent elements, such as the shape, color, and materialproperties of the objects of the 3D scene that produced it, requires acomprehensive understanding of the environment. This requirement limits theability of existing carefully engineered approaches to generalize acrossdomains. Inspired by the zero-shot ability of large language models (LLMs) togeneralize to novel contexts, we investigate the possibility of leveraging thebroad world knowledge encoded in such models in solving inverse-graphicsproblems. To this end, we propose the Inverse-Graphics Large Language Model(IG-LLM), an inverse-graphics framework centered around an LLM, thatautoregressively decodes a visual embedding into a structured, compositional3D-scene representation. We incorporate a frozen pre-trained visual encoder anda continuous numeric head to enable end-to-end training. Through ourinvestigation, we demonstrate the potential of LLMs to facilitate inversegraphics through next-token prediction, without the use of image-spacesupervision. Our analysis opens up new possibilities for precise spatialreasoning about images that exploit the visual knowledge of LLMs. We willrelease our code and data to ensure the reproducibility of our investigationand to facilitate future research at https://ig-llm.is.tue.mpg.de/</description><author>Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria Abrevaya, Michael J. Black</author><pubDate>Tue, 23 Apr 2024 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15228v1</guid></item><item><title>An Economic Solution to Copyright Challenges of Generative AI</title><link>http://arxiv.org/abs/2404.13964v2</link><description>Generative artificial intelligence (AI) systems are trained on large datacorpora to generate new pieces of text, images, videos, and other media. Thereis growing concern that such systems may infringe on the copyright interests oftraining data contributors. To address the copyright challenges of generativeAI, we propose a framework that compensates copyright owners proportionally totheir contributions to the creation of AI-generated content. The metric forcontributions is quantitatively determined by leveraging the probabilisticnature of modern generative AI models and using techniques from cooperativegame theory in economics. This framework enables a platform where AI developersbenefit from access to high-quality training data, thus improving modelperformance. Meanwhile, copyright owners receive fair compensation, driving thecontinued provision of relevant data for generative model training. Experimentsdemonstrate that our framework successfully identifies the most relevant datasources used in artwork generation, ensuring a fair and interpretabledistribution of revenues among copyright owners.</description><author>Jiachen T. Wang, Zhun Deng, Hiroaki Chiba-Okabe, Boaz Barak, Weijie J. Su</author><pubDate>Tue, 23 Apr 2024 17:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13964v2</guid></item><item><title>PHLP: Sole Persistent Homology for Link Prediction -- Interpretable Feature Extraction</title><link>http://arxiv.org/abs/2404.15225v1</link><description>Link prediction (LP), inferring the connectivity between nodes, is asignificant research area in graph data, where a link represents essentialinformation on relationships between nodes. Although graph neural network(GNN)-based models have achieved high performance in LP, understanding why theyperform well is challenging because most comprise complex neural networks. Weemploy persistent homology (PH), a topological data analysis method that helpsanalyze the topological information of graphs, to explain the reasons for thehigh performance. We propose a novel method that employs PH for LP (PHLP)focusing on how the presence or absence of target links influences the overalltopology. The PHLP utilizes the angle hop subgraph and new node labeling calleddegree double radius node labeling (Degree DRNL), distinguishing theinformation of graphs better than DRNL. Using only a classifier, PHLP performssimilarly to state-of-the-art (SOTA) models on most benchmark datasets.Incorporating the outputs calculated using PHLP into the existing GNN-basedSOTA models improves performance across all benchmark datasets. To the best ofour knowledge, PHLP is the first method of applying PH to LP without GNNs. Theproposed approach, employing PH while not relying on neural networks, enablesthe identification of crucial factors for improving performance.</description><author>Junwon You, Eunwoo Heo, Jae-Hun Jung</author><pubDate>Tue, 23 Apr 2024 17:54:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15225v1</guid></item><item><title>Deep Models for Multi-View 3D Object Recognition: A Review</title><link>http://arxiv.org/abs/2404.15224v1</link><description>Human decision-making often relies on visual information from multipleperspectives or views. In contrast, machine learning-based object recognitionutilizes information from a single image of the object. However, theinformation conveyed by a single image may not be sufficient for accuratedecision-making, particularly in complex recognition problems. The utilizationof multi-view 3D representations for object recognition has thus fardemonstrated the most promising results for achieving state-of-the-artperformance. This review paper comprehensively covers recent progress inmulti-view 3D object recognition methods for 3D classification and retrievaltasks. Specifically, we focus on deep learning-based and transformer-basedtechniques, as they are widely utilized and have achieved state-of-the-artperformance. We provide detailed information about existing deep learning-basedand transformer-based multi-view 3D object recognition models, including themost commonly used 3D datasets, camera configurations and number of views, viewselection strategies, pre-trained CNN architectures, fusion strategies, andrecognition performance on 3D classification and 3D retrieval tasks.Additionally, we examine various computer vision applications that usemulti-view classification. Finally, we highlight key findings and futuredirections for developing multi-view 3D object recognition methods to providereaders with a comprehensive understanding of the field.</description><author>Mona Alzahrani, Muhammad Usman, Salma Kammoun, Saeed Anwar, Tarek Helmy</author><pubDate>Tue, 23 Apr 2024 17:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15224v1</guid></item><item><title>The Power of the Noisy Channel: Unsupervised End-to-End Task-Oriented Dialogue with LLMs</title><link>http://arxiv.org/abs/2404.15219v1</link><description>Training task-oriented dialogue systems typically requires turn-levelannotations for interacting with their APIs: e.g. a dialogue state and thesystem actions taken at each step. These annotations can be costly to produce,error-prone, and require both domain and annotation expertise. With advances inLLMs, we hypothesize unlabelled data and a schema definition are sufficient forbuilding a working task-oriented dialogue system, completely unsupervised.Using only (1) a well-defined API schema (2) a set of unlabelled dialoguesbetween a user and agent, we develop a novel approach for inferring turn-levelannotations as latent variables using a noisy channel model. We iterativelyimprove these pseudo-labels with expectation-maximization (EM), and use theinferred labels to train an end-to-end dialogue agent. Evaluating our approachon the MultiWOZ benchmark, our method more than doubles the dialogue successrate of a strong GPT-3.5 baseline.</description><author>Brendan King, Jeffrey Flanigan</author><pubDate>Tue, 23 Apr 2024 17:51:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15219v1</guid></item><item><title>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</title><link>http://arxiv.org/abs/2404.01318v2</link><description>Jailbreak attacks cause large language models (LLMs) to generate harmful,unethical, or otherwise objectionable content. Evaluating these attackspresents a number of challenges, which the current collection of benchmarks andevaluation techniques do not adequately address. First, there is no clearstandard of practice regarding jailbreaking evaluation. Second, existing workscompute costs and success rates in incomparable ways. And third, numerous worksare not reproducible, as they withhold adversarial prompts, involveclosed-source code, or rely on evolving proprietary APIs. To address thesechallenges, we introduce JailbreakBench, an open-sourced benchmark with thefollowing components: (1) an evolving repository of state-of-the-artadversarial prompts, which we refer to as jailbreak artifacts; (2) ajailbreaking dataset comprising 100 behaviors -- both original and sourced fromprior work -- which align with OpenAI's usage policies; (3) a standardizedevaluation framework that includes a clearly defined threat model, systemprompts, chat templates, and scoring functions; and (4) a leaderboard thattracks the performance of attacks and defenses for various LLMs. We havecarefully considered the potential ethical implications of releasing thisbenchmark, and believe that it will be a net positive for the community. Overtime, we will expand and adapt the benchmark to reflect technical andmethodological advances in the research community.</description><author>Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, Eric Wong</author><pubDate>Tue, 23 Apr 2024 17:41:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01318v2</guid></item><item><title>Does Instruction Tuning Make LLMs More Consistent?</title><link>http://arxiv.org/abs/2404.15206v1</link><description>The purpose of instruction tuning is enabling zero-shot performance, butinstruction tuning has also been shown to improve chain-of-thought reasoningand value alignment (Si et al., 2023). Here we consider the impact on$\textit{consistency}$, i.e., the sensitivity of language models to smallperturbations in the input. We compare 10 instruction-tuned LLaMA models to theoriginal LLaMA-7b model and show that almost across-the-board they become moreconsistent, both in terms of their representations and their predictions inzero-shot and downstream tasks. We explain these improvements throughmechanistic analyses of factual recall.</description><author>Constanza Fierro, Jiaang Li, Anders S√∏gaard</author><pubDate>Tue, 23 Apr 2024 17:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15206v1</guid></item><item><title>CORE-BEHRT: A Carefully Optimized and Rigorously Evaluated BEHRT</title><link>http://arxiv.org/abs/2404.15201v1</link><description>BERT-based models for Electronic Health Records (EHR) have surged inpopularity following the release of BEHRT and Med-BERT. Subsequent models havelargely built on these foundations despite the fundamental design choices ofthese pioneering models remaining underexplored. To address this issue, weintroduce CORE-BEHRT, a Carefully Optimized and Rigorously Evaluated BEHRT.Through incremental optimization, we isolate the sources of improvement for keydesign choices, giving us insights into the effect of data representation andindividual technical components on performance. Evaluating this across a set ofgeneric tasks (death, pain treatment, and general infection), we showed thatimproving data representation can increase the average downstream performancefrom 0.785 to 0.797 AUROC, primarily when including medication and timestamps.Improving the architecture and training protocol on top of this increasedaverage downstream performance to 0.801 AUROC. We then demonstrated theconsistency of our optimization through a rigorous evaluation across 25 diverseclinical prediction tasks. We observed significant performance increases in 17out of 25 tasks and improvements in 24 tasks, highlighting the generalizabilityof our findings. Our findings provide a strong foundation for future work andaim to increase the trustworthiness of BERT-based EHR models.</description><author>Mikkel Odgaard, Kiril Vadimovic Klein, Sanne M√∏ller Thysen, Espen Jimenez-Solem, Martin Sillesen, Mads Nielsen</author><pubDate>Tue, 23 Apr 2024 17:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15201v1</guid></item><item><title>Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models</title><link>http://arxiv.org/abs/2403.09567v2</link><description>The deployment of autonomous agents in environments involving humaninteraction has increasingly raised security concerns. Consequently,understanding the circumstances behind an event becomes critical, requiring thedevelopment of capabilities to justify their behaviors to non-expert users.Such explanations are essential in enhancing trustworthiness and safety, actingas a preventive measure against failures, errors, and misunderstandings.Additionally, they contribute to improving communication, bridging the gapbetween the agent and the user, thereby improving the effectiveness of theirinteractions. This work presents an accountability and explainabilityarchitecture implemented for ROS-based mobile robots. The proposed solutionconsists of two main components. Firstly, a black box-like element to provideaccountability, featuring anti-tampering properties achieved through blockchaintechnology. Secondly, a component in charge of generating natural languageexplanations by harnessing the capabilities of Large Language Models (LLMs)over the data contained within the previously mentioned black box. The studyevaluates the performance of our solution in three different scenarios, eachinvolving autonomous agent navigation functionalities. This evaluation includesa thorough examination of accountability and explainability metrics,demonstrating the effectiveness of our approach in using accountable data fromrobot actions to obtain coherent, accurate and understandable explanations,even when facing challenges inherent in the use of autonomous agents inreal-world scenarios.</description><author>Laura Fern√°ndez-Becerra, Miguel √Ångel Gonz√°lez-Santamarta, √Ångel Manuel Guerrero-Higueras, Francisco Javier Rodr√≠guez-Lera, Vicente Matell√°n Olivera</author><pubDate>Tue, 23 Apr 2024 17:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09567v2</guid></item><item><title>Reinforcement Learning with Adaptive Control Regularization for Safe Control of Critical Systems</title><link>http://arxiv.org/abs/2404.15199v1</link><description>Reinforcement Learning (RL) is a powerful method for controlling dynamicsystems, but its learning mechanism can lead to unpredictable actions thatundermine the safety of critical systems. Here, we propose RL with AdaptiveControl Regularization (RL-ACR) that ensures RL safety by combining the RLpolicy with a control regularizer that hard-codes safety constraints overforecasted system behaviors. The adaptability is achieved by using a learnable"focus" weight trained to maximize the cumulative reward of the policycombination. As the RL policy improves through off-policy learning, the focusweight improves the initial sub-optimum strategy by gradually relying more onthe RL policy. We demonstrate the effectiveness of RL-ACR in a critical medicalcontrol application and further investigate its performance in four classiccontrol environments.</description><author>Haozhe Tian, Homayoun Hamedmoghadam, Robert Shorten, Pietro Ferraro</author><pubDate>Tue, 23 Apr 2024 17:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15199v1</guid></item><item><title>Setting up the Data Printer with Improved English to Ukrainian Machine Translation</title><link>http://arxiv.org/abs/2404.15196v1</link><description>To build large language models for Ukrainian we need to expand our corporawith large amounts of new algorithmic tasks expressed in natural language.Examples of task performance expressed in English are abundant, so with ahigh-quality translation system our community will be enabled to curatedatasets faster. To aid this goal, we introduce a recipe to build a translationsystem using supervised finetuning of a large pretrained language model with anoisy parallel dataset of 3M pairs of Ukrainian and English sentences followedby a second phase of training using 17K examples selected by k-fold perplexityfiltering on another dataset of higher quality. Our decoder-only model namedDragoman beats performance of previous state of the art encoder-decoder modelson the FLORES devtest set.</description><author>Yurii Paniv, Dmytro Chaplynskyi, Nikita Trynus, Volodymyr Kyrylov</author><pubDate>Tue, 23 Apr 2024 17:34:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15196v1</guid></item><item><title>Closed Loop Interactive Embodied Reasoning for Robot Manipulation</title><link>http://arxiv.org/abs/2404.15194v1</link><description>Embodied reasoning systems integrate robotic hardware and cognitive processesto perform complex tasks typically in response to a natural language queryabout a specific physical environment. This usually involves changing thebelief about the scene or physically interacting and changing the scene (e.g.'Sort the objects from lightest to heaviest'). In order to facilitate thedevelopment of such systems we introduce a new simulating environment thatmakes use of MuJoCo physics engine and high-quality renderer Blender to providerealistic visual observations that are also accurate to the physical state ofthe scene. Together with the simulator we propose a new benchmark composed of10 classes of multi-step reasoning scenarios that require simultaneous visualand physical measurements. Finally, we develop a new modular Closed LoopInteractive Reasoning (CLIER) approach that takes into account the measurementsof non-visual object properties, changes in the scene caused by externaldisturbances as well as uncertain outcomes of robotic actions. We extensivelyevaluate our reasoning approach in simulation and in the real worldmanipulation tasks with a success rate above 76% and 64%, respectively.</description><author>Michal Nazarczuk, Jan Kristof Behrens, Karla Stepanova, Matej Hoffmann, Krystian Mikolajczyk</author><pubDate>Tue, 23 Apr 2024 17:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15194v1</guid></item><item><title>Linear Optimal Partial Transport Embedding</title><link>http://arxiv.org/abs/2302.03232v5</link><description>Optimal transport (OT) has gained popularity due to its various applicationsin fields such as machine learning, statistics, and signal processing. However,the balanced mass requirement limits its performance in practical problems. Toaddress these limitations, variants of the OT problem, including unbalanced OT,Optimal partial transport (OPT), and Hellinger Kantorovich (HK), have beenproposed. In this paper, we propose the Linear optimal partial transport (LOPT)embedding, which extends the (local) linearization technique on OT and HK tothe OPT problem. The proposed embedding allows for faster computation of OPTdistance between pairs of positive measures. Besides our theoreticalcontributions, we demonstrate the LOPT embedding technique in point-cloudinterpolation and PCA analysis.</description><author>Yikun Bai, Ivan Medri, Rocio Diaz Martin, Rana Muhammad Shahroz Khan, Soheil Kolouri</author><pubDate>Tue, 23 Apr 2024 17:30:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03232v5</guid></item><item><title>Differentially-Private Data Synthetisation for Efficient Re-Identification Risk Control</title><link>http://arxiv.org/abs/2212.00484v3</link><description>Protecting user data privacy can be achieved via many methods, fromstatistical transformations to generative models. However, all of them havecritical drawbacks. For example, creating a transformed data set usingtraditional techniques is highly time-consuming. Also, recent deeplearning-based solutions require significant computational resources inaddition to long training phases, and differentially private-based solutionsmay undermine data utility. In this paper, we propose $\epsilon$-PrivateSMOTE,a technique designed for safeguarding against re-identification and linkageattacks, particularly addressing cases with a high \sloppy re-identificationrisk. Our proposal combines synthetic data generation via noise-inducedinterpolation with differential privacy principles to obfuscate high-riskcases. We demonstrate how $\epsilon$-PrivateSMOTE is capable of achievingcompetitive results in privacy risk and better predictive performance whencompared to multiple traditional and state-of-the-art privacy-preservationmethods, including generative adversarial networks, variational autoencoders,and differential privacy baselines. We also show how our method improves timerequirements by at least a factor of 9 and is a resource-efficient solutionthat ensures high performance without specialised hardware.</description><author>T√¢nia Carvalho, Nuno Moniz, Lu√≠s Antunes, Nitesh Chawla</author><pubDate>Tue, 23 Apr 2024 17:22:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00484v3</guid></item><item><title>Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition</title><link>http://arxiv.org/abs/2404.15176v1</link><description>This paper presents a software allowing to describe voices using a continuousVoice Femininity Percentage (VFP). This system is intended for transgenderspeakers during their voice transition and for voice therapists supporting themin this process. A corpus of 41 French cis- and transgender speakers wasrecorded. A perceptual evaluation allowed 57 participants to estimate the VFPfor each voice. Binary gender classification models were trained on externalgender-balanced data and used on overlapping windows to obtain average genderprediction estimates, which were calibrated to predict VFP and obtained higheraccuracy than $F_0$ or vocal track length-based models. Training data speakingstyle and DNN architecture were shown to impact VFP estimation. Accuracy of themodels was affected by speakers' age. This highlights the importance of style,age, and the conception of gender as binary or not, to build adequatestatistical representations of cultural concepts.</description><author>David Doukhan, Simon Devauchelle, Lucile Girard-Monneron, M√≠a Ch√°vez Ruz, V. Chaddouk, Isabelle Wagner, Albert Rilliard</author><pubDate>Tue, 23 Apr 2024 17:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15176v1</guid></item><item><title>Fourier-enhanced Implicit Neural Fusion Network for Multispectral and Hyperspectral Image Fusion</title><link>http://arxiv.org/abs/2404.15174v1</link><description>Recently, implicit neural representations (INR) have made significant stridesin various vision-related domains, providing a novel solution for Multispectraland Hyperspectral Image Fusion (MHIF) tasks. However, INR is prone to losinghigh-frequency information and is confined to the lack of global perceptualcapabilities. To address these issues, this paper introduces a Fourier-enhancedImplicit Neural Fusion Network (FeINFN) specifically designed for MHIF task,targeting the following phenomena: The Fourier amplitudes of the HR-HSI latentcode and LR-HSI are remarkably similar; however, their phases exhibit differentpatterns. In FeINFN, we innovatively propose a spatial and frequency implicitfusion function (Spa-Fre IFF), helping INR capture high-frequency informationand expanding the receptive field. Besides, a new decoder employing a complexGabor wavelet activation function, called Spatial-Frequency Interactive Decoder(SFID), is invented to enhance the interaction of INR features. Especially, wefurther theoretically prove that the Gabor wavelet activation possesses atime-frequency tightness property that favors learning the optimal bandwidthsin the decoder. Experiments on two benchmark MHIF datasets verify thestate-of-the-art (SOTA) performance of the proposed method, both visually andquantitatively. Also, ablation studies demonstrate the mentioned contributions.The code will be available on Anonymous GitHub(https://anonymous.4open.science/r/FeINFN-15C9/) after possible acceptance.</description><author>Yu-Jie Liang, Zihan Cao, Liang-Jian Deng, Xiao Wu</author><pubDate>Tue, 23 Apr 2024 17:14:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15174v1</guid></item><item><title>A Comparison of Traditional and Deep Learning Methods for Parameter Estimation of the Ornstein-Uhlenbeck Process</title><link>http://arxiv.org/abs/2404.11526v3</link><description>We consider the Ornstein-Uhlenbeck (OU) process, a stochastic process widelyused in finance, physics, and biology. Parameter estimation of the OU processis a challenging problem. Thus, we review traditional tracking methods andcompare them with novel applications of deep learning to estimate theparameters of the OU process. We use a multi-layer perceptron to estimate theparameters of the OU process and compare its performance with traditionalparameter estimation methods, such as the Kalman filter and maximum likelihoodestimation. We find that the multi-layer perceptron can accurately estimate theparameters of the OU process given a large dataset of observed trajectoriesand, on average, outperforms traditional parameter estimation methods.</description><author>Jacob Fein-Ashley</author><pubDate>Tue, 23 Apr 2024 17:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11526v3</guid></item><item><title>Adaptive Mixed-Scale Feature Fusion Network for Blind AI-Generated Image Quality Assessment</title><link>http://arxiv.org/abs/2404.15163v1</link><description>With the increasing maturity of the text-to-image and image-to-imagegenerative models, AI-generated images (AGIs) have shown great applicationpotential in advertisement, entertainment, education, social media, etc.Although remarkable advancements have been achieved in generative models, veryfew efforts have been paid to design relevant quality assessment models. Inthis paper, we propose a novel blind image quality assessment (IQA) network,named AMFF-Net, for AGIs. AMFF-Net evaluates AGI quality from three dimensions,i.e., "visual quality", "authenticity", and "consistency". Specifically,inspired by the characteristics of the human visual system and motivated by theobservation that "visual quality" and "authenticity" are characterized by bothlocal and global aspects, AMFF-Net scales the image up and down and takes thescaled images and original-sized image as the inputs to obtain multi-scalefeatures. After that, an Adaptive Feature Fusion (AFF) block is used toadaptively fuse the multi-scale features with learnable weights. In addition,considering the correlation between the image and prompt, AMFF-Net compares thesemantic features from text encoder and image encoder to evaluate thetext-to-image alignment. We carry out extensive experiments on three AGIquality assessment databases, and the experimental results show that ourAMFF-Net obtains better performance than nine state-of-the-art blind IQAmethods. The results of ablation experiments further demonstrate theeffectiveness of the proposed multi-scale input strategy and AFF block.</description><author>Tianwei Zhou, Songbai Tan, Wei Zhou, Yu Luo, Yuan-Gen Wang, Guanghui Yue</author><pubDate>Tue, 23 Apr 2024 17:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15163v1</guid></item><item><title>Combating Missing Modalities in Egocentric Videos at Test Time</title><link>http://arxiv.org/abs/2404.15161v1</link><description>Understanding videos that contain multiple modalities is crucial, especiallyin egocentric videos, where combining various sensory inputs significantlyimproves tasks like action recognition and moment localization. However,real-world applications often face challenges with incomplete modalities due toprivacy concerns, efficiency needs, or hardware issues. Current methods, whileeffective, often necessitate retraining the model entirely to handle missingmodalities, making them computationally intensive, particularly with largetraining datasets. In this study, we propose a novel approach to address thisissue at test time without requiring retraining. We frame the problem as atest-time adaptation task, where the model adjusts to the available unlabeleddata at test time. Our method, MiDl~(Mutual information withself-Distillation), encourages the model to be insensitive to the specificmodality source present during testing by minimizing the mutual informationbetween the prediction and the available modality. Additionally, we incorporateself-distillation to maintain the model's original performance when bothmodalities are available. MiDl represents the first self-supervised, onlinesolution for handling missing modalities exclusively at test time. Throughexperiments with various pretrained models and datasets, MiDl demonstratessubstantial performance improvement without the need for retraining.</description><author>Merey Ramazanova, Alejandro Pardo, Bernard Ghanem, Motasem Alfarra</author><pubDate>Tue, 23 Apr 2024 17:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15161v1</guid></item><item><title>Regressive Side Effects of Training Language Models to Mimic Student Misconceptions</title><link>http://arxiv.org/abs/2404.15156v1</link><description>This paper presents a novel exploration into the regressive side effects oftraining Large Language Models (LLMs) to mimic student misconceptions forpersonalized education. We highlight the problem that as LLMs are trained tomore accurately mimic student misconceptions, there is a compromise in thefactual integrity and reasoning ability of the models. Our work involvedtraining an LLM on a student-tutor dialogue dataset to predict studentresponses. The results demonstrated a decrease in the model's performanceacross multiple benchmark datasets, including the ARC reasoning challenge andTruthfulQA, which evaluates the truthfulness of model's generated responses.Furthermore, the HaluEval Dial dataset, used for hallucination detection, andMemoTrap, a memory-based task dataset, also reported a decline in the modelaccuracy. To combat these side effects, we introduced a "hallucination token"technique. This token, appended at the beginning of each student responseduring training, instructs the model to switch between mimicking studentmisconceptions and providing factually accurate responses. Despite thesignificant improvement across all datasets, the technique does not completelyrestore the LLM's baseline performance, indicating the need for furtherresearch in this area. This paper contributes to the ongoing discussion on theuse of LLMs for student modeling, emphasizing the need for a balance betweenpersonalized education and factual accuracy.</description><author>Shashank Sonkar, Naiming Liu, Richard G. Baraniuk</author><pubDate>Tue, 23 Apr 2024 16:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15156v1</guid></item><item><title>VT-Former: An Exploratory Study on Vehicle Trajectory Prediction for Highway Surveillance through Graph Isomorphism and Transformer</title><link>http://arxiv.org/abs/2311.06623v4</link><description>Enhancing roadway safety has become an essential computer vision focus areafor Intelligent Transportation Systems (ITS). As a part of ITS, VehicleTrajectory Prediction (VTP) aims to forecast a vehicle's future positions basedon its past and current movements. VTP is a pivotal element for road safety,aiding in applications such as traffic management, accident prevention,work-zone safety, and energy optimization. While most works in this field focuson autonomous driving, with the growing number of surveillance cameras, anothersub-field emerges for surveillance VTP with its own set of challenges. In thispaper, we introduce VT-Former, a novel transformer-based VTP approach forhighway safety and surveillance. In addition to utilizing transformers tocapture long-range temporal patterns, a new Graph Attentive Tokenization (GAT)module has been proposed to capture intricate social interactions amongvehicles. This study seeks to explore both the advantages and the limitationsinherent in combining transformer architecture with graphs for VTP. Ourinvestigation, conducted across three benchmark datasets from diversesurveillance viewpoints, showcases the State-of-the-Art (SotA) or comparableperformance of VT-Former in predicting vehicle trajectories. This studyunderscores the potential of VT-Former and its architecture, opening newavenues for future research and exploration.</description><author>Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi</author><pubDate>Tue, 23 Apr 2024 16:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06623v4</guid></item><item><title>From Reactive to Proactive Volatility Modeling with Hemisphere Neural Networks</title><link>http://arxiv.org/abs/2311.16333v2</link><description>We reinvigorate maximum likelihood estimation (MLE) for macroeconomic densityforecasting through a novel neural network architecture with dedicated mean andvariance hemispheres. Our architecture features several key ingredients makingMLE work in this context. First, the hemispheres share a common core at theentrance of the network which accommodates for various forms of time variationin the error variance. Second, we introduce a volatility emphasis constraintthat breaks mean/variance indeterminacy in this class of overparametrizednonlinear models. Third, we conduct a blocked out-of-bag reality check to curboverfitting in both conditional moments. Fourth, the algorithm utilizesstandard deep learning software and thus handles large data sets - bothcomputationally and statistically. Ergo, our Hemisphere Neural Network (HNN)provides proactive volatility forecasts based on leading indicators when itcan, and reactive volatility based on the magnitude of previous predictionerrors when it must. We evaluate point and density forecasts with an extensiveout-of-sample experiment and benchmark against a suite of models ranging fromclassics to more modern machine learning-based offerings. In all cases, HNNfares well by consistently providing accurate mean/variance forecasts for alltargets and horizons. Studying the resulting volatility paths reveals itsversatility, while probabilistic forecasting evaluation metrics showcase itsenviable reliability. Finally, we also demonstrate how this machinery can bemerged with other structured deep learning models by revisiting Goulet Coulombe(2022)'s Neural Phillips Curve.</description><author>Philippe Goulet Coulombe, Mikael Frenette, Karin Klieber</author><pubDate>Tue, 23 Apr 2024 16:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16333v2</guid></item><item><title>Bias patterns in the application of LLMs for clinical decision support: A comprehensive study</title><link>http://arxiv.org/abs/2404.15149v1</link><description>Large Language Models (LLMs) have emerged as powerful candidates to informclinical decision-making processes. While these models play an increasinglyprominent role in shaping the digital landscape, two growing concerns emerge inhealthcare applications: 1) to what extent do LLMs exhibit social bias based onpatients' protected attributes (like race), and 2) how do design choices (likearchitecture design and prompting strategies) influence the observed biases? Toanswer these questions rigorously, we evaluated eight popular LLMs across threequestion-answering (QA) datasets using clinical vignettes (patientdescriptions) standardized for bias evaluations. We employ red-teamingstrategies to analyze how demographics affect LLM outputs, comparing bothgeneral-purpose and clinically-trained models. Our extensive experiments revealvarious disparities (some significant) across protected groups. We also observeseveral counter-intuitive patterns such as larger models not being necessarilyless biased and fined-tuned models on medical data not being necessarily betterthan the general-purpose models. Furthermore, our study demonstrates the impactof prompt design on bias patterns and shows that specific phrasing caninfluence bias patterns and reflection-type approaches (like Chain of Thought)can reduce biased outcomes effectively. Consistent with prior studies, we callon additional evaluations, scrutiny, and enhancement of LLMs used in clinicaldecision support applications.</description><author>Raphael Poulain, Hamed Fayyaz, Rahmatollah Beheshti</author><pubDate>Tue, 23 Apr 2024 16:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15149v1</guid></item><item><title>Rethinking LLM Memorization through the Lens of Adversarial Compression</title><link>http://arxiv.org/abs/2404.15146v1</link><description>Large language models (LLMs) trained on web-scale datasets raise substantialconcerns regarding permissible data usage. One major question is whether thesemodels "memorize" all their training data or they integrate many data sourcesin some way more akin to how a human would learn and synthesize information.The answer hinges, to a large degree, on $\textit{how we define memorization}$.In this work, we propose the Adversarial Compression Ratio (ACR) as a metricfor assessing memorization in LLMs -- a given string from the training data isconsidered memorized if it can be elicited by a prompt shorter than the stringitself. In other words, these strings can be "compressed" with the model bycomputing adversarial prompts of fewer tokens. We outline the limitations ofexisting notions of memorization and show how the ACR overcomes thesechallenges by (i) offering an adversarial view to measuring memorization,especially for monitoring unlearning and compliance; and (ii) allowing for theflexibility to measure memorization for arbitrary strings at a reasonably lowcompute. Our definition serves as a valuable and practical tool for determiningwhen model owners may be violating terms around data usage, providing apotential legal tool and a critical lens through which to address suchscenarios. Project page: https://locuslab.github.io/acr-memorization.</description><author>Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico Kolter</author><pubDate>Tue, 23 Apr 2024 16:49:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15146v1</guid></item><item><title>CutDiffusion: A Simple, Fast, Cheap, and Strong Diffusion Extrapolation Method</title><link>http://arxiv.org/abs/2404.15141v1</link><description>Transforming large pre-trained low-resolution diffusion models to cater tohigher-resolution demands, i.e., diffusion extrapolation, significantlyimproves diffusion adaptability. We propose tuning-free CutDiffusion, aimed atsimplifying and accelerating the diffusion extrapolation process, making itmore affordable and improving performance. CutDiffusion abides by the existingpatch-wise extrapolation but cuts a standard patch diffusion process into aninitial phase focused on comprehensive structure denoising and a subsequentphase dedicated to specific detail refinement. Comprehensive experimentshighlight the numerous almighty advantages of CutDiffusion: (1) simple methodconstruction that enables a concise higher-resolution diffusion process withoutthird-party engagement; (2) fast inference speed achieved through a single-stephigher-resolution diffusion process, and fewer inference patches required; (3)cheap GPU cost resulting from patch-wise inference and fewer patches during thecomprehensive structure denoising; (4) strong generation performance, stemmingfrom the emphasis on specific detail refinement.</description><author>Mingbao Lin, Zhihang Lin, Wengyi Zhan, Liujuan Cao, Rongrong Ji</author><pubDate>Tue, 23 Apr 2024 16:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15141v1</guid></item><item><title>Explicit Second-Order Min-Max Optimization Methods with Optimal Convergence Guarantee</title><link>http://arxiv.org/abs/2210.12860v4</link><description>We propose and analyze several inexact regularized Newton-type methods forfinding a global saddle point of \emph{convex-concave} unconstrained min-maxoptimization problems. Compared to first-order methods, our understanding ofsecond-order methods for min-max optimization is relatively limited, asobtaining global rates of convergence with second-order information is muchmore involved. In this paper, we examine how second-order information can beused to speed up extra-gradient methods, even under inexactness. Specifically,we show that the proposed methods generate iterates that remain within abounded set and that the averaged iterates converge to an $\epsilon$-saddlepoint within $O(\epsilon^{-2/3})$ iterations in terms of a restricted gapfunction. This matched the theoretically established lower bound in thiscontext. We also provide a simple routine for solving the subproblem at eachiteration, requiring a single Schur decomposition and $O(\log\log(1/\epsilon))$calls to a linear system solver in a quasi-upper-triangular system. Thus, ourmethod improves the existing line-search-based second-order min-maxoptimization methods by shaving off an $O(\log\log(1/\epsilon))$ factor in therequired number of Schur decompositions. Finally, we present numericalexperiments on synthetic and real data that demonstrate the efficiency of theproposed methods.</description><author>Tianyi Lin, Panayotis Mertikopoulos, Michael I. Jordan</author><pubDate>Tue, 23 Apr 2024 16:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12860v4</guid></item><item><title>Gallbladder Cancer Detection in Ultrasound Images based on YOLO and Faster R-CNN</title><link>http://arxiv.org/abs/2404.15129v1</link><description>Medical image analysis is a significant application of artificialintelligence for disease diagnosis. A crucial step in this process is theidentification of regions of interest within the images. This task can beautomated using object detection algorithms. YOLO and Faster R-CNN are renownedfor such algorithms, each with its own strengths and weaknesses. This studyaims to explore the advantages of both techniques to select more accuratebounding boxes for gallbladder detection from ultrasound images, therebyenhancing gallbladder cancer classification. A fusion method that leverages thebenefits of both techniques is presented in this study. The proposed methoddemonstrated superior classification performance, with an accuracy of 92.62%,compared to the individual use of Faster R-CNN and YOLOv8, which yieldedaccuracies of 90.16% and 82.79%, respectively.</description><author>Sara Dadjouy, Hedieh Sajedi</author><pubDate>Tue, 23 Apr 2024 16:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15129v1</guid></item><item><title>MedDr: Diagnosis-Guided Bootstrapping for Large-Scale Medical Vision-Language Learning</title><link>http://arxiv.org/abs/2404.15127v1</link><description>The rapid advancement of large-scale vision-language models has showcasedremarkable capabilities across various tasks. However, the lack of extensiveand high-quality image-text data in medicine has greatly hindered thedevelopment of large-scale medical vision-language models. In this work, wepresent a diagnosis-guided bootstrapping strategy that exploits both image andlabel information to construct vision-language datasets. Based on theconstructed dataset, we developed MedDr, a generalist foundation model forhealthcare capable of handling diverse medical data modalities, includingradiology, pathology, dermatology, retinography, and endoscopy. Moreover,during inference, we propose a simple but effective retrieval-augmented medicaldiagnosis strategy, which enhances the model's generalization ability.Extensive experiments on visual question answering, medical report generation,and medical image diagnosis demonstrate the superiority of our method.</description><author>Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, Hao Chen</author><pubDate>Tue, 23 Apr 2024 16:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15127v1</guid></item><item><title>Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks</title><link>http://arxiv.org/abs/2403.11830v2</link><description>Machine Learning (ML) algorithms have become increasingly popular forsupporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensiveresearch has shown their vulnerability to adversarial attacks, which involvesubtle perturbations to the inputs of the models aimed at compromising theirperformance. Recent proposals have effectively leveraged Graph Neural Networks(GNN) to produce predictions based also on the structural patterns exhibited byintrusions to enhance the detection robustness. However, the adoption ofGNN-based NIDS introduces new types of risks. In this paper, we propose thefirst formalization of adversarial attacks specifically tailored for GNN innetwork intrusion detection. Moreover, we outline and model the problem spaceconstraints that attackers need to consider to carry out feasible structuralattacks in real-world scenarios. As a final contribution, we conduct anextensive experimental campaign in which we launch the proposed attacks againststate-of-the-art GNN-based NIDS. Our findings demonstrate the increasedrobustness of the models against classical feature-based adversarial attacks,while highlighting their susceptibility to structure-based attacks.</description><author>Andrea Venturi, Dario Stabili, Mirco Marchetti</author><pubDate>Tue, 23 Apr 2024 16:21:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11830v2</guid></item><item><title>Taming Diffusion Probabilistic Models for Character Control</title><link>http://arxiv.org/abs/2404.15121v1</link><description>We present a novel character control framework that effectively utilizesmotion diffusion probabilistic models to generate high-quality and diversecharacter animations, responding in real-time to a variety of dynamicuser-supplied control signals. At the heart of our method lies atransformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM),which takes as input the character's historical motion and can generate a rangeof diverse potential future motions conditioned on high-level, coarse usercontrol. To meet the demands for diversity, controllability, and computationalefficiency required by a real-time controller, we incorporate several keyalgorithmic designs. These include separate condition tokenization,classifier-free guidance on past motion, and heuristic future trajectoryextension, all designed to address the challenges associated with taming motiondiffusion probabilistic models for character control. As a result, our workrepresents the first model that enables real-time generation of high-quality,diverse character animations based on user interactive control, supportinganimating the character in multiple styles with a single unified model. Weevaluate our method on a diverse set of locomotion skills, demonstrating themerits of our method over existing character controllers. Project page andsource codes: https://aiganimation.github.io/CAMDM/</description><author>Rui Chen, Mingyi Shi, Shaoli Huang, Ping Tan, Taku Komura, Xuelin Chen</author><pubDate>Tue, 23 Apr 2024 16:20:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15121v1</guid></item><item><title>Co-Speech Gesture Detection through Multi-Phase Sequence Labeling</title><link>http://arxiv.org/abs/2308.10680v2</link><description>Gestures are integral components of face-to-face communication. They unfoldover time, often following predictable movement phases of preparation, stroke,and retraction. Yet, the prevalent approach to automatic gesture detectiontreats the problem as binary classification, classifying a segment as eithercontaining a gesture or not, thus failing to capture its inherently sequentialand contextual nature. To address this, we introduce a novel framework thatreframes the task as a multi-phase sequence labeling problem rather than binaryclassification. Our model processes sequences of skeletal movements over timewindows, uses Transformer encoders to learn contextual embeddings, andleverages Conditional Random Fields to perform sequence labeling. We evaluateour proposal on a large dataset of diverse co-speech gestures in task-orientedface-to-face dialogues. The results consistently demonstrate that our methodsignificantly outperforms strong baseline models in detecting gesture strokes.Furthermore, applying Transformer encoders to learn contextual embeddings frommovement sequences substantially improves gesture unit detection. These resultshighlight our framework's capacity to capture the fine-grained dynamics ofco-speech gesture phases, paving the way for more nuanced and accurate gesturedetection and analysis.</description><author>Esam Ghaleb, Ilya Burenko, Marlou Rasenberg, Wim Pouw, Peter Uhrig, Judith Holler, Ivan Toni, Aslƒ± √ñzy√ºrek, Raquel Fern√°ndez</author><pubDate>Tue, 23 Apr 2024 16:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10680v2</guid></item><item><title>Identifying phase transitions in physical systems with neural networks: a neural architecture search perspective</title><link>http://arxiv.org/abs/2404.15118v1</link><description>The use of machine learning algorithms to investigate phase transitions inphysical systems is a valuable way to better understand the characteristics ofthese systems. Neural networks have been used to extract information of phasesand phase transitions directly from many-body configurations. However, onelimitation of neural networks is that they require the definition of the modelarchitecture and parameters previous to their application, and suchdetermination is itself a difficult problem. In this paper, we investigate forthe first time the relationship between the accuracy of neural networks forinformation of phases and the network configuration (that comprises thearchitecture and hyperparameters). We formulate the phase analysis as aregression task, address the question of generating data that reflects thedifferent states of the physical system, and evaluate the performance of neuralarchitecture search for this task. After obtaining the optimized architectures,we further implement smart data processing and analytics by means of neuroncoverage metrics, assessing the capability of these metrics to estimate phasetransitions. Our results identify the neuron coverage metric as promising fordetecting phase transitions in physical systems.</description><author>Rodrigo Carmo Terin, Zochil Gonz√°lez Arenas, Roberto Santana</author><pubDate>Tue, 23 Apr 2024 16:16:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15118v1</guid></item><item><title>Emergent Dominance Hierarchies in Reinforcement Learning Agents</title><link>http://arxiv.org/abs/2401.12258v6</link><description>Modern Reinforcement Learning (RL) algorithms are able to outperform humansin a wide variety of tasks. Multi-agent reinforcement learning (MARL) settingspresent additional challenges, and successful cooperation in mixed-motivegroups of agents depends on a delicate balancing act between individual andgroup objectives. Social conventions and norms, often inspired by humaninstitutions, are used as tools for striking this balance. In this paper, we examine a fundamental, well-studied social convention thatunderlies cooperation in both animal and human societies: dominancehierarchies. We adapt the ethological theory of dominance hierarchies to artificialagents, borrowing the established terminology and definitions with as fewamendments as possible. We demonstrate that populations of RL agents, operatingwithout explicit programming or intrinsic rewards, can invent, learn, enforce,and transmit a dominance hierarchy to new populations. The dominancehierarchies that emerge have a similar structure to those studied in chickens,mice, fish, and other species.</description><author>Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky</author><pubDate>Tue, 23 Apr 2024 16:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12258v6</guid></item><item><title>Compete and Compose: Learning Independent Mechanisms for Modular World Models</title><link>http://arxiv.org/abs/2404.15109v1</link><description>We present COmpetitive Mechanisms for Efficient Transfer (COMET), a modularworld model which leverages reusable, independent mechanisms across differentenvironments. COMET is trained on multiple environments with varying dynamicsvia a two-step process: competition and composition. This enables the model torecognise and learn transferable mechanisms. Specifically, in the competitionphase, COMET is trained with a winner-takes-all gradient allocation,encouraging the emergence of independent mechanisms. These are then re-used inthe composition phase, where COMET learns to re-compose learnt mechanisms inways that capture the dynamics of intervened environments. In so doing, COMETexplicitly reuses prior knowledge, enabling efficient and interpretableadaptation. We evaluate COMET on environments with image-based observations. Incontrast to competitive baselines, we demonstrate that COMET capturesrecognisable mechanisms without supervision. Moreover, we show that COMET isable to adapt to new environments with varying numbers of objects with improvedsample efficiency compared to more conventional finetuning approaches.</description><author>Anson Lei, Frederik Nolte, Bernhard Sch√∂lkopf, Ingmar Posner</author><pubDate>Tue, 23 Apr 2024 16:03:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15109v1</guid></item><item><title>Identifying Fairness Issues in Automatically Generated Testing Content</title><link>http://arxiv.org/abs/2404.15104v1</link><description>Natural language generation tools are powerful and effective for generatingcontent. However, language models are known to display bias and fairnessissues, making them impractical to deploy for many use cases. We here focus onhow fairness issues impact automatically generated test content, which can havestringent requirements to ensure the test measures only what it was intended tomeasure. Specifically, we identify test content that is focused on particulardomains and experiences that only reflect a certain demographic or that arepotentially emotionally upsetting; both of which could inadvertently impact atest-taker's score. This kind of content doesn't reflect typical biases out ofcontext, making it challenging even for modern models that contain safeguards.We build a dataset of 621 generated texts annotated for fairness and explore avariety of methods for classification: fine-tuning, topic-based classification,and prompting, including few-shot and self-correcting prompts. We find thatcombining prompt self-correction and few-shot learning performs best, yieldingan F1 score of .791 on our held-out test set, while much smaller BERT- andtopic-based models have competitive performance on out-of-domain data.</description><author>Kevin Stowe, Benny Longwill, Alyssa Francis, Tatsuya Aoyama, Debanjan Ghosh, Swapna Somasundaran</author><pubDate>Tue, 23 Apr 2024 15:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15104v1</guid></item><item><title>Multi-view Content-aware Indexing for Long Document Retrieval</title><link>http://arxiv.org/abs/2404.15103v1</link><description>Long document question answering (DocQA) aims to answer questions from longdocuments over 10k words. They usually contain content structures such assections, sub-sections, and paragraph demarcations. However, the indexingmethods of long documents remain under-explored, while existing systemsgenerally employ fixed-length chunking. As they do not consider contentstructures, the resultant chunks can exclude vital information or includeirrelevant content. Motivated by this, we propose the Multi-view Content-awareindexing (MC-indexing) for more effective long DocQA via (i) segment structureddocument into content chunks, and (ii) represent each content chunk inraw-text, keywords, and summary views. We highlight that MC-indexing requiresneither training nor fine-tuning. Having plug-and-play capability, it can beseamlessly integrated with any retrievers to boost their performance. Besides,we propose a long DocQA dataset that includes not only question-answer pair,but also document structure and answer scope. When compared to state-of-artchunking schemes, MC-indexing has significantly increased the recall by 42.8%,30.0%, 23.9%, and 16.3% via top k= 1.5, 3, 5, and 10 respectively. Theseimproved scores are the average of 8 widely used retrievers (2 sparse and 6dense) via extensive experiments.</description><author>Kuicai Dong, Derrick Goh Xin Deik, Yi Quan Lee, Hao Zhang, Xiangyang Li, Cong Zhang, Yong Liu</author><pubDate>Tue, 23 Apr 2024 15:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15103v1</guid></item><item><title>Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples</title><link>http://arxiv.org/abs/2309.03847v3</link><description>We study the problem of estimating mixtures of Gaussians under the constraintof differential privacy (DP). Our main result is that$\text{poly}(k,d,1/\alpha,1/\varepsilon,\log(1/\delta))$ samples are sufficientto estimate a mixture of $k$ Gaussians in $\mathbb{R}^d$ up to total variationdistance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is thefirst finite sample complexity upper bound for the problem that does not makeany structural assumptions on the GMMs. To solve the problem, we devise a new framework which may be useful for othertasks. On a high level, we show that if a class of distributions (such asGaussians) is (1) list decodable and (2) admits a "locally small'' cover (Bunet al., 2021) with respect to total variation distance, then the class of itsmixtures is privately learnable. The proof circumvents a known barrierindicating that, unlike Gaussians, GMMs do not admit a locally small cover(Aden-Ali et al., 2021b).</description><author>Mohammad Afzali, Hassan Ashtiani, Christopher Liaw</author><pubDate>Tue, 23 Apr 2024 15:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03847v3</guid></item><item><title>Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation</title><link>http://arxiv.org/abs/2404.15100v1</link><description>Recent studies have demonstrated the exceptional potentials of leveraginghuman preference datasets to refine text-to-image generative models, enhancingthe alignment between generated images and textual prompts. Despite theseadvances, current human preference datasets are either prohibitively expensiveto construct or suffer from a lack of diversity in preference dimensions,resulting in limited applicability for instruction tuning in open-sourcetext-to-image generative models and hinder further exploration. To addressthese challenges and promote the alignment of generative models throughinstruction tuning, we leverage multimodal large language models to createVisionPrefer, a high-quality and fine-grained preference dataset that capturesmultiple preference aspects. We aggregate feedback from AI annotators acrossfour aspects: prompt-following, aesthetic, fidelity, and harmlessness toconstruct VisionPrefer. To validate the effectiveness of VisionPrefer, we traina reward model VP-Score over VisionPrefer to guide the training oftext-to-image generative models and the preference prediction accuracy ofVP-Score is comparable to human annotators. Furthermore, we use tworeinforcement learning methods to supervised fine-tune generative models toevaluate the performance of VisionPrefer, and extensive experimental resultsdemonstrate that VisionPrefer significantly improves text-image alignment incompositional image generation across diverse aspects, e.g., aesthetic, andgeneralizes better than previous human-preference metrics across various imagedistributions. Moreover, VisionPrefer indicates that the integration ofAI-generated synthetic data as a supervisory signal is a promising avenue forachieving improved alignment with human preferences in vision generativemodels.</description><author>Xun Wu, Shaohan Huang, Furu Wei</author><pubDate>Tue, 23 Apr 2024 15:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15100v1</guid></item><item><title>Uncertainty Quantification of Data-Driven Output Predictors in the Output Error Setting</title><link>http://arxiv.org/abs/2404.15098v1</link><description>We revisit the problem of predicting the output of an LTI system directlyusing offline input-output data (and without the use of a parametric model) inthe behavioral setting. Existing works calculate the output predictions byprojecting the recent samples of the input and output signals onto the columnspan of a Hankel matrix consisting of the offline input-output data. However,if the offline data is corrupted by noise, the output prediction is no longerexact. While some prior works propose mitigating noisy data through matrixlow-ranking approximation heuristics, such as truncated singular valuedecomposition, the ensuing prediction accuracy remains unquantified. This paperfills these gaps by introducing two upper bounds on the prediction error underthe condition that the noise is sufficiently small relative to the offlinedata's magnitude. The first bound pertains to prediction using the raw offlinedata directly, while the second one applies to the case of low-rankingapproximation heuristic. Notably, the bounds do not require the ground truthabout the system output, relying solely on noisy measurements with a knownnoise level and system order. Extensive numerical simulations show that bothbounds decrease monotonically (and linearly) as a function of the noise level.Furthermore, our results demonstrate that applying the de-noising heuristic inthe output error setup does not generally lead to a better prediction accuracyas compared to using raw data directly, nor a smaller upper bound on theprediction error. However, it allows for a more general upper bound, as thefirst upper bound requires a specific condition on the partitioning of theHankel matrix.</description><author>Farzan Kaviani, Ivan Markovsky, Hamid R. Ossareh</author><pubDate>Tue, 23 Apr 2024 15:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15098v1</guid></item><item><title>Impedance Matching: Enabling an RL-Based Running Jump in a Quadruped Robot</title><link>http://arxiv.org/abs/2404.15096v1</link><description>Replicating the remarkable athleticism seen in animals has long been achallenge in robotics control. Although Reinforcement Learning (RL) hasdemonstrated significant progress in dynamic legged locomotion control, thesubstantial sim-to-real gap often hinders the real-world demonstration of trulydynamic movements. We propose a new framework to mitigate this gap throughfrequency-domain analysis-based impedance matching between simulated and realrobots. Our framework offers a structured guideline for parameter selection andthe range for dynamics randomization in simulation, thus facilitating a safesim-to-real transfer. The learned policy using our framework enabled jumpsacross distances of 55 cm and heights of 38 cm. The results are, to the best ofour knowledge, one of the highest and longest running jumps demonstrated by anRL-based control policy in a real quadruped robot. Note that the achievedjumping height is approximately 85% of that obtained from a state-of-the-arttrajectory optimization method, which can be seen as the physical limit for thegiven robot hardware. In addition, our control policy accomplished stablewalking at speeds up to 2 m/s in the forward and backward directions, and 1 m/sin the sideway direction.</description><author>Neil Guan, Shangqun Yu, Shifan Zhu, Donghyun Kim</author><pubDate>Tue, 23 Apr 2024 15:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15096v1</guid></item><item><title>Using ARIMA to Predict the Expansion of Subscriber Data Consumption</title><link>http://arxiv.org/abs/2404.15095v1</link><description>This study discusses how insights retrieved from subscriber data can impactdecision-making in telecommunications, focusing on predictive modeling usingmachine learning techniques such as the ARIMA model. The study explores timeseries forecasting to predict subscriber usage trends, evaluating the ARIMAmodel's performance using various metrics. It also compares ARIMA withConvolutional Neural Network (CNN) models, highlighting ARIMA's superiority inaccuracy and execution speed. The study suggests future directions forresearch, including exploring additional forecasting models and consideringother factors affecting subscriber data usage.</description><author>Mike Wa Nkongolo</author><pubDate>Tue, 23 Apr 2024 15:49:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15095v1</guid></item><item><title>CLIP-QDA: An Explainable Concept Bottleneck Model</title><link>http://arxiv.org/abs/2312.00110v2</link><description>In this paper, we introduce an explainable algorithm designed from amulti-modal foundation model, that performs fast and explainable imageclassification. Drawing inspiration from CLIP-based Concept Bottleneck Models(CBMs), our method creates a latent space where each neuron is linked to aspecific word. Observing that this latent space can be modeled with simpledistributions, we use a Mixture of Gaussians (MoG) formalism to enhance theinterpretability of this latent space. Then, we introduce CLIP-QDA, aclassifier that only uses statistical values to infer labels from the concepts.In addition, this formalism allows for both local and global explanations.These explanations come from the inner design of our architecture, our work ispart of a new family of greybox models, combining performances of opaquefoundation models and the interpretability of transparent models. Our empiricalfindings show that in instances where the MoG assumption holds, CLIP-QDAachieves similar accuracy with state-of-the-art methods CBMs. Our explanationscompete with existing XAI methods while being faster to compute.</description><author>R√©mi Kazmierczak, Elo√Øse Berthier, Goran Frehse, Gianni Franchi</author><pubDate>Tue, 23 Apr 2024 15:49:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00110v2</guid></item><item><title>Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</title><link>http://arxiv.org/abs/2404.14219v2</link><description>We introduce phi-3-mini, a 3.8 billion parameter language model trained on3.3 trillion tokens, whose overall performance, as measured by both academicbenchmarks and internal testing, rivals that of models such as Mixtral 8x7B andGPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despitebeing small enough to be deployed on a phone. The innovation lies entirely inour dataset for training, a scaled-up version of the one used for phi-2,composed of heavily filtered web data and synthetic data. The model is alsofurther aligned for robustness, safety, and chat format. We also provide someinitial parameter-scaling results with a 7B and 14B models trained for 4.8Ttokens, called phi-3-small and phi-3-medium, both significantly more capablethan phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 onMT-bench).</description><author>Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S√©bastien Bubeck, Martin Cai, Caio C√©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzan</author><pubDate>Tue, 23 Apr 2024 15:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14219v2</guid></item><item><title>Fully Automatic Neural Network Reduction for Formal Verification</title><link>http://arxiv.org/abs/2305.01932v2</link><description>Formal verification of neural networks is essential before their deploymentin safety-critical applications. However, existing methods for formallyverifying neural networks are not yet scalable enough to handle practicalproblems involving a large number of neurons. We address this challenge byintroducing a fully automatic and sound reduction of neural networks usingreachability analysis. The soundness ensures that the verification of thereduced network entails the verification of the original network. To the bestof our knowledge, we present the first sound reduction approach that isapplicable to neural networks with any type of element-wise activationfunction, such as ReLU, sigmoid, and tanh. The network reduction is computed onthe fly while simultaneously verifying the original network and itsspecifications. All parameters are automatically tuned to minimize the networksize without compromising verifiability. We further show the applicability ofour approach to convolutional neural networks by explicitly exploiting similarneighboring pixels. Our evaluation shows that our approach can reduce thenumber of neurons to a fraction of the original number of neurons with minorouter-approximation and thus reduce the verification time to a similar degree.</description><author>Tobias Ladner, Matthias Althoff</author><pubDate>Tue, 23 Apr 2024 15:45:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01932v2</guid></item><item><title>Tenplex: Dynamic Parallelism for Deep Learning using Parallelizable Tensor Collections</title><link>http://arxiv.org/abs/2312.05181v2</link><description>Deep learning (DL) jobs use multi-dimensional parallelism, i.e. combiningdata, model, and pipeline parallelism, to use large GPU clusters efficiently.Long-running jobs may experience changes to their GPU allocation: (i) resourceelasticity during training adds or removes GPUs; (ii) hardware maintenance mayrequire redeployment on different GPUs; and (iii) GPU failures force jobs torun with fewer devices. Current DL frameworks tie jobs to a set of GPUs andthus lack support for these scenarios. In particular, they cannot change themulti-dimensional parallelism of an already-running job in an efficient andmodel-independent way. We describe Scalai, a state management library for DL systems that enablesjobs to change their parallelism dynamically after the GPU allocation isupdated at runtime. Scalai achieves this through a new abstraction, aparallelizable tensor collection (PTC), that externalizes the job state duringtraining. After a GPU change, Scalai uses the PTC to transform the job state:the PTC repartitions the dataset state under data parallelism and exposes it toDL workers through a virtual file system; and the PTC obtains the model stateas partitioned checkpoints and transforms them to reflect the newparallelization configuration. For efficiency, Scalai executes PTCtransformations in parallel with minimum data movement between workers. Ourexperiments show that Scalai enables DL jobs to support dynamic parallelizationwith low overhead.</description><author>Marcel Wagenl√§nder, Guo Li, Bo Zhao, Luo Mai, Peter Pietzuch</author><pubDate>Tue, 23 Apr 2024 15:42:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05181v2</guid></item><item><title>Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement</title><link>http://arxiv.org/abs/2402.13576v2</link><description>Video Corpus Moment Retrieval (VCMR) is a new video retrieval task aimed atretrieving a relevant moment from a large corpus of untrimmed videos using atext query. The relevance between the video and query is partial, mainlyevident in two aspects:~(1)~Scope: The untrimmed video contains many frames,but not all are relevant to the query. Strong relevance is typically observedonly within the relevant moment.~(2)~Modality: The relevance of the queryvaries with different modalities. Action descriptions align more with visualelements, while character conversations are more related to textualinformation.Existing methods often treat all video contents equally, leading tosub-optimal moment retrieval. We argue that effectively capturing the partialrelevance between the query and video is essential for the VCMR task. To thisend, we propose a Partial Relevance Enhanced Model~(PREM) to improve VCMR. VCMRinvolves two sub-tasks: video retrieval and moment localization. To align withtheir distinct objectives, we implement specialized partial relevanceenhancement strategies. For video retrieval, we introduce a multi-modalcollaborative video retriever, generating different query representations forthe two modalities by modality-specific pooling, ensuring a more effectivematch. For moment localization, we propose the focus-then-fuse momentlocalizer, utilizing modality-specific gates to capture essential content. Wealso introduce relevant content-enhanced training methods for both retrieverand localizer to enhance the ability of model to capture relevant content.Experimental results on TVR and DiDeMo datasets show that the proposed modeloutperforms the baselines, achieving a new state-of-the-art of VCMR. The codeis available at \url{https://github.com/hdy007007/PREM}.</description><author>Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng</author><pubDate>Tue, 23 Apr 2024 15:37:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13576v2</guid></item><item><title>ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations</title><link>http://arxiv.org/abs/2308.10457v6</link><description>Federated Learning (FL) is a distributed machine learning technique thatallows model training among multiple devices or organizations by sharingtraining parameters instead of raw data. However, adversaries can still inferindividual information through inference attacks (e.g. differential attacks) onthese training parameters. As a result, Differential Privacy (DP) has beenwidely used in FL to prevent such attacks. We consider differentially private federated learning in aresource-constrained scenario, where both privacy budget and communicationrounds are constrained. By theoretically analyzing the convergence, we can findthe optimal number of local DPSGD iterations for clients between any twosequential global updates. Based on this, we design an algorithm ofDifferentially Private Federated Learning with Adaptive Local Iterations(ALI-DPFL). We experiment our algorithm on the MNIST, FashionMNIST and Cifar10datasets, and demonstrate significantly better performances than previous workin the resource-constraint scenario. Code is available athttps://github.com/KnightWan/ALI-DPFL.</description><author>Xinpeng Ling, Jie Fu, Kuncan Wang, Haitao Liu, Zhili Chen</author><pubDate>Tue, 23 Apr 2024 15:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10457v6</guid></item><item><title>Hyperparameter Optimization Can Even be Harmful in Off-Policy Learning and How to Deal with It</title><link>http://arxiv.org/abs/2404.15084v1</link><description>There has been a growing interest in off-policy evaluation in the literaturesuch as recommender systems and personalized medicine. We have so far seensignificant progress in developing estimators aimed at accurately estimatingthe effectiveness of counterfactual policies based on biased logged data.However, there are many cases where those estimators are used not only toevaluate the value of decision making policies but also to search for the besthyperparameters from a large candidate space. This work explores the latterhyperparameter optimization (HPO) task for off-policy learning. We empiricallyshow that naively applying an unbiased estimator of the generalizationperformance as a surrogate objective in HPO can cause an unexpected failure,merely pursuing hyperparameters whose generalization performance is greatlyoverestimated. We then propose simple and computationally efficient correctionsto the typical HPO procedure to deal with the aforementioned issuessimultaneously. Empirical investigations demonstrate the effectiveness of ourproposed HPO algorithm in situations where the typical procedure failsseverely.</description><author>Yuta Saito, Masahiro Nomura</author><pubDate>Tue, 23 Apr 2024 15:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15084v1</guid></item><item><title>Harnessing Optical Imaging Limit through Atmospheric Scattering Media</title><link>http://arxiv.org/abs/2404.15082v1</link><description>Recording and identifying faint objects through atmospheric scattering mediaby an optical system are fundamentally interesting and technologicallyimportant. In this work, we introduce a comprehensive model that incorporatescontributions from target characteristics, atmospheric effects, imaging system,digital processing, and visual perception to assess the ultimate perceptiblelimit of geometrical imaging, specifically the angular resolution at theboundary of visible distance. The model allows to reevaluate the effectivenessof conventional imaging recording, processing, and perception and to analyzethe limiting factors that constrain image recognition capabilities inatmospheric media. The simulations were compared with the experimental resultsmeasured in a fog chamber and outdoor settings. The results reveal general goodagreement between analysis and experimental, pointing out the way to harnessingthe physical limit for optical imaging in scattering media. An immediateapplication of the study is the extension of the image range by an amount of1.2 times with noise reduction via multi-frame averaging, hence greatlyenhancing the capability of optical imaging in the atmosphere.</description><author>Libang Chen, Jun Yang, Lingye Chen, Yuyang Shui, Yikun Liu, Jianying Zhou</author><pubDate>Tue, 23 Apr 2024 15:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15082v1</guid></item><item><title>Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models</title><link>http://arxiv.org/abs/2404.15081v1</link><description>Diffusion models (DMs) embark a new era of generative modeling and offer moreopportunities for efficient generating high-quality and realistic data samples.However, their widespread use has also brought forth new challenges in modelsecurity, which motivates the creation of more effective adversarial attackerson DMs to understand its vulnerability. We propose CAAT, a simple but genericand efficient approach that does not require costly training to effectivelyfool latent diffusion models (LDMs). The approach is based on the observationthat cross-attention layers exhibits higher sensitivity to gradient change,allowing for leveraging subtle perturbations on published images tosignificantly corrupt the generated images. We show that a subtle perturbationon an image can significantly impact the cross-attention layers, thus changingthe mapping between text and image during the fine-tuning of customizeddiffusion models. Extensive experiments demonstrate that CAAT is compatiblewith diverse diffusion models and outperforms baseline attack methods in a moreeffective (more noise) and efficient (twice as fast as Anti-DreamBooth andMist) manner.</description><author>Jingyao Xu, Yuetong Lu, Yandong Li, Siyang Lu, Dongdong Wang, Xiang Wei</author><pubDate>Tue, 23 Apr 2024 15:31:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15081v1</guid></item><item><title>Dynamicity-aware Social Bot Detection with Dynamic Graph Transformers</title><link>http://arxiv.org/abs/2404.15070v1</link><description>Detecting social bots has evolved into a pivotal yet intricate task, aimed atcombating the dissemination of misinformation and preserving the authenticityof online interactions. While earlier graph-based approaches, which leveragetopological structure of social networks, yielded notable outcomes, theyoverlooked the inherent dynamicity of social networks -- In reality, theylargely depicted the social network as a static graph and solely relied on itsmost recent state. Due to the absence of dynamicity modeling, such approachesare vulnerable to evasion, particularly when advanced social bots interact withother users to camouflage identities and escape detection. To tackle thesechallenges, we propose BotDGT, a novel framework that not only considers thetopological structure, but also effectively incorporates dynamic nature ofsocial network. Specifically, we characterize a social network as a dynamicgraph. A structural module is employed to acquire topological information fromeach historical snapshot. Additionally, a temporal module is proposed tointegrate historical context and model the evolving behavior patterns exhibitedby social bots and legitimate users. Experimental results demonstrate thesuperiority of BotDGT against the leading methods that neglected the dynamicnature of social networks in terms of accuracy, recall, and F1-score.</description><author>Buyun He, Yingguang Yang, Qi Wu, Hao Liu, Renyu Yang, Hao Peng, Xiang Wang, Yong Liao, Pengyuan Zhou</author><pubDate>Tue, 23 Apr 2024 15:19:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15070v1</guid></item><item><title>Decoupling Long- and Short-Term Patterns in Spatiotemporal Inference</title><link>http://arxiv.org/abs/2109.09506v3</link><description>Sensors are the key to environmental monitoring, which impart benefits tosmart cities in many aspects, such as providing real-time air qualityinformation to assist human decision-making. However, it is impractical todeploy massive sensors due to the expensive costs, resulting in sparse datacollection. Therefore, how to get fine-grained data measurement has long been apressing issue. In this paper, we aim to infer values at non-sensor locationsbased on observations from available sensors (termed spatiotemporal inference),where capturing spatiotemporal relationships among the data plays a criticalrole. Our investigations reveal two significant insights that have not beenexplored by previous works. Firstly, data exhibits distinct patterns at bothlong- and short-term temporal scales, which should be analyzed separately.Secondly, short-term patterns contain more delicate relations including thoseacross spatial and temporal dimensions simultaneously, while long-term patternsinvolve high-level temporal trends. Based on these observations, we propose todecouple the modeling of short-term and long-term patterns. Specifically, weintroduce a joint spatiotemporal graph attention network to learn the relationsacross space and time for short-term patterns. Furthermore, we propose a graphrecurrent network with a time skip strategy to alleviate the gradient vanishingproblem and model the long-term dependencies. Experimental results on fourpublic real-world datasets demonstrate that our method effectively capturesboth long- and short-term relations, achieving state-of-the-art performanceagainst existing methods.</description><author>Junfeng Hu, Yuxuan Liang, Zhencheng Fan, Li Liu, Yifang Yin, Roger Zimmermann</author><pubDate>Tue, 23 Apr 2024 15:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.09506v3</guid></item><item><title>Enhancing Textual Personality Detection toward Social Media: Integrating Long-term and Short-term Perspectives</title><link>http://arxiv.org/abs/2404.15067v1</link><description>Textual personality detection aims to identify personality characteristics byanalyzing user-generated content toward social media platforms. Numerouspsychological literature highlighted that personality encompasses bothlong-term stable traits and short-term dynamic states. However, existingstudies often concentrate only on either long-term or short-term personalityrepresentations, without effectively combining both aspects. This limitationhinders a comprehensive understanding of individuals' personalities, as bothstable traits and dynamic states are vital. To bridge this gap, we propose aDual Enhanced Network(DEN) to jointly model users' long-term and short-termpersonality for textual personality detection. In DEN, a Long-term PersonalityEncoding is devised to effectively model long-term stable personality traits.Short-term Personality Encoding is presented to capture short-term dynamicpersonality states. The Bi-directional Interaction component facilitates theintegration of both personality aspects, allowing for a comprehensiverepresentation of the user's personality. Experimental results on twopersonality detection datasets demonstrate the effectiveness of the DEN modeland the benefits of considering both the dynamic and stable nature ofpersonality characteristics for textual personality detection.</description><author>Haohao Zhu, Xiaokun Zhang, Junyu Lu, Youlin Wu, Zewen Bai, Changrong Min, Liang Yang, Bo Xu, Dongyu Zhang, Hongfei Lin</author><pubDate>Tue, 23 Apr 2024 15:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15067v1</guid></item><item><title>Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure</title><link>http://arxiv.org/abs/2404.15065v1</link><description>Graph neural networks are becoming increasingly popular in the field ofmachine learning due to their unique ability to process data structured ingraphs. They have also been applied in safety-critical environments whereperturbations inherently occur. However, these perturbations require us toformally verify neural networks before their deployment in safety-criticalenvironments as neural networks are prone to adversarial attacks. While thereexists research on the formal verification of neural networks, there is no workverifying the robustness of generic graph convolutional network architectureswith uncertainty in the node features and in the graph structure over multiplemessage-passing steps. This work addresses this research gap by explicitlypreserving the non-convex dependencies of all elements in the underlyingcomputations through reachability analysis with (matrix) polynomial zonotopes.We demonstrate our approach on three popular benchmark datasets.</description><author>Tobias Ladner, Michael Eichelbeck, Matthias Althoff</author><pubDate>Tue, 23 Apr 2024 15:12:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15065v1</guid></item><item><title>Using deep reinforcement learning to promote sustainable human behaviour on a common pool resource problem</title><link>http://arxiv.org/abs/2404.15059v1</link><description>A canonical social dilemma arises when finite resources are allocated to agroup of people, who can choose to either reciprocate with interest, or keepthe proceeds for themselves. What resource allocation mechanisms will encouragelevels of reciprocation that sustain the commons? Here, in an iteratedmultiplayer trust game, we use deep reinforcement learning (RL) to design anallocation mechanism that endogenously promotes sustainable contributions fromhuman participants to a common pool resource. We first trained neural networksto behave like human players, creating a stimulated economy that allowed us tostudy how different mechanisms influenced the dynamics of receipt andreciprocation. We then used RL to train a social planner to maximise aggregatereturn to players. The social planner discovered a redistributive policy thatled to a large surplus and an inclusive economy, in which players made roughlyequal gains. The RL agent increased human surplus over baseline mechanismsbased on unrestricted welfare or conditional cooperation, by conditioning itsgenerosity on available resources and temporarily sanctioning defectors byallocating fewer resources to them. Examining the AI policy allowed us todevelop an explainable mechanism that performed similarly and was more popularamong players. Deep reinforcement learning can be used to discover mechanismsthat promote sustainable human behaviour.</description><author>Raphael Koster, Miruna P√Æslar, Andrea Tacchetti, Jan Balaguer, Leqi Liu, Romuald Elie, Oliver P. Hauser, Karl Tuyls, Matt Botvinick, Christopher Summerfield</author><pubDate>Tue, 23 Apr 2024 15:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15059v1</guid></item><item><title>A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI</title><link>http://arxiv.org/abs/2404.15058v1</link><description>Recent generative AI systems have demonstrated more advanced persuasivecapabilities and are increasingly permeating areas of life where they caninfluence decision-making. Generative AI presents a new risk profile ofpersuasion due the opportunity for reciprocal exchange and prolongedinteractions. This has led to growing concerns about harms from AI persuasionand how they can be mitigated, highlighting the need for a systematic study ofAI persuasion. The current definitions of AI persuasion are unclear and relatedharms are insufficiently studied. Existing harm mitigation approachesprioritise harms from the outcome of persuasion over harms from the process ofpersuasion. In this paper, we lay the groundwork for the systematic study of AIpersuasion. We first put forward definitions of persuasive generative AI. Wedistinguish between rationally persuasive generative AI, which relies onproviding relevant facts, sound reasoning, or other forms of trustworthyevidence, and manipulative generative AI, which relies on taking advantage ofcognitive biases and heuristics or misrepresenting information. We also putforward a map of harms from AI persuasion, including definitions and examplesof economic, physical, environmental, psychological, sociocultural, political,privacy, and autonomy harm. We then introduce a map of mechanisms thatcontribute to harmful persuasion. Lastly, we provide an overview of approachesthat can be used to mitigate against process harms of persuasion, includingprompt engineering for manipulation classification and red teaming. Future workwill operationalise these mitigations and study the interaction betweendifferent types of mechanisms of persuasion.</description><author>Seliem El-Sayed, Canfer Akbulut, Amanda McCroskery, Geoff Keeling, Zachary Kenton, Zaria Jalan, Nahema Marchal, Arianna Manzini, Toby Shevlane, Shannon Vallor, Daniel Susser, Matija Franklin, Sophie Bridgers, Harry Law, Matthew Rahtz, Murray Shanahan, Michael Henry Tessler, Arthur Douillard, Tom Everitt, Sasha Brown</author><pubDate>Tue, 23 Apr 2024 15:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15058v1</guid></item><item><title>On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes</title><link>http://arxiv.org/abs/2304.12477v4</link><description>Optimizing static risk-averse objectives in Markov decision processes isdifficult because they do not admit standard dynamic programming equationscommon in Reinforcement Learning (RL) algorithms. Dynamic programmingdecompositions that augment the state space with discrete risk levels haverecently gained popularity in the RL community. Prior work has shown that thesedecompositions are optimal when the risk level is discretized sufficiently.However, we show that these popular decompositions forConditional-Value-at-Risk (CVaR) and Entropic-Value-at-Risk (EVaR) areinherently suboptimal regardless of the discretization level. In particular, weshow that a saddle point property assumed to hold in prior literature may beviolated. However, a decomposition does hold for Value-at-Risk and our proofdemonstrates how this risk measure differs from CVaR and EVaR. Our findings aresignificant because risk-averse algorithms are used in high-stake environments,making their correctness much more critical.</description><author>Jia Lin Hau, Erick Delage, Mohammad Ghavamzadeh, Marek Petrik</author><pubDate>Tue, 23 Apr 2024 15:00:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12477v4</guid></item><item><title>Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm</title><link>http://arxiv.org/abs/2403.16829v2</link><description>Given a dataset of expert demonstrations, inverse reinforcement learning(IRL) aims to recover a reward for which the expert is optimal. This workproposes a model-free algorithm to solve entropy-regularized IRL problem. Inparticular, we employ a stochastic gradient descent update for the reward and astochastic soft policy iteration update for the policy. Assuming access to agenerative model, we prove that our algorithm is guaranteed to recover a rewardfor which the expert is $\varepsilon$-optimal using$\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP).Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that theoptimal policy corresponding to the recovered reward is $\varepsilon$-close tothe expert policy in total variation distance.</description><author>Titouan Renard, Andreas Schlaginhaufen, Tingting Ni, Maryam Kamgarpour</author><pubDate>Tue, 23 Apr 2024 14:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16829v2</guid></item><item><title>Multi-Head Mixture-of-Experts</title><link>http://arxiv.org/abs/2404.15045v1</link><description>Sparse Mixtures of Experts (SMoE) scales model capacity without significantincreases in training and inference costs, but exhibits the following twoissues: (1) Low expert activation, where only a small subset of experts areactivated for optimization. (2) Lacking fine-grained analytical capabilitiesfor multiple semantic concepts within individual tokens. We propose Multi-HeadMixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split eachtoken into multiple sub-tokens. These sub-tokens are then assigned to andprocessed by a diverse set of experts in parallel, and seamlessly reintegratedinto the original token form. The multi-head mechanism enables the model tocollectively attend to information from various representation spaces withindifferent experts, while significantly enhances expert activation, thus deepenscontext understanding and alleviate overfitting. Moreover, our MH-MoE isstraightforward to implement and decouples from other SMoE optimizationmethods, making it easy to integrate with other SMoE models for enhancedperformance. Extensive experimental results across three tasks: English-focusedlanguage modeling, Multi-lingual language modeling and Masked multi-modalitymodeling tasks, demonstrate the effectiveness of MH-MoE.</description><author>Xun Wu, Shaohan Huang, Wenhui Wang, Furu Wei</author><pubDate>Tue, 23 Apr 2024 14:47:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15045v1</guid></item><item><title>Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models</title><link>http://arxiv.org/abs/2403.01535v2</link><description>Graph generation has emerged as a crucial task in machine learning, withsignificant challenges in generating graphs that accurately reflect specificproperties. Existing methods often fall short in efficiently addressing thisneed as they struggle with the high-dimensional complexity and varied nature ofgraph properties. In this paper, we introduce the Neural Graph Generator (NGG),a novel approach which utilizes conditioned latent diffusion models for graphgeneration. NGG demonstrates a remarkable capacity to model complex graphpatterns, offering control over the graph generation process. NGG employs avariational graph autoencoder for graph compression and a diffusion process inthe latent vector space, guided by vectors summarizing graph statistics. Wedemonstrate NGG's versatility across various graph generation tasks, showingits capability to capture desired graph properties and generalize to unseengraphs. This work signifies a significant shift in graph generationmethodologies, offering a more practical and efficient solution for generatingdiverse types of graphs with specific characteristics.</description><author>Iakovos Evdaimon, Giannis Nikolentzos, Michail Chatzianastasis, Hadi Abdine, Michalis Vazirgiannis</author><pubDate>Tue, 23 Apr 2024 14:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01535v2</guid></item><item><title>Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order</title><link>http://arxiv.org/abs/2404.00399v2</link><description>Pretrained language models underpin several AI applications, but their highcomputational cost for training limits accessibility. Initiatives such as BLOOMand StarCoder aim to democratize access to pretrained models for collaborativecommunity development. However, such existing models face challenges: limitedmultilingual capabilities, continual pretraining causing catastrophicforgetting, whereas pretraining from scratch is computationally expensive, andcompliance with AI safety and development laws. This paper presents Aurora-M, a15B parameter multilingual open-source model trained on English, Finnish,Hindi, Japanese, Vietnamese, and code. Continually pretrained fromStarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trilliontokens in total training token count. It is the first open-source multilingualmodel fine-tuned on human-reviewed safety instructions, thus aligning itsdevelopment not only with conventional red-teaming considerations, but alsowith the specific concerns articulated in the Biden-Harris Executive Order onthe Safe, Secure, and Trustworthy Development and Use of ArtificialIntelligence. Aurora-M is rigorously evaluated across various tasks andlanguages, demonstrating robustness against catastrophic forgetting andoutperforming alternatives in multilingual settings, particularly in safetyevaluations. To promote responsible open-source LLM development, Aurora-M andits variants are released athttps://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 .</description><author>Taishi Nakamura, Mayank Mishra, Simone Tedeschi, Yekun Chai, Jason T Stillerman, Felix Friedrich, Prateek Yadav, Tanmay Laud, Vu Minh Chien, Terry Yue Zhuo, Diganta Misra, Ben Bogin, Xuan-Son Vu, Marzena Karpinska, Arnav Varma Dantuluri, Wojciech Kusa, Tommaso Furlanello, Rio Yokota, Niklas Muennighoff, Suhas Pai, Tosin Adewumi, Veronika Laippala, Xiaozhe Yao, Adalberto Junior, Alpay Ariyak, Aleksandr Drozd, Jordan Clive, Kshitij Gupta, Liangyu Chen, Qi Sun, Ken Tsui, Noah Persaud, Nour Fahmy, Tianlong Chen, Mohit Bansal, Nicolo Monti, Tai Dang, Ziyang Luo, Tien-Tung Bui, Roberto Navigli, Virendra Mehta, Matthew Blumberg, Victor May, Huu Nguyen, Sampo Pyysalo</author><pubDate>Tue, 23 Apr 2024 14:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00399v2</guid></item><item><title>Leverage Variational Graph Representation For Model Poisoning on Federated Learning</title><link>http://arxiv.org/abs/2404.15042v1</link><description>This paper puts forth a new training data-untethered model poisoning (MP)attack on federated learning (FL). The new MP attack extends an adversarialvariational graph autoencoder (VGAE) to create malicious local models basedsolely on the benign local models overheard without any access to the trainingdata of FL. Such an advancement leads to the VGAE-MP attack that is not onlyefficacious but also remains elusive to detection. VGAE-MP attack extractsgraph structural correlations among the benign local models and the trainingdata features, adversarially regenerates the graph structure, and generatesmalicious local models using the adversarial graph structure and benign models'features. Moreover, a new attacking algorithm is presented to train themalicious local models using VGAE and sub-gradient descent, while enabling anoptimal selection of the benign local models for training the VGAE. Experimentsdemonstrate a gradual drop in FL accuracy under the proposed VGAE-MP attack andthe ineffectiveness of existing defense mechanisms in detecting the attack,posing a severe threat to FL.</description><author>Kai Li, Xin Yuan, Jingjing Zheng, Wei Ni, Falko Dressler, Abbas Jamalipour</author><pubDate>Tue, 23 Apr 2024 14:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15042v1</guid></item><item><title>LEAF: Unveiling Two Sides of the Same Coin in Semi-supervised Facial Expression Recognition</title><link>http://arxiv.org/abs/2404.15041v1</link><description>Semi-supervised learning has emerged as a promising approach to tackle thechallenge of label scarcity in facial expression recognition (FER) task.However, current state-of-the-art methods primarily focus on one side of thecoin, i.e., generating high-quality pseudo-labels, while overlooking the otherside: enhancing expression-relevant representations. In this paper, we unveilboth sides of the coin by proposing a unified framework termed hierarchicaLdEcoupling And Fusing (LEAF) to coordinate expression-relevant representationsand pseudo-labels for semi-supervised FER. LEAF introduces a hierarchicalexpression-aware aggregation strategy that operates at three levels: semantic,instance, and category. (1) At the semantic and instance levels, LEAF decouplesrepresentations into expression-agnostic and expression-relevant components,and adaptively fuses them using learnable gating weights. (2) At the categorylevel, LEAF assigns ambiguous pseudo-labels by decoupling predictions intopositive and negative parts, and employs a consistency loss to ensure agreementbetween two augmented views of the same image. Extensive experiments onbenchmark datasets demonstrate that by unveiling and harmonizing both sides ofthe coin, LEAF outperforms state-of-the-art semi-supervised FER methods,effectively leveraging both labeled and unlabeled data. Moreover, the proposedexpression-aware aggregation strategy can be seamlessly integrated intoexisting semi-supervised frameworks, leading to significant performance gains.</description><author>Fan Zhang, Zhi-Qi Cheng, Jian Zhao, Xiaojiang Peng, Xuelong Li</author><pubDate>Tue, 23 Apr 2024 14:43:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15041v1</guid></item><item><title>Deferred NAM: Low-latency Top-K Context Injection via Deferred Context Encoding for Non-Streaming ASR</title><link>http://arxiv.org/abs/2404.10180v2</link><description>Contextual biasing enables speech recognizers to transcribe important phrasesin the speaker's context, such as contact names, even if they are rare in, orabsent from, the training data. Attention-based biasing is a leading approachwhich allows for full end-to-end cotraining of the recognizer and biasingsystem and requires no separate inference-time components. Such biaserstypically consist of a context encoder; followed by a context filter whichnarrows down the context to apply, improving per-step inference time; and,finally, context application via cross attention. Though much work has goneinto optimizing per-frame performance, the context encoder is at least asimportant: recognition cannot begin before context encoding ends. Here, we showthe lightweight phrase selection pass can be moved before context encoding,resulting in a speedup of up to 16.1 times and enabling biasing to scale to 20Kphrases with a maximum pre-decoding delay under 33ms. With the addition ofphrase- and wordpiece-level cross-entropy losses, our technique also achievesup to a 37.5% relative WER reduction over the baseline without the losses andlightweight phrase selection pass.</description><author>Zelin Wu, Gan Song, Christopher Li, Pat Rondon, Zhong Meng, Xavier Velez, Weiran Wang, Diamantino Caseiro, Golan Pundak, Tsendsuren Munkhdalai, Angad Chandorkar, Rohit Prabhavalkar</author><pubDate>Tue, 23 Apr 2024 14:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10180v2</guid></item><item><title>DP-Net: Learning Discriminative Parts for image recognition</title><link>http://arxiv.org/abs/2404.15037v1</link><description>This paper presents Discriminative Part Network (DP-Net), a deep architecturewith strong interpretation capabilities, which exploits a pretrainedConvolutional Neural Network (CNN) combined with a part-based recognitionmodule. This system learns and detects parts in the images that arediscriminative among categories, without the need for fine-tuning the CNN,making it more scalable than other part-based models. While part-basedapproaches naturally offer interpretable representations, we proposeexplanations at image and category levels and introduce specific constraints onthe part learning process to make them more discrimative.</description><author>Ronan Sicre, Hanwei Zhang, Julien Dejasmin, Chiheb Daaloul, St√©phane Ayache, Thierry Arti√®res</author><pubDate>Tue, 23 Apr 2024 14:42:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15037v1</guid></item><item><title>Subobject-level Image Tokenization</title><link>http://arxiv.org/abs/2402.14327v2</link><description>Transformer-based vision models typically tokenize images into fixed-sizesquare patches as input units, which lacks the adaptability to image contentand overlooks the inherent pixel grouping structure. Inspired by the subwordtokenization widely adopted in language models, we propose an image tokenizerat a subobject level, where the subobjects are represented by semanticallymeaningful image segments obtained by segmentation models (e.g., segmentanything models). To implement a learning system based on subobjecttokenization, we first introduced a Direct Segment Anything Model (DirectSAM)that efficiently produces comprehensive segmentation of subobjects, then embedsubobjects into compact latent vectors and fed them into a large language modelfor vision language learning. Empirical results demonstrated that oursubobject-level tokenization significantly facilitates efficient learning oftranslating images into object and attribute descriptions compared to thetraditional patch-level tokenization. Codes and models are open-sourced athttps://github.com/ChenDelong1999/subobjects.</description><author>Delong Chen, Samuel Cahyawijaya, Jianfeng Liu, Baoyuan Wang, Pascale Fung</author><pubDate>Tue, 23 Apr 2024 14:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14327v2</guid></item></channel></rss>