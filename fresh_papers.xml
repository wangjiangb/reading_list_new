<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 22 Jan 2025 13:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Towards Affordance-Aware Articulation Synthesis for Rigged Objects</title><link>http://arxiv.org/abs/2501.12393v1</link><description>Rigged objects are commonly used in artist pipelines, as they can flexiblyadapt to different scenes and postures. However, articulating the rigs intorealistic affordance-aware postures (e.g., following the context, respectingthe physics and the personalities of the object) remains time-consuming andheavily relies on human labor from experienced artists. In this paper, wetackle the novel problem and design A3Syn. With a given context, such as theenvironment mesh and a text prompt of the desired posture, A3Syn synthesizesarticulation parameters for arbitrary and open-domain rigged objects obtainedfrom the Internet. The task is incredibly challenging due to the lack oftraining data, and we do not make any topological assumptions about theopen-domain rigs. We propose using 2D inpainting diffusion model and severalcontrol techniques to synthesize in-context affordance information. Then, wedevelop an efficient bone correspondence alignment using a combination ofdifferentiable rendering and semantic correspondence. A3Syn has stableconvergence, completes in minutes, and synthesizes plausible affordance ondifferent combinations of in-the-wild object rigs and scenes.</description><author>Yu-Chu Yu, Chieh Hubert Lin, Hsin-Ying Lee, Chaoyang Wang, Yu-Chiang Frank Wang, Ming-Hsuan Yang</author><pubDate>Tue, 21 Jan 2025 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12393v1</guid></item><item><title>Learning segmentation from point trajectories</title><link>http://arxiv.org/abs/2501.12392v1</link><description>We consider the problem of segmenting objects in videos based on their motionand no other forms of supervision. Prior work has often approached this problemby using the principle of common fate, namely the fact that the motion ofpoints that belong to the same object is strongly correlated. However, mostauthors have only considered instantaneous motion from optical flow. In thiswork, we present a way to train a segmentation network using long-term pointtrajectories as a supervisory signal to complement optical flow. The keydifficulty is that long-term motion, unlike instantaneous motion, is difficultto model -- any parametric approximation is unlikely to capture complex motionpatterns over long periods of time. We instead draw inspiration from subspaceclustering approaches, proposing a loss function that seeks to group thetrajectories into low-rank matrices where the motion of object points can beapproximately explained as a linear combination of other point tracks. Ourmethod outperforms the prior art on motion-based segmentation, which shows theutility of long-term motion and the effectiveness of our formulation.</description><author>Laurynas Karazija, Iro Laina, Christian Rupprecht, Andrea Vedaldi</author><pubDate>Tue, 21 Jan 2025 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12392v1</guid></item><item><title>Physics of Skill Learning</title><link>http://arxiv.org/abs/2501.12391v1</link><description>We aim to understand physics of skill learning, i.e., how skills are learnedin neural networks during training. We start by observing the Domino effect,i.e., skills are learned sequentially, and notably, some skills kick offlearning right after others complete learning, similar to the sequential fallof domino cards. To understand the Domino effect and relevant behaviors ofskill learning, we take physicists' approach of abstraction and simplification.We propose three models with varying complexities -- the Geometry model, theResource model, and the Domino model, trading between reality and simplicity.The Domino effect can be reproduced in the Geometry model, whose resourceinterpretation inspires the Resource model, which can be further simplified tothe Domino model. These models present different levels of abstraction andsimplification; each is useful to study some aspects of skill learning. TheGeometry model provides interesting insights into neural scaling laws andoptimizers; the Resource model sheds light on the learning dynamics ofcompositional tasks; the Domino model reveals the benefits of modularity. Thesemodels are not only conceptually interesting -- e.g., we show how Chinchillascaling laws can emerge from the Geometry model, but also are useful inpractice by inspiring algorithmic development -- e.g., we show how simplealgorithmic changes, motivated by these toy models, can speed up the trainingof deep learning models.</description><author>Ziming Liu, Yizhou Liu, Eric J. Michaud, Jeff Gore, Max Tegmark</author><pubDate>Tue, 21 Jan 2025 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12391v1</guid></item><item><title>GPS as a Control Signal for Image Generation</title><link>http://arxiv.org/abs/2501.12390v1</link><description>We show that the GPS tags contained in photo metadata provide a usefulcontrol signal for image generation. We train GPS-to-image models and use themfor tasks that require a fine-grained understanding of how images vary within acity. In particular, we train a diffusion model to generate images conditionedon both GPS and text. The learned model generates images that capture thedistinctive appearance of different neighborhoods, parks, and landmarks. Wealso extract 3D models from 2D GPS-to-image models through score distillationsampling, using GPS conditioning to constrain the appearance of thereconstruction from each viewpoint. Our evaluations suggest that ourGPS-conditioned models successfully learn to generate images that vary based onlocation, and that GPS conditioning improves estimated 3D structure.</description><author>Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens</author><pubDate>Tue, 21 Jan 2025 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12390v1</guid></item><item><title>Taming Teacher Forcing for Masked Autoregressive Video Generation</title><link>http://arxiv.org/abs/2501.12389v1</link><description>We introduce MAGI, a hybrid video generation framework that combines maskedmodeling for intra-frame generation with causal modeling for next-framegeneration. Our key innovation, Complete Teacher Forcing (CTF), conditionsmasked frames on complete observation frames rather than masked ones (namelyMasked Teacher Forcing, MTF), enabling a smooth transition from token-level(patch-level) to frame-level autoregressive generation. CTF significantlyoutperforms MTF, achieving a +23% improvement in FVD scores on first-frameconditioned video prediction. To address issues like exposure bias, we employtargeted training strategies, setting a new benchmark in autoregressive videogeneration. Experiments show that MAGI can generate long, coherent videosequences exceeding 100 frames, even when trained on as few as 16 frames,highlighting its potential for scalable, high-quality video generation.</description><author>Deyu Zhou, Quan Sun, Yuang Peng, Kun Yan, Runpei Dong, Duomin Wang, Zheng Ge, Nan Duan, Xiangyu Zhang, Lionel M. Ni, Heung-Yeung Shum</author><pubDate>Tue, 21 Jan 2025 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12389v1</guid></item><item><title>Continuous 3D Perception Model with Persistent State</title><link>http://arxiv.org/abs/2501.12387v1</link><description>We present a unified framework capable of solving a broad range of 3D tasks.Our approach features a stateful recurrent model that continuously updates itsstate representation with each new observation. Given a stream of images, thisevolving state can be used to generate metric-scale pointmaps (per-pixel 3Dpoints) for each new input in an online fashion. These pointmaps reside withina common coordinate system, and can be accumulated into a coherent, dense scenereconstruction that updates as new images arrive. Our model, called CUT3R(Continuous Updating Transformer for 3D Reconstruction), captures rich priorsof real-world scenes: not only can it predict accurate pointmaps from imageobservations, but it can also infer unseen regions of the scene by probing atvirtual, unobserved views. Our method is simple yet highly flexible, naturallyaccepting varying lengths of images that may be either video streams orunordered photo collections, containing both static and dynamic content. Weevaluate our method on various 3D/4D tasks and demonstrate competitive orstate-of-the-art performance in each. Project Page: https://cut3r.github.io/</description><author>Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, Angjoo Kanazawa</author><pubDate>Tue, 21 Jan 2025 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12387v1</guid></item><item><title>InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling</title><link>http://arxiv.org/abs/2501.12386v1</link><description>This paper aims to improve the performance of video multimodal large languagemodels (MLLM) via long and rich context (LRC) modeling. As a result, we developa new version of InternVideo2.5 with a focus on enhancing the original MLLMs'ability to perceive fine-grained details and capture long-form temporalstructure in videos. Specifically, our approach incorporates dense vision taskannotations into MLLMs using direct preference optimization and developscompact spatiotemporal representations through adaptive hierarchical tokencompression. Experimental results demonstrate this unique design of LRC greatlyimproves the results of video MLLM in mainstream video understanding benchmarks(short &amp; long), enabling the MLLM to memorize significantly longer video inputs(at least 6x longer than the original), and master specialized visioncapabilities like object tracking and segmentation. Our work highlights theimportance of multimodal context richness (length and fineness) in empoweringMLLM's innate abilites (focus and memory), providing new insights for futureresearch on video MLLM. Code and models are available athttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5</description><author>Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Tue, 21 Jan 2025 18:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12386v1</guid></item><item><title>Audio Texture Manipulation by Exemplar-Based Analogy</title><link>http://arxiv.org/abs/2501.12385v1</link><description>Audio texture manipulation involves modifying the perceptual characteristicsof a sound to achieve specific transformations, such as adding, removing, orreplacing auditory elements. In this paper, we propose an exemplar-basedanalogy model for audio texture manipulation. Instead of conditioning ontext-based instructions, our method uses paired speech examples, where one cliprepresents the original sound and another illustrates the desiredtransformation. The model learns to apply the same transformation to new input,allowing for the manipulation of sound textures. We construct a quadrupletdataset representing various editing tasks, and train a latent diffusion modelin a self-supervised manner. We show through quantitative evaluations andperceptual studies that our model outperforms text-conditioned baselines andgeneralizes to real-world, out-of-distribution, and non-speech scenarios.Project page: https://berkeley-speech-group.github.io/audio-texture-analogy/</description><author>Kan Jen Cheng, Tingle Li, Gopala Anumanchipalli</author><pubDate>Tue, 21 Jan 2025 18:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12385v1</guid></item><item><title>CCESAR: Coastline Classification-Extraction From SAR Images Using CNN-U-Net Combination</title><link>http://arxiv.org/abs/2501.12384v1</link><description>In this article, we improve the deep learning solution for coastlineextraction from Synthetic Aperture Radar (SAR) images by proposing a two-stagemodel involving image classification followed by segmentation. We hypothesizethat a single segmentation model usually used for coastline detection isinsufficient to characterize different coastline types. We demonstrate that theneed for a two-stage workflow prevails through different compression levels ofthese images. Our results from experiments using a combination of CNN and U-Netmodels on Sentinel-1 images show that the two-stage workflow, coastlineclassification-extraction from SAR images (CCESAR) outperforms a single U-Netsegmentation model.</description><author>Vidhu Arora, Shreyan Gupta, Ananthakrishna Kudupu, Aditya Priyadarshi, Aswathi Mundayatt, Jaya Sreevalsan-Nair</author><pubDate>Tue, 21 Jan 2025 18:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12384v1</guid></item><item><title>DiffDoctor: Diagnosing Image Diffusion Models Before Treating</title><link>http://arxiv.org/abs/2501.12382v1</link><description>In spite of the recent progress, image diffusion models still produceartifacts. A common solution is to refine an established model with a qualityassessment system, which generally rates an image in its entirety. In thiswork, we believe problem-solving starts with identification, yielding therequest that the model should be aware of not just the presence of defects inan image, but their specific locations. Motivated by this, we proposeDiffDoctor, a two-stage pipeline to assist image diffusion models in generatingfewer artifacts. Concretely, the first stage targets developing a robustartifact detector, for which we collect a dataset of over 1M flawed synthesizedimages and set up an efficient human-in-the-loop annotation process,incorporating a carefully designed class-balance strategy. The learned artifactdetector is then involved in the second stage to tune the diffusion modelthrough assigning a per-pixel confidence map for each synthesis. Extensiveexperiments on text-to-image diffusion models demonstrate the effectiveness ofour artifact detector as well as the soundness of our diagnose-then-treatdesign.</description><author>Yiyang Wang, Xi Chen, Xiaogang Xu, Sihui Ji, Yu Liu, Yujun Shen, Hengshuang Zhao</author><pubDate>Tue, 21 Jan 2025 18:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12382v1</guid></item><item><title>Parallel Sequence Modeling via Generalized Spatial Propagation Network</title><link>http://arxiv.org/abs/2501.12381v1</link><description>We present the Generalized Spatial Propagation Network (GSPN), a newattention mechanism optimized for vision tasks that inherently captures 2Dspatial structures. Existing attention models, including transformers, linearattention, and state-space models like Mamba, process multi-dimensional data as1D sequences, compromising spatial coherence and efficiency. GSPN overcomesthese limitations by directly operating on spatially coherent image data andforming dense pairwise connections through a line-scan approach. Central toGSPN is the Stability-Context Condition, which ensures stable, context-awarepropagation across 2D sequences and reduces the effective sequence length to$\sqrt{N}$ for a square map with N elements, significantly enhancingcomputational efficiency. With learnable, input-dependent weights and noreliance on positional embeddings, GSPN achieves superior spatial fidelity andstate-of-the-art performance in vision tasks, including ImageNetclassification, class-guided image generation, and text-to-image generation.Notably, GSPN accelerates SD-XL with softmax-attention by over $84\times$ whengenerating 16K images.</description><author>Hongjun Wang, Wonmin Byeon, Jiarui Xu, Jinwei Gu, Ka Chun Cheung, Xiaolong Wang, Kai Han, Jan Kautz, Sifei Liu</author><pubDate>Tue, 21 Jan 2025 18:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12381v1</guid></item><item><title>MMVU: Measuring Expert-Level Multi-Discipline Video Understanding</title><link>http://arxiv.org/abs/2501.12380v1</link><description>We introduce MMVU, a comprehensive expert-level, multi-discipline benchmarkfor evaluating foundation models in video understanding. MMVU includes 3,000expert-annotated questions spanning 27 subjects across four core disciplines:Science, Healthcare, Humanities &amp; Social Sciences, and Engineering. Compared toprior benchmarks, MMVU features three key advancements. First, it challengesmodels to apply domain-specific knowledge and perform expert-level reasoning toanalyze specialized-domain videos, moving beyond the basic visual perceptiontypically assessed in current video benchmarks. Second, each example isannotated by human experts from scratch. We implement strict data qualitycontrols to ensure the high quality of the dataset. Finally, each example isenriched with expert-annotated reasoning rationals and relevant domainknowledge, facilitating in-depth analysis. We conduct an extensive evaluationof 32 frontier multimodal foundation models on MMVU. The latestSystem-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highestperformance among the tested models. However, they still fall short of matchinghuman expertise. Through in-depth error analyses and case studies, we offeractionable insights for future advancements in expert-level,knowledge-intensive video understanding for specialized domains.</description><author>Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, Arman Cohan</author><pubDate>Tue, 21 Jan 2025 18:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12380v1</guid></item><item><title>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</title><link>http://arxiv.org/abs/2501.12375v1</link><description>Depth Anything has achieved remarkable success in monocular depth estimationwith strong generalization ability. However, it suffers from temporalinconsistency in videos, hindering its practical applications. Various methodshave been proposed to alleviate this issue by leveraging video generationmodels or introducing priors from optical flow and camera poses. Nonetheless,these methods are only applicable to short videos (&lt; 10 seconds) and require atrade-off between quality and computational efficiency. We propose Video DepthAnything for high-quality, consistent depth estimation in super-long videos(over several minutes) without sacrificing efficiency. We base our model onDepth Anything V2 and replace its head with an efficient spatial-temporal head.We design a straightforward yet effective temporal consistency loss byconstraining the temporal depth gradient, eliminating the need for additionalgeometric priors. The model is trained on a joint dataset of video depth andunlabeled images, similar to Depth Anything V2. Moreover, a novelkey-frame-based strategy is developed for long video inference. Experimentsshow that our model can be applied to arbitrarily long videos withoutcompromising quality, consistency, or generalization ability. Comprehensiveevaluations on multiple video benchmarks demonstrate that our approach sets anew state-of-the-art in zero-shot video depth estimation. We offer models ofdifferent scales to support a range of scenarios, with our smallest modelcapable of real-time performance at 30 FPS.</description><author>Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang</author><pubDate>Tue, 21 Jan 2025 18:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12375v1</guid></item><item><title>Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists</title><link>http://arxiv.org/abs/2501.12374v1</link><description>Novel capacities of generative AI to analyze and generate cultural artifactsraise inevitable questions about the nature and value of artistic education andhuman expertise. Has AI already leveled the playing field between professionalartists and laypeople, or do trained artistic expressive capacity, curationskills and experience instead enhance the ability to use these new tools? Inthis pre-registered study, we conduct experimental comparisons between 50active artists and a demographically matched sample of laypeople. We designedtwo tasks to approximate artistic practice for testing their capabilities inboth faithful and creative image creation: replicating a reference image, andmoving as far away as possible from it. We developed a bespoke platform whereparticipants used a modern text-to-image model to complete both tasks. We alsocollected and compared participants' sentiments towards AI. On average, artistsproduced more faithful and creative outputs than their lay counterparts,although only by a small margin. While AI may ease content creation,professional expertise is still valuable - even within the confined space ofgenerative AI itself. Finally, we also explored how well an exemplaryvision-capable large language model (GPT-4o) would complete the same tasks, ifgiven the role of an image generation agent, and found it performed on par incopying but outperformed even artists in the creative task. The very bestresults were still produced by humans in both tasks. These outcomes highlightthe importance of integrating artistic skills with AI training to prepareartists and other visual professionals for a technologically evolvinglandscape. We see a potential in collaborative synergy with generative AI,which could reshape creative industries and education in the arts.</description><author>Thomas F. Eisenmann, Andres Karjus, Mar Canet Sola, Levin Brinkmann, Bramantyo Ibrahim Supriyatno, Iyad Rahwan</author><pubDate>Tue, 21 Jan 2025 18:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12374v1</guid></item><item><title>Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL</title><link>http://arxiv.org/abs/2501.12372v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities acrossa range of natural language processing tasks. In particular, improvements inreasoning abilities and the expansion of context windows have opened newavenues for leveraging these powerful models. NL2SQL is challenging in that thenatural language question is inherently ambiguous, while the SQL generationrequires a precise understanding of complex data schema and semantics. Oneapproach to this semantic ambiguous problem is to provide more and sufficientcontextual information. In this work, we explore the performance and the latency trade-offs of theextended context window (a.k.a., long context) offered by Google'sstate-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of variouscontextual information, including column example values, question and SQL querypairs, user-provided hints, SQL documentation, and schema. To the best of ourknowledge, this is the first work to study how the extended context window andextra contextual information can help NL2SQL generation with respect to bothaccuracy and latency cost. We show that long context LLMs are robust and do notget lost in the extended contextual information. Additionally, our long-contextNL2SQL pipeline based on Google's \textit{gemini-pro-1.5} achieve a strongperformance with 67.41\% on BIRD benchmark (dev) without finetuning andexpensive self-consistency based techniques.</description><author>Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan</author><pubDate>Tue, 21 Jan 2025 18:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12372v1</guid></item><item><title>Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models</title><link>http://arxiv.org/abs/2501.12370v1</link><description>Scaling the capacity of language models has consistently proven to be areliable approach for improving performance and unlocking new capabilities.Capacity can be primarily defined by two dimensions: the number of modelparameters and the compute per example. While scaling typically involvesincreasing both, the precise interplay between these factors and their combinedcontribution to overall capacity remains not fully understood. We explore thisrelationship in the context of sparse Mixture-of-Expert models (MoEs), whichallow scaling the number of parameters without proportionally increasing theFLOPs per example. We investigate how varying the sparsity level, i.e., theratio of non-active to total parameters, affects model performance in terms ofboth pretraining and downstream performance. We find that under differentconstraints (e.g. parameter size and total training compute), there is anoptimal level of sparsity that improves both training efficiency and modelperformance. These results provide a better understanding of the impact ofsparsity in scaling laws for MoEs and complement existing works in this area,offering insights for designing more efficient architectures.</description><author>Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Mohamed Elnouby Ali, Josh Susskind, Vimal Thilak</author><pubDate>Tue, 21 Jan 2025 18:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12370v1</guid></item><item><title>DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial Basis Functions</title><link>http://arxiv.org/abs/2501.12369v1</link><description>Splatting-based 3D reconstruction methods have gained popularity with theadvent of 3D Gaussian Splatting, efficiently synthesizing high-quality novelviews. These methods commonly resort to using exponential family functions,such as the Gaussian function, as reconstruction kernels due to theiranisotropic nature, ease of projection, and differentiability in rasterization.However, the field remains restricted to variations within the exponentialfamily, leaving generalized reconstruction kernels largely underexplored,partly due to the lack of easy integrability in 3D to 2D projections. In thislight, we show that a class of decaying anisotropic radial basis functions(DARBFs), which are non-negative functions of the Mahalanobis distance,supports splatting by approximating the Gaussian function's closed-formintegration advantage. With this fresh perspective, we demonstrate up to 34%faster convergence during training and a 15% reduction in memory consumptionacross various DARB reconstruction kernels, while maintaining comparable PSNR,SSIM, and LPIPS results. We will make the code available.</description><author>Vishagar Arunan, Saeedha Nazar, Hashiru Pramuditha, Vinasirajan Viruthshaan, Sameera Ramasinghe, Simon Lucey, Ranga Rodrigo</author><pubDate>Tue, 21 Jan 2025 18:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12369v1</guid></item><item><title>InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model</title><link>http://arxiv.org/abs/2501.12368v1</link><description>Despite the promising performance of Large Vision Language Models (LVLMs) invisual understanding, they occasionally generate incorrect outputs. Whilereward models (RMs) with reinforcement learning or test-time scaling offer thepotential for improving generation quality, a critical gap remains: publiclyavailable multi-modal RMs for LVLMs are scarce, and the implementation detailsof proprietary models are often unclear. We bridge this gap withInternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effectivemulti-modal reward model that aligns LVLMs with human preferences. To ensurethe robustness and versatility of IXC-2.5-Reward, we set up a high-qualitymulti-modal preference corpus spanning text, image, and video inputs acrossdiverse domains, such as instruction following, general understanding,text-rich documents, mathematical reasoning, and video understanding.IXC-2.5-Reward achieves excellent results on the latest multi-modal rewardmodel benchmark and shows competitive performance on text-only reward modelbenchmarks. We further demonstrate three key applications of IXC-2.5-Reward:(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Rewardwith Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which showsconsistent improvements in instruction following and multi-modal open-endeddialogue; (2) Selecting the best response from candidate responses fortest-time scaling; and (3) Filtering outlier or noisy samples from existingimage and video instruction tuning training data. To ensure reproducibility andfacilitate further research, we have open-sourced all model weights andtraining recipes at https://github.com/InternLM/InternLM-XComposer</description><author>Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang</author><pubDate>Tue, 21 Jan 2025 18:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12368v1</guid></item><item><title>FoundationStereo: Zero-Shot Stereo Matching</title><link>http://arxiv.org/abs/2501.09898v2</link><description>Tremendous progress has been made in deep stereo matching to excel onbenchmark datasets through per-domain fine-tuning. However, achieving strongzero-shot generalization - a hallmark of foundation models in other computervision tasks - remains challenging for stereo matching. We introduceFoundationStereo, a foundation model for stereo depth estimation designed toachieve strong zero-shot generalization. To this end, we first construct alarge-scale (1M stereo pairs) synthetic training dataset featuring largediversity and high photorealism, followed by an automatic self-curationpipeline to remove ambiguous samples. We then design a number of networkarchitecture components to enhance scalability, including a side-tuning featurebackbone that adapts rich monocular priors from vision foundation models tomitigate the sim-to-real gap, and long-range context reasoning for effectivecost volume filtering. Together, these components lead to strong robustness andaccuracy across domains, establishing a new standard in zero-shot stereo depthestimation. Project page: https://nvlabs.github.io/FoundationStereo/</description><author>Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield</author><pubDate>Tue, 21 Jan 2025 18:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09898v2</guid></item><item><title>Budget-constrained Collaborative Renewable Energy Forecasting Market</title><link>http://arxiv.org/abs/2501.12367v1</link><description>Accurate power forecasting from renewable energy sources (RES) is crucial forintegrating additional RES capacity into the power system and realizingsustainability goals. This work emphasizes the importance of integratingdecentralized spatio-temporal data into forecasting models. However,decentralized data ownership presents a critical obstacle to the success ofsuch spatio-temporal models, and incentive mechanisms to foster data-sharingneed to be considered. The main contributions are a) a comparative analysis ofthe forecasting models, advocating for efficient and interpretable spline LASSOregression models, and b) a bidding mechanism within the data/analytics marketto ensure fair compensation for data providers and enable both buyers andsellers to express their data price requirements. Furthermore, an incentivemechanism for time series forecasting is proposed, effectively incorporatingprice constraints and preventing redundant feature allocation. Results showsignificant accuracy improvements and potential monetary gains for datasellers. For wind power data, an average root mean squared error improvement ofover 10% was achieved by comparing forecasts generated by the proposal withlocally generated ones.</description><author>Carla Goncalves, Ricardo J. Bessa, Tiago Teixeira, Joao Vinagre</author><pubDate>Tue, 21 Jan 2025 18:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12367v1</guid></item><item><title>Efficient Algorithm for Sparse Fourier Transform of Generalized q-ary Functions</title><link>http://arxiv.org/abs/2501.12365v1</link><description>Computing the Fourier transform of a $q$-ary function$f:\mathbb{Z}_{q}^n\rightarrow \mathbb{R}$, which maps $q$-ary sequences toreal numbers, is an important problem in mathematics with wide-rangingapplications in biology, signal processing, and machine learning. Previousstudies have shown that, under the sparsity assumption, the Fourier transformcan be computed efficiently using fast and sample-efficient algorithms.However, in many practical settings, the function is defined over a moregeneral space -- the space of generalized $q$-ary sequences $\mathbb{Z}_{q_1}\times \mathbb{Z}_{q_2} \times \cdots \times \mathbb{Z}_{q_n}$ -- where each$\mathbb{Z}_{q_i}$ corresponds to integers modulo $q_i$. A naive approachinvolves setting $q=\max_i{q_i}$ and treating the function as $q$-ary, whichresults in heavy computational overheads. Herein, we develop GFast, analgorithm that computes the $S$-sparse Fourier transform of $f$ with a samplecomplexity of $O(Sn)$, computational complexity of $O(Sn \log N)$, and afailure probability that approaches zero as $N=\prod_{i=1}^n q_i \rightarrow\infty$ with $S = N^\delta$ for some $0 \leq \delta &lt; 1$. In the presence ofnoise, we further demonstrate that a robust version of GFast computes thetransform with a sample complexity of $O(Sn^2)$ and computational complexity of$O(Sn^2 \log N)$ under the same high probability guarantees. Using large-scalesynthetic experiments, we demonstrate that GFast computes the sparse Fouriertransform of generalized $q$-ary functions using $16\times$ fewer samples andrunning $8\times$ faster than existing algorithms. In real-world proteinfitness datasets, GFast explains the predictive interactions of a neuralnetwork with $&gt;25\%$ smaller normalized mean-squared error compared to existingalgorithms.</description><author>Darin Tsui, Kunal Talreja, Amirali Aghazadeh</author><pubDate>Tue, 21 Jan 2025 18:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12365v1</guid></item><item><title>The Choice of Normalization Influences Shrinkage in Regularized Regression</title><link>http://arxiv.org/abs/2501.03821v2</link><description>Regularized models are often sensitive to the scales of the features in thedata and it has therefore become standard practice to normalize (center andscale) the features before fitting the model. But there are many different waysto normalize the features and the choice may have dramatic effects on theresulting model. In spite of this, there has so far been no research on thistopic. In this paper, we begin to bridge this knowledge gap by studyingnormalization in the context of lasso, ridge, and elastic net regression. Wefocus on normal and binary features and show that the class balances of binaryfeatures directly influences the regression coefficients and that this effectdepends on the combination of normalization and regularization methods used. Wedemonstrate that this effect can be mitigated by scaling binary features withtheir variance in the case of the lasso and standard deviation in the case ofridge regression, but that this comes at the cost of increased variance. Forthe elastic net, we show that scaling the penalty weights, rather than thefeatures, can achieve the same effect. Finally, we also tackle mixes of binaryand normal features as well as interactions and provide some initial results onhow to normalize features in these cases.</description><author>Johan Larsson, Jonas Wallin</author><pubDate>Tue, 21 Jan 2025 18:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.03821v2</guid></item><item><title>Measured Hockey-Stick Divergence and its Applications to Quantum Pufferfish Privacy</title><link>http://arxiv.org/abs/2501.12359v1</link><description>The hockey-stick divergence is a fundamental quantity characterizing severalstatistical privacy frameworks that ensure privacy for classical and quantumdata. In such quantum privacy frameworks, the adversary is allowed to performall possible measurements. However, in practice, there are typicallylimitations to the set of measurements that can be performed. To this end,here, we comprehensively analyze the measured hockey-stick divergence underseveral classes of practically relevant measurement classes. We prove severalof its properties, including data processing and convexity. We show that it isefficiently computable by semi-definite programming for some classes ofmeasurements and can be analytically evaluated for Werner and isotropic states.Notably, we show that the measured hockey-stick divergence characterizesoptimal privacy parameters in the quantum pufferfish privacy framework. Withthis connection and the developed technical tools, we enable methods toquantify and audit privacy for several practically relevant settings. Lastly,we introduce the measured hockey-stick divergence of channels and explore itsapplications in ensuring privacy for channels.</description><author>Theshani Nuradha, Vishal Singh, Mark M. Wilde</author><pubDate>Tue, 21 Jan 2025 18:39:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12359v1</guid></item><item><title>Vision-Language Models for Automated Chest X-ray Interpretation: Leveraging ViT and GPT-2</title><link>http://arxiv.org/abs/2501.12356v1</link><description>Radiology plays a pivotal role in modern medicine due to its non-invasivediagnostic capabilities. However, the manual generation of unstructured medicalreports is time consuming and prone to errors. It creates a significantbottleneck in clinical workflows. Despite advancements in AI-generatedradiology reports, challenges remain in achieving detailed and accurate reportgeneration. In this study we have evaluated different combinations ofmultimodal models that integrate Computer Vision and Natural LanguageProcessing to generate comprehensive radiology reports. We employed apretrained Vision Transformer (ViT-B16) and a SWIN Transformer as the imageencoders. The BART and GPT-2 models serve as the textual decoders. We usedChest X-ray images and reports from the IU-Xray dataset to evaluate theusability of the SWIN Transformer-BART, SWIN Transformer-GPT-2, ViT-B16-BARTand ViT-B16-GPT-2 models for report generation. We aimed at finding the bestcombination among the models. The SWIN-BART model performs as thebest-performing model among the four models achieving remarkable results inalmost all the evaluation metrics like ROUGE, BLEU and BERTScore.</description><author>Md. Rakibul Islam, Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu</author><pubDate>Tue, 21 Jan 2025 18:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12356v1</guid></item><item><title>Diffusion-aware Censored Gaussian Processes for Demand Modelling</title><link>http://arxiv.org/abs/2501.12354v1</link><description>Inferring the true demand for a product or a service from aggregate data isoften challenging due to the limited available supply, thus resulting inobservations that are censored and correspond to the realized demand, therebynot accounting for the unsatisfied demand. Censored regression models are ableto account for the effect of censoring due to the limited supply, but theydon't consider the effect of substitutions, which may cause the demand forsimilar alternative products or services to increase. This paper proposesDiffusion-aware Censored Demand Models, which combine a Tobit likelihood with agraph diffusion process in order to model the latent process of transfer ofunsatisfied demand between similar products or services. We instantiate thisnew class of models under the framework of GPs and, based on both simulated andreal-world data for modeling sales, bike-sharing demand, and EV chargingdemand, demonstrate its ability to better recover the true demand and producemore accurate out-of-sample predictions.</description><author>Filipe Rodrigues</author><pubDate>Tue, 21 Jan 2025 18:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12354v1</guid></item><item><title>Test-time regression: a unifying framework for designing sequence models with associative memory</title><link>http://arxiv.org/abs/2501.12352v1</link><description>Sequences provide a remarkably general way to represent and processinformation. This powerful abstraction has placed sequence modeling at thecenter of modern deep learning applications, inspiring numerous architecturesfrom transformers to recurrent networks. While this fragmented development hasyielded powerful models, it has left us without a unified framework tounderstand their fundamental similarities and explain their effectiveness. Wepresent a unifying framework motivated by an empirical observation: effectivesequence models must be able to perform associative recall. Our key insight isthat memorizing input tokens through an associative memory is equivalent toperforming regression at test-time. This regression-memory correspondenceprovides a framework for deriving sequence models that can perform associativerecall, offering a systematic lens to understand seemingly ad-hoc architecturalchoices. We show numerous recent architectures -- including linear attentionmodels, their gated variants, state-space models, online learners, and softmaxattention -- emerge naturally as specific approaches to test-time regression.Each architecture corresponds to three design choices: the relative importanceof each association, the regressor function class, and the optimizationalgorithm. This connection leads to new understanding: we provide theoreticaljustification for QKNorm in softmax attention, and we motivate higher-ordergeneralizations of softmax attention. Beyond unification, our work unlocksdecades of rich statistical tools that can guide future development of morepowerful yet principled sequence models.</description><author>Ke Alexander Wang, Jiaxin Shi, Emily B. Fox</author><pubDate>Tue, 21 Jan 2025 18:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12352v1</guid></item><item><title>CYCle: Choosing Your Collaborators Wisely to Enhance Collaborative Fairness in Decentralized Learning</title><link>http://arxiv.org/abs/2501.12344v1</link><description>Collaborative learning (CL) enables multiple participants to jointly trainmachine learning (ML) models on decentralized data sources without raw datasharing. While the primary goal of CL is to maximize the expected accuracy gainfor each participant, it is also important to ensure that the gains are fairlydistributed. Specifically, no client should be negatively impacted by thecollaboration, and the individual gains must ideally be commensurate with thecontributions. Most existing CL algorithms require central coordination andfocus on the gain maximization objective while ignoring collaborative fairness.In this work, we first show that the existing measure of collaborative fairnessbased on the correlation between accuracy values without and with collaborationhas drawbacks because it does not account for negative collaboration gain. Weargue that maximizing mean collaboration gain (MCG) while simultaneouslyminimizing the collaboration gain spread (CGS) is a fairer alternative. Next,we propose the CYCle protocol that enables individual participants in a privatedecentralized learning (PDL) framework to achieve this objective through anovel reputation scoring method based on gradient alignment between the localcross-entropy and distillation losses. Experiments on the CIFAR-10, CIFAR-100,and Fed-ISIC2019 datasets empirically demonstrate the effectiveness of theCYCle protocol to ensure positive and fair collaboration gain for allparticipants, even in cases where the data distributions of participants arehighly skewed. For the simple mean estimation problem with two participants, wealso theoretically show that CYCle performs better than standard FedAvg,especially when there is large statistical heterogeneity.</description><author>Nurbek Tastan, Samuel Horvath, Karthik Nandakumar</author><pubDate>Tue, 21 Jan 2025 18:22:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12344v1</guid></item><item><title>Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens</title><link>http://arxiv.org/abs/2410.14655v2</link><description>Language models are often trained to maximize the likelihood of the nexttoken given past tokens in the training dataset. However, during inferencetime, they are utilized differently, generating text sequentially andauto-regressively by using previously generated tokens as input to predict thenext one. Marginal differences in predictions at each step can cascade oversuccessive steps, resulting in different distributions from what the modelswere trained for and potentially leading to unpredictable behavior. This paperproposes two simple approaches based on model own generation to address thisdiscrepancy between the training and inference time. Our first approach isBatch-Scheduled Sampling, where, during training, we stochastically choosebetween the ground-truth token from the dataset and the model's own generatedtoken as input to predict the next token. This is done in an offline manner,modifying the context window by interleaving ground-truth tokens with thosegenerated by the model. Our second approach is Reference-Answer-basedCorrection, where we explicitly incorporate a self-correction capability intothe model during training. This enables the model to effectively self-correctthe gaps between the generated sequences and the ground truth data withoutrelying on an external oracle model. By incorporating our proposed strategiesduring training, we have observed an overall improvement in performancecompared to baseline methods, as demonstrated by our extensive experimentsusing summarization, general question-answering, and math question-answeringtasks.</description><author>Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhari, Huzefa Rangwala, George Karypis, Rasool Fakoor</author><pubDate>Tue, 21 Jan 2025 18:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14655v2</guid></item><item><title>Treefix: Enabling Execution with a Tree of Prefixes</title><link>http://arxiv.org/abs/2501.12339v1</link><description>The ability to execute code is a prerequisite for various dynamic programanalyses. Learning-guided execution has been proposed as an approach to enablethe execution of arbitrary code snippets by letting a neural model predictlikely values for any missing variables. Although state-of-the-artlearning-guided execution approaches, such as LExecutor, can enable theexecution of a relative high amount of code, they are limited to predicting arestricted set of possible values and do not use any feedback from previousexecutions to execute even more code. This paper presents Treefix, a novellearning-guided execution approach that leverages LLMs to iteratively createcode prefixes that enable the execution of a given code snippet. The approachaddresses the problem in a multi-step fashion, where each step uses feedbackabout the code snippet and its execution to instruct an LLM to improve apreviously generated prefix. This process iteratively creates a tree ofprefixes, a subset of which is returned to the user as prefixes that maximizethe number of executed lines in the code snippet. In our experiments with twodatasets of Python code snippets, Treefix achieves 25% and 7% more coveragerelative to the current state of the art in learning-guided execution, coveringa total of 84% and 82% of all lines in the code snippets.</description><author>Beatriz Souza, Michael Pradel</author><pubDate>Tue, 21 Jan 2025 18:13:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12339v1</guid></item><item><title>FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression</title><link>http://arxiv.org/abs/2501.12336v1</link><description>This paper presents results of our system for CoMeDi Shared Task, focusing onSubtask 2: Disagreement Ranking. Our system leverages sentence embeddingsgenerated by the paraphrase-xlm-r-multilingual-v1 model, combined with a deepneural regression model incorporating batch normalization and dropout forimproved generalization. By predicting the mean of pairwise judgmentdifferences between annotators, our method explicitly targets disagreementranking, diverging from traditional "gold label" aggregation approaches. Weoptimized our system with a customized architecture and training procedure,achieving competitive performance in Spearman correlation against meandisagreement labels. Our results highlight the importance of robust embeddings,effective model architecture, and careful handling of judgment differences forranking disagreement in multilingual contexts. These findings provide insightsinto the use of contextualized representations for ordinal judgment tasks andopen avenues for further refinement of disagreement prediction models.</description><author>Phuoc Duong Huy Chu</author><pubDate>Tue, 21 Jan 2025 18:10:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12336v1</guid></item><item><title>Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration</title><link>http://arxiv.org/abs/2501.12332v1</link><description>Acquiring labelled training data remains a costly task in real world machinelearning projects to meet quantity and quality requirements. Recently LargeLanguage Models (LLMs), notably GPT-4, have shown great promises in labellingdata with high accuracy. However, privacy and cost concerns prevent theubiquitous use of GPT-4. In this work, we explore effectively leveragingopen-source models for automatic labelling. We identify integrating labelschema as a promising technology but found that naively using the labeldescription for classification leads to poor performance on high cardinalitytasks. To address this, we propose Retrieval Augmented Classification (RAC) forwhich LLM performs inferences for one label at a time using corresponding labelschema; we start with the most related label and iterates until a label ischosen by the LLM. We show that our method, which dynamically integrates labeldescription, leads to performance improvements in labelling tasks. We furthershow that by focusing only on the most promising labels, RAC can trade offbetween label quality and coverage - a property we leverage to automaticallylabel our internal datasets.</description><author>Thomas Walshe, Sae Young Moon, Chunyang Xiao, Yawwani Gunawardana, Fran Silavong</author><pubDate>Tue, 21 Jan 2025 18:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12332v1</guid></item><item><title>$\spadesuit$ SPADE $\spadesuit$ Split Peak Attention DEcomposition</title><link>http://arxiv.org/abs/2411.05852v2</link><description>Demand forecasting faces challenges induced by Peak Events (PEs)corresponding to special periods such as promotions and holidays. Peak eventscreate significant spikes in demand followed by demand ramp down periods.Neural networks like MQCNN and MQT overreact to demand peaks by carrying overthe elevated PE demand into subsequent Post-Peak-Event (PPE) periods, resultingin significantly over-biased forecasts. To tackle this challenge, we introducea neural forecasting model called Split Peak Attention DEcomposition, SPADE.This model reduces the impact of PEs on subsequent forecasts by modelingforecasting as consisting of two separate tasks: one for PEs; and the other forthe rest. Its architecture then uses masked convolution filters and aspecialized Peak Attention module. We show SPADE's performance on a worldwideretail dataset with hundreds of millions of products. Our results reveal anoverall PPE improvement of 4.5%, a 30% improvement for most affected forecastsafter promotions and holidays, and an improvement in PE accuracy by 3.9%,relative to current production models.</description><author>Malcolm Wolff, Kin G. Olivares, Boris Oreshkin, Sunny Ruan, Sitan Yang, Abhinav Katoch, Shankar Ramasubramanian, Youxin Zhang, Michael W. Mahoney, Dmitry Efimov, Vincent Quenneville-Blair</author><pubDate>Tue, 21 Jan 2025 18:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05852v2</guid></item><item><title>Cinepro: Robust Training of Foundation Models for Cancer Detection in Prostate Ultrasound Cineloops</title><link>http://arxiv.org/abs/2501.12331v1</link><description>Prostate cancer (PCa) detection using deep learning (DL) models has shownpotential for enhancing real-time guidance during biopsies. However, prostateultrasound images lack pixel-level cancer annotations, introducing label noise.Current approaches often focus on limited regions of interest (ROIs),disregarding anatomical context necessary for accurate diagnosis. Foundationmodels can overcome this limitation by analyzing entire images to captureglobal spatial relationships; however, they still encounter challenges stemmingfrom the weak labels associated with coarse pathology annotations in ultrasounddata. We introduce Cinepro, a novel framework that strengthens foundationmodels' ability to localize PCa in ultrasound cineloops. Cinepro adapts robusttraining by integrating the proportion of cancer tissue reported by pathologyin a biopsy core into its loss function to address label noise, providing amore nuanced supervision. Additionally, it leverages temporal data acrossmultiple frames to apply robust augmentations, enhancing the model's ability tolearn stable cancer-related features. Cinepro demonstrates superior performanceon a multi-center prostate ultrasound dataset, achieving an AUROC of 77.1% anda balanced accuracy of 83.8%, surpassing current benchmarks. These findingsunderscore Cinepro's promise in advancing foundation models for weakly labeledultrasound data.</description><author>Mohamed Harmanani, Amoon Jamzad, Minh Nguyen Nhat To, Paul F. R. Wilson, Zhuoxin Guo, Fahimeh Fooladgar, Samira Sojoudi, Mahdi Gilany, Silvia Chang, Peter Black, Michael Leveridge, Robert Siemens, Purang Abolmaesumi, Parvin Mousavi</author><pubDate>Tue, 21 Jan 2025 18:05:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12331v1</guid></item><item><title>The Gap Between Principle and Practice of Lossy Image Coding</title><link>http://arxiv.org/abs/2501.12330v1</link><description>Lossy image coding is the art of computing that is principally bounded by theimage's rate-distortion function. This bound, though never accuratelycharacterized, has been approached practically via deep learning technologiesin recent years. Indeed, learned image coding schemes allow direct optimizationof the joint rate-distortion cost, thereby outperforming the handcrafted imagecoding schemes by a large margin. Still, it is observed that there is room forfurther improvement in the rate-distortion performance of learned image coding.In this article, we identify the gap between the ideal rate-distortion functionforecasted by Shannon's information theory and the empirical rate-distortionfunction achieved by the state-of-the-art learned image coding schemes,revealing that the gap is incurred by five different effects: modeling effect,approximation effect, amortization effect, digitization effect, and asymptoticeffect. We design simulations and experiments to quantitively evaluate the lastthree effects, which demonstrates the high potential of future lossy imagecoding technologies.</description><author>Haotian Zhang, Dong Liu</author><pubDate>Tue, 21 Jan 2025 17:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12330v1</guid></item><item><title>Beyond Position: the emergence of wavelet-like properties in Transformers</title><link>http://arxiv.org/abs/2410.18067v3</link><description>This paper studies how transformer models develop robust wavelet-likeproperties that effectively compensate for the theoretical limitations ofRotary Position Embeddings (RoPE), providing insights into how these networksprocess sequential information across different scales. Through theoreticalanalysis and empirical validation across models ranging from 1B to 12Bparameters, we show that attention heads naturally evolve to implementmulti-resolution processing analogous to wavelet transforms. Our analysisestablishes that attention heads consistently organize into complementaryfrequency bands with systematic power distribution patterns, and thesewavelet-like characteristics become more pronounced in larger models. Weprovide mathematical analysis showing how these properties align with optimalsolutions to the fundamental uncertainty principle between positional precisionand frequency resolution. Our findings suggest that the effectiveness of moderntransformer architectures stems significantly from their development of optimalmulti-resolution decompositions that naturally address the theoreticalconstraints of position encoding.</description><author>Valeria Ruscio, Fabrizio Silvestri</author><pubDate>Tue, 21 Jan 2025 17:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18067v3</guid></item><item><title>VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model</title><link>http://arxiv.org/abs/2501.12327v1</link><description>We present VARGPT, a novel multimodal large language model (MLLM) thatunifies visual understanding and generation within a single autoregressiveframework. VARGPT employs a next-token prediction paradigm for visualunderstanding and a next-scale prediction paradigm for visual autoregressivegeneration. VARGPT innovatively extends the LLaVA architecture, achievingefficient scale-wise autoregressive visual generation within MLLMs whileseamlessly accommodating mixed-modal input and output within a single modelframework. Our VARGPT undergoes a three-stage unified training process onspecially curated datasets, comprising a pre-training phase and two mixedvisual instruction-tuning phases. The unified training strategy are designed toachieve alignment between visual and textual features, enhance instructionfollowing for both understanding and generation, and improve visual generationquality, respectively. Despite its LLAVA-based architecture for multimodelunderstanding, VARGPT significantly outperforms LLaVA-1.5 across variousvision-centric benchmarks, such as visual question-answering and reasoningtasks. Notably, VARGPT naturally supports capabilities in autoregressive visualgeneration and instruction-to-image synthesis, showcasing its versatility inboth visual understanding and generation tasks. Project page is at:\url{https://vargpt-1.github.io/}</description><author>Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou</author><pubDate>Tue, 21 Jan 2025 17:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12327v1</guid></item><item><title>UI-TARS: Pioneering Automated GUI Interaction with Native Agents</title><link>http://arxiv.org/abs/2501.12326v1</link><description>This paper introduces UI-TARS, a native GUI agent model that solely perceivesthe screenshots as input and performs human-like interactions (e.g., keyboardand mouse operations). Unlike prevailing agent frameworks that depend onheavily wrapped commercial models (e.g., GPT-4o) with expert-crafted promptsand workflows, UI-TARS is an end-to-end model that outperforms thesesophisticated frameworks. Experiments demonstrate its superior performance:UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluatingperception, grounding, and GUI task execution. Notably, in the OSWorldbenchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates severalkey innovations: (1) Enhanced Perception: leveraging a large-scale dataset ofGUI screenshots for context-aware understanding of UI elements and precisecaptioning; (2) Unified Action Modeling, which standardizes actions into aunified space across platforms and achieves precise grounding and interactionthrough large-scale action traces; (3) System-2 Reasoning, which incorporatesdeliberate reasoning into multi-step decision making, involving multiplereasoning patterns such as task decomposition, reflection thinking, milestonerecognition, etc. (4) Iterative Training with Reflective Online Traces, whichaddresses the data bottleneck by automatically collecting, filtering, andreflectively refining new interaction traces on hundreds of virtual machines.Through iterative training and reflection tuning, UI-TARS continuously learnsfrom its mistakes and adapts to unforeseen situations with minimal humanintervention. We also analyze the evolution path of GUI agents to guide thefurther development of this domain.</description><author>Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, Guang Shi</author><pubDate>Tue, 21 Jan 2025 17:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12326v1</guid></item><item><title>Let There Be Light: Robust Lensless Imaging Under External Illumination With Deep Learning</title><link>http://arxiv.org/abs/2409.16766v2</link><description>Lensless cameras relax the design constraints of traditional cameras byshifting image formation from analog optics to digital post-processing. Whilenew camera designs and applications can be enabled, lensless imaging is verysensitive to unwanted interference (other sources, noise, etc.). In this work,we address a prevalent noise source that has not been studied for lenslessimaging: external illumination e.g. from ambient and direct lighting. Beingrobust to a variety of lighting conditions would increase the practicality andadoption of lensless imaging. To this end, we propose multiple recoveryapproaches that account for external illumination by incorporating its estimateinto the image recovery process. At the core is a physics-based reconstructionthat combines learnable image recovery and denoisers, all of whose parametersare trained using experimentally gathered data. Compared to standardreconstruction methods, our approach yields significant qualitative andquantitative improvements. We open-source our implementations and a 25K datasetof measurements under multiple lighting conditions.</description><author>Eric Bezzam, Stefan Peters, Martin Vetterli</author><pubDate>Tue, 21 Jan 2025 17:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16766v2</guid></item><item><title>Deep Learning Based Segmentation of Blood Vessels from H&amp;E Stained Oesophageal Adenocarcinoma Whole-Slide Images</title><link>http://arxiv.org/abs/2501.12323v1</link><description>Blood vessels (BVs) play a critical role in the Tumor Micro-Environment(TME), potentially influencing cancer progression and treatment response.However, manually quantifying BVs in Hematoxylin and Eosin (H&amp;E) stained imagesis challenging and labor-intensive due to their heterogeneous appearances. Wepropose a novel approach of constructing guiding maps to improve theperformance of state-of-the-art segmentation models for BV segmentation, theguiding maps encourage the models to learn representative features of BVs. Thisis particularly beneficial for computational pathology, where labeled trainingdata is often limited and large models are prone to overfitting. We havequantitative and qualitative results to demonstrate the efficacy of ourapproach in improving segmentation accuracy. In future, we plan to validatethis method to segment BVs across various tissue types and investigate the roleof cellular structures in relation to BVs in the TME.</description><author>Jiaqi Lv, Stefan S Antonowicz, Shan E Ahmed Raza</author><pubDate>Tue, 21 Jan 2025 17:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12323v1</guid></item><item><title>Metric for Evaluating Performance of Reference-Free Demorphing Methods</title><link>http://arxiv.org/abs/2501.12319v1</link><description>A facial morph is an image created by combining two (or more) face imagespertaining to two (or more) distinct identities. Reference-free face demorphinginverts the process and tries to recover the face images constituting a facialmorph without using any other information. However, there is no consensus onthe evaluation metrics to be used to evaluate and compare such demorphingtechniques. In this paper, we first analyze the shortcomings of the demorphingmetrics currently used in the literature. We then propose a new metric calledbiometrically cross-weighted IQA that overcomes these issues and extensivelybenchmark current methods on the proposed metric to show its efficacy.Experiments on three existing demorphing methods and six datasets on twocommonly used face matchers validate the efficacy of our proposed metric.</description><author>Nitish Shukla, Arun Ross</author><pubDate>Tue, 21 Jan 2025 17:38:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12319v1</guid></item><item><title>BlanketGen2-Fit3D: Synthetic Blanket Augmentation Towards Improving Real-World In-Bed Blanket Occluded Human Pose Estimation</title><link>http://arxiv.org/abs/2501.12318v1</link><description>Human Pose Estimation (HPE) from monocular RGB images is crucial for clinicalin-bed skeleton-based action recognition, however, it poses unique challengesfor HPE models due to the frequent presence of blankets occluding the person,while labeled HPE data in this scenario is scarce. To address this we introduceBlanketGen2-Fit3D (BG2-Fit3D), an augmentation of Fit3D dataset that contains1,217,312 frames with synthetic photo-realistic blankets. To generate it weused BlanketGen2, our new and improved version of our BlanketGen pipeline thatsimulates synthetic blankets using ground-truth Skinned Multi-Person Linearmodel (SMPL) meshes and then renders them as transparent images that can belayered on top of the original frames. This dataset was used in combinationwith the original Fit3D to finetune the ViTPose-B HPE model, to evaluatesynthetic blanket augmentation effectiveness. The trained models were furtherevaluated on a real-world blanket occluded in-bed HPE dataset (SLP dataset).Comparing architectures trained on only Fit3D with the ones trained with oursynthetic blanket augmentation the later improved pose estimation performanceon BG2-Fit3D, the synthetic blanket occluded dataset significantly to (0.977Percentage of Correct Keypoints (PCK), 0.149 Normalized Mean Error (NME)) withan absolute 4.4% PCK increase. Furthermore, the test results on SLPdemonstrated the utility of synthetic data augmentation by improvingperformance by an absolute 2.3% PCK, on real-world images with the posesoccluded by real blankets. These results show synthetic blanket augmentationhas the potential to improve in-bed blanket occluded HPE from RGB images. Thedataset as well as the code will be made available to the public.</description><author>Tams Karcsony, Joo Carmona, Joo Paulo Silva Cunha</author><pubDate>Tue, 21 Jan 2025 17:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12318v1</guid></item><item><title>Uncertainty Quantification With Noise Injection in Neural Networks: A Bayesian Perspective</title><link>http://arxiv.org/abs/2501.12314v1</link><description>Model uncertainty quantification involves measuring and evaluating theuncertainty linked to a model's predictions, helping assess their reliabilityand confidence. Noise injection is a technique used to enhance the robustnessof neural networks by introducing randomness. In this paper, we establish aconnection between noise injection and uncertainty quantification from aBayesian standpoint. We theoretically demonstrate that injecting noise into theweights of a neural network is equivalent to Bayesian inference on a deepGaussian process. Consequently, we introduce a Monte Carlo Noise Injection(MCNI) method, which involves injecting noise into the parameters duringtraining and performing multiple forward propagations during inference toestimate the uncertainty of the prediction. Through simulation and experimentson regression and classification tasks, our method demonstrates superiorperformance compared to the baseline model.</description><author>Xueqiong Yuan, Jipeng Li, Ercan Engin Kuruoglu</author><pubDate>Tue, 21 Jan 2025 17:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12314v1</guid></item><item><title>A Hybrid Supervised and Self-Supervised Graph Neural Network for Edge-Centric Applications</title><link>http://arxiv.org/abs/2501.12309v1</link><description>This paper presents a novel graph-based deep learning model for tasksinvolving relations between two nodes (edge-centric tasks), where the focuslies on predicting relationships and interactions between pairs of nodes ratherthan node properties themselves. This model combines supervised andself-supervised learning, taking into account for the loss function theembeddings learned and patterns with and without ground truth. Additionally itincorporates an attention mechanism that leverages both node and edge features.The architecture, trained end-to-end, comprises two primary components:embedding generation and prediction. First, a graph neural network (GNN)transform raw node features into dense, low-dimensional embeddings,incorporating edge attributes. Then, a feedforward neural model processes thenode embeddings to produce the final output. Experiments demonstrate that ourmodel matches or exceeds existing methods for protein-protein interactionsprediction and Gene Ontology (GO) terms prediction. The model also performseffectively with one-hot encoding for node features, providing a solution forthe previously unsolved problem of predicting similarity between compounds withunknown structures.</description><author>Eugenio Borzone, Leandro Di Persia, Matias Gerard</author><pubDate>Tue, 21 Jan 2025 17:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12309v1</guid></item><item><title>A recent evaluation on the performance of LLMs on radiation oncology physics using questions of randomly shuffled options</title><link>http://arxiv.org/abs/2412.10622v3</link><description>Purpose: We present an updated study evaluating the performance of largelanguage models (LLMs) in answering radiation oncology physics questions,focusing on the recently released models. Methods: A set of 100 multiple-choice radiation oncology physics questions,previously created by a well-experienced physicist, was used for this study.The answer options of the questions were randomly shuffled to create "new" examsets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,and Claude 3.5 Sonnet -- with the versions released before September 30, 2024,were queried using these new exam sets. To evaluate their deductive reasoningability, the correct answer options in the questions were replaced with "Noneof the above." Then, the explain-first and step-by-step instruction promptswere used to test if this strategy improved their reasoning ability. Theperformance of the LLMs was compared with the answers from medical physicists. Results: All models demonstrated expert-level performance on these questions,with o1-preview even surpassing medical physicists with a majority vote. Whenreplacing the correct answer options with 'None of the above', all modelsexhibited a considerable decline in performance, suggesting room forimprovement. The explain-first and step-by-step instruction prompts helpedenhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, andClaude 3.5 Sonnet models. Conclusion: These recently released LLMs demonstrated expert-levelperformance in answering radiation oncology physics questions, exhibiting greatpotential to assist in radiation oncology physics education and training.</description><author>Peilong Wang, Jason Holmes, Zhengliang Liu, Dequan Chen, Tianming Liu, Jiajian Shen, Wei Liu</author><pubDate>Tue, 21 Jan 2025 17:20:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10622v3</guid></item><item><title>LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models</title><link>http://arxiv.org/abs/2405.14477v2</link><description>Advances in latent diffusion models (LDMs) have revolutionizedhigh-resolution image generation, but the design space of the autoencoder thatis central to these systems remains underexplored. In this paper, we introduceLiteVAE, a new autoencoder design for LDMs, which leverages the 2D discretewavelet transform to enhance scalability and computational efficiency overstandard variational autoencoders (VAEs) with no sacrifice in output quality.We investigate the training methodologies and the decoder architecture ofLiteVAE and propose several enhancements that improve the training dynamics andreconstruction quality. Our base LiteVAE model matches the quality of theestablished VAEs in current LDMs with a six-fold reduction in encoderparameters, leading to faster training and lower GPU memory requirements, whileour larger model outperforms VAEs of comparable complexity across all evaluatedmetrics (rFID, LPIPS, PSNR, and SSIM).</description><author>Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber</author><pubDate>Tue, 21 Jan 2025 17:15:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14477v2</guid></item><item><title>LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations</title><link>http://arxiv.org/abs/2501.12300v1</link><description>While learning personalization offers great potential for learners, modernpractices in higher education require a deeper consideration of domain modelsand learning contexts, to develop effective personalization algorithms. Thispaper introduces an innovative approach to higher education curriculummodelling that utilizes large language models (LLMs) for knowledge graph (KG)completion, with the goal of creating personalized learning-pathrecommendations. Our research focuses on modelling university subjects andlinking their topics to corresponding domain models, enabling the integrationof learning modules from different faculties and institutions in the student'slearning path. Central to our approach is a collaborative process, where LLMsassist human experts in extracting high-quality, fine-grained topics fromlecture materials. We develop a domain, curriculum, and user models foruniversity modules and stakeholders. We implement this model to create the KGfrom two study modules: Embedded Systems and Development of Embedded SystemsUsing FPGA. The resulting KG structures the curriculum and links it to thedomain models. We evaluate our approach through qualitative expert feedback andquantitative graph quality metrics. Domain experts validated the relevance andaccuracy of the model, while the graph quality metrics measured the structuralproperties of our KG. Our results show that the LLM-assisted graph completionapproach enhances the ability to connect related courses across disciplines topersonalize the learning experience. Expert feedback also showed highacceptance of the proposed collaborative approach for concept extraction andclassification.</description><author>Hasan Abu-Rasheed, Constance Jumbo, Rashed Al Amin, Christian Weber, Veit Wiese, Roman Obermaisser, Madjid Fathi</author><pubDate>Tue, 21 Jan 2025 17:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12300v1</guid></item><item><title>Sublinear Variational Optimization of Gaussian Mixture Models with Millions to Billions of Parameters</title><link>http://arxiv.org/abs/2501.12299v1</link><description>Gaussian Mixture Models (GMMs) range among the most frequently used machinelearning models. However, training large, general GMMs becomes computationallyprohibitive for datasets with many data points $N$ of high-dimensionality $D$.For GMMs with arbitrary covariances, we here derive a highly efficientvariational approximation, which is integrated with mixtures of factoranalyzers (MFAs). For GMMs with $C$ components, our proposed algorithmsignificantly reduces runtime complexity per iteration from$\mathcal{O}(NCD^2)$ to a complexity scaling linearly with $D$ and remainingconstant w.r.t. $C$. Numerical validation of this theoretical complexityreduction then shows the following: the distance evaluations required for theentire GMM optimization process scale sublinearly with $NC$. On large-scalebenchmarks, this sublinearity results in speed-ups of an order-of-magnitudecompared to the state-of-the-art. As a proof of concept, we train GMMs withover 10 billion parameters on about 100 million images, and observe trainingtimes of approximately nine hours on a single state-of-the-art CPU.</description><author>Sebastian Salwig, Till Kahlke, Florian Hirschberger, Dennis Forster, Jrg Lcke</author><pubDate>Tue, 21 Jan 2025 17:11:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12299v1</guid></item><item><title>RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with Retrieval-Augmented Learning</title><link>http://arxiv.org/abs/2501.12296v1</link><description>In the pursuit of robust autonomous driving systems, models trained onreal-world datasets often struggle to adapt to new environments, particularlywhen confronted with corner cases such as extreme weather conditions.Collecting these corner cases in the real world is non-trivial, whichnecessitates the use of simulators for validation. However,the highcomputational cost and the domain gap in data distribution have hindered theseamless transition between real and simulated driving scenarios. To tacklethis challenge, we propose Retrieval-Augmented Learning for Autonomous Driving(RALAD), a novel framework designed to bridge the real-to-sim gap at a lowcost. RALAD features three primary designs, including (1) domain adaptation viaan enhanced Optimal Transport (OT) method that accounts for both individual andgrouped image distances, (2) a simple and unified framework that can be appliedto various models, and (3) efficient fine-tuning techniques that freeze thecomputationally expensive layers while maintaining robustness. Experimentalresults demonstrate that RALAD compensates for the performance degradation insimulated environments while maintaining accuracy in real-world scenariosacross three different models. Taking Cross View as an example, the mIOU andmAP metrics in real-world scenarios remain stable before and after RALADfine-tuning, while in simulated environments,the mIOU and mAP metrics areimproved by 10.30% and 12.29%, respectively. Moreover, the re-training cost ofour approach is reduced by approximately 88.1%. Our code is available athttps://github.com/JiachengZuo/RALAD.git.</description><author>Jiacheng Zuo, Haibo Hu, Zikang Zhou, Yufei Cui, Ziquan Liu, Jianping Wang, Nan Guan, Jin Wang, Chun Jason Xue</author><pubDate>Tue, 21 Jan 2025 17:03:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12296v1</guid></item><item><title>Towards Accurate Unified Anomaly Segmentation</title><link>http://arxiv.org/abs/2501.12295v1</link><description>Unsupervised anomaly detection (UAD) from images strives to model normal datadistributions, creating discriminative representations to distinguish andprecisely localize anomalies. Despite recent advancements in the efficient andunified one-for-all scheme, challenges persist in accurately segmentinganomalies for further monitoring. Moreover, this problem is obscured by thewidely-used AUROC metric under imbalanced UAD settings. This motivates us toemphasize the significance of precise segmentation of anomaly pixels using pAPand DSC as metrics. To address the unsolved segmentation task, we introduce theUnified Anomaly Segmentation (UniAS). UniAS presents a multi-level hybridpipeline that progressively enhances normal information from coarse to fine,incorporating a novel multi-granularity gated CNN (MGG-CNN) into Transformerlayers to explicitly aggregate local details from different granularities.UniAS achieves state-of-the-art anomaly segmentation performance, attaining65.12/59.33 and 40.06/32.50 in pAP/DSC on the MVTec-AD and VisA datasets,respectively, surpassing previous methods significantly. The codes are sharedat https://github.com/Mwxinnn/UniAS.</description><author>Wenxin Ma, Qingsong Yao, Xiang Zhang, Zhelong Huang, Zihang Jiang, S. Kevin Zhou</author><pubDate>Tue, 21 Jan 2025 17:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12295v1</guid></item><item><title>Regressor-Guided Image Editing Regulates Emotional Response to Reduce Online Engagement</title><link>http://arxiv.org/abs/2501.12289v1</link><description>Emotions are known to mediate the relationship between users' contentconsumption and their online engagement, with heightened emotional intensityleading to increased engagement. Building on this insight, we propose threeregressor-guided image editing approaches aimed at diminishing the emotionalimpact of images. These include (i) a parameter optimization approach based onglobal image transformations known to influence emotions, (ii) an optimizationapproach targeting the style latent space of a generative adversarial network,and (iii) a diffusion-based approach employing classifier guidance andclassifier-free guidance. Our findings demonstrate that approaches caneffectively alter the emotional properties of images while maintaining highvisual quality. Optimization-based methods primarily adjust low-levelproperties like color hues and brightness, whereas the diffusion-based approachintroduces semantic changes, such as altering appearance or facial expressions.Notably, results from a behavioral study reveal that only the diffusion-basedapproach successfully elicits changes in viewers' emotional responses whilepreserving high perceived image quality. In future work, we will investigatethe impact of these image adaptations on internet user behavior.</description><author>Christoph Gebhardt, Robin Willardt, Seyedmorteza Sadat, Chih-Wei Ning, Andreas Brombach, Jie Song, Otmar Hilliges, Christian Holz</author><pubDate>Tue, 21 Jan 2025 16:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12289v1</guid></item><item><title>Implementation of an Asymmetric Adjusted Activation Function for Class Imbalance Credit Scoring</title><link>http://arxiv.org/abs/2501.12285v1</link><description>Credit scoring is a systematic approach to evaluate a borrower's probabilityof default (PD) on a bank loan. The data associated with such scenarios arecharacteristically imbalanced, complicating binary classification owing to theoften-underestimated cost of misclassification during the classifier's learningprocess. Considering the high imbalance ratio (IR) of these datasets, weintroduce an innovative yet straightforward optimized activation function byincorporating an IR-dependent asymmetric adjusted factor embedded Sigmoidactivation function (ASIG). The embedding of ASIG makes the sensitive margin ofthe Sigmoid function auto-adjustable, depending on the imbalance nature of thedatasets distributed, thereby giving the activation function an asymmetriccharacteristic that prevents the underrepresentation of the minority class(positive samples) during the classifier's learning process. The experimentalresults show that the ASIG-embedded-classifier outperforms traditionalclassifiers on datasets across wide-ranging IRs in the downstreamcredit-scoring task. The algorithm also shows robustness and stability, evenwhen the IR is ultra-high. Therefore, the algorithm provides a competitivealternative in the financial industry, especially in credit scoring, possessingthe ability to effectively process highly imbalanced distribution data.</description><author>Xia Li, Hanghang Zheng, Kunpeng Tao, Mao Mao</author><pubDate>Tue, 21 Jan 2025 16:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12285v1</guid></item><item><title>MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in Dynamic Sensing Networks</title><link>http://arxiv.org/abs/2501.12281v1</link><description>Given a partially observed road network, how can we predict the traffic stateof unobserved locations? While deep learning approaches show exceptionalperformance in traffic prediction, most assume sensors at all locations ofinterest, which is impractical due to financial constraints. Furthermore, thesemethods typically require costly retraining when sensor configurations change.We propose MoGERNN, an inductive spatio-temporal graph representation model, toaddress these challenges. Inspired by the Mixture of Experts approach in LargeLanguage Models, we introduce a Mixture of Graph Expert (MoGE) block to modelcomplex spatial dependencies through multiple graph message aggregators and asparse gating network. This block estimates initial states for unobservedlocations, which are then processed by a GRU-based Encoder-Decoder thatintegrates a graph message aggregator to capture spatio-temporal dependenciesand predict future states. Experiments on two real-world datasets show MoGERNNconsistently outperforms baseline methods for both observed and unobservedlocations. MoGERNN can accurately predict congestion evolution even in areaswithout sensors, offering valuable information for traffic management.Moreover, MoGERNN is adaptable to dynamic sensing networks, maintainingcompetitive performance even compared to its retrained counterpart. Tests withdifferent numbers of available sensors confirm its consistent superiority, andablation studies validate the effectiveness of its key modules.</description><author>Qishen Zhou, Yifan Zhang, Michail A. Makridis, Anastasios Kouvelas, Yibing Wang, Simon Hu</author><pubDate>Tue, 21 Jan 2025 16:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12281v1</guid></item><item><title>With Great Backbones Comes Great Adversarial Transferability</title><link>http://arxiv.org/abs/2501.12275v1</link><description>Advances in self-supervised learning (SSL) for machine vision have improvedrepresentation robustness and model performance, giving rise to pre-trainedbackbones like \emph{ResNet} and \emph{ViT} models tuned with SSL methods suchas \emph{SimCLR}. Due to the computational and data demands of pre-training,the utilization of such backbones becomes a strenuous necessity. However,employing these backbones may inherit vulnerabilities to adversarial attacks.While adversarial robustness has been studied under \emph{white-box} and\emph{black-box} settings, the robustness of models tuned on pre-trainedbackbones remains largely unexplored. Additionally, the role of tuningmeta-information in mitigating exploitation risks is unclear. This worksystematically evaluates the adversarial robustness of such models across$20,000$ combinations of tuning meta-information, including fine-tuningtechniques, backbone families, datasets, and attack types. We propose usingproxy models to transfer attacks, simulating varying levels of target knowledgeby fine-tuning these proxies with diverse configurations. Our findings revealthat proxy-based attacks approach the effectiveness of \emph{white-box}methods, even with minimal tuning knowledge. We also introduce a naive"backbone attack," leveraging only the backbone to generate adversarialsamples, which outperforms \emph{black-box} attacks and rivals \emph{white-box}methods, highlighting critical risks in model-sharing practices. Finally, ourablations reveal how increasing tuning meta-information impacts attacktransferability, measuring each meta-information combination.</description><author>Erik Arakelyan, Karen Hambardzumyan, Davit Papikyan, Pasquale Minervini, Albert Gordo, Isabelle Augenstein, Aram H. Markosyan</author><pubDate>Tue, 21 Jan 2025 16:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12275v1</guid></item><item><title>Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement</title><link>http://arxiv.org/abs/2501.12273v1</link><description>The quality of Supervised Fine-Tuning (SFT) data plays a critical role inenhancing the conversational capabilities of Large Language Models (LLMs).However, as LLMs become more advanced, the availability of high-qualityhuman-annotated SFT data has become a significant bottleneck, necessitating agreater reliance on synthetic training data. In this work, we introduce Condor,a novel two-stage synthetic data generation framework that incorporates WorldKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT dataat scale. Our experimental results demonstrate that a base model fine-tuned ononly 20K Condor-generated samples achieves superior performance compared tocounterparts. The additional refinement stage in Condor further enablesiterative self-improvement for LLMs at various scales (up to 72B), validatingthe effectiveness of our approach. Furthermore, our investigation into thescaling for synthetic data in post-training reveals substantial unexploredpotential for performance improvements, opening promising avenues for futureresearch.</description><author>Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, Kai Chen</author><pubDate>Tue, 21 Jan 2025 16:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12273v1</guid></item><item><title>Leveraging Explicit Reasoning for Inference Integration in Commonsense-Augmented Dialogue Models</title><link>http://arxiv.org/abs/2406.09138v2</link><description>Open-domain dialogue systems need to grasp social commonsense to understandand respond effectively to human users. Commonsense-augmented dialogue modelshave been proposed that aim to infer commonsense knowledge from dialoguecontexts in order to improve response quality. However, existing approaches tocommonsense-augmented dialogue rely on implicit reasoning to integratecommonsense inferences during response generation. In this study, we explorethe impact of explicit reasoning against implicit reasoning over commonsensefor dialogue response generation. Our findings demonstrate that separatingcommonsense reasoning into explicit steps for generating, selecting, andintegrating commonsense into responses leads to better dialogue interactions,improving naturalness, engagement, specificity, and overall quality. Subsequentanalyses of these findings unveil insights into the effectiveness of varioustypes of commonsense in generating responses and the particular response traitsenhanced through explicit reasoning for commonsense integration. Our workadvances research in open-domain dialogue by achieving a new state-of-the-artin commonsense-augmented response generation.</description><author>Sarah E. Finch, Jinho D. Choi</author><pubDate>Tue, 21 Jan 2025 16:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09138v2</guid></item><item><title>Benchmarking Image Perturbations for Testing Automated Driving Assistance Systems</title><link>http://arxiv.org/abs/2501.12269v1</link><description>Advanced Driver Assistance Systems (ADAS) based on deep neural networks(DNNs) are widely used in autonomous vehicles for critical perception taskssuch as object detection, semantic segmentation, and lane recognition. However,these systems are highly sensitive to input variations, such as noise andchanges in lighting, which can compromise their effectiveness and potentiallylead to safety-critical failures. This study offers a comprehensive empirical evaluation of imageperturbations, techniques commonly used to assess the robustness of DNNs, tovalidate and improve the robustness and generalization of ADAS perceptionsystems. We first conducted a systematic review of the literature, identifying38 categories of perturbations. Next, we evaluated their effectiveness inrevealing failures in two different ADAS, both at the component and at thesystem level. Finally, we explored the use of perturbation-based dataaugmentation and continuous learning strategies to improve ADAS adaptation tonew operational design domains. Our results demonstrate that all categories ofimage perturbations successfully expose robustness issues in ADAS and that theuse of dataset augmentation and continuous learning significantly improves ADASperformance in novel, unseen environments.</description><author>Stefano Carlo Lambertenghi, Hannes Leonhard, Andrea Stocco</author><pubDate>Tue, 21 Jan 2025 16:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12269v1</guid></item><item><title>SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP</title><link>http://arxiv.org/abs/2408.10202v2</link><description>Large-scale vision-language models, such as CLIP, are known to containsocietal bias regarding protected attributes (e.g., gender, age). This paperaims to address the problems of societal bias in CLIP. Although previousstudies have proposed to debias societal bias through adversarial learning ortest-time projecting, our comprehensive study of these works identifies twocritical limitations: 1) loss of attribute information when it is explicitlydisclosed in the input and 2) use of the attribute annotations during debiasingprocess. To mitigate societal bias in CLIP and overcome these limitationssimultaneously, we introduce a simple-yet-effective debiasing method calledSANER (societal attribute neutralizer) that eliminates attribute informationfrom CLIP text features only of attribute-neutral descriptions. Experimentalresults show that SANER, which does not require attribute annotations andpreserves original information for attribute-specific descriptions,demonstrates superior debiasing ability than the existing methods.Additionally, we observe that SANER does not require retraining CLIP fromscratch with the original dataset. Moreover, the debiased model can be directlyapplied to the text-to-image generation model by simply replacing the textencoder.</description><author>Yusuke Hirota, Min-Hung Chen, Chien-Yi Wang, Yuta Nakashima, Yu-Chiang Frank Wang, Ryo Hachiuma</author><pubDate>Tue, 21 Jan 2025 16:39:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10202v2</guid></item><item><title>VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models</title><link>http://arxiv.org/abs/2501.12267v1</link><description>Recent video inpainting methods have achieved encouraging improvements byleveraging optical flow to guide pixel propagation from reference frames eitherin the image space or feature space. However, they would produce severeartifacts in the mask center when the masked area is too large and no pixelcorrespondences can be found for the center. Recently, diffusion models havedemonstrated impressive performance in generating diverse and high-qualityimages, and have been exploited in a number of works for image inpainting.These methods, however, cannot be applied directly to videos to producetemporal-coherent inpainting results. In this paper, we propose a training-freeframework, named VipDiff, for conditioning diffusion model on the reversediffusion process to produce temporal-coherent inpainting results withoutrequiring any training data or fine-tuning the pre-trained diffusion models.VipDiff takes optical flow as guidance to extract valid pixels from referenceframes to serve as constraints in optimizing the randomly sampled Gaussiannoise, and uses the generated results for further pixel propagation andconditional generation. VipDiff also allows for generating diverse videoinpainting results over different sampled noise. Experiments demonstrate thatVipDiff can largely outperform state-of-the-art video inpainting methods interms of both spatial-temporal coherence and fidelity.</description><author>Chaohao Xie, Kai Han, Kwan-Yee K. Wong</author><pubDate>Tue, 21 Jan 2025 16:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12267v1</guid></item><item><title>CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification</title><link>http://arxiv.org/abs/2501.12266v1</link><description>The main challenges limiting the adoption of deep learning-based solutions inmedical workflows are the availability of annotated data and the lack ofinterpretability of such systems. Concept Bottleneck Models (CBMs) tackle thelatter by constraining the final disease prediction on a set of predefined andhuman-interpretable concepts. However, the increased interpretability achievedthrough these concept-based explanations implies a higher annotation burden.Moreover, if a new concept needs to be added, the whole system needs to beretrained. Inspired by the remarkable performance shown by LargeVision-Language Models (LVLMs) in few-shot settings, we propose a simple, yeteffective, methodology, CBVLM, which tackles both of the aforementionedchallenges. First, for each concept, we prompt the LVLM to answer if theconcept is present in the input image. Then, we ask the LVLM to classify theimage based on the previous concept predictions. Moreover, in both stages, weincorporate a retrieval module responsible for selecting the best examples forin-context learning. By grounding the final diagnosis on the predictedconcepts, we ensure explainability, and by leveraging the few-shot capabilitiesof LVLMs, we drastically lower the annotation cost. We validate our approachwith extensive experiments across four medical datasets and twelve LVLMs (bothgeneric and medical) and show that CBVLM consistently outperforms CBMs andtask-specific supervised methods without requiring any training and using justa few annotated examples. More information on our project page:https://cristianopatricio.github.io/CBVLM/.</description><author>Cristiano Patrcio, Isabel Rio-Torto, Jaime S. Cardoso, Lus F. Teixeira, Joo C. Neves</author><pubDate>Tue, 21 Jan 2025 16:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12266v1</guid></item><item><title>mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework</title><link>http://arxiv.org/abs/2501.12263v1</link><description>Collaborative perception significantly enhances individual vehicle perceptionperformance through the exchange of sensory information among agents. However,real-world deployment faces challenges due to bandwidth constraints andinevitable calibration errors during information exchange. To address theseissues, we propose mmCooper, a novel multi-agent, multi-stage,communication-efficient, and collaboration-robust cooperative perceptionframework. Our framework leverages a multi-stage collaboration strategy thatdynamically and adaptively balances intermediate- and late-stage information toshare among agents, enhancing perceptual performance while maintainingcommunication efficiency. To support robust collaboration despite potentialmisalignments and calibration errors, our framework captures multi-scalecontextual information for robust fusion in the intermediate stage andcalibrates the received detection results to improve accuracy in the latestage. We validate the effectiveness of mmCooper through extensive experimentson real-world and simulated datasets. The results demonstrate the superiorityof our proposed framework and the effectiveness of each component.</description><author>Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, Libing Wu</author><pubDate>Tue, 21 Jan 2025 16:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12263v1</guid></item><item><title>HAC++: Towards 100X Compression of 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2501.12255v1</link><description>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novelview synthesis, boasting rapid rendering speed with high fidelity. However, thesubstantial Gaussians and their associated attributes necessitate effectivecompression techniques. Nevertheless, the sparse and unorganized nature of thepoint cloud of Gaussians (or anchors in our paper) presents challenges forcompression. To achieve a compact size, we propose HAC++, which leverages therelationships between unorganized anchors and a structured hash grid, utilizingtheir mutual information for context modeling. Additionally, HAC++ capturesintra-anchor contextual relationships to further enhance compressionperformance. To facilitate entropy coding, we utilize Gaussian distributions toprecisely estimate the probability of each quantized attribute, where anadaptive quantization module is proposed to enable high-precision quantizationof these attributes for improved fidelity restoration. Moreover, we incorporatean adaptive masking strategy to eliminate invalid Gaussians and anchors.Overall, HAC++ achieves a remarkable size reduction of over 100X compared tovanilla 3DGS when averaged on all datasets, while simultaneously improvingfidelity. It also delivers more than 20X size reduction compared toScaffold-GS. Our code is available athttps://github.com/YihangChen-ee/HAC-plus.</description><author>Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai</author><pubDate>Tue, 21 Jan 2025 16:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12255v1</guid></item><item><title>Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos</title><link>http://arxiv.org/abs/2501.12254v1</link><description>Self-supervised learning holds the promise to learn good representations fromreal-world continuous uncurated data streams. However, most existing works invisual self-supervised learning focus on static images or artificial datastreams. Towards exploring a more realistic learning substrate, we investigatestreaming self-supervised learning from long-form real-world egocentric videostreams. Inspired by the event segmentation mechanism in human perception andmemory, we propose "Memory Storyboard" that groups recent past frames intotemporal segments for more effective summarization of the past visual streamsfor memory replay. To accommodate efficient temporal segmentation, we propose atwo-tier memory hierarchy: the recent past is stored in a short-term memory,and the storyboard temporal segments are then transferred to a long-termmemory. Experiments on real-world egocentric video datasets including SAYCamand KrishnaCam show that contrastive learning objectives on top of storyboardframes result in semantically meaningful representations which outperform thoseproduced by state-of-the-art unsupervised continual learning methods.</description><author>Yanlai Yang, Mengye Ren</author><pubDate>Tue, 21 Jan 2025 16:19:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12254v1</guid></item><item><title>Hire Me or Not? Examining Language Model's Behavior with Occupation Attributes</title><link>http://arxiv.org/abs/2405.06687v3</link><description>With the impressive performance in various downstream tasks, large languagemodels (LLMs) have been widely integrated into production pipelines, likerecruitment and recommendation systems. A known issue of models trained onnatural language data is the presence of human biases, which can impact thefairness of the system. This paper investigates LLMs' behavior with respect togender stereotypes, in the context of occupation decision making. Our frameworkis designed to investigate and quantify the presence of gender stereotypes inLLMs' behavior via multi-round question answering. Inspired by prior works, weconstruct a dataset by leveraging a standard occupation classificationknowledge base released by authoritative agencies. We tested three LLMs(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all modelsexhibit gender stereotypes analogous to human biases, but with differentpreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat mayimply the current alignment methods are insufficient for debiasing and couldintroduce new biases contradicting the traditional gender stereotypes.</description><author>Damin Zhang, Yi Zhang, Geetanjali Bihani, Julia Rayz</author><pubDate>Tue, 21 Jan 2025 16:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06687v3</guid></item><item><title>Untrained Perceptual Loss for image denoising of line-like structures in MR images</title><link>http://arxiv.org/abs/2411.05884v2</link><description>In the acquisition of Magnetic Resonance (MR) images shorter scan times leadto higher image noise. Therefore, automatic image denoising using deep learningmethods is of high interest. MR images containing line-like structures such asroots or vessels yield special characteristics as they display connectedstructures and yield sparse information. For this kind of data, it is importantto consider voxel neighborhoods when training a denoising network. In thispaper, we translate the Perceptual Loss to 3D data by comparing feature maps ofuntrained networks in the loss function as done previously for 2D data. Wetested the performance of untrained Perceptual Loss (uPL) on 3D image denoisingof MR images displaying brain vessels (MR angiograms - MRA) and images of plantroots in soil. We investigate the impact of various uPL characteristics such asweight initialization, network depth, kernel size, and pooling operations onthe results. We tested the performance of the uPL loss on four Rician noiselevels using evaluation metrics such as the Structural Similarity Index Metric(SSIM). We observe, that our uPL outperforms conventional loss functions suchas the L1 loss or a loss based on the Structural Similarity Index Metric(SSIM). The uPL network's initialization is not important, while network depthand pooling operations impact denoising performance. E.g. for both datasets anetwork with five convolutional layers led to the best performance while anetwork with more layers led to a performance drop. We also find that small uPLnetworks led to better or comparable results than using large networks such asVGG. We observe superior performance of our loss for both datasets, all noiselevels, and three network architectures. In conclusion, for images containingline-like structures, uPL is an alternative to other loss functions for 3Dimage denoising.</description><author>Elisabeth Pfaehler, Daniel Pflugfelder, Hanno Scharr</author><pubDate>Tue, 21 Jan 2025 16:11:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05884v2</guid></item><item><title>Video Deblurring by Sharpness Prior Detection and Edge Information</title><link>http://arxiv.org/abs/2501.12246v1</link><description>Video deblurring is essential task for autonomous driving, facialrecognition, and security surveillance. Traditional methods directly estimatemotion blur kernels, often introducing artifacts and leading to poor results.Recent approaches utilize the detection of sharp frames within video sequencesto enhance deblurring. However, existing datasets rely on fixed number of sharpframes, which may be too restrictive for some applications and may introduce abias during model training. To address these limitations and enhance domainadaptability, this work first introduces GoPro Random Sharp (GoProRS), a newdataset where the the frequency of sharp frames within the sequence iscustomizable, allowing more diverse training and testing scenarios.Furthermore, it presents a novel video deblurring model, called SPEINet, thatintegrates sharp frame features into blurry frame reconstruction through anattention-based encoder-decoder architecture, a lightweight yet robust sharpframe detection and an edge extraction phase. Extensive experimental resultsdemonstrate that SPEINet outperforms state-of-the-art methods across multipledatasets, achieving an average of +3.2% PSNR improvement over recenttechniques. Given such promising results, we believe that both the proposedmodel and dataset pave the way for future advancements in video deblurringbased on the detection of sharp frames.</description><author>Yang Tian, Fabio Brau, Giulio Rossolini, Giorgio Buttazzo, Hao Meng</author><pubDate>Tue, 21 Jan 2025 16:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12246v1</guid></item><item><title>ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability</title><link>http://arxiv.org/abs/2410.11414v2</link><description>Retrieval-Augmented Generation (RAG) models are designed to incorporateexternal knowledge, reducing hallucinations caused by insufficient parametric(internal) knowledge. However, even with accurate and relevant retrievedcontent, RAG models can still produce hallucinations by generating outputs thatconflict with the retrieved information. Detecting such hallucinations requiresdisentangling how Large Language Models (LLMs) utilize external and parametricknowledge. Current detection methods often focus on one of these mechanisms orwithout decoupling their intertwined effects, making accurate detectiondifficult. In this paper, we investigate the internal mechanisms behindhallucinations in RAG scenarios. We discover hallucinations occur when theKnowledge FFNs in LLMs overemphasize parametric knowledge in the residualstream, while Copying Heads fail to effectively retain or integrate externalknowledge from retrieved content. Based on these findings, we propose ReDeEP, anovel method that detects hallucinations by decoupling LLM's utilization ofexternal context and parametric knowledge. Our experiments show that ReDeEPsignificantly improves RAG hallucination detection accuracy. Additionally, weintroduce AARF, which mitigates hallucinations by modulating the contributionsof Knowledge FFNs and Copying Heads.</description><author>Zhongxiang Sun, Xiaoxue Zang, Kai Zheng, Yang Song, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, Han Li</author><pubDate>Tue, 21 Jan 2025 16:05:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11414v2</guid></item><item><title>Quality Enhancement of Radiographic X-ray Images by Interpretable Mapping</title><link>http://arxiv.org/abs/2501.12245v1</link><description>X-ray imaging is the most widely used medical imaging modality. However, inthe common practice, inconsistency in the initial presentation of X-ray imagesis a common complaint by radiologists. Different patient positions, patienthabitus and scanning protocols can lead to differences in image presentations,e.g., differences in brightness and contrast globally or regionally. Tocompensate for this, additional work will be executed by clinical experts toadjust the images to the desired presentation, which can be time-consuming.Existing deep-learning-based end-to-end solutions can automatically correctimages with promising performances. Nevertheless, these methods are hard to beinterpreted and difficult to be understood by clinical experts. In thismanuscript, a novel interpretable mapping method by deep learning is proposed,which automatically enhances the image brightness and contrast globally andlocally. Meanwhile, because the model is inspired by the workflow of thebrightness and contrast manipulation, it can provide interpretable pixel mapsfor explaining the motivation of image enhancement. The experiment on theclinical datasets show the proposed method can provide consistent brightnessand contrast correction on X-ray images with accuracy of 24.75 dB PSNR and0.8431 SSIM.</description><author>Hongxu Yang, Najib Akram Aboobacker, Xiaomeng Dong, German Gonzalez, Lehel Ferenczi, Gopal Avinash</author><pubDate>Tue, 21 Jan 2025 16:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12245v1</guid></item><item><title>Zero-shot Bias Correction: Efficient MR Image Inhomogeneity Reduction Without Any Data</title><link>http://arxiv.org/abs/2501.12244v1</link><description>In recent years, deep neural networks for image inhomogeneity reduction haveshown promising results. However, current methods with (un)supervised solutionsrequire preparing a training dataset, which is expensive and laborious for datacollection. In this work, we demonstrate a novel zero-shot deep neuralnetworks, which requires no data for pre-training and dedicated assumption ofthe bias field. The designed light-weight CNN enables an efficient zero-shotadaptation for bias-corrupted image correction. Our method provides a novelsolution to mitigate the biased corrupted image as iterative homogeneityrefinement, which therefore ensures the considered issue can be solved easierwith stable convergence of zero-shot optimization. Extensive comparison ondifferent datasets show that the proposed method performs better than currentdata-free N4 methods in both efficiency and accuracy.</description><author>Hongxu Yang, Edina Timko, Brice Fernandez</author><pubDate>Tue, 21 Jan 2025 16:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12244v1</guid></item><item><title>FOCUS: First Order Concentrated Updating Scheme</title><link>http://arxiv.org/abs/2501.12243v1</link><description>Large language models (LLMs) demonstrate remarkable performance, andimproving their pre-training process appears to be key to enhancing theircapabilities further. Based on the documented success of Adam, learning ratedecay, and weight decay, we hypothesize that the pre-training loss landscapefeatures a narrowing valley structure. Through experiments with synthetic lossfunctions, we discover that when gradient query noise is high relative to thevalley's sharpness, Adam's performance falls behind that of Signum because Adamreduces the effective step size too drastically. This observation led us todevelop FOCUS, an optimizer that enhances Signum by incorporating attractiontoward moving averaged parameters, allowing it to handle noise better whilemaintaining larger step sizes. In training GPT-2, FOCUS proves to be morestable than Signum and faster than Adam. These results suggest that gradientnoise may be an underappreciated limiting factor in LLM training, and FOCUSoffers promising solutions.</description><author>Yizhou Liu, Ziming Liu, Jeff Gore</author><pubDate>Tue, 21 Jan 2025 16:03:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12243v1</guid></item><item><title>Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification</title><link>http://arxiv.org/abs/2409.02481v2</link><description>Effective question classification is crucial for AI-driven educational tools,enabling adaptive learning systems to categorize questions by skill area,difficulty level, and competence. This classification not only supportseducational diagnostics and analytics but also enhances complex tasks likeinformation retrieval and question answering by associating questions withrelevant categories. Traditional methods, often based on word embeddings andconventional classifiers, struggle to capture the nuanced relationships innatural language, leading to suboptimal performance. To address this, wepropose a novel approach leveraging graph convolutional networks, named PhraseQuestion-Graph Convolutional Network (PQ-GCN) to better model the inherentstructure of questions. By representing questions as graphs-where nodes signifywords or phrases and edges denote syntactic or semantic relationships-ourmethod allows the model to learn from the interconnected nature of languagemore effectively. Additionally, we explore the incorporation of phrase-basedfeatures to enhance classification performance on question datasets of variousdomains and characteristics. Our findings demonstrate that the proposed model,augmented with these features, offer a promising solution for more robust andcontext-aware question classification, bridging the gap between graph neuralnetwork research and practical educational applications of AI.</description><author>Junyoung Lee, Ninad Dixit, Kaustav Chakrabarti, S. Supraja</author><pubDate>Tue, 21 Jan 2025 16:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02481v2</guid></item><item><title>Investigating Market Strength Prediction with CNNs on Candlestick Chart Images</title><link>http://arxiv.org/abs/2501.12239v1</link><description>This paper investigates predicting market strength solely from candlestickchart images to assist investment decisions. The core research problem isdeveloping an effective computer vision-based model using raw candlestickvisuals without time-series data. We specifically analyze the impact ofincorporating candlestick patterns that were detected by YOLOv8. The studyimplements two approaches: pure CNN on chart images and a Decomposerarchitecture detecting patterns. Experiments utilize diverse financial datasetsspanning stocks, cryptocurrencies, and forex assets. Key findings demonstratecandlestick patterns do not improve model performance over only image data inour research. The significance is illuminating limitations in candlestick imagesignals. Performance peaked at approximately 0.7 accuracy, below more complextime-series models. Outcomes reveal challenges in distilling sufficientpredictive power from visual shapes alone, motivating the incorporation ofother data modalities. This research clarifies how purely image-based modelscan inform trading while confirming patterns add little value over raw charts.Our content is endeavored to be delineated into distinct sections, eachautonomously furnishing a unique contribution while maintaining cohesivelinkage. Note that, the examples discussed herein are not limited to the scope,applicability, or knowledge outlined in the paper.</description><author>Thanh Nam Duong, Trung Kien Hoang, Quoc Khanh Duong, Quoc Dat Dinh, Duc Hoan Le, Huy Tuan Nguyen, Xuan Bach Nguyen, Quy Ban Tran</author><pubDate>Tue, 21 Jan 2025 15:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12239v1</guid></item><item><title>Fast sparse optimization via adaptive shrinkage</title><link>http://arxiv.org/abs/2501.12236v1</link><description>The need for fast sparse optimization is emerging, e.g., to deal withlarge-dimensional data-driven problems and to track time-varying systems. Inthe framework of linear sparse optimization, the iterativeshrinkage-thresholding algorithm is a valuable method to solve Lasso, which isparticularly appreciated for its ease of implementation. Nevertheless, itconverges slowly. In this paper, we develop a proximal method, based onlogarithmic regularization, which turns out to be an iterativeshrinkage-thresholding algorithm with adaptive shrinkage hyperparameter. Thisadaptivity substantially enhances the trajectory of the algorithm, in a waythat yields faster convergence, while keeping the simplicity of the originalmethod. Our contribution is twofold: on the one hand, we derive and analyze theproposed algorithm; on the other hand, we validate its fast convergence vianumerical experiments and we discuss the performance with respect tostate-of-the-art algorithms.</description><author>Vito Cerone, Sophie M. Fosson, Diego Regruto</author><pubDate>Tue, 21 Jan 2025 15:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12236v1</guid></item><item><title>DLEN: Dual Branch of Transformer for Low-Light Image Enhancement in Dual Domains</title><link>http://arxiv.org/abs/2501.12235v1</link><description>Low-light image enhancement (LLE) aims to improve the visual quality ofimages captured in poorly lit conditions, which often suffer from lowbrightness, low contrast, noise, and color distortions. These issues hinder theperformance of computer vision tasks such as object detection, facialrecognition, and autonomous driving.Traditional enhancement techniques, such asmulti-scale fusion and histogram equalization, fail to preserve fine detailsand often struggle with maintaining the natural appearance of enhanced imagesunder complex lighting conditions. Although the Retinex theory provides afoundation for image decomposition, it often amplifies noise, leading tosuboptimal image quality. In this paper, we propose the Dual Light EnhanceNetwork (DLEN), a novel architecture that incorporates two distinct attentionmechanisms, considering both spatial and frequency domains. Our modelintroduces a learnable wavelet transform module in the illumination estimationphase, preserving high- and low-frequency components to enhance edge andtexture details. Additionally, we design a dual-branch structure that leveragesthe power of the Transformer architecture to enhance both the illumination andstructural components of the image.Through extensive experiments, our modeloutperforms state-of-the-art methods on standard benchmarks.Code is availablehere: https://github.com/LaLaLoXX/DLEN</description><author>Junyu Xia, Jiesong Bai, Yihang Dong</author><pubDate>Tue, 21 Jan 2025 15:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12235v1</guid></item><item><title>InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2501.12231v1</link><description>The improved competence of generative models can help building multi-modalvirtual assistants that leverage modalities beyond language. By observinghumans performing multi-step tasks, one can build assistants that havesituational awareness of actions and tasks being performed, enabling them tocater assistance based on this understanding. In this paper, we develop aContext-aware Instructional Task Assistant with Multi-modal Large LanguageModels (InsTALL) that leverages an online visual stream (e.g. a user's screenshare or video recording) and responds in real-time to user queries related tothe task at hand. To enable useful assistance, InsTALL 1) trains a multi-modalmodel on task videos and paired textual data, and 2) automatically extractstask graph from video data and leverages it at training and inference time. Weshow InsTALL achieves state-of-the-art performance across proposed sub-tasksconsidered for multimodal activity understanding -- task recognition (TR),action recognition (AR), next action prediction (AP), and plan prediction (PP)-- and outperforms existing baselines on two novel sub-tasks related toautomatic error identification.</description><author>Pha Nguyen, Sailik Sengupta, Girik Malik, Arshit Gupta, Bonan Min</author><pubDate>Tue, 21 Jan 2025 15:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12231v1</guid></item><item><title>S+t-SNE -- Bringing Dimensionality Reduction to Data Streams</title><link>http://arxiv.org/abs/2403.17643v3</link><description>We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handleinfinite data streams. The core idea behind S+t-SNE is to update the t-SNEembedding incrementally as new data arrives, ensuring scalability andadaptability to handle streaming scenarios. By selecting the most importantpoints at each step, the algorithm ensures scalability while keepinginformative visualisations. By employing a blind method for drift management,the algorithm adjusts the embedding space, which facilitates the visualisationof evolving data dynamics. Our experimental evaluations demonstrate theeffectiveness and efficiency of S+t-SNE, whilst highlighting its ability tocapture patterns in a streaming scenario. We hope our approach offersresearchers and practitioners a real-time tool for understanding andinterpreting high-dimensional data.</description><author>Pedro C. Vieira, Joo P. Montrezol, Joo T. Vieira, Joo Gama</author><pubDate>Tue, 21 Jan 2025 15:52:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17643v3</guid></item><item><title>CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning</title><link>http://arxiv.org/abs/2501.12226v1</link><description>Large Language Models (LLMs) have recently achieved impressive results incomplex reasoning tasks through Chain of Thought (CoT) prompting. However, mostexisting CoT methods rely on using the same prompts, whether manually designedor automatically generated, to handle the entire dataset. Thisone-size-fits-all approach may fail to meet the specific needs arising from thediversities within a single dataset. To solve this problem, we propose theClustered Distance-Weighted Chain of Thought (CDW-CoT) method, whichdynamically constructs prompts tailored to the characteristics of each datainstance by integrating clustering and prompt optimization techniques. Ourmethod employs clustering algorithms to categorize the dataset into distinctgroups, from which a candidate pool of prompts is selected to reflect theinherent diversity within the dataset. For each cluster, CDW-CoT trains theoptimal prompt probability distribution tailored to their specificcharacteristics. Finally, it dynamically constructs a unique prompt probabilitydistribution for each test instance, based on its proximity to cluster centers,from which prompts are selected for reasoning. CDW-CoT consistently outperformstraditional CoT methods across six datasets, including commonsense, symbolic,and mathematical reasoning tasks. Specifically, when compared to manual CoT,CDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and15.72% on LLaMA3 (8B).</description><author>Yuanheng Fang, Guoqing Chao, Wenqiang Lei, Shaobo Li, Dianhui Chu</author><pubDate>Tue, 21 Jan 2025 15:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12226v1</guid></item><item><title>TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space</title><link>http://arxiv.org/abs/2501.12224v1</link><description>We present TokenVerse -- a method for multi-concept personalization,leveraging a pre-trained text-to-image diffusion model. Our framework candisentangle complex visual elements and attributes from as little as a singleimage, while enabling seamless plug-and-play generation of combinations ofconcepts extracted from multiple images. As opposed to existing works,TokenVerse can handle multiple images with multiple concepts each, and supportsa wide-range of concepts, including objects, accessories, materials, pose, andlighting. Our work exploits a DiT-based text-to-image model, in which the inputtext affects the generation through both attention and modulation (shift andscale). We observe that the modulation space is semantic and enables localizedcontrol over complex concepts. Building on this insight, we devise anoptimization-based framework that takes as input an image and a textdescription, and finds for each word a distinct direction in the modulationspace. These directions can then be used to generate new images that combinethe learned concepts in a desired configuration. We demonstrate theeffectiveness of TokenVerse in challenging personalization settings, andshowcase its advantages over existing methods. project's webpage inhttps://token-verse.github.io/</description><author>Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, Tali Dekel</author><pubDate>Tue, 21 Jan 2025 15:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12224v1</guid></item><item><title>Strong phonon-mediated high temperature superconductivity in Li$_2$AuH$_6$ under ambient pressure</title><link>http://arxiv.org/abs/2501.12222v1</link><description>We used our developed AI search engine~(InvDesFlow) to perform extensiveinvestigations regarding ambient stable superconducting hydrides. A cubicstructure Li$_2$AuH$_6$ with Au-H octahedral motifs is identified to be acandidate. After performing thermodynamical analysis, we provide a feasibleroute to experimentally synthesize this material via the known LiAu and LiHcompounds under ambient pressure. The further first-principles calculationssuggest that Li$_2$AuH$_6$ shows a high superconducting transition temperature($T_c$) $\sim$ 140 K under ambient pressure. The H-1$s$ electrons stronglycouple with phonon modes of vibrations of Au-H octahedrons as well asvibrations of Li atoms, where the latter is not taken seriously in otherpreviously similar cases. Hence, different from previous claims of searchingmetallic covalent bonds to find high-$T_c$ superconductors, we emphasize herethe importance of those phonon modes with strong electron-phonon coupling(EPC). And we suggest that one can intercalate atoms into binary or ternaryhydrides to introduce more potential phonon modes with strong EPC, which is aneffective approach to find high-$T_c$ superconductors within multicomponentcompounds.</description><author>Zhenfeng Ouyang, Bo-Wen Yao, Xiao-Qi Han, Peng-Jie Guo, Ze-Feng Gao, Zhong-Yi Lu</author><pubDate>Tue, 21 Jan 2025 15:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12222v1</guid></item><item><title>Exploring Temporally-Aware Features for Point Tracking</title><link>http://arxiv.org/abs/2501.12218v1</link><description>Point tracking in videos is a fundamental task with applications in robotics,video editing, and more. While many vision tasks benefit from pre-trainedfeature backbones to improve generalizability, point tracking has primarilyrelied on simpler backbones trained from scratch on synthetic data, which maylimit robustness in real-world scenarios. Additionally, point tracking requirestemporal awareness to ensure coherence across frames, but usingtemporally-aware features is still underexplored. Most current methods oftenemploy a two-stage process: an initial coarse prediction followed by arefinement stage to inject temporal information and correct errors from thecoarse stage. These approach, however, is computationally expensive andpotentially redundant if the feature backbone itself captures sufficienttemporal information. In this work, we introduce Chrono, a feature backbone specifically designedfor point tracking with built-in temporal awareness. Leveraging pre-trainedrepresentations from self-supervised learner DINOv2 and enhanced with atemporal adapter, Chrono effectively captures long-term temporal context,enabling precise prediction even without the refinement stage. Experimentalresults demonstrate that Chrono achieves state-of-the-art performance in arefiner-free setting on the TAP-Vid-DAVIS and TAP-Vid-Kinetics datasets, amongcommon feature backbones used in point tracking as well as DINOv2, withexceptional efficiency. Project page: https://cvlab-kaist.github.io/Chrono/</description><author>Ins Hyeonsu Kim, Seokju Cho, Jiahui Huang, Jung Yi, Joon-Young Lee, Seungryong Kim</author><pubDate>Tue, 21 Jan 2025 15:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12218v1</guid></item><item><title>Early Detection and Classification of Breast Cancer Using Deep Learning Techniques</title><link>http://arxiv.org/abs/2501.12217v1</link><description>Breast cancer is one of the deadliest cancers causing about massive number ofpatients to die annually all over the world according to the WHO. It is a kindof cancer that develops when the tissues of the breast grow rapidly andunboundly. This fatality rate can be prevented if the cancer is detected beforeit gets malignant. Using automation for early-age detection of breast cancer,Artificial Intelligence and Machine Learning technologies can be implementedfor the best outcome. In this study, we are using the Breast Cancer ImageClassification dataset collected from the Kaggle depository, which comprises9248 Breast Ultrasound Images and is classified into three categories: Benign,Malignant, and Normal which refers to non-cancerous, cancerous, and normalimages.This research introduces three pretrained model featuring customclassifiers that includes ResNet50, MobileNet, and VGG16, along with a customCNN model utilizing the ReLU activation function.The models ResNet50,MobileNet, VGG16, and a custom CNN recorded accuracies of 98.41%, 97.91%,98.19%, and 92.94% on the dataset, correspondingly, with ResNet50 achieving thehighest accuracy of 98.41%.This model, with its deep and powerful architecture,is particularly successful in detecting aberrant cells as well as cancerous ornon-cancerous tumors. These accuracies show that the Machine Learning methodsare more compatible for the classification and early detection of breastcancer.</description><author>Mst. Mumtahina Labonno, D. M. Asadujjaman, Md. Mahfujur Rahman, Abdullah Tamim, Mst. Jannatul Ferdous, Rafi Muttaki Mahi</author><pubDate>Tue, 21 Jan 2025 15:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12217v1</guid></item><item><title>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</title><link>http://arxiv.org/abs/2501.01957v3</link><description>Recent Multimodal Large Language Models (MLLMs) have typically focused onintegrating visual and textual modalities, with less emphasis placed on therole of speech in enhancing interaction. However, speech plays a crucial rolein multimodal dialogue systems, and implementing high-performance in bothvision and speech tasks remains a significant challenge due to the fundamentalmodality differences. In this paper, we propose a carefully designedmulti-stage training methodology that progressively trains LLM to understandboth visual and speech information, ultimately enabling fluent vision andspeech interaction. Our approach not only preserves strong vision-languagecapacity, but also enables efficient speech-to-speech dialogue capabilitieswithout separate ASR and TTS modules, significantly accelerating multimodalend-to-end response speed. By comparing our method against state-of-the-artcounterparts across benchmarks for image, video, and speech tasks, wedemonstrate that our model is equipped with both strong visual and speechcapabilities, making near real-time vision and speech interaction.</description><author>Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</author><pubDate>Tue, 21 Jan 2025 15:36:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01957v3</guid></item><item><title>RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression</title><link>http://arxiv.org/abs/2501.12216v1</link><description>Video encoders optimize compression for human perception by minimizingreconstruction error under bit-rate constraints. In many modern applicationssuch as autonomous driving, an overwhelming majority of videos serve as inputfor AI systems performing tasks like object recognition or segmentation, ratherthan being watched by humans. It is therefore useful to optimize the encoderfor a downstream task instead of for perceptual image quality. However, a majorchallenge is how to combine such downstream optimization with existing standardvideo encoders, which are highly efficient and popular. Here, we address thischallenge by controlling the Quantization Parameters (QPs) at the macro-blocklevel to optimize the downstream task. This granular control allows us toprioritize encoding for task-relevant regions within each frame. We formulatethis optimization problem as a Reinforcement Learning (RL) task, where theagent learns to balance long-term implications of choosing QPs on both taskperformance and bit-rate constraints. Notably, our policy does not require thedownstream task as an input during inference, making it suitable for streamingapplications and edge devices such as vehicles. We demonstrate significantimprovements in two tasks, car detection, and ROI (saliency) encoding. Ourapproach improves task performance for a given bit rate compared to traditionaltask agnostic encoding methods, paving the way for more efficient task-awarevideo compression.</description><author>Uri Gadot, Assaf Shocher, Shie Mannor, Gal Chechik, Assaf Hallak</author><pubDate>Tue, 21 Jan 2025 15:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12216v1</guid></item><item><title>Automatic selection of the best neural architecture for time series forecasting via multi-objective optimization and Pareto optimality conditions</title><link>http://arxiv.org/abs/2501.12215v1</link><description>Time series forecasting plays a pivotal role in a wide range of applications,including weather prediction, healthcare, structural health monitoring,predictive maintenance, energy systems, and financial markets. While modelssuch as LSTM, GRU, Transformers, and State-Space Models (SSMs) have becomestandard tools in this domain, selecting the optimal architecture remains achallenge. Performance comparisons often depend on evaluation metrics and thedatasets under analysis, making the choice of a universally optimal modelcontroversial. In this work, we introduce a flexible automated framework fortime series forecasting that systematically designs and evaluates diversenetwork architectures by integrating LSTM, GRU, multi-head Attention, and SSMblocks. Using a multi-objective optimization approach, our framework determinesthe number, sequence, and combination of blocks to align with specificrequirements and evaluation objectives. From the resulting Pareto-optimalarchitectures, the best model for a given context is selected via auser-defined preference function. We validate our framework across fourdistinct real-world applications. Results show that a single-layer GRU or LSTMis usually optimal when minimizing training time alone. However, whenmaximizing accuracy or balancing multiple objectives, the best architecturesare often composite designs incorporating multiple block types in specificconfigurations. By employing a weighted preference function, users can resolvetrade-offs between objectives, revealing novel, context-specific optimalarchitectures. Our findings underscore that no single neural architecture isuniversally optimal for time series forecasting. Instead, the best-performingmodel emerges as a data-driven composite architecture tailored to user-definedcriteria and evaluation objectives.</description><author>Qianying Cao, Shanqing Liu, Alan John Varghese, Jerome Darbon, Michael Triantafyllou, George Em Karniadakis</author><pubDate>Tue, 21 Jan 2025 15:33:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12215v1</guid></item><item><title>Quantitative Error Bounds for Scaling Limits of Stochastic Iterative Algorithms</title><link>http://arxiv.org/abs/2501.12212v1</link><description>Stochastic iterative algorithms, including stochastic gradient descent (SGD)and stochastic gradient Langevin dynamics (SGLD), are widely utilized foroptimization and sampling in large-scale and high-dimensional problems inmachine learning, statistics, and engineering. Numerous works have bounded theparameter error in, and characterized the uncertainty of, these approximations.One common approach has been to use scaling limit analyses to relate thedistribution of algorithm sample paths to a continuous-time stochastic processapproximation, particularly in asymptotic setups. Focusing on the univariatesetting, in this paper, we build on previous work to derive non-asymptoticfunctional approximation error bounds between the algorithm sample paths andthe Ornstein-Uhlenbeck approximation using an infinite-dimensional version ofStein's method of exchangeable pairs. We show that this bound implies weakconvergence under modest additional assumptions and leads to a bound on theerror of the variance of the iterate averages of the algorithm. Furthermore, weuse our main result to construct error bounds in terms of two common metrics:the L\'{e}vy-Prokhorov and bounded Wasserstein distances. Our results provide afoundation for developing similar error bounds for the multivariate setting andfor more sophisticated stochastic approximation algorithms.</description><author>Xiaoyu Wang, Mikolaj J. Kasprzak, Jeffrey Negrea, Solesne Bourguin, Jonathan H. Huggins</author><pubDate>Tue, 21 Jan 2025 15:29:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12212v1</guid></item><item><title>Multi-Scale Texture Loss for CT denoising with GANs</title><link>http://arxiv.org/abs/2403.16640v2</link><description>Generative Adversarial Networks (GANs) have proved as a powerful frameworkfor denoising applications in medical imaging. However, GAN-based denoisingalgorithms still suffer from limitations in capturing complex relationshipswithin the images. In this regard, the loss function plays a crucial role inguiding the image generation process, encompassing how much a synthetic imagediffers from a real image. To grasp highly complex and non-linear texturalrelationships in the training process, this work presents a novel approach tocapture and embed multi-scale texture information into the loss function. Ourmethod introduces a differentiable multi-scale texture representation of theimages dynamically aggregated by a self-attention layer, thus exploitingend-to-end gradient-based optimization. We validate our approach by carryingout extensive experiments in the context of low-dose CT denoising, achallenging application that aims to enhance the quality of noisy CT scans. Weutilize three publicly available datasets, including one simulated and two realdatasets. The results are promising as compared to other well-established lossfunctions, being also consistent across three different GAN architectures. Thecode is available at:https://github.com/TrainLaboratory/MultiScaleTextureLoss-MSTLF</description><author>Francesco Di Feola, Lorenzo Tronchin, Valerio Guarrasi, Paolo Soda</author><pubDate>Tue, 21 Jan 2025 15:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16640v2</guid></item><item><title>Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model</title><link>http://arxiv.org/abs/2501.12206v1</link><description>Large Vision Language Models (LVLMs) have demonstrated remarkablecapabilities in understanding and describing visual content, achievingstate-of-the-art performance across various vision-language tasks. However,these models frequently exhibit hallucination behavior, where they generatedescriptions containing objects or details absent in the input image. Our workinvestigates this phenomenon by analyzing attention patterns across transformerlayers and heads, revealing that hallucinations often stem from progressivedegradation of visual grounding in deeper layers. We propose a novel attentionmodification approach that combines selective token emphasis and head-specificmodulation to maintain visual grounding throughout the generation process. Ourmethod introduces two key components: (1) a dual-stream token selectionmechanism that identifies and prioritizes both locally informative andspatially significant visual tokens, and (2) an attention head-specificmodulation strategy that differentially amplifies visual information processingbased on measured visual sensitivity of individual attention heads. Throughextensive experimentation on the MSCOCO dataset, we demonstrate that ourapproach reduces hallucination rates by up to 62.3\% compared to baselinemodels while maintaining comparable task performance. Our analysis reveals thatselectively modulating tokens across attention heads with varying levels ofvisual sensitivity can significantly improve visual grounding without requiringmodel retraining.</description><author>Kazi Hasan Ibn Arif, Sajib Acharjee Dip, Khizar Hussain, Lang Zhang, Chris Thomas</author><pubDate>Tue, 21 Jan 2025 15:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12206v1</guid></item><item><title>Time-Series Foundation Model for Value-at-Risk Forecasting</title><link>http://arxiv.org/abs/2410.11773v6</link><description>This study is the first to analyze the performance of a time-seriesfoundation model for Value-at-Risk (VaR), which essentially forecasts theleft-tail quantiles of returns. Foundation models, pre-trained on diversedatasets, can be applied in a zero-shot setting with minimal data or furtherimproved through finetuning. We compare Google's TimesFM model to conventionalparametric and non-parametric models, including GARCH and GeneralizedAutoregressive Score (GAS), using 19 years of daily returns from the S&amp;P 100index and its constituents. Backtesting with over 8.5 years of out-of-sampledata shows that the fine-tuned foundation model consistently outperformstraditional methods in actual-over-expected ratios. For the quantile score lossfunction, it performs comparably to the best econometric model, GAS. Overall,the foundation model ranks as the best or among the top performers across the0.01, 0.025, 0.05, and 0.1 quantile forecasting. Fine-tuning significantlyimproves accuracy, showing that zero-shot use is not optimal for VaR.</description><author>Anubha Goel, Puneet Pasricha, Juho Kanniainen</author><pubDate>Tue, 21 Jan 2025 15:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11773v6</guid></item><item><title>Score Combining for Contrastive OOD Detection</title><link>http://arxiv.org/abs/2501.12204v1</link><description>In out-of-distribution (OOD) detection, one is asked to classify whether atest sample comes from a known inlier distribution or not. We focus on the casewhere the inlier distribution is defined by a training dataset and there existsno additional knowledge about the novelties that one is likely to encounter.This problem is also referred to as novelty detection, one-classclassification, and unsupervised anomaly detection. The current literaturesuggests that contrastive learning techniques are state-of-the-art for OODdetection. We aim to improve on those techniques by combining/ensembling theirscores using the framework of null hypothesis testing and, in particular, anovel generalized likelihood ratio test (GLRT). We demonstrate that ourproposed GLRT-based technique outperforms the state-of-the-art CSI and SupCSItechniques from Tack et al. 2020 in dataset-vs-dataset experiments withCIFAR-10, SVHN, LSUN, ImageNet, and CIFAR-100, as well as leave-one-class-outexperiments with CIFAR-10. We also demonstrate that our GLRT outperforms thescore-combining methods of Fisher, Bonferroni, Simes, Benjamini-Hochwald, andStouffer in our application.</description><author>Edward T. Reehorst, Philip Schniter</author><pubDate>Tue, 21 Jan 2025 15:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12204v1</guid></item><item><title>Explainability for Vision Foundation Models: A Survey</title><link>http://arxiv.org/abs/2501.12203v1</link><description>As artificial intelligence systems become increasingly integrated into dailylife, the field of explainability has gained significant attention. This trendis particularly driven by the complexity of modern AI models and theirdecision-making processes. The advent of foundation models, characterized bytheir extensive generalization capabilities and emergent uses, has furthercomplicated this landscape. Foundation models occupy an ambiguous position inthe explainability domain: their complexity makes them inherently challengingto interpret, yet they are increasingly leveraged as tools to constructexplainable models. In this survey, we explore the intersection of foundationmodels and eXplainable AI (XAI) in the vision domain. We begin by compiling acomprehensive corpus of papers that bridge these fields. Next, we categorizethese works based on their architectural characteristics. We then discuss thechallenges faced by current research in integrating XAI within foundationmodels. Furthermore, we review common evaluation methodologies for thesecombined approaches. Finally, we present key observations and insights from oursurvey, offering directions for future research in this rapidly evolving field.</description><author>Rmi Kazmierczak, Elose Berthier, Goran Frehse, Gianni Franchi</author><pubDate>Tue, 21 Jan 2025 15:18:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12203v1</guid></item><item><title>TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models</title><link>http://arxiv.org/abs/2412.18675v3</link><description>Multi-head self-attention (MHSA) is a key component of Transformers, a widelypopular architecture in both language and vision. Multiple heads intuitivelyenable different parallel processes over the same input. Yet, they also obscurethe attribution of each input patch to the output of a model. We propose anovel 1-head Transformer Attention Bottleneck (TAB) layer, inserted after thetraditional MHSA architecture, to serve as an attention bottleneck forinterpretability and intervention. Unlike standard self-attention, TABconstrains the total attention over all patches to $\in [0, 1]$. That is, whenthe total attention is 0, no visual information is propagated further into thenetwork and the vision-language model (VLM) would default to a generic,image-independent response. To demonstrate the advantages of TAB, we train VLMswith TAB to perform image difference captioning. Over three datasets, ourmodels perform similarly to baseline VLMs in captioning but the bottleneck issuperior in localizing changes and in identifying when no changes occur. TAB isthe first architecture to enable users to intervene by editing attention, whichoften produces expected outputs by VLMs.</description><author>Pooyan Rahmanzadehgervi, Hung Huy Nguyen, Rosanne Liu, Long Mai, Anh Totti Nguyen</author><pubDate>Tue, 21 Jan 2025 15:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18675v3</guid></item><item><title>Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</title><link>http://arxiv.org/abs/2501.12202v1</link><description>We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system forgenerating high-resolution textured 3D assets. This system includes twofoundation components: a large-scale shape generation model -- Hunyuan3D-DiT,and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shapegenerative model, built on a scalable flow-based diffusion transformer, aims tocreate geometry that properly aligns with a given condition image, laying asolid foundation for downstream applications. The texture synthesis model,benefiting from strong geometric and diffusion priors, produces high-resolutionand vibrant texture maps for either generated or hand-crafted meshes.Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly productionplatform that simplifies the re-creation process of 3D assets. It allows bothprofessional and amateur users to manipulate or even animate their meshesefficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0outperforms previous state-of-the-art models, including the open-source modelsand closed-source models in geometry details, condition alignment, texturequality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gapsin the open-source 3D community for large-scale foundation generative models.The code and pre-trained weights of our models are available at:https://github.com/Tencent/Hunyuan3D-2</description><author>Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo, Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu, Changrong Hu, Tianyu Huang, Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu, Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing He, Tian Liu, Yong Yang, Di Wang</author><pubDate>Tue, 21 Jan 2025 15:16:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12202v1</guid></item><item><title>TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation</title><link>http://arxiv.org/abs/2402.10178v2</link><description>The emergence of Large Language Models (LLMs) like ChatGPT has inspired thedevelopment of LLM-based agents capable of addressing complex, real-worldtasks. However, these agents often struggle during task execution due tomethodological constraints, such as error propagation and limited adaptability.To address this issue, we propose a multi-agent framework based on dynamic TaskDecomposition and Agent Generation (TDAG). This framework dynamicallydecomposes complex tasks into smaller subtasks and assigns each to aspecifically generated subagent, thereby enhancing adaptability in diverse andunpredictable real-world tasks. Simultaneously, existing benchmarks often lackthe granularity needed to evaluate incremental progress in complex, multi-steptasks. In response, we introduce ItineraryBench in the context of travelplanning, featuring interconnected, progressively complex tasks with afine-grained evaluation system. ItineraryBench is designed to assess agents'abilities in memory, planning, and tool usage across tasks of varyingcomplexity. Our experimental results reveal that TDAG significantly outperformsestablished baselines, showcasing its superior adaptability and contextawareness in complex task scenarios.</description><author>Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, Jinsong Su</author><pubDate>Tue, 21 Jan 2025 15:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10178v2</guid></item><item><title>Experience-replay Innovative Dynamics</title><link>http://arxiv.org/abs/2501.12199v1</link><description>Despite its groundbreaking success, multi-agent reinforcement learning (MARL)still suffers from instability and nonstationarity. Replicator dynamics, themost well-known model from evolutionary game theory (EGT), provide atheoretical framework for the convergence of the trajectories to Nashequilibria and, as a result, have been used to ensure formal guarantees forMARL algorithms in stable game settings. However, they exhibit the oppositebehavior in other settings, which poses the problem of finding alternatives toensure convergence. In contrast, innovative dynamics, such as the Brown-vonNeumann-Nash (BNN) or Smith, result in periodic trajectories with the potentialto approximate Nash equilibria. Yet, no MARL algorithms based on these dynamicshave been proposed. In response to this challenge, we develop a novelexperience replay-based MARL algorithm that incorporates revision protocols astunable hyperparameters. We demonstrate, by appropriately adjusting therevision protocols, that the behavior of our algorithm mirrors the trajectoriesresulting from these dynamics. Importantly, our contribution provides aframework capable of extending the theoretical guarantees of MARL algorithmsbeyond replicator dynamics. Finally, we corroborate our theoretical findingswith empirical results.</description><author>Tuo Zhang, Leonardo Stella, Julian Barreiro Gomez</author><pubDate>Tue, 21 Jan 2025 15:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12199v1</guid></item><item><title>An End-to-End Approach for Korean Wakeword Systems with Speaker Authentication</title><link>http://arxiv.org/abs/2501.12194v1</link><description>Wakeword detection plays a critical role in enabling AI assistants to listento user voices and interact effectively. However, for languages other thanEnglish, there is a significant lack of pre-trained wakeword models.Additionally, systems that merely determine the presence of a wakeword can poseserious privacy concerns. In this paper, we propose an end-to-end approach thattrains wakewords for Non-English languages, particulary Korean, and uses thisto develop a Voice Authentication model to protect user privacy. Ourimplementation employs an open-source platform OpenWakeWord, which performswakeword detection using an FCN (Fully-Connected Network) architecture. Once awakeword is detected, our custom-developed code calculates cosine similarityfor robust user authentication. Experimental results demonstrate theeffectiveness of our approach, achieving a 16.79% and a 6.6% Equal Error Rate(EER) each in the Wakeword Detection and the Voice Authentication. Thesefindings highlight the model's potential in providing secure and accuratewakeword detection and authentication for Korean users.</description><author>Geonwoo Seo</author><pubDate>Tue, 21 Jan 2025 15:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12194v1</guid></item><item><title>MyDigiTwin: A Privacy-Preserving Framework for Personalized Cardiovascular Risk Prediction and Scenario Exploration</title><link>http://arxiv.org/abs/2501.12193v1</link><description>Cardiovascular disease (CVD) remains a leading cause of death, and primaryprevention through personalized interventions is crucial. This paper introducesMyDigiTwin, a framework that integrates health digital twins with personalhealth environments to empower patients in exploring personalized healthscenarios while ensuring data privacy. MyDigiTwin uses federated learning totrain predictive models across distributed datasets without transferring rawdata, and a novel data harmonization framework addresses semantic and formatinconsistencies in health data. A proof-of-concept demonstrates the feasibilityof harmonizing and using cohort data to train privacy-preserving CVD predictionmodels. This framework offers a scalable solution for proactive, personalizedcardiovascular care and sets the stage for future applications in real-worldhealthcare settings.</description><author>Hctor Cadavid, Hyunho Mo, Bauke Arends, Katarzyna Dziopa, Esther E. Bron, Daniel Bos, Sonja Georgievska, Pim van der Harst</author><pubDate>Tue, 21 Jan 2025 15:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12193v1</guid></item><item><title>FLARE: Faithful Logic-Aided Reasoning and Exploration</title><link>http://arxiv.org/abs/2410.11900v4</link><description>Modern Question Answering (QA) and Reasoning approaches based on LargeLanguage Models (LLMs) commonly use prompting techniques, such asChain-of-Thought (CoT), assuming the resulting generation will have a moregranular exploration and reasoning over the question space and scope. However,such methods struggle with generating outputs that are faithful to theintermediate chain of reasoning produced by the model. On the other end of thespectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose tocombine LLMs with external symbolic solvers. While such approaches boast a highdegree of faithfulness, they usually require a model trained for codegeneration and struggle with tasks that are ambiguous or hard to formalisestrictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided$\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novelinterpretable approach for traversing the problem space using taskdecompositions. We use the LLM to plan a solution, soft-formalise the queryinto facts and predicates using a logic programming code and simulate that codeexecution using an exhaustive multi-hop search over the defined space. Ourmethod allows us to compute the faithfulness of the reasoning process w.r.t.the generated code and analyse the steps of the multi-hop search withoutrelying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that modelfaithfulness positively correlates with overall performance and furtherdemonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factorssufficient for and leading to the correct answer with optimal reasoning duringthe multi-hop search.</description><author>Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle Augenstein</author><pubDate>Tue, 21 Jan 2025 14:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11900v4</guid></item><item><title>A margin-based replacement for cross-entropy loss</title><link>http://arxiv.org/abs/2501.12191v1</link><description>Cross-entropy (CE) loss is the de-facto standard for training deep neuralnetworks to perform classification. However, CE-trained deep neural networksstruggle with robustness and generalisation issues. To alleviate these issues,we propose high error margin (HEM) loss, a variant of multi-class margin lossthat overcomes the training issues of other margin-based losses. We evaluateHEM extensively on a range of architectures and datasets. We find that HEM lossis more effective than cross-entropy loss across a wide range of tasks: unknownclass rejection, adversarial robustness, learning with imbalanced data,continual learning, and semantic segmentation (a pixel-level classificationtask). Despite all training hyper-parameters being chosen for CE loss, HEM isinferior to CE only in terms of clean accuracy and this difference isinsignificant. We also compare HEM to specialised losses that have previouslybeen proposed to improve performance on specific tasks. LogitNorm, a lossachieving state-of-the-art performance on unknown class rejection, producessimilar performance to HEM for this task, but is much poorer for continuallearning and semantic segmentation. Logit-adjusted loss, designed forimbalanced data, has superior results to HEM for that task, but performs morepoorly on unknown class rejection and semantic segmentation. DICE, a popularloss for semantic segmentation, is inferior to HEM loss on all tasks, includingsemantic segmentation. Thus, HEM often out-performs specialised losses, and incontrast to them, is a general-purpose replacement for CE loss.</description><author>Michael W. Spratling, Heiko H. Schtt</author><pubDate>Tue, 21 Jan 2025 14:56:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12191v1</guid></item><item><title>Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis</title><link>http://arxiv.org/abs/2406.07455v2</link><description>In this paper, we study reinforcement learning from human feedback (RLHF)under an episodic Markov decision process with a general trajectory-wise rewardmodel. We developed a model-free RLHF best policy identification algorithm,called $\mathsf{BSAD}$, without explicit reward model inference, which is acritical intermediate step in the contemporary RLHF paradigms for traininglarge language models (LLM). The algorithm identifies the optimal policydirectly from human preference information in a backward manner, employing adueling bandit sub-routine that constantly duels actions to identify thesuperior one. $\mathsf{BSAD}$ adopts a reward-free exploration andbest-arm-identification-like adaptive stopping criteria to equalize thevisitation among all states in the same decision step while moving to theprevious step as soon as the optimal action is identifiable, leading to aprovable, instance-dependent sample complexity$\tilde{\mathcal{O}}(c_{\mathcal{M}}SA^3H^3M\log\frac{1}{\delta})$ whichresembles the result in classic RL, where $c_{\mathcal{M}}$ is theinstance-dependent constant and $M$ is the batch size. Moreover,$\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm withlogarithmic regret and generalized to discounted MDPs using a frame-basedapproach. Our results show: (i) sample-complexity-wise, RLHF is notsignificantly harder than classic RL and (ii) end-to-end RLHF may deliverimproved performance by avoiding pitfalls in reward inferring such as overfitand distribution shift.</description><author>Qining Zhang, Honghao Wei, Lei Ying</author><pubDate>Tue, 21 Jan 2025 14:53:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07455v2</guid></item><item><title>COmoving Computer Acceleration (COCA): $N$-body simulations in an emulated frame of reference</title><link>http://arxiv.org/abs/2409.02154v2</link><description>$N$-body simulations are computationally expensive, so machine-learning(ML)-based emulation techniques have emerged as a way to increase their speed.Although fast, surrogate models have limited trustworthiness due to potentiallysubstantial emulation errors that current approaches cannot correct for. Toalleviate this problem, we introduce COmoving Computer Acceleration (COCA), ahybrid framework interfacing ML with an $N$-body simulator. The correctphysical equations of motion are solved in an emulated frame of reference, sothat any emulation error is corrected by design. This approach corresponds tosolving for the perturbation of particle trajectories around the machine-learntsolution, which is computationally cheaper than obtaining the full solution,yet is guaranteed to converge to the truth as one increases the number of forceevaluations. Although applicable to any ML algorithm and $N$-body simulator,this approach is assessed in the particular case of particle-mesh cosmologicalsimulations in a frame of reference predicted by a convolutional neuralnetwork, where the time dependence is encoded as an additional input parameterto the network. COCA efficiently reduces emulation errors in particletrajectories, requiring far fewer force evaluations than running thecorresponding simulation without ML. We obtain accurate final density andvelocity fields for a reduced computational budget. We demonstrate that thismethod shows robustness when applied to examples outside the range of thetraining data. When compared to the direct emulation of the Lagrangiandisplacement field using the same training resources, COCA's ability to correctemulation errors results in more accurate predictions. COCA makes $N$-bodysimulations cheaper by skipping unnecessary force evaluations, while stillsolving the correct equations of motion and correcting for emulation errorsmade by ML.</description><author>Deaglan J. Bartlett, Marco Chiarenza, Ludvig Doeser, Florent Leclercq</author><pubDate>Tue, 21 Jan 2025 14:51:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02154v2</guid></item><item><title>Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation</title><link>http://arxiv.org/abs/2403.02302v4</link><description>Multimodal Large Language Models (MLLMs) have recently gained immensepopularity. Powerful commercial models like ChatGPT-4V and Gemini, as well asopen-source ones such as LLaVA, are essentially general-purpose models and areapplied to solve a wide variety of tasks, including those in computer vision.These neural networks possess such strong general knowledge and reasoningabilities that they have proven capable of working even on tasks for which theywere not specifically trained. We compared the capabilities of the mostpowerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized taskof age and gender estimation with our state-of-the-art specialized model,MiVOLO. We also updated MiVOLO and provide details and new metrics in thisarticle. This comparison has yielded some interesting results and insightsabout the strengths and weaknesses of the participating models. Furthermore, weattempted various ways to fine-tune the ShareGPT4V model for this specifictask, aiming to achieve state-of-the-art results in this particular challenge.Although such a model would not be practical in production, as it is incrediblyexpensive compared to a specialized model like MiVOLO, it could be very usefulin some tasks, like data annotation.</description><author>Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh</author><pubDate>Tue, 21 Jan 2025 14:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02302v4</guid></item></channel></rss>