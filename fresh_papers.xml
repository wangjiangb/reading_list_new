<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 10 Nov 2025 12:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Visual Spatial Tuning</title><link>http://arxiv.org/abs/2511.05491v1</link><description>Capturing spatial relationships from visual inputs is a cornerstone ofhuman-like general intelligence. Several previous studies have tried to enhancethe spatial awareness of Vision-Language Models (VLMs) by adding extra expertencoders, which brings extra overhead and usually harms general capabilities.To enhance the spatial ability in general architectures, we introduce VisualSpatial Tuning (VST), a comprehensive framework to cultivate VLMs withhuman-like visuospatial abilities, from spatial perception to reasoning. Wefirst attempt to enhance spatial perception in VLMs by constructing alarge-scale dataset termed VST-P, which comprises 4.1 million samples spanning19 skills across single views, multiple images, and videos. Then, we presentVST-R, a curated dataset with 135K samples that instruct models to reason inspace. In particular, we adopt a progressive training pipeline: supervisedfine-tuning to build foundational spatial knowledge, followed by reinforcementlearning to further improve spatial reasoning abilities. Without theside-effect to general capabilities, the proposed VST consistently achievesstate-of-the-art results on several spatial benchmarks, including $34.8\%$ onMMSI-Bench and $61.2\%$ on VSIBench. It turns out that theVision-Language-Action models can be significantly enhanced with the proposedspatial tuning paradigm, paving the way for more physically grounded AI.</description><author>Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, Hengshuang Zhao</author><pubDate>Fri, 07 Nov 2025 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05491v1</guid></item><item><title>TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</title><link>http://arxiv.org/abs/2511.05489v1</link><description>Temporal search aims to identify a minimal set of relevant frames from tensof thousands based on a given query, serving as a foundation for accuratelong-form video understanding. Existing works attempt to progressively narrowthe search space. However, these approaches typically rely on a hand-craftedsearch process, lacking end-to-end optimization for learning optimal searchstrategies. In this paper, we propose TimeSearch-R, which reformulates temporalsearch as interleaved text-video thinking, seamlessly integrating searchingvideo clips into the reasoning process through reinforcement learning (RL).However, applying RL training methods, such as Group Relative PolicyOptimization (GRPO), to video reasoning can result in unsupervised intermediatesearch decisions. This leads to insufficient exploration of the video contentand inconsistent logical reasoning. To address these issues, we introduce GRPOwith Completeness Self-Verification (GRPO-CSV), which gathers searched videoframes from the interleaved reasoning process and utilizes the same policymodel to verify the adequacy of searched frames, thereby improving thecompleteness of video reasoning. Additionally, we construct datasetsspecifically designed for the SFT cold-start and RL training of GRPO-CSV,filtering out samples with weak temporal dependencies to enhance taskdifficulty and improve temporal search capabilities. Extensive experimentsdemonstrate that TimeSearch-R achieves significant improvements on temporalsearch benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well aslong-form video understanding benchmarks like VideoMME and MLVU. Notably,TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%improvement over the base model Qwen2.5-VL and 2.0% over the advanced videoreasoning model Video-R1. Our code is available athttps://github.com/Time-Search/TimeSearch-R.</description><author>Junwen Pan, Qizhe Zhang, Rui Zhang, Ming Lu, Xin Wan, Yuan Zhang, Chang Liu, Qi She</author><pubDate>Fri, 07 Nov 2025 18:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05489v1</guid></item><item><title>MIMIC-SR-ICD11: A Dataset for Narrative-Based Diagnosis</title><link>http://arxiv.org/abs/2511.05485v1</link><description>Disease diagnosis is a central pillar of modern healthcare, enabling earlydetection and timely intervention for acute conditions while guiding lifestyleadjustments and medication regimens to prevent or slow chronic disease.Self-reports preserve clinically salient signals that templated electronichealth record (EHR) documentation often attenuates or omits, especially subtlebut consequential details. To operationalize this shift, we introduceMIMIC-SR-ICD11, a large English diagnostic dataset built from EHR dischargenotes and natively aligned to WHO ICD-11 terminology. We further presentLL-Rank, a likelihood-based re-ranking framework that computes alength-normalized joint likelihood of each label given the clinical reportcontext and subtracts the corresponding report-free prior likelihood for thatlabel. Across seven model backbones, LL-Rank consistently outperforms a stronggeneration-plus-mapping baseline (GenMap). Ablation experiments show thatLL-Rank's gains primarily stem from its PMI-based scoring, which isolatessemantic compatibility from label frequency bias.</description><author>Yuexin Wu, Shiqi Wang, Vasile Rus</author><pubDate>Fri, 07 Nov 2025 18:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05485v1</guid></item><item><title>DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction</title><link>http://arxiv.org/abs/2511.05483v1</link><description>Predicting the effect of amino acid mutations on enzyme thermodynamicstability (DDG) is fundamental to protein engineering and drug design. Whilerecent deep learning approaches have shown promise, they often process sequenceand structure information independently, failing to capture the intricatecoupling between local structural geometry and global sequential patterns. Wepresent DGTN (Diffused Graph-Transformer Network), a novel architecture thatco-learns graph neural network (GNN) weights for structural priors andtransformer attention through a diffusion mechanism. Our key innovation is abidirectional diffusion process where: (1) GNN-derived structural embeddingsguide transformer attention via learnable diffusion kernels, and (2)transformer representations refine GNN message passing throughattention-modulated graph updates. We provide rigorous mathematical analysisshowing this co-learning scheme achieves provably better approximation boundsthan independent processing. On ProTherm and SKEMPI benchmarks, DGTN achievesstate-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with6.2% improvement over best baselines. Ablation studies confirm the diffusionmechanism contributes 4.8 points to correlation. Our theoretical analysisproves the diffused attention converges to optimal structure-sequence coupling,with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This workestablishes a principled framework for integrating heterogeneous proteinrepresentations through learnable diffusion.</description><author>Abigail Lin</author><pubDate>Fri, 07 Nov 2025 18:52:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05483v1</guid></item><item><title>Characterizing the Training Dynamics of Private Fine-tuning with Langevin diffusion</title><link>http://arxiv.org/abs/2402.18905v2</link><description>We show that differentially private full fine-tuning (DP-FFT) can distortpre-trained backbone features based on both theoretical and empirical results.We identify the cause of the distortion as the misalignment between thepre-trained backbone and the randomly initialized linear head. We prove that asequential fine-tuning strategy can mitigate the feature distortion:first-linear-probing-then-fine-tuning (DP-LP-FFT). A new approximation schemeallows us to derive approximate upper and lower bounds on the training loss ofDP-LP and DP-FFT, in a simple but canonical setting of 2-layer neural networkswith ReLU activation. Experiments on real-world datasets and architectures areconsistent with our theoretical insights. We also derive new upper bounds for2-layer linear networks without the approximation. Moreover, our theorysuggests a trade-off of privacy budget allocation in multi-phase fine-tuningmethods like DP-LP-FFT.</description><author>Shuqi Ke, Charlie Hou, Sewoong Oh, Giulia Fanti</author><pubDate>Fri, 07 Nov 2025 18:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18905v2</guid></item><item><title>SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive Cross-Component Learning</title><link>http://arxiv.org/abs/2511.05482v1</link><description>Precision agriculture demands continuous and accurate monitoring of soilmoisture (M) and key macronutrients, including nitrogen (N), phosphorus (P),and potassium (K), to optimize yields and conserve resources. Wireless soilsensing has been explored to measure these four components; however, currentsolutions require recalibration (i.e., retraining the data processing model) tohandle variations in soil texture, characterized by aluminosilicates (Al) andorganic carbon (C), limiting their practicality. To address this, we introduceSoilX, a calibration-free soil sensing system that jointly measures six keycomponents: {M, N, P, K, C, Al}. By explicitly modeling C and Al, SoilXeliminates texture- and carbon-dependent recalibration. SoilX incorporatesContrastive Cross-Component Learning (3CL), with two customized terms: theOrthogonality Regularizer and the Separation Loss, to effectively disentanglecross-component interference. Additionally, we design a novel tetrahedralantenna array with an antenna-switching mechanism, which can robustly measuresoil dielectric permittivity independent of device placement. Extensiveexperiments demonstrate that SoilX reduces estimation errors by 23.8% to 31.5%over baselines and generalizes well to unseen fields.</description><author>Kang Yang, Yuanlin Yang, Yuning Chen, Sikai Yang, Xinyu Zhang, Wan Du</author><pubDate>Fri, 07 Nov 2025 18:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05482v1</guid></item><item><title>On Flow Matching KL Divergence</title><link>http://arxiv.org/abs/2511.05480v1</link><description>We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler(KL) divergence of the flow-matching distribution approximation. In particular,if the $L_2$ flow-matching loss is bounded by $\epsilon^2 &gt; 0$, then the KLdivergence between the true data distribution and the estimated distribution isbounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$depend only on the regularities of the data and velocity fields. Consequently,this bound implies statistical convergence rates of Flow Matching Transformersunder the Total Variation (TV) distance. We show that, flow matching achievesnearly minimax-optimal efficiency in estimating smooth distributions. Ourresults make the statistical efficiency of flow matching comparable to that ofdiffusion models under the TV distance. Numerical studies on synthetic andlearned velocities corroborate our theory.</description><author>Maojiang Su, Jerry Yao-Chieh Hu, Sophia Pi, Han Liu</author><pubDate>Fri, 07 Nov 2025 18:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05480v1</guid></item><item><title>FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning</title><link>http://arxiv.org/abs/2506.03777v2</link><description>With the emerging application of Federated Learning (FL) in decision-makingscenarios, it is imperative to regulate model fairness to prevent disparitiesacross sensitive groups (e.g., female, male). Current research predominantlyfocuses on two concepts of group fairness within FL: Global Fairness (overallmodel disparity across all clients) and Local Fairness (the disparity withineach client). However, the non-decomposable, non-differentiable nature offairness criteria poses two fundamental, unresolved challenges for fair FL: (i)Harmonizing global and local fairness, especially in multi-class setting; (ii)Enabling a controllable, optimal accuracy-fairness trade-off. To tackle thesechallenges, we propose a novel controllable federated group-fairnesscalibration framework, named FedFACT. FedFACT identifies the Bayes-optimalclassifiers under both global and local fairness constraints, yielding modelswith minimal performance decline while guaranteeing fairness. Building on thecharacterization of the optimal fair classifiers, we reformulate fair federatedlearning as a personalized cost-sensitive learning problem for in-processingand a bi-level optimization for post-processing. Theoretically, we provideconvergence and generalization guarantees for FedFACT to approach thenear-optimal accuracy under given fairness levels. Extensive experiments onmultiple datasets across various data heterogeneity demonstrate that FedFACTconsistently outperforms baselines in balancing accuracy and global-localfairness.</description><author>Li Zhang, Zhongxuan Han, Xiaohua Feng, Jiaming Zhang, Yuyuan Li, Chaochao Chen</author><pubDate>Fri, 07 Nov 2025 18:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.03777v2</guid></item><item><title>Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward</title><link>http://arxiv.org/abs/2403.06524v2</link><description>We develop a deep reinforcement learning framework for tactical decisionmaking in an autonomous truck, specifically for Adaptive Cruise Control (ACC)and lane change maneuvers in a highway scenario. Our results demonstrate thatit is beneficial to separate high-level decision-making processes and low-levelcontrol actions between the reinforcement learning agent and the low-levelcontrollers based on physical models. In the following, we study optimizing theperformance with a realistic and multi-objective reward function based on TotalCost of Operation (TCOP) of the truck using different approaches; by addingweights to reward components, by normalizing the reward components and by usingcurriculum learning techniques.</description><author>Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani</author><pubDate>Fri, 07 Nov 2025 18:45:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06524v2</guid></item><item><title>FPGA-Based Real-Time Waveform Classification</title><link>http://arxiv.org/abs/2511.05479v1</link><description>For self-triggered readout of SiPM sum signals, a waveform classification canaid a simple threshold trigger to reliably extract calorimetric particle hitinformation online at an early stage and thus reduce the volume of transmitteddata. Typically, the ADC data acquisition is based on FPGAs for edge dataprocessing. In this study, we consider look-up-table-based neural-networks andaddress challenges of binary multi-layer neural networks' layout, footprint,performance and training. We show that these structures can be trained using agenetic algorithm and achieve the inference latency compatible with dead-timefree processing online.</description><author>Alperen Aksoy, Ilja Bekman, Chimezie Eguzo, Christian Grewing, Andre Zambanini</author><pubDate>Fri, 07 Nov 2025 18:44:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05479v1</guid></item><item><title>TRACE: Textual Relevance Augmentation and Contextual Encoding for Multimodal Hate Detection</title><link>http://arxiv.org/abs/2504.17902v2</link><description>Social media memes are a challenging domain for hate detection because theyintertwine visual and textual cues into culturally nuanced messages. To tacklethese challenges, we introduce TRACE, a hierarchical multimodal framework thatleverages visually grounded context augmentation, along with a novelcaption-scoring network to emphasize hate-relevant content, andparameter-efficient fine-tuning of CLIP's text encoder. Our experimentsdemonstrate that selectively fine-tuning deeper text encoder layerssignificantly enhances performance compared to simpler projection-layerfine-tuning methods. Specifically, our framework achieves state-of-the-artaccuracy (0.807) and F1-score (0.806) on the widely-used Hateful Memes dataset,matching the performance of considerably larger models while maintainingefficiency. Moreover, it achieves superior generalization on the MultiOFFoffensive meme dataset (F1-score 0.673), highlighting robustness across memecategories. Additional analyses confirm that robust visual grounding andnuanced text representations significantly reduce errors caused by benignconfounders. We publicly release our code to facilitate future research.</description><author>Girish A. Koushik, Helen Treharne, Aditya Joshi, Diptesh Kanojia</author><pubDate>Fri, 07 Nov 2025 18:41:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.17902v2</guid></item><item><title>GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling for Efficient Medical Image Segmentation</title><link>http://arxiv.org/abs/2511.05477v1</link><description>Medical image segmentation requires models that are accurate, lightweight,and interpretable. Convolutional architectures lack adaptive nonlinearity andtransparent decision-making, whereas Transformer architectures are hindered byquadratic complexity and opaque attention mechanisms. U-KAN addresses thesechallenges using Kolmogorov-Arnold Networks, achieving higher accuracy thanboth convolutional and attention-based methods, fewer parameters thanTransformer variants, and improved interpretability compared to conventionalapproaches. However, its O(C^2) complexity due to full-channel transformationslimits its scalability as the number of channels increases. To overcome this,we introduce GroupKAN, a lightweight segmentation network that incorporates twonovel, structured functional modules: (1) Grouped KAN Transform, whichpartitions channels into G groups for multivariate spline mappings, reducingcomplexity to O(C^2/G), and (2) Grouped KAN Activation, which applies sharedspline-based mappings within each channel group for efficient, token-wisenonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC),GroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11percent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M),and shows improved interpretability.</description><author>Guojie Li, Anwar P. P. Abdul Majeed, Muhammad Ateeq, Anh Nguyen, Fan Zhang</author><pubDate>Fri, 07 Nov 2025 18:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05477v1</guid></item><item><title>A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?</title><link>http://arxiv.org/abs/2511.05476v1</link><description>Transformer-based language models of code have achieved state-of-the-artperformance across a wide range of software analytics tasks, but theirpractical deployment remains limited due to high computational costs, slowinference speeds, and significant environmental impact. To address thesechallenges, recent research has increasingly explored knowledge distillation asa method for compressing a large language model of code (the teacher) into asmaller model (the student) while maintaining performance. However, the degreeto which a student model deeply mimics the predictive behavior and internalrepresentations of its teacher remains largely unexplored, as currentaccuracy-based evaluation provides only a surface-level view of model qualityand often fails to capture more profound discrepancies in behavioral fidelitybetween the teacher and student models. To address this gap, we empiricallyshow that the student model often fails to deeply mimic the teacher model,resulting in up to 285% greater performance drop under adversarial attacks,which is not captured by traditional accuracy-based evaluation. Therefore, wepropose MetaCompress, a metamorphic testing framework that systematicallyevaluates behavioral fidelity by comparing the outputs of teacher and studentmodels under a set of behavior-preserving metamorphic relations. We evaluateMetaCompress on two widely studied tasks, using compressed versions of popularlanguage models of code, obtained via three different knowledge distillationtechniques: Compressor, AVATAR, and MORPH. The results show that MetaCompressidentifies up to 62% behavioral discrepancies in student models, underscoringthe need for behavioral fidelity evaluation within the knowledge distillationpipeline and establishing MetaCompress as a practical framework for testingcompressed language models of code derived through knowledge distillation.</description><author>Md. Abdul Awal, Mrigank Rochan, Chanchal K. Roy</author><pubDate>Fri, 07 Nov 2025 18:38:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05476v1</guid></item><item><title>AI Literacy Assessment Revisited: A Task-Oriented Approach Aligned with Real-world Occupations</title><link>http://arxiv.org/abs/2511.05475v1</link><description>As artificial intelligence (AI) systems become ubiquitous in professionalcontexts, there is an urgent need to equip workers, often with backgroundsoutside of STEM, with the skills to use these tools effectively as well asresponsibly, that is, to be AI literate. However, prevailing definitions andtherefore assessments of AI literacy often emphasize foundational technicalknowledge, such as programming, mathematics, and statistics, over practicalknowledge such as interpreting model outputs, selecting tools, or identifyingethical concerns. This leaves a noticeable gap in assessing someone's AIliteracy for real-world job use. We propose a work-task-oriented assessmentmodel for AI literacy which is grounded in the competencies required foreffective use of AI tools in professional settings. We describe the developmentof a novel AI literacy assessment instrument, and accompanying formativeassessments, in the context of a US Navy robotics training program. The programincluded training in robotics and AI literacy, as well as a competition withpractical tasks and a multiple choice scenario task meant to simulate use of AIin a job setting. We found that, as a measure of applied AI literacy, thecompetition's scenario task outperformed the tests we adopted from pastresearch or developed ourselves. We argue that when training people forAI-related work, educators should consider evaluating them with instrumentsthat emphasize highly contextualized practical skills rather than abstracttechnical knowledge, especially when preparing workers without technicalbackgrounds for AI-integrated roles.</description><author>Christopher Bogart, Aparna Warrier, Arav Agarwal, Ross Higashi, Yufan Zhang, Jesse Flot, Jaromir Savelka, Heather Burte, Majd Sakr</author><pubDate>Fri, 07 Nov 2025 18:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05475v1</guid></item><item><title>Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection</title><link>http://arxiv.org/abs/2511.05474v1</link><description>This paper introduces a cutting-edge approach to cross-modal interaction fortiny object detection by combining semantic-guided natural language processingwith advanced visual recognition backbones. The proposed method integrates theBERT language model with the CNN-based Parallel Residual Bi-Fusion FeaturePyramid Network (PRB-FPN-Net), incorporating innovative backbone architecturessuch as ELAN, MSP, and CSP to optimize feature extraction and fusion. Byemploying lemmatization and fine-tuning techniques, the system aligns semanticcues from textual inputs with visual features, enhancing detection precisionfor small and complex objects. Experimental validation using the COCO andObjects365 datasets demonstrates that the model achieves superior performance.On the COCO2017 validation set, it attains a 52.6% average precision (AP),outperforming YOLO-World significantly while maintaining half the parameterconsumption of Transformer-based models like GLIP. Several test on different ofbackbones such ELAN, MSP, and CSP further enable efficient handling ofmulti-scale objects, ensuring scalability and robustness inresource-constrained environments. This study underscores the potential ofintegrating natural language understanding with advanced backbonearchitectures, setting new benchmarks in object detection accuracy, efficiency,and adaptability to real-world challenges.</description><author>Xian-Hong Huang, Hui-Kai Su, Chi-Chia Sun, Jun-Wei Hsieh</author><pubDate>Fri, 07 Nov 2025 18:38:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05474v1</guid></item><item><title>Precipitation nowcasting of satellite data using physically conditioned neural networks</title><link>http://arxiv.org/abs/2511.05471v1</link><description>Accurate short-term precipitation forecasts predominantly rely on denseweather-radar networks, limiting operational value in places most exposed toclimate extremes. We present TUPANN (Transferable and Universal Physics-AlignedNowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlikemost deep learning models for nowcasting, TUPANN decomposes the forecast intophysically meaningful components: a variational encoder-decoder infers motionand intensity fields from recent imagery under optical-flow supervision, alead-time-conditioned MaxViT evolves the latent state, and a differentiableadvection operator reconstructs future frames. We evaluate TUPANN on bothGOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro,Manaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metricsover 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning andhybrid baselines show that TUPANN achieves the best or second-best skill inmost settings, with pronounced gains at higher thresholds. Training on multiplecities further improves performance, while cross-city experiments show modestdegradation and occasional gains for rare heavy-rain regimes. The modelproduces smooth, interpretable motion fields aligned with numerical opticalflow and runs in near real time due to the low latency of GOES-16. Theseresults indicate that physically aligned learning can provide nowcasts that areskillful, transferable and global.</description><author>Antônio Catão, Melvin Poveda, Leonardo Voltarelli, Paulo Orenstein</author><pubDate>Fri, 07 Nov 2025 18:33:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05471v1</guid></item><item><title>Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer for Drilling Rate of Penetration Prediction</title><link>http://arxiv.org/abs/2508.05210v3</link><description>Rate of Penetration (ROP) prediction is critical for drilling optimizationyet remains challenging due to the nonlinear, dynamic, and heterogeneouscharacteristics of drilling data. Conventional empirical, physics-based, andstandard machine learning models rely on oversimplified assumptions orintensive feature engineering, constraining their capacity to model long-termdependencies and intricate feature interactions. To address these issues, thisstudy presents a new deep learning Hybrid LSTM-Trans-Mixer-Att framework thatfirst processes input data through a customized Long Short-Term Memory (LSTM)network to capture multi-scale temporal dependencies aligned with drillingcycles. Subsequently, an Enhanced Transformer encoder with drilling-specificpositional encodings and real-time optimization refines the features.Concurrently, a parallel Time-Series Mixer (TS-Mixer) block introducedfacilitates efficient cross-feature interaction modeling of static andcategorical parameters, including lithological indices and mud properties. Thefeature representations extracted from the Enhanced Transformer and TS-Mixermodules are integrated through a dedicated fusion layer. Finally, an adaptiveattention mechanism then dynamically assigns contextual weights to salientfeatures, enhancing discriminative representation learning and enablinghigh-fidelity ROP prediction. The proposed framework combines sequentialmemory, static feature interactions, global context learning, and dynamicfeature weighting, providing a comprehensive solution for the heterogeneous andevent-driven nature of drilling dynamics. Experimental validation on real-worlddrilling datasets demonstrates superior performance, achieving an Rsquare of0.9991 and a MAPE of 1.447%, significantly outperforming existing baseline andhybrid models.</description><author>Saddam Hussain Khan</author><pubDate>Fri, 07 Nov 2025 18:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.05210v3</guid></item><item><title>Towards Explainable Fake Image Detection with Multi-Modal Large Language Models</title><link>http://arxiv.org/abs/2504.14245v2</link><description>Progress in image generation raises significant public security concerns. Weargue that fake image detection should not operate as a "black box". Instead,an ideal approach must ensure both strong generalization and transparency.Recent progress in Multi-modal Large Language Models (MLLMs) offers newopportunities for reasoning-based AI-generated image detection. In this work,we evaluate the capabilities of MLLMs in comparison to traditional detectionmethods and human evaluators, highlighting their strengths and limitations.Furthermore, we design six distinct prompts and propose a framework thatintegrates these prompts to develop a more robust, explainable, andreasoning-driven detection system. The code is available athttps://github.com/Gennadiyev/mllm-defake.</description><author>Yikun Ji, Yan Hong, Jiahui Zhan, Haoxing Chen, jun lan, Huijia Zhu, Weiqiang Wang, Liqing Zhang, Jianfu Zhang</author><pubDate>Fri, 07 Nov 2025 18:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.14245v2</guid></item><item><title>To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models</title><link>http://arxiv.org/abs/2406.20054v3</link><description>Polysemy and synonymy are two crucial interrelated facets of lexicalambiguity. While both phenomena are widely documented in lexical resources andhave been studied extensively in NLP, leading to dedicated systems, they areoften being considered independently in practical problems. While many tasksdealing with polysemy (e.g. Word Sense Disambiguation or Induction) highlightthe role of word's senses, the study of synonymy is rooted in the study ofconcepts, i.e. meanings shared across the lexicon. In this paper, we introduceConcept Induction, the unsupervised task of learning a soft clustering amongwords that defines a set of concepts directly from data. This task generalizesWord Sense Induction. We propose a bi-level approach to Concept Induction thatleverages both a local lemma-centric view and a global cross-lexicon view toinduce concepts. We evaluate the obtained clustering on SemCor's annotated dataand obtain good performance (BCubed F1 above 0.60). We find that the local andthe global levels are mutually beneficial to induce concepts and also senses inour setting. Finally, we create static embeddings representing our inducedconcepts and use them on the Word-in-Context task, obtaining competitiveperformance with the State-of-the-Art.</description><author>Bastien Liétard, Pascal Denis, Mikaela Keller</author><pubDate>Fri, 07 Nov 2025 18:16:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.20054v3</guid></item><item><title>EventFlow: Real-Time Neuromorphic Event-Driven Classification of Two-Phase Boiling Flow Regimes</title><link>http://arxiv.org/abs/2511.05467v1</link><description>Flow boiling is an efficient heat transfer mechanism capable of dissipatinghigh heat loads with minimal temperature variation, making it an ideal thermalmanagement method. However, sudden shifts between flow regimes can disruptthermal performance and system reliability, highlighting the need for accurateand low-latency real-time monitoring. Conventional optical imaging methods arelimited by high computational demands and insufficient temporal resolution,making them inadequate for capturing transient flow behavior. To address this,we propose a real-time framework based on signals from neuromorphic sensors forflow regime classification. Neuromorphic sensors detect changes in brightnessat individual pixels, which typically correspond to motion at edges, enablingfast and efficient detection without full-frame reconstruction, providingevent-based information. We develop five classification models using bothtraditional image data and event-based data, demonstrating that modelsleveraging event data outperform frame-based approaches due to theirsensitivity to dynamic flow features. Among these models, the event-based longshort-term memory model provides the best balance between accuracy and speed,achieving 97.6% classification accuracy with a processing time of 0.28 ms. Ourasynchronous processing pipeline supports continuous, low-latency predictionsand delivers stable output through a majority voting mechanisms, enablingreliable real-time feedback for experimental control and intelligent thermalmanagement.</description><author>Sanghyeon Chang, Srikar Arani, Nishant Sai Nuthalapati, Youngjoon Suh, Nicholas Choi, Siavash Khodakarami, Md Rakibul Hasan Roni, Nenad Miljkovic, Aparna Chandramowlishwaran, Yoonjin Won</author><pubDate>Fri, 07 Nov 2025 18:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05467v1</guid></item><item><title>USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining</title><link>http://arxiv.org/abs/2507.05843v2</link><description>Immunohistochemical (IHC) virtual staining is a task that generates virtualIHC images from H\&amp;E images while maintaining pathological semantic consistencywith adjacent slices. This task aims to achieve cross-domain mapping betweenmorphological structures and staining patterns through generative models,providing an efficient and cost-effective solution for pathological analysis.However, under weakly paired conditions, spatial heterogeneity between adjacentslices presents significant challenges. This can lead to inaccurate one-to-manymappings and generate results that are inconsistent with the pathologicalsemantics of adjacent slices. To address this issue, we propose a novelunbalanced self-information feature transport for IHC virtual staining, namedUSIGAN, which extracts global morphological semantics without relying onpositional correspondence.By removing weakly paired terms in the joint marginaldistribution, we effectively mitigate the impact of weak pairing on jointdistributions, thereby significantly improving the content consistency andpathological semantic consistency of the generated results. Moreover, we designthe Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and thePathology Self-Correspondence (PC-SCM) mechanism to construct correlationmatrices between H\&amp;E and generated IHC in image-level and real IHC andgenerated IHC image sets in intra-group level.. Experiments conducted on twopublicly available datasets demonstrate that our method achieves superiorperformance across multiple clinically significant metrics, such as IoD andPearson-R correlation, demonstrating better clinical relevance.</description><author>Yue Peng, Bing Xiong, Fuqiang Chen, De Eybo, RanRan Zhang, Wanming Hu, Jing Cai, Wenjian Qin</author><pubDate>Fri, 07 Nov 2025 18:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05843v2</guid></item><item><title>Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning</title><link>http://arxiv.org/abs/2511.02818v3</link><description>Tabular data remain the predominant format for real-world applications. Yet,developing effective neural models for tabular data remains challenging due toheterogeneous feature types and complex interactions occurring at multiplescales. Recent advances in tabular in-context learning (ICL), such as TabPFNand TabICL, have achieved state-of-the-art performance comparable togradient-boosted trees (GBTs) without task-specific fine-tuning. However,current architectures exhibit key limitations: (1) single-scale featureprocessing that overlooks hierarchical dependencies, (2) dense attention withquadratic scaling in table width, and (3) strictly sequential componentprocessing that prevents iterative representation refinement andcross-component communication. To address these challenges, we introduceOrion-MSP, a tabular ICL architecture featuring three key innovations: (1)multi-scale processing to capture hierarchical feature interactions; (2)block-sparse attention combining windowed, global, and random patterns forscalable efficiency and long-range connectivity; and (3) a Perceiver-stylememory enabling safe bidirectional information flow across components. Acrossdiverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performancewhile scaling effectively to high-dimensional tables, establishing a newstandard for efficient tabular in-context learning. The model is publiclyavailable at https://github.com/Lexsi-Labs/Orion-MSP .</description><author>Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu</author><pubDate>Fri, 07 Nov 2025 18:13:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.02818v3</guid></item><item><title>Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics</title><link>http://arxiv.org/abs/2510.17797v2</link><description>As information grows exponentially, enterprises face increasing pressure totransform unstructured data into coherent, actionable insights. Whileautonomous agents show promise, they often struggle with domain-specificnuances, intent alignment, and enterprise integration. We present EnterpriseDeep Research (EDR), a multi-agent system that integrates (1) a Master PlanningAgent for adaptive query decomposition, (2) four specialized search agents(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based toolecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) aVisualization Agent for data-driven insights, and (5) a reflection mechanismthat detects knowledge gaps and updates research direction with optionalhuman-in-the-loop steering guidance. These components enable automated reportgeneration, real-time streaming, and seamless enterprise deployment, asvalidated on internal datasets. On open-ended benchmarks including DeepResearchBench and DeepConsult, EDR outperforms state-of-the-art agentic systems withoutany human steering. We release the EDR framework and benchmark trajectories toadvance research on multi-agent reasoning applications. Code at https://github.com/SalesforceAIResearch/enterprise-deep-research andDataset at https://huggingface.co/datasets/Salesforce/EDR-200</description><author>Akshara Prabhakar, Roshan Ram, Zixiang Chen, Silvio Savarese, Frank Wang, Caiming Xiong, Huan Wang, Weiran Yao</author><pubDate>Fri, 07 Nov 2025 18:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.17797v2</guid></item><item><title>Photo Dating by Facial Age Aggregation</title><link>http://arxiv.org/abs/2511.05464v1</link><description>We introduce a novel method for Photo Dating which estimates the year aphotograph was taken by leveraging information from the faces of people presentin the image. To facilitate this research, we publicly release CSFD-1.6M, a newdataset containing over 1.6 million annotated faces, primarily from moviestills, with identity and birth year annotations. Uniquely, our datasetprovides annotations for multiple individuals within a single image, enablingthe study of multi-face information aggregation. We propose a probabilisticframework that formally combines visual evidence from modern face recognitionand age estimation models, and career-based temporal priors to infer the photocapture year. Our experiments demonstrate that aggregating evidence frommultiple faces consistently improves the performance and the approachsignificantly outperforms strong, scene-based baselines, particularly forimages containing several identifiable individuals.</description><author>Jakub Paplham, Vojtech Franc</author><pubDate>Fri, 07 Nov 2025 18:08:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05464v1</guid></item><item><title>SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning</title><link>http://arxiv.org/abs/2511.05462v1</link><description>Recent studies have demonstrated the effectiveness of clustering-basedapproaches for self-supervised and unsupervised learning. However, theapplication of clustering is often heuristic, and the optimal methodologyremains unclear. In this work, we establish connections between theseunsupervised clustering methods and classical mixture models from statistics.Through this framework, we demonstrate significant enhancements to theseclustering methods, leading to the development of a novel model named SiamMM.Our method attains state-of-the-art performance across various self-supervisedlearning benchmarks. Inspection of the learned clusters reveals a strongresemblance to unseen ground truth labels, uncovering potential instances ofmislabeling.</description><author>Xiaodong Wang, Jing Huang, Kevin J Liang</author><pubDate>Fri, 07 Nov 2025 18:07:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05462v1</guid></item><item><title>On the Brittleness of CLIP Text Encoders</title><link>http://arxiv.org/abs/2511.04247v2</link><description>Multimodal co-embedding models, especially CLIP, have advanced the state ofthe art in zero-shot classification and multimedia information retrieval inrecent years by aligning images and text in a shared representation space.However, such modals trained on a contrastive alignment can lack stabilitytowards small input perturbations. Especially when dealing with manuallyexpressed queries, minor variations in the query can cause large differences inthe ranking of the best-matching results. In this paper, we present asystematic analysis of the effect of multiple classes of non-semantic queryperturbations in an multimedia information retrieval scenario. We evaluate adiverse set of lexical, syntactic, and semantic perturbations across multipleCLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 videocollection. Across models, we find that syntactic and semantic perturbationsdrive the largest instabilities, while brittleness is concentrated in trivialsurface edits such as punctuation and case. Our results highlight robustness asa critical dimension for evaluating vision-language models beyond benchmarkaccuracy.</description><author>Allie Tran, Luca Rossetto</author><pubDate>Fri, 07 Nov 2025 18:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04247v2</guid></item><item><title>The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2</title><link>http://arxiv.org/abs/2511.05461v1</link><description>Natural disasters demand rapid damage assessment to guide humanitarianresponse. Here, we investigate whether medium-resolution Earth observationimages from the Copernicus program can support building damage assessment,complementing very-high resolution imagery with often limited availability. Weintroduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs fromboth Sentinel-1 and Sentinel-2, spatially and temporally aligned with theestablished xBD benchmark. In a series of experiments, we demonstrate thatbuilding damage can be detected and mapped rather well in many disasterscenarios, despite the moderate 10$\,$m ground sampling distance. We also findthat, for damage mapping at that resolution, architectural sophistication doesnot seem to bring much advantage: more complex model architectures tend tostruggle with generalization to unseen disasters, and geospatial foundationmodels bring little practical benefit. Our results suggest that Copernicusimages are a viable data source for rapid, wide-area damage assessment andcould play an important role alongside VHR imagery. We release the xBD-S12dataset, code, and trained models to support further research.</description><author>Olivier Dietrich, Merlin Alfredsson, Emilia Arens, Nando Metzger, Torben Peters, Linus Scheibenreif, Jan Dirk Wegner, Konrad Schindler</author><pubDate>Fri, 07 Nov 2025 18:02:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05461v1</guid></item><item><title>Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models</title><link>http://arxiv.org/abs/2511.05460v1</link><description>Pre-trained Time Series Foundational Models (TSFMs) represent a significantadvance, capable of forecasting diverse time series with complexcharacteristics, including varied seasonalities, trends, and long-rangedependencies. Despite their primary goal of universal time series forecasting,their efficacy is far from uniform; divergent training protocols and datasources cause individual TSFMs to exhibit highly variable performance acrossdifferent forecasting tasks, domains, and horizons. Leveraging thiscomplementary expertise by arbitrating existing TSFM outputs presents acompelling strategy, yet this remains a largely unexplored area of research. Inthis paper, we conduct a thorough examination of how different TSFMs exhibitspecialized performance profiles across various forecasting settings, and howwe can effectively leverage this behavior in arbitration between different timeseries models. We specifically analyze how factors such as model selection andforecast horizon distribution can influence the efficacy of arbitrationstrategies. Based on this analysis, we propose Synapse, a novel arbitrationframework for TSFMs. Synapse is designed to dynamically leverage a pool ofTSFMs, assign and adjust predictive weights based on their relative,context-dependent performance, and construct a robust forecast distribution byadaptively sampling from the output quantiles of constituent models.Experimental results demonstrate that Synapse consistently outperforms otherpopular ensembling techniques as well as individual TSFMs, demonstratingSynapse's efficacy in time series forecasting.</description><author>Sarkar Snigdha Sarathi Das, Palash Goyal, Mihir Parmar, Yiwen Song, Long T. Le, Lesly Miculicich, Jinsung Yoon, Rui Zhang, Hamid Palangi, Tomas Pfister</author><pubDate>Fri, 07 Nov 2025 18:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05460v1</guid></item><item><title>SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models</title><link>http://arxiv.org/abs/2511.05459v1</link><description>Evaluating large language models (LLMs) for software engineering has beenlimited by narrow task coverage, language bias, and insufficient alignment withreal-world developer workflows. Existing benchmarks often focus on algorithmicproblems or Python-centric bug fixing, leaving critical dimensions of softwareengineering underexplored. To address these gaps, we introduce SWE-Compass1, acomprehensive benchmark that unifies heterogeneous code-related evaluationsinto a structured and production-aligned framework. SWE-Compass spans 8 tasktypes, 8 programming scenarios, and 10 programming languages, with 2000high-quality instances curated from authentic GitHub pull requests and refinedthrough systematic filtering and validation. We benchmark ten state-of-the-artLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clearhierarchy of difficulty across task types, languages, and scenarios. Moreover,by aligning evaluation with real-world developer practices, SWE-Compassprovides a rigorous and reproducible foundation for diagnosing and advancingagentic coding capabilities in large language models.</description><author>Jingxuan Xu, Ken Deng, Weihao Li, Songwei Yu, Huaixi Tang, Haoyang Huang, Zhiyi Lai, Zizheng Zhan, Yanan Wu, Chenchen Zhang, Kepeng Lei, Yifan Yao, Xinping Lei, Wenqiang Zhu, Zongxian Feng, Han Li, Junqi Xiong, Dailin Li, Zuchen Gao, Kun Wu, Wen Xiang, Ziqi Zhan, Yuanxing Zhang, Wuxuan Gong, Ziyuan Gao, Guanxiang Wang, Yirong Xue, Xiaojiang Zhang, Jinghui Wang, Huiming Wang, Wenhao Zhuang, Zhaoxiang Zhang, Yuqun Zhang, Haotian Zhang, Bin Chen, Jiaheng Liu</author><pubDate>Fri, 07 Nov 2025 18:01:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05459v1</guid></item><item><title>FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models</title><link>http://arxiv.org/abs/2403.20105v2</link><description>Foundation models have exhibited unprecedented capabilities in tackling manydomains and tasks. Models such as CLIP are currently widely used to bridgecross-modal representations, and text-to-image diffusion models are arguablythe leading models in terms of realistic image generation. Image generativemodels are trained on massive datasets that provide them with powerful internalspatial representations. In this work, we explore the potential benefits ofsuch representations, beyond image generation, in particular, for dense visualprediction tasks. We focus on the task of image segmentation, which istraditionally solved by training models on closed-vocabulary datasets, withpixel-level annotations. To avoid the annotation cost or training largediffusion models, we constraint our setup to be zero-shot and training-free. Ina nutshell, our pipeline leverages different and relatively small-sized,open-source foundation models for zero-shot open-vocabulary segmentation. Thepipeline is as follows: the image is passed to both a captioner model (i.e.BLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a textdescription and visual representation, respectively. The features are clusteredand binarized to obtain class agnostic masks for each object. These masks arethen mapped to a textual class, using the CLIP model to supportopen-vocabulary. Finally, we add a refinement step that allows to obtain a moreprecise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does notrely on any training, outperforms many training-based approaches on both PascalVOC and COCO datasets. In addition, we show very competitive results comparedto the recent weakly-supervised segmentation approaches. We providecomprehensive experiments showing the superiority of diffusion model featurescompared to other pretrained models. Project page:https://bcorrad.github.io/freesegdiff/</description><author>Barbara Toniella Corradini, Mustafa Shukor, Paul Couairon, Guillaume Couairon, Franco Scarselli, Matthieu Cord</author><pubDate>Fri, 07 Nov 2025 17:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20105v2</guid></item><item><title>Parameter-Efficient Conditioning for Material Generalization in Graph-Based Simulators</title><link>http://arxiv.org/abs/2511.05456v1</link><description>Graph network-based simulators (GNS) have demonstrated strong potential forlearning particle-based physics (such as fluids, deformable solids, andgranular flows) while generalizing to unseen geometries due to their inherentinductive biases. However, existing models are typically trained for a singlematerial type and fail to generalize across distinct constitutive behaviors,limiting their applicability in real-world engineering settings. Using granularflows as a running example, we propose a parameter-efficient conditioningmechanism that makes the GNS model adaptive to material parameters. We identifythat sensitivity to material properties is concentrated in the earlymessage-passing (MP) layers, a finding we link to the local nature ofconstitutive models (e.g., Mohr-Coulomb) and their effects on informationpropagation. We empirically validate this by showing that fine-tuning only thefirst few (1-5) of 10 MP layers of a pretrained model achieves comparable testperformance as compared to fine-tuning the entire network. Building on thisinsight, we propose a parameter-efficient Feature-wise Linear Modulation (FiLM)conditioning mechanism designed to specifically target these early layers. Thisapproach produces accurate long-term rollouts on unseen, interpolated, ormoderately extrapolated values (e.g., up to 2.5 degrees for friction angle and0.25 kPa for cohesion) when trained exclusively on as few as 12 shortsimulation trajectories from new materials, representing a 5-fold datareduction compared to a baseline multi-task learning method. Finally, wevalidate the model's utility by applying it to an inverse problem, successfullyidentifying unknown cohesion parameters from trajectory data. This approachenables the use of GNS in inverse design and closed-loop control tasks wherematerial properties are treated as design variables.</description><author>Naveen Raj Manoharan, Hassan Iqbal, Krishna Kumar</author><pubDate>Fri, 07 Nov 2025 17:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05456v1</guid></item><item><title>Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in Interactive Urban Driving</title><link>http://arxiv.org/abs/2508.14926v3</link><description>Autonomous vehicles hold great promise for reducing traffic fatalities andimproving transportation efficiency, yet their widespread adoption hinges onembedding credible and transparent ethical reasoning into routine and emergencymaneuvers, particularly to protect vulnerable road users (VRUs) such aspedestrians and cyclists. Here, we present a hierarchical Safe ReinforcementLearning (Safe RL) framework that augments standard driving objectives withethics-aware cost signals. At the decision level, a Safe RL agent is trainedusing a composite ethical risk cost, combining collision probability and harmseverity, to generate high-level motion targets. A dynamic, risk-sensitivePrioritized Experience Replay mechanism amplifies learning from rare butcritical, high-risk events. At the execution level, polynomial path planningcoupled with Proportional-Integral-Derivative (PID) and Stanley controllerstranslates these targets into smooth, feasible trajectories, ensuring bothaccuracy and comfort. We train and validate our approach on closed-loopsimulation environments derived from large-scale, real-world traffic datasetsencompassing diverse vehicles, cyclists, and pedestrians, and demonstrate thatit outperforms baseline methods in reducing risk to others while maintainingego performance and comfort. This work provides a reproducible benchmark forSafe RL with explicitly ethics-aware objectives in human-mixed trafficscenarios. Our results highlight the potential of combining formal controltheory and data-driven learning to advance ethically accountable autonomy thatexplicitly protects those most at risk in urban traffic environments. Acrosstwo interactive benchmarks and five random seeds, our policy decreases conflictfrequency by 25-45% compared to matched task successes while maintainingcomfort metrics within 5%.</description><author>Dianzhao Li, Ostap Okhrin</author><pubDate>Fri, 07 Nov 2025 17:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14926v3</guid></item><item><title>P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication</title><link>http://arxiv.org/abs/2507.23247v2</link><description>Although explainability and interpretability have received significantattention in artificial intelligence (AI) and natural language processing (NLP)for mental health, reasoning has not been examined in the same depth.Addressing this gap is essential to bridge NLP and mental health throughinterpretable and reasoning-capable AI systems. To this end, we investigate thepragmatic reasoning capability of large-language models (LLMs) in the mentalhealth domain. We introduce PRiMH dataset, and propose pragmatic reasoningtasks in mental health with pragmatic implicature and presupposition phenomena.In particular, we formulate two tasks in implicature and one task inpresupposition. To benchmark the dataset and the tasks presented, we considerfour models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of theexperiments suggest that Mistral and Qwen show substantial reasoning abilitiesin the domain. Subsequently, we study the behavior of MentaLLaMA on theproposed reasoning tasks with the rollout attention mechanism. In addition, wealso propose three StiPRompts to study the stigma around mental health with thestate-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Ourevaluated findings show that Claude-3.5-haiku deals with stigma moreresponsibly compared to the other two LLMs.</description><author>Sneha Oram, Pushpak Bhattacharyya</author><pubDate>Fri, 07 Nov 2025 17:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.23247v2</guid></item><item><title>Self-adaptive weighting and sampling for physics-informed neural networks</title><link>http://arxiv.org/abs/2511.05452v1</link><description>Physics-informed deep learning has emerged as a promising framework forsolving partial differential equations (PDEs). Nevertheless, training thesemodels on complex problems remains challenging, often leading to limitedaccuracy and efficiency. In this work, we introduce a hybrid adaptive samplingand weighting method to enhance the performance of physics-informed neuralnetworks (PINNs). The adaptive sampling component identifies training points inregions where the solution exhibits rapid variation, while the adaptiveweighting component balances the convergence rate across training points.Numerical experiments show that applying only adaptive sampling or onlyadaptive weighting is insufficient to consistently achieve accuratepredictions, particularly when training points are scarce. Since each methodemphasizes different aspects of the solution, their effectiveness is problemdependent. By combining both strategies, the proposed framework consistentlyimproves prediction accuracy and training efficiency, offering a more robustapproach for solving PDEs with PINNs.</description><author>Wenqian Chen, Amanda Howard, Panos Stinis</author><pubDate>Fri, 07 Nov 2025 17:48:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05452v1</guid></item><item><title>How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</title><link>http://arxiv.org/abs/2511.05449v1</link><description>Recent advances in 3D point cloud transformers have led to state-of-the-artresults in tasks such as semantic segmentation and reconstruction. However,these models typically rely on dense token representations, incurring highcomputational and memory costs during training and inference. In this work, wepresent the finding that tokens are remarkably redundant, leading tosubstantial inefficiency. We introduce gitmerge3D, a globally informed graphtoken merging method that can reduce the token count by up to 90-95% whilemaintaining competitive performance. This finding challenges the prevailingassumption that more tokens inherently yield better performance and highlightsthat many current models are over-tokenized and under-optimized forscalability. We validate our method across multiple 3D vision tasks and showconsistent improvements in computational efficiency. This work is the first toassess redundancy in large-scale 3D transformer models, providing insights intothe development of more efficient 3D foundation architectures. Our code andcheckpoints are publicly available at https://gitmerge3d.github.io</description><author>Tuan Anh Tran, Duy M. H. Nguyen, Hoai-Chau Tran, Michael Barz, Khoa D. Doan, Roger Wattenhofer, Ngo Anh Vien, Mathias Niepert, Daniel Sonntag, Paul Swoboda</author><pubDate>Fri, 07 Nov 2025 17:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05449v1</guid></item><item><title>Linear combinations of latents in generative models: subspaces and beyond</title><link>http://arxiv.org/abs/2408.08558v8</link><description>Sampling from generative models has become a crucial tool for applicationslike data synthesis and augmentation. Diffusion, Flow Matching and ContinuousNormalising Flows have shown effectiveness across various modalities, and relyon latent variables for generation. For experimental design or creativeapplications that require more control over the generation process, it hasbecome common to manipulate the latent variable directly. However, existingapproaches for performing such manipulations (e.g. interpolation or forminglow-dimensional representations) only work well in special cases or are networkor data-modality specific. We propose Latent Optimal Linear combinations (LOL)as a general-purpose method to form linear combinations of latent variablesthat adhere to the assumptions of the generative model. As LOL is easy toimplement and naturally addresses the broader task of forming any linearcombinations, e.g. the construction of subspaces of the latent space, LOLdramatically simplifies the creation of expressive low-dimensionalrepresentations of high-dimensional objects.</description><author>Erik Bodin, Alexandru Stere, Dragos D. Margineantu, Carl Henrik Ek, Henry Moss</author><pubDate>Fri, 07 Nov 2025 17:37:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08558v8</guid></item><item><title>Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants</title><link>http://arxiv.org/abs/2511.02043v3</link><description>Attention is a fundamental building block of large language models (LLMs), sothere have been many efforts to implement it efficiently. For example,FlashAttention leverages tiling and kernel fusion to optimize attention.Recently, a number of variants of attention have been introduced to enhancemodel quality or efficiency. Supporting them efficiently remains difficultsince they usually require specialized kernels or hand-tuned implementations.FlexAttention recently addressed part of this gap by using static programmingtemplates to support FlashAttention-like kernels for a subset of attentionvariants. In this paper, we introduce Flashlight, a compiler-native framework withinthe PyTorch ecosystem that automatically generates fused, FlashAttention-stylekernels for arbitrary attention-based programs, without relying on statictemplates or predefined kernel specializations. Flashlight leverages PyTorch'scompilation workflow to fuse and tile attention computations transparently,enabling efficient execution for diverse attention patterns. Not only does itsupport all variants expressible in the FlexAttention model but it also handlesmore general, data-dependent attention formulations that are beyond thecapabilities of FlexAttention. Our results show that Flashlight produces kernels with competitive orsuperior performance to FlexAttention, while offering the flexibility of nativePyTorch code, enabling developers to rapidly explore new attention modelswithout sacrificing performance.</description><author>Bozhi You, Irene Wang, Zelal Su Mustafaoglu, Abhinav Jangda, Angélica Moreira, Roshan Dathathri, Divya Mahajan, Keshav Pingali</author><pubDate>Fri, 07 Nov 2025 17:26:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.02043v3</guid></item><item><title>Adversarially Robust Multitask Adaptive Control</title><link>http://arxiv.org/abs/2511.05444v1</link><description>We study adversarially robust multitask adaptive linear quadratic control; asetting where multiple systems collaboratively learn control policies undermodel uncertainty and adversarial corruption. We propose a clustered multitaskapproach that integrates clustering and system identification with resilientaggregation to mitigate corrupted model updates. Our analysis characterizes howclustering accuracy, intra-cluster heterogeneity, and adversarial behavioraffect the expected regret of certainty-equivalent (CE) control across LQRtasks. We establish non-asymptotic bounds demonstrating that the regretdecreases inversely with the number of honest systems per cluster and that thisreduction is preserved under a bounded fraction of adversarial systems withineach cluster.</description><author>Kasra Fallah, Leonardo F. Toso, James Anderson</author><pubDate>Fri, 07 Nov 2025 17:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05444v1</guid></item><item><title>APP: Accelerated Path Patching with Task-Specific Pruning</title><link>http://arxiv.org/abs/2511.05442v1</link><description>Circuit discovery is a key step in many mechanistic interpretabilitypipelines. Current methods, such as Path Patching, are computationallyexpensive and have limited in-depth circuit analysis for smaller models. Inthis study, we propose Accelerated Path Patching (APP), a hybrid approachleveraging our novel contrastive attention head pruning method to drasticallyreduce the search space of circuit discovery methods. Our Contrastive-FLAPpruning algorithm uses techniques from causal mediation analysis to assignhigher pruning scores to task-specific attention heads, leading to higherperforming sparse models compared to traditional pruning techniques. AlthoughContrastive-FLAP is successful at preserving task-specific heads that existingpruning algorithms remove at low sparsity ratios, the circuits found byContrastive-FLAP alone are too large to satisfy the minimality constraintrequired in circuit analysis. APP first applies Contrastive-FLAP to reduce thesearch space on required for circuit discovery algorithms by, on average, 56\%.Next, APP, applies traditional Path Patching on the remaining attention heads,leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied tothe dense model. Despite the substantial computational saving that APPprovides, circuits obtained from APP exhibit substantial overlap and similarperformance to previously established Path Patching circuits</description><author>Frauke Andersen, William Rudman, Ruochen Zhang, Carsten Eickhoff</author><pubDate>Fri, 07 Nov 2025 17:20:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05442v1</guid></item><item><title>In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies</title><link>http://arxiv.org/abs/2405.01425v3</link><description>We present a new random walk for uniformly sampling high-dimensional convexbodies. It achieves state-of-the-art runtime complexity with strongerguarantees on the output than previously known, namely in R\'enyi divergence(which implies TV, $\mathcal{W}_2$, KL, $\chi^2$). The proof departs from knownapproaches for polytime algorithms for the problem -- we utilize a stochasticdiffusion perspective to show contraction to the target distribution with therate of convergence determined by functional isoperimetric constants of thetarget distribution.</description><author>Yunbum Kook, Santosh S. Vempala, Matthew S. Zhang</author><pubDate>Fri, 07 Nov 2025 17:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01425v3</guid></item><item><title>Large language models as uncertainty-calibrated optimizers for experimental discovery</title><link>http://arxiv.org/abs/2504.06265v3</link><description>Scientific discovery increasingly depends on efficient experimentaloptimization to navigate vast design spaces under time and resourceconstraints. Traditional approaches often require extensive domain expertiseand feature engineering. While large language models, with their vastscientific knowledge, circumvent the feature engineering limitations, they lackthe calibrated uncertainty estimates required for high-stakes decision making.Hence, current optimization methods force a choice between domain knowledge andreliability, with no principled approach that affords both. In this work, weshow that training language models through the uncertainty-aware objectives oftraditional optimization methods enables their use as reliable optimizersguided by natural language. By teaching LLMs from experimental outcomes underuncertainty, we transform their overconfidence from a fundamental limitationinto a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, acornerstone of pharmaceutical synthesis, our method nearly doubles thediscovery rate of high-yielding reaction conditions, from 24% to 43% in 50experimental iterations starting from 10 unsuccessful conditions. Across 19diverse optimization problems spanning organic synthesis, materials science andcatalysis, process chemistry, and molecular design, our approach ranks first onaverage, establishing a new paradigm for reliable, uncertainty-guidedoptimization with LLMs. Our approach can accelerate discovery by lowering thebarrier to using powerful optimization methods, replacing the need fordomain-specific feature engineering with more accessible natural languageinterfaces. These findings highlight that ensuring reliability throughprincipled uncertainty quantification is critical for realizing the fullpotential of AI-guided experimentation.</description><author>Bojana Ranković, Ryan-Rhys Griffiths, Philippe Schwaller</author><pubDate>Fri, 07 Nov 2025 17:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.06265v3</guid></item><item><title>Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis</title><link>http://arxiv.org/abs/2511.05432v1</link><description>We propose a text-to-talking-face synthesis framework leveraging latentspeech representations from HierSpeech++. A Text-to-Vec module generatesWav2Vec2 embeddings from text, which jointly condition speech and facegeneration. To handle distribution shifts between clean and TTS-predictedfeatures, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings andfinetuning on TTS outputs. This enables tight audio-visual alignment, preservesspeaker identity, and produces natural, expressive speech and synchronizedfacial motion without ground-truth audio at inference. Experiments show thatconditioning on TTS-predicted latent features outperforms cascaded pipelines,improving both lip-sync and visual realism.</description><author>Dogucan Yaman, Seymanur Akti, Fevziye Irem Eyiokur, Alexander Waibel</author><pubDate>Fri, 07 Nov 2025 17:07:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05432v1</guid></item><item><title>Comparative Study on Noise-Augmented Training and its Effect on Adversarial Robustness in ASR Systems</title><link>http://arxiv.org/abs/2409.01813v4</link><description>In this study, we investigate whether noise-augmented training canconcurrently improve adversarial robustness in automatic speech recognition(ASR) systems. We conduct a comparative analysis of the adversarial robustnessof four different ASR architectures, each trained under three differentaugmentation conditions: (1) background noise, speed variations, andreverberations; (2) speed variations only; (3) no data augmentation. We thenevaluate the robustness of all resulting models against attacks with white-boxor black-box adversarial examples. Our results demonstrate that noiseaugmentation not only enhances model performance on noisy speech but alsoimproves the model's robustness to adversarial attacks.</description><author>Karla Pizzi, Matías Pizarro, Asja Fischer</author><pubDate>Fri, 07 Nov 2025 17:07:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01813v4</guid></item><item><title>"I Like That You Have to Poke Around": Instructors on How Experiential Approaches to AI Literacy Spark Inquiry and Critical Thinking</title><link>http://arxiv.org/abs/2511.05430v1</link><description>As artificial intelligence (AI) increasingly shapes decision-making acrossdomains, there is a growing need to support AI literacy among learners beyondcomputer science. However, many current approaches rely on programming-heavytools or abstract lecture-based content, limiting accessibility for non-STEMaudiences. This paper presents findings from a study of AI User, a modular,web-based curriculum that teaches core AI concepts through interactive, no-codeprojects grounded in real-world scenarios. The curriculum includes eightprojects; this study focuses on instructor feedback on Projects 5-8, whichaddress applied topics such as natural language processing, computer vision,decision support, and responsible AI. Fifteen community college instructorsparticipated in structured focus groups, completing the projects as learnersand providing feedback through individual reflection and group discussion.Using thematic analysis, we examined how instructors evaluated the design,instructional value, and classroom applicability of these experientialactivities. Findings highlight instructors' appreciation for exploratory tasks,role-based simulations, and real-world relevance, while also surfacing designtrade-offs around cognitive load, guidance, and adaptability for diverselearners. This work extends prior research on AI literacy by centeringinstructor perspectives on teaching complex AI topics without code. It offersactionable insights for designing inclusive, experiential AI learning resourcesthat scale across disciplines and learner backgrounds.</description><author>Aparna Maya Warrier, Arav Agarwal, Jaromir Savelka, Christopher Bogart, Heather Burte</author><pubDate>Fri, 07 Nov 2025 17:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05430v1</guid></item><item><title>A Unified Framework for Inference with General Missingness Patterns and Machine Learning Imputation</title><link>http://arxiv.org/abs/2508.15162v2</link><description>Pre-trained machine learning (ML) predictions have been increasingly used tocomplement incomplete data to enable downstream scientific inquiries, but theirnaive integration risks biased inferences. Recently, multiple methods have beendeveloped to provide valid inference with ML imputations regardless ofprediction quality and to enhance efficiency relative to complete-caseanalyses. However, existing approaches are often limited to missing outcomesunder a missing-completely-at-random (MCAR) assumption, failing to handlegeneral missingness patterns (missing in both the outcome and exposures) underthe more realistic missing-at-random (MAR) assumption. This paper develops anovel method that delivers a valid statistical inference framework for generalZ-estimation problems using ML imputations under the MAR assumption and forgeneral missingness patterns. The core technical idea is to stratifyobservations by distinct missingness patterns and construct an estimator byappropriately weighting and aggregating pattern-specific information through amasking-and-imputation procedure on the complete cases. We provide theoreticalguarantees of asymptotic normality of the proposed estimator and efficiencydominance over weighted complete-case analyses. Practically, the method affordssimple implementations by leveraging existing weighted complete-case analysissoftware. Extensive simulations are carried out to validate theoreticalresults. A real data example is provided to further illustrate the practicalutility of the proposed method. The paper concludes with a brief discussion onpractical implications, limitations, and potential future directions.</description><author>Xingran Chen, Tyler McCormick, Bhramar Mukherjee, Zhenke Wu</author><pubDate>Fri, 07 Nov 2025 17:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15162v2</guid></item><item><title>Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing</title><link>http://arxiv.org/abs/2511.00801v3</link><description>Medical image editing has emerged as a pivotal technology with broadapplications in data augmentation, model interpretability, medical education,and treatment simulation. However, the lack of large-scale, high-quality, andopenly accessible datasets tailored for medical contexts with strict anatomicaland clinical constraints has significantly hindered progress in this domain. Tobridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over50k medically curated image edits spanning chest X-ray, brain MRI, and fundusphotography across 23 diseases. Each sample supports bidirectional lesionediting (addition and removal) and is constructed using Gemini-2.5-Flash-Imagebased on real clinical images. A key differentiator of our dataset is themedically grounded quality control protocol: we employ an LLM-as-Judgeevaluation framework with criteria such as instruction compliance, structuralplausibility, image realism, and fidelity preservation, alongside iterativerefinement over up to five rounds. Additionally, Med-Banana-50K includes around37,000 failed editing attempts with full evaluation logs to support preferencelearning and alignment research. By offering a large-scale, medically rigorous,and fully documented resource, Med-Banana-50K establishes a critical foundationfor developing and evaluating reliable medical image editing systems. Ourdataset and code are publicly available.[https://github.com/richardChenzhihui/med-banana-50k].</description><author>Zhihui Chen, Mengling Feng</author><pubDate>Fri, 07 Nov 2025 16:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.00801v3</guid></item><item><title>Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration</title><link>http://arxiv.org/abs/2511.05421v1</link><description>Continual learning is an emerging topic in the field of deep learning, wherea model is expected to learn continuously for new upcoming tasks withoutforgetting previous experiences. This field has witnessed numerousadvancements, but few works have been attempted in the direction of imagerestoration. Handling large image sizes and the divergent nature of variousdegradation poses a unique challenge in the restoration domain. However,existing works require heavily engineered architectural modifications for newtask adaptation, resulting in significant computational overhead.Regularization-based methods are unsuitable for restoration, as differentrestoration challenges require different kinds of feature processing. In thisdirection, we propose a simple modification of the convolution layer to adaptthe knowledge from previous restoration tasks without touching the mainbackbone architecture. Therefore, it can be seamlessly applied to any deeparchitecture without any structural modifications. Unlike other approaches, wedemonstrate that our model can increase the number of trainable parameterswithout significantly increasing computational overhead or inference time.Experimental validation demonstrates that new restoration tasks can beintroduced without compromising the performance of existing tasks. We also showthat performance on new restoration tasks improves by adapting the knowledgefrom the knowledge base created by previous restoration tasks. The code isavailable at https://github.com/aupendu/continual-restore.</description><author>Aupendu Kar, Krishnendu Ghosh, Prabir Kumar Biswas</author><pubDate>Fri, 07 Nov 2025 16:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05421v1</guid></item><item><title>ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids</title><link>http://arxiv.org/abs/2511.05420v1</link><description>As smart grids evolve to meet growing energy demands and modern operationalchallenges, the ability to accurately predict faults becomes increasinglycritical. However, existing AI-based fault prediction models struggle to ensurereliability in evolving environments where they are required to adapt to newfault types and operational zones. In this paper, we propose a continuallearning (CL) framework in the smart grid context to evolve the model togetherwith the environment. We design four realistic evaluation scenarios grounded inclass-incremental and domain-incremental learning to emulate evolving gridconditions. We further introduce Prototype-based Dark Experience Replay(ProDER), a unified replay-based approach that integrates prototype-basedfeature regularization, logit distillation, and a prototype-guided replaymemory. ProDER achieves the best performance among tested CL techniques, withonly a 0.045 accuracy drop for fault type prediction and 0.015 for fault zoneprediction. These results demonstrate the practicality of CL for scalable,real-world fault prediction in smart grids.</description><author>Emad Efatinasab, Nahal Azadi, Davide Dalle Pezze, Gian Antonio Susto, Chuadhry Mujeeb Ahmed, Mirco Rampazzo</author><pubDate>Fri, 07 Nov 2025 16:51:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.05420v1</guid></item><item><title>LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence</title><link>http://arxiv.org/abs/2509.03505v2</link><description>We argue that progress toward general intelligence requires complementaryfoundation models grounded in language, the physical world, and structureddata. This report presents LimiX-16M and LimiX-2M, two instantiations of ourlarge structured-data models (LDMs). Both models treat structured data as ajoint distribution over variables and missingness, thus capable of addressing awide range of tabular tasks through query-based conditional prediction via asingle model. They are pretrained using masked joint-distribution modeling withan episodic, context-conditional objective, supporting rapid, training-freeadaptation at inference. We evaluate LimiX models across 11 largestructured-data benchmarks with broad regimes of sample size, featuredimensionality, class number, categorical-to-numerical feature ratio,missingness, and sample-to-feature ratios. LimiX-16M consistently surpassesstrong baselines, as shown in Figure 1 and Figure 2. The superiority holdsacross a wide range of tasks, such as classification, regression, missing valueimputation, and data generation, often by substantial margins, while avoidingtask-specific architectures or bespoke training per task. Notably, LimiX-2Mdelivers strong results under tight compute and memory budgets. We also presentthe first scaling law study for LDMs, revealing how data and model scalingjointly influence downstream performance and offering quantitative guidance fortabular foundation modeling. All LimiX models are publicly accessible underApache 2.0.</description><author>Xingxuan Zhang, Gang Ren, Han Yu, Hao Yuan, Hui Wang, Jiansheng Li, Jiayun Wu, Lang Mo, Li Mao, Mingchao Hao, Ningbo Dai, Renzhe Xu, Shuyang Li, Tianyang Zhang, Yue He, Yuanrui Wang, Yunjia Zhang, Zijing Xu, Dongzhe Li, Fang Gao, Hao Zou, Jiandong Liu, Jiashuo Liu, Jiawei Xu, Kaijie Cheng, Kehan Li, Linjun Zhou, Qing Li, Shaohua Fan, Xiaoyu Lin, Xinyan Han, Xuanyue Li, Yan Lu, Yuan Xue, Yuanyuan Jiang, Zimu Wang, Zhenlei Wang, Peng Cui</author><pubDate>Fri, 07 Nov 2025 16:49:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.03505v2</guid></item><item><title>Inference-Time Hyper-Scaling with KV Cache Compression</title><link>http://arxiv.org/abs/2506.05345v2</link><description>Inference-time scaling trades efficiency for increased reasoning accuracy bygenerating longer or more parallel sequences. However, in Transformer LLMs,generation cost is bottlenecked by the size of the key-value (KV) cache, ratherthan the number of generated tokens. Hence, we explore inference-timehyper-scaling: by compressing the KV cache, we can generate more tokens withinthe same compute budget and further improve the accuracy of scaled inference.The success of this approach, however, hinges on the ability of compressionmethods to preserve accuracy even at high compression ratios. To makehyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), anovel method for sparsifying KV caches that only requires 1K training steps toachieve 8$\times$ compression, while maintaining better accuracy thantraining-free sparse attention. Instead of prematurely discarding cachedtokens, DMS delays token eviction, implicitly merging representations andpreserving critical information. We demonstrate the effectiveness ofinference-time hyper-scaling with DMS on multiple families of LLMs, showingthat it boosts accuracy for comparable inference latency and memory load. Forinstance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and9.7 on LiveCodeBench on average for an equivalent number of memory reads.</description><author>Adrian Łańcucki, Konrad Staniszewski, Piotr Nawrot, Edoardo M. Ponti</author><pubDate>Fri, 07 Nov 2025 16:42:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05345v2</guid></item><item><title>Dark Energy Survey Year 3 results: Simulation-based $w$CDM inference from weak lensing and galaxy clustering maps with deep learning. I. Analysis design</title><link>http://arxiv.org/abs/2511.04681v1</link><description>Data-driven approaches using deep learning are emerging as powerfultechniques to extract non-Gaussian information from cosmological large-scalestructure. This work presents the first simulation-based inference (SBI)pipeline that combines weak lensing and galaxy clustering maps in a realisticDark Energy Survey Year 3 (DES Y3) configuration and serves as preparation fora forthcoming analysis of the survey data. We develop a scalable forward modelbased on the CosmoGridV1 suite of N-body simulations to generate over onemillion self-consistent mock realizations of DES Y3 at the map level.Leveraging this large dataset, we train deep graph convolutional neuralnetworks on the full survey footprint in spherical geometry to learnlow-dimensional features that approximately maximize mutual information withtarget parameters. These learned compressions enable neural density estimationof the implicit likelihood via normalizing flows in a ten-dimensional parameterspace spanning cosmological $w$CDM, intrinsic alignment, and linear galaxy biasparameters, while marginalizing over baryonic, photometric redshift, and shearbias nuisances. To ensure robustness, we extensively validate our inferencepipeline using synthetic observations derived from both systematiccontaminations in our forward model and independent Buzzard galaxy catalogs.Our forecasts yield significant improvements in cosmological parameterconstraints, achieving $2-3\times$ higher figures of merit in the $\Omega_m -S_8$ plane relative to our implementation of baseline two-point statistics andeffectively breaking parameter degeneracies through probe combination. Theseresults demonstrate the potential of SBI analyses powered by deep learning forupcoming Stage-IV wide-field imaging surveys.</description><author>A. Thomsen, J. Bucko, T. Kacprzak, V. Ajani, J. Fluri, A. Refregier, D. Anbajagane, F. J. Castander, A. Ferté, M. Gatti, N. Jeffrey, A. Alarcon, A. Amon, K. Bechtol, M. R. Becker, G. M. Bernstein, A. Campos, A. Carnero Rosell, C. Chang, R. Chen, A. Choi, M. Crocce, C. Davis, J. DeRose, S. Dodelson, C. Doux, K. Eckert, J. Elvin-Poole, S. Everett, P. Fosalba, D. Gruen, I. Harrison, K. Herner, E. M. Huff, M. Jarvis, N. Kuropatkin, P. -F. Leget, N. MacCrann, J. McCullough, J. Myles, A. Navarro-Alsina, S. Pandey, A. Porredon, J. Prat, M. Raveri, M. Rodriguez-Monroy, R. P. Rollins, A. Roodman, E. S. Rykoff, C. Sánchez, L. F. Secco, E. Sheldon, T. Shin, M. A. Troxel, I. Tutusaus, T. N. Varga, N. Weaverdyck, R. H. Wechsler, B. Yanny, B. Yin, Y. Zhang, J. Zuntz, S. Allam, F. Andrade-Oliveira, D. Ba</author><pubDate>Thu, 06 Nov 2025 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04681v1</guid></item><item><title>TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models</title><link>http://arxiv.org/abs/2505.23769v2</link><description>Image-text models excel at image-level tasks but struggle with detailedvisual understanding. While these models provide strong visual-languagealignment, segmentation models like SAM2 offer precise spatial boundaries forobjects. To this end, we propose TextRegion, a simple, effective, andtraining-free framework that combines the strengths of image-text models andSAM2 to generate powerful text-aligned region tokens. These tokens enabledetailed visual understanding while preserving open-vocabulary capabilities.They can be directly applied to various downstream tasks, including open-worldsemantic segmentation, referring expression comprehension, and grounding. Weconduct extensive evaluations and consistently achieve superior or competitiveperformance compared to state-of-the-art training-free methods. Additionally,our framework is compatible with many image-text models, making it highlypractical and easily extensible as stronger models emerge. Code is availableat: https://github.com/avaxiao/TextRegion.</description><author>Yao Xiao, Qiqian Fu, Heyi Tao, Yuqun Wu, Zhen Zhu, Derek Hoiem</author><pubDate>Thu, 06 Nov 2025 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.23769v2</guid></item><item><title>Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping</title><link>http://arxiv.org/abs/2511.04680v1</link><description>Automatic image cropping is a method for maximizing the human-perceivedquality of cropped regions in photographs. Although several works have proposedtechniques for producing singular crops, little work has addressed the problemof producing multiple, distinct crops with aesthetic appeal. In this paper, wemotivate the problem with a discussion on modern social media applications,introduce a dataset of 277 relevant images and human labels, and evaluate theefficacy of several single-crop models with an image partitioning algorithm asa pre-processing step. The dataset is available athttps://github.com/RafeLoya/carousel.</description><author>Rafe Loya, Andrew Hamara, Benjamin Estell, Benjamin Kilpatrick, Andrew C. Freeman</author><pubDate>Thu, 06 Nov 2025 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04680v1</guid></item><item><title>GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction</title><link>http://arxiv.org/abs/2511.04679v1</link><description>Humanoid robots are expected to operate in human-centered environments wheresafe and natural physical interaction is essential. However, most recentreinforcement learning (RL) policies emphasize rigid tracking and suppressexternal forces. Existing impedance-augmented approaches are typicallyrestricted to base or end-effector control and focus on resisting extremeforces rather than enabling compliance. We introduce GentleHumanoid, aframework that integrates impedance control into a whole-body motion trackingpolicy to achieve upper-body compliance. At its core is a unified spring-basedformulation that models both resistive contacts (restoring forces when pressingagainst surfaces) and guiding contacts (pushes or pulls sampled from humanmotion data). This formulation ensures kinematically consistent forces acrossthe shoulder, elbow, and wrist, while exposing the policy to diverseinteraction scenarios. Safety is further supported through task-adjustableforce thresholds. We evaluate our approach in both simulation and on theUnitree G1 humanoid across tasks requiring different levels of compliance,including gentle hugging, sit-to-stand assistance, and safe objectmanipulation. Compared to baselines, our policy consistently reduces peakcontact forces while maintaining task success, resulting in smoother and morenatural interactions. These results highlight a step toward humanoid robotsthat can safely and effectively collaborate with humans and handle objects inreal-world environments.</description><author>Qingzhou Lu, Yao Feng, Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu</author><pubDate>Thu, 06 Nov 2025 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04679v1</guid></item><item><title>Residual Kolmogorov-Arnold Network for Enhanced Deep Learning</title><link>http://arxiv.org/abs/2410.05500v4</link><description>Despite their immense success, deep convolutional neural networks (CNNs) canbe difficult to optimize and costly to train due to hundreds of layers withinthe network depth. Conventional convolutional operations are fundamentallylimited by their linear nature along with fixed activations, where many layersare needed to learn meaningful patterns in data. Because of the sheer size ofthese networks, this approach is simply computationally inefficient, and posesoverfitting or gradient explosion risks, especially in small datasets. As aresult, we introduce a "plug-in" module, called Residual Kolmogorov-ArnoldNetwork (RKAN). Our module is highly compact, so it can be easily added intoany stage (level) of traditional deep networks, where it learns to integratesupportive polynomial feature transformations to existing convolutionalframeworks. RKAN offers consistent improvements over baseline models indifferent vision tasks and widely tested benchmarks, accomplishing cutting-edgeperformance on them.</description><author>Ray Congrui Yu, Sherry Wu, Jiang Gui</author><pubDate>Thu, 06 Nov 2025 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05500v4</guid></item><item><title>Tracking and Understanding Object Transformations</title><link>http://arxiv.org/abs/2511.04678v1</link><description>Real-world objects frequently undergo state transformations. From an applebeing cut into pieces to a butterfly emerging from its cocoon, tracking throughthese changes is important for understanding real-world objects and dynamics.However, existing methods often lose track of the target object aftertransformation, due to significant changes in object appearance. To addressthis limitation, we introduce the task of Track Any State: tracking objectsthrough transformations while detecting and describing state changes,accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, wepresent TubeletGraph, a zero-shot system that recovers missing objects aftertransformation and maps out how object states are evolving over time.TubeletGraph first identifies potentially overlooked tracks, and determineswhether they should be integrated based on semantic and proximity priors. Then,it reasons about the added tracks and generates a state graph describing eachobserved transformation. TubeletGraph achieves state-of-the-art trackingperformance under transformations, while demonstrating deeper understanding ofobject transformations and promising capabilities in temporal grounding andsemantic reasoning for complex object transformations. Code, additionalresults, and the benchmark dataset are available athttps://tubelet-graph.github.io.</description><author>Yihong Sun, Xinyu Yang, Jennifer J. Sun, Bharath Hariharan</author><pubDate>Thu, 06 Nov 2025 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04678v1</guid></item><item><title>InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation</title><link>http://arxiv.org/abs/2511.04675v1</link><description>We introduce InfinityStar, a unified spacetime autoregressive framework forhigh-resolution image and dynamic video synthesis. Building on the recentsuccess of autoregressive modeling in both vision and language, our purelydiscrete approach jointly captures spatial and temporal dependencies within asingle architecture. This unified design naturally supports a variety ofgeneration tasks such as text-to-image, text-to-video, image-to-video, and longinteractive video synthesis via straightforward temporal autoregression.Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,outperforming all autoregressive models by large margins, even surpassing somediffusion competitors like HunyuanVideo. Without extra optimizations, our modelgenerates a 5s, 720p video approximately 10x faster than leadingdiffusion-based methods. To our knowledge, InfinityStar is the first discreteautoregressive video generator capable of producing industrial level 720pvideos. We release all code and models to foster further research in efficient,high-quality video generation.</description><author>Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu, Xing Wang, Yi Jiang, Bingyue Peng, Zehuan Yuan</author><pubDate>Thu, 06 Nov 2025 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04675v1</guid></item><item><title>Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</title><link>http://arxiv.org/abs/2506.15680v2</link><description>Modeling the dynamics of deformable objects is challenging due to theirdiverse physical properties and the difficulty of estimating states fromlimited visual information. We address these challenges with a neural dynamicsframework that combines object particles and spatial grids in a hybridrepresentation. Our particle-grid model captures global shape and motioninformation while predicting dense particle movements, enabling the modeling ofobjects with varied shapes and materials. Particles represent object shapes,while the spatial grid discretizes the 3D space to ensure spatial continuityand enhance learning efficiency. Coupled with Gaussian Splattings for visualrendering, our framework achieves a fully learning-based digital twin ofdeformable objects and generates 3D action-conditioned videos. Throughexperiments, we demonstrate that our model learns the dynamics of diverseobjects -- such as ropes, cloths, stuffed animals, and paper bags -- fromsparse-view RGB-D recordings of robot-object interactions, while alsogeneralizing at the category level to unseen instances. Our approachoutperforms state-of-the-art learning-based and physics-based simulators,particularly in scenarios with limited camera views. Furthermore, we showcasethe utility of our learned models in model-based planning, enablinggoal-conditioned object manipulation across a range of tasks. The project pageis available at https://kywind.github.io/pgnd .</description><author>Kaifeng Zhang, Baoyu Li, Kris Hauser, Yunzhu Li</author><pubDate>Thu, 06 Nov 2025 18:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.15680v2</guid></item><item><title>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences</title><link>http://arxiv.org/abs/2509.16189v2</link><description>When do machine learning systems fail to generalize, and what mechanismscould improve their generalization? Here, we draw inspiration from cognitivescience to argue that one weakness of parametric machine learning systems istheir failure to exhibit latent learning -- learning information that is notrelevant to the task at hand, but that might be useful in a future task. Weshow how this perspective links failures ranging from the reversal curse inlanguage modeling to new findings on agent-based navigation. We then highlighthow cognitive science points to episodic memory as a potential part of thesolution to these issues. Correspondingly, we show that a system with an oracleretrieval mechanism can use learning experiences more flexibly to generalizebetter across many of these challenges. We also identify some of the essentialcomponents for effectively using retrieval, including the importance ofwithin-example in-context learning for acquiring the ability to use informationacross retrieved examples. In summary, our results illustrate one possiblecontributor to the relative data inefficiency of current machine learningsystems compared to natural intelligence, and help to understand how retrievalmethods can complement parametric learning to improve generalization. We closeby discussing some of the links between these findings and prior results incognitive science and neuroscience, and the broader implications.</description><author>Andrew Kyle Lampinen, Martin Engelcke, Yuxuan Li, Arslan Chaudhry, James L. McClelland</author><pubDate>Thu, 06 Nov 2025 18:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.16189v2</guid></item><item><title>X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations</title><link>http://arxiv.org/abs/2511.04671v1</link><description>Human videos can be recorded quickly and at scale, making them an appealingsource of training data for robot learning. However, humans and robots differfundamentally in embodiment, resulting in mismatched action execution. Directkinematic retargeting of human hand motion can therefore produce actions thatare physically infeasible for robots. Despite these low-level differences,human demonstrations provide valuable motion cues about how to manipulate andinteract with objects. Our key idea is to exploit the forward diffusionprocess: as noise is added to actions, low-level execution differences fadewhile high-level task guidance is preserved. We present X-Diffusion, aprincipled framework for training diffusion policies that maximally leverageshuman data without learning dynamically infeasible motions. X-Diffusion firsttrains a classifier to predict whether a noisy action is executed by a human orrobot. Then, a human action is incorporated into policy training only afteradding sufficient noise such that the classifier cannot discern its embodiment.Actions consistent with robot execution supervise fine-grained denoising at lownoise levels, while mismatched human actions provide only coarse guidance athigher noise levels. Our experiments show that naive co-training underexecution mismatches degrades policy performance, while X-Diffusionconsistently improves it. Across five manipulation tasks, X-Diffusion achievesa 16% higher average success rate than the best baseline. The project websiteis available at https://portal-cornell.github.io/X-Diffusion/.</description><author>Maximus A. Pace, Prithwish Dan, Chuanruo Ning, Atiksh Bhardwaj, Audrey Du, Edward W. Duan, Wei-Chiu Ma, Kushal Kedia</author><pubDate>Thu, 06 Nov 2025 18:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04671v1</guid></item><item><title>Cambrian-S: Towards Spatial Supersensing in Video</title><link>http://arxiv.org/abs/2511.04670v1</link><description>We argue that progress in true multimodal intelligence calls for a shift fromreactive, task-driven systems and brute-force long context towards a broaderparadigm of supersensing. We frame spatial supersensing as four stages beyondlinguistic-only understanding: semantic perception (naming what is seen),streaming event cognition (maintaining memory across continuous experiences),implicit 3D spatial cognition (inferring the world behind pixels), andpredictive world modeling (creating internal models that filter and organizeinformation). Current benchmarks largely test only the early stages, offeringnarrow coverage of spatial cognition and rarely challenging models in ways thatrequire true world modeling. To drive progress in spatial supersensing, wepresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatialrecall) and VSC (continual visual spatial counting). These tasks requirearbitrarily long video inputs yet are resistant to brute-force contextexpansion. We then test data scaling limits by curating VSI-590K and trainingCambrian-S, achieving +30% absolute improvement on VSI-Bench withoutsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,indicating that scale alone is insufficient for spatial supersensing. Wepropose predictive sensing as a path forward, presenting a proof-of-concept inwhich a self-supervised next-latent-frame predictor leverages surprise(prediction error) to drive memory and event segmentation. On VSI-SUPER, thisapproach substantially outperforms leading proprietary baselines, showing thatspatial supersensing requires models that not only see but also anticipate,select, and organize experience.</description><author>Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie</author><pubDate>Thu, 06 Nov 2025 18:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04670v1</guid></item><item><title>SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</title><link>http://arxiv.org/abs/2511.04668v1</link><description>Despite impressive high-level video comprehension, multimodal language modelsstruggle with spatial reasoning across time and space. While current spatialtraining approaches rely on real-world video data, obtaining diverse footagewith precise spatial annotations remains a bottleneck. To alleviate thisbottleneck, we present SIMS-V -- a systematic data-generation framework thatleverages the privileged information of 3D simulators to create spatially-richvideo training data for multimodal language models. Using this framework, weinvestigate which properties of simulated data drive effective real-worldtransfer through systematic ablations of question types, mixes, and scales. Weidentify a minimal set of three question categories (metric measurement,perspective-dependent reasoning, and temporal tracking) that prove mosteffective for developing transferable spatial intelligence, outperformingcomprehensive coverage despite using fewer question types. These insightsenable highly efficient training: our 7B-parameter video LLM fine-tuned on just25K simulated examples outperforms the larger 72B baseline and achievescompetitive performance with proprietary models on rigorous real-world spatialreasoning benchmarks. Our approach demonstrates robust generalization,maintaining performance on general video understanding while showingsubstantial improvements on embodied and real-world spatial tasks.</description><author>Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie</author><pubDate>Thu, 06 Nov 2025 18:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04668v1</guid></item><item><title>Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches</title><link>http://arxiv.org/abs/2511.04667v1</link><description>This study evaluates a 40-item mathematics placement examination administeredto 198 students using a multi-method framework combining Classical Test Theory,machine learning, and unsupervised clustering. Classical Test Theory analysisreveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$)while 30\% demonstrate poor discrimination ($D &lt; 0.20$) requiring replacement.Question 6 (Graph Interpretation) emerges as the examination's most powerfuldiscriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVAF-statistic ($F = 4609.1$), and maximum Random Forest feature importance(0.206), accounting for 20.6\% of predictive power. Machine learning algorithmsdemonstrate exceptional performance, with Random Forest and Gradient Boostingachieving 97.5\% and 96.0\% cross-validation accuracy. K-means clusteringidentifies a natural binary competency structure with a boundary at 42.5\%,diverging from the institutional threshold of 55\% and suggesting potentialoverclassification into remedial categories. The two-cluster solution exhibitsexceptional stability (bootstrap ARI = 0.855) with perfect lower-clusterpurity. Convergent evidence across methods supports specific refinements:replace poorly discriminating items, implement a two-stage assessment, andintegrate Random Forest predictions with transparency mechanisms. Thesefindings demonstrate that multi-method integration provides a robust empiricalfoundation for evidence-based mathematics placement optimization.</description><author>Julian D. Allagan, Dasia A. Singleton, Shanae N. Perry, Gabrielle C. Morgan, Essence A. Morgan</author><pubDate>Thu, 06 Nov 2025 18:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04667v1</guid></item><item><title>Forgetting is Everywhere</title><link>http://arxiv.org/abs/2511.04666v1</link><description>A fundamental challenge in developing general learning algorithms is theirtendency to forget past knowledge when adapting to new data. Addressing thisproblem requires a principled understanding of forgetting; yet, despite decadesof study, no unified definition has emerged that provides insights into theunderlying dynamics of learning. We propose an algorithm- and task-agnostictheory that characterises forgetting as a lack of self-consistency in alearner's predictive distribution over future experiences, manifesting as aloss of predictive information. Our theory naturally yields a general measureof an algorithm's propensity to forget. To validate the theory, we design acomprehensive set of experiments that span classification, regression,generative modelling, and reinforcement learning. We empirically demonstratehow forgetting is present across all learning settings and plays a significantrole in determining learning efficiency. Together, these results establish aprincipled understanding of forgetting and lay the foundation for analysing andimproving the information retention capabilities of general learningalgorithms.</description><author>Ben Sanati, Thomas L. Lee, Trevor McInroe, Aidan Scannell, Nikolay Malkin, David Abel, Amos Storkey</author><pubDate>Thu, 06 Nov 2025 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04666v1</guid></item><item><title>Distillation versus Contrastive Learning: How to Train Your Rerankers</title><link>http://arxiv.org/abs/2507.08336v3</link><description>Training effective text rerankers is crucial for information retrieval. Twostrategies are widely used: contrastive learning (optimizing directly onground-truth labels) and knowledge distillation (transferring knowledge from alarger reranker). While both have been studied extensively, a clear comparisonof their effectiveness for training cross-encoder rerankers under practicalconditions is needed. This paper empirically compares these strategies by training rerankers ofdifferent sizes (0.5B, 1.5B, 3B, 7B) and architectures (Transformer, Recurrent)using both methods on the same data, with a strong contrastive learning modelacting as the distillation teacher. Our results show that knowledgedistillation generally yields better in-domain and out-of-domain rankingperformance than contrastive learning when distilling from a more performantteacher model. This finding is consistent across student model sizes andarchitectures. However, distilling from a teacher of the same capacity does notprovide the same advantage, particularly for out-of-domain tasks. Thesefindings offer practical guidance for choosing a training strategy based onavailable teacher models. We recommend using knowledge distillation to trainsmaller rerankers if a larger, more performant teacher is accessible; in itsabsence, contrastive learning remains a robust baseline. Our codeimplementation is made available to facilitate reproducbility.</description><author>Zhichao Xu, Zhiqi Huang, Shengyao Zhuang, Vivek Srikumar</author><pubDate>Thu, 06 Nov 2025 18:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.08336v3</guid></item><item><title>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</title><link>http://arxiv.org/abs/2511.04665v1</link><description>Robotic manipulation policies are advancing rapidly, but their directevaluation in the real world remains costly, time-consuming, and difficult toreproduce, particularly for tasks involving deformable objects. Simulationprovides a scalable and systematic alternative, yet existing simulators oftenfail to capture the coupled visual and physical complexity of soft-bodyinteractions. We present a real-to-sim policy evaluation framework thatconstructs soft-body digital twins from real-world videos and renders robots,objects, and environments with photorealistic fidelity using 3D GaussianSplatting. We validate our approach on representative deformable manipulationtasks, including plush toy packing, rope routing, and T-block pushing,demonstrating that simulated rollouts correlate strongly with real-worldexecution performance and reveal key behavioral patterns of learned policies.Our results suggest that combining physics-informed reconstruction withhigh-quality rendering enables reproducible, scalable, and accurate evaluationof robotic manipulation policies. Website: https://real2sim-eval.github.io/</description><author>Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, Yunzhu Li</author><pubDate>Thu, 06 Nov 2025 18:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04665v1</guid></item><item><title>VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks</title><link>http://arxiv.org/abs/2511.04662v1</link><description>LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), butthey cannot reliably verify their own logic. Even when they reach correctanswers, the underlying reasoning may be flawed, undermining trust inhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, aneuro-symbolic method that extracts and verifies formal logical arguments fromCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-orderlogic and identifies premises that ground the argument in source context,commonsense knowledge, or prior reasoning steps. The symbolic representationenables automated solvers to verify logical validity while the NL premisesallow humans and systems to identify ungrounded or fallacious reasoning steps.Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoTeffectively identifies flawed reasoning, and serves as a strong predictor offinal answer correctness. We also leverage VeriCoT's verification signal for(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) onVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with directpreference optimization (DPO) using verification-based pairwise rewards,further improving reasoning validity and accuracy.</description><author>Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel, Sapana Chaudhary, Benjamin Kiesl-Reiter, Huzefa Rangwala</author><pubDate>Thu, 06 Nov 2025 18:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04662v1</guid></item><item><title>Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions</title><link>http://arxiv.org/abs/2509.08217v2</link><description>For machine learning datasets to accurately represent diverse opinions in apopulation, they must preserve variation in data labels while filtering outspam or low-quality responses. How can we balance annotator reliability andrepresentation? We empirically evaluate how a range of heuristics for annotatorfiltering affect the preservation of variation on subjective tasks. We findthat these methods, designed for contexts in which variation from a singleground-truth label is considered noise, often remove annotators who disagreeinstead of spam annotators, introducing suboptimal tradeoffs between accuracyand label diversity. We find that conservative settings for annotator removal(&lt;5%) are best, after which all tested methods increase the mean absolute errorfrom the true average label. We analyze performance on synthetic spam toobserve that these methods often assume spam annotators are more random thanreal spammers tend to be: most spammers are distributionally indistinguishablefrom real annotators, and the minority that are distinguishable tend to giverelatively fixed answers, not random ones. Thus, tasks requiring thepreservation of variation reverse the intuition of existing spam filteringmethods: spammers tend to be less random than non-spammers, so metrics thatassume variation is spam fare worse. These results highlight the need for spamremoval methods that account for label diversity.</description><author>Eve Fleisig, Matthias Orlikowski, Philipp Cimiano, Dan Klein</author><pubDate>Thu, 06 Nov 2025 18:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.08217v2</guid></item><item><title>CREA: A Collaborative Multi-Agent Framework for Creative Image Editing and Generation</title><link>http://arxiv.org/abs/2504.05306v2</link><description>Creativity in AI imagery remains a fundamental challenge, requiring not onlythe generation of visually compelling content but also the capacity to addnovel, expressive, and artistically rich transformations to images. Unlikeconventional editing tasks that rely on direct prompt-based modifications,creative image editing requires an autonomous, iterative approach that balancesoriginality, coherence, and artistic intent. To address this, we introduceCREA, a novel multi-agent collaborative framework that mimics the humancreative process. Our framework leverages a team of specialized AI agents whodynamically collaborate to conceptualize, generate, critique, and enhanceimages. Through extensive qualitative and quantitative evaluations, wedemonstrate that CREA significantly outperforms state-of-the-art methods indiversity, semantic alignment, and creative transformation. To the best of ourknowledge, this is the first work to introduce the task of creative editing.</description><author>Kavana Venkatesh, Connor Dunlop, Pinar Yanardag</author><pubDate>Thu, 06 Nov 2025 18:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05306v2</guid></item><item><title>Nowcast3D: Reliable precipitation nowcasting via gray-box learning</title><link>http://arxiv.org/abs/2511.04659v1</link><description>Extreme precipitation nowcasting demands high spatiotemporal fidelity andextended lead times, yet existing approaches remain limited. Numerical WeatherPrediction (NWP) and its deep-learning emulations are too slow and coarse forrapidly evolving convection, while extrapolation and purely data-driven modelssuffer from error accumulation and excessive smoothing. Hybrid 2D radar-basedmethods discard crucial vertical information, preventing accuratereconstruction of height-dependent dynamics. We introduce a gray-box, fullythree-dimensional nowcasting framework that directly processes volumetric radarreflectivity and couples physically constrained neural operators withdatadriven learning. The model learns vertically varying 3D advection fieldsunder a conservative advection operator, parameterizes spatially varyingdiffusion, and introduces a Brownian-motion--inspired stochastic term torepresent unresolved motions. A residual branch captures small-scale convectiveinitiation and microphysical variability, while a diffusion-based stochasticmodule estimates uncertainty. The framework achieves more accurate forecasts upto three-hour lead time across precipitation regimes and ranked first in 57\%of cases in a blind evaluation by 160 meteorologists. By restoring full 3Ddynamics with physical consistency, it offers a scalable and robust pathway forskillful and reliable nowcasting of extreme precipitation.</description><author>Huaguan Chen, Wei Han, Haofei Sun, Ning Lin, Xingtao Song, Yunfan Yang, Jie Tian, Yang Liu, Ji-Rong Wen, Xiaoye Zhang, Xueshun Shen, Hao Sun</author><pubDate>Thu, 06 Nov 2025 18:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04659v1</guid></item><item><title>Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts</title><link>http://arxiv.org/abs/2511.04655v1</link><description>Robust benchmarks are crucial for evaluating Multimodal Large Language Models(MLLMs). Yet we find that models can ace many multimodal benchmarks withoutstrong visual understanding, instead exploiting biases, linguistic priors, andsuperficial patterns. This is especially problematic for vision-centricbenchmarks that are meant to require visual inputs. We adopt a diagnosticprinciple for benchmark design: if a benchmark can be gamed, it will be.Designers should therefore try to ``game'' their own benchmarks first, usingdiagnostic and debiasing procedures to systematically identify and mitigatenon-visual biases. Effective diagnosis requires directly ``training on the testset'' -- probing the released test set for its intrinsic, exploitable patterns. We operationalize this standard with two components. First, we diagnosebenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.Our primary diagnostic tool involves fine-tuning a powerful Large LanguageModel via $k$-fold cross-validation on exclusively the non-visual, textualinputs of the test set to reveal shortcut performance and assign each sample abias score $s(x)$. We complement this with a lightweight Random Forest-baseddiagnostic operating on hand-crafted features for fast, interpretable auditing.Second, we debias benchmarks by filtering high-bias samples using an``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to fourbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasivenon-visual biases. As a case study, we apply our full framework to createVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a widervision-blind performance gap than the original.</description><author>Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie</author><pubDate>Thu, 06 Nov 2025 18:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04655v1</guid></item><item><title>Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2511.04654v1</link><description>Chain-of-Thought (CoT) prompting is a key technique for enabling complexreasoning in large language models. However, generating full, fixed-lengthrationales is computationally wasteful, inflating both token usage and latency.We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-freedecoding algorithm that adaptively halts rationale generation. LEASH monitorstwo intrinsic signals: the slope of token-level entropy and the improvement inthe top-logit margin. It terminates the generation once both signals plateau,indicating the model has reached a stable reasoning state. Across fourinstruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reducesaverage token generation by 30--35% and latency by 27%, while incurring a 10p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires noadditional training or supervision, offering a simple and efficient alternativeto CoT decoding.</description><author>Mohammad Atif Quamar, Mohammad Areeb</author><pubDate>Thu, 06 Nov 2025 18:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04654v1</guid></item><item><title>TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning</title><link>http://arxiv.org/abs/2511.04653v1</link><description>Federated learning (FL) offers new opportunities in machine learning,particularly in addressing data privacy concerns. In contrast to conventionalevent-based federated learning, time-triggered federated learning (TT-Fed), asa general form of both asynchronous and synchronous FL, clusters users intodifferent tiers based on fixed time intervals. However, the FL network consistsof a growing number of user devices with limited wireless bandwidth,consequently magnifying issues such as stragglers and communication overhead.In this paper, we introduce adaptive model pruning to wireless TT-Fed systemsand study the problem of jointly optimizing the pruning ratio and bandwidthallocation to minimize the training loss while ensuring minimal learninglatency. To answer this question, we perform convergence analysis on thegradient l_2 norm of the TT-Fed model based on model pruning. Based on theobtained convergence upper bound, a joint optimization problem of pruning ratioand wireless bandwidth is formulated to minimize the model training loss undera given delay threshold. Then, we derive closed-form solutions for wirelessbandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. Thesimulation results show that model pruning could reduce the communication costby 40% while maintaining the model performance at the same level.</description><author>Xinlu Zhang, Yansha Deng, Toktam Mahmoodi</author><pubDate>Thu, 06 Nov 2025 18:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04653v1</guid></item><item><title>Polarization-resolved imaging improves eye tracking</title><link>http://arxiv.org/abs/2511.04652v1</link><description>Polarization-resolved near-infrared imaging adds a useful optical contrastmechanism to eye tracking by measuring the polarization state of lightreflected by ocular tissues in addition to its intensity. In this paper wedemonstrate how this contrast can be used to enable eye tracking. Specifically,we demonstrate that a polarization-enabled eye tracking (PET) system composedof a polarization--filter--array camera paired with a linearly polarizednear-infrared illuminator can reveal trackable features across the sclera andgaze-informative patterns on the cornea, largely absent in intensity-onlyimages. Across a cohort of 346 participants, convolutional neural network basedmachine learning models trained on data from PET reduced the median95th-percentile absolute gaze error by 10--16\% relative to capacity-matchedintensity baselines under nominal conditions and in the presence of eyelidocclusions, eye-relief changes, and pupil-size variation. These results linklight--tissue polarization effects to practical gains in human--computerinteraction and position PET as a simple, robust sensing modality for futurewearable devices.</description><author>Mantas Žurauskas, Tom Bu, Sanaz Alali, Beyza Kalkanli, Derek Shi, Fernando Alamos, Gauresh Pandit, Christopher Mei, Ali Behrooz, Ramin Mirjalili, Dave Stronks, Alexander Fix, Dmitri Model</author><pubDate>Thu, 06 Nov 2025 18:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04652v1</guid></item><item><title>CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation</title><link>http://arxiv.org/abs/2509.07325v2</link><description>The National Comprehensive Cancer Network (NCCN) provides evidence-basedguidelines for cancer treatment. Translating complex patient presentations intoguideline-compliant treatment recommendations is time-intensive, requiresspecialized expertise, and is prone to error. Advances in large language model(LLM) capabilities promise to reduce the time required to generate treatmentrecommendations and improve accuracy. We present an LLM agent-based approach toautomatically generate guideline-concordant treatment trajectories for patientswith non-small cell lung cancer (NSCLC). Our contributions are threefold.First, we construct a novel longitudinal dataset of 121 cases of NSCLC patientsthat includes clinical encounters, diagnostic results, and medical histories,each expertly annotated with the corresponding NCCN guideline trajectories byboard-certified oncologists. Second, we demonstrate that existing LLMs possessdomain-specific knowledge that enables high-quality proxy benchmark generationfor both model development and evaluation, achieving strong correlation(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.Third, we develop a hybrid approach combining expensive human annotations withmodel consistency information to create both the agent framework that predictsthe relevant guidelines for a patient, as well as a meta-classifier thatverifies prediction accuracy with calibrated confidence scores for treatmentrecommendations (AUROC=0.800), a critical capability for communicating theaccuracy of outputs, custom-tailoring tradeoffs in performance, and supportingregulatory compliance. This work establishes a framework for clinically viableLLM-based guideline adherence systems that balance accuracy, interpretability,and regulatory requirements while reducing annotation costs, providing ascalable pathway toward automated clinical decision support.</description><author>Alyssa Unell, Noel C. F. Codella, Sam Preston, Peniel Argaw, Wen-wai Yim, Zelalem Gero, Cliff Wong, Rajesh Jena, Eric Horvitz, Amanda K. Hall, Ruican Rachel Zhong, Jiachen Li, Shrey Jain, Mu Wei, Matthew Lungren, Hoifung Poon</author><pubDate>Thu, 06 Nov 2025 18:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07325v2</guid></item><item><title>Optimal Inference Schedules for Masked Diffusion Models</title><link>http://arxiv.org/abs/2511.04647v1</link><description>A major bottleneck of standard auto-regressive large language models is thattheir inference process is inherently sequential, resulting in very long andcostly inference times. To circumvent this, practitioners proposed a class oflanguage models called diffusion language models, of which the masked diffusionmodel (MDM) is the most successful. The MDM is able to sample tokensout-of-order and, ostensibly, many tokens at once and in parallel. However,there is very limited rigorous understanding of how much parallel samplingthese models can perform without noticeable degradation in their samplingperformance. Prior work of Li and Cai obtained some preliminary bounds, butthese are not tight for many natural classes of distributions. In this work, wegive a new, exact characterization of the expected divergence between the truedistribution and the sampled distribution, for any distribution and anyunmasking schedule for the sampler, showing an elegant connection to the theoryof univariate function approximation. By leveraging this connection, we then attain a number of novel lower andupper bounds for this problem. While the connection to function approximationin principle gives the optimal unmasking schedule for any distribution, we showthat it is in general impossible to compete with it without strong a prioriknowledge of the distribution, even in seemingly benign settings. However, wealso demonstrate new upper bounds and new sampling schedules in terms ofwell-studied information-theoretic properties of the base distribution, namely,its total correlation and dual total correlation, which show that in somenatural settings, one can sample in $O(log n)$ steps without any visible lossin performance, where $n$ is the total sequence length.</description><author>Sitan Chen, Kevin Cong, Jerry Li</author><pubDate>Thu, 06 Nov 2025 18:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04647v1</guid></item><item><title>DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2511.04646v1</link><description>Cooperative multi-agent planning requires agents to make joint decisions withpartial information and limited communication. Coordination at the trajectorylevel often fails, as small deviations in timing or movement cascade intoconflicts. Symbolic planning mitigates this challenge by raising the level ofabstraction and providing a minimal vocabulary of actions that enablesynchronization and collective progress. We present DR. WELL, a decentralizedneurosymbolic framework for cooperative multi-agent planning. Cooperationunfolds through a two-phase negotiation protocol: agents first proposecandidate roles with reasoning and then commit to a joint allocation underconsensus and environment constraints. After commitment, each agentindependently generates and executes a symbolic plan for its role withoutrevealing detailed trajectories. Plans are grounded in execution outcomes via ashared world model that encodes the current state and is updated as agents act.By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoidsbrittle step-level alignment and enables higher-level operations that arereusable, synchronizable, and interpretable. Experiments on cooperativeblock-push tasks show that agents adapt across episodes, with the dynamic worldmodel capturing reusable patterns and improving task completion rates andefficiency. Experiments on cooperative block-push tasks show that our dynamicworld model improves task completion and efficiency through negotiation andself-refinement, trading a time overhead for evolving, more efficientcollaboration strategies.</description><author>Narjes Nourzad, Hanqing Yang, Shiyu Chen, Carlee Joe-Wong</author><pubDate>Thu, 06 Nov 2025 18:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04646v1</guid></item><item><title>When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection</title><link>http://arxiv.org/abs/2511.04643v1</link><description>The proliferation of misinformation necessitates robust yet computationallyefficient fact verification systems. While current state-of-the-art approachesleverage Large Language Models (LLMs) for generating explanatory rationales,these methods face significant computational barriers and hallucination risksin real-world deployments. We present DeReC (Dense Retrieval Classification), alightweight framework that demonstrates how general-purpose text embeddings caneffectively replace autoregressive LLM-based approaches in fact verificationtasks. By combining dense retrieval with specialized classification, our systemachieves better accuracy while being significantly more efficient. DeReCoutperforms explanation-generating LLMs in efficiency, reducing runtime by 95%on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),showcasing its effectiveness across varying dataset sizes. On the RAWFCdataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-artmethod L-Defense (61.20%). Our results demonstrate that carefully engineeredretrieval-based systems can match or exceed LLM performance in specializedtasks while being significantly more practical for real-world deployment.</description><author>Alamgir Munir Qazi, John P. McCrae, Jamal Abdul Nasir</author><pubDate>Thu, 06 Nov 2025 18:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04643v1</guid></item><item><title>Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems</title><link>http://arxiv.org/abs/2511.04641v1</link><description>This paper is concerned with probabilistic techniques for forecastingdynamical systems described by partial differential equations (such as, forexample, the Navier-Stokes equations). In particular, it is investigating andcomparing various extensions to the flow matching paradigm that reduce thenumber of sampling steps. In this regard, it compares direct distillation,progressive distillation, adversarial diffusion distillation, Wasserstein GANsand rectified flows. Moreover, experiments are conducted on a set ofchallenging systems. In particular, we also address the challenge of directlypredicting 2D slices of large-scale 3D simulations, paving the way forefficient inflow generation for solvers.</description><author>Hans Harder, Abhijeet Vishwasrao, Luca Guastoni, Ricardo Vinuesa, Sebastian Peitz</author><pubDate>Thu, 06 Nov 2025 18:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04641v1</guid></item><item><title>Addressing divergent representations from causal interventions on neural networks</title><link>http://arxiv.org/abs/2511.04638v1</link><description>A common approach to mechanistic interpretability is to causally manipulatemodel representations via targeted interventions in order to understand whatthose representations encode. Here we ask whether such interventions createout-of-distribution (divergent) representations, and whether this raisesconcerns about how faithful their resulting explanations are to the targetmodel in its natural state. First, we demonstrate empirically that commoncausal intervention techniques often do shift internal representations awayfrom the natural distribution of the target model. Then, we provide atheoretical analysis of two classes of such divergences: `harmless' divergencesthat occur in the null-space of the weights and from covariance withinbehavioral decision boundaries, and `pernicious' divergences that activatehidden network pathways and cause dormant behavioral changes. Finally, in aneffort to mitigate the pernicious cases, we modify the Counterfactual Latent(CL) loss from Grant (2025) that regularizes interventions to remain closer tothe natural distributions, reducing the likelihood of harmful divergences whilepreserving the interpretive power of interventions. Together, these resultshighlight a path towards more reliable interpretability methods.</description><author>Satchel Grant, Simon Jerome Han, Alexa Tartaglini, Christopher Potts</author><pubDate>Thu, 06 Nov 2025 18:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04638v1</guid></item><item><title>SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</title><link>http://arxiv.org/abs/2511.03092v2</link><description>The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+context length support have resulted in increasing demands for on-chip memoryto support large KV caches. Techniques such as StreamingLLM and SnapKVdemonstrate how to control KV cache size while maintaining model accuracy. Yet,these techniques are not commonly used within industrial deployments usingframeworks like vLLM or SGLang. The reason is twofold: on one hand, the staticgraphs and continuous batching methodology employed by these frameworks make itdifficult to admit modifications to the standard multi-head attentionalgorithm, while on the other hand, the accuracy implications of suchtechniques on modern instruction-following and reasoning models are not wellunderstood, obfuscating the need for implementing these techniques. In thispaper, we explore these accuracy implications on Llama-3.1-8B-Instruct andDeepSeek-R1, and develop SnapStream, a KV cache compression method that can bedeployed at scale. We demonstrate the efficacy of SnapStream in a 16-waytensor-parallel deployment of DeepSeek-671B on SambaNova SN40L acceleratorsrunning at 128k context length and up to 1832 tokens per second in a realproduction setting. SnapStream enables $4\times$ improved on-chip memory usageand introduces minimal accuracy degradation on LongBench-v2, AIME24 andLiveCodeBench. To the best of our knowledge, this is the first implementationof sparse KV attention techniques deployed in a production inference systemwith static graphs and continuous batching.</description><author>Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar</author><pubDate>Thu, 06 Nov 2025 18:27:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.03092v2</guid></item><item><title>NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment</title><link>http://arxiv.org/abs/2511.04628v1</link><description>Video quality assessment (VQA) is vital for computer vision tasks, butexisting approaches face major limitations: full-reference (FR) metrics requireclean reference videos, and most no-reference (NR) models depend on training oncostly human opinion labels. Moreover, most opinion-unaware NR methods areimage-based, ignoring temporal context critical for video object detection. Inthis work, we present a scalable, streaming-based VQA model that is bothno-reference and opinion-unaware. Our model leverages synthetic degradations ofthe DAVIS dataset, training a temporal-aware convolutional architecture topredict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, withoutreferences at inference. We show that our streaming approach outperforms ourown image-based baseline by generalizing across diverse degradations,underscoring the value of temporal modeling for scalable VQA in real-worldvision systems. Additionally, we demonstrate that our model achieves highercorrelation with full-reference metrics compared to BRISQUE, a widely-usedopinion-aware image quality assessment baseline, validating the effectivenessof our temporal, opinion-unaware approach.</description><author>Kylie Cancilla, Alexander Moore, Amar Saini, Carmen Carrano</author><pubDate>Thu, 06 Nov 2025 18:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04628v1</guid></item><item><title>ODE approximation for the Adam algorithm: General and overparametrized setting</title><link>http://arxiv.org/abs/2511.04622v1</link><description>The Adam optimizer is currently presumably the most popular optimizationmethod in deep learning. In this article we develop an ODE based method tostudy the Adam optimizer in a fast-slow scaling regime. For fixed momentumparameters and vanishing step-sizes, we show that the Adam algorithm is anasymptotic pseudo-trajectory of the flow of a particular vector field, which isreferred to as the Adam vector field. Leveraging properties of asymptoticpseudo-trajectories, we establish convergence results for the Adam algorithm.In particular, in a very general setting we show that if the Adam algorithmconverges, then the limit must be a zero of the Adam vector field, rather thana local minimizer or critical point of the objective function. In contrast, in the overparametrized empirical risk minimization setting, theAdam algorithm is able to locally find the set of minima. Specifically, we showthat in a neighborhood of the global minima, the objective function serves as aLyapunov function for the flow induced by the Adam vector field. As aconsequence, if the Adam algorithm enters a neighborhood of the global minimainfinitely often, it converges to the set of global minima.</description><author>Steffen Dereich, Arnulf Jentzen, Sebastian Kassing</author><pubDate>Thu, 06 Nov 2025 18:15:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04622v1</guid></item><item><title>Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications</title><link>http://arxiv.org/abs/2509.08604v2</link><description>Large Language Models (LLMs) have demonstrated significant potential inmedicine. To date, LLMs have been widely applied to tasks such as diagnosticassistance, medical question answering, and clinical information synthesis.However, a key open question remains: to what extent do LLMs memorize medicaltraining data. In this study, we present the first comprehensive evaluation ofmemorization of LLMs in medicine, assessing its prevalence (how frequently itoccurs), characteristics (what is memorized), volume (how much content ismemorized), and potential downstream impacts (how memorization may affectmedical applications). We systematically analyze common adaptation scenarios:(1) continued pretraining on medical corpora, (2) fine-tuning on standardmedical benchmarks, and (3) fine-tuning on real-world clinical data, includingover 13,000 unique inpatient records from Yale New Haven Health System. Theresults demonstrate that memorization is prevalent across all adaptationscenarios and significantly higher than reported in the general domain.Memorization affects both the development and adoption of LLMs in medicine andcan be categorized into three types: beneficial (e.g., accurate recall ofclinical guidelines and biomedical references), uninformative (e.g., repeateddisclaimers or templated medical document language), and harmful (e.g.,regeneration of dataset-specific or sensitive clinical content). Based on thesefindings, we offer practical recommendations to facilitate beneficialmemorization that enhances domain-specific reasoning and factual accuracy,minimize uninformative memorization to promote deeper learning beyondsurface-level patterns, and mitigate harmful memorization to prevent theleakage of sensitive or identifiable patient information.</description><author>Anran Li, Lingfei Qian, Mengmeng Du, Yu Yin, Yan Hu, Zihao Sun, Yihang Fu, Erica Stutz, Xuguang Ai, Qianqian Xie, Rui Zhu, Jimin Huang, Yifan Yang, Siru Liu, Yih-Chung Tham, Lucila Ohno-Machado, Hyunghoon Cho, Zhiyong Lu, Hua Xu, Qingyu Chen</author><pubDate>Thu, 06 Nov 2025 18:15:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.08604v2</guid></item><item><title>Dynamic causal discovery in Alzheimer's disease through latent pseudotime modelling</title><link>http://arxiv.org/abs/2511.04619v1</link><description>The application of causal discovery to diseases like Alzheimer's (AD) islimited by the static graph assumptions of most methods; such models cannotaccount for an evolving pathophysiology, modulated by a latent diseasepseudotime. We propose to apply an existing latent variable model to real-worldAD data, inferring a pseudotime that orders patients along a data-drivendisease trajectory independent of chronological age, then learning how causalrelationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledgesubstantially improved graph accuracy and orientation. Our framework revealsdynamic interactions between novel (NfL, GFAP) and established AD markers,enabling practical causal discovery despite violated assumptions.</description><author>Natalia Glazman, Jyoti Mangal, Pedro Borges, Sebastien Ourselin, M. Jorge Cardoso</author><pubDate>Thu, 06 Nov 2025 18:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04619v1</guid></item><item><title>Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality</title><link>http://arxiv.org/abs/2511.04615v1</link><description>Deep learning models can generate virtual immunohistochemistry (IHC) stainsfrom hematoxylin and eosin (H&amp;E) images, offering a scalable and low-costalternative to laboratory IHC. However, reliable evaluation of image qualityremains a challenge as current texture- and distribution-based metrics quantifyimage fidelity rather than the accuracy of IHC staining. Here, we introduce anautomated and accuracy grounded framework to determine image quality acrosssixteen paired or unpaired image translation models. Using color deconvolution,we generate masks of pixels stained brown (i.e., IHC-positive) as predicted byeach virtual IHC model. We use the segmented masks of real and virtual IHC tocompute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directlyquantify correct pixel - level labeling without needing expert manualannotations. Our results demonstrate that conventional image fidelity metrics,including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),and structural similarity (SSIM), correlate poorly with stain accuracy andpathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCEachieve the highest stain accuracy, whereas unpaired diffusion- and GAN-basedmodels are less reliable in providing accurate IHC positive pixel labels.Moreover, whole-slide images (WSI) reveal performance declines that areinvisible in patch-based evaluations, emphasizing the need for WSI-levelbenchmarks. Together, this framework defines a reproducible approach forassessing the quality of virtual IHC models, a critical step to acceleratetranslation towards routine use by pathologists.</description><author>Tushar Kataria, Shikha Dubey, Mary Bronner, Jolanta Jedrzkiewicz, Ben J. Brintz, Shireen Y. Elhabian, Beatrice S. Knudsen</author><pubDate>Thu, 06 Nov 2025 18:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04615v1</guid></item><item><title>Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning</title><link>http://arxiv.org/abs/2510.13865v4</link><description>We introduce the Deep Edge Filter, a novel approach that applies high-passfiltering to deep neural network features to improve model generalizability.Our method is motivated by our hypothesis that neural networks encodetask-relevant semantic information in high-frequency components while storingdomain-specific biases in low-frequency components of deep features. Bysubtracting low-pass filtered outputs from original features, our approachisolates generalizable representations while preserving architecturalintegrity. Experimental results across diverse domains such as Vision, Text,3D, and Audio demonstrate consistent performance improvements regardless ofmodel architecture and data modality. Analysis reveals that our method inducesfeature sparsification and effectively isolates high-frequency components,providing empirical validation of our core hypothesis. The code is available athttps://github.com/dongkwani/DeepEdgeFilter.</description><author>Dongkwan Lee, Junhoo Lee, Nojun Kwak</author><pubDate>Thu, 06 Nov 2025 18:08:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.13865v4</guid></item><item><title>DashCLIP: Leveraging multimodal models for generating semantic embeddings for DoorDash</title><link>http://arxiv.org/abs/2504.07110v2</link><description>Despite the success of vision-language models in various generative tasks,obtaining high-quality semantic representations for products and user intentsis still challenging due to the inability of off-the-shelf models to capturenuanced relationships between the entities. In this paper, we introduce a jointtraining framework for product and user queries by aligning uni-modal andmulti-modal encoders through contrastive learning on image-text data. Our novelapproach trains a query encoder with an LLM-curated relevance dataset,eliminating the reliance on engagement history. These embeddings demonstratestrong generalization capabilities and improve performance across applications,including product categorization and relevance prediction. For personalized adsrecommendation, a significant uplift in the click-through rate and conversionrate after the deployment further confirms the impact on key business metrics.We believe that the flexibility of our framework makes it a promising solutiontoward enriching the user experience across the e-commerce landscape.</description><author>Omkar Gurjar, Kin Sum Liu, Praveen Kolli, Utsaw Kumar, Mandar Rahurkar</author><pubDate>Thu, 06 Nov 2025 18:08:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.07110v2</guid></item><item><title>XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification</title><link>http://arxiv.org/abs/2507.14578v2</link><description>We propose XL-DURel, a finetuned, multilingual Sentence Transformer modeloptimized for ordinal Word-in-Context classification. We test several lossfunctions for regression and ranking tasks managing to outperform previousmodels on ordinal and binary data with a ranking objective based on angulardistance in complex space. We further show that binary WiC can be treated as aspecial case of ordinal WiC and that optimizing models for the general ordinaltask improves performance on the more specific binary task. This paves the wayfor a unified treatment of WiC modeling across different task formulations.</description><author>Sachin Yadav, Dominik Schlechtweg</author><pubDate>Thu, 06 Nov 2025 18:07:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14578v2</guid></item><item><title>evomap: A Toolbox for Dynamic Mapping in Python</title><link>http://arxiv.org/abs/2511.04611v1</link><description>This paper presents evomap, a Python package for dynamic mapping. Mappingmethods are widely used across disciplines to visualize relationships amongobjects as spatial representations, or maps. However, most existing statisticalsoftware supports only static mapping, which captures objects' relationships ata single point in time and lacks tools to analyze how these relationshipsevolve. evomap fills this gap by implementing the dynamic mapping frameworkEvoMap, originally proposed by Matthe, Ringel, and Skiera (2023), which adaptstraditional static mapping methods for dynamic analyses. The package supportsmultiple mapping techniques, including variants of Multidimensional Scaling(MDS), Sammon Mapping, and t-distributed Stochastic Neighbor Embedding (t-SNE).It also includes utilities for data preprocessing, exploration, and resultevaluation, offering a comprehensive toolkit for dynamic mapping applications.This paper outlines the foundations of static and dynamic mapping, describesthe architecture and functionality of evomap, and illustrates its applicationthrough an extensive usage example.</description><author>Maximilian Matthe</author><pubDate>Thu, 06 Nov 2025 18:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04611v1</guid></item><item><title>PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning</title><link>http://arxiv.org/abs/2511.04601v1</link><description>While the Contrastive Language-Image Pretraining(CLIP) model has achievedremarkable success in a variety of downstream vison language understandingtasks, enhancing its capability for fine-grained image-text alignment remainsan active research focus. To this end, most existing works adopt the strategyof explicitly increasing the granularity of visual information processing,e.g., incorporating visual prompts to guide the model focus on specific localregions within the image. Meanwhile, researches on Multimodal Large LanguageModels(MLLMs) have demonstrated that training with long and detailed textualdescriptions can effectively improve the model's fine-grained vision-languagealignment. However, the inherent token length limitation of CLIP's text encoderfundamentally limits CLIP to process more granular textual information embeddedin long text sequences. To synergistically leverage the advantages of enhancingboth visual and textual content processing granularity, we propose PixCLIP, anovel framework designed to concurrently accommodate visual prompt inputs andprocess lengthy textual descriptions. Specifically, we first establish anautomated annotation pipeline capable of generating pixel-level localized,long-form textual descriptions for images. Utilizing this pipeline, weconstruct LongGRIT, a high-quality dataset comprising nearly 1.5 millionsamples. Secondly, we replace CLIP's original text encoder with the LLM andpropose a three-branch pixel-text alignment learning framework, facilitatingfine-grained alignment between image regions and corresponding textualdescriptions at arbitrary granularity. Experiments demonstrate that PixCLIPshowcases breakthroughs in pixel-level interaction and handling long-formtexts, achieving state-of-the-art performance.</description><author>Yicheng Xiao, Yu Chen, Haoxuan Ma, Jiale Hong, Caorui Li, Lingxiang Wu, Haiyun Guo, Jinqiao Wang</author><pubDate>Thu, 06 Nov 2025 17:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04601v1</guid></item><item><title>Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures</title><link>http://arxiv.org/abs/2511.04599v1</link><description>Understanding feature-outcome associations in high-dimensional data remains challenging when relationships vary across subpopulations, yet standard methods assuming global associations miss context-dependent patterns,reducing statistical power and interpretability. We develop a geometric decomposition framework offering two strategies for partitioning inference problems into regional analyses on data-derived Riemannian graphs. Gradient flow decomposition uses path-monotonicity-validated discrete Morse theory to partition samples into basins where outcomes exhibit monotonic behavior. Co-monotonicity decomposition leverages association structure: vertex-level coefficients measuring directional concordance between outcome and features, or between feature pairs, define embeddings of samples into associationspace. These embeddings induce Riemannian k-NN graphs on which biclustering identifies co-monotonicity cells (coherent regions) and feature modules. This extends naturally to multi-modal integration across multiple feature sets. Both strategies apply independently or jointly, with Bayesian posterior sampling providing credible intervals.</description><author>Pawel Gajer, Jacques Ravel</author><pubDate>Thu, 06 Nov 2025 17:51:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04599v1</guid></item><item><title>Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning</title><link>http://arxiv.org/abs/2511.04598v1</link><description>In this paper we study how transforming regular reinforcement learningenvironments into goal-conditioned environments can let agents learn to solvetasks autonomously and reward-free. We show that an agent can learn to solvetasks by selecting its own goals in an environment-agnostic way, at trainingtimes comparable to externally guided reinforcement learning. Our method isindependent of the underlying off-policy learning algorithm. Since our methodis environment-agnostic, the agent does not value any goals higher than others,leading to instability in performance for individual goals. However, in ourexperiments, we show that the average goal success rate improves andstabilizes. An agent trained with this method can be instructed to seek anyobservations made in the environment, enabling generic training of agents priorto specific use cases.</description><author>Hampus Åström, Elin Anna Topp, Jacek Malec</author><pubDate>Thu, 06 Nov 2025 17:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04598v1</guid></item><item><title>UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction</title><link>http://arxiv.org/abs/2511.04595v1</link><description>Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,yet existing methods struggle with the joint challenges of sparse,non-overlapping camera views and complex scene dynamics. We present UniSplat, ageneral feed-forward framework that learns robust dynamic scene reconstructionthrough unified latent spatio-temporal fusion. UniSplat constructs a 3D latentscaffold, a structured representation that captures geometric and semanticscene context by leveraging pretrained foundation models. To effectivelyintegrate information across spatial views and temporal frames, we introduce anefficient fusion mechanism that operates directly within the 3D scaffold,enabling consistent spatio-temporal alignment. To ensure complete and detailedreconstructions, we design a dual-branch decoder that generates dynamic-awareGaussians from the fused scaffold by combining point-anchored refinement withvoxel-based generation, and maintain a persistent memory of static Gaussians toenable streaming scene completion beyond current camera coverage. Extensiveexperiments on real-world datasets demonstrate that UniSplat achievesstate-of-the-art performance in novel view synthesis, while providing robustand high-quality renderings even for viewpoints outside the original cameracoverage.</description><author>Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng, Bo Zhang, Li Jiang</author><pubDate>Thu, 06 Nov 2025 17:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04595v1</guid></item><item><title>Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems</title><link>http://arxiv.org/abs/2511.04594v1</link><description>Multi-agent systems (MAS) are central to applications such as swarm roboticsand traffic routing, where agents must coordinate in a decentralized manner toachieve a common objective. Stochastic Shortest Path (SSP) problems provide anatural framework for modeling decentralized control in such settings. Whilethe problem of learning in SSP has been extensively studied in single-agentsettings, the decentralized multi-agent variant remains largely unexplored. Inthis work, we take a step towards addressing that gap. We study decentralizedmulti-agent SSPs (Dec-MASSPs) under linear function approximation, where thetransition dynamics and costs are represented using linear models. Applyingnovel symmetry-based arguments, we identify the structure of optimal policies.Our main contribution is the first regret lower bound for this setting based onthe construction of hard-to-learn instances for any number of agents, $n$. Ourregret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights theinherent learning difficulty in Dec-MASSPs. These insights clarify the learningcomplexity of decentralized control and can further guide the design ofefficient learning algorithms in multi-agent systems.</description><author>Utkarsh U. Chavan, Prashant Trivedi, Nandyala Hemachandra</author><pubDate>Thu, 06 Nov 2025 17:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04594v1</guid></item><item><title>Neural Computation Without Slots: Steps Towards Biologically Plausible Memory and Attention in Natural and Artificial Intelligence</title><link>http://arxiv.org/abs/2511.04593v1</link><description>Many models used in artificial intelligence and cognitive science rely onmulti-element patterns stored in "slots" - dedicated storage locations - in adigital computer. As biological brains likely lack slots, we consider how theymight achieve similar functional outcomes without them by building on theneurally-inspired modern Hopfield network (MHN; Krotov &amp; Hopfield, 2021), whichstores patterns in the connection weights of an individual neuron. We proposeextensions of this approach to increase its biological plausibility as a modelof memory and to capture an important advantage of slot-based computation incontemporary language models. For memory, neuroscience research suggests thatthe weights of overlapping sparse ensembles of neurons, rather than a dedicatedindividual neuron, are used to store a memory. We introduce the K-winner MHN,extending the approach to ensembles, and find that within a continual learningregime, the ensemble-based MHN exhibits greater retention of older memories, asmeasured by the graded sensitivity measure d', than a standard (one-neuron)MHN. Next, we consider the powerful use of slot-based memory in contemporarylanguage models. These models use slots to store long sequences of past inputsand their learned encodings, supporting later predictions and allowing errorsignals to be transported backward in time to adjust weights underlying thelearned encodings of these past inputs. Inspired by these models' successes, weshow how the MHN can be extended to capture both of these important functionaloutcomes. Collectively, our modeling approaches constitute steps towardsunderstanding how biologically plausible mechanisms can support computationsthat have enabled AI systems to capture human-like abilities that no priormodels have been able to achieve.</description><author>Shaunak Bhandarkar, James L. McClelland</author><pubDate>Thu, 06 Nov 2025 17:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04593v1</guid></item><item><title>Projection Methods for Operator Learning and Universal Approximation</title><link>http://arxiv.org/abs/2406.12264v3</link><description>We obtain a new universal approximation theorem for continuous (possiblynonlinear) operators on arbitrary Banach spaces using the Leray-Schaudermapping. Moreover, we introduce and study a method for operator learning inBanach spaces $L^p$ of functions with multiple variables, based on orthogonalprojections on polynomial bases. We derive a universal approximation result foroperators where we learn a linear projection and a finite dimensional mappingunder some additional assumptions. For the case of $p=2$, we give somesufficient conditions for the approximation results to hold. This articleserves as the theoretical framework for a deep learning methodology in operatorlearning.</description><author>Emanuele Zappala</author><pubDate>Thu, 06 Nov 2025 17:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12264v3</guid></item><item><title>Complexity as Advantage: A Regret-Based Perspective on Emergent Structure</title><link>http://arxiv.org/abs/2511.04590v1</link><description>We introduce Complexity as Advantage (CAA), a framework that defines thecomplexity of a system relative to a family of observers. Instead of measuringcomplexity as an intrinsic property, we evaluate how much predictive regret asystem induces for different observers attempting to model it. A system iscomplex when it is easy for some observers and hard for others, creating aninformation advantage. We show that this formulation unifies several notions ofemergent behavior, including multiscale entropy, predictive information, andobserver-dependent structure. The framework suggests that "interesting" systemsare those positioned to create differentiated regret across observers,providing a quantitative grounding for why complexity can be functionallyvaluable. We demonstrate the idea through simple dynamical models and discussimplications for learning, evolution, and artificial agents.</description><author>Oshri Naparstek</author><pubDate>Thu, 06 Nov 2025 17:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04590v1</guid></item><item><title>Question the Questions: Auditing Representation in Online Deliberative Processes</title><link>http://arxiv.org/abs/2511.04588v1</link><description>A central feature of many deliberative processes, such as citizens'assemblies and deliberative polls, is the opportunity for participants toengage directly with experts. While participants are typically invited topropose questions for expert panels, only a limited number can be selected dueto time constraints. This raises the challenge of how to choose a small set ofquestions that best represent the interests of all participants. We introducean auditing framework for measuring the level of representation provided by aslate of questions, based on the social choice concept known as justifiedrepresentation (JR). We present the first algorithms for auditing JR in thegeneral utility setting, with our most efficient algorithm achieving a runtimeof $O(mn\log n)$, where $n$ is the number of participants and $m$ is the numberof proposed questions. We apply our auditing methods to historicaldeliberations, comparing the representativeness of (a) the actual questionsposed to the expert panel (chosen by a moderator), (b) participants' questionschosen via integer linear programming, (c) summary questions generated by largelanguage models (LLMs). Our results highlight both the promise and currentlimitations of LLMs in supporting deliberative processes. By integrating ourmethods into an online deliberation platform that has been used for overhundreds of deliberations across more than 50 countries, we make it easy forpractitioners to audit and improve representation in future deliberations.</description><author>Soham De, Lodewijk Gelauff, Ashish Goel, Smitha Milli, Ariel Procaccia, Alice Siu</author><pubDate>Thu, 06 Nov 2025 17:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04588v1</guid></item><item><title>VISTA Score: Verification In Sequential Turn-based Assessment</title><link>http://arxiv.org/abs/2510.27052v2</link><description>Hallucination--defined here as generating statements unsupported orcontradicted by available evidence or conversational context--remains a majorobstacle to deploying conversational AI systems in settings that demand factualreliability. Existing metrics either evaluate isolated responses or treatunverifiable content as errors, limiting their use for multi-turn dialogue. Weintroduce VISTA (Verification In Sequential Turn-based Assessment), a frameworkfor evaluating conversational factuality through claim-level verification andsequential consistency tracking. VISTA decomposes each assistant turn intoatomic factual claims, verifies them against trusted sources and dialoguehistory, and categorizes unverifiable statements (subjective, contradicted,lacking evidence, or abstaining). Across eight large language models and fourdialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTAsubstantially improves hallucination detection over FACTSCORE and LLM-as-Judgebaselines. Human evaluation confirms that VISTA's decomposition improvesannotator agreement and reveals inconsistencies in existing benchmarks. Bymodeling factuality as a dynamic property of conversation, VISTA offers a moretransparent, human-aligned measure of truthfulness in dialogue systems.</description><author>Ashley Lewis, Andrew Perrault, Eric Fosler-Lussier, Michael White</author><pubDate>Thu, 06 Nov 2025 17:44:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.27052v2</guid></item></channel></rss>