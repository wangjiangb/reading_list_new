<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 26 Aug 2025 13:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2412.13195v2</link><description>Text-to-image (T2I) diffusion models excel at generating photorealisticimages but often fail to render accurate spatial relationships. We identify twocore issues underlying this common failure: 1) the ambiguous nature of dataconcerning spatial relationships in existing datasets, and 2) the inability ofcurrent text encoders to accurately interpret the spatial semantics of inputdescriptions. We propose CoMPaSS, a versatile framework that enhances spatialunderstanding in T2I models. It first addresses data ambiguity with the SpatialConstraints-Oriented Pairing (SCOP) data engine, which curatesspatially-accurate training data via principled constraints. To leverage thesepriors, CoMPaSS also introduces the Token ENcoding ORdering (TENOR) module,which preserves crucial token ordering information lost by text encoders,thereby reinforcing the prompt's linguistic structure. Extensive experiments onfour popular T2I models (UNet and MMDiT-based) show CoMPaSS sets a new state ofthe art on key spatial benchmarks, with substantial relative gains on VISOR(+98%), T2I-CompBench Spatial (+67%), and GenEval Position (+131%). Code isavailable at https://github.com/blurgyy/CoMPaSS.</description><author>Gaoyang Zhang, Bingtao Fu, Qingnan Fan, Qi Zhang, Runxing Liu, Hong Gu, Huaqi Zhang, Xinguo Liu</author><pubDate>Mon, 25 Aug 2025 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13195v2</guid></item><item><title>ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models</title><link>http://arxiv.org/abs/2508.18271v1</link><description>3D inpainting often relies on multi-view 2D image inpainting, where theinherent inconsistencies across different inpainted views can result in blurredtextures, spatial discontinuities, and distracting visual artifacts. Theseinconsistencies pose significant challenges when striving for accurate andrealistic 3D object completion, particularly in applications that demand highfidelity and structural coherence. To overcome these limitations, we proposeObjFiller-3D, a novel method designed for the completion and editing ofhigh-quality and consistent 3D objects. Instead of employing a conventional 2Dimage inpainting model, our approach leverages a curated selection ofstate-of-the-art video editing model to fill in the masked regions of 3Dobjects. We analyze the representation gap between 3D and videos, and proposean adaptation of a video inpainting model for 3D scene inpainting. In addition,we introduce a reference-based 3D inpainting method to further enhance thequality of reconstruction. Experiments across diverse datasets show thatcompared to previous methods, ObjFiller-3D produces more faithful andfine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential forpractical deployment in real-world 3D editing applications. Project page:https://objfiller3d.github.io/ Code:https://github.com/objfiller3d/ObjFiller-3D .</description><author>Haitang Feng, Jie Liu, Jie Tang, Gangshan Wu, Beiqi Chen, Jianhuang Lai, Guangcong Wang</author><pubDate>Mon, 25 Aug 2025 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18271v1</guid></item><item><title>SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation</title><link>http://arxiv.org/abs/2508.18268v1</link><description>Bimanual manipulation has been widely applied in household services andmanufacturing, which enables the complex task completion with coordinationrequirements. Recent diffusion-based policy learning approaches have achievedpromising performance in modeling action distributions for bimanualmanipulation. However, they ignored the physical safety constraints of bimanualmanipulation, which leads to the dangerous behaviors with damage to robots andobjects. To this end, we propose a test-time trajectory optimization frameworknamed SafeBimanual for any pre-trained diffusion-based bimanual manipulationpolicies, which imposes the safety constraints on bimanual actions to avoiddangerous robot behaviors with improved success rate. Specifically, we designdiverse cost functions for safety constraints in different dual-arm cooperationpatterns including avoidance of tearing objects and collision between arms andobjects, which optimizes the manipulator trajectories with guided sampling ofdiffusion denoising process. Moreover, we employ a vision-language model (VLM)to schedule the cost functions by specifying keypoints and correspondingpairwise relationship, so that the optimal safety constraint is dynamicallygenerated in the entire bimanual manipulation process. SafeBimanualdemonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increasein success rate and a 18.8% reduction in unsafe interactions overstate-of-the-art diffusion-based methods. Extensive experiments on 4 real-worldtasks further verify its practical value by improving the success rate by32.5%.</description><author>Haoyuan Deng, Wenkai Guo, Qianzhun Wang, Zhenyu Wu, Ziwei Wang</author><pubDate>Mon, 25 Aug 2025 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18268v1</guid></item><item><title>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</title><link>http://arxiv.org/abs/2508.18265v1</link><description>We introduce InternVL 3.5, a new family of open-source multimodal models thatsignificantly advances versatility, reasoning capability, and inferenceefficiency along the InternVL series. A key innovation is the CascadeReinforcement Learning (Cascade RL) framework, which enhances reasoning througha two-stage process: offline RL for stable convergence and online RL forrefined alignment. This coarse-to-fine training strategy leads to substantialimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. Tooptimize efficiency, we propose a Visual Resolution Router (ViR) thatdynamically adjusts the resolution of visual tokens without compromisingperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)strategy separates the vision encoder and language model across different GPUs,effectively balancing computational load. These contributions collectivelyenable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoningperformance and a 4.05$\times$ inference speedup compared to its predecessor,i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such asGUI interaction and embodied agency. Notably, our largest model, i.e.,InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMsacross general multimodal, reasoning, text, and agentic tasks -- narrowing theperformance gap with leading commercial models like GPT-5. All models and codeare publicly released.</description><author>Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Bowen Zhou, Weijie Su, Kai Chen, Yu Qiao, Wenhai Wang, Gen Luo</author><pubDate>Mon, 25 Aug 2025 17:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18265v1</guid></item><item><title>MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs</title><link>http://arxiv.org/abs/2508.18264v1</link><description>Vision-Language Models (VLMs) demonstrate impressive performance inunderstanding visual content with language instruction by converting visualinput to vision tokens. However, redundancy in vision tokens results in thedegenerated inference efficiency of VLMs. While many algorithms have beenproposed to reduce the number of vision tokens, most of them apply onlyunimodal information (i.e., vision/text) for pruning and ignore the inherentmultimodal property of vision-language tasks. Moreover, it lacks a genericcriterion that can be applied to different modalities. To mitigate thislimitation, in this work, we propose to leverage both vision and text tokens toselect informative vision tokens by the criterion of coverage. We firstformulate the subset selection problem as a maximum coverage problem.Afterward, a subset of vision tokens is optimized to cover the text tokens andthe original set of vision tokens, simultaneously. Finally, a VLM agent can beadopted to further improve the quality of text tokens for guiding visionpruning. The proposed method MMTok is extensively evaluated on benchmarkdatasets with different VLMs. The comparison illustrates that vision and textinformation are complementary, and combining multimodal information can surpassthe unimodal baseline with a clear margin. Moreover, under the maximum coveragecriterion on the POPE dataset, our method achieves a 1.87x speedup whilemaintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,with only four vision tokens, it still preserves 87.7% of the originalperformance on LLaVA-1.5-7B. These results highlight the effectiveness ofcoverage in token selection.</description><author>Sixun Dong, Juhua Hu, Mian Zhang, Ming Yin, Yanjie Fu, Qi Qian</author><pubDate>Mon, 25 Aug 2025 17:57:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18264v1</guid></item><item><title>TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models</title><link>http://arxiv.org/abs/2410.23266v2</link><description>Existing benchmarks often highlight the remarkable performance achieved bystate-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporalcontext for video understanding. However, how well do the models truly performvisual temporal reasoning? Our study of existing benchmarks shows that thiscapability of MFMs is likely overestimated as many questions can be solved byusing a single, few, or out-of-order frames. To systematically examine currentvisual temporal reasoning tasks, we propose three principles with correspondingmetrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) FrameInformation Disparity. Following these principles, we introduce TOMATO,Temporal Reasoning Multimodal Evaluation, a novel benchmark crafted torigorously assess MFMs' temporal reasoning capabilities in video understanding.TOMATO comprises 1,484 carefully curated, human-annotated questions spanningsix tasks (i.e., action count, direction, rotation, shape &amp; trend, velocity &amp;frequency, and visual cues), applied to 1,417 videos, including 805self-recorded and -generated videos, that encompass human-centric, real-world,and simulated scenarios. Our comprehensive evaluation reveals a human-modelperformance gap of 57.3% with the best-performing model. Moreover, our in-depthanalysis uncovers more fundamental limitations beyond this gap in current MFMs.While they can accurately recognize events in isolated frames, they fail tointerpret these frames as a continuous sequence. We believe TOMATO will serveas a crucial testbed for evaluating the next-generation MFMs and as a call tothe community to develop AI systems capable of comprehending human worlddynamics through the video modality.</description><author>Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, Arman Cohan</author><pubDate>Mon, 25 Aug 2025 17:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23266v2</guid></item><item><title>VIN-NBV: A View Introspection Network for Next-Best-View Selection</title><link>http://arxiv.org/abs/2505.06219v3</link><description>Next Best View (NBV) algorithms aim to maximize 3D scene acquisition qualityusing minimal resources, e.g. number of acquisitions, time taken, or distancetraversed. Prior methods often rely on coverage maximization as a proxy forreconstruction quality, but for complex scenes with occlusions and finerdetails, this is not always sufficient and leads to poor reconstructions. Ourkey insight is to train an acquisition policy that directly optimizes forreconstruction quality rather than just coverage. To achieve this, we introducethe View Introspection Network (VIN): a lightweight neural network thatpredicts the Relative Reconstruction Improvement (RRI) of a potential nextviewpoint without making any new acquisitions. We use this network to power asimple, yet effective, sequential samplingbased greedy NBV policy. Ourapproach, VIN-NBV, generalizes to unseen object categories, operates withoutprior scene knowledge, is adaptable to resource constraints, and can handleocclusions. We show that our RRI fitness criterion leads to a ~30% gain inreconstruction quality over a coverage-based criterion using the same greedystrategy. Furthermore, VIN-NBV also outperforms deep reinforcement learningmethods, Scan-RL and GenNBV, by ~40%.</description><author>Noah Frahm, Dongxu Zhao, Andrea Dunn Beltran, Ron Alterovitz, Jan-Michael Frahm, Junier Oliva, Roni Sengupta</author><pubDate>Mon, 25 Aug 2025 17:56:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.06219v3</guid></item><item><title>MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains</title><link>http://arxiv.org/abs/2508.18260v1</link><description>Large reasoning models (LRMs) have shown significant progress in test-timescaling through chain-of-thought prompting. Current approaches like search-o1integrate retrieval augmented generation (RAG) into multi-step reasoningprocesses but rely on a single, linear reasoning chain while incorporatingunstructured textual information in a flat, context-agnostic manner. As aresult, these approaches can lead to error accumulation throughout thereasoning chain, which significantly limits its effectiveness in medicalquestion-answering (QA) tasks where both accuracy and traceability are criticalrequirements. To address these challenges, we propose MIRAGE (Multi-chainInference with Retrieval-Augmented Graph Exploration), a novel test-timescalable reasoning framework that performs dynamic multi-chain inference overstructured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complexqueries into entity-grounded sub-questions, 2) executes parallel inferencechains, 3) retrieves evidence adaptively via neighbor expansion and multi-hoptraversal, and 4) integrates answers using cross-chain verification to resolvecontradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,Tree-of-Thought variants, and other retrieval-augmented baselines in bothautomatic and human evaluations. Additionally, MIRAGE improves interpretabilityby generating explicit reasoning chains that trace each factual claim toconcrete chains within the knowledge graph, making it well-suited for complexmedical reasoning scenarios. The code will be available for further research.</description><author>Kaiwen Wei, Rui Shan, Dongsheng Zou, Jianzhong Yang, Bi Zhao, Junnan Zhu, Jiang Zhong</author><pubDate>Mon, 25 Aug 2025 17:53:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18260v1</guid></item><item><title>Measuring Sycophancy of Language Models in Multi-turn Dialogues</title><link>http://arxiv.org/abs/2505.23840v2</link><description>Large Language Models (LLMs) are expected to provide helpful and harmlessresponses, yet they often exhibit sycophancy--conforming to user beliefsregardless of factual accuracy or ethical soundness. Prior research onsycophancy has primarily focused on single-turn factual correctness,overlooking the dynamics of real-world interactions. In this work, we introduceSYCON Bench, a novel benchmark for evaluating sycophantic behavior inmulti-turn, free-form conversational settings. Our benchmark measures howquickly a model conforms to the user (Turn of Flip) and how frequently itshifts its stance under sustained user pressure (Number of Flip). ApplyingSYCON Bench to 17 LLMs across three real-world scenarios, we find thatsycophancy remains a prevalent failure mode. Our analysis shows that alignmenttuning amplifies sycophantic behavior, whereas model scaling and reasoningoptimization strengthen the model's ability to resist undesirable user views.Reasoning models generally outperform instruction-tuned models but often failwhen they over-index on logical exposition instead of directly addressing theuser's underlying beliefs. Finally, we evaluate four additional promptingstrategies and demonstrate that adopting a third-person perspective reducessycophancy by up to 63.8% in debate scenario. We release our code and data athttps://github.com/JiseungHong/SYCON-Bench.</description><author>Jiseung Hong, Grace Byun, Seungone Kim, Kai Shu, Jinho Choi</author><pubDate>Mon, 25 Aug 2025 17:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.23840v2</guid></item><item><title>ANO : Faster is Better in Noisy Landscape</title><link>http://arxiv.org/abs/2508.18258v1</link><description>Stochastic optimizers are central to deep learning, yet widely used methodssuch as Adam and Adan can degrade in non-stationary or noisy environments,partly due to their reliance on momentum-based magnitude estimates. Weintroduce Ano, a novel optimizer that decouples direction and magnitude:momentum is used for directional smoothing, while instantaneous gradientmagnitudes determine step size. This design improves robustness to gradientnoise while retaining the simplicity and efficiency of first-order methods. Wefurther propose Anolog, which removes sensitivity to the momentum coefficientby expanding its window over time via a logarithmic schedule. We establishnon-convex convergence guarantees with a convergence rate similar to othersign-based methods, and empirically show that Ano provides substantial gains innoisy and non-stationary regimes such as reinforcement learning, whileremaining competitive on low-noise tasks such as standard computer visionbenchmarks.</description><author>Adrien Kegreisz</author><pubDate>Mon, 25 Aug 2025 17:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18258v1</guid></item><item><title>Fault Detection in New Wind Turbines with Limited Data by Generative Transfer Learning</title><link>http://arxiv.org/abs/2504.17709v2</link><description>Intelligent condition monitoring of wind turbines is essential for reducingdowntimes. Machine learning models trained on wind turbine operation data arecommonly used to detect anomalies and, eventually, operation faults. However,data-driven normal behavior models (NBMs) require a substantial amount oftraining data, as NBMs trained with scarce data may result in unreliable faultdetection. To overcome this limitation, we present a novel generative deeptransfer learning approach to make SCADA samples from one wind turbine lackingtraining data resemble SCADA data from wind turbines with representativetraining data. Through CycleGAN-based domain mapping, our method enables theapplication of an NBM trained on an existing wind turbine to a new one withseverely limited data. We demonstrate our approach on field data mapping SCADAsamples across 7 substantially different WTs. Our findings show significantlyimproved fault detection in wind turbines with scarce data. Our method achievesthe most similar anomaly scores to an NBM trained with abundant data,outperforming NBMs trained on scarce training data with improvements of +10.3%in F1-score when 1 month of training data is available and +16.8% when 2 weeksare available. The domain mapping approach outperforms conventional fine-tuningat all considered degrees of data scarcity, ranging from 1 to 8 weeks oftraining data. The proposed technique enables earlier and more reliable faultdetection in newly installed wind farms, demonstrating a novel and promisingresearch direction to improve anomaly detection when faced with training datascarcity.</description><author>Stefan Jonas, Angela Meyer</author><pubDate>Mon, 25 Aug 2025 15:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.17709v2</guid></item><item><title>Detecting and Characterizing Planning in Language Models</title><link>http://arxiv.org/abs/2508.18098v1</link><description>Modern large language models (LLMs) have demonstrated impressive performanceacross a wide range of multi-step reasoning tasks. Recent work suggests thatLLMs may perform planning - selecting a future target token in advance andgenerating intermediate tokens that lead towards it - rather than merelyimprovising one token at a time. However, existing studies assume fixedplanning horizons and often focus on single prompts or narrow domains. Todistinguish planning from improvisation across models and tasks, we presentformal and causally grounded criteria for detecting planning and operationalizethem as a semi-automated annotation pipeline. We apply this pipeline to bothbase and instruction-tuned Gemma-2-2B models on the MBPP code generationbenchmark and a poem generation task where Claude 3.5 Haiku was previouslyshown to plan. Our findings show that planning is not universal: unlike Haiku,Gemma-2-2B solves the same poem generation task through improvisation, and onMBPP it switches between planning and improvisation across similar tasks andeven successive token predictions. We further show that instruction tuningrefines existing planning behaviors in the base model rather than creating themfrom scratch. Together, these studies provide a reproducible and scalablefoundation for mechanistic studies of planning in LLMs.</description><author>Jatin Nainani, Sankaran Vaidyanathan, Connor Watts, Andre N. Assis, Alice Rigg</author><pubDate>Mon, 25 Aug 2025 14:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18098v1</guid></item><item><title>Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problem</title><link>http://arxiv.org/abs/2508.18095v1</link><description>This paper aims to unify Score-based Generative Models (SGMs), also known asDiffusion models, and the Schr\"odinger Bridge (SB) problem through threereparameterization techniques: Iterative Proportional Mean-Matching (IPMM),Iterative Proportional Terminus-Matching (IPTM), and Iterative ProportionalFlow-Matching (IPFM). These techniques significantly accelerate and stabilizethe training of SB-based models. Furthermore, the paper introduces novelinitialization strategies that use pre-trained SGMs to effectively trainSB-based models. By using SGMs as initialization, we leverage the advantages ofboth SB-based models and SGMs, ensuring efficient training of SB-based modelsand further improving the performance of SGMs. Extensive experimentsdemonstrate the significant effectiveness and improvements of the proposedmethods. We believe this work contributes to and paves the way for futureresearch on generative models.</description><author>Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo</author><pubDate>Mon, 25 Aug 2025 14:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18095v1</guid></item><item><title>Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for Multimodal Medical VQA</title><link>http://arxiv.org/abs/2507.05520v2</link><description>Dermatological care via telemedicine often lacks the rich context ofin-person visits. Clinicians must make diagnoses based on a handful of imagesand brief descriptions, without the benefit of physical exams, second opinions,or reference materials. While many medical AI systems attempt to bridge thesegaps with domain-specific fine-tuning, this work hypothesized that mimickingclinical reasoning processes could offer a more effective path forward. Thisstudy tested seven vision-language models on medical visual question answeringacross six configurations: baseline models, fine-tuned variants, and bothaugmented with either reasoning layers that combine multiple modelperspectives, analogous to peer consultation, or retrieval-augmented generationthat incorporates medical literature at inference time, serving a role similarto reference-checking. While fine-tuning degraded performance in four of sevenmodels with an average 30\% decrease, baseline models collapsed on test data.Clinical-inspired architectures, meanwhile, achieved up to 70\% accuracy,maintaining performance on unseen data while generating explainable,literature-grounded outputs critical for clinical adoption. These findingsdemonstrate that medical AI succeeds by reconstructing the collaborative andevidence-based practices fundamental to clinical diagnosis.</description><author>Karishma Thakrar, Shreyas Basavatia, Akshay Daftardar</author><pubDate>Mon, 25 Aug 2025 14:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05520v2</guid></item><item><title>Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering</title><link>http://arxiv.org/abs/2508.18093v1</link><description>We present a case study evaluating large language models (LLMs) with128K-token context windows on a technical question answering (QA) task. Ourbenchmark is built on a user manual for an agricultural machine, available inEnglish, French, and German. It simulates a cross-lingual information retrievalscenario where questions are posed in English against all three languageversions of the manual. The evaluation focuses on realistic"needle-in-a-haystack" challenges and includes unanswerable questions to testfor hallucinations. We compare nine long-context LLMs using direct promptingagainst three Retrieval-Augmented Generation (RAG) strategies (keyword,semantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for thisspecific manual show that Hybrid RAG consistently outperforms directlong-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.57B achieve high accuracy (over 85%) across all languages with RAG. This papercontributes a detailed analysis of LLM performance in a specialized industrialdomain and an open framework for similar evaluations, highlighting practicaltrade-offs and challenges.</description><author>Julius Gun, Timo Oksanen</author><pubDate>Mon, 25 Aug 2025 14:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18093v1</guid></item><item><title>Speech-Based Depressive Mood Detection in the Presence of Multiple Sclerosis: A Cross-Corpus and Cross-Lingual Study</title><link>http://arxiv.org/abs/2508.18092v1</link><description>Depression commonly co-occurs with neurodegenerative disorders like MultipleSclerosis (MS), yet the potential of speech-based Artificial Intelligence fordetecting depression in such contexts remains unexplored. This study examinesthe transferability of speech-based depression detection methods to people withMS (pwMS) through cross-corpus and cross-lingual analysis using English datafrom the general population and German data from pwMS. Our approach implementssupervised machine learning models using: 1) conventional speech and languagefeatures commonly used in the field, 2) emotional dimensions derived from aSpeech Emotion Recognition (SER) model, and 3) exploratory speech featureanalysis. Despite limited data, our models detect depressive mood in pwMS withmoderate generalisability, achieving a 66% Unweighted Average Recall (UAR) on abinary task. Feature selection further improved performance, boosting UAR to74%. Our findings also highlight the relevant role emotional changes have as anindicator of depressive mood in both the general population and within PwMS.This study provides an initial exploration into generalising speech-baseddepression detection, even in the presence of co-occurring conditions, such asneurodegenerative diseases.</description><author>Monica Gonzalez-Machorro, Uwe Reichel, Pascal Hecker, Helly Hammer, Hesam Sagha, Florian Eyben, Robert Hoepner, Björn W. Schuller</author><pubDate>Mon, 25 Aug 2025 14:54:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18092v1</guid></item><item><title>Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization</title><link>http://arxiv.org/abs/2508.18091v1</link><description>This paper investigates the capabilities of large language models (LLMs) informulating and solving decision-making problems using mathematicalprogramming. We first conduct a systematic review and meta-analysis of recentliterature to assess how well LLMs understand, structure, and solveoptimization problems across domains. The analysis is guided by critical reviewquestions focusing on learning approaches, dataset designs, evaluation metrics,and prompting strategies. Our systematic evidence is complemented by targetedexperiments designed to evaluate the performance of state-of-the-art LLMs inautomatically generating optimization models for problems in computer networks.Using a newly constructed dataset, we apply three prompting strategies:Act-as-expert, chain-of-thought, and self-consistency, and evaluate theobtained outputs based on optimality gap, token-level F1 score, and compilationaccuracy. Results show promising progress in LLMs' ability to parse naturallanguage and represent symbolic formulations, but also reveal key limitationsin accuracy, scalability, and interpretability. These empirical gaps motivateseveral future research directions, including structured datasets,domain-specific fine-tuning, hybrid neuro-symbolic approaches, modularmulti-agent architectures, and dynamic retrieval via chain-of-RAGs. This papercontributes a structured roadmap for advancing LLM capabilities in mathematicalprogramming.</description><author>Mohammad J. Abdel-Rahman, Yasmeen Alslman, Dania Refai, Amro Saleh, Malik A. Abu Loha, Mohammad Yahya Hamed</author><pubDate>Mon, 25 Aug 2025 14:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18091v1</guid></item><item><title>Named Entity Recognition of Historical Text via Large Language Model</title><link>http://arxiv.org/abs/2508.18090v1</link><description>Large language models have demonstrated remarkable versatility across a widerange of natural language processing tasks and domains. One such task is NamedEntity Recognition (NER), which involves identifying and classifying propernames in text, such as people, organizations, locations, dates, and otherspecific entities. NER plays a crucial role in extracting information fromunstructured textual data, enabling downstream applications such as informationretrieval from unstructured text. Traditionally, NER is addressed using supervised machine learning approaches,which require large amounts of annotated training data. However, historicaltexts present a unique challenge, as the annotated datasets are often scarce ornonexistent, due to the high cost and expertise required for manual labeling.In addition, the variability and noise inherent in historical language, such asinconsistent spelling and archaic vocabulary, further complicate thedevelopment of reliable NER systems for these sources. In this study, we explore the feasibility of applying LLMs to NER inhistorical documents using zero-shot and few-shot prompting strategies, whichrequire little to no task-specific training data. Our experiments, conducted onthe HIPE-2022 (Identifying Historical People, Places and other Entities)dataset, show that LLMs can achieve reasonably strong performance on NER tasksin this setting. While their performance falls short of fully supervised modelstrained on domain-specific annotations, the results are nevertheless promising.These findings suggest that LLMs offer a viable and efficient alternative forinformation extraction in low-resource or historically significant corpora,where traditional supervised methods are infeasible.</description><author>Shibingfeng Zhang, Giovanni Colavizza</author><pubDate>Mon, 25 Aug 2025 14:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18090v1</guid></item><item><title>How Quantization Shapes Bias in Large Language Models</title><link>http://arxiv.org/abs/2508.18088v1</link><description>This work presents a comprehensive evaluation of how quantization affectsmodel bias, with particular attention to its impact on individual demographicsubgroups. We focus on weight and activation quantization strategies andexamine their effects across a broad range of bias types, includingstereotypes, toxicity, sentiment, and fairness. We employ both probabilisticand generated text-based metrics across nine benchmarks and evaluate modelsvarying in architecture family and reasoning ability. Our findings show thatquantization has a nuanced impact on bias: while it can reduce model toxicityand does not significantly impact sentiment, it tends to slightly increasestereotypes and unfairness in generative tasks, especially under aggressivecompression. These trends are generally consistent across demographiccategories and model types, although their magnitude depends on the specificsetting. Overall, our results highlight the importance of carefully balancingefficiency and ethical considerations when applying quantization in practice.</description><author>Federico Marcuzzi, Xuefei Ning, Roy Schwartz, Iryna Gurevych</author><pubDate>Mon, 25 Aug 2025 14:48:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18088v1</guid></item><item><title>Quantum-Classical Hybrid Framework for Zero-Day Time-Push GNSS Spoofing Detection</title><link>http://arxiv.org/abs/2508.18085v1</link><description>Global Navigation Satellite Systems (GNSS) are critical for Positioning,Navigation, and Timing (PNT) applications. However, GNSS are highly vulnerableto spoofing attacks, where adversaries transmit counterfeit signals to misleadreceivers. Such attacks can lead to severe consequences, including misdirectednavigation, compromised data integrity, and operational disruptions. Mostexisting spoofing detection methods depend on supervised learning techniquesand struggle to detect novel, evolved, and unseen attacks. To overcome thislimitation, we develop a zero-day spoofing detection method using a HybridQuantum-Classical Autoencoder (HQC-AE), trained solely on authentic GNSSsignals without exposure to spoofed data. By leveraging features extractedduring the tracking stage, our method enables proactive detection before PNTsolutions are computed. We focus on spoofing detection in static GNSSreceivers, which are particularly susceptible to time-push spoofing attacks,where attackers manipulate timing information to induce incorrect timecomputations at the receiver. We evaluate our model against different unseentime-push spoofing attack scenarios: simplistic, intermediate, andsophisticated. Our analysis demonstrates that the HQC-AE consistentlyoutperforms its classical counterpart, traditional supervised learning-basedmodels, and existing unsupervised learning-based methods in detecting zero-day,unseen GNSS time-push spoofing attacks, achieving an average detection accuracyof 97.71% with an average false negative rate of 0.62% (when an attack occursbut is not detected). For sophisticated spoofing attacks, the HQC-AE attains anaccuracy of 98.23% with a false negative rate of 1.85%. These findingshighlight the effectiveness of our method in proactively detecting zero-dayGNSS time-push spoofing attacks across various stationary GNSS receiverplatforms.</description><author>Abyad Enan, Mashrur Chowdhury, Sagar Dasgupta, Mizanur Rahman</author><pubDate>Mon, 25 Aug 2025 14:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18085v1</guid></item><item><title>Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models</title><link>http://arxiv.org/abs/2308.15022v4</link><description>Recently, large language models (LLMs), such as GPT-4, stand out remarkableconversational abilities, enabling them to engage in dynamic and contextuallyrelevant dialogues across a wide range of topics. However, given a longconversation, these chatbots fail to recall past information and tend togenerate inconsistent responses. To address this, we propose to recursivelygenerate summaries/ memory using large language models (LLMs) to enhancelong-term memory ability. Specifically, our method first stimulates LLMs tomemorize small dialogue contexts and then recursively produce new memory usingprevious memory and following contexts. Finally, the chatbot can easilygenerate a highly consistent response with the help of the latest memory. Weevaluate our method on both open and closed LLMs, and the experiments on thewidely-used public dataset show that our method can generate more consistentresponses in a long-context conversation. Also, we show that our strategy couldnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhancedLLMs, bringing further long-term dialogue performance. Notably, our method is apotential solution to enable the LLM to model the extremely long context. Thecode and scripts are released.</description><author>Qingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian, Liang Ding</author><pubDate>Mon, 25 Aug 2025 14:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15022v4</guid></item><item><title>Neither Valid nor Reliable? Investigating the Use of LLMs as Judges</title><link>http://arxiv.org/abs/2508.18076v1</link><description>Evaluating natural language generation (NLG) systems remains a core challengeof natural language processing (NLP), further complicated by the rise of largelanguage models (LLMs) that aims to be general-purpose. Recently, largelanguage models as judges (LLJs) have emerged as a promising alternative totraditional metrics, but their validity remains underexplored. This positionpaper argues that the current enthusiasm around LLJs may be premature, as theiradoption has outpaced rigorous scrutiny of their reliability and validity asevaluators. Drawing on measurement theory from the social sciences, we identifyand critically assess four core assumptions underlying the use of LLJs: theirability to act as proxies for human judgment, their capabilities as evaluators,their scalability, and their cost-effectiveness. We examine how each of theseassumptions may be challenged by the inherent limitations of LLMs, LLJs, orcurrent practices in NLG evaluation. To ground our analysis, we explore threeapplications of LLJs: text summarization, data annotation, and safetyalignment. Finally, we highlight the need for more responsible evaluationpractices in LLJs evaluation, to ensure that their growing role in the fieldsupports, rather than undermines, progress in NLG.</description><author>Khaoula Chehbouni, Mohammed Haddou, Jackie Chi Kit Cheung, Golnoosh Farnadi</author><pubDate>Mon, 25 Aug 2025 14:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18076v1</guid></item><item><title>Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype Learning and Clustering</title><link>http://arxiv.org/abs/2508.18075v1</link><description>Open-set few-shot hyperspectral image (HSI) classification aims to classifyimage pixels by using few labeled pixels per class, where the pixels to beclassified may be not all from the classes that have been seen. To address theopen-set HSI classification challenge, current methods focus mainly ondistinguishing the unknown class samples from the known class samples andrejecting them to increase the accuracy of identifying known class samples.They fails to further identify or discovery the unknow classes among thesamples. This paper proposes a prototype learning and clustering method fordiscoverying unknown classes in HSIs under the few-shot environment. Using fewlabeled samples, it strives to develop the ability of infering the prototypesof unknown classes while distinguishing unknown classes from known classes.Once the unknown class samples are rejected by the learned known classclassifier, the proposed method can further cluster the unknown class samplesinto different classes according to their distance to the inferred unknownclass prototypes. Compared to existing state-of-the-art methods, extensiveexperiments on four benchmark HSI datasets demonstrate that our proposed methodexhibits competitive performance in open-set few-shot HSI classification tasks.All the codes are available at \href{https://github.com/KOBEN-ff/OpenFUCD-main}{https://github.com/KOBEN-ff/OpenFUCD-main}</description><author>Chun Liu, Chen Zhang, Zhuo Li, Zheng Li, Wei Yang</author><pubDate>Mon, 25 Aug 2025 14:40:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18075v1</guid></item><item><title>EventTracer: Fast Path Tracing-based Event Stream Rendering</title><link>http://arxiv.org/abs/2508.18071v1</link><description>Simulating event streams from 3D scenes has become a common practice inevent-based vision research, as it meets the demand for large-scale, hightemporal frequency data without setting up expensive hardware devices orundertaking extensive data collections. Yet existing methods in this directiontypically work with noiseless RGB frames that are costly to render, andtherefore they can only achieve a temporal resolution equivalent to 100-300FPS, far lower than that of real-world event data. In this work, we proposeEventTracer, a path tracing-based rendering pipeline that simulateshigh-fidelity event sequences from complex 3D scenes in an efficient andphysics-aware manner. Specifically, we speed up the rendering process via lowsample-per-pixel (SPP) path tracing, and train a lightweight event spikingnetwork to denoise the resulting RGB videos into realistic event sequences. Tocapture the physical properties of event streams, the network is equipped witha bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with abidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs ata speed of about 4 minutes per second of 720p video, and it inherits the meritof accurate spatiotemporal modeling from its path tracing backbone. We show intwo downstream tasks that EventTracer captures better scene details anddemonstrates a greater similarity to real-world event data than other eventsimulators, which establishes it as a promising tool for creating large-scaleevent-RGB datasets at a low cost, narrowing the sim-to-real gap in event-basedvision, and boosting various application scenarios such as robotics, autonomousdriving, and VRAR.</description><author>Zhenyang Li, Xiaoyang Bai, Jinfan Lu, Pengfei Shen, Edmund Y. Lam, Yifan Peng</author><pubDate>Mon, 25 Aug 2025 14:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18071v1</guid></item><item><title>V2X-R: Cooperative LiDAR-4D Radar Fusion with Denoising Diffusion for 3D Object Detection</title><link>http://arxiv.org/abs/2411.08402v5</link><description>Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3Dobject detection using LiDAR and camera data. However, these methods sufferfrom performance degradation in adverse weather conditions. The weather-robust4D radar provides Doppler and additional geometric information, raising thepossibility of addressing this challenge. To this end, we present V2X-R, thefirst simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-Rcontains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar pointclouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes.Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for3D object detection and implement it with various fusion strategies. To achieveweather-robust detection, we additionally propose a Multi-modal DenoisingDiffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4Dradar feature as a condition to prompt the diffusion model to denoise noisyLiDAR features. Experiments show that our LiDAR-4D radar fusion pipelinedemonstrates superior performance in the V2X-R dataset. Over and above this,our MDD module further improved the performance of basic fusion model by up to5.73%/6.70% in foggy/snowy conditions with barely disrupting normalperformance. The dataset and code will be publicly available at:https://github.com/ylwhxht/V2X-R.</description><author>Xun Huang, Jinlong Wang, Qiming Xia, Siheng Chen, Bisheng Yang, Xin Li, Cheng Wang, Chenglu Wen</author><pubDate>Mon, 25 Aug 2025 14:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08402v5</guid></item><item><title>Forgotten Polygons: Multimodal Large Language Models are Shape-Blind</title><link>http://arxiv.org/abs/2502.15969v4</link><description>Despite strong performance on vision-language tasks, Multimodal LargeLanguage Models (MLLMs) struggle with mathematical problem-solving, with bothopen-source and state-of-the-art models falling short of human performance onvisual-math benchmarks. To systematically examine visual-mathematical reasoningin MLLMs, we (1) evaluate their understanding of geometric primitives, (2) testmulti-step reasoning, and (3) explore a potential solution to improve visualreasoning capabilities. Our findings reveal fundamental shortcomings in shaperecognition, with top models achieving under 50% accuracy in identifyingregular polygons. We analyze these failures through the lens of dual-processtheory and show that MLLMs rely on System 1 (intuitive, memorized associations)rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to countthe sides of both familiar and novel shapes, suggesting they have neitherlearned the concept of sides nor effectively process visual inputs. Finally, wepropose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhancesmulti-step mathematical reasoning by explicitly referencing visual annotationsin diagrams, boosting GPT-4o's accuracy on an irregular polygon side-countingtask from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMsremains an open problem, and visually-guided prompting is essential forsuccessfully engaging visual reasoning. Code available at:https://github.com/rsinghlab/Shape-Blind.</description><author>William Rudman, Michal Golovanevsky, Amir Bar, Vedant Palit, Yann LeCun, Carsten Eickhoff, Ritambhara Singh</author><pubDate>Mon, 25 Aug 2025 14:28:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.15969v4</guid></item><item><title>Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering</title><link>http://arxiv.org/abs/2508.12672v3</link><description>Federated Learning (FL) enables collaborative model training across multipleclients without sharing private data. We consider FL scenarios wherein FLclients are subject to adversarial (Byzantine) attacks, while the FL server istrusted (honest) and has a trustworthy side dataset. This may correspond to,e.g., cases where the server possesses trusted data prior to federation, or tothe presence of a trusted client that temporarily assumes the server role. Ourapproach requires only two honest participants, i.e., the server and oneclient, to function effectively, without prior knowledge of the number ofmalicious clients. Theoretical analysis demonstrates bounded optimality gapseven under strong Byzantine attacks. Experimental results show that ouralgorithm significantly outperforms standard and robust FL baselines such asMean, Trimmed Mean, Median, Krum, and Multi-Krum under various attackstrategies including label flipping, sign flipping, and Gaussian noise additionacross MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.</description><author>Emmanouil Kritharakis, Dusan Jakovetic, Antonios Makris, Konstantinos Tserpes</author><pubDate>Mon, 25 Aug 2025 14:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12672v3</guid></item><item><title>Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images</title><link>http://arxiv.org/abs/2508.18067v1</link><description>Semantic segmentation of remote sensing (RS) images is pivotal forcomprehensive Earth observation, but the demand for interpreting new objectcategories, coupled with the high expense of manual annotation, posessignificant challenges. Although open-vocabulary semantic segmentation (OVSS)offers a promising solution, existing frameworks designed for natural imagesare insufficient for the unique complexities of RS data. They struggle withvast scale variations and fine-grained details, and their adaptation oftenrelies on extensive, costly annotations. To address this critical gap, thispaper introduces SegEarth-OV, the first framework for annotation-freeopen-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,a universal upsampler that robustly restores high-resolution spatial detailsfrom coarse features, correcting distorted target shapes without anytask-specific post-training. We also present a simple yet effective Global BiasAlleviation operation to subtract the inherent global context from patchfeatures, significantly enhancing local semantic fidelity. These componentsempower SegEarth-OV to effectively harness the rich semantics of pre-trainedVLMs, making OVSS possible in optical RS contexts. Furthermore, to extend theframework's universality to other challenging RS modalities like SAR images,where large-scale VLMs are unavailable and expensive to create, we introduceAlignEarth, which is a distillation-based strategy and can efficiently transfersemantic knowledge from an optical VLM encoder to an SAR encoder, bypassing theneed to build SAR foundation models from scratch and enabling universal OVSSacross diverse sensor types. Extensive experiments on both optical and SARdatasets validate that SegEarth-OV can achieve dramatic improvements over theSOTA methods, establishing a robust foundation for annotation-free andopen-world Earth observation.</description><author>Kaiyu Li, Xiangyong Cao, Ruixun Liu, Shihong Wang, Zixuan Jiang, Zhi Wang, Deyu Meng</author><pubDate>Mon, 25 Aug 2025 14:22:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18067v1</guid></item><item><title>Arnold: a generalist muscle transformer policy</title><link>http://arxiv.org/abs/2508.18066v1</link><description>Controlling high-dimensional and nonlinear musculoskeletal models of thehuman body is a foundational scientific challenge. Recent machine learningbreakthroughs have heralded policies that master individual skills likereaching, object manipulation and locomotion in musculoskeletal systems withmany degrees of freedom. However, these agents are merely "specialists",achieving high performance for a single skill. In this work, we develop Arnold,a generalist policy that masters multiple tasks and embodiments. Arnoldcombines behavior cloning and fine-tuning with PPO to achieve expert orsuper-expert performance in 14 challenging control tasks from dexterous objectmanipulation to locomotion. A key innovation is Arnold's sensorimotorvocabulary, a compositional representation of the semantics of heterogeneoussensory modalities, objectives, and actuators. Arnold leverages this vocabularyvia a transformer architecture to deal with the variable observation and actionspaces of each task. This framework supports efficient multi-task,multi-embodiment learning and facilitates rapid adaptation to novel tasks.Finally, we analyze Arnold to provide insights into biological motor control,corroborating recent findings on the limited transferability of musclesynergies across tasks.</description><author>Alberto Silvio Chiappa, Boshi An, Merkourios Simos, Chengkun Li, Alexander Mathis</author><pubDate>Mon, 25 Aug 2025 14:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18066v1</guid></item><item><title>FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated Learning</title><link>http://arxiv.org/abs/2508.18060v1</link><description>Federated Learning (FL) enables collaborative model training across multipleclients while preserving data privacy by keeping local datasets on-device. Inthis work, we address FL settings where clients may behave adversarially,exhibiting Byzantine attacks, while the central server is trusted and equippedwith a reference dataset. We propose FedGreed, a resilient aggregation strategyfor federated learning that does not require any assumptions about the fractionof adversarial participants. FedGreed orders clients' local model updates basedon their loss metrics evaluated against a trusted dataset on the server andgreedily selects a subset of clients whose models exhibit the minimalevaluation loss. Unlike many existing approaches, our method is designed tooperate reliably under heterogeneous (non-IID) data distributions, which areprevalent in real-world deployments. FedGreed exhibits convergence guaranteesand bounded optimality gaps under strong adversarial behavior. Experimentalevaluations on MNIST, FMNIST, and CIFAR-10 demonstrate that our methodsignificantly outperforms standard and robust federated learning baselines,such as Mean, Trimmed Mean, Median, Krum, and Multi-Krum, in the majority ofadversarial scenarios considered, including label flipping and Gaussian noiseinjection attacks. All experiments were conducted using the Flower federatedlearning framework.</description><author>Emmanouil Kritharakis, Antonios Makris, Dusan Jakovetic, Konstantinos Tserpes</author><pubDate>Mon, 25 Aug 2025 14:20:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18060v1</guid></item><item><title>Evasive Active Hypothesis Testing with Deep Neuroevolution: The Single- and Multi-Agent Cases</title><link>http://arxiv.org/abs/2403.10112v2</link><description>Active hypothesis testing is a thoroughly studied problem that finds numerousapplications in wireless communications and sensor networks. In this paper, wefocus on one centralized and one decentralized problem of active hypothesistesting in the presence of an eavesdropper. For the centralized problemincluding a single legitimate agent, we present a new framework based on deepNeuroEvolution (NE), whereas, for the decentralized problem, we develop a novelNE-based method for solving collaborative multi-agent tasks, which,interestingly, maintains all computational benefits of our single-agentNE-based scheme. To further reduce the computational complexity of the latterscheme, a novel multi-agent joint NE and pruning framework is also designed.The superiority of the proposed NE-based evasive active hypothesis testingschemes over conventional active hypothesis testing policies, as well aslearning-based methods, is validated through extensive numerical investigationsin an example use case of anomaly detection over wireless sensor networks. Itis demonstrated that the proposed joint optimization and pruning frameworkachieves nearly identical performance with its unpruned counterpart, whileremoving a very large percentage of redundant deep neural network weights.</description><author>George Stamatelis, Angelos-Nikolaos Kanatas, Ioannis Asprogerakas, George C. Alexandropoulos</author><pubDate>Mon, 25 Aug 2025 14:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10112v2</guid></item><item><title>Dynamic Fusion Multimodal Network for SpeechWellness Detection</title><link>http://arxiv.org/abs/2508.18057v1</link><description>Suicide is one of the leading causes of death among adolescents. Previoussuicide risk prediction studies have primarily focused on either textual oracoustic information in isolation, the integration of multimodal signals, suchas speech and text, offers a more comprehensive understanding of anindividual's mental state. Motivated by this, and in the context of the 1stSpeechWellness detection challenge, we explore a lightweight multi-branchmultimodal system based on a dynamic fusion mechanism for speechwellnessdetection. To address the limitation of prior approaches that rely ontime-domain waveforms for acoustic analysis, our system incorporates bothtime-domain and time-frequency (TF) domain acoustic features, as well assemantic representations. In addition, we introduce a dynamic fusion block toadaptively integrate information from different modalities. Specifically, itapplies learnable weights to each modality during the fusion process, enablingthe model to adjust the contribution of each modality. To enhance computationalefficiency, we design a lightweight structure by simplifying the originalbaseline model. Experimental results demonstrate that the proposed systemexhibits superior performance compared to the challenge baseline, achieving a78% reduction in model parameters and a 5% improvement in accuracy.</description><author>Wenqiang Sun, Han Yin, Jisheng Bai, Jianfeng Chen</author><pubDate>Mon, 25 Aug 2025 14:18:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18057v1</guid></item><item><title>Explainable Prediction of the Mechanical Properties of Composites with CNNs</title><link>http://arxiv.org/abs/2505.14745v2</link><description>Composites are amongst the most important materials manufactured today, asevidenced by their use in countless applications. In order to establish thesuitability of composites in specific applications, finite element (FE)modelling, a numerical method based on partial differential equations, is theindustry standard for assessing their mechanical properties. However, FEmodelling is exceptionally costly from a computational viewpoint, a limitationwhich has led to efforts towards applying AI models to this task. However, inthese approaches: the chosen model architectures were rudimentary, feed-forwardneural networks giving limited accuracy; the studies focused on predictingelastic mechanical properties, without considering material strength limits;and the models lacked transparency, hindering trustworthiness by users. In thispaper, we show that convolutional neural networks (CNNs) equipped with methodsfrom explainable AI (XAI) can be successfully deployed to solve this problem.Our approach uses customised CNNs trained on a dataset we generate usingtransverse tension tests in FE modelling to predict composites' mechanicalproperties, i.e., Young's modulus and yield strength. We show empirically thatour approach achieves high accuracy, outperforming a baseline, ResNet-34, inestimating the mechanical properties. We then use SHAP and IntegratedGradients, two post-hoc XAI methods, to explain the predictions, showing thatthe CNNs use the critical geometrical features that influence the composites'behaviour, thus allowing engineers to verify that the models are trustworthy byrepresenting the science of composites.</description><author>Varun Raaghav, Dimitrios Bikos, Antonio Rago, Francesca Toni, Maria Charalambides</author><pubDate>Mon, 25 Aug 2025 14:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14745v2</guid></item><item><title>BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification with Swin-HAFNet</title><link>http://arxiv.org/abs/2506.14318v3</link><description>Accurate segmentation and classification of brain tumors from MagneticResonance Imaging (MRI) remain key challenges in medical image analysis,primarily due to the lack of high-quality, balanced, and diverse datasets. Inthis work, we present a newly developed MRI dataset named BRISC designedspecifically for brain tumor segmentation and classification tasks. The datasetcomprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certifiedradiologists and physicians. It includes three major tumor types, namelyglioma, meningioma, and pituitary, as well as non-tumorous cases. Each sampleincludes high-resolution labels and is categorized across axial, sagittal, andcoronal imaging planes to facilitate robust model development and cross-viewgeneralization. To demonstrate the utility of the dataset, we propose atransformer-based model, leveraging a Swin Transformer backbone for multi-scalefeature representation, to benchmark both segmentation and classificationtasks. This model serves as a benchmark to demonstrate the utility of the BRISCdataset for advancing methodological research in neuro-oncological imageanalysis. datasetlink: https://www.kaggle.com/datasets/briscdataset/brisc2025/</description><author>Amirreza Fateh, Yasin Rezvani, Sara Moayedi, Sadjad Rezvani, Fatemeh Fateh, Mansoor Fateh, Vahid Abolghasemi</author><pubDate>Mon, 25 Aug 2025 14:11:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.14318v3</guid></item><item><title>Weisfeiler-Lehman meets Events: An Expressivity Analysis for Continuous-Time Dynamic Graph Neural Networks</title><link>http://arxiv.org/abs/2508.18052v1</link><description>Graph Neural Networks (GNNs) are known to match the distinguishing power ofthe 1-Weisfeiler-Lehman (1-WL) test, and the resulting partitions coincide withthe unfolding tree equivalence classes of graphs. Preserving this equivalence,GNNs can universally approximate any target function on graphs in probabilityup to any precision. However, these results are limited to attributeddiscrete-dynamic graphs represented as sequences of connected graph snapshots.Real-world systems, such as communication networks, financial transactionnetworks, and molecular interactions, evolve asynchronously and may split intodisconnected components. In this paper, we extend the theory of attributeddiscrete-dynamic graphs to attributed continuous-time dynamic graphs witharbitrary connectivity. To this end, we introduce a continuous-time dynamic1-WL test, prove its equivalence to continuous-time dynamic unfolding trees,and identify a class of continuous-time dynamic GNNs (CGNNs) based ondiscrete-dynamic GNN architectures that retain both distinguishing power anduniversal approximation guarantees. Our constructive proofs further yieldpractical design guidelines, emphasizing a compact and expressive CGNNarchitecture with piece-wise continuously differentiable temporal functions toprocess asynchronous, disconnected graphs.</description><author>Silvia Beddar-Wiesing, Alice Moallemy-Oureh</author><pubDate>Mon, 25 Aug 2025 14:10:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18052v1</guid></item><item><title>Training Transformers for Mesh-Based Simulations</title><link>http://arxiv.org/abs/2508.18051v1</link><description>Simulating physics using Graph Neural Networks (GNNs) is predominantly drivenby message-passing architectures, which face challenges in scaling andefficiency, particularly in handling large, complex meshes. These architectureshave inspired numerous enhancements, including multigrid approaches and $K$-hopaggregation (using neighbours of distance $K$), yet they often introducesignificant complexity and suffer from limited in-depth investigations. Inresponse to these challenges, we propose a novel Graph Transformer architecturethat leverages the adjacency matrix as an attention mask. The proposed approachincorporates innovative augmentations, including Dilated Sliding Windows andGlobal Attention, to extend receptive fields without sacrificing computationalefficiency. Through extensive experimentation, we evaluate model size,adjacency matrix augmentations, positional encoding and $K$-hop configurationsusing challenging 3D computational fluid dynamics (CFD) datasets. We also trainover 60 models to find a scaling law between training FLOPs and parameters. Theintroduced models demonstrate remarkable scalability, performing on meshes withup to 300k nodes and 3 million edges. Notably, the smallest model achievesparity with MeshGraphNet while being $7\times$ faster and $6\times$ smaller.The largest model surpasses the previous state-of-the-art by $38.8$\% onaverage and outperforms MeshGraphNet by $52$\% on the all-rollout RMSE, whilehaving a similar training speed. Code and datasets are available athttps://github.com/DonsetPG/graph-physics.</description><author>Paul Garnier, Vincent Lannelongue, Jonathan Viquerat, Elie Hachem</author><pubDate>Mon, 25 Aug 2025 14:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18051v1</guid></item><item><title>Probabilistic Classification of Near-Surface Shallow-Water Sediments using A Portable Free-Fall Penetrometer</title><link>http://arxiv.org/abs/2410.00225v2</link><description>The geotechnical evaluation of seabed sediments is important for engineeringprojects and naval applications, offering valuable insights into sedimentproperties, behavior, and strength. Obtaining high-quality seabed samples canbe a challenging task, making in situ testing an essential part of sitecharacterization. Free-fall penetrometers (FFPs) are robust tools for rapidlyprofiling seabed surface sediments, even in energetic nearshore or estuarineconditions and shallow as well as deep depths. Although methods forinterpretation of traditional offshore cone penetration testing (CPT) data arewell-established, their adaptation to FFP data is still an area of research.This study introduces an innovative approach that utilizes machine learningalgorithms to create a sediment behavior classification system based onportable free- fall penetrometer (PFFP) data. The proposed model leverages PFFPmeasurements obtained from multiple locations, such as Sequim Bay (Washington),the Potomac River, and the York River (Virginia). The results show 91.1%accuracy in the class prediction, with the classes representing cohesionlesssediment with little to no plasticity (Class 1), cohesionless sediment withsome plasticity (Class 2), cohesive sediment with low plasticity (Class 3), andcohesive sediment with high plasticity (Class 4). The model prediction not onlypredicts classes but also yields an estimate of inherent uncertainty associatedwith the prediction, which can provide valuable insight into different sedimentbehaviors. Lower uncertainties are more common, but they can increasesignificantly depending on variations in sediment composition, environmentalconditions, and operational techniques. By quantifying uncertainty, the modeloffers a more comprehensive and informed approach to sediment classification</description><author>Md Rejwanur Rahman, Adrian Rodriguez-Marek, Nina Stark, Grace Massey, Carl Friedrichs, Kelly M. Dorgan</author><pubDate>Mon, 25 Aug 2025 14:08:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.00225v2</guid></item><item><title>ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation</title><link>http://arxiv.org/abs/2508.18050v1</link><description>Camouflaged Object Segmentation (COS) poses a significant challenge due tothe intrinsic high similarity between targets and backgrounds, demanding modelscapable of profound holistic understanding beyond superficial cues. Prevailingmethods, often limited by shallow feature representation, inadequate reasoningmechanisms, and weak cross-modal integration, struggle to achieve this depth ofcognition, resulting in prevalent issues like incomplete target separation andimprecise segmentation. Inspired by the perceptual strategy of the Hundred-eyedGiant-emphasizing holistic observation, omnidirectional focus, and intensivescrutiny-we introduce ArgusCogito, a novel zero-shot, chain-of-thoughtframework underpinned by cross-modal synergy and omnidirectional reasoningwithin Vision-Language Models (VLMs). ArgusCogito orchestrates threecognitively-inspired stages: (1) Conjecture: Constructs a strong cognitiveprior through global reasoning with cross-modal fusion (RGB, depth, semanticmaps), enabling holistic scene understanding and enhanced target-backgrounddisambiguation. (2) Focus: Performs omnidirectional, attention-driven scanningand focused reasoning, guided by semantic priors from Conjecture, enablingprecise target localization and region-of-interest refinement. (3) Sculpting:Progressively sculpts high-fidelity segmentation masks by integratingcross-modal information and iteratively generating dense positive/negativepoint prompts within focused regions, emulating Argus' intensive scrutiny.Extensive evaluations on four challenging COS benchmarks and three MedicalImage Segmentation (MIS) benchmarks demonstrate that ArgusCogito achievesstate-of-the-art (SOTA) performance, validating the framework's exceptionalefficacy, superior generalization capability, and robustness.</description><author>Jianwen Tan, Huiyao Zhang, Rui Xiong, Han Zhou, Hongfei Wang, Ye Li</author><pubDate>Mon, 25 Aug 2025 14:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18050v1</guid></item><item><title>HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data</title><link>http://arxiv.org/abs/2508.18048v1</link><description>User queries in real-world recommendation systems often combine structuredconstraints (e.g., category, attributes) with unstructured preferences (e.g.,product descriptions or reviews). We introduce HyST (Hybrid retrieval overSemi-structured Tabular data), a hybrid retrieval framework that combinesLLM-powered structured filtering with semantic embedding search to supportcomplex information needs over semi-structured tabular data. HyST extractsattribute-level constraints from natural language using large language models(LLMs) and applies them as metadata filters, while processing the remainingunstructured query components via embedding-based retrieval. Experiments on asemi-structured benchmark show that HyST consistently outperforms tradtionalbaselines, highlighting the importance of structured filtering in improvingretrieval precision, offering a scalable and accurate solution for real-worlduser queries.</description><author>Jiyoon Myung, Jihyeon Park, Joohyung Han</author><pubDate>Mon, 25 Aug 2025 14:06:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18048v1</guid></item><item><title>One Framework to Rule Them All: Unifying Multimodal Tasks with LLM Neural-Tuning</title><link>http://arxiv.org/abs/2408.03001v3</link><description>Large-scale models have exhibited remarkable capabilities across diversedomains, including automated medical services and intelligent customer support.However, as most large models are trained on single-modality corpora, enablingthem to effectively process and understand multimodal signals remains asignificant challenge. Current research often focuses on designingtask-specific or scenario-specific tuning strategies, which limits thescalability and versatility. To address this limitation, we propose a unifiedframework that concurrently handles multiple tasks and modalities. In thisframework, all modalities and tasks are represented as unified tokens andtrained using a single, consistent approach. To enable efficient multitaskprocessing, we introduce a novel tuning strategy termed neural tuning, inspiredby the concept of sparse distributed representation in the human brain, whereonly specific subsets of neurons are activated for each task. Furthermore, toadvance research in multimodal and multitask learning, we present a newbenchmark, MMUD, which includes samples annotated with multiple task labelsspanning reasoning segmentation, referring segmentation, image captioning, andtext-to-image generation. By applying neural tuning to pretrained large modelson the MMUD benchmark, we demonstrate the ability to handle multiple taskssimultaneously in a streamlined and efficient manner. All models, code, anddatasets will be released publicly upon publication, fostering further researchand innovation in this field.</description><author>Hao Sun, Yu Song, Jiaqing Liu, Jihong Hu, Yen-Wei Chen, Lanfen Lin</author><pubDate>Mon, 25 Aug 2025 14:06:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03001v3</guid></item><item><title>Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models</title><link>http://arxiv.org/abs/2505.18596v3</link><description>The proliferation of misinformation in digital platforms reveals thelimitations of traditional detection methods, which mostly rely on staticclassification and fail to capture the intricate process of real-worldfact-checking. Despite advancements in Large Language Models (LLMs) thatenhance automated reasoning, their application to misinformation detectionremains hindered by issues of logical inconsistency and superficialverification. In response, we introduce Debate-to-Detect (D2D), a novelMulti-Agent Debate (MAD) framework that reformulates misinformation detectionas a structured adversarial debate. Inspired by fact-checking workflows, D2Dassigns domain-specific profiles to each agent and orchestrates a five-stagedebate process, including Opening Statement, Rebuttal, Free Debate, ClosingStatement, and Judgment. To transcend traditional binary classification, D2Dintroduces a multi-dimensional evaluation mechanism that assesses each claimacross five distinct dimensions: Factuality, Source Reliability, ReasoningQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasetsdemonstrate significant improvements over baseline methods, and the case studyhighlight D2D's capability to iteratively refine evidence while improvingdecision transparency, representing a substantial advancement towardsinterpretable misinformation detection. The code will be released publiclyafter the official publication.</description><author>Chen Han, Wenzhen Zheng, Xijin Tang</author><pubDate>Mon, 25 Aug 2025 14:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.18596v3</guid></item><item><title>Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging</title><link>http://arxiv.org/abs/2507.17412v2</link><description>The increasing volume of medical images poses challenges for radiologists inretrieving relevant cases. Content-based image retrieval (CBIR) systems offerpotential for efficient access to similar cases, yet lack standardizedevaluation and comprehensive studies. Building on prior studies for tumorcharacterization via CBIR, this study advances CBIR research for volumetricmedical images through three key contributions: (1) a framework eliminatingreliance on pre-segmented data and organ-specific datasets, aligning with largeand unstructured image archiving systems, i.e. PACS in clinical practice; (2)introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT'scontextualized late interaction mechanism for 3D medical imaging; (3)comprehensive evaluation across four tumor sites using three feature extractorsand three database configurations. Our evaluations highlight the significantadvantages of C-MIR. We demonstrate the successful adaptation of the lateinteraction principle to volumetric medical images, enabling effectivecontext-aware re-ranking. A key finding is C-MIR's ability to effectivelylocalize the region of interest, eliminating the need for pre-segmentation ofdatasets and offering a computationally efficient alternative to systemsrelying on expensive data enrichment steps. C-MIR demonstrates promisingimprovements in tumor flagging, achieving improved performance, particularlyfor colon and lung tumors (p&lt;0.05). C-MIR also shows potential for improvingtumor staging, warranting further exploration of its capabilities. Ultimately,our work seeks to bridge the gap between advanced retrieval techniques andtheir practical applications in healthcare, paving the way for improveddiagnostic processes.</description><author>Farnaz Khun Jush, Steffen Vogler, Matthias Lenga</author><pubDate>Mon, 25 Aug 2025 14:03:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.17412v2</guid></item><item><title>Post-Training Language Models for Continual Relation Extraction</title><link>http://arxiv.org/abs/2504.05214v3</link><description>Real-world data, such as news articles, social media posts, and chatbotconversations, is inherently dynamic and non-stationary, presenting significantchallenges for constructing real-time structured representations throughknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KGcreation, often struggles to adapt to evolving data when traditional modelsrely on static, outdated datasets. Continual Relation Extraction (CRE) methodstackle this issue by incrementally learning new relations while preservingpreviously acquired knowledge. This study investigates the application ofpre-trained language models (PLMs), specifically large language models (LLMs),to CRE, with a focus on leveraging memory replay to address catastrophicforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) andencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.Task-incremental fine-tuning of LLMs demonstrates superior performance overearlier approaches using encoder-only models like BERT on TACRED, excelling inseen-task accuracy and overall performance (measured by whole and averageaccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRelare similarly promising, achieving second place in whole and average accuracymetrics. This work underscores critical factors in knowledge transfer, languagemodel architecture, and KG completeness, advancing CRE with LLMs and memoryreplay for dynamic, real-time relation extraction.</description><author>Sefika Efeoglu, Adrian Paschke, Sonja Schimmler</author><pubDate>Mon, 25 Aug 2025 14:03:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05214v3</guid></item><item><title>Large Language Models in the Task of Automatic Validation of Text Classifier Predictions</title><link>http://arxiv.org/abs/2505.18688v2</link><description>Machine learning models for text classification are trained to predict aclass for a given text. To do this, training and validation samples must beprepared: a set of texts is collected, and each text is assigned a class. Theseclasses are usually assigned by human annotators with different expertiselevels, depending on the specific classification task. Collecting such samplesfrom scratch is labor-intensive because it requires finding specialists andcompensating them for their work; moreover, the number of available specialistsis limited, and their productivity is constrained by human factors. While itmay not be too resource-intensive to collect samples once, the ongoing need toretrain models (especially in incremental learning pipelines) to address datadrift (also called model drift) makes the data collection process crucial andcostly over the model's entire lifecycle. This paper proposes severalapproaches to replace human annotators with Large Language Models (LLMs) totest classifier predictions for correctness, helping ensure model quality andsupport high-quality incremental learning.</description><author>Aleksandr Tsymbalov, Mikhail Khovrichev</author><pubDate>Mon, 25 Aug 2025 14:01:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.18688v2</guid></item><item><title>Riemannian Change Point Detection on Manifolds with Robust Centroid Estimation</title><link>http://arxiv.org/abs/2508.18045v1</link><description>Non-parametric change-point detection in streaming time series data is along-standing challenge in signal processing. Recent advancements in statisticsand machine learning have increasingly addressed this problem for data residingon Riemannian manifolds. One prominent strategy involves monitoring abruptchanges in the center of mass of the time series. Implemented in a streamingfashion, this strategy, however, requires careful step size tuning whencomputing the updates of the center of mass. In this paper, we propose toleverage robust centroid on manifolds from M-estimation theory to address thisissue. Our proposal consists of comparing two centroid estimates: the classicalKarcher mean (sensitive to change) versus one defined from Huber's function(robust to change). This comparison leads to the definition of a test statisticwhose performance is less sensitive to the underlying estimation method. Wepropose a stochastic Riemannian optimization algorithm to estimate both robustcentroids efficiently. Experiments conducted on both simulated and real-worlddata across two representative manifolds demonstrate the superior performanceof our proposed method.</description><author>Xiuheng Wang, Ricardo Borsoi, Arnaud Breloy, Cédric Richard</author><pubDate>Mon, 25 Aug 2025 14:00:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18045v1</guid></item><item><title>Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</title><link>http://arxiv.org/abs/2508.07819v3</link><description>Pre-trained Vision-Language Models (VLMs) face a significant adaptation gapwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack oflocal inductive biases for dense prediction and their reliance on inflexiblefeature fusion paradigms. We address these limitations through an ArchitecturalCo-Design framework that jointly refines feature representation and cross-modalfusion. Our method proposes a parameter-efficient Convolutional Low-RankAdaptation (Conv-LoRA) adapter to inject local inductive biases forfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) thatleverages visual context to adaptively modulate text prompts, enabling apowerful bidirectional fusion. Extensive experiments on diverse industrial andmedical benchmarks demonstrate superior accuracy and robustness, validatingthat this synergistic co-design is critical for robustly adapting foundationmodels to dense perception tasks.</description><author>Ke Ma, Jun Long, Hongxiao Fei, Liujie Hua, Yiran Qian, Zhen Dai, Yueyi Luo</author><pubDate>Mon, 25 Aug 2025 13:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07819v3</guid></item><item><title>PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration</title><link>http://arxiv.org/abs/2508.18040v1</link><description>Vision language model (VLM)-based mobile agents show great potential forassisting users in performing instruction-driven tasks. However, these agentstypically struggle with personalized instructions -- those containingambiguous, user-specific context -- a challenge that has been largelyoverlooked in previous research. In this paper, we define personalizedinstructions and introduce PerInstruct, a novel human-annotated datasetcovering diverse personalized instructions across various mobile scenarios.Furthermore, given the limited personalization capabilities of existing mobileagents, we propose PerPilot, a plug-and-play framework powered by largelanguage models (LLMs) that enables mobile agents to autonomously perceive,understand, and execute personalized user instructions. PerPilot identifiespersonalized elements and autonomously completes instructions via twocomplementary approaches: memory-based retrieval and reasoning-basedexploration. Experimental results demonstrate that PerPilot effectively handlespersonalized tasks with minimal user intervention and progressively improvesits performance with continued use, underscoring the importance ofpersonalization-aware reasoning for next-generation mobile agents. The datasetand code are available at: https://github.com/xinwang-nwpu/PerPilot</description><author>Xin Wang, Zhiyao Cui, Hao Li, Ya Zeng, Chenxu Wang, Ruiqi Song, Yihang Chen, Kun Shao, Qiaosheng Zhang, Jinzhuo Liu, Siyue Ren, Shuyue Hu, Zhen Wang</author><pubDate>Mon, 25 Aug 2025 13:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18040v1</guid></item><item><title>An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals</title><link>http://arxiv.org/abs/2508.09162v2</link><description>Next generation advanced nuclear reactors are expected to be smaller both insize and power output, relying extensively on fully digital instrumentation andcontrol systems. These reactors will generate a large flow of information inthe form of multivariate time series data, conveying simultaneously various nonlinear cyber physical, process, control, sensor, and operational states.Ensuring data integrity against deception attacks is becoming increasinglyimportant for networked communication and a requirement for safe and reliableoperation. Current efforts to address replay attacks, almost universally focuson watermarking or supervised anomaly detection approaches without furtheridentifying and characterizing the root cause of the anomaly. In addition,these approaches rely mostly on synthetic data with uncorrelated Gaussianprocess and measurement noise and full state feedback or are limited tounivariate signals, signal stationarity, linear quadratic regulators, or otherlinear-time invariant state-space which may fail to capture any unmodeledsystem dynamics. In the realm of regulated nuclear cyber-physical systems,additional work is needed on characterization of replay attacks andexplainability of predictions using real data. Here, we propose an unsupervisedexplainable AI framework based on a combination of autoencoder and customizedwindowSHAP algorithm to fully characterize real-time replay attacks, i.e.,detection, source identification, timing and type, of increasing complexityduring a dynamic time evolving reactor process. The proposed XAI framework wasbenchmarked on several real world datasets from Purdue's nuclear reactor PUR-1with up to six signals concurrently being replayed. In all cases, the XAIframework was able to detect and identify the source and number of signalsbeing replayed and the duration of the falsification with 95 percent or betteraccuracy.</description><author>Konstantinos Vasili, Zachery T. Dahm, Stylianos Chatzidakis</author><pubDate>Mon, 25 Aug 2025 13:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09162v2</guid></item><item><title>Enhancing Differentially Private Linear Regression via Public Second-Moment</title><link>http://arxiv.org/abs/2508.18037v1</link><description>Leveraging information from public data has become increasingly crucial inenhancing the utility of differentially private (DP) methods. Traditional DPapproaches often require adding noise based solely on private data, which cansignificantly degrade utility. In this paper, we address this limitation in thecontext of the ordinary least squares estimator (OLSE) of linear regressionbased on sufficient statistics perturbation (SSP) under the unbounded dataassumption. We propose a novel method that involves transforming private datausing the public second-moment matrix to compute a transformed SSP-OLSE, whosesecond-moment matrix yields a better condition number and improves the OLSEaccuracy and robustness. We derive theoretical error bounds about our methodand the standard SSP-OLSE to the non-DP OLSE, which reveal the improvedrobustness and accuracy achieved by our approach. Experiments on synthetic andreal-world datasets demonstrate the utility and effectiveness of our method.</description><author>Zilong Cao, Hai Zhang</author><pubDate>Mon, 25 Aug 2025 13:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18037v1</guid></item><item><title>Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning</title><link>http://arxiv.org/abs/2507.06469v2</link><description>Graph representation learning has become a mainstream method for frauddetection due to its strong expressive power, which focuses on enhancing noderepresentations through improved neighborhood knowledge capture. However, thefocus on local interactions leads to imbalanced transmission of globaltopological information and increased risk of node-specific information beingoverwhelmed during aggregation due to the imbalance between fraud and benignnodes. In this paper, we first summarize the impact of topology and classimbalance on downstream tasks in GNN-based fraud detection, as the problem ofimbalanced supervisory messages is caused by fraudsters' topological behaviorobfuscation and identity feature concealment. Based on statistical validation,we propose a novel dual-view graph representation learning method to mitigateMessage imbalance in Fraud Detection (MimbFD). Specifically, we design atopological message reachability module for high-quality node representationlearning to penetrate fraudsters' camouflage and alleviate insufficientpropagation. Then, we introduce a local confounding debiasing module to adjustnode representations, enhancing the stable association between noderepresentations and labels to balance the influence of different classes.Finally, we conducted experiments on three public fraud datasets, and theresults demonstrate that MimbFD exhibits outstanding performance in frauddetection.</description><author>Yudan Song, Yuecen Wei, Yuhang Lu, Qingyun Sun, Minglai Shao, Li-e Wang, Chunming Hu, Xianxian Li, Xingcheng Fu</author><pubDate>Mon, 25 Aug 2025 13:54:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.06469v2</guid></item><item><title>Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation</title><link>http://arxiv.org/abs/2508.18032v1</link><description>Despite the promising progress of recent autoregressive models intext-to-image (T2I) generation, their ability to handle multi-attribute andambiguous prompts remains limited. To address these limitations, existing workshave applied chain-of-thought (CoT) to enable stage-aware visual synthesis andemployed reinforcement learning (RL) to improve reasoning capabilities.However, most models provide reward signals only at the end of the generationstage. This monolithic final-only guidance makes it difficult to identify whichstages contribute positively to the final outcome and may lead to suboptimalpolicies. To tackle this issue, we propose a Visual-Chain of Guidance(Visual-CoG) paradigm consisting of three stages: semantic reasoning, processrefining, and outcome evaluation, with stage-aware rewards providing immediateguidance throughout the image generation pipeline. We further construct avisual cognition benchmark, VisCog-Bench, which comprises four subtasks toevaluate the effectiveness of semantic reasoning. Comprehensive evaluations onGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,5%, and 19%, respectively, demonstrating the superior performance of theproposed Visual-CoG. We will release all the resources soon.</description><author>Yaqi Li, Peng Chen, Mingyang Han, Bu Pi, Haoxiang Shi, Runzhou Zhao, Yang Yao, Xuan Zhang, Jun Song</author><pubDate>Mon, 25 Aug 2025 13:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18032v1</guid></item><item><title>FCR: Investigating Generative AI models for Forensic Craniofacial Reconstruction</title><link>http://arxiv.org/abs/2508.18031v1</link><description>Craniofacial reconstruction in forensics is one of the processes to identifyvictims of crime and natural disasters. Identifying an individual from theirremains plays a crucial role when all other identification methods fail.Traditional methods for this task, such as clay-based craniofacialreconstruction, require expert domain knowledge and are a time-consumingprocess. At the same time, other probabilistic generative models like thestatistical shape model or the Basel face model fail to capture the skull andface cross-domain attributes. Looking at these limitations, we propose ageneric framework for craniofacial reconstruction from 2D X-ray images. Here,we used various generative models (i.e., CycleGANs, cGANs, etc) and fine-tunethe generator and discriminator parts to generate more realistic images in twodistinct domains, which are the skull and face of an individual. This is thefirst time where 2D X-rays are being used as a representation of the skull bygenerative models for craniofacial reconstruction. We have evaluated thequality of generated faces using FID, IS, and SSIM scores. Finally, we haveproposed a retrieval framework where the query is the generated face image andthe gallery is the database of real faces. By experimental results, we havefound that this can be an effective tool for forensic science.</description><author>Ravi Shankar Prasad, Dinesh Singh</author><pubDate>Mon, 25 Aug 2025 13:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18031v1</guid></item><item><title>EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</title><link>http://arxiv.org/abs/2506.01608v2</link><description>Understanding behavior requires datasets that capture humans while carryingout complex tasks. The kitchen is an excellent environment for assessing humanmotor and cognitive function, as many complex actions are naturally exhibitedin kitchens from chopping to cleaning. Here, we introduce theEPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion captureplatform inside a kitchen environment. Nine static RGB-D cameras, inertialmeasurement units (IMUs) and one head-mounted HoloLens~2 headset were used tocapture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset isa multi-view action dataset with synchronized exocentric, egocentric, depth,IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjectscooking four different recipes. Action sequences were densely annotated with33.78 action segments per minute. Leveraging this multi-modal dataset, wepropose four benchmarks to advance behavior understanding and modeling through1) a vision-language benchmark, 2) a semantic text-to-motion generationbenchmark, 3) a multi-modal action recognition benchmark, 4) a pose-basedaction segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset topave the way for better methods as well as insights to understand the nature ofecologically-valid human behavior. Code and data are available athttps://github.com/amathislab/EPFL-Smart-Kitchen</description><author>Andy Bonnetto, Haozhe Qi, Franklin Leong, Matea Tashkovska, Mahdi Rad, Solaiman Shokur, Friedhelm Hummel, Silvestro Micera, Marc Pollefeys, Alexander Mathis</author><pubDate>Mon, 25 Aug 2025 13:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.01608v2</guid></item><item><title>AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration</title><link>http://arxiv.org/abs/2508.18025v1</link><description>Autonomous planetary exploration missions are critically dependent onreal-time, accurate environmental perception for navigation and hazardavoidance. However, deploying deep learning models on the resource-constrainedcomputational hardware of planetary exploration platforms remains a significantchallenge. This paper introduces the Adaptive Quantized Planetary CraterDetection System (AQ-PCDSys), a novel framework specifically engineered forreal-time, onboard deployment in the computationally constrained environmentsof space exploration missions. AQ-PCDSys synergistically integrates a QuantizedNeural Network (QNN) architecture, trained using Quantization-Aware Training(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecturesignificantly optimizes model size and inference latency suitable for real-timeonboard deployment in space exploration missions, while preserving highaccuracy. The AMF module intelligently fuses data from Optical Imagery (OI) andDigital Elevation Models (DEMs) at the feature level, utilizing an AdaptiveWeighting Mechanism (AWM) to dynamically prioritize the most relevant andreliable sensor modality based on planetary ambient conditions. This approachenhances detection robustness across diverse planetary landscapes. Paired withMulti-Scale Detection Heads specifically designed for robust and efficientdetection of craters across a wide range of sizes, AQ-PCDSys provides acomputationally efficient, reliable and accurate solution for planetary craterdetection, a critical capability for enabling the next generation of autonomousplanetary landing, navigation, and scientific exploration.</description><author>Aditri Paul, Archan Paul</author><pubDate>Mon, 25 Aug 2025 13:44:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18025v1</guid></item><item><title>RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving</title><link>http://arxiv.org/abs/2505.21577v3</link><description>The ultimate goal of code agents is to solve complex tasks autonomously.Although large language models (LLMs) have made substantial progress in codegeneration, real-world tasks typically demand full-fledged code repositoriesrather than simple scripts. Building such repositories from scratch remains amajor challenge. Fortunately, GitHub hosts a vast, evolving collection ofopen-source repositories, which developers frequently reuse as modularcomponents for complex tasks. Yet, existing frameworks like OpenHands andSWE-Agent still struggle to effectively leverage these valuable resources.Relying solely on README files provides insufficient guidance, and deeperexploration reveals two core obstacles: overwhelming information and tangleddependencies of repositories, both constrained by the limited context windowsof current LLMs. To tackle these issues, we propose RepoMaster, an autonomousagent framework designed to explore and reuse GitHub repositories for solvingcomplex tasks. For efficient understanding, RepoMaster constructs function-callgraphs, module-dependency graphs, and hierarchical code trees to identifyessential components, providing only identified core elements to the LLMsrather than the entire repository. During autonomous execution, itprogressively explores related components using our exploration tools andprunes information to optimize context usage. Evaluated on the adjustedMLE-bench, RepoMaster achieves a 110% relative boost in valid submissions overthe strongest baseline OpenHands. On our newly released GitTaskBench,RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing tokenusage by 95%. Our code and demonstration materials are publicly available athttps://github.com/QuantaAlpha/RepoMaster.</description><author>Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Ronghao Chen, Xin Li, Daxin Jiang, Yuntao Du, Pin Lyu</author><pubDate>Mon, 25 Aug 2025 13:40:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.21577v3</guid></item><item><title>A foundation model with multi-variate parallel attention to generate neuronal activity</title><link>http://arxiv.org/abs/2506.20354v2</link><description>Learning from multi-variate time-series with heterogeneous channelconfigurations remains a fundamental challenge for deep neural networks,particularly in clinical domains such as intracranial electroencephalography(iEEG), where channel setups vary widely across subjects. In this work, weintroduce multi-variate parallel attention (MVPA), a novel self-attentionmechanism that disentangles content, temporal, and spatial attention, enablingflexible, generalizable, and efficient modeling of time-series data withvarying channel counts and configurations. We use MVPA to build MVPFormer, agenerative foundation model for human electrophysiology, trained to predict theevolution of iEEG signals across diverse subjects. To support this and futureefforts by the community, we release the SWEC iEEG dataset, the largestpublicly available iEEG dataset to date, comprising nearly 10,000 hours ofrecordings from heterogeneous clinical sources. MVPFormer leverages MVPA toachieve strong generalization across subjects, demonstrating expert-levelperformance in several iEEG tasks. MVPFormer surpasses state-of-the-artTransformer baselines in seizure detection across the SWEC, the MAYO, and theFNUSA datasets, while also achieving state-of-the-art performance on four BrainTreeBank iEEG decoding tasks. We further validate MVPA on standard time-seriesforecasting and classification tasks, where it matches or exceeds theperformance of existing attention-based models. Together, our contributionsestablish MVPA as a general-purpose attention mechanism for heterogeneoustime-series and MVPFormer as the first open-source, open-weights, and open-dataiEEG foundation model with SOTA clinical performance. The code is available athttps://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEGdataset is available athttps://huggingface.co/datasets/NeuroTec/SWEC_iEEG_Dataset.</description><author>Francesco Carzaniga, Michael Hersche, Abu Sebastian, Kaspar Schindler, Abbas Rahimi</author><pubDate>Mon, 25 Aug 2025 13:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.20354v2</guid></item><item><title>Does simple trump complex? Comparing strategies for adversarial robustness in DNNs</title><link>http://arxiv.org/abs/2508.18019v1</link><description>Deep Neural Networks (DNNs) have shown substantial success in variousapplications but remain vulnerable to adversarial attacks. This study aims toidentify and isolate the components of two different adversarial trainingtechniques that contribute most to increased adversarial robustness,particularly through the lens of margins in the input space -- the minimaldistance between data points and decision boundaries. Specifically, we comparetwo methods that maximize margins: a simple approach which modifies the lossfunction to increase an approximation of the margin, and a more complexstate-of-the-art method (Dynamics-Aware Robust Training) which builds upon thisapproach. Using a VGG-16 model as our base, we systematically isolate andevaluate individual components from these methods to determine their relativeimpact on adversarial robustness. We assess the effect of each component on themodel's performance under various adversarial attacks, including AutoAttack andProjected Gradient Descent (PGD). Our analysis on the CIFAR-10 dataset revealswhich elements most effectively enhance adversarial robustness, providinginsights for designing more robust DNNs.</description><author>William Brooks, Marelie H. Davel, Coenraad Mouton</author><pubDate>Mon, 25 Aug 2025 13:33:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18019v1</guid></item><item><title>Memento: Fine-tuning LLM Agents without Fine-tuning LLMs</title><link>http://arxiv.org/abs/2508.16153v2</link><description>In this paper, we introduce a novel learning paradigm for Adaptive LargeLanguage Model (LLM) agents that eliminates the need for fine-tuning theunderlying LLMs. Existing approaches are often either rigid, relying on static,handcrafted reflection workflows, or computationally intensive, requiringgradient updates of LLM model parameters. In contrast, our method enableslow-cost continual adaptation via memory-based online reinforcement learning.We formalise this as a Memory-augmented Markov Decision Process (M-MDP),equipped with a neural case-selection policy to guide action decisions. Pastexperiences are stored in an episodic memory, either differentiable ornon-parametric. The policy is continually updated based on environmentalfeedback through a memory rewriting mechanism, whereas policy improvement isachieved through efficient memory reading (retrieval). We instantiate our agentmodel in the deep research setting, namely \emph{Memento}, which attains top-1on GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. Itreaches $66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset,outperforming the state-of-the-art training-based method, while case-basedmemory adds $4.7\%$ to $9.6\%$ absolute points on out-of-distribution tasks.Our approach offers a scalable and efficient pathway for developing generalistLLM agents capable of continuous, real-time learning without gradient updates,advancing machine learning towards open-ended skill acquisition and deepresearch scenarios. The code is available athttps://github.com/Agent-on-the-Fly/Memento.</description><author>Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, Jun Wang</author><pubDate>Mon, 25 Aug 2025 13:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16153v2</guid></item><item><title>SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance</title><link>http://arxiv.org/abs/2508.11857v2</link><description>Tokenization remains a fundamental yet underexplored bottleneck in naturallanguage processing, with strategies largely static despite remarkable progressin model architectures. We present SupraTok, a novel tokenization architecturethat reimagines subword segmentation through three innovations: cross-boundarypattern learning that discovers multi-word semantic units, entropy-driven datacuration that optimizes training corpus quality, and multi-phase curriculumlearning for stable convergence. Our approach extends Byte-Pair Encoding bylearning "superword" tokens, coherent multi-word expressions that preservesemantic unity while maximizing compression efficiency. SupraTok achieves 31%improvement in English tokenization efficiency (5.91 versus 4.51 characters pertoken) compared to OpenAI's o200k tokenizer and 30% improvement over Google'sGemma 3 tokenizer (256k vocabulary), while maintaining competitive performanceacross 38 languages. When integrated with a GPT-2 scale model (124M parameters)trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%improvement on HellaSWAG and 9.5% on MMLU benchmarks without architecturalmodifications. While these results are promising at this scale, furthervalidation at larger model scales is needed. These findings suggest thatefficient tokenization can complement architectural innovations as a path toimproved language model performance.</description><author>Andrei-Valentin Tănase, Elena Pelican</author><pubDate>Mon, 25 Aug 2025 13:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11857v2</guid></item><item><title>Local Off-Grid Weather Forecasting with Multi-Modal Earth Observation Data</title><link>http://arxiv.org/abs/2410.12938v4</link><description>Urgent applications like wildfire management and renewable energy generationrequire precise, localized weather forecasts near the Earth's surface. However,forecasts produced by machine learning models or numerical weather predictionsystems are typically generated on large-scale regular grids, where directdownscaling fails to capture fine-grained, near-surface weather patterns. Inthis work, we propose a multi-modal transformer model trained end-to-end todownscale gridded forecasts to off-grid locations of interest. Our modeldirectly combines local historical weather observations (e.g., wind,temperature, dewpoint) with gridded forecasts to produce locally accuratepredictions at various lead times. Multiple data modalities are collected andconcatenated at station-level locations, treated as a token at each station.Using self-attention, the token corresponding to the target location aggregatesinformation from its neighboring tokens. Experiments using weather stationsacross the Northeastern United States show that our model outperforms a rangeof data-driven and non-data-driven off-grid forecasting methods. They alsoreveal that direct input of station data provides a phase shift in localweather forecasting accuracy, reducing the prediction error by up to 80%compared to pure gridded data based models. This approach demonstrates how tobridge the gap between large-scale weather models and locally accurateforecasts to support high-stakes, location-sensitive decision-making.</description><author>Qidong Yang, Jonathan Giezendanner, Daniel Salles Civitarese, Johannes Jakubik, Eric Schmitt, Anirban Chandra, Jeremy Vila, Detlef Hohl, Chris Hill, Campbell Watson, Sherrie Wang</author><pubDate>Mon, 25 Aug 2025 13:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12938v4</guid></item><item><title>Towards Continual Visual Anomaly Detection in the Medical Domain</title><link>http://arxiv.org/abs/2508.18013v1</link><description>Visual Anomaly Detection (VAD) seeks to identify abnormal images andprecisely localize the corresponding anomalous regions, relying solely onnormal data during training. This approach has proven essential in domains suchas manufacturing and, more recently, in the medical field, where accurate andexplainable detection is critical. Despite its importance, the impact ofevolving input data distributions over time has received limited attention,even though such changes can significantly degrade model performance. Inparticular, given the dynamic and evolving nature of medical imaging data,Continual Learning (CL) provides a natural and effective framework toincrementally adapt models while preserving previously acquired knowledge. Thisstudy explores for the first time the application of VAD models in a CLscenario for the medical field. In this work, we utilize a CL version of thewell-established PatchCore model, called PatchCoreCL, and evaluate itsperformance using BMAD, a real-world medical imaging dataset with bothimage-level and pixel-level annotations. Our results demonstrate thatPatchCoreCL is an effective solution, achieving performance comparable to thetask-specific models, with a forgetting value less than a 1%, highlighting thefeasibility and potential of CL for adaptive VAD in medical imaging.</description><author>Manuel Barusco, Francesco Borsatti, Nicola Beda, Davide Dalle Pezze, Gian Antonio Susto</author><pubDate>Mon, 25 Aug 2025 13:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18013v1</guid></item><item><title>Development of a Neural Network Model for Currency Detection to aid visually impaired people in Nigeria</title><link>http://arxiv.org/abs/2508.18012v1</link><description>Neural networks in assistive technology for visually impaired leverageartificial intelligence's capacity to recognize patterns in complex data. Theyare used for converting visual data into auditory or tactile representations,helping the visually impaired understand their surroundings. The primary aim ofthis research is to explore the potential of artificial neural networks tofacilitate the differentiation of various forms of cash for individuals withvisual impairments. In this study, we built a custom dataset of 3,468 images,which was subsequently used to train an SSD neural network model. The proposedsystem can accurately identify Nigerian cash, thereby streamlining commercialtransactions. The performance of the system in terms of accuracy was assessed,and the Mean Average Precision score was over 90%. We believe that our systemhas the potential to make a substantial contribution to the field of assistivetechnology while also improving the quality of life of visually challengedpersons in Nigeria and beyond.</description><author>Sochukwuma Nwokoye, Desmond Moru</author><pubDate>Mon, 25 Aug 2025 13:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18012v1</guid></item><item><title>Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference</title><link>http://arxiv.org/abs/2502.18023v3</link><description>Despite the advancements made in Vision Large Language Models (VLLMs), liketext Large Language Models (LLMs), they have limitations in addressingquestions that require real-time information or are knowledge-intensive.Indiscriminately adopting Retrieval Augmented Generation (RAG) techniques is aneffective yet expensive way to enable models to answer queries beyond theirknowledge scopes. To mitigate the dependence on retrieval and simultaneouslymaintain, or even improve, the performance benefits provided by retrieval, wepropose a method to detect the knowledge boundary of VLLMs, allowing for moreefficient use of techniques like RAG. Specifically, we propose a method withtwo variants that fine-tune a VLLM on an automatically constructed dataset forboundary identification. Experimental results on various types of VisualQuestion Answering datasets show that our method successfully depicts a VLLM'sknowledge boundary, based on which we are able to reduce indiscriminateretrieval while maintaining or improving the performance. In addition, we showthat the knowledge boundary identified by our method for one VLLM can be usedas a surrogate boundary for other VLLMs. Code will be released athttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary</description><author>Zhuo Chen, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinyu Geng, Pengjun Xie, Fei Huang, Kewei Tu</author><pubDate>Mon, 25 Aug 2025 13:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.18023v3</guid></item><item><title>Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2508.18007v1</link><description>Fully Unsupervised Anomaly Detection (FUAD) is a practical extension ofUnsupervised Anomaly Detection (UAD), aiming to detect anomalies without anylabels even when the training set may contain anomalous samples. To achieveFUAD, we pioneer the introduction of Knowledge Distillation (KD) paradigm basedon teacher-student framework into the FUAD setting. However, due to thepresence of anomalies in the training data, traditional KD methods riskenabling the student to learn the teacher's representation of anomalies underFUAD setting, thereby resulting in poor anomaly detection performance. Toaddress this issue, we propose a novel Cross-Domain Distillation (CDD)framework based on the widely studied reverse distillation (RD) paradigm.Specifically, we design a Domain-Specific Training, which divides the trainingset into multiple domains with lower anomaly ratios and train a domain-specificstudent for each. Cross-Domain Knowledge Aggregation is then performed, wherepseudo-normal features generated by domain-specific students collaborativelyguide a global student to learn generalized normal representations across allsamples. Experimental results on noisy versions of the MVTec AD and VisAdatasets demonstrate that our method achieves significant performanceimprovements over the baseline, validating its effectiveness under FUADsetting.</description><author>Xinyue Liu, Jianyuan Wang, Biao Leng, Shuo Zhang</author><pubDate>Mon, 25 Aug 2025 13:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18007v1</guid></item><item><title>Unseen Speaker and Language Adaptation for Lightweight Text-To-Speech with Adapters</title><link>http://arxiv.org/abs/2508.18006v1</link><description>In this paper we investigate cross-lingual Text-To-Speech (TTS) synthesisthrough the lens of adapters, in the context of lightweight TTS systems. Inparticular, we compare the tasks of unseen speaker and language adaptation withthe goal of synthesising a target voice in a target language, in which thetarget voice has no recordings therein. Results from objective evaluationsdemonstrate the effectiveness of adapters in learning language-specific andspeaker-specific information, allowing pre-trained models to learn unseenspeaker identities or languages, while avoiding catastrophic forgetting of theoriginal model's speaker or language information. Additionally, to measure hownative the generated voices are in terms of accent, we propose and validate anobjective metric inspired by mispronunciation detection techniques insecond-language (L2) learners. The paper also provides insights into the impactof adapter placement, configuration and the number of speakers used.</description><author>Alessio Falai, Ziyao Zhang, Akos Gangoly</author><pubDate>Mon, 25 Aug 2025 13:14:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18006v1</guid></item><item><title>Data-Driven Discovery of Interpretable Kalman Filter Variants through Large Language Models and Genetic Programming</title><link>http://arxiv.org/abs/2508.11703v2</link><description>Algorithmic discovery has traditionally relied on human ingenuity andextensive experimentation. Here we investigate whether a prominent scientificcomputing algorithm, the Kalman Filter, can be discovered through an automated,data-driven, evolutionary process that relies on Cartesian Genetic Programming(CGP) and Large Language Models (LLM). We evaluate the contributions of bothmodalities (CGP and LLM) in discovering the Kalman filter under varyingconditions. Our results demonstrate that our framework of CGP and LLM-assistedevolution converges to near-optimal solutions when Kalman optimalityassumptions hold. When these assumptions are violated, our framework evolvesinterpretable alternatives that outperform the Kalman filter. These resultsdemonstrate that combining evolutionary algorithms and generative models forinterpretable, data-driven synthesis of simple computational modules is apotent approach for algorithmic discovery in scientific computing.</description><author>Vasileios Saketos, Sebastian Kaltenbach, Sergey Litvinov, Petros Koumoutsakos</author><pubDate>Mon, 25 Aug 2025 13:14:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11703v2</guid></item><item><title>Previously on... Automating Code Review</title><link>http://arxiv.org/abs/2508.18003v1</link><description>Modern Code Review (MCR) is a standard practice in software engineering, yetit demands substantial time and resource investments. Recent research hasincreasingly explored automating core review tasks using machine learning (ML)and deep learning (DL). As a result, there is substantial variability in taskdefinitions, datasets, and evaluation procedures. This study provides the firstcomprehensive analysis of MCR automation research, aiming to characterize thefield's evolution, formalize learning tasks, highlight methodologicalchallenges, and offer actionable recommendations to guide future research.Focusing on the primary code review tasks, we systematically surveyed 691publications and identified 24 relevant studies published between May 2015 andApril 2024. Each study was analyzed in terms of tasks, models, metrics,baselines, results, validity concerns, and artifact availability. Inparticular, our analysis reveals significant potential for standardization,including 48 task metric combinations, 22 of which were unique to theiroriginal paper, and limited dataset reuse. We highlight challenges and deriveconcrete recommendations for examples such as the temporal bias threat, whichare rarely addressed so far. Our work contributes to a clearer overview of thefield, supports the framing of new research, helps to avoid pitfalls, andpromotes greater standardization in evaluation practices.</description><author>Robert Heumüller, Frank Ortmeier</author><pubDate>Mon, 25 Aug 2025 13:12:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18003v1</guid></item><item><title>A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyond</title><link>http://arxiv.org/abs/2508.18001v1</link><description>In this PhD thesis, we propose a novel framework for uncertaintyquantification in machine learning, which is based on proper scores.Uncertainty quantification is an important cornerstone for trustworthy andreliable machine learning applications in practice. Usually, approaches touncertainty quantification are problem-specific, and solutions and insightscannot be readily transferred from one task to another. Proper scores are lossfunctions minimized by predicting the target distribution. Due to their verygeneral definition, proper scores apply to regression, classification, or evengenerative modeling tasks. We contribute several theoretical results, thatconnect epistemic uncertainty, aleatoric uncertainty, and model calibrationwith proper scores, resulting in a general and widely applicable framework. Weachieve this by introducing a general bias-variance decomposition for strictlyproper scores via functional Bregman divergences. Specifically, we use thekernel score, a kernel-based proper score, for evaluating sample-basedgenerative models in various domains, like image, audio, and natural languagegeneration. This includes a novel approach for uncertainty estimation of largelanguage models, which outperforms state-of-the-art baselines. Further, wegeneralize the calibration-sharpness decomposition beyond classification, whichmotivates the definition of proper calibration errors. We then introduce anovel estimator for proper calibration errors in classification, and a novelrisk-based approach to compare different estimators for squared calibrationerrors. Last, we offer a decomposition of the kernel spherical score, anotherkernel-based proper score, allowing a more fine-grained and interpretableevaluation of generative image models.</description><author>Sebastian G. Gruber</author><pubDate>Mon, 25 Aug 2025 13:11:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18001v1</guid></item><item><title>Denoising, segmentation and volumetric rendering of optical coherence tomography angiography (OCTA) image using deep learning techniques: a review</title><link>http://arxiv.org/abs/2502.14935v2</link><description>Optical coherence tomography angiography (OCTA) is a non-invasive imagingtechnique widely used to study vascular structures and micro-circulationdynamics in the retina and choroid. OCTA has been widely used in clinics fordiagnosing ocular disease and monitoring its progression, because OCTA is saferand faster than dye-based angiography while retaining the ability tocharacterize micro-scale structures. However, OCTA data contains many inherentnoises from the devices and acquisition protocols and suffers from varioustypes of artifacts, which impairs diagnostic accuracy and repeatability. Deeplearning (DL) based imaging analysis models are able to automatically detectand remove artifacts and noises, and enhance the quality of image data. It isalso a powerful tool for segmentation and identification of normal andpathological structures in the images. Thus, the value of OCTA imaging can besignificantly enhanced by the DL-based approaches for interpreting andperforming measurements and predictions on the OCTA data. In this study, wereviewed literature on the DL models for OCTA images in the latest five years.In particular, we focused on discussing the current problems in the OCTA dataand the corresponding design principles of the DL models. We also reviewed thestate-of-art DL models for 3D volumetric reconstruction of the vascularnetworks and pathological structures such as the edema and distorted opticdisc. In addition, the publicly available dataset of OCTA images are summarizedat the end of this review. Overall, this review can provide valuable insightsfor engineers to develop novel DL models by utilizing the characteristics ofOCTA signals and images. The pros and cons of each DL methods and theirapplications discussed in this review can be helpful to assist technicians andclinicians to use proper DL models for fundamental research and diseasescreening.</description><author>Kejie Chen, Guanbing Gao, Xiaochun Yang, Wenbo Wang, Jing Na</author><pubDate>Mon, 25 Aug 2025 13:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.14935v2</guid></item><item><title>Topology Aware Neural Interpolation of Scalar Fields</title><link>http://arxiv.org/abs/2508.17995v1</link><description>This paper presents a neural scheme for the topology-aware interpolation oftime-varying scalar fields. Given a time-varying sequence of persistencediagrams, along with a sparse temporal sampling of the corresponding scalarfields, denoted as keyframes, our interpolation approach aims at "inverting"the non-keyframe diagrams to produce plausible estimations of thecorresponding, missing data. For this, we rely on a neural architecture whichlearns the relation from a time value to the corresponding scalar field, basedon the keyframe examples, and reliably extends this relation to thenon-keyframe time steps. We show how augmenting this architecture with specifictopological losses exploiting the input diagrams both improves the geometricaland topological reconstruction of the non-keyframe time steps. At query time,given an input time value for which an interpolation is desired, our approachinstantaneously produces an output, via a single propagation of the time inputthrough the network. Experiments interpolating 2D and 3D time-varying datasetsshow our approach superiority, both in terms of data and topological fitting,with regard to reference interpolation schemes.</description><author>Mohamed Kissi, Keanu Sisouk, Joshua A. Levine, Julien Tierny</author><pubDate>Mon, 25 Aug 2025 13:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17995v1</guid></item><item><title>Fitting Multilevel Factor Models</title><link>http://arxiv.org/abs/2409.12067v4</link><description>We examine a special case of the multilevel factor model, with covariancegiven by multilevel low rank (MLR) matrix~\cite{parshakova2023factor}. Wedevelop a novel, fast implementation of the expectation-maximization algorithm,tailored for multilevel factor models, to maximize the likelihood of theobserved data. This method accommodates any hierarchical structure andmaintains linear time and storage complexities per iteration. This is achievedthrough a new efficient technique for computing the inverse of the positivedefinite MLR matrix. We show that the inverse of positive definite MLR matrixis also an MLR matrix with the same sparsity in factors, and we use therecursive Sherman-Morrison-Woodbury matrix identity to obtain the factors ofthe inverse. Additionally, we present an algorithm that computes the Choleskyfactorization of an expanded matrix with linear time and space complexities,yielding the covariance matrix as its Schur complement. This paper isaccompanied by an open-source package that implements the proposed methods.</description><author>Tetiana Parshakova, Trevor Hastie, Stephen Boyd</author><pubDate>Mon, 25 Aug 2025 13:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12067v4</guid></item><item><title>A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models</title><link>http://arxiv.org/abs/2508.17994v1</link><description>Aspect-based sentiment analysis enhances sentiment detection by associatingit with specific aspects, offering deeper insights than traditional sentimentanalysis. This study introduces a manually annotated dataset of 10,814multilingual customer reviews covering brick-and-mortar retail stores, labeledwith eight aspect categories and their sentiment. Using this dataset, theperformance of GPT-4 and LLaMA-3 in aspect based sentiment analysis isevaluated to establish a baseline for the newly introduced data. The resultsshow both models achieving over 85% accuracy, while GPT-4 outperforms LLaMA-3overall with regard to all relevant metrics.</description><author>Oleg Silcenco, Marcos R. Machad, Wallace C. Ugulino, Daniel Braun</author><pubDate>Mon, 25 Aug 2025 13:02:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17994v1</guid></item><item><title>Automating Conflict-Aware ACL Configurations with Natural Language Intents</title><link>http://arxiv.org/abs/2508.17990v1</link><description>ACL configuration is essential for managing network flow reachability, yetits complexity grows significantly with topologies and pre-existing rules. Tocarry out ACL configuration, the operator needs to (1) understand the newconfiguration policies or intents and translate them into concrete ACL rules,(2) check and resolve any conflicts between the new and existing rules, and (3)deploy them across the network. Existing systems rely heavily on manual effortsfor these tasks, especially for the first two, which are tedious, error-prone,and impractical to scale. We propose Xumi to tackle this problem. Leveraging LLMs with domain knowledgeof the target network, Xumi automatically and accurately translates the naturallanguage intents into complete ACL rules to reduce operators' manual efforts.Xumi then detects all potential conflicts between new and existing rules andgenerates resolved intents for deployment with operators' guidance, and finallyidentifies the best deployment plan that minimizes the rule additions whilesatisfying all intents. Evaluation shows that Xumi accelerates the entireconfiguration pipeline by over 10x compared to current practices, addressesO(100) conflicting ACLs and reduces rule additions by ~40% in modern cloudnetwork.</description><author>Wenlong Ding, Jianqiang Li, Zhixiong Niu, Huangxun Chen, Yongqiang Xiong, Hong Xu</author><pubDate>Mon, 25 Aug 2025 13:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17990v1</guid></item><item><title>DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins</title><link>http://arxiv.org/abs/2508.17988v1</link><description>Digital twins (DTs) are increasingly utilized to monitor, manage, andoptimize complex systems across various domains, including civil engineering. Acore requirement for an effective DT is to act as a fast, accurate, andmaintainable surrogate of its physical counterpart, the physical twin (PT). Tothis end, machine learning (ML) is frequently employed to (i) constructreal-time DT prototypes using efficient reduced-order models (ROMs) derivedfrom high-fidelity simulations of the PT's nominal behavior, and (ii)specialize these prototypes into DT instances by leveraging historical sensordata from the target PT. Despite the broad applicability of ML, its use in DTengineering remains largely ad hoc. Indeed, while conventional ML pipelinesoften train a single model for a specific task, DTs typically require multiple,task- and domain-dependent models. Thus, a more structured approach is requiredto design DTs. In this paper, we introduce DesCartes Builder, an open-source tool to enablethe systematic engineering of ML-based pipelines for real-time DT prototypesand DT instances. The tool leverages an open and flexible visual data flowparadigm to facilitate the specification, composition, and reuse of ML models.It also integrates a library of parameterizable core operations and MLalgorithms tailored for DT design. We demonstrate the effectiveness andusability of DesCartes Builder through a civil engineering use case involvingthe design of a real-time DT prototype to predict the plastic strain of astructure.</description><author>Eduardo de Conto, Blaise Genest, Arvind Easwaran, Nicholas Ng, Shweta Menon</author><pubDate>Mon, 25 Aug 2025 12:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17988v1</guid></item><item><title>Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization</title><link>http://arxiv.org/abs/2508.11365v2</link><description>Decision-focused learning (DFL) trains a machine learning (ML) model topredict parameters of an optimization problem, to directly minimize decisionregret, i.e., maximize decision quality. Gradient-based DFL requires computingthe derivative of the solution to the optimization problem with respect to thepredicted parameters. However, for many optimization problems, such as linearprograms (LPs), the gradient of the regret with respect to the predictedparameters is zero almost everywhere. Existing gradient-based DFL approachesfor LPs try to circumvent this issue in one of two ways: (a) smoothing the LPinto a differentiable optimization problem by adding a quadratic regularizerand then minimizing the regret directly or (b) minimizing surrogate losses thathave informative (sub)gradients. In this paper, we show that the formerapproach still results in zero gradients, because even after smoothing theregret remains constant across large regions of the parameter space. To addressthis, we propose minimizing surrogate losses -- even when a differentiableoptimization layer is used and regret can be minimized directly. Ourexperiments demonstrate that minimizing surrogate losses allows differentiableoptimization layers to achieve regret comparable to or better thansurrogate-loss based DFL methods. Further, we demonstrate that this also holdsfor DYS-Net, a recently proposed differentiable optimization technique for LPs,that computes approximate solutions and gradients through operations that canbe performed using feedforward neural network layers. Because DYS-Net executesthe forward and the backward pass very efficiently, by minimizing surrogatelosses using DYS-Net, we are able to attain regret on par with thestate-of-the-art while reducing training time by a significant margin.</description><author>Jayanta Mandi, Ali İrfan Mahmutoğulları, Senne Berden, Tias Guns</author><pubDate>Mon, 25 Aug 2025 12:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11365v2</guid></item><item><title>Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization</title><link>http://arxiv.org/abs/2508.17976v1</link><description>The increasing sophistication of image manipulation techniques demands robustforensic solutions that can both reliably detect alterations and preciselylocalize tampered regions. Recent Multimodal Large Language Models (MLLMs) showpromise by leveraging world knowledge and semantic understanding forcontext-aware detection, yet they struggle with perceiving subtle, low-levelforensic artifacts crucial for accurate manipulation localization. This paperpresents a novel Propose-Rectify framework that effectively bridges semanticreasoning with forensic-specific analysis. In the proposal stage, our approachutilizes a forensic-adapted LLaVA model to generate initial manipulationanalysis and preliminary localization of suspicious regions based on semanticunderstanding and contextual reasoning. In the rectification stage, weintroduce a Forensics Rectification Module that systematically validates andrefines these initial proposals through multi-scale forensic feature analysis,integrating technical evidence from several specialized filters. Additionally,we present an Enhanced Segmentation Module that incorporates critical forensiccues into SAM's encoded image embeddings, thereby overcoming inherent semanticbiases to achieve precise delineation of manipulated regions. Bysynergistically combining advanced multimodal reasoning with establishedforensic methodologies, our framework ensures that initial semantic proposalsare systematically validated and enhanced through concrete technical evidence,resulting in comprehensive detection accuracy and localization precision.Extensive experimental validation demonstrates state-of-the-art performanceacross diverse datasets with exceptional robustness and generalizationcapabilities.</description><author>Keyang Zhang, Chenqi Kong, Hui Liu, Bo Ding, Xinghao Jiang, Haoliang Li</author><pubDate>Mon, 25 Aug 2025 12:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17976v1</guid></item><item><title>Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</title><link>http://arxiv.org/abs/2508.17975v1</link><description>The use of computer vision in automotive is a trending research in whichsafety and security are a primary concern. In particular, for autonomousdriving, preventing road accidents requires highly accurate object detectionunder diverse conditions. To address this issue, recently the InternationalOrganization for Standardization (ISO) released the 8800 norm, providingstructured frameworks for managing associated AI relevant risks. However,challenging scenarios such as adverse weather or low lighting often introducedata drift, leading to degraded model performance and potential safetyviolations. In this work, we present a novel hybrid computer visionarchitecture trained with thousands of synthetic image data from the roadenvironment to improve robustness in unseen drifted environments. Our dual modeframework utilized YOLO version 8 for swift detection and incorporated afive-layer CNN for verification. The system functioned in sequence and improvedthe detection accuracy by more than 90\% when tested with drift-augmented roadimages. The focus was to demonstrate how such a hybrid model can provide betterroad safety when working together in a hybrid structure.</description><author>Md Shahi Amran Hossain, Abu Shad Ahammed, Sayeri Mukherjee, Roman Obermaisser</author><pubDate>Mon, 25 Aug 2025 12:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17975v1</guid></item><item><title>German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German</title><link>http://arxiv.org/abs/2508.17973v1</link><description>The ability to paraphrase texts across different complexity levels isessential for creating accessible texts that can be tailored toward diversereader groups. Thus, we introduce German4All, the first large-scale Germandataset of aligned readability-controlled, paragraph-level paraphrases. Itspans five readability levels and comprises over 25,000 samples. The dataset isautomatically synthesized using GPT-4 and rigorously evaluated through bothhuman and LLM-based judgments. Using German4All, we train an open-source,readability-controlled paraphrasing model that achieves state-of-the-artperformance in German text simplification, enabling more nuanced andreader-specific adaptations. We opensource both the dataset and the model toencourage further research on multi-level paraphrasing</description><author>Miriam Anschütz, Thanh Mai Pham, Eslam Nasrallah, Maximilian Müller, Cristian-George Craciun, Georg Groh</author><pubDate>Mon, 25 Aug 2025 12:40:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17973v1</guid></item><item><title>SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization</title><link>http://arxiv.org/abs/2508.17972v1</link><description>Scene regression methods, such as VGGT, solve the Structure-from-Motion (SfM)problem by directly regressing camera poses and 3D scene structures from inputimages. They demonstrate impressive performance in handling images underextreme viewpoint changes. However, these methods struggle to handle a largenumber of input images. To address this problem, we introduce SAIL-Recon, afeed-forward Transformer for large scale SfM, by augmenting the sceneregression network with visual localization capabilities. Specifically, ourmethod first computes a neural scene representation from a subset of anchorimages. The regression network is then fine-tuned to reconstruct all inputimages conditioned on this neural scene representation. Comprehensiveexperiments show that our method not only scales efficiently to large-scalescenes, but also achieves state-of-the-art results on both camera poseestimation and novel view synthesis benchmarks, including TUM-RGBD, CO3Dv2, andTanks &amp; Temples. We will publish our model and code. Code and models arepublicly available at: https://hkust-sail.github.io/ sail-recon/.</description><author>Junyuan Deng, Heng Li, Tao Xie, Weiqiang Ren, Qian Zhang, Ping Tan, Xiaoyang Guo</author><pubDate>Mon, 25 Aug 2025 12:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17972v1</guid></item><item><title>Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding</title><link>http://arxiv.org/abs/2508.17971v1</link><description>The development and application of large language models (LLM) havedemonstrated that foundational models can be utilized to solve a wide array oftasks. However, their performance in multi-agent path finding (MAPF) tasks hasbeen less than satisfactory, with only a few studies exploring this area. MAPFis a complex problem requiring both planning and multi-agent coordination. Toimprove the performance of LLM in MAPF tasks, we propose a novel framework,LLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM forMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trainedgraph neural network-based NAR, and a cross-attention mechanism. This is thefirst work to propose using a neural algorithmic reasoner to integrate GNNswith the map information for MAPF, thereby guiding LLM to achieve superiorperformance. LLM-NAR can be easily adapted to various LLM models. Bothsimulation and real-world experiments demonstrate that our method significantlyoutperforms existing LLM-based approaches in solving MAPF problems.</description><author>Pu Feng, Size Wang, Yuhong Cao, Junkang Liang, Rongye Shi, Wenjun Wu</author><pubDate>Mon, 25 Aug 2025 12:38:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17971v1</guid></item><item><title>A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</title><link>http://arxiv.org/abs/2508.17969v1</link><description>This paper introduces a holistic perception system for internal and externalmonitoring of autonomous vehicles, with the aim of demonstrating a novelAI-leveraged self-adaptive framework of advanced vehicle technologies andsolutions that optimize perception and experience on-board. Internal monitoringsystem relies on a multi-camera setup designed for predicting and identifyingdriver and occupant behavior through facial recognition, exploiting in additiona large language model as virtual assistant. Moreover, the in-cabin monitoringsystem includes AI-empowered smart sensors that measure air-quality and performthermal comfort analysis for efficient on and off-boarding. On the other hand,external monitoring system perceives the surrounding environment of vehicle,through a LiDAR-based cost-efficient semantic segmentation approach, thatperforms highly accurate and efficient super-resolution on low-quality raw 3Dpoint clouds. The holistic perception framework is developed in the context ofEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed ona real electric vehicle provided by ALKE. Experimental validation andevaluation at the integration site of Joint Research Centre at Ispra, Italy,highlights increased performance and efficiency of the modular blocks of theproposed perception architecture.</description><author>Alexandros Gkillas, Christos Anagnostopoulos, Nikos Piperigkos, Dimitris Tsiktsiris, Theofilos Christodoulou, Theofanis Siamatras, Dimitrios Triantafyllou, Christos Basdekis, Theoktisti Marinopoulou, Panagiotis Lepentsiotis, Elefterios Blitsis, Aggeliki Zacharaki, Nearchos Stylianidis, Leonidas Katelaris, Lamberto Salvan, Aris S. Lalos, Christos Laoudias, Antonios Lalas, Konstantinos Votis</author><pubDate>Mon, 25 Aug 2025 12:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17969v1</guid></item><item><title>TuningIQA: Fine-Grained Blind Image Quality Assessment for Livestreaming Camera Tuning</title><link>http://arxiv.org/abs/2508.17965v1</link><description>Livestreaming has become increasingly prevalent in modern visualcommunication, where automatic camera quality tuning is essential fordelivering superior user Quality of Experience (QoE). Such tuning requiresaccurate blind image quality assessment (BIQA) to guide parameter optimizationdecisions. Unfortunately, the existing BIQA models typically only predict anoverall coarse-grained quality score, which cannot provide fine-grainedperceptual guidance for precise camera parameter tuning. To bridge this gap, wefirst establish FGLive-10K, a comprehensive fine-grained BIQA databasecontaining 10,185 high-resolution images captured under varying cameraparameter configurations across diverse livestreaming scenarios. The datasetfeatures 50,925 multi-attribute quality annotations and 19,234 fine-grainedpairwise preference annotations. Based on FGLive-10K, we further developTuningIQA, a fine-grained BIQA metric for livestreaming camera tuning, whichintegrates human-aware feature extraction and graph-based camera parameterfusion. Extensive experiments and comparisons demonstrate that TuningIQAsignificantly outperforms state-of-the-art BIQA methods in both scoreregression and fine-grained quality ranking, achieving superior performancewhen deployed for livestreaming camera tuning.</description><author>Xiangfei Sheng, Zhichao Duan, Xiaofeng Pan, Yipo Huang, Zhichao Yang, Pengfei Chen, Leida Li</author><pubDate>Mon, 25 Aug 2025 12:26:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17965v1</guid></item><item><title>Beam Geometry and Input Dimensionality: Impact on Sparse-Sampling Artifact Correction for Clinical CT with U-Nets</title><link>http://arxiv.org/abs/2508.17961v1</link><description>This study aims to investigate the effect of various beam geometries anddimensions of input data on the sparse-sampling streak artifact correction taskwith U-Nets for clinical CT scans as a means of incorporating the volumetriccontext into artifact reduction tasks to improve model performance. A total of22 subjects were retrospectively selected (01.2016-12.2018) from the TechnicalUniversity of Munich's research hospital, TUM Klinikum rechts der Isar.Sparsely-sampled CT volumes were simulated with the Astra toolbox for parallel,fan, and cone beam geometries. 2048 views were taken as full-view scans. 2D and3D U-Nets were trained and validated on 14, and tested on 8 subjects,respectively. For the dimensionality study, in addition to the 512x512 2D CTimages, the CT scans were further pre-processed to generate a so-called '2.5D',and 3D data: Each CT volume was divided into 64x64x64 voxel blocks. The 3D datarefers to individual 64-voxel blocks. An axial, coronal, and sagittal cutthrough the center of each block resulted in three 64x64 2D patches that wererearranged as a single 64x64x3 image, proposed as 2.5D data. Model performancewas assessed with the mean squared error (MSE) and structural similarity indexmeasure (SSIM). For all geometries, the 2D U-Net trained on axial 2D slicesresults in the best MSE and SSIM values, outperforming the 2.5D and 3D inputdata dimensions.</description><author>Tina Dorosti, Johannes Thalhammer, Sebastian Peterhansl, Daniela Pfeiffer, Franz Pfeiffer, Florian Schaff</author><pubDate>Mon, 25 Aug 2025 12:21:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17961v1</guid></item><item><title>Language Models Coupled with Metacognition Can Outperform Reasoning Models</title><link>http://arxiv.org/abs/2508.17959v1</link><description>Large language models (LLMs) excel in speed and adaptability across variousreasoning tasks, but they often struggle when strict logic or constraintenforcement is required. In contrast, Large Reasoning Models (LRMs) arespecifically designed for complex, step-by-step reasoning, although they comewith significant computational costs and slower inference times. To addressthese trade-offs, we employ and generalize the SOFAI (Slow and Fast AI)cognitive architecture into SOFAI-LM, which coordinates a fast LLM with aslower but more powerful LRM through metacognition. The metacognitive moduleactively monitors the LLM's performance and provides targeted, iterativefeedback with relevant examples. This enables the LLM to progressively refineits solutions without requiring the need for additional model fine-tuning.Extensive experiments on graph coloring and code debugging problems demonstratethat our feedback-driven approach significantly enhances the problem-solvingcapabilities of the LLM. In many instances, it achieves performance levels thatmatch or even exceed those of standalone LRMs while requiring considerably lesstime. Additionally, when the LLM and feedback mechanism alone are insufficient,we engage the LRM by providing appropriate information collected during theLLM's feedback loop, tailored to the specific characteristics of the problemdomain and leads to improved overall performance. Evaluations on twocontrasting domains: graph coloring, requiring globally consistent solutions,and code debugging, demanding localized fixes, demonstrate that SOFAI-LMenables LLMs to match or outperform standalone LRMs in accuracy whilemaintaining significantly lower inference time.</description><author>Vedant Khandelwal, Francesca Rossi, Keerthiram Murugesan, Erik Miehling, Murray Campbell, Karthikeyan Natesan Ramamurthy, Lior Horesh</author><pubDate>Mon, 25 Aug 2025 12:19:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17959v1</guid></item><item><title>Generative Feature Imputing - A Technique for Error-resilient Semantic Communication</title><link>http://arxiv.org/abs/2508.17957v1</link><description>Semantic communication (SemCom) has emerged as a promising paradigm forachieving unprecedented communication efficiency in sixth-generation (6G)networks by leveraging artificial intelligence (AI) to extract and transmit theunderlying meanings of source data. However, deploying SemCom over digitalsystems presents new challenges, particularly in ensuring robustness againsttransmission errors that may distort semantically critical content. To addressthis issue, this paper proposes a novel framework, termed generative featureimputing, which comprises three key techniques. First, we introduce a spatialerror concentration packetization strategy that spatially concentrates featuredistortions by encoding feature elements based on their channel mappings, aproperty crucial for both the effectiveness and reduced complexity of thesubsequent techniques. Second, building on this strategy, we propose agenerative feature imputing method that utilizes a diffusion model toefficiently reconstruct missing features caused by packet losses. Finally, wedevelop a semantic-aware power allocation scheme that enables unequal errorprotection by allocating transmission power according to the semanticimportance of each packet. Experimental results demonstrate that the proposedframework outperforms conventional approaches, such as Deep JointSource-Channel Coding (DJSCC) and JPEG2000, under block fading conditions,achieving higher semantic accuracy and lower Learned Perceptual Image PatchSimilarity (LPIPS) scores.</description><author>Jianhao Huang, Qunsong Zeng, Hongyang Du, Kaibin Huang</author><pubDate>Mon, 25 Aug 2025 12:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17957v1</guid></item><item><title>WetCat: Enabling Automated Skill Assessment in Wet-Lab Cataract Surgery Videos</title><link>http://arxiv.org/abs/2506.08896v2</link><description>To meet the growing demand for systematic surgical training, wetlabenvironments have become indispensable platforms for hands-on practice inophthalmology. Yet, traditional wetlab training depends heavily on manualperformance evaluations, which are labor-intensive, time-consuming, and oftensubject to variability. Recent advances in computer vision offer promisingavenues for automated skill assessment, enhancing both the efficiency andobjectivity of surgical education. Despite notable progress in ophthalmicsurgical datasets, existing resources predominantly focus on real surgeries orisolated tasks, falling short of supporting comprehensive skill evaluation incontrolled wetlab settings. To address these limitations, we introduce WetCat,the first dataset of wetlab cataract surgery videos specifically curated forautomated skill assessment. WetCat comprises high-resolution recordings ofsurgeries performed by trainees on artificial eyes, featuring comprehensivephase annotations and semantic segmentations of key anatomical structures.These annotations are meticulously designed to facilitate skill assessmentduring the critical capsulorhexis and phacoemulsification phases, adhering tostandardized surgical skill assessment frameworks. By focusing on theseessential phases, WetCat enables the development of interpretable, AI-drivenevaluation tools aligned with established clinical metrics. This dataset lays astrong foundation for advancing objective, scalable surgical education and setsa new benchmark for automated workflow analysis and skill assessment inophthalmology training. The dataset and annotations are publicly available inSynapse https://www.synapse.org/Synapse:syn66401174/files.</description><author>Negin Ghamsarian, Raphael Sznitman, Klaus Schoeffmann, Jens Kowal</author><pubDate>Mon, 25 Aug 2025 12:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.08896v2</guid></item><item><title>Choice Outweighs Effort: Facilitating Complementary Knowledge Fusion in Federated Learning via Re-calibration and Merit-discrimination</title><link>http://arxiv.org/abs/2508.17954v1</link><description>Cross-client data heterogeneity in federated learning induces biases thatimpede unbiased consensus condensation and the complementary fusion ofgeneralization- and personalization-oriented knowledge. While existingapproaches mitigate heterogeneity through model decoupling and representationcenter loss, they often rely on static and restricted metrics to evaluate localknowledge and adopt global alignment too rigidly, leading to consensusdistortion and diminished model adaptability. To address these limitations, wepropose FedMate, a method that implements bilateral optimization: On the serverside, we construct a dynamic global prototype, with aggregation weightscalibrated by holistic integration of sample size, current parameters, andfuture prediction; a category-wise classifier is then fine-tuned using thisprototype to preserve global consistency. On the client side, we introducecomplementary classification fusion to enable merit-based discriminationtraining and incorporate cost-aware feature transmission to balance modelperformance and communication efficiency. Experiments on five datasets ofvarying complexity demonstrate that FedMate outperforms state-of-the-artmethods in harmonizing generalization and adaptation. Additionally, semanticsegmentation experiments on autonomous driving datasets validate the method'sreal-world scalability.</description><author>Ming Yang, Dongrun Li, Xin Wang, Xiaoyang Yu, Xiaoming Wu, Shibo He</author><pubDate>Mon, 25 Aug 2025 12:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17954v1</guid></item><item><title>Understanding Subword Compositionality of Large Language Models</title><link>http://arxiv.org/abs/2508.17953v1</link><description>Large language models (LLMs) take sequences of subwords as input, requiringthem to effective compose subword representations into meaningful word-levelrepresentations. In this paper, we present a comprehensive set of experimentsto probe how LLMs compose subword information, focusing on three key aspects:structural similarity, semantic decomposability, and form retention. Ouranalysis of the experiments suggests that these five LLM families can beclassified into three distinct groups, likely reflecting difference in theirunderlying composition strategies. Specifically, we observe (i) three distinctpatterns in the evolution of structural similarity between subword compositionsand whole-word representations across layers; (ii) great performance whenprobing layer by layer their sensitivity to semantic decompositionality; and(iii) three distinct patterns when probing sensitivity to formal features,e.g., character sequence length. These findings provide valuable insights intothe compositional dynamics of LLMs and highlight different compositionalpattens in how LLMs encode and integrate subword information.</description><author>Qiwei Peng, Yekun Chai, Anders Søgaard</author><pubDate>Mon, 25 Aug 2025 12:16:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17953v1</guid></item><item><title>Debiasing Multilingual LLMs in Cross-lingual Latent Space</title><link>http://arxiv.org/abs/2508.17948v1</link><description>Debiasing techniques such as SentDebias aim to reduce bias in large languagemodels (LLMs). Previous studies have evaluated their cross-lingualtransferability by directly applying these methods to LLM representations,revealing their limited effectiveness across languages. In this work, wetherefore propose to perform debiasing in a joint latent space rather thandirectly on LLM representations. We construct a well-aligned cross-linguallatent space using an autoencoder trained on parallel TED talk scripts. Ourexperiments with Aya-expanse and two debiasing techniques across four languages(English, French, German, Dutch) demonstrate that a) autoencoders effectivelyconstruct a well-aligned cross-lingual latent space, and b) applying debiasingtechniques in the learned cross-lingual latent space significantly improvesboth the overall debiasing performance and cross-lingual transferability.</description><author>Qiwei Peng, Guimin Hu, Yekun Chai, Anders Søgaard</author><pubDate>Mon, 25 Aug 2025 12:13:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17948v1</guid></item><item><title>Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation</title><link>http://arxiv.org/abs/2411.16789v2</link><description>Sign language translation (SLT) is a challenging task that involvestranslating sign language images into spoken language. For SLT models toperform this task successfully, they must bridge the modality gap and identifysubtle variations in sign language components to understand their meaningsaccurately. To address these challenges, we propose a novel gloss-free SLTframework called Multimodal Sign Language Translation (MMSLT), which leveragesthe representational capabilities of off-the-shelf multimodal large languagemodels (MLLMs). Specifically, we use MLLMs to generate detailed textualdescriptions of sign language components. Then, through our proposedmultimodal-language pre-training module, we integrate these descriptionfeatures with sign video features to align them within the spoken sentencespace. Our approach achieves state-of-the-art performance on benchmark datasetsPHOENIX14T and CSL-Daily, highlighting the potential of MLLMs to be utilizedeffectively in SLT. Code is available at https://github.com/hwjeon98/MMSLT.</description><author>Jungeun Kim, Hyeongwoo Jeon, Jongseong Bae, Ha Young Kim</author><pubDate>Mon, 25 Aug 2025 12:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16789v2</guid></item><item><title>A Feminist Account of Intersectional Algorithmic Fairness</title><link>http://arxiv.org/abs/2508.17944v1</link><description>Intersectionality has profoundly influenced research and political action byrevealing how interconnected systems of privilege and oppression influencelived experiences, yet its integration into algorithmic fairness researchremains limited. Existing approaches often rely on single-axis or formalsubgroup frameworks that risk oversimplifying social realities and neglectingstructural inequalities. We propose Substantive Intersectional AlgorithmicFairness, extending Green's (2022) notion of substantive algorithmic fairnesswith insights from intersectional feminist theory. Building on this foundation,we introduce ten desiderata within the ROOF methodology to guide the design,assessment, and deployment of algorithmic systems in ways that address systemicinequities while mitigating harms to intersectionally marginalized communities.Rather than prescribing fixed operationalizations, these desiderata encouragereflection on assumptions of neutrality, the use of protected attributes, theinclusion of multiply marginalized groups, and enhancing algorithmic systems'potential. Our approach emphasizes that fairness cannot be separated fromsocial context, and that in some cases, principled non-deployment may benecessary. By bridging computational and social science perspectives, weprovide actionable guidance for more equitable, inclusive, andcontext-sensitive intersectional algorithmic practices.</description><author>Marie Mirsch, Laila Wegner, Jonas Strube, Carmen Leicht-Scholten</author><pubDate>Mon, 25 Aug 2025 12:09:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17944v1</guid></item><item><title>Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields</title><link>http://arxiv.org/abs/2505.02005v2</link><description>Recent NeRF methods on large-scale scenes have underlined the importance ofscene decomposition for scalable NeRFs. Although achieving reasonablescalability, there are several critical problems remaining unexplored, i.e.,learnable decomposition, modeling scene heterogeneity, and modeling efficiency.In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of HashExperts (HMoHE) network that addresses these challenges within a unifiedframework. It is a highly scalable NeRF that learns heterogeneous decompositionand heterogeneous NeRFs efficiently for large-scale scenes in an end-to-endmanner. In our framework, a gating network learns to decompose scenes andallocates 3D points to specialized NeRF experts. This gating network isco-optimized with the experts by our proposed Sparsely Gated Mixture of Experts(MoE) NeRF framework. We incorporate a hash-based gating network and distinctheterogeneous hash experts. The hash-based gating efficiently learns thedecomposition of the large-scale scene. The distinct heterogeneous hash expertsconsist of hash grids of different resolution ranges, enabling effectivelearning of the heterogeneous representation of different scene parts. Thesedesign choices make our framework an end-to-end and highly scalable NeRFsolution for real-world large-scale scene modeling to achieve both quality andefficiency. We evaluate our accuracy and scalability on existing large-scaleNeRF datasets and a new dataset with very large-scale scenes ($&gt;6.5km^2$) fromUrbanBIS. Extensive experiments demonstrate that our approach can be easilyscaled to various large-scale scenes and achieve state-of-the-art scenerendering accuracy. Furthermore, our method exhibits significant efficiency,with an 8x acceleration in training and a 16x acceleration in renderingcompared to Switch-NeRF. Codes will be released athttps://github.com/MiZhenxing/Switch-NeRF.</description><author>Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu</author><pubDate>Mon, 25 Aug 2025 12:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.02005v2</guid></item><item><title>Theory of Mind in Large Language Models: Assessment and Enhancement</title><link>http://arxiv.org/abs/2505.00026v2</link><description>Theory of Mind (ToM)-the ability to reason about the mental states of oneselfand others-is a cornerstone of human social intelligence. As Large LanguageModels (LLMs) become increasingly integrated into daily life, understandingtheir ability to interpret and respond to human mental states is crucial forenabling effective interactions. In this paper, we review LLMs' ToMcapabilities by analyzing both evaluation benchmarks and enhancementstrategies. For evaluation, we focus on recently proposed and widely usedstory-based benchmarks. For enhancement, we provide an in-depth analysis ofrecent methods aimed at improving LLMs' ToM abilities. Furthermore, we outlinepromising directions for future research to further advance these capabilitiesand better adapt LLMs to more realistic and diverse scenarios. Our surveyserves as a valuable resource for researchers interested in evaluating andadvancing LLMs' ToM capabilities.</description><author>Ruirui Chen, Weifeng Jiang, Chengwei Qin, Cheston Tan</author><pubDate>Mon, 25 Aug 2025 12:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.00026v2</guid></item><item><title>See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops</title><link>http://arxiv.org/abs/2508.17932v1</link><description>Human video comprehension demonstrates dynamic coordination between reasoningand visual attention, adaptively focusing on query-relevant details. However,current long-form video question answering systems employ rigid pipelines thatdecouple reasoning from perception, leading to either information loss throughpremature visual abstraction or computational inefficiency through exhaustiveprocessing. The core limitation lies in the inability to adapt visualextraction to specific reasoning requirements, different queries demandfundamentally different visual evidence from the same video content. In thiswork, we present CAVIA, a training-free framework that revolutionizes videounderstanding through reasoning, perception coordination. Unlike conventionalapproaches where visual processing operates independently of reasoning, CAVIAcreates a closed-loop system where reasoning continuously guides visualextraction based on identified information gaps. CAVIA introduces threeinnovations: (1) hierarchical reasoning, guided localization to precise frames;(2) cross-modal semantic bridging for targeted extraction; (3)confidence-driven iterative synthesis. CAVIA achieves state-of-the-artperformance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamicreasoning-perception coordination provides a scalable paradigm for videounderstanding.</description><author>Zixuan Dong, Baoyun Peng, Yufei Wang, Lin Liu, Xinxin Dong, Yunlong Cao, Xiaodong Wang</author><pubDate>Mon, 25 Aug 2025 12:00:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17932v1</guid></item><item><title>Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</title><link>http://arxiv.org/abs/2508.17930v1</link><description>Recently, detection of label errors and improvement of label quality indatasets for supervised learning tasks has become an increasingly importantgoal in both research and industry. The consequences of incorrectly annotateddata include reduced model performance, biased benchmark results, and loweroverall accuracy. Current state-of-the-art label error detection methods oftenfocus on a single computer vision task and, consequently, a specific type ofdataset, containing, for example, either bounding boxes or pixel-wiseannotations. Furthermore, previous methods are not learning-based. In thiswork, we overcome this research gap. We present a unified method for detectinglabel errors in object detection, semantic segmentation, and instancesegmentation datasets. In a nutshell, our approach - learning to detect labelerrors by making them - works as follows: we inject different kinds of labelerrors into the ground truth. Then, the detection of label errors, across allmentioned primary tasks, is framed as an instance segmentation problem based ona composite input. In our experiments, we compare the label error detectionperformance of our method with various baselines and state-of-the-artapproaches of each task's domain on simulated label errors across multipletasks, datasets, and base models. This is complemented by a generalizationstudy on real-world label errors. Additionally, we release 459 real labelerrors identified in the Cityscapes dataset and provide a benchmark for reallabel error detection in Cityscapes.</description><author>Sarina Penquitt, Tobias Riedlinger, Timo Heller, Markus Reischl, Matthias Rottmann</author><pubDate>Mon, 25 Aug 2025 11:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17930v1</guid></item><item><title>FlexTSF: A Flexible Forecasting Model for Time Series with Variable Regularities</title><link>http://arxiv.org/abs/2410.23160v2</link><description>Forecasting time series with irregular temporal structures remainschallenging for universal pre-trained models. Existing approaches often assumeregular sampling or depend heavily on imputation, limiting their applicabilityin real-world scenarios where irregularities are prevalent due to diversesensing devices and recording practices. We introduce FlexTSF, a flexibleforecasting model specifically designed for time series data with variabletemporal regularities. At its foundation lies the IVP Patcher, acontinuous-time patching module leveraging Initial Value Problems (IVPs) toinherently support uneven time intervals, variable sequence lengths, andmissing values. FlexTSF employs a decoder-only architecture that integratesnormalized timestamp inputs and domain-specific statistics through aspecialized causal self-attention mechanism, enabling adaptability acrossdomains. Extensive experiments on 16 datasets demonstrate FlexTSF'seffectiveness, significantly outperforming existing models in classicforecasting scenarios, zero-shot generalization, and low-resource fine-tuningconditions. Ablation studies confirm the contributions of each design componentand the advantage of not relying on predefined fixed patch lengths.</description><author>Jingge Xiao, Yile Chen, Gao Cong, Wolfgang Nejdl, Simon Gottschalk</author><pubDate>Mon, 25 Aug 2025 11:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23160v2</guid></item><item><title>SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features</title><link>http://arxiv.org/abs/2504.20970v2</link><description>Accurate and early diagnosis of pneumonia through X-ray imaging is essentialfor effective treatment and improved patient outcomes. Recent advancements inmachine learning have enabled automated diagnostic tools that assistradiologists in making more reliable and efficient decisions. In this work, wepropose a Singular Value Decomposition-based Least Squares (SVD-LS) frameworkfor multi-class pneumonia classification, leveraging powerful featurerepresentations from state-of-the-art self-supervised and transfer learningmodels. Rather than relying on computationally expensive gradient-basedfine-tuning, we employ a closed-form, non-iterative classification approachthat ensures efficiency without compromising accuracy. Experimental resultsdemonstrate that SVD-LS achieves competitive performance while offeringsignificantly reduced computational costs, making it a viable alternative forreal-time medical imaging applications. The implementation is available at:github.com/meterdogan07/SVD-LS.</description><author>Mete Erdogan, Sebnem Demirtas</author><pubDate>Mon, 25 Aug 2025 11:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.20970v2</guid></item><item><title>AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation</title><link>http://arxiv.org/abs/2508.17926v1</link><description>Argument mining is a subfield of argumentation that aims to automaticallyextract argumentative structures and their relations from natural languagetexts. This paper investigates how a single large language model can beleveraged to perform one or several argument mining tasks. Our contributionsare two-fold. First, we construct a multi-task dataset by surveying andconverting 19 well-known argument mining datasets from the literature into aunified format. Second, we explore various training strategies using Meta AI'sLlama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2)fine-tuning jointly on multiple tasks, and (3) merging models fine-tunedseparately on individual tasks. Our experiments show that task-specificfine-tuning significantly improves individual performance across all tasks.Moreover, multi-task fine-tuning maintains strong performance withoutdegradation, suggesting effective transfer learning across related tasks.Finally, we demonstrate that model merging offers a viable compromise: ityields competitive performance while mitigating the computational costsassociated with full multi-task fine-tuning.</description><author>Henri Savigny, Bruno Yun</author><pubDate>Mon, 25 Aug 2025 11:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17926v1</guid></item><item><title>Rethinking Cross-Subject Data Splitting for Brain-to-Text Decoding</title><link>http://arxiv.org/abs/2312.10987v4</link><description>Recent major milestones have successfully reconstructed natural language fromnon-invasive brain signals (e.g. functional Magnetic Resonance Imaging (fMRI)and Electroencephalogram (EEG)) across subjects. However, we find currentdataset splitting strategies for cross-subject brain-to-text decoding arewrong. Specifically, we first demonstrate that all current splitting methodssuffer from data leakage problem, which refers to the leakage of validation andtest data into training set, resulting in significant overfitting andoverestimation of decoding models. In this study, we develop a rightcross-subject data splitting criterion without data leakage for decoding fMRIand EEG signal to text. Some SOTA brain-to-text decoding models arere-evaluated correctly with the proposed criterion for further research.</description><author>Congchi Yin, Qian Yu, Zhiwei Fang, Changping Peng, Piji Li</author><pubDate>Mon, 25 Aug 2025 11:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10987v4</guid></item><item><title>Pr$^2$R: Information-Fused and Style-Aware Privacy-Preserving Replay for Lifelong Person Re-Identification</title><link>http://arxiv.org/abs/2508.01587v2</link><description>Lifelong person re-identification (LReID) aims to incrementally accumulateknowledge across a sequence of tasks under domain shifts. Recently,replay-based methods have demonstrated strong effectiveness in LReID byrehearsing past samples stored in an auxiliary memory. However, storinghistorical exemplars raises concerns over data privacy. To avoid this,exemplar-free approaches attempt to match the distribution of past data withoutstoring raw samples. Despite being privacy-friendly, these methods often sufferfrom performance degradation due to the forgetting of specific past knowledgerepresentations. To this end, we propose to fuse information from sequentialdata into the pixel space in the replay memory, enabling Privacy-PreservingReplay (Pr$^2$R). More specifically, by distilling the training characteristicsof multiple real images into a single image, the fused samples undergopixel-level changes. This not only protects the privacy of the original databut also makes the replay samples more representative for sequential tasks.During the style replay phase, we align the current domain to the previous onewhile simultaneously adapting the replay samples to match the style of thecurrent domain. This dual-alignment strategy effectively mitigates bothclass-incremental challenges and forgetting caused by domain shifts. Extensiveexperiments on multiple benchmarks show that the proposed method significantlyimproves replay effectiveness while preserving data privacy. Specifically,Pr$^2$R achieves 4% and 6% higher accuracy on sequential tasks compared to thecurrent state-of-the-art and other replay-based methods, respectively.</description><author>Mingyu Wang, Haojie Liu, Zhiyong Li, Wei Jiang</author><pubDate>Mon, 25 Aug 2025 11:48:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.01587v2</guid></item></channel></rss>