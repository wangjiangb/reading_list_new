<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 08 Sep 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ImageBind-LLM: Multi-modality Instruction Tuning</title><link>http://arxiv.org/abs/2309.03905v1</link><description>We present ImageBind-LLM, a multi-modality instruction tuning method of largelanguage models (LLMs) via ImageBind. Existing works mainly focus on languageand image instruction tuning, different from which, our ImageBind-LLM canrespond to multi-modality conditions, including audio, 3D point clouds, video,and their embedding-space arithmetic by only image-text alignment training.During training, we adopt a learnable bind network to align the embedding spacebetween LLaMA and ImageBind's image encoder. Then, the image featurestransformed by the bind network are added to word tokens of all layers inLLaMA, which progressively injects visual instructions via an attention-freeand zero-initialized gating mechanism. Aided by the joint embedding ofImageBind, the simple image-text training enables our model to exhibit superiormulti-modality instruction-following capabilities. During inference, themulti-modality inputs are fed into the corresponding ImageBind encoders, andprocessed by a proposed visual cache model for further cross-modal embeddingenhancement. The training-free cache model retrieves from three million imagefeatures extracted by ImageBind, which effectively mitigates thetraining-inference modality discrepancy. Notably, with our approach,ImageBind-LLM can respond to instructions of diverse modalities and demonstratesignificant language generation quality. Code is released athttps://github.com/OpenGVLab/LLaMA-Adapter.</description><author>Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, Yu Qiao</author><pubDate>Thu, 07 Sep 2023 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03905v1</guid></item><item><title>Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis</title><link>http://arxiv.org/abs/2309.03904v1</link><description>Due to the difficulty in scaling up, generative adversarial networks (GANs)seem to be falling from grace on the task of text-conditioned image synthesis.Sparsely-activated mixture-of-experts (MoE) has recently been demonstrated as avalid solution to training large-scale models with limited computationalresources. Inspired by such a philosophy, we present Aurora, a GAN-basedtext-to-image generator that employs a collection of experts to learn featureprocessing, together with a sparse router to help select the most suitableexpert for each feature point. To faithfully decode the sampling stochasticityand the text condition to the final synthesis, our router adaptively makes itsdecision by taking into account the text-integrated global latent code. At64x64 image resolution, our model trained on LAION2B-en and COYO-700M achieves6.2 zero-shot FID on MS COCO. We release the code and checkpoints to facilitatethe community for further development.</description><author>Jiapeng Zhu, Ceyuan Yang, Kecheng Zheng, Yinghao Xu, Zifan Shi, Yujun Shen</author><pubDate>Thu, 07 Sep 2023 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03904v1</guid></item><item><title>Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction</title><link>http://arxiv.org/abs/2309.03900v1</link><description>Deep learning is commonly used to reconstruct HDR images from LDR images. LDRstack-based methods are used for single-image HDR reconstruction, generating anHDR image from a deep learning-generated LDR stack. However, current methodsgenerate the stack with predetermined exposure values (EVs), which may limitthe quality of HDR reconstruction. To address this, we propose the continuousexposure value representation (CEVR), which uses an implicit function togenerate LDR images with arbitrary EVs, including those unseen during training.Our approach generates a continuous stack with more images containing diverseEVs, significantly improving HDR reconstruction. We use a cycle trainingstrategy to supervise the model in generating continuous EV LDR images withoutcorresponding ground truths. Our CEVR model outperforms existing methods, asdemonstrated by experimental results.</description><author>Su-Kai Chen, Hung-Lin Yen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Wen-Hsiao Peng, Yen-Yu Lin</author><pubDate>Thu, 07 Sep 2023 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03900v1</guid></item><item><title>The Making and Breaking of Camouflage</title><link>http://arxiv.org/abs/2309.03899v1</link><description>Not all camouflages are equally effective, as even a partially visiblecontour or a slight color difference can make the animal stand out and breakits camouflage. In this paper, we address the question of what makes acamouflage successful, by proposing three scores for automatically assessingits effectiveness. In particular, we show that camouflage can be measured bythe similarity between background and foreground features and boundaryvisibility. We use these camouflage scores to assess and compare all availablecamouflage datasets. We also incorporate the proposed camouflage score into agenerative model as an auxiliary loss and show that effective camouflage imagesor videos can be synthesised in a scalable manner. The generated syntheticdataset is used to train a transformer-based model for segmenting camouflagedanimals in videos. Experimentally, we demonstrate state-of-the-art camouflagebreaking performance on the public MoCA-Mask benchmark.</description><author>Hala Lamdouar, Weidi Xie, Andrew Zisserman</author><pubDate>Thu, 07 Sep 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03899v1</guid></item><item><title>ProPainter: Improving Propagation and Transformer for Video Inpainting</title><link>http://arxiv.org/abs/2309.03897v1</link><description>Flow-based propagation and spatiotemporal Transformer are two mainstreammechanisms in video inpainting (VI). Despite the effectiveness of thesecomponents, they still suffer from some limitations that affect theirperformance. Previous propagation-based approaches are performed separatelyeither in the image or feature domain. Global image propagation isolated fromlearning may cause spatial misalignment due to inaccurate optical flow.Moreover, memory or computational constraints limit the temporal range offeature propagation and video Transformer, preventing exploration ofcorrespondence information from distant frames. To address these issues, wepropose an improved framework, called ProPainter, which involves enhancedProPagation and an efficient Transformer. Specifically, we introducedual-domain propagation that combines the advantages of image and featurewarping, exploiting global correspondences reliably. We also propose amask-guided sparse video Transformer, which achieves high efficiency bydiscarding unnecessary and redundant tokens. With these components, ProPainteroutperforms prior arts by a large margin of 1.46 dB in PSNR while maintainingappealing efficiency.</description><author>Shangchen Zhou, Chongyi Li, Kelvin C. K. Chan, Chen Change Loy</author><pubDate>Thu, 07 Sep 2023 18:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03897v1</guid></item><item><title>InstructDiffusion: A Generalist Modeling Interface for Vision Tasks</title><link>http://arxiv.org/abs/2309.03895v1</link><description>We present InstructDiffusion, a unifying and generic framework for aligningcomputer vision tasks with human instructions. Unlike existing approaches thatintegrate prior knowledge and pre-define the output space (e.g., categories andcoordinates) for each vision task, we cast diverse vision tasks into ahuman-intuitive image-manipulating process whose output space is a flexible andinteractive pixel space. Concretely, the model is built upon the diffusionprocess and is trained to predict pixels according to user instructions, suchas encircling the man's left shoulder in red or applying a blue mask to theleft car. InstructDiffusion could handle a variety of vision tasks, includingunderstanding tasks (such as segmentation and keypoint detection) andgenerative tasks (such as editing and enhancement). It even exhibits theability to handle unseen tasks and outperforms prior methods on novel datasets.This represents a significant step towards a generalist modeling interface forvision tasks, advancing artificial general intelligence in the field ofcomputer vision.</description><author>Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, Baining Guo</author><pubDate>Thu, 07 Sep 2023 18:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03895v1</guid></item><item><title>DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection</title><link>http://arxiv.org/abs/2309.03893v1</link><description>Data is the cornerstone of deep learning. This paper reveals that therecently developed Diffusion Model is a scalable data engine for objectdetection. Existing methods for scaling up detection-oriented data oftenrequire manual collection or generative models to obtain target images,followed by data augmentation and labeling to produce training pairs, which arecostly, complex, or lacking diversity. To address these issues, wepresentDiffusionEngine (DE), a data scaling-up engine that provideshigh-quality detection-oriented training pairs in a single stage. DE consistsof a pre-trained diffusion model and an effective Detection-Adapter,contributing to generating scalable, diverse and generalizable detection datain a plug-and-play manner. Detection-Adapter is learned to align the implicitsemantic and location knowledge in off-the-shelf diffusion models withdetection-aware signals to make better bounding-box predictions. Additionally,we contribute two datasets, i.e., COCO-DE and VOC-DE, to scale up existingdetection benchmarks for facilitating follow-up research. Extensive experimentsdemonstrate that data scaling-up via DE can achieve significant improvements indiverse scenarios, such as various detection algorithms, self-supervisedpre-training, data-sparse, label-scarce, cross-domain, and semi-supervisedlearning. For example, when using DE with a DINO-based adapter to scale updata, mAP is improved by 3.1% on COCO, 7.6% on VOC, and 11.5% on Clipart.</description><author>Manlin Zhang, Jie Wu, Yuxi Ren, Ming Li, Jie Qin, Xuefeng Xiao, Wei Liu, Rui Wang, Min Zheng, Andy J. Ma</author><pubDate>Thu, 07 Sep 2023 18:55:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03893v1</guid></item><item><title>ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation</title><link>http://arxiv.org/abs/2309.03891v1</link><description>We present ArtiGrasp, a novel method to synthesize bi-manual hand-objectinteractions that include grasping and articulation. This task is challengingdue to the diversity of the global wrist motions and the precise finger controlthat are necessary to articulate objects. ArtiGrasp leverages reinforcementlearning and physics simulations to train a policy that controls the global andlocal hand pose. Our framework unifies grasping and articulation within asingle policy guided by a single hand pose reference. Moreover, to facilitatethe training of the precise finger control required for articulation, wepresent a learning curriculum with increasing difficulty. It starts withsingle-hand manipulation of stationary objects and continues with multi-agenttraining including both hands and non-stationary objects. To evaluate ourmethod, we introduce Dynamic Object Grasping and Articulation, a task thatinvolves bringing an object into a target articulated pose. This task requiresgrasping, relocation, and articulation. We show our method's efficacy towardsthis task. We further demonstrate that our method can generate motions withnoisy hand-object pose estimates from an off-the-shelf image-based regressor.</description><author>Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo, Jie Song, Otmar Hilliges</author><pubDate>Thu, 07 Sep 2023 18:53:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03891v1</guid></item><item><title>Transformers as Support Vector Machines</title><link>http://arxiv.org/abs/2308.16898v2</link><description>Since its inception in "Attention Is All You Need", transformer architecturehas led to revolutionary advancements in NLP. The attention layer within thetransformer admits a sequence of input tokens $X$ and makes them interactthrough pairwise similarities computed as softmax$(XQK^\top X^\top)$, where$(K,Q)$ are the trainable key-query parameters. In this work, we establish aformal equivalence between the optimization geometry of self-attention and ahard-margin SVM problem that separates optimal input tokens from non-optimaltokens using linear constraints on the outer-products of token pairs. Thisformalism allows us to characterize the implicit bias of 1-layer transformersoptimized with gradient descent: (1) Optimizing the attention layer withvanishing regularization, parameterized by $(K,Q)$, converges in direction toan SVM solution minimizing the nuclear norm of the combined parameter$W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius normobjective. We characterize this convergence, highlighting that it can occurtoward locally-optimal directions rather than global ones. (2) Complementingthis, we prove the local/global directional convergence of gradient descentunder suitable geometric conditions. Importantly, we show thatover-parameterization catalyzes global convergence by ensuring the feasibilityof the SVM problem and by guaranteeing a benign optimization landscape devoidof stationary points. (3) While our theory applies primarily to linearprediction heads, we propose a more general SVM equivalence that predicts theimplicit bias with nonlinear heads. Our findings are applicable to arbitrarydatasets and their validity is verified via experiments. We also introduceseveral open problems and research directions. We believe these findingsinspire the interpretation of transformers as a hierarchy of SVMs thatseparates and selects optimal tokens.</description><author>Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, Samet Oymak</author><pubDate>Thu, 07 Sep 2023 18:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16898v2</guid></item><item><title>A Function Interpretation Benchmark for Evaluating Interpretability Methods</title><link>http://arxiv.org/abs/2309.03886v1</link><description>Labeling neural network submodules with human-legible descriptions is usefulfor many downstream tasks: such descriptions can surface failures, guideinterventions, and perhaps even explain important model behaviors. To date,most mechanistic descriptions of trained networks have involved small models,narrowly delimited phenomena, and large amounts of human labor. Labeling allhuman-interpretable sub-computations in models of increasing size andcomplexity will almost certainly require tools that can generate and validatedescriptions automatically. Recently, techniques that use learned modelsin-the-loop for labeling have begun to gain traction, but methods forevaluating their efficacy are limited and ad-hoc. How should we validate andcompare open-ended labeling tools? This paper introduces FIND (FunctionINterpretation and Description), a benchmark suite for evaluating the buildingblocks of automated interpretability methods. FIND contains functions thatresemble components of trained neural networks, and accompanying descriptionsof the kind we seek to generate. The functions are procedurally constructedacross textual and numeric domains, and involve a range of real-worldcomplexities, including noise, composition, approximation, and bias. Weevaluate new and existing methods that use language models (LMs) to producecode-based and language descriptions of function behavior. We find that anoff-the-shelf LM augmented with only black-box access to functions cansometimes infer their structure, acting as a scientist by forming hypotheses,proposing experiments, and updating descriptions in light of new data. However,LM-based descriptions tend to capture global function behavior and miss localcorruptions. These results show that FIND will be useful for characterizing theperformance of more sophisticated interpretability methods before they areapplied to real-world models.</description><author>Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, Antonio Torralba</author><pubDate>Thu, 07 Sep 2023 18:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03886v1</guid></item><item><title>Zero-Shot Audio Captioning via Audibility Guidance</title><link>http://arxiv.org/abs/2309.03884v1</link><description>The task of audio captioning is similar in essence to tasks such as image andvideo captioning. However, it has received much less attention. We proposethree desiderata for captioning audio -- (i) fluency of the generated text,(ii) faithfulness of the generated text to the input audio, and the somewhatrelated (iii) audibility, which is the quality of being able to be perceivedbased only on audio. Our method is a zero-shot method, i.e., we do not learn toperform captioning. Instead, captioning occurs as an inference process thatinvolves three networks that correspond to the three desired qualities: (i) ALarge Language Model, in our case, for reasons of convenience, GPT-2, (ii) Amodel that provides a matching score between an audio file and a text, forwhich we use a multimodal matching network called ImageBind, and (iii) A textclassifier, trained using a dataset we collected automatically by instructingGPT-4 with prompts designed to direct the generation of both audible andinaudible sentences. We present our results on the AudioCap dataset,demonstrating that audibility guidance significantly enhances performancecompared to the baseline, which lacks this objective.</description><author>Tal Shaharabany, Ariel Shaulov, Lior Wolf</author><pubDate>Thu, 07 Sep 2023 18:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03884v1</guid></item><item><title>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</title><link>http://arxiv.org/abs/2309.03883v1</link><description>Despite their impressive capabilities, large language models (LLMs) are proneto hallucinations, i.e., generating content that deviates from facts seenduring pretraining. We propose a simple decoding strategy for reducinghallucinations with pretrained LLMs that does not require conditioning onretrieved external knowledge nor additional fine-tuning. Our approach obtainsthe next-token distribution by contrasting the differences in logits obtainedfrom projecting the later layers versus earlier layers to the vocabulary space,exploiting the fact that factual knowledge in an LLMs has generally been shownto be localized to particular transformer layers. We find that this Decoding byContrasting Layers (DoLa) approach is able to better surface factual knowledgeand reduce the generation of incorrect facts. DoLa consistently improves thetruthfulness across multiple choices tasks and open-ended generation tasks, forexample improving the performance of LLaMA family models on TruthfulQA by12-17% absolute points, demonstrating its potential in making LLMs reliablygenerate truthful facts.</description><author>Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, Pengcheng He</author><pubDate>Thu, 07 Sep 2023 18:45:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03883v1</guid></item><item><title>On Large Language Models' Selection Bias in Multi-Choice Questions</title><link>http://arxiv.org/abs/2309.03882v1</link><description>Multi-choice questions (MCQs) serve as a common yet important task format inthe research of large language models (LLMs). Our work shows that LLMs exhibitan inherent "selection bias" in MCQs, which refers to LLMs' preferences toselect options located at specific positions (like "Option C"). This bias isprevalent across various LLMs, making their performance vulnerable to optionposition changes in MCQs. We identify that one primary cause resulting inselection bias is option numbering, i.e., the ID symbols A/B/C/D associatedwith the options. To mitigate selection bias, we propose a new method calledPriDe. PriDe first decomposes the observed model prediction distribution intoan intrinsic prediction over option contents and a prior distribution overoption IDs. It then estimates the prior by permutating option contents on asmall number of test samples, which is used to debias the subsequent testsamples. We demonstrate that, as a label-free, inference-time method, PriDeachieves a more effective and computation-efficient debiasing than strongbaselines. We further show that the priors estimated by PriDe generalize wellacross different domains, highlighting its practical potential in broaderscenarios.</description><author>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang</author><pubDate>Thu, 07 Sep 2023 18:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03882v1</guid></item><item><title>Better Practices for Domain Adaptation</title><link>http://arxiv.org/abs/2309.03879v1</link><description>Distribution shifts are all too common in real-world applications of machinelearning. Domain adaptation (DA) aims to address this by providing variousframeworks for adapting models to the deployment data without using labels.However, the domain shift scenario raises a second more subtle challenge: thedifficulty of performing hyperparameter optimisation (HPO) for these adaptationalgorithms without access to a labelled validation set. The unclear validationprotocol for DA has led to bad practices in the literature, such as performingHPO using the target test labels when, in real-world scenarios, they are notavailable. This has resulted in over-optimism about DA research progresscompared to reality. In this paper, we analyse the state of DA when using goodevaluation practice, by benchmarking a suite of candidate validation criteriaand using them to assess popular adaptation algorithms. We show that there arechallenges across all three branches of domain adaptation methodology includingUnsupervised Domain Adaptation (UDA), Source-Free Domain Adaptation (SFDA), andTest Time Adaptation (TTA). While the results show that realisticallyachievable performance is often worse than expected, they also show that usingproper validation splits is beneficial, as well as showing that some previouslyunexplored validation metrics provide the best options to date. Altogether, ourimproved practices covering data, training, validation and hyperparameteroptimisation form a new rigorous pipeline to improve benchmarking, and henceresearch progress, within this important field going forward.</description><author>Linus Ericsson, Da Li, Timothy M. Hospedales</author><pubDate>Thu, 07 Sep 2023 18:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03879v1</guid></item><item><title>Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity</title><link>http://arxiv.org/abs/2111.06781v3</link><description>Reinforcement learning algorithms often require finiteness of state andaction spaces in Markov decision processes (MDPs) (also called controlledMarkov chains) and various efforts have been made in the literature towards theapplicability of such algorithms for continuous state and action spaces. Inthis paper, we show that under very mild regularity conditions (in particular,involving only weak continuity of the transition kernel of an MDP), Q-learningfor standard Borel MDPs via quantization of states and actions (calledQuantized Q-Learning) converges to a limit, and furthermore this limitsatisfies an optimality equation which leads to near optimality with eitherexplicit performance bounds or which are guaranteed to be asymptoticallyoptimal. Our approach builds on (i) viewing quantization as a measurementkernel and thus a quantized MDP as a partially observed Markov decision process(POMDP), (ii) utilizing near optimality and convergence results of Q-learningfor POMDPs, and (iii) finally, near-optimality of finite state modelapproximations for MDPs with weakly continuous kernels which we show tocorrespond to the fixed point of the constructed POMDP. Thus, our paperpresents a very general convergence and approximation result for theapplicability of Q-learning for continuous MDPs.</description><author>Ali Devran Kara, Naci Saldi, Serdar YÃ¼ksel</author><pubDate>Thu, 07 Sep 2023 18:42:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.06781v3</guid></item><item><title>Introducing "Forecast Utterance" for Conversational Data Science</title><link>http://arxiv.org/abs/2309.03877v1</link><description>Envision an intelligent agent capable of assisting users in conductingforecasting tasks through intuitive, natural conversations, without requiringin-depth knowledge of the underlying machine learning (ML) processes. Asignificant challenge for the agent in this endeavor is to accuratelycomprehend the user's prediction goals and, consequently, formulate precise MLtasks. In this paper, we take a pioneering step towards this ambitious goal byintroducing a new concept called Forecast Utterance and then focus on theautomatic and accurate interpretation of users' prediction goals from theseutterances. Specifically, we frame the task as a slot-filling problem, whereeach slot corresponds to a specific aspect of the goal prediction task. We thenemploy two zero-shot methods for solving the slot-filling task, namely: 1)Entity Extraction (EE), and 2) Question-Answering (QA) techniques. Ourexperiments, conducted with three meticulously crafted data sets, validate theviability of our ambitious goal and demonstrate the effectiveness of both EEand QA techniques in interpreting Forecast Utterances.</description><author>Md Mahadi Hassan, Alex Knipper, Shubhra Kanti Karmaker</author><pubDate>Thu, 07 Sep 2023 18:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03877v1</guid></item><item><title>OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs</title><link>http://arxiv.org/abs/2309.03876v1</link><description>Instruction-tuned Large Language Models (LLMs) have recently showcasedremarkable ability to generate fitting responses to natural languageinstructions. However, an open research question concerns the inherent biasesof trained models and their responses. For instance, if the data used to tunean LLM is dominantly written by persons with a specific political bias, wemight expect generated answers to share this bias. Current research work seeksto de-bias such models, or suppress potentially biased answers. With thisdemonstration, we take a different view on biases in instruction-tuning: Ratherthan aiming to suppress them, we aim to make them explicit and transparent. Tothis end, we present OpinionGPT, a web demo in which users can ask questionsand select all biases they wish to investigate. The demo will answer thisquestion using a model fine-tuned on text representing each of the selectedbiases, allowing side-by-side comparison. To train the underlying model, weidentified 11 different biases (political, geographic, gender, age) and derivedan instruction-tuning corpus in which each answer was written by members of oneof these demographics. This paper presents OpinionGPT, illustrates how wetrained the bias-aware model and showcases the web application (available athttps://opiniongpt.informatik.hu-berlin.de).</description><author>Patrick Haller, Ansar Aynetdinov, Alan Akbik</author><pubDate>Thu, 07 Sep 2023 18:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03876v1</guid></item><item><title>Box-based Refinement for Weakly Supervised and Unsupervised Localization Tasks</title><link>http://arxiv.org/abs/2309.03874v1</link><description>It has been established that training a box-based detector network canenhance the localization performance of weakly supervised and unsupervisedmethods. Moreover, we extend this understanding by demonstrating that thesedetectors can be utilized to improve the original network, paving the way forfurther advancements. To accomplish this, we train the detectors on top of thenetwork output instead of the image data and apply suitable lossbackpropagation. Our findings reveal a significant improvement in phrasegrounding for the ``what is where by looking'' task, as well as various methodsof unsupervised object discovery. Our code is available athttps://github.com/eyalgomel/box-based-refinement.</description><author>Eyal Gomel, Tal Shaharabany, Lior Wolf</author><pubDate>Thu, 07 Sep 2023 18:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03874v1</guid></item><item><title>A Tutorial on the Non-Asymptotic Theory of System Identification</title><link>http://arxiv.org/abs/2309.03873v1</link><description>This tutorial serves as an introduction to recently developed non-asymptoticmethods in the theory of -- mainly linear -- system identification. Weemphasize tools we deem particularly useful for a range of problems in thisdomain, such as the covering technique, the Hanson-Wright Inequality and themethod of self-normalized martingales. We then employ these tools to givestreamlined proofs of the performance of various least-squares based estimatorsfor identifying the parameters in autoregressive models. We conclude bysketching out how the ideas presented herein can be extended to certainnonlinear identification problems.</description><author>Ingvar Ziemann, Anastasios Tsiamis, Bruce Lee, Yassir Jedra, Nikolai Matni, George J. Pappas</author><pubDate>Thu, 07 Sep 2023 18:33:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03873v1</guid></item><item><title>Text-to-feature diffusion for audio-visual few-shot learning</title><link>http://arxiv.org/abs/2309.03869v1</link><description>Training deep learning models for video classification from audio-visual datacommonly requires immense amounts of labeled training data collected via acostly process. A challenging and underexplored, yet much cheaper, setup isfew-shot learning from video data. In particular, the inherently multi-modalnature of video data with sound and visual information has not been leveragedextensively for the few-shot video classification task. Therefore, we introducea unified audio-visual few-shot video classification benchmark on threedatasets, i.e. the VGGSound-FSL, UCF-FSL, ActivityNet-FSL datasets, where weadapt and compare ten methods. In addition, we propose AV-DIFF, atext-to-feature diffusion framework, which first fuses the temporal andaudio-visual features via cross-modal attention and then generates multi-modalfeatures for the novel classes. We show that AV-DIFF obtains state-of-the-artperformance on our proposed benchmark for audio-visual (generalised) few-shotlearning. Our benchmark paves the way for effective audio-visual classificationwhen only limited labeled data is available. Code and data are available athttps://github.com/ExplainableML/AVDIFF-GFSL.</description><author>Otniel-Bogdan Mercea, Thomas Hummel, A. Sophia Koepke, Zeynep Akata</author><pubDate>Thu, 07 Sep 2023 18:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03869v1</guid></item><item><title>Max-Margin Token Selection in Attention Mechanism</title><link>http://arxiv.org/abs/2306.13596v3</link><description>Attention mechanism is a central component of the transformer architecturewhich led to the phenomenal success of large language models. However, thetheoretical principles underlying the attention mechanism are poorlyunderstood, especially its nonconvex optimization dynamics. In this work, weexplore the seminal softmax-attention model $f(\boldsymbol{X})=\langle\boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where$\boldsymbol{X}$ is the token sequence and$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are trainable parameters. Weprove that running gradient descent on $\boldsymbol{p}$, or equivalently$\boldsymbol{W}$, converges in direction to a max-margin solution thatseparates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearlyformalizes attention as an optimal token selection mechanism. Remarkably, ourresults are applicable to general data and precisely characterize$\textit{optimality}$ of tokens in terms of the value embeddings$\boldsymbol{Xv}$ and problem geometry. We also provide a broaderregularization path analysis that establishes the margin maximizing nature ofattention even for nonlinear prediction heads. When optimizing $\boldsymbol{v}$and $\boldsymbol{p}$ simultaneously with logistic loss, we identify conditionsunder which the regularization paths directionally converge to their respectivehard-margin SVM solutions where $\boldsymbol{v}$ separates the input featuresbased on their labels. Interestingly, the SVM formulation of $\boldsymbol{p}$is influenced by the support vector geometry of $\boldsymbol{v}$. Finally, weverify our theoretical findings via numerical experiments and provide insights.</description><author>Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, Samet Oymak</author><pubDate>Thu, 07 Sep 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13596v3</guid></item><item><title>Non-inferiority of Deep Learning Acute Ischemic Stroke Segmentation on Non-Contrast CT Compared to Expert Neuroradiologists</title><link>http://arxiv.org/abs/2211.15341v3</link><description>To determine if a convolutional neural network (CNN) deep learning model canaccurately segment acute ischemic changes on non-contrast CT compared toneuroradiologists. Non-contrast CT (NCCT) examinations from 232 acute ischemicstroke patients who were enrolled in the DEFUSE 3 trial were included in thisstudy. Three experienced neuroradiologists independently segmented hypodensitythat reflected the ischemic core on each scan. The neuroradiologist with themost experience (expert A) served as the ground truth for deep learning modeltraining. Two additional neuroradiologists (experts B and C) segmentations wereused for data testing. The 232 studies were randomly split into training andtest sets. The training set was further randomly divided into 5 folds withtraining and validation sets. A 3-dimensional CNN architecture was trained andoptimized to predict the segmentations of expert A from NCCT. The performanceof the model was assessed using a set of volume, overlap, and distance metricsusing non-inferiority thresholds of 20%, 3ml, and 3mm. The optimized modeltrained on expert A was compared to test experts B and C. We used a one-sidedWilcoxon signed-rank test to test for the non-inferiority of the model-expertcompared to the inter-expert agreement. The final model performance for theischemic core segmentation task reached a performance of 0.46+-0.09 SurfaceDice at Tolerance 5mm and 0.47+-0.13 Dice when trained on expert A. Compared tothe two test neuroradiologists the model-expert agreement was non-inferior tothe inter-expert agreement, p &lt; 0.05. The CNN accurately delineates thehypodense ischemic core on NCCT in acute ischemic stroke patients with anaccuracy comparable to neuroradiologists.</description><author>Sophie Ostmeier, Brian Axelrod, Benjamin F. J. Verhaaren, Soren Christensen, Abdelkader Mahammedi, Yongkai Liu, Benjamin Pulli, Li-Jia Li, Greg Zaharchuk, Jeremy J. Heit</author><pubDate>Thu, 07 Sep 2023 18:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15341v3</guid></item><item><title>FLM-101B: An Open LLM and How to Train It with $100K Budget</title><link>http://arxiv.org/abs/2309.03852v1</link><description>Large language models (LLMs) have achieved remarkable success in NLP andmultimodal tasks. Despite these successes, their development faces two mainchallenges: (i) high computational cost; and (ii) difficulty in conducting fairand objective evaluations. LLMs are prohibitively expensive, making it feasiblefor only a few major players to undertake their training, thereby constrainingboth research and application opportunities. This underscores the importance ofcost-effective LLM training. In this paper, we utilize a growth strategy tosignificantly reduce LLM training cost. We demonstrate that an LLM with 101Bparameters and 0.31TB tokens can be trained on a $100K budget. We also adopt asystematic evaluation paradigm for the IQ evaluation of LLMs, in complement toexisting evaluations that focus more on knowledge-oriented abilities. Weintroduce our benchmark including evaluations on important aspects ofintelligence including symbolic mapping, itrule understanding, pattern mining,and anti-interference. Such evaluations minimize the potential impact ofmemorization. Experimental results show that our model FLM-101B, trained with abudget of $100K, achieves comparable performance to powerful and well-knownmodels, eg GPT-3 and GLM-130B, especially in the IQ benchmark evaluations withcontexts unseen in training data. The checkpoint of FLM-101B will beopen-sourced at https://huggingface.co/CofeAI/FLM-101B.</description><author>Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang</author><pubDate>Thu, 07 Sep 2023 18:07:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03852v1</guid></item><item><title>CenTime: Event-Conditional Modelling of Censoring in Survival Analysis</title><link>http://arxiv.org/abs/2309.03851v1</link><description>Survival analysis is a valuable tool for estimating the time until specificevents, such as death or cancer recurrence, based on baseline observations.This is particularly useful in healthcare to prognostically predict clinicallyimportant events based on patient data. However, existing approaches often havelimitations; some focus only on ranking patients by survivability, neglectingto estimate the actual event time, while others treat the problem as aclassification task, ignoring the inherent time-ordered structure of theevents. Furthermore, the effective utilization of censored samples - trainingdata points where the exact event time is unknown - is essential for improvingthe predictive accuracy of the model. In this paper, we introduce CenTime, anovel approach to survival analysis that directly estimates the time to event.Our method features an innovative event-conditional censoring mechanism thatperforms robustly even when uncensored data is scarce. We demonstrate that ourapproach forms a consistent estimator for the event model parameters, even inthe absence of uncensored data. Furthermore, CenTime is easily integrated withdeep learning models with no restrictions on batch size or the number ofuncensored samples. We compare our approach with standard survival analysismethods, including the Cox proportional-hazard model and DeepHit. Our resultsindicate that CenTime offers state-of-the-art performance in predictingtime-to-death while maintaining comparable ranking performance. Ourimplementation is publicly available athttps://github.com/ahmedhshahin/CenTime.</description><author>Ahmed H. Shahin, An Zhao, Alexander C. Whitehead, Daniel C. Alexander, Joseph Jacob, David Barber</author><pubDate>Thu, 07 Sep 2023 18:07:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03851v1</guid></item><item><title>Explanation Shift: How Did the Distribution Shift Impact the Model?</title><link>http://arxiv.org/abs/2303.08081v2</link><description>As input data distributions evolve, the predictive performance of machinelearning models tends to deteriorate. In practice, new input data tend to comewithout target labels. Then, state-of-the-art techniques model input datadistributions or model prediction distributions and try to understand issuesregarding the interactions between learned models and shifting distributions.We suggest a novel approach that models how explanation characteristics shiftwhen affected by distribution shifts. We find that the modeling of explanationshifts can be a better indicator for detecting out-of-distribution modelbehaviour than state-of-the-art techniques. We analyze different types ofdistribution shifts using synthetic examples and real-world data sets. Weprovide an algorithmic method that allows us to inspect the interaction betweendata set features and learned models and compare them to the state-of-the-art.We release our methods in an open-source Python package, as well as the codeused to reproduce our experiments.</description><author>Carlos Mougan, Klaus Broelemann, David Masip, Gjergji Kasneci, Thanassis Thiropanis, Steffen Staab</author><pubDate>Thu, 07 Sep 2023 18:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08081v2</guid></item><item><title>Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples</title><link>http://arxiv.org/abs/2309.03847v1</link><description>We study the problem of estimating mixtures of Gaussians under the constraintof differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4\log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate amixture of $k$ Gaussians up to total variation distance $\alpha$ whilesatisfying $(\varepsilon, \delta)$-DP. This is the first finite samplecomplexity upper bound for the problem that does not make any structuralassumptions on the GMMs. To solve the problem, we devise a new framework which may be useful for othertasks. On a high level, we show that if a class of distributions (such asGaussians) is (1) list decodable and (2) admits a "locally small'' cover[BKSW19] with respect to total variation distance, then the class of itsmixtures is privately learnable. The proof circumvents a known barrierindicating that, unlike Gaussians, GMMs do not admit a locally small cover[AAL21].</description><author>Mohammad Afzali, Hassan Ashtiani, Christopher Liaw</author><pubDate>Thu, 07 Sep 2023 18:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03847v1</guid></item><item><title>A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation</title><link>http://arxiv.org/abs/2309.02539v2</link><description>Cinematic audio source separation is a relatively new subtask of audio sourceseparation, with the aim of extracting the dialogue stem, the music stem, andthe effects stem from their mixture. In this work, we developed a modelgeneralizing the Bandsplit RNN for any complete or overcomplete partitions ofthe frequency axis. Psycho-acoustically motivated frequency scales were used toinform the band definitions which are now defined with redundancy for morereliable feature extraction. A loss function motivated by the signal-to-noiseratio and the sparsity-promoting property of the 1-norm was proposed. Weadditionally exploit the information-sharing property of a common-encoder setupto reduce computational complexity during both training and inference, improveseparation performance for hard-to-generalize classes of sounds, and allowflexibility during inference time with easily detachable decoders. Our bestmodel sets the state of the art on the Divide and Remaster dataset withperformance above the ideal ratio mask for the dialogue stem.</description><author>Karn N. Watcharasupat, Chih-Wei Wu, Yiwei Ding, Iroro Orife, Aaron J. Hipple, Phillip A. Williams, Scott Kramer, Alexander Lerch, William Wolcott</author><pubDate>Thu, 07 Sep 2023 17:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02539v2</guid></item><item><title>Gradient-Based Feature Learning under Structured Data</title><link>http://arxiv.org/abs/2309.03843v1</link><description>Recent works have demonstrated that the sample complexity of gradient-basedlearning of single index models, i.e. functions that depend on a 1-dimensionalprojection of the input data, is governed by their information exponent.However, these results are only concerned with isotropic data, while inpractice the input often contains additional structure which can implicitlyguide the algorithm. In this work, we investigate the effect of a spikedcovariance structure and reveal several interesting phenomena. First, we showthat in the anisotropic setting, the commonly used spherical gradient dynamicsmay fail to recover the true direction, even when the spike is perfectlyaligned with the target direction. Next, we show that appropriate weightnormalization that is reminiscent of batch normalization can alleviate thisissue. Further, by exploiting the alignment between the (spiked) inputcovariance and the target, we obtain improved sample complexity compared to theisotropic case. In particular, under the spiked model with a suitably largespike, the sample complexity of gradient-based training can be made independentof the information exponent while also outperforming lower bounds forrotationally invariant kernel methods.</description><author>Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, Murat A. Erdogdu</author><pubDate>Thu, 07 Sep 2023 17:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03843v1</guid></item><item><title>Early warning via transitions in latent stochastic dynamical systems</title><link>http://arxiv.org/abs/2309.03842v1</link><description>Early warnings for dynamical transitions in complex systems orhigh-dimensional observation data are essential in many real worldapplications, such as gene mutation, brain diseases, natural disasters,financial crises, and engineering reliability. To effectively extract earlywarning signals, we develop a novel approach: the directed anisotropicdiffusion map that captures the latent evolutionary dynamics in low-dimensionalmanifold. Applying the methodology to authentic electroencephalogram (EEG)data, we successfully find the appropriate effective coordinates, and deriveearly warning signals capable of detecting the tipping point during the statetransition. Our method bridges the latent dynamics with the original dataset.The framework is validated to be accurate and effective through numericalexperiments, in terms of density and transition probability. It is shown thatthe second coordinate holds meaningful information for critical transition invarious evaluation metrics.</description><author>Lingyu Feng, Ting Gao, Wang Xiao, Jinqiao Duan</author><pubDate>Thu, 07 Sep 2023 17:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03842v1</guid></item><item><title>Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2309.03839v1</link><description>Adaptive interfaces can help users perform sequential decision-making taskslike robotic teleoperation given noisy, high-dimensional command signals (e.g.,from a brain-computer interface). Recent advances in human-in-the-loop machinelearning enable such systems to improve by interacting with users, but tend tobe limited by the amount of data that they can collect from individual users inpractice. In this paper, we propose a reinforcement learning algorithm toaddress this by training an interface to map raw command signals to actionsusing a combination of offline pre-training and online fine-tuning. To addressthe challenges posed by noisy command signals and sparse rewards, we develop anovel method for representing and inferring the user's long-term intent for agiven trajectory. We primarily evaluate our method's ability to assist userswho can only communicate through noisy, high-dimensional input channels througha user study in which 12 participants performed a simulated navigation task byusing their eye gaze to modulate a 128-dimensional command signal from theirwebcam. The results show that our method enables successful goal navigationmore often than a baseline directional interface, by learning to denoise usercommands signals and provide shared autonomy assistance. We further evaluate ona simulated Sawyer pushing task with eye gaze control, and the Lunar Landergame with simulated user commands, and find that our method improves overbaseline interfaces in these domains as well. Extensive ablation experimentswith simulated user commands empirically motivate each component of our method.</description><author>Jensen Gao, Siddharth Reddy, Glen Berseth, Anca D. Dragan, Sergey Levine</author><pubDate>Thu, 07 Sep 2023 17:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03839v1</guid></item><item><title>Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications</title><link>http://arxiv.org/abs/2309.03837v1</link><description>Multi-task learning (MTL) is a powerful approach in deep learning thatleverages the information from multiple tasks during training to improve modelperformance. In medical imaging, MTL has shown great potential to solve varioustasks. However, existing MTL architectures in medical imaging are limited insharing information across tasks, reducing the potential performanceimprovements of MTL. In this study, we introduce a novel attention-based MTLframework to better leverage inter-task interactions for various tasks frompixel-level to image-level predictions. Specifically, we propose a Cross-TaskAttention Network (CTAN) which utilizes cross-task attention mechanisms toincorporate information by interacting across tasks. We validated CTAN on fourmedical imaging datasets that span different domains and tasks including:radiation treatment planning prediction using planning CT images of twodifferent target cancers (Prostate, OpenKBP); pigmented skin lesionsegmentation and diagnosis using dermatoscopic images (HAM10000); and COVID-19diagnosis and severity prediction using chest CT scans (STOIC). Our studydemonstrates the effectiveness of CTAN in improving the accuracy of medicalimaging tasks. Compared to standard single-task learning (STL), CTANdemonstrated a 4.67% improvement in performance and outperformed both widelyused MTL baselines: hard parameter sharing (HPS) with an average performanceimprovement of 3.22%; and multi-task attention network (MTAN) with a relativedecrease of 5.38%. These findings highlight the significance of our proposedMTL framework in solving medical imaging tasks and its potential to improvetheir accuracy across domains.</description><author>Sangwook Kim, Thomas G. Purdie, Chris McIntosh</author><pubDate>Thu, 07 Sep 2023 17:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03837v1</guid></item><item><title>Learning from Demonstration via Probabilistic Diagrammatic Teaching</title><link>http://arxiv.org/abs/2309.03835v1</link><description>Learning for Demonstration (LfD) enables robots to acquire new skills byimitating expert demonstrations, allowing users to communicate theirinstructions in an intuitive manner. Recent progress in LfD often relies onkinesthetic teaching or teleoperation as the medium for users to specify thedemonstrations. Kinesthetic teaching requires physical handling of the robot,while teleoperation demands proficiency with additional hardware. This paperintroduces an alternative paradigm for LfD called Diagrammatic Teaching.Diagrammatic Teaching aims to teach robots novel skills by prompting the userto sketch out demonstration trajectories on 2D images of the scene, these arethen synthesised as a generative model of motion trajectories in 3D task space.Additionally, we present the Ray-tracing Probabilistic Trajectory Learning(RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varyingprobability densities from the 2D sketches, applies ray-tracing to findcorresponding regions in 3D Cartesian space, and fits a probabilistic model ofmotion trajectories to these regions. New motion trajectories, which mimicthose sketched by the user, can then be generated from the probabilistic model.We empirically validate our framework both in simulation and on real robots,which include a fixed-base manipulator and a quadruped-mounted manipulator.</description><author>Weiming Zhi, Tianyi Zhang, Matthew Johnson-Roberson</author><pubDate>Thu, 07 Sep 2023 17:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03835v1</guid></item><item><title>Auto-SDE: Learning effective reduced dynamics from data-driven stochastic dynamical systems</title><link>http://arxiv.org/abs/2205.04151v2</link><description>Multiscale stochastic dynamical systems have been widely adopted toscientific and engineering problems due to their capability of depictingcomplex phenomena in many real world applications. This work is devoted toinvestigating the effective reduced dynamics for a slow-fast stochasticdynamical system. Given observation data on a short-term period satisfying someunknown slow-fast stochastic system, we propose a novel algorithm including aneural network called Auto-SDE to learn invariant slow manifold. Our approachcaptures the evolutionary nature of a series of time-dependent autoencoderneural networks with the loss constructed from a discretized stochasticdifferential equation. Our algorithm is also proved to be accurate, stable andeffective through numerical experiments under various evaluation metrics.</description><author>Lingyu Feng, Ting Gao, Min Dai, Jinqiao Duan</author><pubDate>Thu, 07 Sep 2023 17:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.04151v2</guid></item><item><title>DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications</title><link>http://arxiv.org/abs/2203.09096v5</link><description>The ability to predict the future trajectory of a patient is a key steptoward the development of therapeutics for complex diseases such as Alzheimer'sdisease (AD). However, most machine learning approaches developed forprediction of disease progression are either single-task or single-modalitymodels, which can not be directly adopted to our setting involving multi-tasklearning with high dimensional images. Moreover, most of those approaches aretrained on a single dataset (i.e. cohort), which can not be generalized toother cohorts. We propose a novel multimodal multi-task deep learning model topredict AD progression by analyzing longitudinal clinical and neuroimaging datafrom multiple cohorts. Our proposed model integrates high dimensional MRIfeatures from a 3D convolutional neural network with other data modalities,including clinical and demographic information, to predict the futuretrajectory of patients. Our model employs an adversarial loss to alleviate thestudy-specific imaging bias, in particular the inter-study domain shifts. Inaddition, a Sharpness-Aware Minimization (SAM) optimization technique isapplied to further improve model generalization. The proposed model is trainedand tested on various datasets in order to evaluate and validate the results.Our results showed that 1) our model yields significant improvement over thebaseline models, and 2) models using extracted neuroimaging features from 3Dconvolutional neural network outperform the same models when applied toMRI-derived volumetric features.</description><author>Somaye Hashemifar, Claudia Iriondo, Evan Casey, Mohsen Hejrati, for Alzheimer's Disease Neuroimaging Initiative</author><pubDate>Thu, 07 Sep 2023 17:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.09096v5</guid></item><item><title>Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models</title><link>http://arxiv.org/abs/2309.03831v1</link><description>Drift in machine learning refers to the phenomenon where the statisticalproperties of data or context, in which the model operates, change over timeleading to a decrease in its performance. Therefore, maintaining a constantmonitoring process for machine learning model performance is crucial in orderto proactively prevent any potential performance regression. However,supervised drift detection methods require human annotation and consequentlylead to a longer time to detect and mitigate the drift. In our proposedunsupervised drift detection method, we follow a two step process. Our firststep involves encoding a sample of production data as the target distribution,and the model training data as the reference distribution. In the second step,we employ a kernel-based statistical test that utilizes the maximum meandiscrepancy (MMD) distance metric to compare the reference and targetdistributions and estimate any potential drift. Our method also identifies thesubset of production data that is the root cause of the drift. The modelsretrained using these identified high drift samples show improved performanceon online customer experience quality metrics.</description><author>Saeed Khaki, Akhouri Abhinav Aditya, Zohar Karnin, Lan Ma, Olivia Pan, Samarth Marudheri Chandrashekar</author><pubDate>Thu, 07 Sep 2023 17:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03831v1</guid></item><item><title>ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation</title><link>http://arxiv.org/abs/2309.03827v1</link><description>High Dynamic Range (HDR) content creation has become an important topic formodern media and entertainment sectors, gaming and Augmented/Virtual Realityindustries. Many methods have been proposed to recreate the HDR counterparts ofinput Low Dynamic Range (LDR) images/videos given a single exposure ormulti-exposure LDRs. The state-of-the-art methods focus primarily on thepreservation of the reconstruction's structural similarity and the pixel-wiseaccuracy. However, these conventional approaches do not emphasize preservingthe artistic intent of the images in terms of human visual perception, which isan essential element in media, entertainment and gaming. In this paper, weattempt to study and fill this gap. We propose an architecture calledArtHDR-Net based on a Convolutional Neural Network that uses multi-exposed LDRfeatures as input. Experimental results show that ArtHDR-Net can achievestate-of-the-art performance in terms of the HDR-VDP-2 score (i.e., meanopinion score index) while reaching competitive performance in terms of PSNRand SSIM.</description><author>Hrishav Bakul Barua, Ganesh Krishnasamy, KokSheik Wong, Kalin Stefanov, Abhinav Dhall</author><pubDate>Thu, 07 Sep 2023 17:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03827v1</guid></item><item><title>Prime and Modulate Learning: Generation of forward models with signed back-propagation and environmental cues</title><link>http://arxiv.org/abs/2309.03825v1</link><description>Deep neural networks employing error back-propagation for learning can sufferfrom exploding and vanishing gradient problems. Numerous solutions have beenproposed such as normalisation techniques or limiting activation functions tolinear rectifying units. In this work we follow a different approach which isparticularly applicable to closed-loop learning of forward models whereback-propagation makes exclusive use of the sign of the error signal to primethe learning, whilst a global relevance signal modulates the rate of learning.This is inspired by the interaction between local plasticity and a globalneuromodulation. For example, whilst driving on an empty road, one can allowfor slow step-wise optimisation of actions, whereas, at a busy junction, anerror must be corrected at once. Hence, the error is the priming signal and theintensity of the experience is a modulating factor in the weight change. Theadvantages of this Prime and Modulate paradigm is twofold: it is free fromnormalisation and it makes use of relevant cues from the environment to enrichthe learning. We present a mathematical derivation of the learning rule inz-space and demonstrate the real-time performance with a robotic platform. Theresults show a significant improvement in the speed of convergence compared tothat of the conventional back-propagation.</description><author>Sama Daryanavard, Bernd Porr</author><pubDate>Thu, 07 Sep 2023 17:34:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03825v1</guid></item><item><title>USE-Evaluator: Performance Metrics for Medical Image Segmentation Models with Uncertain, Small or Empty Reference Annotations</title><link>http://arxiv.org/abs/2209.13008v4</link><description>Performance metrics for medical image segmentation models are used to measurethe agreement between the reference annotation and the predicted segmentation.Usually, overlap metrics, such as the Dice, are used as a metric to evaluatethe performance of these models in order for results to be comparable. However,there is a mismatch between the distributions of cases and difficulty level ofsegmentation tasks in public data sets compared to clinical practice. Commonmetrics fail to measure the impact of this mismatch, especially for clinicaldata sets that include low signal pathologies, a difficult segmentation task,and uncertain, small, or empty reference annotations. This limitation mayresult in ineffective research of machine learning practitioners in designingand optimizing models. Dimensions of evaluating clinical value includeconsideration of the uncertainty of reference annotations, independence fromreference annotation volume size, and evaluation of classification of emptyreference annotations. We study how uncertain, small, and empty referenceannotations influence the value of metrics for medical image segmentation on anin-house data set regardless of the model. We examine metrics behavior on thepredictions of a standard deep learning framework in order to identify metricswith clinical value. We compare to a public benchmark data set (BraTS 2019)with a high-signal pathology and certain, larger, and no empty referenceannotations. We may show machine learning practitioners, how uncertain, small,or empty reference annotations require a rethinking of the evaluation andoptimizing procedures. The evaluation code was released to encourage furtheranalysis of this topic.https://github.com/SophieOstmeier/UncertainSmallEmpty.git</description><author>Sophie Ostmeier, Brian Axelrod, Jeroen Bertels, Fabian Isensee, Maarten G. Lansberg, Soren Christensen, Gregory W. Albers, Li-Jia Li, Jeremy J. Heit</author><pubDate>Thu, 07 Sep 2023 17:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.13008v4</guid></item><item><title>Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors</title><link>http://arxiv.org/abs/2308.09199v2</link><description>It is shown that a class of optical physical unclonable functions (PUFs) canbe learned to arbitrary precision with arbitrarily high probability, even inthe presence of noise, given access to polynomially many challenge-responsepairs and polynomially bounded computational power, under mild assumptionsabout the distributions of the noise and challenge vectors. This extends theresults of Rh\"uramir et al. (2013), who showed a subset of this class of PUFsto be learnable in polynomial time in the absence of noise, under theassumption that the optics of the PUF were either linear or had negligiblenonlinear effects. We derive polynomial bounds for the required number ofsamples and the computational complexity of a linear regression algorithm,based on size parameters of the PUF, the distributions of the challenge andnoise vectors, and the probability and accuracy of the regression algorithm,with a similar analysis to one done by Bootle et al. (2018), who demonstrated alearning attack on a poorly implemented version of the Learning With Errorsproblem.</description><author>Apollo Albright, Boris Gelfand, Michael Dixon</author><pubDate>Thu, 07 Sep 2023 17:33:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09199v2</guid></item><item><title>Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization</title><link>http://arxiv.org/abs/2309.03824v1</link><description>Low Rank Decomposition (LRD) is a model compression technique applied to theweight tensors of deep learning models in order to reduce the number oftrainable parameters and computational complexity. However, due to high numberof new layers added to the architecture after applying LRD, it may not lead toa high training/inference acceleration if the decomposition ranks are not smallenough. The issue is that using small ranks increases the risk of significantaccuracy drop after decomposition. In this paper, we propose two techniques foraccelerating low rank decomposed models without requiring to use small ranksfor decomposition. These methods include rank optimization and sequentialfreezing of decomposed layers. We perform experiments on both convolutional andtransformer-based models. Experiments show that these techniques can improvethe model throughput up to 60% during training and 37% during inference whencombined together while preserving the accuracy close to that of the originalmodels</description><author>Habib Hajimolahoseini, Walid Ahmed, Yang Liu</author><pubDate>Thu, 07 Sep 2023 17:33:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03824v1</guid></item><item><title>Off-policy Evaluation in Doubly Inhomogeneous Environments</title><link>http://arxiv.org/abs/2306.08719v2</link><description>This work aims to study off-policy evaluation (OPE) under scenarios where twokey reinforcement learning (RL) assumptions -- temporal stationarity andindividual homogeneity are both violated. To handle the ``doubleinhomogeneities", we propose a class of latent factor models for the reward andobservation transition functions, under which we develop a general OPEframework that consists of both model-based and model-free approaches. To ourknowledge, this is the first paper that develops statistically sound OPEmethods in offline RL with double inhomogeneities. It contributes to a deeperunderstanding of OPE in environments, where standard RL assumptions are notmet, and provides several practical approaches in these settings. We establishthe theoretical properties of the proposed value estimators and empiricallyshow that our approach outperforms competing methods that ignore eithertemporal nonstationarity or individual heterogeneity. Finally, we illustrateour method on a data set from the Medical Information Mart for Intensive Care.</description><author>Zeyu Bian, Chengchun Shi, Zhengling Qi, Lan Wang</author><pubDate>Thu, 07 Sep 2023 17:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08719v2</guid></item><item><title>Domain Generalization for Mammographic Image Analysis with Contrastive Learning</title><link>http://arxiv.org/abs/2304.10226v5</link><description>The deep learning technique has been shown to be effectively addressedseveral image analysis tasks in the computer-aided diagnosis scheme formammography. The training of an efficacious deep learning model requires largedata with diverse styles and qualities. The diversity of data often comes fromthe use of various scanners of vendors. But, in practice, it is impractical tocollect a sufficient amount of diverse data for training. To this end, a novelcontrastive learning is developed to equip the deep learning models with betterstyle generalization capability. Specifically, the multi-style and multi-viewunsupervised self-learning scheme is carried out to seek robust featureembedding against style diversity as a pretrained model. Afterward, thepretrained network is further fine-tuned to the downstream tasks, e.g., massdetection, matching, BI-RADS rating, and breast density classification. Theproposed method has been evaluated extensively and rigorously with mammogramsfrom various vendor style domains and several public datasets. The experimentalresults suggest that the proposed domain generalization method can effectivelyimprove performance of four mammographic image tasks on the data from both seenand unseen domains, and outperform many state-of-the-art (SOTA) generalizationmethods.</description><author>Zheren Li, Zhiming Cui, Lichi Zhang, Sheng Wang, Chenjin Lei, Xi Ouyang, Dongdong Chen, Xiangyu Zhao, Yajia Gu, Zaiyi Liu, Chunling Liu, Dinggang Shen, Jie-Zhi Cheng</author><pubDate>Thu, 07 Sep 2023 17:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10226v5</guid></item><item><title>Empirical Risk Minimization for Losses without Variance</title><link>http://arxiv.org/abs/2309.03818v1</link><description>This paper considers an empirical risk minimization problem underheavy-tailed settings, where data does not have finite variance, but only has$p$-th moment with $p \in (1,2)$. Instead of using estimation procedure basedon truncated observed data, we choose the optimizer by minimizing the riskvalue. Those risk values can be robustly estimated via using the remarkableCatoni's method (Catoni, 2012). Thanks to the structure of Catoni-typeinfluence functions, we are able to establish excess risk upper bounds viausing generalized generic chaining methods. Moreover, we take computationalissues into consideration. We especially theoretically investigate two types ofoptimization methods, robust gradient descent algorithm and empiricalrisk-based methods. With an extensive numerical study, we find that theoptimizer based on empirical risks via Catoni-style estimation indeed showsbetter performance than other baselines. It indicates that estimation directlybased on truncated data may lead to unsatisfactory results.</description><author>Guanhua Fang, Ping Li, Gennady Samorodnitsky</author><pubDate>Thu, 07 Sep 2023 17:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03818v1</guid></item><item><title>Global Optimization for Cardinality-constrained Minimum Sum-of-Squares Clustering via Semidefinite Programming</title><link>http://arxiv.org/abs/2209.08901v3</link><description>The minimum sum-of-squares clustering (MSSC), or k-means type clustering, hasbeen recently extended to exploit prior knowledge on the cardinality of eachcluster. Such knowledge is used to increase performance as well as solutionquality. In this paper, we propose a global optimization approach based on thebranch-and-cut technique to solve the cardinality-constrained MSSC. For thelower bound routine, we use the semidefinite programming (SDP) relaxationrecently proposed by Rujeerapaiboon et al. [SIAM J. Optim. 29(2), 1211-1239,(2019)]. However, this relaxation can be used in a branch-and-cut method onlyfor small-size instances. Therefore, we derive a new SDP relaxation that scalesbetter with the instance size and the number of clusters. In both cases, westrengthen the bound by adding polyhedral cuts. Benefiting from a tailoredbranching strategy which enforces pairwise constraints, we reduce thecomplexity of the problems arising in the children nodes. For the upper bound,instead, we present a local search procedure that exploits the solution of theSDP relaxation solved at each node. Computational results show that theproposed algorithm globally solves, for the first time, real-world instances ofsize 10 times larger than those solved by state-of-the-art exact methods.</description><author>Veronica Piccialli, Antonio M. Sudoso</author><pubDate>Thu, 07 Sep 2023 17:12:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08901v3</guid></item><item><title>T2IW: Joint Text to Image &amp; Watermark Generation</title><link>http://arxiv.org/abs/2309.03815v1</link><description>Recent developments in text-conditioned image generative models haverevolutionized the production of realistic results. Unfortunately, this hasalso led to an increase in privacy violations and the spread of falseinformation, which requires the need for traceability, privacy protection, andother security measures. However, existing text-to-image paradigms lack thetechnical capabilities to link traceable messages with image generation. Inthis study, we introduce a novel task for the joint generation of text to imageand watermark (T2IW). This T2IW scheme ensures minimal damage to image qualitywhen generating a compound image by forcing the semantic feature and thewatermark signal to be compatible in pixels. Additionally, by utilizingprinciples from Shannon information theory and non-cooperative game theory, weare able to separate the revealed image and the revealed watermark from thecompound image. Furthermore, we strengthen the watermark robustness of ourapproach by subjecting the compound image to various post-processing attacks,with minimal pixel distortion observed in the revealed watermark. Extensiveexperiments have demonstrated remarkable achievements in image quality,watermark invisibility, and watermark robustness, supported by our proposed setof evaluation metrics.</description><author>An-An Liu, Guokai Zhang, Yuting Su, Ning Xu, Yongdong Zhang, Lanjun Wang</author><pubDate>Thu, 07 Sep 2023 17:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03815v1</guid></item><item><title>AnthroNet: Conditional Generation of Humans via Anthropometrics</title><link>http://arxiv.org/abs/2309.03812v1</link><description>We present a novel human body model formulated by an extensive set ofanthropocentric measurements, which is capable of generating a wide range ofhuman body shapes and poses. The proposed model enables direct modeling ofspecific human identities through a deep generative architecture, which canproduce humans in any arbitrary pose. It is the first of its kind to have beentrained end-to-end using only synthetically generated data, which not onlyprovides highly accurate human mesh representations but also allows for preciseanthropometry of the body. Moreover, using a highly diverse animation library,we articulated our synthetic humans' body and hands to maximize the diversityof the learnable priors for model training. Our model was trained on a datasetof $100k$ procedurally-generated posed human meshes and their correspondinganthropometric measurements. Our synthetic data generator can be used togenerate millions of unique human identities and poses for non-commercialacademic research purposes.</description><author>Francesco Picetti, Shrinath Deshpande, Jonathan Leban, Soroosh Shahtalebi, Jay Patel, Peifeng Jing, Chunpu Wang, Charles Metze III, Cameron Sun, Cera Laidlaw, James Warren, Kathy Huynh, River Page, Jonathan Hogins, Adam Crespi, Sujoy Ganguly, Salehe Erfanian Ebadi</author><pubDate>Thu, 07 Sep 2023 17:09:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03812v1</guid></item><item><title>Panoramas from Photons</title><link>http://arxiv.org/abs/2309.03811v1</link><description>Scene reconstruction in the presence of high-speed motion and lowillumination is important in many applications such as augmented and virtualreality, drone navigation, and autonomous robotics. Traditional motionestimation techniques fail in such conditions, suffering from too much blur inthe presence of high-speed motion and strong noise in low-light conditions.Single-photon cameras have recently emerged as a promising technology capableof capturing hundreds of thousands of photon frames per second thanks to theirhigh speed and extreme sensitivity. Unfortunately, traditional computer visiontechniques are not well suited for dealing with the binary-valued photon datacaptured by these cameras because these are corrupted by extreme Poisson noise.Here we present a method capable of estimating extreme scene motion underchallenging conditions, such as low light or high dynamic range, from asequence of high-speed image frames such as those captured by a single-photoncamera. Our method relies on iteratively improving a motion estimate bygrouping and aggregating frames after-the-fact, in a stratified manner. Wedemonstrate the creation of high-quality panoramas under fast motion andextremely low light, and super-resolution results using a custom single-photoncamera prototype. For code and supplemental material see our$\href{https://wisionlab.com/project/panoramas-from-photons/}{\text{projectwebpage}}$.</description><author>Sacha Jungerman, Atul Ingle, Mohit Gupta</author><pubDate>Thu, 07 Sep 2023 17:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03811v1</guid></item><item><title>SimNP: Learning Self-Similarity Priors Between Neural Points</title><link>http://arxiv.org/abs/2309.03809v1</link><description>Existing neural field representations for 3D object reconstruction either (1)utilize object-level representations, but suffer from low-quality details dueto conditioning on a global latent code, or (2) are able to perfectlyreconstruct the observations, but fail to utilize object-level prior knowledgeto infer unobserved regions. We present SimNP, a method to learn category-levelself-similarities, which combines the advantages of both worlds by connectingneural point radiance fields with a category-level self-similarityrepresentation. Our contribution is two-fold. (1) We design the first neuralpoint representation on a category level by utilizing the concept of coherentpoint clouds. The resulting neural point radiance fields store a high level ofdetail for locally supported object regions. (2) We learn how information isshared between neural points in an unconstrained and unsupervised fashion,which allows to derive unobserved regions of an object during thereconstruction process from given observations. We show that SimNP is able tooutperform previous methods in reconstructing symmetric unseen object regions,surpassing methods that build upon category-level or pixel-aligned radiancefields, while providing semantic correspondences between instances</description><author>Christopher Wewer, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen</author><pubDate>Thu, 07 Sep 2023 17:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03809v1</guid></item><item><title>Improved theoretical guarantee for rank aggregation via spectral method</title><link>http://arxiv.org/abs/2309.03808v1</link><description>Given pairwise comparisons between multiple items, how to rank them so thatthe ranking matches the observations? This problem, known as rank aggregation,has found many applications in sports, recommendation systems, and other webapplications. As it is generally NP-hard to find a global ranking thatminimizes the mismatch (known as the Kemeny optimization), we focus on theErd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, eachpairwise comparison is a corrupted copy of the true score difference. Weinvestigate spectral ranking algorithms that are based on unnormalized andnormalized data matrices. The key is to understand their performance inrecovering the underlying scores of each item from the observed data. Thisreduces to deriving an entry-wise perturbation error bound between the topeigenvectors of the unnormalized/normalized data matrix and its populationcounterpart. By using the leave-one-out technique, we provide a sharper$\ell_{\infty}$-norm perturbation bound of the eigenvectors and also derive anerror bound on the maximum displacement for each item, with only $\Omega(n\logn)$ samples. Our theoretical analysis improves upon the state-of-the-artresults in terms of sample complexity, and our numerical experiments confirmthese theoretical findings.</description><author>Ziliang Samuel Zhong, Shuyang Ling</author><pubDate>Thu, 07 Sep 2023 17:01:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03808v1</guid></item><item><title>PGFed: Personalize Each Client's Global Objective for Federated Learning</title><link>http://arxiv.org/abs/2212.01448v2</link><description>Personalized federated learning has received an upsurge of attention due tothe mediocre performance of conventional federated learning (FL) overheterogeneous data. Unlike conventional FL which trains a single globalconsensus model, personalized FL allows different models for different clients.However, existing personalized FL algorithms only implicitly transfer thecollaborative knowledge across the federation by embedding the knowledge intothe aggregated model or regularization. We observed that this implicitknowledge transfer fails to maximize the potential of each client's empiricalrisk toward other clients. Based on our observation, in this work, we proposePersonalized Global Federated Learning (PGFed), a novel personalized FLframework that enables each client to personalize its own global objective byexplicitly and adaptively aggregating the empirical risks of itself and otherclients. To avoid massive (O(N^2)) communication overhead and potential privacyleakage while achieving this, each client's risk is estimated through afirst-order approximation for other clients' adaptive risk aggregation. On topof PGFed, we develop a momentum upgrade, dubbed PGFedMo, to more efficientlyutilize clients' empirical risks. Our extensive experiments on four datasetsunder different federated settings show consistent improvements of PGFed overprevious state-of-the-art methods. The code is publicly available athttps://github.com/ljaiverson/pgfed.</description><author>Jun Luo, Matias Mendieta, Chen Chen, Shandong Wu</author><pubDate>Thu, 07 Sep 2023 17:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01448v2</guid></item><item><title>GraPhSyM: Graph Physical Synthesis Model</title><link>http://arxiv.org/abs/2308.03944v2</link><description>In this work, we introduce GraPhSyM, a Graph Attention Network (GATv2) modelfor fast and accurate estimation of post-physical synthesis circuit delay andarea metrics from pre-physical synthesis circuit netlists. Once trained,GraPhSyM provides accurate visibility of final design metrics to early EDAstages, such as logic synthesis, without running the slow physical synthesisflow, enabling global co-optimization across stages. Additionally, the swiftand precise feedback provided by GraPhSyM is instrumental formachine-learning-based EDA optimization frameworks. Given a gate-level netlistof a circuit represented as a graph, GraPhSyM utilizes graph structure,connectivity, and electrical property features to predict the impact ofphysical synthesis transformations such as buffer insertion and gate sizing.When trained on a dataset of 6000 prefix adder designs synthesized at anaggressive delay target, GraPhSyM can accurately predict the post-synthesisdelay (98.3%) and area (96.1%) metrics of unseen adders with a fast 0.22sinference time. Furthermore, we illustrate the compositionality of GraPhSyM byemploying the model trained on a fixed delay target to accurately anticipatepost-synthesis metrics at a variety of unseen delay targets. Lastly, we reportpromising generalization capabilities of the GraPhSyM model when it isevaluated on circuits different from the adders it was exclusively trained on.The results show the potential for GraPhSyM to serve as a powerful tool foradvanced optimization techniques and as an oracle for EDA machine learningframeworks.</description><author>Ahmed Agiza, Rajarshi Roy, Teodor Dumitru Ene, Saad Godil, Sherief Reda, Bryan Catanzaro</author><pubDate>Thu, 07 Sep 2023 16:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03944v2</guid></item><item><title>From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions</title><link>http://arxiv.org/abs/2308.15225v2</link><description>Over the past decades, cognitive neuroscientists and behavioral economistshave recognized the value of describing the process of decision making indetail and modeling the emergence of decisions over time. For example, the timeit takes to decide can reveal more about an agent's true hidden preferencesthan only the decision itself. Similarly, data that track the ongoing decisionprocess such as eye movements or neural recordings contain critical informationthat can be exploited, even if no decision is made. Here, we argue thatartificial intelligence (AI) research would benefit from a stronger focus oninsights about how decisions emerge over time and incorporate related processdata to improve AI predictions in general and human-AI interactions inparticular. First, we introduce a highly established computational frameworkthat assumes decisions to emerge from the noisy accumulation of evidence, andwe present related empirical work in psychology, neuroscience, and economics.Next, we discuss to what extent current approaches in multi-agent AI do or donot incorporate process data and models of decision making. Finally, we outlinehow a more principled inclusion of the evidence-accumulation framework into thetraining and use of AI can help to improve human-AI interactions in the future.</description><author>Mrugsen Nagsen Gopnarayan, Jaan Aru, Sebastian Gluth</author><pubDate>Thu, 07 Sep 2023 16:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15225v2</guid></item><item><title>Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck</title><link>http://arxiv.org/abs/2309.03800v1</link><description>This work investigates the nuanced algorithm design choices for deep learningin the presence of computational-statistical gaps. We begin by consideringoffline sparse parity learning, a supervised classification problem whichadmits a statistical query lower bound for gradient-based training of amultilayer perceptron. This lower bound can be interpreted as a multi-resourcetradeoff frontier: successful learning can only occur if one is sufficientlyrich (large model), knowledgeable (large dataset), patient (many trainingiterations), or lucky (many random guesses). We show, theoretically andexperimentally, that sparse initialization and increasing network width yieldsignificant improvements in sample efficiency in this setting. Here, widthplays the role of parallel search: it amplifies the probability of finding"lottery ticket" neurons, which learn sparse features more sample-efficiently.Finally, we show that the synthetic sparse parity task can be useful as a proxyfor real problems requiring axis-aligned feature learning. We demonstrateimproved sample efficiency on tabular classification benchmarks by using wide,sparsely-initialized MLP models; these networks sometimes outperform tunedrandom forests.</description><author>Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, Cyril Zhang</author><pubDate>Thu, 07 Sep 2023 16:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03800v1</guid></item><item><title>Copula Representations and Error Surface Projections for the Exclusive Or Problem</title><link>http://arxiv.org/abs/1907.04483v2</link><description>The exclusive or (xor) function is one of the simplest examples thatillustrate why nonlinear feedforward networks are superior to linear regressionfor machine learning applications. We review the xor representation andapproximation problems and discuss their solutions in terms of probabilisticlogic and associative copula functions. After briefly reviewing thespecification of feedforward networks, we compare the dynamics of learned errorsurfaces with different activation functions such as RELU and tanh through aset of colorful three-dimensional charts. The copula representations extend xorfrom Boolean to real values, thereby providing a convenient way to demonstratethe concept of cross-validation on in-sample and out-sample data sets. Ourapproach is pedagogical and is meant to be a machine learning prolegomenon.</description><author>Roy S. Freedman</author><pubDate>Thu, 07 Sep 2023 16:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1907.04483v2</guid></item><item><title>FisheyePP4AV: A privacy-preserving method for autonomous vehicles on fisheye camera images</title><link>http://arxiv.org/abs/2309.03799v1</link><description>In many parts of the world, the use of vast amounts of data collected onpublic roadways for autonomous driving has increased. In order to detect andanonymize pedestrian faces and nearby car license plates in actual road-drivingscenarios, there is an urgent need for effective solutions. As more data iscollected, privacy concerns regarding it increase, including but not limited topedestrian faces and surrounding vehicle license plates. Normal and fisheyecameras are the two common camera types that are typically mounted oncollection vehicles. With complex camera distortion models, fisheye cameraimages were deformed in contrast to regular images. It causes computer visiontasks to perform poorly when using numerous deep learning models. In this work,we pay particular attention to protecting privacy while yet adhering to severallaws for fisheye camera photos taken by driverless vehicles. First, we suggesta framework for extracting face and plate identification knowledge from severalteacher models. Our second suggestion is to transform both the image and thelabel from a regular image to fisheye-like data using a varied and realisticfisheye transformation. Finally, we run a test using the open-source PP4AVdataset. The experimental findings demonstrated that our model outperformedbaseline methods when trained on data from autonomous vehicles, even when thedata were softly labeled. The implementation code is available at our github:https://github.com/khaclinh/FisheyePP4AV.</description><author>Linh Trinh, Bach Ha, Tu Tran</author><pubDate>Thu, 07 Sep 2023 16:51:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03799v1</guid></item><item><title>Conformal Autoregressive Generation: Beam Search with Coverage Guarantees</title><link>http://arxiv.org/abs/2309.03797v1</link><description>We introduce two new extensions to the beam search algorithm based onconformal predictions (CP) to produce sets of sequences with theoreticalcoverage guarantees. The first method is very simple and proposesdynamically-sized subsets of beam search results but, unlike typical CPprocedures, has an upper bound on the achievable guarantee depending on apost-hoc calibration measure. Our second algorithm introduces the conformal setprediction procedure as part of the decoding process, producing a variable beamwidth which adapts to the current uncertainty. While more complex, thisprocedure can achieve coverage guarantees selected a priori. We providemarginal coverage bounds for each method, and evaluate them empirically on aselection of tasks drawing from natural language processing and chemistry.</description><author>Nicolas Deutschmann, Marvin Alberts, MarÃ­a RodrÃ­guez MartÃ­nez</author><pubDate>Thu, 07 Sep 2023 16:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03797v1</guid></item><item><title>Bridging the Gap Between Target Networks and Functional Regularization</title><link>http://arxiv.org/abs/2106.02613v4</link><description>Bootstrapping is behind much of the successes of deep Reinforcement Learning.However, learning the value function via bootstrapping often leads to unstabletraining due to fast-changing target values. Target Networks are employed tostabilize training by using an additional set of lagging parameters to estimatethe target values. Despite the popularity of Target Networks, their effect onthe optimization is still misunderstood. In this work, we show that they act asan implicit regularizer which can be beneficial in some cases, but also havedisadvantages such as being inflexible and can result in instabilities, evenwhen vanilla TD(0) converges. To overcome these issues, we propose an explicitFunctional Regularization alternative that is flexible and a convex regularizerin function space and we theoretically study its convergence. We conduct anexperimental study across a range of environments, discount factors, andoff-policiness data collections to investigate the effectiveness of theregularization induced by Target Networks and Functional Regularization interms of performance, accuracy, and stability. Our findings emphasize thatFunctional Regularization can be used as a drop-in replacement for TargetNetworks and result in performance improvement. Furthermore, adjusting both theregularization weight and the network update period in FunctionalRegularization can result in further performance improvements compared tosolely adjusting the network update period as typically done with TargetNetworks. Our approach also enhances the ability to networks to recoveraccurate $Q$-values.</description><author>Alexandre PichÃ©, Valentin Thomas, Rafael Pardinas, Joseph Marino, Gian Maria Marconi, Christopher Pal, Mohammad Emtiyaz Khan</author><pubDate>Thu, 07 Sep 2023 16:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.02613v4</guid></item><item><title>Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences</title><link>http://arxiv.org/abs/2309.03791v1</link><description>We introduce the $ARMOR_D$ methods as novel approaches to enhancing theadversarial robustness of deep learning models. These methods are based on anew class of optimal-transport-regularized divergences, constructed via aninfimal convolution between an information divergence and an optimal-transport(OT) cost. We use these as tools to enhance adversarial robustness bymaximizing the expected loss over a neighborhood of distributions, a techniqueknown as distributionally robust optimization. Viewed as a tool forconstructing adversarial samples, our method allows samples to be bothtransported, according to the OT cost, and re-weighted, according to theinformation divergence. We demonstrate the effectiveness of our method onmalware detection and image recognition applications and find that, to ourknowledge, it outperforms existing methods at enhancing the robustness againstadversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$against $FGSM$ and $98.18\%$ against $PGD^{40}$ on the MNIST dataset, reducingthe error rate by more than $19.7\%$ and $37.2\%$ respectively compared toprior methods. Similarly, in malware detection, a discrete (binary) datadomain, $ARMOR_D$ improves the robustified accuracy under $rFGSM^{50}$ attackcompared to the previous best-performing adversarial training methods by$37.0\%$ while lowering false negative and false positive rates by $51.1\%$ and$57.53\%$, respectively.</description><author>Jeremiah Birrell, Mohammadreza Ebrahimi</author><pubDate>Thu, 07 Sep 2023 16:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03791v1</guid></item><item><title>USA: Universal Sentiment Analysis Model &amp; Construction of Japanese Sentiment Text Classification and Part of Speech Dataset</title><link>http://arxiv.org/abs/2309.03787v1</link><description>Sentiment analysis is a pivotal task in the domain of natural languageprocessing. It encompasses both text-level sentiment polarity classificationand word-level Part of Speech(POS) sentiment polarity determination. Suchanalysis challenges models to understand text holistically while alsoextracting nuanced information. With the rise of Large Language Models(LLMs),new avenues for sentiment analysis have opened. This paper proposes enhancingperformance by leveraging the Mutual Reinforcement Effect(MRE) betweenindividual words and the overall text. It delves into how word polarityinfluences the overarching sentiment of a passage. To support our research, weannotated four novel Sentiment Text Classification and Part of Speech(SCPOS)datasets, building upon existing sentiment classification datasets.Furthermore, we developed a Universal Sentiment Analysis(USA) model, with a7-billion parameter size. Experimental results revealed that our modelsurpassed the performance of gpt-3.5-turbo across all four datasets,underscoring the significance of MRE in sentiment analysis.</description><author>Chengguang Gan, Qinghao Zhang, Tatsunori Mori</author><pubDate>Thu, 07 Sep 2023 16:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03787v1</guid></item><item><title>Stain-invariant self supervised learning for histopathology image analysis</title><link>http://arxiv.org/abs/2211.07590v2</link><description>We present a self-supervised algorithm for several classification taskswithin hematoxylin and eosin (H&amp;E) stained images of breast cancer. Our methodis robust to stain variations inherent to the histology images acquisitionprocess, which has limited the applicability of automated analysis tools. Weaddress this problem by imposing constraints a learnt latent space whichleverages stain normalization techniques during training. At every iteration,we select an image as a normalization target and generate a version of everyimage in the batch normalized to that target. We minimize the distance betweenthe embeddings that correspond to the same image under different stainingvariations while maximizing the distance between other samples. We show thatour method not only improves robustness to stain variations across multi-centerdata, but also classification performance through extensive experiments onvarious normalization targets and methods. Our method achieves thestate-of-the-art performance on several publicly available breast cancerdatasets ranging from tumor classification (CAMELYON17) and subtyping (BRACS)to HER2 status classification and treatment response prediction.</description><author>Alexandre Tiard, Alex Wong, David Joon Ho, Yangchao Wu, Eliram Nof, Alvin C. Goh, Stefano Soatto, Saad Nadeem</author><pubDate>Thu, 07 Sep 2023 16:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07590v2</guid></item><item><title>CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning</title><link>http://arxiv.org/abs/2309.03779v1</link><description>Small devices are frequently used in IoT and smart-city applications toperform periodic dedicated tasks with soft deadlines. This work focuses ondeveloping methods to derive efficient power-management methods for periodictasks on small devices. We first study the limitations of the existing Linuxbuilt-in methods used in small devices. We illustrate three typicalworkload/system patterns that are challenging to manage with Linux's built-insolutions. We develop a reinforcement-learning-based technique with temporalencoding to derive an effective DVFS governor even with the presence of thethree system patterns. The derived governor uses only one performance counter,the same as the built-in Linux mechanism, and does not require an explicit taskmodel for the workload. We implemented a prototype system on the Nvidia JetsonNano Board and experimented with it with six applications, including twoself-designed and four benchmark applications. Under different deadlineconstraints, our approach can quickly derive a DVFS governor that can adapt toperformance requirements and outperform the built-in Linux approach in energysaving. On Mibench workloads, with performance slack ranging from 0.04 s to 0.4s, the proposed method can save 3% - 11% more energy compared to Ondemand.AudioReg and FaceReg applications tested have 5%- 14% energy-savingimprovement. We have open-sourced the implementation of our in-kernel quantizedneural network engine. The codebase can be found at:https://github.com/coladog/tinyagent.</description><author>Ti Zhou, Man Lin</author><pubDate>Thu, 07 Sep 2023 16:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03779v1</guid></item><item><title>Deep Learning Safety Concerns in Automated Driving Perception</title><link>http://arxiv.org/abs/2309.03774v1</link><description>Recent advances in the field of deep learning and impressive performance ofdeep neural networks (DNNs) for perception have resulted in an increased demandfor their use in automated driving (AD) systems. The safety of such systems isof utmost importance and thus requires to consider the unique properties ofDNNs. In order to achieve safety of AD systems with DNN-based perception componentsin a systematic and comprehensive approach, so-called safety concerns have beenintroduced as a suitable structuring element. On the one hand, the concept ofsafety concerns is -- by design -- well aligned to existing standards relevantfor safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it hasalready inspired several academic publications and upcoming standards on AIsafety such as ISO PAS 8800. While the concept of safety concerns has been previously introduced, thispaper extends and refines it, leveraging feedback from various domain andsafety experts in the field. In particular, this paper introduces an additionalcategorization for a better understanding as well as enabling cross-functionalteams to jointly address the concerns.</description><author>Stephanie Abrecht, Alexander Hirsch, Shervin Raafatnia, Matthias Woehrle</author><pubDate>Thu, 07 Sep 2023 16:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03774v1</guid></item><item><title>Extending Transductive Knowledge Graph Embedding Models for Inductive Logical Relational Inference</title><link>http://arxiv.org/abs/2309.03773v1</link><description>Many downstream inference tasks for knowledge graphs, such as relationprediction, have been handled successfully by knowledge graph embeddingtechniques in the transductive setting. To address the inductive settingwherein new entities are introduced into the knowledge graph at inference time,more recent work opts for models which learn implicit representations of theknowledge graph through a complex function of a network's subgraph structure,often parametrized by graph neural network architectures. These come at thecost of increased parametrization, reduced interpretability and limitedgeneralization to other downstream inference tasks. In this work, we bridge thegap between traditional transductive knowledge graph embedding approaches andmore recent inductive relation prediction models by introducing a generalizedform of harmonic extension which leverages representations learned throughtransductive embedding methods to infer representations of new entitiesintroduced at inference time as in the inductive setting. This harmonicextension technique provides the best such approximation, can be implementedvia an efficient iterative scheme, and can be employed to answer a family ofconjunctive logical queries over the knowledge graph, further expanding thecapabilities of transductive embedding methods. In experiments on a number oflarge-scale knowledge graph embedding benchmarks, we find that this approachfor extending the functionality of transductive knowledge graph embeddingmodels to perform knowledge graph completion and answer logical queries in theinductive setting is competitive with--and in some scenariosoutperforms--several state-of-the-art models derived explicitly for suchinductive tasks.</description><author>Thomas Gebhart, John Cobb</author><pubDate>Thu, 07 Sep 2023 16:24:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03773v1</guid></item><item><title>Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models</title><link>http://arxiv.org/abs/2309.02976v2</link><description>Humans excel at robust bipedal walking in complex natural environments. Ineach step, they adequately tune the interaction of biomechanical muscledynamics and neuronal signals to be robust against uncertainties in groundconditions. However, it is still not fully understood how the nervous systemresolves the musculoskeletal redundancy to solve the multi-objective controlproblem considering stability, robustness, and energy efficiency. In computersimulations, energy minimization has been shown to be a successful optimizationtarget, reproducing natural walking with trajectory optimization orreflex-based control methods. However, these methods focus on particularmotions at a time and the resulting controllers are limited when compensatingfor perturbations. In robotics, reinforcement learning~(RL) methods recentlyachieved highly stable (and efficient) locomotion on quadruped systems, but thegeneration of human-like walking with bipedal biomechanical models has requiredextensive use of expert data sets. This strong reliance on demonstrations oftenresults in brittle policies and limits the application to new behaviors,especially considering the potential variety of movements for high-dimensionalmusculoskeletal models in 3D. Achieving natural locomotion with RL withoutsacrificing its incredible robustness might pave the way for a novel approachto studying human walking in complex natural environments. Videos:https://sites.google.com/view/naturalwalkingrl</description><author>Pierre Schumacher, Thomas Geijtenbeek, Vittorio Caggiano, Vikash Kumar, Syn Schmitt, Georg Martius, Daniel F. B. Haeufle</author><pubDate>Thu, 07 Sep 2023 16:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02976v2</guid></item><item><title>Neural lasso: a unifying approach of lasso and neural networks</title><link>http://arxiv.org/abs/2309.03770v1</link><description>In recent years, there is a growing interest in combining techniquesattributed to the areas of Statistics and Machine Learning in order to obtainthe benefits of both approaches. In this article, the statistical techniquelasso for variable selection is represented through a neural network. It isobserved that, although both the statistical approach and its neural versionhave the same objective function, they differ due to their optimization. Inparticular, the neural version is usually optimized in one-step using a singlevalidation set, while the statistical counterpart uses a two-step optimizationbased on cross-validation. The more elaborated optimization of the statisticalmethod results in more accurate parameter estimation, especially when thetraining set is small. For this reason, a modification of the standard approachfor training neural networks, that mimics the statistical framework, isproposed. During the development of the above modification, a new optimizationalgorithm for identifying the significant variables emerged. Experimentalresults, using synthetic and real data sets, show that this new optimizationalgorithm achieves better performance than any of the three previousoptimization approaches.</description><author>David Delgado, Ernesto Curbelo, Danae Carreras</author><pubDate>Thu, 07 Sep 2023 16:17:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03770v1</guid></item><item><title>$L_{2,1}$-Norm Regularized Quaternion Matrix Completion Using Sparse Representation and Quaternion QR Decomposition</title><link>http://arxiv.org/abs/2309.03764v1</link><description>Color image completion is a challenging problem in computer vision, butrecent research has shown that quaternion representations of color imagesperform well in many areas. These representations consider the entire colorimage and effectively utilize coupling information between the three colorchannels. Consequently, low-rank quaternion matrix completion (LRQMC)algorithms have gained significant attention. We propose a method based onquaternion Qatar Riyal decomposition (QQR) and quaternion $L_{2,1}$-norm calledQLNM-QQR. This new approach reduces computational complexity by avoiding theneed to calculate the QSVD of large quaternion matrices. We also present twoimprovements to the QLNM-QQR method: an enhanced version called IRQLNM-QQR thatuses iteratively reweighted quaternion $L_{2,1}$-norm minimization and a methodcalled QLNM-QQR-SR that integrates sparse regularization. Our experiments onnatural color images and color medical images show that IRQLNM-QQR outperformsQLNM-QQR and that the proposed QLNM-QQR-SR method is superior to severalstate-of-the-art methods.</description><author>Juan Han, Kit Ian Kou, Jifei Miao, Lizhi Liu, Haojiang Li</author><pubDate>Thu, 07 Sep 2023 16:08:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03764v1</guid></item><item><title>dacl1k: Real-World Bridge Damage Dataset Putting Open-Source Data to the Test</title><link>http://arxiv.org/abs/2309.03763v1</link><description>Recognising reinforced concrete defects (RCDs) is a crucial element fordetermining the structural integrity, traffic safety and durability of bridges.However, most of the existing datasets in the RCD domain are derived from asmall number of bridges acquired in specific camera poses, lighting conditionsand with fixed hardware. These limitations question the usability of modelstrained on such open-source data in real-world scenarios. We address thisproblem by testing such models on our "dacl1k" dataset, a highly diverse RCDdataset for multi-label classification based on building inspections including1,474 images. Thereby, we trained the models on different combinations ofopen-source data (meta datasets) which were subsequently evaluated bothextrinsically and intrinsically. During extrinsic evaluation, we report metricson dacl1k and the meta datasets. The performance analysis on dacl1k showspractical usability of the meta data, where the best model shows an Exact MatchRatio of 32%. Additionally, we conduct an intrinsic evaluation by clusteringthe bottleneck features of the best model derived from the extrinsic evaluationin order to find out, if the model has learned distinguishing datasets or theclasses (RCDs) which is the aspired goal. The dacl1k dataset and our trainedmodels will be made publicly available, enabling researchers and practitionersto put their models to the real-world test.</description><author>Johannes Flotzinger, Philipp J. RÃ¶sch, Norbert Oswald, Thomas Braml</author><pubDate>Thu, 07 Sep 2023 16:05:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03763v1</guid></item><item><title>LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models</title><link>http://arxiv.org/abs/2308.16137v3</link><description>In recent years, there have been remarkable advancements in the performanceof Transformer-based Large Language Models (LLMs) across various domains. Asthese LLMs are deployed for increasingly complex tasks, they often face theneed to conduct longer reasoning processes or understand larger contexts. Inthese situations, the length generalization failure of LLMs on long sequencesbecomes more prominent. Most pre-training schemes truncate training sequencesto a fixed length. LLMs often struggle to generate fluent and coherent texts,let alone carry out downstream tasks, after longer contexts, even with relativepositional encoding designed to cope with this problem. Common solutions suchas finetuning on longer corpora often involve daunting hardware and time costsand require careful training process design. To more efficiently leverage thegeneration capacity of existing LLMs, we theoretically and empiricallyinvestigate the main out-of-distribution (OOD) factors contributing to thisproblem. Inspired by this diagnosis, we propose a simple yet effective solutionfor on-the-fly length generalization, LM-Infinite. It involves only a$\Lambda$-shaped attention mask (to avoid excessive attended tokens) and adistance limit (to avoid unseen distances) while requiring no parameter updatesor learning. We find it applicable to a variety of LLMs using relative-positionencoding methods. LM-Infinite is computationally efficient with $O(n)$ time andspace, and demonstrates consistent text generation fluency and quality to aslong as 32k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decodingspeedup. On downstream tasks such as passkey retrieval, it continues to work oninputs much longer than training lengths where vanilla models fail immediately.</description><author>Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang</author><pubDate>Thu, 07 Sep 2023 16:04:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16137v3</guid></item><item><title>M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms</title><link>http://arxiv.org/abs/2309.03759v1</link><description>Early detection of cardiac dysfunction through routine screening is vital fordiagnosing cardiovascular diseases. An important metric of cardiac function isthe left ventricular ejection fraction (EF), where lower EF is associated withcardiomyopathy. Echocardiography is a popular diagnostic tool in cardiology,with ultrasound being a low-cost, real-time, and non-ionizing technology.However, human assessment of echocardiograms for calculating EF istime-consuming and expertise-demanding, raising the need for an automatedapproach. In this work, we propose using the M(otion)-mode of echocardiogramsfor estimating the EF and classifying cardiomyopathy. We generate multipleartificial M-mode images from a single echocardiogram and combine them usingoff-the-shelf model architectures. Additionally, we extend contrastive learning(CL) to cardiac imaging to learn meaningful representations from exploitingstructures in unlabeled data allowing the model to achieve high accuracy, evenwith limited annotations. Our experiments show that the supervised settingconverges with only ten modes and is comparable to the baseline method whilebypassing its cumbersome training process and being computationally much moreefficient. Furthermore, CL using M-mode images is helpful for limited datascenarios, such as having labels for only 200 patients, which is common inmedical applications.</description><author>Ece Ozkan, Thomas M. Sutter, Yurong Hu, Sebastian Balzer, Julia E. Vogt</author><pubDate>Thu, 07 Sep 2023 16:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03759v1</guid></item><item><title>Hybrid of representation learning and reinforcement learning for dynamic and complex robotic motion planning</title><link>http://arxiv.org/abs/2309.03758v1</link><description>Motion planning is the soul of robot decision making. Classical planningalgorithms like graph search and reaction-based algorithms face challenges incases of dense and dynamic obstacles. Deep learning algorithms generatesuboptimal one-step predictions that cause many collisions. Reinforcementlearning algorithms generate optimal or near-optimal time-sequentialpredictions. However, they suffer from slow convergence, suboptimal convergedresults, and overfittings. This paper introduces a hybrid algorithm for roboticmotion planning: long short-term memory (LSTM) pooling and skip connection forattention-based discrete soft actor critic (LSA-DSAC). First, graph network(relational graph) and attention network (attention weight) interpret theenvironmental state for the learning of the discrete soft actor criticalgorithm. The expressive power of attention network outperforms that of graphin our task by difference analysis of these two representation methods.However, attention based DSAC faces the overfitting problem in training.Second, the skip connection method is integrated to attention based DSAC tomitigate overfitting and improve convergence speed. Third, LSTM pooling istaken to replace the sum operator of attention weigh and eliminate overfittingby slightly sacrificing convergence speed at early-stage training. Experimentsshow that LSA-DSAC outperforms the state-of-the-art in training and mostevaluations. The physical robot is also implemented and tested in the realworld.</description><author>Chengmin Zhou, Xin Lu, Jiapeng Dai, Bingding Huang, Xiaoxu Liu, Pasi FrÃ¤nti</author><pubDate>Thu, 07 Sep 2023 16:00:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03758v1</guid></item><item><title>TSGBench: Time Series Generation Benchmark</title><link>http://arxiv.org/abs/2309.03755v1</link><description>Synthetic Time Series Generation (TSG) is crucial in a range of applications,including data augmentation, anomaly detection, and privacy preservation.Although significant strides have been made in this field, existing methodsexhibit three key limitations: (1) They often benchmark against similar modeltypes, constraining a holistic view of performance capabilities. (2) The use ofspecialized synthetic and private datasets introduces biases and hampersgeneralizability. (3) Ambiguous evaluation measures, often tied to customnetworks or downstream tasks, hinder consistent and fair comparison. To overcome these limitations, we introduce \textsf{TSGBench}, the inauguralTSG Benchmark, designed for a unified and comprehensive assessment of TSGmethods. It comprises three modules: (1) a curated collection of publiclyavailable, real-world datasets tailored for TSG, together with a standardizedpreprocessing pipeline; (2) a comprehensive evaluation measures suite includingvanilla measures, new distance-based assessments, and visualization tools; (3)a pioneering generalization test rooted in Domain Adaptation (DA), compatiblewith all methods. We have conducted extensive experiments across ten real-worlddatasets from diverse domains, utilizing ten advanced TSG methods and twelveevaluation measures, all gauged through \textsf{TSGBench}. The resultshighlight its remarkable efficacy and consistency. More importantly,\textsf{TSGBench} delivers a statistical breakdown of method rankings,illuminating performance variations across different datasets and measures, andoffering nuanced insights into the effectiveness of each method.</description><author>Yihao Ang, Qiang Huang, Yifan Bao, Anthony K. H. Tung, Zhiyong Huang</author><pubDate>Thu, 07 Sep 2023 15:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03755v1</guid></item><item><title>Convergence Analysis of Decentralized ASGD</title><link>http://arxiv.org/abs/2309.03754v1</link><description>Over the last decades, Stochastic Gradient Descent (SGD) has been intensivelystudied by the Machine Learning community. Despite its versatility andexcellent performance, the optimization of large models via SGD still is atime-consuming task. To reduce training time, it is common to distribute thetraining process across multiple devices. Recently, it has been shown that theconvergence of asynchronous SGD (ASGD) will always be faster than mini-batchSGD. However, despite these improvements in the theoretical bounds, most ASGDconvergence-rate proofs still rely on a centralized parameter server, which isprone to become a bottleneck when scaling out the gradient computations acrossmany distributed processes. In this paper, we present a novel convergence-rate analysis for decentralizedand asynchronous SGD (DASGD) which does not require partial synchronizationamong nodes nor restrictive network topologies. Specifically, we provide abound of $\mathcal{O}(\sigma\epsilon^{-2}) +\mathcal{O}(QS_{avg}\epsilon^{-3/2}) + \mathcal{O}(S_{avg}\epsilon^{-1})$ forthe convergence rate of DASGD, where $S_{avg}$ is the average staleness betweenmodels, $Q$ is a constant that bounds the norm of the gradients, and $\epsilon$is a (small) error that is allowed within the bound. Furthermore, whengradients are not bounded, we prove the convergence rate of DASGD to be$\mathcal{O}(\sigma\epsilon^{-2}) +\mathcal{O}(\sqrt{\hat{S}_{avg}\hat{S}_{max}}\epsilon^{-1})$, with$\hat{S}_{max}$ and $\hat{S}_{avg}$ representing a loose version of the averageand maximum staleness, respectively. Our convergence proof holds for a fixedstepsize and any non-convex, homogeneous, and L-smooth objective function. Weanticipate that our results will be of high relevance for the adoption of DASGDby a broad community of researchers and developers.</description><author>Mauro DL Tosi, Martin Theobald</author><pubDate>Thu, 07 Sep 2023 15:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03754v1</guid></item><item><title>Medoid Silhouette clustering with automatic cluster number selection</title><link>http://arxiv.org/abs/2309.03751v1</link><description>The evaluation of clustering results is difficult, highly dependent on theevaluated data set and the perspective of the beholder. There are manydifferent clustering quality measures, which try to provide a general measureto validate clustering results. A very popular measure is the Silhouette. Wediscuss the efficient medoid-based variant of the Silhouette, perform atheoretical analysis of its properties, provide two fast versions for thedirect optimization, and discuss the use to choose the optimal number ofclusters. We combine ideas from the original Silhouette with the well-known PAMalgorithm and its latest improvements FasterPAM. One of the versions guaranteesequal results to the original variant and provides a run speedup of $O(k^2)$.In experiments on real data with 30000 samples and $k$=100, we observed a10464$\times$ speedup compared to the original PAMMEDSIL algorithm.Additionally, we provide a variant to choose the optimal number of clustersdirectly.</description><author>Lars Lenssen, Erich Schubert</author><pubDate>Thu, 07 Sep 2023 15:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03751v1</guid></item><item><title>PBP: Path-based Trajectory Prediction for Autonomous Driving</title><link>http://arxiv.org/abs/2309.03750v1</link><description>Trajectory prediction plays a crucial role in the autonomous driving stack byenabling autonomous vehicles to anticipate the motion of surrounding agents.Goal-based prediction models have gained traction in recent years foraddressing the multimodal nature of future trajectories. Goal-based predictionmodels simplify multimodal prediction by first predicting 2D goal locations ofagents and then predicting trajectories conditioned on each goal. However, asingle 2D goal location serves as a weak inductive bias for predicting thewhole trajectory, often leading to poor map compliance, i.e., part of thetrajectory going off-road or breaking traffic rules. In this paper, we improveupon goal-based prediction by proposing the Path-based prediction (PBP)approach. PBP predicts a discrete probability distribution over reference pathsin the HD map using the path features and predicts trajectories in thepath-relative Frenet frame. We applied the PBP trajectory decoder on top of theHiVT scene encoder and report results on the Argoverse dataset. Our experimentsshow that PBP achieves competitive performance on the standard trajectoryprediction metrics, while significantly outperforming state-of-the-artbaselines in terms of map compliance.</description><author>Sepideh Afshar, Nachiket Deo, Akshay Bhagat, Titas Chakraborty, Yunming Shao, Balarama Raju Buddharaju, Adwait Deshpande, Henggang Cui</author><pubDate>Thu, 07 Sep 2023 15:45:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03750v1</guid></item><item><title>Enhancing Pipeline-Based Conversational Agents with Large Language Models</title><link>http://arxiv.org/abs/2309.03748v1</link><description>The latest advancements in AI and deep learning have led to a breakthrough inlarge language model (LLM)-based agents such as GPT-4. However, many commercialconversational agent development tools are pipeline-based and have limitationsin holding a human-like conversation. This paper investigates the capabilitiesof LLMs to enhance pipeline-based conversational agents during two phases: 1)in the design and development phase and 2) during operations. In 1) LLMs canaid in generating training data, extracting entities and synonyms,localization, and persona design. In 2) LLMs can assist in contextualization,intent classification to prevent conversational breakdown and handleout-of-scope questions, auto-correcting utterances, rephrasing responses,formulating disambiguation questions, summarization, and enabling closedquestion-answering capabilities. We conducted informal experiments with GPT-4in the private banking domain to demonstrate the scenarios above with apractical example. Companies may be hesitant to replace their pipeline-basedagents with LLMs entirely due to privacy concerns and the need for deepintegration within their existing ecosystems. A hybrid approach in which LLMs'are integrated into the pipeline-based agents allows them to save time andcosts of building and running agents by capitalizing on the capabilities ofLLMs while retaining the integration and privacy safeguards of their existingsystems.</description><author>Mina Foosherian, Hendrik Purwins, Purna Rathnayake, Touhidul Alam, Rui Teimao, Klaus-Dieter Thoben</author><pubDate>Thu, 07 Sep 2023 15:43:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03748v1</guid></item><item><title>The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties</title><link>http://arxiv.org/abs/2309.03747v1</link><description>In this paper, we adopted a retrospective approach to examine and comparefive existing popular sentence encoders, i.e., Sentence-BERT, UniversalSentence Encoder (USE), LASER, InferSent, and Doc2vec, in terms of theirperformance on downstream tasks versus their capability to capture basicsemantic properties. Initially, we evaluated all five sentence encoders on thepopular SentEval benchmark and found that multiple sentence encoders performquite well on a variety of popular downstream tasks. However, being unable tofind a single winner in all cases, we designed further experiments to gain adeeper understanding of their behavior. Specifically, we proposed four semanticevaluation criteria, i.e., Paraphrasing, Synonym Replacement, AntonymReplacement, and Sentence Jumbling, and evaluated the same five sentenceencoders using these criteria. We found that the Sentence-Bert and USE modelspass the paraphrasing criterion, with SBERT being the superior between the two.LASER dominates in the case of the synonym replacement criterion.Interestingly, all the sentence encoders failed the antonym replacement andjumbling criteria. These results suggest that although these popular sentenceencoders perform quite well on the SentEval benchmark, they still struggle tocapture some basic semantic properties, thus, posing a daunting dilemma in NLPresearch.</description><author>Yash Mahajan, Naman Bansal, Shubhra Kanti Karmaker</author><pubDate>Thu, 07 Sep 2023 15:42:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03747v1</guid></item><item><title>Deep Video Codec Control</title><link>http://arxiv.org/abs/2308.16215v3</link><description>Lossy video compression is commonly used when transmitting and storing videodata. Unified video codecs (e.g., H.264 or H.265) remain the de facto standard,despite the availability of advanced (neural) compression approaches.Transmitting videos in the face of dynamic network bandwidth conditionsrequires video codecs to adapt to vastly different compression strengths. Ratecontrol modules augment the codec's compression such that bandwidth constraintsare satisfied and video distortion is minimized. While, both standard videocodes and their rate control modules are developed to minimize video distortionw.r.t. human quality assessment, preserving the downstream performance of deepvision models is not considered. In this paper, we present the first end-to-endlearnable deep video codec control considering both bandwidth constraints anddownstream vision performance, while not breaking existing standardization. Wedemonstrate for two common vision tasks (semantic segmentation and optical flowestimation) and on two different datasets that our deep codec control betterpreserves downstream performance than using 2-pass average bit rate controlwhile meeting dynamic bandwidth constraints and adhering to standardizations.</description><author>Christoph Reich, Biplob Debnath, Deep Patel, Tim Prangemeier, Srimat Chakradhar</author><pubDate>Thu, 07 Sep 2023 15:41:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16215v3</guid></item><item><title>Automotive Object Detection via Learning Sparse Events by Spiking Neurons</title><link>http://arxiv.org/abs/2307.12900v2</link><description>Event-based sensors, distinguished by their high temporal resolution of 1{\mu}s and a dynamic range of 120 dB, stand out as ideal tools for deploymentin fast-paced settings like vehicles and drones. Traditional object detectiontechniques that utilize Artificial Neural Networks (ANNs) face challenges dueto the sparse and asynchronous nature of the events these sensors capture. Incontrast, Spiking Neural Networks (SNNs) offer a promising alternative,providing a temporal representation that is inherently aligned with event-baseddata. This paper explores the unique membrane potential dynamics of SNNs andtheir ability to modulate sparse events. We introduce an innovativespike-triggered adaptive threshold mechanism designed for stable training.Building on these insights, we present a specialized spiking feature pyramidnetwork (SpikeFPN) optimized for automotive event based object detection.Comprehensive evaluations demonstrate that SpikeFPN surpasses both traditionalSNNs and advanced ANNs enhanced with attention mechanisms. Evidently, SpikeFPNachieves a mean Average Precision (mAP) of 0.477 on the GEN1 AutomotiveDetection (GAD) benchmark dataset, marking a significant increase of 9.7% overthe previous best SNN. Moreover, the efficient design of SpikeFPN ensuresrobust performance while optimizing computational resources, attributed to itsinnate sparse computation capabilities.</description><author>Hu Zhang, Yanchen Li, Luziwei Leng, Kaiwei Che, Qian Liu, Qinghai Guo, Jianxing Liao, Ran Cheng</author><pubDate>Thu, 07 Sep 2023 15:40:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12900v2</guid></item><item><title>Label-efficient Contrastive Learning-based model for nuclei detection and classification in 3D Cardiovascular Immunofluorescent Images</title><link>http://arxiv.org/abs/2309.03744v1</link><description>Recently, deep learning-based methods achieved promising performance innuclei detection and classification applications. However, training deeplearning-based methods requires a large amount of pixel-wise annotated data,which is time-consuming and labor-intensive, especially in 3D images. Analternative approach is to adapt weak-annotation methods, such as labeling eachnucleus with a point, but this method does not extend from 2D histopathologyimages (for which it was originally developed) to 3D immunofluorescent images.The reason is that 3D images contain multiple channels (z-axis) for nuclei anddifferent markers separately, which makes training using point annotationsdifficult. To address this challenge, we propose the Label-efficientContrastive learning-based (LECL) model to detect and classify various types ofnuclei in 3D immunofluorescent images. Previous methods use Maximum IntensityProjection (MIP) to convert immunofluorescent images with multiple slices to 2Dimages, which can cause signals from different z-stacks to falsely appearassociated with each other. To overcome this, we devised an Extended MaximumIntensity Projection (EMIP) approach that addresses issues using MIP.Furthermore, we performed a Supervised Contrastive Learning (SCL) approach forweakly supervised settings. We conducted experiments on cardiovascular datasetsand found that our proposed framework is effective and efficient in detectingand classifying various types of nuclei in 3D immunofluorescent images.</description><author>Nazanin Moradinasab, Rebecca A. Deaton, Laura S. Shankman, Gary K. Owens, Donald E. Brown</author><pubDate>Thu, 07 Sep 2023 15:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03744v1</guid></item><item><title>ClusterFusion: Leveraging Radar Spatial Features for Radar-Camera 3D Object Detection in Autonomous Vehicles</title><link>http://arxiv.org/abs/2309.03734v1</link><description>Thanks to the complementary nature of millimeter wave radar and camera, deeplearning-based radar-camera 3D object detection methods may reliably produceaccurate detections even in low-visibility conditions. This makes thempreferable to use in autonomous vehicles' perception systems, especially as thecombined cost of both sensors is cheaper than the cost of a lidar. Recentradar-camera methods commonly perform feature-level fusion which often involvesprojecting the radar points onto the same plane as the image features andfusing the extracted features from both modalities. While performing fusion onthe image plane is generally simpler and faster, projecting radar points ontothe image plane flattens the depth dimension of the point cloud which mightlead to information loss and makes extracting the spatial features of the pointcloud harder. We proposed ClusterFusion, an architecture that leverages thelocal spatial features of the radar point cloud by clustering the point cloudand performing feature extraction directly on the point cloud clusters beforeprojecting the features onto the image plane. ClusterFusion achieved thestate-of-the-art performance among all radar-monocular camera methods on thetest slice of the nuScenes dataset with 48.7% nuScenes detection score (NDS).We also investigated the performance of different radar feature extractionstrategies on point cloud clusters: a handcrafted strategy, a learning-basedstrategy, and a combination of both, and found that the handcrafted strategyyielded the best performance. The main goal of this work is to explore the useof radar's local spatial and point-wise features by extracting them directlyfrom radar point cloud clusters for a radar-monocular camera 3D objectdetection method that performs cross-modal feature fusion on the image plane.</description><author>Irfan Tito Kurniawan, Bambang Riyanto Trilaksono</author><pubDate>Thu, 07 Sep 2023 15:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03734v1</guid></item><item><title>Adaptive Similarity Bootstrapping for Self-Distillation based Representation Learning</title><link>http://arxiv.org/abs/2303.13606v2</link><description>Most self-supervised methods for representation learning leverage across-view consistency objective i.e., they maximize the representationsimilarity of a given image's augmented views. Recent work NNCLR goes beyondthe cross-view paradigm and uses positive pairs from different images obtainedvia nearest neighbor bootstrapping in a contrastive setting. We empiricallyshow that as opposed to the contrastive learning setting which relies onnegative samples, incorporating nearest neighbor bootstrapping in aself-distillation scheme can lead to a performance drop or even collapse. Wescrutinize the reason for this unexpected behavior and provide a solution. Wepropose to adaptively bootstrap neighbors based on the estimated quality of thelatent space. We report consistent improvements compared to the naivebootstrapping approach and the original baselines. Our approach leads toperformance improvements for various self-distillation method/backbonecombinations and standard downstream tasks. Our code is publicly available athttps://github.com/tileb1/AdaSim.</description><author>Tim Lebailly, Thomas StegmÃ¼ller, Behzad Bozorgtabar, Jean-Philippe Thiran, Tinne Tuytelaars</author><pubDate>Thu, 07 Sep 2023 15:21:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13606v2</guid></item><item><title>Learning continuous-valued treatment effects through representation balancing</title><link>http://arxiv.org/abs/2309.03731v1</link><description>Estimating the effects of treatments with an associated dose on an instance'soutcome, the "dose response", is relevant in a variety of domains, fromhealthcare to business, economics, and beyond. Such effects, also known ascontinuous-valued treatment effects, are typically estimated from observationaldata, which may be subject to dose selection bias. This means that theallocation of doses depends on pre-treatment covariates. Previous studies haveshown that conventional machine learning approaches fail to learn accurateindividual estimates of dose responses under the presence of dose selectionbias. In this work, we propose CBRNet, a causal machine learning approach toestimate an individual dose response from observational data. CBRNet adopts theNeyman-Rubin potential outcome framework and extends the concept of balancedrepresentation learning for overcoming selection bias to continuous-valuedtreatments. Our work is the first to apply representation balancing in acontinuous-valued treatment setting. We evaluate our method on a newly proposedbenchmark. Our experiments demonstrate CBRNet's ability to accurately learntreatment effects under selection bias and competitive performance with respectto other state-of-the-art methods.</description><author>Christopher Bockel-Rickermann, Toon Vanderschueren, Jeroen Berrevoets, Tim Verdonck, Wouter Verbeke</author><pubDate>Thu, 07 Sep 2023 15:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03731v1</guid></item><item><title>A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions</title><link>http://arxiv.org/abs/2309.03730v1</link><description>In lending, where prices are specific to both customers and products, havinga well-functioning personalized pricing policy in place is essential toeffective business making. Typically, such a policy must be derived fromobservational data, which introduces several challenges. While the problem of``endogeneity'' is prominently studied in the established pricing literature,the problem of selection bias (or, more precisely, bid selection bias) is not.We take a step towards understanding the effects of selection bias by posingpricing as a problem of causal inference. Specifically, we consider thereaction of a customer to price a treatment effect. In our experiments, wesimulate varying levels of selection bias on a semi-synthetic dataset onmortgage loan applications in Belgium. We investigate the potential ofparametric and nonparametric methods for the identification of individualbid-response functions. Our results illustrate how conventional methods such aslogistic regression and neural networks suffer adversely from selection bias.In contrast, we implement state-of-the-art methods from causal machine learningand show their capability to overcome selection bias in pricing data.</description><author>Christopher Bockel-Rickermann, Sam Verboven, Tim Verdonck, Wouter Verbeke</author><pubDate>Thu, 07 Sep 2023 15:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03730v1</guid></item><item><title>Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</title><link>http://arxiv.org/abs/2309.03729v1</link><description>Training a generative model with limited number of samples is a challengingtask. Current methods primarily rely on few-shot model adaption to train thenetwork. However, in scenarios where data is extremely limited (less than 10),the generative network tends to overfit and suffers from content degradation.To address these problems, we propose a novel phasic content fusing few-shotdiffusion model with directional distribution consistency loss, which targetsdifferent learning objectives at distinct training stages of the diffusionmodel. Specifically, we design a phasic training strategy with phasic contentfusion to help our model learn content and style information when t is large,and learn local details of target domain when t is small, leading to animprovement in the capture of content, style and local details. Furthermore, weintroduce a novel directional distribution consistency loss that ensures theconsistency between the generated and source distributions more efficiently andstably than the prior methods, preventing our model from overfitting. Finally,we propose a cross-domain structure guidance strategy that enhances structureconsistency during domain adaptation. Theoretical analysis, qualitative andquantitative experiments demonstrate the superiority of our approach infew-shot generative model adaption tasks compared to state-of-the-art methods.The source code is available at:https://github.com/sjtuplayer/few-shot-diffusion.</description><author>Teng Hu, Jiangning Zhang, Liang Liu, Ran Yi, Siqi Kou, Haokun Zhu, Xu Chen, Yabiao Wang, Chengjie Wang, Lizhuang Ma</author><pubDate>Thu, 07 Sep 2023 15:14:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03729v1</guid></item><item><title>Interpretable Visual Question Answering via Reasoning Supervision</title><link>http://arxiv.org/abs/2309.03726v1</link><description>Transformer-based architectures have recently demonstrated remarkableperformance in the Visual Question Answering (VQA) task. However, such modelsare likely to disregard crucial visual cues and often rely on multimodalshortcuts and inherent biases of the language modality to predict the correctanswer, a phenomenon commonly referred to as lack of visual grounding. In thiswork, we alleviate this shortcoming through a novel architecture for visualquestion answering that leverages common sense reasoning as a supervisorysignal. Reasoning supervision takes the form of a textual justification of thecorrect answer, with such annotations being already available on large-scaleVisual Common Sense Reasoning (VCR) datasets. The model's visual attention isguided toward important elements of the scene through a similarity loss thataligns the learned attention distributions guided by the question and thecorrect reasoning. We demonstrate both quantitatively and qualitatively thatthe proposed approach can boost the model's visual perception capability andlead to performance increase, without requiring training on explicit groundingannotations.</description><author>Maria Parelli, Dimitrios Mallis, Markos Diomataris, Vassilis Pitsikalis</author><pubDate>Thu, 07 Sep 2023 15:12:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03726v1</guid></item><item><title>Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders</title><link>http://arxiv.org/abs/2202.09671v4</link><description>Employing a forward diffusion chain to gradually map the data to a noisedistribution, diffusion-based generative models learn how to generate the databy inferring a reverse diffusion chain. However, this approach is slow andcostly because it needs many forward and reverse steps. We propose a faster andcheaper approach that adds noise not until the data become pure random noise,but until they reach a hidden noisy data distribution that we can confidentlylearn. Then, we use fewer reverse steps to generate data by starting from thishidden distribution that is made similar to the noisy data. We reveal that theproposed model can be cast as an adversarial auto-encoder empowered by both thediffusion process and a learnable implicit prior. Experimental results showeven with a significantly smaller number of reverse diffusion steps, theproposed truncated diffusion probabilistic models can provide consistentimprovements over the non-truncated ones in terms of performance in bothunconditional and text-guided image generations.</description><author>Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou</author><pubDate>Thu, 07 Sep 2023 15:08:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.09671v4</guid></item><item><title>A boundary-aware point clustering approach in Euclidean and embedding spaces for roof plane segmentation</title><link>http://arxiv.org/abs/2309.03722v1</link><description>Roof plane segmentation from airborne LiDAR point clouds is an importanttechnology for 3D building model reconstruction. One of the key issues of planesegmentation is how to design powerful features that can exactly distinguishadjacent planar patches. The quality of point feature directly determines theaccuracy of roof plane segmentation. Most of existing approaches usehandcrafted features to extract roof planes. However, the abilities of thesefeatures are relatively low, especially in boundary area. To solve thisproblem, we propose a boundary-aware point clustering approach in Euclidean andembedding spaces constructed by a multi-task deep network for roof planesegmentation. We design a three-branch network to predict semantic labels,point offsets and extract deep embedding features. In the first branch, weclassify the input data as non-roof, boundary and plane points. In the secondbranch, we predict point offsets for shifting each point toward its respectiveinstance center. In the third branch, we constrain that points of the sameplane instance should have the similar embeddings. We aim to ensure that pointsof the same plane instance are close as much as possible in both Euclidean andembedding spaces. However, although deep network has strong featurerepresentative ability, it is still hard to accurately distinguish points nearplane instance boundary. Therefore, we first group plane points into manyclusters in the two spaces, and then we assign the rest boundary points totheir closest clusters to generate final complete roof planes. In this way, wecan effectively reduce the influence of unreliable boundary points. Inaddition, we construct a synthetic dataset and a real dataset to train andevaluate our approach. The experiments results show that the proposed approachsignificantly outperforms the existing state-of-the-art approaches.</description><author>Li Li, Qingqing Li, Guozheng Xu, Pengwei Zhou, Jingmin Tu, Jie Li, Jian Yao</author><pubDate>Thu, 07 Sep 2023 14:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03722v1</guid></item><item><title>A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism</title><link>http://arxiv.org/abs/2309.03720v1</link><description>Forecasting natural gas consumption, considering seasonality and trends, iscrucial in planning its supply and consumption and optimizing the cost ofobtaining it, mainly by industrial entities. However, in times of threats toits supply, it is also a critical element that guarantees the supply of thisraw material to meet individual consumers' needs, ensuring society's energysecurity. This article introduces a novel multistep ahead forecasting ofnatural gas consumption with change point detection integration for modelcollection selection with continual learning capabilities using data streamprocessing. The performance of the forecasting models based on the proposedapproach is evaluated in a complex real-world use case of natural gasconsumption forecasting. We employed Hoeffding tree predictors as forecastingmodels and the Pruned Exact Linear Time (PELT) algorithm for the change pointdetection procedure. The change point detection integration enables selecting adifferent model collection for successive time frames. Thus, three modelcollection selection procedures (with and without an error feedback loop) aredefined and evaluated for forecasting scenarios with various densities ofdetected change points. These models were compared with change point agnosticbaseline approaches. Our experiments show that fewer change points result in alower forecasting error regardless of the model collection selection procedureemployed. Also, simpler model collection selection procedures omittingforecasting error feedback leads to more robust forecasting models suitable forcontinual learning tasks.</description><author>Radek Svoboda, Sebastian Basterrech, JÄdrzej Kozal, Jan PlatoÅ¡, MichaÅ WoÅºniak</author><pubDate>Thu, 07 Sep 2023 14:52:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03720v1</guid></item><item><title>Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework</title><link>http://arxiv.org/abs/2309.02428v2</link><description>The burgeoning growth of public domain data and the increasing complexity ofdeep learning model architectures have underscored the need for more efficientdata representation and analysis techniques. This paper is motivated by thework of Helal (2023) and aims to present a comprehensive overview oftensorization. This transformative approach bridges the gap between theinherently multidimensional nature of data and the simplified 2-dimensionalmatrices commonly used in linear algebra-based machine learning algorithms.This paper explores the steps involved in tensorization, multidimensional datasources, various multiway analysis methods employed, and the benefits of theseapproaches. A small example of Blind Source Separation (BSS) is presentedcomparing 2-dimensional algorithms and a multiway algorithm in Python. Resultsindicate that multiway analysis is more expressive. Contrary to the intuitionof the dimensionality curse, utilising multidimensional datasets in theirnative form and applying multiway analysis methods grounded in multilinearalgebra reveal a profound capacity to capture intricate interrelationshipsamong various dimensions while, surprisingly, reducing the number of modelparameters and accelerating processing. A survey of the multi-away analysismethods and integration with various Deep Neural Networks models is presentedusing case studies in different domains.</description><author>Manal Helal</author><pubDate>Thu, 07 Sep 2023 14:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02428v2</guid></item><item><title>Word segmentation granularity in Korean</title><link>http://arxiv.org/abs/2309.03713v1</link><description>This paper describes word {segmentation} granularity in Korean languageprocessing. From a word separated by blank space, which is termed an eojeol, toa sequence of morphemes in Korean, there are multiple possible levels of wordsegmentation granularity in Korean. For specific language processing and corpusannotation tasks, several different granularity levels have been proposed andutilized, because the agglutinative languages including Korean language have aone-to-one mapping between functional morpheme and syntactic category. Thus, weanalyze these different granularity levels, presenting the examples of Koreanlanguage processing systems for future reference. Interestingly, thegranularity by separating only functional morphemes including case markers andverbal endings, and keeping other suffixes for morphological derivation resultsin the optimal performance for phrase structure parsing. This contradictsprevious best practices for Korean language processing, which has been the defacto standard for various applications that require separating all morphemes.</description><author>Jungyeul Park, Mija Kim</author><pubDate>Thu, 07 Sep 2023 14:42:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03713v1</guid></item><item><title>Generative-based Fusion Mechanism for Multi-Modal Tracking</title><link>http://arxiv.org/abs/2309.01728v2</link><description>Generative models (GMs) have received increasing research interest for theirremarkable capacity to achieve comprehensive understanding. However, theirpotential application in the domain of multi-modal tracking has remainedrelatively unexplored. In this context, we seek to uncover the potential ofharnessing generative techniques to address the critical challenge, informationfusion, in multi-modal tracking. In this paper, we delve into two prominent GMtechniques, namely, Conditional Generative Adversarial Networks (CGANs) andDiffusion Models (DMs). Different from the standard fusion process where thefeatures from each modality are directly fed into the fusion block, wecondition these multi-modal features with random noise in the GM framework,effectively transforming the original training samples into harder instances.This design excels at extracting discriminative clues from the features,enhancing the ultimate tracking performance. To quantitatively gauge theeffectiveness of our approach, we conduct extensive experiments across twomulti-modal tracking tasks, three baseline methods, and three challengingbenchmarks. The experimental results demonstrate that the proposedgenerative-based fusion mechanism achieves state-of-the-art performance,setting new records on LasHeR and RGBD1K.</description><author>Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Xiao-Jun Wu, Josef Kittler</author><pubDate>Thu, 07 Sep 2023 14:40:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01728v2</guid></item><item><title>Evaluating Explanation Methods for Multivariate Time Series Classification</title><link>http://arxiv.org/abs/2308.15223v2</link><description>Multivariate time series classification is an important computational taskarising in applications where data is recorded over time and over multiplechannels. For example, a smartwatch can record the acceleration and orientationof a person's motion, and these signals are recorded as multivariate timeseries. We can classify this data to understand and predict human movement andvarious properties such as fitness levels. In many applications classificationalone is not enough, we often need to classify but also understand what themodel learns (e.g., why was a prediction given, based on what information inthe data). The main focus of this paper is on analysing and evaluatingexplanation methods tailored to Multivariate Time Series Classification (MTSC).We focus on saliency-based explanation methods that can point out the mostrelevant channels and time series points for the classification decision. Weanalyse two popular and accurate multivariate time series classifiers, ROCKETand dResNet, as well as two popular explanation methods, SHAP and dCAM. Westudy these methods on 3 synthetic datasets and 2 real-world datasets andprovide a quantitative and qualitative analysis of the explanations provided.We find that flattening the multivariate datasets by concatenating the channelsworks as well as using multivariate classifiers directly and adaptations ofSHAP for MTSC work quite well. Additionally, we also find that the popularsynthetic datasets we used are not suitable for time series analysis.</description><author>Davide Italo Serramazza, Thu Trang Nguyen, Thach Le Nguyen, Georgiana Ifrim</author><pubDate>Thu, 07 Sep 2023 14:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15223v2</guid></item><item><title>A State Representation for Diminishing Rewards</title><link>http://arxiv.org/abs/2309.03710v1</link><description>A common setting in multitask reinforcement learning (RL) demands that anagent rapidly adapt to various stationary reward functions randomly sampledfrom a fixed distribution. In such situations, the successor representation(SR) is a popular framework which supports rapid policy evaluation bydecoupling a policy's expected discounted, cumulative state occupancies from aspecific reward function. However, in the natural world, sequential tasks arerarely independent, and instead reflect shifting priorities based on theavailability and subjective perception of rewarding stimuli. Reflecting thisdisjunction, in this paper we study the phenomenon of diminishing marginalutility and introduce a novel state representation, the $\lambda$representation ($\lambda$R) which, surprisingly, is required for policyevaluation in this setting and which generalizes the SR as well as severalother state representations from the literature. We establish the $\lambda$R'sformal properties and examine its normative advantages in the context ofmachine learning, as well as its usefulness for studying natural behaviors,particularly foraging.</description><author>Ted Moskovitz, Samo Hromadka, Ahmed Touati, Diana Borsa, Maneesh Sahani</author><pubDate>Thu, 07 Sep 2023 14:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03710v1</guid></item><item><title>Chat Failures and Troubles: Reasons and Solutions</title><link>http://arxiv.org/abs/2309.03708v1</link><description>This paper examines some common problems in Human-Robot Interaction (HRI)causing failures and troubles in Chat. A given use case's design decisionsstart with the suitable robot, the suitable chatting model, identifying commonproblems that cause failures, identifying potential solutions, and planningcontinuous improvement. In conclusion, it is recommended to use a closed-loopcontrol algorithm that guides the use of trained Artificial Intelligence (AI)pre-trained models and provides vocabulary filtering, re-train batched modelson new datasets, learn online from data streams, and/or use reinforcementlearning models to self-update the trained models and reduce errors.</description><author>Manal Helal, Patrick Holthaus, Gabriella Lakatos, Farshid Amirabdollahian</author><pubDate>Thu, 07 Sep 2023 14:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03708v1</guid></item><item><title>A Probabilistic Semi-Supervised Approach with Triplet Markov Chains</title><link>http://arxiv.org/abs/2309.03707v1</link><description>Triplet Markov chains are general generative models for sequential data whichtake into account three kinds of random variables: (noisy) observations, theirassociated discrete labels and latent variables which aim at strengthening thedistribution of the observations and their associated labels. However, inpractice, we do not have at our disposal all the labels associated to theobservations to estimate the parameters of such models. In this paper, wepropose a general framework based on a variational Bayesian inference to trainparameterized triplet Markov chain models in a semi-supervised context. Thegenerality of our approach enables us to derive semi-supervised algorithms fora variety of generative models for sequential Bayesian classification.</description><author>Katherine Morales, Yohan Petetin</author><pubDate>Thu, 07 Sep 2023 14:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03707v1</guid></item><item><title>DiffDefense: Defending against Adversarial Attacks via Diffusion Models</title><link>http://arxiv.org/abs/2309.03702v1</link><description>This paper presents a novel reconstruction method that leverages DiffusionModels to protect machine learning classifiers against adversarial attacks, allwithout requiring any modifications to the classifiers themselves. Thesusceptibility of machine learning models to minor input perturbations rendersthem vulnerable to adversarial attacks. While diffusion-based methods aretypically disregarded for adversarial defense due to their slow reverseprocess, this paper demonstrates that our proposed method offers robustnessagainst adversarial threats while preserving clean accuracy, speed, andplug-and-play compatibility. Code at:https://github.com/HondamunigePrasannaSilva/DiffDefence.</description><author>Hondamunige Prasanna Silva, Lorenzo Seidenari, Alberto Del Bimbo</author><pubDate>Thu, 07 Sep 2023 14:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03702v1</guid></item><item><title>Graph Fairing Convolutional Networks for Anomaly Detection</title><link>http://arxiv.org/abs/2010.10274v2</link><description>Graph convolution is a fundamental building block for many deep neuralnetworks on graph-structured data. In this paper, we introduce a simple, yetvery effective graph convolutional network with skip connections forsemi-supervised anomaly detection. The proposed layerwise propagation rule ofour model is theoretically motivated by the concept of implicit fairing ingeometry processing, and comprises a graph convolution module for aggregatinginformation from immediate node neighbors and a skip connection module forcombining layer-wise neighborhood representations. This propagation rule isderived from the iterative solution of the implicit fairing equation via theJacobi method. In addition to capturing information from distant graph nodesthrough skip connections between the network's layers, our approach exploitsboth the graph structure and node features for learning discriminative noderepresentations. These skip connections are integrated by design in ourproposed network architecture. The effectiveness of our model is demonstratedthrough extensive experiments on five benchmark datasets, achieving better orcomparable anomaly detection results against strong baseline methods. We alsodemonstrate through an ablation study that skip connection helps improve themodel performance.</description><author>Mahsa Mesgaran, A. Ben Hamza</author><pubDate>Thu, 07 Sep 2023 14:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2010.10274v2</guid></item><item><title>Revisiting Hidden Representations in Transfer Learning for Medical Imaging</title><link>http://arxiv.org/abs/2302.08272v2</link><description>While a key component to the success of deep learning is the availability ofmassive amounts of training data, medical image datasets are often limited indiversity and size. Transfer learning has the potential to bridge the gapbetween related yet different domains. For medical applications, however, itremains unclear whether it is more beneficial to pre-train on natural ormedical images. We aim to shed light on this problem by comparinginitialization on ImageNet and RadImageNet on seven medical classificationtasks. Our work includes a replication study, which yields results contrary topreviously published findings. In our experiments, ResNet50 models pre-trainedon ImageNet tend to outperform those trained on RadImageNet. To gain furtherinsights, we investigate the learned representations using CanonicalCorrelation Analysis (CCA) and compare the predictions of the different models.Our results indicate that, contrary to intuition, ImageNet and RadImageNet mayconverge to distinct intermediate representations, which appear to divergefurther during fine-tuning. Despite these distinct representations, thepredictions of the models remain similar. Our findings show that the similaritybetween networks before and after fine-tuning does not correlate withperformance gains, suggesting that the advantages of transfer learning mightnot solely originate from the reuse of features in the early layers of aconvolutional neural network.</description><author>Dovile Juodelyte, Amelia JimÃ©nez-SÃ¡nchez, Veronika Cheplygina</author><pubDate>Thu, 07 Sep 2023 14:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08272v2</guid></item><item><title>Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory</title><link>http://arxiv.org/abs/2309.03696v1</link><description>Human Object Interaction (HOI) detection aims to localize and infer therelationships between a human and an object. Arguably, training supervisedmodels for this task from scratch presents challenges due to the performancedrop over rare classes and the high computational cost and time required tohandle long-tailed distributions of HOIs in complex HOI scenes in realisticsettings. This observation motivates us to design an HOI detector that can betrained even with long-tailed labeled data and can leverage existing knowledgefrom pre-trained models. Inspired by the powerful generalization ability of thelarge Vision-Language Models (VLM) on classification and retrieval tasks, wepropose an efficient Adaptive HOI Detector with Concept-guided Memory (ADA-CM).ADA-CM has two operating modes. The first mode makes it tunable withoutlearning new parameters in a training-free paradigm. Its second modeincorporates an instance-aware adapter mechanism that can further efficientlyboost performance if updating a lightweight set of parameters can be afforded.Our proposed method achieves competitive results with state-of-the-art on theHICO-DET and V-COCO datasets with much less training time. Code can be found athttps://github.com/ltttpku/ADA-CM.</description><author>Ting Lei, Fabian Caba, Qingchao Chen, Hailin Jin, Yuxin Peng, Yang Liu</author><pubDate>Thu, 07 Sep 2023 14:10:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03696v1</guid></item></channel></rss>