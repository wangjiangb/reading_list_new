<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 05 Oct 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity</title><link>http://arxiv.org/abs/2510.02315v1</link><description>Text-to-image (T2I) models excel on single-entity prompts but struggle withmulti-subject descriptions, often showing attribute leakage, identityentanglement, and subject omissions. We introduce the first theoreticalframework with a principled, optimizable objective for steering samplingdynamics toward multi-subject fidelity. Viewing flow matching (FM) throughstochastic optimal control (SOC), we formulate subject disentanglement ascontrol over a trained FM sampler. This yields two architecture-agnosticalgorithms: (i) a training-free test-time controller that perturbs the basevelocity with a single-pass update, and (ii) Adjoint Matching, a lightweightfine-tuning rule that regresses a control network to a backward adjoint signalwhile preserving base-model capabilities. The same formulation unifies priorattention heuristics, extends to diffusion models via a flow-diffusioncorrespondence, and provides the first fine-tuning route explicitly designedfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, andStable Diffusion XL, both algorithms consistently improve multi-subjectalignment while maintaining base-model style. Test-time control runsefficiently on commodity GPUs, and fine-tuned controllers trained on limitedprompts generalize to unseen ones. We further highlight FOCUS (Flow OptimalControl for Unentangled Subjects), which achieves state-of-the-artmulti-subject fidelity across models.</description><author>Eric Tillmann Bill, Enis Simsar, Thomas Hofmann</author><pubDate>Thu, 02 Oct 2025 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02315v1</guid></item><item><title>StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions</title><link>http://arxiv.org/abs/2510.02314v1</link><description>3D scene representation methods like Neural Radiance Fields (NeRF) and 3DGaussian Splatting (3DGS) have significantly advanced novel view synthesis. Asthese methods become prevalent, addressing their vulnerabilities becomescritical. We analyze 3DGS robustness against image-level poisoning attacks andpropose a novel density-guided poisoning method. Our method strategicallyinjects Gaussian points into low-density regions identified via Kernel DensityEstimation (KDE), embedding viewpoint-dependent illusory objects clearlyvisible from poisoned views while minimally affecting innocent views.Additionally, we introduce an adaptive noise strategy to disrupt multi-viewconsistency, further enhancing attack effectiveness. We propose a KDE-basedevaluation protocol to assess attack difficulty systematically, enablingobjective benchmarking for future research. Extensive experiments demonstrateour method's superior performance compared to state-of-the-art techniques.Project page: https://hentci.github.io/stealthattack/</description><author>Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, Wei-Chen Chiu</author><pubDate>Thu, 02 Oct 2025 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02314v1</guid></item><item><title>Differential Information Distribution: A Bayesian Perspective on Direct Preference Optimization</title><link>http://arxiv.org/abs/2505.23761v2</link><description>Direct Preference Optimization (DPO) has been widely used for aligninglanguage models with human preferences in a supervised manner. However, severalkey questions remain unresolved: the rationale behind its log-ratio reward, howthe statistical structure of preference datasets shapes its training dynamics,and how those dynamics impact downstream capabilities. We approach thesequestions from a Bayesian perspective, interpreting the goal of preferenceoptimization as learning the differential information required to update areference policy into a target policy. To formalize this view, we introduce theDifferential Information Distribution (DID), defined as the distribution oversamples that carry the Bayesian evidence required to update policies. Weintroduce three complementary insights by viewing preference optimizationthrough the DID. First, we find that DPO's log-ratio reward is uniquelyjustified when preferences encode the Differential Information needed to updatea reference policy into the target policy. Second, we discuss how commonlyobserved training dynamics in DPO, including changes in log-likelihood andpolicy exploration, stem from a power-law DID relationship. Finally, we analyzehow training dynamics influence downstream performance using the entropy ofDID, a principled measure of uncertainty in the learned information. We observethat learning high-entropy DID improves open-ended instruction-following, whilelow-entropy DID benefits knowledge-intensive QA. Taken together, our resultsshow that DPO's reward design, training dynamics, and downstream capabilitiesall emerge as natural consequences of learning Differential Information,offering both a principled theoretical foundation and practical guidance forpreference-based alignment.</description><author>Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo</author><pubDate>Thu, 02 Oct 2025 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.23761v2</guid></item><item><title>Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions</title><link>http://arxiv.org/abs/2510.02313v1</link><description>Can a model distinguish between the sound of a spoon hitting a hardwood floorversus a carpeted one? Everyday object interactions produce sounds unique tothe objects involved. We introduce the sounding object detection task toevaluate a model's ability to link these sounds to the objects directlyinvolved. Inspired by human perception, our multimodal object-aware frameworklearns from in-the-wild egocentric videos. To encourage an object-centricapproach, we first develop an automatic pipeline to compute segmentation masksof the objects involved to guide the model's focus during training towards themost informative regions of the interaction. A slot attention visual encoder isused to further enforce an object prior. We demonstrate state of the artperformance on our new task along with existing multimodal action understandingtasks.</description><author>Mengyu Yang, Yiming Chen, Haozheng Pei, Siddhant Agarwal, Arun Balajee Vasudevan, James Hays</author><pubDate>Thu, 02 Oct 2025 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02313v1</guid></item><item><title>KaVa: Latent Reasoning via Compressed KV-Cache Distillation</title><link>http://arxiv.org/abs/2510.02312v1</link><description>Large Language Models (LLMs) excel at multi-step reasoning problems withexplicit chain-of-thought (CoT), but verbose traces incur significantcomputational costs and memory overhead, and often carry redundant, stylisticartifacts. Latent reasoning has emerged as an efficient alternative thatinternalizes the thought process, but it suffers from a critical lack ofsupervision, limiting its effectiveness on complex, natural-language reasoningtraces. In this work, we propose KaVa, the first framework that bridges thisgap by distilling knowledge directly from a compressed KV-cache of the teacherinto a latent-reasoning student via self-distillation, leveraging therepresentational flexibility of continuous latent tokens to align stepwise KVtrajectories. We show that the abstract, unstructured knowledge withincompressed KV-cache, which lacks direct token correspondence, can serve as arich supervisory signal for a latent reasoning student. Empirically, theapproach consistently outperforms strong latent baselines, exhibits markedlysmaller degradation from equation-only to natural-language traces, and scalesto larger backbones while preserving efficiency. These results establishcompressed KV-cache distillation as a scalable supervision signal for latentreasoning, combining the accuracy of CoT-trained teachers with the efficiencyand deployability of latent inference.</description><author>Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi</author><pubDate>Thu, 02 Oct 2025 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02312v1</guid></item><item><title>Inferring Dynamic Physical Properties from Video Foundation Models</title><link>http://arxiv.org/abs/2510.02311v1</link><description>We study the task of predicting dynamic physical properties from videos. Morespecifically, we consider physical properties that require temporal informationto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,and dynamic friction of an object sliding on a surface. To this end, we makethe following contributions: (i) We collect a new video dataset for eachphysical property, consisting of synthetic training and testing splits, as wellas a real split for real world evaluation. (ii) We explore three ways to inferthe physical property from videos: (a) an oracle method where we supply thevisual cues that intrinsically reflect the property using classical computervision techniques; (b) a simple read out mechanism using a visual prompt andtrainable prompt vector for cross-attention on pre-trained video generative andself-supervised models; and (c) prompt strategies for Multi-modal LargeLanguage Models (MLLMs). (iii) We show that video foundation models trained ina generative or self-supervised manner achieve a similar performance, thoughbehind that of the oracle, and MLLMs are currently inferior to the othermodels, though their performance can be improved through suitable prompting.</description><author>Guanqi Zhan, Xianzheng Ma, Weidi Xie, Andrew Zisserman</author><pubDate>Thu, 02 Oct 2025 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02311v1</guid></item><item><title>Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization</title><link>http://arxiv.org/abs/2510.02308v1</link><description>Estimating the tangent spaces of a data manifold is a fundamental problem indata analysis. The standard approach, Local Principal Component Analysis(LPCA), struggles in high-noise settings due to a critical trade-off inchoosing the neighborhood size. Selecting an optimal size requires priorknowledge of the geometric and noise characteristics of the data that are oftenunavailable. In this paper, we propose a spectral method, Laplacian EigenvectorGradient Orthogonalization (LEGO), that utilizes the global structure of thedata to guide local tangent space estimation. Instead of relying solely onlocal neighborhoods, LEGO estimates the tangent space at each data point byorthogonalizing the gradients of low-frequency eigenvectors of the graphLaplacian. We provide two theoretical justifications of our method. First, adifferential geometric analysis on a tubular neighborhood of a manifold showsthat gradients of the low-frequency Laplacian eigenfunctions of the tube alignclosely with the manifold's tangent bundle, while an eigenfunction with highgradient in directions orthogonal to the manifold lie deeper in the spectrum.Second, a random matrix theoretic analysis also demonstrates that low-frequencyeigenvectors are robust to sub-Gaussian noise. Through comprehensiveexperiments, we demonstrate that LEGO yields tangent space estimates that aresignificantly more robust to noise than those from LPCA, resulting in markedimprovements in downstream tasks such as manifold learning, boundary detection,and local intrinsic dimension estimation.</description><author>Dhruv Kohli, Sawyer J. Robertson, Gal Mishne, Alexander Cloninger</author><pubDate>Thu, 02 Oct 2025 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02308v1</guid></item><item><title>NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation</title><link>http://arxiv.org/abs/2510.02307v1</link><description>Text-to-image diffusion models trained on a fixed set of resolutions oftenfail to generalize, even when asked to generate images at lower resolutionsthan those seen during training. High-resolution text-to-image generators arecurrently unable to easily offer an out-of-the-box budget-efficient alternativeto their users who might not need high-resolution images. We identify a keytechnical insight in diffusion models that when addressed can help tackle thislimitation: Noise schedulers have unequal perceptual effects acrossresolutions. The same level of noise removes disproportionately more signalfrom lower-resolution images than from high-resolution images, leading to atrain-test mismatch. We propose NoiseShift, a training-free method thatrecalibrates the noise level of the denoiser conditioned on resolution size.NoiseShift requires no changes to model architecture or sampling schedule andis compatible with existing models. When applied to Stable Diffusion 3, StableDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantlyimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, andFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These resultsdemonstrate the effectiveness of NoiseShift in mitigating resolution-dependentartifacts and enhancing the quality of low-resolution image generation.</description><author>Ruozhen He, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez</author><pubDate>Thu, 02 Oct 2025 17:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02307v1</guid></item><item><title>Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation</title><link>http://arxiv.org/abs/2510.02306v1</link><description>In arena-style evaluation of large language models (LLMs), two LLMs respondto a user query, and the user chooses the winning response or deems the"battle" a draw, resulting in an adjustment to the ratings of both models. Theprevailing approach for modeling these rating dynamics is to view battles astwo-player game matches, as in chess, and apply the Elo rating system and itsderivatives. In this paper, we critically examine this paradigm. Specifically,we question whether a draw genuinely means that the two models are equal andhence whether their ratings should be equalized. Instead, we conjecture thatdraws are more indicative of query difficulty: if the query is too easy, thenboth models are more likely to succeed equally. On three real-world arenadatasets, we show that ignoring rating updates for draws yields a 1-3% relativeincrease in battle outcome prediction accuracy (which includes draws) for allfour rating systems studied. Further analyses suggest that draws occur more forqueries rated as very easy and those as highly objective, with risk ratios of1.37 and 1.35, respectively. We recommend future rating systems to reconsiderexisting draw semantics and to account for query properties in rating updates.</description><author>Raphael Tang, Crystina Zhang, Wenyan Li, Carmen Lai, Pontus Stenetorp, Yao Lu</author><pubDate>Thu, 02 Oct 2025 17:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02306v1</guid></item><item><title>Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive</title><link>http://arxiv.org/abs/2510.02305v1</link><description>Diffusion models have achieved state-of-the-art performance, demonstratingremarkable generalisation capabilities across diverse domains. However, themechanisms underpinning these strong capabilities remain only partiallyunderstood. A leading conjecture, based on the manifold hypothesis, attributesthis success to their ability to adapt to low-dimensional geometric structurewithin the data. This work provides evidence for this conjecture, focusing onhow such phenomena could result from the formulation of the learning problemthrough score matching. We inspect the role of implicit regularisation byinvestigating the effect of smoothing minimisers of the empirical scorematching objective. Our theoretical and empirical results confirm thatsmoothing the score function -- or equivalently, smoothing in the log-densitydomain -- produces smoothing tangential to the data manifold. In addition, weshow that the manifold along which the diffusion model generalises can becontrolled by choosing an appropriate smoothing.</description><author>Tyler Farghly, Peter Potaptchik, Samuel Howard, George Deligiannidis, Jakiw Pidstrigach</author><pubDate>Thu, 02 Oct 2025 17:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02305v1</guid></item><item><title>Knowledge Distillation Detection for Open-weights Models</title><link>http://arxiv.org/abs/2510.02302v1</link><description>We propose the task of knowledge distillation detection, which aims todetermine whether a student model has been distilled from a given teacher,under a practical setting where only the student's weights and the teacher'sAPI are available. This problem is motivated by growing concerns about modelprovenance and unauthorized replication through distillation. To address thistask, we introduce a model-agnostic framework that combines data-free inputsynthesis and statistical score computation for detecting distillation. Ourapproach is applicable to both classification and generative models.Experiments on diverse architectures for image classification and text-to-imagegeneration show that our method improves detection accuracy over the strongestbaselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-imagegeneration. The code is available athttps://github.com/shqii1j/distillation_detection.</description><author>Qin Shi, Amber Yijia Zheng, Qifan Song, Raymond A. Yeh</author><pubDate>Thu, 02 Oct 2025 17:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02302v1</guid></item><item><title>Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models</title><link>http://arxiv.org/abs/2510.02300v1</link><description>We introduce Equilibrium Matching (EqM), a generative modeling frameworkbuilt from an equilibrium dynamics perspective. EqM discards thenon-equilibrium, time-conditional dynamics in traditional diffusion andflow-based generative models and instead learns the equilibrium gradient of animplicit energy landscape. Through this approach, we can adopt anoptimization-based sampling process at inference time, where samples areobtained by gradient descent on the learned landscape with adjustable stepsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generationperformance of diffusion/flow models empirically, achieving an FID of 1.90 onImageNet 256$\times$256. EqM is also theoretically justified to learn andsample from the data manifold. Beyond generation, EqM is a flexible frameworkthat naturally handles tasks including partially noised image denoising, OODdetection, and image composition. By replacing time-conditional velocities witha unified equilibrium landscape, EqM offers a tighter bridge between flow andenergy-based models and a simple route to optimization-driven inference.</description><author>Runqian Wang, Yilun Du</author><pubDate>Thu, 02 Oct 2025 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02300v1</guid></item><item><title>Interactive Training: Feedback-Driven Neural Network Optimization</title><link>http://arxiv.org/abs/2510.02297v1</link><description>Traditional neural network training typically follows fixed, predefinedoptimization recipes, lacking the flexibility to dynamically respond toinstabilities or emerging training issues. In this paper, we introduceInteractive Training, an open-source framework that enables real-time,feedback-driven intervention during neural network training by human experts orautomated AI agents. At its core, Interactive Training uses a control server tomediate communication between users or agents and the ongoing training process,allowing users to dynamically adjust optimizer hyperparameters, training data,and model checkpoints. Through three case studies, we demonstrate thatInteractive Training achieves superior training stability, reduced sensitivityto initial hyperparameters, and improved adaptability to evolving user needs,paving the way toward a future training paradigm where AI agents autonomouslymonitor training logs, proactively resolve instabilities, and optimize trainingdynamics.</description><author>Wentao Zhang, Yang Young Lu, Yuntian Deng</author><pubDate>Thu, 02 Oct 2025 17:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02297v1</guid></item><item><title>Continual Personalization for Diffusion Models</title><link>http://arxiv.org/abs/2510.02296v1</link><description>Updating diffusion models in an incremental setting would be practical inreal-world applications yet computationally challenging. We present a novellearning strategy of Concept Neuron Selection (CNS), a simple yet effectiveapproach to perform personalization in a continual learning scheme. CNSuniquely identifies neurons in diffusion models that are closely related to thetarget concepts. In order to mitigate catastrophic forgetting problems whilepreserving zero-shot text-to-image generation ability, CNS finetunes conceptneurons in an incremental manner and jointly preserves knowledge learned ofprevious concepts. Evaluation of real-world datasets demonstrates that CNSachieves state-of-the-art performance with minimal parameter adjustments,outperforming previous methods in both single and multi-concept personalizationworks. CNS also achieves fusion-free operation, reducing memory storage andprocessing time for continual personalization.</description><author>Yu-Chien Liao, Jr-Jen Chen, Chi-Pin Huang, Ci-Siang Lin, Meng-Lin Wu, Yu-Chiang Frank Wang</author><pubDate>Thu, 02 Oct 2025 17:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02296v1</guid></item><item><title>VideoNSA: Native Sparse Attention Scales Video Understanding</title><link>http://arxiv.org/abs/2510.02295v1</link><description>Video understanding in multimodal language models remains limited by contextlength: models often miss key transition frames and struggle to maintaincoherence across long time scales. To address this, we adapt Native SparseAttention (NSA) to video-language models. Our method, VideoNSA, adaptsQwen2.5-VL through end-to-end training on a 216K video instruction dataset. Weemploy a hardware-aware hybrid approach to attention, preserving denseattention for text, while employing NSA for video. Compared totoken-compression and training-free sparse baselines, VideoNSA achievesimproved performance on long-video understanding, temporal reasoning, andspatial benchmarks. Further ablation analysis reveals four key findings: (1)reliable scaling to 128K tokens; (2) an optimal global-local attentionallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)the learnable combined sparse attention help induce dynamic attention sinks.</description><author>Enxin Song, Wenhao Chai, Shusheng Yang, Ethan Armand, Xiaojun Shan, Haiyang Xu, Jianwen Xie, Zhuowen Tu</author><pubDate>Thu, 02 Oct 2025 17:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02295v1</guid></item><item><title>F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data</title><link>http://arxiv.org/abs/2510.02294v1</link><description>We introduce F2LLM - Foundation to Feature Large Language Models, a suite ofstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlikeprevious top-ranking embedding models that require massive contrastivepretraining, sophisticated training pipelines, and costly synthetic trainingdata, F2LLM is directly finetuned from foundation models on 6 millionquery-document-negative tuples curated from open-source, non-syntheticdatasets, striking a strong balance between training cost, model size, andembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2ndamong models with approximately 4B parameters and 7th overall, while F2LLM-1.7Branks 1st among models in the 1B-2B size range. To facilitate future researchin the field, we release the models, training dataset, and code, positioningF2LLM as a strong, reproducible, and budget-friendly baseline for future works.</description><author>Ziyin Zhang, Zihan Liao, Hang Yu, Peng Di, Rui Wang</author><pubDate>Thu, 02 Oct 2025 17:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02294v1</guid></item><item><title>From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</title><link>http://arxiv.org/abs/2510.02292v1</link><description>We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,analysis, and interpretation of vision-language models (VLMs) by supporting theextraction of intermediate outputs from any layer during the forward pass ofopen-source VLMs. VLM-Lens provides a unified, YAML-configurable interface thatabstracts away model-specific complexities and supports user-friendly operationacross diverse VLMs. It currently supports 16 state-of-the-art base VLMs andtheir over 30 variants, and is extensible to accommodate new models withoutchanging the core logic. The toolkit integrates easily with various interpretability and analysismethods. We demonstrate its usage with two simple analytical experiments,revealing systematic differences in the hidden representations of VLMs acrosslayers and target concepts. VLM-Lens is released as an open-sourced project toaccelerate community efforts in understanding and improving VLMs.</description><author>Hala Sheta, Eric Huang, Shuyu Wu, Ilia Alenabi, Jiajun Hong, Ryker Lin, Ruoxi Ning, Daniel Wei, Jialin Yang, Jiawei Zhou, Ziqiao Ma, Freda Shi</author><pubDate>Thu, 02 Oct 2025 17:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02292v1</guid></item><item><title>Test-Time Anchoring for Discrete Diffusion Posterior Sampling</title><link>http://arxiv.org/abs/2510.02291v1</link><description>We study the problem of posterior sampling using pretrained discretediffusion foundation models, aiming to recover images from noisy measurementswithout retraining task-specific models. While diffusion models have achievedremarkable success in generative modeling, most advances rely on continuousGaussian diffusion. In contrast, discrete diffusion offers a unified frameworkfor jointly modeling categorical data such as text and images. Beyondunification, discrete diffusion provides faster inference, finer control, andprincipled training-free Bayesian inference, making it particularly well-suitedfor posterior sampling. However, existing approaches to discrete diffusionposterior sampling face severe challenges: derivative-free guidance yieldssparse signals, continuous relaxations limit applicability, and split Gibbssamplers suffer from the curse of dimensionality. To overcome theselimitations, we introduce Anchored Posterior Sampling (APS) for maskeddiffusion foundation models, built on two key innovations -- quantizedexpectation for gradient-like guidance in discrete embedding space, andanchored remasking for adaptive decoding. Our approach achievesstate-of-the-art performance among discrete diffusion samplers across linearand nonlinear inverse problems on the standard benchmarks. We furtherdemonstrate the benefits of our approach in training-free stylization andtext-guided editing.</description><author>Litu Rout, Andreas Lugmayr, Yasamin Jafarian, Srivatsan Varadharajan, Constantine Caramanis, Sanjay Shakkottai, Ira Kemelmacher-Shlizerman</author><pubDate>Thu, 02 Oct 2025 17:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02291v1</guid></item><item><title>MultiModal Action Conditioned Video Generation</title><link>http://arxiv.org/abs/2510.02287v1</link><description>Current video models fail as world model as they lack fine-graiend control.General-purpose household robots require real-time fine motor control to handledelicate tasks and urgent situations. In this work, we introduce fine-grainedmultimodal actions to capture such precise control. We consider senses ofproprioception, kinesthesia, force haptics, and muscle activation. Suchmultimodal senses naturally enables fine-grained interactions that aredifficult to simulate with text-conditioned generative models. To effectivelysimulate fine-grained multisensory actions, we develop a feature learningparadigm that aligns these modalities while preserving the unique informationeach modality provides. We further propose a regularization scheme to enhancecausality of the action trajectory features in representing intricateinteraction dynamics. Experiments show that incorporating multimodal sensesimproves simulation accuracy and reduces temporal drift. Extensive ablationstudies and downstream applications demonstrate the effectiveness andpracticality of our work.</description><author>Yichen Li, Antonio Torralba</author><pubDate>Thu, 02 Oct 2025 17:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02287v1</guid></item><item><title>Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks</title><link>http://arxiv.org/abs/2510.02286v1</link><description>Despite recent rapid progress in AI safety, current large language modelsremain vulnerable to adversarial attacks in multi-turn interaction settings,where attackers strategically adapt their prompts across conversation turns andpose a more critical yet realistic challenge. Existing approaches that discoversafety vulnerabilities either rely on manual red-teaming with human experts oremploy automated methods using pre-defined templates and human-curated attackdata, with most focusing on single-turn attacks. However, these methods did notexplore the vast space of possible multi-turn attacks, failing to considernovel attack trajectories that emerge from complex dialogue dynamics andstrategic conversation planning. This gap is particularly critical given recentfindings that LLMs exhibit significantly higher vulnerability to multi-turnattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policyreinforcement learning framework integrated with tree search that autonomouslydiscovers diverse multi-turn attack strategies by treating the dialogue as asequential decision-making problem, enabling systematic exploration withoutmanually curated data. Through extensive experiments, our approach not onlyachieves more than 25.9% higher ASR across 10 target models compared toprevious state-of-the-art approaches, but also effectively uncovers new attackstrategies by learning optimal dialogue policies that maximize attack successacross multiple turns.</description><author>Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, Dan Roth</author><pubDate>Thu, 02 Oct 2025 17:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02286v1</guid></item><item><title>Learning to Generate Object Interactions with Physics-Guided Video Diffusion</title><link>http://arxiv.org/abs/2510.02284v1</link><description>Recent models for video generation have achieved remarkable progress and arenow deployed in film, social media production, and advertising. Beyond theircreative potential, such models also hold promise as world simulators forrobotics and embodied decision making. Despite strong advances, however,current approaches still struggle to generate physically plausible objectinteractions and lack physics-grounded control mechanisms. To address thislimitation, we introduce KineMask, an approach for physics-guided videogeneration that enables realistic rigid body control, interactions, andeffects. Given a single image and a specified object velocity, our methodgenerates videos with inferred motions and future object interactions. Wepropose a two-stage training strategy that gradually removes future motionsupervision via object masks. Using this strategy we train video diffusionmodels (VDMs) on synthetic scenes of simple interactions and demonstratesignificant improvements of object interactions in real scenes. Furthermore,KineMask integrates low-level motion control with high-level textualconditioning via predictive scene descriptions, leading to effective supportfor synthesis of complex dynamical phenomena. Extensive experiments show thatKineMask achieves strong improvements over recent models of comparable size.Ablation studies further highlight the complementary roles of low- andhigh-level conditioning in VDMs. Our code, model, and data will be madepublicly available.</description><author>David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev</author><pubDate>Thu, 02 Oct 2025 17:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02284v1</guid></item><item><title>Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</title><link>http://arxiv.org/abs/2510.02283v1</link><description>Diffusion models have revolutionized image and video generation, achievingunprecedented visual quality. However, their reliance on transformerarchitectures incurs prohibitively high computational costs, particularly whenextending generation to long videos. Recent work has explored autoregressiveformulations for long video generation, typically by distilling fromshort-horizon bidirectional teachers. Nevertheless, given that teacher modelscannot synthesize long videos, the extrapolation of student models beyond theirtraining horizon often leads to pronounced quality degradation, arising fromthe compounding of errors within the continuous latent space. In this paper, wepropose a simple yet effective approach to mitigate quality degradation inlong-horizon video generation without requiring supervision from long-videoteachers or retraining on long video datasets. Our approach centers onexploiting the rich knowledge of teacher models to provide guidance for thestudent model through sampled segments drawn from self-generated long videos.Our method maintains temporal consistency while scaling video length by up to20x beyond teacher's capability, avoiding common issues such as over-exposureand error-accumulation without recomputing overlapping frames like previousmethods. When scaling up the computation, our method shows the capability ofgenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of themaximum span supported by our base model's position embedding and more than 50xlonger than that of our baseline model. Experiments on standard benchmarks andour proposed improved benchmark demonstrate that our approach substantiallyoutperforms baseline methods in both fidelity and consistency. Our long-horizonvideos demo can be found at https://self-forcing-plus-plus.github.io/</description><author>Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</author><pubDate>Thu, 02 Oct 2025 17:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02283v1</guid></item><item><title>VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</title><link>http://arxiv.org/abs/2510.02282v1</link><description>With the rapid advancement of AI-generated videos, there is an urgent needfor effective detection tools to mitigate societal risks such as misinformationand reputational harm. In addition to accurate classification, it is essentialthat detection models provide interpretable explanations to ensure transparencyfor regulators and end users. To address these challenges, we introduceVidGuard-R1, the first video authenticity detector that fine-tunes amulti-modal large language model (MLLM) using group relative policyoptimization (GRPO). Our model delivers both highly accurate judgments andinsightful reasoning. We curate a challenging dataset of 140k real andAI-generated videos produced by state-of-the-art generation models, carefullydesigning the generation process to maximize discrimination difficulty. We thenfine-tune Qwen-VL using GRPO with two specialized reward models that targettemporal artifacts and generation complexity. Extensive experiments demonstratethat VidGuard-R1 achieves state-of-the-art zero-shot performance on existingbenchmarks, with additional training pushing accuracy above 95%. Case studiesfurther show that VidGuard-R1 produces precise and interpretable rationalesbehind its predictions. The code is publicly available athttps://VidGuard-R1.github.io.</description><author>Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, Lili Qiu</author><pubDate>Thu, 02 Oct 2025 17:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02282v1</guid></item><item><title>What Makes a Good Dataset for Knowledge Distillation?</title><link>http://arxiv.org/abs/2411.12817v2</link><description>Knowledge distillation (KD) has been a popular and effective method for modelcompression. One important assumption of KD is that the teacher's originaldataset will also be available when training the student. However, insituations such as continual learning and distilling large models trained oncompany-withheld datasets, having access to the original data may not always bepossible. This leads practitioners towards utilizing other sources ofsupplemental data, which could yield mixed results. One must then ask: "whatmakes a good dataset for transferring knowledge from teacher to student?" Manywould assume that only real in-domain imagery is viable, but is that the onlyoption? In this work, we explore multiple possible surrogate distillationdatasets and demonstrate that many different datasets, even unnatural syntheticimagery, can serve as a suitable alternative in KD. From examining thesealternative datasets, we identify and present various criteria describing whatmakes a good dataset for distillation. Source code is available athttps://github.com/osu-cvl/good-kd-dataset.</description><author>Logan Frank, Jim Davis</author><pubDate>Thu, 02 Oct 2025 17:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12817v2</guid></item><item><title>Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation</title><link>http://arxiv.org/abs/2510.02279v1</link><description>Hallucinations are a common issue that undermine the reliability of largelanguage models (LLMs). Recent studies have identified a specific subset ofhallucinations, known as confabulations, which arise due to predictiveuncertainty of LLMs. To detect confabulations, various methods for estimatingpredictive uncertainty in natural language generation (NLG) have beendeveloped. These methods are typically evaluated by correlating uncertaintyestimates with the correctness of generated text, with question-answering (QA)datasets serving as the standard benchmark. However, commonly used approximatecorrectness functions have substantial disagreement between each other and,consequently, in the ranking of the uncertainty estimation methods. This allowsone to inflate the apparent performance of uncertainty estimation methods. Wepropose using several alternative risk indicators for risk correlationexperiments that improve robustness of empirical assessment of UE algorithmsfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judgevariants leads to reducing the evaluation biases. Furthermore, we explorestructured tasks as well as out of distribution and perturbation detectiontasks which provide robust and controllable risk indicators. Finally, wepropose to use an Elo rating of uncertainty estimation methods to give anobjective summarization over extensive evaluation settings.</description><author>Mykyta Ielanskyi, Kajetan Schweighofer, Lukas Aichberger, Sepp Hochreiter</author><pubDate>Thu, 02 Oct 2025 17:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02279v1</guid></item><item><title>Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks</title><link>http://arxiv.org/abs/2510.02278v1</link><description>Traffic forecasting on road networks is a complex task of significantpractical importance that has recently attracted considerable attention fromthe machine learning community, with spatiotemporal graph neural networks(GNNs) becoming the most popular approach. The proper evaluation of trafficforecasting methods requires realistic datasets, but current publicly availablebenchmarks have significant drawbacks, including the absence of informationabout road connectivity for road graph construction, limited information aboutroad properties, and a relatively small number of road segments that fallsshort of real-world applications. Further, current datasets mostly containinformation about intercity highways with sparsely located sensors, while cityroad networks arguably present a more challenging forecasting task due to muchdenser roads and more complex urban traffic patterns. In this work, we providea more complete, realistic, and challenging benchmark for traffic forecastingby releasing datasets representing the road networks of two major cities, withthe largest containing almost 100,000 road segments (more than a 10-foldincrease relative to existing datasets). Our datasets contain rich roadfeatures and provide fine-grained data about both traffic volume and trafficspeed, allowing for building more holistic traffic forecasting systems. We showthat most current implementations of neural spatiotemporal models for trafficforecasting have problems scaling to datasets of our size. To overcome thisissue, we propose an alternative approach to neural traffic forecasting thatuses a GNN without a dedicated module for temporal sequence processing, thusachieving much better scalability, while also demonstrating strongerforecasting performance. We hope our datasets and modeling insights will serveas a valuable resource for research in traffic forecasting.</description><author>Fedor Velikonivtsev, Oleg Platonov, Gleb Bazhenov, Liudmila Prokhorenkova</author><pubDate>Thu, 02 Oct 2025 17:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02278v1</guid></item><item><title>BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals</title><link>http://arxiv.org/abs/2510.02276v1</link><description>Biosignals offer valuable insights into the physiological states of the humanbody. Although biosignal modalities differ in functionality, signal fidelity,sensor comfort, and cost, they are often intercorrelated, reflecting theholistic and interconnected nature of human physiology. This opens up thepossibility of performing the same tasks using alternative biosignalmodalities, thereby improving the accessibility, usability, and adaptability ofhealth monitoring systems. However, the limited availability of large labeleddatasets presents challenges for training models tailored to specific tasks andmodalities of interest. Unsupervised cross-modal knowledge transfer offers apromising solution by leveraging knowledge from an existing modality to supportmodel training for a new modality. Existing methods are typically based onknowledge distillation, which requires running a teacher model alongsidestudent model training, resulting in high computational and memory overhead.This challenge is further exacerbated by the recent development of foundationmodels that demonstrate superior performance and generalization across tasks atthe cost of large model sizes. To this end, we explore a new framework forunsupervised cross-modal knowledge transfer of biosignals by training alightweight bridge network to align the intermediate representations and enableinformation flow between foundation models and across modalities. Specifically,we introduce an efficient strategy for selecting alignment positions where thebridge should be constructed, along with a flexible prototype network as thebridge architecture. Extensive experiments across multiple biosignalmodalities, tasks, and datasets show that BioX-Bridge reduces the number oftrainable parameters by 88--99\% while maintaining or even improving transferperformance compared to state-of-the-art methods.</description><author>Chenqi Li, Yu Liu, Timothy Denison, Tingting Zhu</author><pubDate>Thu, 02 Oct 2025 17:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02276v1</guid></item><item><title>Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps</title><link>http://arxiv.org/abs/2510.02274v1</link><description>Modeling radio frequency (RF) signal propagation is essential forunderstanding the environment, as RF signals offer valuable insights beyond thecapabilities of RGB cameras, which are limited by the visible-light spectrum,lens coverage, and occlusions. It is also useful for supporting wirelessdiagnosis, deployment, and optimization. However, accurately predicting RFsignals in complex environments remains a challenge due to interactions withobstacles such as absorption and reflection. We introduce Diffusion^2, adiffusion-based approach that uses 3D point clouds to model the propagation ofRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.To effectively capture RF-related features from 3D data, we present the RF-3DEncoder, which encapsulates the complexities of 3D geometry along withsignal-specific details. These features undergo multi-scale embedding tosimulate the actual RF signal dissemination process. Our evaluation, based onsynthetic and real-world measurements, demonstrates that Diffusion^2 accuratelyestimates the behavior of RF signals in various frequency bands andenvironmental conditions, with an error margin of just 1.9 dB and 27x fasterthan existing methods, marking a significant advancement in the field. Refer tohttps://rfvision-project.github.io/ for more information.</description><author>Kyoungjun Park, Yifan Yang, Changhan Ge, Lili Qiu, Shiqi Jiang</author><pubDate>Thu, 02 Oct 2025 17:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02274v1</guid></item><item><title>Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective</title><link>http://arxiv.org/abs/2510.02272v1</link><description>Recent advancements in Reinforcement Post-Training (RPT) have significantlyenhanced the capabilities of Large Reasoning Models (LRMs), sparking increasedinterest in the generalization of RL-based reasoning. While existing work hasprimarily focused on investigating its generalization across tasks ormodalities, this study proposes a novel cross-linguistic perspective toinvestigate reasoning generalization. This raises a crucial question:$\textit{Does the reasoning capability achieved from English RPT effectivelytransfer to other languages?}$ We address this by systematically evaluatingEnglish-centric LRMs on multilingual reasoning benchmarks and introducing ametric to quantify cross-lingual transferability. Our findings reveal thatcross-lingual transferability varies significantly across initial model, targetlanguage, and training paradigm. Through interventional studies, we find thatmodels with stronger initial English capabilities tend to over-rely onEnglish-specific patterns, leading to diminished cross-lingual generalization.To address this, we conduct a thorough parallel training study. Experimentalresults yield three key findings: $\textbf{First-Parallel Leap}$, a substantialleap in performance when transitioning from monolingual to just a singleparallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealingthat cross-lingual reasoning transfer follows a power-law with the number oftraining parallel languages. Moreover, we identify the discrepancy betweenactual monolingual performance and the power-law prediction as$\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMsfail to fully generalize across languages. Our study challenges the assumptionthat LRM reasoning mirrors human cognition, providing critical insights for thedevelopment of more language-agnostic LRMs.</description><author>Wen Yang, Junhong Wu, Chong Li, Chengqing Zong, Jiajun Zhang</author><pubDate>Thu, 02 Oct 2025 17:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02272v1</guid></item><item><title>Riemannian Variational Flow Matching for Material and Protein Design</title><link>http://arxiv.org/abs/2502.12981v2</link><description>We present Riemannian Gaussian Variational Flow Matching (RG-VFM), ageometric extension of Variational Flow Matching (VFM) for generative modelingon manifolds. In Euclidean space, predicting endpoints (VFM), velocities (FM),or noise (diffusion) are largely equivalent due to affine interpolations. Oncurved manifolds this equivalence breaks down, and we hypothesize that endpointprediction provides a stronger learning signal by directly minimizing geodesicdistances. Building on this insight, we derive a variational flow matchingobjective based on Riemannian Gaussian distributions, applicable to manifoldswith closed-form geodesics. We formally analyze its relationship to RiemannianFlow Matching (RFM), exposing that the RFM objective lacks acurvature-dependent penalty - encoded via Jacobi fields - that is naturallypresent in RG-VFM. Experiments on synthetic spherical and hyperbolicbenchmarks, as well as real-world tasks in material and protein generation,demonstrate that RG-VFM more effectively captures manifold structure andimproves downstream performance over Euclidean and velocity-based baselines.</description><author>Olga Zaghen, Floor Eijkelboom, Alison Pouplin, Cong Liu, Max Welling, Jan-Willem van de Meent, Erik J. Bekkers</author><pubDate>Thu, 02 Oct 2025 17:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.12981v2</guid></item><item><title>InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents</title><link>http://arxiv.org/abs/2510.02271v1</link><description>Information seeking is a fundamental requirement for humans. However,existing LLM agents rely heavily on open-web search, which exposes twofundamental weaknesses: online content is noisy and unreliable, and manyreal-world tasks require precise, domain-specific knowledge unavailable fromthe web. The emergence of the Model Context Protocol (MCP) now allows agents tointerface with thousands of specialized tools, seemingly resolving thislimitation. Yet it remains unclear whether agents can effectively leverage suchtools -- and more importantly, whether they can integrate them withgeneral-purpose search to solve complex tasks. Therefore, we introduceInfoMosaic-Bench, the first benchmark dedicated to multi-source informationseeking in tool-augmented agents. Covering six representative domains(medicine, finance, maps, video, web, and multi-domain integration),InfoMosaic-Bench requires agents to combine general-purpose search withdomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalablepipeline that grounds task conditions in verified tool outputs, enforcescross-source dependencies, and filters out shortcut cases solvable by triviallookup. This design guarantees both reliability and non-triviality. Experimentswith 14 state-of-the-art LLM agents reveal three findings: (i) web informationalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% passrate; (ii) domain tools provide selective but inconsistent benefits, improvingsome domains while degrading others; and (iii) 22.4% of failures arise fromincorrect tool usage or selection, highlighting that current LLMs stillstruggle with even basic tool handling.</description><author>Yaxin Du, Yuanshuo Zhang, Xiyuan Yang, Yifan Zhou, Cheng Wang, Gongyi Zou, Xianghe Pang, Wenhao Wang, Menglan Chen, Shuo Tang, Zhiyu Li, Siheng Chen</author><pubDate>Thu, 02 Oct 2025 17:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02271v1</guid></item><item><title>microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</title><link>http://arxiv.org/abs/2510.02270v1</link><description>Unsupervised adaptation of CLIP-based vision-language models (VLMs) forfine-grained image classification requires sensitivity to microscopic localcues. While CLIP exhibits strong zero-shot transfer, its reliance on coarseglobal features restricts its performance on fine-grained classification tasks.Prior efforts inject fine-grained knowledge by aligning large language model(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approachoverlooks spatial precision. We propose $\textbf{microCLIP}$, a self-trainingframework that jointly refines CLIP's visual and textual representations usingfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)within a lightweight TokenFusion module, which builds a saliency-guided$\texttt{[FG]}$ token from patch embeddings and fuses it with the global$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, weintroduce a two-headed LLM-derived classifier: a frozen classifier that, viamulti-view alignment, provides a stable text-based prior for pseudo-labeling,and a learnable classifier initialized from LLM descriptions and fine-tunedwith TokenFusion. We further develop Dynamic Knowledge Aggregation, whichconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits toiteratively refine pseudo-labels. Together, these components uncover latentfine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracygain across 13 fine-grained benchmarks while requiring only light adaptation.Our code is available at https://github.com/sathiiii/microCLIP.</description><author>Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan</author><pubDate>Thu, 02 Oct 2025 17:47:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02270v1</guid></item><item><title>Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning</title><link>http://arxiv.org/abs/2510.02268v1</link><description>We study view-invariant imitation learning by explicitly conditioningpolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, weshow that conditioning on extrinsics significantly improves generalizationacross viewpoints for standard behavior cloning policies, including ACT,Diffusion Policy, and SmolVLA. To evaluate policy robustness under realisticviewpoint shifts, we introduce six manipulation tasks in RoboSuite andManiSkill that pair "fixed" and "randomized" scene variants, decouplingbackground cues from camera pose. Our analysis reveals that policies withoutextrinsics often infer camera pose using visual cues from static backgrounds infixed scenes; this shortcut collapses when workspace geometry or cameraplacement shifts. Conditioning on extrinsics restores performance and yieldsrobust RGB-only control without depth. We release the tasks, demonstrations,and code at https://ripl.github.io/know_your_camera/ .</description><author>Tianchong Jiang, Jingtian Ji, Xiangshan Tan, Jiading Fang, Anand Bhattad, Vitor Guizilini, Matthew R. Walter</author><pubDate>Thu, 02 Oct 2025 17:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02268v1</guid></item><item><title>NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes</title><link>http://arxiv.org/abs/2510.02266v1</link><description>Reconstructing visual information from brain activity via computer visiontechnology provides an intuitive understanding of visual neural mechanisms.Despite progress in decoding fMRI data with generative models, achievingaccurate cross-subject reconstruction of visual stimuli remains challenging andcomputationally demanding. This difficulty arises from inter-subjectvariability in neural representations and the brain's abstract encoding of coresemantic features in complex visual inputs. To address these challenges, wepropose NeuroSwift, which integrates complementary adapters via diffusion:AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapteris trained on Stable Diffusion generated images paired with COCO captions toemulate higher visual cortex encoding. For cross-subject generalization, wepretrain on one subject and then fine-tune only 17 percent of parameters (fullyconnected layers) for new subjects, while freezing other components. Thisenables state-of-the-art performance with only one hour of training per subjecton lightweight GPUs (three RTX 4090), and it outperforms existing methods.</description><author>Shiyi Zhang, Dong Liang, Yihang Zhou</author><pubDate>Thu, 02 Oct 2025 17:45:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02266v1</guid></item><item><title>How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning</title><link>http://arxiv.org/abs/2510.02265v1</link><description>This paper studies the problem of mitigating reactive jamming, where a jammeradopts a dynamic policy of selecting channels and sensing thresholds to detectand jam ongoing transmissions. The transmitter-receiver pair learns to avoidjamming and optimize throughput over time (without prior knowledge of channelconditions or jamming strategies) by using reinforcement learning (RL) to adapttransmit power, modulation, and channel selection. Q-learning is employed fordiscrete jamming-event states, while Deep Q-Networks (DQN) are employed forcontinuous states based on received power. Through different reward functionsand action sets, the results show that RL can adapt rapidly to spectrumdynamics and sustain high rates as channels and jamming policies change overtime.</description><author>Yalin E. Sagduyu, Tugba Erpek, Kemal Davaslioglu, Sastry Kompella</author><pubDate>Thu, 02 Oct 2025 17:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02265v1</guid></item><item><title>Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities</title><link>http://arxiv.org/abs/2510.02264v1</link><description>Advances in machine learning and wearable sensors offer new opportunities forcapturing and analyzing human movement outside specialized laboratories.Accurate assessment of human movement under real-world conditions is essentialfor telemedicine, sports science, and rehabilitation. This preclinicalbenchmark compares monocular video-based 3D human pose estimation models withinertial measurement units (IMUs), leveraging the VIDIMU dataset containing atotal of 13 clinically relevant daily activities which were captured using bothcommodity video cameras and five IMUs. During this initial study only healthysubjects were recorded, so results cannot be generalized to pathologicalcohorts. Joint angles derived from state-of-the-art deep learning frameworks(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIABodyTrack) were evaluated against joint angles computed from IMU data usingOpenSim inverse kinematics following the Human3.6M dataset format with 17keypoints. Among them, MotionAGFormer demonstrated superior performance,achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). Theresults reveal that both technologies are viable for out-of-the-lab kinematicassessment. However, they also highlight key trade-offs between video- andsensor-based approaches including costs, accessibility, and precision. Thisstudy clarifies where off-the-shelf video models already provide clinicallypromising kinematics in healthy adults and where they lag behind IMU-basedestimates while establishing valuable guidelines for researchers and cliniciansseeking to develop robust, cost-effective, and user-friendly solutions fortelehealth and remote patient monitoring.</description><author>Mario Medrano-Paredes, Carmen Fernndez-Gonzlez, Francisco-Javier Daz-Pernas, Hichem Saoudi, Javier Gonzlez-Alonso, Mario Martnez-Zarzuela</author><pubDate>Thu, 02 Oct 2025 17:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02264v1</guid></item><item><title>RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems</title><link>http://arxiv.org/abs/2510.02263v1</link><description>Reasoning requires going beyond pattern matching or memorization of solutionsto identify and implement "algorithmic procedures" that can be used to deduceanswers to hard problems. Doing so requires realizing the most relevantprimitives, intermediate results, or shared procedures, and building upon them.While RL post-training on long chains of thought ultimately aims to uncoverthis kind of algorithmic behavior, most reasoning traces learned by largemodels fail to consistently capture or reuse procedures, instead drifting intoverbose and degenerate exploration. To address more effective reasoning, weintroduce reasoning abstractions: concise natural language descriptions ofprocedural and factual knowledge that guide the model toward learningsuccessful reasoning. We train models to be capable of proposing multipleabstractions given a problem, followed by RL that incentivizes building asolution while using the information provided by these abstractions. Thisresults in a two-player RL training paradigm, abbreviated as RLAD, that jointlytrains an abstraction generator and a solution generator. This setupeffectively enables structured exploration, decouples learning signals ofabstraction proposal and solution generation, and improves generalization toharder problems. We also show that allocating more test-time compute togenerating abstractions is more beneficial for performance than generating moresolutions at large test budgets, illustrating the role of abstractions inguiding meaningful exploration.</description><author>Yuxiao Qu, Anikait Singh, Yoonho Lee, Amrith Setlur, Ruslan Salakhutdinov, Chelsea Finn, Aviral Kumar</author><pubDate>Thu, 02 Oct 2025 17:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02263v1</guid></item><item><title>From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</title><link>http://arxiv.org/abs/2510.02262v1</link><description>Video Large Language Models (VLMs) have achieved remarkable results on avariety of vision language tasks, yet their practical use is limited by the"needle in a haystack" problem: the massive number of visual tokens producedfrom raw video frames exhausts the model's context window. Existing solutionsalleviate this issue by selecting a sparse set of frames, thereby reducingtoken count, but such frame-wise selection discards essential temporaldynamics, leading to suboptimal reasoning about motion and event continuity. Inthis work we systematically explore the impact of temporal information anddemonstrate that extending selection from isolated key frames to key clips,which are short, temporally coherent segments, improves video understanding. Tomaintain a fixed computational budget while accommodating the larger tokenfootprint of clips, we propose an adaptive resolution strategy that dynamicallybalances spatial resolution and clip length, ensuring a constant token countper video. Experiments on three long-form video benchmarks demonstrate that ourtraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. Theseresults highlight the importance of preserving temporal coherence in frameselection and provide a practical pathway for scaling Video LLMs to real worldvideo understanding applications. Project webpage is available athttps://guangyusun.com/f2c .</description><author>Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler</author><pubDate>Thu, 02 Oct 2025 17:43:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02262v1</guid></item><item><title>VITA: Vision-to-Action Flow Matching Policy</title><link>http://arxiv.org/abs/2507.13231v2</link><description>Conventional flow matching and diffusion-based policies sample throughiterative denoising from standard noise distributions (e.g., Gaussian), andrequire conditioning mechanisms to incorporate visual information during thegenerative process, incurring substantial time and memory overhead. To reducethe complexity, we develop VITA(VIsion-To-Action policy), a noise-free andconditioning-free policy learning framework that directly maps visualrepresentations to latent actions using flow matching. VITA treats latentvisual representations as the source of the flow, thus eliminating the need ofconditioning. As expected, bridging vision and action is challenging, becauseactions are lower-dimensional, less structured, and sparser than visualrepresentations; moreover, flow matching requires the source and target to havethe same dimensionality. To overcome this, we introduce an action autoencoderthat maps raw actions into a structured latent space aligned with visuallatents, trained jointly with flow matching. To further prevent latent spacecollapse, we propose flow latent decoding, which anchors the latent generationprocess by backpropagating the action reconstruction loss through the flowmatching ODE (ordinary differential equations) solving steps. We evaluate VITAon 8 simulation and 2 real-world tasks from ALOHA and Robomimic. VITAoutperforms or matches state-of-the-art generative policies, while achieving1.5-2.3x faster inference compared to conventional methods with conditioning.Project page: https://ucd-dare.github.io/VITA/</description><author>Dechen Gao, Boqi Zhao, Andrew Lee, Ian Chuang, Hanchu Zhou, Hang Wang, Zhe Zhao, Junshan Zhang, Iman Soltani</author><pubDate>Thu, 02 Oct 2025 17:42:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.13231v2</guid></item><item><title>Transformers Discover Molecular Structure Without Graph Priors</title><link>http://arxiv.org/abs/2510.02259v1</link><description>Graph Neural Networks (GNNs) are the dominant architecture for molecularmachine learning, particularly for molecular property prediction and machinelearning interatomic potentials (MLIPs). GNNs perform message passing onpredefined graphs often induced by a fixed radius cutoff or k-nearest neighborscheme. While this design aligns with the locality present in many moleculartasks, a hard-coded graph can limit expressivity due to the fixed receptivefield and slows down inference with sparse graph operations. In this work, weinvestigate whether pure, unmodified Transformers trained directly on Cartesiancoordinates$\unicode{x2013}$without predefined graphs or physicalpriors$\unicode{x2013}$can approximate molecular energies and forces. As astarting point for our analysis, we demonstrate how to train a Transformer tocompetitive energy and force mean absolute errors under a matched trainingcompute budget, relative to a state-of-the-art equivariant GNN on the OMol25dataset. We discover that the Transformer learns physically consistentpatterns$\unicode{x2013}$such as attention weights that decay inversely withinteratomic distance$\unicode{x2013}$and flexibly adapts them across differentmolecular environments due to the absence of hard-coded biases. The use of astandard Transformer also unlocks predictable improvements with respect toscaling training resources, consistent with empirical scaling laws observed inother domains. Our results demonstrate that many favorable properties of GNNscan emerge adaptively in Transformers, challenging the necessity of hard-codedgraph inductive biases and pointing toward standardized, scalable architecturesfor molecular modeling.</description><author>Tobias Kreiman, Yutong Bai, Fadi Atieh, Elizabeth Weaver, Eric Qu, Aditi S. Krishnapriyan</author><pubDate>Thu, 02 Oct 2025 17:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02259v1</guid></item><item><title>Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation</title><link>http://arxiv.org/abs/2507.20210v2</link><description>News recommendation systems play a critical role in alleviating informationoverload by delivering personalized content. A key challenge lies in jointlymodeling multi-view representations of news articles and capturing the dynamic,dual-scale nature of user interests-encompassing both short- and long-termpreferences. Prior methods often rely on single-view features or insufficientlymodel user behavior across time. In this work, we introduce Co-NAML-LSTUR, ahybrid news recommendation framework that integrates NAML for attentivemulti-view news encoding and LSTUR for hierarchical user modeling, designed fortraining on limited data resources. Our approach leverages BERT-basedembeddings to enhance semantic representation. We evaluate Co-NAML-LSTUR on twowidely used benchmarks, MIND-small and MIND-large. Results show that our modelsignificantly outperforms strong baselines, achieving improvements over NRMS by1.55% in AUC and 1.15% in MRR, and over NAML by 2.45% in AUC and 1.71% in MRR.These findings highlight the effectiveness of our efficiency-focused hybridmodel, which combines multi-view news modeling with dual-scale userrepresentations for practical, resource-limited resources rather than a claimto absolute state-of-the-art (SOTA). The implementation of our model ispublicly available at https://github.com/MinhNguyenDS/Co-NAML-LSTUR</description><author>Minh Hoang Nguyen, Thuat Thien Nguyen, Minh Nhat Ta, Tung Le, Huy Tien Nguyen</author><pubDate>Thu, 02 Oct 2025 17:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.20210v2</guid></item><item><title>NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields</title><link>http://arxiv.org/abs/2405.18213v4</link><description>Sound plays a major role in human perception. Along with vision, it providesessential information for understanding our surroundings. Despite advances inneural implicit representations, learning acoustics that align with visualscenes remains a challenge. We propose NeRAF, a method that jointly learnsacoustic and radiance fields. NeRAF synthesizes both novel views andspatialized room impulse responses (RIR) at new positions by conditioning theacoustic field on 3D scene geometric and appearance priors from the radiancefield. The generated RIR can be applied to auralize any audio signal. Eachmodality can be rendered independently and at spatially distinct positions,offering greater versatility. We demonstrate that NeRAF generates high-qualityaudio on SoundSpaces and RAF datasets, achieving significant performanceimprovements over prior methods while being more data-efficient. Additionally,NeRAF enhances novel view synthesis of complex scenes trained with sparse datathrough cross-modal learning. NeRAF is designed as a Nerfstudio module,providing convenient access to realistic audio-visual generation.</description><author>Amandine Brunetto, Sascha Hornauer, Fabien Moutarde</author><pubDate>Thu, 02 Oct 2025 17:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18213v4</guid></item><item><title>Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching</title><link>http://arxiv.org/abs/2410.18436v4</link><description>Recent large language models (LLMs) demonstrate multilingual abilities, yetthey are English-centric due to dominance of English in training corpora. Thelimited resource for low-resource languages remains a crucial challenge.Code-switching (CS), a phenomenon where multilingual speakers alternate betweenlanguages in a discourse, can convey subtle cultural and linguistic nuancesthat can be otherwise lost in translation and elicits language-specificknowledge in human communications. In light of this, we investigate whethercode-switching can activate, or identify and leverage knowledge for reasoningwhen LLMs solve low-resource language tasks. To facilitate the research, wefirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.We provide comprehensive analysis on a variety of multilingual LLMs bysubdividing activation process into knowledge identification and knowledgeleveraging. Our results demonstrate that compared to English text, CS canfaithfully activate knowledge inside LLMs especially on language-specificdomains, suggesting the potential of code-switching on low-resource languagetasks.</description><author>Seoyeon Kim, Huiseo Kim, Chanjun Park, Jinyoung Yeo, Dongha Lee</author><pubDate>Thu, 02 Oct 2025 17:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18436v4</guid></item><item><title>DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</title><link>http://arxiv.org/abs/2510.02253v1</link><description>Drag-based image editing has long suffered from distortions in the targetregion, largely because the priors of earlier base models, Stable Diffusion,are insufficient to project optimized latents back onto the natural imagemanifold. With the shift from UNet-based DDPMs to more scalable DiT with flowmatching (e.g., SD3.5, FLUX), generative priors have become significantlystronger, enabling advances across diverse editing tasks. However, drag-basedediting has yet to benefit from these stronger priors. This work proposes thefirst framework to effectively harness FLUX's rich prior for drag-basedediting, dubbed DragFlow, achieving substantial gains over baselines. We firstshow that directly applying point-based drag editing to DiTs performs poorly:unlike the highly compressed features of UNets, DiT features are insufficientlystructured to provide reliable guidance for point-wise motion supervision. Toovercome this limitation, DragFlow introduces a region-based editing paradigm,where affine transformations enable richer and more consistent featuresupervision. Additionally, we integrate pretrained open-domain personalizationadapters (e.g., IP-Adapter) to enhance subject consistency, while preservingbackground fidelity through gradient mask-based hard constraints. Multimodallarge language models (MLLMs) are further employed to resolve task ambiguities.For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)featuring region-level dragging instructions. Extensive experiments onDragBench-DR and ReD Bench show that DragFlow surpasses both point-based andregion-based baselines, setting a new state-of-the-art in drag-based imageediting. Code and datasets will be publicly available upon publication.</description><author>Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</author><pubDate>Thu, 02 Oct 2025 17:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02253v1</guid></item><item><title>The Unreasonable Effectiveness of Scaling Agents for Computer Use</title><link>http://arxiv.org/abs/2510.02250v1</link><description>Computer-use agents (CUAs) hold promise for automating everyday digitaltasks, but their unreliability and high variance hinder their application tolong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a methodthat scales over agents by generating multiple rollouts and selecting amongthem using behavior narratives that describe the agents' rollouts. It enablesboth wide exploration and principled trajectory selection, substantiallyimproving robustness and success rates. On OSWorld, our bBoN scaling methodestablishes a new state of the art (SoTA) at 69.9%, significantly outperformingprior methods and approaching human-level performance at 72%, withcomprehensive ablations validating key design choices. We further demonstratestrong generalization results to different operating systems onWindowsAgentArena and AndroidWorld. Crucially, our results highlight theunreasonable effectiveness of scaling CUAs, when you do it right: effectivescaling requires structured trajectory understanding and selection, and bBoNprovides a practical framework to achieve this.</description><author>Gonzalo Gonzalez-Pumariega, Vincent Tu, Chih-Lun Lee, Jiachen Yang, Ang Li, Xin Eric Wang</author><pubDate>Thu, 02 Oct 2025 17:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02250v1</guid></item><item><title>Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation</title><link>http://arxiv.org/abs/2510.02249v1</link><description>Large Language Models (LLMs) have demonstrated remarkable reasoning abilitieson complex problems using long Chain-of-Thought (CoT) reasoning. However, theyoften suffer from overthinking, meaning generating unnecessarily lengthyreasoning steps for simpler problems. This issue may degrade the efficiency ofthe models and make them difficult to adapt the reasoning depth to thecomplexity of problems. To address this, we introduce a novel metric TokenEntropy Cumulative Average (TECA), which measures the extent of explorationthroughout the reasoning process. We further propose a novel reasoning paradigm-- Explore Briefly, Then Decide -- with an associated Cumulative EntropyRegulation (CER) mechanism. This paradigm leverages TECA to help the modeldynamically determine the optimal point to conclude its thought process andprovide a final answer, thus achieving efficient reasoning. Experimentalresults across diverse mathematical benchmarks show that our approachsubstantially mitigates overthinking without sacrificing problem-solvingability. With our thinking paradigm, the average response length decreases byup to 71% on simpler datasets, demonstrating the effectiveness of our method increating a more efficient and adaptive reasoning process.</description><author>Tianyi Jiang, Yi Bin, Yujuan Ding, Kainian Zhu, Fei Ma, Jingkuan Song, Heng Tao Shen</author><pubDate>Thu, 02 Oct 2025 17:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02249v1</guid></item><item><title>Probabilistic Reasoning with LLMs for k-anonymity Estimation</title><link>http://arxiv.org/abs/2503.09674v4</link><description>Probabilistic reasoning is a key aspect of both human and artificialintelligence that allows for handling uncertainty and ambiguity indecision-making. In this paper, we introduce a new numerical reasoning taskunder uncertainty for large language models, focusing on estimating the privacyrisk of user-generated documents containing privacy-sensitive information. Wepropose BRANCH, a new LLM methodology that estimates the k-privacy value of atext-the size of the population matching the given information. BRANCHfactorizes a joint probability distribution of personal information as randomvariables. The probability of each factor in a population is estimatedseparately using a Bayesian network and combined to compute the final k-value.Our experiments show that this method successfully estimates the k-value 73% ofthe time, a 13% increase compared to o3-mini with chain-of-thought reasoning.We also find that LLM uncertainty is a good indicator for accuracy, ashigh-variance predictions are 37.47% less accurate on average.</description><author>Jonathan Zheng, Sauvik Das, Alan Ritter, Wei Xu</author><pubDate>Thu, 02 Oct 2025 17:36:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.09674v4</guid></item><item><title>Learning to Weight Parameters for Training Data Attribution</title><link>http://arxiv.org/abs/2506.05647v2</link><description>We study gradient-based data attribution, aiming to identify which trainingexamples most influence a given output. Existing methods for this task eithertreat network parameters uniformly or rely on implicit weighting derived fromHessian approximations, which do not fully model functional heterogeneity ofnetwork parameters. To address this, we propose a method to explicitly learnparameter importance weights directly from data, without requiring annotatedlabels. Our approach improves attribution accuracy across diverse tasks,including image classification, language modeling, and diffusion, and enablesfine-grained attribution for concepts like subject and style.</description><author>Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann</author><pubDate>Thu, 02 Oct 2025 17:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05647v2</guid></item><item><title>GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction</title><link>http://arxiv.org/abs/2509.25075v2</link><description>Cryo-electron microscopy (cryo-EM) has become a central tool forhigh-resolution structural biology, yet the massive scale of datasets (oftenexceeding 100k particle images) renders 3D reconstruction both computationallyexpensive and memory intensive. Traditional Fourier-space methods are efficientbut lose fidelity due to repeated transforms, while recent real-spaceapproaches based on neural radiance fields (NeRFs) improve accuracy but incurcubic memory and computation overhead. Therefore, we introduce GEM, a novelcryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) thatoperates directly in real-space while maintaining high efficiency. Instead ofmodeling the entire density volume, GEM represents proteins with compact 3DGaussians, each parameterized by only 11 values. To further improve thetraining efficiency, we designed a novel gradient computation to 3D Gaussiansthat contribute to each voxel. This design substantially reduced both memoryfootprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to48% faster training and 12% lower memory usage compared to state-of-the-artmethods, while improving local resolution by as much as 38.8%. These resultsestablish GEM as a practical and scalable paradigm for cryo-EM reconstruction,unifying speed, efficiency, and high-resolution accuracy. Our code is availableat https://github.com/UNITES-Lab/GEM.</description><author>Huaizhi Qu, Xiao Wang, Gengwei Zhang, Jie Peng, Tianlong Chen</author><pubDate>Thu, 02 Oct 2025 17:31:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.25075v2</guid></item><item><title>Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier</title><link>http://arxiv.org/abs/2504.00178v2</link><description>Pre-tokenization, the initial step in many modern tokenization pipelines,segments text into smaller units called pretokens, typically splitting onwhitespace and punctuation. While this process encourages having full,individual words as tokens, it introduces a fundamental limitation in mosttokenization algorithms such as Byte Pair Encoding (BPE). Specifically,pre-tokenization causes the distribution of tokens in a corpus to heavily skewtowards common, full-length words. This skewed distribution limits the benefitsof expanding to larger vocabularies, since the additional tokens appear withprogressively lower counts. To overcome this barrier, we propose BoundlessBPE,a modified BPE algorithm that relaxes the pretoken boundary constraint. Ourapproach selectively merges two complete pretokens into a larger unit we term asuperword. Superwords are not necessarily semantically cohesive. For example,the pretokens " of" and " the" might be combined to form the superword " ofthe". This merging strategy results in a substantially more uniformdistribution of tokens across a corpus than standard BPE, and compresses textmore effectively, with up to a 15% increase in bytes per token.</description><author>Craig W. Schmidt, Varshini Reddy, Chris Tanner, Yuval Pinter</author><pubDate>Thu, 02 Oct 2025 17:31:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.00178v2</guid></item><item><title>ExGRPO: Learning to Reason from Experience</title><link>http://arxiv.org/abs/2510.02245v1</link><description>Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigmfor improving the reasoning ability of large language models. However, standardon-policy training discards rollout experiences after a single update, leadingto computational inefficiency and instability. While prior work on RL hashighlighted the benefits of reusing past experience, the role of experiencecharacteristics in shaping learning dynamics of large reasoning models remainsunderexplored. In this paper, we are the first to investigate what makes areasoning experience valuable and identify rollout correctness and entropy aseffective indicators of experience value. Based on these insights, we proposeExGRPO (Experiential Group Relative Policy Optimization), a framework thatorganizes and prioritizes valuable experiences, and employs a mixed-policyobjective to balance exploration with experience exploitation. Experiments onfive backbone models (1.5B-8B parameters) show that ExGRPO consistentlyimproves reasoning performance on mathematical/general benchmarks, with anaverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPOstabilizes training on both stronger and weaker models where on-policy methodsfail. These results highlight principled experience management as a keyingredient for efficient and scalable RLVR.</description><author>Runzhe Zhan, Yafu Li, Zhi Wang, Xiaoye Qu, Dongrui Liu, Jing Shao, Derek F. Wong, Yu Cheng</author><pubDate>Thu, 02 Oct 2025 17:31:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02245v1</guid></item><item><title>AccurateRAG: A Framework for Building Accurate Retrieval-Augmented Question-Answering Applications</title><link>http://arxiv.org/abs/2510.02243v1</link><description>We introduce AccurateRAG -- a novel framework for constructinghigh-performance question-answering applications based on retrieval-augmentedgeneration (RAG). Our framework offers a pipeline for development efficiencywith tools for raw dataset processing, fine-tuning data generation, textembedding &amp; LLM fine-tuning, output evaluation, and building RAG systemslocally. Experimental results show that our framework outperforms previousstrong baselines and obtains new state-of-the-art question-answeringperformance on benchmark datasets.</description><author>Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Van-Cuong Pham, Hoang Ngo, Dat Quoc Nguyen</author><pubDate>Thu, 02 Oct 2025 17:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02243v1</guid></item><item><title>Study on LLMs for Promptagator-Style Dense Retriever Training</title><link>http://arxiv.org/abs/2510.02241v1</link><description>Promptagator demonstrated that Large Language Models (LLMs) with few-shotprompts can be used as task-specific query generators for fine-tuningdomain-specialized dense retrieval models. However, the original Promptagatorapproach relied on proprietary and large-scale LLMs which users may not haveaccess to or may be prohibited from using with sensitive data. In this work, westudy the impact of open-source LLMs at accessible scales ($\leq$14Bparameters) as an alternative. Our results demonstrate that open-source LLMs assmall as 3B parameters can serve as effective Promptagator-style querygenerators. We hope our work will inform practitioners with reliablealternatives for synthetic data generation and give insights to maximizefine-tuning results for domain-specific applications.</description><author>Daniel Gwon, Nour Jedidi, Jimmy Lin</author><pubDate>Thu, 02 Oct 2025 17:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02241v1</guid></item><item><title>RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning</title><link>http://arxiv.org/abs/2510.02240v1</link><description>Fine-grained visual reasoning remains a core challenge for multimodal largelanguage models (MLLMs). The recently introduced ReasonMap highlights this gapby showing that even advanced MLLMs struggle with spatial reasoning instructured and information-rich settings such as transit maps, a task of clearpractical and scientific importance. However, standard reinforcement learning(RL) on such tasks is impeded by sparse rewards and unstable optimization. Toaddress this, we first construct ReasonMap-Plus, an extended dataset thatintroduces dense reward signals through Visual Question Answering (VQA) tasks,enabling effective cold-start training of fine-grained visual understandingskills. Next, we propose RewardMap, a multi-stage RL framework designed toimprove both visual understanding and reasoning capabilities of MLLMs.RewardMap incorporates two key designs. First, we introduce a difficulty-awarereward design that incorporates detail rewards, directly tackling the sparserewards while providing richer supervision. Second, we propose a multi-stage RLscheme that bootstraps training from simple perception to complex reasoningtasks, offering a more effective cold-start strategy than conventionalSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plusdemonstrate that each component of RewardMap contributes to consistentperformance gains, while their combination yields the best results. Moreover,models trained with RewardMap achieve an average improvement of 3.47% across 6benchmarks spanning spatial reasoning, fine-grained visual reasoning, andgeneral tasks beyond transit maps, underscoring enhanced visual understandingand reasoning capabilities.</description><author>Sicheng Feng, Kaiwen Tuo, Song Wang, Lingdong Kong, Jianke Zhu, Huan Wang</author><pubDate>Thu, 02 Oct 2025 17:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02240v1</guid></item><item><title>FalconWing: An Ultra-Light Indoor Fixed-Wing UAV Platform for Vision-Based Autonomy</title><link>http://arxiv.org/abs/2505.01383v2</link><description>We introduce FalconWing, an ultra-light (150 g) indoor fixed-wing UAVplatform for vision-based autonomy. Controlled indoor environment enablesyear-round repeatable UAV experiment but imposes strict weight andmaneuverability limits on the UAV, motivating our ultra-light FalconWingdesign. FalconWing couples a lightweight hardware stack (137g airframe with a9g camera) and offboard computation with a software stack featuring aphotorealistic 3D Gaussian Splat (GSplat) simulator for developing andevaluating vision-based controllers. We validate FalconWing on two challengingvision-based aerial case studies. In the leader-follower case study, our bestvision-based controller, trained via imitation learning on GSplat-rendered dataaugmented with domain randomization, achieves 100% tracking success across 3types of leader maneuvers over 30 trials and shows robustness to leader'sappearance shifts in simulation. In the autonomous landing case study, ourvision-based controller trained purely in simulation transfers zero-shot toreal hardware, achieving an 80% success rate over ten landing trials. We willrelease hardware designs, GSplat scenes, and dynamics models upon publicationto make FalconWing an open-source flight kit for engineering students andresearch labs.</description><author>Yan Miao, Will Shen, Hang Cui, Sayan Mitra</author><pubDate>Thu, 02 Oct 2025 17:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.01383v2</guid></item><item><title>Drop-Muon: Update Less, Converge Faster</title><link>http://arxiv.org/abs/2510.02239v1</link><description>Conventional wisdom in deep learning optimization dictates updating alllayers at every step-a principle followed by all recent state-of-the-artoptimizers such as Muon. In this work, we challenge this assumption, showingthat full-network updates can be fundamentally suboptimal, both in theory andin practice. We introduce a non-Euclidean Randomized Progressive Trainingmethod-Drop-Muon-a simple yet powerful framework that updates only a subset oflayers per step according to a randomized schedule, combining the efficiency ofprogressive training with layer-specific non-Euclidean updates for top-tierperformance. We provide rigorous convergence guarantees under both layer-wisesmoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic andstochastic gradient settings, marking the first such results for progressivetraining in the stochastic and non-smooth regime. Our cost analysis furtherreveals that full-network updates are not optimal unless a very specificrelationship between layer smoothness constants holds. Through controlled CNNexperiments, we empirically demonstrate that Drop-Muon consistently outperformsfull-network Muon, achieving the same accuracy up to $1.4\times$ faster inwall-clock time. Together, our results suggest a shift in how large-scalemodels can be efficiently trained, challenging the status quo and offering ahighly efficient, theoretically grounded alternative to full-network updates.</description><author>Kaja Gruntkowska, Yassine Maziane, Zheng Qu, Peter Richtrik</author><pubDate>Thu, 02 Oct 2025 17:28:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02239v1</guid></item><item><title>AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features</title><link>http://arxiv.org/abs/2510.00404v2</link><description>Sparse autoencoders (SAEs) have emerged as powerful techniques forinterpretability of large language models (LLMs), aiming to decompose hiddenstates into meaningful semantic features. While several SAE variants have beenproposed, there remains no principled framework to derive SAEs from theoriginal dictionary learning formulation. In this work, we introduce such aframework by unrolling the proximal gradient method for sparse coding. We showthat a single-step update naturally recovers common SAE variants, includingReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitationof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,preventing a single feature from representing bidirectional concepts (e.g.,male vs. female). This structural constraint fragments semantic axes intoseparate, redundant features, limiting representational completeness. Toaddress this issue, we propose AbsTopK SAE, a new variant derived from the$\ell_0$ sparsity constraint that applies hard thresholding over thelargest-magnitude activations. By preserving both positive and negativeactivations, AbsTopK uncovers richer, bidirectional conceptual representations.Comprehensive experiments across four LLMs and seven probing and steering tasksshow that AbsTopK improves reconstruction fidelity, enhances interpretability,and enables single features to encode contrasting concepts. Remarkably, AbsTopKmatches or even surpasses the Difference-in-Mean method, a supervised approachthat requires labeled data for each concept and has been shown in prior work tooutperform SAEs.</description><author>Xudong Zhu, Mohammad Mahdi Khalili, Zhihui Zhu</author><pubDate>Thu, 02 Oct 2025 17:28:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.00404v2</guid></item><item><title>Forecasting Generative Amplification</title><link>http://arxiv.org/abs/2509.08048v2</link><description>Generative networks are perfect tools to enhance the speed and precision ofLHC simulations. It is important to understand their statistical precision,especially when generating events beyond the size of the training dataset. Wepresent two complementary methods to estimate the amplification factor withoutlarge holdout datasets. Averaging amplification uses Bayesian networks orensembling to estimate amplification from the precision of integrals over givenphase-space volumes. Differential amplification uses hypothesis testing toquantify amplification without any resolution loss. Applied to state-of-the-artevent generators, both methods indicate that amplification is possible inspecific regions of phase space, but not yet across the entire distribution.</description><author>Henning Bahl, Sascha Diefenbacher, Nina Elmer, Tilman Plehn, Jonas Spinner</author><pubDate>Thu, 02 Oct 2025 17:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.08048v2</guid></item><item><title>PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks</title><link>http://arxiv.org/abs/2510.02236v1</link><description>Network Slices (NSs) are virtual networks operating over a shared physicalinfrastructure, each designed to meet specific application requirements whilemaintaining consistent Quality of Service (QoS). In Fifth Generation (5G)networks, User Equipment (UE) can connect to and seamlessly switch betweenmultiple NSs to access diverse services. However, this flexibility, known asInter-Slice Switching (ISS), introduces a potential vulnerability that can beexploited to launch Distributed Slice Mobility (DSM) attacks, a form ofDistributed Denial of Service (DDoS) attack. To secure 5G networks and theirNSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; ananomaly detection solution that leverages Positive Unlabeled Learning (PUL) andincorporates a combination of Long Short-Term Memory Autoencoders and K-Meansclustering. PUL-Inter-Slice Defender leverages the Third Generation PartnershipProject (3GPP) key performance indicators and performance measurement countersas features for its machine learning models to detect DSM attack variants whilemaintaining robustness in the presence of contaminated training data. Whenevaluated on data collected from our 5G testbed based on the open-sourcefree5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on trainingdatasets with 10% to 40% attack contamination, consistently outperforming itscounterpart Inter-Slice Defender and other PUL based solutions combiningOne-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.</description><author>Ricardo Misael Ayala Molina, Hyame Assem Alameddine, Makan Pourzandi, Chadi Assi</author><pubDate>Thu, 02 Oct 2025 17:24:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02236v1</guid></item><item><title>Adapting Large Language Models for Character-based Augmentative and Alternative Communication</title><link>http://arxiv.org/abs/2501.10582v3</link><description>Users of Augmentative and Alternative Communication (AAC) may writeletter-by-letter via an interface that uses a character language model.However, most state-of-the-art large pretrained language models predict subwordtokens of variable length. We investigate how to practically use such models tomake accurate and efficient character predictions. Our algorithm for producingcharacter predictions from a subword large language model (LLM) provides moreaccurate predictions than using a classification layer, a byte-level LLM, or ann-gram model. Additionally, we investigate a domain adaptation procedure basedon a large dataset of sentences we curated based on scoring how useful eachsentence might be for spoken or written AAC communication. We find ourprocedure further improves model performance on simple, conversational text.</description><author>Dylan Gaines, Keith Vertanen</author><pubDate>Thu, 02 Oct 2025 17:20:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.10582v3</guid></item><item><title>Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches</title><link>http://arxiv.org/abs/2510.02232v1</link><description>Recent technological advances in smartphones and communications, includingthe growth of such online platforms as massive social media networks such as X(formerly known as Twitter) endangers young people and their emotionalwell-being by exposing them to cyberbullying, taunting, and bullying content.Most proposed approaches for automatically detecting cyberbullying have beendeveloped around the English language, and methods for detectingArabic-language cyberbullying are scarce. Methods for detecting Arabic-languagecyberbullying are especially scarce. This paper aims to enhance theeffectiveness of methods for detecting cyberbullying in Arabic-languagecontent. We assembled a dataset of 10,662 X posts, pre-processed the data, andused the kappa tool to verify and enhance the quality of our annotations. Weconducted four experiments to test numerous deep learning models forautomatically detecting Arabic-language cyberbullying. We first tested a longshort-term memory (LSTM) model and a bidirectional long short-term memory(Bi-LSTM) model with several experimental word embeddings. We also tested theLSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder fromrepresentations (BERT) and then tested them on a different experimental modelsBERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTMwith FastText embedding word performed even better, achieving 98% accuracy. Asa result, the outcomes are generalize</description><author>Ebtesam Jaber Aljohani, Wael M. S. Yafoo</author><pubDate>Thu, 02 Oct 2025 17:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02232v1</guid></item><item><title>Unraveling Indirect In-Context Learning Using Influence Functions</title><link>http://arxiv.org/abs/2501.01473v3</link><description>In this work, we introduce a novel paradigm for generalized In-ContextLearning (ICL), termed Indirect In-Context Learning. In Indirect ICL, weexplore demonstration selection strategies tailored for two distinct real-worldscenarios: Mixture of Tasks and Noisy ICL. We systematically evaluate theeffectiveness of Influence Functions (IFs) as a selection tool for thesesettings, highlighting the potential of IFs to better capture theinformativeness of examples within the demonstration pool. For the Mixture ofTasks setting, demonstrations are drawn from 28 diverse tasks, including MMLU,BigBench, StrategyQA, and CommonsenseQA. We demonstrate that combiningBertScore-Recall (BSR) with an IF surrogate model can further improveperformance, leading to average absolute accuracy gains of 0.37\% and 1.45\%for 3-shot and 5-shot setups when compared to traditional ICL metrics. In theNoisy ICL setting, we examine scenarios where demonstrations might bemislabeled or have adversarial noise. Our experiments show that reweightingtraditional ICL selectors (BSR and Cosine Similarity) with IF-based selectorsboosts accuracy by an average of 2.90\% for Cosine Similarity and 2.94\% forBSR on noisy GLUE benchmarks. For the adversarial sub-setting, we show theutility of using IFs for task-agnostic demonstration selection for backdoorattack mitigation. Showing a 32.89\% reduction in Attack Success Rate comparedto task-aware methods. In sum, we propose a robust framework for demonstrationselection that generalizes beyond traditional ICL, offering valuable insightsinto the role of IFs for Indirect ICL.</description><author>Hadi Askari, Shivanshu Gupta, Terry Tong, Fei Wang, Anshuman Chhabra, Muhao Chen</author><pubDate>Thu, 02 Oct 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01473v3</guid></item><item><title>The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models</title><link>http://arxiv.org/abs/2510.02230v1</link><description>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a keymethod for improving Large Language Models' reasoning capabilities, yet recentevidence suggests it may paradoxically shrink the reasoning boundary ratherthan expand it. This paper investigates the shrinkage issue of RLVR byanalyzing its learning dynamics and reveals two critical phenomena that explainthis failure. First, we expose negative interference in RLVR, where learning tosolve certain training problems actively reduces the likelihood of correctsolutions for others, leading to the decline of Pass@$k$ performance, or theprobability of generating a correct solution within $k$ attempts. Second, weuncover the winner-take-all phenomenon: RLVR disproportionately reinforcesproblems with high likelihood, correct solutions, under the base model, whilesuppressing other initially low-likelihood ones. Through extensive theoreticaland empirical analysis on multiple mathematical reasoning benchmarks, we showthat this effect arises from the inherent on-policy sampling in standard RLobjectives, causing the model to converge toward narrow solution strategies.Based on these insights, we propose a simple yet effective data curationalgorithm that focuses RLVR learning on low-likelihood problems, achievingnotable improvement in Pass@$k$ performance. Our code is available athttps://github.com/mail-research/SELF-llm-interference.</description><author>Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan</author><pubDate>Thu, 02 Oct 2025 17:17:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02230v1</guid></item><item><title>xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity</title><link>http://arxiv.org/abs/2510.02228v1</link><description>Scaling laws play a central role in the success of Large Language Models(LLMs), enabling the prediction of model performance relative to computebudgets prior to training. While Transformers have been the dominantarchitecture, recent alternatives such as xLSTM offer linear complexity withrespect to context length while remaining competitive in the billion-parameterregime. We conduct a comparative investigation on the scaling behavior ofTransformers and xLSTM along the following lines, providing insights to guidefuture model design and deployment. First, we study the scaling behavior forxLSTM in compute-optimal and over-training regimes using both IsoFLOP andparametric fit approaches on a wide range of model sizes (80M-7B) and number oftraining tokens (2B-2T). Second, we examine the dependence of optimal modelsizes on context length, a pivotal aspect that was largely ignored in previouswork. Finally, we analyze inference-time scaling characteristics. Our findingsreveal that in typical LLM training and inference scenarios, xLSTM scalesfavorably compared to Transformers. Importantly, xLSTM's advantage widens astraining and inference contexts grow.</description><author>Maximilian Beck, Kajetan Schweighofer, Sebastian Bck, Sebastian Lehner, Sepp Hochreiter</author><pubDate>Thu, 02 Oct 2025 17:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02228v1</guid></item><item><title>More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration</title><link>http://arxiv.org/abs/2510.02227v1</link><description>Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigmfor enhancing the reasoning ability in Large Language Models (LLMs). However,prevailing methods primarily rely on self-exploration or a single off-policyteacher to elicit long chain-of-thought (LongCoT) reasoning, which mayintroduce intrinsic model biases and restrict exploration, ultimately limitingreasoning diversity and performance. Drawing inspiration from multi-teacherstrategies in knowledge distillation, we introduce Adaptive Multi-GuidancePolicy Optimization (AMPO), a novel framework that adaptively leveragesguidance from multiple proficient teacher models, but only when the on-policymodel fails to generate correct solutions. This "guidance-on-demand" approachexpands exploration while preserving the value of self-discovery. Moreover,AMPO incorporates a comprehension-based selection mechanism, prompting thestudent to learn from the reasoning paths that it is most likely to comprehend,thus balancing broad exploration with effective exploitation. Extensiveexperiments show AMPO substantially outperforms a strong baseline (GRPO), witha 4.3% improvement on mathematical reasoning tasks and 12.2% onout-of-distribution tasks, while significantly boosting Pass@k performance andenabling more diverse exploration. Notably, using four peer-sized teachers, ourmethod achieves comparable results to approaches that leverage a single, morepowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstratea more efficient and scalable path to superior reasoning and generalizability.Our code is available at https://github.com/SII-Enigma/AMPO.</description><author>Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Hengtao Shen</author><pubDate>Thu, 02 Oct 2025 17:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02227v1</guid></item><item><title>TempoControl: Temporal Attention Guidance for Text-to-Video Models</title><link>http://arxiv.org/abs/2510.02226v1</link><description>Recent advances in generative video models have enabled the creation ofhigh-quality videos based on natural language prompts. However, these modelsfrequently lack fine-grained temporal control, meaning they do not allow usersto specify when particular visual elements should appear within a generatedsequence. In this work, we introduce TempoControl, a method that allows fortemporal alignment of visual concepts during inference, without requiringretraining or additional supervision. TempoControl utilizes cross-attentionmaps, a key component of text-to-video diffusion models, to guide the timing ofconcepts through a novel optimization approach. Our method steers attentionusing three complementary principles: aligning its temporal shape with acontrol signal (via correlation), amplifying it where visibility is needed (viaenergy), and maintaining spatial focus (via entropy). TempoControl allowsprecise control over timing while ensuring high video quality and diversity. Wedemonstrate its effectiveness across various video generation applications,including temporal reordering for single and multiple objects, as well asaction and audio-aligned generation.</description><author>Shira Schiber, Ofir Lindenbaum, Idan Schwartz</author><pubDate>Thu, 02 Oct 2025 17:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02226v1</guid></item><item><title>Segmentor-Guided Counterfactual Fine-Tuning for Locally Coherent and Targeted Image Synthesis</title><link>http://arxiv.org/abs/2509.24913v2</link><description>Counterfactual image generation is a powerful tool for augmenting trainingdata, de-biasing datasets, and modeling disease. Current approaches rely onexternal classifiers or regressors to increase the effectiveness ofsubject-level interventions (e.g., changing the patient's age). Forstructure-specific interventions (e.g., changing the area of the left lung in achest radiograph), we show that this is insufficient, and can result inundesirable global effects across the image domain. Previous work usedpixel-level label maps as guidance, requiring a user to provide hypotheticalsegmentations which are tedious and difficult to obtain. We proposeSegmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves thesimplicity of intervening on scalar-valued, structure-specific variables whileproducing locally coherent and effective counterfactuals. We demonstrate thecapability of generating realistic chest radiographs, and we show promisingresults for modeling coronary artery disease. Code:https://github.com/biomedia-mira/seg-cft.</description><author>Tian Xia, Matthew Sinclair, Andreas Schuh, Fabio De Sousa Ribeiro, Raghav Mehta, Rajat Rasal, Esther Puyol-Antn, Samuel Gerber, Kersten Petersen, Michiel Schaap, Ben Glocker</author><pubDate>Thu, 02 Oct 2025 17:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.24913v2</guid></item><item><title>Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models</title><link>http://arxiv.org/abs/2510.02224v1</link><description>Many time series applications require access to multi-step forecasttrajectories in the form of sample paths. Recently, time series foundationmodels have leveraged multi-step lookahead predictions to improve the qualityand efficiency of multi-step forecasts. However, these models only predictindependent marginal distributions for each time step, rather than a full jointpredictive distribution. To generate forecast sample paths with realisticcorrelation structures, one typically resorts to autoregressive sampling, whichcan be extremely expensive. In this paper, we present a copula-based approachto efficiently generate accurate, correlated sample paths from existingmulti-step time series foundation models in one forward pass. Our copula-basedapproach generates correlated sample paths orders of magnitude faster thanautoregressive sampling, and it yields improved sample path quality bymitigating the snowballing error phenomenon.</description><author>Ethan Baron, Boris Oreshkin, Ruijun Ma, Hanyu Zhang, Kari Torkkola, Michael W. Mahoney, Andrew Gordon Wilson, Tatiana Konstantinova</author><pubDate>Thu, 02 Oct 2025 17:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02224v1</guid></item><item><title>Quantum Fisher information matrices from Rnyi relative entropies</title><link>http://arxiv.org/abs/2510.02218v1</link><description>Quantum generalizations of the Fisher information are important in quantuminformation science, with applications in high energy and condensed matterphysics and in quantum estimation theory, machine learning, and optimization.One can derive a quantum generalization of the Fisher information matrix in anatural way as the Hessian matrix arising in a Taylor expansion of a smoothdivergence. Such an approach is appealing for quantum information theorists,given the ubiquity of divergences in quantum information theory. In contrast tothe classical case, there is not a unique quantum generalization of the Fisherinformation matrix, similar to how there is not a unique quantum generalizationof the relative entropy or the R\'enyi relative entropy. In this paper, Iderive information matrices arising from the log-Euclidean, $\alpha$-$z$, andgeometric R\'enyi relative entropies, with the main technical tool for doing sobeing the method of divided differences for calculating matrix derivatives.Interestingly, for all non-negative values of the R\'enyi parameter $\alpha$,the log-Euclidean R\'enyi relative entropy leads to the Kubo-Mori informationmatrix, and the geometric R\'enyi relative entropy leads to theright-logarithmic derivative Fisher information matrix. Thus, the resultinginformation matrices obey the data-processing inequality for all non-negativevalues of the R\'enyi parameter $\alpha$ even though the original quantities donot. Additionally, I derive and establish basic properties of $\alpha$-$z$information matrices resulting from the $\alpha$-$z$ R\'enyi relativeentropies. For parameterized thermal states, I establish formulas for their$\alpha$-$z$ information matrices and hybrid quantum-classical algorithms forestimating them, with applications in quantum Boltzmann machine learning.</description><author>Mark M. Wilde</author><pubDate>Thu, 02 Oct 2025 17:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02218v1</guid></item><item><title>Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification</title><link>http://arxiv.org/abs/2510.02216v1</link><description>Imputation methods play a critical role in enhancing the quality of practicaltime-series data, which often suffer from pervasive missing values. Recently,diffusion-based generative imputation methods have demonstrated remarkablesuccess compared to autoregressive and conventional statistical approaches.Despite their empirical success, the theoretical understanding of how welldiffusion-based models capture complex spatial and temporal dependenciesbetween the missing values and observed ones remains limited. Our workaddresses this gap by investigating the statistical efficiency of conditionaldiffusion transformers for imputation and quantifying the uncertainty inmissing values. Specifically, we derive statistical sample complexity boundsbased on a novel approximation theory for conditional score functions usingtransformers, and, through this, construct tight confidence regions for missingvalues. Our findings also reveal that the efficiency and accuracy of imputationare significantly influenced by the missing patterns. Furthermore, we validatethese theoretical insights through simulation and propose a mixed-maskingtraining strategy to enhance the imputation performance.</description><author>Zeqi Ye, Minshuo Chen</author><pubDate>Thu, 02 Oct 2025 17:00:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02216v1</guid></item><item><title>C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale Recommendation Systems</title><link>http://arxiv.org/abs/2510.02215v1</link><description>Training large-scale recommendation models under a single global objectiveimplicitly assumes homogeneity across user populations. However, real-worlddata are composites of heterogeneous cohorts with distinct conditionaldistributions. As models increase in scale and complexity and as more data isused for training, they become dominated by central distribution patterns,neglecting head and tail regions. This imbalance limits the model's learningability and can result in inactive attention weights or dead neurons. In thispaper, we reveal how the attention mechanism can play a key role infactorization machines for shared embedding selection, and propose to addressthis challenge by analyzing the substructures in the dataset and exposing thosewith strong distributional contrast through auxiliary learning. Unlike previousresearch, which heuristically applies weighted labels or multi-task heads tomitigate such biases, we leverage partially conflicting auxiliary labels toregularize the shared representation. This approach customizes the learningprocess of attention layers to preserve mutual information with minoritycohorts while improving global performance. We evaluated C2AL on massiveproduction datasets with billions of data points each for six SOTA models.Experiments show that the factorization machine is able to capture fine-graineduser-ad interactions using the proposed method, achieving up to a 0.16%reduction in normalized entropy overall and delivering gains exceeding 0.30% ontargeted minority cohorts.</description><author>Mertcan Cokbas, Ziteng Liu, Zeyi Tao, Chengkai Zhang, Elder Veliz, Qin Huang, Ellie Wen, Huayu Li, Qiang Jin, Murat Duman, Benjamin Au, Guy Lebanon, Sagar Chordia</author><pubDate>Thu, 02 Oct 2025 17:00:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02215v1</guid></item><item><title>Multiple Queries with Multiple Keys: A Precise Prompt Matching Paradigm for Prompt-based Continual Learning</title><link>http://arxiv.org/abs/2501.12635v3</link><description>Continual learning requires machine learning models to continuously acquirenew knowledge in dynamic environments while avoiding the forgetting of previousknowledge. Prompt-based continual learning methods effectively address theissue of catastrophic forgetting through prompt expansion and selection.However, existing approaches often suffer from low accuracy in promptselection, which can result in the model receiving biased knowledge and makingbiased predictions. To address this issue, we propose the Multiple Queries withMultiple Keys (MQMK) prompt matching paradigm for precise prompt selection. Thegoal of MQMK is to select the prompts whose training data distribution mostclosely matches that of the test sample. Specifically, Multiple Queries enableprecise breadth search by introducing task-specific knowledge, while MultipleKeys perform deep search by representing the feature distribution of trainingsamples at a fine-grained level. Each query is designed to perform localmatching with a designated task to reduce interference across queries.Experiments show that MQMK enhances the prompt matching rate by over 30\% inchallenging scenarios and achieves state-of-the-art performance on three widelyadopted continual learning benchmarks. The code is available athttps://github.com/DunweiTu/MQMK.</description><author>Dunwei Tu, Huiyu Yi, Yuchi Wang, Baile Xu, Jian Zhao, Furao Shen</author><pubDate>Thu, 02 Oct 2025 16:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12635v3</guid></item><item><title>MMDEW: Multipurpose Multiclass Density Estimation in the Wild</title><link>http://arxiv.org/abs/2510.02213v1</link><description>Density map estimation can be used to estimate object counts in dense andoccluded scenes where discrete counting-by-detection methods fail. We propose amulticategory counting framework that leverages a Twins pyramidvision-transformer backbone and a specialised multi-class counting head builton a state-of-the-art multiscale decoding approach. A two-task design adds asegmentation-based Category Focus Module, suppressing inter-category cross-talkat training time. Training and evaluation on the VisDrone and iSAID benchmarksdemonstrates superior performance versus prior multicategory crowd-countingapproaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11underscores the necessity of crowd counting methods in dense scenes. Themethod's regional loss opens up multi-class crowd counting to new domains,demonstrated through the application to a biodiversity monitoring dataset,highlighting its capacity to inform conservation efforts and enable scalableecological insights.</description><author>Villanelle O'Reilly, Jonathan Cox, Georgios Leontidis, Marc Hanheide, Petra Bosilj, James Brown</author><pubDate>Thu, 02 Oct 2025 16:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02213v1</guid></item><item><title>DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning</title><link>http://arxiv.org/abs/2510.02212v1</link><description>We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unifiedframework for training masked diffusion large language models (dLLMs) to reasonnot only better (furious), but also faster via reinforcement learning (RL). Wefirst unify the existing baseline approach such as d1 by proposing to trainsurrogate policies via off-policy RL, whose likelihood is much more tractableas an approximation to the true dLLM policy. This naturally motivates a moreaccurate and informative two-stage likelihood approximation combined withimportance sampling correction, which leads to generalized RL algorithms withbetter sample efficiency and superior task performance. Second, we propose anew direction of joint training efficient samplers/controllers of dLLMs policy.Via RL, we incentivize dLLMs' natural multi-token prediction capabilities byletting the model learn to adaptively allocate an inference threshold for eachprompt. By jointly training the sampler, we yield better accuracies with lowernumber of function evaluations (NFEs) compared to training the model only,obtaining the best performance in improving the Pareto frontier of theinference-time compute of dLLMs. We showcase the effectiveness of our pipelineby training open source large diffusion language models over benchmark math andplanning tasks.</description><author>Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, Nathan Kallus</author><pubDate>Thu, 02 Oct 2025 16:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02212v1</guid></item><item><title>DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains</title><link>http://arxiv.org/abs/2506.00708v2</link><description>Knowledge graph completion (KGC) aims to predict missing triples in knowledgegraphs (KGs) by leveraging existing triples and textual information. Recently,generative large language models (LLMs) have been increasingly employed forgraph tasks. However, current approaches typically encode graph context intextual form, which fails to fully exploit the potential of LLMs for perceivingand reasoning about graph structures. To address this limitation, we proposeDrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge GraphCompletion). DrKGC employs a flexible lightweight model training strategy tolearn structural embeddings and logical rules within the KG. It then leveragesa novel bottom-up graph retrieval method to extract a subgraph for each queryguided by the learned rules. Finally, a graph convolutional network (GCN)adapter uses the retrieved subgraph to enhance the structural embeddings, whichare then integrated into the prompt for effective LLM fine-tuning. Experimentalresults on two general domain benchmark datasets and two biomedical datasetsdemonstrate the superior performance of DrKGC. Furthermore, a realistic casestudy in the biomedical domain highlights its interpretability and practicalutility.</description><author>Yongkang Xiao, Sinian Zhang, Yi Dai, Huixue Zhou, Jue Hou, Jie Ding, Rui Zhang</author><pubDate>Thu, 02 Oct 2025 16:56:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.00708v2</guid></item><item><title>StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?</title><link>http://arxiv.org/abs/2510.02209v1</link><description>Large language models (LLMs) have recently demonstrated strong capabilitiesas autonomous agents, showing promise in reasoning, tool use, and sequentialdecision-making. While prior benchmarks have evaluated LLM agents in domainssuch as software engineering and scientific discovery, the finance domainremains underexplored, despite its direct relevance to economic value andhigh-stakes decision-making. Existing financial benchmarks primarily teststatic knowledge through question answering, but they fall short of capturingthe dynamic and iterative nature of trading. To address this gap, we introduceStockBench, a contamination-free benchmark designed to evaluate LLM agents inrealistic, multi-month stock trading environments. Agents receive daily marketsignals -- including prices, fundamentals, and news -- and must make sequentialbuy, sell, or hold decisions. Performance is assessed using financial metricssuch as cumulative return, maximum drawdown, and the Sortino ratio. Ourevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) andopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLMagents struggle to outperform the simple buy-and-hold baseline, several modelsdemonstrate the potential to deliver higher returns and manage risk moreeffectively. These findings highlight both the challenges and opportunities indeveloping LLM-powered financial agents, showing that excelling at staticfinancial knowledge tasks does not necessarily translate into successfultrading strategies. We release StockBench as an open-source resource to supportreproducibility and advance future research in this domain.</description><author>Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, Juanzi Li</author><pubDate>Thu, 02 Oct 2025 16:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02209v1</guid></item><item><title>Measurement-Guided Consistency Model Sampling for Inverse Problems</title><link>http://arxiv.org/abs/2510.02208v1</link><description>Diffusion models have become powerful generative priors for solving inverseimaging problems, but their reliance on slow multi-step sampling limitspractical deployment. Consistency models address this bottleneck by enablinghigh-quality generation in a single or only a few steps, yet their directadaptation to inverse problems is underexplored. In this paper, we present amodified consistency sampling approach tailored for inverse problemreconstruction: the sampler's stochasticity is guided by ameasurement-consistency mechanism tied to the measurement operator, whichenforces fidelity to the acquired measurements while retaining the efficiencyof consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroomdatasets demonstrate consistent improvements in perceptual and pixel-levelmetrics, including Fr\'echet Inception Distance, Kernel Inception Distance,peak signal-to-noise ratio, and structural similarity index measure, comparedto baseline consistency sampling, yielding competitive or superiorreconstructions with only a handful of steps.</description><author>Amirreza Tanevardi, Pooria Abbas Rad Moghadam, Sajjad Amini</author><pubDate>Thu, 02 Oct 2025 16:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02208v1</guid></item><item><title>Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling</title><link>http://arxiv.org/abs/2510.02206v1</link><description>Sequence-to-sequence models have become central in Artificial Intelligence,particularly following the introduction of the transformer architecture. Whileinitially developed for Natural Language Processing, these models havedemonstrated utility across domains, including Computer Vision. Such modelsrequire mechanisms to exchange information along the time dimension, typicallyusing recurrent or self-attention layers. However, self-attention scalesquadratically with sequence length, limiting its practicality for very longsequences. We introduce Poolformer, a sequence-to-sequence model that replacesself-attention with recurrent layers and incorporates pooling operations toreduce sequence length. Poolformer is defined recursively using SkipBlocks,which contain residual blocks, a down-pooling layer, a nested SkipBlock, anup-pooling layer, and additional residual blocks. We conduct extensiveexperiments to support our architectural choices. Our results show that pooling greatly accelerates training, improvesperceptual metrics (FID and IS), and prevents overfitting. Our experiments alsosuggest that long-range dependencies are handled by deep layers, while shallowlayers take care of short-term features. Evaluated on raw audio, which naturally features long sequence lengths,Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.Future directions include applications to text and vision, as well asmulti-modal scenarios, where a Poolformer-based LLM could effectively processdense representations of images and videos.</description><author>Daniel Gallo Fernndez</author><pubDate>Thu, 02 Oct 2025 16:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02206v1</guid></item><item><title>Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents</title><link>http://arxiv.org/abs/2510.02204v1</link><description>Mobile-use agents powered by vision-language models (VLMs) have shown greatpotential in interpreting natural language instructions and generatingcorresponding actions based on mobile graphical user interface. Recent studiessuggest that incorporating chain-of-thought (CoT) reasoning tends to improvethe execution accuracy. However, existing evaluations emphasize executionaccuracy while neglecting whether CoT reasoning aligns with ground-truthactions. This oversight fails to assess potential reasoning-execution gaps,which in turn foster over-trust: users relying on seemingly plausible CoTs mayunknowingly authorize harmful actions, potentially resulting in financial lossor trust crisis. In this work, we introduce a new evaluation framework todiagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment(GTA), which measures whether the action implied by a CoT matches theground-truth action. By combining GTA with the standard Exact Match (EM)metric, we jointly assess both the reasoning accuracy and execution accuracy.This joint perspective reveals two types of reasoning-execution gaps: (i)Execution Gap (EG), where the reasoning correctly identifies the correct actionbut execution fails, and (ii) Reasoning Gap (RG), where execution succeeds butreasoning process conflicts with the actual execution. Experimental resultsacross a wide range of mobile interaction tasks reveal that reasoning-executiongaps are prevalent, with execution gaps occurring more frequently thanreasoning gaps. Moreover, while scaling up model size reduces the overall gap,sizable execution gaps persist even in the largest models. Further analysisshows that our framework reliably reflects systematic EG/RG patterns instate-of-the-art models. These findings offer concrete diagnostics and supportthe development of more trustworthy mobile-use agents.</description><author>Lingzhong Dong, Ziqi Zhou, Shuaibo Yang, Haiyue Sheng, Pengzhou Cheng, Zongru Wu, Zheng Wu, Gongshen Liu, Zhuosheng Zhang</author><pubDate>Thu, 02 Oct 2025 16:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02204v1</guid></item><item><title>Towards end-to-end ASP computation</title><link>http://arxiv.org/abs/2306.06821v3</link><description>We propose an end-to-end approach for Answer Set Programming (ASP) and linearalgebraically compute stable models satisfying given constraints. The idea isto implement Lin-Zhao's theorem together with constraints directly in vectorspaces as numerical minimization of a cost function constructed from amatricized normal logic program, loop formulas in Lin-Zhao's theorem andconstraints, thereby no use of symbolic ASP or SAT solvers involved in ourapproach. We also propose precomputation that shrinks the program size andheuristics for loop formulas to reduce computational difficulty. We empiricallytest our approach with programming examples including the 3-coloring andHamiltonian cycle problems.</description><author>Taisuke Sato, Akihiro Takemura, Katsumi Inoue</author><pubDate>Thu, 02 Oct 2025 16:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06821v3</guid></item><item><title>Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025</title><link>http://arxiv.org/abs/2510.02202v1</link><description>Objective: Chagas disease is a parasitic infection that is endemic to SouthAmerica, Central America, and, more recently, the U.S., primarily transmittedby insects. Chronic Chagas disease can cause cardiovascular diseases anddigestive problems. Serological testing capacities for Chagas disease arelimited, but Chagas cardiomyopathy often manifests in ECGs, providing anopportunity to prioritize patients for testing and treatment. Approach: TheGeorge B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmicapproaches for identifying Chagas disease from electrocardiograms (ECGs). Mainresults: This Challenge provides multiple innovations. First, we leveragedseveral datasets with labels from patient reports and serological testing,provided a large dataset with weak labels and smaller datasets with stronglabels. Second, we augmented the data to support model robustness andgeneralizability to unseen data sources. Third, we applied an evaluation metricthat captured the local serological testing capacity for Chagas disease toframe the machine learning problem as a triage task. Significance: Over 630participants from 111 teams submitted over 1300 entries during the Challenge,representing diverse approaches from academia and industry worldwide.</description><author>Matthew A. Reyna, Zuzana Koscova, Jan Pavlus, Soheil Saghafi, James Weigle, Andoni Elola, Salman Seyedi, Kiersten Campbell, Qiao Li, Ali Bahrami Rad, Antnio H. Ribeiro, Antonio Luiz P. Ribeiro, Reza Sameni, Gari D. Clifford</author><pubDate>Thu, 02 Oct 2025 16:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02202v1</guid></item><item><title>ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities</title><link>http://arxiv.org/abs/2510.02200v1</link><description>Interacting with knowledge graphs can be a daunting task for people without abackground in computer science since the query language that is used (SPARQL)has a high barrier of entry. Large language models (LLMs) can lower thatbarrier by providing support in the form of Text2SPARQL translation. In thispaper we introduce a generalized method based on SPINACH, an LLM backed agentthat translates natural language questions to SPARQL queries not in a singleshot, but as an iterative process of exploration and execution. We describe theoverall architecture and reasoning behind our design decisions, and alsoconduct a thorough analysis of the agent behavior to gain insights into futureareas for targeted improvements. This work was motivated by the Text2SPARQLchallenge, a challenge that was held to facilitate improvements in theText2SPARQL domain.</description><author>Felix Brei, Lorenz Bhmann, Johannes Frey, Daniel Gerber, Lars-Peter Meyer, Claus Stadler, Kirill Bulert</author><pubDate>Thu, 02 Oct 2025 16:49:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02200v1</guid></item><item><title>LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control</title><link>http://arxiv.org/abs/2401.04855v4</link><description>Coverage control is the problem of navigating a robot swarm tocollaboratively monitor features or a phenomenon of interest not known apriori. The problem is challenging in decentralized settings with robots thathave limited communication and sensing capabilities. We propose a learnablePerception-Action-Communication (LPAC) architecture for the problem, wherein aconvolutional neural network (CNN) processes localized perception; a graphneural network (GNN) facilitates robot communications; finally, a shallowmulti-layer perceptron (MLP) computes robot actions. The GNN enablescollaboration in the robot swarm by computing what information to communicatewith nearby robots and how to incorporate received information. Evaluationsshow that the LPAC models -- trained using imitation learning -- outperformstandard decentralized and centralized coverage control algorithms. The learnedpolicy generalizes to environments different from the training dataset,transfers to larger environments with more robots, and is robust to noisyposition estimates. The results indicate the suitability of LPAC architecturesfor decentralized navigation in robot swarms to achieve collaborative behavior.</description><author>Saurav Agarwal, Ramya Muthukrishnan, Walker Gosrich, Vijay Kumar, Alejandro Ribeiro</author><pubDate>Thu, 02 Oct 2025 16:48:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04855v4</guid></item><item><title>Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications</title><link>http://arxiv.org/abs/2510.02197v1</link><description>Accurate livestock identification is a cornerstone of modern farming: itsupports health monitoring, breeding programs, and productivity tracking.However, common pig identification methods, such as ear tags and microchips,are often unreliable, costly, target pure breeds, and thus impractical forsmall-scale farmers. To address this gap, we propose a noninvasive biometricidentification approach that leverages uniqueness of the auricular veinpatterns. To this end, we have collected 800 ear images from 20 mixed-breedpigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using astandard smartphone and simple back lighting. A multistage computer visionpipeline was developed to enhance vein visibility, extract structural andspatial features, and generate biometric signatures. These features were thenclassified using machine learning models. Support Vector Machines (SVM)achieved the highest accuracy: correctly identifying pigs with 98.12% precisionacross mixed-breed populations. The entire process from image processing toclassification was completed in an average of 8.3 seconds, demonstratingfeasibility for real-time farm deployment. We believe that by replacing fragilephysical identifiers with permanent biological markers, this system providesfarmers with a cost-effective and stress-free method of animal identification.More broadly, the findings confirm the practicality of auricular veinbiometrics for digitizing livestock management, reinforcing its potential toextend the benefits of precision farming to resource-constrained agriculturalcommunities.</description><author>Emmanuel Nsengiyumvaa, Leonard Niyitegekaa, Eric Umuhoza</author><pubDate>Thu, 02 Oct 2025 16:45:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02197v1</guid></item><item><title>GenExam: A Multidisciplinary Text-to-Image Exam</title><link>http://arxiv.org/abs/2509.14232v2</link><description>Exams are a fundamental test of expert-level intelligence and requireintegrated understanding, reasoning, and generation. Existing exam-stylebenchmarks mainly focus on understanding and reasoning tasks, and currentgeneration benchmarks emphasize the illustration of world knowledge and visualconcepts, neglecting the evaluation of rigorous drawing exams. We introduceGenExam, the first benchmark for multidisciplinary text-to-image exams,featuring 1,000 samples across 10 subjects with exam-style prompts organizedunder a four-level taxonomy. Each problem is equipped with ground-truth imagesand fine-grained scoring points to enable a precise evaluation of semanticcorrectness and visual plausibility. Experiments show that evenstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieveless than 15% strict scores, and most models yield almost 0%, suggesting thegreat challenge of our benchmark. By framing image generation as an exam,GenExam offers a rigorous assessment of models' ability to integrateunderstanding, reasoning, and generation, providing insights on the path togeneral AGI. Our benchmark and evaluation code are released athttps://github.com/OpenGVLab/GenExam.</description><author>Zhaokai Wang, Penghao Yin, Xiangyu Zhao, Changyao Tian, Yu Qiao, Wenhai Wang, Jifeng Dai, Gen Luo</author><pubDate>Thu, 02 Oct 2025 16:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.14232v2</guid></item><item><title>On Code-Induced Reasoning in LLMs</title><link>http://arxiv.org/abs/2509.21499v2</link><description>Code data has been shown to enhance the reasoning capabilities of largelanguage models (LLMs), but it remains unclear which aspects of code are mostresponsible. We investigate this question with a systematic, data-centricframework. We construct parallel instruction datasets in ten programminglanguages and apply controlled perturbations that selectively disruptstructural or semantic properties of code. We then finetune LLMs from fivemodel families and eight scales on each variant and evaluate their performanceon natural language, math, and code tasks. Across 3,331 experiments, ourresults show that LLMs are more vulnerable to structural perturbations thansemantic ones, particularly on math and code tasks. Appropriate abstractionslike pseudocode and flowcharts can be as effective as code, while encoding thesame information with fewer tokens without adhering to original syntax canoften retain or even improve performance. Remarkably, even corrupted code withmisleading signals remains competitive when surface-level regularities persist.Finally, syntactic styles also shape task-specific gains with Python favoringnatural language reasoning and lower-level languages such as Java and Rustfavoring math. Through our systematic framework, we aim to provide insight intohow different properties of code influence reasoning and inform the design oftraining data for enhancing LLM reasoning capabilities.</description><author>Abdul Waheed, Zhen Wu, Carolyn Ros, Daphne Ippolito</author><pubDate>Thu, 02 Oct 2025 16:45:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21499v2</guid></item><item><title>UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models</title><link>http://arxiv.org/abs/2510.02194v1</link><description>Large Language Models (LLMs) have achieved remarkable progress across a widerange of tasks, but remain vulnerable to safety risks such as harmful contentgeneration and jailbreak attacks. Existing safety techniques -- includingexternal guardrails, inference-time guidance, and post-training alignment --each face limitations in balancing safety, utility, and controllability. Inthis work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLMsafety through safety-aware upcycling. Our approach first identifiessafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)structure, where the router acts as a soft guardrail that selectively activatesoriginal MLPs and added safety experts. We further introduce a two-stage SFTstrategy to strengthen safety discrimination while preserving generalcapabilities. To enable flexible control at inference time, we introduce asafety temperature mechanism, allowing dynamic adjustment of the trade-offbetween safety and utility. Experiments across multiple benchmarks, base model,and model scales demonstrate that UpSafe$^\circ$C achieves robust safetyimprovements against harmful and jailbreak inputs, while maintainingcompetitive performance on general tasks. Moreover, analysis shows that safetytemperature provides fine-grained inference-time control that achieves thePareto-optimal frontier between utility and safety. Our results highlight a newdirection for LLM safety: moving from static alignment toward dynamic, modular,and inference-aware control.</description><author>Yuhao Sun, Zhuoer Xu, Shiwen Cui, Kun Yang, Lingyun Yu, Yongdong Zhang, Hongtao Xie</author><pubDate>Thu, 02 Oct 2025 16:43:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02194v1</guid></item><item><title>Convergence analysis of online algorithms for vector-valued kernel regression</title><link>http://arxiv.org/abs/2309.07779v5</link><description>We consider the problem of approximating the regression function $f_\mu:\,\Omega \to Y$ from noisy $\mu$-distributed vector-valued data$(\omega_m,y_m)\in\Omega\times Y$ by an online learning algorithm using areproducing kernel Hilbert space $H$ (RKHS) as prior. In an online algorithm,i.i.d. samples become available one by one via a random process and aresuccessively processed to build approximations to the regression function.Assuming that the regression function essentially belongs to $H$ (soft learningscenario), we provide estimates for the expected squared error in the RKHS normof the approximations $f^{(m)}\in H$ obtained by a standard regularized onlineapproximation algorithm. In particular, we show an order-optimal estimate $$\mathbb{E}(\|\epsilon^{(m)}\|_H^2)\le C (m+1)^{-s/(2+s)},\qquad m=1,2,\ldots,$$ where $\epsilon^{(m)}$ denotes the error term after $m$ processed data, theparameter $0&lt;s\leq 1$ expresses an additional smoothness assumption on theregression function, and the constant $C$ depends on the variance of the inputnoise, the smoothness of the regression function, and other parameters of thealgorithm. The proof, which is inspired by results on Schwarz iterative methodsin the noiseless case, uses only elementary Hilbert space techniques andminimal assumptions on the noise, the feature map that defines $H$ and theassociated covariance operator.</description><author>Michael Griebel, Peter Oswald</author><pubDate>Thu, 02 Oct 2025 16:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07779v5</guid></item><item><title>A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports</title><link>http://arxiv.org/abs/2510.02190v1</link><description>Artificial intelligence is undergoing the paradigm shift from closed languagemodels to interconnected agent systems capable of external perception andinformation integration. As a representative embodiment, Deep Research Agents(DRAs) systematically exhibit the capabilities for task decomposition,cross-source retrieval, multi-stage reasoning, and structured output, whichmarkedly enhance performance on complex and open-ended tasks. However, existingbenchmarks remain deficient in evaluation dimensions, response formatting, andscoring mechanisms, limiting their capacity to assess such systems effectively.This paper introduces a rigorous benchmark and a multidimensional evaluationframework tailored to DRAs and report-style responses. The benchmark comprises214 expert-curated challenging queries distributed across 10 broad thematicdomains, each accompanied by manually constructed reference bundles to supportcomposite evaluation. The framework enables comprehensive evaluation oflong-form reports generated by DRAs, incorporating integrated scoring metricsfor semantic quality, topical focus, and retrieval trustworthiness. Extensiveexperimentation confirms the superior performance of mainstream DRAs overweb-search-tool-augmented reasoning models, yet reveals considerable scope forfurther improvement. This study provides a robust foundation for capabilityassessment, architectural refinement, and paradigm advancement in DRA systems.</description><author>Yang Yao, Yixu Wang, Yuxuan Zhang, Yi Lu, Tianle Gu, Lingyu Li, Dingyi Zhao, Keming Wu, Haozhe Wang, Ping Nie, Yan Teng, Yingchun Wang</author><pubDate>Thu, 02 Oct 2025 16:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02190v1</guid></item><item><title>Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure Risk at Record 2.9-Million Observation Scale</title><link>http://arxiv.org/abs/2510.02189v1</link><description>Arctic warming threatens over 100 billion in permafrost-dependentinfrastructure across Northern territories, yet existing risk assessmentframeworks lack spatiotemporal validation, uncertainty quantification, andoperational decision-support capabilities. We present a hybrid physics-machinelearning framework integrating 2.9 million observations from 171,605 locations(2005-2021) combining permafrost fraction data with climate reanalysis. Ourstacked ensemble model (Random Forest + Histogram Gradient Boosting + ElasticNet) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporalcross-validation preventing data leakage. To address machine learninglimitations in extrapolative climate scenarios, we develop a hybrid approachcombining learned climate-permafrost relationships (60%) with physicalpermafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over10 years), we project mean permafrost fraction decline of -20.3 pp (median:-20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage pointloss. Infrastructure risk classification identifies 15% high-risk zones (25%medium-risk) with spatially explicit uncertainty maps. Our framework representsthe largest validated permafrost ML dataset globally, provides the firstoperational hybrid physics-ML forecasting system for Arctic infrastructure, anddelivers open-source tools enabling probabilistic permafrost projections forengineering design codes and climate adaptation planning. The methodology isgeneralizable to other permafrost regions and demonstrates how hybridapproaches can overcome pure data-driven limitations in climate changeapplications.</description><author>Boris Kriuk</author><pubDate>Thu, 02 Oct 2025 16:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02189v1</guid></item><item><title>High-Fidelity Speech Enhancement via Discrete Audio Tokens</title><link>http://arxiv.org/abs/2510.02187v1</link><description>Recent autoregressive transformer-based speech enhancement (SE) methods haveshown promising results by leveraging advanced semantic understanding andcontextual modeling of speech. However, these approaches often rely on complexmulti-stage pipelines and low sampling rate codecs, limiting them to narrow andtask-specific speech enhancement. In this work, we introduce DAC-SE1, asimplified language model-based SE framework leveraging discretehigh-resolution audio representations; DAC-SE1 preserves fine-grained acousticdetails while maintaining semantic coherence. Our experiments show that DAC-SE1surpasses state-of-the-art autoregressive SE methods on both objectiveperceptual metrics and in a MUSHRA human evaluation. We release our codebaseand model checkpoints to support further research in scalable, unified, andhigh-quality speech enhancement.</description><author>Luca A. Lanzendrfer, Frdric Berdoz, Antonis Asonitis, Roger Wattenhofer</author><pubDate>Thu, 02 Oct 2025 16:38:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02187v1</guid></item><item><title>GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</title><link>http://arxiv.org/abs/2510.02186v1</link><description>Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to3D semantic segmentation expose a persistent trade-off. Directly projecting 2Dfeatures into 3D yields noisy and fragmented predictions, whereas enforcinggeometric coherence necessitates costly training pipelines and large-scaleannotated 3D data. We argue that this limitation stems from the dominantsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with3D geometric structure. The geometric cues are not eliminated during the2D-to-3D transfer but remain latent within the noisy and view-aggregatedfeatures. To exploit this property, we propose GeoPurify that applies a smallStudent Affinity Network to purify 2D VLM-generated 3D point features usinggeometric priors distilled from a 3D self-supervised teacher model. Duringinference, we devise a Geometry-Guided Pooling module to further denoise thepoint cloud and ensure the semantic and structural consistency. Benefiting fromlatent geometric information and the learned affinity network, GeoPurifyeffectively mitigates the trade-off and achieves superior data efficiency.Extensive experiments on major 3D benchmarks demonstrate that GeoPurifyachieves or surpasses state-of-the-art performance while utilizing only about1.5% of the training data. Our codes and checkpoints are available at[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).</description><author>Weijia Dou, Xu Zhang, Yi Bin, Jian Liu, Bo Peng, Guoqing Wang, Yang Yang, Heng Tao Shen</author><pubDate>Thu, 02 Oct 2025 16:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02186v1</guid></item><item><title>Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</title><link>http://arxiv.org/abs/2504.03206v3</link><description>Effective conversational agents like large language models (LLMs) mustpersonalize their interactions to adapt to user preferences, personalities, andattributes across diverse domains like education and healthcare. Currentmethods like Reinforcement Learning from Human Feedback (RLHF), oftenprioritize helpfulness and safety but fall short in fostering truly empathetic,adaptive, and personalized dialogues. Existing personalization approachestypically rely on extensive user history, limiting their effectiveness for newor context-limited users. To address these limitations, we propose leveraging auser model to incorporate a curiosity-based intrinsic reward into multi-turnRLHF. This novel reward mechanism encourages the LLM agent to actively inferuser traits by optimizing conversations to improve its user model's accuracy.Consequently, the agent delivers more personalized interactions by learningmore about the user. We demonstrate our method's effectiveness in two distinctdomains: significantly improving personalization performance in aconversational recommendation task, and personalizing conversations fordifferent learning styles in an educational setting. We show improvedgeneralization capabilities compared to traditional multi-turn RLHF, all whilemaintaining conversation quality. Our method offers a promising solution forcreating more personalized, adaptive, and engaging conversational agents.</description><author>Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques</author><pubDate>Thu, 02 Oct 2025 16:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.03206v3</guid></item><item><title>ABBA-Adapters: Efficient and Expressive Fine-Tuning of Foundation Models</title><link>http://arxiv.org/abs/2505.14238v3</link><description>Large Language Models have demonstrated strong performance across a widerange of tasks, but adapting them efficiently to new domains remains a keychallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this byintroducing lightweight, trainable modules while keeping most pre-trainedweights fixed. The prevailing approach, LoRA, models updates using a low-rankdecomposition, but its expressivity is inherently constrained by the rank.Recent methods like HiRA aim to increase expressivity by incorporating aHadamard product with the frozen weights, but still rely on the structure ofthe pre-trained model. We introduce ABBA, a new PEFT architecture thatreparameterizes the update as a Hadamard product of two independently learnablelow-rank matrices. In contrast to prior work, ABBA fully decouples the updatefrom the pre-trained weights, enabling both components to be optimized freely.This leads to significantly higher expressivity under the same parameterbudget, a property we validate through matrix reconstruction experiments.Empirically, ABBA achieves state-of-the-art results on arithmetic andcommonsense reasoning benchmarks, consistently outperforming existing PEFTmethods by a significant margin across multiple models. Our code is publiclyavailable at: https://github.com/CERT-Lab/abba.</description><author>Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Praneeth Vepakomma</author><pubDate>Thu, 02 Oct 2025 16:35:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14238v3</guid></item><item><title>Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion</title><link>http://arxiv.org/abs/2510.02182v1</link><description>Understanding how neural populations in higher visual areas encodeobject-centered visual information remains a central challenge in computationalneuroscience. Prior works have investigated representational alignment betweenartificial neural networks and the visual cortex. Nevertheless, these findingsare indirect and offer limited insights to the structure of neural populationsthemselves. Similarly, decoding-based methods have quantified semantic featuresfrom neural populations but have not uncovered their underlying organizations.This leaves open a scientific question: "how feature-specific visualinformation is distributed across neural populations in higher visual areas,and whether it is organized into structured, semantically meaningfulsubspaces." To tackle this problem, we present MIG-Vis, a method that leveragesthe generative power of diffusion models to visualize and validate thevisual-semantic attributes encoded in neural latent subspaces. Our method firstuses a variational autoencoder to infer a group-wise disentangled neural latentsubspace from neural populations. Subsequently, we propose a mutual information(MI)-guided diffusion synthesis procedure to visualize the specificvisual-semantic features encoded by each latent group. We validate MIG-Vis onmulti-session neural spiking datasets from the inferior temporal (IT) cortex oftwo macaques. The synthesized results demonstrate that our method identifiesneural latent groups with clear semantic selectivity to diverse visualfeatures, including object pose, inter-category transformations, andintra-class content. These findings provide direct, interpretable evidence ofstructured semantic representation in the higher visual cortex and advance ourunderstanding of its encoding principles.</description><author>Yule Wang, Joseph Yu, Chengrui Li, Weihan Li, Anqi Wu</author><pubDate>Thu, 02 Oct 2025 16:33:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02182v1</guid></item><item><title>EvolveCaptions: Empowering DHH Users Through Real-Time Collaborative Captioning</title><link>http://arxiv.org/abs/2510.02181v1</link><description>Automatic Speech Recognition (ASR) systems often fail to accuratelytranscribe speech from Deaf and Hard of Hearing (DHH) individuals, especiallyduring real-time conversations. Existing personalization approaches typicallyrequire extensive pre-recorded data and place the burden of adaptation on theDHH speaker. We present EvolveCaptions, a real-time, collaborative ASRadaptation system that supports in-situ personalization with minimal effort.Hearing participants correct ASR errors during live conversations. Based onthese corrections, the system generates short, phonetically targeted promptsfor the DHH speaker to record, which are then used to fine-tune the ASR model.In a study with 12 DHH and six hearing participants, EvolveCaptions reducedWord Error Rate (WER) across all DHH users within one hour of use, using onlyfive minutes of recording time on average. Participants described the system asintuitive, low-effort, and well-integrated into communication. These findingsdemonstrate the promise of collaborative, real-time ASR adaptation for moreequitable communication.</description><author>Liang-Yuan Wu, Dhruv Jain</author><pubDate>Thu, 02 Oct 2025 16:32:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02181v1</guid></item><item><title>GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning</title><link>http://arxiv.org/abs/2510.02180v1</link><description>Inverse Reinforcement Learning aims to recover reward models from expertdemonstrations, but traditional methods yield "black-box" models that aredifficult to interpret and debug. In this work, we introduce GRACE (GeneratingRewards As CodE), a method for using Large Language Models within anevolutionary search to reverse-engineer an interpretable, code-based rewardfunction directly from expert trajectories. The resulting reward function isexecutable code that can be inspected and verified. We empirically validateGRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learnshighly accurate rewards, even in complex, multi-task settings. Further, wedemonstrate that the resulting reward leads to strong policies, compared toboth competitive Imitation Learning and online RL approaches with ground-truthrewards. Finally, we show that GRACE is able to build complex reward APIs inmulti-task setups.</description><author>Silvia Sapora, Devon Hjelm, Alexander Toshev, Omar Attia, Bogdan Mazoure</author><pubDate>Thu, 02 Oct 2025 16:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02180v1</guid></item><item><title>DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis</title><link>http://arxiv.org/abs/2510.02178v1</link><description>3D indoor layout synthesis is crucial for creating virtual environments.Traditional methods struggle with generalization due to fixed datasets. Whilerecent LLM and VLM-based approaches offer improved semantic richness, theyoften lack robust and flexible refinement, resulting in suboptimal layouts. Wedevelop DisCo-Layout, a novel framework that disentangles and coordinatesphysical and semantic refinement. For independent refinement, our SemanticRefinement Tool (SRT) corrects abstract object relationships, while thePhysical Refinement Tool (PRT) resolves concrete spatial issues via agrid-matching algorithm. For collaborative refinement, a multi-agent frameworkintelligently orchestrates these tools, featuring a planner for placementrules, a designer for initial layouts, and an evaluator for assessment.Experiments demonstrate DisCo-Layout's state-of-the-art performance, generatingrealistic, coherent, and generalizable 3D indoor layouts. Our code will bepublicly available.</description><author>Jialin Gao, Donghao Zhou, Mingjian Liang, Lihao Liu, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng</author><pubDate>Thu, 02 Oct 2025 16:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02178v1</guid></item><item><title>Neurosymbolic Association Rule Mining from Tabular Data</title><link>http://arxiv.org/abs/2504.19354v3</link><description>Association Rule Mining (ARM) is the task of mining patterns among datafeatures in the form of logical rules, with applications across a myriad ofdomains. However, high-dimensional datasets often result in an excessive numberof rules, increasing execution time and negatively impacting downstream taskperformance. Managing this rule explosion remains a central challenge in ARMresearch. To address this, we introduce Aerial+, a novel neurosymbolic ARMmethod. Aerial+ leverages an under-complete autoencoder to create a neuralrepresentation of the data, capturing associations between features. Itextracts rules from this neural representation by exploiting the model'sreconstruction mechanism. Extensive evaluations on five datasets against sevenbaselines demonstrate that Aerial+ achieves state-of-the-art results bylearning more concise, high-quality rule sets with full data coverage. Whenintegrated into rule-based interpretable machine learning models, Aerial+significantly reduces execution time while maintaining or improving accuracy.</description><author>Erkan Karabulut, Paul Groth, Victoria Degeler</author><pubDate>Thu, 02 Oct 2025 16:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19354v3</guid></item><item><title>Flatness-Aware Stochastic Gradient Langevin Dynamics</title><link>http://arxiv.org/abs/2510.02174v1</link><description>Generalization in deep learning is closely tied to the pursuit of flat minimain the loss landscape, yet classical Stochastic Gradient Langevin Dynamics(SGLD) offers no mechanism to bias its dynamics toward such low-curvaturesolutions. This work introduces Flatness-Aware Stochastic Gradient LangevinDynamics (fSGLD), designed to efficiently and provably seek flat minima inhigh-dimensional nonconvex optimization problems. At each iteration, fSGLD usesthe stochastic gradient evaluated at parameters perturbed by isotropic Gaussiannoise, commonly referred to as Random Weight Perturbation (RWP), therebyoptimizing a randomized-smoothing objective that implicitly captures curvatureinformation. Leveraging these properties, we prove that the invariant measureof fSGLD stays close to a stationary measure concentrated on the globalminimizers of a loss function regularized by the Hessian trace whenever theinverse temperature and the scale of random weight perturbation are properlycoupled. This result provides a rigorous theoretical explanation for thebenefits of random weight perturbation. In particular, we establishnon-asymptotic convergence guarantees in Wasserstein distance with the bestknown rate and derive an excess-risk bound for the Hessian-trace regularizedobjective. Extensive experiments on noisy-label and large-scale vision tasks,in both training-from-scratch and fine-tuning settings, demonstrate that fSGLDachieves superior or comparable generalization and robustness to baselinealgorithms while maintaining the computational cost of SGD, about half that ofSAM. Hessian-spectrum analysis further confirms that fSGLD converges tosignificantly flatter minima.</description><author>Stefano Bruno, Youngsik Hwang, Jaehyeon An, Sotirios Sabanis, Dong-Young Lim</author><pubDate>Thu, 02 Oct 2025 16:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02174v1</guid></item></channel></rss>