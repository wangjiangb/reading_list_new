<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 13 Apr 2024 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Connecting NeRFs, Images, and Text</title><link>http://arxiv.org/abs/2404.07993v1</link><description>Neural Radiance Fields (NeRFs) have emerged as a standard framework forrepresenting 3D scenes and objects, introducing a novel data type forinformation exchange and storage. Concurrently, significant progress has beenmade in multimodal representation learning for text and image data. This paperexplores a novel research direction that aims to connect the NeRF modality withother modalities, similar to established methodologies for images and text. Tothis end, we propose a simple framework that exploits pre-trained models forNeRF representations alongside multimodal models for text and image processing.Our framework learns a bidirectional mapping between NeRF embeddings and thoseobtained from corresponding images and text. This mapping unlocks several noveland useful applications, including NeRF zero-shot classification and NeRFretrieval from images or text.</description><author>Francesco Ballerini, Pierluigi Zama Ramirez, Roberto Mirabella, Samuele Salti, Luigi Di Stefano</author><pubDate>Thu, 11 Apr 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07993v1</guid></item><item><title>GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo</title><link>http://arxiv.org/abs/2404.07992v1</link><description>Matching cost aggregation plays a fundamental role in learning-basedmulti-view stereo networks. However, directly aggregating adjacent costs canlead to suboptimal results due to local geometric inconsistency. Relatedmethods either seek selective aggregation or improve aggregated depth in the 2Dspace, both are unable to handle geometric inconsistency in the cost volumeeffectively. In this paper, we propose GoMVS to aggregate geometricallyconsistent costs, yielding better utilization of adjacent geometries. Morespecifically, we correspond and propagate adjacent costs to the reference pixelby leveraging the local geometric smoothness in conjunction with surfacenormals. We achieve this by the geometric consistent propagation (GCP) module.It computes the correspondence from the adjacent depth hypothesis space to thereference depth space using surface normals, then uses the correspondence topropagate adjacent costs to the reference geometry, followed by a convolutionfor aggregation. Our method achieves new state-of-the-art performance on DTU,Tanks &amp; Temple, and ETH3D datasets. Notably, our method ranks 1st on the Tanks&amp; Temple Advanced benchmark.</description><author>Jiang Wu, Rui Li, Haofei Xu, Wenxun Zhao, Yu Zhu, Jinqiu Sun, Yanning Zhang</author><pubDate>Thu, 11 Apr 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07992v1</guid></item><item><title>GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh</title><link>http://arxiv.org/abs/2404.07991v1</link><description>We introduce GoMAvatar, a novel approach for real-time, memory-efficient,high-quality animatable human modeling. GoMAvatar takes as input a singlemonocular video to create a digital avatar capable of re-articulation in newposes and real-time rendering from novel viewpoints, while seamlesslyintegrating with rasterization-based graphics pipelines. Central to our methodis the Gaussians-on-Mesh representation, a hybrid 3D model combining renderingquality and speed of Gaussian splatting with geometry modeling andcompatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data andvarious YouTube videos. GoMAvatar matches or surpasses current monocular humanmodeling algorithms in rendering quality and significantly outperforms them incomputational efficiency (43 FPS) while being memory-efficient (3.63 MB persubject).</description><author>Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang</author><pubDate>Thu, 11 Apr 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07991v1</guid></item><item><title>OpenBias: Open-set Bias Detection in Text-to-Image Generative Models</title><link>http://arxiv.org/abs/2404.07990v1</link><description>Text-to-image generative models are becoming increasingly popular andaccessible to the general public. As these models see large-scale deployments,it is necessary to deeply investigate their safety and fairness to notdisseminate and perpetuate any kind of biases. However, existing works focus ondetecting closed sets of biases defined a priori, limiting the studies towell-known concepts. In this paper, we tackle the challenge of open-set biasdetection in text-to-image generative models presenting OpenBias, a newpipeline that identifies and quantifies the severity of biases agnostically,without access to any precompiled set. OpenBias has three stages. In the firstphase, we leverage a Large Language Model (LLM) to propose biases given a setof captions. Secondly, the target generative model produces images using thesame set of captions. Lastly, a Vision Question Answering model recognizes thepresence and extent of the previously proposed biases. We study the behavior ofStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigatedbefore. Via quantitative experiments, we demonstrate that OpenBias agrees withcurrent closed-set bias detection methods and human judgement.</description><author>Moreno D'Incà, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, Nicu Sebe</author><pubDate>Thu, 11 Apr 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07990v1</guid></item><item><title>Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding</title><link>http://arxiv.org/abs/2404.07989v1</link><description>Large foundation models have recently emerged as a prominent focus ofinterest, attaining superior performance in widespread scenarios. Due to thescarcity of 3D data, many efforts have been made to adapt pre-trainedtransformers from vision to 3D domains. However, such 2D-to-3D approaches arestill limited, due to the potential loss of spatial geometries and highcomputation cost. More importantly, their frameworks are mainly designed for 2Dmodels, lacking a general any-to-3D paradigm. In this paper, we introduceAny2Point, a parameter-efficient method to empower any-modality large models(vision, language, audio) for 3D understanding. Given a frozen transformer fromany source modality, we propose a 3D-to-any (1D or 2D) virtual projectionstrategy that correlates the input 3D points to the original 1D or 2D positionswithin the source modality. This mechanism enables us to assign each 3D tokenwith a positional encoding paired with the pre-trained model, which avoids 3Dgeometry loss caused by the true projection and better motivates thetransformer for 3D learning with 1D/2D positional priors. Then, within eachtransformer block, we insert an any-to-3D guided adapter module forparameter-efficient fine-tuning. The adapter incorporates prior spatialknowledge from the source modality to guide the local feature aggregation of 3Dtokens, compelling the semantic adaption of any-modality transformers. Weconduct extensive experiments to showcase the effectiveness and efficiency ofour method. Code and models are released athttps://github.com/Ivan-Tang-3D/Any2Point.</description><author>Yiwen Tang, Jiaming Liu, Dong Wang, Zhigang Wang, Shanghang Zhang, Bin Zhao, Xuelong Li</author><pubDate>Thu, 11 Apr 2024 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07989v1</guid></item><item><title>Supervised Fine-tuning in turn Improves Visual Foundation Models</title><link>http://arxiv.org/abs/2401.10222v2</link><description>Image-text training like CLIP has dominated the pretraining of visionfoundation models in recent years. Subsequent efforts have been made tointroduce region-level visual learning into CLIP's pretraining but facescalability challenges due to the lack of large-scale region-level datasets.Drawing inspiration from supervised fine-tuning (SFT) in natural languageprocessing such as instruction tuning, we explore the potential of fine-grainedSFT in enhancing the generation of vision foundation models after theirpretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleashthe fine-grained knowledge of vision foundation models. In ViSFT, the visionfoundation model is enhanced by performing visual joint learning on somein-domain tasks and then tested on out-of-domain benchmarks. With updatingusing ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over4.4B parameters shows improvements across various out-of-domain benchmarksincluding vision and vision-linguistic scenarios.</description><author>Xiaohu Jiang, Yixiao Ge, Yuying Ge, Dachuan Shi, Chun Yuan, Ying Shan</author><pubDate>Thu, 11 Apr 2024 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10222v2</guid></item><item><title>QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer</title><link>http://arxiv.org/abs/2404.07988v1</link><description>We explore the dexterous manipulation transfer problem by designingsimulators. The task wishes to transfer human manipulations to dexterous robothand simulations and is inherently difficult due to its intricate,highly-constrained, and discontinuous dynamics and the need to control adexterous hand with a DoF to accurately replicate human manipulations. Previousapproaches that optimize in high-fidelity black-box simulators or a modifiedone with relaxed constraints only demonstrate limited capabilities or arerestricted by insufficient simulation fidelity. We introduce parameterizedquasi-physical simulators and a physics curriculum to overcome theselimitations. The key ideas are 1) balancing between fidelity and optimizabilityof the simulation via a curriculum of parameterized simulators, and 2) solvingthe problem in each of the simulators from the curriculum, with propertiesranging from high task optimizability to high fidelity. We successfully enablea dexterous hand to track complex and diverse manipulations in high-fidelitysimulated environments, boosting the success rate by 11\%+ from thebest-performed baseline. The project website is available athttps://meowuu7.github.io/QuasiSim/.</description><author>Xueyi Liu, Kangbo Lyu, Jieqiong Zhang, Tao Du, Li Yi</author><pubDate>Thu, 11 Apr 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07988v1</guid></item><item><title>ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback</title><link>http://arxiv.org/abs/2404.07987v1</link><description>To enhance the controllability of text-to-image diffusion models, existingefforts like ControlNet incorporated image-based conditional controls. In thispaper, we reveal that existing methods still face significant challenges ingenerating images that align with the image conditional controls. To this end,we propose ControlNet++, a novel approach that improves controllable generationby explicitly optimizing pixel-level cycle consistency between generated imagesand conditional controls. Specifically, for an input conditional control, weuse a pre-trained discriminative reward model to extract the correspondingcondition of the generated images, and then optimize the consistency lossbetween the input conditional control and extracted condition. Astraightforward implementation would be generating images from random noisesand then calculating the consistency loss, but such an approach requiresstoring gradients for multiple sampling timesteps, leading to considerable timeand memory costs. To address this, we introduce an efficient reward strategythat deliberately disturbs the input images by adding noise, and then uses thesingle-step denoised images for reward fine-tuning. This avoids the extensivecosts associated with image sampling, allowing for more efficient rewardfine-tuning. Extensive experiments show that ControlNet++ significantlyimproves controllability under various conditional controls. For example, itachieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE,respectively, for segmentation mask, line-art edge, and depth conditions.</description><author>Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, Chen Chen</author><pubDate>Thu, 11 Apr 2024 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07987v1</guid></item><item><title>BAMBOO: a predictive and transferable machine learning force field framework for liquid electrolyte development</title><link>http://arxiv.org/abs/2404.07181v2</link><description>Despite the widespread applications of machine learning force field (MLFF) onsolids and small molecules, there is a notable gap in applying MLFF to complexliquid electrolytes. In this work, we introduce BAMBOO (ByteDance AI MolecularSimulation Booster), a novel framework for molecular dynamics (MD) simulations,with a demonstration of its capabilities in the context of liquid electrolytesfor lithium batteries. We design a physics-inspired graph equivarianttransformer architecture as the backbone of BAMBOO to learn from quantummechanical simulations. Additionally, we pioneer an ensemble knowledgedistillation approach and apply it on MLFFs to improve the stability of MDsimulations. Finally, we propose the density alignment algorithm to alignBAMBOO with experimental measurements. BAMBOO demonstrates state-of-the-artaccuracy in predicting key electrolyte properties such as density, viscosity,and ionic conductivity across various solvents and salt combinations. Ourcurrent model, trained on more than 15 chemical species, achieves the averagedensity error of 0.01 g/cm$^3$ on various compositions compared withexperimental data. Moreover, our model demonstrates transferability tomolecules not included in the quantum mechanical dataset. We envision this workas paving the way to a "universal MLFF" capable of simulating properties ofcommon organic liquids.</description><author>Sheng Gong, Yumin Zhang, Zhenliang Mu, Zhichen Pu, Hongyi Wang, Zhiao Yu, Mengyi Chen, Tianze Zheng, Zhi Wang, Lifei Chen, Xiaojie Wu, Shaochen Shi, Weihao Gao, Wen Yan, Liang Xiang</author><pubDate>Thu, 11 Apr 2024 18:58:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07181v2</guid></item><item><title>WaveMo: Learning Wavefront Modulations to See Through Scattering</title><link>http://arxiv.org/abs/2404.07985v1</link><description>Imaging through scattering media is a fundamental and pervasive challenge infields ranging from medical diagnostics to astronomy. A promising strategy toovercome this challenge is wavefront modulation, which induces measurementdiversity during image acquisition. Despite its importance, designing optimalwavefront modulations to image through scattering remains under-explored. Thispaper introduces a novel learning-based framework to address the gap. Ourapproach jointly optimizes wavefront modulations and a computationallylightweight feedforward "proxy" reconstruction network. This network is trainedto recover scenes obscured by scattering, using measurements that are modifiedby these modulations. The learned modulations produced by our frameworkgeneralize effectively to unseen scattering scenarios and exhibit remarkableversatility. During deployment, the learned modulations can be decoupled fromthe proxy network to augment other more computationally expensive restorationalgorithms. Through extensive experiments, we demonstrate our approachsignificantly advances the state of the art in imaging through scatteringmedia. Our project webpage is at https://wavemo-2024.github.io/.</description><author>Mingyang Xie, Haiyun Guo, Brandon Y. Feng, Lingbo Jin, Ashok Veeraraghavan, Christopher A. Metzler</author><pubDate>Thu, 11 Apr 2024 18:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07985v1</guid></item><item><title>View Selection for 3D Captioning via Diffusion Ranking</title><link>http://arxiv.org/abs/2404.07984v1</link><description>Scalable annotation approaches are crucial for constructing extensive 3D-textdatasets, facilitating a broader range of applications. However, existingmethods sometimes lead to the generation of hallucinated captions, compromisingcaption quality. This paper explores the issue of hallucination in 3D objectcaptioning, with a focus on Cap3D method, which renders 3D objects into 2Dviews for captioning using pre-trained models. We pinpoint a major challenge:certain rendered views of 3D objects are atypical, deviating from the trainingdata of standard image captioning models and causing hallucinations. To tacklethis, we present DiffuRank, a method that leverages a pre-trained text-to-3Dmodel to assess the alignment between 3D objects and their 2D rendered views,where the view with high alignment closely represent the object'scharacteristics. By ranking all rendered views and feeding the top-ranked onesinto GPT4-Vision, we enhance the accuracy and detail of captions, enabling thecorrection of 200k captions in the Cap3D dataset and extending it to 1 millioncaptions across Objaverse and Objaverse-XL datasets. Additionally, we showcasethe adaptability of DiffuRank by applying it to pre-trained text-to-imagemodels for a Visual Question Answering task, where it outperforms the CLIPmodel.</description><author>Tiange Luo, Justin Johnson, Honglak Lee</author><pubDate>Thu, 11 Apr 2024 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07984v1</guid></item><item><title>Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning</title><link>http://arxiv.org/abs/2404.07983v1</link><description>Contrastive vision-language models like CLIP have gained popularity for theirversatile applicable learned representations in various downstream tasks.Despite their successes in some tasks, like zero-shot image recognition, theyalso perform surprisingly poor on other tasks, like attribute detection.Previous work has attributed these challenges to the modality gap, a separationof image and text in the shared representation space, and a bias towardsobjects over other factors, such as attributes. In this work we investigateboth phenomena. We find that only a few embedding dimensions drive the modalitygap. Further, we propose a measure for object bias and find that object biasdoes not lead to worse performance on other concepts, such as attributes. Butwhat leads to the emergence of the modality gap and object bias? To answer thisquestion we carefully designed an experimental setting which allows us tocontrol the amount of shared information between the modalities. This revealedthat the driving factor behind both, the modality gap and the object bias, isthe information imbalance between images and captions.</description><author>Simon Schrodi, David T. Hoffmann, Max Argus, Volker Fischer, Thomas Brox</author><pubDate>Thu, 11 Apr 2024 18:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07983v1</guid></item><item><title>Language Imbalance Can Boost Cross-lingual Generalisation</title><link>http://arxiv.org/abs/2404.07982v1</link><description>Multilinguality is crucial for extending recent advancements in languagemodelling to diverse linguistic communities. To maintain high performance whilerepresenting multiple languages, multilingual models ideally alignrepresentations, allowing what is learned in one language to generalise toothers. Prior research has emphasised the importance of parallel data andshared vocabulary elements as key factors for such alignment. In this study, weinvestigate an unintuitive novel driver of cross-lingual generalisation:language imbalance. In controlled experiments on perfectly equivalent clonedlanguages, we observe that the existence of a predominant language duringtraining boosts the performance of less frequent languages and leads tostronger alignment of model representations across languages. Furthermore, wefind that this trend is amplified with scale: with large enough models or longenough training, we observe that bilingual training data with a 90/10 languagesplit yields better performance on both languages than a balanced 50/50 split.Building on these insights, we design training schemes that can improveperformance in all cloned languages, even without altering the training data.As we extend our analysis to real languages, we find that infrequent languagesstill benefit from frequent ones, yet whether language imbalance causescross-lingual generalisation there is not conclusive.</description><author>Anton Schäfer, Shauli Ravfogel, Thomas Hofmann, Tiago Pimentel, Imanol Schlag</author><pubDate>Thu, 11 Apr 2024 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07982v1</guid></item><item><title>Manipulating Large Language Models to Increase Product Visibility</title><link>http://arxiv.org/abs/2404.07981v1</link><description>Large language models (LLMs) are increasingly being integrated into searchengines to provide natural language responses tailored to user queries.Customers and end-users are also becoming more dependent on these models forquick and easy purchase decisions. In this work, we investigate whetherrecommendations from LLMs can be manipulated to enhance a product's visibility.We demonstrate that adding a strategic text sequence (STS) -- a carefullycrafted message -- to a product's information page can significantly increaseits likelihood of being listed as the LLM's top recommendation. To understandthe impact of STS, we use a catalog of fictitious coffee machines and analyzeits effect on two target products: one that seldom appears in the LLM'srecommendations and another that usually ranks second. We observe that thestrategic text sequence significantly enhances the visibility of both productsby increasing their chances of appearing as the top recommendation. Thisability to manipulate LLM-generated search responses provides vendors with aconsiderable competitive advantage and has the potential to disrupt fair marketcompetition. Just as search engine optimization (SEO) revolutionized howwebpages are customized to rank higher in search engine results, influencingLLM recommendations could profoundly impact content optimization for AI-drivensearch services. Code for our experiments is available athttps://github.com/aounon/llm-rank-optimizer.</description><author>Aounon Kumar, Himabindu Lakkaraju</author><pubDate>Thu, 11 Apr 2024 18:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07981v1</guid></item><item><title>LLoCO: Learning Long Contexts Offline</title><link>http://arxiv.org/abs/2404.07979v1</link><description>Processing long contexts remains a challenge for large language models (LLMs)due to the quadratic computational and memory overhead of the self-attentionmechanism and the substantial KV cache sizes during generation. We propose anovel approach to address this problem by learning contexts offline throughcontext compression and in-domain parameter-efficient finetuning. Our methodenables an LLM to create a concise representation of the original context andefficiently retrieve relevant information to answer questions accurately. Weintroduce LLoCO, a technique that combines context compression, retrieval, andparameter-efficient finetuning using LoRA. Our approach extends the effectivecontext window of a 4k token LLaMA2-7B model to handle up to 128k tokens. Weevaluate our approach on several long-context question-answering datasets,demonstrating that LLoCO significantly outperforms in-context learning whileusing $30\times$ fewer tokens during inference. LLoCO achieves up to$7.62\times$ speed-up and substantially reduces the cost of long documentquestion answering, making it a promising solution for efficient long contextprocessing. Our code is publicly available athttps://github.com/jeffreysijuntan/lloco.</description><author>Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez, Raluca Ada Popa</author><pubDate>Thu, 11 Apr 2024 18:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07979v1</guid></item><item><title>Gaga: Group Any Gaussians via 3D-aware Memory Bank</title><link>http://arxiv.org/abs/2404.07977v1</link><description>We introduce Gaga, a framework that reconstructs and segments open-world 3Dscenes by leveraging inconsistent 2D masks predicted by zero-shot segmentationmodels. Contrasted to prior 3D scene segmentation approaches that heavily relyon video object tracking, Gaga utilizes spatial information and effectivelyassociates object masks across diverse camera poses. By eliminating theassumption of continuous view changes in training images, Gaga demonstratesrobustness to variations in camera poses, particularly beneficial for sparselysampled images, ensuring precise mask label consistency. Furthermore, Gagaaccommodates 2D segmentation masks from diverse sources and demonstrates robustperformance with different open-world zero-shot segmentation models, enhancingits versatility. Extensive qualitative and quantitative evaluations demonstratethat Gaga performs favorably against state-of-the-art methods, emphasizing itspotential for real-world applications such as scene understanding andmanipulation.</description><author>Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang</author><pubDate>Thu, 11 Apr 2024 18:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07977v1</guid></item><item><title>Self-supervised Dataset Distillation: A Good Compression Is All You Need</title><link>http://arxiv.org/abs/2404.07976v1</link><description>Dataset distillation aims to compress information from a large-scale originaldataset to a new compact dataset while striving to preserve the utmost degreeof the original data informational essence. Previous studies have predominantlyconcentrated on aligning the intermediate statistics between the original anddistilled data, such as weight trajectory, features, gradient, BatchNorm, etc.In this work, we consider addressing this task through the new lens of modelinformativeness in the compression stage on the original dataset pretraining.We observe that with the prior state-of-the-art SRe$^2$L, as model sizesincrease, it becomes increasingly challenging for supervised pretrained modelsto recover learned information during data synthesis, as the channel-wise meanand variance inside the model are flatting and less informative. We furthernotice that larger variances in BN statistics from self-supervised modelsenable larger loss signals to update the recovered data by gradients, enjoyingmore informativeness during synthesis. Building on this observation, weintroduce SC-DD, a simple yet effective Self-supervised Compression frameworkfor Dataset Distillation that facilitates diverse information compression andrecovery compared to traditional supervised learning schemes, further reaps thepotential of large pretrained models with enhanced capabilities. Extensiveexperiments are conducted on CIFAR-100, Tiny-ImageNet and ImageNet-1K datasetsto demonstrate the superiority of our proposed approach. The proposed SC-DDoutperforms all previous state-of-the-art supervised dataset distillationmethods when employing larger models, such as SRe$^2$L, MTT, TESLA, DC, CAFE,etc., by large margins under the same recovery and post-training budgets. Codeis available at https://github.com/VILA-Lab/SRe2L/tree/main/SCDD/.</description><author>Muxin Zhou, Zeyuan Yin, Shitong Shao, Zhiqiang Shen</author><pubDate>Thu, 11 Apr 2024 18:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07976v1</guid></item><item><title>OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</title><link>http://arxiv.org/abs/2404.07972v1</link><description>Autonomous agents that accomplish complex computer tasks with minimal humaninterventions have the potential to transform human-computer interaction,significantly enhancing accessibility and productivity. However, existingbenchmarks either lack an interactive environment or are limited toenvironments specific to certain applications or domains, failing to reflectthe diverse and complex nature of real-world computer use, thereby limiting thescope of tasks and agent scalability. To address this issue, we introduceOSWorld, the first-of-its-kind scalable, real computer environment formultimodal agents, supporting task setup, execution-based evaluation, andinteractive learning across various operating systems such as Ubuntu, Windows,and macOS. OSWorld can serve as a unified, integrated computer environment forassessing open-ended computer tasks that involve arbitrary applications.Building upon OSWorld, we create a benchmark of 369 computer tasks involvingreal web and desktop apps in open domains, OS file I/O, and workflows spanningmultiple applications. Each task example is derived from real-world computeruse cases and includes a detailed initial state setup configuration and acustom execution-based evaluation script for reliable, reproducible evaluation.Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorldreveals significant deficiencies in their ability to serve as computerassistants. While humans can accomplish over 72.36% of the tasks, the bestmodel achieves only 12.24% success, primarily struggling with GUI grounding andoperational knowledge. Comprehensive analysis using OSWorld provides valuableinsights for developing multimodal generalist agents that were not possiblewith previous benchmarks. Our code, environment, baseline models, and data arepublicly available at https://os-world.github.io.</description><author>Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu</author><pubDate>Thu, 11 Apr 2024 18:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07972v1</guid></item><item><title>Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models</title><link>http://arxiv.org/abs/2404.07973v1</link><description>While Ferret seamlessly integrates regional understanding into the LargeLanguage Model (LLM) to facilitate its referring and grounding capability, itposes certain limitations: constrained by the pre-trained fixed visual encoderand failed to perform well on broader tasks. In this work, we unveil Ferret-v2,a significant upgrade to Ferret, with three key designs. (1) Any resolutiongrounding and referring: A flexible approach that effortlessly handles higherimage resolution, improving the model's ability to process and understandimages in greater detail. (2) Multi-granularity visual encoding: By integratingthe additional DINOv2 encoder, the model learns better and diverse underlyingcontexts for global and fine-grained visual information. (3) A three-stagetraining paradigm: Besides image-caption alignment, an additional stage isproposed for high-resolution dense alignment before the final instructiontuning. Experiments show that Ferret-v2 provides substantial improvements overFerret and other state-of-the-art methods, thanks to its high-resolutionscaling and fine-grained visual processing.</description><author>Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, Yinfei Yang</author><pubDate>Thu, 11 Apr 2024 18:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07973v1</guid></item><item><title>Differentiable All-pole Filters for Time-varying Audio Systems</title><link>http://arxiv.org/abs/2404.07970v1</link><description>Infinite impulse response filters are an essential building block of manytime-varying audio systems, such as audio effects and synthesisers. However,their recursive structure impedes end-to-end training of these systems usingautomatic differentiation. Although non-recursive filter approximations likefrequency sampling and frame-based processing have been proposed and widelyused in previous works, they cannot accurately reflect the gradient of theoriginal system. We alleviate this difficulty by re-expressing a time-varyingall-pole filter to backpropagate the gradients through itself, so the filterimplementation is not bound to the technical limitations of automaticdifferentiation frameworks. This implementation can be employed within anyaudio system containing filters with poles for efficient gradient evaluation.We demonstrate its training efficiency and expressive capabilities formodelling real-world dynamic audio systems on a phaser, time-varyingsubtractive synthesiser, and feed-forward compressor. We make our codeavailable and provide the trained audio effect and synth models in a VST pluginat https://christhetree.github.io/all_pole_filters/.</description><author>Chin-Yun Yu, Christopher Mitcheltree, Alistair Carson, Stefan Bilbao, Joshua D. Reiss, György Fazekas</author><pubDate>Thu, 11 Apr 2024 18:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07970v1</guid></item><item><title>Disguised Copyright Infringement of Latent Diffusion Models</title><link>http://arxiv.org/abs/2404.06737v2</link><description>Copyright infringement may occur when a generative model produces samplessubstantially similar to some copyrighted data that it had access to during thetraining phase. The notion of access usually refers to including copyrightedsamples directly in the training dataset, which one may inspect to identify aninfringement. We argue that such visual auditing largely overlooks a concealedcopyright infringement, where one constructs a disguise that looks drasticallydifferent from the copyrighted sample yet still induces the effect of trainingLatent Diffusion Models on it. Such disguises only require indirect access tothe copyrighted material and cannot be visually distinguished, thus easilycircumventing the current auditing tools. In this paper, we provide a betterunderstanding of such disguised copyright infringement by uncovering thedisguises generation algorithm, the revelation of the disguises, andimportantly, how to detect them to augment the existing toolbox. Additionally,we introduce a broader notion of acknowledgment for comprehending such indirectaccess.</description><author>Yiwei Lu, Matthew Y. R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu</author><pubDate>Thu, 11 Apr 2024 18:54:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06737v2</guid></item><item><title>VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing</title><link>http://arxiv.org/abs/2404.06674v2</link><description>We present VoiceShop, a novel speech-to-speech framework that can modifymultiple attributes of speech, such as age, gender, accent, and speech style,in a single forward pass while preserving the input speaker's timbre. Previousworks have been constrained to specialized models that can only edit theseattributes individually and suffer from the following pitfalls: the magnitudeof the conversion effect is weak, there is no zero-shot capability forout-of-distribution speakers, or the synthesized outputs exhibit undesirabletimbre leakage. Our work proposes solutions for each of these issues in asimple modular framework based on a conditional diffusion backbone model withoptional normalizing flow-based and sequence-to-sequence speakerattribute-editing modules, whose components can be combined or removed duringinference to meet a wide array of tasks without additional model finetuning.Audio samples are available at \url{https://voiceshopai.github.io}.</description><author>Philip Anastassiou, Zhenyu Tang, Kainan Peng, Dongya Jia, Jiaxin Li, Ming Tu, Yuping Wang, Yuxuan Wang, Mingbo Ma</author><pubDate>Thu, 11 Apr 2024 18:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06674v2</guid></item><item><title>Rho-1: Not All Tokens Are What You Need</title><link>http://arxiv.org/abs/2404.07965v1</link><description>Previous language model pre-training methods have uniformly applied anext-token prediction loss to all training tokens. Challenging this norm, weposit that "Not all tokens in a corpus are equally important for language modeltraining". Our initial analysis delves into token-level training dynamics oflanguage model, revealing distinct loss patterns for different tokens.Leveraging these insights, we introduce a new language model called Rho-1.Unlike traditional LMs that learn to predict every next token in a corpus,Rho-1 employs Selective Language Modeling (SLM), which selectively trains onuseful tokens that aligned with the desired distribution. This approachinvolves scoring pretraining tokens using a reference model, and then trainingthe language model with a focused loss on tokens with higher excess loss. Whencontinual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absoluteimprovement in few-shot accuracy of up to 30% in 9 math tasks. Afterfine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of thepretraining tokens. Furthermore, when pretraining on 80B general tokens, Rho-1achieves 6.8% average enhancement across 15 diverse tasks, increasing bothefficiency and performance of the language model pre-training.</description><author>Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen</author><pubDate>Thu, 11 Apr 2024 18:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07965v1</guid></item><item><title>Lyapunov-stable Neural Control for State and Output Feedback: A Novel Formulation for Efficient Synthesis and Verification</title><link>http://arxiv.org/abs/2404.07956v1</link><description>Learning-based neural network (NN) control policies have shown impressiveempirical performance in a wide range of tasks in robotics and control.However, formal (Lyapunov) stability guarantees over the region-of-attraction(ROA) for NN controllers with nonlinear dynamical systems are challenging toobtain, and most existing approaches rely on expensive solvers such assums-of-squares (SOS), mixed-integer programming (MIP), or satisfiabilitymodulo theories (SMT). In this paper, we demonstrate a new framework forlearning NN controllers together with Lyapunov certificates using fastempirical falsification and strategic regularizations. We propose a novelformulation that defines a larger verifiable region-of-attraction (ROA) thanshown in the literature, and refines the conventional restrictive constraintson Lyapunov derivatives to focus only on certifiable ROAs. The Lyapunovcondition is rigorously verified post-hoc using branch-and-bound with scalablelinear bound propagation-based NN verification techniques. The approach isefficient and flexible, and the full training and verification procedure isaccelerated on GPUs without relying on expensive solvers for SOS, MIP, nor SMT.The flexibility and efficiency of our framework allow us to demonstrateLyapunov-stable output feedback control with synthesized NN-based controllersand NN-based observers with formal stability guarantees, for the first time inliterature. Source code athttps://github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers.</description><author>Lujie Yang, Hongkai Dai, Zhouxing Shi, Cho-Jui Hsieh, Russ Tedrake, Huan Zhang</author><pubDate>Thu, 11 Apr 2024 18:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07956v1</guid></item><item><title>Performance is not enough: the story told by a Rashomon quartet</title><link>http://arxiv.org/abs/2302.13356v4</link><description>The usual goal of supervised learning is to find the best model, the one thatoptimizes a particular performance measure. However, what if the explanationprovided by this model is completely different from another model and differentagain from another model despite all having similarly good fit statistics? Isit possible that the equally effective models put the spotlight on differentrelationships in the data? Inspired by Anscombe's quartet, this paperintroduces a Rashomon Quartet, i.e. a set of four models built on a syntheticdataset which have practically identical predictive performance. However, thevisual exploration reveals distinct explanations of the relations in the data.This illustrative example aims to encourage the use of methods for modelvisualization to compare predictive models beyond their performance.</description><author>Przemyslaw Biecek, Hubert Baniecki, Mateusz Krzyzinski, Dianne Cook</author><pubDate>Thu, 11 Apr 2024 18:46:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13356v4</guid></item><item><title>Taming Stable Diffusion for Text to 360° Panorama Image Generation</title><link>http://arxiv.org/abs/2404.07949v1</link><description>Generative models, e.g., Stable Diffusion, have enabled the creation ofphotorealistic images from text prompts. Yet, the generation of 360-degreepanorama images from text remains a challenge, particularly due to the dearthof paired text-panorama data and the domain gap between panorama andperspective images. In this paper, we introduce a novel dual-branch diffusionmodel named PanFusion to generate a 360-degree image from a text prompt. Weleverage the stable diffusion model as one branch to provide prior knowledge innatural image generation and register it to another panorama branch forholistic image generation. We propose a unique cross-attention mechanism withprojection awareness to minimize distortion during the collaborative denoisingprocess. Our experiments validate that PanFusion surpasses existing methodsand, thanks to its dual-branch structure, can integrate additional constraintslike room layout for customized panorama outputs. Code is available athttps://chengzhag.github.io/publication/panfusion.</description><author>Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, Jianfei Cai</author><pubDate>Thu, 11 Apr 2024 18:46:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07949v1</guid></item><item><title>On Unified Prompt Tuning for Request Quality Assurance in Public Code Review</title><link>http://arxiv.org/abs/2404.07942v1</link><description>Public Code Review (PCR) can be implemented through a Software QuestionAnswering (SQA) community, which facilitates high knowledge dissemination.Current methods mainly focus on the reviewer's perspective, including finding acapable reviewer, predicting comment quality, and recommending/generatingreview comments. Our intuition is that satisfying review necessity requests canincrease their visibility, which in turn is a prerequisite for better reviewresponses. To this end, we propose a unified framework called UniPCR tocomplete developer-based request quality assurance (i.e., predicting requestnecessity and recommending tags subtask) under a Masked Language Model (MLM).Specifically, we reformulate both subtasks via 1) text prompt tuning, whichconverts two subtasks into MLM by constructing prompt templates using hardprompt; 2) code prefix tuning, which optimizes a small segment of generatedcontinuous vectors as the prefix of the code representation using soft prompt.Experimental results on the Public Code Review dataset for the time span2011-2022 demonstrate that our UniPCR framework adapts to the two subtasks andoutperforms comparable accuracy-based results with state-of-the-art methods forrequest quality assurance. These conclusions highlight the effectiveness of ourunified framework from the developer's perspective in public code review.</description><author>Xinyu Chen, Lin Li, Rui Zhang, Peng Liang</author><pubDate>Thu, 11 Apr 2024 18:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07942v1</guid></item><item><title>Neural population geometry and optimal coding of tasks with shared latent structure</title><link>http://arxiv.org/abs/2402.16770v2</link><description>Humans and animals can recognize latent structures in their environment andapply this information to efficiently navigate the world. However, it remainsunclear what aspects of neural activity contribute to these computationalcapabilities. Here, we develop an analytical theory linking the geometry of aneural population's activity to the generalization performance of a linearreadout on a set of tasks that depend on a common latent structure. We showthat four geometric measures of the activity determine performance acrosstasks. Using this theory, we find that experimentally observed disentangledrepresentations naturally emerge as an optimal solution to the multi-tasklearning problem. When data is scarce, these optimal neural codes compress lessinformative latent variables, and when data is abundant, they expand thesevariables in the state space. We validate our theory using macaque ventralstream recordings. Our results therefore tie population geometry to multi-tasklearning.</description><author>Albert J. Wakhloo, Will Slatton, SueYeon Chung</author><pubDate>Thu, 11 Apr 2024 18:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16770v2</guid></item><item><title>AfriMTE and AfriCOMET: Enhancing COMET to Embrace Under-resourced African Languages</title><link>http://arxiv.org/abs/2311.09828v2</link><description>Despite the recent progress on scaling multilingual machine translation (MT)to several under-resourced African languages, accurately measuring thisprogress remains challenging, since evaluation is often performed on n-grammatching metrics such as BLEU, which typically show a weaker correlation withhuman judgments. Learned metrics such as COMET have higher correlation;however, the lack of evaluation data with human ratings for under-resourcedlanguages, complexity of annotation guidelines like Multidimensional QualityMetrics (MQM), and limited language coverage of multilingual encoders havehampered their applicability to African languages. In this paper, we addressthese challenges by creating high-quality human evaluation data with simplifiedMQM guidelines for error detection and direct assessment (DA) scoring for 13typologically diverse African languages. Furthermore, we develop AfriCOMET:COMET evaluation metrics for African languages by leveraging DA data fromwell-resourced languages and an African-centric multilingual encoder(AfroXLM-R) to create the state-of-the-art MT evaluation metrics for Africanlanguages with respect to Spearman-rank correlation with human judgments(0.441).</description><author>Jiayi Wang, David Ifeoluwa Adelani, Sweta Agrawal, Marek Masiak, Ricardo Rei, Eleftheria Briakou, Marine Carpuat, Xuanli He, Sofia Bourhim, Andiswa Bukula, Muhidin Mohamed, Temitayo Olatoye, Tosin Adewumi, Hamam Mokayede, Christine Mwase, Wangui Kimotho, Foutse Yuehgoh, Anuoluwapo Aremu, Jessica Ojo, Shamsuddeen Hassan Muhammad, Salomey Osei, Abdul-Hakeem Omotayo, Chiamaka Chukwuneke, Perez Ogayo, Oumaima Hourrane, Salma El Anigri, Lolwethu Ndolela, Thabiso Mangwana, Shafie Abdi Mohamed, Ayinde Hassan, Oluwabusayo Olufunke Awoyomi, Lama Alkhaled, Sana Al-Azzawi, Naome A. Etori, Millicent Ochieng, Clemencia Siro, Samuel Njoroge, Eric Muchiri, Wangari Kimotho, Lyse Naomi Wamba Momo, Daud Abolade, Simbiat Ajao, Iyanuoluwa Shode, Ricky Macharm, Ruqayya Nasir Iro, Saheed S. Abdullahi, Stephen E</author><pubDate>Thu, 11 Apr 2024 18:38:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09828v2</guid></item><item><title>Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method</title><link>http://arxiv.org/abs/2404.07937v1</link><description>We study the quadratic prediction error method -- i.e., nonlinear leastsquares -- for a class of time-varying parametric predictor models satisfying acertain identifiability condition. While this method is known to asymptoticallyachieve the optimal rate for a wide range of problems, there have been nonon-asymptotic results matching these optimal rates outside of a select few,typically linear, model classes. By leveraging modern tools from learning withdependent data, we provide the first rate-optimal non-asymptotic analysis ofthis method for our more general setting of nonlinearly parametrized modelclasses. Moreover, we show that our results can be applied to a particularclass of identifiable AutoRegressive Moving Average (ARMA) models, resulting inthe first optimal non-asymptotic rates for identification of ARMA models.</description><author>Charis Stamouli, Ingvar Ziemann, George J. Pappas</author><pubDate>Thu, 11 Apr 2024 18:36:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07937v1</guid></item><item><title>Goal Recognition via Linear Programming</title><link>http://arxiv.org/abs/2404.07934v1</link><description>Goal Recognition is the task by which an observer aims to discern the goalsthat correspond to plans that comply with the perceived behavior of subjectagents given as a sequence of observations. Research on Goal Recognition asPlanning encompasses reasoning about the model of a planning task, theobservations, and the goals using planning techniques, resulting in veryefficient recognition approaches. In this article, we design novel recognitionapproaches that rely on the Operator-Counting framework, proposing newconstraints, and analyze their constraints' properties both theoretically andempirically. The Operator-Counting framework is a technique that efficientlycomputes heuristic estimates of cost-to-goal using Integer/Linear Programming(IP/LP). In the realm of theory, we prove that the new constraints providelower bounds on the cost of plans that comply with observations. We alsoprovide an extensive empirical evaluation to assess how the new constraintsimprove the quality of the solution, and we found that they are especiallyinformed in deciding which goals are unlikely to be part of the solution. Ournovel recognition approaches have two pivotal advantages: first, they employnew IP/LP constraints for efficiently recognizing goals; second, we show howthe new IP/LP constraints can improve the recognition of goals under bothpartial and noisy observability.</description><author>Felipe Meneguzzi, Luísa R. de A. Santos, Ramon Fraga Pereira, André G. Pereira</author><pubDate>Thu, 11 Apr 2024 18:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07934v1</guid></item><item><title>Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation</title><link>http://arxiv.org/abs/2404.07933v1</link><description>Inferring scene geometry from images via Structure from Motion is along-standing and fundamental problem in computer vision. While classicalapproaches and, more recently, depth map predictions only focus on the visibleparts of a scene, the task of scene completion aims to reason about geometryeven in occluded regions. With the popularity of neural radiance fields(NeRFs), implicit representations also became popular for scene completion bypredicting so-called density fields. Unlike explicit approaches. e.g.voxel-based methods, density fields also allow for accurate depth predictionand novel-view synthesis via image-based rendering. In this work, we propose tofuse the scene reconstruction from multiple images and distill this knowledgeinto a more accurate single-view scene reconstruction. To this end, we proposeMulti-View Behind the Scenes (MVBTS) to fuse density fields from multiple posedimages, trained fully self-supervised only from image data. Using knowledgedistillation, we use MVBTS to train a single-view scene completion network viadirect supervision called KDBTS. It achieves state-of-the-art performance onoccupancy prediction, especially in occluded regions.</description><author>Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers</author><pubDate>Thu, 11 Apr 2024 18:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07933v1</guid></item><item><title>FusionMamba: Efficient Image Fusion with State Space Model</title><link>http://arxiv.org/abs/2404.07932v1</link><description>Image fusion aims to generate a high-resolution multi/hyper-spectral image bycombining a high-resolution image with limited spectral information and alow-resolution image with abundant spectral data. Current deep learning(DL)-based methods for image fusion primarily rely on CNNs or Transformers toextract features and merge different types of data. While CNNs are efficient,their receptive fields are limited, restricting their capacity to captureglobal context. Conversely, Transformers excel at learning global informationbut are hindered by their quadratic complexity. Fortunately, recentadvancements in the State Space Model (SSM), particularly Mamba, offer apromising solution to this issue by enabling global awareness with linearcomplexity. However, there have been few attempts to explore the potential ofSSM in information fusion, which is a crucial ability in domains like imagefusion. Therefore, we propose FusionMamba, an innovative method for efficientimage fusion. Our contributions mainly focus on two aspects. Firstly,recognizing that images from different sources possess distinct properties, weincorporate Mamba blocks into two U-shaped networks, presenting a novelarchitecture that extracts spatial and spectral features in an efficient,independent, and hierarchical manner. Secondly, to effectively combine spatialand spectral information, we extend the Mamba block to accommodate dual inputs.This expansion leads to the creation of a new module called the FusionMambablock, which outperforms existing fusion techniques such as concatenation andcross-attention. To validate FusionMamba's effectiveness, we conduct a seriesof experiments on five datasets related to three image fusion tasks. Thequantitative and qualitative evaluation results demonstrate that our methodachieves state-of-the-art (SOTA) performance, underscoring the superiority ofFusionMamba.</description><author>Siran Peng, Xiangyu Zhu, Haoyu Deng, Zhen Lei, Liang-Jian Deng</author><pubDate>Thu, 11 Apr 2024 18:29:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07932v1</guid></item><item><title>Parameter Hierarchical Optimization for Visible-Infrared Person Re-Identification</title><link>http://arxiv.org/abs/2404.07930v1</link><description>Visible-infrared person re-identification (VI-reID) aims at matchingcross-modality pedestrian images captured by disjoint visible or infraredcameras. Existing methods alleviate the cross-modality discrepancies viadesigning different kinds of network architectures. Different from availablemethods, in this paper, we propose a novel parameter optimizing paradigm,parameter hierarchical optimization (PHO) method, for the task of VI-ReID. Itallows part of parameters to be directly optimized without any training, whichnarrows the search space of parameters and makes the whole network more easierto be trained. Specifically, we first divide the parameters into differenttypes, and then introduce a self-adaptive alignment strategy (SAS) toautomatically align the visible and infrared images through transformation.Considering that features in different dimension have varying importance, wedevelop an auto-weighted alignment learning (AAL) module that can automaticallyweight features according to their importance. Importantly, in the alignmentprocess of SAS and AAL, all the parameters are immediately optimized withoptimization principles rather than training the whole network, which yields abetter parameter training manner. Furthermore, we establish the cross-modalityconsistent learning (CCL) loss to extract discriminative person representationswith translation consistency. We provide both theoretical justification andempirical evidence that our proposed PHO method outperform existing VI-reIDapproaches.</description><author>Zeng YU, Yunxiao Shi</author><pubDate>Thu, 11 Apr 2024 18:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07930v1</guid></item><item><title>Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space</title><link>http://arxiv.org/abs/2307.01177v2</link><description>To characterize the function space explored by neural networks (NNs) is animportant aspect of learning theory. In this work, noticing that a multi-layerNN generates implicitly a hierarchy of reproducing kernel Hilbert spaces(RKHSs) - named a neural Hilbert ladder (NHL) - we define the function space asan infinite union of RKHSs, which generalizes the existing Barron space theoryof two-layer NNs. We then establish several theoretical properties of the newspace. First, we prove a correspondence between functions expressed by L-layerNNs and those belonging to L-level NHLs. Second, we prove generalizationguarantees for learning an NHL with a controlled complexity measure. Third, wederive a non-Markovian dynamics of random fields that governs the evolution ofthe NHL which is induced by the training of multi-layer NNs in aninfinite-width mean-field limit. Fourth, we show examples of depth separationin NHLs under the ReLU activation function. Finally, we perform numericalexperiments to illustrate the feature learning aspect of NN training throughthe lens of NHLs.</description><author>Zhengdao Chen</author><pubDate>Thu, 11 Apr 2024 18:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01177v2</guid></item><item><title>BCDDO: Binary Child Drawing Development Optimization</title><link>http://arxiv.org/abs/2308.01270v3</link><description>A lately created metaheuristic algorithm called Child Drawing DevelopmentOptimization (CDDO) has proven to be effective in a number of benchmark tests.A Binary Child Drawing Development Optimization (BCDDO) is suggested forchoosing the wrapper features in this study. To achieve the best classificationaccuracy, a subset of crucial features is selected using the suggested BCDDO.The proposed feature selection technique's efficiency and effectiveness areassessed using the Harris Hawk, Grey Wolf, Salp, and Whale optimizationalgorithms. The suggested approach has significantly outperformed thepreviously discussed techniques in the area of feature selection to increaseclassification accuracy. Moderate COVID, breast cancer, and big COVID are thethree datasets utilized in this study. The classification accuracy for each ofthe three datasets was (98.75, 98.83%, and 99.36) accordingly.</description><author>Abubakr S. Issa, Yossra H. Ali, Tarik A. Rashid</author><pubDate>Thu, 11 Apr 2024 18:21:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01270v3</guid></item><item><title>Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation</title><link>http://arxiv.org/abs/2404.07926v1</link><description>In this position paper, we discuss the potential for leveraging LLMs asinteractive research tools to facilitate collaboration between human coders andAI to effectively annotate online risk data at scale. Collaborative human-AIlabeling is a promising approach to annotating large-scale and complex data forvarious tasks. Yet, tools and methods to support effective human-AIcollaboration for data annotation are under-studied. This gap is pertinentbecause co-labeling tasks need to support a two-way interactive discussion thatcan add nuance and context, particularly in the context of online risk, whichis highly subjective and contextualized. Therefore, we provide some of theearly benefits and challenges of using LLMs-based tools for risk annotation andsuggest future directions for the HCI research community to leverage LLMs asresearch tools to facilitate human-AI collaboration in contextualized onlinedata annotation. Our research interests align very well with the purposes ofthe LLMs as Research Tools workshop to identify ongoing applications andchallenges of using LLMs to work with data in HCI research. We anticipatelearning valuable insights from organizers and participants into how LLMs canhelp reshape the HCI community's methods for working with data.</description><author>Jinkyung Park, Pamela Wisniewski, Vivek Singh</author><pubDate>Thu, 11 Apr 2024 18:20:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07926v1</guid></item><item><title>Generating High-Precision Force Fields for Molecular Dynamics Simulations to Study Chemical Reaction Mechanisms using Molecular Configuration Transformer</title><link>http://arxiv.org/abs/2401.00499v3</link><description>Theoretical studies on chemical reaction mechanisms have been crucial inorganic chemistry. Traditionally, calculating the manually constructedmolecular conformations of transition states for chemical reactions usingquantum chemical calculations is the most commonly used method. However, thisway is heavily dependent on individual experience and chemical intuition. Inour previous study, we proposed a research paradigm that uses enhanced samplingin molecular dynamics simulations to study chemical reactions. This approachcan directly simulate the entire process of a chemical reaction. However, thecomputational speed limits the use of high-precision potential energy functionsfor simulations. To address this issue, we present a scheme for traininghigh-precision force fields for molecular modeling using a previously developedgraph-neural-network-based molecular model, molecular configurationtransformer. This potential energy function allows for highly accuratesimulations at a low computational cost, leading to more precise calculationsof the mechanism of chemical reactions. We applied this approach to study aClaisen rearrangement reaction and a Carbonyl insertion reaction catalyzed byManganese.</description><author>Sihao Yuan, Xu Han, Jun Zhang, Zhaoxin Xie, Cheng Fan, Yunlong Xiao, Yi Qin Gao, Yi Isaac Yang</author><pubDate>Thu, 11 Apr 2024 18:15:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00499v3</guid></item><item><title>A Parsimonious Setup for Streamflow Forecasting using CNN-LSTM</title><link>http://arxiv.org/abs/2404.07924v1</link><description>Significant strides have been made in advancing streamflow predictions,notably with the introduction of cutting-edge machine-learning models.Predominantly, Long Short-Term Memories (LSTMs) and Convolution Neural Networks(CNNs) have been widely employed in this domain. While LSTMs are applicable inboth rainfall-runoff and time series settings, CNN-LSTMs have primarily beenutilized in rainfall-runoff scenarios. In this study, we extend the applicationof CNN-LSTMs to time series settings, leveraging lagged streamflow data inconjunction with precipitation and temperature data to predict streamflow. Ourresults show a substantial improvement in predictive performance in 21 out of32 HUC8 basins in Nebraska, showcasing noteworthy increases in the Kling-GuptaEfficiency (KGE) values. These results highlight the effectiveness of CNN-LSTMsin time series settings, particularly for spatiotemporal hydrological modeling,for more accurate and robust streamflow predictions.</description><author>Sudan Pokharel, Tirthankar Roy</author><pubDate>Thu, 11 Apr 2024 18:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07924v1</guid></item><item><title>LaVy: Vietnamese Multimodal Large Language Model</title><link>http://arxiv.org/abs/2404.07922v1</link><description>Large Language Models (LLMs) and Multimodal Large language models (MLLMs)have taken the world by storm with impressive abilities in complex reasoningand linguistic comprehension. Meanwhile there are plethora of works related toVietnamese Large Language Models, the lack of high-quality resources inmultimodality limits the progress of Vietnamese MLLMs. In this paper, wepioneer in address this by introducing LaVy, a state-of-the-art VietnameseMLLM, and we also introduce LaVy-Bench benchmark designated for evaluatingMLLMs's understanding on Vietnamese visual language tasks. All code and modelweights are public at https://github.com/baochi0212/LaVy</description><author>Chi Tran, Huong Le Thanh</author><pubDate>Thu, 11 Apr 2024 18:09:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07922v1</guid></item><item><title>AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs</title><link>http://arxiv.org/abs/2404.07921v1</link><description>As large language models (LLMs) become increasingly prevalent and integratedinto autonomous systems, ensuring their safety is imperative. Despitesignificant strides toward safety alignment, recent workGCG~\citep{zou2023universal} proposes a discrete token optimization algorithmand selects the single suffix with the lowest loss to successfully jailbreakaligned LLMs. In this work, we first discuss the drawbacks of solely pickingthe suffix with the lowest loss during GCG optimization for jailbreaking anduncover the missed successful suffixes during the intermediate steps. Moreover,we utilize those successful suffixes as training data to learn a generativemodel, named AmpleGCG, which captures the distribution of adversarial suffixesgiven a harmful query and enables the rapid generation of hundreds of suffixesfor any harmful queries in seconds. AmpleGCG achieves near 100\% attack successrate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing twostrongest attack baselines. More interestingly, AmpleGCG also transfersseamlessly to attack different models, including closed-source LLMs, achievinga 99\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impactof GCG by training a generative model of adversarial suffixes that is universalto any harmful queries and transferable from attacking open-source LLMs toclosed-source LLMs. In addition, it can generate 200 adversarial suffixes forone harmful query in only 4 seconds, rendering it more challenging to defend.</description><author>Zeyi Liao, Huan Sun</author><pubDate>Thu, 11 Apr 2024 18:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07921v1</guid></item><item><title>Low-rank Adaptation for Spatio-Temporal Forecasting</title><link>http://arxiv.org/abs/2404.07919v1</link><description>Spatio-temporal forecasting is crucial in real-world dynamic systems,predicting future changes using historical data from diverse locations.Existing methods often prioritize the development of intricate neural networksto capture the complex dependencies of the data, yet their accuracy fails toshow sustained improvement. Besides, these methods also overlook nodeheterogeneity, hindering customized prediction modules from handling diverseregional nodes effectively. In this paper, our goal is not to propose a newmodel but to present a novel low-rank adaptation framework as an off-the-shelfplugin for existing spatial-temporal prediction models, termed ST-LoRA, whichalleviates the aforementioned problems through node-level adjustments.Specifically, we first tailor a node adaptive low-rank layer comprisingmultiple trainable low-rank matrices. Additionally, we devise a multi-layerresidual fusion stacking module, injecting the low-rank adapters into predictormodules of various models. Across six real-world traffic datasets and sixdifferent types of spatio-temporal prediction models, our approach minimallyincreases the parameters and training time of the original models by less than4%, still achieving consistent and sustained performance enhancement.</description><author>Weilin Ruan, Wei Chen, Xilin Dang, Jianxiang Zhou, Weichuang Li, Xu Liu, Yuxuan Liang</author><pubDate>Thu, 11 Apr 2024 18:04:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07919v1</guid></item><item><title>DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation</title><link>http://arxiv.org/abs/2404.07917v1</link><description>This research introduces DesignQA, a novel benchmark aimed at evaluating theproficiency of multimodal large language models (MLLMs) in comprehending andapplying engineering requirements in technical documentation. Developed with afocus on real-world engineering challenges, DesignQA uniquely combinesmultimodal data-including textual design requirements, CAD images, andengineering drawings-derived from the Formula SAE student competition.Different from many existing MLLM benchmarks, DesignQA containsdocument-grounded visual questions where the input image and input documentcome from different sources. The benchmark features automatic evaluationmetrics and is divided into segments-Rule Comprehension, Rule Compliance, andRule Extraction-based on tasks that engineers perform when designing accordingto requirements. We evaluate state-of-the-art models like GPT4 and LLaVAagainst the benchmark, and our study uncovers the existing gaps in MLLMs'abilities to interpret complex engineering documentation. Key findings suggestthat while MLLMs demonstrate potential in navigating technical documents,substantial limitations exist, particularly in accurately extracting andapplying detailed requirements to engineering designs. This benchmark sets afoundation for future advancements in AI-supported engineering designprocesses. DesignQA is publicly available at:https://github.com/anniedoris/design_qa/.</description><author>Anna C. Doris, Daniele Grandi, Ryan Tomich, Md Ferdous Alam, Hyunmin Cheong, Faez Ahmed</author><pubDate>Thu, 11 Apr 2024 17:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07917v1</guid></item><item><title>A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models</title><link>http://arxiv.org/abs/2311.00445v2</link><description>A central component of rational behavior is logical inference: the process ofdetermining which conclusions follow from a set of premises. Psychologists havedocumented several ways in which humans' inferences deviate from the rules oflogic. Do language models, which are trained on text generated by humans,replicate such human biases, or are they able to overcome them? Focusing on thecase of syllogisms -- inferences from two simple premises -- we show that,within the PaLM2 family of transformer language models, larger models are morelogical than smaller ones, and also more logical than humans. At the same time,even the largest models make systematic errors, some of which mirror humanreasoning biases: they show sensitivity to the (irrelevant) ordering of thevariables in the syllogism, and draw confident but incorrect inferences fromparticular syllogisms (syllogistic fallacies). Overall, we find that languagemodels often mimic the human biases included in their training data, but areable to overcome them in some cases.</description><author>Tiwalayo Eisape, MH Tessler, Ishita Dasgupta, Fei Sha, Sjoerd van Steenkiste, Tal Linzen</author><pubDate>Thu, 11 Apr 2024 17:49:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00445v2</guid></item><item><title>Low-Resource Vision Challenges for Foundation Models</title><link>http://arxiv.org/abs/2401.04716v3</link><description>Low-resource settings are well-established in natural language processing,where many languages lack sufficient data for deep learning at scale. However,low-resource problems are under-explored in computer vision. In this paper, weaddress this gap and explore the challenges of low-resource image tasks withvision foundation models. We first collect a benchmark of genuinelylow-resource image data, covering historic maps, circuit diagrams, andmechanical drawings. These low-resource settings all share three challenges:data scarcity, fine-grained differences, and the distribution shift fromnatural images to the specialized domain of interest. While existing foundationmodels have shown impressive generalizability, we find they cannot transferwell to our low-resource tasks. To begin to tackle the challenges oflow-resource vision, we introduce one simple baseline per challenge.Specifically, we i) enlarge the data space by generative models, ii) adopt thebest sub-kernels to encode local regions for fine-grained difference discoveryand iii) learn attention for specialized domains. Experiments on our threelow-resource tasks demonstrate our proposals already provide a better baselinethan transfer learning, data augmentation, and fine-grained methods. Thishighlights the unique characteristics and challenges of low-resource vision forfoundation models that warrant further investigation. Project page:https://xiaobai1217.github.io/Low-Resource-Vision/.</description><author>Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek</author><pubDate>Thu, 11 Apr 2024 17:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04716v3</guid></item><item><title>HGRN2: Gated Linear RNNs with State Expansion</title><link>http://arxiv.org/abs/2404.07904v1</link><description>Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstratedcompetitive training speed and performance in language modeling, while offeringefficient inference. However, the recurrent state size of HGRN remainsrelatively small, which limits its expressiveness.To address this issue,inspired by linear attention, we introduce a simple outer-product-based stateexpansion mechanism so that the recurrent state size can be significantlyenlarged without introducing any additional parameters. The linear attentionform also allows for hardware-efficient training.Our extensive experimentsverify the advantage of HGRN2 over HGRN1 in language modeling, imageclassification, and Long Range Arena.Our largest 3B HGRN2 model slightlyoutperforms Mamba and LLaMa Architecture Transformer for language modeling in acontrolled experiment setting; and performs competitively with many open-source3B models in downstream evaluation while using much fewer total trainingtokens.</description><author>Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong</author><pubDate>Thu, 11 Apr 2024 17:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07904v1</guid></item><item><title>Me LLaMA: Foundation Large Language Models for Medical Applications</title><link>http://arxiv.org/abs/2402.12749v4</link><description>Recent advancements in large language models (LLMs) such as ChatGPT and LLaMAhave hinted at their potential to revolutionize medical applications, yet theirapplication in clinical settings often reveals limitations due to a lack ofspecialized training on medical-specific data. In response to this challenge,this study introduces Me-LLaMA, a novel medical LLM family that includesfoundation models - Me-LLaMA 13/70B, along with their chat-enhanced versions -Me-LLaMA 13/70B-chat, developed through continual pre-training and instructiontuning of LLaMA2 using large medical datasets. Our methodology leverages acomprehensive domain-specific data suite, including a large-scale, continualpre-training dataset with 129B tokens, an instruction tuning dataset with 214ksamples, and a new medical evaluation benchmark (MIBE) across six criticalmedical tasks with 12 datasets. Our extensive evaluation using the MIBE showsthat Me-LLaMA models achieve overall better performance than existingopen-source medical LLMs in zero-shot, few-shot and supervised learningabilities. With task-specific instruction tuning, Me-LLaMA models outperformChatGPT on 7 out of 8 datasets and GPT-4 on 5 out of 8 datasets. In addition,we investigated the catastrophic forgetting problem, and our results show thatMe-LLaMA models outperform other open-source medical LLMs in mitigating thisissue. Me-LLaMA is one of the largest open-source medical foundation LLMs thatuse both biomedical and clinical data. It exhibits superior performance acrossboth general and medical tasks compared to other open-source medical LLMs,rendering it an attractive choice for medical AI applications. We release ourmodels, datasets, and evaluation scripts at:https://github.com/BIDS-Xu-Lab/Me-LLaMA.</description><author>Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Xinyu Zhou, Huan He, Lucila Ohno-Machado, Yonghui Wu, Hua Xu, Jiang Bian</author><pubDate>Thu, 11 Apr 2024 17:42:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12749v4</guid></item><item><title>KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing</title><link>http://arxiv.org/abs/2403.15304v2</link><description>Knowledge Tracing (KT) is concerned with predicting students' futureperformance on learning items in intelligent tutoring systems. Learning itemsare tagged with skill labels called knowledge concepts (KCs). Many KT modelsexpand the sequence of item-student interactions into KC-student interactionsby replacing learning items with their constituting KCs. This often results ina longer sequence length. This approach addresses the issue of sparseitem-student interactions and minimises model parameters. However, two problemshave been identified with such models. The first problem is the model's ability to learn correlations between KCsbelonging to the same item, which can result in the leakage of ground truthlabels and hinder performance. This problem can lead to a significant decreasein performance on datasets with a higher number of KCs per item. The secondproblem is that the available benchmark implementations ignore accounting forchanges in sequence length when expanding KCs, leading to different modelsbeing tested with varying sequence lengths but still compared against the samebenchmark. To address these problems, we introduce a general masking framework thatmitigates the first problem and enhances the performance of such KT modelswhile preserving the original model architecture without significantalterations. Additionally, we introduce KTbench, an open-source benchmarklibrary designed to ensure the reproducibility of this work while mitigatingthe second problem.</description><author>Yahya Badran, Christine Preisach</author><pubDate>Thu, 11 Apr 2024 17:39:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15304v2</guid></item><item><title>High-Dimension Human Value Representation in Large Language Models</title><link>http://arxiv.org/abs/2404.07900v1</link><description>The widespread application of Large Language Models (LLMs) across varioustasks and fields has necessitated the alignment of these models with humanvalues and preferences. Given various approaches of human value alignment,ranging from Reinforcement Learning with Human Feedback (RLHF), toconstitutional learning, etc. there is an urgent need to understand the scopeand nature of human values injected into these models before their release.There is also a need for model alignment without a costly large scale humanannotation effort. We propose UniVaR, a high-dimensional representation ofhuman value distributions in LLMs, orthogonal to model architecture andtraining data. Trained from the value-relevant output of eight multilingualLLMs and tested on the output from four multilingual LLMs, namely LlaMA2,ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare thedistribution of human values embedded in different LLMs with different langaugesources. Through UniVaR, we explore how different LLMs prioritize variousvalues in different languages and cultures, shedding light on the complexinterplay between human values and language modeling.</description><author>Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, Pascale Fung</author><pubDate>Thu, 11 Apr 2024 17:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07900v1</guid></item><item><title>Anomaly Detection in Power Grids via Context-Agnostic Learning</title><link>http://arxiv.org/abs/2404.07898v1</link><description>An important tool grid operators use to safeguard against failures, whethernaturally occurring or malicious, involves detecting anomalies in the powersystem SCADA data. In this paper, we aim to solve a real-time anomaly detectionproblem. Given time-series measurement values coming from a fixed set ofsensors on the grid, can we identify anomalies in the network topology ormeasurement data? Existing methods, primarily optimization-based, mostly useonly a single snapshot of the measurement values and do not scale well with thenetwork size. Recent data-driven ML techniques have shown promise by using acombination of current and historical data for anomaly detection but generallydo not consider physical attributes like the impact of topology orload/generation changes on sensor measurements and thus cannot accommodateregular context-variability in the historical data. To address this gap, wepropose a novel context-aware anomaly detection algorithm, GridCAL, thatconsiders the effect of regular topology and load/generation changes. Thisalgorithm converts the real-time power flow measurements to context-agnosticvalues, which allows us to analyze measurement coming from different gridcontexts in an aggregate fashion, enabling us to derive a unified statisticalmodel that becomes the basis of anomaly detection. Through numericalsimulations on networks up to 2383 nodes, we show that our approach isaccurate, outperforming state-of-the-art approaches, and is computationallyefficient.</description><author>SangWoo Park, Amritanshu Pandey</author><pubDate>Thu, 11 Apr 2024 17:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07898v1</guid></item><item><title>EgoGen: An Egocentric Synthetic Data Generator</title><link>http://arxiv.org/abs/2401.08739v2</link><description>Understanding the world in first-person view is fundamental in AugmentedReality (AR). This immersive perspective brings dramatic visual changes andunique challenges compared to third-person views. Synthetic data has empoweredthird-person-view vision models, but its application to embodied egocentricperception tasks remains largely unexplored. A critical challenge lies insimulating natural human movements and behaviors that effectively steer theembodied cameras to capture a faithful egocentric representation of the 3Dworld. To address this challenge, we introduce EgoGen, a new synthetic datagenerator that can produce accurate and rich ground-truth training data foregocentric perception tasks. At the heart of EgoGen is a novel human motionsynthesis model that directly leverages egocentric visual inputs of a virtualhuman to sense the 3D environment. Combined with collision-avoiding motionprimitives and a two-stage reinforcement learning approach, our motionsynthesis model offers a closed-loop solution where the embodied perception andmovement of the virtual human are seamlessly coupled. Compared to previousworks, our model eliminates the need for a pre-defined global path, and isdirectly applicable to dynamic environments. Combined with our easy-to-use andscalable data generation pipeline, we demonstrate EgoGen's efficacy in threetasks: mapping and localization for head-mounted cameras, egocentric cameratracking, and human mesh recovery from egocentric views. EgoGen will be fullyopen-sourced, offering a practical solution for creating realistic egocentrictraining data and aiming to serve as a useful tool for egocentric computervision research. Refer to our project page: https://ego-gen.github.io/.</description><author>Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang</author><pubDate>Thu, 11 Apr 2024 17:35:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08739v2</guid></item><item><title>Optimal Task Assignment and Path Planning using Conflict-Based Search with Precedence and Temporal Constraints</title><link>http://arxiv.org/abs/2402.08772v2</link><description>The Multi-Agent Path Finding (MAPF) problem entails finding collision-freepaths for a set of agents, guiding them from their start to goal locations.However, MAPF does not account for several practical task-related constraints.For example, agents may need to perform actions at goal locations with specificexecution times, adhering to predetermined orders and timeframes. Moreover,goal assignments may not be predefined for agents, and the optimizationobjective may lack an explicit definition. To incorporate task assignment, pathplanning, and a user-defined objective into a coherent framework, this paperexamines the Task Assignment and Path Finding with Precedence and TemporalConstraints (TAPF-PTC) problem. We augment Conflict-Based Search (CBS) tosimultaneously generate task assignments and collision-free paths that adhereto precedence and temporal constraints, maximizing an objective quantified bythe return from a user-defined reward function in reinforcement learning (RL).Experimentally, we demonstrate that our algorithm, CBS-TA-PTC, can solve highlychallenging bomb-defusing tasks with precedence and temporal constraintsefficiently relative to MARL and adapted Target Assignment and Path Finding(TAPF) methods.</description><author>Yu Quan Chong, Jiaoyang Li, Katia Sycara</author><pubDate>Thu, 11 Apr 2024 17:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08772v2</guid></item><item><title>Source-Aware Training Enables Knowledge Attribution in Language Models</title><link>http://arxiv.org/abs/2404.01019v2</link><description>Large language models (LLMs) learn a vast amount of knowledge duringpretraining, but they are often oblivious to the source(s) of such knowledge.We investigate the problem of intrinsic source citation, where LLMs arerequired to cite the pretraining source supporting a generated response.Intrinsic source citation can enhance LLM transparency, interpretability, andverifiability. To give LLMs such ability, we explore source-aware training -- apost pretraining recipe that involves (i) training the LLM to associate uniquesource document identifiers with the knowledge in each document, followed by(ii) an instruction-tuning to teach the LLM to cite a supporting pretrainingsource when prompted. Source-aware training can easily be applied to pretrainedLLMs off the shelf, and diverges minimally from existingpretraining/fine-tuning frameworks. Through experiments on carefully curateddata, we demonstrate that our training recipe can enable faithful attributionto the pretraining data without a substantial impact on the model's qualitycompared to standard pretraining. Our results also highlight the importance ofdata augmentation in achieving attribution. Code and data available here:\url{https://github.com/mukhal/intrinsic-source-citation}</description><author>Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang, Iz Beltagy, Hao Peng</author><pubDate>Thu, 11 Apr 2024 17:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01019v2</guid></item><item><title>Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation</title><link>http://arxiv.org/abs/2311.11321v2</link><description>State-of-the-art methods for conditional average treatment effect (CATE)estimation make widespread use of representation learning. Here, the idea is toreduce the variance of the low-sample CATE estimation by a (potentiallyconstrained) low-dimensional representation. However, low-dimensionalrepresentations can lose information about the observed confounders and thuslead to bias, because of which the validity of representation learning for CATEestimation is typically violated. In this paper, we propose a new,representation-agnostic refutation framework for estimating bounds on therepresentation-induced confounding bias that comes from dimensionalityreduction (or other constraints on the representations) in CATE estimation.First, we establish theoretically under which conditions CATE isnon-identifiable given low-dimensional (constrained) representations. Second,as our remedy, we propose a neural refutation framework which performs partialidentification of CATE or, equivalently, aims at estimating lower and upperbounds of the representation-induced confounding bias. We demonstrate theeffectiveness of our bounds in a series of experiments. In sum, our refutationframework is of direct relevance in practice where the validity of CATEestimation is of importance.</description><author>Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel</author><pubDate>Thu, 11 Apr 2024 17:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11321v2</guid></item><item><title>SE(3)-Stochastic Flow Matching for Protein Backbone Generation</title><link>http://arxiv.org/abs/2310.02391v4</link><description>The computational design of novel protein structures has the potential toimpact numerous scientific disciplines greatly. Toward this goal, we introduceFoldFlow, a series of novel generative models of increasing modeling powerbased on the flow-matching paradigm over $3\mathrm{D}$ rigid motions -- i.e.the group $\text{SE}(3)$ -- enabling accurate modeling of protein backbones. Wefirst introduce FoldFlow-Base, a simulation-free approach to learningdeterministic continuous-time dynamics and matching invariant targetdistributions on $\text{SE}(3)$. We next accelerate training by incorporatingRiemannian optimal transport to create FoldFlow-OT, leading to the constructionof both more simple and stable flows. Finally, we design FoldFlow-SFM, couplingboth Riemannian OT and simulation-free training to learn stochasticcontinuous-time dynamics over $\text{SE}(3)$. Our family of FoldFlow,generative models offers several key advantages over previous approaches to thegenerative modeling of proteins: they are more stable and faster to train thandiffusion-based approaches, and our models enjoy the ability to map anyinvariant source distribution to any invariant target distribution over$\text{SE}(3)$. Empirically, we validate FoldFlow, on protein backbonegeneration of up to $300$ amino acids leading to high-quality designable,diverse, and novel samples.</description><author>Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian Fatras, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, Alexander Tong</author><pubDate>Thu, 11 Apr 2024 17:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02391v4</guid></item><item><title>Is Your Anomaly Detector Ready for Change? Adapting AIOps Solutions to the Real World</title><link>http://arxiv.org/abs/2311.10421v2</link><description>Anomaly detection techniques are essential in automating the monitoring of ITsystems and operations. These techniques imply that machine learning algorithmsare trained on operational data corresponding to a specific period of time andthat they are continuously evaluated on newly emerging data. Operational datais constantly changing over time, which affects the performance of deployedanomaly detection models. Therefore, continuous model maintenance is requiredto preserve the performance of anomaly detectors over time. In this work, weanalyze two different anomaly detection model maintenance techniques in termsof the model update frequency, namely blind model retraining and informed modelretraining. We further investigate the effects of updating the model byretraining it on all the available data (full-history approach) and only thenewest data (sliding window approach). Moreover, we investigate whether a datachange monitoring tool is capable of determining when the anomaly detectionmodel needs to be updated through retraining.</description><author>Lorena Poenaru-Olaru, Natalia Karpova, Luis Cruz, Jan Rellermeyer, Arie van Deursen</author><pubDate>Thu, 11 Apr 2024 17:28:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10421v2</guid></item><item><title>Automatic nonlinear MPC approximation with closed-loop guarantees</title><link>http://arxiv.org/abs/2312.10199v2</link><description>Safety guarantees are vital in many control applications, such as robotics.Model predictive control (MPC) provides a constructive framework forcontrolling safety-critical systems, but is limited by its computationalcomplexity. We address this problem by presenting a novel algorithm thatautomatically computes an explicit approximation to nonlinear MPC schemes whileretaining closed-loop guarantees. Specifically, the problem can be reduced to afunction approximation problem, which we then tackle by proposing ALKIA-X, theAdaptive and Localized Kernel Interpolation Algorithm with eXtrapolatedreproducing kernel Hilbert space norm. ALKIA-X is a non-iterative algorithmthat ensures numerically well-conditioned computations, a fast-to-evaluateapproximating function, and the guaranteed satisfaction of any desired bound onthe approximation error. Hence, ALKIA-X automatically computes an explicitfunction that approximates the MPC, yielding a controller suitable forsafety-critical systems and high sampling rates. We apply ALKIA-X toapproximate two nonlinear MPC schemes, demonstrating reduced computationaldemand and applicability to realistic problems.</description><author>Abdullah Tokmak, Christian Fiedler, Melanie N. Zeilinger, Sebastian Trimpe, Johannes Köhler</author><pubDate>Thu, 11 Apr 2024 17:22:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10199v2</guid></item><item><title>Context-aware Video Anomaly Detection in Long-Term Datasets</title><link>http://arxiv.org/abs/2404.07887v1</link><description>Video anomaly detection research is generally evaluated on short, isolatedbenchmark videos only a few minutes long. However, in real-world environments,security cameras observe the same scene for months or years at a time, and thenotion of anomalous behavior critically depends on context, such as the time ofday, day of week, or schedule of events. Here, we propose a context-aware videoanomaly detection algorithm, Trinity, specifically targeted to these scenarios.Trinity is especially well-suited to crowded scenes in which individuals cannotbe easily tracked, and anomalies are due to speed, direction, or absence ofgroup motion. Trinity is a contrastive learning framework that aims to learnalignments between context, appearance, and motion, and uses alignment qualityto classify videos as normal or anomalous. We evaluate our algorithm on bothconventional benchmarks and a public webcam-based dataset we collected thatspans more than three months of activity.</description><author>Zhengye Yang, Richard Radke</author><pubDate>Thu, 11 Apr 2024 17:17:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07887v1</guid></item><item><title>Grokking as the Transition from Lazy to Rich Training Dynamics</title><link>http://arxiv.org/abs/2310.06110v3</link><description>We propose that the grokking phenomenon, where the train loss of a neuralnetwork decreases much earlier than its test loss, can arise due to a neuralnetwork transitioning from lazy training dynamics to a rich, feature learningregime. To illustrate this mechanism, we study the simple setting of vanillagradient descent on a polynomial regression problem with a two layer neuralnetwork which exhibits grokking without regularization in a way that cannot beexplained by existing theories. We identify sufficient statistics for the testloss of such a network, and tracking these over training reveals that grokkingarises in this setting when the network first attempts to fit a kernelregression solution with its initial features, followed by late-time featurelearning where a generalizing solution is identified after train loss isalready low. We find that the key determinants of grokking are the rate offeature learning -- which can be controlled precisely by parameters that scalethe network output -- and the alignment of the initial features with the targetfunction $y(x)$. We argue this delayed generalization arises when (1) the topeigenvectors of the initial neural tangent kernel and the task labels $y(x)$are misaligned, but (2) the dataset size is large enough so that it is possiblefor the network to generalize eventually, but not so large that train lossperfectly tracks test loss at all epochs, and (3) the network begins trainingin the lazy regime so does not learn features immediately. We conclude withevidence that this transition from lazy (linear model) to rich training(feature learning) can control grokking in more general settings, like onMNIST, one-layer Transformers, and student-teacher networks.</description><author>Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, Cengiz Pehlevan</author><pubDate>Thu, 11 Apr 2024 17:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06110v3</guid></item><item><title>Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors</title><link>http://arxiv.org/abs/2404.07883v1</link><description>Intelligent tutoring systems (ITS) are effective for improving students'learning outcomes. However, their development is often complex, time-consuming,and requires specialized programming and tutor design knowledge, thus hinderingtheir widespread application and personalization. We present the ApprenticeTutor Builder (ATB) , a platform that simplifies tutor creation andpersonalization. Instructors can utilize ATB's drag-and-drop tool to buildtutor interfaces. Instructors can then interactively train the tutors'underlying AI agent to produce expert models that can solve problems. Trainingis achieved via using multiple interaction modalities including demonstrations,feedback, and user labels. We conducted a user study with 14 instructors toevaluate the effectiveness of ATB's design with end users. We found that usersenjoyed the flexibility of the interface builder and ease and speed of agentteaching, but often desired additional time-saving features. With theseinsights, we identified a set of design recommendations for our platform andothers that utilize interactive AI agents for tutor creation and customization.</description><author>Glen Smith, Adit Gupta, Christopher MacLellan</author><pubDate>Thu, 11 Apr 2024 17:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07883v1</guid></item><item><title>CAVIAR: Categorical-Variable Embeddings for Accurate and Robust Inference</title><link>http://arxiv.org/abs/2404.04979v2</link><description>Social science research often hinges on the relationship between categoricalvariables and outcomes. We introduce CAVIAR, a novel method for embeddingcategorical variables that assume values in a high-dimensional ambient spacebut are sampled from an underlying manifold. Our theoretical and numericalanalyses outline challenges posed by such categorical variables in causalinference. Specifically, dynamically varying and sparse levels can lead toviolations of the Donsker conditions and a failure of the estimationfunctionals to converge to a tight Gaussian process. Traditional approaches,including the exclusion of rare categorical levels and principled variableselection models like LASSO, fall short. CAVIAR embeds the data into alower-dimensional global coordinate system. The mapping can be derived fromboth structured and unstructured data, and ensures stable and robust estimatesthrough dimensionality reduction. In a dataset of direct-to-consumer apparelsales, we illustrate how high-dimensional categorical variables, such as zipcodes, can be succinctly represented, facilitating inference and analysis.</description><author>Anirban Mukherjee, Hannah Hanwen Chang</author><pubDate>Thu, 11 Apr 2024 17:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04979v2</guid></item><item><title>Analyzing Toxicity in Deep Conversations: A Reddit Case Study</title><link>http://arxiv.org/abs/2404.07879v1</link><description>Online social media has become increasingly popular in recent years due toits ease of access and ability to connect with others. One of social media'smain draws is its anonymity, allowing users to share their thoughts andopinions without fear of judgment or retribution. This anonymity has also madesocial media prone to harmful content, which requires moderation to ensureresponsible and productive use. Several methods using artificial intelligencehave been employed to detect harmful content. However, conversation andcontextual analysis of hate speech are still understudied. Most promising worksonly analyze a single text at a time rather than the conversation supportingit. In this work, we employ a tree-based approach to understand how usersbehave concerning toxicity in public conversation settings. To this end, wecollect both the posts and the comment sections of the top 100 posts from 8Reddit communities that allow profanity, totaling over 1 million responses. Wefind that toxic comments increase the likelihood of subsequent toxic commentsbeing produced in online conversations. Our analysis also shows that immediatecontext plays a vital role in shaping a response rather than the original post.We also study the effect of consensual profanity and observe overlappingsimilarities with non-consensual profanity in terms of user behavior andpatterns.</description><author>Vigneshwaran Shankaran, Rajesh Sharma</author><pubDate>Thu, 11 Apr 2024 17:10:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07879v1</guid></item><item><title>FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning</title><link>http://arxiv.org/abs/2402.13989v2</link><description>Federated learning (FL) is a promising framework for learning fromdistributed data while maintaining privacy. The development of efficient FLalgorithms encounters various challenges, including heterogeneous data andsystems, limited communication capacities, and constrained local computationalresources. Recently developed FedADMM methods show great resilience to bothdata and system heterogeneity. However, they still suffer from performancedeterioration if the hyperparameters are not carefully tuned. To address thisissue, we propose an inexact and self-adaptive FedADMM algorithm, termedFedADMM-InSa. First, we design an inexactness criterion for the clients' localupdates to eliminate the need for empirically setting the local trainingaccuracy. This inexactness criterion can be assessed by each clientindependently based on its unique condition, thereby reducing the localcomputational cost and mitigating the undesirable straggle effect. Theconvergence of the resulting inexact ADMM is proved under the assumption ofstrongly convex loss functions. Additionally, we present a self-adaptive schemethat dynamically adjusts each client's penalty parameter, enhancing algorithmrobustness by mitigating the need for empirical penalty parameter choices foreach client. Extensive numerical experiments on both synthetic and real-worlddatasets are conducted. As validated by some numerical tests, our proposedalgorithm can reduce the clients' local computational load significantly andalso accelerate the learning process compared to the vanilla FedADMM.</description><author>Yongcun Song, Ziqi Wang, Enrique Zuazua</author><pubDate>Thu, 11 Apr 2024 17:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13989v2</guid></item><item><title>MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2404.06564v2</link><description>Recent advancements in anomaly detection have seen the efficacy of CNN- andtransformer-based approaches. However, CNNs struggle with long-rangedependencies, while transformers are burdened by quadratic computationalcomplexity. Mamba-based models, with their superior long-range modeling andlinear efficiency, have garnered substantial attention. This study pioneers theapplication of Mamba to multi-class unsupervised anomaly detection, presentingMambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring(Locality-Enhanced State Space) LSS modules at multi-scales. The proposed LSSmodule, integrating parallel cascaded (Hybrid State Space) HSS blocks andmulti-kernel convolutions operations, effectively captures both long-range andlocal information. The HSS block, utilizing (Hybrid Scanning) HS encoders,encodes feature maps into five scanning methods and eight directions, therebystrengthening global connections through the (State Space Model) SSM. The useof Hilbert scanning and eight directions significantly improves featuresequence modeling. Comprehensive experiments on six diverse anomaly detectiondatasets and seven metrics demonstrate state-of-the-art performance,substantiating the method's effectiveness.</description><author>Haoyang He, Yuhu Bai, Jiangning Zhang, Qingdong He, Hongxu Chen, Zhenye Gan, Chengjie Wang, Xiangtai Li, Guanzhong Tian, Lei Xie</author><pubDate>Thu, 11 Apr 2024 17:06:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06564v2</guid></item><item><title>The Power of Properties: Uncovering the Influential Factors in Emotion Classification</title><link>http://arxiv.org/abs/2404.07867v1</link><description>Facial expression-based human emotion recognition is a critical research areain psychology and medicine. State-of-the-art classification performance is onlyreached by end-to-end trained neural networks. Nevertheless, such black-boxmodels lack transparency in their decision-making processes, prompting effortsto ascertain the rules that underlie classifiers' decisions. Analyzing singleinputs alone fails to expose systematic learned biases. These biases can becharacterized as facial properties summarizing abstract information like age ormedical conditions. Therefore, understanding a model's prediction behaviorrequires an analysis rooted in causality along such selected properties. Wedemonstrate that up to 91.25% of classifier output behavior changes arestatistically significant concerning basic properties. Among those are age,gender, and facial symmetry. Furthermore, the medical usage of surfaceelectromyography significantly influences emotion prediction. We introduce aworkflow to evaluate explicit properties and their impact. These insights mighthelp medical professionals select and apply classifiers regarding theirspecialized data and properties.</description><author>Tim Büchner, Niklas Penzel, Orlando Guntinas-Lichius, Joachim Denzler</author><pubDate>Thu, 11 Apr 2024 17:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07867v1</guid></item><item><title>Uncertainty-aware Evidential Fusion-based Learning for Semi-supervised Medical Image Segmentation</title><link>http://arxiv.org/abs/2404.06177v2</link><description>Although the existing uncertainty-based semi-supervised medical segmentationmethods have achieved excellent performance, they usually only consider asingle uncertainty evaluation, which often fails to solve the problem relatedto credibility completely. Therefore, based on the framework of evidential deeplearning, this paper integrates the evidential predictive results in thecross-region of mixed and original samples to reallocate the confidence degreeand uncertainty measure of each voxel, which is realized by emphasizinguncertain information of probability assignments fusion rule of traditionalevidence theory. Furthermore, we design a voxel-level asymptotic learningstrategy by introducing information entropy to combine with the fuseduncertainty measure to estimate voxel prediction more precisely. The model willgradually pay attention to the prediction results with high uncertainty in thelearning process, to learn the features that are difficult to master. Theexperimental results on LA, Pancreas-CT, ACDC and TBAD datasets demonstrate thesuperior performance of our proposed method in comparison with the existingstate of the arts.</description><author>Yuanpeng He, Lijian Li</author><pubDate>Thu, 11 Apr 2024 16:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06177v2</guid></item><item><title>Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing</title><link>http://arxiv.org/abs/2404.07864v1</link><description>We consider the problem of localizing change points in high-dimensionallinear regression. We propose an Approximate Message Passing (AMP) algorithmfor estimating both the signals and the change point locations. AssumingGaussian covariates, we give an exact asymptotic characterization of itsestimation performance in the limit where the number of samples growsproportionally to the signal dimension. Our algorithm can be tailored toexploit any prior information on the signal, noise, and change points. It alsoenables uncertainty quantification in the form of an efficiently computableapproximate posterior distribution, whose asymptotic form we characterizeexactly. We validate our theory via numerical experiments, and demonstrate thefavorable performance of our estimators on both synthetic data and images.</description><author>Gabriel Arpino, Xiaoqi Liu, Ramji Venkataramanan</author><pubDate>Thu, 11 Apr 2024 16:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07864v1</guid></item><item><title>Streaming detection of significant delay changes in public transport systems</title><link>http://arxiv.org/abs/2404.07860v1</link><description>Public transport systems are expected to reduce pollution and contribute tosustainable development. However, disruptions in public transport such asdelays may negatively affect mobility choices. To quantify delays, aggregateddata from vehicle locations systems are frequently used. However, delaysobserved at individual stops are caused inter alia by fluctuations in runningtimes and propagation of delays occurring in other locations. Hence, in thiswork, we propose both the method detecting significant delays and referencearchitecture, relying on stream processing engines, in which the method isimplemented. The method can complement the calculation of delays defined asdeviation from schedules. This provides both online rather than batchidentification of significant and repetitive delays, and resilience to thelimited quality of location data. The method we propose can be used withdifferent change detectors, such as ADWIN, applied to location data streamshuffled to individual edges of a transport graph. It can detect in an onlinemanner at which edges statistically significant delays are observed and atwhich edges delays arise and are reduced. Detections can be used to modelmobility choices and quantify the impact of repetitive rather than randomdisruptions on feasible trips with multimodal trip modelling engines. Theevaluation performed with the public transport data of over 2000 vehiclesconfirms the merits of the method and reveals that a limited-size subgraph of atransport system graph causes statistically significant delays</description><author>Przemysław Wrona, Maciej Grzenda, Marcin Luckner</author><pubDate>Thu, 11 Apr 2024 16:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07860v1</guid></item><item><title>Resolve Domain Conflicts for Generalizable Remote Physiological Measurement</title><link>http://arxiv.org/abs/2404.07855v1</link><description>Remote photoplethysmography (rPPG) technology has become increasingly populardue to its non-invasive monitoring of various physiological indicators, makingit widely applicable in multimedia interaction, healthcare, and emotionanalysis. Existing rPPG methods utilize multiple datasets for training toenhance the generalizability of models. However, they often overlook theunderlying conflict issues across different datasets, such as (1) labelconflict resulting from different phase delays between physiological signallabels and face videos at the instance level, and (2) attribute conflictstemming from distribution shifts caused by head movements, illuminationchanges, skin types, etc. To address this, we introduce the DOmain-HArmoniousframework (DOHA). Specifically, we first propose a harmonious phase strategy toeliminate uncertain phase delays and preserve the temporal variation ofphysiological signals. Next, we design a harmonious hyperplane optimizationthat reduces irrelevant attribute shifts and encourages the model'soptimization towards a global solution that fits more valid scenarios. Ourexperiments demonstrate that DOHA significantly improves the performance ofexisting methods under multiple protocols. Our code is available athttps://github.com/SWY666/rPPG-DOHA.</description><author>Weiyu Sun, Xinyu Zhang, Hao Lu, Ying Chen, Yun Ge, Xiaolin Huang, Jie Yuan, Yingcong Chen</author><pubDate>Thu, 11 Apr 2024 16:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07855v1</guid></item><item><title>Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations</title><link>http://arxiv.org/abs/2404.07851v1</link><description>Machine Translation (MT) remains one of the last NLP tasks where largelanguage models (LLMs) have not yet replaced dedicated supervised systems. Thiswork exploits the complementary strengths of LLMs and supervised MT by guidingLLMs to automatically post-edit MT with external feedback on its quality,derived from Multidimensional Quality Metric (MQM) annotations. Working withLLaMA-2 models, we consider prompting strategies varying the nature of feedbackprovided and then fine-tune the LLM to improve its ability to exploit theprovided guidance. Through experiments on Chinese-English, English-German, andEnglish-Russian MQM data, we demonstrate that prompting LLMs to post-edit MTimproves TER, BLEU and COMET scores, although the benefits of fine-grainedfeedback are not clear. Fine-tuning helps integrate fine-grained feedback moreeffectively and further improves translation quality based on both automaticand human evaluation.</description><author>Dayeon Ki, Marine Carpuat</author><pubDate>Thu, 11 Apr 2024 16:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07851v1</guid></item><item><title>MindBridge: A Cross-Subject Brain Decoding Framework</title><link>http://arxiv.org/abs/2404.07850v1</link><description>Brain decoding, a pivotal field in neuroscience, aims to reconstruct stimulifrom acquired brain signals, primarily utilizing functional magnetic resonanceimaging (fMRI). Currently, brain decoding is confined to aper-subject-per-model paradigm, limiting its applicability to the sameindividual for whom the decoding model is trained. This constraint stems fromthree key challenges: 1) the inherent variability in input dimensions acrosssubjects due to differences in brain size; 2) the unique intrinsic neuralpatterns, influencing how different individuals perceive and process sensoryinformation; 3) limited data availability for new subjects in real-worldscenarios hampers the performance of decoding models. In this paper, we presenta novel approach, MindBridge, that achieves cross-subject brain decoding byemploying only one model. Our proposed framework establishes a generic paradigmcapable of addressing these challenges by introducing biological-inspiredaggregation function and novel cyclic fMRI reconstruction mechanism forsubject-invariant representation learning. Notably, by cycle reconstruction offMRI, MindBridge can enable novel fMRI synthesis, which also can serve aspseudo data augmentation. Within the framework, we also devise a novelreset-tuning method for adapting a pretrained model to a new subject.Experimental results demonstrate MindBridge's ability to reconstruct images formultiple subjects, which is competitive with dedicated subject-specific models.Furthermore, with limited data for a new subject, we achieve a high level ofdecoding accuracy, surpassing that of subject-specific models. This advancementin cross-subject brain decoding suggests promising directions for widerapplications in neuroscience and indicates potential for more efficientutilization of limited fMRI data in real-world scenarios. Project page:https://littlepure2333.github.io/MindBridge</description><author>Shizun Wang, Songhua Liu, Zhenxiong Tan, Xinchao Wang</author><pubDate>Thu, 11 Apr 2024 16:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07850v1</guid></item><item><title>Multi-Neuron Representations of Hierarchical Concepts in Spiking Neural Networks</title><link>http://arxiv.org/abs/2401.04628v2</link><description>We describe how hierarchical concepts can be represented in three types oflayered neural networks. The aim is to support recognition of the concepts whenpartial information about the concepts is presented, and also when some of theneurons in the network might fail. Our failure model involves initial randomfailures. The three types of networks are: feed-forward networks with highconnectivity, feed-forward networks with low connectivity, and layered networkswith low connectivity and with both forward edges and "lateral" edges withinlayers. In order to achieve fault-tolerance, the representations all usemultiple representative neurons for each concept. We show how recognition canwork in all three of these settings, and quantify how the probability ofcorrect recognition depends on several parameters, including the number ofrepresentatives and the neuron failure probability. We also discuss how theserepresentations might be learned, in all three types of networks. For thefeed-forward networks, the learning algorithms are similar to ones used in [4],whereas for networks with lateral edges, the algorithms are generally inspiredby work on the assembly calculus [3, 6, 7].</description><author>Nancy A. Lynch</author><pubDate>Thu, 11 Apr 2024 16:43:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04628v2</guid></item><item><title>Overparameterized Multiple Linear Regression as Hyper-Curve Fitting</title><link>http://arxiv.org/abs/2404.07849v1</link><description>The paper shows that the application of the fixed-effect multiple linearregression model to an overparameterized dataset is equivalent to fitting thedata with a hyper-curve parameterized by a single scalar parameter. Thisequivalence allows for a predictor-focused approach, where each predictor isdescribed by a function of the chosen parameter. It is proven that a linearmodel will produce exact predictions even in the presence of nonlineardependencies that violate the model assumptions. Parameterization in terms ofthe dependent variable and the monomial basis in the predictor function spaceare applied here to both synthetic and experimental data. The hyper-curveapproach is especially suited for the regularization of problems with noise inpredictor variables and can be used to remove noisy and "improper" predictorsfrom the model.</description><author>E. Atza, N. Budko</author><pubDate>Thu, 11 Apr 2024 16:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07849v1</guid></item><item><title>Fuss-Free Network: A Simplified and Efficient Neural Network for Crowd Counting</title><link>http://arxiv.org/abs/2404.07847v1</link><description>In the field of crowd-counting research, many recent deep learning basedmethods have demonstrated robust capabilities for accurately estimating crowdsizes. However, the enhancement in their performance often arises from anincrease in the complexity of the model structure. This paper introduces theFuss-Free Network (FFNet), a crowd counting deep learning model that ischaracterized by its simplicity and efficiency in terms of its structure. Themodel comprises only a backbone of a neural network and a multi-scale featurefusion structure.The multi-scale feature fusion structure is a simplearchitecture consisting of three branches, each only equipped with a focustransition module, and combines the features from these branches through theconcatenation operation.Our proposed crowd counting model is trained andevaluated on four widely used public datasets, and it achieves accuracy that iscomparable to that of existing complex models.The experimental results furtherindicate that excellent performance in crowd counting tasks can also beachieved by utilizing a simple, low-parameter, and computationally efficientneural network structure.</description><author>Lei Chen, Xingen Gao</author><pubDate>Thu, 11 Apr 2024 16:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07847v1</guid></item><item><title>MetaCheckGPT -- A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models</title><link>http://arxiv.org/abs/2404.06948v2</link><description>Hallucinations in large language models (LLMs) have recently become asignificant problem. A recent effort in this direction is a shared task atSemeval 2024 Task 6, SHROOM, a Shared-task on Hallucinations and RelatedObservable Overgeneration Mistakes. This paper describes our winning solutionranked 1st and 2nd in the 2 sub-tasks of model agnostic and model aware tracksrespectively. We propose a meta-regressor framework of LLMs for modelevaluation and integration that achieves the highest scores on the leaderboard.We also experiment with various transformer-based models and black box methodslike ChatGPT, Vectara, and others. In addition, we perform an error analysiscomparing GPT4 against our best model which shows the limitations of theformer.</description><author>Rahul Mehta, Andrew Hoblitzell, Jack O'Keefe, Hyeju Jang, Vasudeva Varma</author><pubDate>Thu, 11 Apr 2024 16:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06948v2</guid></item><item><title>TBSN: Transformer-Based Blind-Spot Network for Self-Supervised Image Denoising</title><link>http://arxiv.org/abs/2404.07846v1</link><description>Blind-spot networks (BSN) have been prevalent network architectures inself-supervised image denoising (SSID). Existing BSNs are mostly conducted withconvolution layers. Although transformers offer potential solutions to thelimitations of convolutions and have demonstrated success in various imagerestoration tasks, their attention mechanisms may violate the blind-spotrequirement, thus restricting their applicability in SSID. In this paper, wepresent a transformer-based blind-spot network (TBSN) by analyzing andredesigning the transformer operators that meet the blind-spot requirement.Specifically, TBSN follows the architectural principles of dilated BSNs, andincorporates spatial as well as channel self-attention layers to enhance thenetwork capability. For spatial self-attention, an elaborate mask is applied tothe attention matrix to restrict its receptive field, thus mimicking thedilated convolution. For channel self-attention, we observe that it may leakthe blind-spot information when the channel number is greater than spatial sizein the deep layers of multi-scale architectures. To eliminate this effect, wedivide the channel into several groups and perform channel attentionseparately. Furthermore, we introduce a knowledge distillation strategy thatdistills TBSN into smaller denoisers to improve computational efficiency whilemaintaining performance. Extensive experiments on real-world image denoisingdatasets show that TBSN largely extends the receptive field and exhibitsfavorable performance against state-of-the-art SSID methods. The code andpre-trained models will be publicly available athttps://github.com/nagejacob/TBSN.</description><author>Junyi Li, Zhilu Zhang, Wangmeng Zuo</author><pubDate>Thu, 11 Apr 2024 16:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07846v1</guid></item><item><title>Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks</title><link>http://arxiv.org/abs/2302.08890v3</link><description>Event cameras are bio-inspired sensors that capture the per-pixel intensitychanges asynchronously and produce event streams encoding the time, pixelposition, and polarity (sign) of the intensity changes. Event cameras possess amyriad of advantages over canonical frame-based cameras, such as high temporalresolution, high dynamic range, low latency, etc. Being capable of capturinginformation in challenging visual conditions, event cameras have the potentialto overcome the limitations of frame-based cameras in the computer vision androbotics community. In very recent years, deep learning (DL) has been broughtto this emerging field and inspired active research endeavors in mining itspotential. However, there is still a lack of taxonomies in DL techniques forevent-based vision. We first scrutinize the typical event representations withquality enhancement methods as they play a pivotal role as inputs to the DLmodels. We then provide a comprehensive survey of existing DL-based methods bystructurally grouping them into two major categories: 1) image/videoreconstruction and restoration; 2) event-based scene understanding and 3Dvision. We conduct benchmark experiments for the existing methods in somerepresentative research directions, i.e., image reconstruction, deblurring, andobject recognition, to identify some critical insights and problems. Finally,we have discussions regarding the challenges and provide new perspectives forinspiring more research studies.</description><author>Xu Zheng, Yexin Liu, Yunfan Lu, Tongyan Hua, Tianbo Pan, Weiming Zhang, Dacheng Tao, Lin Wang</author><pubDate>Thu, 11 Apr 2024 16:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08890v3</guid></item><item><title>MoCha-Stereo: Motif Channel Attention Network for Stereo Matching</title><link>http://arxiv.org/abs/2404.06842v2</link><description>Learning-based stereo matching techniques have made significant progress.However, existing methods inevitably lose geometrical structure informationduring the feature channel generation process, resulting in edge detailmismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network(MoCha-Stereo) is designed to address this problem. We provide the MotifChannel Correlation Volume (MCCV) to determine more accurate edge matchingcosts. MCCV is achieved by projecting motif channels, which capture commongeometric structures in feature channels, onto feature maps and cost volumes.In addition, edge variations in %potential feature channels of thereconstruction error map also affect details matching, we propose theReconstruction Error Motif Penalty (REMP) module to further refine thefull-resolution disparity estimation. REMP integrates the frequency informationof typical channel features from the reconstruction error. MoCha-Stereo ranks1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structurealso shows excellent performance in Multi-View Stereo. Code is avaliable athttps://github.com/ZYangChen/MoCha-Stereo.</description><author>Ziyang Chen, Wei Long, He Yao, Yongjun Zhang, Bingshu Wang, Yongbin Qin, Jia Wu</author><pubDate>Thu, 11 Apr 2024 16:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06842v2</guid></item><item><title>On Training Data Influence of GPT Models</title><link>http://arxiv.org/abs/2404.07840v1</link><description>Amidst the rapid advancements in generative language models, theinvestigation of how training data shapes the performance of GPT models isstill emerging. This paper presents GPTfluence, a novel approach that leveragesa featurized simulation to assess the impact of training examples on thetraining dynamics of GPT models. Our approach not only traces the influence ofindividual training instances on performance trajectories, such as loss andother key metrics, on targeted test points but also enables a comprehensivecomparison with existing methods across various training scenarios in GPTmodels, ranging from 14 million to 2.8 billion parameters, across a range ofdownstream tasks. Contrary to earlier methods that struggle with generalizationto new data, GPTfluence introduces a parameterized simulation of trainingdynamics, demonstrating robust generalization capabilities to unseen trainingdata. This adaptability is evident across both fine-tuning andinstruction-tuning scenarios, spanning tasks in natural language understandingand generation. We will make our code and data publicly available.</description><author>Qingyi Liu, Yekun Chai, Shuohuan Wang, Yu Sun, Keze Wang, Hua Wu</author><pubDate>Thu, 11 Apr 2024 16:27:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07840v1</guid></item><item><title>RecurrentGemma: Moving Past Transformers for Efficient Open Language Models</title><link>http://arxiv.org/abs/2404.07839v1</link><description>We introduce RecurrentGemma, an open language model which uses Google's novelGriffin architecture. Griffin combines linear recurrences with local attentionto achieve excellent performance on language. It has a fixed-sized state, whichreduces memory use and enables efficient inference on long sequences. Weprovide a pre-trained model with 2B non-embedding parameters, and aninstruction tuned variant. Both models achieve comparable performance toGemma-2B despite being trained on fewer tokens.</description><author>Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz GUStavo Martins, Elisa Bandy, David Huntsperger, Glen</author><pubDate>Thu, 11 Apr 2024 16:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07839v1</guid></item><item><title>Question Generation in Knowledge-Driven Dialog: Explainability and Evaluation</title><link>http://arxiv.org/abs/2404.07836v1</link><description>We explore question generation in the context of knowledge-grounded dialogsfocusing on explainability and evaluation. Inspired by previous work onplanning-based summarisation, we present a model which instead of directlygenerating a question, sequentially predicts first a fact then a question. Weevaluate our approach on 37k test dialogs adapted from the KGConv dataset andwe show that, although more demanding in terms of inference, our approachperforms on par with a standard model which solely generates a question whileallowing for a detailed referenceless evaluation of the model behaviour interms of relevance, factuality and pronominalisation.</description><author>Juliette Faille, Quentin Brabant, Gwenole Lecorve, Lina M. Rojas-Barahona, Claire Gardent</author><pubDate>Thu, 11 Apr 2024 16:24:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07836v1</guid></item><item><title>Streamlined Photoacoustic Image Processing with Foundation Models: A Training-Free Solution</title><link>http://arxiv.org/abs/2404.07833v1</link><description>Foundation models have rapidly evolved and have achieved significantaccomplishments in computer vision tasks. Specifically, the prompt mechanismconveniently allows users to integrate image prior information into the model,making it possible to apply models without any training. Therefore, we proposea method based on foundation models and zero training to solve the tasks ofphotoacoustic (PA) image segmentation. We employed the segment anything model(SAM) by setting simple prompts and integrating the model's outputs with priorknowledge of the imaged objects to accomplish various tasks, including: (1)removing the skin signal in three-dimensional PA image rendering; (2) dualspeed-of-sound reconstruction, and (3) segmentation of finger blood vessels.Through these demonstrations, we have concluded that deep learning can bedirectly applied in PA imaging without the requirement for network design andtraining. This potentially allows for a hands-on, convenient approach toachieving efficient and accurate segmentation of PA images. This letter servesas a comprehensive tutorial, facilitating the mastery of the technique throughthe provision of code and sample datasets.</description><author>Handi Deng, Yucheng Zhou, Jiaxuan Xiang, Liujie Gu, Yan Luo, Hai Feng, Mingyuan Liu, Cheng Ma</author><pubDate>Thu, 11 Apr 2024 16:18:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07833v1</guid></item><item><title>On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning</title><link>http://arxiv.org/abs/2404.07826v1</link><description>The use of Potential Based Reward Shaping (PBRS) has shown great promise inthe ongoing research effort to tackle sample inefficiency in ReinforcementLearning (RL). However, the choice of the potential function is critical forthis technique to be effective. Additionally, RL techniques are usuallyconstrained to use a finite horizon for computational limitations. Thisintroduces a bias when using PBRS, thus adding an additional layer ofcomplexity. In this paper, we leverage abstractions to automatically produce a"good" potential function. We analyse the bias induced by finite horizons inthe context of PBRS producing novel insights. Finally, to asses sampleefficiency and performance impact, we evaluate our approach on fourenvironments including a goal-oriented navigation task and three ArcadeLearning Environments (ALE) games demonstrating that we can reach the samelevel of performance as CNN-based solutions with a simple fully-connectednetwork.</description><author>Giuseppe Canonaco, Leo Ardon, Alberto Pozanco, Daniel Borrajo</author><pubDate>Thu, 11 Apr 2024 16:09:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07826v1</guid></item><item><title>Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese</title><link>http://arxiv.org/abs/2404.07824v1</link><description>Vision Language Models (VLMs) have undergone a rapid evolution, giving riseto significant advancements in the realm of multimodal understanding tasks.However, the majority of these models are trained and evaluated onEnglish-centric datasets, leaving a gap in the development and evaluation ofVLMs for other languages, such as Japanese. This gap can be attributed to thelack of methodologies for constructing VLMs and the absence of benchmarks toaccurately measure their performance. To address this issue, we introduce anovel benchmark, Japanese Heron-Bench, for evaluating Japanese capabilities ofVLMs. The Japanese Heron-Bench consists of a variety of imagequestion answerpairs tailored to the Japanese context. Additionally, we present a baselineJapanese VLM that has been trained with Japanese visual instruction tuningdatasets. Our Heron-Bench reveals the strengths and limitations of the proposedVLM across various ability dimensions. Furthermore, we clarify the capabilitygap between strong closed models like GPT-4V and the baseline model, providingvaluable insights for future research in this domain. We release the benchmarkdataset and training code to facilitate further developments in Japanese VLMresearch.</description><author>Yuichi Inoue, Kento Sasaki, Yuma Ochi, Kazuki Fujii, Kotaro Tanahashi, Yu Yamaguchi</author><pubDate>Thu, 11 Apr 2024 16:09:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07824v1</guid></item><item><title>Sparse Laneformer</title><link>http://arxiv.org/abs/2404.07821v1</link><description>Lane detection is a fundamental task in autonomous driving, and has achievedgreat progress as deep learning emerges. Previous anchor-based methods oftendesign dense anchors, which highly depend on the training dataset and remainfixed during inference. We analyze that dense anchors are not necessary forlane detection, and propose a transformer-based lane detection framework basedon a sparse anchor mechanism. To this end, we generate sparse anchors withposition-aware lane queries and angle queries instead of traditional explicitanchors. We adopt Horizontal Perceptual Attention (HPA) to aggregate the lanefeatures along the horizontal direction, and adopt Lane-Angle Cross Attention(LACA) to perform interactions between lane queries and angle queries. We alsopropose Lane Perceptual Attention (LPA) based on deformable cross attention tofurther refine the lane predictions. Our method, named Sparse Laneformer, iseasy-to-implement and end-to-end trainable. Extensive experiments demonstratethat Sparse Laneformer performs favorably against the state-of-the-art methods,e.g., surpassing Laneformer by 3.0% F1 score and O2SFormer by 0.7% F1 scorewith fewer MACs on CULane with the same ResNet-34 backbone.</description><author>Ji Liu, Zifeng Zhang, Mingjie Lu, Hongyang Wei, Dong Li, Yile Xie, Jinzhang Peng, Lu Tian, Ashish Sirasao, Emad Barsoum</author><pubDate>Thu, 11 Apr 2024 16:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07821v1</guid></item><item><title>Novelty Heuristics, Multi-Queue Search, and Portfolios for Numeric Planning</title><link>http://arxiv.org/abs/2404.05235v2</link><description>Heuristic search is a powerful approach for solving planning problems andnumeric planning is no exception. In this paper, we boost the performance ofheuristic search for numeric planning with various powerful techniquesorthogonal to improving heuristic informedness: numeric novelty heuristics, theManhattan distance heuristic, and exploring the use of multi-queue search andportfolios for combining heuristics.</description><author>Dillon Z. Chen, Sylvie Thiébaux</author><pubDate>Thu, 11 Apr 2024 16:00:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05235v2</guid></item><item><title>Calibration of Continual Learning Models</title><link>http://arxiv.org/abs/2404.07817v1</link><description>Continual Learning (CL) focuses on maximizing the predictive performance of amodel across a non-stationary stream of data. Unfortunately, CL models tend toforget previous knowledge, thus often underperforming when compared with anoffline model trained jointly on the entire data stream. Given that any CLmodel will eventually make mistakes, it is of crucial importance to buildcalibrated CL models: models that can reliably tell their confidence whenmaking a prediction. Model calibration is an active research topic in machinelearning, yet to be properly investigated in CL. We provide the first empiricalstudy of the behavior of calibration approaches in CL, showing that CLstrategies do not inherently learn calibrated models. To mitigate this issue,we design a continual calibration approach that improves the performance ofpost-processing calibration methods over a wide range of different benchmarksand CL strategies. CL does not necessarily need perfect predictive models, butrather it can benefit from reliable predictive models. We believe our study oncontinual calibration represents a first step towards this direction.</description><author>Lanpei Li, Elia Piccoli, Andrea Cossu, Davide Bacciu, Vincenzo Lomonaco</author><pubDate>Thu, 11 Apr 2024 15:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07817v1</guid></item><item><title>Group Decision-Making among Privacy-Aware Agents</title><link>http://arxiv.org/abs/2402.08156v4</link><description>How can individuals exchange information to learn from each other despitetheir privacy needs and security concerns? For example, consider individualsdeliberating a contentious topic and being concerned about divulging theirprivate experiences. Preserving individual privacy and enabling efficientsocial learning are both important desiderata but seem fundamentally at oddswith each other and very hard to reconcile. We do so by controlling informationleakage using rigorous statistical guarantees that are based on differentialprivacy (DP). Our agents use log-linear rules to update their beliefs aftercommunicating with their neighbors. Adding DP randomization noise to beliefsprovides communicating agents with plausible deniability with regard to theirprivate information and their network neighborhoods. We consider two learningenvironments one for distributed maximum-likelihood estimation given a finitenumber of private signals and another for online learning from an infinite,intermittent signal stream. Noisy information aggregation in the finite caseleads to interesting tradeoffs between rejecting low-quality states and makingsure all high-quality states are accepted in the algorithm output. Our resultsflesh out the nature of the trade-offs in both cases between the quality of thegroup decision outcomes, learning accuracy, communication cost, and the levelof privacy protections that the agents are afforded.</description><author>Marios Papachristou, M. Amin Rahimian</author><pubDate>Thu, 11 Apr 2024 15:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08156v4</guid></item><item><title>Post-Hoc Reversal: Are We Selecting Models Prematurely?</title><link>http://arxiv.org/abs/2404.07815v1</link><description>Trained models are often composed with post-hoc transforms such astemperature scaling (TS), ensembling and stochastic weight averaging (SWA) toimprove performance, robustness, uncertainty estimation, etc. However, suchtransforms are typically applied only after the base models have already beenfinalized by standard means. In this paper, we challenge this practice with anextensive empirical study. In particular, we demonstrate a phenomenon that wecall post-hoc reversal, where performance trends are reversed after applyingthese post-hoc transforms. This phenomenon is especially prominent inhigh-noise settings. For example, while base models overfit badly early intraining, both conventional ensembling and SWA favor base models trained formore epochs. Post-hoc reversal can also suppress the appearance of doubledescent and mitigate mismatches between test loss and test error seen in basemodels. Based on our findings, we propose post-hoc selection, a simpletechnique whereby post-hoc metrics inform model development decisions such asearly stopping, checkpointing, and broader hyperparameter choices. Ourexperimental analyses span real-world vision, language, tabular and graphdatasets from domains like satellite imaging, language modeling, censusprediction and social network analysis. On an LLM instruction tuning dataset,post-hoc selection results in &gt; 1.5x MMLU improvement compared to naiveselection. Code is available athttps://github.com/rishabh-ranjan/post-hoc-reversal.</description><author>Rishabh Ranjan, Saurabh Garg, Mrigank Raman, Carlos Guestrin, Zachary Chase Lipton</author><pubDate>Thu, 11 Apr 2024 15:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07815v1</guid></item><item><title>MultiLS-SP/CA: Lexical Complexity Prediction and Lexical Simplification Resources for Catalan and Spanish</title><link>http://arxiv.org/abs/2404.07814v1</link><description>Automatic lexical simplification is a task to substitute lexical items thatmay be unfamiliar and difficult to understand with easier and more commonwords. This paper presents MultiLS-SP/CA, a novel dataset for lexicalsimplification in Spanish and Catalan. This dataset represents the first of itskind in Catalan and a substantial addition to the sparse data on automaticlexical simplification which is available for Spanish. Specifically, MultiLS-SPis the first dataset for Spanish which includes scalar ratings of theunderstanding difficulty of lexical items. In addition, we describe experimentswith this dataset, which can serve as a baseline for future work on the samedata.</description><author>Stefan Bott, Horacio Saggion, Nelson Peréz Rojas, Martin Solis Salazar, Saul Calderon Ramirez</author><pubDate>Thu, 11 Apr 2024 15:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07814v1</guid></item><item><title>KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels</title><link>http://arxiv.org/abs/2404.01140v2</link><description>In this paper, we present KoCoNovel, a novel character coreference datasetderived from Korean literary texts, complete with detailed annotationguidelines. Comprising 178K tokens from 50 modern and contemporary novels,KoCoNovel stands as one of the largest public coreference resolution corpora inKorean, and the first to be based on literary texts. KoCoNovel offers fourdistinct versions to accommodate a wide range of literary coreference analysisneeds. These versions are designed to support perspectives of the omniscientauthor or readers, and to manage multiple entities as either separate oroverlapping, thereby broadening its applicability. One of KoCoNovel'sdistinctive features is that 24% of all character mentions are single commonnouns, lacking possessive markers or articles. This feature is particularlyinfluenced by the nuances of Korean address term culture, which favors the useof terms denoting social relationships and kinship over personal names. Inexperiments with a BERT-based coreference model, we observe notable performanceenhancements with KoCoNovel in character coreference tasks within literarytexts, compared to a larger non-literary coreference dataset. Such findingsunderscore KoCoNovel's potential to significantly enhance coreferenceresolution models through the integration of Korean cultural and linguisticdynamics.</description><author>Kyuhee Kim, Surin Lee, Sangah Lee</author><pubDate>Thu, 11 Apr 2024 15:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01140v2</guid></item><item><title>Voice-Assisted Real-Time Traffic Sign Recognition System Using Convolutional Neural Network</title><link>http://arxiv.org/abs/2404.07807v1</link><description>Traffic signs are important in communicating information to drivers. Thus,comprehension of traffic signs is essential for road safety and ignorance mayresult in road accidents. Traffic sign detection has been a research spotlightover the past few decades. Real-time and accurate detections are thepreliminaries of robust traffic sign detection system which is yet to beachieved. This study presents a voice-assisted real-time traffic signrecognition system which is capable of assisting drivers. This system functionsunder two subsystems. Initially, the detection and recognition of the trafficsigns are carried out using a trained Convolutional Neural Network (CNN). Afterrecognizing the specific traffic sign, it is narrated to the driver as a voicemessage using a text-to-speech engine. An efficient CNN model for a benchmarkdataset is developed for real-time detection and recognition using DeepLearning techniques. The advantage of this system is that even if the drivermisses a traffic sign, or does not look at the traffic sign, or is unable tocomprehend the sign, the system detects it and narrates it to the driver. Asystem of this type is also important in the development of autonomousvehicles.</description><author>Mayura Manawadu, Udaya Wijenayake</author><pubDate>Thu, 11 Apr 2024 15:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07807v1</guid></item><item><title>Wu's Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry</title><link>http://arxiv.org/abs/2404.06405v2</link><description>Proving geometric theorems constitutes a hallmark of visual reasoningcombining both intuitive and logical skills. Therefore, automated theoremproving of Olympiad-level geometry problems is considered a notable milestonein human-level automated reasoning. The introduction of AlphaGeometry, aneuro-symbolic model trained with 100 million synthetic samples, marked a majorbreakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO)problems whereas the reported baseline based on Wu's method solved only ten. Inthis note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry,and find that Wu's method is surprisingly strong. Wu's method alone can solve15 problems, and some of them are not solved by any of the other methods. Thisleads to two key findings: (i) Combining Wu's method with the classic syntheticmethods of deductive databases and angle, ratio, and distance chasing solves 21out of 30 methods by just using a CPU-only laptop with a time limit of 5minutes per problem. Essentially, this classic method solves just 4 problemsless than AlphaGeometry and establishes the first fully symbolic baselinestrong enough to rival the performance of an IMO silver medalist. (ii) Wu'smethod even solves 2 of the 5 problems that AlphaGeometry failed to solve.Thus, by combining AlphaGeometry with Wu's method we set a new state-of-the-artfor automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, thefirst AI method which outperforms an IMO gold medalist.</description><author>Shiven Sinha, Ameya Prabhu, Ponnurangam Kumaraguru, Siddharth Bhat, Matthias Bethge</author><pubDate>Thu, 11 Apr 2024 15:37:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06405v2</guid></item><item><title>DGMamba: Domain Generalization via Generalized State Space Model</title><link>http://arxiv.org/abs/2404.07794v1</link><description>Domain generalization~(DG) aims at solving distribution shift problems invarious scenes. Existing approaches are based on Convolution Neural Networks(CNNs) or Vision Transformers (ViTs), which suffer from limited receptivefields or quadratic complexities issues. Mamba, as an emerging state spacemodel (SSM), possesses superior linear complexity and global receptive fields.Despite this, it can hardly be applied to DG to address distribution shifts,due to the hidden state issues and inappropriate scan mechanisms. In thispaper, we propose a novel framework for DG, named DGMamba, that excels instrong generalizability toward unseen domains and meanwhile has the advantagesof global receptive fields, and efficient linear complexity. Our DGMambacompromises two core components: Hidden State Suppressing~(HSS) andSemantic-aware Patch refining~(SPR). In particular, HSS is introduced tomitigate the influence of hidden states associated with domain-specificfeatures during output prediction. SPR strives to encourage the model toconcentrate more on objects rather than context, consisting of two designs:Prior-Free Scanning~(PFS), and Domain Context Interchange~(DCI). Concretely,PFS aims to shuffle the non-semantic patches within images, creating moreflexible and effective sequences from images, and DCI is designed to regularizeMamba with the combination of mismatched non-semantic and semantic informationby fusing patches among domains. Extensive experiments on four commonly used DGbenchmarks demonstrate that the proposed DGMamba achieves remarkably superiorresults to state-of-the-art models. The code will be made publicly available.</description><author>Shaocong Long, Qianyu Zhou, Xiangtai Li, Xuequan Lu, Chenhao Ying, Yuan Luo, Lizhuang Ma, Shuicheng Yan</author><pubDate>Thu, 11 Apr 2024 15:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07794v1</guid></item><item><title>Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection through Data Augmentation</title><link>http://arxiv.org/abs/2404.07792v1</link><description>This paper describes submissions from the team Nostra Domina to the EvaLatin2024 shared task of emotion polarity detection. Given the low-resourceenvironment of Latin and the complexity of sentiment in rhetorical genres likepoetry, we augmented the available data through automatic polarity annotation.We present two methods for doing so on the basis of the $k$-means algorithm,and we employ a variety of Latin large language models (LLMs) in a neuralarchitecture to better capture the underlying contextual sentimentrepresentations. Our best approach achieved the second highest macro-averagedMacro-$F_1$ score on the shared task's test set.</description><author>Stephen Bothwell, Abigail Swenor, David Chiang</author><pubDate>Thu, 11 Apr 2024 15:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07792v1</guid></item><item><title>VIFNet: An End-to-end Visible-Infrared Fusion Network for Image Dehazing</title><link>http://arxiv.org/abs/2404.07790v1</link><description>Image dehazing poses significant challenges in environmental perception.Recent research mainly focus on deep learning-based methods with singlemodality, while they may result in severe information loss especially indense-haze scenarios. The infrared image exhibits robustness to the haze,however, existing methods have primarily treated the infrared modality asauxiliary information, failing to fully explore its rich information indehazing. To address this challenge, the key insight of this study is to designa visible-infrared fusion network for image dehazing. In particular, we proposea multi-scale Deep Structure Feature Extraction (DSFE) module, whichincorporates the Channel-Pixel Attention Block (CPAB) to restore more spatialand marginal information within the deep structural features. Additionally, weintroduce an inconsistency weighted fusion strategy to merge the two modalitiesby leveraging the more reliable information. To validate this, we construct avisible-infrared multimodal dataset called AirSim-VID based on the AirSimsimulation platform. Extensive experiments performed on challenging real andsimulated image datasets demonstrate that VIFNet can outperform manystate-of-the-art competing methods. The code and dataset are available athttps://github.com/mengyu212/VIFNet_dehazing.</description><author>Meng Yu, Te Cui, Haoyang Lu, Yufeng Yue</author><pubDate>Thu, 11 Apr 2024 15:31:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07790v1</guid></item><item><title>AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene Graph Generation</title><link>http://arxiv.org/abs/2404.07788v1</link><description>Scene graph generation (SGG) aims to understand the visual objects and theirsemantic relationships from one given image. Until now, lots of SGG datasetswith the eyelevel view are released but the SGG dataset with the overhead viewis scarcely studied. By contrast to the object occlusion problem in theeyelevel view, which impedes the SGG, the overhead view provides a newperspective that helps to promote the SGG by providing a clear perception ofthe spatial relationships of objects in the ground scene. To fill in the gap ofthe overhead view dataset, this paper constructs and releases an aerial imageurban scene graph generation (AUG) dataset. Images from the AUG dataset arecaptured with the low-attitude overhead view. In the AUG dataset, 25,594objects, 16,970 relationships, and 27,175 attributes are manually annotated. Toavoid the local context being overwhelmed in the complex aerial urban scene,this paper proposes one new locality-preserving graph convolutional network(LPG). Different from the traditional graph convolutional network, which hasthe natural advantage of capturing the global context for SGG, theconvolutional layer in the LPG integrates the non-destructive initial featuresof the objects with dynamically updated neighborhood information to preservethe local context under the premise of mining the global context. To addressthe problem that there exists an extra-large number of potential objectrelationship pairs but only a small part of them is meaningful in AUG, wepropose the adaptive bounding box scaling factor for potential relationshipdetection (ABS-PRD) to intelligently prune the meaningless relationship pairs.Extensive experiments on the AUG dataset show that our LPG can significantlyoutperform the state-of-the-art methods and the effectiveness of the proposedlocality-preserving strategy.</description><author>Yansheng Li, Kun Li, Yongjun Zhang, Linlin Wang, Dingwen Zhang</author><pubDate>Thu, 11 Apr 2024 15:29:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07788v1</guid></item><item><title>PRAM: Place Recognition Anywhere Model for Efficient Visual Localization</title><link>http://arxiv.org/abs/2404.07785v1</link><description>Humans localize themselves efficiently in known environments by firstrecognizing landmarks defined on certain objects and their spatialrelationships, and then verifying the location by aligning detailed structuresof recognized objects with those in the memory. Inspired by this, we proposethe place recognition anywhere model (PRAM) to perform visual localization asefficiently as humans do. PRAM consists of two main components - recognitionand registration. In detail, first of all, a self-supervised map-centriclandmark definition strategy is adopted, making places in either indoor oroutdoor scenes act as unique landmarks. Then, sparse keypoints extracted fromimages, are utilized as the input to a transformer-based deep neural networkfor landmark recognition; these keypoints enable PRAM to recognize hundreds oflandmarks with high time and memory efficiency. Keypoints along with recognizedlandmark labels are further used for registration between query images and the3D landmark map. Different from previous hierarchical methods, PRAM discardsglobal and local descriptors, and reduces over 90% storage. Since PRAM utilizesrecognition and landmark-wise verification to replace global reference searchand exhaustive matching respectively, it runs 2.4 times faster than priorstate-of-the-art approaches. Moreover, PRAM opens new directions for visuallocalization including multi-modality localization, map-centric featurelearning, and hierarchical scene coordinate regression.</description><author>Fei Xue, Ignas Budvytis, Roberto Cipolla</author><pubDate>Thu, 11 Apr 2024 15:28:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07785v1</guid></item><item><title>Diffusion Time-step Curriculum for One Image to 3D Generation</title><link>http://arxiv.org/abs/2404.04562v2</link><description>Score distillation sampling~(SDS) has been widely adopted to overcome theabsence of unseen views in reconstructing 3D objects from a \textbf{single}image. It leverages pre-trained 2D diffusion models as teacher to guide thereconstruction of student 3D models. Despite their remarkable success,SDS-based methods often encounter geometric artifacts and texture saturation.We find out the crux is the overlooked indiscriminate treatment of diffusiontime-steps during optimization: it unreasonably treats the student-teacherknowledge distillation to be equal at all time-steps and thus entanglescoarse-grained and fine-grained modeling. Therefore, we propose the DiffusionTime-step Curriculum one-image-to-3D pipeline (DTC123), which involves both theteacher and student models collaborating with the time-step curriculum in acoarse-to-fine manner. Extensive experiments on NeRF4, RealFusion15, GSO andLevel50 benchmark demonstrate that DTC123 can produce multi-view consistent,high-quality, and diverse 3D assets. Codes and more generation demos will bereleased in https://github.com/yxymessi/DTC123.</description><author>Xuanyu Yi, Zike Wu, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Hanwang Zhang</author><pubDate>Thu, 11 Apr 2024 15:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04562v2</guid></item><item><title>Exploring Masked Autoencoders for Sensor-Agnostic Image Retrieval in Remote Sensing</title><link>http://arxiv.org/abs/2401.07782v2</link><description>Self-supervised learning through masked autoencoders (MAEs) has recentlyattracted great attention for remote sensing (RS) image representationlearning, and thus embodies a significant potential for content-based imageretrieval (CBIR) from ever-growing RS image archives. However, the existingstudies on MAEs in RS assume that the considered RS images are acquired by asingle image sensor, and thus are only suitable for uni-modal CBIR problems.The effectiveness of MAEs for cross-sensor CBIR, which aims to searchsemantically similar images across different image modalities, has not beenexplored yet. In this paper, we take the first step to explore theeffectiveness of MAEs for sensor-agnostic CBIR in RS. To this end, we present asystematic overview on the possible adaptations of the vanilla MAE to exploitmasked image modeling on multi-sensor RS image archives (denoted ascross-sensor masked autoencoders [CSMAEs]). Based on different adjustmentsapplied to the vanilla MAE, we introduce different CSMAE models. We alsoprovide an extensive experimental analysis of these CSMAE models. We finallyderive a guideline to exploit masked image modeling for uni-modal andcross-modal CBIR problems in RS. The code of this work is publicly available athttps://github.com/jakhac/CSMAE.</description><author>Jakob Hackstein, Gencer Sumbul, Kai Norman Clasen, Begüm Demir</author><pubDate>Thu, 11 Apr 2024 15:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07782v2</guid></item></channel></rss>