<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 03 Jun 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles</title><link>http://arxiv.org/abs/2306.00989v1</link><description>Modern hierarchical vision transformers have added several vision-specificcomponents in the pursuit of supervised classification performance. While thesecomponents lead to effective accuracies and attractive FLOP counts, the addedcomplexity actually makes these transformers slower than their vanilla ViTcounterparts. In this paper, we argue that this additional bulk is unnecessary.By pretraining with a strong visual pretext task (MAE), we can strip out allthe bells-and-whistles from a state-of-the-art multi-stage vision transformerwithout losing accuracy. In the process, we create Hiera, an extremely simplehierarchical vision transformer that is more accurate than previous modelswhile being significantly faster both at inference and during training. Weevaluate Hiera on a variety of tasks for image and video recognition. Our codeand models are available at https://github.com/facebookresearch/hiera.</description><author>Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer</author><pubDate>Thu, 01 Jun 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00989v1</guid></item><item><title>StyleGAN knows Normal, Depth, Albedo, and More</title><link>http://arxiv.org/abs/2306.00987v1</link><description>Intrinsic images, in the original sense, are image-like maps of sceneproperties like depth, normal, albedo or shading. This paper demonstrates thatStyleGAN can easily be induced to produce intrinsic images. The procedure isstraightforward. We show that, if StyleGAN produces $G({w})$ from latents${w}$, then for each type of intrinsic image, there is a fixed offset ${d}_c$so that $G({w}+{d}_c)$ is that type of intrinsic image for $G({w})$. Here${d}_c$ is {\em independent of ${w}$}. The StyleGAN we used was pretrained byothers, so this property is not some accident of our training regime. We showthat there are image transformations StyleGAN will {\em not} produce in thisfashion, so StyleGAN is not a generic image regression engine. It is conceptually exciting that an image generator should ``know'' andrepresent intrinsic images. There may also be practical advantages to using agenerative model to produce intrinsic images. The intrinsic images obtainedfrom StyleGAN compare well both qualitatively and quantitatively with thoseobtained by using SOTA image regression techniques; but StyleGAN's intrinsicimages are robust to relighting effects, unlike SOTA methods.</description><author>Anand Bhattad, Daniel McKee, Derek Hoiem, D. A. Forsyth</author><pubDate>Thu, 01 Jun 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00987v1</guid></item><item><title>Continual Learning for Abdominal Multi-Organ and Tumor Segmentation</title><link>http://arxiv.org/abs/2306.00988v1</link><description>The ability to dynamically extend a model to new data and classes is criticalfor multiple organ and tumor segmentation. However, due to privacy regulations,accessing previous data and annotations can be problematic in the medicaldomain. This poses a significant barrier to preserving the high segmentationaccuracy of the old classes when learning from new classes because of thecatastrophic forgetting problem. In this paper, we first empiricallydemonstrate that simply using high-quality pseudo labels can fairly mitigatethis problem in the setting of organ segmentation. Furthermore, we put forwardan innovative architecture designed specifically for continuous organ and tumorsegmentation, which incurs minimal computational overhead. Our proposed designinvolves replacing the conventional output layer with a suite of lightweight,class-specific heads, thereby offering the flexibility to accommodate newlyemerging classes. These heads enable independent predictions for newlyintroduced and previously learned classes, effectively minimizing the impact ofnew classes on old ones during the course of continual learning. We furtherpropose incorporating Contrastive Language-Image Pretraining (CLIP) embeddingsinto the organ-specific heads. These embeddings encapsulate the semanticinformation of each class, informed by extensive image-text co-training. Theproposed method is evaluated on both in-house and public abdominal CT datasetsunder organ and tumor segmentation tasks. Empirical results suggest that theproposed design improves the segmentation performance of a baseline neuralnetwork on newly-introduced and previously-learned classes along the learningtrajectory.</description><author>Yixiao Zhang, Xinyi Li, Huimiao Chen, Alan Yuille, Yaoyao Liu, Zongwei Zhou</author><pubDate>Thu, 01 Jun 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00988v1</guid></item><item><title>Diffusion Self-Guidance for Controllable Image Generation</title><link>http://arxiv.org/abs/2306.00986v1</link><description>Large-scale generative models are capable of producing high-quality imagesfrom detailed text descriptions. However, many aspects of an image aredifficult or impossible to convey through text. We introduce self-guidance, amethod that provides greater control over generated images by guiding theinternal representations of diffusion models. We demonstrate that propertiessuch as the shape, location, and appearance of objects can be extracted fromthese representations and used to steer sampling. Self-guidance works similarlyto classifier guidance, but uses signals present in the pretrained modelitself, requiring no additional models or training. We show how a simple set ofproperties can be composed to perform challenging image manipulations, such asmodifying the position or size of objects, merging the appearance of objects inone image with the layout of another, composing objects from many images intoone, and more. We also show that self-guidance can be used to edit real images.For results and an interactive demo, see our project page athttps://dave.ml/selfguidance/</description><author>Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, Aleksander Holynski</author><pubDate>Thu, 01 Jun 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00986v1</guid></item><item><title>Using generative AI to investigate medical imagery models and datasets</title><link>http://arxiv.org/abs/2306.00985v1</link><description>AI models have shown promise in many medical imaging tasks. However, ourability to explain what signals these models have learned is severely lacking.Explanations are needed in order to increase the trust in AI-based models, andcould enable novel scientific discovery by uncovering signals in the data thatare not yet known to experts. In this paper, we present a method for automaticvisual explanations leveraging team-based expertise by generating hypotheses ofwhat visual signals in the images are correlated with the task. We propose thefollowing 4 steps: (i) Train a classifier to perform a given task (ii) Train aclassifier guided StyleGAN-based image generator (StylEx) (iii) Automaticallydetect and visualize the top visual attributes that the classifier is sensitivetowards (iv) Formulate hypotheses for the underlying mechanisms, to stimulatefuture research. Specifically, we present the discovered attributes to aninterdisciplinary panel of experts so that hypotheses can account for socialand structural determinants of health. We demonstrate results on eightprediction tasks across three medical imaging modalities: retinal fundusphotographs, external eye photographs, and chest radiographs. We showcaseexamples of attributes that capture clinically known features, confounders thatarise from factors beyond physiological mechanisms, and reveal a number ofphysiologically plausible novel attributes. Our approach has the potential toenable researchers to better understand, improve their assessment, and extractnew knowledge from AI-based models. Importantly, we highlight that attributesgenerated by our framework can capture phenomena beyond physiology orpathophysiology, reflecting the real world nature of healthcare delivery andsocio-cultural factors. Finally, we intend to release code to enableresearchers to train their own StylEx models and analyze their predictivetasks.</description><author>Oran Lang, Doron Yaya-Stupp, Ilana Traynis, Heather Cole-Lewis, Chloe R. Bennett, Courtney Lyles, Charles Lau, Christopher Semturs, Dale R. Webster, Greg S. Corrado, Avinatan Hassidim, Yossi Matias, Yun Liu, Naama Hammel, Boris Babenko</author><pubDate>Thu, 01 Jun 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00985v1</guid></item><item><title>StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners</title><link>http://arxiv.org/abs/2306.00984v1</link><description>We investigate the potential of learning visual representations usingsynthetic images generated by text-to-image models. This is a natural questionin the light of the excellent performance of such models in generatinghigh-quality images. We consider specifically the Stable Diffusion, one of theleading open source text-to-image models. We show that (1) when the generativemodel is configured with proper classifier-free guidance scale, trainingself-supervised methods on synthetic images can match or beat the real imagecounterpart; (2) by treating the multiple images generated from the same textprompt as positives for each other, we develop a multi-positive contrastivelearning method, which we call StableRep. With solely synthetic images, therepresentations learned by StableRep surpass the performance of representationslearned by SimCLR and CLIP using the same set of text prompts and correspondingreal images, on large scale datasets. When we further add language supervision,StableRep trained with 20M synthetic images achieves better accuracy than CLIPtrained with 50M real images.</description><author>Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan</author><pubDate>Thu, 01 Jun 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00984v1</guid></item><item><title>StyleDrop: Text-to-Image Generation in Any Style</title><link>http://arxiv.org/abs/2306.00983v1</link><description>Pre-trained large text-to-image models synthesize impressive images with anappropriate use of text prompts. However, ambiguities inherent in naturallanguage and out-of-distribution effects make it hard to synthesize imagestyles, that leverage a specific design pattern, texture or material. In thispaper, we introduce StyleDrop, a method that enables the synthesis of imagesthat faithfully follow a specific style using a text-to-image model. Theproposed method is extremely versatile and captures nuances and details of auser-provided style, such as color schemes, shading, design patterns, and localand global effects. It efficiently learns a new style by fine-tuning very fewtrainable parameters (less than $1\%$ of total model parameters) and improvingthe quality via iterative training with either human or automated feedback.Better yet, StyleDrop is able to deliver impressive results even when the usersupplies only a single image that specifies the desired style. An extensivestudy shows that, for the task of style tuning text-to-image models, StyleDropimplemented on Muse convincingly outperforms other methods, includingDreamBooth and textual inversion on Imagen or Stable Diffusion. More resultsare available at our project website: https://styledrop.github.io</description><author>Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, Dilip Krishnan</author><pubDate>Thu, 01 Jun 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00983v1</guid></item><item><title>SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds</title><link>http://arxiv.org/abs/2306.00980v1</link><description>Text-to-image diffusion models can create stunning images from naturallanguage descriptions that rival the work of professional artists andphotographers. However, these models are large, with complex networkarchitectures and tens of denoising iterations, making them computationallyexpensive and slow to run. As a result, high-end GPUs and cloud-based inferenceare required to run diffusion models at scale. This is costly and has privacyimplications, especially when user data is sent to a third party. To overcomethese challenges, we present a generic approach that, for the first time,unlocks running text-to-image diffusion models on mobile devices in less than$2$ seconds. We achieve so by introducing efficient network architecture andimproving step distillation. Specifically, we propose an efficient UNet byidentifying the redundancy of the original model and reducing the computationof the image decoder via data distillation. Further, we enhance the stepdistillation by exploring training strategies and introducing regularizationfrom classifier-free guidance. Our extensive experiments on MS-COCO show thatour model with $8$ denoising steps achieves better FID and CLIP scores thanStable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creationby bringing powerful text-to-image diffusion models to the hands of users.</description><author>Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, Jian Ren</author><pubDate>Thu, 01 Jun 2023 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00980v1</guid></item><item><title>Building Rearticulable Models for Arbitrary 3D Objects from 4D Point Clouds</title><link>http://arxiv.org/abs/2306.00979v1</link><description>We build rearticulable models for arbitrary everyday man-made objectscontaining an arbitrary number of parts that are connected together inarbitrary ways via 1 degree-of-freedom joints. Given point cloud videos of sucheveryday objects, our method identifies the distinct object parts, what partsare connected to what other parts, and the properties of the joints connectingeach part pair. We do this by jointly optimizing the part segmentation,transformation, and kinematics using a novel energy minimization framework. Ourinferred animatable models, enables retargeting to novel poses with sparsepoint correspondences guidance. We test our method on a new articulating robotdataset, and the Sapiens dataset with common daily objects, as well asreal-world scans. Experiments show that our method outperforms two leadingprior works on various metrics.</description><author>Shaowei Liu, Saurabh Gupta, Shenlong Wang</author><pubDate>Thu, 01 Jun 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00979v1</guid></item><item><title>TopEx: Topic-based Explanations for Model Comparison</title><link>http://arxiv.org/abs/2306.00976v1</link><description>Meaningfully comparing language models is challenging with currentexplanation methods. Current explanations are overwhelming for humans due tolarge vocabularies or incomparable across models. We present TopEx, anexplanation method that enables a level playing field for comparing languagemodels via model-agnostic topics. We demonstrate how TopEx can identifysimilarities and differences between DistilRoBERTa and GPT-2 on a variety ofNLP tasks.</description><author>Shreya Havaldar, Adam Stein, Eric Wong, Lyle Ungar</author><pubDate>Thu, 01 Jun 2023 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00976v1</guid></item><item><title>AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation</title><link>http://arxiv.org/abs/2306.00977v1</link><description>During interactive segmentation, a model and a user work together todelineate objects of interest in a 3D point cloud. In an iterative process, themodel assigns each data point to an object (or the background), while the usercorrects errors in the resulting segmentation and feeds them back into themodel. From a machine learning perspective the goal is to design the model andthe feedback mechanism in a way that minimizes the required user input. Thecurrent best practice segments objects one at a time, and asks the user toprovide positive clicks to indicate regions wrongly assigned to the backgroundand negative clicks to indicate regions wrongly assigned to the object(foreground). Sequentially visiting objects is wasteful, since it disregardssynergies between objects: a positive click for a given object can, bydefinition, serve as a negative click for nearby objects, moreover a directcompetition between adjacent objects can speed up the identification of theircommon boundary. We introduce AGILE3D, an efficient, attention-based model that(1) supports simultaneous segmentation of multiple 3D objects, (2) yields moreaccurate segmentation masks with fewer user clicks, and (3) offers fasterinference. We encode the point cloud into a latent feature representation, andview user clicks as queries and employ cross-attention to represent contextualrelations between different click locations as well as between clicks and the3D point cloud features. Every time new clicks are added, we only need to run alightweight decoder that produces updated segmentation masks. In experimentswith four different point cloud datasets, AGILE3D sets a new state of the art,moreover, we also verify its practicality in real-world setups with a real userstudy.</description><author>Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis Engelmann, Bastian Leibe, Konrad Schindler, Theodora Kontogianni</author><pubDate>Thu, 01 Jun 2023 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00977v1</guid></item><item><title>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</title><link>http://arxiv.org/abs/2306.00978v1</link><description>Large language models (LLMs) have shown excellent performance on varioustasks, but the astronomical model size raises the hardware barrier for serving(memory size) and slows down token generation (memory bandwidth). In thispaper, we propose Activation-aware Weight Quantization (AWQ), ahardware-friendly approach for LLM low-bit weight-only quantization. Our methodis based on the observation that weights are not equally important: protectingonly 1% of salient weights can greatly reduce quantization error. We thenpropose to search for the optimal per-channel scaling that protects the salientweights by observing the activation, not weights. AWQ does not rely on anybackpropagation or reconstruction, so it can well preserve LLMs' generalizationability on different domains and modalities, without overfitting to thecalibration set; it also does not rely on any data layout reordering,maintaining the hardware efficiency. AWQ outperforms existing work on variouslanguage modeling, common sense QA, and domain-specific benchmarks. Thanks tobetter generalization, it achieves excellent quantization performance forinstruction-tuned LMs and, for the first time, multi-modal LMs. We alsoimplement efficient tensor core kernels with reorder-free online dequantizationto accelerate AWQ, achieving a 1.45x speedup over GPTQ and is 1.85x faster thanthe cuBLAS FP16 implementation. Our method provides a turn-key solution tocompress LLMs to 3/4 bits for efficient deployment.</description><author>Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han</author><pubDate>Thu, 01 Jun 2023 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00978v1</guid></item><item><title>Active Reinforcement Learning under Limited Visual Observability</title><link>http://arxiv.org/abs/2306.00975v1</link><description>In this work, we investigate Active Reinforcement Learning (Active-RL), wherean embodied agent simultaneously learns action policy for the task while alsocontrolling its visual observations in partially observable environments. Wedenote the former as motor policy and the latter as sensory policy. Forexample, humans solve real world tasks by hand manipulation (motor policy)together with eye movements (sensory policy). Active-RL poses challenges oncoordinating two policies given their mutual influence. We propose SUGARL,Sensorimotor Understanding Guided Active Reinforcement Learning, a frameworkthat models motor and sensory policies separately, but jointly learns themusing with an intrinsic sensorimotor reward. This learnable reward is assignedby sensorimotor reward module, incentivizes the sensory policy to selectobservations that are optimal to infer its own motor action, inspired by thesensorimotor stage of humans. Through a series of experiments, we show theeffectiveness of our method across a range of observability conditions and itsadaptability to existed RL algorithms. The sensory policies learned through ourmethod are observed to exhibit effective active vision strategies.</description><author>Jinghuan Shang, Michael S. Ryoo</author><pubDate>Thu, 01 Jun 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00975v1</guid></item><item><title>Intriguing Properties of Text-guided Diffusion Models</title><link>http://arxiv.org/abs/2306.00974v1</link><description>Text-guided diffusion models (TDMs) are widely applied but can failunexpectedly. Common failures include: (i) natural-looking text promptsgenerating images with the wrong content, or (ii) different random samples ofthe latent variables that generate vastly different, and even unrelated,outputs despite being conditioned on the same text prompt. In this work, we aimto study and understand the failure modes of TDMs in more detail. To achievethis, we propose SAGE, an adversarial attack on TDMs that uses imageclassifiers as surrogate loss functions, to search over the discrete promptspace and the high-dimensional latent space of TDMs to automatically discoverunexpected behaviors and failure cases in the image generation. We make severaltechnical contributions to ensure that SAGE finds failure cases of thediffusion model, rather than the classifier, and verify this in a human study.Our study reveals four intriguing properties of TDMs that have not beensystematically studied before: (1) We find a variety of natural text promptsproducing images that fail to capture the semantics of input texts. Wecategorize these failures into ten distinct types based on the underlyingcauses. (2) We find samples in the latent space (which are not outliers) thatlead to distorted images independent of the text prompt, suggesting that partsof the latent space are not well-structured. (3) We also find latent samplesthat lead to natural-looking images which are unrelated to the text prompt,implying a potential misalignment between the latent and prompt spaces. (4) Byappending a single adversarial token embedding to an input prompt we cangenerate a variety of specified target objects, while only minimally affectingthe CLIP score. This demonstrates the fragility of language representations andraises potential safety concerns.</description><author>Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, Alan Yuille</author><pubDate>Thu, 01 Jun 2023 18:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00974v1</guid></item><item><title>Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion Models</title><link>http://arxiv.org/abs/2306.00973v1</link><description>Generative models have recently exhibited exceptional capabilities in variousscenarios, for example, image generation based on text description. In thiswork, we focus on the task of generating a series of coherent image sequencebased on a given storyline, denoted as open-ended visual storytelling. We makethe following three contributions: (i) to fulfill the task of visualstorytelling, we introduce two modules into a pre-trained stable diffusionmodel, and construct an auto-regressive image generator, termed as StoryGen,that enables to generate the current frame by conditioning on both a textprompt and a preceding frame; (ii) to train our proposed model, we collectpaired image and text samples by sourcing from various online sources, such asvideos, E-books, and establish a data processing pipeline for constructing adiverse dataset, named StorySalon, with a far larger vocabulary than existinganimation-specific datasets; (iii) we adopt a three-stage curriculum trainingstrategy, that enables style transfer, visual context conditioning, and humanfeedback alignment, respectively. Quantitative experiments and human evaluationhave validated the superiority of our proposed model, in terms of imagequality, style consistency, content consistency, and visual-language alignment.We will make the code, model, and dataset publicly available to the researchcommunity.</description><author>Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Weidi Xie</author><pubDate>Thu, 01 Jun 2023 18:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00973v1</guid></item><item><title>Improving and Benchmarking Offline Reinforcement Learning Algorithms</title><link>http://arxiv.org/abs/2306.00972v1</link><description>Recently, Offline Reinforcement Learning (RL) has achieved remarkableprogress with the emergence of various algorithms and datasets. However, thesemethods usually focus on algorithmic advancements, ignoring that many low-levelimplementation choices considerably influence or even drive the finalperformance. As a result, it becomes hard to attribute the progress in OfflineRL as these choices are not sufficiently discussed and aligned in theliterature. In addition, papers focusing on a dataset (e.g., D4RL) often ignorealgorithms proposed on another dataset (e.g., RL Unplugged), causing isolationamong the algorithms, which might slow down the overall progress. Therefore,this work aims to bridge the gaps caused by low-level choices and datasets. Tothis end, we empirically investigate 20 implementation choices using threerepresentative algorithms (i.e., CQL, CRR, and IQL) and present a guidebook forchoosing implementations. Following the guidebook, we find two variants CRR+and CQL+ , achieving new state-of-the-art on D4RL. Moreover, we benchmark eightpopular offline RL algorithms across datasets under unified training andevaluation framework. The findings are inspiring: the success of a learningparadigm severely depends on the data distribution, and some previousconclusions are biased by the dataset used. Our code is available athttps://github.com/sail-sg/offbench.</description><author>Bingyi Kang, Xiao Ma, Yirui Wang, Yang Yue, Shuicheng Yan</author><pubDate>Thu, 01 Jun 2023 18:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00972v1</guid></item><item><title>ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image Generation</title><link>http://arxiv.org/abs/2306.00971v1</link><description>Personalized text-to-image generation using diffusion models has recentlybeen proposed and attracted lots of attention. Given a handful of imagescontaining a novel concept (e.g., a unique toy), we aim to tune the generativemodel to capture fine visual details of the novel concept and generatephotorealistic images following a text condition. We present a plug-in method,named ViCo, for fast and lightweight personalized generation. Specifically, wepropose an image attention module to condition the diffusion process on thepatch-wise visual semantics. We introduce an attention-based object mask thatcomes almost at no cost from the attention module. In addition, we design asimple regularization based on the intrinsic properties of text-image attentionmaps to alleviate the common overfitting degradation. Unlike many existingmodels, our method does not finetune any parameters of the original diffusionmodel. This allows more flexible and transferable model deployment. With onlylight parameter training (~6% of the diffusion U-Net), our method achievescomparable or even better performance than all state-of-the-art models bothqualitatively and quantitatively.</description><author>Shaozhe Hao, Kai Han, Shihao Zhao, Kwan-Yee K. Wong</author><pubDate>Thu, 01 Jun 2023 18:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00971v1</guid></item><item><title>GRES: Generalized Referring Expression Segmentation</title><link>http://arxiv.org/abs/2306.00968v1</link><description>Referring Expression Segmentation (RES) aims to generate a segmentation maskfor the object described by a given language expression. Existing classic RESdatasets and methods commonly support single-target expressions only, i.e., oneexpression refers to one target object. Multi-target and no-target expressionsare not considered. This limits the usage of RES in practice. In this paper, weintroduce a new benchmark called Generalized Referring Expression Segmentation(GRES), which extends the classic RES to allow expressions to refer to anarbitrary number of target objects. Towards this, we construct the firstlarge-scale GRES dataset called gRefCOCO that contains multi-target, no-target,and single-target expressions. GRES and gRefCOCO are designed to bewell-compatible with RES, facilitating extensive experiments to study theperformance gap of the existing RES methods on the GRES task. In theexperimental study, we find that one of the big challenges of GRES is complexrelationship modeling. Based on this, we propose a region-based GRES baselineReLA that adaptively divides the image into regions with sub-instance clues,and explicitly models the region-region and region-language dependencies. Theproposed approach ReLA achieves new state-of-the-art performance on the bothnewly proposed GRES and classic RES tasks. The proposed gRefCOCO dataset andmethod are available at https://henghuiding.github.io/GRES.</description><author>Chang Liu, Henghui Ding, Xudong Jiang</author><pubDate>Thu, 01 Jun 2023 18:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00968v1</guid></item><item><title>The Hidden Language of Diffusion Models</title><link>http://arxiv.org/abs/2306.00966v1</link><description>Text-to-image diffusion models have demonstrated an unparalleled ability togenerate high-quality, diverse images from a textual concept (e.g., "a doctor","love"). However, the internal process of mapping text to a rich visualrepresentation remains an enigma. In this work, we tackle the challenge ofunderstanding concept representations in text-to-image models by decomposing aninput text prompt into a small set of interpretable elements. This is achievedby learning a pseudo-token that is a sparse weighted combination of tokens fromthe model's vocabulary, with the objective of reconstructing the imagesgenerated for the given concept. Applied over the state-of-the-art StableDiffusion model, this decomposition reveals non-trivial and surprisingstructures in the representations of concepts. For example, we find that someconcepts such as "a president" or "a composer" are dominated by specificinstances (e.g., "Obama", "Biden") and their interpolations. Other concepts,such as "happiness" combine associated terms that can be concrete ("family","laughter") or abstract ("friendship", "emotion"). In addition to peering intothe inner workings of Stable Diffusion, our method also enables applicationssuch as single-image decomposition to tokens, bias detection and mitigation,and semantic image manipulation. Our code will be available at:https://hila-chefer.github.io/Conceptor/</description><author>Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, Lior Wolf</author><pubDate>Thu, 01 Jun 2023 18:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00966v1</guid></item><item><title>BUOL: A Bottom-Up Framework with Occupancy-aware Lifting for Panoptic 3D Scene Reconstruction From A Single Image</title><link>http://arxiv.org/abs/2306.00965v1</link><description>Understanding and modeling the 3D scene from a single image is a practicalproblem. A recent advance proposes a panoptic 3D scene reconstruction task thatperforms both 3D reconstruction and 3D panoptic segmentation from a singleimage. Although having made substantial progress, recent works only focus ontop-down approaches that fill 2D instances into 3D voxels according toestimated depth, which hinders their performance by two ambiguities. (1)instance-channel ambiguity: The variable ids of instances in each scene lead toambiguity during filling voxel channels with 2D information, confusing thefollowing 3D refinement. (2) voxel-reconstruction ambiguity: 2D-to-3D liftingwith estimated single view depth only propagates 2D information onto thesurface of 3D regions, leading to ambiguity during the reconstruction ofregions behind the frontal view surface. In this paper, we propose BUOL, aBottom-Up framework with Occupancy-aware Lifting to address the two issues forpanoptic 3D scene reconstruction from a single image. For instance-channelambiguity, a bottom-up framework lifts 2D information to 3D voxels based ondeterministic semantic assignments rather than arbitrary instance idassignments. The 3D voxels are then refined and grouped into 3D instancesaccording to the predicted 2D instance centers. For voxel-reconstructionambiguity, the estimated multi-plane occupancy is leveraged together with depthto fill the whole regions of things and stuff. Our method shows a tremendousperformance advantage over state-of-the-art methods on synthetic dataset3D-Front and real-world dataset Matterport3D. Code and models are available inhttps://github.com/chtsy/buol.</description><author>Tao Chu, Pan Zhang, Qiong Liu, Jiaqi Wang</author><pubDate>Thu, 01 Jun 2023 18:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00965v1</guid></item><item><title>Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation</title><link>http://arxiv.org/abs/2306.00964v1</link><description>Text-conditional diffusion models are able to generate high-fidelity imageswith diverse contents. However, linguistic representations frequently exhibitambiguous descriptions of the envisioned objective imagery, requiring theincorporation of additional control signals to bolster the efficacy oftext-guided diffusion models. In this work, we propose Cocktail, a pipeline tomix various modalities into one embedding, amalgamated with a generalizedControlNet (gControlNet), a controllable normalisation (ControlNorm), and aspatial guidance sampling method, to actualize multi-modal andspatially-refined control for text-conditional diffusion models. Specifically,we introduce a hyper-network gControlNet, dedicated to the alignment andinfusion of the control signals from disparate modalities into the pre-traineddiffusion model. gControlNet is capable of accepting flexible modality signals,encompassing the simultaneous reception of any combination of modality signals,or the supplementary fusion of multiple modality signals. The control signalsare then fused and injected into the backbone model according to our proposedControlNorm. Furthermore, our advanced spatial guidance sampling methodologyproficiently incorporates the control signal into the designated region,thereby circumventing the manifestation of undesired objects within thegenerated image. We demonstrate the results of our method in controllingvarious modalities, proving high-quality synthesis and fidelity to multipleexternal signals.</description><author>Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, Tat-Jen Cham</author><pubDate>Thu, 01 Jun 2023 18:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00964v1</guid></item><item><title>Dynamic Algorithms for Matroid Submodular Maximization</title><link>http://arxiv.org/abs/2306.00959v1</link><description>Submodular maximization under matroid and cardinality constraints areclassical problems with a wide range of applications in machine learning,auction theory, and combinatorial optimization. In this paper, we considerthese problems in the dynamic setting where (1) we have oracle access to amonotone submodular function $f: 2^{V} \rightarrow \mathbb{R}^+$ and (2) we aregiven a sequence $\mathcal{S}$ of insertions and deletions of elements of anunderlying ground set $V$. We develop the first parameterized (by the rank $k$ of a matroid$\mathcal{M}$) dynamic $(4+\epsilon)$-approximation algorithm for thesubmodular maximization problem under the matroid constraint using an expectedworst-case $O(k\log(k)\log^3{(k/\epsilon)})$ query complexity where $0 &lt;\epsilon \le 1$. Chen and Peng at STOC'22 studied the complexity of thisproblem in the insertion-only dynamic model (a restricted version of the fullydynamic model where deletion is not allowed), and they raised the followingimportant open question: *"for fully dynamic streams [sequences of insertionsand deletions of elements], there is no known constant-factor approximationalgorithm with poly(k) amortized queries for matroid constraints."* Our dynamicalgorithm answers this question as well as an open problem of Lattanzi et al.(NeurIPS'20) affirmatively. As a byproduct, for the submodular maximization under the cardinalityconstraint $k$, we propose a parameterized (by the cardinality constraint $k$)dynamic algorithm that maintains a $(2+\epsilon)$-approximate solution of thesequence $\mathcal{S}$ at any time $t$ using the expected amortized worst-casecomplexity $O(k\epsilon^{-1}\log^2(k))$. This is the first dynamic algorithmfor the problem that has a query complexity independent of the size of groundset $V$.</description><author>Kiarash Banihashem, Leyla Biabani, Samira Goudarzi, MohammadTaghi Hajiaghayi, Peyman Jabbarzade, Morteza Monemizadeh</author><pubDate>Thu, 01 Jun 2023 18:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00959v1</guid></item><item><title>LIV: Language-Image Representations and Rewards for Robotic Control</title><link>http://arxiv.org/abs/2306.00958v1</link><description>We present Language-Image Value learning (LIV), a unified objective forvision-language representation and reward learning from action-free videos withtext annotations. Exploiting a novel connection between dual reinforcementlearning and mutual information contrastive learning, the LIV objective trainsa multi-modal representation that implicitly encodes a universal value functionfor tasks specified as language or image goals. We use LIV to pre-train thefirst control-centric vision-language representation from large human videodatasets such as EpicKitchen. Given only a language or image goal, thepre-trained LIV model can assign dense rewards to each frame in videos ofunseen robots or humans attempting that task in unseen environments. Further,when some target domain-specific data is available, the same objective can beused to fine-tune and improve LIV and even other pre-trained representationsfor robotic control and reward specification in that domain. In our experimentson several simulated and real-world robot environments, LIV models consistentlyoutperform the best prior input state representations for imitation learning,as well as reward specification methods for policy synthesis. Our resultsvalidate the advantages of joint vision-language representation and rewardlearning within the unified, compact LIV framework.</description><author>Yecheng Jason Ma, William Liang, Vaidehi Som, Vikash Kumar, Amy Zhang, Osbert Bastani, Dinesh Jayaraman</author><pubDate>Thu, 01 Jun 2023 18:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00958v1</guid></item><item><title>The ObjectFolder Benchmark: Multisensory Learning with Neural and Real Objects</title><link>http://arxiv.org/abs/2306.00956v1</link><description>We introduce the ObjectFolder Benchmark, a benchmark suite of 10 tasks formultisensory object-centric learning, centered around object recognition,reconstruction, and manipulation with sight, sound, and touch. We alsointroduce the ObjectFolder Real dataset, including the multisensorymeasurements for 100 real-world household objects, building upon a newlydesigned pipeline for collecting the 3D meshes, videos, impact sounds, andtactile readings of real-world objects. We conduct systematic benchmarking onboth the 1,000 multisensory neural objects from ObjectFolder, and the realmultisensory data from ObjectFolder Real. Our results demonstrate theimportance of multisensory perception and reveal the respective roles ofvision, audio, and touch for different object-centric learning tasks. Bypublicly releasing our dataset and benchmark suite, we hope to catalyze andenable new research in multisensory object-centric learning in computer vision,robotics, and beyond. Project page: https://objectfolder.stanford.edu</description><author>Ruohan Gao, Yiming Dou, Hao Li, Tanmay Agarwal, Jeannette Bohg, Yunzhu Li, Li Fei-Fei, Jiajun Wu</author><pubDate>Thu, 01 Jun 2023 18:51:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00956v1</guid></item><item><title>Speaker-specific Thresholding for Robust Imposter Identification in Unseen Speaker Recognition</title><link>http://arxiv.org/abs/2306.00952v1</link><description>Speaker identification systems are deployed in diverse environments, oftendifferent from the lab conditions on which they are trained and tested. In thispaper, first, we show the problem of generalization using fixed thresholdscomputed using the equal error rate metric. Secondly, we introduce a novel andgeneralizable speaker-specific thresholding technique for robust imposteridentification in unseen speaker identification. We propose a speaker-specificadaptive threshold, which can be computed using the enrollment audio samples,for identifying imposters in unseen speaker identification. Furthermore, weshow the efficacy of the proposed technique on VoxCeleb1, VCTK and the FFSVC2022 datasets, beating the baseline fixed thresholding by up to 25%. Finally,we exhibit that the proposed algorithm is also generalizable, demonstrating itsperformance on ResNet50, ECAPA-TDNN and RawNet3 speaker encoders.</description><author>Ashutosh Chaubey, Sparsh Sinha, Susmita Ghose</author><pubDate>Thu, 01 Jun 2023 18:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00952v1</guid></item><item><title>Differential Diffusion: Giving Each Pixel Its Strength</title><link>http://arxiv.org/abs/2306.00950v1</link><description>Text-based image editing has advanced significantly in recent years. With therise of diffusion models, image editing via textual instructions has becomeubiquitous. Unfortunately, current models lack the ability to customize thequantity of the change per pixel or per image fragment, resorting to changingthe entire image in an equal amount, or editing a specific region using abinary mask. In this paper, we suggest a new framework which enables the userto customize the quantity of change for each image fragment, thereby enhancingthe flexibility and verbosity of modern diffusion models. Our framework doesnot require model training or fine-tuning, but instead performs everything atinference time, making it easily applicable to an existing model. We show bothqualitatively and quantitatively that our method allows better controllabilityand can produce results which are unattainable by existing models. Our code isavailable at: https://github.com/exx8/differential-diffusion</description><author>Eran Levin, Ohad Fried</author><pubDate>Thu, 01 Jun 2023 18:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00950v1</guid></item><item><title>EEL: Efficiently Encoding Lattices for Reranking</title><link>http://arxiv.org/abs/2306.00947v1</link><description>Standard decoding approaches for conditional text generation tasks typicallysearch for an output hypothesis with high model probability, but this may notyield the best hypothesis according to human judgments of quality. Reranking tooptimize for "downstream" metrics can better optimize for quality, but manymetrics of interest are computed with pre-trained language models, which areslow to apply to large numbers of hypotheses. We explore an approach forreranking hypotheses by using Transformers to efficiently encode lattices ofgenerated outputs, a method we call EEL. With a single Transformer pass overthe entire lattice, we can approximately compute a contextualizedrepresentation of each token as if it were only part of a single hypothesis inisolation. We combine this approach with a new class of token-factoredrerankers (TFRs) that allow for efficient extraction of high reranker-scoringhypotheses from the lattice. Empirically, our approach incurs minimaldegradation error compared to the exponentially slower approach of encodingeach hypothesis individually. When applying EEL with TFRs across three textgeneration tasks, our results show both substantial speedup compared to naivereranking and often better performance on downstream metrics than comparableapproaches.</description><author>Prasann Singhal, Jiacheng Xu, Xi Ye, Greg Durrett</author><pubDate>Thu, 01 Jun 2023 18:45:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00947v1</guid></item><item><title>Exposing Attention Glitches with Flip-Flop Language Modeling</title><link>http://arxiv.org/abs/2306.00946v1</link><description>Why do large language models sometimes output factual inaccuracies andexhibit erroneous reasoning? The brittleness of these models, particularly whenexecuting long chains of reasoning, currently seems to be an inevitable priceto pay for their advanced capabilities of coherently synthesizing knowledge,pragmatics, and abstract thought. Towards making sense of this fundamentallyunsolved problem, this work identifies and analyzes the phenomenon of attentionglitches, in which the Transformer architecture's inductive biasesintermittently fail to capture robust reasoning. To isolate the issue, weintroduce flip-flop language modeling (FFLM), a parametric family of syntheticbenchmarks designed to probe the extrapolative behavior of neural languagemodels. This simple generative task requires a model to copy binary symbolsover long-range dependencies, ignoring the tokens in between. We find thatTransformer FFLMs suffer from a long tail of sporadic reasoning errors, some ofwhich we can eliminate using various regularization techniques. Our preliminarymechanistic analyses show why the remaining errors may be very difficult todiagnose and resolve. We hypothesize that attention glitches account for (someof) the closed-domain hallucinations in natural LLMs.</description><author>Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang</author><pubDate>Thu, 01 Jun 2023 18:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00946v1</guid></item><item><title>CS4ML: A general framework for active learning with arbitrary data based on Christoffel functions</title><link>http://arxiv.org/abs/2306.00945v1</link><description>We introduce a general framework for active learning in regression problems.Our framework extends the standard setup by allowing for general types of data,rather than merely pointwise samples of the target function. Thisgeneralization covers many cases of practical interest, such as data acquiredin transform domains (e.g., Fourier data), vector-valued data (e.g.,gradient-augmented data), data acquired along continuous curves, and,multimodal data (i.e., combinations of different types of measurements). Ourframework considers random sampling according to a finite number of samplingmeasures and arbitrary nonlinear approximation spaces (model classes). Weintroduce the concept of generalized Christoffel functions and show how thesecan be used to optimize the sampling measures. We prove that this leads tonear-optimal sample complexity in various important cases. This paper focuseson applications in scientific computing, where active learning is oftendesirable, since it is usually expensive to generate data. We demonstrate theefficacy of our framework for gradient-augmented learning with polynomials,Magnetic Resonance Imaging (MRI) using generative models and adaptive samplingfor solving PDEs using Physics-Informed Neural Networks (PINNs).</description><author>Ben Adcock, Juan M. Cardenas, Nick Dexter</author><pubDate>Thu, 01 Jun 2023 18:44:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00945v1</guid></item><item><title>Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance</title><link>http://arxiv.org/abs/2306.00943v1</link><description>Creating a vivid video from the event or scenario in our imagination is atruly fascinating experience. Recent advancements in text-to-video synthesishave unveiled the potential to achieve this with prompts only. While text isconvenient in conveying the overall scene context, it may be insufficient tocontrol precisely. In this paper, we explore customized video generation byutilizing text as context description and motion structure (e.g. frame-wisedepth) as concrete guidance. Our method, dubbed Make-Your-Video, involvesjoint-conditional video generation using a Latent Diffusion Model that ispre-trained for still image synthesis and then promoted for video generationwith the introduction of temporal modules. This two-stage learning scheme notonly reduces the computing resources required, but also improves theperformance by transferring the rich concepts available in image datasetssolely into video generation. Moreover, we use a simple yet effective causalattention mask strategy to enable longer video synthesis, which mitigates thepotential quality degradation effectively. Experimental results show thesuperiority of our method over existing baselines, particularly in terms oftemporal coherence and fidelity to users' guidance. In addition, our modelenables several intriguing applications that demonstrate potential forpractical usage.</description><author>Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, Ying Shan, Tien-Tsin Wong</author><pubDate>Thu, 01 Jun 2023 18:43:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00943v1</guid></item><item><title>LFTK: Handcrafted Features in Computational Linguistics</title><link>http://arxiv.org/abs/2305.15878v2</link><description>Past research has identified a rich set of handcrafted linguistic featuresthat can potentially assist various tasks. However, their extensive numbermakes it difficult to effectively select and utilize existing handcraftedfeatures. Coupled with the problem of inconsistent implementation acrossresearch works, there has been no categorization scheme or generally-acceptedfeature names. This creates unwanted confusion. Also, most existing handcraftedfeature extraction libraries are not open-source or not actively maintained. Asa result, a researcher often has to build such an extraction system from theground up. We collect and categorize more than 220 popular handcrafted features groundedon past literature. Then, we conduct a correlation analysis study on severaltask-specific datasets and report the potential use cases of each feature.Lastly, we devise a multilingual handcrafted linguistic feature extractionsystem in a systematically expandable manner. We open-source our system forpublic access to a rich set of pre-implemented handcrafted features. Our systemis coined LFTK and is the largest of its kind. Find it atgithub.com/brucewlee/lftk.</description><author>Bruce W. Lee, Jason Hyung-Jong Lee</author><pubDate>Thu, 01 Jun 2023 18:42:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15878v2</guid></item><item><title>Train Offline, Test Online: A Real Robot Learning Benchmark</title><link>http://arxiv.org/abs/2306.00942v1</link><description>Three challenges limit the progress of robot learning research: robots areexpensive (few labs can participate), everyone uses different robots (findingsdo not generalize across labs), and we lack internet-scale robotics data. Wetake on these challenges via a new benchmark: Train Offline, Test Online(TOTO). TOTO provides remote users with access to shared robotic hardware forevaluating methods on common tasks and an open-source dataset of these tasksfor offline training. Its manipulation task suite requires challenginggeneralization to unseen objects, positions, and lighting. We present initialresults on TOTO comparing five pretrained visual representations and fouroffline policy learning baselines, remotely contributed by five institutions.The real promise of TOTO, however, lies in the future: we release the benchmarkfor additional submissions from any user, enabling easy, direct comparison toseveral methods without the need to obtain hardware or collect data.</description><author>Gaoyue Zhou, Victoria Dean, Mohan Kumar Srirama, Aravind Rajeswaran, Jyothish Pari, Kyle Hatch, Aryan Jain, Tianhe Yu, Pieter Abbeel, Lerrel Pinto, Chelsea Finn, Abhinav Gupta</author><pubDate>Thu, 01 Jun 2023 18:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00942v1</guid></item><item><title>chemSKI with tokens: world building and economy in the SKI universe</title><link>http://arxiv.org/abs/2306.00938v1</link><description>chemSKI with tokens is a confluent graph rewrite system where all rewritesare local, which moreover can be used to do SKI calculus reductions. The graphrewrites of chemSKI are made conservative by the use of tokens. We thus achieveseveral goals: conservative rewrites in a chemical style, a solution to theproblem of new edge names in a distributed, decentralized graphical reductionand a new estimation of the cost of a combinatory calculus computation. Thisformalism can be used either as an artificial chemistry or as a model of avirtual decentralized machine which performs only local reductions. A programsrepository and the same article with simulations are available at github athttps://mbuliga.github.io/chemski/chemski-with-tokens.html</description><author>Marius Buliga</author><pubDate>Thu, 01 Jun 2023 18:40:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00938v1</guid></item><item><title>STEVE-1: A Generative Model for Text-to-Behavior in Minecraft</title><link>http://arxiv.org/abs/2306.00937v1</link><description>Constructing AI models that respond to text instructions is challenging,especially for sequential decision-making tasks. This work introduces aninstruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1,demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effectivefor creating instruction-following sequential decision-making agents. STEVE-1is trained in two steps: adapting the pretrained VPT model to follow commandsin MineCLIP's latent space, then training a prior to predict latent codes fromtext. This allows us to finetune VPT through self-supervised behavioral cloningand hindsight relabeling, bypassing the need for costly human text annotations.By leveraging pretrained models like VPT and MineCLIP and employing bestpractices from text-conditioned image generation, STEVE-1 costs just $60 totrain and can follow a wide range of short-horizon open-ended text and visualinstructions in Minecraft. STEVE-1 sets a new bar for open-ended instructionfollowing in Minecraft with low-level controls (mouse and keyboard) and rawpixel inputs, far outperforming previous baselines. We provide experimentalevidence highlighting key factors for downstream performance, includingpretraining, classifier-free guidance, and data scaling. All resources,including our model weights, training scripts, and evaluation tools are madeavailable for further research.</description><author>Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, Sheila McIlraith</author><pubDate>Thu, 01 Jun 2023 18:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00937v1</guid></item><item><title>AMR4NLI: Interpretable and robust NLI measures from semantic graphs</title><link>http://arxiv.org/abs/2306.00936v1</link><description>The task of natural language inference (NLI) asks whether a given premise(expressed in NL) entails a given NL hypothesis. NLI benchmarks contain humanratings of entailment, but the meaning relationships driving these ratings arenot formalized. Can the underlying sentence pair relationships be made moreexplicit in an interpretable yet robust fashion? We compare semantic structuresto represent premise and hypothesis, including sets of contextualizedembeddings and semantic graphs (Abstract Meaning Representations), and measurewhether the hypothesis is a semantic substructure of the premise, utilizinginterpretable metrics. Our evaluation on three English benchmarks finds valuein both contextualized embeddings and semantic graphs; moreover, they providecomplementary signals, and can be leveraged together in a hybrid model.</description><author>Juri Opitz, Shira Wein, Julius Steen, Anette Frank, Nathan Schneider</author><pubDate>Thu, 01 Jun 2023 18:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00936v1</guid></item><item><title>Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features</title><link>http://arxiv.org/abs/2306.00934v1</link><description>The black-box nature of complex Neural Network (NN)-based models has hinderedtheir widespread adoption in security domains due to the lack of logicalexplanations and actionable follow-ups for their predictions. To enhance thetransparency and accountability of Graph Neural Network (GNN) security modelsused in system provenance analysis, we propose PROVEXPLAINER, a framework forprojecting abstract GNN decision boundaries onto interpretable feature spaces. We first replicate the decision-making process of GNNbased security modelsusing simpler and explainable models such as Decision Trees (DTs). To maximizethe accuracy and fidelity of the surrogate models, we propose novel graphstructural features founded on classical graph theory and enhanced by extensivedata study with security domain knowledge. Our graph structural features areclosely tied to problem-space actions in the system provenance domain, whichallows the detection results to be explained in descriptive, human language.PROVEXPLAINER allowed simple DT models to achieve 95% fidelity to the GNN onprogram classification tasks with general graph structural features, and 99%fidelity on malware detection tasks with a task-specific feature packagetailored for direct interpretation. The explanations for malware classificationare demonstrated with case studies of five real-world malware samples acrossthree malware families.</description><author>Kunal Mukherjee, Joshua Wiedemeier, Tianhao Wang, Muhyun Kim, Feng Chen, Murat Kantarcioglu, Kangkook Jee</author><pubDate>Thu, 01 Jun 2023 18:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00934v1</guid></item><item><title>Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective</title><link>http://arxiv.org/abs/2305.15408v2</link><description>Recent studies have discovered that Chain-of-Thought prompting (CoT) candramatically improve the performance of Large Language Models (LLMs),particularly when dealing with complex tasks involving mathematics orreasoning. Despite the enormous empirical success, the underlying mechanismsbehind CoT and how it unlocks the potential of LLMs remain elusive. In thispaper, we take a first step towards theoretically answering these questions.Specifically, we examine the expressivity of LLMs with CoT in solvingfundamental mathematical and decision-making problems. We start by giving animpossibility result showing that bounded-depth Transformers are unable todirectly produce correct answers for basic arithmetic/equation tasks unless themodel size grows super-polynomially with respect to the input length. Incontrast, we then prove by construction that autoregressive Transformers ofconstant size suffice to solve both tasks by generating CoT derivations using acommonly-used math language format. Moreover, we show LLMs with CoT are capableof solving a general class of decision-making problems known as DynamicProgramming, thus justifying its power in tackling complex real-world tasks.Finally, extensive experiments on four tasks show that, while Transformersalways fail to predict the answers directly, they can consistently learn togenerate correct solutions step-by-step given sufficient CoT demonstrations.</description><author>Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang</author><pubDate>Thu, 01 Jun 2023 18:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15408v2</guid></item><item><title>Cross Modal Data Discovery over Structured and Unstructured Data Lakes</title><link>http://arxiv.org/abs/2306.00932v1</link><description>Organizations are collecting increasingly large amounts of data for datadriven decision making. These data are often dumped into a centralizedrepository, e.g., a data lake, consisting of thousands of structured andunstructured datasets. Perversely, such mixture of datasets makes the problemof discovering elements (e.g., tables or documents) that are relevant to auser's query or an analytical task very challenging. Despite the recent effortsin data discovery, the problem remains widely open especially in the two frontsof (1) discovering relationships and relatedness across structured andunstructured datasets where existing techniques suffer from either scalability,being customized for a specific problem type (e.g., entity matching or dataintegration), or demolishing the structural properties on its way, and (2)developing a holistic system for integrating various similarity measurementsand sketches in an effective way to boost the discovery accuracy. In thispaper, we propose a new data discovery system, named CMDL, for addressing thesetwo limitations. CMDL supports the data discovery process over both structuredand unstructured data while retaining the structural properties of tables.</description><author>Mohamed Y. Eltabakh, Mayuresh Kunjir, Ahmed Elmagarmid, Mohammad Shahmeer Ahmad</author><pubDate>Thu, 01 Jun 2023 18:34:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00932v1</guid></item><item><title>"Let's not Quote out of Context": Unified Vision-Language Pretraining for Context Assisted Image Captioning</title><link>http://arxiv.org/abs/2306.00931v1</link><description>Well-formed context aware image captions and tags in enterprise content suchas marketing material are critical to ensure their brand presence and contentrecall. Manual creation and updates to ensure the same is non trivial given thescale and the tedium towards this task. We propose a new unifiedVision-Language (VL) model based on the One For All (OFA) model, with a focuson context-assisted image captioning where the caption is generated based onboth the image and its context. Our approach aims to overcome thecontext-independent (image and text are treated independently) nature of theexisting approaches. We exploit context by pretraining our model with datasetsof three tasks: news image captioning where the news article is the context,contextual visual entailment, and keyword extraction from the context. Thesecond pretraining task is a new VL task, and we construct and release twodatasets for the task with 1.1M and 2.2K data instances. Our system achievesstate-of-the-art results with an improvement of up to 8.34 CIDEr score on thebenchmark news image captioning datasets. To the best of our knowledge, ours isthe first effort at incorporating contextual information in pretraining themodels for the VL tasks.</description><author>Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, Niyati Chhaya, Sumit Shekhar</author><pubDate>Thu, 01 Jun 2023 18:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00931v1</guid></item><item><title>ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER</title><link>http://arxiv.org/abs/2306.00928v1</link><description>Complex Named Entity Recognition (NER) is the task of detectinglinguistically complex named entities in low-context text. In this paper, wepresent ACLM Attention-map aware keyword selection for Conditional LanguageModel fine-tuning), a novel data augmentation approach based on conditionalgeneration to address the data scarcity problem in low-resource complex NER.ACLM alleviates the context-entity mismatch issue, a problem existing NER dataaugmentation techniques suffer from and often generates incoherentaugmentations by placing complex named entities in the wrong context. ACLMbuilds on BART and is optimized on a novel text reconstruction or denoisingtask - we use selective masking (aided by attention maps) to retain the namedentities and certain keywords in the input sentence that provide contextuallyrelevant additional knowledge or hints about the named entities. Compared withother data augmentation strategies, ACLM can generate more diverse and coherentaugmentations preserving the true word sense of complex entities in thesentence. We demonstrate the effectiveness of ACLM both qualitatively andquantitatively on monolingual, cross-lingual, and multilingual complex NERacross various low-resource settings. ACLM outperforms all our neural baselinesby a significant margin (1%-36%). In addition, we demonstrate the applicationof ACLM to other domains that suffer from data scarcity (e.g., biomedical). Inpractice, ACLM generates more effective and factual augmentations for thesedomains than prior methods. Code: https://github.com/Sreyan88/ACLM</description><author>Sreyan Ghosh, Utkarsh Tyagi, Manan Suri, Sonal Kumar, S Ramaneswaran, Dinesh Manocha</author><pubDate>Thu, 01 Jun 2023 18:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00928v1</guid></item><item><title>D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts</title><link>http://arxiv.org/abs/2301.11497v2</link><description>We present D$^2$CSG, a neural model composed of two dual and complementarynetwork branches, with dropouts, for unsupervised learning of compactconstructive solid geometry (CSG) representations of 3D CAD shapes. Our networkis trained to reconstruct a 3D shape by a fixed-order assembly of quadricprimitives, with both branches producing a union of primitive intersections orinverses. A key difference between D$^2$CSG and all prior neural CSG models isits dedicated residual branch to assemble the potentially complex shapecomplement, which is subtracted from an overall shape modeled by the coverbranch. With the shape complements, our network is provably general, while theweight dropout further improves compactness of the CSG tree by removingredundant primitives. We demonstrate both quantitatively and qualitatively thatD$^2$CSG produces compact CSG reconstructions with superior quality and morenatural primitives than all existing alternatives, especially over complex andhigh-genus CAD shapes.</description><author>Fenggen Yu, Qimin Chen, Maham Tanveer, Ali Mahdavi Amiri, Hao Zhang</author><pubDate>Thu, 01 Jun 2023 18:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11497v2</guid></item><item><title>Offline Meta Reinforcement Learning with In-Distribution Online Adaptation</title><link>http://arxiv.org/abs/2305.19529v2</link><description>Recent offline meta-reinforcement learning (meta-RL) methods typicallyutilize task-dependent behavior policies (e.g., training RL agents on eachindividual task) to collect a multi-task dataset. However, these methods alwaysrequire extra information for fast adaptation, such as offline context fortesting tasks. To address this problem, we first formally characterize a uniquechallenge in offline meta-RL: transition-reward distribution shift betweenoffline datasets and online adaptation. Our theory finds thatout-of-distribution adaptation episodes may lead to unreliable policyevaluation and that online adaptation with in-distribution episodes can ensureadaptation performance guarantee. Based on these theoretical insights, wepropose a novel adaptation framework, called In-Distribution online Adaptationwith uncertainty Quantification (IDAQ), which generates in-distribution contextusing a given uncertainty quantification and performs effective task beliefinference to address new tasks. We find a return-based uncertaintyquantification for IDAQ that performs effectively. Experiments show that IDAQachieves state-of-the-art performance on the Meta-World ML1 benchmark comparedto baselines with/without offline adaptation.</description><author>Jianhao Wang, Jin Zhang, Haozhe Jiang, Junyu Zhang, Liwei Wang, Chongjie Zhang</author><pubDate>Thu, 01 Jun 2023 18:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19529v2</guid></item><item><title>Second Sight: Using brain-optimized encoding models to align image distributions with human brain activity</title><link>http://arxiv.org/abs/2306.00927v1</link><description>Two recent developments have accelerated progress in image reconstructionfrom human brain activity: large datasets that offer samples of brain activityin response to many thousands of natural scenes, and the open-sourcing ofpowerful stochastic image-generators that accept both low- and high-levelguidance. Most work in this space has focused on obtaining point estimates ofthe target image, with the ultimate goal of approximating literal pixel-wisereconstructions of target images from the brain activity patterns they evoke.This emphasis belies the fact that there is always a family of images that areequally compatible with any evoked brain activity pattern, and the fact thatmany image-generators are inherently stochastic and do not by themselves offera method for selecting the single best reconstruction from among the samplesthey generate. We introduce a novel reconstruction procedure (Second Sight)that iteratively refines an image distribution to explicitly maximize thealignment between the predictions of a voxel-wise encoding model and the brainactivity patterns evoked by any target image. We show that our processconverges on a distribution of high-quality reconstructions by refining bothsemantic content and low-level image details across iterations. Images sampledfrom these converged image distributions are competitive with state-of-the-artreconstruction algorithms. Interestingly, the time-to-convergence variessystematically across visual cortex, with earlier visual areas generally takinglonger and converging on narrower image distributions, relative to higher-levelbrain areas. Second Sight thus offers a succinct and novel method for exploringthe diversity of representations across visual brain areas.</description><author>Reese Kneeland, Jordyn Ojeda, Ghislain St-Yves, Thomas Naselaris</author><pubDate>Thu, 01 Jun 2023 18:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00927v1</guid></item><item><title>Inserting Anybody in Diffusion Models via Celeb Basis</title><link>http://arxiv.org/abs/2306.00926v1</link><description>Exquisite demand exists for customizing the pretrained large text-to-imagemodel, $\textit{e.g.}$, Stable Diffusion, to generate innovative concepts, suchas the users themselves. However, the newly-added concept from previouscustomization methods often shows weaker combination abilities than theoriginal ones even given several images during training. We thus propose a newpersonalization method that allows for the seamless integration of a uniqueindividual into the pre-trained diffusion model using just $\textbf{one facialphotograph}$ and only $\textbf{1024 learnable parameters}$ under $\textbf{3minutes}$. So as we can effortlessly generate stunning images of this person inany pose or position, interacting with anyone and doing anything imaginablefrom text prompts. To achieve this, we first analyze and build a well-definedceleb basis from the embedding space of the pre-trained large text encoder.Then, given one facial photo as the target identity, we generate its ownembedding by optimizing the weight of this basis and locking all otherparameters. Empowered by the proposed celeb basis, the new identity in ourcustomized model showcases a better concept combination ability than previouspersonalization methods. Besides, our model can also learn several newidentities at once and interact with each other where the previouscustomization model fails to. The code will be released.</description><author>Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan, Huicheng Zheng</author><pubDate>Thu, 01 Jun 2023 18:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00926v1</guid></item><item><title>Federated Conformal Predictors for Distributed Uncertainty Quantification</title><link>http://arxiv.org/abs/2305.17564v2</link><description>Conformal prediction is emerging as a popular paradigm for providing rigorousuncertainty quantification in machine learning since it can be easily appliedas a post-processing step to already trained models. In this paper, we extendconformal prediction to the federated learning setting. The main challenge weface is data heterogeneity across the clients - this violates the fundamentaltenet of exchangeability required for conformal prediction. We propose a weakernotion of partial exchangeability, better suited to the FL setting, and use itto develop the Federated Conformal Prediction (FCP) framework. We show FCPenjoys rigorous theoretical guarantees and excellent empirical performance onseveral computer vision and medical imaging datasets. Our results demonstrate apractical approach to incorporating meaningful uncertainty quantification indistributed and heterogeneous environments. We provide code used in ourexperiments https://github.com/clu5/federated-conformal.</description><author>Charles Lu, Yaodong Yu, Sai Praneeth Karimireddy, Michael I. Jordan, Ramesh Raskar</author><pubDate>Thu, 01 Jun 2023 18:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17564v2</guid></item><item><title>DISCO: Distilling Phrasal Counterfactuals with Large Language Models</title><link>http://arxiv.org/abs/2212.10534v2</link><description>Models trained with counterfactually augmented data learn representations ofthe causal structure of tasks, enabling robust generalization. However,high-quality counterfactual data is scarce for most tasks and not easilygenerated at scale. When crowdsourced, such data is typically limited in scaleand diversity; when generated using supervised methods, it is computationallyexpensive to extend to new counterfactual dimensions. In this work, weintroduce DISCO (DIStilled COunterfactual Data), a new method for automaticallygenerating high quality counterfactual data at scale. DISCO engineers promptsto generate phrasal perturbations with a large general language model. Then, atask-specific teacher model filters these generations to distill high-qualitycounterfactual data. While task-agnostic, we apply our pipeline to the task ofnatural language inference (NLI) and find that on challenging evaluations suchas the NLI stress test, comparatively smaller student models trained with DISCOgenerated counterfactuals are more robust (6% absolute) and generalize betteracross distributions (2%) compared to models trained without data augmentation.Furthermore, DISCO augmented models are 10% more consistent betweencounterfactual pairs on three evaluation sets, demonstrating that DISCOaugmentation enables models to more reliably learn causal representations. Ourrepository is available at: https://github.com/eric11eca/disco</description><author>Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle Richardson</author><pubDate>Thu, 01 Jun 2023 18:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10534v2</guid></item><item><title>Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker</title><link>http://arxiv.org/abs/2306.00924v1</link><description>Theory of Mind (ToM)$\unicode{x2014}$the ability to reason about the mentalstates of other people$\unicode{x2014}$is a key element of our socialintelligence. Yet, despite their ever more impressive performance, large-scaleneural language models still lack basic theory of mind capabilitiesout-of-the-box. We posit that simply scaling up models will not imbue them withtheory of mind due to the inherently symbolic and implicit nature of thephenomenon, and instead investigate an alternative: can we design adecoding-time algorithm that enhances theory of mind of off-the-shelf neurallanguage models without explicit supervision? We present SymbolicToM, aplug-and-play approach to reason about the belief states of multiple charactersin reading comprehension tasks via explicit symbolic representation. Moreconcretely, our approach tracks each entity's beliefs, their estimation ofother entities' beliefs, and higher-order levels of reasoning, all throughgraphical representations, allowing for more precise and interpretablereasoning than previous approaches. Empirical results on the well-known ToMibenchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhancesoff-the-shelf neural networks' theory of mind in a zero-shot setting whileshowing robust out-of-distribution performance compared to supervisedbaselines. Our work also reveals spurious patterns in existing theory of mindbenchmarks, emphasizing the importance of out-of-distribution evaluation andmethods that do not overfit a particular dataset.</description><author>Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia Tsvetkov</author><pubDate>Thu, 01 Jun 2023 18:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00924v1</guid></item><item><title>Sonicverse: A Multisensory Simulation Platform for Embodied Household Agents that See and Hear</title><link>http://arxiv.org/abs/2306.00923v1</link><description>Developing embodied agents in simulation has been a key research topic inrecent years. Exciting new tasks, algorithms, and benchmarks have beendeveloped in various simulators. However, most of them assume deaf agents insilent environments, while we humans perceive the world with multiple senses.We introduce Sonicverse, a multisensory simulation platform with integratedaudio-visual simulation for training household agents that can both see andhear. Sonicverse models realistic continuous audio rendering in 3D environmentsin real-time. Together with a new audio-visual VR interface that allows humansto interact with agents with audio, Sonicverse enables a series of embodied AItasks that need audio-visual perception. For semantic audio-visual navigationin particular, we also propose a new multi-task learning model that achievesstate-of-the-art performance. In addition, we demonstrate Sonicverse's realismvia sim-to-real transfer, which has not been achieved by other simulators: anagent trained in Sonicverse can successfully perform audio-visual navigation inreal-world environments. Sonicverse is available at:https://github.com/StanfordVL/Sonicverse.</description><author>Ruohan Gao, Hao Li, Gokul Dharan, Zhuzhu Wang, Chengshu Li, Fei Xia, Silvio Savarese, Li Fei-Fei, Jiajun Wu</author><pubDate>Thu, 01 Jun 2023 18:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00923v1</guid></item><item><title>Better Private Linear Regression Through Better Private Feature Selection</title><link>http://arxiv.org/abs/2306.00920v1</link><description>Existing work on differentially private linear regression typically assumesthat end users can precisely set data bounds or algorithmic hyperparameters.End users often struggle to meet these requirements without directly examiningthe data (and violating privacy). Recent work has attempted to developsolutions that shift these burdens from users to algorithms, but they struggleto provide utility as the feature dimension grows. This work extends thesealgorithms to higher-dimensional problems by introducing a differentiallyprivate feature selection method based on Kendall rank correlation. We prove autility guarantee for the setting where features are normally distributed andconduct experiments across 25 datasets. We find that adding this privatefeature selection step before regression significantly broadens theapplicability of ``plug-and-play'' private linear regression algorithms atlittle additional cost to privacy, computation, or decision-making by the enduser.</description><author>Travis Dick, Jennifer Gillenwater, Matthew Joseph</author><pubDate>Thu, 01 Jun 2023 18:21:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00920v1</guid></item><item><title>Vocabulary-free Image Classification</title><link>http://arxiv.org/abs/2306.00917v1</link><description>Recent advances in large vision-language models have revolutionized the imageclassification paradigm. Despite showing impressive zero-shot capabilities, apre-defined set of categories, a.k.a. the vocabulary, is assumed at test timefor composing the textual prompts. However, such assumption can be impracticalwhen the semantic context is unknown and evolving. We thus formalize a noveltask, termed as Vocabulary-free Image Classification (VIC), where we aim toassign to an input image a class that resides in an unconstrainedlanguage-induced semantic space, without the prerequisite of a knownvocabulary. VIC is a challenging task as the semantic space is extremely large,containing millions of concepts, with hard-to-discriminate fine-grainedcategories. In this work, we first empirically verify that representing thissemantic space by means of an external vision-language database is the mosteffective way to obtain semantically relevant content for classifying theimage. We then propose Category Search from External Databases (CaSED), amethod that exploits a pre-trained vision-language model and an externalvision-language database to address VIC in a training-free manner. CaSED firstextracts a set of candidate categories from captions retrieved from thedatabase based on their semantic similarity to the image, and then assigns tothe image the best matching candidate category according to the samevision-language model. Experiments on benchmark datasets validate that CaSEDoutperforms other complex vision-language frameworks, while being efficientwith much fewer parameters, paving the way for future research in thisdirection.</description><author>Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, Elisa Ricci</author><pubDate>Thu, 01 Jun 2023 18:19:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00917v1</guid></item><item><title>The feasibility of artificial consciousness through the lens of neuroscience</title><link>http://arxiv.org/abs/2306.00915v1</link><description>Interactions with large language models have led to the suggestion that thesemodels may be conscious. From the perspective of neuroscience, this position isdifficult to defend. For one, the architecture of large language models ismissing key features of the thalamocortical system that have been linked toconscious awareness in mammals. Secondly, the inputs to large language modelslack the embodied, embedded information content characteristic of our sensorycontact with the world around us. Finally, while the previous two arguments canbe overcome in future AI systems, the third one might be harder to bridge inthe near future. Namely, we argue that consciousness might depend on having'skin in the game', in that the existence of the system depends on its actions,which is not true for present-day artificial intelligence.</description><author>Jaan Aru, Matthew Larkum, James M. Shine</author><pubDate>Thu, 01 Jun 2023 18:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00915v1</guid></item><item><title>Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions</title><link>http://arxiv.org/abs/2303.12789v2</link><description>We propose a method for editing NeRF scenes with text-instructions. Given aNeRF of a scene and the collection of images used to reconstruct it, our methoduses an image-conditioned diffusion model (InstructPix2Pix) to iteratively editthe input images while optimizing the underlying scene, resulting in anoptimized 3D scene that respects the edit instruction. We demonstrate that ourproposed method is able to edit large-scale, real-world scenes, and is able toaccomplish more realistic, targeted edits than prior work.</description><author>Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa</author><pubDate>Thu, 01 Jun 2023 18:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12789v2</guid></item><item><title>Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning</title><link>http://arxiv.org/abs/2305.18888v2</link><description>Recent studies have shown great promise in unsupervised representationlearning (URL) for multivariate time series, because URL has the capability inlearning generalizable representation for many downstream tasks without usinginaccessible labels. However, existing approaches usually adopt the modelsoriginally designed for other domains (e.g., computer vision) to encode thetime series data and rely on strong assumptions to design learning objectives,which limits their ability to perform well. To deal with these problems, wepropose a novel URL framework for multivariate time series by learningtime-series-specific shapelet-based representation through a popularcontrasting learning paradigm. To the best of our knowledge, this is the firstwork that explores the shapelet-based embedding in the unsupervisedgeneral-purpose representation learning. A unified shapelet-based encoder and anovel learning objective with multi-grained contrasting and multi-scalealignment are particularly designed to achieve our goal, and a dataaugmentation library is employed to improve the generalization. We conductextensive experiments using tens of real-world datasets to assess therepresentation quality on many downstream tasks, including classification,clustering, and anomaly detection. The results demonstrate the superiority ofour method against not only URL competitors, but also techniques speciallydesigned for downstream tasks. Our code has been made publicly available athttps://github.com/real2fish/CSL.</description><author>Zhiyu Liang, Jianfeng Zhang, Chen Liang, Hongzhi Wang, Zheng Liang, Lujia Pan</author><pubDate>Thu, 01 Jun 2023 18:16:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18888v2</guid></item><item><title>Conditioning Diffusion Models via Attributes and Semantic Masks for Face Generation</title><link>http://arxiv.org/abs/2306.00914v1</link><description>Deep generative models have shown impressive results in generating realisticimages of faces. GANs managed to generate high-quality, high-fidelity imageswhen conditioned on semantic masks, but they still lack the ability todiversify their output. Diffusion models partially solve this problem and areable to generate diverse samples given the same condition. In this paper, wepropose a multi-conditioning approach for diffusion models via cross-attentionexploiting both attributes and semantic masks to generate high-quality andcontrollable face images. We also studied the impact of applyingperceptual-focused loss weighting into the latent space instead of the pixelspace. Our method extends the previous approaches by introducing conditioningon more than one set of features, guaranteeing a more fine-grained control overthe generated face images. We evaluate our approach on the CelebA-HQ dataset,and we show that it can generate realistic and diverse samples while allowingfor fine-grained control over multiple attributes and semantic regions.Additionally, we perform an ablation study to evaluate the impact of differentconditioning strategies on the quality and diversity of the generated images.</description><author>Nico Giambi, Giuseppe Lisanti</author><pubDate>Thu, 01 Jun 2023 18:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00914v1</guid></item><item><title>MOSAIC: Masked Optimisation with Selective Attention for Image Reconstruction</title><link>http://arxiv.org/abs/2306.00906v1</link><description>Compressive sensing (CS) reconstructs images from sub-Nyquist measurements bysolving a sparsity-regularized inverse problem. Traditional CS solvers useiterative optimizers with hand crafted sparsifiers, while early data-drivenmethods directly learn an inverse mapping from the low-dimensional measurementspace to the original image space. The latter outperforms the former, but isrestrictive to a pre-defined measurement domain. More recent, deep unrollingmethods combine traditional proximal gradient methods and data-drivenapproaches to iteratively refine an image approximation. To achieve higheraccuracy, it has also been suggested to learn both the sampling matrix, and thechoice of measurement vectors adaptively. Contrary to the current trend, inthis work we hypothesize that a general inverse mapping from a random set ofcompressed measurements to the image domain exists for a given measurementbasis, and can be learned. Such a model is single-shot, non-restrictive anddoes not parametrize the sampling process. To this end, we propose MOSAIC, anovel compressive sensing framework to reconstruct images given any randomselection of measurements, sampled using a fixed basis. Motivated by the unevendistribution of information across measurements, MOSAIC incorporates anembedding technique to efficiently apply attention mechanisms on an encodedsequence of measurements, while dispensing the need to use unrolled deepnetworks. A range of experiments validate our proposed architecture as apromising alternative for existing CS reconstruction methods, by achieving thestate-of-the-art for metrics of reconstruction accuracy on standard datasets.</description><author>Pamuditha Somarathne, Tharindu Wickremasinghe, Amashi Niwarthana, A. Thieshanthan, Chamira U. S. Edussooriya, Dushan N. Wadduwage</author><pubDate>Thu, 01 Jun 2023 18:05:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00906v1</guid></item><item><title>NN2Poly: A polynomial representation for deep feed-forward artificial neural networks</title><link>http://arxiv.org/abs/2112.11397v3</link><description>Interpretability of neural networks and their underlying theoretical behaviorremain an open field of study even after the great success of their practicalapplications, particularly with the emergence of deep learning. In this work,NN2Poly is proposed: a theoretical approach to obtain an explicit polynomialmodel that provides an accurate representation of an already trainedfully-connected feed-forward artificial neural network (a multilayer perceptronor MLP). This approach extends a previous idea proposed in the literature,which was limited to single hidden layer networks, to work with arbitrarilydeep MLPs in both regression and classification tasks. The objective of thispaper is to achieve this by using a Taylor expansion on the activationfunction, at each layer, and then using several combinatorial properties tocalculate the coefficients of the desired polynomials. Discussion is presentedon the main computational challenges of this method, and the way to overcomethem by imposing certain constraints during the training phase. Finally,simulation experiments as well as an application to a real data set arepresented to demonstrate the effectiveness of the proposed method.</description><author>Pablo Morala, Jenny Alexandra Cifuentes, Rosa E. Lillo, Iaki Ucar</author><pubDate>Thu, 01 Jun 2023 18:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.11397v3</guid></item><item><title>T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation</title><link>http://arxiv.org/abs/2306.00905v1</link><description>Warning: This paper contains several contents that may be toxic, harmful, oroffensive. In the last few years, text-to-image generative models have gained remarkablesuccess in generating images with unprecedented quality accompanied by abreakthrough of inference speed. Despite their rapid progress, human biasesthat manifest in the training examples, particularly with regard to commonstereotypical biases, like gender and skin tone, still have been found in thesegenerative models. In this work, we seek to measure more complex human biasesexist in the task of text-to-image generations. Inspired by the well-knownImplicit Association Test (IAT) from social psychology, we propose a novelText-to-Image Association Test (T2IAT) framework that quantifies the implicitstereotypes between concepts and valence, and those in the images. We replicatethe previously documented bias tests on generative models, including morallyneutral tests on flowers and insects as well as demographic stereotypical testson diverse social attributes. The results of these experiments demonstrate thepresence of complex stereotypical behaviors in image generations.</description><author>Jialu Wang, Xinyue Gabby Liu, Zonglin Di, Yang Liu, Xin Eric Wang</author><pubDate>Thu, 01 Jun 2023 18:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00905v1</guid></item><item><title>Heterogeneous Value Evaluation for Large Language Models</title><link>http://arxiv.org/abs/2305.17147v2</link><description>The emergent capabilities of Large Language Models (LLMs) have made itcrucial to align their values with those of humans. Current methodologiestypically attempt alignment with a homogeneous human value and requires humanverification, yet lack consensus on the desired aspect and depth of alignmentand resulting human biases. In this paper, we propose A2EHV, an AutomatedAlignment Evaluation with a Heterogeneous Value system that (1) is automated tominimize individual human biases, and (2) allows assessments against varioustarget values to foster heterogeneous agents. Our approach pivots on theconcept of value rationality, which represents the ability for agents toexecute behaviors that satisfy a target value the most. The quantification ofvalue rationality is facilitated by the Social Value Orientation framework fromsocial psychology, which partitions the value space into four categories toassess social preferences from agents' behaviors. We evaluate the valuerationality of eight mainstream LLMs and observe that large models are moreinclined to align neutral values compared to those with strong personal values.By examining the behavior of these LLMs, we contribute to a deeperunderstanding of value alignment within a heterogeneous value system.</description><author>Zhaowei Zhang, Nian Liu, Siyuan Qi, Ceyao Zhang, Ziqi Rong, Song-Chun Zhu, Shuguang Cui, Yaodong Yang</author><pubDate>Thu, 01 Jun 2023 18:00:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17147v2</guid></item><item><title>Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions</title><link>http://arxiv.org/abs/2306.00904v1</link><description>Models that rely solely on pairwise relationships often fail to capture thecomplete statistical structure of the complex multivariate data found indiverse domains, such as socio-economic, ecological, or biomedical systems.Non-trivial dependencies between groups of more than two variables can play asignificant role in the analysis and modelling of such systems, yet extractingsuch high-order interactions from data remains challenging. Here, we introducea hierarchy of $d$-order ($d \geq 2$) interaction measures, increasinglyinclusive of possible factorisations of the joint probability distribution, anddefine non-parametric, kernel-based tests to establish systematically thestatistical significance of $d$-order interactions. We also establishmathematical links with lattice theory, which elucidate the derivation of theinteraction measures and their composite permutation tests; clarify theconnection of simplicial complexes with kernel matrix centring; and provide ameans to enhance computational efficiency. We illustrate our resultsnumerically with validations on synthetic data, and through an application toneuroimaging data.</description><author>Zhaolu Liu, Robert L. Peach, Pedro A. M. Mediano, Mauricio Barahona</author><pubDate>Thu, 01 Jun 2023 17:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00904v1</guid></item><item><title>SpotTarget: Rethinking the Effect of Target Edges for Link Prediction in Graph Neural Networks</title><link>http://arxiv.org/abs/2306.00899v1</link><description>Graph Neural Networks (GNNs) have demonstrated promising outcomes acrossvarious tasks, including node classification and link prediction. Despite theirremarkable success in various high-impact applications, we have identifiedthree common pitfalls in message passing for link prediction. Particularly, inprevalent GNN frameworks (e.g., DGL and PyTorch-Geometric), the target edges(i.e., the edges being predicted) consistently exist as message passing edgesin the graph during training. Consequently, this results in overfitting anddistribution shift, both of which adversely impact the generalizability to testthe target edges. Additionally, during test time, the failure to exclude thetest target edges leads to implicit test leakage caused by neighborhoodaggregation. In this paper, we analyze these three pitfalls and investigate theimpact of including or excluding target edges on the performance of nodes withvarying degrees during training and test phases. Our theoretical and empiricalanalysis demonstrates that low-degree nodes are more susceptible to thesepitfalls. These pitfalls can have detrimental consequences when GNNs areimplemented in production systems. To systematically address these pitfalls, wepropose SpotTarget, an effective and efficient GNN training framework. Duringtraining, SpotTarget leverages our insight regarding low-degree nodes andexcludes train target edges connected to at least one low-degree node. Duringtest time, it emulates real-world scenarios of GNN usage in production andexcludes all test target edges. Our experiments conducted on diverse real-worlddatasets, demonstrate that SpotTarget significantly enhances GNNs, achieving upto a 15x increase in accuracy in sparse graphs. Furthermore, SpotTargetconsistently and dramatically improves the performance for low-degree nodes indense graphs.</description><author>Jing Zhu, Yuhang Zhou, Vassilis N. Ioannidis, Shengyi Qian, Wei Ai, Xiang Song, Danai Koutra</author><pubDate>Thu, 01 Jun 2023 17:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00899v1</guid></item><item><title>A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU</title><link>http://arxiv.org/abs/2305.17473v2</link><description>Deep learning (DL) has emerged as a powerful subset of machine learning (ML)and artificial intelligence (AI), outperforming traditional ML methods,especially in handling unstructured and large datasets. Its impact spans acrossvarious domains, including speech recognition, healthcare, autonomous vehicles,cybersecurity, predictive analytics, and more. However, the complexity anddynamic nature of real-world problems present challenges in designing effectivedeep learning models. Consequently, several deep learning models have beendeveloped to address different problems and applications. In this article, weconduct a comprehensive survey of various deep learning models, includingConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),Generative Models, Deep Reinforcement Learning (DRL), and Deep TransferLearning. We examine the structure, applications, benefits, and limitations ofeach model. Furthermore, we perform an analysis using three publicly availabledatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowneddeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),Bidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.</description><author>Farhad Mortezapour Shiri, Thinagaran Perumal, Norwati Mustapha, Raihani Mohamed</author><pubDate>Thu, 01 Jun 2023 17:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17473v2</guid></item><item><title>A Probabilistic Relaxation of the Two-Stage Object Pose Estimation Paradigm</title><link>http://arxiv.org/abs/2306.00892v1</link><description>Existing object pose estimation methods commonly require a one-to-one pointmatching step that forces them to be separated into two consecutive stages:visual correspondence detection (e.g., by matching feature descriptors as partof a perception front-end) followed by geometric alignment (e.g., by optimizinga robust estimation objective for pointcloud registration orperspective-n-point). Instead, we propose a matching-free probabilisticformulation with two main benefits: i) it enables unified and concurrentoptimization of both visual correspondence and geometric alignment, and ii) itcan represent different plausible modes of the entire distribution of likelyposes. This in turn allows for a more graceful treatment of geometricperception scenarios where establishing one-to-one matches between points isconceptually ill-defined, such as textureless, symmetrical and/or occludedobjects and scenes where the correct pose is uncertain or there are multipleequally valid solutions.</description><author>Onur Beker</author><pubDate>Thu, 01 Jun 2023 17:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00892v1</guid></item><item><title>LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day</title><link>http://arxiv.org/abs/2306.00890v1</link><description>Conversational generative AI has demonstrated remarkable promise forempowering biomedical practitioners, but current investigations focus onunimodal text. Multimodal conversational AI has seen rapid progress byleveraging billions of image-text pairs from the public web, but suchgeneral-domain vision-language models still lack sophistication inunderstanding and conversing about biomedical images. In this paper, we proposea cost-efficient approach for training a vision-language conversationalassistant that can answer open-ended research questions of biomedical images.The key idea is to leverage a large-scale, broad-coverage biomedicalfigure-caption dataset extracted from PubMed Central, use GPT-4 toself-instruct open-ended instruction-following data from the captions, and thenfine-tune a large general-domain vision-language model using a novel curriculumlearning method. Specifically, the model first learns to align biomedicalvocabulary using the figure-caption pairs as is, then learns to masteropen-ended conversational semantics using GPT-4 generated instruction-followingdata, broadly mimicking how a layperson gradually acquires biomedicalknowledge. This enables us to train a Large Language and Vision Assistant forBioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Medexhibits excellent multimodal conversational capability and can followopen-ended instruction to assist with inquiries about a biomedical image. Onthree standard biomedical visual question answering datasets, LLaVA-Medoutperforms previous supervised state-of-the-art on certain metrics. Tofacilitate biomedical multimodal research, we will release ourinstruction-following data and the LLaVA-Med model.</description><author>Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, Jianfeng Gao</author><pubDate>Thu, 01 Jun 2023 17:50:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00890v1</guid></item><item><title>Decision-Oriented Dialogue for Human-AI Collaboration</title><link>http://arxiv.org/abs/2305.20076v2</link><description>We describe a class of tasks called decision-oriented dialogues, in which AIassistants must collaborate with one or more humans via natural language tohelp them make complex decisions. We formalize three domains in which usersface everyday decisions: (1) choosing an assignment of reviewers to conferencepapers, (2) planning a multi-step itinerary in a city, and (3) negotiatingtravel plans for a group of friends. In each of these settings, AI assistantsand users have disparate abilities that they must combine to arrive at the bestdecision: assistants can access and process large amounts of information, whileusers have preferences and constraints external to the system. For each task,we build a dialogue environment where agents receive a reward based on thequality of the final decision they reach. Using these environments, we collecthuman-human dialogues with humans playing the role of assistant. To compare howcurrent AI assistants communicate in these settings, we present baselines usinglarge language models in self-play. Finally, we highlight a number ofchallenges models face in decision-oriented dialogues, ranging from efficientcommunication to reasoning and optimization, and release our environments as atestbed for future modeling work.</description><author>Jessy Lin, Nicholas Tomlin, Jacob Andreas, Jason Eisner</author><pubDate>Thu, 01 Jun 2023 17:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20076v2</guid></item><item><title>OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking</title><link>http://arxiv.org/abs/2306.00887v1</link><description>Open-vocabulary state tracking is a more practical version of state trackingthat aims to track state changes of entities throughout a process withoutrestricting the state space and entity space. OpenPI is to date the onlydataset annotated for open-vocabulary state tracking. However, we identifyissues with the dataset quality and evaluation metric. For the dataset, wecategorize 3 types of problems on the procedure level, step level and statechange level respectively, and build a clean dataset OpenPI-C using multiplerounds of human judgment. For the evaluation metric, we propose a cluster-basedmetric to fix the original metric's preference for repetition. Model-wise, we enhance the seq2seq generation baseline by reinstating two keyproperties for state tracking: temporal dependency and entity awareness. Thestate of the world after an action is inherently dependent on the previousstate. We model this dependency through a dynamic memory bank and allow themodel to attend to the memory slots during decoding. On the other hand, thestate of the world is naturally a union of the states of involved entities.Since the entities are unknown in the open-vocabulary setting, we propose atwo-stage model that refines the state change prediction conditioned onentities predicted from the first stage. Empirical results show theeffectiveness of our proposed model especially on the cluster-based metric. Thecode and data are released at https://github.com/shirley-wu/openpi-c</description><author>Xueqing Wu, Sha Li, Heng Ji</author><pubDate>Thu, 01 Jun 2023 17:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00887v1</guid></item><item><title>VOCALExplore: Pay-as-You-Go Video Data Exploration and Model Building [Technical Report]</title><link>http://arxiv.org/abs/2303.04068v2</link><description>We introduce VOCALExplore, a system designed to support users in buildingdomain-specific models over video datasets. VOCALExplore supports interactivelabeling sessions and trains models using user-supplied labels. VOCALExploremaximizes model quality by automatically deciding how to select samples basedon observed skew in the collected labels. It also selects the optimal videorepresentations to use when training models by casting feature selection as arising bandit problem. Finally, VOCALExplore implements optimizations toachieve low latency without sacrificing model performance. We demonstrate thatVOCALExplore achieves close to the best possible model quality given candidateacquisition functions and feature extractors, and it does so with low visiblelatency (~1 second per iteration) and no expensive preprocessing.</description><author>Maureen Daum, Enhao Zhang, Dong He, Stephen Mussmann, Brandon Haynes, Ranjay Krishna, Magdalena Balazinska</author><pubDate>Thu, 01 Jun 2023 17:48:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04068v2</guid></item><item><title>Bounds on BDD-Based Bucket Elimination</title><link>http://arxiv.org/abs/2306.00886v1</link><description>We study BDD-based bucket elimination, an approach to satisfiability testingusing variable elimination which has seen several practical implementations inthe past. We prove that it allows solving the standard pigeonhole principleformulas efficiently, when allowing different orders for variable eliminationand BDD-representations, a variant of bucket elimination that was recentlyintroduced. Furthermore, we show that this upper bound is somewhat brittle asfor formulas which we get from the pigeonhole principle by restriction, i.e.,fixing some of the variables, the same approach with the same variable ordershas exponential runtime. We also show that the more common implementation ofbucket elimination using the same order for variable elimination and the BDDshas exponential runtime for the pigeonhole principle when using either of thetwo orders from our upper bound, which suggests that the combination of both isthe key to efficiency in the setting.</description><author>Stefan Mengel</author><pubDate>Thu, 01 Jun 2023 17:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00886v1</guid></item><item><title>Sample Complexity of Forecast Aggregation</title><link>http://arxiv.org/abs/2207.13126v3</link><description>We consider a Bayesian forecast aggregation model where $n$ experts, afterobserving private signals about an unknown binary event, report their posteriorbeliefs about the event to a principal, who then aggregates the reports into asingle prediction for the event. The signals of the experts and the outcome ofthe event follow a joint distribution that is unknown to the principal, but theprincipal has access to i.i.d. "samples" from the distribution, where eachsample is a tuple of the experts' reports (not signals) and the realization ofthe event. Using these samples, the principal aims to find an$\varepsilon$-approximately optimal aggregator, where optimality is measured interms of the expected squared distance between the aggregated prediction andthe realization of the event. We show that the sample complexity of thisproblem is at least $\tilde \Omega(m^{n-2} / \varepsilon)$ for arbitrarydiscrete distributions, where $m$ is the size of each expert's signal space.This sample complexity grows exponentially in the number of experts $n$. But,if the experts' signals are independent conditioned on the realization of theevent, then the sample complexity is significantly reduced, to $\tilde O(1 /\varepsilon^2)$, which does not depend on $n$. Our results can be generalizedto non-binary events. The proof of our results uses a reduction from thedistribution learning problem and reveals the fact that forecast aggregation isalmost as difficult as distribution learning.</description><author>Yiling Chen, Tao Lin</author><pubDate>Thu, 01 Jun 2023 17:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.13126v3</guid></item><item><title>Domain Generalization for Domain-Linked Classes</title><link>http://arxiv.org/abs/2306.00879v1</link><description>Domain generalization (DG) focuses on transferring domain-invariant knowledgefrom multiple source domains (available at train time) to an, a priori, unseentarget domain(s). This requires a class to be expressed in multiple domains forthe learning algorithm to break the spurious correlations between domain andclass. However, in the real-world, classes may often be domain-linked, i.e.expressed only in a specific domain, which leads to extremely poorgeneralization performance for these classes. In this work, we aim to learngeneralizable representations for these domain-linked classes by transferringdomain-invariant knowledge from classes expressed in multiple source domains(domain-shared classes). To this end, we introduce this task to the communityand propose a Fair and cONtrastive feature-space regularization algorithm forDomain-linked DG, FOND. Rigorous and reproducible experiments with baselinesacross popular DG tasks demonstrate our method and its variants' ability toaccomplish state-of-the-art DG results for domain-linked classes. We alsoprovide practical insights on data conditions that increase domain-linked classgeneralizability to tackle real-world data scarcity.</description><author>Kimathi Kaai, Saad Hossain, Sirisha Rambhatla</author><pubDate>Thu, 01 Jun 2023 17:39:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00879v1</guid></item><item><title>Privately Estimating a Gaussian: Efficient, Robust and Optimal</title><link>http://arxiv.org/abs/2212.08018v2</link><description>In this work, we give efficient algorithms for privately estimating aGaussian distribution in both pure and approximate differential privacy (DP)models with optimal dependence on the dimension in the sample complexity. Inthe pure DP setting, we give an efficient algorithm that estimates an unknown$d$-dimensional Gaussian distribution up to an arbitrary tiny total variationerror using $\widetilde{O}(d^2 \log \kappa)$ samples while tolerating aconstant fraction of adversarial outliers. Here, $\kappa$ is the conditionnumber of the target covariance matrix. The sample bound matches bestnon-private estimators in the dependence on the dimension (up to apolylogarithmic factor). We prove a new lower bound on differentially privatecovariance estimation to show that the dependence on the condition number$\kappa$ in the above sample bound is also tight. Prior to our work, onlyidentifiability results (yielding inefficient super-polynomial time algorithms)were known for the problem. In the approximate DP setting, we give an efficientalgorithm to estimate an unknown Gaussian distribution up to an arbitrarilytiny total variation error using $\widetilde{O}(d^2)$ samples while toleratinga constant fraction of adversarial outliers. Prior to our work, all efficientapproximate DP algorithms incurred a super-quadratic sample cost or were notoutlier-robust. For the special case of mean estimation, our algorithm achievesthe optimal sample complexity of $\widetilde O(d)$, improving on a $\widetildeO(d^{1.5})$ bound from prior work. Our pure DP algorithm relies on a recursiveprivate preconditioning subroutine that utilizes the recent work on privatemean estimation [Hopkins et al., 2022]. Our approximate DP algorithms are basedon a substantial upgrade of the method of stabilizing convex relaxationsintroduced in [Kothari et al., 2022].</description><author>Daniel Alabi, Pravesh K. Kothari, Pranay Tankala, Prayaag Venkat, Fred Zhang</author><pubDate>Thu, 01 Jun 2023 17:38:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08018v2</guid></item><item><title>Quantifying Deep Learning Model Uncertainty in Conformal Prediction</title><link>http://arxiv.org/abs/2306.00876v1</link><description>Precise estimation of predictive uncertainty in deep neural networks is acritical requirement for reliable decision-making in machine learning andstatistical modeling, particularly in the context of medical AI. ConformalPrediction (CP) has emerged as a promising framework for representing the modeluncertainty by providing well-calibrated confidence levels for individualpredictions. However, the quantification of model uncertainty in conformalprediction remains an active research area, yet to be fully addressed. In thispaper, we explore state-of-the-art CP methodologies and their theoreticalfoundations. We propose a probabilistic approach in quantifying the modeluncertainty derived from the produced prediction sets in conformal predictionand provide certified boundaries for the computed uncertainty. By doing so, weallow model uncertainty measured by CP to be compared by other uncertaintyquantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) andEvidential approaches.</description><author>Hamed Karimi, Reza Samavi</author><pubDate>Thu, 01 Jun 2023 17:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00876v1</guid></item><item><title>Is novelty predictable?</title><link>http://arxiv.org/abs/2306.00872v1</link><description>Machine learning-based design has gained traction in the sciences, mostnotably in the design of small molecules, materials, and proteins, withsocietal implications spanning drug development and manufacturing, plasticdegradation, and carbon sequestration. When designing objects to achieve novelproperty values with machine learning, one faces a fundamental challenge: howto push past the frontier of current knowledge, distilled from the trainingdata into the model, in a manner that rationally controls the risk of failure.If one trusts learned models too much in extrapolation, one is likely to designrubbish. In contrast, if one does not extrapolate, one cannot find novelty.Herein, we ponder how one might strike a useful balance between these twoextremes. We focus in particular on designing proteins with novel propertyvalues, although much of our discussion addresses machine learning-based designmore broadly.</description><author>Clara Fannjiang, Jennifer Listgarten</author><pubDate>Thu, 01 Jun 2023 17:32:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00872v1</guid></item><item><title>Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art</title><link>http://arxiv.org/abs/2305.16259v2</link><description>The adoption of Deep Neural Networks (DNNs) has greatly benefited NaturalLanguage Processing (NLP) during the past decade. However, the demands of longdocument analysis are quite different from those of shorter texts, while theever increasing size of documents uploaded on-line renders automatedunderstanding of long texts a critical area of research. This article has twogoals: a) it overviews the relevant neural building blocks, thus serving as ashort tutorial, and b) it surveys the state-of-the-art in long document NLP,mainly focusing on two central tasks: document classification and documentsummarization. Sentiment analysis for long texts is also covered, since it istypically treated as a particular case of document classification.Additionally, this article discusses the main challenges, issues and currentsolutions related to long document NLP. Finally, the relevant, publiclyavailable, annotated datasets are presented, in order to facilitate furtherresearch.</description><author>Dimitrios Tsirmpas, Ioannis Gkionis, Ioannis Mademlis</author><pubDate>Thu, 01 Jun 2023 17:29:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16259v2</guid></item><item><title>IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control</title><link>http://arxiv.org/abs/2306.00867v1</link><description>Model-based reinforcement learning (RL) has shown great promise due to itssample efficiency, but still struggles with long-horizon sparse-reward tasks,especially in offline settings where the agent learns from a fixed dataset. Wehypothesize that model-based RL agents struggle in these environments due to alack of long-term planning capabilities, and that planning in a temporallyabstract model of the environment can alleviate this issue. In this paper, wemake two key contributions: 1) we introduce an offline model-based RLalgorithm, IQL-TD-MPC, that extends the state-of-the-art Temporal DifferenceLearning for Model Predictive Control (TD-MPC) with Implicit Q-Learning (IQL);2) we propose to use IQL-TD-MPC as a Manager in a hierarchical setting with anyoff-the-shelf offline RL algorithm as a Worker. More specifically, we pre-traina temporally abstract IQL-TD-MPC Manager to predict "intent embeddings", whichroughly correspond to subgoals, via planning. We empirically show thataugmenting state representations with intent embeddings generated by anIQL-TD-MPC manager significantly improves off-the-shelf offline RL agents'performance on some of the most challenging D4RL benchmark tasks. For instance,the offline RL algorithms AWAC, TD3-BC, DT, and CQL all get zero or near-zeronormalized evaluation scores on the medium and large antmaze tasks, while ourmodification gives an average score over 40.</description><author>Rohan Chitnis, Yingchen Xu, Bobak Hashemi, Lucas Lehnert, Urun Dogan, Zheqing Zhu, Olivier Delalleau</author><pubDate>Thu, 01 Jun 2023 17:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00867v1</guid></item><item><title>A Transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics</title><link>http://arxiv.org/abs/2306.00864v1</link><description>During the diagnostic process, clinicians leverage multimodal information,such as chief complaints, medical images, and laboratory-test results.Deep-learning models for aiding diagnosis have yet to meet this requirement.Here we report a Transformer-based representation-learning model as a clinicaldiagnostic aid that processes multimodal input in a unified manner. Rather thanlearning modality-specific features, the model uses embedding layers to convertimages and unstructured and structured text into visual tokens and text tokens,and bidirectional blocks with intramodal and intermodal attention to learn aholistic representation of radiographs, the unstructured chief complaint andclinical history, structured clinical information such as laboratory-testresults and patient demographic information. The unified model outperformed animage-only model and non-unified multimodal diagnosis models in theidentification of pulmonary diseases (by 12% and 9%, respectively) and in theprediction of adverse clinical outcomes in patients with COVID-19 (by 29% and7%, respectively). Leveraging unified multimodal Transformer-based models mayhelp streamline triage of patients and facilitate the clinical decisionprocess.</description><author>Hong-Yu Zhou, Yizhou Yu, Chengdi Wang, Shu Zhang, Yuanxu Gao, Jia Pan, Jun Shao, Guangming Lu, Kang Zhang, Weimin Li</author><pubDate>Thu, 01 Jun 2023 17:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00864v1</guid></item><item><title>DeepFake-Adapter: Dual-Level Adapter for DeepFake Detection</title><link>http://arxiv.org/abs/2306.00863v1</link><description>Existing deepfake detection methods fail to generalize well to unseen ordegraded samples, which can be attributed to the over-fitting of low-levelforgery patterns. Here we argue that high-level semantics are alsoindispensable recipes for generalizable forgery detection. Recently, largepre-trained Vision Transformers (ViTs) have shown promising generalizationcapability. In this paper, we propose the first parameter-efficient tuningapproach for deepfake detection, namely DeepFake-Adapter, to effectively andefficiently adapt the generalizable high-level semantics from large pre-trainedViTs to aid deepfake detection. Given large pre-trained models but limiteddeepfake data, DeepFake-Adapter introduces lightweight yet dedicated dual-leveladapter modules to a ViT while keeping the model backbone frozen. Specifically,to guide the adaptation process to be aware of both global and local forgerycues of deepfake data, 1) we not only insert Globally-aware Bottleneck Adaptersin parallel to MLP layers of ViT, 2) but also actively cross-attendLocally-aware Spatial Adapters with features from ViT. Unlike existing deepfakedetection methods merely focusing on low-level forgery patterns, the forgerydetection process of our model can be regularized by generalizable high-levelsemantics from a pre-trained ViT and adapted by global and local low-levelforgeries of deepfake data. Extensive experiments on several standard deepfakedetection benchmarks validate the effectiveness of our approach. Notably,DeepFake-Adapter demonstrates a convincing advantage under cross-dataset andcross-manipulation settings. The source code is released athttps://github.com/rshaojimmy/DeepFake-Adapter</description><author>Rui Shao, Tianxing Wu, Liqiang Nie, Ziwei Liu</author><pubDate>Thu, 01 Jun 2023 17:23:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00863v1</guid></item><item><title>Continual Vision-Language Representation Learning with Off-Diagonal Information</title><link>http://arxiv.org/abs/2305.07437v5</link><description>Large-scale multi-modal contrastive learning frameworks like CLIP typicallyrequire a large amount of image-text samples for training. However, thesesamples are always collected continuously in real scenarios. This paperdiscusses the feasibility of continual CLIP training using streaming data.Unlike continual learning based on self-supervised learning methods for pureimages, which is empirically robust against catastrophic forgetting, CLIP'sperformance degeneration in the continual setting is significant andnon-neglectable. By analyzing the changes in the model's representation spaceduring continual CLIP training from a spatial geometry perspective, we exploreand summarize these spatial variations as Spatial Disorder (SD), which can bedivided into Intra-modal Rotation and Inter-modal Deviation. Moreover, weempirically and theoretically demonstrate how SD leads to a performance declinefor CLIP on cross-modal retrieval tasks. To alleviate SD, we propose a newcontinual vision-language representation learning framework Mod-X: Maintainoff-diagonal information-matriX. By selectively aligning the off-diagonalinformation distribution of contrastive matrices, the Mod-X improves thecapability of the multi-modal model by maintaining the multi-modalrepresentation space alignment on the old data domain during continuouslyfitting the new training data domain. Experiments on commonly used datasetswith different scales and scopes have demonstrated the effectiveness of ourmethod.</description><author>Zixuan Ni, Longhui Wei, Siliang Tang, Yueting Zhuang, Qi Tian</author><pubDate>Thu, 01 Jun 2023 17:22:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07437v5</guid></item><item><title>Non-stationary Reinforcement Learning under General Function Approximation</title><link>http://arxiv.org/abs/2306.00861v1</link><description>General function approximation is a powerful tool to handle large state andaction spaces in a broad range of reinforcement learning (RL) scenarios.However, theoretical understanding of non-stationary MDPs with general functionapproximation is still limited. In this paper, we make the first such anattempt. We first propose a new complexity metric called dynamic Bellman Eluder(DBE) dimension for non-stationary MDPs, which subsumes majority of existingtractable RL problems in static MDPs as well as non-stationary MDPs. Based onthe proposed complexity metric, we propose a novel confidence-set basedmodel-free algorithm called SW-OPEA, which features a sliding window mechanismand a new confidence set design for non-stationary MDPs. We then establish anupper bound on the dynamic regret for the proposed algorithm, and show thatSW-OPEA is provably efficient as long as the variation budget is notsignificantly large. We further demonstrate via examples of non-stationarylinear and tabular MDPs that our algorithm performs better in small variationbudget scenario than the existing UCB-type algorithms. To the best of ourknowledge, this is the first dynamic regret analysis in non-stationary MDPswith general function approximation.</description><author>Songtao Feng, Ming Yin, Ruiquan Huang, Yu-Xiang Wang, Jing Yang, Yingbin Liang</author><pubDate>Thu, 01 Jun 2023 17:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00861v1</guid></item><item><title>Power Grid Behavioral Patterns and Risks of Generalization in Applied Machine Learning</title><link>http://arxiv.org/abs/2304.10702v2</link><description>Recent years have seen a rich literature of data-driven approaches designedfor power grid applications. However, insufficient consideration of domainknowledge can impose a high risk to the practicality of the methods.Specifically, ignoring the grid-specific spatiotemporal patterns (in load,generation, and topology, etc.) can lead to outputting infeasible,unrealizable, or completely meaningless predictions on new inputs. To addressthis concern, this paper investigates real-world operational data to provideinsights into power grid behavioral patterns, including the time-varyingtopology, load, and generation, as well as the spatial differences (in peakhours, diverse styles) between individual loads and generations. Then based onthese observations, we evaluate the generalization risks in some existing MLworks causedby ignoring these grid-specific patterns in model design andtraining.</description><author>Shimiao Li, Jan Drgona, Shrirang Abhyankar, Larry Pileggi</author><pubDate>Thu, 01 Jun 2023 17:19:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10702v2</guid></item><item><title>Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models</title><link>http://arxiv.org/abs/2210.15458v2</link><description>Decoding methods for large language models often trade-off between diversityof outputs and parallelism of computation. Methods such as beam search andGumbel top-k sampling can guarantee a different output for each element of thebeam, but are not easy to parallelize. Alternatively, methods such astemperature sampling and its modifications (top-k sampling, nucleus sampling,typical decoding, and others), are embarrassingly parallel, but have noguarantees about duplicate samples. We present a framework for samplingaccording to an arithmetic code book implicitly defined by a large languagemodel, compatible with common sampling variations, with provable beam diversityunder certain conditions, as well as being embarrassingly parallel andproviding unbiased and consistent expectations from the original model. Wedemonstrate the effectiveness of our approach on WMT machine translation, morethan halving the standard deviation when estimating expected BLEU score reward,and closing the BLEU score gap between independent sampling and beam search byup to 63%.</description><author>Luke Vilnis, Yury Zemlyanskiy, Patrick Murray, Alexandre Passos, Sumit Sanghai</author><pubDate>Thu, 01 Jun 2023 17:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.15458v2</guid></item><item><title>Adversarial learning of neural user simulators for dialogue policy optimisation</title><link>http://arxiv.org/abs/2306.00858v1</link><description>Reinforcement learning based dialogue policies are typically trained ininteraction with a user simulator. To obtain an effective and robust policy,this simulator should generate user behaviour that is both realistic andvaried. Current data-driven simulators are trained to accurately model the userbehaviour in a dialogue corpus. We propose an alternative method usingadversarial learning, with the aim to simulate realistic user behaviour withmore variation. We train and evaluate several simulators on a corpus ofrestaurant search dialogues, and then use them to train dialogue systempolicies. In policy cross-evaluation experiments we demonstrate that anadversarially trained simulator produces policies with 8.3% higher success ratethan those trained with a maximum likelihood simulator. Subjective results froma crowd-sourced dialogue system user evaluation confirm the effectiveness ofadversarially training user simulators.</description><author>Simon Keizer, Caroline Dockes, Norbert Braunschweiler, Svetlana Stoyanchev, Rama Doddipatla</author><pubDate>Thu, 01 Jun 2023 17:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00858v1</guid></item><item><title>Near-optimal fitting of ellipsoids to random points</title><link>http://arxiv.org/abs/2208.09493v4</link><description>Given independent standard Gaussian points $v_1, \ldots, v_n$ in dimension$d$, for what values of $(n, d)$ does there exist with high probability anorigin-symmetric ellipsoid that simultaneously passes through all of thepoints? This basic problem of fitting an ellipsoid to random points hasconnections to low-rank matrix decompositions, independent component analysis,and principal component analysis. Based on strong numerical evidence,Saunderson, Parrilo, and Willsky [Proc. of Conference on Decision and Control,pp. 6031-6036, 2013] conjecture that the ellipsoid fitting problem transitionsfrom feasible to infeasible as the number of points $n$ increases, with a sharpthreshold at $n \sim d^2/4$. We resolve this conjecture up to logarithmicfactors by constructing a fitting ellipsoid for some $n = \Omega( \,d^2/\mathrm{polylog}(d) \,)$, improving prior work of Ghosh et al. [Proc. ofSymposium on Foundations of Computer Science, pp. 954-965, 2020] that requires$n = o(d^{3/2})$. Our proof demonstrates feasibility of the least squaresconstruction of Saunderson et al. using a convenient decomposition of a certainnon-standard random matrix and a careful analysis of its Neumann expansion viathe theory of graph matrices.</description><author>Aaron Potechin, Paxton Turner, Prayaag Venkat, Alexander S. Wein</author><pubDate>Thu, 01 Jun 2023 17:16:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09493v4</guid></item><item><title>Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models</title><link>http://arxiv.org/abs/2305.19595v2</link><description>Vision and Language (VL) models offer an effective method for aligningrepresentation spaces of images and text, leading to numerous applications suchas cross-modal retrieval, visual question answering, captioning, and more.However, the aligned image-text spaces learned by all the popular VL models arestill suffering from the so-called `object bias' - their representations behaveas `bags of nouns', mostly ignoring or downsizing the attributes, relations,and states of objects described/appearing in texts/images. Although some greatattempts at fixing these `compositional reasoning' issues were proposed in therecent literature, the problem is still far from being solved. In this paper,we uncover two factors limiting the VL models' compositional reasoningperformance. These two factors are properties of the paired VL dataset used forfinetuning and pre-training the VL model: (i) the caption quality, or in otherwords `image-alignment', of the texts; and (ii) the `density' of the captionsin the sense of mentioning all the details appearing on the image. We propose afine-tuning approach for automatically treating these factors leveraging astandard VL dataset (CC3M). Applied to CLIP, we demonstrate its significantcompositional reasoning performance increase of up to $\sim27\%$ over the basemodel, up to $\sim20\%$ over the strongest baseline, and by $6.7\%$ on average.</description><author>Sivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig, Donghyun Kim, Paola Cascante-bonilla, Amit Alfassy, Rameswar Panda, Raja Giryes, Rogerio Feris, Shimon Ullman, Leonid Karlinsky</author><pubDate>Thu, 01 Jun 2023 17:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19595v2</guid></item><item><title>Loss-Optimal Classification Trees: A Generalized Framework and the Logistic Case</title><link>http://arxiv.org/abs/2306.00857v1</link><description>The Classification Tree (CT) is one of the most common models ininterpretable machine learning. Although such models are usually built withgreedy strategies, in recent years, thanks to remarkable advances inMixer-Integer Programming (MIP) solvers, several exact formulations of thelearning problem have been developed. In this paper, we argue that some of themost relevant ones among these training models can be encapsulated within ageneral framework, whose instances are shaped by the specification of lossfunctions and regularizers. Next, we introduce a novel realization of thisframework: specifically, we consider the logistic loss, handled in the MIPsetting by a linear piece-wise approximation, and couple it with$\ell_1$-regularization terms. The resulting Optimal Logistic Tree modelnumerically proves to be able to induce trees with enhanced interpretabilityfeatures and competitive generalization capabilities, compared to thestate-of-the-art MIP-based approaches.</description><author>Tommaso Aldinucci, Matteo Lapucci</author><pubDate>Thu, 01 Jun 2023 17:14:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00857v1</guid></item><item><title>A deep-learning approach to early identification of suggested sexual harassment from videos</title><link>http://arxiv.org/abs/2306.00856v1</link><description>Sexual harassment, sexual abuse, and sexual violence are prevalent problemsin this day and age. Women's safety is an important issue that needs to behighlighted and addressed. Given this issue, we have studied each of theseconcerns and the factors that affect it based on images generated from movies.We have classified the three terms (harassment, abuse, and violence) based onthe visual attributes present in images depicting these situations. Weidentified that factors such as facial expression of the victim and perpetratorand unwanted touching had a direct link to identifying the scenes containingsexual harassment, abuse and violence. We also studied and outlined howstate-of-the-art explicit content detectors such as Google Cloud Vision API andClarifai API fail to identify and categorise these images. Based on thesedefinitions and characteristics, we have developed a first-of-its-kind datasetfrom various Indian movie scenes. These scenes are classified as sexualharassment, sexual abuse, or sexual violence and exported in the PASCAL VOC 1.1format. Our dataset is annotated on the identified relevant features and can beused to develop and train a deep-learning computer vision model to identifythese issues. The dataset is publicly available for research and development.</description><author>Shreya Shetye, Anwita Maiti, Tannistha Maiti, Tarry Singh</author><pubDate>Thu, 01 Jun 2023 17:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00856v1</guid></item><item><title>SMARAGD: Learning SMatch for Accurate and Rapid Approximate Graph Distance</title><link>http://arxiv.org/abs/2203.13226v2</link><description>The similarity of graph structures, such as Meaning Representations (MRs), isoften assessed via structural matching algorithms, such as Smatch (Cai andKnight, 2013). However, Smatch involves a combinatorial problem that suffersfrom NP-completeness, making large-scale applications, e.g., graph clusteringor search, infeasible. To alleviate this issue, we learn SMARAGD: SemanticMatch for Accurate and Rapid Approximate Graph Distance. We show the potentialof neural networks to approximate Smatch scores, i) in linear time using amachine translation framework to predict alignments, or ii) in constant timeusing a Siamese CNN to directly predict Smatch scores. We show that theapproximation error can be substantially reduced through data augmentation andgraph anonymization.</description><author>Juri Opitz, Philipp Meier, Anette Frank</author><pubDate>Thu, 01 Jun 2023 17:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.13226v2</guid></item><item><title>Spatio-Angular Convolutions for Super-resolution in Diffusion MRI</title><link>http://arxiv.org/abs/2306.00854v1</link><description>Diffusion MRI (dMRI) is a widely used imaging modality, but requires longscanning times to acquire high resolution datasets. By leveraging the uniquegeometry present within this domain, we present a novel approach to dMRIangular super-resolution that extends upon the parametric continuousconvolution (PCConv) framework. We introduce several additions to the operationincluding a Fourier feature mapping, global coordinates, and domain specificcontext. Using this framework, we build a fully parametric continuousconvolution network (PCCNN) and compare against existing models. We demonstratethe PCCNN performs competitively while using significantly less parameters.Moreover, we show that this formulation generalises well to clinically relevantdownstream analyses such as fixel-based analysis, and neurite orientationdispersion and density imaging.</description><author>Matthew Lyon, Paul Armitage, Mauricio A lvarez</author><pubDate>Thu, 01 Jun 2023 17:10:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00854v1</guid></item><item><title>Grounding Language Models to Images for Multimodal Inputs and Outputs</title><link>http://arxiv.org/abs/2301.13823v3</link><description>We propose an efficient method to ground pretrained text-only language modelsto the visual domain, enabling them to process arbitrarily interleavedimage-and-text data, and generate text interleaved with retrieved images. Ourmethod leverages the abilities of language models learnt from large scaletext-only pretraining, such as in-context learning and free-form textgeneration. We keep the language model frozen, and finetune input and outputlinear layers to enable cross-modality interactions. This allows our model toprocess arbitrarily interleaved image-and-text inputs, and generate free-formtext interleaved with retrieved images. We achieve strong zero-shot performanceon grounded tasks such as contextual image retrieval and multimodal dialogue,and showcase compelling interactive abilities. Our approach works with anyoff-the-shelf language model and paves the way towards an effective, generalsolution for leveraging pretrained language models in visually groundedsettings.</description><author>Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried</author><pubDate>Thu, 01 Jun 2023 17:09:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13823v3</guid></item><item><title>Learning Sampling Dictionaries for Efficient and Generalizable Robot Motion Planning with Transformers</title><link>http://arxiv.org/abs/2306.00851v1</link><description>Motion planning is integral to robotics applications such as autonomousdriving, surgical robots, and industrial manipulators. Existing planningmethods lack scalability to higher-dimensional spaces, while recent learningbased planners have shown promise in accelerating sampling-based motionplanners (SMP) but lack generalizability to out-of-distribution environments.To address this, we present a novel approach, Vector Quantized-Motion PlanningTransformers (VQ-MPT) that overcomes the key generalization and scalingdrawbacks of previous learning-based methods. VQ-MPT consists of two stages.Stage 1 is a Vector Quantized-Variational AutoEncoder model that learns torepresent the planning space using a finite number of sampling distributions,and stage 2 is an Auto-Regressive model that constructs a sampling region forSMPs by selecting from the learned sampling distribution sets. By splittinglarge planning spaces into discrete sets and selectively choosing the samplingregions, our planner pairs well with out-of-the-box SMPs, generatingnear-optimal paths faster than without VQ-MPT's aid. It is generalizable inthat it can be applied to systems of varying complexities, from 2D planar to14D bi-manual robots with diverse environment representations, includingcostmaps and point clouds. Trained VQ-MPT models generalize to environmentsunseen during training and achieve higher success rates than previous methods.</description><author>Jacob J Johnson, Ahmed H Qureshi, Michael Yip</author><pubDate>Thu, 01 Jun 2023 17:08:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00851v1</guid></item><item><title>Model Transferability With Responsive Decision Subjects</title><link>http://arxiv.org/abs/2107.05911v4</link><description>Given an algorithmic predictor that is accurate on some source populationconsisting of strategic human decision subjects, will it remain accurate if thepopulation respond to it? In our setting, an agent or a user corresponds to asample $(X,Y)$ drawn from a distribution $\cal{D}$ and will face a model $h$and its classification result $h(X)$. Agents can modify $X$ to adapt to $h$,which will incur a distribution shift on $(X,Y)$. Our formulation is motivatedby applications where the deployed machine learning models are subjected tohuman agents, and will ultimately face responsive and interactive datadistributions. We formalize the discussions of the transferability of a modelby studying how the performance of the model trained on the available sourcedistribution (data) would translate to the performance on its induced domain.We provide both upper bounds for the performance gap due to the induced domainshift, as well as lower bounds for the trade-offs that a classifier has tosuffer on either the source training distribution or the induced targetdistribution. We provide further instantiated analysis for two popular domainadaptation settings, including covariate shift and target shift.</description><author>Yatong Chen, Zeyu Tang, Kun Zhang, Yang Liu</author><pubDate>Thu, 01 Jun 2023 17:07:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.05911v4</guid></item><item><title>BitE : Accelerating Learned Query Optimization in a Mixed-Workload Environment</title><link>http://arxiv.org/abs/2306.00845v1</link><description>Although the many efforts to apply deep reinforcement learning to queryoptimization in recent years, there remains room for improvement as queryoptimizers are complex entities that require hand-designed tuning of workloadsand datasets. Recent research present learned query optimizations resultsmostly in bulks of single workloads which focus on picking up the unique traitsof the specific workload. This proves to be problematic in scenarios where thedifferent characteristics of multiple workloads and datasets are to be mixedand learned together. Henceforth, in this paper, we propose BitE, a novelensemble learning model using database statistics and metadata to tune alearned query optimizer for enhancing performance. On the way, we introducemultiple revisions to solve several challenges: we extend the search space forthe optimal Abstract SQL Plan(represented as a JSON object called ASP) byexpanding hintsets, we steer the model away from the default plans that may bebiased by configuring the experience with all unique plans of queries, and wedeviate from the traditional loss functions and choose an alternative method tocope with underestimation and overestimation of reward. Our model achieves19.6% more improved queries and 15.8% less regressed queries compared to theexisting traditional methods whilst using a comparable level of resources.</description><author>Yuri Kim, Yewon Choi, Yujung Gil, Sanghee Lee, Heesik Shin, Jaehyok Chong</author><pubDate>Thu, 01 Jun 2023 17:05:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00845v1</guid></item><item><title>What model does MuZero learn?</title><link>http://arxiv.org/abs/2306.00840v1</link><description>Model-based reinforcement learning has drawn considerable interest in recentyears, given its promise to improve sample efficiency. Moreover, when usingdeep-learned models, it is potentially possible to learn compact models fromcomplex sensor data. However, the effectiveness of these learned models,particularly their capacity to plan, i.e., to improve the current policy,remains unclear. In this work, we study MuZero, a well-known deep model-basedreinforcement learning algorithm, and explore how far it achieves its learningobjective of a value-equivalent model and how useful the learned models are forpolicy improvement. Amongst various other insights, we conclude that the modellearned by MuZero cannot effectively generalize to evaluate unseen policies,which limits the extent to which we can additionally improve the current policyby planning with the model.</description><author>Jinke He, Thomas M. Moerland, Frans A. Oliehoek</author><pubDate>Thu, 01 Jun 2023 17:01:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00840v1</guid></item><item><title>Generalization for slowly mixing processes</title><link>http://arxiv.org/abs/2305.00977v2</link><description>A bound uniform over various loss-classes is given for data generated bystationary and phi-mixing processes, where the mixing time (the time needed toobtain approximate independence) enters the sample complexity only in anadditive way. For slowly mixing processes this can be a considerable advantageover results with multiplicative dependence on the mixing time. The admissibleloss-classes include functions with prescribed Lipschitz norms or smoothnessparameters. The bound can also be applied to be uniform over unconstrainedloss-classes, where it depends on local Lipschitz properties of the function onthe sample path.</description><author>Andreas Maurer</author><pubDate>Thu, 01 Jun 2023 17:00:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00977v2</guid></item><item><title>Disentangled Causal Graph Learning forOnline Unsupervised Root Cause Analysis</title><link>http://arxiv.org/abs/2305.10638v2</link><description>The task of root cause analysis (RCA) is to identify the root causes ofsystem faults/failures by analyzing system monitoring data. Efficient RCA cangreatly accelerate system failure recovery and mitigate system damages orfinancial losses. However, previous research has mostly focused on developingoffline RCA algorithms, which often require manually initiating the RCAprocess, a significant amount of time and data to train a robust model, andthen being retrained from scratch for a new system fault. In this paper, we propose CORAL, a novel online RCA framework that canautomatically trigger the RCA process and incrementally update the RCA model.CORAL consists of Trigger Point Detection, Incremental Disentangled CausalGraph Learning, and Network Propagation-based Root Cause Localization. TheTrigger Point Detection component aims to detect system state transitionsautomatically and in near-real-time. To achieve this, we develop an onlinetrigger point detection approach based on multivariate singular spectrumanalysis and cumulative sum statistics. To efficiently update the RCA model, wepropose an incremental disentangled causal graph learning approach to decouplethe state-invariant and state-dependent information. After that, CORAL appliesa random walk with restarts to the updated causal graph to accurately identifyroot causes. The online RCA process terminates when the causal graph and thegenerated root cause list converge. Extensive experiments on three real-worlddatasets with case studies demonstrate the effectiveness and superiority of theproposed framework.</description><author>Dongjie Wang, Zhengzhang Chen, Yanjie Fu, Yanchi Liu, Haifeng Chen</author><pubDate>Thu, 01 Jun 2023 16:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10638v2</guid></item><item><title>Deformable Convolutions and LSTM-based Flexible Event Frame Fusion Network for Motion Deblurring</title><link>http://arxiv.org/abs/2306.00834v1</link><description>Event cameras differ from conventional RGB cameras in that they produceasynchronous data sequences. While RGB cameras capture every frame at a fixedrate, event cameras only capture changes in the scene, resulting in sparse andasynchronous data output. Despite the fact that event data carries usefulinformation that can be utilized in motion deblurring of RGB cameras,integrating event and image information remains a challenge. Recentstate-of-the-art CNN-based deblurring solutions produce multiple 2-D eventframes based on the accumulation of event data over a time period. In most ofthese techniques, however, the number of event frames is fixed and predefined,which reduces temporal resolution drastically, particularly for scenarios whenfast-moving objects are present or when longer exposure times are required. Itis also important to note that recent modern cameras (e.g., cameras in mobilephones) dynamically set the exposure time of the image, which presents anadditional problem for networks developed for a fixed number of event frames. ALong Short-Term Memory (LSTM)-based event feature extraction module has beendeveloped for addressing these challenges, which enables us to use adynamically varying number of event frames. Using these modules, we constructeda state-of-the-art deblurring network, Deformable Convolutions and LSTM-basedFlexible Event Frame Fusion Network (DLEFNet). It is particularly useful forscenarios in which exposure times vary depending on factors such as lightingconditions or the presence of fast-moving objects in the scene. It has beendemonstrated through evaluation results that the proposed method can outperformthe existing state-of-the-art networks for deblurring task in synthetic andreal-world data sets.</description><author>Dan Yang, Mehmet Yamac</author><pubDate>Thu, 01 Jun 2023 16:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00834v1</guid></item><item><title>When Does Bottom-up Beat Top-down in Hierarchical Community Detection?</title><link>http://arxiv.org/abs/2306.00833v1</link><description>Hierarchical clustering of networks consists in finding a tree ofcommunities, such that lower levels of the hierarchy reveal finer-grainedcommunity structures. There are two main classes of algorithms tackling thisproblem. Divisive ($\textit{top-down}$) algorithms recursively partition thenodes into two communities, until a stopping rule indicates that no furthersplit is needed. In contrast, agglomerative ($\textit{bottom-up}$) algorithmsfirst identify the smallest community structure and then repeatedly merge thecommunities using a $\textit{linkage}$ method. In this article, we establishtheoretical guarantees for the recovery of the hierarchical tree and communitystructure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. Wealso establish that this bottom-up algorithm attains the information-theoreticthreshold for exact recovery at intermediate levels of the hierarchy. Notably,these recovery conditions are less restrictive compared to those existing fortop-down algorithms. This shows that bottom-up algorithms extend the feasibleregion for achieving exact recovery at intermediate levels. Numericalexperiments on both synthetic and real data sets confirm the superiority ofbottom-up algorithms over top-down algorithms. We also observe that top-downalgorithms can produce dendrograms with inversions. These findings contributeto a better understanding of hierarchical clustering techniques and theirapplications in network analysis.</description><author>Maximilien Dreveton, Daichi Kuroda, Matthias Grossglauser, Patrick Thiran</author><pubDate>Thu, 01 Jun 2023 16:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00833v1</guid></item><item><title>Iterative autoregression: a novel trick to improve your low-latency speech enhancement model</title><link>http://arxiv.org/abs/2211.01751v2</link><description>Streaming models are an essential component of real-time speech enhancementtools. The streaming regime constrains speech enhancement models to use only atiny context of future information. As a result, the low-latency streamingsetup is generally considered a challenging task and has a significant negativeimpact on the model's quality. However, the sequential nature of streaminggeneration offers a natural possibility for autoregression, that is, utilizingprevious predictions while making current ones. The conventional method fortraining autoregressive models is teacher forcing, but its primary drawbacklies in the training-inference mismatch that can lead to a substantialdegradation in quality. In this study, we propose a straightforward yeteffective alternative technique for training autoregressive low-latency speechenhancement models. We demonstrate that the proposed approach leads to stableimprovement across diverse architectures and training scenarios.</description><author>Pavel Andreev, Nicholas Babaev, Azat Saginbaev, Ivan Shchekotov</author><pubDate>Thu, 01 Jun 2023 16:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01751v2</guid></item><item><title>Transferable Energy Storage Bidder</title><link>http://arxiv.org/abs/2301.01233v2</link><description>Energy storage resources must consider both price uncertainties and theirphysical operating characteristics when participating in wholesale electricitymarkets. This is a challenging problem as electricity prices are highlyvolatile, and energy storage has efficiency losses, power, and energyconstraints. This paper presents a novel, versatile, and transferable approachcombining model-based optimization with a convolutional long short-term memorynetwork for energy storage to respond to or bid into wholesale electricitymarkets. We test our proposed approach using historical prices from New YorkState, showing it achieves state-of-the-art results, achieving between 70% tonear 90% profit ratio compared to perfect foresight cases, in both priceresponse and wholesale market bidding setting with various energy storagedurations. We also test a transfer learning approach by pre-training thebidding model using New York data and applying it to arbitrage in Queensland,Australia. The result shows transfer learning achieves exceptional arbitrageprofitability with as little as three days of local training data,demonstrating its significant advantage over training from scratch in scenarioswith very limited data availability.</description><author>Yousuf Baker, Ningkun Zheng, Bolun Xu</author><pubDate>Thu, 01 Jun 2023 16:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.01233v2</guid></item><item><title>In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation</title><link>http://arxiv.org/abs/2306.00826v1</link><description>Out-of-distribution (OOD) detection is the problem of identifying inputswhich are unrelated to the in-distribution task. The OOD detection performancewhen the in-distribution (ID) is ImageNet-1K is commonly being tested on asmall range of test OOD datasets. We find that most of the currently used testOOD datasets, including datasets from the open set recognition (OSR)literature, have severe issues: In some cases more than 50$\%$ of the datasetcontains objects belonging to one of the ID classes. These erroneous samplesheavily distort the evaluation of OOD detectors. As a solution, we introducewith NINCO a novel test OOD dataset, each sample checked to be ID free, whichwith its fine-grained range of OOD classes allows for a detailed analysis of anOOD detector's strengths and failure modes, particularly when paired with anumber of synthetic "OOD unit-tests". We provide detailed evaluations across alarge set of architectures and OOD detection methods on NINCO and theunit-tests, revealing new insights about model weaknesses and the effects ofpretraining on OOD detection performance. We provide code and data athttps://github.com/j-cb/NINCO.</description><author>Julian Bitterwolf, Maximilian Mller, Matthias Hein</author><pubDate>Thu, 01 Jun 2023 16:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00826v1</guid></item><item><title>Zero and Few-shot Semantic Parsing with Ambiguous Inputs</title><link>http://arxiv.org/abs/2306.00824v1</link><description>Despite the ubiquity of ambiguity in natural language, it is often ignored ordeliberately removed in semantic parsing tasks, which generally assume that agiven surface form has only one correct logical form. We attempt to addressthis shortcoming by introducing AmP, a framework, dataset, and challenge forparsing with linguistic ambiguity. We define templates and generate data forfive well-documented linguistic ambiguities. Using AmP, we investigate howseveral few-shot semantic parsing systems handle ambiguity, introducing threenew metrics. We find that large pre-trained models perform poorly at capturingthe distribution of possible meanings without deliberate instruction. However,models are able to capture distribution well when ambiguity is attested intheir inputs. These results motivate a call for ambiguity to be explicitlyincluded in semantic parsing, and promotes considering the distribution ofpossible outputs when evaluating semantic parsing systems.</description><author>Elias Stengel-Eskin, Kyle Rawlins, Benjamin Van Durme</author><pubDate>Thu, 01 Jun 2023 16:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00824v1</guid></item></channel></rss>