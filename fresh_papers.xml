<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 17 Dec 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LIME: Localized Image Editing via Attention Regularization in Diffusion Models</title><link>http://arxiv.org/abs/2312.09256v1</link><description>Diffusion models (DMs) have gained prominence due to their ability togenerate high-quality, varied images, with recent advancements in text-to-imagegeneration. The research focus is now shifting towards the controllability ofDMs. A significant challenge within this domain is localized editing, wherespecific areas of an image are modified without affecting the rest of thecontent. This paper introduces LIME for localized image editing in diffusionmodels that do not require user-specified regions of interest (RoI) oradditional text input. Our method employs features from pre-trained methods anda simple clustering technique to obtain precise semantic segmentation maps.Then, by leveraging cross-attention maps, it refines these segments forlocalized edits. Finally, we propose a novel cross-attention regularizationtechnique that penalizes unrelated cross-attention scores in the RoI during thedenoising steps, ensuring localized edits. Our approach, without re-trainingand fine-tuning, consistently improves the performance of existing methods invarious editing benchmarks.</description><author>Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari</author><pubDate>Thu, 14 Dec 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09256v1</guid></item><item><title>Revisiting Depth Completion from a Stereo Matching Perspective for Cross-domain Generalization</title><link>http://arxiv.org/abs/2312.09254v1</link><description>This paper proposes a new framework for depth completion robust againstdomain-shifting issues. It exploits the generalization capability of modernstereo networks to face depth completion, by processing fictitious stereo pairsobtained through a virtual pattern projection paradigm. Any stereo network ortraditional stereo matcher can be seamlessly plugged into our framework,allowing for the deployment of a virtual stereo setup that is future-proofagainst advancement in the stereo field. Exhaustive experiments on cross-domaingeneralization support our claims. Hence, we argue that our framework can helpdepth completion to reach new deployment scenarios.</description><author>Luca Bartolomei, Matteo Poggi, Andrea Conti, Fabio Tosi, Stefano Mattoccia</author><pubDate>Thu, 14 Dec 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09254v1</guid></item><item><title>FineControlNet: Fine-level Text Control for Image Generation with Spatially Aligned Text Control Injection</title><link>http://arxiv.org/abs/2312.09252v1</link><description>Recently introduced ControlNet has the ability to steer the text-driven imagegeneration process with geometric input such as human 2D pose, or edgefeatures. While ControlNet provides control over the geometric form of theinstances in the generated image, it lacks the capability to dictate the visualappearance of each instance. We present FineControlNet to provide fine controlover each instance's appearance while maintaining the precise pose controlcapability. Specifically, we develop and demonstrate FineControlNet withgeometric control via human pose images and appearance control viainstance-level text prompts. The spatial alignment of instance-specific textprompts and 2D poses in latent space enables the fine control capabilities ofFineControlNet. We evaluate the performance of FineControlNet with rigorouscomparison against state-of-the-art pose-conditioned text-to-image diffusionmodels. FineControlNet achieves superior performance in generating images thatfollow the user-provided instance-specific text prompts and poses compared withexisting methods. Project webpage:https://samsunglabs.github.io/FineControlNet-project-page</description><author>Hongsuk Choi, Isaac Kasahara, Selim Engin, Moritz Graule, Nikhil Chavan-Dafle, Volkan Isler</author><pubDate>Thu, 14 Dec 2023 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09252v1</guid></item><item><title>VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation</title><link>http://arxiv.org/abs/2312.09251v1</link><description>In this work, we introduce Vision-Language Generative Pre-trained Transformer(VL-GPT), a transformer model proficient at concurrently perceiving andgenerating visual and linguistic data. VL-GPT achieves a unified pre-trainingapproach for both image and text modalities by employing a straightforwardauto-regressive objective, thereby enabling the model to process image and textas seamlessly as a language model processes text. To accomplish this, weinitially propose a novel image tokenizer-detokenizer framework for visualdata, specifically designed to transform raw images into a sequence ofcontinuous embeddings and reconstruct them accordingly. In combination with theexisting text tokenizer and detokenizer, this framework allows for the encodingof interleaved image-text data into a multimodal sequence, which cansubsequently be fed into the transformer model. Consequently, VL-GPT canperform large-scale pre-training on multimodal corpora utilizing a unifiedauto-regressive objective (i.e., next-token prediction). Upon completion ofpre-training, VL-GPT exhibits remarkable zero-shot and few-shot performanceacross a diverse range of vision and language understanding and generationtasks, including image captioning, visual question answering, text-to-imagegeneration, and more. Additionally, the pre-trained model retrains in-contextlearning capabilities when provided with multimodal prompts. We further conductinstruction tuning on our VL-GPT, highlighting its exceptional potential formultimodal assistance. The source code and model weights shall be released.</description><author>Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Ying Shan</author><pubDate>Thu, 14 Dec 2023 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09251v1</guid></item><item><title>Single Mesh Diffusion Models with Field Latents for Texture Generation</title><link>http://arxiv.org/abs/2312.09250v1</link><description>We introduce a framework for intrinsic latent diffusion models operatingdirectly on the surfaces of 3D shapes, with the goal of synthesizinghigh-quality textures. Our approach is underpinned by two contributions: fieldlatents, a latent representation encoding textures as discrete vector fields onthe mesh vertices, and field latent diffusion models, which learn to denoise adiffusion process in the learned latent space on the surface. We consider asingle-textured-mesh paradigm, where our models are trained to generatevariations of a given texture on a mesh. We show the synthesized textures areof superior fidelity compared those from existing single-textured-meshgenerative models. Our models can also be adapted for user-controlled editingtasks such as inpainting and label-guided generation. The efficacy of ourapproach is due in part to the equivariance of our proposed framework underisometries, allowing our models to seamlessly reproduce details across locallysimilar regions and opening the door to a notion of generative texturetransfer.</description><author>Thomas W. Mitchel, Carlos Esteves, Ameesh Makadia</author><pubDate>Thu, 14 Dec 2023 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09250v1</guid></item><item><title>ZeroRF: Fast Sparse View 360° Reconstruction with Zero Pretraining</title><link>http://arxiv.org/abs/2312.09249v1</link><description>We present ZeroRF, a novel per-scene optimization method addressing thechallenge of sparse view 360{\deg} reconstruction in neural fieldrepresentations. Current breakthroughs like Neural Radiance Fields (NeRF) havedemonstrated high-fidelity image synthesis but struggle with sparse inputviews. Existing methods, such as Generalizable NeRFs and per-scene optimizationapproaches, face limitations in data dependency, computational cost, andgeneralization across diverse scenarios. To overcome these challenges, wepropose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior intoa factorized NeRF representation. Unlike traditional methods, ZeroRFparametrizes feature grids with a neural network generator, enabling efficientsparse view 360{\deg} reconstruction without any pretraining or additionalregularization. Extensive experiments showcase ZeroRF's versatility andsuperiority in terms of both quality and speed, achieving state-of-the-artresults on benchmark datasets. ZeroRF's significance extends to applications in3D content generation and editing. Project page:https://sarahweiii.github.io/zerorf/</description><author>Ruoxi Shi, Xinyue Wei, Cheng Wang, Hao Su</author><pubDate>Thu, 14 Dec 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09249v1</guid></item><item><title>SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds</title><link>http://arxiv.org/abs/2312.09246v1</link><description>We propose a novel feed-forward 3D editing framework called Shap-Editor.Prior research on editing 3D objects primarily concentrated on editingindividual objects by leveraging off-the-shelf 2D image editing networks. Thisis achieved via a process called distillation, which transfers knowledge fromthe 2D network to 3D assets. Distillation necessitates at least tens of minutesper asset to attain satisfactory editing results, and is thus not verypractical. In contrast, we ask whether 3D editing can be carried out directlyby a feed-forward network, eschewing test-time optimisation. In particular, wehypothesise that editing can be greatly simplified by first encoding 3D objectsin a suitable latent space. We validate this hypothesis by building upon thelatent space of Shap-E. We demonstrate that direct 3D editing in this space ispossible and efficient by building a feed-forward editor network that onlyrequires approximately one second per edit. Our experiments show thatShap-Editor generalises well to both in-distribution and out-of-distribution 3Dassets with different prompts, exhibiting comparable performance with methodsthat carry out test-time optimisation for each edited instance.</description><author>Minghao Chen, Junyu Xie, Iro Laina, Andrea Vedaldi</author><pubDate>Thu, 14 Dec 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09246v1</guid></item><item><title>DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving</title><link>http://arxiv.org/abs/2312.09245v1</link><description>Large language models (LLMs) have opened up new possibilities for intelligentagents, endowing them with human-like thinking and cognitive abilities. In thiswork, we delve into the potential of large language models (LLMs) in autonomousdriving (AD). We introduce DriveMLM, an LLM-based AD framework that can performclose-loop autonomous driving in realistic simulators. To this end, (1) webridge the gap between the language decisions and the vehicle control commandsby standardizing the decision states according to the off-the-shelf motionplanning module. (2) We employ a multi-modal LLM (MLLM) to model the behaviorplanning module of a module AD system, which uses driving rules, user commands,and inputs from various sensors (e.g., camera, lidar) as input and makesdriving decisions and provide explanations; This model can plug-and-play inexisting AD systems such as Apollo for close-loop driving. (3) We design aneffective data engine to collect a dataset that includes decision state andcorresponding explanation annotation for model training and evaluation. Weconduct extensive experiments and show that our model achieves 76.1 drivingscore on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 pointsunder the same settings, demonstrating the effectiveness of our model. We hopethis work can serve as a baseline for autonomous driving with LLMs. Code andmodels shall be released at https://github.com/OpenGVLab/DriveMLM.</description><author>Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, Zhiqi Li, Hao Tian, Lewei Lu, Xizhou Zhu, Xiaogang Wang, Yu Qiao, Jifeng Dai</author><pubDate>Thu, 14 Dec 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09245v1</guid></item><item><title>Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking</title><link>http://arxiv.org/abs/2312.09244v1</link><description>Reward models play a key role in aligning language model applications towardshuman preferences. However, this setup creates an incentive for the languagemodel to exploit errors in the reward model to achieve high estimated reward, aphenomenon often termed \emph{reward hacking}. A natural mitigation is to trainan ensemble of reward models, aggregating over model outputs to obtain a morerobust reward estimate. We explore the application of reward ensembles toalignment at both training time (through reinforcement learning) and inferencetime (through reranking). First, we show that reward models are\emph{underspecified}: reward models that perform similarly in-distribution canyield very different rewards when used in alignment, due to distribution shift.Second, underspecification results in overoptimization, where alignment to onereward model does not improve reward as measured by another reward modeltrained on the same data. Third, overoptimization is mitigated by the use ofreward ensembles, and ensembles that vary by their \emph{pretraining} seedslead to better generalization than ensembles that differ only by their\emph{fine-tuning} seeds, with both outperforming individual reward models.However, even pretrain reward ensembles do not eliminate reward hacking: weshow several qualitative reward hacking phenomena that are not mitigated byensembling because all reward models in the ensemble exhibit similar errorpatterns.</description><author>Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, Peter Shaw, Jonathan Berant</author><pubDate>Thu, 14 Dec 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09244v1</guid></item><item><title>OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural Radiance Fields</title><link>http://arxiv.org/abs/2312.09243v1</link><description>As a fundamental task of vision-based perception, 3D occupancy predictionreconstructs 3D structures of surrounding environments. It provides detailedinformation for autonomous driving planning and navigation. However, mostexisting methods heavily rely on the LiDAR point clouds to generate occupancyground truth, which is not available in the vision-based system. In this paper,we propose an OccNeRF method for self-supervised multi-camera occupancyprediction. Different from bounded 3D occupancy labels, we need to considerunbounded scenes with raw image supervision. To solve the issue, weparameterize the reconstructed occupancy fields and reorganize the samplingstrategy. The neural rendering is adopted to convert occupancy fields tomulti-camera depth maps, supervised by multi-frame photometric consistency.Moreover, for semantic occupancy prediction, we design several strategies topolish the prompts and filter the outputs of a pretrained open-vocabulary 2Dsegmentation model. Extensive experiments for both self-supervised depthestimation and semantic occupancy prediction tasks on nuScenes datasetdemonstrate the effectiveness of our method.</description><author>Chubin Zhang, Juncheng Yan, Yi Wei, Jiaxin Li, Li Liu, Yansong Tang, Yueqi Duan, Jiwen Lu</author><pubDate>Thu, 14 Dec 2023 18:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09243v1</guid></item><item><title>Text2Immersion: Generative Immersive Scene with 3D Gaussians</title><link>http://arxiv.org/abs/2312.09242v1</link><description>We introduce Text2Immersion, an elegant method for producing high-quality 3Dimmersive scenes from text prompts. Our proposed pipeline initiates byprogressively generating a Gaussian cloud using pre-trained 2D diffusion anddepth estimation models. This is followed by a refining stage on the Gaussiancloud, interpolating and refining it to enhance the details of the generatedscene. Distinct from prevalent methods that focus on single object or indoorscenes, or employ zoom-out trajectories, our approach generates diverse sceneswith various objects, even extending to the creation of imaginary scenes.Consequently, Text2Immersion can have wide-ranging implications for variousapplications such as virtual reality, game development, and automated contentcreation. Extensive evaluations demonstrate that our system surpasses othermethods in rendering quality and diversity, further progressing towardstext-driven 3D scene generation. We will make the source code publiclyaccessible at the project page.</description><author>Hao Ouyang, Kathryn Heal, Stephen Lombardi, Tiancheng Sun</author><pubDate>Thu, 14 Dec 2023 18:58:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09242v1</guid></item><item><title>TinyGSM: achieving &gt;80% on GSM8k with small language models</title><link>http://arxiv.org/abs/2312.09241v1</link><description>Small-scale models offer various computational advantages, and yet to whichextent size is critical for problem-solving abilities remains an open question.Specifically for solving grade school math, the smallest model size so farrequired to break the 80\% barrier on the GSM8K benchmark remains to be 34B.Our work studies how high-quality datasets may be the key for small languagemodels to acquire mathematical reasoning. We introduce \texttt{TinyGSM}, asynthetic dataset of 12.3M grade school math problems paired with Pythonsolutions, generated fully by GPT-3.5. After finetuning on \texttt{TinyGSM}, wefind that a duo of a 1.3B generation model and a 1.3B verifier model canachieve 81.5\% accuracy, outperforming existing models that are orders ofmagnitude larger. This also rivals the performance of the GPT-3.5 ``teacher''model (77.4\%), from which our model's training data is generated. Our approachis simple and has two key components: 1) the high-quality dataset\texttt{TinyGSM}, 2) the use of a verifier, which selects the final outputsfrom multiple candidate generations.</description><author>Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, Yi Zhang</author><pubDate>Thu, 14 Dec 2023 18:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09241v1</guid></item><item><title>Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</title><link>http://arxiv.org/abs/2312.09238v1</link><description>Traditional reinforcement-learning-based agents rely on sparse rewards thatoften only use binary values to indicate task completion or failure. Thechallenge in exploration efficiency makes it difficult to effectively learncomplex tasks in Minecraft. To address this, this paper introduces an advancedlearning system, named Auto MC-Reward, that leverages Large Language Models(LLMs) to automatically design dense reward functions, thereby enhancing thelearning efficiency. Auto MC-Reward consists of three important components:Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environmentinformation and task descriptions, the Reward Designer first design the rewardfunction by coding an executable Python function with predefined observationinputs. Then, our Reward Critic will be responsible for verifying the code,checking whether the code is self-consistent and free of syntax and semanticerrors. Further, the Trajectory Analyzer summarizes possible failure causes andprovides refinement suggestions according to collected trajectories. In thenext round, Reward Designer will take further refine and iterate the densereward function based on feedback. Experiments demonstrate a significantimprovement in the success rate and learning efficiency of our agents incomplex tasks in Minecraft, such as obtaining diamond with the efficientability to avoid lava, and efficiently explore trees and animals that aresparse on the plains biome.</description><author>Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</author><pubDate>Thu, 14 Dec 2023 18:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09238v1</guid></item><item><title>Pixel Aligned Language Models</title><link>http://arxiv.org/abs/2312.09237v1</link><description>Large language models have achieved great success in recent years, so astheir variants in vision. Existing vision-language models can describe imagesin natural languages, answer visual-related questions, or perform complexreasoning about the image. However, it is yet unclear how localization tasks,such as word grounding or referring localization, can be performed using largelanguage models. In this work, we aim to develop a vision-language model thatcan take locations, for example, a set of points or boxes, as either inputs oroutputs. When taking locations as inputs, the model performslocation-conditioned captioning, which generates captions for the indicatedobject or region. When generating locations as outputs, our model regressespixel coordinates for each output word generated by the language model, andthus performs dense word grounding. Our model is pre-trained on the LocalizedNarrative dataset, which contains pixel-word-aligned captioning from humanattention. We show our model can be applied to various location-awarevision-language tasks, including referring localization, location-conditionedcaptioning, and dense object captioning, archiving state-of-the-art performanceon RefCOCO and Visual Genome. Project page: https://jerryxu.net/PixelLLM .</description><author>Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, Cordelia Schmid</author><pubDate>Thu, 14 Dec 2023 18:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09237v1</guid></item><item><title>A framework for conditional diffusion modelling with applications in motif scaffolding for protein design</title><link>http://arxiv.org/abs/2312.09236v1</link><description>Many protein design applications, such as binder or enzyme design, requirescaffolding a structural motif with high precision. Generative modellingparadigms based on denoising diffusion processes emerged as a leading candidateto address this motif scaffolding problem and have shown early experimentalsuccess in some cases. In the diffusion paradigm, motif scaffolding is treatedas a conditional generation task, and several conditional generation protocolswere proposed or imported from the Computer Vision literature. However, most ofthese protocols are motivated heuristically, e.g. via analogies to Langevindynamics, and lack a unifying framework, obscuring connections between thedifferent approaches. In this work, we unify conditional training andconditional sampling procedures under one common framework based on themathematically well-understood Doob's h-transform. This new perspective allowsus to draw connections between existing methods and propose a new variation onexisting conditional training protocols. We illustrate the effectiveness ofthis new protocol in both, image outpainting and motif scaffolding and findthat it outperforms standard methods.</description><author>Kieran Didi, Francisco Vargas, Simon V Mathis, Vincent Dutordoir, Emile Mathieu, Urszula J Komorowska, Pietro Lio</author><pubDate>Thu, 14 Dec 2023 18:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09236v1</guid></item><item><title>Let's do the time-warp-attend: Learning topological invariants of dynamical systems</title><link>http://arxiv.org/abs/2312.09234v1</link><description>Dynamical systems across the sciences, from electrical circuits to ecologicalnetworks, undergo qualitative and often catastrophic changes in behavior,called bifurcations, when their underlying parameters cross a threshold.Existing methods predict oncoming catastrophes in individual systems but areprimarily time-series-based and struggle both to categorize qualitativedynamical regimes across diverse systems and to generalize to real data. Toaddress this challenge, we propose a data-driven, physically-informeddeep-learning framework for classifying dynamical regimes and characterizingbifurcation boundaries based on the extraction of topologically invariantfeatures. We focus on the paradigmatic case of the supercritical Hopfbifurcation, which is used to model periodic dynamics across a wide range ofapplications. Our convolutional attention method is trained with dataaugmentations that encourage the learning of topological invariants which canbe used to detect bifurcation boundaries in unseen systems and to design modelsof biological systems like oscillatory gene regulatory networks. We furtherdemonstrate our method's use in analyzing real data by recovering distinctproliferation and differentiation dynamics along pancreatic endocrinogenesistrajectory in gene expression space based on single-cell data. Our methodprovides valuable insights into the qualitative, long-term behavior of a widerange of dynamical systems, and can detect bifurcations or catastrophictransitions in large-scale physical and biological systems.</description><author>Noa Moriel, Matthew Ricci, Mor Nitzan</author><pubDate>Thu, 14 Dec 2023 18:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09234v1</guid></item><item><title>DVQI: A Multi-task, Hardware-integrated Artificial Intelligence System for Automated Visual Inspection in Electronics Manufacturing</title><link>http://arxiv.org/abs/2312.09232v1</link><description>As electronics manufacturers continue to face pressure to increase productionefficiency amid difficulties with supply chains and labour shortages, manyprinted circuit board assembly (PCBA) manufacturers have begun to invest inautomation and technological innovations to remain competitive. One such methodis to leverage artificial intelligence (AI) to greatly augment existingmanufacturing processes. In this paper, we present the DarwinAI Visual QualityInspection (DVQI) system, a hardware-integration artificial intelligence systemfor the automated inspection of printed circuit board assembly defects in anelectronics manufacturing environment. The DVQI system enables multi-taskinspection via minimal programming and setup for manufacturing engineers whileimproving cycle time relative to manual inspection. We also present a casestudy of the deployed DVQI system's performance and impact for a topelectronics manufacturer.</description><author>Audrey Chung, Francis Li, Jeremy Ward, Andrew Hryniowski, Alexander Wong</author><pubDate>Thu, 14 Dec 2023 18:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09232v1</guid></item><item><title>Reliability in Semantic Segmentation: Can We Use Synthetic Data?</title><link>http://arxiv.org/abs/2312.09231v1</link><description>Assessing the reliability of perception models to covariate shifts andout-of-distribution (OOD) detection is crucial for safety-critical applicationssuch as autonomous vehicles. By nature of the task, however, the relevant datais difficult to collect and annotate. In this paper, we challenge cutting-edgegenerative models to automatically synthesize data for assessing reliability insemantic segmentation. By fine-tuning Stable Diffusion, we perform zero-shotgeneration of synthetic data in OOD domains or inpainted with OOD objects.Synthetic data is employed to provide an initial assessment of pretrainedsegmenters, thereby offering insights into their performance when confrontedwith real edge cases. Through extensive experiments, we demonstrate a highcorrelation between the performance on synthetic data and the performance onreal OOD data, showing the validity approach. Furthermore, we illustrate howsynthetic data can be utilized to enhance the calibration and OOD detectioncapabilities of segmenters.</description><author>Thibaut Loiseau, Tuan-Hung Vu, Mickael Chen, Patrick Pérez, Matthieu Cord</author><pubDate>Thu, 14 Dec 2023 18:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09231v1</guid></item><item><title>Successor Heads: Recurring, Interpretable Attention Heads In The Wild</title><link>http://arxiv.org/abs/2312.09230v1</link><description>In this work we present successor heads: attention heads that incrementtokens with a natural ordering, such as numbers, months, and days. For example,successor heads increment 'Monday' into 'Tuesday'. We explain the successorhead behavior with an approach rooted in mechanistic interpretability, thefield that aims to explain how models complete tasks in human-understandableterms. Existing research in this area has found interpretable language modelcomponents in small toy models. However, results in toy models have not yet ledto insights that explain the internals of frontier models and little iscurrently understood about the internal operations of large language models. Inthis paper, we analyze the behavior of successor heads in large language models(LLMs) and find that they implement abstract representations that are common todifferent architectures. They form in LLMs with as few as 31 millionparameters, and at least as many as 12 billion parameters, such as GPT-2,Pythia, and Llama-2. We find a set of 'mod-10 features' that underlie howsuccessor heads increment in LLMs across different architectures and sizes. Weperform vector arithmetic with these features to edit head behavior and provideinsights into numeric representations within LLMs. Additionally, we study thebehavior of successor heads on natural language data, identifying interpretablepolysemanticity in a Pythia successor head.</description><author>Rhys Gould, Euan Ong, George Ogden, Arthur Conmy</author><pubDate>Thu, 14 Dec 2023 18:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09230v1</guid></item><item><title>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2312.09228v1</link><description>We introduce an approach that creates animatable human avatars from monocularvideos using 3D Gaussian Splatting (3DGS). Existing methods based on neuralradiance fields (NeRFs) achieve high-quality novel-view/novel-pose imagesynthesis but often require days of training, and are extremely slow atinference time. Recently, the community has explored fast grid structures forefficient training of clothed avatars. Albeit being extremely fast at training,these methods can barely achieve an interactive rendering frame rate witharound 15 FPS. In this paper, we use 3D Gaussian Splatting and learn anon-rigid deformation network to reconstruct animatable clothed human avatarsthat can be trained within 30 minutes and rendered at real-time frame rates(50+ FPS). Given the explicit nature of our representation, we furtherintroduce as-isometric-as-possible regularizations on both the Gaussian meanvectors and the covariance matrices, enhancing the generalization of our modelon highly articulated unseen poses. Experimental results show that our methodachieves comparable and even better performance compared to state-of-the-artapproaches on animatable avatar creation from a monocular input, while being400x and 250x faster in training and inference, respectively.</description><author>Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang</author><pubDate>Thu, 14 Dec 2023 18:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09228v1</guid></item><item><title>Gaussian Process Regression under Computational and Epistemic Misspecification</title><link>http://arxiv.org/abs/2312.09225v1</link><description>Gaussian process regression is a classical kernel method for functionestimation and data interpolation. In large data applications, computationalcosts can be reduced using low-rank or sparse approximations of the kernel.This paper investigates the effect of such kernel approximations on theinterpolation error. We introduce a unified framework to analyze Gaussianprocess regression under important classes of computational misspecification:Karhunen-Lo\`eve expansions that result in low-rank kernel approximations,multiscale wavelet expansions that induce sparsity in the covariance matrix,and finite element representations that induce sparsity in the precisionmatrix. Our theory also accounts for epistemic misspecification in the choiceof kernel parameters.</description><author>Daniel Sanz-Alonso, Ruiyi Yang</author><pubDate>Thu, 14 Dec 2023 18:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09225v1</guid></item><item><title>Mosaic-SDF for 3D Generative Models</title><link>http://arxiv.org/abs/2312.09222v1</link><description>Current diffusion or flow-based generative models for 3D shapes divide totwo: distilling pre-trained 2D image diffusion models, and training directly on3D shapes. When training a diffusion or flow models on 3D shapes a crucialdesign choice is the shape representation. An effective shape representationneeds to adhere three design principles: it should allow an efficientconversion of large 3D datasets to the representation form; it should provide agood tradeoff of approximation power versus number of parameters; and it shouldhave a simple tensorial form that is compatible with existing powerful neuralarchitectures. While standard 3D shape representations such as volumetric gridsand point clouds do not adhere to all these principles simultaneously, weadvocate in this paper a new representation that does. We introduce Mosaic-SDF(M-SDF): a simple 3D shape representation that approximates the Signed DistanceFunction (SDF) of a given shape by using a set of local grids spread near theshape's boundary. The M-SDF representation is fast to compute for each shapeindividually making it readily parallelizable; it is parameter efficient as itonly covers the space around the shape's boundary; and it has a simple matrixform, compatible with Transformer-based architectures. We demonstrate theefficacy of the M-SDF representation by using it to train a 3D generative flowmodel including class-conditioned generation with the 3D Warehouse dataset, andtext-to-3D generation using a dataset of about 600k caption-shape pairs.</description><author>Lior Yariv, Omri Puny, Natalia Neverova, Oran Gafni, Yaron Lipman</author><pubDate>Thu, 14 Dec 2023 18:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09222v1</guid></item><item><title>NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning</title><link>http://arxiv.org/abs/2312.09219v1</link><description>Reasoning with knowledge graphs (KGs) has primarily focused on triple-shapedfacts. Recent advancements have been explored to enhance the semantics of thesefacts by incorporating more potent representations, such as hyper-relationalfacts. However, these approaches are limited to \emph{atomic facts}, whichdescribe a single piece of information. This paper extends beyond \emph{atomicfacts} and delves into \emph{nested facts}, represented by quoted triples wheresubjects and objects are triples themselves (e.g., ((\emph{BarackObama},\emph{holds\_position}, \emph{President}), \emph{succeed\_by},(\emph{DonaldTrump}, \emph{holds\_position}, \emph{President}))). These nestedfacts enable the expression of complex semantics like \emph{situations} overtime and \emph{logical patterns} over entities and relations. In response, weintroduce NestE, a novel KG embedding approach that captures the semantics ofboth atomic and nested factual knowledge. NestE represents each atomic fact asa $1\times3$ matrix, and each nested relation is modeled as a $3\times3$ matrixthat rotates the $1\times3$ atomic fact matrix through matrix multiplication.Each element of the matrix is represented as a complex number in thegeneralized 4D hypercomplex space, including (spherical) quaternions,hyperbolic quaternions, and split-quaternions. Through thorough analysis, wedemonstrate the embedding's efficacy in capturing diverse logical patterns overnested facts, surpassing the confines of first-order logic-like expressions.Our experimental results showcase NestE's significant performance gains overcurrent baselines in triple prediction and conditional link prediction. Thecode and pre-trained models are open available athttps://github.com/xiongbo010/NestE.</description><author>Bo Xiong, Mojtaba Nayyeri, Linhao Luo, Zihao Wang, Shirui Pan, Steffen Staab</author><pubDate>Thu, 14 Dec 2023 18:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09219v1</guid></item><item><title>Physics-Informed Quantum Machine Learning for Solving Partial Differential Equations</title><link>http://arxiv.org/abs/2312.09215v1</link><description>In this work, we solve differential equations using quantum Chebyshev featuremaps. We propose a tensor product over a summation of Pauli-Z operators as achange in the measurement observables resulting in improved accuracy andreduced computation time for initial value problems processed by floatingboundary handling. This idea has been tested on solving the complex dynamics ofa Riccati equation as well as on a system of differential equations.Furthermore, a second-order differential equation is investigated in which wepropose adding entangling layers to improve accuracy without increasing thevariational parameters. Additionally, a modified self-adaptivity approach ofphysics-informed neural networks is incorporated to balance the multi-objectiveloss function. Finally, a new quantum circuit structure is proposed toapproximate multivariable functions, tested on solving a 2D Poisson's equation.</description><author>Abhishek Setty, Rasul Abdusalamov, Mikhail Itskov</author><pubDate>Thu, 14 Dec 2023 18:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09215v1</guid></item><item><title>Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models</title><link>http://arxiv.org/abs/2312.09211v1</link><description>Low-precision fine-tuning of language models has gained prominence as acost-effective and energy-efficient approach to deploying large-scale models invarious applications. However, this approach is susceptible to the existence ofoutlier values in activation. The outlier values in the activation cannegatively affect the performance of fine-tuning language models in thelow-precision regime since they affect the scaling factor and thus makerepresenting smaller values harder. This paper investigates techniques formitigating outlier activation in low-precision integer fine-tuning of thelanguage models. Our proposed novel approach enables us to represent theoutlier activation values in 8-bit integers instead of floating-point (FP16)values. The benefit of using integers for outlier values is that it enables usto use operator tiling to avoid performing 16-bit integer matrix multiplicationto address this problem effectively. We provide theoretical analysis andsupporting experiments to demonstrate the effectiveness of our approach inimproving the robustness and performance of low-precision fine-tuned languagemodels.</description><author>Alireza Ghaffari, Justin Yu, Mahsa Ghazvini Nejad, Masoud Asgharian, Boxing Chen, Vahid Partovi Nia</author><pubDate>Thu, 14 Dec 2023 18:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09211v1</guid></item><item><title>WikiMuTe: A web-sourced dataset of semantic descriptions for music audio</title><link>http://arxiv.org/abs/2312.09207v1</link><description>Multi-modal deep learning techniques for matching free-form text with musichave shown promising results in the field of Music Information Retrieval (MIR).Prior work is often based on large proprietary data while publicly availabledatasets are few and small in size. In this study, we present WikiMuTe, a newand open dataset containing rich semantic descriptions of music. The data issourced from Wikipedia's rich catalogue of articles covering musical works.Using a dedicated text-mining pipeline, we extract both long and short-formdescriptions covering a wide range of topics related to music content such asgenre, style, mood, instrumentation, and tempo. To show the use of this data,we train a model that jointly learns text and audio representations andperforms cross-modal retrieval. The model is evaluated on two tasks: tag-basedmusic retrieval and music auto-tagging. The results show that while ourapproach has state-of-the-art performance on multiple tasks, but still observea difference in performance depending on the data used for training.</description><author>Benno Weck, Holger Kirchhoff, Peter Grosche, Xavier Serra</author><pubDate>Thu, 14 Dec 2023 18:38:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09207v1</guid></item><item><title>Measurement in the Age of LLMs: An Application to Ideological Scaling</title><link>http://arxiv.org/abs/2312.09203v1</link><description>Much of social science is centered around terms like ``ideology'' or``power'', which generally elude precise definition, and whose contextualmeanings are trapped in surrounding language. This paper explores the use oflarge language models (LLMs) to flexibly navigate the conceptual clutterinherent to social scientific measurement tasks. We rely on LLMs' remarkablelinguistic fluency to elicit ideological scales of both legislators and text,which accord closely to established methods and our own judgement. A key aspectof our approach is that we elicit such scores directly, instructing the LLM tofurnish numeric scores itself. This approach affords a great deal offlexibility, which we showcase through a variety of different case studies. Ourresults suggest that LLMs can be used to characterize highly subtle and diffusemanifestations of political ideology in text.</description><author>Sean O'Hagan, Aaron Schein</author><pubDate>Thu, 14 Dec 2023 18:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09203v1</guid></item><item><title>Semiparametric Efficient Inference in Adaptive Experiments</title><link>http://arxiv.org/abs/2311.18274v2</link><description>We consider the problem of efficient inference of the Average TreatmentEffect in a sequential experiment where the policy governing the assignment ofsubjects to treatment or control can change over time. We first provide acentral limit theorem for the Adaptive Augmented Inverse-Probability Weightedestimator, which is semiparametric efficient, under weaker assumptions thanthose previously made in the literature. This central limit theorem enablesefficient inference at fixed sample sizes. We then consider a sequentialinference setting, deriving both asymptotic and nonasymptotic confidencesequences that are considerably tighter than previous methods. Theseanytime-valid methods enable inference under data-dependent stopping times(sample sizes). Additionally, we use propensity score truncation techniquesfrom the recent off-policy estimation literature to reduce the finite samplevariance of our estimator without affecting the asymptotic variance. Empiricalresults demonstrate that our methods yield narrower confidence sequences thanthose previously developed in the literature while maintaining time-uniformerror control.</description><author>Thomas Cook, Alan Mishler, Aaditya Ramdas</author><pubDate>Thu, 14 Dec 2023 18:24:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18274v2</guid></item><item><title>Weaving Pathways for Justice with GPT: LLM-driven automated drafting of interactive legal applications</title><link>http://arxiv.org/abs/2312.09198v1</link><description>Can generative AI help us speed up the authoring of tools to helpself-represented litigants? In this paper, we describe 3 approaches to automating the completion of courtforms: a generative AI approach that uses GPT-3 to iteratively prompt the userto answer questions, a constrained template-driven approach that usesGPT-4-turbo to generate a draft of questions that are subject to human review,and a hybrid method. We use the open source Docassemble platform in all 3experiments, together with a tool created at Suffolk University Law Schoolcalled the Assembly Line Weaver. We conclude that the hybrid model ofconstrained automated drafting with human review is best suited to the task ofauthoring guided interviews.</description><author>Quinten Steenhuis, David Colarusso, Bryce Willey</author><pubDate>Thu, 14 Dec 2023 18:20:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09198v1</guid></item><item><title>DIRECT: Deep Active Learning under Imbalance and Label Noise</title><link>http://arxiv.org/abs/2312.09196v1</link><description>Class imbalance is a prevalent issue in real world machine learningapplications, often leading to poor performance in rare and minority classes.With an abundance of wild unlabeled data, active learning is perhaps the mosteffective technique in solving the problem at its root -- collecting a morebalanced and informative set of labeled examples during annotation. In thiswork, we propose a novel algorithm that first identifies the class separationthreshold and then annotate the most uncertain examples from the minorityclasses, close to the separation threshold. Through a novel reduction toone-dimensional active learning, our algorithm DIRECT is able to leverage theclassic active learning literature to address issues such as batch labeling andtolerance towards label noise. Compared to existing algorithms, our algorithmsaves more than 15\% of the annotation budget compared to state-of-art activelearning algorithm and more than 90\% of annotation budget compared to randomsampling.</description><author>Shyam Nuggehalli, Jifan Zhang, Lalit Jain, Robert Nowak</author><pubDate>Thu, 14 Dec 2023 18:18:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09196v1</guid></item><item><title>Fast Sampling via De-randomization for Discrete Diffusion Models</title><link>http://arxiv.org/abs/2312.09193v1</link><description>Diffusion models have emerged as powerful tools for high-quality datageneration, such as image generation. Despite its success in continuous spaces,discrete diffusion models, which apply to domains such as texts and naturallanguages, remain under-studied and often suffer from slow generation speed. Inthis paper, we propose a novel de-randomized diffusion process, which leads toan accelerated algorithm for discrete diffusion models. Our techniquesignificantly reduces the number of function evaluations (i.e., calls to theneural network), making the sampling process much faster. Furthermore, weintroduce a continuous-time (i.e., infinite-step) sampling algorithm that canprovide even better sample qualities than its discrete-time (finite-step)counterpart. Extensive experiments on natural language generation and machinetranslation tasks demonstrate the superior performance of our method in termsof both generation speed and sample quality over existing methods for discretediffusion models.</description><author>Zixiang Chen, Huizhuo Yuan, Yongqian Li, Yiwen Kou, Junkai Zhang, Quanquan Gu</author><pubDate>Thu, 14 Dec 2023 18:14:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09193v1</guid></item><item><title>Vision-Language Models as a Source of Rewards</title><link>http://arxiv.org/abs/2312.09187v1</link><description>Building generalist agents that can accomplish many goals in rich open-endedenvironments is one of the research frontiers for reinforcement learning. A keylimiting factor for building generalist agents with RL has been the need for alarge number of reward functions for achieving different goals. We investigatethe feasibility of using off-the-shelf vision-language models, or VLMs, assources of rewards for reinforcement learning agents. We show how rewards forvisual achievement of a variety of language goals can be derived from the CLIPfamily of models, and used to train RL agents that can achieve a variety oflanguage goals. We showcase this approach in two distinct visual domains andpresent a scaling trend showing how larger VLMs lead to more accurate rewardsfor visual goal achievement, which in turn produces more capable RL agents.</description><author>Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris Chan, Gheorghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin, Clare Lyle, Hussain Masoom, Kay McKinney, Volodymyr Mnih, Alexander Neitz, Fabio Pardo, Jack Parker-Holder, John Quan, Tim Rocktäschel, Himanshu Sahni, Tom Schaul, Yannick Schroecker, Stephen Spencer, Richie Steigerwald, Luyu Wang, Lei Zhang</author><pubDate>Thu, 14 Dec 2023 18:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09187v1</guid></item><item><title>CLIP in Medical Imaging: A Comprehensive Survey</title><link>http://arxiv.org/abs/2312.07353v2</link><description>Contrastive Language-Image Pre-training (CLIP), a simple yet effectivepre-training paradigm, successfully introduces text supervision to visionmodels. It has shown promising results across various tasks, attributable toits generalizability and interpretability. The use of CLIP has recently gainedincreasing interest in the medical imaging domain, serving both as apre-training paradigm for aligning medical vision and language, and as acritical component in diverse clinical tasks. With the aim of facilitating adeeper understanding of this promising direction, this survey offers anin-depth exploration of the CLIP paradigm within the domain of medical imaging,regarding both refined CLIP pre-training and CLIP-driven applications. In thisstudy, We (1) start with a brief introduction to the fundamentals of CLIPmethodology. (2) Then, we investigate the adaptation of CLIP pre-training inthe medical domain, focusing on how to optimize CLIP given characteristics ofmedical images and reports. (3) Furthermore, we explore the practicalutilization of CLIP pre-trained models in various tasks, includingclassification, dense prediction, and cross-modal tasks. (4) Finally, wediscuss existing limitations of CLIP in the context of medical imaging andpropose forward-looking directions to address the demands of medical imagingdomain. We expect that this comprehensive survey will provide researchers inthe field of medical image analysis with a holistic understanding of the CLIPparadigm and its potential implications. The project page can be found onhttps://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.</description><author>Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, Dinggang Shen</author><pubDate>Thu, 14 Dec 2023 18:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07353v2</guid></item><item><title>Learning to Optimize Permutation Flow Shop Scheduling via Graph-based Imitation Learning</title><link>http://arxiv.org/abs/2210.17178v2</link><description>The permutation flow shop scheduling (PFSS), aiming at finding the optimalpermutation of jobs, is widely used in manufacturing systems. When solvinglarge-scale PFSS problems, traditional optimization algorithms such asheuristics could hardly meet the demands of both solution accuracy andcomputational efficiency, thus learning-based methods have recently garneredmore attention. Some work attempts to solve the problems by reinforcementlearning methods, which suffer from slow convergence issues during training andare still not accurate enough regarding the solutions. To that end, we proposeto train the model via expert-driven imitation learning, which acceleratesconvergence more stably and accurately. Moreover, in order to extract betterfeature representations of input jobs, we incorporate the graph structure asthe encoder. The extensive experiments reveal that our proposed model obtainssignificant promotion and presents excellent generalizability in large-scaleproblems with up to 1000 jobs. Compared to the state-of-the-art reinforcementlearning method, our model's network parameters are reduced to only 37\% oftheirs, and the solution gap of our model towards the expert solutionsdecreases from 6.8\% to 1.3\% on average. The code is available at:\url{https://github.com/longkangli/PFSS-IL}.</description><author>Longkang Li, Siyuan Liang, Zihao Zhu, Chris Ding, Hongyuan Zha, Baoyuan Wu</author><pubDate>Thu, 14 Dec 2023 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.17178v2</guid></item><item><title>Improving Efficiency of Diffusion Models via Multi-Stage Framework and Tailored Multi-Decoder Architectures</title><link>http://arxiv.org/abs/2312.09181v1</link><description>Diffusion models, emerging as powerful deep generative tools, excel invarious applications. They operate through a two-steps process: introducingnoise into training samples and then employing a model to convert random noiseinto new samples (e.g., images). However, their remarkable generativeperformance is hindered by slow training and sampling. This is due to thenecessity of tracking extensive forward and reverse diffusion trajectories, andemploying a large model with numerous parameters across multiple timesteps(i.e., noise levels). To tackle these challenges, we present a multi-stageframework inspired by our empirical findings. These observations indicate theadvantages of employing distinct parameters tailored to each timestep whileretaining universal parameters shared across all time steps. Our approachinvolves segmenting the time interval into multiple stages where we employcustom multi-decoder U-net architecture that blends time-dependent models witha universally shared encoder. Our framework enables the efficient distributionof computational resources and mitigates inter-stage interference, whichsubstantially improves training efficiency. Extensive numerical experimentsaffirm the effectiveness of our framework, showcasing significant training andsampling efficiency enhancements on three state-of-the-art diffusion models,including large-scale latent diffusion models. Furthermore, our ablationstudies illustrate the impact of two important components in our framework: (i)a novel timestep clustering algorithm for stage division, and (ii) aninnovative multi-decoder U-net architecture, seamlessly integrating universaland customized hyperparameters.</description><author>Huijie Zhang, Yifu Lu, Ismail Alkhouri, Saiprasad Ravishankar, Dogyoon Song, Qing Qu</author><pubDate>Thu, 14 Dec 2023 17:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09181v1</guid></item><item><title>Reconstruction of Fields from Sparse Sensing: Differentiable Sensor Placement Enhances Generalization</title><link>http://arxiv.org/abs/2312.09176v1</link><description>Recreating complex, high-dimensional global fields from limited data pointsis a grand challenge across various scientific and industrial domains. Giventhe prohibitive costs of specialized sensors and the frequent inaccessibilityof certain regions of the domain, achieving full field coverage is typicallynot feasible. Therefore, the development of algorithms that intelligentlyimprove sensor placement is of significant value. In this study, we introduce ageneral approach that employs differentiable programming to exploit sensorplacement within the training of a neural network model in order to improvefield reconstruction. We evaluated our method using two distinct datasets; theresults show that our approach improved test scores. Ultimately, our method ofdifferentiable placement strategies has the potential to significantly increasedata collection efficiency, enable more thorough area coverage, and reduceredundancy in sensor deployment.</description><author>Agnese Marcato, Daniel O'Malley, Hari Viswanathan, Eric Guiltinan, Javier E. Santos</author><pubDate>Thu, 14 Dec 2023 17:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09176v1</guid></item><item><title>Towards Efficient Quantum Anomaly Detection: One-Class SVMs using Variable Subsampling and Randomized Measurements</title><link>http://arxiv.org/abs/2312.09174v1</link><description>Quantum computing, with its potential to enhance various machine learningtasks, allows significant advancements in kernel calculation and modelprecision. Utilizing the one-class Support Vector Machine alongside a quantumkernel, known for its classically challenging representational capacity,notable improvements in average precision compared to classical counterpartswere observed in previous studies. Conventional calculations of these kernels,however, present a quadratic time complexity concerning data size, posingchallenges in practical applications. To mitigate this, we explore two distinctapproaches: utilizing randomized measurements to evaluate the quantum kerneland implementing the variable subsampling ensemble method, both targetinglinear time complexity. Experimental results demonstrate a substantialreduction in training and inference times by up to 95\% and 25\% respectively,employing these methods. Although unstable, the average precision of randomizedmeasurements discernibly surpasses that of the classical Radial Basis Functionkernel, suggesting a promising direction for further research in scalable,efficient quantum computing applications in machine learning.</description><author>Michael Kölle, Afrae Ahouzi, Pascal Debus, Robert Müller, Danielle Schuman, Claudia Linnhoff-Popien</author><pubDate>Thu, 14 Dec 2023 17:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09174v1</guid></item><item><title>DiffusionLight: Light Probes for Free by Painting a Chrome Ball</title><link>http://arxiv.org/abs/2312.09168v1</link><description>We present a simple yet effective technique to estimate lighting in a singleinput image. Current techniques rely heavily on HDR panorama datasets to trainneural networks to regress an input with limited field-of-view to a fullenvironment map. However, these approaches often struggle with real-world,uncontrolled settings due to the limited diversity and size of their datasets.To address this problem, we leverage diffusion models trained on billions ofstandard images to render a chrome ball into the input image. Despite itssimplicity, this task remains challenging: the diffusion models often insertincorrect or inconsistent objects and cannot readily generate images in HDRformat. Our research uncovers a surprising relationship between the appearanceof chrome balls and the initial diffusion noise map, which we utilize toconsistently generate high-quality chrome balls. We further fine-tune an LDRdifusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposurebracketing for HDR light estimation. Our method produces convincing lightestimates across diverse settings and demonstrates superior generalization toin-the-wild scenarios.</description><author>Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn</author><pubDate>Thu, 14 Dec 2023 17:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09168v1</guid></item><item><title>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</title><link>http://arxiv.org/abs/2311.18259v2</link><description>We present Ego-Exo4D, a diverse, large-scale multimodal multiview videodataset and benchmark challenge. Ego-Exo4D centers aroundsimultaneously-captured egocentric and exocentric video of skilled humanactivities (e.g., sports, music, dance, bike repair). More than 800participants from 13 cities worldwide performed these activities in 131different natural scene contexts, yielding long-form captures from 1 to 42minutes each and 1,422 hours of video combined. The multimodal nature of thedataset is unprecedented: the video is accompanied by multichannel audio, eyegaze, 3D point clouds, camera poses, IMU, and multiple paired languagedescriptions -- including a novel "expert commentary" done by coaches andteachers and tailored to the skilled-activity domain. To push the frontier offirst-person video understanding of skilled human activity, we also present asuite of benchmark tasks and their annotations, including fine-grained activityunderstanding, proficiency estimation, cross-view translation, and 3D hand/bodypose. All resources will be open sourced to fuel new research in the community.</description><author>Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Wesli</author><pubDate>Thu, 14 Dec 2023 17:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18259v2</guid></item><item><title>Approximation Algorithms for Preference Aggregation Using CP-Nets</title><link>http://arxiv.org/abs/2312.09162v1</link><description>This paper studies the design and analysis of approximation algorithms foraggregating preferences over combinatorial domains, represented usingConditional Preference Networks (CP-nets). Its focus is on aggregatingpreferences over so-called \emph{swaps}, for which optimal solutions in generalare already known to be of exponential size. We first analyze a trivial2-approximation algorithm that simply outputs the best of the given inputpreferences, and establish a structural condition under which the approximationratio of this algorithm is improved to $4/3$. We then propose a polynomial-timeapproximation algorithm whose outputs are provably no worse than those of thetrivial algorithm, but often substantially better. A family of probleminstances is presented for which our improved algorithm produces optimalsolutions, while, for any $\varepsilon$, the trivial algorithm can\emph{not}\/attain a $(2-\varepsilon)$-approximation. These results may lead to the firstpolynomial-time approximation algorithm that solves the CP-net aggregationproblem for swaps with an approximation ratio substantially better than $2$.</description><author>Abu Mohammmad Hammad Ali, Boting Yang, Sandra Zilles</author><pubDate>Thu, 14 Dec 2023 17:31:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09162v1</guid></item><item><title>Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques</title><link>http://arxiv.org/abs/2307.12971v3</link><description>This article intends to systematically identify and comparatively analyzestate-of-the-art supply chain (SC) forecasting strategies and technologies. Anovel framework has been proposed incorporating Big Data Analytics in SCManagement (problem identification, data sources, exploratory data analysis,machine-learning model training, hyperparameter tuning, performance evaluation,and optimization), forecasting effects on human-workforce, inventory, andoverall SC. Initially, the need to collect data according to SC strategy andhow to collect them has been discussed. The article discusses the need fordifferent types of forecasting according to the period or SC objective. The SCKPIs and the error-measurement systems have been recommended to optimize thetop-performing model. The adverse effects of phantom inventory on forecastingand the dependence of managerial decisions on the SC KPIs for determining modelperformance parameters and improving operations management, transparency, andplanning efficiency have been illustrated. The cyclic connection within theframework introduces preprocessing optimization based on the post-process KPIs,optimizing the overall control process (inventory management, workforcedetermination, cost, production and capacity planning). The contribution ofthis research lies in the standard SC process framework proposal, recommendedforecasting data analysis, forecasting effects on SC performance, machinelearning algorithms optimization followed, and in shedding light on futureresearch.</description><author>Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed Ridoy, Yoichi Tomioka, M. F. Mridha</author><pubDate>Thu, 14 Dec 2023 17:30:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12971v3</guid></item><item><title>WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets From Aerial Views</title><link>http://arxiv.org/abs/2312.09159v1</link><description>We present the Wildland-fire Infrared Thermal (WIT-UAS) dataset for long-waveinfrared sensing of crew and vehicle assets amidst prescribed wildland fireenvironments. While such a dataset is crucial for safety monitoring in wildlandfire applications, to the authors' awareness, no such dataset focusing onassets near fire is publicly available. Presumably, this is due to the barrierto entry of collaborating with fire management personnel. We present tworelated data subsets: WIT-UAS-ROS consists of full ROS bag files containingsensor and robot data of UAS flight over the fire, and WIT-UAS-Image containshand-labeled long-wave infrared (LWIR) images extracted from WIT-UAS-ROS. Ourdataset is the first to focus on asset detection in a wildland fireenvironment. We show that thermal detection models trained without fire datafrequently detect false positives by classifying fire as people. By adding ourdataset to training, we show that the false positive rate is reducedsignificantly. Yet asset detection in wildland fire environments is stillsignificantly more challenging than detection in urban environments, due todense obscuring trees, greater heat variation, and overbearing thermal signalof the fire. We publicize this dataset to encourage the community to study moreadvanced models to tackle this challenging environment. The dataset, code andpretrained models are available at\url{https://github.com/castacks/WIT-UAS-Dataset}.</description><author>Andrew Jong, Mukai Yu, Devansh Dhrafani, Siva Kailas, Brady Moon, Katia Sycara, Sebastian Scherer</author><pubDate>Thu, 14 Dec 2023 17:29:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09159v1</guid></item><item><title>General Object Foundation Model for Images and Videos at Scale</title><link>http://arxiv.org/abs/2312.09158v1</link><description>We present GLEE in this work, an object-level foundation model for locatingand identifying objects in images and videos. Through a unified framework, GLEEaccomplishes detection, segmentation, tracking, grounding, and identificationof arbitrary objects in the open world scenario for various object perceptiontasks. Adopting a cohesive learning strategy, GLEE acquires knowledge fromdiverse data sources with varying supervision levels to formulate generalobject representations, excelling in zero-shot transfer to new data and tasks.Specifically, we employ an image encoder, text encoder, and visual prompter tohandle multi-modal inputs, enabling to simultaneously solve variousobject-centric downstream tasks while maintaining state-of-the-art performance.Demonstrated through extensive training on over five million images fromdiverse benchmarks, GLEE exhibits remarkable versatility and improvedgeneralization performance, efficiently tackling downstream tasks without theneed for task-specific adaptation. By integrating large volumes ofautomatically labeled data, we further enhance its zero-shot generalizationcapabilities. Additionally, GLEE is capable of being integrated into LargeLanguage Models, serving as a foundational model to provide universalobject-level information for multi-modal tasks. We hope that the versatilityand universality of our method will mark a significant step in the developmentof efficient visual foundation models for AGI systems. The model and code willbe released at https://glee-vision.github.io .</description><author>Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai</author><pubDate>Thu, 14 Dec 2023 17:26:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09158v1</guid></item><item><title>CMG-Net: Robust Normal Estimation for Point Clouds via Chamfer Normal Distance and Multi-scale Geometry</title><link>http://arxiv.org/abs/2312.09154v1</link><description>This work presents an accurate and robust method for estimating normals frompoint clouds. In contrast to predecessor approaches that minimize thedeviations between the annotated and the predicted normals directly, leading todirection inconsistency, we first propose a new metric termed Chamfer NormalDistance to address this issue. This not only mitigates the challenge but alsofacilitates network training and substantially enhances the network robustnessagainst noise. Subsequently, we devise an innovative architecture thatencompasses Multi-scale Local Feature Aggregation and Hierarchical GeometricInformation Fusion. This design empowers the network to capture intricategeometric details more effectively and alleviate the ambiguity in scaleselection. Extensive experiments demonstrate that our method achieves thestate-of-the-art performance on both synthetic and real-world datasets,particularly in scenarios contaminated by noise. Our implementation isavailable at https://github.com/YingruiWoo/CMG-Net_Pytorch.</description><author>Yingrui Wu, Mingyang Zhao, Keqiang Li, Weize Quan, Tianqi Yu, Jianfeng Yang, Xiaohong Jia, Dong-Ming Yan</author><pubDate>Thu, 14 Dec 2023 17:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09154v1</guid></item><item><title>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</title><link>http://arxiv.org/abs/2309.15112v5</link><description>We propose InternLM-XComposer, a vision-language large model that enablesadvanced image-text comprehension and composition. The innovative nature of ourmodel is highlighted by three appealing properties: 1) Interleaved Text-ImageComposition: InternLM-XComposer can effortlessly generate coherent andcontextual articles that seamlessly integrate images, providing a more engagingand immersive reading experience. Simply provide a writing instruction, and oursystem will generate the corresponding manuscript. It can intelligentlyidentify the areas in the text where images would enhance the content andautomatically insert the most appropriate visual candidates. 2) Comprehensionwith Rich Multilingual Knowledge: The text-image comprehension is empowered bytraining on an extensive multi-modal multilingual database with carefullycrafted strategies, resulting in a deep understanding of visual content. 3)State-of-the-art Performance: Our model consistently achieves state-of-the-artresults across various mainstream benchmarks for vision-language foundationalmodels, including MME Benchmark, MMBench, MMBench-CN, Seed-Bench, CCBench(Chinese Cultural Benchmark), QBench and Tiny LVLM. Owing to the absence ofestablished metrics for quantitatively assessing text-image composition, wehave devised a robust evaluation procedure that comprises both human andGPT4-Vision (GPT4-V) to ensure reliability. Notably, our InternLM-XComposerachieves competitive text-image composition scores compared to publicsolutions, including GPT4-V and GPT3.5. Collectively, InternLM-XComposerseamlessly blends advanced text-image comprehension and composition,revolutionizing vision-language interaction and offering new insights andopportunities. The InternLM-XComposer model series are publicly available athttps://github.com/InternLM/InternLM-XComposer.</description><author>Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang</author><pubDate>Thu, 14 Dec 2023 17:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15112v5</guid></item><item><title>Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context</title><link>http://arxiv.org/abs/2312.06528v2</link><description>Many neural network architectures have been shown to be Turing Complete, andcan thus implement arbitrary algorithms. However, Transformers are unique inthat they can implement gradient-based learning algorithms \emph{under simpleparameter configurations}. A line of recent work shows that linear Transformersnaturally learn to implement gradient descent (GD) when trained on a linearregression in-context learning task. But the linearity assumption (either inthe Transformer architecture or in the learning task) is far from realisticsettings where non-linear activations crucially enable Transformers to learncomplicated non-linear functions. In this paper, we provide theoretical andempirical evidence that non-linear Transformers can, and \emph{in fact do},learn to implement learning algorithms to learn non-linear functions incontext. Our results apply to a broad class of combinations of non-lineararchitectures, and non-linear in-context learning tasks. Interestingly, we showthat the optimal choice of non-linear activation depends in a natural way onthe non-linearity of the learning task.</description><author>Xiang Cheng, Yuxin Chen, Suvrit Sra</author><pubDate>Thu, 14 Dec 2023 17:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06528v2</guid></item><item><title>Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting</title><link>http://arxiv.org/abs/2312.09148v1</link><description>Uncertainty estimation is crucial for machine learning models to detectout-of-distribution (OOD) inputs. However, the conventional discriminative deeplearning classifiers produce uncalibrated closed-set predictions for OOD data.A more robust classifiers with the uncertainty estimation typically require apotentially unavailable OOD dataset for outlier exposure training, or aconsiderable amount of additional memory and compute to build ensemble models.In this work, we improve on uncertainty estimation without extra OOD data oradditional inference costs using an alternative Split-Ensemble method.Specifically, we propose a novel subtask-splitting ensemble training objective,where a common multiclass classification task is split into severalcomplementary subtasks. Then, each subtask's training data can be considered asOOD to the other subtasks. Diverse submodels can therefore be trained on eachsubtask with OOD-aware objectives. The subtask-splitting objective enables usto share low-level features across submodels to avoid parameter andcomputational overheads. In particular, we build a tree-like Split-Ensemblearchitecture by performing iterative splitting and pruning from a sharedbackbone model, where each branch serves as a submodel corresponding to asubtask. This leads to improved accuracy and uncertainty estimation acrosssubmodels under a fixed ensemble computation budget. Empirical study withResNet-18 backbone shows Split-Ensemble, without additional computation cost,improves accuracy over a single model by 0.8%, 1.8%, and 25.5% on CIFAR-10,CIFAR-100, and Tiny-ImageNet, respectively. OOD detection for the same backboneand in-distribution datasets surpasses a single model baseline by,correspondingly, 2.2%, 8.1%, and 29.6% mean AUROC. Codes will be publiclyavailable at https://antonioo-c.github.io/projects/split-ensemble</description><author>Anthony Chen, Huanrui Yang, Yulu Gan, Denis A Gudovskiy, Zhen Dong, Haofan Wang, Tomoyuki Okuno, Yohei Nakata, Shanghang Zhang, Kurt Keutzer</author><pubDate>Thu, 14 Dec 2023 17:18:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09148v1</guid></item><item><title>Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers</title><link>http://arxiv.org/abs/2312.09147v1</link><description>Recent advancements in 3D reconstruction from single images have been drivenby the evolution of generative models. Prominent among these are methods basedon Score Distillation Sampling (SDS) and the adaptation of diffusion models inthe 3D domain. Despite their progress, these techniques often face limitationsdue to slow optimization or rendering processes, leading to extensive trainingand optimization times. In this paper, we introduce a novel approach forsingle-view reconstruction that efficiently generates a 3D model from a singleimage via feed-forward inference. Our method utilizes two transformer-basednetworks, namely a point decoder and a triplane decoder, to reconstruct 3Dobjects using a hybrid Triplane-Gaussian intermediate representation. Thishybrid representation strikes a balance, achieving a faster rendering speedcompared to implicit representations while simultaneously delivering superiorrendering quality than explicit representations. The point decoder is designedfor generating point clouds from single images, offering an explicitrepresentation which is then utilized by the triplane decoder to query Gaussianfeatures for each point. This design choice addresses the challenges associatedwith directly regressing explicit 3D Gaussian attributes characterized by theirnon-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP toenable rapid rendering through splatting. Both decoders are built upon ascalable, transformer-based architecture and have been efficiently trained onlarge-scale 3D datasets. The evaluations conducted on both synthetic datasetsand real-world images demonstrate that our method not only achieves higherquality but also ensures a faster runtime in comparison to previousstate-of-the-art techniques. Please see our project page athttps://zouzx.github.io/TriplaneGaussian/.</description><author>Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, Song-Hai Zhang</author><pubDate>Thu, 14 Dec 2023 17:18:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09147v1</guid></item><item><title>CSGNN: Conquering Noisy Node labels via Dynamic Class-wise Selection</title><link>http://arxiv.org/abs/2311.11473v2</link><description>Graph Neural Networks (GNNs) have emerged as a powerful tool forrepresentation learning on graphs, but they often suffer from overfitting andlabel noise issues, especially when the data is scarce or imbalanced. Differentfrom the paradigm of previous methods that rely on single-node confidence, inthis paper, we introduce a novel Class-wise Selection for Graph NeuralNetworks, dubbed CSGNN, which employs a neighbor-aggregated latent space toadaptively select reliable nodes across different classes. Specifically, 1) totackle the class imbalance issue, we introduce a dynamic class-wise selectionmechanism, leveraging the clustering technique to identify clean nodes based onthe neighbor-aggregated confidences. In this way, our approach can avoid thepitfalls of biased sampling which is common with global threshold techniques.2) To alleviate the problem of noisy labels, built on the concept of thememorization effect, CSGNN prioritizes learning from clean nodes before noisyones, thereby iteratively enhancing model performance while mitigating labelnoise. Through extensive experiments, we demonstrate that CSGNN outperformsstate-of-the-art methods in terms of both effectiveness and robustness.</description><author>Yifan Li, Zhen Tan, Kai Shu, Zongsheng Cao, Yu Kong, Huan Liu</author><pubDate>Thu, 14 Dec 2023 17:17:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11473v2</guid></item><item><title>Featuring Koopman Mode Decomposition</title><link>http://arxiv.org/abs/2312.09146v1</link><description>This article introduces an advanced Koopman mode decomposition (KMD)technique -- coined Featurized Koopman Mode Decomposition (FKMD) -- that usestime embedding and Mahalanobis scaling to enhance analysis and prediction ofhigh dimensional dynamical systems. The time embedding expands the observationspace to better capture underlying manifold structure, while the Mahalanobisscaling, applied to kernel or random Fourier features, adjusts observationsbased on the system's dynamics. This aids in featurizing KMD in cases wheregood features are not a priori known. We show that our method improves KMDpredictions for a high dimensional Lorenz attractor and for a cell signalingproblem from cancer research.</description><author>David Aristoff, Jeremy Copperman, Nathan Mankovich, Alexander Davies</author><pubDate>Thu, 14 Dec 2023 17:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09146v1</guid></item><item><title>Class-Wise Buffer Management for Incremental Object Detection: An Effective Buffer Training Strategy</title><link>http://arxiv.org/abs/2312.09139v1</link><description>Class incremental learning aims to solve a problem that arises whencontinuously adding unseen class instances to an existing model This approachhas been extensively studied in the context of image classification; howeverits applicability to object detection is not well established yet. Existingframeworks using replay methods mainly collect replay data without consideringthe model being trained and tend to rely on randomness or the number of labelsof each sample. Also, despite the effectiveness of the replay, it was not yetoptimized for the object detection task. In this paper, we introduce aneffective buffer training strategy (eBTS) that creates the optimized replaybuffer on object detection. Our approach incorporates guarantee minimum andhierarchical sampling to establish the buffer customized to the trained model.%These methods can facilitate effective retrieval of prior knowledge.Furthermore, we use the circular experience replay training to optimallyutilize the accumulated buffer data. Experiments on the MS COCO datasetdemonstrate that our eBTS achieves state-of-the-art performance compared to theexisting replay schemes.</description><author>Junsu Kim, Sumin Hong, Chanwoo Kim, Jihyeon Kim, Yihalem Yimolal Tiruneh, Jeongwan On, Jihyun Song, Sunhwa Choi, Seungryul Baek</author><pubDate>Thu, 14 Dec 2023 17:10:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09139v1</guid></item><item><title>Living Scenes: Multi-object Relocalization and Reconstruction in Changing 3D Environments</title><link>http://arxiv.org/abs/2312.09138v1</link><description>Research into dynamic 3D scene understanding has primarily focused onshort-term change tracking from dense observations, while little attention hasbeen paid to long-term changes with sparse observations. We address this gapwith MoRE, a novel approach for multi-object relocalization and reconstructionin evolving environments. We view these environments as "living scenes" andconsider the problem of transforming scans taken at different points in timeinto a 3D reconstruction of the object instances, whose accuracy andcompleteness increase over time. At the core of our method lies anSE(3)-equivariant representation in a single encoder-decoder network, trainedon synthetic data. This representation enables us to seamlessly tackle instancematching, registration, and reconstruction. We also introduce a jointoptimization algorithm that facilitates the accumulation of point cloudsoriginating from the same instance across multiple scans taken at differentpoints in time. We validate our method on synthetic and real-world data anddemonstrate state-of-the-art performance in both end-to-end performance andindividual subtasks.</description><author>Liyuan Zhu, Shengyu Huang, Konrad Schindler, Iro Armeni</author><pubDate>Thu, 14 Dec 2023 17:09:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09138v1</guid></item><item><title>Physics-Informed Neural Network Lyapunov Functions: PDE Characterization, Learning, and Verification</title><link>http://arxiv.org/abs/2312.09131v1</link><description>We provide a systematic investigation of using physics-informed neuralnetworks to compute Lyapunov functions. We encode Lyapunov conditions as apartial differential equation (PDE) and use this for training neural networkLyapunov functions. We analyze the analytical properties of the solutions tothe Lyapunov and Zubov PDEs. In particular, we show that employing the Zubovequation in training neural Lyapunov functions can lead to approximate regionsof attraction close to the true domain of attraction. We then providesufficient conditions for the learned neural Lyapunov functions that can bereadily verified by satisfiability modulo theories (SMT) solvers, enablingformal verification of both local stability analysis and region-of-attractionestimates in the large. Through a number of nonlinear examples, ranging fromlow to high dimensions, we demonstrate that the proposed framework canoutperform traditional sums-of-squares (SOS) Lyapunov functions obtained usingsemidefinite programming (SDP).</description><author>Jun Liu, Yiming Meng, Maxwell Fitzsimmons, Ruikun Zhou</author><pubDate>Thu, 14 Dec 2023 17:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09131v1</guid></item><item><title>Tokenize Anything via Prompting</title><link>http://arxiv.org/abs/2312.09128v1</link><description>We present a unified, promptable model capable of simultaneously segmenting,recognizing, and captioning anything. Unlike SAM, we aim to build a versatileregion representation in the wild via visual prompting. To achieve this, wetrain a generalizable model with massive segmentation masks, e.g., SA-1B masks,and semantic priors from a pre-trained CLIP model with 5 billion parameters.Specifically, we construct a promptable image decoder by adding a semantictoken to each mask token. The semantic token is responsible for learning thesemantic priors in a predefined concept space. Through joint optimization ofsegmentation on mask tokens and concept prediction on semantic tokens, ourmodel exhibits strong regional recognition and localization capabilities. Forexample, an additional 38M-parameter causal text decoder trained from scratchsets a new record with a CIDEr score of 150.7 on the Visual Genome regioncaptioning task. We believe this model can be a versatile region-level imagetokenizer, capable of encoding general-purpose region context for a broad rangeof perception tasks. Code and models are available athttps://github.com/baaivision/tokenize-anything.</description><author>Ting Pan, Lulu Tang, Xinlong Wang, Shiguang Shan</author><pubDate>Thu, 14 Dec 2023 17:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09128v1</guid></item><item><title>Towards Trustworthy AI Software Development Assistance</title><link>http://arxiv.org/abs/2312.09126v1</link><description>It is expected that in the near future, AI software development assistantswill play an important role in the software industry. However, current softwaredevelopment assistants tend to be unreliable, often producing incorrect,unsafe, or low-quality code. We seek to resolve these issues by introducing aholistic architecture for constructing, training, and using trustworthy AIsoftware development assistants. In the center of the architecture, there is afoundational LLM trained on datasets representative of real-world codingscenarios and complex software architectures, and fine-tuned on code qualitycriteria beyond correctness. The LLM will make use of graph-based coderepresentations for advanced semantic comprehension. We envision a knowledgegraph integrated into the system to provide up-to-date background knowledge andto enable the assistant to provide appropriate explanations. Finally, a modularframework for constrained decoding will ensure that certain guarantees (e.g.,for correctness and security) hold for the generated code.</description><author>Daniel Maninger, Krishna Narasimhan, Mira Mezini</author><pubDate>Thu, 14 Dec 2023 16:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09126v1</guid></item><item><title>Robot Learning with Sensorimotor Pre-training</title><link>http://arxiv.org/abs/2306.10007v2</link><description>We present a self-supervised sensorimotor pre-training approach for robotics.Our model, called RPT, is a Transformer that operates on sequences ofsensorimotor tokens. Given a sequence of camera images, proprioceptive robotstates, and actions, we encode the sequence into tokens, mask out a subset, andtrain a model to predict the missing content from the rest. We hypothesize thatif a robot can predict the masked-out content it will have acquired a goodmodel of the physical world that can enable it to act. RPT is designed tooperate on latent visual representations which makes prediction tractable,enables scaling to larger models, and allows fast inference on a real robot. Toevaluate our approach, we collected a dataset of 20,000 real-world trajectoriesover 9 months using a combination of motion planning and grasping algorithms.We find that sensorimotor pre-training consistently outperforms training fromscratch, has favorable scaling properties, and enables transfer acrossdifferent tasks, environments, and robots.</description><author>Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, Jitendra Malik</author><pubDate>Thu, 14 Dec 2023 16:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10007v2</guid></item><item><title>Data Portraits: Recording Foundation Model Training Data</title><link>http://arxiv.org/abs/2303.03919v2</link><description>Foundation models are trained on increasingly immense and opaque datasets.Even while these models are now key in AI system building, it can be difficultto answer the straightforward question: has the model already encountered agiven example during training? We therefore propose a widespread adoption ofData Portraits: artifacts that record training data and allow for downstreaminspection. First we outline the properties of such an artifact and discuss howexisting solutions can be used to increase transparency. We then propose andimplement a solution based on data sketching, stressing fast and spaceefficient querying. Using our tools, we document a popular language modelingcorpus (The Pile) and a recently released code modeling dataset (The Stack). Weshow that our solution enables answering questions about test set leakage andmodel plagiarism. Our tool is lightweight and fast, costing only 3% of thedataset size in overhead. We release a live interface of our tools athttps://dataportraits.org/ and call on dataset and model creators to releaseData Portraits as a complement to current documentation practices.</description><author>Marc Marone, Benjamin Van Durme</author><pubDate>Thu, 14 Dec 2023 16:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.03919v2</guid></item><item><title>Does provable absence of barren plateaus imply classical simulability? Or, why we need to rethink variational quantum computing</title><link>http://arxiv.org/abs/2312.09121v1</link><description>A large amount of effort has recently been put into understanding the barrenplateau phenomenon. In this perspective article, we face the increasingly loudelephant in the room and ask a question that has been hinted at by many but notexplicitly addressed: Can the structure that allows one to avoid barrenplateaus also be leveraged to efficiently simulate the loss classically? Wepresent strong evidence that commonly used models with provable absence ofbarren plateaus are also classically simulable, provided that one can collectsome classical data from quantum devices during an initial data acquisitionphase. This follows from the observation that barren plateaus result from acurse of dimensionality, and that current approaches for solving them end upencoding the problem into some small, classically simulable, subspaces. Thissheds serious doubt on the non-classicality of the information processingcapabilities of parametrized quantum circuits for barren plateau-freelandscapes and on the possibility of superpolynomial advantages from runningthem on quantum hardware. We end by discussing caveats in our arguments, therole of smart initializations, and by highlighting new opportunities that ourperspective raises.</description><author>M. Cerezo, Martin Larocca, Diego García-Martín, N. L. Diaz, Paolo Braccia, Enrico Fontana, Manuel S. Rudolph, Pablo Bermejo, Aroosa Ijaz, Supanut Thanasilp, Eric R. Anschuetz, Zoë Holmes</author><pubDate>Thu, 14 Dec 2023 16:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09121v1</guid></item><item><title>Less is more -- the Dispatcher/ Executor principle for multi-task Reinforcement Learning</title><link>http://arxiv.org/abs/2312.09120v1</link><description>Humans instinctively know how to neglect details when it comes to solvecomplex decision making problems in environments with unforeseeable variations.This abstraction process seems to be a vital property for most biologicalsystems and helps to 'abstract away' unnecessary details and boostgeneralisation. In this work we introduce the dispatcher/ executor principlefor the design of multi-task Reinforcement Learning controllers. It suggests topartition the controller in two entities, one that understands the task (thedispatcher) and one that computes the controls for the specific device (theexecutor) - and to connect these two by a strongly regularizing communicationchannel. The core rationale behind this position paper is that changes instructure and design principles can improve generalisation properties anddrastically enforce data-efficiency. It is in some sense a 'yes, and ...'response to the current trend of using large neural networks trained on vastamounts of data and bet on emerging generalisation properties. While we agreeon the power of scaling - in the sense of Sutton's 'bitter lesson' - we willgive some evidence, that considering structure and adding design principles canbe a valuable and critical component in particular when data is not abundantand infinite, but is a precious resource.</description><author>Martin Riedmiller, Tim Hertweck, Roland Hafner</author><pubDate>Thu, 14 Dec 2023 16:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09120v1</guid></item><item><title>VideoLCM: Video Latent Consistency Model</title><link>http://arxiv.org/abs/2312.09109v1</link><description>Consistency models have demonstrated powerful capability in efficient imagegeneration and allowed synthesis within a few sampling steps, alleviating thehigh computational cost in diffusion models. However, the consistency model inthe more challenging and resource-consuming video generation is still lessexplored. In this report, we present the VideoLCM framework to fill this gap,which leverages the concept of consistency models from image generation toefficiently synthesize videos with minimal steps while maintaining highquality. VideoLCM builds upon existing latent video diffusion models andincorporates consistency distillation techniques for training the latentconsistency model. Experimental results reveal the effectiveness of ourVideoLCM in terms of computational efficiency, fidelity and temporalconsistency. Notably, VideoLCM achieves high-fidelity and smooth videosynthesis with only four sampling steps, showcasing the potential for real-timesynthesis. We hope that VideoLCM can serve as a simple yet effective baselinefor subsequent research. The source code and models will be publicly available.</description><author>Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, Nong Sang</author><pubDate>Thu, 14 Dec 2023 16:45:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09109v1</guid></item><item><title>Greedy Shapley Client Selection for Communication-Efficient Federated Learning</title><link>http://arxiv.org/abs/2312.09108v1</link><description>The standard client selection algorithms for Federated Learning (FL) areoften unbiased and involve uniform random sampling of clients. This has beenproven sub-optimal for fast convergence under practical settings characterizedby significant heterogeneity in data distribution and computing andcommunication resources across clients. For applications having timingconstraints due to limited communication opportunities, the client selectionstrategy is critical to complete model training within the fixed budget ofcommunication rounds. To address this, we develop a biased client selectionstrategy, GreedyFed that identifies and greedily selects the most contributingclients in each communication round. This method builds on a fast approximationalgorithm for the Shapley Value at the parameter server (PS), making thecomputation tractable for real-world applications with many clients. Comparedto various client selection strategies on several real-world datasets,GreedyFed demonstrates fast and stable convergence with high accuracy undertiming constraints and a higher degree of heterogeneity in data distribution,systems constraints, and privacy requirements.</description><author>Pranava Singhal, Shashi Raj Pandey, Petar Popovski</author><pubDate>Thu, 14 Dec 2023 16:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09108v1</guid></item><item><title>QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models</title><link>http://arxiv.org/abs/2302.00919v3</link><description>In practical compressed sensing (CS), the obtained measurements typicallynecessitate quantization to a limited number of bits prior to transmission orstorage. This nonlinear quantization process poses significant recoverychallenges, particularly with extreme coarse quantization such as 1-bit.Recently, an efficient algorithm called QCS-SGM was proposed for quantized CS(QCS) which utilizes score-based generative models (SGM) as an implicit prior.Due to the adeptness of SGM in capturing the intricate structures of naturalsignals, QCS-SGM substantially outperforms previous QCS methods. However,QCS-SGM is constrained to (approximately) row-orthogonal sensing matrices asthe computation of the likelihood score becomes intractable otherwise. Toaddress this limitation, we introduce an advanced variant of QCS-SGM, termedQCS-SGM+, capable of handling general matrices effectively. The key idea is aBayesian inference perspective on the likelihood score computation, whereinexpectation propagation is employed for its approximate computation. Extensiveexperiments are conducted, demonstrating the substantial superiority ofQCS-SGM+ over QCS-SGM for general sensing matrices beyond mererow-orthogonality.</description><author>Xiangming Meng, Yoshiyuki Kabashima</author><pubDate>Thu, 14 Dec 2023 16:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00919v3</guid></item><item><title>Aggregation Model Hyperparameters Matter in Digital Pathology</title><link>http://arxiv.org/abs/2311.17804v2</link><description>Digital pathology has significantly advanced disease detection andpathologist efficiency through the analysis of gigapixel whole-slide images(WSI). In this process, WSIs are first divided into patches, for which afeature extractor model is applied to obtain feature vectors, which aresubsequently processed by an aggregation model to predict the respective WSIlabel. With the rapid evolution of representation learning, numerous newfeature extractor models, often termed foundational models, have emerged.Traditional evaluation methods, however, rely on fixed aggregation modelhyperparameters, a framework we identify as potentially biasing the results.Our study uncovers a co-dependence between feature extractor models andaggregation model hyperparameters, indicating that performance comparabilitycan be skewed based on the chosen hyperparameters. By accounting for thisco-dependency, we find that the performance of many current feature extractormodels is notably similar. We support this insight by evaluating seven featureextractor models across three different datasets with 162 different aggregationmodel configurations. This comprehensive approach provides a more nuancedunderstanding of the relationship between feature extractors and aggregationmodels, leading to a fairer and more accurate assessment of feature extractormodels in digital pathology.</description><author>Gustav Bredell, Marcel Fischer, Przemyslaw Szostak, Samaneh Abbasi-Sureshjani, Alvaro Gomariz</author><pubDate>Thu, 14 Dec 2023 16:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17804v2</guid></item><item><title>Subspace Identification for Multi-Source Domain Adaptation</title><link>http://arxiv.org/abs/2310.04723v2</link><description>Multi-source domain adaptation (MSDA) methods aim to transfer knowledge frommultiple labeled source domains to an unlabeled target domain. Although currentmethods achieve target joint distribution identifiability by enforcing minimalchanges across domains, they often necessitate stringent conditions, such as anadequate number of domains, monotonic transformation of latent variables, andinvariant label distributions. These requirements are challenging to satisfy inreal-world applications. To mitigate the need for these strict assumptions, wepropose a subspace identification theory that guarantees the disentanglement ofdomain-invariant and domain-specific variables under less restrictiveconstraints regarding domain numbers and transformation properties, therebyfacilitating domain adaptation by minimizing the impact of domain shifts oninvariant variables. Based on this theory, we develop a Subspace IdentificationGuarantee (SIG) model that leverages variational inference. Furthermore, theSIG model incorporates class-aware conditional alignment to accommodate targetshifts where label distributions change with the domains. Experimental resultsdemonstrate that our SIG model outperforms existing MSDA techniques on variousbenchmark datasets, highlighting its effectiveness in real-world applications.</description><author>Zijian Li, Ruichu Cai, Guangyi Chen, Boyang Sun, Zhifeng Hao, Kun Zhang</author><pubDate>Thu, 14 Dec 2023 16:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04723v2</guid></item><item><title>ColNeRF: Collaboration for Generalizable Sparse Input Neural Radiance Field</title><link>http://arxiv.org/abs/2312.09095v1</link><description>Neural Radiance Fields (NeRF) have demonstrated impressive potential insynthesizing novel views from dense input, however, their effectiveness ischallenged when dealing with sparse input. Existing approaches that incorporateadditional depth or semantic supervision can alleviate this issue to an extent.However, the process of supervision collection is not only costly but alsopotentially inaccurate, leading to poor performance and generalization abilityin diverse scenarios. In our work, we introduce a novel model: theCollaborative Neural Radiance Fields (ColNeRF) designed to work with sparseinput. The collaboration in ColNeRF includes both the cooperation betweensparse input images and the cooperation between the output of the neuralradiation field. Through this, we construct a novel collaborative module thataligns information from various views and meanwhile imposes self-supervisedconstraints to ensure multi-view consistency in both geometry and appearance. ACollaborative Cross-View Volume Integration module (CCVI) is proposed tocapture complex occlusions and implicitly infer the spatial location ofobjects. Moreover, we introduce self-supervision of target rays projected inmultiple directions to ensure geometric and color consistency in adjacentregions. Benefiting from the collaboration at the input and output ends,ColNeRF is capable of capturing richer and more generalized scenerepresentation, thereby facilitating higher-quality results of the novel viewsynthesis. Extensive experiments demonstrate that ColNeRF outperformsstate-of-the-art sparse input generalizable NeRF methods. Furthermore, ourapproach exhibits superiority in fine-tuning towards adapting to new scenes,achieving competitive performance compared to per-scene optimized NeRF-basedmethods while significantly reducing computational costs. Our code is availableat: https://github.com/eezkni/ColNeRF.</description><author>Zhangkai Ni, Peiqi Yang, Wenhan Yang, Lin Ma, Sam Kwong</author><pubDate>Thu, 14 Dec 2023 16:26:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09095v1</guid></item><item><title>Agent Attention: On the Integration of Softmax and Linear Attention</title><link>http://arxiv.org/abs/2312.08874v1</link><description>The attention module is the key component in Transformers. While the globalattention mechanism offers high expressiveness, its excessive computationalcost restricts its applicability in various scenarios. In this paper, wepropose a novel attention paradigm, Agent Attention, to strike a favorablebalance between computational efficiency and representation power.Specifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$,introduces an additional set of agent tokens $A$ into the conventionalattention module. The agent tokens first act as the agent for the query tokens$Q$ to aggregate information from $K$ and $V$, and then broadcast theinformation back to $Q$. Given the number of agent tokens can be designed to bemuch smaller than the number of query tokens, the agent attention issignificantly more efficient than the widely adopted Softmax attention, whilepreserving global context modelling capability. Interestingly, we show that theproposed agent attention is equivalent to a generalized form of linearattention. Therefore, agent attention seamlessly integrates the powerfulSoftmax attention and the highly efficient linear attention. Extensiveexperiments demonstrate the effectiveness of agent attention with variousvision Transformers and across diverse vision tasks, including imageclassification, object detection, semantic segmentation and image generation.Notably, agent attention has shown remarkable performance in high-resolutionscenarios, owning to its linear attention nature. For instance, when applied toStable Diffusion, our agent attention accelerates generation and substantiallyenhances image generation quality without any additional training. Code isavailable at https://github.com/LeapLabTHU/Agent-Attention.</description><author>Dongchen Han, Tianzhu Ye, Yizeng Han, Zhuofan Xia, Shiji Song, Gao Huang</author><pubDate>Thu, 14 Dec 2023 16:26:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08874v1</guid></item><item><title>Verification of Locally Tight Programs</title><link>http://arxiv.org/abs/2204.10789v2</link><description>Program completion is a translation from the language of logic programs intothe language of first-order theories. Its original definition has been extendedto programs that include integer arithmetic, accept input, and distinguishbetween output predicates and auxiliary predicates. For tight programs, thatgeneralization of completion is known to match the stable model semantics,which is the basis of answer set programming. We show that the tightnesscondition in this theorem can be replaced by a less restrictive "localtightness" requirement. From this fact we conclude that the proof assistantanthem-p2p can be used to verify equivalence between locally tight programs.Under consideration for publication in Theory and Practice of Logic Programming</description><author>Jorge Fandinno, Vladimir Lifschitz, Nathan Temple</author><pubDate>Thu, 14 Dec 2023 16:24:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.10789v2</guid></item><item><title>Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption</title><link>http://arxiv.org/abs/2312.09093v1</link><description>The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centeredmethodology, entangling the aspects of illumination and material reflectanceinto emission solely from 3D points. This simplified rendering approachpresents challenges in accurately modeling images captured under adverselighting conditions, such as low light or over-exposure. Motivated by theancient Greek emission theory that posits visual perception as a result of raysemanating from the eyes, we slightly refine the conventional NeRF framework totrain NeRF under challenging light conditions and generate normal-lightcondition novel views unsupervised. We introduce the concept of a "ConcealingField," which assigns transmittance values to the surrounding air to accountfor illumination effects. In dark scenarios, we assume that object emissionsmaintain a standard lighting level but are attenuated as they traverse the airduring the rendering process. Concealing Field thus compel NeRF to learnreasonable density and colour estimations for objects even in dimly litsituations. Similarly, the Concealing Field can mitigate over-exposed emissionsduring the rendering stage. Furthermore, we present a comprehensive multi-viewdataset captured under challenging illumination conditions for evaluation. Ourcode and dataset available at https://github.com/cuiziteng/Aleth-NeRF</description><author>Ziteng Cui, Lin Gu, Xiao Sun, Xianzheng Ma, Yu Qiao, Tatsuya Harada</author><pubDate>Thu, 14 Dec 2023 16:24:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09093v1</guid></item><item><title>OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning</title><link>http://arxiv.org/abs/2305.14973v2</link><description>The remarkable performance of pre-trained large language models hasrevolutionised various natural language processing applications. Due to hugeparametersizes and extensive running costs, companies or organisations tend totransfer the models to the target task by zero-shot prompting techniques.However, the prohibitive costs of tokens and time have hindered their adoptionin applications. We propose OverPrompt, leveraging the in-context learningcapability of LLMs to handle multiple task inputs, thereby reducing token andtime costs. This approach could potentially improve task performance during APIqueries due to better conditional distribution mapping. Evaluated acrossdiverse classification datasets, our experiments show that OverPrompt canachieve cost-efficient zero-shot classification without causing significantdetriment to task performance, and in some cases, even improving it. Anablation study conducted on various LLMs, along with an investigation into therobustness of our prompting strategy to different input ordering, offersvaluable insights into the broader applicability of our method across diversetasks. These findings also suggest a more seamless integration of our methodwith LLMs through an API.</description><author>Jiazheng Li, Runcong Zhao, Yongxin Yang, Yulan He, Lin Gui</author><pubDate>Thu, 14 Dec 2023 16:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14973v2</guid></item><item><title>COMBHelper: A Neural Approach to Reduce Search Space for Graph Combinatorial Problems</title><link>http://arxiv.org/abs/2312.09086v1</link><description>Combinatorial Optimization (CO) problems over graphs appear routinely in manyapplications such as in optimizing traffic, viral marketing in social networks,and matching for job allocation. Due to their combinatorial nature, theseproblems are often NP-hard. Existing approximation algorithms and heuristicsrely on the search space to find the solutions and become time-consuming whenthis space is large. In this paper, we design a neural method called COMBHelperto reduce this space and thus improve the efficiency of the traditional COalgorithms based on node selection. Specifically, it employs a Graph NeuralNetwork (GNN) to identify promising nodes for the solution set. This prunedsearch space is then fed to the traditional CO algorithms. COMBHelper also usesa Knowledge Distillation (KD) module and a problem-specific boosting module tobring further efficiency and efficacy. Our extensive experiments show that thetraditional CO algorithms with COMBHelper are at least 2 times faster thantheir original versions.</description><author>Hao Tian, Sourav Medya, Wei Ye</author><pubDate>Thu, 14 Dec 2023 16:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09086v1</guid></item><item><title>The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation</title><link>http://arxiv.org/abs/2312.09085v1</link><description>Large Language Models (LLMs) encapsulate vast amounts of knowledge but stillremain vulnerable to external misinformation. Existing research mainly studiedthis susceptibility behavior in a single-turn setting. However, belief canchange during a multi-turn conversation, especially a persuasive one.Therefore, in this study, we delve into LLMs' susceptibility to persuasiveconversations, particularly on factual questions that they can answercorrectly. We first curate the Farm (i.e., Fact to Misinform) dataset, whichcontains factual questions paired with systematically generated persuasivemisinformation. Then, we develop a testing framework to track LLMs' beliefchanges in a persuasive dialogue. Through extensive experiments, we find thatLLMs' correct beliefs on factual knowledge can be easily manipulated by variouspersuasive strategies.</description><author>Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, Han Qiu</author><pubDate>Thu, 14 Dec 2023 16:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09085v1</guid></item><item><title>Language Modeling on a SpiNNaker 2 Neuromorphic Chip</title><link>http://arxiv.org/abs/2312.09084v1</link><description>As large language models continue to scale in size rapidly, so too does thecomputational power required to run them. Event-based networks on neuromorphicdevices offer a potential way to reduce energy consumption for inferencesignificantly. However, to date, most event-based networks that can run onneuromorphic hardware, including spiking neural networks (SNNs), have notachieved task performance even on par with LSTM models for language modeling.As a result, language modeling on neuromorphic devices has seemed a distantprospect. In this work, we demonstrate the first-ever implementation of alanguage model on a neuromorphic device - specifically the SpiNNaker 2 chip -based on a recently published event-based architecture called the EGRU.SpiNNaker 2 is a many-core neuromorphic chip designed for large-scaleasynchronous processing, while the EGRU is architected to leverage suchhardware efficiently while maintaining competitive task performance. Thisimplementation marks the first time a neuromorphic language model matchesLSTMs, setting the stage for taking task performance to the level of largelanguage models. We also demonstrate results on a gesture recognition taskbased on inputs from a DVS camera. Overall, our results showcase thefeasibility of this neuro-inspired neural network in hardware, highlightingsignificant gains versus conventional hardware in energy efficiency for thecommon use case of single batch inference.</description><author>Khaleelulla Khan Nazeer, Mark Schöne, Rishav Mukherji, Christian Mayr, David Kappel, Anand Subramoney</author><pubDate>Thu, 14 Dec 2023 16:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09084v1</guid></item><item><title>Learned Fusion: 3D Object Detection using Calibration-Free Transformer Feature Fusion</title><link>http://arxiv.org/abs/2312.09082v1</link><description>The state of the art in 3D object detection using sensor fusion heavilyrelies on calibration quality, which is difficult to maintain in large scaledeployment outside a lab environment. We present the first calibration-freeapproach for 3D object detection. Thus, eliminating the need for complex andcostly calibration procedures. Our approach uses transformers to map thefeatures between multiple views of different sensors at multiple abstractionlevels. In an extensive evaluation for object detection, we not only show thatour approach outperforms single modal setups by 14.1% in BEV mAP, but also thatthe transformer indeed learns mapping. By showing calibration is not necessaryfor sensor fusion, we hope to motivate other researchers following thedirection of calibration-free fusion. Additionally, resulting approaches have asubstantial resilience against rotation and translation changes.</description><author>Michael Fürst, Rahul Jakkamsetty, René Schuster, Didier Stricker</author><pubDate>Thu, 14 Dec 2023 16:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09082v1</guid></item><item><title>Two-dimensional total absorption spectroscopy with conditional generative adversarial networks</title><link>http://arxiv.org/abs/2206.11792v3</link><description>We explore the use of machine learning techniques to remove the response oflarge volume $\gamma$-ray detectors from experimental spectra. Segmented$\gamma$-ray total absorption spectrometers (TAS) allow for the simultaneousmeasurement of individual $\gamma$-ray energy (E$_\gamma$) and total excitationenergy (E$_x$). Analysis of TAS detector data is complicated by the fact thatthe E$_x$ and E$_\gamma$ quantities are correlated, and therefore, techniquesthat simply unfold using E$_x$ and E$_\gamma$ response functions independentlyare not as accurate. In this work, we investigate the use of conditionalgenerative adversarial networks (cGANs) to simultaneously unfold $E_{x}$ and$E_{\gamma}$ data in TAS detectors. Specifically, we employ a \texttt{Pix2Pix}cGAN, a generative modeling technique based on recent advances in deeplearning, to treat \rawmatrix~ matrix unfolding as an image-to-imagetranslation problem. We present results for simulated and experimental matricesof single-$\gamma$ and double-$\gamma$ decay cascades. Our model demonstratescharacterization capabilities within detector resolution limits for upwards of93% of simulated test cases.</description><author>Cade Dembski, Michelle P. Kuchera, Sean Liddick, Raghu Ramanujan, Artemis Spyrou</author><pubDate>Thu, 14 Dec 2023 16:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11792v3</guid></item><item><title>Controllable Citation Sentence Generation with Language Models</title><link>http://arxiv.org/abs/2211.07066v2</link><description>Citation generation aims to generate a citation sentence that refers to achosen paper in the context of a manuscript. However, a rigid citationgeneration process is at odds with an author's desire to control specificattributes, such as 1) the citation intent, e.g., either introducing backgroundinformation or comparing results, and 2) keywords that should appear in thecitation text. To provide these degrees of controllability during citationgeneration, we propose to integrate the manuscript context, the context of thereferenced paper, and the desired control attributes into a structured templateand use it to fine-tune a language model (LM) via next-token prediction. Wethen utilize Proximal Policy Optimization to directly optimize the LM in favorof a high score of our proposed controllability metric. The proposed workflowharmoniously combines citation attribute suggestion and conditional citationgeneration into one LM, allowing for better user control.</description><author>Nianlong Gu, Richard H. R. Hahnloser</author><pubDate>Thu, 14 Dec 2023 16:13:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07066v2</guid></item><item><title>Coevolutionary Algorithm for Building Robust Decision Trees under Minimax Regret</title><link>http://arxiv.org/abs/2312.09078v1</link><description>In recent years, there has been growing interest in developing robust machinelearning (ML) models that can withstand adversarial attacks, including one ofthe most widely adopted, efficient, and interpretable ML algorithms-decisiontrees (DTs). This paper proposes a novel coevolutionary algorithm (CoEvoRDT)designed to create robust DTs capable of handling noisy high-dimensional datain adversarial contexts. Motivated by the limitations of traditional DTalgorithms, we leverage adaptive coevolution to allow DTs to evolve and learnfrom interactions with perturbed input data. CoEvoRDT alternately evolvescompeting populations of DTs and perturbed features, enabling construction ofDTs with desired properties. CoEvoRDT is easily adaptable to various targetmetrics, allowing the use of tailored robustness criteria such as minimaxregret. Furthermore, CoEvoRDT has potential to improve the results of otherstate-of-the-art methods by incorporating their outcomes (DTs they produce)into the initial population and optimize them in the process of coevolution.Inspired by the game theory, CoEvoRDT utilizes mixed Nash equilibrium toenhance convergence. The method is tested on 20 popular datasets and showssuperior performance compared to 4 state-of-the-art algorithms. It outperformedall competing methods on 13 datasets with adversarial accuracy metrics, and onall 20 considered datasets with minimax regret. Strong experimental results andflexibility in choosing the error measure make CoEvoRDT a promising approachfor constructing robust DTs in real-world applications.</description><author>Adam Żychowski, Andrew Perrault, Jacek Mańdziuk</author><pubDate>Thu, 14 Dec 2023 16:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09078v1</guid></item><item><title>ProSGNeRF: Progressive Dynamic Neural Scene Graph with Frequency Modulated Auto-Encoder in Urban Scenes</title><link>http://arxiv.org/abs/2312.09076v1</link><description>Implicit neural representation has demonstrated promising results in viewsynthesis for large and complex scenes. However, existing approaches eitherfail to capture the fast-moving objects or need to build the scene graphwithout camera ego-motions, leading to low-quality synthesized views of thescene. We aim to jointly solve the view synthesis problem of large-scale urbanscenes and fast-moving vehicles, which is more practical and challenging. Tothis end, we first leverage a graph structure to learn the local scenerepresentations of dynamic objects and the background. Then, we design aprogressive scheme that dynamically allocates a new local scene graph trainedwith frames within a temporal window, allowing us to scale up therepresentation to an arbitrarily large scene. Besides, the training views ofurban scenes are relatively sparse, which leads to a significant decline inreconstruction accuracy for dynamic objects. Therefore, we design a frequencyauto-encoder network to encode the latent code and regularize the frequencyrange of objects, which can enhance the representation of dynamic objects andaddress the issue of sparse image inputs. Additionally, we employ lidar pointprojection to maintain geometry consistency in large-scale urban scenes.Experimental results demonstrate that our method achieves state-of-the-art viewsynthesis accuracy, object manipulation, and scene roaming ability. The codewill be open-sourced upon paper acceptance.</description><author>Tianchen Deng, Siyang Liu, Xuan Wang, Yejia Liu, Danwei Wang, Weidong Chen</author><pubDate>Thu, 14 Dec 2023 16:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09076v1</guid></item><item><title>Real-World Humanoid Locomotion with Reinforcement Learning</title><link>http://arxiv.org/abs/2303.03381v2</link><description>Humanoid robots that can autonomously operate in diverse environments havethe potential to help address labour shortages in factories, assist elderly athomes, and colonize new planets. While classical controllers for humanoidrobots have shown impressive results in a number of settings, they arechallenging to generalize and adapt to new environments. Here, we present afully learning-based approach for real-world humanoid locomotion. Ourcontroller is a causal transformer that takes the history of proprioceptiveobservations and actions as input and predicts the next action. We hypothesizethat the observation-action history contains useful information about the worldthat a powerful transformer model can use to adapt its behavior in-context,without updating its weights. We train our model with large-scale model-freereinforcement learning on an ensemble of randomized environments in simulationand deploy it to the real world zero-shot. Our controller can walk over variousoutdoor terrains, is robust to external disturbances, and can adapt in context.</description><author>Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Darrell, Jitendra Malik, Koushil Sreenath</author><pubDate>Thu, 14 Dec 2023 16:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.03381v2</guid></item><item><title>Towards Verifiable Text Generation with Evolving Memory and Self-Reflection</title><link>http://arxiv.org/abs/2312.09075v1</link><description>Large Language Models (LLMs) face several challenges, including the tendencyto produce incorrect outputs, known as hallucination. An effective solution isverifiable text generation, which prompts LLMs to generate content withcitations for accuracy verification. However, verifiable text generation isnon-trivial due to the focus-shifting phenomenon, the dilemma between theprecision and scope in document retrieval, and the intricate reasoning requiredto discern the relationship between the claim and citations. In this paper, wepresent VTG, an innovative approach for Verifiable Text Generation withevolving memory and self-reflection. VTG maintains evolving long short-termmemory to retain both valuable documents and up-to-date documents. Activeretrieval and diverse query generation are utilized to enhance both theprecision and scope of the retrieved documents. Furthermore, VTG features atwo-tier verifier and an evidence finder, enabling rethinking and reflection onthe relationship between the claim and citations. We conduct extensiveexperiments on five datasets across three knowledge-intensive tasks and theresults reveal that VTG significantly outperforms existing baselines.</description><author>Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xiaochi Wei, Shuaiqiang Wang, Yan Zhang, Dawei Yin</author><pubDate>Thu, 14 Dec 2023 16:10:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09075v1</guid></item><item><title>Optimal Motion Planning using Finite Fourier Series in a Learning-based Collision Field</title><link>http://arxiv.org/abs/2312.09073v1</link><description>This paper utilizes finite Fourier series to represent a time-continuousmotion and proposes a novel planning method that adjusts the motion harmonicsof each manipulator joint. Primarily, we sum the potential energy for collisiondetection and the kinetic energy up to calculate the Hamiltonian of themanipulator motion harmonics. Though the adaptive interior-point method isdesigned to modify the harmonics in its finite frequency domain, we stillencounter the local minima due to the non-convexity of the collision field. Inthis way, we learn the collision field through a support vector machine with aGaussian kernel, which is highly convex. The learning-based collision field isapplied for Hamiltonian, and the experiment results show our method's highreliability and efficiency.</description><author>Feng Yichang, Wang Jin, Lu Guodong</author><pubDate>Thu, 14 Dec 2023 16:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09073v1</guid></item><item><title>Deep Reinforcement Learning for Image-to-Image Translation</title><link>http://arxiv.org/abs/2309.13672v2</link><description>Most existing Image-to-Image Translation (I2IT) methods generate images in asingle run of a deep learning (DL) model. However, designing such a single-stepmodel is always challenging, requiring a huge number of parameters and easilyfalling into bad global minimums and overfitting. In this work, we reformulateI2IT as a step-wise decision-making problem via deep reinforcement learning(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). Thekey feature in the RL-I2IT framework is to decompose a monolithic learningprocess into small steps with a lightweight model to progressively transform asource image successively to a target image. Considering that it is challengingto handle high dimensional continuous state and action spaces in theconventional RL framework, we introduce meta policy with a new concept Plan tothe standard Actor-Critic model, which is of a lower dimension than theoriginal image and can facilitate the actor to generate a tractable highdimensional action. In the RL-I2IT framework, we also employ a task-specificauxiliary learning strategy to stabilize the training process and improve theperformance of the corresponding task. Experiments on several I2IT tasksdemonstrate the effectiveness and robustness of the proposed method when facinghigh-dimensional continuous action space problems.</description><author>Xin Wang, Ziwei Luo, Jing Hu, Chengming Feng, Shu Hu, Bin Zhu, Xi Wu, Siwei Lyu</author><pubDate>Thu, 14 Dec 2023 16:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13672v2</guid></item><item><title>PI3D: Efficient Text-to-3D Generation with Pseudo-Image Diffusion</title><link>http://arxiv.org/abs/2312.09069v1</link><description>In this paper, we introduce PI3D, a novel and efficient framework thatutilizes the pre-trained text-to-image diffusion models to generatehigh-quality 3D shapes in minutes. On the one hand, it fine-tunes a pre-trained2D diffusion model into a 3D diffusion model, enabling both 3D generativecapabilities and generalization derived from the 2D model. On the other, itutilizes score distillation sampling of 2D diffusion models to quickly improvethe quality of the sampled 3D shapes. PI3D enables the migration of knowledgefrom image to triplane generation by treating it as a set of pseudo-images. Weadapt the modules in the pre-training model to enable hybrid training usingpseudo and real images, which has proved to be a well-established strategy forimproving generalizability. The efficiency of PI3D is highlighted by itsability to sample diverse 3D models in seconds and refine them in minutes. Theexperimental results confirm the advantages of PI3D over existing methods basedon either 3D diffusion models or lifting 2D diffusion models in terms of fastgeneration of 3D consistent and high-quality models. The proposed PI3D standsas a promising advancement in the field of text-to-3D generation, and we hopeit will inspire more research into 3D generation leveraging the knowledge inboth 2D and 3D data.</description><author>Ying-Tian Liu, Guan Luo, Heyi Sun, Wei Yin, Yuan-Chen Guo, Song-Hai Zhang</author><pubDate>Thu, 14 Dec 2023 16:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09069v1</guid></item><item><title>Holodeck: Language Guided Generation of 3D Embodied AI Environments</title><link>http://arxiv.org/abs/2312.09067v1</link><description>3D simulated environments play a critical role in Embodied AI, but theircreation requires expertise and extensive manual effort, restricting theirdiversity and scope. To mitigate this limitation, we present Holodeck, a systemthat generates 3D environments to match a user-supplied prompt fullyautomatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, andmuseums, adjust the designs for styles, and can capture the semantics ofcomplex queries such as "apartment for a researcher with a cat" and "office ofa professor who is a fan of Star Wars". Holodeck leverages a large languagemodel (GPT-4) for common sense knowledge about what the scene might look likeand uses a large collection of 3D assets from Objaverse to populate the scenewith diverse objects. To address the challenge of positioning objectscorrectly, we prompt GPT-4 to generate spatial relational constraints betweenobjects and then optimize the layout to satisfy those constraints. Ourlarge-scale human evaluation shows that annotators prefer Holodeck overmanually designed procedural baselines in residential scenes and that Holodeckcan produce high-quality outputs for diverse scene types. We also demonstratean exciting application of Holodeck in Embodied AI, training agents to navigatein novel scenes like music rooms and daycares without human-constructed data,which is a significant step forward in developing general-purpose embodiedagents.</description><author>Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark</author><pubDate>Thu, 14 Dec 2023 16:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09067v1</guid></item><item><title>CMOSE: Comprehensive Multi-Modality Online Student Engagement Dataset with High-Quality Labels</title><link>http://arxiv.org/abs/2312.09066v1</link><description>Online learning is a rapidly growing industry due to its convenience.However, a major challenge in online learning is whether students are asengaged as they are in face-to-face classes. An engagement recognition systemcan significantly improve the learning experience in online classes. Currentchallenges in engagement detection involve poor label quality in the dataset,intra-class variation, and extreme data imbalance. To address these problems,we present the CMOSE dataset, which contains a large number of data indifferent engagement levels and high-quality labels generated according to thepsychological advice. We demonstrate the advantage of transferability byanalyzing the model performance on other engagement datasets. We also developeda training mechanism, MocoRank, to handle the intra-class variation, theordinal relationship between different classes, and the data imbalance problem.MocoRank outperforms prior engagement detection losses, achieving a 1.32%enhancement in overall accuracy and 5.05% improvement in average accuracy. Wefurther demonstrate the effectiveness of multi-modality by conducting ablationstudies on features such as pre-trained video features, high-level facialfeatures, and audio features.</description><author>Chi-hsuan Wu, Shih-yang Liu, Xijie Huang, Xingbo Wang, Rong Zhang, Luca Minciullo, Wong Kai Yiu, Kenny Kwan, Kwang-Ting Cheng</author><pubDate>Thu, 14 Dec 2023 16:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09066v1</guid></item><item><title>Image Demoireing in RAW and sRGB Domains</title><link>http://arxiv.org/abs/2312.09063v1</link><description>Moir\'e patterns frequently appear when capturing screens with smartphones orcameras, potentially compromising image quality. Previous studies suggest thatmoir\'e pattern elimination in the RAW domain offers greater efficiencycompared to demoir\'eing in the sRGB domain. Nevertheless, relying solely onraw data for image demoir\'eing is insufficient in mitigating color cast due tothe absence of essential information required for color correction by the ImageSignal Processor (ISP). In this paper, we propose perform Image Demoir\'eingconcurrently utilizing both RAW and sRGB data (RRID), which is readilyaccessible in both smartphones and digital cameras. We developSkip-Connection-based Demoir\'eing Module (SCDM) with specific modules embededin skip-connections for the efficient and effective demoir\'eing of RAW andsRGB features, respectively. Subsequently, we propose RGB Guided Image SignalProcessor (RGISP) to incorporate color information from coarsely demoir\'edsRGB features during the ISP stage, assisting the process of color recovery.Extensive experiments demonstrate that our RRID outperforms state-of-the-artapproaches by 0.62dB in PSNR and 0.003 in SSIM, exhibiting superior performanceboth in moir\'e pattern removal and color cast correction.</description><author>Shuning Xu, Binbin Song, Xiangyu Chen, Jiantao Zhou</author><pubDate>Thu, 14 Dec 2023 16:00:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09063v1</guid></item><item><title>Fair Clustering: A Causal Perspective</title><link>http://arxiv.org/abs/2312.09061v1</link><description>Clustering algorithms may unintentionally propagate or intensify existingdisparities, leading to unfair representations or biased decision-making.Current fair clustering methods rely on notions of fairness that do not captureany information on the underlying causal mechanisms. We show that optimisingfor non-causal fairness notions can paradoxically induce direct discriminatoryeffects from a causal standpoint. We present a clustering approach thatincorporates causal fairness metrics to provide a more nuanced approach tofairness in unsupervised learning. Our approach enables the specification ofthe causal fairness metrics that should be minimised. We demonstrate theefficacy of our methodology using datasets known to harbour unfair biases.</description><author>Fritz Bayer, Drago Plecko, Niko Beerenwinkel, Jack Kuipers</author><pubDate>Thu, 14 Dec 2023 15:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09061v1</guid></item><item><title>Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery</title><link>http://arxiv.org/abs/2312.09059v1</link><description>The substantial success of Vision Transformer (ViT) in computer vision tasksis largely attributed to the architecture design. This underscores thenecessity of efficient architecture search for designing better ViTsautomatically. As training-based architecture search methods arecomputationally intensive, there is a growing interest in training-free methodsthat use zero-cost proxies to score ViTs. However, existing training-freeapproaches require expert knowledge to manually design specific zero-costproxies. Moreover, these zero-cost proxies exhibit limitations to generalizeacross diverse domains. In this paper, we introduce Auto-Prox, an automaticproxy discovery framework, to address the problem. First, we build theViT-Bench-101, which involves different ViT candidates and their actualperformance on multiple datasets. Utilizing ViT-Bench-101, we can evaluatezero-cost proxies based on their score-accuracy correlation. Then, we representzero-cost proxies with computation graphs and organize the zero-cost proxysearch space with ViT statistics and primitive operations. To discover genericzero-cost proxies, we propose a joint correlation metric to evolve and mutatedifferent zero-cost proxy candidates. We introduce an elitism-preserve strategyfor search efficiency to achieve a better trade-off between exploitation andexploration. Based on the discovered zero-cost proxy, we conduct a ViTarchitecture search in a training-free manner. Extensive experimentsdemonstrate that our method generalizes well to different datasets and achievesstate-of-the-art results both in ranking correlation and final accuracy. Codescan be found at https://github.com/lilujunai/Auto-Prox-AAAI24.</description><author>Zimian Wei, Lujun Li, Peijie Dong, Zheng Hui, Anggeng Li, Menglong Lu, Hengyue Pan, Zhiliang Tian, Dongsheng Li</author><pubDate>Thu, 14 Dec 2023 15:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09059v1</guid></item><item><title>On the Difficulty of Defending Contrastive Learning against Backdoor Attacks</title><link>http://arxiv.org/abs/2312.09057v1</link><description>Recent studies have shown that contrastive learning, like supervisedlearning, is highly vulnerable to backdoor attacks wherein malicious functionsare injected into target models, only to be activated by specific triggers.However, thus far it remains under-explored how contrastive backdoor attacksfundamentally differ from their supervised counterparts, which impedes thedevelopment of effective defenses against the emerging threat. This work represents a solid step toward answering this critical question.Specifically, we define TRL, a unified framework that encompasses bothsupervised and contrastive backdoor attacks. Through the lens of TRL, weuncover that the two types of attacks operate through distinctive mechanisms:in supervised attacks, the learning of benign and backdoor tasks tends to occurindependently, while in contrastive attacks, the two tasks are deeplyintertwined both in their representations and throughout their learningprocesses. This distinction leads to the disparate learning dynamics andfeature distributions of supervised and contrastive attacks. More importantly,we reveal that the specificities of contrastive backdoor attacks entailimportant implications from a defense perspective: existing defenses forsupervised attacks are often inadequate and not easily retrofitted tocontrastive attacks. We also explore several alternative defenses and discusstheir potential challenges. Our findings highlight the need for defensestailored to the specificities of contrastive backdoor attacks, pointing topromising directions for future research.</description><author>Changjiang Li, Ren Pang, Bochuan Cao, Zhaohan Xi, Jinghui Chen, Shouling Ji, Ting Wang</author><pubDate>Thu, 14 Dec 2023 15:54:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09057v1</guid></item><item><title>ReCoRe: Regularized Contrastive Representation Learning of World Model</title><link>http://arxiv.org/abs/2312.09056v1</link><description>While recent model-free Reinforcement Learning (RL) methods have demonstratedhuman-level effectiveness in gaming environments, their success in everydaytasks like visual navigation has been limited, particularly under significantappearance variations. This limitation arises from (i) poor sample efficiencyand (ii) over-fitting to training scenarios. To address these challenges, wepresent a world model that learns invariant features using (i) contrastiveunsupervised learning and (ii) an intervention-invariant regularizer. Learningan explicit representation of the world dynamics i.e. a world model, improvessample efficiency while contrastive learning implicitly enforces learning ofinvariant features, which improves generalization. However, the naiveintegration of contrastive loss to world models fails due to a lack ofsupervisory signals to the visual encoder, as world-model-based RL methodsindependently optimize representation learning and agent policy. To overcomethis issue, we propose an intervention-invariant regularizer in the form of anauxiliary task such as depth prediction, image denoising, etc., that explicitlyenforces invariance to style-interventions. Our method outperforms currentstate-of-the-art model-based and model-free RL methods and significantly onout-of-distribution point navigation task evaluated on the iGibson benchmark.We further demonstrate that our approach, with only visual observations,outperforms recent language-guided foundation models for point navigation,which is essential for deployment on robots with limited computationcapabilities. Finally, we demonstrate that our proposed model excels at thesim-to-real transfer of its perception module on Gibson benchmark.</description><author>Rudra P. K. Poudel, Harit Pandya, Stephan Liwicki, Roberto Cipolla</author><pubDate>Thu, 14 Dec 2023 15:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09056v1</guid></item><item><title>A Sparse Cross Attention-based Graph Convolution Network with Auxiliary Information Awareness for Traffic Flow Prediction</title><link>http://arxiv.org/abs/2312.09050v1</link><description>Deep graph convolution networks (GCNs) have recently shown excellentperformance in traffic prediction tasks. However, they face some challenges.First, few existing models consider the influence of auxiliary information,i.e., weather and holidays, which may result in a poor grasp ofspatial-temporal dynamics of traffic data. Second, both the construction of adynamic adjacent matrix and regular graph convolution operations have quadraticcomputation complexity, which restricts the scalability of GCN-based models. Toaddress such challenges, this work proposes a deep encoder-decoder modelentitled AIMSAN. It contains an auxiliary information-aware module (AIM) andsparse cross attention-based graph convolution network (SAN). The former learnsmulti-attribute auxiliary information and obtains its embedded presentation ofdifferent time-window sizes. The latter uses a cross-attention mechanism toconstruct dynamic adjacent matrices by fusing traffic data and embeddedauxiliary data. Then, SAN applies diffusion GCN on traffic data to mine richspatial-temporal dynamics. Furthermore, AIMSAN considers and uses the spatialsparseness of traffic nodes to reduce the quadratic computation complexity.Experimental results on three public traffic datasets demonstrate that theproposed method outperforms other counterparts in terms of various performanceindices. Specifically, the proposed method has competitive performance with thestate-of-the-art algorithms but saves 35.74% of GPU memory usage, 42.25% oftraining time, and 45.51% of validation time on average.</description><author>Lingqiang Chen, Qinglin Zhao, Guanghui Li, Mengchu Zhou, Chenglong Dai, Yiming Feng</author><pubDate>Thu, 14 Dec 2023 15:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09050v1</guid></item><item><title>On The Expressivity of Recurrent Neural Cascades</title><link>http://arxiv.org/abs/2312.09048v1</link><description>Recurrent Neural Cascades (RNCs) are the recurrent neural networks with nocyclic dependencies among recurrent neurons. This class of recurrent networkshas received a lot of attention in practice. Besides training methods for afixed architecture such as backpropagation, the cascade architecture naturallyallows for constructive learning methods, where recurrent nodes are addedincrementally one at a time, often yielding smaller networks. Furthermore,acyclicity amounts to a structural prior that even for the same number ofneurons yields a more favourable sample complexity compared to afully-connected architecture. A central question is whether the advantages ofthe cascade architecture come at the cost of a reduced expressivity. We providenew insights into this question. We show that the regular languages captured byRNCs with sign and tanh activation with positive recurrent weights are thestar-free regular languages. In order to establish our results we developed anovel framework where capabilities of RNCs are accessed by analysing whichsemigroups and groups a single neuron is able to implement. A notableimplication of our framework is that RNCs can achieve the expressivity of allregular languages by introducing neurons that can implement groups.</description><author>Nadezda Alexandrovna Knorozova, Alessandro Ronca</author><pubDate>Thu, 14 Dec 2023 15:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09048v1</guid></item><item><title>Exploring the Naturalness of AI-Generated Images</title><link>http://arxiv.org/abs/2312.05476v2</link><description>The proliferation of Artificial Intelligence-Generated Images (AGIs) hasgreatly expanded the Image Naturalness Assessment (INA) problem. Different fromearly definitions that mainly focus on tone-mapped images with limiteddistortions (e.g., exposure, contrast, and color reproduction), INA onAI-generated images is especially challenging as it has more diverse contentsand could be affected by factors from multiple perspectives, includinglow-level technical distortions and high-level rationality distortions. In thispaper, we take the first step to benchmark and assess the visual naturalness ofAI-generated images. First, we construct the AI-Generated Image Naturalness(AGIN) database by conducting a large-scale subjective study to collect humanopinions on the overall naturalness as well as perceptions from technical andrationality perspectives. AGIN verifies that naturalness is universally anddisparately affected by both technical and rationality distortions. Second, wepropose the Joint Objective Image Naturalness evaluaTor (JOINT), toautomatically learn the naturalness of AGIs that aligns human ratings.Specifically, JOINT imitates human reasoning in naturalness evaluation byjointly learning both technical and rationality perspectives. Experimentalresults show our proposed JOINT significantly surpasses baselines for providingmore subjectively consistent results on naturalness assessment. Our databaseand code will be released in https://github.com/zijianchen98/AGIN.</description><author>Zijian Chen, Wei Sun, Haoning Wu, Zicheng Zhang, Jun Jia, Xiongkuo Min, Guangtao Zhai, Wenjun Zhang</author><pubDate>Thu, 14 Dec 2023 15:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05476v2</guid></item><item><title>The impact of memory on learning sequence-to-sequence tasks</title><link>http://arxiv.org/abs/2205.14683v2</link><description>The recent success of neural networks in natural language processing hasdrawn renewed attention to learning sequence-to-sequence (seq2seq) tasks. Whilethere exists a rich literature that studies classification and regression tasksusing solvable models of neural networks, seq2seq tasks have not yet beenstudied from this perspective. Here, we propose a simple model for a seq2seqtask that has the advantage of providing explicit control over the degree ofmemory, or non-Markovianity, in the sequences -- the stochasticswitching-Ornstein-Uhlenbeck (SSOU) model. We introduce a measure ofnon-Markovianity to quantify the amount of memory in the sequences. For aminimal auto-regressive (AR) learning model trained on this task, we identifytwo learning regimes corresponding to distinct phases in the stationary stateof the SSOU process. These phases emerge from the interplay between twodifferent time scales that govern the sequence statistics. Moreover, we observethat while increasing the integration window of the AR model always improvesperformance, albeit with diminishing returns, increasing the non-Markovianityof the input sequences can improve or degrade its performance. Finally, weperform experiments with recurrent and convolutional neural networks that showthat our observations carry over to more complicated neural networkarchitectures.</description><author>Alireza Seif, Sarah A. M. Loos, Gennaro Tucci, Édgar Roldán, Sebastian Goldt</author><pubDate>Thu, 14 Dec 2023 15:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.14683v2</guid></item><item><title>Concealing Sensitive Samples against Gradient Leakage in Federated Learning</title><link>http://arxiv.org/abs/2209.05724v2</link><description>Federated Learning (FL) is a distributed learning paradigm that enhancesusers privacy by eliminating the need for clients to share raw, private datawith the server. Despite the success, recent studies expose the vulnerabilityof FL to model inversion attacks, where adversaries reconstruct users privatedata via eavesdropping on the shared gradient information. We hypothesize thata key factor in the success of such attacks is the low entanglement amonggradients per data within the batch during stochastic optimization. Thiscreates a vulnerability that an adversary can exploit to reconstruct thesensitive data. Building upon this insight, we present a simple, yet effectivedefense strategy that obfuscates the gradients of the sensitive data withconcealed samples. To achieve this, we propose synthesizing concealed samplesto mimic the sensitive data at the gradient level while ensuring their visualdissimilarity from the actual sensitive data. Compared to the previous art, ourempirical evaluations suggest that the proposed technique provides thestrongest protection while simultaneously maintaining the FL performance.</description><author>Jing Wu, Munawar Hayat, Mingyi Zhou, Mehrtash Harandi</author><pubDate>Thu, 14 Dec 2023 15:42:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05724v2</guid></item><item><title>Topic Bias in Emotion Classification</title><link>http://arxiv.org/abs/2312.09043v1</link><description>Emotion corpora are typically sampled based on keyword/hashtag search or byasking study participants to generate textual instances. In any case, thesecorpora are not uniform samples representing the entirety of a domain. Wehypothesize that this practice of data acquisition leads to unrealisticcorrelations between overrepresented topics in these corpora that harm thegeneralizability of models. Such topic bias could lead to wrong predictions forinstances like "I organized the service for my aunt's funeral." when funeralevents are over-represented for instances labeled with sadness, despite theemotion of pride being more appropriate here. In this paper, we study thistopic bias both from the data and the modeling perspective. We first label aset of emotion corpora automatically via topic modeling and show that emotionsin fact correlate with specific topics. Further, we see that emotionclassifiers are confounded by such topics. Finally, we show that theestablished debiasing method of adversarial correction via gradient reversalmitigates the issue. Our work points out issues with existing emotion corporaand that more representative resources are required for fair evaluation ofmodels predicting affective concepts from text.</description><author>Maximilian Wegge, Roman Klinger</author><pubDate>Thu, 14 Dec 2023 15:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09043v1</guid></item><item><title>SER_AMPEL: a multi-source dataset for speech emotion recognition of Italian older adults</title><link>http://arxiv.org/abs/2311.14483v2</link><description>In this paper, SER_AMPEL, a multi-source dataset for speech emotionrecognition (SER) is presented. The peculiarity of the dataset is that it iscollected with the aim of providing a reference for speech emotion recognitionin case of Italian older adults. The dataset is collected following differentprotocols, in particular considering acted conversations, extracted from moviesand TV series, and recording natural conversations where the emotions areelicited by proper questions. The evidence of the need for such a datasetemerges from the analysis of the state of the art. Preliminary considerationson the critical issues of SER are reported analyzing the classification resultson a subset of the proposed dataset.</description><author>Alessandra Grossi, Francesca Gasparini</author><pubDate>Thu, 14 Dec 2023 15:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14483v2</guid></item><item><title>Graph Neural Networks with Diverse Spectral Filtering</title><link>http://arxiv.org/abs/2312.09041v1</link><description>Spectral Graph Neural Networks (GNNs) have achieved tremendous success ingraph machine learning, with polynomial filters applied for graph convolutions,where all nodes share the identical filter weights to mine their localcontexts. Despite the success, existing spectral GNNs usually fail to deal withcomplex networks (e.g., WWW) due to such homogeneous spectral filtering settingthat ignores the regional heterogeneity as typically seen in real-worldnetworks. To tackle this issue, we propose a novel diverse spectral filtering(DSF) framework, which automatically learns node-specific filter weights toexploit the varying local structure properly. Particularly, the diverse filterweights consist of two components -- A global one shared among all nodes, and alocal one that varies along network edges to reflect node difference arisingfrom distinct graph parts -- to balance between local and global information.As such, not only can the global graph characteristics be captured, but alsothe diverse local patterns can be mined with awareness of different nodepositions. Interestingly, we formulate a novel optimization problem to assistin learning diverse filters, which also enables us to enhance any spectral GNNswith our DSF framework. We showcase the proposed framework on threestate-of-the-arts including GPR-GNN, BernNet, and JacobiConv. Extensiveexperiments over 10 benchmark datasets demonstrate that our framework canconsistently boost model performance by up to 4.92% in node classificationtasks, producing diverse filters with enhanced interpretability. Code isavailable at \url{https://github.com/jingweio/DSF}.</description><author>Jingwei Guo, Kaizhu Huang, Xinping Yi, Rui Zhang</author><pubDate>Thu, 14 Dec 2023 15:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09041v1</guid></item><item><title>STaR: Distilling Speech Temporal Relation for Lightweight Speech Self-Supervised Learning Models</title><link>http://arxiv.org/abs/2312.09040v1</link><description>Albeit great performance of Transformer-based speech selfsupervised learning(SSL) models, their large parameter size and computational cost make themunfavorable to utilize. In this study, we propose to compress the speech SSLmodels by distilling speech temporal relation (STaR). Unlike previous worksthat directly match the representation for each speech frame, STaR distillationtransfers temporal relation between speech frames, which is more suitable forlightweight student with limited capacity. We explore three STaR distillationobjectives and select the best combination as the final STaR loss. Our modeldistilled from HuBERT BASE achieves an overall score of 79.8 on SUPERBbenchmark, the best performance among models with up to 27 million parameters.We show that our method is applicable across different speech SSL models andmaintains robust performance with further reduced parameters.</description><author>Kangwook Jang, Sungnyun Kim, Hoirin Kim</author><pubDate>Thu, 14 Dec 2023 15:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09040v1</guid></item><item><title>TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning</title><link>http://arxiv.org/abs/2312.09039v1</link><description>Table reasoning has shown remarkable progress in a wide range of table-basedtasks. These challenging tasks require reasoning over both free-form naturallanguage (NL) questions and semi-structured tabular data. However, previoustable reasoning solutions suffer from significant performance degradation on"huge" tables. In addition, most existing methods struggle to reason overcomplex questions since they lack essential information or they are scatteredin different places. To alleviate these challenges, we exploit a tableprovider, namely TAP4LLM, on versatile sampling, augmentation, and packingmethods to achieve effective semi-structured data reasoning using largelanguage models (LLMs), which 1) decompose raw tables into sub-tables withspecific rows or columns based on the rules or semantic similarity; 2) augmenttable information by extracting semantic and statistical metadata from rawtables while retrieving relevant knowledge from trustworthy knowledge sources(e.g., Wolfram Alpha, Wikipedia); 3) pack sampled tables with augmentedknowledge into sequence prompts for LLMs reasoning while balancing the tokenallocation trade-off. We show that TAP4LLM allows for different components asplug-ins, enhancing LLMs' understanding of structured data in diverse tabulartasks.</description><author>Yuan Sui, Jiaru Zou, Mengyu Zhou, Xinyi He, Lun Du, Shi Han, Dongmei Zhang</author><pubDate>Thu, 14 Dec 2023 15:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09039v1</guid></item><item><title>Object Recognition from Scientific Document based on Compartment Refinement Framework</title><link>http://arxiv.org/abs/2312.09038v1</link><description>With the rapid development of the internet in the past decade, it has becomeincreasingly important to extract valuable information from vast resourcesefficiently, which is crucial for establishing a comprehensive digitalecosystem, particularly in the context of research surveys and comprehension.The foundation of these tasks focuses on accurate extraction and deep mining ofdata from scientific documents, which are essential for building a robust datainfrastructure. However, parsing raw data or extracting data from complexscientific documents have been ongoing challenges. Current data extractionmethods for scientific documents typically use rule-based (RB) or machinelearning (ML) approaches. However, using rule-based methods can incur highcoding costs for articles with intricate typesetting. Conversely, relyingsolely on machine learning methods necessitates annotation work for complexcontent types within the scientific document, which can be costly.Additionally, few studies have thoroughly defined and explored the hierarchicallayout within scientific documents. The lack of a comprehensive definition ofthe internal structure and elements of the documents indirectly impacts theaccuracy of text classification and object recognition tasks. From theperspective of analyzing the standard layout and typesetting used in thespecified publication, we propose a new document layout analysis frameworkcalled CTBR(Compartment &amp; Text Blocks Refinement). Firstly, we definescientific documents into hierarchical divisions: base domain, compartment, andtext blocks. Next, we conduct an in-depth exploration and classification of themeanings of text blocks. Finally, we utilize the results of text blockclassification to implement object recognition within scientific documentsbased on rule-based compartment segmentation.</description><author>Jinghong Li, Wen Gu, Koichi Ota, Shinobu Hasegawa</author><pubDate>Thu, 14 Dec 2023 15:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09038v1</guid></item></channel></rss>