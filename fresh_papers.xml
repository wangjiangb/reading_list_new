<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 29 Aug 2025 01:00:17 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models</title><link>http://arxiv.org/abs/2508.19720v2</link><description>In Large Language Models (LLMs) generation, there exist knowledge conflictsand scenarios where parametric knowledge contradicts knowledge provided in thecontext. Previous works studied tuning, decoding algorithms, or locating andediting context-aware neurons to adapt LLMs to be faithful to new contextualknowledge. However, they are usually inefficient or ineffective for largemodels, not workable for black-box models, or unable to continuously adjustLLMs' sensitivity to the knowledge provided in the context. To mitigate theseproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), asimple framework that can steer LLMs' sensitivity to contextual knowledgecontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.proxy models) and use the difference in their output distributions to shift theoriginal distribution of an LLM without modifying the LLM weights. In theevaluation process, we not only design synthetic data and fine-grained metricsto measure models' sensitivity to contextual knowledge but also use a realconflict dataset to validate CSKS's practical efficacy. Extensive experimentsdemonstrate that our framework achieves continuous and precise control overLLMs' sensitivity to contextual knowledge, enabling both increased sensitivityand reduced sensitivity, thereby allowing LLMs to prioritize either contextualor parametric knowledge as needed flexibly. Our data and code are available athttps://github.com/OliveJuiceLin/CSKS.</description><author>Yilin Wang, Heng Wang, Yuyang Bai, Minnan Luo</author><pubDate>Thu, 28 Aug 2025 10:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19720v2</guid></item><item><title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title><link>http://arxiv.org/abs/2508.19650v2</link><description>Large video language models (LVLMs) have made notable progress in videounderstanding, spurring the development of corresponding evaluation benchmarks.However, existing benchmarks generally assess overall performance across entirevideo sequences, overlooking nuanced behaviors such as contextual positionalbias, a critical yet under-explored aspect of LVLM performance. We presentVideo-LevelGauge, a dedicated benchmark designed to systematically assesspositional bias in LVLMs. We employ standardized probes and customizedcontextual setups, allowing flexible control over context length, probeposition, and contextual types to simulate diverse real-world scenarios. Inaddition, we introduce a comprehensive analysis method that combinesstatistical measures with morphological pattern recognition to characterizebias. Our benchmark comprises 438 manually curated videos spanning multipletypes, yielding 1,177 high-quality multiple-choice questions and 120 open-endedquestions, validated for their effectiveness in exposing positional bias. Basedon these, we evaluate 27 state-of-the-art LVLMs, including both commercial andopen-source models. Our findings reveal significant positional biases in manyleading open-source models, typically exhibiting head or neighbor-contentpreferences. In contrast, commercial models such as Gemini2.5-Pro showimpressive, consistent performance across entire video sequences. Furtheranalyses on context length, context variation, and model scale provideactionable insights for mitigating bias and guiding modelenhancement.https://github.com/Cola-any/Video-LevelGauge</description><author>Hou Xia, Zheren Fu, Fangcan Ling, Jiajun Li, Yi Tu, Zhendong Mao, Yongdong Zhang</author><pubDate>Thu, 28 Aug 2025 09:44:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19650v2</guid></item><item><title>AraHealthQA 2025: The First Shared Task on Arabic Health Question Answering</title><link>http://arxiv.org/abs/2508.20047v2</link><description>We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health QuestionAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-locatedwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabicmedical QA resources by offering two complementary tracks: {MentalQA}, focusingon Arabic mental health Q\&amp;A (e.g., anxiety, depression, stigma reduction), and{MedArabiQ}, covering broader medical domains such as internal medicine,pediatrics, and clinical decision making. Each track comprises multiplesubtasks, evaluation datasets, and standardized metrics, facilitating fairbenchmarking. The task was structured to promote modeling under realistic,multilingual, and culturally nuanced healthcare contexts. We outline thedataset creation, task design and evaluation framework, participationstatistics, baseline systems, and summarize the overall outcomes. We concludewith reflections on the performance trends observed and prospects for futureiterations in Arabic health QA.</description><author>Hassan Alhuzali, Farah Shamout, Muhammad Abdul-Mageed, Chaimae Abouzahir, Mouath Abu-Daoud, Ashwag Alasmari, Walid Al-Eisawi, Renad Al-Monef, Ali Alqahtani, Lama Ayash, Nizar Habash, Leen Kharouf</author><pubDate>Thu, 28 Aug 2025 07:48:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20047v2</guid></item><item><title>Ego-centric Predictive Model Conditioned on Hand Trajectories</title><link>http://arxiv.org/abs/2508.19852v2</link><description>In egocentric scenarios, anticipating both the next action and its visualoutcome is essential for understanding human-object interactions and forenabling robotic planning. However, existing paradigms fall short of jointlymodeling these aspects. Vision-Language-Action (VLA) models focus on actionprediction but lack explicit modeling of how actions influence the visualscene, while video prediction models generate future frames withoutconditioning on specific actions, often resulting in implausible orcontextually inconsistent outcomes. To bridge this gap, we propose a unifiedtwo-stage predictive framework that jointly models action and visual future inegocentric scenarios, conditioned on hand trajectories. In the first stage, weperform consecutive state modeling to process heterogeneous inputs (visualobservations, language, and action history) and explicitly predict future handtrajectories. In the second stage, we introduce causal cross-attention to fusemulti-modal cues, leveraging inferred action signals to guide an image-basedLatent Diffusion Model (LDM) for frame-by-frame future video generation. Ourapproach is the first unified model designed to handle both egocentric humanactivity understanding and robotic manipulation tasks, providing explicitpredictions of both upcoming actions and their visual consequences. Extensiveexperiments on Ego4D, BridgeData, and RLBench demonstrate that our methodoutperforms state-of-the-art baselines in both action prediction and futurevideo synthesis.</description><author>Binjie Zhang, Mike Zheng Shou</author><pubDate>Thu, 28 Aug 2025 07:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19852v2</guid></item><item><title>MTS-Net: Dual-Enhanced Positional Multi-Head Self-Attention for 3D CT Diagnosis of May-Thurner Syndrome</title><link>http://arxiv.org/abs/2406.04680v3</link><description>May-Thurner Syndrome (MTS) is a vascular condition that affects over 20\% ofthe population and significantly increases the risk of iliofemoral deep venousthrombosis. Accurate and early diagnosis of MTS using computed tomography (CT)remains a clinical challenge due to the subtle anatomical compression andvariability across patients. In this paper, we propose MTS-Net, an end-to-end3D deep learning framework designed to capture spatial-temporal patterns fromCT volumes for reliable MTS diagnosis. MTS-Net builds upon 3D ResNet-18 byembedding a novel dual-enhanced positional multi-head self-attention (DEP-MHSA)module into the Transformer encoder of the network's final stages. The proposedDEP-MHSA employs multi-scale convolution and integrates positional embeddingsinto both attention weights and residual paths, enhancing spatial contextpreservation, which is crucial for identifying venous compression. To validateour approach, we curate the first publicly available dataset for MTS, MTS-CT,containing over 747 gender-balanced subjects with standard and enhanced CTscans. Experimental results demonstrate that MTS-Net achieves average 0.79accuracy, 0.84 AUC, and 0.78 F1-score, outperforming baseline models including3D ResNet, DenseNet-BC, and BabyNet. Our work not only introduces a newdiagnostic architecture for MTS but also provides a high-quality benchmarkdataset to facilitate future research in automated vascular syndrome detection.We make our code and dataset publicly availableat:https://github.com/Nutingnon/MTS_dep_mhsa.</description><author>Yixin Huang, Yiqi Jin, Ke Tao, Kaijian Xia, Jianfeng Gu, Lei Yu, Haojie Li, Lan Du, Cunjian Chen</author><pubDate>Thu, 28 Aug 2025 04:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04680v3</guid></item><item><title>Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks</title><link>http://arxiv.org/abs/2508.19884v2</link><description>Graph Neural Networks (GNNs) have shown remarkable performance in structureddata modeling tasks such as node classification. However, mainstream approachesgenerally rely on a large number of trainable parameters and fixed aggregationrules, making it difficult to adapt to graph data with strong structuralheterogeneity and complex feature distributions. This often leads toover-smoothing of node representations and semantic degradation. To addressthese issues, this paper proposes a parameter-free graph neural networkframework based on structural diversity, namely SDGNN (Structural-DiversityGraph Neural Network). The framework is inspired by structural diversity theoryand designs a unified structural-diversity message passing mechanism thatsimultaneously captures the heterogeneity of neighborhood structures and thestability of feature semantics, without introducing additional trainableparameters. Unlike traditional parameterized methods, SDGNN does not rely oncomplex model training, but instead leverages complementary modeling from bothstructure-driven and feature-driven perspectives, thereby effectively improvingadaptability across datasets and scenarios. Experimental results show that oneight public benchmark datasets and an interdisciplinary PubMed citationnetwork, SDGNN consistently outperforms mainstream GNNs under challengingconditions such as low supervision, class imbalance, and cross-domain transfer.This work provides a new theoretical perspective and general approach for thedesign of parameter-free graph neural networks, and further validates theimportance of structural diversity as a core signal in graph representationlearning. To facilitate reproducibility and further research, the fullimplementation of SDGNN has been released at:https://github.com/mingyue15694/SGDNN/tree/main</description><author>Mingyue Kong, Yinglong Zhang, Chengda Xu, Xuewen Xia, Xing Xu</author><pubDate>Thu, 28 Aug 2025 03:49:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19884v2</guid></item><item><title>Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks</title><link>http://arxiv.org/abs/2508.20038v2</link><description>Despite advances in improving large language model (LLM) to refuse to answermalicious instructions, widely used LLMs remain vulnerable to jailbreak attackswhere attackers generate instructions with distributions differing from safetyalignment corpora. New attacks expose LLMs' inability to recognize unseenmalicious instructions, highlighting a critical distributional mismatch betweentraining data and real-world attacks that forces developers into reactivepatching cycles. To tackle this challenge, we propose IMAGINE, a synthesisframework that leverages embedding space distribution analysis to generatejailbreak-like instructions. This approach effectively fills the distributionalgap between authentic jailbreak patterns and safety alignment corpora. IMAGINEfollows an iterative optimization process that dynamically evolves textgeneration distributions across iterations, thereby augmenting the coverage ofsafety alignment data distributions through synthesized data examples. Based onthe safety-aligned corpus enhanced through IMAGINE, our framework demonstratessignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2without compromising their utility.</description><author>Sheng Liu, Qiang Sheng, Danding Wang, Yang Li, Guang Yang, Juan Cao</author><pubDate>Thu, 28 Aug 2025 03:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20038v2</guid></item><item><title>Interact-Custom: Customized Human Object Interaction Image Generation</title><link>http://arxiv.org/abs/2508.19575v2</link><description>Compositional Customized Image Generation aims to customize multiple targetconcepts within generation content, which has gained attention for its wildapplication. Existing approaches mainly concentrate on the target entity'sappearance preservation, while neglecting the fine-grained interaction controlamong target entities. To enable the model of such interaction controlcapability, we focus on human object interaction scenario and propose the taskof Customized Human Object Interaction Image Generation(CHOI), whichsimultaneously requires identity preservation for target human object and theinteraction semantic control between them. Two primary challenges exist forCHOI:(1)simultaneous identity preservation and interaction control demandsrequire the model to decompose the human object into self-contained identityfeatures and pose-oriented interaction features, while the current HOI imagedatasets fail to provide ideal samples for such feature-decomposedlearning.(2)inappropriate spatial configuration between human and object maylead to the lack of desired interaction semantics. To tackle it, we firstprocess a large-scale dataset, where each sample encompasses the same pair ofhuman object involving different interactive poses. Then we design a two-stagemodel Interact-Custom, which firstly explicitly models the spatialconfiguration by generating a foreground mask depicting the interactionbehavior, then under the guidance of this mask, we generate the target humanobject interacting while preserving their identities features. Furthermore, ifthe background image and the union location of where the target human objectshould appear are provided by users, Interact-Custom also provides the optionalfunctionality to specify them, offering high content controllability. Extensiveexperiments on our tailored metrics for CHOI task demonstrate the effectivenessof our approach.</description><author>Zhu Xu, Zhaowen Wang, Yuxin Peng, Yang Liu</author><pubDate>Thu, 28 Aug 2025 01:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19575v2</guid></item><item><title>HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling</title><link>http://arxiv.org/abs/2508.20016v2</link><description>Schedulers are critical for optimal resource utilization in high-performancecomputing. Traditional methods to evaluate schedulers are limited topost-deployment analysis, or simulators, which do not model associatedinfrastructure. In this work, we present the first-of-its-kind integration ofscheduling and digital twins in HPC. This enables what-if studies to understandthe impact of parameter configurations and scheduling decisions on the physicalassets, even before deployment, or regarching changes not easily realizable inproduction. We (1) provide the first digital twin framework extended withscheduling capabilities, (2) integrate various top-tier HPC systems given theirpublicly available datasets, (3) implement extensions to integrate externalscheduling simulators. Finally, we show how to (4) implement and evaluateincentive structures, as-well-as (5) evaluate machine learning basedscheduling, in such novel digital-twin based meta-framework to prototypescheduling. Our work enables what-if scenarios of HPC systems to evaluatesustainability, and the impact on the simulated system.</description><author>Matthias Maiterth, Wesley H. Brewer, Jaya S. Kuruvella, Arunavo Dey, Tanzima Z. Islam, Kevin Menear, Dmitry Duplyakin, Rashadul Kabir, Tapasya Patki, Terry Jones, Feiyi Wang</author><pubDate>Thu, 28 Aug 2025 01:16:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20016v2</guid></item><item><title>Selective Retrieval-Augmentation for Long-Tail Legal Text Classification</title><link>http://arxiv.org/abs/2508.19997v2</link><description>Legal text classification is a fundamental NLP task in the legal domain.Benchmark datasets in this area often exhibit a long-tail label distribution,where many labels are underrepresented, leading to poor model performance onrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as asolution to this problem. SRA focuses on augmenting samples belonging tolow-frequency labels in the training set, preventing the introduction of noisefor well-represented classes, and requires no changes to the modelarchitecture. Retrieval is performed only from the training data to ensurethere is no potential information leakage, removing the need for externalcorpora simultaneously. The proposed SRA method is tested on two legal textclassification benchmark datasets with long-tail distributions: LEDGAR(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRAattains higher micro-F1 and macro-F1 scores compared to all current LexGLUEbaselines across both datasets, illustrating consistent improvements inlong-tail legal text classification.</description><author>Boheng Mao</author><pubDate>Thu, 28 Aug 2025 01:16:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19997v2</guid></item><item><title>Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging</title><link>http://arxiv.org/abs/2412.19512v3</link><description>Fine-tuning large language models (LLMs) for downstream tasks often leads tocatastrophic forgetting, notably degrading the safety of originally alignedmodels. While some existing methods attempt to restore safety by incorporatingadditional safety data, the quality of such data typically falls short of thatused in the original alignment process. Moreover, these high-quality safetydatasets are generally inaccessible, making it difficult to fully recover themodel's original safety. We ask: How can we preserve safety while improvingdownstream task performance without additional safety data? We show that simplymerging the weights of pre- and post-fine-tuned models effectively mitigatessafety degradation while enhancing performance. Experiments across differentdownstream tasks and models validate the method's practicality andeffectiveness.</description><author>Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee</author><pubDate>Thu, 28 Aug 2025 01:13:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19512v3</guid></item><item><title>A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation</title><link>http://arxiv.org/abs/2508.19507v2</link><description>In e-commerce, where users face a vast array of possible item choices,recommender systems are vital for helping them discover suitable items theymight otherwise overlook. While many recommender systems primarily rely on auser's purchase history, recent multi-behavior recommender systems incorporatevarious auxiliary user behaviors, such as item clicks and cart additions, toenhance recommendations. Despite their overall performance gains, theireffectiveness varies considerably between visited items (i.e., those a user hasinteracted with through auxiliary behaviors) and unvisited items (i.e., thosewith which the user has had no such interactions). Specifically, our analysisreveals that (1) existing multi-behavior recommender systems exhibit asignificant gap in recommendation quality between the two item types (visitedand unvisited items) and (2) achieving strong performance on both types with asingle model architecture remains challenging. To tackle these issues, wepropose a novel multi-behavior recommender system, MEMBER. It employs amixture-of-experts framework, with experts designed to recommend the two itemtypes, respectively. Each expert is trained using a self-supervised methodspecialized for its design goal. In our comprehensive experiments, we show theeffectiveness of MEMBER across both item types, achieving up to 65.46%performance gain over the best competitor in terms of Hit Ratio@20.</description><author>Kyungho Kim, Sunwoo Kim, Geon Lee, Kijung Shin</author><pubDate>Thu, 28 Aug 2025 01:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19507v2</guid></item><item><title>CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning</title><link>http://arxiv.org/abs/2508.20096v1</link><description>Autonomous agents for Graphical User Interfaces (GUIs) face significantchallenges in specialized domains such as scientific computing, where bothlong-horizon planning and precise execution are required. Existing approachessuffer from a trade-off: generalist agents excel at planning but perform poorlyin execution, while specialized agents demonstrate the opposite weakness.Recent compositional frameworks attempt to bridge this gap by combining aplanner and an actor, but they are typically static and non-trainable, whichprevents adaptation from experience. This is a critical limitation given thescarcity of high-quality data in scientific domains. To address theselimitations, we introduce CODA, a novel and trainable compositional frameworkthat integrates a generalist planner (Cerebrum) with a specialist executor(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,Specialization, we apply a decoupled GRPO approach to train an expert plannerfor each scientific application individually, bootstrapping from a small set oftask trajectories. In the second stage, Generalization, we aggregate allsuccessful trajectories from the specialized experts to build a consolidateddataset, which is then used for supervised fine-tuning of the final planner.This equips CODA with both robust execution and cross-domain generalization.Evaluated on four challenging applications from the ScienceBoard benchmark,CODA significantly outperforms baselines and establishes a new state of the artamong open-source models.</description><author>Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang</author><pubDate>Wed, 27 Aug 2025 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20096v1</guid></item><item><title>Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning</title><link>http://arxiv.org/abs/2508.20095v1</link><description>Multi-Robot Motion Planning (MRMP) involves generating collision-freetrajectories for multiple robots operating in a shared continuous workspace.While discrete multi-agent path finding (MAPF) methods are broadly adopted dueto their scalability, their coarse discretization severely limits trajectoryquality. In contrast, continuous optimization-based planners offerhigher-quality paths but suffer from the curse of dimensionality, resulting inpoor scalability with respect to the number of robots. This paper tackles thelimitations of these two approaches by introducing a novel framework thatintegrates discrete MAPF solvers with constrained generative diffusion models.The resulting framework, called Discrete-Guided Diffusion (DGD), has three keycharacteristics: (1) it decomposes the original nonconvex MRMP problem intotractable subproblems with convex configuration spaces, (2) it combinesdiscrete MAPF solutions with constrained optimization techniques to guidediffusion models capture complex spatiotemporal dependencies among robots, and(3) it incorporates a lightweight constraint repair mechanism to ensuretrajectory feasibility. The proposed method sets a new state-of-the-artperformance in large-scale, complex environments, scaling to 100 robots whileachieving planning efficiency and high success rates.</description><author>Jinhao Liang, Sven Koenig, Ferdinando Fioretto</author><pubDate>Wed, 27 Aug 2025 17:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20095v1</guid></item><item><title>LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning</title><link>http://arxiv.org/abs/2406.05881v6</link><description>Large language models (LLMs) have shown remarkable abilities in logicalreasoning, in-context learning, and code generation. However, translatingnatural language instructions into effective robotic control policies remains asignificant challenge, especially for tasks requiring long-horizon planning andoperating under sparse reward conditions. Hierarchical Reinforcement Learning(HRL) provides a natural framework to address this challenge in robotics;however, it typically suffers from non-stationarity caused by the changingbehavior of the lower-level policy during training, destabilizing higher-levelpolicy learning. We introduce LGR2, a novel HRL framework that leverages LLMsto generate language-guided reward functions for the higher-level policy. Bydecoupling high-level reward generation from low-level policy changes, LGR2fundamentally mitigates the non-stationarity problem in off-policy HRL,enabling stable and efficient learning. To further enhance sample efficiency insparse environments, we integrate goal-conditioned hindsight experiencerelabeling. Extensive experiments across simulated and real-world roboticnavigation and manipulation tasks demonstrate LGR2 outperforms bothhierarchical and non-hierarchical baselines, achieving over 55% success rateson challenging tasks and robust transfer to real robots, without additionalfine-tuning.</description><author>Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri</author><pubDate>Wed, 27 Aug 2025 17:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05881v6</guid></item><item><title>Unifying the Extremes: Developing a Unified Model for Detecting and Predicting Extremist Traits and Radicalization</title><link>http://arxiv.org/abs/2501.04820v2</link><description>The proliferation of ideological movements into extremist factions via socialmedia has become a global concern. While radicalization has been studiedextensively within the context of specific ideologies, our ability toaccurately characterize extremism in more generalizable terms remainsunderdeveloped. In this paper, we propose a novel method for extracting andanalyzing extremist discourse across a range of online community forums. Byfocusing on verbal behavioral signatures of extremist traits, we develop aframework for quantifying extremism at both user and community levels. Ourresearch identifies 11 distinct factors, which we term ``The ExtremistEleven,'' as a generalized psychosocial model of extremism. Applying our methodto various online communities, we demonstrate an ability to characterizeideologically diverse communities across the 11 extremist traits. Wedemonstrate the power of this method by analyzing user histories from membersof the incel community. We find that our framework accurately predicts whichusers join the incel community up to 10 months before their actual entry withan AUC of $&gt;0.6$, steadily increasing to AUC ~0.9 three to four months beforethe event. Further, we find that upon entry into an extremist forum, the userstend to maintain their level of extremism within the community, while stillremaining distinguishable from the general online discourse. Our findingscontribute to the study of extremism by introducing a more holistic,cross-ideological approach that transcends traditional, trait-specific models.</description><author>Allison Lahnala, Vasudha Varadarajan, Lucie Flek, H. Andrew Schwartz, Ryan L. Boyd</author><pubDate>Wed, 27 Aug 2025 17:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.04820v2</guid></item><item><title>Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</title><link>http://arxiv.org/abs/2508.20089v1</link><description>Labelling images of Lepidoptera (moths) from automated camera systems isvital for understanding insect declines. However, accurate speciesidentification is challenging due to domain shifts between curated images andnoisy field imagery. We propose a lightweight classification approach,combining limited expert-labelled field data with knowledge distillation fromthe high-performance BioCLIP2 foundation model into a ConvNeXt-tinyarchitecture. Experiments on 101 Danish moth species from AMI camera systemsdemonstrate that BioCLIP2 substantially outperforms other methods and that ourdistilled lightweight model achieves comparable accuracy with significantlyreduced computational cost. These insights offer practical guidelines for thedevelopment of efficient insect monitoring systems and bridging domain gaps forfine-grained classification.</description><author>Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas Høye</author><pubDate>Wed, 27 Aug 2025 17:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20089v1</guid></item><item><title>AudioStory: Generating Long-Form Narrative Audio with Large Language Models</title><link>http://arxiv.org/abs/2508.20088v1</link><description>Recent advances in text-to-audio (TTA) generation excel at synthesizing shortaudio clips but struggle with long-form narrative audio, which requirestemporal coherence and compositional reasoning. To address this gap, we proposeAudioStory, a unified framework that integrates large language models (LLMs)with TTA systems to generate structured, long-form audio narratives. AudioStorypossesses strong instruction-following reasoning generation capabilities. Itemploys LLMs to decompose complex narrative queries into temporally orderedsub-tasks with contextual cues, enabling coherent scene transitions andemotional tone consistency. AudioStory has two appealing features: (1)Decoupled bridging mechanism: AudioStory disentangles LLM-diffusercollaboration into two specialized components, i.e., a bridging query forintra-event semantic alignment and a residual query for cross-event coherencepreservation. (2) End-to-end training: By unifying instruction comprehensionand audio generation within a single end-to-end framework, AudioStoryeliminates the need for modular training pipelines while enhancing synergybetween components. Furthermore, we establish a benchmark AudioStory-10K,encompassing diverse domains such as animated soundscapes and natural soundnarratives. Extensive experiments show the superiority of AudioStory on bothsingle-audio generation and narrative audio generation, surpassing prior TTAbaselines in both instruction-following ability and audio fidelity. Our code isavailable at https://github.com/TencentARC/AudioStory</description><author>Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan</author><pubDate>Wed, 27 Aug 2025 17:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20088v1</guid></item><item><title>Pseudo-Simulation for Autonomous Driving</title><link>http://arxiv.org/abs/2506.04218v2</link><description>Existing evaluation paradigms for Autonomous Vehicles (AVs) face criticallimitations. Real-world evaluation is often challenging due to safety concernsand a lack of reproducibility, whereas closed-loop simulation can faceinsufficient realism or high computational costs. Open-loop evaluation, whilebeing efficient and data-driven, relies on metrics that generally overlookcompounding errors. In this paper, we propose pseudo-simulation, a novelparadigm that addresses these limitations. Pseudo-simulation operates on realdatasets, similar to open-loop evaluation, but augments them with syntheticobservations generated prior to evaluation using 3D Gaussian Splatting. Our keyidea is to approximate potential future states the AV might encounter bygenerating a diverse set of observations that vary in position, heading, andspeed. Our method then assigns a higher importance to synthetic observationsthat best match the AV's likely behavior using a novel proximity-basedweighting scheme. This enables evaluating error recovery and the mitigation ofcausal confusion, as in closed-loop benchmarks, without requiring sequentialinteractive simulation. We show that pseudo-simulation is better correlatedwith closed-loop simulations ($R^2=0.8$) than the best existing open-loopapproach ($R^2=0.7$). We also establish a public leaderboard for the communityto benchmark new methodologies with pseudo-simulation. Our code is available athttps://github.com/autonomousvision/navsim.</description><author>Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta</author><pubDate>Wed, 27 Aug 2025 17:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.04218v2</guid></item><item><title>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</title><link>http://arxiv.org/abs/2506.18088v2</link><description>Simulation-based data synthesis has emerged as a powerful paradigm foradvancing real-world robotic manipulation. Yet existing datasets remaininsufficient for robust bimanual manipulation due to (1) the lack of scalabletask generation methods and (2) oversimplified simulation environments. Wepresent RoboTwin 2.0, a scalable framework for automated, large-scalegeneration of diverse and realistic data, together with unified evaluationprotocols for dual-arm manipulation. At its core is RoboTwin-OD, an objectlibrary of 731 instances across 147 categories with semantic andmanipulation-relevant annotations. Building on this, we design an expert datasynthesis pipeline that leverages multimodal language models (MLLMs) andsimulation-in-the-loop refinement to automatically generate task-levelexecution code. To improve sim-to-real transfer, RoboTwin 2.0 appliesstructured domain randomization along five axes: clutter, lighting, background,tabletop height, and language, enhancing data diversity and policy robustness.The framework is instantiated across 50 dual-arm tasks and five robotembodiments. Empirically, it yields a 10.9% gain in code generation successrate. For downstream policy learning, a VLA model trained with synthetic dataplus only 10 real demonstrations achieves a 367% relative improvement over the10-demo baseline, while zero-shot models trained solely on synthetic dataobtain a 228% gain. These results highlight the effectiveness of RoboTwin 2.0in strengthening sim-to-real transfer and robustness to environmentalvariations. We release the data generator, benchmark, dataset, and code tosupport scalable research in robust bimanual manipulation. Project Page:https://robotwin-platform.github.io/, Code:https://github.com/robotwin-Platform/robotwin/.</description><author>Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, Weiliang Deng, Yubin Guo, Tian Nian, Xuanbing Xie, Qiangyu Chen, Kailun Su, Tianling Xu, Guodong Liu, Mengkang Hu, Huan-ang Gao, Kaixuan Wang, Zhixuan Liang, Yusen Qin, Xiaokang Yang, Ping Luo, Yao Mu</author><pubDate>Wed, 27 Aug 2025 17:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.18088v2</guid></item><item><title>Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning</title><link>http://arxiv.org/abs/2508.20083v1</link><description>Retrieval-Augmented Generation (RAG) has become a standard approach forimproving the reliability of large language models (LLMs). Prior workdemonstrates the vulnerability of RAG systems by misleading them intogenerating attacker-chosen outputs through poisoning the knowledge base.However, this paper uncovers that such attacks could be mitigated by the strong\textit{self-correction ability (SCA)} of modern LLMs, which can reject falsecontext once properly configured. This SCA poses a significant challenge forattackers aiming to manipulate RAG systems. In contrast to previous poisoning methods, which primarily target theknowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm thatcompromises the retriever itself to suppress the SCA and enforceattacker-chosen outputs. This compromisation enables the attacker tostraightforwardly embed anti-SCA instructions into the context provided to thegenerator, thereby bypassing the SCA. To this end, we present acontrastive-learning-based model editing technique that performs localized andstealthy edits, ensuring the retriever returns a malicious instruction only forspecific victim queries while preserving benign retrieval behavior. To furtherstrengthen the attack, we design an iterative co-optimization framework thatautomatically discovers robust instructions capable of bypassing prompt-baseddefenses. We extensively evaluate DisarmRAG across six LLMs and three QAbenchmarks. Our results show near-perfect retrieval of malicious instructions,which successfully suppress SCA and achieve attack success rates exceeding 90\%under diverse defensive prompts. Also, the edited retriever remains stealthyunder several detection methods, highlighting the urgent need forretriever-centric defenses.</description><author>Yanbo Dai, Zhenlan Ji, Zongjie Li, Kuan Li, Shuai Wang</author><pubDate>Wed, 27 Aug 2025 17:49:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20083v1</guid></item><item><title>Approximate Lifted Model Construction</title><link>http://arxiv.org/abs/2504.20784v3</link><description>Probabilistic relational models such as parametric factor graphs enableefficient (lifted) inference by exploiting the indistinguishability of objects.In lifted inference, a representative of indistinguishable objects is used forcomputations. To obtain a relational (i.e., lifted) representation, theAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACPalgorithm, however, requires underlying distributions, encoded aspotential-based factorisations, to exactly match to identify and exploitindistinguishabilities. Hence, ACP is unsuitable for practical applicationswhere potentials learned from data inevitably deviate even if associatedobjects are indistinguishable. To mitigate this problem, we introduce the$\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, whichallows for a deviation of potentials depending on a hyperparameter$\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploitsindistinguishabilities that are not exact. We prove that the approximationerror induced by $\varepsilon$-ACP is strictly bounded and our experiments showthat the approximation error is close to zero in practice.</description><author>Malte Luttermann, Jan Speller, Marcel Gehrke, Tanya Braun, Ralf Möller, Mattis Hartwig</author><pubDate>Wed, 27 Aug 2025 17:48:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.20784v3</guid></item><item><title>Evaluating the Fitness of Ontologies for the Task of Question Generation</title><link>http://arxiv.org/abs/2504.07994v2</link><description>Ontology-based question generation is an important application ofsemantic-aware systems that enables the creation of large question banks fordiverse learning environments. The effectiveness of these systems, both interms of the calibre and cognitive difficulty of the resulting questions,depends heavily on the quality and modelling approach of the underlyingontologies, making it crucial to assess their fitness for this task. To date,there has been no comprehensive investigation into the specific ontologyaspects or characteristics that affect the question generation process.Therefore, this paper proposes a set of requirements and task-specific metricsfor evaluating the fitness of ontologies for question generation tasks inpedagogical settings. Using the ROMEO methodology (a structured framework usedfor identifying task-specific metrics), a set of evaluation metrics have beenderived from an expert assessment of questions generated by a questiongeneration model. To validate the proposed metrics, we apply them to a set ofontologies previously used in question generation to illustrate how the metricscores align with and complement findings reported in earlier studies. Theanalysis confirms that ontology characteristics significantly impact theeffectiveness of question generation, with different ontologies exhibitingvarying performance levels. This highlights the importance of assessingontology quality with respect to Automatic Question Generation (AQG) tasks.</description><author>Samah Alkhuzaey, Floriana Grasso, Terry R. Payne, Valentina Tamma</author><pubDate>Wed, 27 Aug 2025 17:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.07994v2</guid></item><item><title>Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images</title><link>http://arxiv.org/abs/2508.20080v1</link><description>360-degree visual content is widely shared on platforms such as YouTube andplays a central role in virtual reality, robotics, and autonomous navigation.However, consumer-grade dual-fisheye systems consistently yield imperfectpanoramas due to inherent lens separation and angular distortions. In thiswork, we introduce a novel calibration framework that incorporates adual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approachnot only simulates the realistic visual artifacts produced by dual-fisheyecameras but also enables the synthesis of seamlessly rendered 360-degreeimages. By jointly optimizing 3D Gaussian parameters alongside calibrationvariables that emulate lens gaps and angular distortions, our frameworktransforms imperfect omnidirectional inputs into flawless novel view synthesis.Extensive evaluations on real-world datasets confirm that our method producesseamless renderings-even from imperfect images-and outperforms existing360-degree rendering models.</description><author>Changha Shin, Woong Oh Cho, Seon Joo Kim</author><pubDate>Wed, 27 Aug 2025 17:46:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20080v1</guid></item><item><title>Refining Czech GEC: Insights from a Multi-Experiment Approach</title><link>http://arxiv.org/abs/2506.22402v2</link><description>We present a grammar error correction (GEC) system that achieves state of theart for the Czech language. Our system is based on a neural network translationapproach with the Transformer architecture, and its key feature is itsreal-time synthetic generation pipeline, which dynamically augments sentenceswith artificial errors by introducing both language-agnostic and Czech-specificerrors. We conduct a comprehensive series of experiments, investigating theCzech GEC corpora as bases for synthetic error introduction, several errorgeneration strategies, domain balancing, tokenization granularity, model size,and data scaling during fine-tuning. Additionally, we evaluate the performanceof large language models (LLMs) on Czech GEC in both end-user and expertfine-tuning scenarios. Our best-performing model is superior both inperformance and computational efficiency. The source code and the trained modellinks are available on https://github.com/ufal/tsd2025-gec.</description><author>Petr Pechman, Milan Straka, Jana Straková, Jakub Náplava</author><pubDate>Wed, 27 Aug 2025 17:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.22402v2</guid></item><item><title>Anomaly Detection in Networked Bandits</title><link>http://arxiv.org/abs/2508.20076v1</link><description>The nodes' interconnections on a social network often reflect theirdependencies and information-sharing behaviors. Nevertheless, abnormal nodes,which significantly deviate from most of the network concerning patterns orbehaviors, can lead to grave consequences. Therefore, it is imperative todesign efficient online learning algorithms that robustly learn users'preferences while simultaneously detecting anomalies. We introduce a novel bandit algorithm to address this problem. Throughnetwork knowledge, the method characterizes the users' preferences andresiduals of feature information. By learning and analyzing these preferencesand residuals, it develops a personalized recommendation strategy for each userand simultaneously detects anomalies. We rigorously prove an upper bound on theregret of the proposed algorithm and experimentally compare it with severalstate-of-the-art collaborative contextual bandit algorithms on both syntheticand real-world datasets.</description><author>Xiaotong Cheng, Setareh Maghsudi</author><pubDate>Wed, 27 Aug 2025 17:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20076v1</guid></item><item><title>Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</title><link>http://arxiv.org/abs/2508.20072v1</link><description>Vision-Language-Action (VLA) models adapt large vision-language backbones tomap images and instructions to robot actions. However, prevailing VLA decoderseither generate actions autoregressively in a fixed left-to-right order orattach continuous diffusion or flow matching heads outside the backbone,demanding specialized training and iterative sampling that hinder a unified,scalable architecture. We present Discrete Diffusion VLA, a single-transformerpolicy that models discretized action chunks with discrete diffusion and istrained with the same cross-entropy objective as the VLM backbone. The designretains diffusion's progressive refinement paradigm while remaining nativelycompatible with the discrete token interface of VLMs. Our method achieves anadaptive decoding order that resolves easy action elements before harder onesand uses secondary remasking to revisit uncertain predictions across refinementrounds, which improves consistency and enables robust error correction. Thisunified decoder preserves pretrained vision language priors, supports paralleldecoding, breaks the autoregressive bottleneck, and reduces the number offunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnvBridge, improving over both autoregressive and continuous diffusion baselines.These findings indicate that discrete-diffusion action decoder supports preciseaction modeling and consistent training, laying groundwork for scaling VLA tolarger models and datasets.</description><author>Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo</author><pubDate>Wed, 27 Aug 2025 17:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20072v1</guid></item><item><title>Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems</title><link>http://arxiv.org/abs/2506.22971v3</link><description>This paper introduces a two-timescale hierarchical decentralized controlarchitecture for Cyber-Physical Systems (CPS). The system consists of a globalcontroller (GC), and N local controllers (LCs). The GC operates at a slowertimescale, imposing budget constraints on the actions of LCs, which function ata faster timescale. Applications can be found in energy grid planning, wildfiremanagement, and other decentralized resource allocation problems. We proposeand analyze two optimization frameworks for this setting: COpt and FOpt. InCOpt, both GC and LCs together optimize infinite-horizon discounted rewards,while in FOpt the LCs optimize finite-horizon episodic rewards, and the GCoptimizes infinite-horizon rewards. Although both frameworks share identicalreward functions, their differing horizons can lead to different optimalpolicies. In particular, FOpt grants greater autonomy to LCs by allowing theirpolicies to be determined only by local objectives, unlike COpt. To ourknowledge, these frameworks have not been studied in the literature. Weestablish the formulations, prove the existence of optimal policies, and provethe convergence of their value iteration algorithms. We further show that COptalways achieves a higher value function than FOpt and derive explicit bounds ontheir difference. Finally, we establish a set of sufficient structuralconditions under which the two frameworks become equivalent.</description><author>Kesav Kaza, Ramachandran Anantharaman, Rahul Meshram</author><pubDate>Wed, 27 Aug 2025 17:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.22971v3</guid></item><item><title>11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis</title><link>http://arxiv.org/abs/2508.20068v1</link><description>For human cognitive process, spatial reasoning and perception are closelyentangled, yet the nature of this interplay remains underexplored in theevaluation of multimodal large language models (MLLMs). While recent MLLMadvancements show impressive performance on reasoning, their capacity forhuman-like spatial cognition remains an open question. In this work, weintroduce a systematic evaluation framework to assess the spatial reasoningabilities of state-of-the-art MLLMs relative to human performance. Central toour work is 11Plus-Bench, a high-quality benchmark derived from realisticstandardized spatial aptitude tests. 11Plus-Bench also features fine-grainedexpert annotations of both perceptual complexity and reasoning process,enabling detailed instance-level analysis of model behavior. Through extensiveexperiments across 14 MLLMs and human evaluation, we find that current MLLMsexhibit early signs of spatial cognition. Despite a large performance gapcompared to humans, MLLMs' cognitive profiles resemble those of humans in thatcognitive effort correlates strongly with reasoning-related complexity.However, instance-level performance in MLLMs remains largely random, whereashuman correctness is highly predictable and shaped by abstract patterncomplexity. These findings highlight both emerging capabilities and limitationsin current MLLMs' spatial reasoning capabilities and provide actionableinsights for advancing model design.</description><author>Chengzu Li, Wenshan Wu, Huanyu Zhang, Qingtao Li, Zeyu Gao, Yan Xia, José Hernández-Orallo, Ivan Vulić, Furu Wei</author><pubDate>Wed, 27 Aug 2025 17:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20068v1</guid></item><item><title>Neural Conditional Simulation for Complex Spatial Processes</title><link>http://arxiv.org/abs/2508.20067v1</link><description>A key objective in spatial statistics is to simulate from the distribution ofa spatial process at a selection of unobserved locations conditional onobservations (i.e., a predictive distribution) to enable spatial prediction anduncertainty quantification. However, exact conditional simulation from thispredictive distribution is intractable or inefficient for many spatial processmodels. In this paper, we propose neural conditional simulation (NCS), ageneral method for spatial conditional simulation that is based on neuraldiffusion models. Specifically, using spatial masks, we implement a conditionalscore-based diffusion model that evolves Gaussian noise into samples from apredictive distribution when given a partially observed spatial field andspatial process parameters as inputs. The diffusion model relies on a neuralnetwork that only requires unconditional samples from the spatial process fortraining. Once trained, the diffusion model is amortized with respect to theobservations in the partially observed field, the number and locations of thoseobservations, and the spatial process parameters, and can therefore be used toconditionally simulate from a broad class of predictive distributions withoutretraining the neural network. We assess the NCS-generated simulations againstsimulations from the true conditional distribution of a Gaussian process model,and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnickprocess model for spatial extremes. In the latter case, we show that it is moreefficient and accurate to conditionally simulate using NCS than classical MCMCtechniques implemented in standard software. We conclude that NCS enablesefficient and accurate conditional simulation from spatial predictivedistributions that are challenging to sample from using traditional methods.</description><author>Julia Walchessen, Andrew Zammit-Mangion, Raphaël Huser, Mikael Kuusela</author><pubDate>Wed, 27 Aug 2025 17:21:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20067v1</guid></item><item><title>PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence</title><link>http://arxiv.org/abs/2508.20066v1</link><description>Cross-view geo-localization is a critical task for UAV navigation, eventdetection, and aerial surveying, as it enables matching between drone-capturedand satellite imagery. Most existing approaches embed multi-modal data into ajoint feature space to maximize the similarity of paired images. However, thesemethods typically assume perfect alignment of image pairs during training,which rarely holds true in real-world scenarios. In practice, factors such asurban canyon effects, electromagnetic interference, and adverse weatherfrequently induce GPS drift, resulting in systematic alignment shifts whereonly partial correspondences exist between pairs. Despite its prevalence, thissource of noisy correspondence has received limited attention in currentresearch. In this paper, we formally introduce and address the NoisyCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming tobridge the gap between idealized benchmarks and practical applications. To thisend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), anovel framework that partitions and augments training data based on estimateddata uncertainty through uncertainty-aware co-augmentation and evidentialco-training. Specifically, PAUL selectively augments regions with highcorrespondence confidence and utilizes uncertainty estimation to refine featurelearning, effectively suppressing noise from misaligned pairs. Distinct fromtraditional filtering or label correction, PAUL leverages both data uncertaintyand loss discrepancy for targeted partitioning and augmentation, thus providingrobust supervision for noisy samples. Comprehensive experiments validate theeffectiveness of individual components in PAUL,which consistently achievessuperior performance over other competitive noisy-correspondence-driven methodsin various noise ratios.</description><author>Zheng Li, Yanming Guo, WenZhe Liu, Xueyi Zhang, Zhaoyun Ding, Long Xu, Mingrui Lao</author><pubDate>Wed, 27 Aug 2025 17:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20066v1</guid></item><item><title>Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices</title><link>http://arxiv.org/abs/2508.20064v1</link><description>Age-related Macular Degeneration (AMD) is a prevalent eye condition affectingvisual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatmentshave been effective in slowing the progression of neovascular AMD, with betteroutcomes achieved through timely diagnosis and consistent monitoring. Trackingthe progression of neovascular activity in OCT scans of patients with exudativeAMD allows for the development of more personalized and effective treatmentplans. This was the focus of the Monitoring Age-related Macular DegenerationProgression in Optical Coherence Tomography (MARIO) challenge, in which weparticipated. In Task 1, which involved classifying the evolution between twopairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNNnetwork with model ensembling to further enhance the model's performance. ForTask 2, which focused on predicting progression over the next three monthsbased on current exam data, we proposed the Patch Progression MaskedAutoencoder that generates an OCT for the next exam and then classifies theevolution between the current OCT and the one generated using our solution fromTask 1. The results we achieved allowed us to place in the Top 10 for bothtasks. Some team members are part of the same organization as the challengeorganizers; therefore, we are not eligible to compete for the prize.</description><author>Philippe Zhang, Weili Jiang, Yihao Li, Jing Zhang, Sarah Matta, Yubo Tan, Hui Lin, Haoshen Wang, Jiangtian Pan, Hui Xu, Laurent Borderie, Alexandre Le Guilcher, Béatrice Cochener, Chubin Ou, Gwenolé Quellec, Mathieu Lamard</author><pubDate>Wed, 27 Aug 2025 17:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20064v1</guid></item><item><title>StepWiser: Stepwise Generative Judges for Wiser Reasoning</title><link>http://arxiv.org/abs/2508.19229v2</link><description>As models increasingly leverage multi-step reasoning strategies to solvecomplex problems, supervising the logical validity of these intermediate stepshas become a critical research challenge. Process reward models address this byproviding step-by-step feedback, but current approaches have two majordrawbacks: they typically function as classifiers without providingexplanations, and their reliance on supervised fine-tuning with static datasetslimits generalization. Inspired by recent advances, we reframe stepwise rewardmodeling from a classification task to a reasoning task itself. We thus proposea generative judge that reasons about the policy model's reasoning steps (i.e.,meta-reasons), outputting thinking tokens before delivering a final verdict.Our model, StepWiser, is trained by reinforcement learning using relativeoutcomes of rollouts. We show it provides (i) better judgment accuracy onintermediate steps than existing methods; (ii) can be used to improve thepolicy model at training time; and (iii) improves inference-time search.</description><author>Wei Xiong, Wenting Zhao, Weizhe Yuan, Olga Golovneva, Tong Zhang, Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Wed, 27 Aug 2025 17:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19229v2</guid></item><item><title>OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</title><link>http://arxiv.org/abs/2508.20063v1</link><description>Open-vocabulary (OV) 3D object detection is an emerging field, yet itsexploration through image-based methods remains limited compared to 3D pointcloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-viewindoor 3D object detector trained without human annotations. In particular,OpenM3D is a single-stage detector adapting the 2D-induced voxel features fromthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic3D localization loss requiring high-quality 3D pseudo boxes and avoxel-semantic alignment loss requiring diverse pre-trained CLIP features. Wefollow the training setting of OV-3DET where posed RGB-D images are given butno human annotations of 3D boxes or classes are available. We propose a 3DPseudo Box Generation method using a graph embedding technique that combines 2Dsegments into coherent 3D structures. Our pseudo-boxes achieve higher precisionand recall than other methods, including the method proposed in OV-3DET. Wefurther sample diverse CLIP features from 2D segments associated with eachcoherent 3D structure to align with the corresponding voxel feature. The key totraining a highly accurate single-stage detector requires both losses to belearned toward high-quality targets. At inference, OpenM3D, a highly efficientdetector, requires only multi-view images for input and demonstrates superioraccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoorbenchmarks compared to existing methods. We outperform a strong two-stagemethod that leverages our class-agnostic detector with a ViT CLIP-based OVclassifier and a baseline incorporating multi-view depth estimator on bothaccuracy and speed.</description><author>Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo</author><pubDate>Wed, 27 Aug 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20063v1</guid></item><item><title>Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks</title><link>http://arxiv.org/abs/2508.20056v1</link><description>Failure-Directed Search (FDS) is a significant complete generic searchalgorithm used in Constraint Programming (CP) to efficiently explore the searchspace, proven particularly effective on scheduling problems. This paperanalyzes FDS's properties, showing that minimizing the size of its search treeguided by ranked branching decisions is closely related to the Multi-armedbandit (MAB) problem. Building on this insight, MAB reinforcement learningalgorithms are applied to FDS, extended with problem-specific refinements andparameter tuning, and evaluated on the two most fundamental schedulingproblems, the Job Shop Scheduling Problem (JSSP) and Resource-ConstrainedProject Scheduling Problem (RCPSP). The resulting enhanced FDS, using the bestextended MAB algorithm and configuration, performs 1.7 times faster on the JSSPand 2.1 times faster on the RCPSP benchmarks compared to the originalimplementation in a new solver called OptalCP, while also being 3.5 timesfaster on the JSSP and 2.1 times faster on the RCPSP benchmarks than thecurrent state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,using only a 900-second time limit per instance, the enhanced FDS improved theexisting state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSPstandard open benchmark instances while also completely closing a few of them.</description><author>Vilém Heinz, Petr Vilím, Zdeněk Hanzálek</author><pubDate>Wed, 27 Aug 2025 17:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20056v1</guid></item><item><title>GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation</title><link>http://arxiv.org/abs/2508.14036v2</link><description>We introduce GeoSAM2, a prompt-controllable framework for 3D partsegmentation that casts the task as multi-view 2D mask prediction. Given atextureless object, we render normal and point maps from predefined viewpointsand accept simple 2D prompts - clicks or boxes - to guide part selection. Theseprompts are processed by a shared SAM2 backbone augmented with LoRA andresidual geometry fusion, enabling view-specific reasoning while preservingpretrained priors. The predicted masks are back-projected to the object andaggregated across views. Our method enables fine-grained, part-specific controlwithout requiring text prompts, per-shape optimization, or full 3D labels. Incontrast to global clustering or scale-based methods, prompts are explicit,spatially grounded, and interpretable. We achieve state-of-the-artclass-agnostic performance on PartObjaverse-Tiny and PartNetE, outperformingboth slow optimization-based pipelines and fast but coarse feedforwardapproaches. Our results highlight a new paradigm: aligning the paradigm of 3Dsegmentation with SAM2, leveraging interactive 2D inputs to unlockcontrollability and precision in object-level part understanding.</description><author>Ken Deng, Yunhan Yang, Jingxiang Sun, Xihui Liu, Yebin Liu, Ding Liang, Yan-Pei Cao</author><pubDate>Wed, 27 Aug 2025 17:10:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14036v2</guid></item><item><title>Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution</title><link>http://arxiv.org/abs/2508.16557v2</link><description>Diffusion-based real-world image super-resolution (Real-ISR) methods havedemonstrated impressive performance. To achieve efficient Real-ISR, many worksemploy Variational Score Distillation (VSD) to distill pre-trainedstable-diffusion (SD) model for one-step SR with a fixed timestep. However, dueto the different noise injection timesteps, the SD will perform differentgenerative priors. Therefore, a fixed timestep is difficult for these methodsto fully leverage the generative priors in SD, leading to suboptimalperformance. To address this, we propose a Time-Aware one-step DiffusionNetwork for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder,which projects the same image into different latent features based ontimesteps. Through joint dynamic variation of timesteps and latent features,the student model can better align with the input pattern distribution of thepre-trained SD, thereby enabling more effective utilization of SD's generativecapabilities. To better activate the generative prior of SD at differenttimesteps, we propose a Time-Aware VSD loss that bridges the timesteps of thestudent model and those of the teacher model, thereby producing more consistentgenerative prior guidance conditioned on timesteps. Additionally, thoughutilizing the generative prior in SD at different timesteps, our method cannaturally achieve controllable trade-offs between fidelity and realism bychanging the timestep condition. Experimental results demonstrate that ourmethod achieves both state-of-the-art performance and controllable SR resultswith only a single step.</description><author>Tainyi Zhang, Zheng-Peng Duan, Peng-Tao Jiang, Bo Li, Ming-Ming Cheng, Chun-Le Guo, Chongyi Li</author><pubDate>Wed, 27 Aug 2025 17:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16557v2</guid></item><item><title>A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions</title><link>http://arxiv.org/abs/2504.06121v9</link><description>Lane detection is a critical component of Advanced Driver Assistance Systems(ADAS). Existing lane detection algorithms generally perform well underfavorable weather conditions. However, their performance degrades significantlyin adverse conditions, such as fog, which increases the risk of trafficaccidents. This challenge is compounded by the lack of specialized datasets andmethods designed for foggy environments. To address this, we introduce theFoggyLane dataset, captured in real-world foggy scenarios, and synthesize twoadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lanedetection datasets. Furthermore, we propose a robust Fog-Enhanced Network forlane detection, incorporating a Global Feature Fusion Module (GFFM) to captureglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) tomodel the structural and positional relationships of lane instances, and aLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggyconditions. Comprehensive experiments demonstrate that our method achievesstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 onFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRTacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIAJetson AGX Orin, confirming its real-time capabilities and robustness in foggyenvironments.</description><author>Ronghui Zhang, Yuhang Ma, Tengfei Li, Ziyu Lin, Yueying Wu, Junzhou Chen, Lin Zhang, Jia Hu, Tony Z. Qiu, Konghui Guo</author><pubDate>Wed, 27 Aug 2025 16:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.06121v9</guid></item><item><title>Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful</title><link>http://arxiv.org/abs/2507.07101v2</link><description>Conventional wisdom dictates that small batch sizes make language modelpretraining and fine-tuning unstable, motivating gradient accumulation, whichtrades off the number of optimizer steps for a proportional increase in batchsize. While it is common to decrease the learning rate for smaller batch sizes,other hyperparameters are often held fixed. In this work, we revisit smallbatch sizes all the way down to batch size one, and we propose a rule forscaling Adam hyperparameters to small batch sizes. In particular, rather thanholding the decay rate of the second moment fixed across batch sizes, wepropose to hold its half-life fixed in terms of tokens. We find that smallbatch sizes (1) train stably, (2) are consistently more robust tohyperparameter choices, (3) achieve equal or better per-FLOP performance thanlarger batch sizes, and (4) notably enable stable language model training withvanilla SGD, even without momentum, despite storing no optimizer state.Building on these results, we provide practical recommendations for selecting abatch size and setting optimizer hyperparameters. We further recommend againstgradient accumulation unless training on multiple devices with multiple modelreplicas. Finally, we show that a small batch size combined with an optimizerwith a small state size can provide the performance benefits of fullfine-tuning while maintaining a similar memory footprint to LoRA.</description><author>Martin Marek, Sanae Lotfi, Aditya Somasundaram, Andrew Gordon Wilson, Micah Goldblum</author><pubDate>Wed, 27 Aug 2025 16:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.07101v2</guid></item><item><title>AraHealthQA 2025 Shared Task Description Paper</title><link>http://arxiv.org/abs/2508.20047v1</link><description>We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health QuestionAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-locatedwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabicmedical QA resources by offering two complementary tracks: {MentalQA}, focusingon Arabic mental health Q\&amp;A (e.g., anxiety, depression, stigma reduction), and{MedArabiQ}, covering broader medical domains such as internal medicine,pediatrics, and clinical decision making. Each track comprises multiplesubtasks, evaluation datasets, and standardized metrics, facilitating fairbenchmarking. The task was structured to promote modeling under realistic,multilingual, and culturally nuanced healthcare contexts. We outline thedataset creation, task design and evaluation framework, participationstatistics, baseline systems, and summarize the overall outcomes. We concludewith reflections on the performance trends observed and prospects for futureiterations in Arabic health QA.</description><author>Hassan Alhuzali, Farah Shamout, Muhammad Abdul-Mageed, Chaimae Abouzahir, Mouath Abu-Daoud, Ashwag Alasmari, Walid Al-Eisawi, Renad Al-Monef, Ali Alqahtani, Lama Ayash, Nizar Habash, Leen Kharouf</author><pubDate>Wed, 27 Aug 2025 16:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20047v1</guid></item><item><title>Scaling Decentralized Learning with FLock</title><link>http://arxiv.org/abs/2507.15349v2</link><description>Fine-tuning the large language models (LLMs) are prevented by the deficiencyof centralized control and the massive computing and communication overhead onthe decentralized schemes. While the typical standard federated learning (FL)supports data privacy, the central server requirement creates a single point ofattack and vulnerability to poisoning attacks. Generalizing the result in thisdirection to 70B-parameter models in the heterogeneous, trustless environmentshas turned out to be a huge, yet unbroken bottleneck. This paper introducesFLock, a decentralized framework for secure and efficient collaborative LLMfine-tuning. Integrating a blockchain-based trust layer with economicincentives, FLock replaces the central aggregator with a secure, auditableprotocol for cooperation among untrusted parties. We present the firstempirical validation of fine-tuning a 70B LLM in a secure, multi-domain,decentralized setting. Our experiments show the FLock framework defends againstbackdoor poisoning attacks that compromise standard FL optimizers and fosterssynergistic knowledge transfer. The resulting models show a &gt;68% reduction inadversarial attack success rates. The global model also demonstrates superiorcross-domain generalization, outperforming models trained in isolation on theirown specialized data.</description><author>Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo</author><pubDate>Wed, 27 Aug 2025 16:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.15349v2</guid></item><item><title>Model Science: getting serious about verification, explanation and control of AI systems</title><link>http://arxiv.org/abs/2508.20040v1</link><description>The growing adoption of foundation models calls for a paradigm shift fromData Science to Model Science. Unlike data-centric approaches, Model Scienceplaces the trained model at the core of analysis, aiming to interact, verify,explain, and control its behavior across diverse operational contexts. Thispaper introduces a conceptual framework for a new discipline called ModelScience, along with the proposal for its four key pillars: Verification, whichrequires strict, context-aware evaluation protocols; Explanation, which isunderstood as various approaches to explore of internal model operations;Control, which integrates alignment techniques to steer model behavior; andInterface, which develops interactive and visual explanation tools to improvehuman calibration and decision-making. The proposed framework aims to guide thedevelopment of credible, safe, and human-aligned AI systems.</description><author>Przemyslaw Biecek, Wojciech Samek</author><pubDate>Wed, 27 Aug 2025 16:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20040v1</guid></item><item><title>TAGS: 3D Tumor-Adaptive Guidance for SAM</title><link>http://arxiv.org/abs/2505.17096v2</link><description>Foundation models (FMs) such as CLIP and SAM have recently shown greatpromise in image segmentation tasks, yet their adaptation to 3D medicalimaging-particularly for pathology detection and segmentation-remainsunderexplored. A critical challenge arises from the domain gap between naturalimages and medical volumes: existing FMs, pre-trained on 2D data, struggle tocapture 3D anatomical context, limiting their utility in clinical applicationslike tumor segmentation. To address this, we propose an adaptation frameworkcalled TAGS: Tumor Adaptive Guidance for SAM, which unlocks 2D FMs for 3Dmedical tasks through multi-prompt fusion. By preserving most of thepre-trained weights, our approach enhances SAM's spatial feature extractionusing CLIP's semantic insights and anatomy-specific prompts. Extensiveexperiments on three open-source tumor segmentation datasets prove that ourmodel surpasses the state-of-the-art medical image segmentation models (+46.88%over nnUNet), interactive segmentation frameworks, and other establishedmedical FMs, including SAM-Med2D, SAM-Med3D, SegVol, Universal, 3D-Adapter, andSAM-B (at least +13% over them). This highlights the robustness andadaptability of our proposed framework across diverse medical segmentationtasks.</description><author>Sirui Li, Linkai Peng, Zheyuan Zhang, Gorkem Durak, Ulas Bagci</author><pubDate>Wed, 27 Aug 2025 16:47:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.17096v2</guid></item><item><title>Step-Audio 2 Technical Report</title><link>http://arxiv.org/abs/2507.16632v3</link><description>This paper presents Step-Audio 2, an end-to-end multi-modal large languagemodel designed for industry-strength audio understanding and speechconversation. By integrating a latent audio encoder and reasoning-centricreinforcement learning (RL), Step-Audio 2 achieves promising performance inautomatic speech recognition (ASR) and audio understanding. To facilitategenuine end-to-end speech conversation, Step-Audio 2 incorporates thegeneration of discrete audio tokens into language modeling, significantlyenhancing its responsiveness to paralinguistic information such as speakingstyles and emotions. To effectively leverage the rich textual and acousticknowledge in real-world data, Step-Audio 2 integrates retrieval-augmentedgeneration (RAG) and is able to call external tools such as web search tomitigate hallucination and audio search to switch timbres. Trained on millionsof hours of speech and audio data, Step-Audio 2 delivers intelligence andexpressiveness across diverse conversational scenarios. Evaluation resultsdemonstrate that Step-Audio 2 achieves state-of-the-art performance on variousaudio understanding and conversational benchmarks compared to other open-sourceand commercial solutions. Please visithttps://github.com/stepfun-ai/Step-Audio2 for more information.</description><author>Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Y</author><pubDate>Wed, 27 Aug 2025 16:42:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.16632v3</guid></item><item><title>Eigenvalue distribution of the Neural Tangent Kernel in the quadratic scaling</title><link>http://arxiv.org/abs/2508.20036v1</link><description>We compute the asymptotic eigenvalue distribution of the neural tangentkernel of a two-layer neural network under a specific scaling of dimension.Namely, if $X\in\mathbb{R}^{n\times d}$ is an i.i.d random matrix,$W\in\mathbb{R}^{d\times p}$ is an i.i.d $\mathcal{N}(0,1)$ matrix and$D\in\mathbb{R}^{p\times p}$ is a diagonal matrix with i.i.d bounded entries,we consider the matrix \[ \mathrm{NTK} = \frac{1}{d}XX^\top \odot \frac{1}{p} \sigma'\left( \frac{1}{\sqrt{d}}XW \right)D^2 \sigma'\left( \frac{1}{\sqrt{d}}XW \right)^\top \] where $\sigma'$ is a pseudo-Lipschitz function applied entrywise and underthe scaling $\frac{n}{dp}\to \gamma_1$ and $\frac{p}{d}\to \gamma_2$. Wedescribe the asymptotic distribution as the free multiplicative convolution ofthe Marchenko--Pastur distribution with a deterministic distribution dependingon $\sigma$ and $D$.</description><author>Lucas Benigni, Elliot Paquette</author><pubDate>Wed, 27 Aug 2025 16:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20036v1</guid></item><item><title>Pricing AI Model Accuracy</title><link>http://arxiv.org/abs/2504.13375v2</link><description>This paper examines the market for AI models in which firms compete toprovide accurate model predictions and consumers exhibit heterogeneouspreferences for model accuracy. We develop a consumer-firm duopoly model toanalyze how competition affects firms' incentives to improve model accuracy.Each firm aims to minimize its model's error, but this choice can often besuboptimal. Counterintuitively, we find that in a competitive market, firmsthat improve overall accuracy do not necessarily improve their profits. Rather,each firm's optimal decision is to invest further on the error dimension whereit has a competitive advantage. By decomposing model errors into false positiveand false negative rates, firms can reduce errors in each dimension throughinvestments. Firms are strictly better off investing on their superiordimension and strictly worse off with investments on their inferior dimension.Profitable investments adversely affect consumers but increase overall welfare.</description><author>Nikhil Kumar</author><pubDate>Wed, 27 Aug 2025 16:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.13375v2</guid></item><item><title>DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis</title><link>http://arxiv.org/abs/2508.20033v1</link><description>The ability to research and synthesize knowledge is central to humanexpertise and progress. An emerging class of systems promises these excitingcapabilities through generative research synthesis, performing retrieval overthe live web and synthesizing discovered sources into long-form, citedsummaries. However, evaluating such systems remains an open challenge: existingquestion-answering benchmarks focus on short-form factual responses, whileexpert-curated datasets risk staleness and data contamination. Both fail tocapture the complexity and evolving nature of real research synthesis tasks. Inthis work, we introduce DeepScholar-bench, a live benchmark and holistic,automated evaluation framework designed to evaluate generative researchsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXivpapers and focuses on a real research synthesis task: generating the relatedwork sections of a paper by retrieving, synthesizing, and citing priorresearch. Our evaluation framework holistically assesses performance acrossthree key dimensions, knowledge synthesis, retrieval quality, andverifiability. We also develop DeepScholar-base, a reference pipelineimplemented efficiently using the LOTUS API. Using the DeepScholar-benchframework, we perform a systematic evaluation of prior open-source systems,search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find thatDeepScholar-base establishes a strong baseline, attaining competitive or higherperformance than each other method. We also find that DeepScholar-bench remainsfar from saturated, with no system exceeding a score of $19\%$ across allmetrics. These results underscore the difficulty of DeepScholar-bench, as wellas its importance for progress towards AI systems capable of generativeresearch synthesis. We make our code available athttps://github.com/guestrin-lab/deepscholar-bench.</description><author>Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, Carlos Guestrin</author><pubDate>Wed, 27 Aug 2025 16:36:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20033v1</guid></item><item><title>Pruning Strategies for Backdoor Defense in LLMs</title><link>http://arxiv.org/abs/2508.20032v1</link><description>Backdoor attacks are a significant threat to the performance and integrity ofpre-trained language models. Although such models are routinely fine-tuned fordownstream NLP tasks, recent work shows they remain vulnerable to backdoorattacks that survive vanilla fine-tuning. These attacks are difficult to defendbecause end users typically lack knowledge of the attack triggers. Such attacksconsist of stealthy malicious triggers introduced through subtle syntactic orstylistic manipulations, which can bypass traditional detection and remain inthe model, making post-hoc purification essential. In this study, we explorewhether attention-head pruning can mitigate these threats without any knowledgeof the trigger or access to a clean reference model. To this end, we design andimplement six pruning-based strategies: (i) gradient-based pruning, (ii)layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2sparsification, (iv) randomized ensemble pruning, (v)reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.Each method iteratively removes the least informative heads while monitoringvalidation accuracy to avoid over-pruning. Experimental evaluation shows thatgradient-based pruning performs best while defending the syntactic triggers,whereas reinforcement learning and Bayesian pruning better withstand stylisticattacks.</description><author>Santosh Chapagain, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi</author><pubDate>Wed, 27 Aug 2025 16:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20032v1</guid></item><item><title>Apple Intelligence Foundation Language Models: Tech Report 2025</title><link>http://arxiv.org/abs/2507.13575v3</link><description>We introduce two multilingual, multimodal foundation language models thatpower Apple Intelligence features across Apple devices and services: i a3B-parameter on-device model optimized for Apple silicon through architecturalinnovations such as KV-cache sharing and 2-bit quantization-aware training; andii a scalable server model built on a novel Parallel-Track Mixture-of-ExpertsPT-MoE transformer that combines track parallelism, mixture-of-experts sparsecomputation, and interleaved global-local attention to deliver high qualitywith competitive cost on Apple's Private Cloud Compute platform. Both modelsare trained on large-scale multilingual and multimodal datasets sourced viaresponsible web crawling, licensed corpora, and high-quality synthetic data,then further refined with supervised fine-tuning and reinforcement learning ona new asynchronous platform. The resulting models support several additionallanguages while understanding images and executing tool calls. In publicbenchmarks and human evaluations, both the server model and the on-device modelmatch or surpass comparably sized open baselines. A new Swift-centric Foundation Models framework exposes guided generation,constrained tool calling, and LoRA adapter fine-tuning, allowing developers tointegrate these capabilities with a few lines of code. The latest advancementsin Apple Intelligence models are grounded in our Responsible AI approach withsafeguards like content filtering and locale-specific evaluation, as well asour commitment to protecting our users' privacy with innovations like PrivateCloud Compute.</description><author>Ethan Li, Anders Boesen Lindbo Larsen, Chen Zhang, Xiyou Zhou, Jun Qin, Dian Ang Yap, Narendran Raghavan, Xuankai Chang, Margit Bowler, Eray Yildiz, John Peebles, Hannah Gillis Coleman, Matteo Ronchi, Peter Gray, Keen You, Anthony Spalvieri-Kruse, Ruoming Pang, Reed Li, Yuli Yang, Emad Soroush, Zhiyun Lu, Crystal Xiao, Rong Situ, Jordan Huffaker, David Griffiths, Zaid Ahmed, Peng Zhang, Daniel Parilla, Asaf Liberman, Jennifer Mallalieu, Parsa Mazaheri, Qibin Chen, Manjot Bilkhu, Aonan Zhang, Eric Wang, Dave Nelson, Michael FitzMaurice, Thomas Voice, Jeremy Liu, Josh Shaffer, Shiwen Zhao, Prasanth Yadla, Farzin Rasteh, Pengsheng Guo, Arsalan Farooq, Jeremy Snow, Stephen Murphy, Tao Lei, Minsik Cho, George Horrell, Sam Dodge, Lindsay Hislop, Sumeet Singh, Alex Dombrowski, Aiswarya Raghavan, </author><pubDate>Wed, 27 Aug 2025 16:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.13575v3</guid></item><item><title>Large Language Models (LLMs) for Electronic Design Automation (EDA)</title><link>http://arxiv.org/abs/2508.20030v1</link><description>With the growing complexity of modern integrated circuits, hardware engineersare required to devote more effort to the full design-to-manufacturingworkflow. This workflow involves numerous iterations, making it bothlabor-intensive and error-prone. Therefore, there is an urgent demand for moreefficient Electronic Design Automation (EDA) solutions to accelerate hardwaredevelopment. Recently, large language models (LLMs) have shown remarkableadvancements in contextual comprehension, logical reasoning, and generativecapabilities. Since hardware designs and intermediate scripts can berepresented as text, integrating LLM for EDA offers a promising opportunity tosimplify and even automate the entire workflow. Accordingly, this paperprovides a comprehensive overview of incorporating LLMs into EDA, with emphasison their capabilities, limitations, and future opportunities. Three casestudies, along with their outlook, are introduced to demonstrate thecapabilities of LLMs in hardware design, testing, and optimization. Finally,future directions and challenges are highlighted to further explore thepotential of LLMs in shaping the next-generation EDA, providing valuableinsights for researchers interested in leveraging advanced AI technologies forEDA.</description><author>Kangwei Xu, Denis Schwachhofer, Jason Blocklove, Ilia Polian, Peter Domanski, Dirk Pflüger, Siddharth Garg, Ramesh Karri, Ozgur Sinanoglu, Johann Knechtel, Zhuorui Zhao, Ulf Schlichtmann, Bing Li</author><pubDate>Wed, 27 Aug 2025 16:33:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20030v1</guid></item><item><title>FaceEditTalker: Controllable Talking Head Generation with Facial Attribute Editing</title><link>http://arxiv.org/abs/2505.22141v2</link><description>Recent advances in audio-driven talking head generation have achievedimpressive results in lip synchronization and emotional expression. However,they largely overlook the crucial task of facial attribute editing. Thiscapability is indispensable for achieving deep personalization and expandingthe range of practical applications, including user-tailored digital avatars,engaging online education content, and brand-specific digital customer service.In these key domains, flexible adjustment of visual attributes, such ashairstyle, accessories, and subtle facial features, is essential for aligningwith user preferences, reflecting diverse brand identities and adapting tovarying contextual demands. In this paper, we present FaceEditTalker, a unifiedframework that enables controllable facial attribute manipulation whilegenerating high-quality, audio-synchronized talking head videos. Our methodconsists of two key components: an image feature space editing module, whichextracts semantic and detail features and allows flexible control overattributes like expression, hairstyle, and accessories; and an audio-drivenvideo generation module, which fuses these edited features with audio-guidedfacial landmarks to drive a diffusion-based generator. This design ensurestemporal coherence, visual fidelity, and identity preservation across frames.Extensive experiments on public datasets demonstrate that our method achievescomparable or superior performance to representative baseline methods inlip-sync accuracy, video quality, and attribute controllability. Project page:https://peterfanfan.github.io/FaceEditTalker/</description><author>Guanwen Feng, Zhiyuan Ma, Yunan Li, Jiahao Yang, Junwei Jing, Qiguang Miao</author><pubDate>Wed, 27 Aug 2025 16:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.22141v2</guid></item><item><title>Segmentation Assisted Incremental Test Time Adaptation in an Open World</title><link>http://arxiv.org/abs/2508.20029v1</link><description>In dynamic environments, unfamiliar objects and distribution shifts are oftenencountered, which challenge the generalization abilities of the deployedtrained models. This work addresses Incremental Test Time Adaptation of VisionLanguage Models, tackling scenarios where unseen classes and unseen domainscontinuously appear during testing. Unlike traditional Test Time Adaptationapproaches, where the test stream comes only from a predefined set of classes,our framework allows models to adapt simultaneously to both covariate and labelshifts, actively incorporating new classes as they emerge. Towards this goal,we establish a new benchmark for ITTA, integrating single image TTA methods forVLMs with active labeling techniques that query an oracle for samplespotentially representing unseen classes during test time. We propose asegmentation assisted active labeling module, termed SegAssist, which istraining free and repurposes the segmentation capabilities of VLMs to refineactive sample selection, prioritizing samples likely to belong to unseenclasses. Extensive experiments on several benchmark datasets demonstrate thepotential of SegAssist to enhance the performance of VLMs in real worldscenarios, where continuous adaptation to emerging data is essential.Project-page:https://manogna-s.github.io/segassist/</description><author>Manogna Sreenivas, Soma Biswas</author><pubDate>Wed, 27 Aug 2025 16:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20029v1</guid></item><item><title>Using item recommendations and LLMs in marketing email titles</title><link>http://arxiv.org/abs/2508.20024v1</link><description>E-commerce marketplaces make use of a number of marketing channels likeemails, push notifications, etc. to reach their users and stimulate purchases.Personalized emails especially are a popular touch point for marketers toinform users of latest items in stock, especially for those who stoppedvisiting the marketplace. Such emails contain personalized recommendationstailored to each user's interests, enticing users to buy relevant items. Acommon limitation of these emails is that the primary entry point, the title ofthe email, tends to follow fixed templates, failing to inspire enough interestin the contents. In this work, we explore the potential of large languagemodels (LLMs) for generating thematic titles that reflect the personalizedcontent of the emails. We perform offline simulations and conduct onlineexperiments on the order of millions of users, finding our techniques useful inimproving the engagement between customers and our emails. We highlight keyfindings and learnings as we productionize the safe and automated generation ofemail titles for millions of users.</description><author>Deddy Jobson, Muktti Shukla, Phuong Dinh, Julio Christian Young, Nick Pitton, Nina Chen, Ryan Ginstrom</author><pubDate>Wed, 27 Aug 2025 16:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20024v1</guid></item><item><title>FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring</title><link>http://arxiv.org/abs/2508.20021v1</link><description>Sensitive attributes like gender or age can lead to unfair predictions inmachine learning tasks such as predictive business process monitoring,particularly when used without considering context. We present FairLoop1, atool for human-guided bias mitigation in neural network-based predictionmodels. FairLoop distills decision trees from neural networks, allowing usersto inspect and modify unfair decision logic, which is then used to fine-tunethe original model towards fairer predictions. Compared to other approaches tofairness, FairLoop enables context-aware bias removal through humaninvolvement, addressing the influence of sensitive attributes selectivelyrather than excluding them uniformly.</description><author>Felix Möhrlein, Martin Käppel, Julian Neuberger, Sven Weinzierl, Lars Ackermann, Martin Matzner, Stefan Jablonski</author><pubDate>Wed, 27 Aug 2025 16:30:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20021v1</guid></item><item><title>GS: Generative Segmentation via Label Diffusion</title><link>http://arxiv.org/abs/2508.20020v1</link><description>Language-driven image segmentation is a fundamental task in vision-languageunderstanding, requiring models to segment regions of an image corresponding tonatural language expressions. Traditional methods approach this as adiscriminative problem, assigning each pixel to foreground or background basedon semantic alignment. Recently, diffusion models have been introduced to thisdomain, but existing approaches remain image-centric: they either (i) use imagediffusion models as visual feature extractors, (ii) synthesize segmentationdata via image generation to train discriminative models, or (iii) performdiffusion inversion to extract attention cues from pre-trained image diffusionmodels-thereby treating segmentation as an auxiliary process. In this paper, wepropose GS (Generative Segmentation), a novel framework that formulatessegmentation itself as a generative task via label diffusion. Instead ofgenerating images conditioned on label maps and text, GS reverses thegenerative process: it directly generates segmentation masks from noise,conditioned on both the input image and the accompanying language description.This paradigm makes label generation the primary modeling target, enablingend-to-end training with explicit control over spatial and semantic fidelity.To demonstrate the effectiveness of our approach, we evaluate GS on PanopticNarrative Grounding (PNG), a representative and challenging benchmark formultimodal segmentation that requires panoptic-level reasoning guided bynarrative captions. Experimental results show that GS significantly outperformsexisting discriminative and diffusion-based methods, setting a newstate-of-the-art for language-driven segmentation.</description><author>Yuhao Chen, Shubin Chen, Liang Lin, Guangrun Wang</author><pubDate>Wed, 27 Aug 2025 16:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20020v1</guid></item><item><title>Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence</title><link>http://arxiv.org/abs/2508.20019v1</link><description>Most existing Large Language Model (LLM)-based agent frameworks rely oncentralized orchestration, incurring high deployment costs, rigid communicationtopologies, and limited adaptability. To address these challenges, we introduceSymphony, a decentralized multi-agent system which enables lightweight LLMs onconsumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:(1) a decentralized ledger that records capabilities, (2) a Beacon-selectionprotocol for dynamic task allocation, and (3) weighted result voting based onCoTs. This design forms a privacy-saving, scalable, and fault-tolerantorchestration with low overhead. Empirically, Symphony outperforms existingbaselines on reasoning benchmarks, achieving substantial accuracy gains anddemonstrating robustness across models of varying capacities.</description><author>Ji Wang, Kashing Chen, Xinyuan Song, Ke Zhang, Lynn Ai, Eric Yang, Bill Shi</author><pubDate>Wed, 27 Aug 2025 16:27:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20019v1</guid></item><item><title>SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control</title><link>http://arxiv.org/abs/2508.20018v1</link><description>The rapid advancement of large vision language models (LVLMs) and agentsystems has heightened interest in mobile GUI agents that can reliablytranslate natural language into interface operations. Existing single-agentapproaches, however, remain limited by structural constraints. Althoughmulti-agent systems naturally decouple different competencies, recent progressin multi-agent reinforcement learning (MARL) has often been hindered byinefficiency and remains incompatible with current LVLM architectures. Toaddress these challenges, we introduce SWIRL, a staged workflow for interleavedreinforcement learning designed for multi-agent systems. SWIRL reformulatesMARL into a sequence of single-agent reinforcement learning tasks, updating oneagent at a time while keeping the others fixed. This formulation enables stabletraining and promotes efficient coordination across agents. Theoretically, weprovide a stepwise safety bound, a cross-round monotonic improvement theorem,and convergence guarantees on return, ensuring robust and principledoptimization. In application to mobile GUI control, SWIRL instantiates aNavigator that converts language and screen context into structured plans, andan Interactor that grounds these plans into executable atomic actions.Extensive experiments demonstrate superior performance on both high-level andlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strongcapability in multi-agent mathematical reasoning, underscoring its potential asa general framework for developing efficient and robust multi-agent systems.</description><author>Quanfeng Lu, Zhantao Ma, Shuai Zhong, Jin Wang, Dahai Yu, Michael K. Ng, Ping Luo</author><pubDate>Wed, 27 Aug 2025 16:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20018v1</guid></item><item><title>GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2410.05229v2</link><description>Recent advancements in Large Language Models (LLMs) have sparked interest intheir formal reasoning capabilities, particularly in mathematics. The GSM8Kbenchmark is widely used to assess the mathematical reasoning of models ongrade-school-level questions. While the performance of LLMs on GSM8K hassignificantly improved in recent years, it remains unclear whether theirmathematical reasoning capabilities have genuinely advanced, raising questionsabout the reliability of the reported metrics. To address these concerns, weconduct a large-scale study on several SOTA open and closed models. To overcomethe limitations of existing evaluations, we introduce GSM-Symbolic, an improvedbenchmark created from symbolic templates that allow for the generation of adiverse set of questions. GSM-Symbolic enables more controllable evaluations,providing key insights and more reliable metrics for measuring the reasoningcapabilities of models.Our findings reveal that LLMs exhibit noticeablevariance when responding to different instantiations of the same question.Specifically, the performance of all models declines when only the numericalvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,we investigate the fragility of mathematical reasoning in these models and showthat their performance significantly deteriorates as the number of clauses in aquestion increases. We hypothesize that this decline is because current LLMscannot perform genuine logical reasoning; they replicate reasoning steps fromtheir training data. Adding a single clause that seems relevant to the questioncauses significant performance drops (up to 65%) across all state-of-the-artmodels, even though the clause doesn't contribute to the reasoning chain neededfor the final answer. Overall, our work offers a more nuanced understanding ofLLMs' capabilities and limitations in mathematical reasoning.</description><author>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar</author><pubDate>Wed, 27 Aug 2025 16:24:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05229v2</guid></item><item><title>Scalable Bayesian Structure Learning for Gaussian Graphical Models Using Marginal Pseudo-likelihood</title><link>http://arxiv.org/abs/2307.00127v4</link><description>Bayesian methods for learning Gaussian graphical models offer a principledframework for quantifying model uncertainty and incorporating prior knowledge.However, their scalability is constrained by the computational cost of jointlyexploring graph structures and precision matrices. To address this challenge,we perform inference directly on the graph by integrating out the precisionmatrix. We adopt a marginal pseudo-likelihood approach, eliminating the need tocompute intractable normalizing constants and perform computationally intensiveprecision matrix sampling. Building on this framework, we developcontinuous-time (birth-death) and discrete-time (reversible jump) Markov chainMonte Carlo (MCMC) algorithms that efficiently explore the posterior over graphspace. We establish theoretical guarantees for posterior contraction,convergence, and graph selection consistency. The algorithms scale to largegraph spaces, enabling parallel exploration for graphs with over 1,000 nodes,while providing uncertainty quantification and supporting flexible priorspecification over the graph space. Extensive simulations show substantialcomputational gains over state-of-the-art Bayesian approaches withoutsacrificing graph recovery accuracy. Applications to human and mouse geneexpression datasets demonstrate the ability of our approach to recoverbiologically meaningful structures and quantify uncertainty in complexnetworks. An implementation is available in the R package BDgraph.</description><author>Reza Mohammadi, Marit Schoonhoven, Lucas Vogels, S. Ilker Birbil</author><pubDate>Wed, 27 Aug 2025 16:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00127v4</guid></item><item><title>Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment</title><link>http://arxiv.org/abs/2508.20015v1</link><description>Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that isbroadly misaligned with respect to human values. To understand when and howthis emergent misalignment occurs, we develop a comprehensive framework fordetecting and characterizing rapid transitions during fine-tuning using bothdistributional change detection methods as well as order parameters that areformulated in plain English and evaluated by an LLM judge. Using an objectivestatistical dissimilarity measure, we quantify how the phase transition thatoccurs during fine-tuning affects multiple aspects of the model. In particular,we assess what percentage of the total distributional change in model outputsis captured by different aspects, such as alignment or verbosity, providing adecomposition of the overall transition. We also find that the actualbehavioral transition occurs later in training than indicated by the peak inthe gradient norm alone. Our framework enables the automated discovery andquantification of language-based order parameters, which we demonstrate onexamples ranging from knowledge questions to politics and ethics.</description><author>Julian Arnold, Niels Lörch</author><pubDate>Wed, 27 Aug 2025 16:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20015v1</guid></item><item><title>Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach</title><link>http://arxiv.org/abs/2508.20013v1</link><description>This study addresses critical industrial challenges in e-commerce productcategorization, namely platform heterogeneity and the structural limitations ofexisting taxonomies, by developing and deploying a multimodal hierarchicalclassification framework. Using a dataset of 271,700 products from 40international fashion e-commerce platforms, we integrate textual features(RoBERTa), visual features (ViT), and joint vision--language representations(CLIP). We investigate fusion strategies, including early, late, andattention-based fusion within a hierarchical architecture enhanced by dynamicmasking to ensure taxonomic consistency. Results show that CLIP embeddingscombined via an MLP-based late-fusion strategy achieve the highest hierarchicalF1 (98.59\%), outperforming unimodal baselines. To address shallow orinconsistent categories, we further introduce a self-supervised ``productrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, whichdiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') withcluster purities above 86\%. Cross-platform experiments reveal adeployment-relevant trade-off: complex late-fusion methods maximize accuracywith diverse training data, while simpler early-fusion methods generalize moreeffectively to unseen platforms. Finally, we demonstrate the framework'sindustrial scalability through deployment in EURWEB's commercial transactionintelligence platform via a two-stage inference pipeline, combining alightweight RoBERTa stage with a GPU--accelerated multimodal stage to balancecost and accuracy.</description><author>Lotte Gross, Rebecca Walter, Nicole Zoppi, Adrien Justus, Alessandro Gambetti, Qiwei Han, Maximilian Kaiser</author><pubDate>Wed, 27 Aug 2025 16:16:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20013v1</guid></item><item><title>Emotions as Ambiguity-aware Ordinal Representations</title><link>http://arxiv.org/abs/2508.19193v2</link><description>Emotions are inherently ambiguous and dynamic phenomena, yet existingcontinuous emotion recognition approaches either ignore their ambiguity ortreat ambiguity as an independent and static variable over time. Motivated bythis gap in the literature, in this paper we introduce ambiguity-aware ordinalemotion representations, a novel framework that captures both the ambiguitypresent in emotion annotation and the inherent temporal dynamics of emotionaltraces. Specifically, we propose approaches that model emotion ambiguitythrough its rate of change. We evaluate our framework on two affective corpora-- RECOLA and GameVibe -- testing our proposed approaches on both bounded(arousal, valence) and unbounded (engagement) continuous traces. Our resultsdemonstrate that ordinal representations outperform conventionalambiguity-aware models on unbounded labels, achieving the highest ConcordanceCorrelation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,highlighting their effectiveness in modeling the traces' dynamics. For boundedtraces, ordinal representations excel in SDA, revealing their superior abilityto capture relative changes of annotated emotion traces.</description><author>Jingyao Wu, Matthew Barthet, David Melhart, Georgios N. Yannakakis</author><pubDate>Wed, 27 Aug 2025 16:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19193v2</guid></item><item><title>Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition</title><link>http://arxiv.org/abs/2506.14243v3</link><description>LiDAR-based place recognition serves as a crucial enabler for long-termautonomy in robotics and autonomous driving systems. Yet, prevailingmethodologies relying on handcrafted feature extraction face dual challenges:(1) Inconsistent point cloud density, induced by ego-motion dynamics andenvironmental disturbances during repeated traversals, leads to descriptorinstability, and (2) Representation fragility stems from reliance onsingle-level geometric abstractions that lack discriminative power instructurally complex scenarios. To address these limitations, we propose anovel framework that redefines 3D place recognition through density-agnosticgeometric reasoning. Specifically, we introduce an implicit 3D representationbased on elastic points, which is immune to the interference of original scenepoint cloud density and achieves the characteristic of uniform distribution.Subsequently, we derive the occupancy grid and normal vector information of thescene from this implicit representation. Finally, with the aid of these twotypes of information, we obtain descriptors that fuse geometric informationfrom both bird's-eye view (capturing macro-level spatial layouts) and 3Dsegment (encoding micro-scale surface geometries) perspectives. We conductedextensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT)across diverse environments. The experimental results demonstrate that ourmethod achieves state-of-the-art performance. Moreover, our approach strikes anoptimal balance between accuracy, runtime, and memory optimization forhistorical maps, showcasing excellent Resilient and scalability. Our code willbe open-sourced in the future.</description><author>Xiaohui Jiang, Haijiang Zhu, Chade Li, Fulin Tang, Ning An</author><pubDate>Wed, 27 Aug 2025 16:09:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.14243v3</guid></item><item><title>NAPER: Fault Protection for Real-Time Resource-Constrained Deep Neural Networks</title><link>http://arxiv.org/abs/2504.06591v2</link><description>Fault tolerance in Deep Neural Networks (DNNs) deployed onresource-constrained systems presents unique challenges for high-accuracyapplications with strict timing requirements. Memory bit-flips can severelydegrade DNN accuracy, while traditional protection approaches like TripleModular Redundancy (TMR) often sacrifice accuracy to maintain reliability,creating a three-way dilemma between reliability, accuracy, and timeliness. Weintroduce NAPER, a novel protection approach that addresses this challengethrough ensemble learning. Unlike conventional redundancy methods, NAPERemploys heterogeneous model redundancy, where diverse models collectivelyachieve higher accuracy than any individual model. This is complemented by anefficient fault detection mechanism and a real-time scheduler that prioritizesmeeting deadlines by intelligently scheduling recovery operations withoutinterrupting inference. Our evaluations demonstrate NAPER's superiority: 40%faster inference in both normal and fault conditions, maintained accuracy 4.2%higher than TMR-based strategies, and guaranteed uninterrupted operation evenduring fault recovery. NAPER effectively balances the competing demands ofaccuracy, reliability, and timeliness in real-time DNN applications</description><author>Rian Adam Rajagede, Muhammad Husni Santriaji, Muhammad Arya Fikriansyah, Hilal Hudan Nuha, Yanjie Fu, Yan Solihin</author><pubDate>Wed, 27 Aug 2025 16:07:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.06591v2</guid></item><item><title>X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</title><link>http://arxiv.org/abs/2505.07096v4</link><description>Human videos offer a scalable way to train robot manipulation policies, butlack the action labels needed by standard imitation learning algorithms.Existing cross-embodiment approaches try to map human motion to robot actions,but often fail when the embodiments differ significantly. We propose X-Sim, areal-to-sim-to-real framework that uses object motion as a dense andtransferable signal for learning robot policies. X-Sim starts by reconstructinga photorealistic simulation from an RGBD human video and tracking objecttrajectories to define object-centric rewards. These rewards are used to traina reinforcement learning (RL) policy in simulation. The learned policy is thendistilled into an image-conditioned diffusion policy using synthetic rolloutsrendered with varied viewpoints and lighting. To transfer to the real world,X-Sim introduces an online domain adaptation technique that aligns real andsimulated observations during deployment. Importantly, X-Sim does not requireany robot teleoperation data. We evaluate it across 5 manipulation tasks in 2environments and show that it: (1) improves task progress by 30% on averageover hand-tracking and sim-to-real baselines, (2) matches behavior cloning with10x less data collection time, and (3) generalizes to new camera viewpoints andtest-time changes. Code and videos are available athttps://portal-cornell.github.io/X-Sim/.</description><author>Prithwish Dan, Kushal Kedia, Angela Chao, Edward Weiyi Duan, Maximus Adrian Pace, Wei-Chiu Ma, Sanjiban Choudhury</author><pubDate>Wed, 27 Aug 2025 16:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.07096v4</guid></item><item><title>Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation</title><link>http://arxiv.org/abs/2508.19999v1</link><description>This paper introduces an algorithm to select demonstration examples forin-context learning of a query set. Given a set of $n$ examples, how can wequickly select $k$ out of $n$ to best serve as the conditioning for downstreaminference? This problem has broad applications in prompt tuning andchain-of-thought reasoning. Since model weights remain fixed during in-contextlearning, previous work has sought to design methods based on the similarity oftoken embeddings. This work proposes a new approach based on gradients of theoutput taken in the input embedding space. Our approach estimates model outputsthrough a first-order approximation using the gradients. Then, we apply thisestimation to multiple randomly sampled subsets. Finally, we aggregate thesampled subset outcomes to form an influence score for each demonstration, andselect $k$ most relevant examples. This procedure only requires pre-computingmodel outputs and gradients once, resulting in a linear-time algorithm relativeto model and training set sizes. Extensive experiments across various modelsand datasets validate the efficiency of our approach. We show that the gradientestimation procedure yields approximations of full inference with less than$\mathbf{1}\%$ error across six datasets. This allows us to scale up subsetselection that would otherwise run full inference by up to$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, andoutperform existing selection methods based on input embeddings by$\mathbf{11}\%$ on average.</description><author>Ziniu Zhang, Zhenshuo Zhang, Dongyue Li, Lu Wang, Jennifer Dy, Hongyang R. Zhang</author><pubDate>Wed, 27 Aug 2025 15:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19999v1</guid></item><item><title>Local Learning Rules for Out-of-Equilibrium Physical Generative Models</title><link>http://arxiv.org/abs/2506.19136v3</link><description>We show that the out-of-equilibrium driving protocol of score-basedgenerative models (SGMs) can be learned via local learning rules. The gradientwith respect to the parameters of the driving protocol is computed directlyfrom force measurements or from observed system dynamics. As a demonstration,we implement an SGM in a network of driven, nonlinear, overdamped oscillatorscoupled to a thermal bath. We first apply it to the problem of sampling from amixture of two Gaussians in 2D. Finally, we train a 12x12 oscillator network onthe MNIST dataset to generate images of handwritten digits 0 and 1.</description><author>Cyrill Bösch, Geoffrey Roeder, Marc Serra-Garcia, Ryan P. Adams</author><pubDate>Wed, 27 Aug 2025 15:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.19136v3</guid></item><item><title>ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning</title><link>http://arxiv.org/abs/2508.19996v1</link><description>Fine-tuning multi-turn dialogue systems requires high-quality supervision butoften suffers from degraded performance when exposed to low-quality data.Supervision errors in early turns can propagate across subsequent turns,undermining coherence and response quality. Existing methods typically addressdata quality via static prefiltering, which decouples quality control fromtraining and fails to mitigate turn-level error propagation. In this context,we propose ReSURE (Regularizing Supervision UnREliability), an adaptivelearning method that dynamically down-weights unreliable supervision withoutexplicit filtering. ReSURE estimates per-turn loss distributions usingWelford's online statistics and reweights sample losses on the fly accordingly.Experiments on both single-source and mixed-quality datasets show improvedstability and response quality. Notably, ReSURE enjoys positive Spearmancorrelations (0.21 ~ 1.0 across multiple benchmarks) between response scoresand number of samples regardless of data quality, which potentially paves theway for utilizing large-scale data effectively. Code is publicly available athttps://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.</description><author>Yiming Du, Yifan Xiang, Bin Liang, Dahua Lin, Kam-Fai Wong, Fei Tan</author><pubDate>Wed, 27 Aug 2025 15:54:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19996v1</guid></item><item><title>MathBuddy: A Multimodal System for Affective Math Tutoring</title><link>http://arxiv.org/abs/2508.19993v1</link><description>The rapid adoption of LLM-based conversational systems is alreadytransforming the landscape of educational technology. However, the currentstate-of-the-art learning models do not take into account the student'saffective states. Multiple studies in educational psychology support the claimthat positive or negative emotional states can impact a student's learningcapabilities. To bridge this gap, we present MathBuddy, an emotionally awareLLM-powered Math Tutor, which dynamically models the student's emotions andmaps them to relevant pedagogical strategies, making the tutor-studentconversation a more empathetic one. The student's emotions are captured fromthe conversational text as well as from their facial expressions. The student'semotions are aggregated from both modalities to confidently prompt our LLMTutor for an emotionally-aware response. We have effectively evaluated ourmodel using automatic evaluation metrics across eight pedagogical dimensionsand user studies. We report a massive 23 point performance gain using the winrate and a 3 point gain at an overall level using DAMR scores which stronglysupports our hypothesis of improving LLM-based tutor's pedagogical abilities bymodeling students' emotions.</description><author>Debanjana Kar, Leopold Böss, Dacia Braca, Sebastian Maximilian Dennerlein, Nina Christine Hubig, Philipp Wintersberger, Yufang Hou</author><pubDate>Wed, 27 Aug 2025 15:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19993v1</guid></item><item><title>Self-Supervised Pre-Training with Equilibrium Constraints</title><link>http://arxiv.org/abs/2508.19990v1</link><description>Self-supervised pre-training using unlabeled data is widely used in machinelearning. In this paper, we propose a new self-supervised pre-training approachto dealing with heterogeneous data. Instead of mixing all the data andminimizing the averaged global loss in the conventional way, we imposeadditional equilibrium constraints to ensure that the models optimizes eachsource of heterogeneous data to its local optima after $K$-step gradientdescent initialized from the model. We formulate this as a bilevel optimizationproblem, and use the first-order approximation method to solve the problem. Wediscuss its connection to model-agnostic meta learning (MAML). Experiments arecarried out on self-supervised pre-training using multi-domain and multilingualdatasets, demonstrating that the proposed approach can significantly improvethe adaptivity of the self-supervised pre-trained model for the downstreamsupervised fine-tuning tasks.</description><author>Xiaodong Cui, A F M Saif, Brian Kingsbury, Tianyi Chen</author><pubDate>Wed, 27 Aug 2025 15:48:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19990v1</guid></item><item><title>Unfolding AlphaFold's Bayesian Roots in Probability Kinematics</title><link>http://arxiv.org/abs/2505.19763v2</link><description>We present a novel theoretical interpretation of AlphaFold1 that reveals thepotential of generalized Bayesian updating for probabilistic deep learning. Theseminal breakthrough of AlphaFold1 in protein structure prediction by deeplearning relied on a learned potential energy function, in contrast to thelater end-to-end architectures of AlphaFold2 and AlphaFold3. While thispotential was originally justified by referring to physical potentials of meanforce (PMFs), we reinterpret AlphaFold1's potential as an instance of {\emprobability kinematics} -- also known as {\em Jeffrey conditioning} -- aprincipled but under-recognised generalization of conventional Bayesianupdating. Probability kinematics accommodates uncertain or {\em soft} evidencein the form of updated probabilities over a partition. This perspective revealsAlphaFold1's potential as a form of generalized Bayesian updating, rather thana thermodynamic potential. To confirm our probabilistic framework's scope andprecision, we analyze a synthetic 2D model in which an angular random walkprior is updated with evidence on distances via probability kinematics,mirroring AlphaFold1's approach. This theoretical contribution connectsAlphaFold1 to a broader class of well-justified Bayesian methods, allowingprecise quantification, surpassing merely qualitative heuristics based on PMFs.Our contribution is theoretical: we replace AlphaFold1's heuristic analogy witha principled probabilistic framework, tested in a controlled synthetic settingwhere correctness can be assessed. More broadly, our results point to theconsiderable promise of probability kinematics for probabilistic deep learning,by allowing the formulation of complex models from a few simpler components.</description><author>Thomas Hamelryck, Kanti V. Mardia</author><pubDate>Wed, 27 Aug 2025 15:47:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.19763v2</guid></item><item><title>AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios</title><link>http://arxiv.org/abs/2508.19988v1</link><description>Large Language Models (LLMs) have achieved high accuracy on complexcommonsense and mathematical problems that involve the composition of multiplereasoning steps. However, current compositional benchmarks testing these skillstend to focus on either commonsense or math reasoning, whereas LLM agentssolving real-world tasks would require a combination of both. In this work, weintroduce an Agentic Commonsense and Math benchmark (AgentCoMa), where eachcompositional task requires a commonsense reasoning step and a math reasoningstep. We test it on 61 LLMs of different sizes, model families, and trainingstrategies. We find that LLMs can usually solve both steps in isolation, yettheir accuracy drops by ~30% on average when the two are combined. This is asubstantially greater performance gap than the one we observe in priorcompositional benchmarks that combine multiple steps of the same reasoningtype. In contrast, non-expert human annotators can solve the compositionalquestions and the individual steps in AgentCoMa with similarly high accuracy.Furthermore, we conduct a series of interpretability studies to betterunderstand the performance gap, examining neuron patterns, attention maps andmembership inference. Our work underscores a substantial degree of modelbrittleness in the context of mixed-type compositional reasoning and offers atest bed for future improvement.</description><author>Lisa Alazraki, Lihu Chen, Ana Brassard, Joe Stacey, Hossein A. Rahmani, Marek Rei</author><pubDate>Wed, 27 Aug 2025 15:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19988v1</guid></item><item><title>Diffusion Language Models Know the Answer Before Decoding</title><link>http://arxiv.org/abs/2508.19982v1</link><description>Diffusion language models (DLMs) have recently emerged as an alternative toautoregressive approaches, offering parallel sequence generation and flexibletoken orders. However, their inference remains slower than that ofautoregressive models, primarily due to the cost of bidirectional attention andthe large number of refinement steps required for high quality outputs. In thiswork, we highlight and leverage an overlooked property of DLMs early answerconvergence: in many cases, the correct answer can be internally identified byhalf steps before the final decoding step, both under semi-autoregressive andrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%of instances, respectively, can be decoded correctly using only half of therefinement steps. Building on this observation, we introduce Prophet, atraining-free fast decoding paradigm that enables early commit decoding.Specifically, Prophet dynamically decides whether to continue refinement or togo "all-in" (i.e., decode all remaining tokens in one step), using theconfidence gap between the top-2 prediction candidates as the criterion. Itintegrates seamlessly into existing DLM implementations, incurs negligibleoverhead, and requires no additional training. Empirical evaluations ofLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces thenumber of decoding steps by up to 3.4x while preserving high generationquality. These results recast DLM decoding as a problem of when to stopsampling, and demonstrate that early decode convergence provides a simple yetpowerful mechanism for accelerating DLM inference, complementary to existingspeedup techniques. Our code is publicly available athttps://github.com/pixeli99/Prophet.</description><author>Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu</author><pubDate>Wed, 27 Aug 2025 15:40:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19982v1</guid></item><item><title>Evaluating Language Model Reasoning about Confidential Information</title><link>http://arxiv.org/abs/2508.19980v1</link><description>As language models are increasingly deployed as autonomous agents inhigh-stakes settings, ensuring that they reliably follow user-defined rules hasbecome a critical safety concern. To this end, we study whether language modelsexhibit contextual robustness, or the capability to adhere to context-dependentsafety specifications. For this analysis, we develop a benchmark (PasswordEval)that measures whether language models can correctly determine when a userrequest is authorized (i.e., with a correct password). We find that currentopen- and closed-source models struggle with this seemingly simple task, andthat, perhaps surprisingly, reasoning capabilities do not generally improveperformance. In fact, we find that reasoning traces frequently leakconfidential information, which calls into question whether reasoning tracesshould be exposed to users in such applications. We also scale the difficultyof our evaluation along multiple axes: (i) by adding adversarial user pressurethrough various jailbreaking strategies, and (ii) through longer multi-turnconversations where password verification is more challenging. Overall, ourresults suggest that current frontier models are not well-suited to handlingconfidential information, and that reasoning capabilities may need to betrained in a different manner to make them safer for release in high-stakessettings.</description><author>Dylan Sam, Alexander Robey, Andy Zou, Matt Fredrikson, J. Zico Kolter</author><pubDate>Wed, 27 Aug 2025 15:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19980v1</guid></item><item><title>Reducing Street Parking Search Time via Smart Assignment Strategies</title><link>http://arxiv.org/abs/2508.19979v1</link><description>In dense metropolitan areas, searching for street parking adds to trafficcongestion. Like many other problems, real-time assistants based on mobilephones have been proposed, but their effectiveness is understudied. This workquantifies how varying levels of user coordination and information availabilitythrough such apps impact search time and the probability of finding streetparking. Through a data-driven simulation of Madrid's street parking ecosystem,we analyze four distinct strategies: uncoordinated search (Unc-Agn),coordinated parking without awareness of non-users (Cord-Agn), an idealizedoracle system that knows the positions of all non-users (Cord-Oracle), and ournovel/practical Cord-Approx strategy that estimates non-users' behaviorprobabilistically. The Cord-Approx strategy, instead of requiring knowledge ofhow close non-users are to a certain spot in order to decide whether tonavigate toward it, uses past occupancy distributions to elongate physicaldistances between system users and alternative parking spots, and then solves aHungarian matching problem to dispatch accordingly. In high-fidelitysimulations of Madrid's parking network with real traffic data, users ofCord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutesfor non-users without an app. A zone-level snapshot shows that Cord-Approxreduces search time for system users by 72% (range = 67-76%) in central hubs,and up to 73% in residential areas, relative to non-users.</description><author>Behafarid Hemmatpour, Javad Dogani, Nikolaos Laoutaris</author><pubDate>Wed, 27 Aug 2025 15:39:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19979v1</guid></item><item><title>Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning</title><link>http://arxiv.org/abs/2508.19974v1</link><description>This study presents a machine learning framework for forecasting short-termfaults in industrial centrifugal pumps using real-time sensor data. Theapproach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes inadvance based on patterns extracted from historical operation. Two lookbackperiods, 60 minutes and 120 minutes, were evaluated using a sliding windowapproach. For each window, statistical features including mean, standarddeviation, minimum, maximum, and linear trend were extracted, and classimbalance was addressed using the SMOTE algorithm. Random Forest and XGBoostclassifiers were trained and tested on the labeled dataset. Results show thatthe Random Forest model achieved the best short-term forecasting performancewith a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the RandomForest model achieved 57.6\% recall at 5 minutes, and improved predictiveaccuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar butslightly lower performance. These findings highlight that optimal historylength depends on the prediction horizon, and that different fault patterns mayevolve at different timescales. The proposed method offers an interpretable andscalable solution for integrating predictive maintenance into real-timeindustrial monitoring systems.</description><author>Khaled M. A. Alghtus, Aiyad Gannan, Khalid M. Alhajri, Ali L. A. Al Jubouri, Hassan A. I. Al-Janahi</author><pubDate>Wed, 27 Aug 2025 15:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19974v1</guid></item><item><title>GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity</title><link>http://arxiv.org/abs/2508.19972v1</link><description>Object hallucination in large vision-language models presents a significantchallenge to their safe deployment in real-world applications. Recent workshave proposed object-level hallucination scores to estimate the likelihood ofobject hallucination; however, these methods typically adopt either a global orlocal perspective in isolation, which may limit detection reliability. In thispaper, we introduce GLSim, a novel training-free object hallucination detectionframework that leverages complementary global and local embedding similaritysignals between image and text modalities, enabling more accurate and reliablehallucination detection in diverse scenarios. We comprehensively benchmarkexisting object hallucination detection methods and demonstrate that GLSimachieves superior detection performance, outperforming competitive baselines bya significant margin.</description><author>Seongheon Park, Yixuan Li</author><pubDate>Wed, 27 Aug 2025 15:30:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19972v1</guid></item><item><title>Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing</title><link>http://arxiv.org/abs/2402.02817v3</link><description>Machine learning algorithms may have disparate impacts on protected groups.To address this, we develop methods for Bayes-optimal fair classification,aiming to minimize classification error subject to given group fairnessconstraints. We introduce the notion of \emph{linear disparity measures}, whichare linear functions of a probabilistic classifier; and \emph{bilineardisparity measures}, which are also linear in the group-wise regressionfunctions. We show that several popular disparity measures -- the deviationsfrom demographic parity, equality of opportunity, and predictive equality --are bilinear. We find the form of Bayes-optimal fair classifiers under a single lineardisparity measure, by uncovering a connection with the Neyman-Pearson lemma.For bilinear disparity measures, we are able to find the explicit form ofBayes-optimal fair classifiers as group-wise thresholding rules with explicitlycharacterized thresholds. We develop similar algorithms for when protectedattribute cannot be used at the prediction phase. Moreover, we obtain analogoustheoretical characterizations of optimal classifiers for a multi-classprotected attribute and for equalized odds. Leveraging our theoretical results, we design methods that learn fairBayes-optimal classifiers under bilinear disparity constraints. Our methodscover three popular approaches to fairness-aware classification, viapre-processing (Fair Up- and Down-Sampling), in-processing (Fair cost-sensitiveClassification) and post-processing (a Fair Plug-In Rule). Our methods controldisparity directly while achieving near-optimal fairness-accuracy tradeoffs. Weshow empirically that our methods have state-of-the-art performance compared toexisting algorithms. In particular, our pre-processing method can a reachhigher accuracy than prior pre-processing methods at low disparity levels.</description><author>Xianli Zeng, Kevin Jiang, Guang Cheng, Edgar Dobriban</author><pubDate>Wed, 27 Aug 2025 15:29:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02817v3</guid></item><item><title>mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks</title><link>http://arxiv.org/abs/2506.08400v3</link><description>Large Language models (LLMs) have demonstrated impressive performance on awide range of tasks, including in multimodal settings such as speech. However,their evaluation is often limited to English and a few high-resource languages.For low-resource languages, there is no standardized evaluation benchmark. Inthis paper, we address this gap by introducing mSTEB, a new benchmark toevaluate the performance of LLMs on a wide range of tasks covering languageidentification, text classification, question answering, and translation taskson both speech and text modalities. We evaluated the performance of leadingLLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art openmodels such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap inperformance between high-resource and low-resource languages, especially forlanguages spoken in Africa and Americas/Oceania. Our findings show that moreinvestment is needed to address their under-representation in LLMs coverage.</description><author>Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, David Ifeoluwa Adelani</author><pubDate>Wed, 27 Aug 2025 15:27:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.08400v3</guid></item><item><title>On Domain-Adaptive Post-Training for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2411.19930v4</link><description>Adapting general multimodal large language models (MLLMs) to specificdomains, such as scientific and industrial fields, is highly significant inpromoting their practical applications. This paper systematically investigatesdomain adaptation of MLLMs via post-training, focusing on data synthesis,training pipeline, and task evaluation. (1) Data Synthesis: Using onlyopen-source models, we develop a generate-then-filter pipeline that curatesdiverse visual instruction tasks based on domain-specific image-caption pairs.The resulting data surpass the data synthesized by manual rules or strongclosed-source models in enhancing domain-specific performance. (2) TrainingPipeline: Unlike general MLLMs that typically adopt a two-stage trainingparadigm, we find that a single-stage approach is more effective for domainadaptation. (3) Task Evaluation: We conduct extensive experiments inhigh-impact domains such as biomedicine, food, and remote sensing, bypost-training a variety of MLLMs and then evaluating MLLM performance onvarious domain-specific tasks. Finally, we fully open-source our models, code,and data to encourage future research in this area.</description><author>Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang</author><pubDate>Wed, 27 Aug 2025 15:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19930v4</guid></item><item><title>Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models</title><link>http://arxiv.org/abs/2508.19967v1</link><description>Geo-localization is the task of identifying the location of an image usingvisual cues alone. It has beneficial applications, such as improving disasterresponse, enhancing navigation, and geography education. Recently,Vision-Language Models (VLMs) are increasingly demonstrating capabilities asaccurate image geo-locators. This brings significant privacy risks, includingthose related to stalking and surveillance, considering the widespread uses ofAI models and sharing of photos on social media. The precision of these modelsis likely to improve in the future. Despite these risks, there is little workon systematically evaluating the geolocation precision of Generative VLMs,their limits and potential for unintended inferences. To bridge this gap, weconduct a comprehensive assessment of the geolocation capabilities of 25state-of-the-art VLMs on four benchmark image datasets captured in diverseenvironments. Our results offer insight into the internal reasoning of VLMs andhighlight their strengths, limitations, and potential societal risks. Ourfindings indicate that current VLMs perform poorly on generic street-levelimages yet achieve notably high accuracy (61\%) on images resembling socialmedia content, raising significant and urgent privacy concerns.</description><author>Oliver Grainge, Sania Waheed, Jack Stilgoe, Michael Milford, Shoaib Ehsan</author><pubDate>Wed, 27 Aug 2025 15:21:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19967v1</guid></item><item><title>Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation</title><link>http://arxiv.org/abs/2508.19966v1</link><description>Despite its significance, Arabic, a linguistically rich and morphologicallycomplex language, faces the challenge of being under-resourced. The scarcity oflarge annotated datasets hampers the development of accurate tools forsubjectivity analysis in Arabic. Recent advances in deep learning andTransformers have proven highly effective for text classification in Englishand French. This paper proposes a new approach for subjectivity assessment inArabic textual data. To address the dearth of specialized annotated datasets,we developed a comprehensive dataset, AraDhati+, by leveraging existing Arabicdatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, wefine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, andArabianGPT) on AraDhati+ for effective subjectivity classification.Furthermore, we experimented with an ensemble decision approach to harness thestrengths of individual models. Our approach achieves a remarkable accuracy of97.79\,\% for Arabic subjectivity classification. Results demonstrate theeffectiveness of the proposed approach in addressing the challenges posed bylimited resources in Arabic language processing.</description><author>Slimane Bellaouar, Attia Nehar, Soumia Souffi, Mounia Bouameur</author><pubDate>Wed, 27 Aug 2025 15:20:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19966v1</guid></item><item><title>Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants</title><link>http://arxiv.org/abs/2508.19963v1</link><description>Optimizing modern production plants using the job-shop principle is a knownhard problem. For very large plants, like semiconductor fabs, the problembecomes unsolvable on a plant-wide scale in a reasonable amount of time usingclassical linear optimization. An alternative approach is the use of swarmintelligence algorithms. These have been applied to the job-shop problembefore, but often in a centrally calculated way where they are applied to thesolution space, but they can be implemented in a bottom-up fashion to avoidglobal result computation as well. One of the problems in semiconductorproduction is that the production process requires a lot of switching betweenmachines that process lots one after the other and machines that processbatches of lots at once, often with long processing times. In this paper, weaddress this switching problem with the ``boids'' flocking algorithm that wasoriginally used in robotics and movie industry. The flocking behavior is abio-inspired algorithm that uses only local information and interaction basedon simple heuristics. We show that this algorithm addresses these validconsiderations in production plant optimization, as it reacts to the switchingof machine kinds similar to how a swarm of flocking animals would react toobstacles in its course.</description><author>M. Umlauft, M. Schranz</author><pubDate>Wed, 27 Aug 2025 15:17:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19963v1</guid></item><item><title>Analysis and Synthesis Denoisers for Forward-Backward Plug-and-Play Algorithms</title><link>http://arxiv.org/abs/2411.13276v3</link><description>In this work we study the behavior of the forward-backward (FB) algorithmwhen the proximity operator is replaced by a sub-iterative procedure toapproximate a Gaussian denoiser, in a Plug-and-Play (PnP) fashion. Inparticular, we consider both analysis and synthesis Gaussian denoisers within adictionary framework, obtained by unrolling dual-FB iterations or FBiterations, respectively. We analyze the associated minimization problems aswell as the asymptotic behavior of the resulting FB-PnP iterations. Inparticular, we show that the synthesis Gaussian denoising problem can be viewedas a proximity operator. For each case, analysis and synthesis, we show thatthe FB-PnP algorithms solve the same problem whether we use only one or aninfinite number of sub-iteration to solve the denoising problem at eachiteration. To this aim, we show that each "one sub-iteration" strategy withinthe FB-PnP can be interpreted as a primal-dual algorithm when a warm-restartstrategy is used. We further present similar results when using a Moreau-Yosidasmoothing of the global problem, for an arbitrary number of sub-iterations.Finally, we provide numerical simulations to illustrate our theoreticalresults. In particular we first consider a toy compressive sensing example, aswell as an image restoration problem in a deep dictionary framework.</description><author>Matthieu Kowalski, Benoît Malézieux, Thomas Moreau, Audrey Repetti</author><pubDate>Wed, 27 Aug 2025 15:10:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.13276v3</guid></item><item><title>Global Permutation Entropy</title><link>http://arxiv.org/abs/2508.19955v1</link><description>Permutation Entropy, introduced by Bandt and Pompe, is a widely usedcomplexity measure for real-valued time series that is based on the relativeorder of values within consecutive segments of fixed length. Afterstandardizing each segment to a permutation and computing the frequencydistribution of these permutations, Shannon Entropy is then applied to quantifythe series' complexity. We introduce Global Permutation Entropy (GPE), a novelindex that considers all possible patterns of a given length, includingnon-consecutive ones. Its computation relies on recently developed algorithmsthat enable the efficient extraction of full permutation profiles. Weillustrate some properties of GPE and demonstrate its effectiveness throughexperiments on synthetic datasets, showing that it reveals structuralinformation not accessible through standard permutation entropy. We provide aJulia package for the calculation of GPE at`https://github.com/AThreeH1/Global-Permutation-Entropy'.</description><author>Abhijeet Avhale, Joscha Diehl, Niraj Velankar, Emanuele Verri</author><pubDate>Wed, 27 Aug 2025 15:08:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19955v1</guid></item><item><title>UltraRay: Introducing Full-Path Ray Tracing in Physics-Based Ultrasound Simulation</title><link>http://arxiv.org/abs/2501.05828v3</link><description>Traditional ultrasound simulators solve the wave equation to model pressuredistribution fields, achieving high accuracy but requiring significantcomputational time and resources. To address this, ray tracing approaches havebeen introduced, modeling wave propagation as rays interacting with boundariesand scatterers. However, existing models simplify ray propagation, generatingechoes at interaction points without considering return paths to the sensor.This can result in unrealistic artifacts and necessitates careful scene tuningfor plausible results. We propose a novel ultrasound simulation pipeline thatutilizes a ray tracing algorithm to generate echo data, tracing each ray fromthe transducer through the scene and back to the sensor. To replicate advancedultrasound imaging, we introduce a ray emission scheme optimized for plane waveimaging, incorporating delay and steering capabilities. Furthermore, weintegrate a standard signal processing pipeline to simulate end-to-endultrasound image formation. We showcase the efficacy of the proposed pipelineby modeling synthetic scenes featuring highly reflective objects, such asbones. In doing so, our proposed approach, UltraRay, not only enhances theoverall visual quality but also improves the realism of the simulated images byaccurately capturing secondary reflections and reducing unnatural artifacts. Bybuilding on top of a differentiable framework, the proposed pipeline lays thegroundwork for a fast and differentiable ultrasound simulation tool necessaryfor gradient-based optimization, enabling advanced ultrasound beamformingstrategies, neural network integration, and accurate inverse scenereconstruction.</description><author>Felix Duelmer, Mohammad Farid Azampour, Magdalena Wysocki, Nassir Navab</author><pubDate>Wed, 27 Aug 2025 15:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.05828v3</guid></item><item><title>Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework</title><link>http://arxiv.org/abs/2508.19946v1</link><description>In this paper, we present a comprehensive study and analysis of the Chan-Vesealgorithm for image segmentation. We employ a discretized scheme derived fromthe empirical study of the Chan-Vese model's functional energy and its partialdifferential equation based on its level set function. We provide a proof ofthe results and an implementation using MATLAB. Leveraging modern computervision methodologies, we propose a functional segmentation loss based on activecontours, utilizing pytorch.nn.ModuleLoss and a level set based on theChan-Vese algorithm. We compare our results with common computer visionsegmentation datasets and evaluate the performance of classical loss functionsagainst our proposed method. All code and materials used are available athttps://github.com/gguzzy/chan_vese_functional_loss.</description><author>Gianluca Guzzetta</author><pubDate>Wed, 27 Aug 2025 15:01:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19946v1</guid></item><item><title>Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions</title><link>http://arxiv.org/abs/2508.19945v1</link><description>We present an inverse dynamic game-based algorithm to learn parametricconstraints from a given dataset of local generalized Nash equilibriuminteractions between multiple agents. Specifically, we introduce mixed-integerlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of theinteracting agents, which recover constraints consistent with the Nashstationarity of the interaction demonstrations. We establish theoreticalguarantees that our method learns inner approximations of the true safe andunsafe sets, as well as limitations of constraint learnability fromdemonstrations of Nash equilibrium interactions. We also use the interactionconstraints recovered by our method to design motion plans that robustlysatisfy the underlying constraints. Across simulations and hardwareexperiments, our methods proved capable of inferring constraints and designinginteractive motion plans for various classes of constraints, both convex andnon-convex, from interaction demonstrations of agents with nonlinear dynamics.</description><author>Zhouyu Zhang, Chih-Yuan Chiu, Glen Chou</author><pubDate>Wed, 27 Aug 2025 15:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19945v1</guid></item><item><title>KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts</title><link>http://arxiv.org/abs/2508.19944v1</link><description>Understanding and reasoning over text within visual contexts poses asignificant challenge for Vision-Language Models (VLMs), given the complexityand diversity of real-world scenarios. To address this challenge, text-richVisual Question Answering (VQA) datasets and benchmarks have emerged forhigh-resource languages like English. However, a critical gap persists forlow-resource languages such as Korean, where the lack of comprehensivebenchmarks hinders robust model evaluation and comparison. To bridge this gap,we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-richVQA Attuned to diverse visual contexts. KRETA facilitates an in-depthevaluation of both visual text understanding and reasoning capabilities, whilealso supporting a multifaceted assessment across 15 domains and 26 image types.Additionally, we introduce a semi-automated VQA generation pipelinespecifically optimized for text-rich settings, leveraging refined stepwiseimage decomposition and a rigorous seven-metric evaluation protocol to ensuredata quality. While KRETA is tailored for Korean, we hope our adaptable andextensible pipeline will facilitate the development of similar benchmarks inother languages, thereby accelerating multilingual VLM research. The code anddataset for KRETA are available at https://github.com/tabtoyou/KRETA.</description><author>Taebaek Hwang, Minseo Kim, Gisang Lee, Seonuk Kim, Hyunjun Eun</author><pubDate>Wed, 27 Aug 2025 15:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19944v1</guid></item><item><title>Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure</title><link>http://arxiv.org/abs/2507.22893v2</link><description>Contemporary human-AI interaction research overlooks how AI systemsfundamentally reshape human cognition pre-consciously, a critical blind spotfor understanding distributed cognition. This paper introduces "CognitiveInfrastructure Studies" (CIS) as a new interdisciplinary domain toreconceptualize AI as "cognitive infrastructures": foundational, ofteninvisible systems conditioning what is knowable and actionable in digitalsocieties. These semantic infrastructures transport meaning, operate throughanticipatory personalization, and exhibit adaptive invisibility, making theirinfluence difficult to detect. Critically, they automate "relevance judgment,"shifting the "locus of epistemic agency" to non-human systems. Throughnarrative scenarios spanning individual (cognitive dependency), collective(democratic deliberation), and societal (governance) scales, we describe howcognitive infrastructures reshape human cognition, public reasoning, and socialepistemologies. CIS aims to address how AI preprocessing reshapes distributedcognition across individual, collective, and cultural scales, requiringunprecedented integration of diverse disciplinary methods. The framework alsoaddresses critical gaps across disciplines: cognitive science lackspopulation-scale preprocessing analysis capabilities, digital sociology cannotaccess individual cognitive mechanisms, and computational approaches misscultural transmission dynamics. To achieve this goal CIS also providesmethodological innovations for studying invisible algorithmic influence:"infrastructure breakdown methodologies", experimental approaches that revealcognitive dependencies by systematically withdrawing AI preprocessing afterperiods of habituation.</description><author>Giuseppe Riva</author><pubDate>Wed, 27 Aug 2025 14:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22893v2</guid></item><item><title>Principled Detection of Hallucinations in Large Language Models via Multiple Testing</title><link>http://arxiv.org/abs/2508.18473v2</link><description>While Large Language Models (LLMs) have emerged as powerful foundationalmodels to solve a variety of tasks, they have also been shown to be prone tohallucinations, i.e., generating responses that sound confident but areactually incorrect or even nonsensical. In this work, we formulate the problemof detecting hallucinations as a hypothesis testing problem and draw parallelsto the problem of out-of-distribution detection in machine learning models. Wepropose a multiple-testing-inspired method to solve the hallucination detectionproblem, and provide extensive experimental results to validate the robustnessof our approach against state-of-the-art methods.</description><author>Jiawei Li, Akshayaa Magesh, Venugopal V. Veeravalli</author><pubDate>Wed, 27 Aug 2025 14:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18473v2</guid></item><item><title>Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation</title><link>http://arxiv.org/abs/2504.15876v3</link><description>In swarm robotics, confrontation scenarios, including strategicconfrontations, require efficient decision-making that integrates discretecommands and continuous actions. Traditional task and motion planning methodsseparate decision-making into two layers, but their unidirectional structurefails to capture the interdependence between these layers, limitingadaptability in dynamic environments. Here, we propose a novel bidirectionalapproach based on hierarchical reinforcement learning, enabling dynamicinteraction between the layers. This method effectively maps commands to taskallocation and actions to path planning, while leveraging cross-trainingtechniques to enhance learning across the hierarchical framework. Furthermore,we introduce a trajectory prediction model that bridges abstract taskrepresentations with actionable planning goals. In our experiments, it achievesover 80% in confrontation win rate and under 0.01 seconds in decision time,outperforming existing approaches. Demonstrations through large-scale tests andreal-world robot experiments further emphasize the generalization capabilitiesand practical applicability of our method.</description><author>Qizhen Wu, Lei Chen, Kexin Liu, Jinhu Lu</author><pubDate>Wed, 27 Aug 2025 14:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.15876v3</guid></item><item><title>CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments</title><link>http://arxiv.org/abs/2508.19932v1</link><description>The proliferation of digital payment platforms has transformed commerce,offering unmatched convenience and accessibility globally. However, this growthhas also attracted malicious actors, leading to a corresponding increase insophisticated social engineering scams. These scams are often initiated andorchestrated on multiple surfaces outside the payment platform, making user andtransaction-based signals insufficient for a complete understanding of thescam's methodology and underlying patterns, without which it is very difficultto prevent it in a timely manner. This paper presents CASE (ConversationalAgent for Scam Elucidation), a novel Agentic AI framework that addresses thisproblem by collecting and managing user scam feedback in a safe and scalablemanner. A conversational agent is uniquely designed to proactively interviewpotential victims to elicit intelligence in the form of a detailedconversation. The conversation transcripts are then consumed by another AIsystem that extracts information and converts it into structured data fordownstream usage in automated and manual enforcement mechanisms. Using Google'sGemini family of LLMs, we implemented this framework on Google Pay (GPay)India. By augmenting our existing features with this new intelligence, we haveobserved a 21% uplift in the volume of scam enforcements. The architecture andits robust evaluation framework are highly generalizable, offering a blueprintfor building similar AI-driven systems to collect and manage scam intelligencein other sensitive domains.</description><author>Nitish Jaipuria, Lorenzo Gatto, Zijun Kan, Shankey Poddar, Bill Cheung, Diksha Bansal, Ramanan Balakrishnan, Aviral Suri, Jose Estevez</author><pubDate>Wed, 27 Aug 2025 14:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19932v1</guid></item><item><title>InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</title><link>http://arxiv.org/abs/2508.18265v2</link><description>We introduce InternVL 3.5, a new family of open-source multimodal models thatsignificantly advances versatility, reasoning capability, and inferenceefficiency along the InternVL series. A key innovation is the CascadeReinforcement Learning (Cascade RL) framework, which enhances reasoning througha two-stage process: offline RL for stable convergence and online RL forrefined alignment. This coarse-to-fine training strategy leads to substantialimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. Tooptimize efficiency, we propose a Visual Resolution Router (ViR) thatdynamically adjusts the resolution of visual tokens without compromisingperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)strategy separates the vision encoder and language model across different GPUs,effectively balancing computational load. These contributions collectivelyenable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoningperformance and a 4.05$\times$ inference speedup compared to its predecessor,i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such asGUI interaction and embodied agency. Notably, our largest model, i.e.,InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMsacross general multimodal, reasoning, text, and agentic tasks -- narrowing theperformance gap with leading commercial models like GPT-5. All models and codeare publicly released.</description><author>Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao, Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin, Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang, Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou</author><pubDate>Wed, 27 Aug 2025 14:39:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18265v2</guid></item><item><title>Synthesizing High-Quality Programming Tasks with LLM-based Expert and Student Agents</title><link>http://arxiv.org/abs/2504.07655v2</link><description>Generative AI is transforming computing education by enabling the automaticgeneration of personalized content and feedback. We investigate itscapabilities in providing high-quality programming tasks to students. Despitepromising advancements in task generation, a quality gap remains betweenAI-generated and expert-created tasks. The AI-generated tasks may not alignwith target programming concepts, could be incomprehensible to students, or maycontain critical issues such as incorrect tests. Existing works often requireinterventions from human teachers for validation. We address these challengesby introducing PyTaskSyn, a novel synthesis technique that first generates aprogramming task and then decides whether it meets certain quality criteria tobe given to students. The key idea is to break this process into multiplestages performed by expert and student agents simulated using both strong andweaker generative models. Through extensive evaluation, we show that PyTaskSynsignificantly improves task quality compared to baseline techniques andshowcases the importance of each specialized agent type in our validationpipeline. Additionally, we conducted user studies using our publicly availableweb application and show that PyTaskSyn can deliver high-quality programmingtasks comparable to expert-designed ones while reducing workload and costs, andbeing more engaging than programming tasks that are available in onlineresources.</description><author>Manh Hung Nguyen, Victor-Alexandru Pădurean, Alkis Gotovos, Sebastian Tschiatschek, Adish Singla</author><pubDate>Wed, 27 Aug 2025 14:39:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.07655v2</guid></item><item><title>WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution</title><link>http://arxiv.org/abs/2508.19927v1</link><description>Transformers have demonstrated promising performance in computer visiontasks, including image super-resolution (SR). The quadratic computationalcomplexity of window self-attention mechanisms in many transformer-based SRmethods forces the use of small, fixed windows, limiting the receptive field.In this paper, we propose a new approach by embedding the wavelet transformwithin a hierarchical transformer framework, called (WaveHiT-SR). First, usingadaptive hierarchical windows instead of static small windows allows to capturefeatures across different levels and greatly improve the ability to modellong-range dependencies. Secondly, the proposed model utilizes wavelettransforms to decompose images into multiple frequency subbands, allowing thenetwork to focus on both global and local features while preserving structuraldetails. By progressively reconstructing high-resolution images throughhierarchical processing, the network reduces computational complexity withoutsacrificing performance. The multi-level decomposition strategy enables thenetwork to capture fine-grained information in lowfrequency components whileenhancing high-frequency textures. Through extensive experimentation, weconfirm the effectiveness and efficiency of our WaveHiT-SR. Our refinedversions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SRresults, achieving higher efficiency with fewer parameters, lower FLOPs, andfaster speeds.</description><author>Fayaz Ali, Muhammad Zawish, Steven Davy, Radu Timofte</author><pubDate>Wed, 27 Aug 2025 14:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19927v1</guid></item><item><title>Understanding Fairness-Accuracy Trade-offs in Machine Learning Models: Does Promoting Fairness Undermine Performance?</title><link>http://arxiv.org/abs/2411.17374v2</link><description>Fairness in both Machine Learning (ML) predictions and human decision-makingis essential, yet both are susceptible to different forms of bias, such asalgorithmic and data-driven in ML, and cognitive or subjective in humans. Inthis study, we examine fairness using a real-world university admissionsdataset comprising 870 applicant profiles, leveraging three ML models: XGB,Bi-LSTM, and KNN, alongside BERT embeddings for textual features. To evaluateindividual fairness, we introduce a consistency metric that quantifiesagreement in decisions among ML models and human experts with diversebackgrounds. Our analysis reveals that ML models surpass human evaluators infairness consistency by margins ranging from 14.08\% to 18.79\%. Our findingshighlight the potential of using ML to enhance fairness in admissions whilemaintaining high accuracy, advocating a hybrid approach combining humanjudgement and ML models.</description><author>Junhua Liu, Roy Ka-Wei Lee, Kwan Hui Lim</author><pubDate>Wed, 27 Aug 2025 14:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17374v2</guid></item><item><title>FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification</title><link>http://arxiv.org/abs/2508.19924v1</link><description>Network traffic classification using pre-training models has shown promisingresults, but existing methods struggle to capture packet structuralcharacteristics, flow-level behaviors, hierarchical protocol semantics, andinter-packet contextual relationships. To address these challenges, we proposeFlowletFormer, a BERT-based pre-training model specifically designed fornetwork traffic analysis. FlowletFormer introduces a Coherent Behavior-AwareTraffic Representation Model for segmenting traffic into semanticallymeaningful units, a Protocol Stack Alignment-Based Embedding Layer to capturemultilayer protocol semantics, and Field-Specific and Context-Aware PretrainingTasks to enhance both inter-packet and inter-flow learning. Experimentalresults demonstrate that FlowletFormer significantly outperforms existingmethods in the effectiveness of traffic representation, classificationaccuracy, and few-shot learning capability. Moreover, by effectivelyintegrating domain-specific network knowledge, FlowletFormer shows bettercomprehension of the principles of network transmission (e.g., statefulconnections of TCP), providing a more robust and trustworthy framework fortraffic analysis.</description><author>Liming Liu, Ruoyu Li, Qing Li, Meijia Hou, Yong Jiang, Mingwei Xu</author><pubDate>Wed, 27 Aug 2025 14:32:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19924v1</guid></item><item><title>From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving</title><link>http://arxiv.org/abs/2508.07029v2</link><description>Learning robust driving policies from large-scale, real-world datasets is acentral challenge in autonomous driving, as online data collection is oftenunsafe and impractical. While Behavioral Cloning (BC) offers a straightforwardapproach to imitation learning, policies trained with BC are notoriouslybrittle and suffer from compounding errors in closed-loop execution. This workpresents a comprehensive pipeline and a comparative study to address thislimitation. We first develop a series of increasingly sophisticated BCbaselines, culminating in a Transformer-based model that operates on astructured, entity-centric state representation. While this model achieves lowimitation loss, we show that it still fails in long-horizon simulations. Wethen demonstrate that by applying a state-of-the-art Offline ReinforcementLearning algorithm, Conservative Q-Learning (CQL), to the same data andarchitecture, we can learn a significantly more robust policy. Using acarefully engineered reward function, the CQL agent learns a conservative valuefunction that enables it to recover from minor errors and avoidout-of-distribution states. In a large-scale evaluation on 1,000 unseenscenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a3.2x higher success rate and a 7.4x lower collision rate than the strongest BCbaseline, proving that an offline RL approach is critical for learning robust,long-horizon driving policies from static expert data.</description><author>Antonio Guillen-Perez</author><pubDate>Wed, 27 Aug 2025 14:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07029v2</guid></item><item><title>Predicting the cardinality and maximum degree of a reduced Gröbner basis</title><link>http://arxiv.org/abs/2302.05364v3</link><description>We construct neural network regression models to predict key metrics ofcomplexity for Gr\"obner bases of binomial ideals. This work illustrates whypredictions with neural networks from Gr\"obner computations are not astraightforward process. Using two probabilistic models for random binomialideals, we generate and make available a large data set that is able to capturesufficient variability in Gr\"obner complexity. We use this data to trainneural networks and predict the cardinality of a reduced Gr\"obner basis andthe maximum total degree of its elements. While the cardinality predictionproblem is unlike classical problems tackled by machine learning, oursimulations show that neural networks, providing performance statistics such as$r^2 = 0.401$, outperform naive guess or multiple regression models with $r^2 =0.180$.</description><author>Shahrzad Jamshidi, Eric Kang, Sonja Petrović</author><pubDate>Wed, 27 Aug 2025 14:30:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05364v3</guid></item></channel></rss>