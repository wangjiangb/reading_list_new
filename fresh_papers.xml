<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 May 2023 06:00:35 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models</title><link>http://arxiv.org/abs/2305.08854v1</link><description>Speech-driven animation has gained significant traction in recent years, withcurrent methods achieving near-photorealistic results. However, the fieldremains underexplored regarding non-verbal communication despite evidencedemonstrating its importance in human interaction. In particular, generatinglaughter sequences presents a unique challenge due to the intricacy and nuancesof this behaviour. This paper aims to bridge this gap by proposing a novelmodel capable of generating realistic laughter sequences, given a stillportrait and an audio clip containing laughter. We highlight the failure casesof traditional facial animation methods and leverage recent advances indiffusion models to produce convincing laughter videos. We train our model on adiverse set of laughter datasets and introduce an evaluation metricspecifically designed for laughter. When compared with previous speech-drivenapproaches, our model achieves state-of-the-art performance across all metrics,even when these are re-trained for laughter generation.</description><author>Antoni Bigata Casademunt, Rodrigo Mira, Nikita Drobyshev, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic</author><pubDate>Mon, 15 May 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08854v1</guid></item><item><title>CQE: A Comprehensive Quantity Extractor</title><link>http://arxiv.org/abs/2305.08853v1</link><description>Quantities are essential in documents to describe factual information. Theyare ubiquitous in application domains such as finance, business, medicine, andscience in general. Compared to other information extraction approaches,interestingly only a few works exist that describe methods for a properextraction and representation of quantities in text. In this paper, we presentsuch a comprehensive quantity extraction framework from text data. Itefficiently detects combinations of values and units, the behavior of aquantity (e.g., rising or falling), and the concept a quantity is associatedwith. Our framework makes use of dependency parsing and a dictionary of units,and it provides for a proper normalization and standardization of detectedquantities. Using a novel dataset for evaluation, we show that our open sourceframework outperforms other systems and -- to the best of our knowledge -- isthe first to detect concepts associated with identified quantities. The codeand data underlying our framework are available athttps://github.com/vivkaz/CQE.</description><author>Satya Almasian, Vivian Kazakova, Philip Göldner, Michael Gertz</author><pubDate>Mon, 15 May 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08853v1</guid></item><item><title>Python Tool for Visualizing Variability of Pareto Fronts over Multiple Runs</title><link>http://arxiv.org/abs/2305.08852v1</link><description>Hyperparameter optimization is crucial to achieving high performance in deeplearning. On top of the performance, other criteria such as inference time ormemory requirement often need to be optimized due to some practical reasons.This motivates research on multi-objective optimization (MOO). However, Paretofronts of MOO methods are often shown without considering the variabilitycaused by random seeds and this makes the performance stability evaluationdifficult. Although there is a concept named empirical attainment surface toenable the visualization with uncertainty over multiple runs, there is no majorPython package for empirical attainment surface. We, therefore, develop aPython package for this purpose and describe the usage. The package isavailable at https://github.com/nabenabe0928/empirical-attainment-func.</description><author>Shuhei Watanabe</author><pubDate>Mon, 15 May 2023 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08852v1</guid></item><item><title>MV-Map: Offboard HD-Map Generation with Multi-view Consistency</title><link>http://arxiv.org/abs/2305.08851v1</link><description>While bird's-eye-view (BEV) perception models can be useful for buildinghigh-definition maps (HD-Maps) with less human labor, their results are oftenunreliable and demonstrate noticeable inconsistencies in the predicted HD-Mapsfrom different viewpoints. This is because BEV perception is typically set upin an 'onboard' manner, which restricts the computation and consequentlyprevents algorithms from reasoning multiple views simultaneously. This paperovercomes these limitations and advocates a more practical 'offboard' HD-Mapgeneration setup that removes the computation constraints, based on the factthat HD-Maps are commonly reusable infrastructures built offline in datacenters. To this end, we propose a novel offboard pipeline called MV-Map thatcapitalizes multi-view consistency and can handle an arbitrary number of frameswith the key design of a 'region-centric' framework. In MV-Map, the targetHD-Maps are created by aggregating all the frames of onboard predictions,weighted by the confidence scores assigned by an 'uncertainty network'. Tofurther enhance multi-view consistency, we augment the uncertainty network withthe global 3D structure optimized by a voxelized neural radiance field(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Mapsignificantly improves the quality of HD-Maps, further highlighting theimportance of offboard methods for HD-Map generation.</description><author>Ziyang Xie, Ziqi Pang, Yuxiong Wang</author><pubDate>Mon, 15 May 2023 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08851v1</guid></item><item><title>Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts</title><link>http://arxiv.org/abs/2305.08850v1</link><description>The text-driven image and video diffusion models have achieved unprecedentedsuccess in generating realistic and diverse content. Recently, the editing andvariation of existing images and videos in diffusion-based generative modelshave garnered significant attention. However, previous works are limited toediting content with text or providing coarse personalization using a singlevisual clue, rendering them unsuitable for indescribable content that requiresfine-grained and detailed control. In this regard, we propose a generic videoediting framework called Make-A-Protagonist, which utilizes textual and visualclues to edit videos with the goal of empowering individuals to become theprotagonists. Specifically, we leverage multiple experts to parse source video,target visual and textual clues, and propose a visual-textual-based videogeneration model that employs mask-guided denoising sampling to generate thedesired output. Extensive results demonstrate the versatile and remarkableediting capabilities of Make-A-Protagonist.</description><author>Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li, Gim Hee Lee</author><pubDate>Mon, 15 May 2023 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08850v1</guid></item><item><title>Learning on Manifolds: Universal Approximations Properties using Geometric Controllability Conditions for Neural ODEs</title><link>http://arxiv.org/abs/2305.08849v1</link><description>In numerous robotics and mechanical engineering applications, among others,data is often constrained on smooth manifolds due to the presence of rotationaldegrees of freedom. Common datadriven and learning-based methods such as neuralordinary differential equations (ODEs), however, typically fail to satisfythese manifold constraints and perform poorly for these applications. Toaddress this shortcoming, in this paper we study a class of neural ordinarydifferential equations that, by design, leave a given manifold invariant, andcharacterize their properties by leveraging the controllability properties ofcontrol affine systems. In particular, using a result due to Agrachev andCaponigro on approximating diffeomorphisms with flows of feedback controlsystems, we show that any map that can be represented as the flow of amanifold-constrained dynamical system can also be approximated using the flowof manifold-constrained neural ODE, whenever a certain controllabilitycondition is satisfied. Additionally, we show that this universal approximationproperty holds when the neural ODE has limited width in each layer, thusleveraging the depth of network instead for approximation. We verify ourtheoretical findings using numerical experiments on PyTorch for the manifoldsS2 and the 3-dimensional orthogonal group SO(3), which are model manifolds formechanical systems such as spacecrafts and satellites. We also compare theperformance of the manifold invariant neural ODE with classical neural ODEsthat ignore the manifold invariant properties and show the superiority of ourapproach in terms of accuracy and sample complexity.</description><author>Karthik Elamvazhuthi, Xuechen Zhang, Samet Oymak, Fabio Pasqualetti</author><pubDate>Mon, 15 May 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08849v1</guid></item><item><title>Small Models are Valuable Plug-ins for Large Language Models</title><link>http://arxiv.org/abs/2305.08848v1</link><description>Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but theirweights are often publicly unavailable and their immense sizes make the modelsdifficult to be tuned with common hardware. As a result, effectively tuningthese models with large-scale supervised data can be challenging. As analternative, In-Context Learning (ICL) can only use a small number ofsupervised examples due to context length limits. In this paper, we proposeSuper In-Context Learning (SuperICL) which allows black-box LLMs to work withlocally fine-tuned smaller models, resulting in superior performance onsupervised tasks. Our experiments demonstrate that SuperICL can improveperformance beyond state-of-the-art fine-tuned models while addressing theinstability problem of in-context learning. Furthermore, SuperICL can enhancethe capabilities of smaller models, such as multilinguality andinterpretability.</description><author>Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, Julian McAuley</author><pubDate>Mon, 15 May 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08848v1</guid></item><item><title>Privacy Auditing with One (1) Training Run</title><link>http://arxiv.org/abs/2305.08846v1</link><description>We propose a scheme for auditing differentially private machine learningsystems with a single training run. This exploits the parallelism of being ableto add or remove multiple training examples independently. We analyze thisusing the connection between differential privacy and statisticalgeneralization, which avoids the cost of group privacy. Our auditing schemerequires minimal assumptions about the algorithm and can be applied in theblack-box or white-box setting.</description><author>Thomas Steinke, Milad Nasr, Matthew Jagielski</author><pubDate>Mon, 15 May 2023 18:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08846v1</guid></item><item><title>Large Language Models are Zero-Shot Rankers for Recommender Systems</title><link>http://arxiv.org/abs/2305.08845v1</link><description>Recently, large language models (LLMs) (e.g. GPT-4) have demonstratedimpressive general-purpose task-solving abilities, including the potential toapproach recommendation tasks. Along this line of research, this work aims toinvestigate the capacity of LLMs that act as the ranking model for recommendersystems. To conduct our empirical study, we first formalize the recommendationproblem as a conditional ranking task, considering sequential interactionhistories as conditions and the items retrieved by the candidate generationmodel as candidates. We adopt a specific prompting approach to solving theranking task by LLMs: we carefully design the prompting template by includingthe sequential interaction history, the candidate items, and the rankinginstruction. We conduct extensive experiments on two widely-used datasets forrecommender systems and derive several key findings for the use of LLMs inrecommender systems. We show that LLMs have promising zero-shot rankingabilities, even competitive to or better than conventional recommendationmodels on candidates retrieved by multiple candidate generators. We alsodemonstrate that LLMs struggle to perceive the order of historical interactionsand can be affected by biases like position bias, while these issues can bealleviated via specially designed prompting and bootstrapping strategies. Thecode to reproduce this work is available athttps://github.com/RUCAIBox/LLMRank.</description><author>Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, Wayne Xin Zhao</author><pubDate>Mon, 15 May 2023 18:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08845v1</guid></item><item><title>RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs</title><link>http://arxiv.org/abs/2305.08844v1</link><description>Despite their unprecedented success, even the largest language models makemistakes. Similar to how humans learn and improve using feedback, previous workproposed providing language models with natural language feedback to guide themin repairing their outputs. Because human-generated critiques are expensive toobtain, researchers have devised learned critique generators in lieu of humancritics while assuming one can train downstream models to utilize generatedfeedback. However, this approach does not apply to black-box or limited accessmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era oflarge general-purpose language agents, fine-tuning is neither computationallynor spatially efficient as it results in multiple copies of the network. Inthis work, we introduce RL4F (Reinforcement Learning for Feedback), amulti-agent collaborative framework where the critique generator is trained tomaximize end-task performance of GPT-3, a fixed model more than 200 times itssize. RL4F produces critiques that help GPT-3 revise its outputs. We studythree datasets for action planning, summarization and alphabetization and showimprovements (~5% on average) in multiple text similarity metrics over strongbaselines across all three tasks.</description><author>Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, Niket Tandon</author><pubDate>Mon, 15 May 2023 18:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08844v1</guid></item><item><title>Straightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks</title><link>http://arxiv.org/abs/2305.08842v1</link><description>This work examines the challenges of training neural networks using vectorquantization using straight-through estimation. We find that a primary cause oftraining instability is the discrepancy between the model embedding and thecode-vector distribution. We identify the factors that contribute to thisissue, including the codebook gradient sparsity and the asymmetric nature ofthe commitment loss, which leads to misaligned code-vector assignments. Wepropose to address this issue via affine re-parameterization of the codevectors. Additionally, we introduce an alternating optimization to reduce thegradient error introduced by the straight-through estimation. Moreover, wepropose an improvement to the commitment loss to ensure better alignmentbetween the codebook representation and the model embedding. These optimizationmethods improve the mathematical approximation of the straight-throughestimation and, ultimately, the model performance. We demonstrate theeffectiveness of our methods on several common model architectures, such asAlexNet, ResNet, and ViT, across various tasks, including image classificationand generative modeling.</description><author>Minyoung Huh, Brian Cheung, Pulkit Agrawal, Phillip Isola</author><pubDate>Mon, 15 May 2023 18:56:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08842v1</guid></item><item><title>A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes</title><link>http://arxiv.org/abs/2305.08841v1</link><description>The proximal policy optimization (PPO) algorithm stands as one of the mostprosperous methods in the field of reinforcement learning (RL). Despite itssuccess, the theoretical understanding of PPO remains deficient. Specifically,it is unclear whether PPO or its optimistic variants can effectively solvelinear Markov decision processes (MDPs), which are arguably the simplest modelsin RL with function approximation. To bridge this gap, we propose an optimisticvariant of PPO for episodic adversarial linear MDPs with full-informationfeedback, and establish a $\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret forit. Here $d$ is the ambient dimension of linear MDPs, $H$ is the length of eachepisode, and $K$ is the number of episodes. Compared with existing policy-basedalgorithms, we achieve the state-of-the-art regret bound in both stochasticlinear MDPs and adversarial linear MDPs with full information. Additionally,our algorithm design features a novel multi-batched updating mechanism and thetheoretical analysis utilizes a new covering number argument of value andpolicy classes, which might be of independent interest.</description><author>Han Zhong, Tong Zhang</author><pubDate>Mon, 15 May 2023 18:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08841v1</guid></item><item><title>Attacking Perceptual Similarity Metrics</title><link>http://arxiv.org/abs/2305.08840v1</link><description>Perceptual similarity metrics have progressively become more correlated withhuman judgments on perceptual similarity; however, despite recent advances, theaddition of an imperceptible distortion can still compromise these metrics. Inour study, we systematically examine the robustness of these metrics toimperceptible adversarial perturbations. Following the two-alternativeforced-choice experimental design with two distorted images and one referenceimage, we perturb the distorted image closer to the reference via anadversarial attack until the metric flips its judgment. We first show that allmetrics in our study are susceptible to perturbations generated via commonadversarial attacks such as FGSM, PGD, and the One-pixel attack. Next, weattack the widely adopted LPIPS metric using spatial-transformation-basedadversarial perturbations (stAdv) in a white-box setting to craft adversarialexamples that can effectively transfer to other similarity metrics in ablack-box setting. We also combine the spatial attack stAdv with PGD($\ell_\infty$-bounded) attack to increase transferability and use theseadversarial examples to benchmark the robustness of both traditional andrecently developed metrics. Our benchmark provides a good starting point fordiscussion and further research on the robustness of metrics to imperceptibleadversarial perturbations.</description><author>Abhijay Ghildyal, Feng Liu</author><pubDate>Mon, 15 May 2023 18:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08840v1</guid></item><item><title>Adaptive Bias Correction for Improved Subseasonal Forecasting</title><link>http://arxiv.org/abs/2209.10666v3</link><description>Subseasonal forecasting -- predicting temperature and precipitation 2 to 6weeks ahead -- is critical for effective water allocation, wildfire management,and drought and flood mitigation. Recent international research efforts haveadvanced the subseasonal capabilities of operational dynamical models, yettemperature and precipitation prediction skills remain poor, partly due tostubborn errors in representing atmospheric dynamics and physics insidedynamical models. Here, to counter these errors, we introduce an adaptive biascorrection (ABC) method that combines state-of-the-art dynamical forecasts withobservations using machine learning. We show that, when applied to the leadingsubseasonal model from the European Centre for Medium-Range Weather Forecasts(ECMWF), ABC improves temperature forecasting skill by 60-90% (over baselineskills of 0.18-0.25) and precipitation forecasting skill by 40-69% (overbaseline skills of 0.11-0.15) in the contiguous U.S. We couple theseperformance improvements with a practical workflow to explain ABC skill gainsand identify higher-skill windows of opportunity based on specific climateconditions.</description><author>Soukayna Mouatadid, Paulo Orenstein, Genevieve Flaspohler, Judah Cohen, Miruna Oprescu, Ernest Fraenkel, Lester Mackey</author><pubDate>Mon, 15 May 2023 18:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.10666v3</guid></item><item><title>CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds</title><link>http://arxiv.org/abs/2305.00969v3</link><description>This paper describes the Ubenwa CryCeleb dataset - a labeled collection ofinfant cries, and the accompanying CryCeleb 2023 task - a public speakerverification challenge based on infant cry sounds. We release for academicusage more than 6 hours of manually segmented cry sounds from 786 newborns toencourage research in infant cry analysis.</description><author>David Budaghyan, Arsenii Gorin, Cem Subakan, Charles C. Onu</author><pubDate>Mon, 15 May 2023 18:48:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00969v3</guid></item><item><title>Defending Against Misinformation Attacks in Open-Domain Question Answering</title><link>http://arxiv.org/abs/2212.10002v2</link><description>Recent work in open-domain question answering (ODQA) has shown thatadversarial poisoning of the search collection can cause large drops inaccuracy for production systems. However, little to no work has proposedmethods to defend against these attacks. To do so, we rely on the intuitionthat redundant information often exists in large corpora. To find it, weintroduce a method that uses query augmentation to search for a diverse set ofpassages that could answer the original question but are less likely to havebeen poisoned. We integrate these new passages into the model through thedesign of a novel confidence method, comparing the predicted answer to itsappearance in the retrieved contexts (what we call \textit{Confidence fromAnswer Redundancy}, i.e. CAR). Together these methods allow for a simple buteffective way to defend against poisoning attacks that provides gains of nearly20\% exact match across varying levels of data poisoning/knowledge conflicts.</description><author>Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, Benjamin Van Durme</author><pubDate>Mon, 15 May 2023 18:46:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10002v2</guid></item><item><title>PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India</title><link>http://arxiv.org/abs/2305.08828v1</link><description>This paper introduces PMIndiaSum, a new multilingual and massively parallelheadline summarization corpus focused on languages in India. Our corpus coversfour language families, 14 languages, and the largest to date, 196 languagepairs. It provides a testing ground for all cross-lingual pairs. We detail ourworkflow to construct the corpus, including data acquisition, processing, andquality assurance. Furthermore, we publish benchmarks for monolingual,cross-lingual, and multilingual summarization by fine-tuning, prompting, aswell as translate-and-summarize. Experimental results confirm the crucial roleof our data in aiding the summarization of Indian texts. Our dataset ispublicly available and can be freely modified and re-distributed.</description><author>Ashok Urlana, Pinzhen Chen, Zheng Zhao, Shay B. Cohen, Manish Shrivastava, Barry Haddow</author><pubDate>Mon, 15 May 2023 18:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08828v1</guid></item><item><title>Learning Better Contrastive View from Radiologist's Gaze</title><link>http://arxiv.org/abs/2305.08826v1</link><description>Recent self-supervised contrastive learning methods greatly benefit from theSiamese structure that aims to minimizing distances between positive pairs.These methods usually apply random data augmentation to input images, expectingthe augmented views of the same images to be similar and positively paired.However, random augmentation may overlook image semantic information anddegrade the quality of augmented views in contrastive learning. This issuebecomes more challenging in medical images since the abnormalities related todiseases can be tiny, and are easy to be corrupted (e.g., being cropped out) inthe current scheme of random augmentation. In this work, we first demonstratethat, for widely-used X-ray images, the conventional augmentation prevalent incontrastive pre-training can affect the performance of the downstream diagnosisor classification tasks. Then, we propose a novel augmentation method, i.e.,FocusContrast, to learn from radiologists' gaze in diagnosis and generatecontrastive views for medical images with guidance from radiologists' visualattention. Specifically, we track the gaze movement of radiologists and modeltheir visual attention when reading to diagnose X-ray images. The learned modelcan predict visual attention of the radiologists given a new input image, andfurther guide the attention-aware augmentation that hardly neglects thedisease-related abnormalities. As a plug-and-play and framework-agnosticmodule, FocusContrast consistently improves state-of-the-art contrastivelearning methods of SimCLR, MoCo, and BYOL by 4.0~7.0% in classificationaccuracy on a knee X-ray dataset.</description><author>Sheng Wang, Zixu Zhuang, Xi Ouyang, Lichi Zhang, Zheren Li, Chong Ma, Tianming Liu, Dinggang Shen, Qian Wang</author><pubDate>Mon, 15 May 2023 18:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08826v1</guid></item><item><title>Five A$^{+}$ Network: You Only Need 9K Parameters for Underwater Image Enhancement</title><link>http://arxiv.org/abs/2305.08824v1</link><description>A lightweight underwater image enhancement network is of great significancefor resource-constrained platforms, but balancing model size, computationalefficiency, and enhancement performance has proven difficult for previousapproaches. In this work, we propose the Five A$^{+}$ Network (FA$^{+}$Net), ahighly efficient and lightweight real-time underwater image enhancement networkwith only $\sim$ 9k parameters and $\sim$ 0.01s processing time. TheFA$^{+}$Net employs a two-stage enhancement structure. The strong prior stageaims to decompose challenging underwater degradations into sub-problems, whilethe fine-grained stage incorporates multi-branch color enhancement module andpixel attention module to amplify the network's perception of details. To thebest of our knowledge, FA$^{+}$Net is the only network with the capability ofreal-time enhancement of 1080P images. Thorough extensive experiments andcomprehensive visual comparison, we show that FA$^{+}$Net outperforms previousapproaches by obtaining state-of-the-art performance on multiple datasets whilesignificantly reducing both parameter count and computational complexity. Thecode is open source at https://github.com/Owen718/FiveAPlus-Network.</description><author>Jingxia Jiang, Tian Ye, Jinbin Bai, Sixiang Chen, Wenhao Chai, Shi Jun, Yun Liu, Erkang Chen</author><pubDate>Mon, 15 May 2023 18:33:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08824v1</guid></item><item><title>Dragon-Alpha&amp;cu32: A Java-based Tensor Computing Framework With its High-Performance CUDA Library</title><link>http://arxiv.org/abs/2305.08819v1</link><description>Java is very powerful, but in Deep Learning field, its capabilities probablyhas not been sufficiently exploited. Compared to the Java-baseddeep-learning-frameworks, the Python-based (PyTorch, TensorFlow, etc) areundoubtedly the mainstream, due to their easy-to-use, flexibility and betterecosystem. Dragon-Alpha is a Java-based Tensor Computing Framework, witheasy-to-use, high-scalability and high-performance, trying to break Java'sdilemma in deep learning field and make it more effective. Dragon-Alphasupports different levels of APIs, and can be used as a deep-learning-frameworkthrough its user-friendly high-level APIs. Dragon-Alpha has potential toaggregate computing-power across heterogeneous platforms and devices, based onits multi-layer architecture and Java's big-data ecosystem. Dragon-Alpha hasits asynchronized APIs to improve parallelism, and highly-optimized CUDAlibrary cu32 which adopts unique convolution\deconvolution operators for smallfeature maps. The experiments show that, compared to PyTorch&amp;cuDNN,Dragon-Alpha&amp;cu32 costs less time and memory (75.38% to 97.32%, 29.2% to66.4%), to train some typical neural networks (AlexNet, VGG, GoogleNet, ResNet)on Cifar-10.</description><author>Zhiyi Zhang, Pengfei Zhang, Qi Wang</author><pubDate>Mon, 15 May 2023 18:30:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08819v1</guid></item><item><title>Sentence Level Curriculum Learning for Improved Neural Conversational Models</title><link>http://arxiv.org/abs/2305.08818v1</link><description>Designing machine intelligence to converse with a human user necessarilyrequires an understanding of how humans participate in conversation, and thusconversation modeling is an important task in natural language processing. Newbreakthroughs in architecture and data gathering continue to push theperformance of such conversational AI models. However, designs neglect thegradual buildup in sentence structure and complexity experienced by humans aswe learn to communicate. During training, our model accepts one or moresentences as input and attempts to predict the next sentence in theconversation one word at a time, so our goal is to separate training intosegments, with each segment's corpus comprised of longer sentence pairs thanthe previous one. This will mimic the desired "buildup" component of humanlearning. We begin with only "short" length sentence pairs, then only "medium"length pairs, and so on. A majority of our experiments were toward optimizingthis technique, ensuring a proper representation of the technique's potential,since many of the details were new questions. Our segment-trained models werethen able to achieve lower validation loss at the end of training than modelstrained with standard text preparation. This segmented training isstraightforward to implement and our results provide a general direction forfuture research to implement and improve it.</description><author>Sean Paulsen</author><pubDate>Mon, 15 May 2023 18:28:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08818v1</guid></item><item><title>ReLU soothes the NTK condition number and accelerates optimization for wide neural networks</title><link>http://arxiv.org/abs/2305.08813v1</link><description>Rectified linear unit (ReLU), as a non-linear activation function, is wellknown to improve the expressivity of neural networks such that any continuousfunction can be approximated to arbitrary precision by a sufficiently wideneural network. In this work, we present another interesting and importantfeature of ReLU activation function. We show that ReLU leads to: {\it betterseparation} for similar data, and {\it better conditioning} of neural tangentkernel (NTK), which are closely related. Comparing with linear neural networks,we show that a ReLU activated wide neural network at random initialization hasa larger angle separation for similar data in the feature space of modelgradient, and has a smaller condition number for NTK. Note that, for a linearneural network, the data separation and NTK condition number always remain thesame as in the case of a linear model. Furthermore, we show that a deeper ReLUnetwork (i.e., with more ReLU activation operations), has a smaller NTKcondition number than a shallower one. Our results imply that ReLU activation,as well as the depth of ReLU network, helps improve the gradient descentconvergence rate, which is closely related to the NTK condition number.</description><author>Chaoyue Liu, Like Hui</author><pubDate>Mon, 15 May 2023 18:22:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08813v1</guid></item><item><title>AutoRecon: Automated 3D Object Discovery and Reconstruction</title><link>http://arxiv.org/abs/2305.08810v1</link><description>A fully automated object reconstruction pipeline is crucial for digitalcontent creation. While the area of 3D reconstruction has witnessed profounddevelopments, the removal of background to obtain a clean object model stillrelies on different forms of manual labor, such as bounding box labeling, maskannotations, and mesh manipulations. In this paper, we propose a novelframework named AutoRecon for the automated discovery and reconstruction of anobject from multi-view images. We demonstrate that foreground objects can berobustly located and segmented from SfM point clouds by leveragingself-supervised 2D vision transformer features. Then, we reconstruct decomposedneural scene representations with dense supervision provided by the decomposedpoint clouds, resulting in accurate object reconstruction and segmentation.Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate theeffectiveness and robustness of AutoRecon.</description><author>Yuang Wang, Xingyi He, Sida Peng, Haotong Lin, Hujun Bao, Xiaowei Zhou</author><pubDate>Mon, 15 May 2023 18:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08810v1</guid></item><item><title>Interpretability at Scale: Identifying Causal Mechanisms in Alpaca</title><link>http://arxiv.org/abs/2305.08809v1</link><description>Obtaining human-interpretable explanations of large, general-purpose languagemodels is an urgent goal for AI safety. However, it is just as important thatour interpretability methods are faithful to the causal dynamics underlyingmodel behavior and able to robustly generalize to unseen inputs. DistributedAlignment Search (DAS) is a powerful gradient descent method grounded in atheory of causal abstraction that uncovered perfect alignments betweeninterpretable symbolic algorithms and small deep learning models fine-tuned forspecific tasks. In the present paper, we scale DAS significantly by replacingthe remaining brute-force search steps with learned parameters -- an approachwe call DAS. This enables us to efficiently search for interpretable causalstructure in large language models while they follow instructions. We apply DASto the Alpaca model (7B parameters), which, off the shelf, solves a simplenumerical reasoning problem. With DAS, we discover that Alpaca does this byimplementing a causal model with two interpretable boolean variables.Furthermore, we find that the alignment of neural representations with thesevariables is robust to changes in inputs and instructions. These findings marka first step toward deeply understanding the inner-workings of our largest andmost widely deployed language models.</description><author>Zhengxuan Wu, Atticus Geiger, Christopher Potts, Noah D. Goodman</author><pubDate>Mon, 15 May 2023 18:15:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08809v1</guid></item><item><title>GeoMAE: Masked Geometric Target Prediction for Self-supervised Point Cloud Pre-Training</title><link>http://arxiv.org/abs/2305.08808v1</link><description>This paper tries to address a fundamental question in point cloudself-supervised learning: what is a good signal we should leverage to learnfeatures from point clouds without annotations? To answer that, we introduce apoint cloud representation learning framework, based on geometric featurereconstruction. In contrast to recent papers that directly adopt maskedautoencoder (MAE) and only predict original coordinates or occupancy frommasked point clouds, our method revisits differences between images and pointclouds and identifies three self-supervised learning objectives peculiar topoint clouds, namely centroid prediction, normal estimation, and curvatureprediction. Combined with occupancy prediction, these four objectives yield annontrivial self-supervised learning task and mutually facilitate models tobetter reason fine-grained geometry of point clouds. Our pipeline isconceptually simple and it consists of two major steps: first, it randomlymasks out groups of points, followed by a Transformer-based point cloudencoder; second, a lightweight Transformer decoder predicts centroid, normal,and curvature for points in each voxel. We transfer the pre-trained Transformerencoder to a downstream peception model. On the nuScene Datset, our modelachieves 3.38 mAP improvment for object detection, 2.1 mIoU gain forsegmentation, and 1.7 AMOTA gain for multi-object tracking. We also conductexperiments on the Waymo Open Dataset and achieve significant performanceimprovements over baselines as well.</description><author>Xiaoyu Tian, Haoxi Ran, Yue Wang, Hang Zhao</author><pubDate>Mon, 15 May 2023 18:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08808v1</guid></item><item><title>Smoothness and monotonicity constraints for neural networks using ICEnet</title><link>http://arxiv.org/abs/2305.08807v1</link><description>Deep neural networks have become an important tool for use in actuarialtasks, due to the significant gains in accuracy provided by these techniquescompared to traditional methods, but also due to the close connection of thesemodels to the Generalized Linear Models (GLMs) currently used in industry.Whereas constraining GLM parameters relating to insurance risk factors to besmooth or exhibit monotonicity is trivial, methods to incorporate suchconstraints into deep neural networks have not yet been developed. This is abarrier for the adoption of neural networks in insurance practice sinceactuaries often impose these constraints for commercial or statistical reasons.In this work, we present a novel method for enforcing constraints within deepneural network models, and we show how these models can be trained. Moreover,we provide example applications using real-world datasets. We call our proposedmethod ICEnet to emphasize the close link of our proposal to the individualconditional expectation (ICE) model interpretability technique.</description><author>Ronald Richman, Mario Wüthrich</author><pubDate>Mon, 15 May 2023 18:14:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08807v1</guid></item><item><title>Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text</title><link>http://arxiv.org/abs/2305.08804v1</link><description>Knowledge graphs can represent information about the real-world usingentities and their relations in a structured and semantically rich manner andthey enable a variety of downstream applications such as question-answering,recommendation systems, semantic search, and advanced analytics. However, atthe moment, building a knowledge graph involves a lot of manual effort and thushinders their application in some situations and the automation of this processmight benefit especially for small organizations. Automatically generatingstructured knowledge graphs from a large volume of natural language is still achallenging task and the research on sub-tasks such as named entity extraction,relation extraction, entity and relation linking, and knowledge graphconstruction aims to improve the state of the art of automatic construction andcompletion of knowledge graphs from text. The recent advancement of foundationmodels with billions of parameters trained in a self-supervised manner withlarge volumes of training data that can be adapted to a variety of downstreamtasks has helped to demonstrate high performance on a large range of NaturalLanguage Processing (NLP) tasks. In this context, one emerging paradigm isin-context learning where a language model is used as it is with a prompt thatprovides instructions and some examples to perform a task without changing theparameters of the model using traditional approaches such as fine-tuning. Thisway, no computing resources are needed for re-training/fine-tuning the modelsand the engineering effort is minimal. Thus, it would be beneficial to utilizesuch capabilities for generating knowledge graphs from text.</description><author>Hanieh Khorashadizadeh, Nandana Mihindukulasooriya, Sanju Tiwari, Jinghua Groppe, Sven Groppe</author><pubDate>Mon, 15 May 2023 18:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08804v1</guid></item><item><title>Detecting Histologic &amp; Clinical Glioblastoma Patterns of Prognostic Relevance</title><link>http://arxiv.org/abs/2302.00669v2</link><description>Glioblastoma is the most common and aggressive malignant adult tumor of thecentral nervous system, with a grim prognosis and heterogeneous morphologic andmolecular profiles. Since adopting the current standard-of-care treatment 18years ago, no substantial prognostic improvement has been noticed. Accurateprediction of patient overall survival (OS) from histopathology whole slideimages (WSI) integrated with clinical data using advanced computational methodscould optimize clinical decision-making and patient management. Here, we focuson identifying prognostically relevant glioblastoma characteristics from H&amp;Estained WSI &amp; clinical data relating to OS. The exact approach for WSIcapitalizes on the comprehensive curation of apparent artifactual content andan interpretability mechanism via a weakly supervised attention-basedmultiple-instance learning algorithm that further utilizes clustering toconstrain the search space. The automatically placed pat- terns of highdiagnostic value classify each WSI as representative of short orlong-survivors. Further assessment of the prognostic relevance of theassociated clinical patient data is performed both in isolation and in anintegrated manner, using XGBoost and SHapley Additive exPlanations (SHAP).Identifying tumor morphological &amp; clinical patterns associated with short andlong OS will enable the clinical neuropathologist to provide additionalrelevant prognostic information to the treating team and suggest avenues ofbiological investigation for understanding and potentially treatingglioblastoma.</description><author>Bhakti Baheti, Sunny Rai, Shubham Innani, Garv Mehdiratta, Sharath Chandra Guntuku, MacLean P. Nasrallah, Spyridon Bakas</author><pubDate>Mon, 15 May 2023 18:07:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00669v2</guid></item><item><title>Measuring Cross-Lingual Transferability of Multilingual Transformers on Sentence Classification</title><link>http://arxiv.org/abs/2305.08800v1</link><description>Recent studies have exhibited remarkable capabilities of pre-trainedmultilingual Transformers, especially cross-lingual transferability. However,current methods do not measure cross-lingual transferability well, hinderingthe understanding of multilingual Transformers. In this paper, we propose IGap,a cross-lingual transferability metric for multilingual Transformers onsentence classification tasks. IGap takes training error into consideration,and can also estimate transferability without end-task data. Experimentalresults show that IGap outperforms baseline metrics for transferabilitymeasuring and transfer direction ranking. Besides, we conduct extensivesystematic experiments where we compare transferability among variousmultilingual Transformers, fine-tuning algorithms, and transfer directions.More importantly, our results reveal three findings about cross-lingualtransfer, which helps us to better understand multilingual Transformers.</description><author>Zewen Chi, Heyan Huang, Xian-Ling Mao</author><pubDate>Mon, 15 May 2023 18:05:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08800v1</guid></item><item><title>Masked Autoencoding Does Not Help Natural Language Supervision at Scale</title><link>http://arxiv.org/abs/2301.07836v4</link><description>Self supervision and natural language supervision have emerged as twoexciting ways to train general purpose image encoders which excel at a varietyof downstream tasks. Recent works such as M3AE and SLIP have suggested thatthese approaches can be effectively combined, but most notably their resultsuse small pre-training datasets (&lt;50M samples) and don't effectively reflectthe large-scale regime (&gt;100M examples) that is commonly used for theseapproaches. Here we investigate whether a similar approach can be effectivewhen trained with a much larger amount of data. We find that a combination oftwo state of the art approaches: masked auto-encoders, MAE and contrastivelanguage image pre-training, CLIP provides a benefit over CLIP when trained ona corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on asuite of common vision tasks) over CLIP when trained on a large corpus of 1.4Bimages. Our work provides some much needed clarity into the effectiveness (orlack thereof) of self supervision for large-scale image-text training.</description><author>Floris Weers, Vaishaal Shankar, Angelos Katharopoulos, Yinfei Yang, Tom Gunter</author><pubDate>Mon, 15 May 2023 18:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07836v4</guid></item><item><title>Environmental Sensor Placement with Convolutional Gaussian Neural Processes</title><link>http://arxiv.org/abs/2211.10381v5</link><description>Environmental sensors are crucial for monitoring weather conditions and theimpacts of climate change. However, it is challenging to place sensors in a waythat maximises the informativeness of their measurements, particularly inremote regions like Antarctica. Probabilistic machine learning models cansuggest informative sensor placements by finding sites that maximally reduceprediction uncertainty. Gaussian process (GP) models are widely used for thispurpose, but they struggle with capturing complex non-stationary behaviour andscaling to large datasets. This paper proposes using a convolutional Gaussianneural process (ConvGNP) to address these issues. A ConvGNP uses neuralnetworks to parameterise a joint Gaussian distribution at arbitrary targetlocations, enabling flexibility and scalability. Using simulated surface airtemperature anomaly over Antarctica as training data, the ConvGNP learnsspatial and seasonal non-stationarities, outperforming a non-stationary GPbaseline. In a simulated sensor placement experiment, the ConvGNP betterpredicts the performance boost obtained from new observations than GPbaselines, leading to more informative sensor placements. We contrast ourapproach with physics-based sensor placement methods and propose future stepstowards an operational sensor placement recommendation system. Our work couldhelp to realise environmental digital twins that actively direct measurementsampling to improve the digital representation of reality.</description><author>Tom R. Andersson, Wessel P. Bruinsma, Stratis Markou, James Requeima, Alejandro Coca-Castro, Anna Vaughan, Anna-Louise Ellis, Matthew A. Lazzara, Dani Jones, J. Scott Hosking, Richard E. Turner</author><pubDate>Mon, 15 May 2023 18:03:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10381v5</guid></item><item><title>Optimum Output Long Short-Term Memory Cell for High-Frequency Trading Forecasting</title><link>http://arxiv.org/abs/2304.09840v3</link><description>High-frequency trading requires fast data processing without information lagsfor precise stock price forecasting. This high-paced stock price forecasting isusually based on vectors that need to be treated as sequential andtime-independent signals due to the time irregularities that are inherent inhigh-frequency trading. A well-documented and tested method that considersthese time-irregularities is a type of recurrent neural network, named longshort-term memory neural network. This type of neural network is formed basedon cells that perform sequential and stale calculations via gates and stateswithout knowing whether their order, within the cell, is optimal. In thispaper, we propose a revised and real-time adjusted long short-term memory cellthat selects the best gate or state as its final output. Our cell is runningunder a shallow topology, has a minimal look-back period, and is trainedonline. This revised cell achieves lower forecasting error compared to otherrecurrent neural networks for online high-frequency trading forecasting taskssuch as the limit order book mid-price prediction as it has been tested on twohigh-liquid US and two less-liquid Nordic stocks.</description><author>Adamantios Ntakaris, Moncef Gabbouj, Juho Kanniainen</author><pubDate>Mon, 15 May 2023 18:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09840v3</guid></item><item><title>Predictive Models from Quantum Computer Benchmarks</title><link>http://arxiv.org/abs/2305.08796v1</link><description>Holistic benchmarks for quantum computers are essential for testing andsummarizing the performance of quantum hardware. However, holistic benchmarks-- such as algorithmic or randomized benchmarks -- typically do not predict aprocessor's performance on circuits outside the benchmark's necessarily verylimited set of test circuits. In this paper, we introduce a general frameworkfor building predictive models from benchmarking data using capability models.Capability models can be fit to many kinds of benchmarking data and used for avariety of predictive tasks. We demonstrate this flexibility with two casestudies. In the first case study, we predict circuit (i) process fidelities and(ii) success probabilities by fitting error rates models to two kinds ofvolumetric benchmarking data. Error rates models are simple, yet versatilecapability models which assign effective error rates to individual gates, ormore general circuit components. In the second case study, we construct acapability model for predicting circuit success probabilities by applyingtransfer learning to ResNet50, a neural network trained for imageclassification. Our case studies use data from cloud-accessible quantumcomputers and simulations of noisy quantum computers.</description><author>Daniel Hothem, Jordan Hines, Karthik Nataraj, Robin Blume-Kohout, Timothy Proctor</author><pubDate>Mon, 15 May 2023 18:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08796v1</guid></item><item><title>Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems</title><link>http://arxiv.org/abs/2106.01263v5</link><description>Sample-and-rank is a key decoding strategy for modern generation-baseddialogue systems. It helps achieve diverse and high-quality responses byselecting an answer from a small pool of generated candidates. The currentstate-of-the-art ranking methods mainly use an encoding paradigm calledCross-Encoder, which separately encodes each context-candidate pair and ranksthe candidates according to their fitness scores. However, Cross-Encoderrepeatedly encodes the same lengthy context for each candidate, resulting inhigh computational costs. Poly-Encoder addresses the above problems by reducingthe interaction between context and candidates, but with a price of performancedrop. In this work, we develop a new paradigm called Uni-Encoder, that keepsthe full attention over each pair as in Cross-Encoder while only encoding thecontext once, as in Poly-Encoder. Uni-Encoder encodes all the candidates withthe context in one forward pass. We use the same positional embedding for allcandidates to ensure they are treated equally and design a new attentionmechanism to avoid confusion. Our Uni-Encoder can simulate other rankingparadigms using different attention and response concatenation methods.Extensive experiments show that our proposed paradigm achieves newstate-of-the-art results on four benchmark datasets with high computationalefficiency. For instance, it improves R10@1 by 2.9% with an approximately 4Xfaster inference speed on the Ubuntu V2 dataset.</description><author>Chiyu Song, Hongliang He, Haofei Yu, Pengfei Fang, Leyang Cui, Zhenzhong Lan</author><pubDate>Mon, 15 May 2023 17:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.01263v5</guid></item><item><title>Multi-Task Attentive Residual Networks for Argument Mining</title><link>http://arxiv.org/abs/2102.12227v2</link><description>We explore the use of residual networks and neural attention for multipleargument mining tasks. We propose a residual architecture that exploitsattention, multi-task learning, and makes use of ensemble, without anyassumption on document or argument structure. We present an extensiveexperimental evaluation on five different corpora of user-generated comments,scientific publications, and persuasive essays. Our results show that ourapproach is a strong competitor against state-of-the-art architectures with ahigher computational footprint or corpus-specific design, representing aninteresting compromise between generality, performance accuracy and reducedmodel size.</description><author>Andrea Galassi, Marco Lippi, Paolo Torroni</author><pubDate>Mon, 15 May 2023 17:53:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.12227v2</guid></item><item><title>Fair Information Spread on Social Networks with Community Structure</title><link>http://arxiv.org/abs/2305.08791v1</link><description>Information spread through social networks is ubiquitous. Influence maximiza-tion (IM) algorithms aim to identify individuals who will generate the greatestspread through the social network if provided with information, and have beenlargely devel- oped with marketing in mind. In social networks with communitystructure, which are very common, IM algorithms focused solely on maximizingspread may yield signifi- cant disparities in information coverage betweencommunities, which is problematic in settings such as public health messaging.While some IM algorithms aim to remedy disparity in information coverage usingnode attributes, none use the empirical com- munity structure within thenetwork itself, which may be beneficial since communities directly affect thespread of information. Further, the use of empirical network struc- ture allowsus to leverage community detection techniques, making it possible to runfair-aware algorithms when there are no relevant node attributes available, orwhen node attributes do not accurately capture network community structure. Incontrast to other fair IM algorithms, this work relies on fitting a model tothe social network which is then used to determine a seed allocation strategyfor optimal fair information spread. We develop an algorithm to determineoptimal seed allocations for expected fair coverage, defined through maximumentropy, provide some theoretical guarantees under appropriate conditions, anddemonstrate its empirical accuracy on both simu- lated and real networks.Because this algorithm relies on a fitted network model and not on the networkdirectly, it is well-suited for partially observed and noisy social networks.</description><author>Octavio Mesner, Elizaveta Levina, Ji Zhu</author><pubDate>Mon, 15 May 2023 17:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08791v1</guid></item><item><title>Comparing Variation in Tokenizer Outputs Using a Series of Problematic and Challenging Biomedical Sentences</title><link>http://arxiv.org/abs/2305.08787v1</link><description>Background &amp; Objective: Biomedical text data are increasingly available forresearch. Tokenization is an initial step in many biomedical text miningpipelines. Tokenization is the process of parsing an input biomedical sentence(represented as a digital character sequence) into a discrete set of word/tokensymbols, which convey focused semantic/syntactic meaning. The objective of thisstudy is to explore variation in tokenizer outputs when applied across a seriesof challenging biomedical sentences. Method: Diaz [2015] introduce 24 challenging example biomedical sentences forcomparing tokenizer performance. In this study, we descriptively explorevariation in outputs of eight tokenizers applied to each example biomedicalsentence. The tokenizers compared in this study are the NLTK white spacetokenizer, the NLTK Penn Tree Bank tokenizer, Spacy and SciSpacy tokenizers,Stanza/Stanza-Craft tokenizers, the UDPipe tokenizer, and R-tokenizers. Results: For many examples, tokenizers performed similarly effectively;however, for certain examples, there were meaningful variation in returnedoutputs. The white space tokenizer often performed differently than othertokenizers. We observed performance similarities for tokenizers implementingrule-based systems (e.g. pattern matching and regular expressions) andtokenizers implementing neural architectures for token classification.Oftentimes, the challenging tokens resulting in the greatest variation inoutputs, are those words which convey substantive and focusedbiomedical/clinical meaning (e.g. x-ray, IL-10, TCR/CD3, CD4+ CD8+, and(Ca2+)-regulated). Conclusion: When state-of-the-art, open-source tokenizers from Python and Rwere applied to a series of challenging biomedical example sentences, weobserved subtle variation in the returned outputs.</description><author>Christopher Meaney, Therese A Stukel, Peter C Austin, Michael Escobar</author><pubDate>Mon, 15 May 2023 17:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08787v1</guid></item><item><title>On Noisy Evaluation in Federated Hyperparameter Tuning</title><link>http://arxiv.org/abs/2212.08930v4</link><description>Hyperparameter tuning is critical to the success of federated learningapplications. Unfortunately, appropriately selecting hyperparameters ischallenging in federated networks. Issues of scale, privacy, and heterogeneityintroduce noise in the tuning process and make it difficult to evaluate theperformance of various hyperparameters. In this work, we perform the firstsystematic study on the effect of noisy evaluation in federated hyperparametertuning. We first identify and rigorously explore key sources of noise,including client subsampling, data and systems heterogeneity, and data privacy.Surprisingly, our results indicate that even small amounts of noise cansignificantly impact tuning methods-reducing the performance ofstate-of-the-art approaches to that of naive baselines. To address noisyevaluation in such scenarios, we propose a simple and effective approach thatleverages public proxy data to boost the evaluation signal. Our workestablishes general challenges, baselines, and best practices for future workin federated hyperparameter tuning.</description><author>Kevin Kuo, Pratiksha Thaker, Mikhail Khodak, John Nguyen, Daniel Jiang, Ameet Talwalkar, Virginia Smith</author><pubDate>Mon, 15 May 2023 17:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08930v4</guid></item><item><title>Measuring Massive Multitask Chinese Understanding</title><link>http://arxiv.org/abs/2304.12986v2</link><description>The development of large-scale Chinese language models is flourishing, yetthere is a lack of corresponding capability assessments. Therefore, we proposea test to measure the multitask accuracy of large Chinese language models. Thistest encompasses four major domains, including medicine, law, psychology, andeducation, with 15 subtasks in medicine and 8 subtasks in education. We foundthat the best-performing models in the zero-shot setting outperformed theworst-performing models by nearly 18.6 percentage points on average. Across thefour major domains, the highest average zero-shot accuracy of all models is0.512. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shotaccuracy of 0.693 in clinical medicine, which was the highest accuracy amongall models across all subtasks. All models performed poorly in the legaldomain, with the highest zero-shot accuracy reaching only 0.239. Bycomprehensively evaluating the breadth and depth of knowledge across multipledisciplines, this test can more accurately identify the shortcomings of themodels.</description><author>Hui Zeng</author><pubDate>Mon, 15 May 2023 17:41:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12986v2</guid></item><item><title>TAA-GCN: A Temporally Aware Adaptive Graph Convolutional Network for Age Estimation</title><link>http://arxiv.org/abs/2305.08779v1</link><description>This paper proposes a novel age estimation algorithm, the Temporally-AwareAdaptive Graph Convolutional Network (TAA-GCN). Using a new representationbased on graphs, the TAA-GCN utilizes skeletal, posture, clothing, and facialinformation to enrich the feature set associated with various ages. Such anovel graph representation has several advantages: First, reduced sensitivityto facial expression and other appearance variances; Second, robustness topartial occlusion and non-frontal-planar viewpoint, which is commonplace inreal-world applications such as video surveillance. The TAA-GCN employs twonovel components, (1) the Temporal Memory Module (TMM) to compute temporaldependencies in age; (2) Adaptive Graph Convolutional Layer (AGCL) to refinethe graphs and accommodate the variance in appearance. The TAA-GCN outperformsthe state-of-the-art methods on four public benchmarks, UTKFace, MORPHII, CACD,and FG-NET. Moreover, the TAA-GCN showed reliability in different cameraviewpoints and reduced quality images.</description><author>Matthew Korban, Peter Young, Scott T. Acton</author><pubDate>Mon, 15 May 2023 17:38:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08779v1</guid></item><item><title>Learning Empirical Bregman Divergence for Uncertain Distance Representation</title><link>http://arxiv.org/abs/2304.07689v3</link><description>Deep metric learning techniques have been used for visual representation invarious supervised and unsupervised learning tasks through learning embeddingsof samples with deep networks. However, classic approaches, which employ afixed distance metric as a similarity function between two embeddings, may leadto suboptimal performance for capturing the complex data distribution. TheBregman divergence generalizes measures of various distance metrics and arisesthroughout many fields of deep metric learning. In this paper, we first showhow deep metric learning loss can arise from the Bregman divergence. We thenintroduce a novel method for learning empirical Bregman divergence directlyfrom data based on parameterizing the convex function underlying the Bregmandivergence with a deep learning setting. We further experimentally show thatour approach performs effectively on five popular public datasets compared toother SOTA deep metric learning methods, particularly for pattern recognitionproblems.</description><author>Zhiyuan Li, Ziru Liu, Anna Zou, Anca L. Ralescu</author><pubDate>Mon, 15 May 2023 17:38:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07689v3</guid></item><item><title>Question-Answering System Extracts Information on Injection Drug Use from Clinical Progress Notes</title><link>http://arxiv.org/abs/2305.08777v1</link><description>Injection drug use (IDU) is a dangerous health behavior that increasesmortality and morbidity. Identifying IDU early and initiating harm reductioninterventions can benefit individuals at risk. However, extracting IDUbehaviors from patients' electronic health records (EHR) is difficult becausethere is no International Classification of Disease (ICD) code and the onlyplace IDU information can be indicated are unstructured free-text clinicalprogress notes. Although natural language processing (NLP) can efficientlyextract this information from unstructured data, there are no validated tools.To address this gap in clinical information, we design and demonstrate aquestion-answering (QA) framework to extract information on IDU from clinicalprogress notes. Unlike other methods discussed in the literature, the QA modelis able to extract various types of information without being constrained bypredefined entities, relations, or concepts. Our framework involves two mainsteps: (1) generating a gold-standard QA dataset and (2) developing and testingthe QA model. This paper also demonstrates the QA model's ability to extractIDU-related information on temporally out-of-distribution data. The resultsindicate that the majority (51%) of the extracted information by the QA modelexactly matches the gold-standard answer and 73% of them contain thegold-standard answer with some additional surrounding words.</description><author>Maria Mahbub, Ian Goethert, Ioana Danciu, Kathryn Knight, Sudarshan Srinivasan, Suzanne Tamang, Karine Rozenberg-Ben-Dror, Hugo Solares, Susana Martins, Edmon Begoli, Gregory D. Peterson</author><pubDate>Mon, 15 May 2023 17:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08777v1</guid></item><item><title>Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models</title><link>http://arxiv.org/abs/2305.08776v1</link><description>Foundation models have made significant strides in 2D and language tasks suchas image segmentation, object detection, and visual-language understanding.Nevertheless, their potential to enhance 3D scene representation learningremains largely untapped due to the domain gap. In this paper, we propose aninnovative methodology Bridge3D to address this gap, pre-training 3D modelsusing features, semantic masks, and captions sourced from foundation models.Specifically, our approach utilizes semantic masks from these models to guidethe masking and reconstruction process in the masked autoencoder. This strategyenables the network to concentrate more on foreground objects, therebyenhancing 3D representation learning. Additionally, we bridge the 3D-text gapat the scene level by harnessing image captioning foundation models. To furtherfacilitate knowledge distillation from well-learned 2D and text representationsto the 3D model, we introduce a novel method that employs foundation models togenerate highly accurate object-level masks and semantic text information atthe object level. Our approach notably outshines state-of-the-art methods in 3Dobject detection and semantic segmentation tasks. For instance, on the ScanNetdataset, our method surpasses the previous state-of-the-art method, PiMAE, by asignificant margin of 5.3%.</description><author>Zhimin Chen, Bing Li</author><pubDate>Mon, 15 May 2023 17:36:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08776v1</guid></item><item><title>Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks</title><link>http://arxiv.org/abs/2208.09554v3</link><description>Autonomous agents are able to draw on a wide variety of potential sources oftask knowledge; however current approaches invariably focus on only one or two.Here we investigate the challenges and impact of exploiting diverse knowledgesources to learn online, in one-shot, new tasks for a simulated office mobilerobot. The resulting agent, developed in the Soar cognitive architecture, usesthe following sources of domain and task knowledge: interaction with theenvironment, task execution and search knowledge, human natural languageinstruction, and responses retrieved from a large language model (GPT-3). Weexplore the distinct contributions of these knowledge sources and evaluate theperformance of different combinations in terms of learning correct taskknowledge and human workload. Results show that an agent's online integrationof diverse knowledge sources improves one-shot task learning overall, reducinghuman feedback needed for rapid and reliable task learning.</description><author>James R. Kirk, Robert E. Wray, Peter Lindes, John E. Laird</author><pubDate>Mon, 15 May 2023 17:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09554v3</guid></item><item><title>Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking</title><link>http://arxiv.org/abs/2303.17907v3</link><description>The advancement of Virtual Reality (VR) technology is focused on improvingits immersiveness, supporting multiuser Virtual Experiences (VEs), and enablingthe users to move freely within their VEs while still being confined withinspecialized VR setups through Redirected Walking (RDW). To meet their extremedata-rate and latency requirements, future VR systems will require supportingwireless networking infrastructures operating in millimeter Wave (mmWave)frequencies that leverage highly directional communication in both transmissionand reception through beamforming and beamsteering. We propose the use ofpredictive context-awareness to optimize transmitter and receiver-sidebeamforming and beamsteering. By predicting users' short-term lateral movementsin multiuser VR setups with Redirected Walking (RDW), transmitter-sidebeamforming and beamsteering can be optimized through Line-of-Sight (LoS)"tracking" in the users' directions. At the same time, predictions ofshort-term orientational movements can be utilized for receiver-sidebeamforming for coverage flexibility enhancements. We target two open problemsin predicting these two context information instances: i) predicting lateralmovements in multiuser VR settings with RDW, and ii) generating synthetic headrotation datasets for training orientational movements predictors. Ourexperimental results demonstrate that Long Short-Term Memory (LSTM) networksfeature promising accuracy in predicting lateral movements, andcontext-awareness stemming from VEs further enhances this accuracy.Additionally, we show that a TimeGAN-based approach for orientational datageneration can create synthetic samples that closely match experimentallyobtained ones.</description><author>Filip Lemic, Jakob Struye, Thomas Van Onsem, Jeroen Famaey, Xavier Costa Perez</author><pubDate>Mon, 15 May 2023 17:34:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17907v3</guid></item><item><title>SVD-DIP: Overcoming the Overfitting Problem in DIP-based CT Reconstruction</title><link>http://arxiv.org/abs/2303.15748v3</link><description>The deep image prior (DIP) is a well-established unsupervised deep learningmethod for image reconstruction; yet it is far from being flawless. The DIPoverfits to noise if not early stopped, or optimized via a regularizedobjective. We build on the regularized fine-tuning of a pretrained DIP, byadopting a novel strategy that restricts the learning to the adaptation ofsingular values. The proposed SVD-DIP uses ad hoc convolutional layers whosepretrained parameters are decomposed via the singular value decomposition.Optimizing the DIP then solely consists in the fine-tuning of the singularvalues, while keeping the left and right singular vectors fixed. We thoroughlyvalidate the proposed method on real-measured $\mu$CT data of a lotus root aswell as two medical datasets (LoDoPaB and Mayo). We report significantlyimproved stability of the DIP optimization, by overcoming the overfitting tonoise.</description><author>Marco Nittscher, Michael Lameter, Riccardo Barbano, Johannes Leuschner, Bangti Jin, Peter Maass</author><pubDate>Mon, 15 May 2023 17:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15748v3</guid></item><item><title>Transactional Python for Durable Machine Learning: Vision, Challenges, and Feasibility</title><link>http://arxiv.org/abs/2305.08770v1</link><description>In machine learning (ML), Python serves as a convenient abstraction forworking with key libraries such as PyTorch, scikit-learn, and others. UnlikeDBMS, however, Python applications may lose important data, such as trainedmodels and extracted features, due to machine failures or human errors, leadingto a waste of time and resources. Specifically, they lack four essentialproperties that could make ML more reliable and user-friendly -- durability,atomicity, replicability, and time-versioning (DART). This paper presents our vision of Transactional Python that provides DARTwithout any code modifications to user programs or the Python kernel, bynon-intrusively monitoring application states at the object level anddetermining a minimal amount of information sufficient to reconstruct a wholeapplication. Our evaluation of a proof-of-concept implementation with publicPyTorch and scikit-learn applications shows that DART can be offered withoverheads ranging 1.5%--15.6%.</description><author>Supawit Chockchowwat, Zhaoheng Li, Yongjoo Park</author><pubDate>Mon, 15 May 2023 17:27:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08770v1</guid></item><item><title>DA-LSTM: A Dynamic Drift-Adaptive Learning Framework for Interval Load Forecasting with LSTM Networks</title><link>http://arxiv.org/abs/2305.08767v1</link><description>Load forecasting is a crucial topic in energy management systems (EMS) due toits vital role in optimizing energy scheduling and enabling more flexible andintelligent power grid systems. As a result, these systems allow power utilitycompanies to respond promptly to demands in the electricity market. Deeplearning (DL) models have been commonly employed in load forecasting problemssupported by adaptation mechanisms to cope with the changing pattern ofconsumption by customers, known as concept drift. A drift magnitude thresholdshould be defined to design change detection methods to identify drifts. Whilethe drift magnitude in load forecasting problems can vary significantly overtime, existing literature often assumes a fixed drift magnitude threshold,which should be dynamically adjusted rather than fixed during system evolution.To address this gap, in this paper, we propose a dynamic drift-adaptive LongShort-Term Memory (DA-LSTM) framework that can improve the performance of loadforecasting models without requiring a drift threshold setting. We integrateseveral strategies into the framework based on active and passive adaptationapproaches. To evaluate DA-LSTM in real-life settings, we thoroughly analyzethe proposed framework and deploy it in a real-world problem through acloud-based environment. Efficiency is evaluated in terms of the predictionperformance of each approach and computational cost. The experiments showperformance improvements on multiple evaluation metrics achieved by ourframework compared to baseline methods from the literature. Finally, we presenta trade-off analysis between prediction performance and computational costs.</description><author>Firas Bayram, Phil Aupke, Bestoun S. Ahmed, Andreas Kassler, Andreas Theocharis, Jonas Forsman</author><pubDate>Mon, 15 May 2023 17:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08767v1</guid></item><item><title>HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level</title><link>http://arxiv.org/abs/2305.06588v2</link><description>Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhileendeavor. HKG consists of hyper-relational facts (H-Facts), composed of a maintriple and several auxiliary attribute-value qualifiers, which can effectivelyrepresent factually comprehensive information. The internal structure of HKGcan be represented as a hypergraph-based representation globally and a semanticsequence-based representation locally. However, existing research seldomsimultaneously models the graphical and sequential structure of HKGs, limitingHKGs' representation. To overcome this limitation, we propose a novelHierarchical Attention model for HKG Embedding (HAHE), including global-leveland local-level attention. The global-level attention can model the graphicalstructure of HKG using hypergraph dual-attention layers, while the local-levelattention can learn the sequential structure inside H-Facts via heterogeneousself-attention layers. Experiment results indicate that HAHE achievesstate-of-the-art performance in link prediction tasks on HKG standard datasets.In addition, HAHE addresses the issue of HKG multi-position prediction for thefirst time, increasing the applicability of the HKG link prediction task. Ourcode is publicly available.</description><author>Haoran Luo, Haihong E, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, Wei Lin</author><pubDate>Mon, 15 May 2023 17:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06588v2</guid></item><item><title>ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human</title><link>http://arxiv.org/abs/2304.07849v3</link><description>In this paper, we present ChatPLUG, a Chinese open-domain dialogue system fordigital human applications that instruction finetunes on a wide range ofdialogue tasks in a unified internet-augmented format. Different from otheropen-domain dialogue models that focus on large-scale pre-training and scalingup model size or dialogue corpus, we aim to build a powerful and practicaldialogue system for digital human with diverse skills and good multi-taskgeneralization by internet-augmented instruction tuning. To this end, we firstconduct large-scale pre-training on both common document corpus and dialoguedata with curriculum learning, so as to inject various world knowledge anddialogue abilities into ChatPLUG. Then, we collect a wide range of dialoguetasks spanning diverse features of knowledge, personality, multi-turn memory,and empathy, on which we further instruction tune \modelname via unifiednatural language instruction templates. External knowledge from an internetsearch is also used during instruction finetuning for alleviating the problemof knowledge hallucinations. We show that \modelname outperformsstate-of-the-art Chinese dialogue systems on both automatic and humanevaluation, and demonstrates strong multi-task generalization on a variety oftext understanding and generation tasks. In addition, we deploy \modelname toreal-world applications such as Smart Speaker and Instant Message applicationswith fast inference. Our models and code will be made publicly available onModelScope: https://modelscope.cn/models/damo/ChatPLUG-3.7B and Github:https://github.com/X-PLUG/ChatPLUG .</description><author>Junfeng Tian, Hehong Chen, Guohai Xu, Ming Yan, Xing Gao, Jianhai Zhang, Chenliang Li, Jiayi Liu, Wenshen Xu, Haiyang Xu, Qi Qian, Wei Wang, Qinghao Ye, Jiejing Zhang, Ji Zhang, Fei Huang, Jingren Zhou</author><pubDate>Mon, 15 May 2023 17:17:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07849v3</guid></item><item><title>Physics Informed Token Transformer</title><link>http://arxiv.org/abs/2305.08757v1</link><description>Solving Partial Differential Equations (PDEs) is the core of many fields ofscience and engineering. While classical approaches are often prohibitivelyslow, machine learning models often fail to incorporate complete systeminformation. Over the past few years, transformers have had a significantimpact on the field of Artificial Intelligence and have seen increased usage inPDE applications. However, despite their success, transformers currently lackintegration with physics and reasoning. This study aims to address this issueby introducing PITT: Physics Informed Token Transformer. The purpose of PITT isto incorporate the knowledge of physics by embedding partial differentialequations (PDEs) into the learning process. PITT uses an equation tokenizationmethod to learn an analytically-driven numerical update operator. By tokenizingPDEs and embedding partial derivatives, the transformer models become aware ofthe underlying knowledge behind physical processes. To demonstrate this, PITTis tested on challenging PDE neural operators in both 1D and 2D predictiontasks. The results show that PITT outperforms the popular Fourier NeuralOperator and has the ability to extract physically relevant information fromgoverning equations.</description><author>Cooper Lorsung, Zijie Li, Amir Barati Farimani</author><pubDate>Mon, 15 May 2023 17:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08757v1</guid></item><item><title>Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra</title><link>http://arxiv.org/abs/2304.09987v2</link><description>Neural Radiance Fields (NeRFs) are a very recent and very popular approachfor the problems of novel view synthesis and 3D reconstruction. A popular scenerepresentation used by NeRFs is to combine a uniform, voxel-based subdivisionof the scene with an MLP. Based on the observation that a (sparse) point cloudof the scene is often available, this paper proposes to use an adaptiverepresentation based on tetrahedra obtained by the Delaunay triangulationinstead of the uniform subdivision or point-based representations. We show thatsuch a representation enables efficient training and leads to state-of-the-artresults. Our approach elegantly combines concepts from 3D geometry processing,triangle-based rendering, and modern neural radiance fields. Compared tovoxel-based representations, ours provides more detail around parts of thescene likely to be close to the surface. Compared to point-basedrepresentations, our approach achieves better performance.</description><author>Jonas Kulhanek, Torsten Sattler</author><pubDate>Mon, 15 May 2023 17:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09987v2</guid></item><item><title>Neural Oscillators are Universal</title><link>http://arxiv.org/abs/2305.08753v1</link><description>Coupled oscillators are being increasingly used as the basis of machinelearning (ML) architectures, for instance in sequence modeling, graphrepresentation learning and in physical neural networks that are used in analogML devices. We introduce an abstract class of neural oscillators thatencompasses these architectures and prove that neural oscillators areuniversal, i.e, they can approximate any continuous and casual operator mappingbetween time-varying functions, to desired accuracy. This universality resultprovides theoretical justification for the use of oscillator based ML systems.The proof builds on a fundamental result of independent interest, which showsthat a combination of forced harmonic oscillators with a nonlinear read-outsuffices to approximate the underlying operators.</description><author>Samuel Lanthaler, T. Konstantin Rusch, Siddhartha Mishra</author><pubDate>Mon, 15 May 2023 17:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08753v1</guid></item><item><title>A Matter of Annotation: An Empirical Study on In Situ and Self-Recall Activity Annotations from Wearable Sensors</title><link>http://arxiv.org/abs/2305.08752v1</link><description>Research into the detection of human activities from wearable sensors is ahighly active field, benefiting numerous applications, from ambulatorymonitoring of healthcare patients via fitness coaching to streamlining manualwork processes. We present an empirical study that compares 4 differentcommonly used annotation methods utilized in user studies that focus onin-the-wild data. These methods can be grouped in user-driven, in situannotations - which are performed before or during the activity is recorded -and recall methods - where participants annotate their data in hindsight at theend of the day. Our study illustrates that different labeling methodologiesdirectly impact the annotations' quality, as well as the capabilities of a deeplearning classifier trained with the data respectively. We noticed that in situmethods produce less but more precise labels than recall methods. Furthermore,we combined an activity diary with a visualization tool that enables theparticipant to inspect and label their activity data. Due to the introductionof such a tool were able to decrease missing annotations and increase theannotation consistency, and therefore the F1-score of the deep learning modelby up to 8% (ranging between 82.1 and 90.4% F1-score). Furthermore, we discussthe advantages and disadvantages of the methods compared in our study, thebiases they may could introduce and the consequences of their usage on humanactivity recognition studies and as well as possible solutions.</description><author>Alexander Hoelzemann, Kristof Van Laerhoven</author><pubDate>Mon, 15 May 2023 17:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08752v1</guid></item><item><title>Fast and Attributed Change Detection on Dynamic Graphs with Density of States</title><link>http://arxiv.org/abs/2305.08750v1</link><description>How can we detect traffic disturbances from international flighttransportation logs or changes to collaboration dynamics in academic networks?These problems can be formulated as detecting anomalous change points in adynamic graph. Current solutions do not scale well to large real-world graphs,lack robustness to large amounts of node additions/deletions, and overlookchanges in node attributes. To address these limitations, we propose a novelspectral method: Scalable Change Point Detection (SCPD). SCPD generates anembedding for each graph snapshot by efficiently approximating the distributionof the Laplacian spectrum at each step. SCPD can also capture shifts in nodeattributes by tracking correlations between attributes and eigenvectors.Through extensive experiments using synthetic and real-world data, we show thatSCPD (a) achieves state-of-the art performance, (b) is significantly fasterthan the state-of-the-art methods and can easily process millions of edges in afew CPU minutes, (c) can effectively tackle a large quantity of nodeattributes, additions or deletions and (d) discovers interesting events inlarge real-world graphs. The code is publicly available athttps://github.com/shenyangHuang/SCPD.git</description><author>Shenyang Huang, Jacob Danovitch, Guillaume Rabusseau, Reihaneh Rabbany</author><pubDate>Mon, 15 May 2023 17:02:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08750v1</guid></item><item><title>Automating privacy decisions -- where to draw the line?</title><link>http://arxiv.org/abs/2305.08747v1</link><description>Users are often overwhelmed by privacy decisions to manage their personaldata, which can happen on the web, in mobile, and in IoT environments. Thesedecisions can take various forms -- such as decisions for setting privacypermissions or privacy preferences, decisions responding to consent requests,or to intervene and ``reject'' processing of one's personal data --, and eachcan have different legal impacts. In all cases and for all types of decisions, scholars and industry have beenproposing tools to better automate the process of privacy decisions atdifferent levels, in order to enhance usability. We provide in this paper an overview of the main challenges raised by theautomation of privacy decisions, together with a classification scheme of theexisting and envisioned work and proposals addressing automation of privacydecisions.</description><author>Victor Morel, Simone Fischer-Hübner</author><pubDate>Mon, 15 May 2023 16:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08747v1</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v2</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Mon, 15 May 2023 16:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v2</guid></item><item><title>Integrating Uncertainty into Neural Network-based Speech Enhancement</title><link>http://arxiv.org/abs/2305.08744v1</link><description>Supervised masking approaches in the time-frequency domain aim to employ deepneural networks to estimate a multiplicative mask to extract clean speech. Thisleads to a single estimate for each input without any guarantees or measures ofreliability. In this paper, we study the benefits of modeling uncertainty inclean speech estimation. Prediction uncertainty is typically categorized intoaleatoric uncertainty and epistemic uncertainty. The former refers to inherentrandomness in data, while the latter describes uncertainty in the modelparameters. In this work, we propose a framework to jointly model aleatoric andepistemic uncertainties in neural network-based speech enhancement. Theproposed approach captures aleatoric uncertainty by estimating the statisticalmoments of the speech posterior distribution and explicitly incorporates theuncertainty estimate to further improve clean speech estimation. For epistemicuncertainty, we investigate two Bayesian deep learning approaches: Monte Carlodropout and Deep ensembles to quantify the uncertainty of the neural networkparameters. Our analyses show that the proposed framework promotes capturingpractical and reliable uncertainty, while combining different sources ofuncertainties yields more reliable predictive uncertainty estimates.Furthermore, we demonstrate the benefits of modeling uncertainty on speechenhancement performance by evaluating the framework on different datasets,exhibiting notable improvement over comparable models that fail to account foruncertainty.</description><author>Huajian Fang, Dennis Becker, Stefan Wermter, Timo Gerkmann</author><pubDate>Mon, 15 May 2023 16:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08744v1</guid></item><item><title>Lie Group Forced Variational Integrator Networks for Learning and Control of Robot Systems</title><link>http://arxiv.org/abs/2211.16006v4</link><description>Incorporating prior knowledge of physics laws and structural properties ofdynamical systems into the design of deep learning architectures has proven tobe a powerful technique for improving their computational efficiency andgeneralization capacity. Learning accurate models of robot dynamics is criticalfor safe and stable control. Autonomous mobile robots, including wheeled,aerial, and underwater vehicles, can be modeled as controlled Lagrangian orHamiltonian rigid-body systems evolving on matrix Lie groups. In this paper, weintroduce a new structure-preserving deep learning architecture, the Lie groupForced Variational Integrator Network (LieFVIN), capable of learning controlledLagrangian or Hamiltonian dynamics on Lie groups, either from position-velocityor position-only data. By design, LieFVINs preserve both the Lie groupstructure on which the dynamics evolve and the symplectic structure underlyingthe Hamiltonian or Lagrangian systems of interest. The proposed architecturelearns surrogate discrete-time flow maps allowing accurate and fast predictionwithout numerical-integrator, neural-ODE, or adjoint techniques, which areneeded for vector fields. Furthermore, the learnt discrete-time dynamics can beutilized with computationally scalable discrete-time (optimal) controlstrategies.</description><author>Valentin Duruisseaux, Thai Duong, Melvin Leok, Nikolay Atanasov</author><pubDate>Mon, 15 May 2023 16:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16006v4</guid></item><item><title>Refining Amortized Posterior Approximations using Gradient-Based Summary Statistics</title><link>http://arxiv.org/abs/2305.08733v1</link><description>We present an iterative framework to improve the amortized approximations ofposterior distributions in the context of Bayesian inverse problems, which isinspired by loop-unrolled gradient descent methods and is theoreticallygrounded in maximally informative summary statistics. Amortized variationalinference is restricted by the expressive power of the chosen variationaldistribution and the availability of training data in the form of joint dataand parameter samples, which often lead to approximation errors such as theamortization gap. To address this issue, we propose an iterative framework thatrefines the current amortized posterior approximation at each step. Ourapproach involves alternating between two steps: (1) constructing a trainingdataset consisting of pairs of summarized data residuals and parameters, wherethe summarized data residual is generated using a gradient-based summarystatistic, and (2) training a conditional generative model -- a normalizingflow in our examples -- on this dataset to obtain a probabilistic update of theunknown parameter. This procedure leads to iterative refinement of theamortized posterior approximations without the need for extra training data. Wevalidate our method in a controlled setting by applying it to a stylizedproblem, and observe improved posterior approximations with each iteration.Additionally, we showcase the capability of our method in tacklingrealistically sized problems by applying it to transcranial ultrasound, ahigh-dimensional, nonlinear inverse problem governed by wave physics, andobserve enhanced posterior quality through better image reconstruction with theposterior mean.</description><author>Rafael Orozco, Ali Siahkoohi, Mathias Louboutin, Felix J. Herrmann</author><pubDate>Mon, 15 May 2023 16:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08733v1</guid></item><item><title>Knowledge Rumination for Pre-trained Language Models</title><link>http://arxiv.org/abs/2305.08732v1</link><description>Previous studies have revealed that vanilla pre-trained language models(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,several works have attempted to integrate external knowledge into PLMs.However, despite the promising outcome, we empirically observe that PLMs mayhave already encoded rich knowledge in their pre-trained parameters but failsto fully utilize them when applying to knowledge-intensive tasks. In thispaper, we propose a new paradigm dubbed Knowledge Rumination to help thepre-trained language model utilize those related latent knowledge withoutretrieving them from the external corpus. By simply adding a prompt like ``Asfar as I know'' to the PLMs, we try to review related latent knowledge andinject them back to the model for knowledge consolidation. We apply theproposed knowledge rumination to various language models, including RoBERTa,DeBERTa, GPT-3 and OPT. Experimental results on six commonsense reasoning tasksand GLUE benchmarks demonstrate the effectiveness of our proposed approach,which further proves that the knowledge stored in PLMs can be better exploitedto enhance the downstream performance. Code will be available inhttps://github.com/zjunlp/knowledge-rumination.</description><author>Yunzhi Yao, Peng Wang, Shengyu Mao, Chuanqi Tan, Fei Huang, Huajun Chen, Ningyu Zhang</author><pubDate>Mon, 15 May 2023 16:47:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08732v1</guid></item><item><title>Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks</title><link>http://arxiv.org/abs/2303.15919v2</link><description>Real-world visual data exhibit intrinsic hierarchical structures that can berepresented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs)are a promising approach for learning feature representations in such spaces.However, current HNNs in computer vision rely on Euclidean backbones and onlyproject features to the hyperbolic space in the task heads, limiting theirability to fully leverage the benefits of hyperbolic geometry. To address this,we present HCNN, the first fully hyperbolic convolutional neural network (CNN)designed for computer vision tasks. Based on the Lorentz model, we generalizefundamental components of CNNs and propose novel formulations of theconvolutional layer, batch normalization, and multinomial logistic regression.Experimentation on standard vision tasks demonstrates the superiority of ourHCNN framework and the Lorentz model in both hybrid and fully hyperbolicsettings. Overall, we believe our contributions provide a foundation fordeveloping more powerful HNNs that can better represent complex structuresfound in image data. Our code is publicly available athttps://github.com/kschwethelm/HyperbolicCV.</description><author>Ahmad Bdeir, Kristian Schwethelm, Niels Landwehr</author><pubDate>Mon, 15 May 2023 16:43:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15919v2</guid></item><item><title>COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks</title><link>http://arxiv.org/abs/2204.09593v2</link><description>Vision outlooker improves the performance of vision transformers, whichimplements a self-attention mechanism by adding an outlook attention, a form oflocal attention. In natural language processing, as has been the case in computer vision andother domains, transformer-based models constitute the state-of-the-art formost processing tasks. In this domain, too, many authors have argued anddemonstrated the importance of local context. We present an outlook attention mechanism, COOL, for natural languageprocessing. COOL, added on top of the self-attention layers of atransformer-based model, encodes local syntactic context considering wordproximity and more pair-wise constraints than dynamic convolution used byexisting approaches. A comparative empirical performance evaluation of an implementation of COOLwith different transformer-based models confirms the opportunity forimprovement over a baseline using the original models alone for various naturallanguage processing tasks, including question answering. The proposed approachachieves competitive performance with existing state-of-the-art methods on sometasks.</description><author>Fangyi Zhu, See-Kiong Ng, Stéphane Bressan</author><pubDate>Mon, 15 May 2023 16:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.09593v2</guid></item><item><title>Unveiling the Latent Space Geometry of Push-Forward Generative Models</title><link>http://arxiv.org/abs/2207.10541v3</link><description>Many deep generative models are defined as a push-forward of a Gaussianmeasure by a continuous generator, such as Generative Adversarial Networks(GANs) or Variational Auto-Encoders (VAEs). This work explores the latent spaceof such deep generative models. A key issue with these models is their tendencyto output samples outside of the support of the target distribution whenlearning disconnected distributions. We investigate the relationship betweenthe performance of these models and the geometry of their latent space.Building on recent developments in geometric measure theory, we prove asufficient condition for optimality in the case where the dimension of thelatent space is larger than the number of modes. Through experiments on GANs,we demonstrate the validity of our theoretical results and gain new insightsinto the latent space geometry of these models. Additionally, we propose atruncation method that enforces a simplicial cluster structure in the latentspace and improves the performance of GANs.</description><author>Thibaut Issenhuth, Ugo Tanielian, Jérémie Mary, David Picard</author><pubDate>Mon, 15 May 2023 16:39:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.10541v3</guid></item><item><title>Learning More Discriminative Local Descriptors for Few-shot Learning</title><link>http://arxiv.org/abs/2305.08721v1</link><description>Few-shot learning for image classification comes up as a hot topic incomputer vision, which aims at fast learning from a limited number of labeledimages and generalize over the new tasks. In this paper, motivated by the ideaof Fisher Score, we propose a Discriminative Local Descriptors Attention (DLDA)model that adaptively selects the representative local descriptors and does notintroduce any additional parameters, while most of the existing localdescriptors based methods utilize the neural networks that inevitably involvethe tedious parameter tuning. Moreover, we modify the traditional $k$-NNclassification model by adjusting the weights of the $k$ nearest neighborsaccording to their distances from the query point. Experiments on fourbenchmark datasets show that our method not only achieves higher accuracycompared with the state-of-art approaches for few-shot learning, but alsopossesses lower sensitivity to the choices of $k$.</description><author>Qijun Song, Siyun Zhou, Liwei Xu</author><pubDate>Mon, 15 May 2023 16:33:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08721v1</guid></item><item><title>M$^{6}$Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis</title><link>http://arxiv.org/abs/2305.08719v1</link><description>Document layout analysis is a crucial prerequisite for documentunderstanding, including document retrieval and conversion. Most publicdatasets currently contain only PDF documents and lack realistic documents.Models trained on these datasets may not generalize well to real-worldscenarios. Therefore, this paper introduces a large and diverse document layoutanalysis dataset called $M^{6}Doc$. The $M^6$ designation represents sixproperties: (1) Multi-Format (including scanned, photographed, and PDFdocuments); (2) Multi-Type (such as scientific articles, textbooks, books, testpapers, magazines, newspapers, and notes); (3) Multi-Layout (rectangular,Manhattan, non-Manhattan, and multi-column Manhattan); (4) Multi-Language(Chinese and English); (5) Multi-Annotation Category (74 types of annotationlabels with 237,116 annotation instances in 9,080 manually annotated pages);and (6) Modern documents. Additionally, we propose a transformer-based documentlayout analysis method called TransDLANet, which leverages an adaptive elementmatching mechanism that enables query embedding to better match ground truth toimprove recall, and constructs a segmentation branch for more precise documentimage instance segmentation. We conduct a comprehensive evaluation of$M^{6}Doc$ with various layout analysis methods and demonstrate itseffectiveness. TransDLANet achieves state-of-the-art performance on $M^{6}Doc$with 64.5\% mAP. The $M^{6}Doc$ dataset will be available athttps://github.com/HCIILAB/M6Doc.</description><author>Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li, Kai Ding, Lianwen Jin</author><pubDate>Mon, 15 May 2023 16:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08719v1</guid></item><item><title>Federated X-Armed Bandit</title><link>http://arxiv.org/abs/2205.15268v3</link><description>This work establishes the first framework of federated $\mathcal{X}$-armedbandit, where different clients face heterogeneous local objective functionsdefined on the same domain and are required to collaboratively figure out theglobal optimum. We propose the first federated algorithm for such problems,named \texttt{Fed-PNE}. By utilizing the topological structure of the globalobjective inside the hierarchical partitioning and the weak smoothnessproperty, our algorithm achieves sublinear cumulative regret with respect toboth the number of clients and the evaluation budget. Meanwhile, it onlyrequires logarithmic communications between the central server and clients,protecting the client privacy. Experimental results on synthetic functions andreal datasets validate the advantages of \texttt{Fed-PNE} over variouscentralized and federated baseline algorithms.</description><author>Wenjie Li, Qifan Song, Jean Honorio, Guang Lin</author><pubDate>Mon, 15 May 2023 16:24:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.15268v3</guid></item><item><title>Sensitivity and Robustness of Large Language Models to Prompt in Japanese</title><link>http://arxiv.org/abs/2305.08714v1</link><description>Prompt Engineering has gained significant relevance in recent years, fueledby advancements in pre-trained and large language models. However, a criticalissue has been identified within this domain: the lack of sensitivity androbustness of these models towards Prompt Templates, particularly inlesser-studied languages such as Japanese. This paper explores this issuethrough a comprehensive evaluation of several representative Large LanguageModels (LLMs) and a widely-utilized pre-trained model(PLM), T5. These modelsare scrutinized using a benchmark dataset in Japanese, with the aim to assessand analyze the performance of the current multilingual models in this context.Our experimental results reveal startling discrepancies. A simple modificationin the sentence structure of the Prompt Template led to a drastic drop in theaccuracy of GPT-4 from 49.21 to 25.44. This observation underscores the factthat even the highly performance GPT-4 model encounters significant stabilityissues when dealing with diverse Japanese prompt templates, rendering theconsistency of the model's output results questionable. In light of thesefindings, we conclude by proposing potential research trajectories to furtherenhance the development and performance of Large Language Models in theircurrent stage.</description><author>Chengguang Gan, Tatsunori Mori</author><pubDate>Mon, 15 May 2023 16:19:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08714v1</guid></item><item><title>sustain.AI: a Recommender System to analyze Sustainability Reports</title><link>http://arxiv.org/abs/2305.08711v1</link><description>We present sustain.AI, an intelligent, context-aware recommender system thatassists auditors and financial investors as well as the general public toefficiently analyze companies' sustainability reports. The tool leverages anend-to-end trainable architecture that couples a BERT-based encoding modulewith a multi-label classification head to match relevant text passages fromsustainability reports to their respective law regulations from the GlobalReporting Initiative (GRI) standards. We evaluate our model on two novel Germansustainability reporting data sets and consistently achieve a significantlyhigher recommendation performance compared to multiple strong baselines.Furthermore, sustain.AI will be publicly available for everyone within the nextmonths.</description><author>Lars Hillebrand, Maren Pielka, David Leonhard, Tobias Deußer, Tim Dilmaghani, Bernd Kliem, Rüdiger Loitz, Milad Morad, Christian Temath, Thiago Bell, Robin Stenzel, Rafet Sifa</author><pubDate>Mon, 15 May 2023 16:16:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08711v1</guid></item><item><title>Back Translation for Speech-to-text Translation Without Transcripts</title><link>http://arxiv.org/abs/2305.08709v1</link><description>The success of end-to-end speech-to-text translation (ST) is often achievedby utilizing source transcripts, e.g., by pre-training with automatic speechrecognition (ASR) and machine translation (MT) tasks, or by introducingadditional ASR and MT data. Unfortunately, transcripts are only sometimesavailable since numerous unwritten languages exist worldwide. In this paper, weaim to utilize large amounts of target-side monolingual data to enhance STwithout transcripts. Motivated by the remarkable success of back translation inMT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudoST data from monolingual target data. To ease the challenges posed byshort-to-long generation and one-to-many mapping, we introduce self-superviseddiscrete units and achieve back translation by cascading a target-to-unit modeland a unit-to-speech model. With our synthetic ST data, we achieve an averageboost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experimentsshow that our method is especially effective in low-resource scenarios.</description><author>Qingkai Fang, Yang Feng</author><pubDate>Mon, 15 May 2023 16:12:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08709v1</guid></item><item><title>Decentralization and Acceleration Enables Large-Scale Bundle Adjustment</title><link>http://arxiv.org/abs/2305.07026v2</link><description>Scaling to arbitrarily large bundle adjustment problems requires data andcompute to be distributed across multiple devices. Centralized methods in priorworks are only able to solve small or medium size problems due to overhead incomputation and communication. In this paper, we present a fully decentralizedmethod that alleviates computation and communication bottlenecks to solvearbitrarily large bundle adjustment problems. We achieve this by reformulatingthe reprojection error and deriving a novel surrogate function that decouplesoptimization variables from different devices. This function makes it possibleto use majorization minimization techniques and reduces bundle adjustment toindependent optimization subproblems that can be solved in parallel. We furtherapply Nesterov's acceleration and adaptive restart to improve convergence whilemaintaining its theoretical guarantees. Despite limited peer-to-peercommunication, our method has provable convergence to first-order criticalpoints under mild conditions. On extensive benchmarks with public datasets, ourmethod converges much faster than decentralized baselines with similar memoryusage and communication load. Compared to centralized baselines using a singledevice, our method, while being decentralized, yields more accurate solutionswith significant speedups of up to 953.7x over Ceres and 174.6x over DeepLM.Code: https://github.com/facebookresearch/DABA.</description><author>Taosha Fan, Joseph Ortiz, Ming Hsiao, Maurizio Monge, Jing Dong, Todd Murphey, Mustafa Mukadam</author><pubDate>Mon, 15 May 2023 16:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07026v2</guid></item><item><title>On Low Rank Directed Acyclic Graphs and Causal Structure Learning</title><link>http://arxiv.org/abs/2006.05691v2</link><description>Despite several advances in recent years, learning causal structuresrepresented by directed acyclic graphs (DAGs) remains a challenging task inhigh dimensional settings when the graphs to be learned are not sparse. In thispaper, we propose to exploit a low rank assumption regarding the (weighted)adjacency matrix of a DAG causal model to help address this problem. We utilizeexisting low rank techniques to adapt causal structure learning methods to takeadvantage of this assumption and establish several useful results relatinginterpretable graphical conditions to the low rank assumption. Specifically, weshow that the maximum rank is highly related to hubs, suggesting thatscale-free networks, which are frequently encountered in practice, tend to below rank. Our experiments demonstrate the utility of the low rank adaptationsfor a variety of data models, especially with relatively large and densegraphs. Moreover, with a validation procedure, the adaptations maintain asuperior or comparable performance even when graphs are not restricted to below rank.</description><author>Zhuangyan Fang, Shengyu Zhu, Jiji Zhang, Yue Liu, Zhitang Chen, Yangbo He</author><pubDate>Mon, 15 May 2023 16:09:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2006.05691v2</guid></item><item><title>Understanding and Bridging the Modality Gap for Speech Translation</title><link>http://arxiv.org/abs/2305.08706v1</link><description>How to achieve better end-to-end speech translation (ST) by leveraging (text)machine translation (MT) data? Among various existing techniques, multi-tasklearning is one of the effective ways to share knowledge between ST and MT inwhich additional MT data can help to learn source-to-target mapping. However,due to the differences between speech and text, there is always a gap betweenST and MT. In this paper, we first aim to understand this modality gap from thetarget-side representation differences, and link the modality gap to anotherwell-known problem in neural machine translation: exposure bias. We find thatthe modality gap is relatively small during training except for some difficultcases, but keeps increasing during inference due to the cascading effect. Toaddress these problems, we propose the Cross-modal Regularization withScheduled Sampling (Cress) method. Specifically, we regularize the outputpredictions of ST and MT, whose target-side contexts are derived by samplingbetween ground truth words and self-generated words with a varying probability.Furthermore, we introduce token-level adaptive training which assigns differenttraining weights to target tokens to handle difficult cases with large modalitygaps. Experiments and analysis show that our approach effectively bridges themodality gap, and achieves promising results in all eight directions of theMuST-C dataset.</description><author>Qingkai Fang, Yang Feng</author><pubDate>Mon, 15 May 2023 16:09:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08706v1</guid></item><item><title>Arbitrary Decisions are a Hidden Cost of Differentially Private Training</title><link>http://arxiv.org/abs/2302.14517v2</link><description>Mechanisms used in privacy-preserving machine learning often aim to guaranteedifferential privacy (DP) during model training. Practical DP-ensuring trainingmethods use randomization when fitting model parameters to privacy-sensitivedata (e.g., adding Gaussian noise to clipped gradients). We demonstrate thatsuch randomization incurs predictive multiplicity: for a given input example,the output predicted by equally-private models depends on the randomness usedin training. Thus, for a given input, the predicted output can vary drasticallyif a model is re-trained, even if the same training dataset is used. Thepredictive-multiplicity cost of DP training has not been studied, and iscurrently neither audited for nor communicated to model designers andstakeholders. We derive a bound on the number of re-trainings required toestimate predictive multiplicity reliably. We analyze--both theoretically andthrough extensive experiments--the predictive-multiplicity cost of threeDP-ensuring algorithms: output perturbation, objective perturbation, andDP-SGD. We demonstrate that the degree of predictive multiplicity rises as thelevel of privacy increases, and is unevenly distributed across individuals anddemographic groups in the data. Because randomness used to ensure DP duringtraining explains predictions for some examples, our results highlight afundamental challenge to the justifiability of decisions supported bydifferentially private models in high-stakes settings. We conclude thatpractitioners should audit the predictive multiplicity of their DP-ensuringalgorithms before deploying them in applications of individual-levelconsequence.</description><author>Bogdan Kulynych, Hsiang Hsu, Carmela Troncoso, Flavio P. Calmon</author><pubDate>Mon, 15 May 2023 16:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14517v2</guid></item><item><title>Schema-adaptable Knowledge Graph Construction</title><link>http://arxiv.org/abs/2305.08703v1</link><description>Conventional Knowledge Graph Construction (KGC) approaches typically followthe static information extraction paradigm with a closed set of pre-definedschema. As a result, such approaches fall short when applied to dynamicscenarios or domains, whereas a new type of knowledge emerges. Thisnecessitates a system that can handle evolving schema automatically to extractinformation for KGC. To address this need, we propose a new task calledschema-adaptable KGC, which aims to continually extract entity, relation, andevent based on a dynamically changing schema graph without re-training. Wefirst split and convert existing datasets based on three principles to build abenchmark, i.e., horizontal schema expansion, vertical schema expansion, andhybrid schema expansion; then investigate the schema-adaptable performance ofseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3. Wefurther propose a simple yet effective baseline dubbed AdaKGC, which containsschema-enriched prefix instructor and schema-conditioned dynamic decoding tobetter handle evolving schema. Comprehensive experimental results illustratethat AdaKGC can outperform baselines but still have room for improvement. Wehope the proposed work can deliver benefits to the community. Code and datasetswill be available in https://github.com/zjunlp/AdaKGC.</description><author>Hongbin Ye, Honghao Gui, Xin Xu, Huajun Chen, Ningyu Zhang</author><pubDate>Mon, 15 May 2023 16:06:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08703v1</guid></item><item><title>Recyclable Tuning for Continual Pre-training</title><link>http://arxiv.org/abs/2305.08702v1</link><description>Continual pre-training is the paradigm where pre-trained language models(PLMs) continually acquire fresh knowledge from growing data and gradually getupgraded. Before an upgraded PLM is released, we may have tuned the originalPLM for various tasks and stored the adapted weights. However, when tuning theupgraded PLM, these outdated adapted weights will typically be ignored anddiscarded, causing a potential waste of resources. We bring this issue to theforefront and contend that proper algorithms for recycling outdated adaptedweights should be developed. To this end, we formulate the task of recyclabletuning for continual pre-training. In pilot studies, we find that aftercontinual pre-training, the upgraded PLM remains compatible with the outdatedadapted weights to some extent. Motivated by this finding, we analyze theconnection between continually pre-trained PLMs from two novel aspects, i.e.,mode connectivity, and functional similarity. Based on the correspondingfindings, we propose both an initialization-based method and adistillation-based method for our task. We demonstrate their feasibility inimproving the convergence and performance for tuning the upgraded PLM. We alsoshow that both methods can be combined to achieve better performance. Thesource codes are publicly available athttps://github.com/thunlp/RecyclableTuning.</description><author>Yujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong Wang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou</author><pubDate>Mon, 15 May 2023 16:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08702v1</guid></item><item><title>Quantum compiling with a variational instruction set for accurate and fast quantum computing</title><link>http://arxiv.org/abs/2203.15574v4</link><description>The quantum instruction set (QIS) is defined as the quantum gates that arephysically realizable by controlling the qubits in quantum hardware. Compilingquantum circuits into the product of the gates in a properly defined QIS is afundamental step in quantum computing. We here propose the quantum variationalinstruction set (QuVIS) formed by flexibly designed multi-qubit gates forhigher speed and accuracy of quantum computing. The controlling of qubits forrealizing the gates in a QuVIS is variationally achieved using the fine-grainedtime optimization algorithm. Significant reductions in both the erroraccumulation and time cost are demonstrated in realizing the swaps of multiplequbits and quantum Fourier transformations, compared with the compiling by astandard QIS such as the quantum microinstruction set (QuMIS, formed by severalone- and two-qubit gates including one-qubit rotations and controlled-NOTgates). With the same requirement on quantum hardware, the time cost for QuVISis reduced to less than one half of that for QuMIS. Simultaneously, the erroris suppressed algebraically as the depth of the compiled circuit is reduced. Asa general compiling approach with high flexibility and efficiency, QuVIS can bedefined for different quantum circuits and be adapted to the quantum hardwarewith different interactions.</description><author>Ying Lu, Peng-Fei Zhou, Shao-Ming Fei, Shi-Ju Ran</author><pubDate>Mon, 15 May 2023 16:01:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.15574v4</guid></item><item><title>Continual Multimodal Knowledge Graph Construction</title><link>http://arxiv.org/abs/2305.08698v1</link><description>Multimodal Knowledge Graph Construction (MMKC) refers to the process ofcreating a structured representation of entities and relationships throughmultiple modalities such as text, images, videos, etc. However, existing MMKCmodels have limitations in handling the introduction of new entities andrelations due to the dynamic nature of the real world. Moreover, moststate-of-the-art studies in MMKC only consider entity and relation extractionfrom text data while neglecting other multi-modal sources. Meanwhile, thecurrent continual setting for knowledge graph construction only consider entityand relation extraction from text data while neglecting other multi-modalsources. Therefore, there arises the need to explore the challenge ofcontinuous multimodal knowledge graph construction to address the phenomenon ofcatastrophic forgetting and ensure the retention of past knowledge extractedfrom different forms of data. This research focuses on investigating thiscomplex topic by developing lifelong multimodal benchmark datasets. Based onthe empirical findings that several state-of-the-art MMKC models, when trainedon multimedia data, might unexpectedly underperform compared to those solelyutilizing textual resources in a continual setting, we propose a LifelongMultiModal Consistent Transformer Framework (LMC) for continuous multimodalknowledge graph construction. By combining the advantages of consistent KGCstrategies within the context of continual learning, we achieve greater balancebetween stability and plasticity. Our experiments demonstrate the superiorperformance of our method over prevailing continual learning techniques ormultimodal approaches in dynamic scenarios. Code and datasets can be found athttps://github.com/zjunlp/ContinueMKGC.</description><author>Xiang Chen, Jintian Zhang, Xiaohan Wang, Tongtong Wu, Shumin Deng, Yongheng Wang, Luo Si, Huajun Chen, Ningyu Zhang</author><pubDate>Mon, 15 May 2023 15:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08698v1</guid></item><item><title>A Reproducible Extraction of Training Images from Diffusion Models</title><link>http://arxiv.org/abs/2305.08694v1</link><description>Recently, Carlini et al. demonstrated the widely used model Stable Diffusioncan regurgitate real training samples, which is troublesome from a copyrightperspective. In this work, we provide an efficient extraction attack on parwith the recent attack, with several order of magnitudes less networkevaluations. In the process, we expose a new phenomena, which we dub templateverbatims, wherein a diffusion model will regurgitate a training sample largelyin tact. Template verbatims are harder to detect as they require retrieval andmasking to correctly label. Furthermore, they are still generated by newersystems, even those which de-duplicate their training set, and we give insightinto why they still appear during generation. We extract training images fromseveral state of the art systems, including Stable Diffusion 2.0, Deep ImageFloyd, and finally Midjourney v4. We release code to verify our extractionattack, perform the attack, as well as all extracted prompts at\url{https://github.com/ryanwebster90/onestep-extraction}.</description><author>Ryan Webster</author><pubDate>Mon, 15 May 2023 15:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08694v1</guid></item><item><title>Accelerated Algorithms for Nonlinear Matrix Decomposition with the ReLU function</title><link>http://arxiv.org/abs/2305.08687v1</link><description>In this paper, we study the following nonlinear matrix decomposition (NMD)problem: given a sparse nonnegative matrix $X$, find a low-rank matrix $\Theta$such that $X \approx f(\Theta)$, where $f$ is an element-wise nonlinearfunction. We focus on the case where $f(\cdot) = \max(0, \cdot)$, the rectifiedunit (ReLU) non-linear activation. We refer to the corresponding problem asReLU-NMD. We first provide a brief overview of the existing approaches thatwere developed to tackle ReLU-NMD. Then we introduce two new algorithms: (1)aggressive accelerated NMD (A-NMD) which uses an adaptive Nesterovextrapolation to accelerate an existing algorithm, and (2) three-block NMD(3B-NMD) which parametrizes $\Theta = WH$ and leads to a significant reductionin the computational cost. We also propose an effective initialization strategybased on the nuclear norm as a proxy for the rank function. We illustrate theeffectiveness of the proposed algorithms (available on gitlab) on synthetic andreal-world data sets.</description><author>Giovanni Seraghiti, Atharva Awari, Arnaud Vandaele, Margherita Porcelli, Nicolas Gillis</author><pubDate>Mon, 15 May 2023 15:43:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08687v1</guid></item><item><title>CLIP-VG: Self-paced Curriculum Adapting of CLIP via Exploiting Pseudo-Language Labels for Visual Grounding</title><link>http://arxiv.org/abs/2305.08685v1</link><description>Visual Grounding (VG) refers to locating a region described by expressions ina specific image, which is a critical topic in vision-language fields. Toalleviate the dependence on labeled data, existing unsupervised methods try tolocate regions using task-unrelated pseudo-labels. However, a large proportionof pseudo-labels are noisy and diversity scarcity in language taxonomy.Inspired by the advances in V-L pretraining, we consider utilizing the VLPmodels to realize unsupervised transfer learning in downstream grounding task.Thus, we propose CLIP-VG, a novel method that can conduct self-paced curriculumadapting of CLIP via exploiting pseudo-language labels to solve VG problem. Byelaborating an efficient model structure, we first propose a single-source andmulti-source curriculum adapting method for unsupervised VG to progressivelysample more reliable cross-modal pseudo-labels to obtain the optimal model,thus achieving implicit knowledge exploiting and denoising. Our methodoutperforms the existing state-of-the-art unsupervised VG method Pseudo-Q inboth single-source and multi-source scenarios with a large margin, i.e.,6.78%~10.67% and 11.39%~24.87% on RefCOCO/+/g datasets, even outperformsexisting weakly supervised methods. The code and models will be released at\url{https://github.com/linhuixiao/CLIP-VG}.</description><author>Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan, Yaowei Wang, Changsheng Xu</author><pubDate>Mon, 15 May 2023 15:42:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08685v1</guid></item><item><title>Sequential Recommendation with Probabilistic Logical Reasoning</title><link>http://arxiv.org/abs/2304.11383v2</link><description>Deep learning and symbolic learning are two frequently employed methods inSequential Recommendation (SR). Recent neural-symbolic SR models demonstratetheir potential to enable SR to be equipped with concurrent perception andcognition capacities. However, neural-symbolic SR remains a challenging problemdue to open issues like representing users and items in logical reasoning. Inthis paper, we combine the Deep Neural Network (DNN) SR models with logicalreasoning and propose a general framework named Sequential Recommendation withProbabilistic Logical Reasoning (short for SR-PLR). This framework allowsSR-PLR to benefit from both similarity matching and logical reasoning bydisentangling feature embedding and logic embedding in the DNN andprobabilistic logic network. To better capture the uncertainty and evolution ofuser tastes, SR-PLR embeds users and items with a probabilistic method andconducts probabilistic logical reasoning on users' interaction patterns. Thenthe feature and logic representations learned from the DNN and logic networkare concatenated to make the prediction. Finally, experiments on varioussequential recommendation models demonstrate the effectiveness of the SR-PLR.</description><author>Huanhuan Yuan, Pengpeng Zhao, Xuefeng Xian, Guanfeng Liu, Victor S. Sheng, Lei Zhao</author><pubDate>Mon, 15 May 2023 15:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11383v2</guid></item><item><title>Natural Language Decomposition and Interpretation of Complex Utterances</title><link>http://arxiv.org/abs/2305.08677v1</link><description>Natural language interfaces often require supervised data to translate userrequests into programs, database queries, or other structured intentrepresentations. During data collection, it can be difficult to anticipate andformalize the full range of user needs -- for example, in a system designed tohandle simple requests (like $\textit{find my meetings tomorrow}$ or$\textit{move my meeting with my manager to noon})$, users may also expressmore elaborate requests (like $\textit{swap all my calls on Monday andTuesday}$). We introduce an approach for equipping a simple language-to-codemodel to handle complex utterances via a process of hierarchical naturallanguage decomposition. Our approach uses a pre-trained language model todecompose a complex utterance into a sequence of smaller natural languagesteps, then interprets each step using the language-to-code model. To test ourapproach, we collect and release DeCU -- a new NL-to-program benchmark toevaluate Decomposition of Complex Utterances. Experiments show that theproposed approach enables the interpretation of complex utterances with almostno complex training data, while outperforming standard few-shot promptingapproaches.</description><author>Harsh Jhamtani, Hao Fang, Patrick Xia, Eran Levy, Jacob Andreas, Ben Van Durme</author><pubDate>Mon, 15 May 2023 15:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08677v1</guid></item><item><title>An Ensemble Approach for Automated Theorem Proving Based on Efficient Name Invariant Graph Neural Representations</title><link>http://arxiv.org/abs/2305.08676v1</link><description>Using reinforcement learning for automated theorem proving has recentlyreceived much attention. Current approaches use representations of logicalstatements that often rely on the names used in these statements and, as aresult, the models are generally not transferable from one domain to another.The size of these representations and whether to include the whole theory orpart of it are other important decisions that affect the performance of theseapproaches as well as their runtime efficiency. In this paper, we presentNIAGRA; an ensemble Name InvAriant Graph RepresentAtion. NIAGRA addresses thisproblem by using 1) improved Graph Neural Networks for learning name-invariantformula representations that is tailored for their unique characteristics and2) an efficient ensemble approach for automated theorem proving. Ourexperimental evaluation shows state-of-the-art performance on multiple datasetsfrom different domains with improvements up to 10% compared to the bestlearning-based approaches. Furthermore, transfer learning experiments show thatour approach significantly outperforms other learning-based approaches by up to28%.</description><author>Achille Fokoue, Ibrahim Abdelaziz, Maxwell Crouse, Shajith Ikbal, Akihiro Kishimoto, Guilherme Lima, Ndivhuwo Makondo, Radu Marinescu</author><pubDate>Mon, 15 May 2023 15:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08676v1</guid></item><item><title>Uncertainty Estimation in Deep Speech Enhancement Using Complex Gaussian Mixture Models</title><link>http://arxiv.org/abs/2212.04831v2</link><description>Single-channel deep speech enhancement approaches often estimate a singlemultiplicative mask to extract clean speech without a measure of its accuracy.Instead, in this work, we propose to quantify the uncertainty associated withclean speech estimates in neural network-based speech enhancement. Predictiveuncertainty is typically categorized into aleatoric uncertainty and epistemicuncertainty. The former accounts for the inherent uncertainty in data and thelatter corresponds to the model uncertainty. Aiming for robust clean speechestimation and efficient predictive uncertainty quantification, we propose tointegrate statistical complex Gaussian mixture models (CGMMs) into a deepspeech enhancement framework. More specifically, we model the dependencybetween input and output stochastically by means of a conditional probabilitydensity and train a neural network to map the noisy input to the full posteriordistribution of clean speech, modeled as a mixture of multiple complex Gaussiancomponents. Experimental results on different datasets show that the proposedalgorithm effectively captures predictive uncertainty and that combiningpowerful statistical models and deep learning also delivers a superior speechenhancement performance.</description><author>Huajian Fang, Timo Gerkmann</author><pubDate>Mon, 15 May 2023 15:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04831v2</guid></item><item><title>Improved baselines for vision-language pre-training</title><link>http://arxiv.org/abs/2305.08675v1</link><description>Contrastive learning has emerged as an efficient framework to learnmultimodal representations. CLIP, a seminal work in this area, achievedimpressive results by training on paired image-text data using the contrastiveloss. Recent work claims improvements over CLIP using additionalnon-contrastive losses inspired from self-supervised learning. However, it issometimes hard to disentangle the contribution of these additional losses fromother implementation details, e.g., data augmentation or regularizationtechniques, used to train the model. To shed light on this matter, in thispaper, we first propose, implement and evaluate several baselines obtained bycombining contrastive learning with recent advances in self-supervisedlearning. In particular, we use the loss functions that were proven successfulfor visual self-supervised learning to align image and text modalities. We findthat these baselines outperform a basic implementation of CLIP. However, when astronger training recipe is employed, the advantage disappears. Indeed, we findthat a simple CLIP baseline can also be improved substantially, up to a 25%relative improvement on downstream zero-shot tasks, by using well-knowntraining techniques that are popular in other subfields. Moreover, we discoverthat it is enough to apply image and text augmentations to make up for most ofthe improvement attained by prior works. With our improved training recipe forCLIP, we obtain state-of-the-art performance on four standard datasets, andconsistently outperform prior work (up to +4% on the largest dataset), whilebeing substantially simpler.</description><author>Enrico Fini, Pietro Astolfi, Adriana Romero-Soriano, Jakob Verbeek, Michal Drozdzal</author><pubDate>Mon, 15 May 2023 15:31:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08675v1</guid></item><item><title>Discovery of sparse hysteresis models for piezoelectric materials</title><link>http://arxiv.org/abs/2302.05313v5</link><description>This article presents an approach for modelling hysteresis in piezoelectricmaterials, that leverages recent advancements in machine learning, particularlyin sparse-regression techniques. While sparse regression has previously beenused to model various scientific and engineering phenomena, its application tononlinear hysteresis modelling in piezoelectric materials has yet to beexplored. The study employs the least-squares algorithm with a sequentialthreshold to model the dynamic system responsible for hysteresis, resulting ina concise model that accurately predicts hysteresis for both simulated andexperimental piezoelectric material data. Several numerical experiments areperformed, including learning butterfly-shaped hysteresis and modellingreal-world hysteresis data for a piezoelectric actuator. The presented approachis compared to traditional regression-based and neural network methods,demonstrating its efficiency and robustness. Source code is available athttps://github.com/chandratue/SmartHysteresis</description><author>Abhishek Chandra, Bram Daniels, Mitrofan Curti, Koen Tiels, Elena A. Lomonova, Daniel M. Tartakovsky</author><pubDate>Mon, 15 May 2023 15:29:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05313v5</guid></item><item><title>aUToLights: A Robust Multi-Camera Traffic Light Detection and Tracking System</title><link>http://arxiv.org/abs/2305.08673v1</link><description>Following four successful years in the SAE AutoDrive Challenge Series I, theUniversity of Toronto is participating in the Series II competition to developa Level 4 autonomous passenger vehicle capable of handling various urbandriving scenarios by 2025. Accurate detection of traffic lights and correctidentification of their states is essential for safe autonomous operation incities. Herein, we describe our recently-redesigned traffic light perceptionsystem for autonomous vehicles like the University of Toronto's self-drivingcar, Artemis. Similar to most traffic light perception systems, we relyprimarily on camera-based object detectors. We deploy the YOLOv5 detector forbounding box regression and traffic light classification across multiplecameras and fuse the observations. To improve robustness, we incorporate priorsfrom high-definition semantic maps and perform state filtering using hiddenMarkov models. We demonstrate a multi-camera, real time-capable traffic lightperception pipeline that handles complex situations including multiple visibleintersections, traffic light variations, temporary occlusion, and flashinglight states. To validate our system, we collected and annotated a varieddataset incorporating flashing states and a range of occlusion types. Ourresults show superior performance in challenging real-world scenarios comparedto single-frame, single-camera object detection.</description><author>Sean Wu, Nicole Amenta, Jiachen Zhou, Sandro Papais, Jonathan Kelly</author><pubDate>Mon, 15 May 2023 15:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08673v1</guid></item><item><title>Stochastic Texture Filtering</title><link>http://arxiv.org/abs/2305.05810v2</link><description>2D texture maps and 3D voxel arrays are widely used to add rich detail to thesurfaces and volumes of rendered scenes, and filtered texture lookups areintegral to producing high-quality imagery. We show that filtering texturesafter evaluating lighting, rather than before BSDF evaluation as is currentpractice, gives a more accurate solution to the rendering equation. Thesebenefits are not merely theoretical, but are apparent in common cases. Wefurther show that stochastically sampling texture filters is crucial forenabling this approach, which has not been possible previously except inlimited cases. Stochastic texture filtering offers additional benefits,including efficient implementation of high-quality texture filters andefficient filtering of textures stored in compressed and sparse datastructures, including neural representations. We demonstrate applications inboth real-time and offline rendering and show that the additional stochasticerror is minimal. Furthermore, this error is handled well by eitherspatiotemporal denoising or moderate pixel sampling rates.</description><author>Marcos Fajardo, Bartlomiej Wronski, Marco Salvi, Matt Pharr</author><pubDate>Mon, 15 May 2023 15:17:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05810v2</guid></item><item><title>MADDM: Multi-Advisor Dynamic Binary Decision-Making by Maximizing the Utility</title><link>http://arxiv.org/abs/2305.08664v1</link><description>Being able to infer ground truth from the responses of multiple imperfectadvisors is a problem of crucial importance in many decision-makingapplications, such as lending, trading, investment, and crowd-sourcing. Inpractice, however, gathering answers from a set of advisors has a cost.Therefore, finding an advisor selection strategy that retrieves a reliableanswer and maximizes the overall utility is a challenging problem. To addressthis problem, we propose a novel strategy for optimally selecting a set ofadvisers in a sequential binary decision-making setting, where multipledecisions need to be made over time. Crucially, we assume no access to groundtruth and no prior knowledge about the reliability of advisers. Specifically,our approach considers how to simultaneously (1) select advisors by balancingthe advisors' costs and the value of making correct decisions, (2) learn thetrustworthiness of advisers dynamically without prior information by askingmultiple advisers, and (3) make optimal decisions without access to the groundtruth, improving this over time. We evaluate our algorithm through severalnumerical experiments. The results show that our approach outperforms two othermethods that combine state-of-the-art models.</description><author>Zhaori Guo, Timothy J. Norman, Enrico H. Gerding</author><pubDate>Mon, 15 May 2023 15:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08664v1</guid></item><item><title>Global and Local Mixture Consistency Cumulative Learning for Long-tailed Visual Recognitions</title><link>http://arxiv.org/abs/2305.08661v1</link><description>In this paper, our goal is to design a simple learning paradigm for long-tailvisual recognition, which not only improves the robustness of the featureextractor but also alleviates the bias of the classifier towards head classeswhile reducing the training skills and overhead. We propose an efficientone-stage training strategy for long-tailed visual recognition called Globaland Local Mixture Consistency cumulative learning (GLMC). Our core ideas aretwofold: (1) a global and local mixture consistency loss improves therobustness of the feature extractor. Specifically, we generate two augmentedbatches by the global MixUp and local CutMix from the same batch data,respectively, and then use cosine similarity to minimize the difference. (2) Acumulative head tail soft label reweighted loss mitigates the head class biasproblem. We use empirical class frequencies to reweight the mixed label of thehead-tail class for long-tailed data and then balance the conventional loss andthe rebalanced loss with a coefficient accumulated by epochs. Our approachachieves state-of-the-art accuracy on CIFAR10-LT, CIFAR100-LT, and ImageNet-LTdatasets. Additional experiments on balanced ImageNet and CIFAR demonstratethat GLMC can significantly improve the generalization of backbones. Code ismade publicly available at https://github.com/ynu-yangpeng/GLMC.</description><author>Fei Du, Peng Yang, Qi Jia, Fengtao Nan, Xiaoting Chen, Yun Yang</author><pubDate>Mon, 15 May 2023 15:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08661v1</guid></item><item><title>Towards Automated COVID-19 Presence and Severity Classification</title><link>http://arxiv.org/abs/2305.08660v1</link><description>COVID-19 presence classification and severity prediction via (3D) thoraxcomputed tomography scans have become important tasks in recent times.Especially for capacity planning of intensive care units, predicting the futureseverity of a COVID-19 patient is crucial. The presented approach followsstate-of-theart techniques to aid medical professionals in these situations. Itcomprises an ensemble learning strategy via 5-fold cross-validation thatincludes transfer learning and combines pre-trained 3D-versions of ResNet34 andDenseNet121 for COVID19 classification and severity prediction respectively.Further, domain-specific preprocessing was applied to optimize modelperformance. In addition, medical information like the infection-lung-ratio,patient age, and sex were included. The presented model achieves an AUC of79.0% to predict COVID-19 severity, and 83.7% AUC to classify the presence ofan infection, which is comparable with other currently popular methods. Thisapproach is implemented using the AUCMEDI framework and relies on well-knownnetwork architectures to ensure robustness and reproducibility.</description><author>Dominik Müller, Niklas Schröter, Silvan Mertes, Fabio Hellmann, Miriam Elia, Wolfgang Reif, Bernhard Bauer, Elisabeth André, Frank Kramer</author><pubDate>Mon, 15 May 2023 15:07:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08660v1</guid></item><item><title>LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models</title><link>http://arxiv.org/abs/2305.03445v2</link><description>Figurative language is a challenge for language models since itsinterpretation is based on the use of words in a way that deviates from theirconventional order and meaning. Yet, humans can easily understand and interpretmetaphors, similes or idioms as they can be derived from embodied metaphors.Language is a proxy for embodiment and if a metaphor is conventional andlexicalised, it becomes easier for a system without a body to make sense ofembodied concepts. Yet, the intricate relation between embodiment and featuressuch as concreteness or age of acquisition has not been studied in the contextof figurative language interpretation concerning language models. Hence, thepresented study shows how larger language models perform better at interpretingmetaphoric sentences when the action of the metaphorical sentence is moreembodied. The analysis rules out multicollinearity with other features (e.g.word length or concreteness) and provides initial evidence that larger languagemodels conceptualise embodied concepts to a degree that facilitates figurativelanguage understanding.</description><author>Philipp Wicke</author><pubDate>Mon, 15 May 2023 15:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03445v2</guid></item><item><title>Segment anything, from space?</title><link>http://arxiv.org/abs/2304.13000v2</link><description>Recently, the first foundation model developed specifically for vision taskswas developed, termed the "Segment Anything Model" (SAM). SAM can segmentobjects in input imagery based upon cheap input prompts, such as one (or more)points, a bounding box, or a mask. The authors examined the zero-shot imagesegmentation accuracy of SAM on a large number of vision benchmark tasks andfound that SAM usually achieved recognition accuracy similar to, or sometimesexceeding, vision models that had been trained on the target tasks. Theimpressive generalization of SAM for segmentation has major implications forvision researchers working on natural imagery. In this work, we examine whetherSAM's impressive performance extends to overhead imagery problems, and helpguide the community's response to its development. We examine SAM's performanceon a set of diverse and widely-studied benchmark tasks. We find that SAM doesoften generalize well to overhead imagery, although it fails in some cases dueto the unique characteristics of overhead imagery and the target objects. Wereport on these unique systematic failure cases for remote sensing imagery thatmay comprise useful future research for the community. Note that this is aworking paper, and it will be updated as additional analysis and results arecompleted.</description><author>Simiao Ren, Francesco Luzi, Saad Lahrichi, Kaleb Kassaw, Leslie M. Collins, Kyle Bradbury, Jordan M. Malof</author><pubDate>Mon, 15 May 2023 15:05:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13000v2</guid></item><item><title>On the connections between optimization algorithms, Lyapunov functions, and differential equations: theory and insights</title><link>http://arxiv.org/abs/2305.08658v1</link><description>We study connections between differential equations and optimizationalgorithms for $m$-strongly and $L$-smooth convex functions through the use ofLyapunov functions by generalizing the Linear Matrix Inequality frameworkdeveloped by Fazylab et al. in 2018. Using the new framework we deriveanalytically a new (discrete) Lyapunov function for a two-parameter family ofNesterov optimization methods and characterize their convergence rate. Thisallows us to prove a convergence rate that improves substantially on thepreviously proven rate of Nesterov's method for the standard choice ofcoefficients, as well as to characterize the choice of coefficients that yieldsthe optimal rate. We obtain a new Lyapunov function for the Polyak ODE andrevisit the connection between this ODE and the Nesterov's algorithms. Inaddition discuss a new interpretation of Nesterov method as an additiveRunge-Kutta discretization and explain the structural conditions thatdiscretizations of the Polyak equation should satisfy in order to lead toaccelerated optimization algorithms.</description><author>Paul Dobson, Jesus Maria Sanz-Serna, Konstantinos Zygalakis</author><pubDate>Mon, 15 May 2023 15:03:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08658v1</guid></item><item><title>Encoding Domain Expertise into Multilevel Models for Source Location</title><link>http://arxiv.org/abs/2305.08657v1</link><description>Data from populations of systems are prevalent in many industrialapplications. Machines and infrastructure are increasingly instrumented withsensing systems, emitting streams of telemetry data with complexinterdependencies. In practice, data-centric monitoring procedures tend toconsider these assets (and respective models) as distinct -- operating inisolation and associated with independent data. In contrast, this work capturesthe statistical correlations and interdependencies between models of a group ofsystems. Utilising a Bayesian multilevel approach, the value of data can beextended, since the population can be considered as a whole, rather thanconstituent parts. Most interestingly, domain expertise and knowledge of theunderlying physics can be encoded in the model at the system, subgroup, orpopulation level. We present an example of acoustic emission (time-of-arrival)mapping for source location, to illustrate how multilevel models naturally lendthemselves to representing aggregate systems in engineering. In particular, wefocus on constraining the combined models with domain knowledge to enhancetransfer learning and enable further insights at the population level.</description><author>Lawrence A. Bull, Matthew R. Jones, Elizabeth J. Cross, Andrew Duncan, Mark Girolami</author><pubDate>Mon, 15 May 2023 15:02:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08657v1</guid></item><item><title>Unsupervised Sentence Representation Learning with Frequency-induced Adversarial Tuning and Incomplete Sentence Filtering</title><link>http://arxiv.org/abs/2305.08655v1</link><description>Pre-trained Language Model (PLM) is nowadays the mainstay of UnsupervisedSentence Representation Learning (USRL). However, PLMs are sensitive to thefrequency information of words from their pre-training corpora, resulting inanisotropic embedding space, where the embeddings of high-frequency words areclustered but those of low-frequency words disperse sparsely. This anisotropicphenomenon results in two problems of similarity bias and information bias,lowering the quality of sentence embeddings. To solve the problems, wefine-tune PLMs by leveraging the frequency information of words and propose anovel USRL framework, namely Sentence Representation Learning withFrequency-induced Adversarial tuning and Incomplete sentence filtering(SLT-FAI). We calculate the word frequencies over the pre-training corpora ofPLMs and assign words thresholding frequency labels. With them, (1) weincorporate a similarity discriminator used to distinguish the embeddings ofhigh-frequency and low-frequency words, and adversarially tune the PLM with it,enabling to achieve uniformly frequency-invariant embedding space; and (2) wepropose a novel incomplete sentence detection task, where we incorporate aninformation discriminator to distinguish the embeddings of original sentencesand incomplete sentences by randomly masking several low-frequency words,enabling to emphasize the more informative low-frequency words. Our SLT-FAI isa flexible and plug-and-play framework, and it can be integrated with existingUSRL techniques. We evaluate SLT-FAI with various backbones on benchmarkdatasets. Empirical results indicate that SLT-FAI can be superior to theexisting USRL baselines. Our code is released in\url{https://github.com/wangbing1416/SLT-FAI}.</description><author>Bing Wang, Ximing Li, Zhiyao Yang, Yuanyuan Guan, Jiayin Li, Shengsheng Wang</author><pubDate>Mon, 15 May 2023 14:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08655v1</guid></item><item><title>Unsupervised Semantic Variation Prediction using the Distribution of Sibling Embeddings</title><link>http://arxiv.org/abs/2305.08654v1</link><description>Languages are dynamic entities, where the meanings associated with wordsconstantly change with time. Detecting the semantic variation of words is animportant task for various NLP applications that must make time-sensitivepredictions. Existing work on semantic variation prediction have predominantlyfocused on comparing some form of an averaged contextualised representation ofa target word computed from a given corpus. However, some of the previouslyassociated meanings of a target word can become obsolete over time (e.g.meaning of gay as happy), while novel usages of existing words are observed(e.g. meaning of cell as a mobile phone). We argue that mean representationsalone cannot accurately capture such semantic variations and propose a methodthat uses the entire cohort of the contextualised embeddings of the targetword, which we refer to as the sibling distribution. Experimental results onSemEval-2020 Task 1 benchmark dataset for semantic variation prediction showthat our method outperforms prior work that consider only the mean embeddings,and is comparable to the current state-of-the-art. Moreover, a qualitativeanalysis shows that our method detects important semantic changes in words thatare not captured by the existing methods. Source code is available athttps://github.com/a1da4/svp-gauss .</description><author>Taichi Aida, Danushka Bollegala</author><pubDate>Mon, 15 May 2023 14:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08654v1</guid></item><item><title>Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation</title><link>http://arxiv.org/abs/2208.10734v2</link><description>Dynamic contextualised word embeddings (DCWEs) represent the temporalsemantic variations of words. We propose a method for learning DCWEs bytime-adapting a pretrained Masked Language Model (MLM) using time-sensitivetemplates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectivelyat two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervisedmethod to select (a) \emph{pivot} terms related to both $C_1$ and $C_2$, and(b) \emph{anchor} terms that are associated with a specific pivot term in eachindividual snapshot. We then generate prompts by filling manually compiledtemplates using the extracted pivot and anchor terms. Moreover, we propose anautomatic method to learn time-sensitive templates from $C_1$ and $C_2$,without requiring any human supervision. Next, we use the generated prompts toadapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multipleexperiments show that our proposed method reduces the perplexity of testsentences in $C_2$, outperforming the current state-of-the-art.</description><author>Xiaohang Tang, Yi Zhou, Danushka Bollegala</author><pubDate>Mon, 15 May 2023 14:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10734v2</guid></item><item><title>Automated Audio Captioning and Language-Based Audio Retrieval</title><link>http://arxiv.org/abs/2207.04156v2</link><description>This project involved participation in the DCASE 2022 Competition (Task 6)which had two subtasks: (1) Automated Audio Captioning and (2) Language-BasedAudio Retrieval. The first subtask involved the generation of a textualdescription for audio samples, while the goal of the second was to find audiosamples within a fixed dataset that match a given description. For bothsubtasks, the Clotho dataset was used. The models were evaluated on BLEU1,BLEU2, BLEU3, ROUGEL, METEOR, CIDEr, SPICE, and SPIDEr scores for audiocaptioning and R1, R5, R10 and mARP10 scores for audio retrieval. We haveconducted a handful of experiments that modify the baseline models for thesetasks. Our final architecture for Automated Audio Captioning is close to thebaseline performance, while our model for Language-Based Audio Retrieval hassurpassed its counterpart.</description><author>Clive Gomes, Hyejin Park, Patrick Kollman, Yi Song, Iffanice Houndayi, Ankit Shah</author><pubDate>Mon, 15 May 2023 14:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.04156v2</guid></item></channel></rss>