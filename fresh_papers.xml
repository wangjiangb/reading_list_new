<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 18 Oct 2023 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending</title><link>http://arxiv.org/abs/2310.10651v1</link><description>Hair editing has made tremendous progress in recent years. Early hair editingmethods use well-drawn sketches or masks to specify the editing conditions.Even though they can enable very fine-grained local control, such interactionmodes are inefficient for the editing conditions that can be easily specifiedby language descriptions or reference images. Thanks to the recent breakthroughof cross-modal models (e.g., CLIP), HairCLIP is the first work that enableshair editing based on text descriptions or reference images. However, suchtext-driven and reference-driven interaction modes make HairCLIP unable tosupport fine-grained controls specified by sketch or mask. In this paper, wepropose HairCLIPv2, aiming to support all the aforementioned interactions withone unified framework. Simultaneously, it improves upon HairCLIP with betterirrelevant attributes (e.g., identity, background) preservation and unseen textdescriptions support. The key idea is to convert all the hair editing tasksinto hair transfer tasks, with editing conditions converted into differentproxies accordingly. The editing effects are added upon the input image byblending the corresponding proxy features within the hairstyle or hair colorfeature spaces. Besides the unprecedented user interaction mode support,quantitative and qualitative experiments demonstrate the superiority ofHairCLIPv2 in terms of editing effects, irrelevant attribute preservation andvisual naturalness. Our code is available at\url{https://github.com/wty-ustc/HairCLIPv2}.</description><author>Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Gang Hua, Nenghai Yu</author><pubDate>Mon, 16 Oct 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10651v1</guid></item><item><title>TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through Neural Radiance Fields</title><link>http://arxiv.org/abs/2310.10650v1</link><description>Implicit representations like Neural Radiance Fields (NeRF) showed impressiveresults for photorealistic rendering of complex scenes with fine details.However, ideal or near-perfectly specular reflecting objects such as mirrors,which are often encountered in various indoor scenes, impose ambiguities andinconsistencies in the representation of the reconstructed scene leading tosevere artifacts in the synthesized renderings. In this paper, we present anovel reflection tracing method tailored for the involved volume renderingwithin NeRF that takes these mirror-like objects into account while avoidingthe cost of straightforward but expensive extensions through standard pathtracing. By explicitly modeling the reflection behavior using physicallyplausible materials and estimating the reflected radiance with Monte-Carlomethods within the volume rendering formulation, we derive efficient strategiesfor importance sampling and the transmittance computation along rays from onlyfew samples. We show that our novel method enables the training of consistentrepresentations of such challenging scenes and achieves superior results incomparison to previous state-of-the-art approaches.</description><author>Leif Van Holland, Ruben Bliersbach, Jan U. Müller, Patrick Stotko, Reinhard Klein</author><pubDate>Mon, 16 Oct 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10650v1</guid></item><item><title>A Computational Framework for Solving Wasserstein Lagrangian Flows</title><link>http://arxiv.org/abs/2310.10649v1</link><description>The dynamical formulation of the optimal transport can be extended throughvarious choices of the underlying geometry ($\textit{kinetic energy}$), and theregularization of density paths ($\textit{potential energy}$). Thesecombinations yield different variational problems ($\textit{Lagrangians}$),encompassing many variations of the optimal transport problem such as theSchr\"odinger bridge, unbalanced optimal transport, and optimal transport withphysical constraints, among others. In general, the optimal density path isunknown, and solving these variational problems can be computationallychallenging. Leveraging the dual formulation of the Lagrangians, we propose anovel deep learning based framework approaching all of these problems from aunified perspective. Our method does not require simulating or backpropagatingthrough the trajectories of the learned dynamics, and does not need access tooptimal couplings. We showcase the versatility of the proposed framework byoutperforming previous approaches for the single-cell trajectory inference,where incorporating prior knowledge into the dynamics is crucial for correctpredictions.</description><author>Kirill Neklyudov, Rob Brekelmans, Alexander Tong, Lazar Atanackovic, Qiang Liu, Alireza Makhzani</author><pubDate>Mon, 16 Oct 2023 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10649v1</guid></item><item><title>Step-by-Step Remediation of Students' Mathematical Mistakes</title><link>http://arxiv.org/abs/2310.10648v1</link><description>Scaling high-quality tutoring is a major challenge in education. Because ofthe growing demand, many platforms employ novice tutors who, unlikeprofessional educators, struggle to effectively address student mistakes andthus fail to seize prime learning opportunities for students. In this paper, weexplore the potential for large language models (LLMs) to assist math tutors inremediating student mistakes. We present ReMath, a benchmark co-developed withexperienced math teachers that deconstructs their thought process forremediation. The benchmark consists of three step-by-step tasks: (1) infer thetype of student error, (2) determine the strategy to address the error, and (3)generate a response that incorporates that information. We evaluate theperformance of state-of-the-art instruct-tuned and dialog models on ReMath. Ourfindings suggest that although models consistently improve upon original tutorresponses, we cannot rely on models alone to remediate mistakes. Providingmodels with the error type (e.g., the student is guessing) and strategy (e.g.,simplify the problem) leads to a 75% improvement in the response quality overmodels without that information. Nonetheless, despite the improvement, thequality of the best model's responses still falls short of experienced mathteachers. Our work sheds light on the potential and limitations of usingcurrent LLMs to provide high-quality learning experiences for both tutors andstudents at scale. Our work is open-sourced at this link:\url{https://github.com/rosewang2008/remath}.</description><author>Rose E. Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, Dorottya Demszky</author><pubDate>Mon, 16 Oct 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10648v1</guid></item><item><title>A Survey on Video Diffusion Models</title><link>http://arxiv.org/abs/2310.10647v1</link><description>The recent wave of AI-generated content (AIGC) has witnessed substantialsuccess in computer vision, with the diffusion model playing a crucial role inthis achievement. Due to their impressive generative capabilities, diffusionmodels are gradually superseding methods based on GANs and auto-regressiveTransformers, demonstrating exceptional performance not only in imagegeneration and editing, but also in the realm of video-related research.However, existing surveys mainly focus on diffusion models in the context ofimage generation, with few up-to-date reviews on their application in the videodomain. To address this gap, this paper presents a comprehensive review ofvideo diffusion models in the AIGC era. Specifically, we begin with a conciseintroduction to the fundamentals and evolution of diffusion models.Subsequently, we present an overview of research on diffusion models in thevideo domain, categorizing the work into three key areas: video generation,video editing, and other video understanding tasks. We conduct a thoroughreview of the literature in these three key areas, including furthercategorization and practical contributions in the field. Finally, we discussthe challenges faced by research in this domain and outline potential futuredevelopmental trends. A comprehensive list of video diffusion models studied inthis survey is available athttps://github.com/ChenHsing/Awesome-Video-Diffusion-Models.</description><author>Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Mon, 16 Oct 2023 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10647v1</guid></item><item><title>Interactive Task Planning with Language Models</title><link>http://arxiv.org/abs/2310.10645v1</link><description>An interactive robot framework accomplishes long-horizon task planning andcan easily generalize to new goals or distinct tasks, even during execution.However, most traditional methods require predefined module design, which makesit hard to generalize to different goals. Recent large language model basedapproaches can allow for more open-ended planning but often require heavyprompt engineering or domain-specific pretrained models. To tackle this, wepropose a simple framework that achieves interactive task planning withlanguage models. Our system incorporates both high-level planning and low-levelfunction execution via language. We verify the robustness of our system ingenerating novel high-level instructions for unseen objectives and its ease ofadaptation to different tasks by merely substituting the task guidelines,without the need for additional complex prompt engineering. Furthermore, whenthe user sends a new request, our system is able to replan accordingly withprecision based on the new request, task guidelines and previously executedsteps. Please check more details on our https://wuphilipp.github.io/itp_siteand https://youtu.be/TrKLuyv26_g.</description><author>Boyi Li, Philipp Wu, Pieter Abbeel, Jitendra Malik</author><pubDate>Mon, 16 Oct 2023 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10645v1</guid></item><item><title>TOSS:High-quality Text-guided Novel View Synthesis from a Single Image</title><link>http://arxiv.org/abs/2310.10644v1</link><description>In this paper, we present TOSS, which introduces text to the task of novelview synthesis (NVS) from just a single RGB image. While Zero-1-to-3 hasdemonstrated impressive zero-shot open-set NVS capability, it treats NVS as apure image-to-image translation problem. This approach suffers from thechallengingly under-constrained nature of single-view NVS: the process lacksmeans of explicit user control and often results in implausible NVSgenerations. To address this limitation, TOSS uses text as high-level semanticinformation to constrain the NVS solution space. TOSS fine-tunes text-to-imageStable Diffusion pre-trained on large-scale text-image pairs and introducesmodules specifically tailored to image and camera pose conditioning, as well asdedicated training for pose correctness and preservation of fine details.Comprehensive experiments are conducted with results showing that our proposedTOSS outperforms Zero-1-to-3 with more plausible, controllable andmultiview-consistent NVS results. We further support these results withcomprehensive ablations that underscore the effectiveness and potential of theintroduced semantic guidance and architecture design.</description><author>Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao Qi, Tianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang, Heung-Yeung Shum</author><pubDate>Mon, 16 Oct 2023 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10644v1</guid></item><item><title>Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting</title><link>http://arxiv.org/abs/2310.10642v1</link><description>Reconstructing dynamic 3D scenes from 2D images and generating diverse viewsover time is challenging due to scene complexity and temporal dynamics. Despiteadvancements in neural implicit models, limitations persist: (i) InadequateScene Structure: Existing methods struggle to reveal the spatial and temporalstructure of dynamic scenes from directly learning the complex 6D plenopticfunction. (ii) Scaling Deformation Modeling: Explicitly modeling scene elementdeformation becomes impractical for complex dynamics. To address these issues,we consider the spacetime as an entirety and propose to approximate theunderlying spatio-temporal 4D volume of a dynamic scene by optimizing acollection of 4D primitives, with explicit geometry and appearance modeling.Learning to optimize the 4D primitives enables us to synthesize novel views atany desired time with our tailored rendering routine. Our model is conceptuallysimple, consisting of a 4D Gaussian parameterized by anisotropic ellipses thatcan rotate arbitrarily in space and time, as well as view-dependent andtime-evolved appearance represented by the coefficient of 4D spherindricalharmonics. This approach offers simplicity, flexibility for variable-lengthvideo and end-to-end training, and efficient real-time rendering, making itsuitable for capturing complex dynamic scene motions. Experiments acrossvarious benchmarks, including monocular and multi-view scenarios, demonstrateour 4DGS model's superior visual quality and efficiency.</description><author>Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, Li Zhang</author><pubDate>Mon, 16 Oct 2023 18:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10642v1</guid></item><item><title>LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts</title><link>http://arxiv.org/abs/2310.10640v1</link><description>Diffusion-based generative models have significantly advanced text-to-imagegeneration but encounter challenges when processing lengthy and intricate textprompts describing complex scenes with multiple objects. While excelling ingenerating images from short, single-object descriptions, these models oftenstruggle to faithfully capture all the nuanced details within longer and moreelaborate textual inputs. In response, we present a novel approach leveragingLarge Language Models (LLMs) to extract critical components from text prompts,including bounding box coordinates for foreground objects, detailed textualdescriptions for individual objects, and a succinct background context. Thesecomponents form the foundation of our layout-to-image generation model, whichoperates in two phases. The initial Global Scene Generation utilizes objectlayouts and background context to create an initial scene but often falls shortin faithfully representing object characteristics as specified in the prompts.To address this limitation, we introduce an Iterative Refinement Scheme thatiteratively evaluates and refines box-level content to align them with theirtextual descriptions, recomposing objects as needed to ensure consistency. Ourevaluation on complex prompts featuring multiple objects demonstrates asubstantial improvement in recall compared to baseline diffusion models. Thisis further validated by a user study, underscoring the efficacy of our approachin generating coherent and detailed scenes from intricate textual inputs.</description><author>Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, Peter Wonka</author><pubDate>Mon, 16 Oct 2023 18:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10640v1</guid></item><item><title>In-Context Pretraining: Language Modeling Beyond Document Boundaries</title><link>http://arxiv.org/abs/2310.10638v1</link><description>Large language models (LMs) are currently trained to predict tokens givendocument prefixes, enabling them to directly perform long-form generation andprompting-style tasks which can be reduced to document completion. Existingpretraining pipelines train LMs by concatenating random sets of short documentsto create input contexts but the prior documents provide no signal forpredicting the next document. We instead present In-Context Pretraining, a newapproach where language models are pretrained on a sequence of relateddocuments, thereby explicitly encouraging them to read and reason acrossdocument boundaries. We can do In-Context Pretraining by simply changing thedocument ordering so that each context contains related documents, and directlyapplying existing pretraining pipelines. However, this document sorting problemis challenging. There are billions of documents and we would like the sort tomaximize contextual similarity for every document without repeating any data.To do this, we introduce approximate algorithms for finding related documentswith efficient nearest neighbor search and constructing coherent input contextswith a graph traversal algorithm. Our experiments show In-Context Pretrainingoffers a simple and scalable approach to significantly enhance LMs'performance:we see notable improvements in tasks that require more complex contextualreasoning, including in-context learning (+8%), reading comprehension (+15%),faithfulness to previous contexts (+16%), long-context reasoning (+5%), andretrieval augmentation (+9%).</description><author>Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, Mike Lewis</author><pubDate>Mon, 16 Oct 2023 18:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10638v1</guid></item><item><title>"Mistakes Help Us Grow": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms</title><link>http://arxiv.org/abs/2310.10637v1</link><description>Teachers' growth mindset supportive language (GMSL)--rhetoric emphasizingthat one's skills can be improved over time--has been shown to significantlyreduce disparities in academic achievement and enhance students' learningoutcomes. Although teachers espouse growth mindset principles, most find itdifficult to adopt GMSL in their practice due the lack of effective coaching inthis area. We explore whether large language models (LLMs) can provideautomated, personalized coaching to support teachers' use of GMSL. We establishan effective coaching tool to reframe unsupportive utterances to GMSL bydeveloping (i) a parallel dataset containing GMSL-trained teacher reframings ofunsupportive statements with an accompanying annotation guide, (ii) a GMSLprompt framework to revise teachers' unsupportive language, and (iii) anevaluation framework grounded in psychological theory for evaluating GMSL withthe help of students and teachers. We conduct a large-scale evaluationinvolving 174 teachers and 1,006 students, finding that both teachers andstudents perceive GMSL-trained teacher and model reframings as more effectivein fostering a growth mindset and promoting challenge-seeking behavior, amongother benefits. We also find that model-generated reframings outperform thosefrom the GMSL-trained teachers. These results show promise for harnessing LLMsto provide automated GMSL feedback for teachers and, more broadly, LLMs'potentiality for supporting students' learning in the classroom. Our findingsalso demonstrate the benefit of large-scale human evaluations when applyingLLMs in educational domains.</description><author>Kunal Handa, Margaret Clapper, Jessica Boyle, Rose E Wang, Diyi Yang, David S Yeager, Dorottya Demszky</author><pubDate>Mon, 16 Oct 2023 18:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10637v1</guid></item><item><title>Efficacy of Dual-Encoders for Extreme Multi-Label Classification</title><link>http://arxiv.org/abs/2310.10636v1</link><description>Dual-encoder models have demonstrated significant success in dense retrievaltasks for open-domain question answering that mostly involves zero-shot andfew-shot scenarios. However, their performance in many-shot retrieval problemswhere training data is abundant, such as extreme multi-label classification(XMC), remains under-explored. Existing empirical evidence suggests that, forsuch problems, the dual-encoder method's accuracies lag behind the performanceof state-of-the-art (SOTA) extreme classification methods that grow the numberof learnable parameters linearly with the number of classes. As a result, somerecent extreme classification techniques use a combination of dual-encoders anda learnable classification head for each class to excel on these tasks. In thispaper, we investigate the potential of "pure" DE models in XMC tasks. Ourfindings reveal that when trained correctly standard dual-encoders can match oroutperform SOTA extreme classification methods by up to 2% at Precision@1 evenon the largest XMC datasets while being 20x smaller in terms of the number oftrainable parameters. We further propose a differentiable topk error-based lossfunction, which can be used to specifically optimize for Recall@k metrics. Weinclude our PyTorch implementation along with other resources for reproducingthe results in the supplementary material.</description><author>Nilesh Gupta, Devvrit Khatri, Ankit S Rawat, Srinadh Bhojanapalli, Prateek Jain, Inderjit S Dhillon</author><pubDate>Mon, 16 Oct 2023 18:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10636v1</guid></item><item><title>Towards Scenario-based Safety Validation for Autonomous Trains with Deep Generative Models</title><link>http://arxiv.org/abs/2310.10635v1</link><description>Modern AI techniques open up ever-increasing possibilities for autonomousvehicles, but how to appropriately verify the reliability of such systemsremains unclear. A common approach is to conduct safety validation based on apredefined Operational Design Domain (ODD) describing specific conditions underwhich a system under test is required to operate properly. However, collectingsufficient realistic test cases to ensure comprehensive ODD coverage ischallenging. In this paper, we report our practical experiences regarding theutility of data simulation with deep generative models for scenario-based ODDvalidation. We consider the specific use case of a camera-based rail-scenesegmentation system designed to support autonomous train operation. Wedemonstrate the capabilities of semantically editing railway scenes with deepgenerative models to make a limited amount of test data more representative. Wealso show how our approach helps to analyze the degree to which a systemcomplies with typical ODD requirements. Specifically, we focus on evaluatingproper operation under different lighting and weather conditions as well aswhile transitioning between them.</description><author>Thomas Decker, Ananta R. Bhattarai, Michael Lebacher</author><pubDate>Mon, 16 Oct 2023 18:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10635v1</guid></item><item><title>OpenAgents: An Open Platform for Language Agents in the Wild</title><link>http://arxiv.org/abs/2310.10634v1</link><description>Language agents show potential in being capable of utilizing natural languagefor varied and intricate tasks in diverse environments, particularly when builtupon large language models (LLMs). Current language agent frameworks aim tofacilitate the construction of proof-of-concept language agents whileneglecting the non-expert user access to agents and paying little attention toapplication-level designs. We present OpenAgents, an open platform for usingand hosting language agents in the wild of everyday life. OpenAgents includesthree agents: (1) Data Agent for data analysis with Python/SQL and data tools;(2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous webbrowsing. OpenAgents enables general users to interact with agentfunctionalities through a web user interface optimized for swift responses andcommon failures while offering developers and researchers a seamless deploymentexperience on local setups, providing a foundation for crafting innovativelanguage agents and facilitating real-world evaluations. We elucidate thechallenges and opportunities, aspiring to set a foundation for future researchand development of real-world language agents.</description><author>Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu</author><pubDate>Mon, 16 Oct 2023 18:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10634v1</guid></item><item><title>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</title><link>http://arxiv.org/abs/2310.10632v1</link><description>The ability to automatically generate accurate protocols for scientificexperiments would represent a major step towards the automation of science.Large Language Models (LLMs) have impressive capabilities on a wide range oftasks, such as question answering and the generation of coherent text and code.However, LLMs can struggle with multi-step problems and long-term planning,which are crucial for designing scientific experiments. Moreover, evaluation ofthe accuracy of scientific protocols is challenging, because experiments can bedescribed correctly in many different ways, require expert knowledge toevaluate, and cannot usually be executed automatically. Here we present anautomatic evaluation framework for the task of planning experimental protocols,and we introduce BioProt: a dataset of biology protocols with correspondingpseudocode representations. To measure performance on generating scientificprotocols, we use an LLM to convert a natural language protocol intopseudocode, and then evaluate an LLM's ability to reconstruct the pseudocodefrom a high-level description and a list of admissible pseudocode functions. Weevaluate GPT-3 and GPT-4 on this task and explore their robustness. Weexternally validate the utility of pseudocode representations of text bygenerating accurate novel protocols using retrieved pseudocode, and we run agenerated protocol successfully in our biological laboratory. Our framework isextensible to the evaluation and improvement of language model planningabilities in other areas of science or other areas that lack automaticevaluation.</description><author>Odhran O'Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Justin Booth, Samuel G Rodriques</author><pubDate>Mon, 16 Oct 2023 18:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10632v1</guid></item><item><title>Llemma: An Open Language Model For Mathematics</title><link>http://arxiv.org/abs/2310.10631v1</link><description>We present Llemma, a large language model for mathematics. We continuepretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, webdata containing mathematics, and mathematical code, yielding Llemma. On theMATH benchmark Llemma outperforms all known open base models, as well as theunreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma iscapable of tool use and formal theorem proving without any further finetuning.We openly release all artifacts, including 7 billion and 34 billion parametermodels, the Proof-Pile-2, and code to replicate our experiments.</description><author>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, Sean Welleck</author><pubDate>Mon, 16 Oct 2023 18:54:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10631v1</guid></item><item><title>Segment Any Building For Remote Sensing</title><link>http://arxiv.org/abs/2310.01164v2</link><description>The task of identifying and segmenting buildings within remote sensingimagery has perennially stood at the forefront of scholarly investigations.This manuscript accentuates the potency of harnessing diversified datasets intandem with cutting-edge representation learning paradigms for buildingsegmentation in such images. Through the strategic amalgamation of disparatedatasets, we have not only expanded the informational horizon accessible formodel training but also manifested unparalleled performance metrics acrossmultiple datasets. Our avant-garde joint training regimen underscores the meritof our approach, bearing significant implications in pivotal domains such asurban infrastructural development, disaster mitigation strategies, andecological surveillance. Our methodology, predicated upon the fusion ofdatasets and gleaning insights from pre-trained models, carves a new benchmarkin the annals of building segmentation endeavors. The outcomes of this researchboth fortify the foundations for ensuing scholarly pursuits and presage ahorizon replete with innovative applications in the discipline of buildingsegmentation.</description><author>Lei Li</author><pubDate>Mon, 16 Oct 2023 18:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01164v2</guid></item><item><title>Certainty In, Certainty Out: REVQCs for Quantum Machine Learning</title><link>http://arxiv.org/abs/2310.10629v1</link><description>The field of Quantum Machine Learning (QML) has emerged recently in the hopesof finding new machine learning protocols or exponential speedups for classicalones. Apart from problems with vanishing gradients and efficient encodingmethods, these speedups are hard to find because the sampling nature of quantumcomputers promotes either simulating computations classically or running themmany times on quantum computers in order to use approximate expectation valuesin gradient calculations. In this paper, we make a case for setting highsingle-sample accuracy as a primary goal. We discuss the statistical theorywhich enables highly accurate and precise sample inference, and propose amethod of reversed training towards this end. We show the effectiveness of thistraining method by assessing several effective variational quantum circuits(VQCs), trained in both the standard and reversed directions, on random binarysubsets of the MNIST and MNIST Fashion datasets, on which our method providesan increase of $10-15\%$ in single-sample inference accuracy.</description><author>Hannah Helgesen, Michael Felsberg, Jan-Åke Larsson</author><pubDate>Mon, 16 Oct 2023 18:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10629v1</guid></item><item><title>Data Contamination Through the Lens of Time</title><link>http://arxiv.org/abs/2310.10628v1</link><description>Recent claims about the impressive abilities of large language models (LLMs)are often supported by evaluating publicly available benchmarks. Since LLMstrain on wide swaths of the internet, this practice raises concerns of datacontamination, i.e., evaluating on examples that are explicitly or implicitlyincluded in the training data. Data contamination remains notoriouslychallenging to measure and mitigate, even with partial attempts like controlledexperimentation of training data, canary strings, or embedding similarities. Inthis work, we conduct the first thorough longitudinal analysis of datacontamination in LLMs by using the natural experiment of training cutoffs inGPT models to look at benchmarks released over time. Specifically, we considertwo code/mathematical problem-solving datasets, Codeforces and Project Euler,and find statistically significant trends among LLM pass rate vs. GitHubpopularity and release date that provide strong evidence of contamination. Byopen-sourcing our dataset, raw results, and evaluation framework, our workpaves the way for rigorous analyses of data contamination in modern models. Weconclude with a discussion of best practices and future steps for publiclyreleasing benchmarks in the age of LLMs that train on webscale data.</description><author>Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, Samuel Dooley</author><pubDate>Mon, 16 Oct 2023 18:51:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10628v1</guid></item><item><title>Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers</title><link>http://arxiv.org/abs/2310.10627v1</link><description>Hallucination plagues even frontier LLMs--but how bad is it really forsummarizing academic papers? We evaluate Factored Verification, a simpleautomated method for detecting hallucinations in abstractive summaries. Thismethod sets a new SotA on hallucination detection in the summarization task ofthe HaluEval benchmark, achieving 76.2% accuracy. We then use this method toestimate how often language models hallucinate when summarizing across multipleacademic papers and find 0.62 hallucinations in the average ChatGPT (16k)summary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correctusing Factored Critiques and find that this lowers the number of hallucinationsto 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinationswe find are often subtle, so we advise caution when using models to synthesizeacademic papers.</description><author>Charlie George, Andreas Stuhlmüller</author><pubDate>Mon, 16 Oct 2023 18:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10627v1</guid></item><item><title>Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors</title><link>http://arxiv.org/abs/2305.12883v2</link><description>In recent years, there has been a significant growth in research focusing onminimum $\ell_2$ norm (ridgeless) interpolation least squares estimators.However, the majority of these analyses have been limited to a simpleregression error structure, assuming independent and identically distributederrors with zero mean and common variance. In this paper, we explore predictionrisk as well as estimation risk under more general regression errorassumptions, highlighting the benefits of overparameterization in a finitesample. We find that including a large number of unimportant parametersrelative to the sample size can effectively reduce both risks. Notably, weestablish that the estimation difficulties associated with the variancecomponents of both risks can be summarized through the trace of thevariance-covariance matrix of the regression errors.</description><author>Sungyoon Lee, Sokbae Lee</author><pubDate>Mon, 16 Oct 2023 18:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12883v2</guid></item><item><title>Video Language Planning</title><link>http://arxiv.org/abs/2310.10625v1</link><description>We are interested in enabling visual planning for complex long-horizon tasksin the space of generated videos and language, leveraging recent advances inlarge generative models pretrained on Internet-scale data. To this end, wepresent video language planning (VLP), an algorithm that consists of a treesearch procedure, where we train (i) vision-language models to serve as bothpolicies and value functions, and (ii) text-to-video models as dynamics models.VLP takes as input a long-horizon task instruction and current imageobservation, and outputs a long video plan that provides detailed multimodal(video and language) specifications that describe how to complete the finaltask. VLP scales with increasing computation budget where more computation timeresults in improved video plans, and is able to synthesize long-horizon videoplans across different robotics domains: from multi-object rearrangement, tomulti-camera bi-arm dexterous manipulation. Generated video plans can betranslated into real robot actions via goal-conditioned policies, conditionedon each intermediate frame of the generated video. Experiments show that VLPsubstantially improves long-horizon task success rates compared to priormethods on both simulated and real robots (across 3 hardware platforms).</description><author>Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Kaelbling, Andy Zeng, Jonathan Tompson</author><pubDate>Mon, 16 Oct 2023 18:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10625v1</guid></item><item><title>Deep Image Clustering with Contrastive Learning and Multi-scale Graph Convolutional Networks</title><link>http://arxiv.org/abs/2207.07173v2</link><description>Deep clustering has shown its promising capability in joint representationlearning and clustering via deep neural networks. Despite the significantprogress, the existing deep clustering works mostly utilize somedistribution-based clustering loss, lacking the ability to unify representationlearning and multi-scale structure learning. To address this, this paperpresents a new deep clustering approach termed image clustering withcontrastive learning and multi-scale graph convolutional networks (IcicleGCN),which bridges the gap between convolutional neural network (CNN) and graphconvolutional network (GCN) as well as the gap between contrastive learning andmulti-scale structure learning for the deep clustering task. Our frameworkconsists of four main modules, namely, the CNN-based backbone, the InstanceSimilarity Module (ISM), the Joint Cluster Structure Learning and Instancereconstruction Module (JC-SLIM), and the Multi-scale GCN module (M-GCN).Specifically, the backbone network with two weight-sharing views is utilized tolearn the representations for the two augmented samples (from each image). Thelearned representations are then fed to ISM and JC-SLIM for jointinstance-level and cluster-level contrastive learning, respectively, duringwhich an auto-encoder in JC-SLIM is also pretrained to serve as a bridge to theM-GCN module. Further, to enforce multi-scale neighborhood structure learning,two streams of GCNs and the auto-encoder are simultaneously trained via (i) thelayer-wise interaction with representation fusion and (ii) the jointself-adaptive learning. Experiments on multiple image datasets demonstrate thesuperior clustering performance of IcicleGCN over the state-of-the-art. Thecode is available at https://github.com/xuyuankun631/IcicleGCN.</description><author>Yuankun Xu, Dong Huang, Chang-Dong Wang, Jian-Huang Lai</author><pubDate>Mon, 16 Oct 2023 18:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.07173v2</guid></item><item><title>DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</title><link>http://arxiv.org/abs/2310.10624v1</link><description>Despite remarkable research advances in diffusion-based video editing,existing methods are limited to short-length videos due to the contradictionbetween long-range consistency and frame-wise editing. Recent approachesattempt to tackle this challenge by introducing video-2D representations todegrade video editing to image editing. However, they encounter significantdifficulties in handling large-scale motion- and view-change videos especiallyfor human-centric videos. This motivates us to introduce the dynamic NeuralRadiance Fields (NeRF) as the human-centric video representation to ease thevideo editing problem to a 3D space editing task. As such, editing can beperformed in the 3D spaces and propagated to the entire video via thedeformation field. To provide finer and direct controllable editing, we proposethe image-based 3D space editing pipeline with a set of effective designs.These include multi-view multi-pose Score Distillation Sampling (SDS) from both2D personalized diffusion priors and 3D diffusion priors, reconstruction losseson the reference image, text-guided local parts super-resolution, and styletransfer for 3D background space. Extensive experiments demonstrate that ourmethod, dubbed as DynVideo-E, significantly outperforms SOTA approaches on twochallenging datasets by a large margin of 50% ~ 95% in terms of humanpreference. Compelling video comparisons are provided in the project pagehttps://showlab.github.io/DynVideo-E/. Our code and data will be released tothe community.</description><author>Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou</author><pubDate>Mon, 16 Oct 2023 18:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10624v1</guid></item><item><title>Generating Summaries with Controllable Readability Levels</title><link>http://arxiv.org/abs/2310.10623v1</link><description>Readability refers to how easily a reader can understand a written text.Several factors affect the readability level, such as the complexity of thetext, its subject matter, and the reader's background knowledge. Generatingsummaries based on different readability levels is critical for enablingknowledge consumption by diverse audiences. However, current text generationapproaches lack refined control, resulting in texts that are not customized toreaders' proficiency levels. In this work, we bridge this gap and studytechniques to generate summaries at specified readability levels. Unlikeprevious methods that focus on a specific readability level (e.g., laysummarization), we generate summaries with fine-grained control over theirreadability. We develop three text generation techniques for controllingreadability: (1) instruction-based readability control, (2) reinforcementlearning to minimize the gap between requested and observed readability and (3)a decoding approach that uses lookahead to estimate the readability of upcomingdecoding steps. We show that our generation methods significantly improvereadability control on news summarization (CNN/DM dataset), as measured byvarious readability metrics and human judgement, establishing strong baselinesfor controllable readability in summarization.</description><author>Leonardo F. R. Ribeiro, Mohit Bansal, Markus Dreyer</author><pubDate>Mon, 16 Oct 2023 18:46:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10623v1</guid></item><item><title>Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models</title><link>http://arxiv.org/abs/2301.04213v2</link><description>Language models learn a great quantity of factual information duringpretraining, and recent work localizes this information to specific modelweights like mid-layer MLP weights. In this paper, we find that we can changehow a fact is stored in a model by editing weights that are in a differentlocation than where existing methods suggest that the fact is stored. This issurprising because we would expect that localizing facts to specific modelparameters would tell us where to manipulate knowledge in models, and thisassumption has motivated past work on model editing methods. Specifically, weshow that localization conclusions from representation denoising (also known asCausal Tracing) do not provide any insight into which model MLP layer would bebest to edit in order to override an existing stored fact with a new one. Thisfinding raises questions about how past work relies on Causal Tracing to selectwhich model layers to edit. Next, we consider several variants of the editingproblem, including erasing and amplifying facts. For one of our editingproblems, editing performance does relate to localization results fromrepresentation denoising, but we find that which layer we edit is a far betterpredictor of performance. Our results suggest, counterintuitively, that bettermechanistic understanding of how pretrained language models work may not alwaystranslate to insights about how to best change their behavior. Our code isavailable at https://github.com/google/belief-localization</description><author>Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun</author><pubDate>Mon, 16 Oct 2023 18:42:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04213v2</guid></item><item><title>How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations</title><link>http://arxiv.org/abs/2310.10616v1</link><description>While large language models based on the transformer architecture havedemonstrated remarkable in-context learning (ICL) capabilities, understandingsof such capabilities are still in an early stage, where existing theory andmechanistic understanding focus mostly on simple scenarios such as learningsimple function classes. This paper takes initial steps on understanding ICL inmore complex scenarios, by studying learning with representations. Concretely,we construct synthetic in-context learning problems with a compositionalstructure, where the label depends on the input through a possibly complex butfixed representation function, composed with a linear function that differs ineach instance. By construction, the optimal ICL algorithm first transforms theinputs by the representation function, and then performs linear ICL on top ofthe transformed dataset. We show theoretically the existence of transformersthat approximately implement such algorithms with mild depth and size.Empirically, we find trained transformers consistently achieve near-optimal ICLperformance in this setting, and exhibit the desired dissection where lowerlayers transforms the dataset and upper layers perform linear ICL. Throughextensive probing and a new pasting experiment, we further reveal severalmechanisms within the trained transformers, such as concrete copying behaviorson both the inputs and the representations, linear ICL capability of the upperlayers alone, and a post-ICL representation selection mechanism in a hardermixture setting. These observed mechanisms align well with our theory and mayshed light on how transformers perform ICL in more realistic scenarios.</description><author>Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, Yu Bai</author><pubDate>Mon, 16 Oct 2023 18:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10616v1</guid></item><item><title>IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation</title><link>http://arxiv.org/abs/2310.10611v1</link><description>Reasoning about a model's accuracy on a test sample from its confidence is acentral problem in machine learning, being connected to important applicationssuch as uncertainty representation, model selection, and exploration. Whilethese connections have been well-studied in the i.i.d. settings, distributionshifts pose significant challenges to the traditional methods. Therefore, modelcalibration and model selection remain challenging in the unsupervised domainadaptation problem--a scenario where the goal is to perform well in adistribution shifted domain without labels. In this work, we tackledifficulties coming from distribution shifts by developing a novel importanceweighted group accuracy estimator. Specifically, we formulate an optimizationproblem for finding an importance weight that leads to an accurate groupaccuracy estimation in the distribution shifted domain with theoreticalanalyses. Extensive experiments show the effectiveness of group accuracyestimation on model calibration and model selection. Our results emphasize thesignificance of group accuracy estimation for addressing challenges inunsupervised domain adaptation, as an orthogonal improvement direction withimproving transferability of accuracy.</description><author>Taejong Joo, Diego Klabjan</author><pubDate>Mon, 16 Oct 2023 18:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10611v1</guid></item><item><title>Quantifying Assistive Robustness Via the Natural-Adversarial Frontier</title><link>http://arxiv.org/abs/2310.10610v1</link><description>Our ultimate goal is to build robust policies for robots that assist people.What makes this hard is that people can behave unexpectedly at test time,potentially interacting with the robot outside its training distribution andleading to failures. Even just measuring robustness is a challenge. Adversarialperturbations are the default, but they can paint the wrong picture: they cancorrespond to human motions that are unlikely to occur during naturalinteractions with people. A robot policy might fail under small adversarialperturbations but work under large natural perturbations. We propose thatcapturing robustness in these interactive settings requires constructing andanalyzing the entire natural-adversarial frontier: the Pareto-frontier of humanpolicies that are the best trade-offs between naturalness and low robotperformance. We introduce RIGID, a method for constructing this frontier bytraining adversarial human policies that trade off between minimizing robotreward and acting human-like (as measured by a discriminator). On an AssistiveGym task, we use RIGID to analyze the performance of standard collaborativeReinforcement Learning, as well as the performance of existing methods meant toincrease robustness. We also compare the frontier RIGID identifies with thefailures identified in expert adversarial interaction, and withnaturally-occurring failures during user interaction. Overall, we find evidencethat RIGID can provide a meaningful measure of robustness predictive ofdeployment performance, and uncover failure cases in human-robot interactionthat are difficult to find manually. https://ood-human.github.io.</description><author>Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Anca D. Dragan</author><pubDate>Mon, 16 Oct 2023 18:34:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10610v1</guid></item><item><title>RLSynC: Offline-Online Reinforcement Learning for Synthon Completion</title><link>http://arxiv.org/abs/2309.02671v2</link><description>Retrosynthesis is the process of determining the set of reactant moleculesthat can react to form a desired product. Semi-template-based retrosynthesismethods, which imitate the reverse logic of synthesis reactions, first predictthe reaction centers in the products, and then complete the resulting synthonsback into reactants. These methods enable necessary interpretability and highpractical utility to inform synthesis planning. We develop a new offline-onlinereinforcement learning method RLSynC for synthon completion insemi-template-based methods. RLSynC assigns one agent to each synthon, all ofwhich complete the synthons by conducting actions step by step in asynchronized fashion. RLSynC learns the policy from both offline trainingepisodes and online interactions which allow RLSynC to explore new reactionspaces. RLSynC uses a forward synthesis model to evaluate the likelihood of thepredicted reactants in synthesizing a product, and thus guides the actionsearch. We compare RLSynC with the state-of-the-art retrosynthesis methods. Ourexperimental results demonstrate that RLSynC can outperform these methods withimprovement as high as 14.9% on synthon completion, and 14.0% onretrosynthesis, highlighting its potential in synthesis planning.</description><author>Frazier N. Baker, Ziqi Chen, Xia Ning</author><pubDate>Mon, 16 Oct 2023 18:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02671v2</guid></item><item><title>Quality control using convolutional neural networks applied to samples of very small size</title><link>http://arxiv.org/abs/2310.10608v1</link><description>Although there is extensive literature on the application of artificialneural networks (NNs) in quality control (QC), to monitor the conformity of aprocess to quality specifications, at least five QC measurements are required,increasing the related cost. To explore the application of neural networks tosamples of QC measurements of very small size, four one-dimensional (1-D)convolutional neural networks (CNNs) were designed, trained, and tested withdatasets of $ n $-tuples of simulated standardized normally distributed QCmeasurements, for $ 1 \leq n \leq 4$. The designed neural networks werecompared to statistical QC functions with equal probabilities for falserejection, applied to samples of the same size. When the $ n $-tuples includedat least two QC measurements distributed as $ \mathcal{N}(\mu, \sigma^2) $,where $ 0.2 &lt; |\mu| \leq 6.0 $, and $ 1.0 &lt; \sigma \leq 7.0 $, the designedneural networks outperformed the respective statistical QC functions.Therefore, 1-D CNNs applied to samples of 2-4 quality control measurements canbe used to increase the probability of detection of the nonconformity of aprocess to the quality specifications, with lower cost.</description><author>Rallou A. Chatzimichail, Aristides T. Hatjimihail</author><pubDate>Mon, 16 Oct 2023 18:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10608v1</guid></item><item><title>BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning</title><link>http://arxiv.org/abs/2310.10606v1</link><description>Domain randomization (DR), which entails training a policy with randomizeddynamics, has proven to be a simple yet effective algorithm for reducing thegap between simulation and the real world. However, DR often requires carefultuning of randomization parameters. Methods like Bayesian Domain Randomization(Bayesian DR) and Active Domain Randomization (Adaptive DR) address this issueby automating parameter range selection using real-world experience. Whileeffective, these algorithms often require long computation time, as a newpolicy is trained from scratch every iteration. In this work, we proposeAdaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune),which inherits the spirit of BayRn but aims to significantly accelerate thelearning processes by fine-tuning from previously learned policy. This idealeads to a critical question: which previous policy should we use as a priorduring fine-tuning? We investigated four different fine-tuning strategies andcompared them against baseline algorithms in five simulated environments,ranging from simple benchmark tasks to more complex legged robot environments.Our analysis demonstrates that our method yields better rewards in the sameamount of timesteps compared to vanilla domain randomization or Bayesian DR.</description><author>Tianle Huang, Nitish Sontakke, K. Niranjan Kumar, Irfan Essa, Stefanos Nikolaidis, Dennis W. Hong, Sehoon Ha</author><pubDate>Mon, 16 Oct 2023 18:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10606v1</guid></item><item><title>ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model</title><link>http://arxiv.org/abs/2310.10605v1</link><description>Through evolution, nature has presented a set of remarkable proteinmaterials, including elastins, silks, keratins and collagens with superiormechanical performances that play crucial roles in mechanobiology. However,going beyond natural designs to discover proteins that meet specifiedmechanical properties remains challenging. Here we report a generative modelthat predicts protein designs to meet complex nonlinear mechanicalproperty-design objectives. Our model leverages deep knowledge on proteinsequences from a pre-trained protein language model and maps mechanicalunfolding responses to create novel proteins. Via full-atom molecularsimulations for direct validation, we demonstrate that the designed proteinsare novel, and fulfill the targeted mechanical properties, including unfoldingenergy and mechanical strength, as well as the detailed unfoldingforce-separation curves. Our model offers rapid pathways to explore theenormous mechanobiological protein sequence space unconstrained by biologicalsynthesis, using mechanical features as target to enable the discovery ofprotein materials with superior mechanical properties.</description><author>Bo Ni, David L. Kaplan, Markus J. Buehler</author><pubDate>Mon, 16 Oct 2023 18:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10605v1</guid></item><item><title>Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems</title><link>http://arxiv.org/abs/2310.10603v1</link><description>Recently, machine learning, particularly message-passing graph neuralnetworks (MPNNs), has gained traction in enhancing exact optimizationalgorithms. For example, MPNNs speed up solving mixed-integer optimizationproblems by imitating computational intensive heuristics like strong branching,which entails solving multiple linear optimization problems (LPs). Despite theempirical success, the reasons behind MPNNs' effectiveness in emulating linearoptimization remain largely unclear. Here, we show that MPNNs can simulatestandard interior-point methods for LPs, explaining their practical success.Furthermore, we highlight how MPNNs can serve as a lightweight proxy forsolving LPs, adapting to a given problem instance distribution. Empirically, weshow that MPNNs solve LP relaxations of standard combinatorial optimizationproblems close to optimality, often surpassing conventional solvers andcompeting approaches in solving time.</description><author>Chendi Qian, Didier Chételat, Christopher Morris</author><pubDate>Mon, 16 Oct 2023 18:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10603v1</guid></item><item><title>Physics-informed neural wavefields with Gabor basis functions</title><link>http://arxiv.org/abs/2310.10602v1</link><description>Recently, Physics-Informed Neural Networks (PINNs) have gained significantattention for their versatile interpolation capabilities in solving partialdifferential equations (PDEs). Despite their potential, the training can becomputationally demanding, especially for intricate functions like wavefields.This is primarily due to the neural-based (learned) basis functions, biasedtoward low frequencies, as they are dominated by polynomial calculations, whichare not inherently wavefield-friendly. In response, we propose an approach toenhance the efficiency and accuracy of neural network wavefield solutions bymodeling them as linear combinations of Gabor basis functions that satisfy thewave equation. Specifically, for the Helmholtz equation, we augment the fullyconnected neural network model with an adaptable Gabor layer constituting thefinal hidden layer, employing a weighted summation of these Gabor neurons tocompute the predictions (output). These weights/coefficients of the Gaborfunctions are learned from the previous hidden layers that include nonlinearactivation functions. To ensure the Gabor layer's utilization across the modelspace, we incorporate a smaller auxiliary network to forecast the center ofeach Gabor function based on input coordinates. Realistic assessments showcasethe efficacy of this novel implementation compared to the vanilla PINN,particularly in scenarios involving high-frequencies and realistic models thatare often challenging for PINNs.</description><author>Tariq Alkhalifah, Xinquan Huang</author><pubDate>Mon, 16 Oct 2023 18:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10602v1</guid></item><item><title>Computation with Sequences in a Model of the Brain</title><link>http://arxiv.org/abs/2306.03812v2</link><description>Even as machine learning exceeds human-level performance on manyapplications, the generality, robustness, and rapidity of the brain's learningcapabilities remain unmatched. How cognition arises from neural activity is acentral open question in neuroscience, inextricable from the study ofintelligence itself. A simple formal model of neural activity was proposed inPapadimitriou [2020] and has been subsequently shown, through both mathematicalproofs and simulations, to be capable of implementing certain simple cognitiveoperations via the creation and manipulation of assemblies of neurons. However,many intelligent behaviors rely on the ability to recognize, store, andmanipulate temporal sequences of stimuli (planning, language, navigation, tolist a few). Here we show that, in the same model, time can be capturednaturally as precedence through synaptic weights and plasticity, and, as aresult, a range of computations on sequences of assemblies can be carried out.In particular, repeated presentation of a sequence of stimuli leads to thememorization of the sequence through corresponding neural assemblies: uponfuture presentation of any stimulus in the sequence, the corresponding assemblyand its subsequent ones will be activated, one after the other, until the endof the sequence. Finally, we show that any finite state machine can be learnedin a similar way, through the presentation of appropriate patterns ofsequences. Through an extension of this mechanism, the model can be shown to becapable of universal computation. We support our analysis with a number ofexperiments to probe the limits of learning in this model in key ways. Takentogether, these results provide a concrete hypothesis for the basis of thebrain's remarkable abilities to compute and learn, with sequences playing avital role.</description><author>Max Dabagia, Christos H. Papadimitriou, Santosh S. Vempala</author><pubDate>Mon, 16 Oct 2023 18:30:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03812v2</guid></item><item><title>Pareto Optimization to Accelerate Multi-Objective Virtual Screening</title><link>http://arxiv.org/abs/2310.10598v1</link><description>The discovery of therapeutic molecules is fundamentally a multi-objectiveoptimization problem. One formulation of the problem is to identify moleculesthat simultaneously exhibit strong binding affinity for a target protein,minimal off-target interactions, and suitable pharmacokinetic properties.Inspired by prior work that uses active learning to accelerate theidentification of strong binders, we implement multi-objective Bayesianoptimization to reduce the computational cost of multi-property virtualscreening and apply it to the identification of ligands predicted to beselective based on docking scores to on- and off-targets. We demonstrate thesuperiority of Pareto optimization over scalarization across three casestudies. Further, we use the developed optimization tool to search a virtuallibrary of over 4M molecules for those predicted to be selective dualinhibitors of EGFR and IGF1R, acquiring 100% of the molecules that form thelibrary's Pareto front after exploring only 8% of the library. This workflowand associated open source software can reduce the screening burden ofmolecular design projects and is complementary to research aiming to improvethe accuracy of binding predictions and other molecular properties.</description><author>Jenna C. Fromer, David E. Graff, Connor W. Coley</author><pubDate>Mon, 16 Oct 2023 18:19:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10598v1</guid></item><item><title>Motion2Language, Unsupervised learning of synchronized semantic motion segmentation</title><link>http://arxiv.org/abs/2310.10594v1</link><description>In this paper, we investigate building a sequence to sequence architecturefor motion to language translation and synchronization. The aim is to translatemotion capture inputs into English natural-language descriptions, such that thedescriptions are generated synchronously with the actions performed, enablingsemantic segmentation as a byproduct, but without requiring synchronizedtraining data. We propose a new recurrent formulation of local attention thatis suited for synchronous/live text generation, as well as an improved motionencoder architecture better suited to smaller data and for synchronousgeneration. We evaluate both contributions in individual experiments, using thestandard BLEU4 metric, as well as a simple semantic equivalence measure, on theKIT motion language dataset. In a follow-up experiment, we assess the qualityof the synchronization of generated text in our proposed approaches throughmultiple evaluation metrics. We find that both contributions to the attentionmechanism and the encoder architecture additively improve the quality ofgenerated text (BLEU and semantic equivalence), but also of synchronization.Our code will be made available at\url{https://github.com/rd20karim/M2T-Segmentation/tree/main}</description><author>Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, Julien Lagarde</author><pubDate>Mon, 16 Oct 2023 18:16:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10594v1</guid></item><item><title>Interpreting and Controlling Vision Foundation Models via Text Explanations</title><link>http://arxiv.org/abs/2310.10591v1</link><description>Large-scale pre-trained vision foundation models, such as CLIP, have becomede facto backbones for various vision tasks. However, due to their black-boxnature, understanding the underlying rules behind these models' predictions andcontrolling model behaviors have remained open challenges. We present aframework for interpreting vision transformer's latent tokens with naturallanguage. Given a latent token, our framework retains its semantic informationto the final layer using transformer's local operations and retrieves theclosest text for explanation. Our approach enables understanding of modelvisual reasoning procedure without needing additional model training or datacollection. Based on the obtained interpretations, our framework allows formodel editing that controls model reasoning behaviors and improves modelrobustness against biases and spurious correlations.</description><author>Haozhe Chen, Junfeng Yang, Carl Vondrick, Chengzhi Mao</author><pubDate>Mon, 16 Oct 2023 18:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10591v1</guid></item><item><title>Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment</title><link>http://arxiv.org/abs/2310.10590v1</link><description>Open Information Extraction (OIE) aims to extract objective structuredknowledge from natural texts, which has attracted growing attention to builddedicated models with human experience. As the large language models (LLMs)have exhibited remarkable in-context learning capabilities, a question arisesas to whether the task of OIE can be effectively tackled with this paradigm? Inthis paper, we explore solving the OIE problem by constructing an appropriatereasoning environment for LLMs. Specifically, we first propose a method toeffectively estimate the discrepancy of syntactic distribution between a LLMand test samples, which can serve as correlation evidence for preparingpositive demonstrations. Upon the evidence, we introduce a simple yet effectivemechanism to establish the reasoning environment for LLMs on specific tasks.Without bells and whistles, experimental results on the standard CaRB benchmarkdemonstrate that our $6$-shot approach outperforms state-of-the-art supervisedmethod, achieving an $55.3$ $F_1$ score. Further experiments on TACRED andACE05 show that our method can naturally generalize to other informationextraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores,respectively.</description><author>Ji Qi, Kaixuan Ji, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Lei Hou, Juanzi Li, Bin Xu</author><pubDate>Mon, 16 Oct 2023 18:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10590v1</guid></item><item><title>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2308.09678v2</link><description>Existing 3D human pose estimators face challenges in adapting to new datasetsdue to the lack of 2D-3D pose pairs in training sets. To overcome this issue,we propose \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis\textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to bridgethis data disparity gap in target domain. Typically, PoSynDA uses adiffusion-inspired structure to simulate 3D pose distribution in the targetdomain. By incorporating a multi-hypothesis network, PoSynDA generates diversepose hypotheses and aligns them with the target domain. To do this, it firstutilizes target-specific source augmentation to obtain the target domaindistribution data from the source domain by decoupling the scale and positionparameters. The process is then further refined through the teacher-studentparadigm and low-rank adaptation. With extensive comparison of benchmarks suchas Human3.6M and MPI-INF-3DHP, PoSynDA demonstrates competitive performance,even comparable to the target-trained MixSTE model\cite{zhang2022mixste}. Thiswork paves the way for the practical application of 3D human pose estimation inunseen domains. The code is available at https://github.com/hbing-l/PoSynDA.</description><author>Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 16 Oct 2023 18:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09678v2</guid></item><item><title>BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation</title><link>http://arxiv.org/abs/2310.10586v1</link><description>Building models that generate textual responses to user instructions forvideos is a practical and challenging topic, as it requires both visionunderstanding and knowledge reasoning. Compared to language and imagemodalities, training efficiency remains a serious problem as existing studiestrain models on massive sparse videos aligned with brief descriptions. In thispaper, we introduce BiLL-VTG, a fast adaptive framework that leverages largelanguage models (LLMs) to reasoning on videos based on essential lightweightvisual tools. Specifically, we reveal the key to response specific instructionsis the concentration on relevant video events, and utilize two visual tools ofstructured scene graph generation and descriptive image caption generation togather and represent the events information. Thus, a LLM equipped with worldknowledge is adopted as the reasoning agent to achieve the response byperforming multiple reasoning steps on specified video events.To address thedifficulty of specifying events from agent, we further propose anInstruction-oriented Video Events Recognition (InsOVER) algorithm based on theefficient Hungarian matching to localize corresponding video events usinglinguistic instructions, enabling LLMs to interact with long videos. Extensiveexperiments on two typical video-based texts generations tasks show that ourtuning-free framework outperforms the pre-trained models includingFlamingo-80B, to achieve the state-of-the-art performance.</description><author>Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, Lei Hou, Juanzi Li</author><pubDate>Mon, 16 Oct 2023 18:05:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10586v1</guid></item><item><title>Who Are All The Stochastic Parrots Imitating? They Should Tell Us!</title><link>http://arxiv.org/abs/2310.10583v1</link><description>Both standalone language models (LMs) as well as LMs within downstream-tasksystems have been shown to generate statements which are factually untrue. Thisproblem is especially severe for low-resource languages, where training data isscarce and of worse quality than for high-resource languages. In this opinionpiece, we argue that LMs in their current state will never be fully trustworthyin critical settings and suggest a possible novel strategy to handle thisissue: by building LMs such that can cite their sources - i.e., point a user tothe parts of their training data that back up their outputs. We first discusswhich current NLP tasks would or would not benefit from such models. We thenhighlight the expected benefits such models would bring, e.g., quickverifiability of statements. We end by outlining the individual tasks thatwould need to be solved on the way to developing LMs with the ability to cite.We hope to start a discussion about the field's current approach to buildingLMs, especially for low-resource languages, and the role of the training datain explaining model generations.</description><author>Sagi Shaier, Lawrence E. Hunter, Katharina von der Wense</author><pubDate>Mon, 16 Oct 2023 17:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10583v1</guid></item><item><title>Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?</title><link>http://arxiv.org/abs/2203.06487v2</link><description>Being able to explain the prediction to clinical end-users is a necessity toleverage the power of artificial intelligence (AI) models for clinical decisionsupport. For medical images, a feature attribution map, or heatmap, is the mostcommon form of explanation that highlights important features for AI models'prediction. However, it is unknown how well heatmaps perform on explainingdecisions on multi-modal medical images, where each image modality or channelvisualizes distinct clinical information of the same underlying biomedicalphenomenon. Understanding such modality-dependent features is essential forclinical users' interpretation of AI decisions. To tackle this clinicallyimportant but technically ignored problem, we propose the modality-specificfeature importance (MSFI) metric. It encodes clinical image and explanationinterpretation patterns of modality prioritization and modality-specificfeature localization. We conduct a clinical requirement-grounded, systematicevaluation using computational methods and a clinician user study. Results showthat the examined 16 heatmap algorithms failed to fulfill clinical requirementsto correctly indicate AI model decision process or decision quality. Theevaluation and MSFI metric can guide the design and selection of XAI algorithmsto meet clinical requirements on multi-modal explanation.</description><author>Weina Jin, Xiaoxiao Li, Ghassan Hamarneh</author><pubDate>Mon, 16 Oct 2023 17:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.06487v2</guid></item><item><title>Matching the Neuronal Representations of V1 is Necessary to Improve Robustness in CNNs with V1-like Front-ends</title><link>http://arxiv.org/abs/2310.10575v1</link><description>While some convolutional neural networks (CNNs) have achieved great successin object recognition, they struggle to identify objects in images corruptedwith different types of common noise patterns. Recently, it was shown thatsimulating computations in early visual areas at the front of CNNs leads toimprovements in robustness to image corruptions. Here, we further explore thisresult and show that the neuronal representations that emerge from preciselymatching the distribution of RF properties found in primate V1 is key for thisimprovement in robustness. We built two variants of a model with a front-endmodeling the primate primary visual cortex (V1): one sampling RF propertiesuniformly and the other sampling from empirical biological distributions. Themodel with the biological sampling has a considerably higher robustness toimage corruptions that the uniform variant (relative difference of 8.72%).While similar neuronal sub-populations across the two variants have similarresponse properties and learn similar downstream weights, the impact ondownstream processing is strikingly different. This result sheds light on theorigin of the improvements in robustness observed in some biologically-inspiredmodels, pointing to the need of precisely mimicking the neuronalrepresentations found in the primate brain.</description><author>Ruxandra Barbulescu, Tiago Marques, Arlindo L. Oliveira</author><pubDate>Mon, 16 Oct 2023 17:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10575v1</guid></item><item><title>Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems</title><link>http://arxiv.org/abs/2310.10571v1</link><description>State-of-the-art question answering (QA) models exhibit a variety of socialbiases (e.g., with respect to sex or race), generally explained by similarissues in their training data. However, what has been overlooked so far is thatin the critical domain of biomedicine, any unjustified change in model outputdue to patient demographics is problematic: it results in the unfair treatmentof patients. Selecting only questions on biomedical topics whose answers do notdepend on ethnicity, sex, or sexual orientation, we ask the following researchquestions: (RQ1) Do the answers of QA models change when being provided withirrelevant demographic information? (RQ2) Does the answer of RQ1 differ betweenknowledge graph (KG)-grounded and text-based QA systems? We find thatirrelevant demographic information change up to 15% of the answers of aKG-grounded system and up to 23% of the answers of a text-based system,including changes that affect accuracy. We conclude that unjustified answerchanges caused by patient demographics are a frequent phenomenon, which raisesfairness concerns and should be paid more attention to.</description><author>Sagi Shaier, Kevin Bennett, Lawrence Hunter, Katharina von der Wense</author><pubDate>Mon, 16 Oct 2023 17:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10571v1</guid></item><item><title>On Position Bias in Summarization with Large Language Models</title><link>http://arxiv.org/abs/2310.10570v1</link><description>Large language models (LLMs) excel in zero-shot abstractive summarizationtasks, delivering fluent and pertinent summaries. Recent advancements haveextended their capabilities to handle long-input contexts, surpassing tokenlimits of 32k or more. However, in the realm of multi-document questionanswering, language models exhibit uneven utilization of their input context.They tend to favor the initial and final segments, resulting in a U-shapedperformance pattern concerning where the answer is located within the input.This bias raises concerns, particularly in summarization tasks where crucialcontent may be dispersed throughout the source document(s). This paper presentsa comprehensive investigation encompassing 10 datasets, 4 LLMs, and 5evaluation metrics to analyze how these models leverage their input forabstractive summarization. Our findings reveal a pronounced bias towards theintroductory content (and to a lesser extent, the final content), posingchallenges for LLM performance across a range of diverse summarizationbenchmarks.</description><author>Mathieu Ravaut, Shafiq Joty, Aixin Sun, Nancy F. Chen</author><pubDate>Mon, 16 Oct 2023 17:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10570v1</guid></item><item><title>RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling</title><link>http://arxiv.org/abs/2310.10567v1</link><description>Retrieval-augmented language models show promise in addressing issues likeoutdated information and hallucinations in language models (LMs). However,current research faces two main problems: 1) determining what information toretrieve, and 2) effectively combining retrieved information during generation.We argue that valuable retrieved information should not only be related to thecurrent source text but also consider the future target text, given the natureof LMs that model future tokens. Moreover, we propose that aggregation usinglatent variables derived from a compact latent space is more efficient thanutilizing explicit raw text, which is limited by context length and susceptibleto noise. Therefore, we introduce RegaVAE, a retrieval-augmented language modelbuilt upon the variational auto-encoder (VAE). It encodes the text corpus intoa latent space, capturing current and future information from both source andtarget text. Additionally, we leverage the VAE to initialize the latent spaceand adopt the probabilistic form of the retrieval generation paradigm byexpanding the Gaussian prior distribution into a Gaussian mixture distribution.Theoretical analysis provides an optimizable upper bound for RegaVAE.Experimental results on various datasets demonstrate significant improvementsin text generation quality and hallucination removal.</description><author>Jingcheng Deng, Liang Pang, Huawei Shen, Xueqi Cheng</author><pubDate>Mon, 16 Oct 2023 17:42:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10567v1</guid></item><item><title>HelmSim: Learning Helmholtz Dynamics for Interpretable Fluid Simulation</title><link>http://arxiv.org/abs/2310.10565v1</link><description>Fluid simulation is a long-standing challenge due to the intrinsichigh-dimensional non-linear dynamics. Previous methods usually utilize thenon-linear modeling capability of deep models to directly estimate velocityfields for future prediction. However, skipping over inherent physicalproperties but directly learning superficial velocity fields will overwhelm themodel from generating precise or physics-reliable results. In this paper, wepropose the HelmSim toward an accurate and interpretable simulator for fluid.Inspired by the Helmholtz theorem, we design a HelmDynamic block to learn theHelmholtz dynamics, which decomposes fluid dynamics into more solvablecurl-free and divergence-free parts, physically corresponding to potential andstream functions of fluid. By embedding the HelmDynamic block into a MultiscaleIntegration Network, HelmSim can integrate learned Helmholtz dynamics alongtemporal dimension in multiple spatial scales to yield future fluid. Comparingwith previous velocity estimating methods, HelmSim is faithfully derived fromHelmholtz theorem and ravels out complex fluid dynamics with physicallyinterpretable evidence. Experimentally, our proposed HelmSim achieves theconsistent state-of-the-art in both numerical simulated and real-world observedbenchmarks, even for scenarios with complex boundaries.</description><author>Lanxiang Xing, Haixu Wu, Yuezhou Ma, Jianmin Wang, Mingsheng Long</author><pubDate>Mon, 16 Oct 2023 17:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10565v1</guid></item><item><title>RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets</title><link>http://arxiv.org/abs/2310.10563v1</link><description>We propose Re-parameterized Refocusing Convolution (RefConv) as a replacementfor regular convolutional layers, which is a plug-and-play module to improvethe performance without any inference costs. Specifically, given a pre-trainedmodel, RefConv applies a trainable Refocusing Transformation to the basiskernels inherited from the pre-trained model to establish connections among theparameters. For example, a depth-wise RefConv can relate the parameters of aspecific channel of convolution kernel to the parameters of the other kernel,i.e., make them refocus on the other parts of the model they have neverattended to, rather than focus on the input features only. From anotherperspective, RefConv augments the priors of existing model structures byutilizing the representations encoded in the pre-trained parameters as thepriors and refocusing on them to learn novel representations, thus furtherenhancing the representational capacity of the pre-trained model. Experimentalresults validated that RefConv can improve multiple CNN-based models by a clearmargin on image classification (up to 1.47% higher top-1 accuracy on ImageNet),object detection and semantic segmentation without introducing any extrainference costs or altering the original model structure. Further studiesdemonstrated that RefConv can reduce the redundancy of channels and smooth theloss landscape, which explains its effectiveness.</description><author>Zhicheng Cai, Xiaohan Ding, Qiu Shen, Xun Cao</author><pubDate>Mon, 16 Oct 2023 17:36:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10563v1</guid></item><item><title>Towards the Imagenets of ML4EDA</title><link>http://arxiv.org/abs/2310.10560v1</link><description>Despite the growing interest in ML-guided EDA tools from RTL to GDSII, thereare no standard datasets or prototypical learning tasks defined for the EDAproblem domain. Experience from the computer vision community suggests thatsuch datasets are crucial to spur further progress in ML for EDA. Here wedescribe our experience curating two large-scale, high-quality datasets forVerilog code generation and logic synthesis. The first, VeriGen, is a datasetof Verilog code collected from GitHub and Verilog textbooks. The second,OpenABC-D, is a large-scale, labeled dataset designed to aid ML for logicsynthesis tasks. The dataset consists of 870,000 And-Inverter-Graphs (AIGs)produced from 1500 synthesis runs on a large number of open-source hardwareprojects. In this paper we will discuss challenges in curating, maintaining andgrowing the size and scale of these datasets. We will also touch upon questionsof dataset quality and security, and the use of novel data augmentation toolsthat are tailored for the hardware domain.</description><author>Animesh Basak Chowdhury, Shailja Thakur, Hammond Pearce, Ramesh Karri, Siddharth Garg</author><pubDate>Mon, 16 Oct 2023 17:35:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10560v1</guid></item><item><title>Causal Dynamic Variational Autoencoder for Counterfactual Regression in Longitudinal Data</title><link>http://arxiv.org/abs/2310.10559v1</link><description>Estimating treatment effects over time is relevant in many real-worldapplications, such as precision medicine, epidemiology, economy, and marketing.Many state-of-the-art methods either assume the observations of all confoundersor seek to infer the unobserved ones. We take a different perspective byassuming unobserved risk factors, i.e., adjustment variables that affect onlythe sequence of outcomes. Under unconfoundedness, we target the IndividualTreatment Effect (ITE) estimation with unobserved heterogeneity in thetreatment response due to missing risk factors. We address the challenges posedby time-varying effects and unobserved adjustment variables. Led by theoreticalresults over the validity of the learned adjustment variables andgeneralization bounds over the treatment effect, we devise Causal DVAE (CDVAE).This model combines a Dynamic Variational Autoencoder (DVAE) framework with aweighting strategy using propensity scores to estimate counterfactualresponses. The CDVAE model allows for accurate estimation of ITE and capturesthe underlying heterogeneity in longitudinal data. Evaluations of our modelshow superior performance over state-of-the-art models.</description><author>Mouad El Bouchattaoui, Myriam Tami, Benoit Lepetit, Paul-Henry Cournède</author><pubDate>Mon, 16 Oct 2023 17:32:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10559v1</guid></item><item><title>A correlation-based fuzzy cluster validity index with secondary options detector</title><link>http://arxiv.org/abs/2308.14785v2</link><description>The optimal number of clusters is one of the main concerns when applyingcluster analysis. Several cluster validity indexes have been introduced toaddress this problem. However, in some situations, there is more than oneoption that can be chosen as the final number of clusters. This aspect has beenoverlooked by most of the existing works in this area. In this study, weintroduce a correlation-based fuzzy cluster validity index known as theWiroonsri-Preedasawakul (WP) index. This index is defined based on thecorrelation between the actual distance between a pair of data points and thedistance between adjusted centroids with respect to that pair. We evaluate andcompare the performance of our index with several existing indexes, includingXie-Beni, Pakhira-Bandyopadhyay-Maulik, Tang, Wu-Li, generalized C, and Kwon2.We conduct this evaluation on four types of datasets: artificial datasets,real-world datasets, simulated datasets with ranks, and image datasets, usingthe fuzzy c-means algorithm. Overall, the WP index outperforms most, if notall, of these indexes in terms of accurately detecting the optimal number ofclusters and providing accurate secondary options. Moreover, our index remainseffective even when the fuzziness parameter $m$ is set to a large value. Our Rpackage called UniversalCVI used in this work is available athttps://CRAN.R-project.org/package=UniversalCVI.</description><author>Nathakhun Wiroonsri, Onthada Preedasawakul</author><pubDate>Mon, 16 Oct 2023 17:30:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14785v2</guid></item><item><title>The Expresssive Power of Transformers with Chain of Thought</title><link>http://arxiv.org/abs/2310.07923v2</link><description>Recent theoretical work has identified surprisingly simple reasoningproblems, such as checking if two nodes in a graph are connected or simulatingfinite-state machines, that are provably unsolvable by standard transformersthat answer immediately after reading their input. However, in practice,transformers' reasoning can be improved by allowing them to use a "chain ofthought" or "scratchpad", i.e., generate and condition on a sequence ofintermediate tokens before answering. Motivated by this, we ask: Does suchintermediate generation fundamentally extend the computational power of adecoder-only transformer? We show that the answer is yes, but the amount ofincrease depends crucially on the amount of intermediate generation. Forinstance, we find that transformer decoders with a logarithmic number ofdecoding steps (w.r.t. the input length) push the limits of standardtransformers only slightly, while a linear number of decoding steps adds aclear new ability (under standard complexity conjectures): recognizing allregular languages. Our results also imply that linear steps keep transformerdecoders within context-sensitive languages, and polynomial steps make themrecognize exactly the class of polynomial-time solvable problems -- the firstexact characterization of a type of transformers in terms of standardcomplexity classes. Together, our results provide a nuanced framework forunderstanding how the length of a transformer's chain of thought or scratchpadimpacts its reasoning power.</description><author>William Merrill, Ashish Sabharwal</author><pubDate>Mon, 16 Oct 2023 17:30:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07923v2</guid></item><item><title>Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks</title><link>http://arxiv.org/abs/2310.10556v1</link><description>A recently popular approach to solving reinforcement learning is with datafrom human preferences. In fact, human preference data are now used withclassic reinforcement learning algorithms such as actor-critic methods, whichinvolve evaluating an intermediate policy over a reward learned from humanpreference data with distribution shift, known as off-policy evaluation (OPE).Such algorithm includes (i) learning reward function from human preferencedataset, and (ii) learning expected cumulative reward of a target policy.Despite the huge empirical success, existing OPE methods with preference dataoften lack theoretical understanding and rely heavily on heuristics. In thispaper, we study the sample efficiency of OPE with human preference andestablish a statistical guarantee for it. Specifically, we approach OPE bylearning the value function by fitted-Q-evaluation with a deep neural network.By appropriately selecting the size of a ReLU network, we show that one canleverage any low-dimensional manifold structure in the Markov decision processand obtain a sample-efficient estimator without suffering from the curse ofhigh data ambient dimensionality. Under the assumption of high rewardsmoothness, our results \textit{almost align with the classical OPE resultswith observable reward data}. To the best of our knowledge, this is the firstresult that establishes a \textit{provably efficient} guarantee for off-policyevaluation with RLHF.</description><author>Zihao Li, Xiang Ji, Minshuo Chen, Mengdi Wang</author><pubDate>Mon, 16 Oct 2023 17:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10556v1</guid></item><item><title>Bridging Discrete and Backpropagation: Straight-Through and Beyond</title><link>http://arxiv.org/abs/2304.08612v3</link><description>Backpropagation, the cornerstone of deep learning, is limited to computinggradients for continuous variables. This limitation poses challenges forproblems involving discrete latent variables. To address this issue, we proposea novel approach to approximate the gradient of parameters involved ingenerating discrete latent variables. First, we examine the widely usedStraight-Through (ST) heuristic and demonstrate that it works as a first-orderapproximation of the gradient. Guided by our findings, we propose ReinMax,which achieves second-order accuracy by integrating Heun's method, asecond-order numerical method for solving ODEs. ReinMax does not requireHessian or other second-order derivatives, thus having negligible computationoverheads. Extensive experimental results on various tasks demonstrate thesuperiority of ReinMax over the state of the art. Implementations are releasedat https://github.com/microsoft/ReinMax.</description><author>Liyuan Liu, Chengyu Dong, Xiaodong Liu, Bin Yu, Jianfeng Gao</author><pubDate>Mon, 16 Oct 2023 17:27:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08612v3</guid></item><item><title>Population-based wind farm monitoring based on a spatial autoregressive approach</title><link>http://arxiv.org/abs/2310.10555v1</link><description>An important challenge faced by wind farm operators is to reduce operationand maintenance cost. Structural health monitoring provides a means of costreduction through minimising unnecessary maintenance trips as well asprolonging turbine service life. Population-based structural health monitoringcan further reduce the cost of health monitoring systems by implementing onesystem for multiple structures (i.e.~turbines). At the same time, shared datawithin a population of structures may improve the predictions of structuralbehaviour. To monitor turbine performance at a population/farm level, animportant initial step is to construct a model that describes the behaviour ofall turbines under normal conditions. This paper proposes a population-levelmodel that explicitly captures the spatial and temporal correlations (betweenturbines) induced by the wake effect. The proposed model is a Gaussianprocess-based spatial autoregressive model, named here a GP-SPARX model. Thisapproach is developed since (a) it reflects our physical understanding of thewake effect, and (b) it benefits from a stochastic data-based learner. A casestudy is provided to demonstrate the capability of the GP-SPARX model incapturing spatial and temporal variations as well as its potentialapplicability in a health monitoring system.</description><author>W. Lin, K. Worden, E. J. Cross</author><pubDate>Mon, 16 Oct 2023 17:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10555v1</guid></item><item><title>TacticAI: an AI assistant for football tactics</title><link>http://arxiv.org/abs/2310.10553v1</link><description>Identifying key patterns of tactics implemented by rival teams, anddeveloping effective responses, lies at the heart of modern football. However,doing so algorithmically remains an open research challenge. To address thisunmet need, we propose TacticAI, an AI football tactics assistant developed andevaluated in close collaboration with domain experts from Liverpool FC. Wefocus on analysing corner kicks, as they offer coaches the most directopportunities for interventions and improvements. TacticAI incorporates both apredictive and a generative component, allowing the coaches to effectivelysample and explore alternative player setups for each corner kick routine andto select those with the highest predicted likelihood of success. We validateTacticAI on a number of relevant benchmark tasks: predicting receivers and shotattempts and recommending player position adjustments. The utility of TacticAIis validated by a qualitative study conducted with football domain experts atLiverpool FC. We show that TacticAI's model suggestions are not onlyindistinguishable from real tactics, but also favoured over existing tactics90% of the time, and that TacticAI offers an effective corner kick retrievalsystem. TacticAI achieves these results despite the limited availability ofgold-standard data, achieving data efficiency through geometric deep learning.</description><author>Zhe Wang, Petar Veličković, Daniel Hennes, Nenad Tomašev, Laurel Prince, Michael Kaisers, Yoram Bachrach, Romuald Elie, Li Kevin Wenliang, Federico Piccinini, William Spearman, Ian Graham, Jerome Connor, Yi Yang, Adrià Recasens, Mina Khan, Nathalie Beauguerlange, Pablo Sprechmann, Pol Moreno, Nicolas Heess, Michael Bowling, Demis Hassabis, Karl Tuyls</author><pubDate>Mon, 16 Oct 2023 17:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10553v1</guid></item><item><title>Deep learning applied to EEG data with different montages using spatial attention</title><link>http://arxiv.org/abs/2310.10550v1</link><description>The ability of Deep Learning to process and extract relevant information incomplex brain dynamics from raw EEG data has been demonstrated in variousrecent works. Deep learning models, however, have also been shown to performbest on large corpora of data. When processing EEG, a natural approach is tocombine EEG datasets from different experiments to train large deep-learningmodels. However, most EEG experiments use custom channel montages, requiringthe data to be transformed into a common space. Previous methods have used theraw EEG signal to extract features of interest and focused on using a commonfeature space across EEG datasets. While this is a sensible approach, itunderexploits the potential richness of EEG raw data. Here, we explore usingspatial attention applied to EEG electrode coordinates to perform channelharmonization of raw EEG data, allowing us to train deep learning on EEG datausing different montages. We test this model on a gender classification task.We first show that spatial attention increases model performance. Then, we showthat a deep learning model trained on data using different channel montagesperforms significantly better than deep learning models trained on fixed 23-and 128-channel data montages.</description><author>Dung Truong, Muhammad Abdullah Khalid, Arnaud Delorme</author><pubDate>Mon, 16 Oct 2023 17:17:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10550v1</guid></item><item><title>InfoGCN++: Learning Representation by Predicting the Future for Online Human Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2310.10547v1</link><description>Skeleton-based action recognition has made significant advancements recently,with models like InfoGCN showcasing remarkable accuracy. However, these modelsexhibit a key limitation: they necessitate complete action observation prior toclassification, which constrains their applicability in real-time situationssuch as surveillance and robotic systems. To overcome this barrier, weintroduce InfoGCN++, an innovative extension of InfoGCN, explicitly developedfor online skeleton-based action recognition. InfoGCN++ augments the abilitiesof the original InfoGCN model by allowing real-time categorization of actiontypes, independent of the observation sequence's length. It transcendsconventional approaches by learning from current and anticipated futuremovements, thereby creating a more thorough representation of the entiresequence. Our approach to prediction is managed as an extrapolation issue,grounded on observed actions. To enable this, InfoGCN++ incorporates NeuralOrdinary Differential Equations, a concept that lets it effectively model thecontinuous evolution of hidden states. Following rigorous evaluations on threeskeleton-based action recognition benchmarks, InfoGCN++ demonstratesexceptional performance in online action recognition. It consistently equals orexceeds existing techniques, highlighting its significant potential to reshapethe landscape of real-time action recognition applications. Consequently, thiswork represents a major leap forward from InfoGCN, pushing the limits of what'spossible in online, skeleton-based action recognition. The code for InfoGCN++is publicly available at https://github.com/stnoah1/infogcn2 for furtherexploration and validation.</description><author>Seunggeun Chi, Hyung-gun Chi, Qixing Huang, Karthik Ramani</author><pubDate>Mon, 16 Oct 2023 17:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10547v1</guid></item><item><title>Optimal vintage factor analysis with deflation varimax</title><link>http://arxiv.org/abs/2310.10545v1</link><description>Vintage factor analysis is one important type of factor analysis that aims tofirst find a low-dimensional representation of the original data, and then toseek a rotation such that the rotated low-dimensional representation isscientifically meaningful. Perhaps the most widely used vintage factor analysisis the Principal Component Analysis (PCA) followed by the varimax rotation.Despite its popularity, little theoretical guarantee can be provided mainlybecause varimax rotation requires to solve a non-convex optimization over theset of orthogonal matrices. In this paper, we propose a deflation varimax procedure that solves each rowof an orthogonal matrix sequentially. In addition to its net computational gainand flexibility, we are able to fully establish theoretical guarantees for theproposed procedure in a broad context. Adopting this new varimax approach as the second step after PCA, we furtheranalyze this two step procedure under a general class of factor models. Ourresults show that it estimates the factor loading matrix in the optimal ratewhen the signal-to-noise-ratio (SNR) is moderate or large. In the low SNRregime, we offer possible improvement over using PCA and the deflationprocedure when the additive noise under the factor model is structured. Themodified procedure is shown to be optimal in all SNR regimes. Our theory isvalid for finite sample and allows the number of the latent factors to growwith the sample size as well as the ambient dimension to grow with, or evenexceed, the sample size. Extensive simulation and real data analysis further corroborate ourtheoretical findings.</description><author>Xin Bing, Dian Jin, Yuqian Zhang</author><pubDate>Mon, 16 Oct 2023 17:14:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10545v1</guid></item><item><title>Use of probabilistic phrases in a coordination game: human versus GPT-4</title><link>http://arxiv.org/abs/2310.10544v1</link><description>English speakers use probabilistic phrases such as likely to communicateinformation about the probability or likelihood of events. Communication issuccessful to the extent that the listener grasps what the speaker means toconvey and, if communication is successful, two individuals can potentiallycoordinate their actions based on shared knowledge about uncertainty. We firstassessed human ability to estimate the probability and the ambiguity(imprecision) of 23 probabilistic phrases in two different contexts, investmentadvice and medical advice. We then had GPT4 (OpenAI), a recent Large LanguageModel, complete the same tasks as the human participants. We found that themedian human participant and GPT4 assigned probability estimates that were ingood agreement (proportions of variance accounted were close to .90). GPT4'sestimates of probability both in the investment and Medical contexts were asclose or closer to that of the human participants as the human participantswere to one another. Estimates of probability for both the human participantsand GPT4 were little affected by context. In contrast, human and GPT4 estimatesof ambiguity were not in as good agreement. We repeated some of the GPT4estimates to assess their stability: does GPT4, if run twice, produce the sameor similar estimates? There is some indication that it does not.</description><author>Laurence T Maloney, Maria F Dal Martello, Vivian Fei, Valerie Ma</author><pubDate>Mon, 16 Oct 2023 17:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10544v1</guid></item><item><title>ViPE: Visualise Pretty-much Everything</title><link>http://arxiv.org/abs/2310.10543v1</link><description>Figurative and non-literal expressions are profoundly integrated in humancommunication. Visualising such expressions allow us to convey our creativethoughts, and evoke nuanced emotions. Recent text-to-image models like StableDiffusion, on the other hand, struggle to depict non-literal expressions.Recent works primarily deal with this issue by compiling humanly annotateddatasets on a small scale, which not only demands specialised expertise butalso proves highly inefficient. To address this issue, we introduce ViPE:Visualise Pretty-much Everything. ViPE offers a series of lightweight androbust language models that have been trained on a large-scale set of lyricswith noisy visual descriptions that represent their implicit meaning. Thesynthetic visual descriptions are generated by GPT3.5 relying on neither humanannotations nor images. ViPE effectively expresses any arbitrary piece of textinto a visualisable description, enabling meaningful and high-quality imagegeneration. We provide compelling evidence that ViPE is more robust than GPT3.5in synthesising visual elaborations. ViPE also exhibits an understanding offigurative expressions comparable to human experts, providing a powerful andopen-source backbone to many downstream applications such as music video andcaption generation.</description><author>Hassan Shahmohammadi, Adhiraj Ghosh, Hendrik P. A. Lensch</author><pubDate>Mon, 16 Oct 2023 17:14:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10543v1</guid></item><item><title>Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories</title><link>http://arxiv.org/abs/2310.10541v1</link><description>Training a large and state-of-the-art machine learning model typicallynecessitates the use of large-scale datasets, which, in turn, makes thetraining and parameter-tuning process expensive and time-consuming. Someresearchers opt to distil information from real-world datasets into tiny andcompact synthetic datasets while maintaining their ability to train awell-performing model, hence proposing a data-efficient method known as DatasetDistillation (DD). Despite recent progress in this field, existing methodsstill underperform and cannot effectively replace large datasets. In thispaper, unlike previous methods that focus solely on improving the efficacy ofstudent distillation, we are the first to recognize the important interplaybetween expert and student. We argue the significant impact of expertsmoothness when employing more potent expert trajectories in subsequent datasetdistillation. Based on this, we introduce the integration of clipping loss andgradient penalty to regulate the rate of parameter changes in experttrajectories. Furthermore, in response to the sensitivity exhibited towardsrandomly initialized variables during distillation, we propose representativeinitialization for synthetic dataset and balanced inner-loop loss. Finally, wepresent two enhancement strategies, namely intermediate matching loss andweight perturbation, to mitigate the potential occurrence of cumulative errors.We conduct extensive experiments on datasets of different scales, sizes, andresolutions. The results demonstrate that the proposed method significantlyoutperforms prior methods.</description><author>Jiyuan Shen, Wenzhuo Yang, Kwok-Yan Lam</author><pubDate>Mon, 16 Oct 2023 17:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10541v1</guid></item><item><title>Improving Anomaly Segmentation with Multi-Granularity Cross-Domain Alignment</title><link>http://arxiv.org/abs/2308.08696v2</link><description>Anomaly segmentation plays a pivotal role in identifying atypical objects inimages, crucial for hazard detection in autonomous driving systems. Whileexisting methods demonstrate noteworthy results on synthetic data, they oftenfail to consider the disparity between synthetic and real-world data domains.Addressing this gap, we introduce the Multi-Granularity Cross-Domain Alignment(MGCDA) framework, tailored to harmonize features across domains at both thescene and individual sample levels. Our contributions are twofold: i) Wepresent the Multi-source Domain Adversarial Training module. This integrates amulti-source adversarial loss coupled with dynamic label smoothing,facilitating the learning of domain-agnostic representations across multipleprocessing stages. ii) We propose an innovative Cross-domain Anomaly-awareContrastive Learning methodology.} This method adeptly selects challenginganchor points and images using an anomaly-centric strategy, ensuring precisealignment at the sample level. Extensive evaluations of the Fishyscapes andRoadAnomaly datasets demonstrate MGCDA's superior performance and adaptability.Additionally, its ability to perform parameter-free inference and function withvarious network architectures highlights its distinctiveness in advancing thefrontier of anomaly segmentation.</description><author>Ji Zhang, Xiao Wu, Zhi-Qi Cheng, Qi He, Wei Li</author><pubDate>Mon, 16 Oct 2023 17:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08696v2</guid></item><item><title>Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned Receipt Images</title><link>http://arxiv.org/abs/2212.05525v3</link><description>Digitization of scanned receipts aims to extract text from receipt images andsave it into structured documents. This is usually split into two sub-tasks:text localization and optical character recognition (OCR). Most existing OCRmodels only focus on the cropped text instance images, which require thebounding box information provided by a text region detection model. Introducingan additional detector to identify the text instance images in advance addscomplexity, however instance-level OCR models have very low accuracy whenprocessing the whole image for the document-level OCR, such as receipt imagescontaining multiple text lines arranged in various layouts. To this end, wepropose a localization-free document-level OCR model for transcribing all thecharacters in a receipt image into an ordered sequence end-to-end.Specifically, we finetune the pretrained instance-level model TrOCR withrandomly cropped image chunks, and gradually increase the image chunk size togeneralize the recognition ability from instance images to full-page images. Inour experiments on the SROIE receipt OCR dataset, the model finetuned with ourstrategy achieved 64.4 F1-score and a 22.8% character error rate (CER),respectively, which outperforms the baseline results with 48.5 F1-score and50.6% CER. The best model, which splits the full image into 15 equally sizedchunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre orpost-processing of the output. Moreover, the characters in the generateddocument-level sequences are arranged in the reading order, which is practicalfor real-world applications.</description><author>Hongkuan Zhang, Edward Whittaker, Ikuo Kitagishi</author><pubDate>Mon, 16 Oct 2023 17:11:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05525v3</guid></item><item><title>Microscaling Data Formats for Deep Learning</title><link>http://arxiv.org/abs/2310.10537v1</link><description>Narrow bit-width data formats are key to reducing the computational andstorage costs of modern deep learning applications. This paper evaluatesMicroscaling (MX) data formats that combine a per-block scaling factor withnarrow floating-point and integer types for individual elements.MX formatsbalance the competing needs of hardware efficiency, model accuracy, and userfriction. Empirical results on over two dozen benchmarks demonstratepracticality of MX data formats as a drop-in replacement for baseline FP32 forAI inference and training with low user friction. We also show the firstinstance of training generative language models at sub-8-bit weights,activations, and gradients with minimal accuracy loss and no modifications tothe training recipe.</description><author>Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, Stosic Dusan, Venmugil Elango, Maximilian Golub, Alexander Heinecke, Phil James-Roxby, Dharmesh Jani, Gaurav Kolhe, Martin Langhammer, Ada Li, Levi Melnick, Maral Mesmakhosroshahi, Andres Rodriguez, Michael Schulte, Rasoul Shafipour, Lei Shao, Michael Siu, Pradeep Dubey, Paulius Micikevicius, Maxim Naumov, Colin Verilli, Ralph Wittig, Eric Chung</author><pubDate>Mon, 16 Oct 2023 17:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10537v1</guid></item><item><title>Comparing Comparators in Generalization Bounds</title><link>http://arxiv.org/abs/2310.10534v1</link><description>We derive generic information-theoretic and PAC-Bayesian generalizationbounds involving an arbitrary convex comparator function, which measures thediscrepancy between the training and population loss. The bounds hold under theassumption that the cumulant-generating function (CGF) of the comparator isupper-bounded by the corresponding CGF within a family of boundingdistributions. We show that the tightest possible bound is obtained with thecomparator being the convex conjugate of the CGF of the bounding distribution,also known as the Cram\'er function. This conclusion applies more broadly togeneralization bounds with a similar structure. This confirms thenear-optimality of known bounds for bounded and sub-Gaussian losses and leadsto novel bounds under other bounding distributions.</description><author>Fredrik Hellström, Benjamin Guedj</author><pubDate>Mon, 16 Oct 2023 17:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10534v1</guid></item><item><title>Label-efficient Segmentation via Affinity Propagation</title><link>http://arxiv.org/abs/2310.10533v1</link><description>Weakly-supervised segmentation with label-efficient sparse annotations hasattracted increasing research attention to reduce the cost of laboriouspixel-wise labeling process, while the pairwise affinity modeling techniquesplay an essential role in this task. Most of the existing approaches focus onusing the local appearance kernel to model the neighboring pairwise potentials.However, such a local operation fails to capture the long-range dependenciesand ignores the topology of objects. In this work, we formulate the affinitymodeling as an affinity propagation process, and propose a local and a globalpairwise affinity terms to generate accurate soft pseudo labels. An efficientalgorithm is also developed to reduce significantly the computational cost. Theproposed approach can be conveniently plugged into existing segmentationnetworks. Experiments on three typical label-efficient segmentation tasks, i.e.box-supervised instance segmentation, point/scribble-supervised semanticsegmentation and CLIP-guided semantic segmentation, demonstrate the superiorperformance of the proposed approach.</description><author>Wentong Li, Yuqian Yuan, Song Wang, Wenyu Liu, Dongqi Tang, Jian Liu, Jianke Zhu, Lei Zhang</author><pubDate>Mon, 16 Oct 2023 16:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10533v1</guid></item><item><title>One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning</title><link>http://arxiv.org/abs/2306.07967v2</link><description>We present Generalized LoRA (GLoRA), an advanced approach for universalparameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA),GLoRA employs a generalized prompt module to optimize pre-trained model weightsand adjust intermediate activations, providing more flexibility and capabilityacross diverse tasks and datasets. Moreover, GLoRA facilitates efficientparameter adaptation by employing a scalable, modular, layer-wise structuresearch that learns individual adapter of each layer. Originating from a unifiedmathematical formulation, GLoRA exhibits strong transfer learning, few-shotlearning and domain generalization abilities, as it adapts to new tasks throughnot only weights but also additional dimensions like activations. Comprehensiveexperiments demonstrate that GLoRA outperforms all previous methods in natural,specialized, and structured vision benchmarks, achieving superior accuracy withfewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2also show considerable enhancements compared to the original LoRA in thelanguage domain. Furthermore, our structural re-parameterization design ensuresthat GLoRA incurs no extra inference cost, rendering it a practical solutionfor resource-limited applications. Code and models are available at:https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.</description><author>Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, Zhiqiang Shen</author><pubDate>Mon, 16 Oct 2023 16:52:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07967v2</guid></item><item><title>One For All &amp; All For One: Bypassing Hyperparameter Tuning with Model Averaging For Cross-Lingual Transfer</title><link>http://arxiv.org/abs/2310.10532v1</link><description>Multilingual language models enable zero-shot cross-lingual transfer(ZS-XLT): fine-tuned on sizable source-language task data, they perform thetask in target languages without labeled instances. The effectiveness of ZS-XLThinges on the linguistic proximity between languages and the amount ofpretraining data for a language. Because of this, model selection based onsource-language validation is unreliable: it picks model snapshots withsuboptimal target-language performance. As a remedy, some work optimizes ZS-XLTby extensively tuning hyperparameters: the follow-up work then routinelystruggles to replicate the original results. Other work searches over narrowerhyperparameter grids, reporting substantially lower performance. In this work,we therefore propose an unsupervised evaluation protocol for ZS-XLT thatdecouples performance maximization from hyperparameter tuning. As a robust andmore transparent alternative to extensive hyperparameter tuning, we propose toaccumulatively average snapshots from different runs into a single model. Werun broad ZS-XLT experiments on both higher-level semantic tasks (NLI,extractive QA) and a lower-level token classification task (NER) and find thatconventional model selection based on source-language validation quicklyplateaus to suboptimal ZS-XLT performance. On the other hand, our accumulativerun-by-run averaging of models trained with different hyperparameters boostsZS-XLT performance and closely correlates with "oracle" ZS-XLT, i.e., modelselection based on target-language validation performance.</description><author>Fabian David Schmidt, Ivan Vulić, Goran Glavaš</author><pubDate>Mon, 16 Oct 2023 16:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10532v1</guid></item><item><title>Learning optimal integration of spatial and temporal information in noisy chemotaxis</title><link>http://arxiv.org/abs/2310.10531v1</link><description>We investigate the boundary between chemotaxis driven by spatial estimationof gradients and chemotaxis driven by temporal estimation. While it is wellknown that spatial chemotaxis becomes disadvantageous for small organisms athigh noise levels, it is unclear whether there is a discontinuous switch ofoptimal strategies or a continuous transition exists. Here, we employ deepreinforcement learning to study the possible integration of spatial andtemporal information in an a priori unconstrained manner. We parameterize sucha combined chemotactic policy by a recurrent neural network and evaluate itusing a minimal theoretical model of a chemotactic cell. By comparing withconstrained variants of the policy, we show that it converges to purelytemporal and spatial strategies at small and large cell sizes, respectively. Wefind that the transition between the regimes is continuous, with the combinedstrategy outperforming in the transition region both the constrained variantsas well as models that explicitly integrate spatial and temporal information.Finally, by utilizing the attribution method of integrated gradients, we showthat the policy relies on a non-trivial combination of spatially and temporallyderived gradient information in a ratio that varies dynamically during thechemotactic trajectories.</description><author>Albert Alonso, Julius B. Kirkegaard</author><pubDate>Mon, 16 Oct 2023 16:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10531v1</guid></item><item><title>Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models</title><link>http://arxiv.org/abs/2305.15171v3</link><description>We introduce Deceptive-NeRF, a novel methodology for few-shot NeRFreconstruction, which leverages diffusion models to synthesize plausiblepseudo-observations to improve the reconstruction. This approach unfoldsthrough three key steps: 1) reconstructing a coarse NeRF from sparse inputdata; 2) utilizing the coarse NeRF to render images and subsequently generatingpseudo-observations based on them; 3) training a refined NeRF model utilizinginput images augmented with pseudo-observations. We develop a deceptivediffusion model that adeptly transitions RGB images and depth maps from coarseNeRFs into photo-realistic pseudo-observations, all while preserving scenesemantics for reconstruction. Furthermore, we propose a progressive strategyfor training the Deceptive-NeRF, using the current NeRF renderings to createpseudo-observations that enhance the next iteration's NeRF. Extensiveexperiments demonstrate that our approach is capable of synthesizingphoto-realistic novel views, even for highly complex scenes with very sparseinputs. Codes will be released.</description><author>Xinhang Liu, Jiaben Chen, Shiu-hong Kao, Yu-Wing Tai, Chi-Keung Tang</author><pubDate>Mon, 16 Oct 2023 16:47:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15171v3</guid></item><item><title>From Spectral Theorem to Statistical Independence with Application to System Identification</title><link>http://arxiv.org/abs/2310.10523v1</link><description>High dimensional random dynamical systems are ubiquitous, including -- butnot limited to -- cyber-physical systems, daily return on different stocks ofS&amp;P 1500 and velocity profile of interacting particle systems aroundMcKeanVlasov limit. Mathematically, underlying phenomenon can be captured via astable $n$-dimensional linear transformation `$A$' and additive randomness.System identification aims at extracting useful information about underlyingdynamical system, given a length $N$ trajectory from it (corresponds to an $n\times N$ dimensional data matrix). We use spectral theorem for non-Hermitianoperators to show that spatio-temperal correlations are dictated by thediscrepancy between algebraic and geometric multiplicity of distincteigenvalues corresponding to state transition matrix. Small discrepancies implythat original trajectory essentially comprises of multiple lower dimensionalrandom dynamical systems living on $A$ invariant subspaces and arestatistically independent of each other. In the process, we provide firstquantitative handle on decay rate of finite powers of state transition matrix$\|A^{k}\|$ . It is shown that when a stable dynamical system has only onedistinct eigenvalue and discrepancy of $n-1$: $\|A\|$ has a dependence on $n$,resulting dynamics are spatially inseparable and consequently there exist atleast one row with covariates of typical size $\Theta\big(\sqrt{N-n+1}$$e^{n}\big)$ i.e., even under stability assumption, covariates can suffer fromcurse of dimensionality. In the light of these findings we set the stage fornon-asymptotic error analysis in estimation of state transition matrix $A$ vialeast squares regression on observed trajectory by showing that element-wiseerror is essentially a variant of well-know Littlewood-Offord problem.</description><author>Muhammad Abdullah Naeem, Amir Khazraei, Miroslav Pajic</author><pubDate>Mon, 16 Oct 2023 16:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10523v1</guid></item><item><title>Reproducing Bayesian Posterior Distributions for Exoplanet Atmospheric Parameter Retrievals with a Machine Learning Surrogate Model</title><link>http://arxiv.org/abs/2310.10521v1</link><description>We describe a machine-learning-based surrogate model for reproducing theBayesian posterior distributions for exoplanet atmospheric parameters derivedfrom transmission spectra of transiting planets with typical retrieval softwaresuch as TauRex. The model is trained on ground truth distributions for sevenparameters: the planet radius, the atmospheric temperature, and the mixingratios for five common absorbers: $H_2O$, $CH_4$, $NH_3$, $CO$ and $CO_2$. Themodel performance is enhanced by domain-inspired preprocessing of the featuresand the use of semi-supervised learning in order to leverage the large amountof unlabelled training data available. The model was among the winningsolutions in the 2023 Ariel Machine Learning Data Challenge.</description><author>Eyup B. Unlu, Roy T. Forestano, Konstantin T. Matchev, Katia Matcheva</author><pubDate>Mon, 16 Oct 2023 16:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10521v1</guid></item><item><title>BEAUTY Powered BEAST</title><link>http://arxiv.org/abs/2103.00674v5</link><description>We study distribution-free goodness-of-fit tests with the proposed BinaryExpansion Approximation of UniformiTY (BEAUTY) approach. This methodgeneralizes the renowned Euler's formula, and approximates the characteristicfunction of any copula through a linear combination of expectations of binaryinteractions from marginal binary expansions. This novel theory enables aunification of many important tests of independence via approximations fromspecific quadratic forms of symmetry statistics, where the deterministic weightmatrix characterizes the power properties of each test. To achieve a robustpower, we examine test statistics with data-adaptive weights, referred to asthe Binary Expansion Adaptive Symmetry Test (BEAST). Using properties of thebinary expansion filtration, we demonstrate that the Neyman-Pearson test ofuniformity can be approximated by an oracle weighted sum of symmetrystatistics. The BEAST with this oracle provides a useful benchmark of feasiblepower. To approach this oracle power, we devise the BEAST through a regularizedresampling approximation of the oracle test. The BEAST improves the empiricalpower of many existing tests against a wide spectrum of common alternatives anddelivers a clear interpretation of dependency forms when significant.</description><author>Kai Zhang, Zhigen Zhao, Wen Zhou</author><pubDate>Mon, 16 Oct 2023 16:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.00674v5</guid></item><item><title>Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking</title><link>http://arxiv.org/abs/2310.10520v1</link><description>Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiringand annotating task-oriented dialogues, which can be time consuming and costly.However, DST extends beyond simple slot-filling and requires effective updatingstrategies for tracking dialogue state as conversations progress. In thispaper, we propose ParsingDST, a new In-Context Learning (ICL) method, tointroduce additional intricate updating strategies in zero-shot DST. Ourapproach reformulates the DST task by leveraging powerful Large Language Models(LLMs) and translating the original dialogue text to JSON through semanticparsing as an intermediate state. We also design a novel framework thatincludes more modules to ensure the effectiveness of updating strategies in thetext-to-JSON process. Experimental results demonstrate that our approachoutperforms existing zero-shot DST methods on MultiWOZ, exhibiting significantimprovements in Joint Goal Accuracy (JGA) and slot accuracy compared toexisting ICL methods.</description><author>Yuxiang Wu, Guanting Dong, Weiran Xu</author><pubDate>Mon, 16 Oct 2023 16:38:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10520v1</guid></item><item><title>Distribution prediction for image compression: An experimental re-compressor for JPEG images</title><link>http://arxiv.org/abs/2310.10517v1</link><description>We propose a new scheme to re-compress JPEG images in a lossless way. Using aJPEG image as an input the algorithm partially decodes the signal to obtainquantized DCT coefficients and then re-compress them in a more effective way.</description><author>Maxim Koroteev, Yaroslav Borisov, Pavel Frolov</author><pubDate>Mon, 16 Oct 2023 16:33:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10517v1</guid></item><item><title>Unifying Image Processing as Visual Prompting Question Answering</title><link>http://arxiv.org/abs/2310.10513v1</link><description>Image processing is a fundamental task in computer vision, which aims atenhancing image quality and extracting essential features for subsequent visionapplications. Traditionally, task-specific models are developed for individualtasks and designing such models requires distinct expertise. Building upon thesuccess of large language models (LLMs) in natural language processing (NLP),there is a similar trend in computer vision, which focuses on developinglarge-scale models through pretraining and in-context learning. This paradigmshift reduces the reliance on task-specific models, yielding a powerful unifiedmodel to deal with various tasks. However, these advances have predominantlyconcentrated on high-level vision tasks, with less attention paid to low-levelvision tasks. To address this issue, we propose a universal model for generalimage processing that covers image restoration, image enhancement, imagefeature extraction tasks, \textit{etc}. Our proposed framework, namedPromptGIP, unifies these diverse image processing tasks within a universalframework. Inspired by NLP question answering (QA) techniques, we employ avisual prompting question answering paradigm. Specifically, we treat theinput-output image pair as a structured question-answer sentence, therebyreprogramming the image processing task as a prompting QA problem. PromptGIPcan undertake diverse \textbf{cross-domain} tasks using provided visualprompts, eliminating the need for task-specific finetuning. Our methodologyoffers a universal and adaptive solution to general image processing. WhilePromptGIP has demonstrated a certain degree of out-of-domain taskgeneralization capability, further research is expected to fully explore itsmore powerful emergent generalization.</description><author>Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong</author><pubDate>Mon, 16 Oct 2023 16:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10513v1</guid></item><item><title>Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for Traffic Flow Prediction</title><link>http://arxiv.org/abs/2310.08328v2</link><description>As a core technology of Intelligent Transportation System (ITS), traffic flowprediction has a wide range of applications. Traffic flow data arespatial-temporal, which are not only correlated to spatial locations in roadnetworks, but also vary with temporal time indices. Existing methods havesolved the challenges in traffic flow prediction partly, focusing on modelingspatial-temporal dependencies effectively, while not all intrinsic propertiesof traffic flow data are utilized fully. Besides, there are very few attemptsat incremental learning of spatial-temporal data mining, and few previous workscan be easily transferred to the traffic flow prediction task. Motivated by thechallenge of incremental learning methods for traffic flow prediction and theunderutilization of intrinsic properties of road networks, we propose aTransport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer)for traffic flow prediction. Specifically, we first design a novel spatialself-attention module to capture the dynamic spatial dependencies. Three graphmasking matrices are integrated into spatial self-attentions to highlight bothshort- and long-term dependences. Additionally, we employ a temporalself-attention module to detect dynamic temporal patterns in the traffic flowdata. Finally, we design an extra spatial-temporal knowledge distillationmodule for incremental learning of traffic flow prediction tasks. Throughextensive experiments, we show the effectiveness of H-STFormer in normal andincremental traffic flow prediction tasks. The code is available athttps://github.com/Fantasy-Shaw/H-STFormer.</description><author>Xiao Xu, Lei Zhang, Bailong Liu, Zhizhen Liang, Xuefei Zhang</author><pubDate>Mon, 16 Oct 2023 16:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08328v2</guid></item><item><title>ReMax: A Simple, Effective, and Efficient Method for Aligning Large Language Models</title><link>http://arxiv.org/abs/2310.10505v1</link><description>Alignment is of critical importance for training large language models(LLMs). The predominant strategy to address this is through ReinforcementLearning from Human Feedback (RLHF), where PPO serves as the de-factoalgorithm. Yet, PPO is known to suffer from computational inefficiency, achallenge that this paper aims to address. We identify three importantproperties in RLHF tasks: fast simulation, deterministic transitions, andtrajectory-level rewards, which are not leveraged in PPO. Based on suchobservations, we develop a new algorithm tailored for RLHF, called ReMax. Thealgorithm design of ReMax is built on a celebrated algorithm REINFORCE but isequipped with a new variance-reduction technique. Our method has three-fold advantages over PPO: first, it saves about 50%memory usage in principle. As a result, PPO runs out-of-memory when fine-tuninga Llama2 (7B) model on 8xA100-40GB GPUs, whereas ReMax can afford training.This memory improvement is achieved by removing the value model in PPO. Second,ReMax is simple to implement and removes many hyper-parameters in PPO, whichare scale-sensitive and laborious to tune. Third, on GPT2 (137M), we observe2.2x speed-up in terms of wall-clock time. Importantly, the above computationalimprovements do not sacrifice the performance. We hypothesize these advantagescan be maintained in larger-scaled models. Our implementation of ReMax isavailable at https://github.com/liziniu/ReMax</description><author>Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo</author><pubDate>Mon, 16 Oct 2023 16:25:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10505v1</guid></item><item><title>Sharp Bounds for Generalized Causal Sensitivity Analysis</title><link>http://arxiv.org/abs/2305.16988v2</link><description>Causal inference from observational data is crucial for many disciplines suchas medicine and economics. However, sharp bounds for causal effects underrelaxations of the unconfoundedness assumption (causal sensitivity analysis)are subject to ongoing research. So far, works with sharp bounds are restrictedto fairly simple settings (e.g., a single binary treatment). In this paper, wepropose a unified framework for causal sensitivity analysis under unobservedconfounding in various settings. For this, we propose a flexible generalizationof the marginal sensitivity model (MSM) and then derive sharp bounds for alarge class of causal effects. This includes (conditional) average treatmenteffects, effects for mediation analysis and path analysis, and distributionaleffects. Furthermore, our sensitivity model is applicable to discrete,continuous, and time-varying treatments. It allows us to interpret the partialidentification problem under unobserved confounding as a distribution shift inthe latent confounders while evaluating the causal effect of interest. In thespecial case of a single binary treatment, our bounds for (conditional) averagetreatment effects coincide with recent optimality results for causalsensitivity analysis. Finally, we propose a scalable algorithm to estimate oursharp bounds from observational data.</description><author>Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel</author><pubDate>Mon, 16 Oct 2023 16:22:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16988v2</guid></item><item><title>NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails</title><link>http://arxiv.org/abs/2310.10501v1</link><description>NeMo Guardrails is an open-source toolkit for easily adding programmableguardrails to LLM-based conversational systems. Guardrails (or rails for short)are a specific way of controlling the output of an LLM, such as not talkingabout topics considered harmful, following a predefined dialogue path, using aparticular language style, and more. There are several mechanisms that allowLLM providers and developers to add guardrails that are embedded into aspecific model at training, e.g. using model alignment. Differently, using aruntime inspired from dialogue management, NeMo Guardrails allows developers toadd programmable rails to LLM applications - these are user-defined,independent of the underlying LLM, and interpretable. Our initial results showthat the proposed approach can be used with several LLM providers to developcontrollable and safe LLM applications using programmable rails.</description><author>Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen</author><pubDate>Mon, 16 Oct 2023 16:20:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10501v1</guid></item><item><title>Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies</title><link>http://arxiv.org/abs/2310.10500v1</link><description>Forecasting models for systematic trading strategies do not adapt quicklywhen financial market conditions change, as was seen in the advent of theCOVID-19 pandemic in 2020, when market conditions changed dramatically causingmany forecasting models to take loss-making positions. To deal with suchsituations, we propose a novel time-series trend-following forecaster that isable to quickly adapt to new market conditions, referred to as regimes. Weleverage recent developments from the deep learning community and use few-shotlearning. We propose the Cross Attentive Time-Series Trend Network - X-Trend -which takes positions attending over a context set of financial time-seriesregimes. X-Trend transfers trends from similar patterns in the context set tomake predictions and take positions for a new distinct target regime. X-Trendis able to quickly adapt to new financial regimes with a Sharpe ratio increaseof 18.9% over a neural forecaster and 10-fold over a conventional Time-seriesMomentum strategy during the turbulent market period from 2018 to 2023. Ourstrategy recovers twice as quickly from the COVID-19 drawdown compared to theneural-forecaster. X-Trend can also take zero-shot positions on novel unseenfinancial assets obtaining a 5-fold Sharpe ratio increase versus a neuraltime-series trend forecaster over the same period. X-Trend both forecastsnext-day prices and outputs a trading signal. Furthermore, the cross-attentionmechanism allows us to interpret the relationship between forecasts andpatterns in the context set.</description><author>Kieran Wood, Samuel Kessler, Stephen J. Roberts, Stefan Zohren</author><pubDate>Mon, 16 Oct 2023 16:20:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10500v1</guid></item><item><title>LocSelect: Target Speaker Localization with an Auditory Selective Hearing Mechanism</title><link>http://arxiv.org/abs/2310.10497v1</link><description>The prevailing noise-resistant and reverberation-resistant localizationalgorithms primarily emphasize separating and providing directional output foreach speaker in multi-speaker scenarios, without association with the identityof speakers.In this paper, we present a target speaker localization algorithmwith a selective hearing mechanism. Given a reference speech of the targetspeaker, we first produce a speaker-dependent spectrogram mask to eliminateinterfering speakers' speech. Subsequently, a Long short-term memory (LSTM)network is employed to extract the target speaker's location from the filteredspectrogram. Experiments validate the superiority of our proposed method overthe existing algorithms for different scale invariant signal-to-noise ratios(SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelectachieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.</description><author>Yu Chen, Xinyuan Qian, Zexu Pan, Kainan Chen, Haizhou Li</author><pubDate>Mon, 16 Oct 2023 16:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10497v1</guid></item><item><title>Metric Ensembles For Hallucination Detection</title><link>http://arxiv.org/abs/2310.10495v1</link><description>Abstractive text summarization has garnered increased interest as of late, inpart due to the proliferation of large language models (LLMs). One of the mostpressing problems related to generation of abstractive summaries is the need toreduce "hallucinations," information that was not included in the documentbeing summarized, and which may be wholly incorrect. Due to this need, a widearray of metrics estimating consistency with the text being summarized havebeen proposed. We examine in particular a suite of unsupervised metrics forsummary consistency, and measure their correlations with each other and withhuman evaluation scores in the wiki_bio_gpt3_hallucination dataset. We thencompare these evaluations to models made from a simple linear ensemble of thesemetrics. We find that LLM-based methods outperform other unsupervised metricsfor hallucination detection. We also find that ensemble methods can improvethese scores even further, provided that the metrics in the ensemble havesufficiently similar and uncorrelated error rates. Finally, we present anensemble method for LLM-based evaluations that we show improves over thisprevious SOTA.</description><author>Grant C. Forbes, Parth Katlana, Zeydy Ortiz</author><pubDate>Mon, 16 Oct 2023 16:17:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10495v1</guid></item><item><title>Evaluation and improvement of Segment Anything Model for interactive histopathology image segmentation</title><link>http://arxiv.org/abs/2310.10493v1</link><description>With the emergence of the Segment Anything Model (SAM) as a foundationalmodel for image segmentation, its application has been extensively studiedacross various domains, including the medical field. However, its potential inthe context of histopathology data, specifically in region segmentation, hasreceived relatively limited attention. In this paper, we evaluate SAM'sperformance in zero-shot and fine-tuned scenarios on histopathology data, witha focus on interactive segmentation. Additionally, we compare SAM with otherstate-of-the-art interactive models to assess its practical potential andevaluate its generalization capability with domain adaptability. In theexperimental results, SAM exhibits a weakness in segmentation performancecompared to other models while demonstrating relative strengths in terms ofinference time and generalization capability. To improve SAM's limited localrefinement ability and to enhance prompt stability while preserving its corestrengths, we propose a modification of SAM's decoder. The experimental resultssuggest that the proposed modification is effective to make SAM useful forinteractive histology image segmentation. The code is available at\url{https://github.com/hvcl/SAM_Interactive_Histopathology}</description><author>SeungKyu Kim, Hyun-Jic Oh, Seonghui Min, Won-Ki Jeong</author><pubDate>Mon, 16 Oct 2023 16:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10493v1</guid></item><item><title>UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking</title><link>http://arxiv.org/abs/2310.10492v1</link><description>Previous zero-shot dialogue state tracking (DST) methods only apply transferlearning, but ignore unlabelled data in the target domain. We transformzero-shot DST into few-shot DST by utilising such unlabelled data via joint andself-training methods. Our method incorporates auxiliary tasks that generateslot types as inverse prompts for main tasks, creating slot values during jointtraining. Cycle consistency between these two tasks enables the generation andselection of quality samples in unknown target domains for subsequentfine-tuning. This approach also facilitates automatic label creation, therebyoptimizing the training and fine-tuning of DST models. We demonstrate thismethod's effectiveness on large language models in zero-shot scenarios,improving average joint goal accuracy by $8\%$ across all domains in MultiWOZ.</description><author>Chuang Li, Yan Zhang, Min-Yen Kan, Haizhou Li</author><pubDate>Mon, 16 Oct 2023 16:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10492v1</guid></item><item><title>On the Transferability of Learning Models for Semantic Segmentation for Remote Sensing Data</title><link>http://arxiv.org/abs/2310.10490v1</link><description>Recent deep learning-based methods outperform traditional learning methods onremote sensing (RS) semantic segmentation/classification tasks. However, theyrequire large training datasets and are generally known for lack oftransferability due to the highly disparate RS image content across differentgeographical regions. Yet, there is no comprehensive analysis of theirtransferability, i.e., to which extent a model trained on a source domain canbe readily applicable to a target domain. Therefore, in this paper, we aim toinvestigate the raw transferability of traditional and deep learning (DL)models, as well as the effectiveness of domain adaptation (DA) approaches inenhancing the transferability of the DL models (adapted transferability). Byutilizing four highly diverse RS datasets, we train six models with and withoutthree DA approaches to analyze their transferability between these datasetsquantitatively. Furthermore, we developed a straightforward method to quantifythe transferability of a model using the spectral indices as a medium and havedemonstrated its effectiveness in evaluating the model transferability at thetarget domain when the labels are unavailable. Our experiments yield severalgenerally important yet not well-reported observations regarding the raw andadapted transferability. Moreover, our proposed label-free transferabilityassessment method is validated to be better than posterior model confidence.The findings can guide the future development of generalized RS learningmodels. The trained models are released under this link:https://github.com/GDAOSU/Transferability-Remote-Sensing</description><author>Rongjun Qin, Guixiang Zhang, Yang Tang</author><pubDate>Mon, 16 Oct 2023 16:13:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10490v1</guid></item><item><title>H2RBox-v2: Incorporating Symmetry for Boosting Horizontal Box Supervised Oriented Object Detection</title><link>http://arxiv.org/abs/2304.04403v4</link><description>With the rapidly increasing demand for oriented object detection, e.g. inautonomous driving and remote sensing, the recently proposed paradigm involvingweakly-supervised detector H2RBox for learning rotated box (RBox) from the morereadily-available horizontal box (HBox) has shown promise. This paper presentsH2RBox-v2, to further bridge the gap between HBox-supervised andRBox-supervised oriented object detection. Specifically, we propose to leveragethe reflection symmetry via flip and rotate consistencies, using aweakly-supervised network branch similar to H2RBox, together with a novelself-supervised branch that learns orientations from the symmetry inherent invisual objects. The detector is further stabilized and enhanced by practicaltechniques to cope with peripheral issues e.g. angular periodicity. To our bestknowledge, H2RBox-v2 is the first symmetry-aware self-supervised paradigm fororiented object detection. In particular, our method shows less susceptibilityto low-quality annotation and insufficient training data compared to H2RBox.Specifically, H2RBox-v2 achieves very close performance to a rotationannotation trained counterpart -- Rotated FCOS: 1) DOTA-v1.0/1.5/2.0:72.31%/64.76%/50.33% vs. 72.44%/64.53%/51.77%; 2) HRSC: 89.66% vs. 88.99%; 3)FAIR1M: 42.27% vs. 41.25%.</description><author>Yi Yu, Xue Yang, Qingyun Li, Yue Zhou, Gefan Zhang, Feipeng Da, Junchi Yan</author><pubDate>Mon, 16 Oct 2023 16:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04403v4</guid></item><item><title>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</title><link>http://arxiv.org/abs/2302.02088v3</link><description>Can machines recording an audio-visual scene produce realistic, matchingaudio-visual experiences at novel positions and novel view directions? Weanswer it by studying a new task -- real-world audio-visual scene synthesis --and a first-of-its-kind NeRF-based approach for multimodal learning.Concretely, given a video recording of an audio-visual scene, the task is tosynthesize new videos with spatial audios along arbitrary novel cameratrajectories in that scene. We propose an acoustic-aware audio generationmodule that integrates prior knowledge of audio propagation into NeRF, in whichwe implicitly associate audio generation with the 3D geometry and materialproperties of a visual environment. Furthermore, we present a coordinatetransformation module that expresses a view direction relative to the soundsource, enabling the model to learn sound source-centric acoustic fields. Tofacilitate the study of this new task, we collect a high-quality Real-WorldAudio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our methodon this real-world dataset and the simulation-based SoundSpaces dataset.</description><author>Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu</author><pubDate>Mon, 16 Oct 2023 16:11:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02088v3</guid></item><item><title>Type-aware Decoding via Explicitly Aggregating Event Information for Document-level Event Extraction</title><link>http://arxiv.org/abs/2310.10487v1</link><description>Document-level event extraction (DEE) faces two main challenges:arguments-scattering and multi-event. Although previous methods attempt toaddress these challenges, they overlook the interference of event-unrelatedsentences during event detection and neglect the mutual interference ofdifferent event roles during argument extraction. Therefore, this paperproposes a novel Schema-based Explicitly Aggregating~(SEA) model to addressthese limitations. SEA aggregates event information into event type and rolerepresentations, enabling the decoding of event records based on specifictype-aware representations. By detecting each event based on its event typerepresentation, SEA mitigates the interference caused by event-unrelatedinformation. Furthermore, SEA extracts arguments for each role based on itsrole-aware representations, reducing mutual interference between differentroles. Experimental results on the ChFinAnn and DuEE-fin datasets show that SEAoutperforms the SOTA methods.</description><author>Gang Zhao, Yidong Shi, Shudong Lu, Xinjie Yang, Guanting Dong, Jian Xu, Xiaocheng Gong, Si Li</author><pubDate>Mon, 16 Oct 2023 16:10:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10487v1</guid></item><item><title>Statistical Guarantees of Group-Invariant GANs</title><link>http://arxiv.org/abs/2305.13517v2</link><description>Group-invariant generative adversarial networks (GANs) are a type of GANs inwhich the generators and discriminators are hardwired with group symmetries.Empirical studies have shown that these networks are capable of learninggroup-invariant distributions with significantly improved data efficiency. Inthis study, we aim to rigorously quantify this improvement by analyzing thereduction in sample complexity for group-invariant GANs. Our findings indicatethat when learning group-invariant distributions, the number of samplesrequired for group-invariant GANs decreases proportionally with a power of thegroup size, and this power depends on the intrinsic dimension of thedistribution's support. To our knowledge, this work presents the firststatistical estimation for group-invariant generative models, specifically forGANs, and it may shed light on the study of other group-invariant generativemodels.</description><author>Ziyu Chen, Markos A. Katsoulakis, Luc Rey-Bellet, Wei Zhu</author><pubDate>Mon, 16 Oct 2023 16:09:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13517v2</guid></item><item><title>Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference</title><link>http://arxiv.org/abs/2302.11944v3</link><description>We present counterfactual situation testing (CST), a causal data miningframework for detecting discrimination in classifiers. CST aims to answer in anactionable and meaningful way the intuitive question "what would have been themodel outcome had the individual, or complainant, been of a different protectedstatus?" It extends the legally-grounded situation testing of Thanh et al.(2011) by operationalizing the notion of fairness given the difference usingcounterfactual reasoning. For any complainant, we find and compare similarprotected and non-protected instances in the dataset used by the classifier toconstruct a control and test group, where a difference between the decisionoutcomes of the two groups implies potential individual discrimination. Unlikesituation testing, which builds both groups around the complainant, we buildthe test group on the complainant's counterfactual generated using causalknowledge. The counterfactual is intended to reflect how the protectedattribute when changed affects the seemingly neutral attributes used by theclassifier, which is taken for granted in many frameworks for discrimination.Under CST, we compare similar individuals within each group but dissimilarindividuals across both groups due to the possible difference between thecomplainant and its counterfactual. Evaluating our framework on twoclassification scenarios, we show that it uncovers a greater number of casesthan situation testing, even when the classifier satisfies the counterfactualfairness condition of Kusner et al. (2017).</description><author>Jose M. Alvarez, Salvatore Ruggieri</author><pubDate>Mon, 16 Oct 2023 16:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11944v3</guid></item><item><title>ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots</title><link>http://arxiv.org/abs/2310.10486v1</link><description>Learning a locomotion policy for quadruped robots has traditionally beenconstrained to specific robot morphology, mass, and size. The learning processmust usually be repeated for every new robot, where hyperparameters and rewardfunction weights must be re-tuned to maximize performance for each new system.Alternatively, attempting to train a single policy to accommodate differentrobot sizes, while maintaining the same degrees of freedom (DoF) andmorphology, requires either complex learning frameworks, or mass, inertia, anddimension randomization, which leads to prolonged training periods. In ourstudy, we show that drawing inspiration from animal motor control allows us toeffectively train a single locomotion policy capable of controlling a diverserange of quadruped robots. These differences encompass a variable number ofDoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass rangespanning from 2 kg to 200 kg, and nominal standing heights ranging from 16 cmto 100 cm. Our policy modulates a representation of the Central PatternGenerator (CPG) in the spinal cord, effectively coordinating both frequenciesand amplitudes of the CPG to produce rhythmic output (Rhythm Generation), whichis then mapped to a Pattern Formation (PF) layer. Across different robots, theonly varying component is the PF layer, which adjusts the scaling parametersfor the stride height and length. Subsequently, we evaluate the sim-to-realtransfer by testing the single policy on both the Unitree Go1 and A1 robots.Remarkably, we observe robust performance, even when adding a 15 kg load,equivalent to 125% of the A1 robot's nominal mass.</description><author>Milad Shafiee, Guillaume Bellegarda, Auke Ijspeert</author><pubDate>Mon, 16 Oct 2023 16:06:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10486v1</guid></item><item><title>Passive Inference Attacks on Split Learning via Adversarial Regularization</title><link>http://arxiv.org/abs/2310.10483v1</link><description>Split Learning (SL) has emerged as a practical and efficient alternative totraditional federated learning. While previous attempts to attack SL have oftenrelied on overly strong assumptions or targeted easily exploitable models, weseek to develop more practical attacks. We introduce SDAR, a novel attackframework against SL with an honest-but-curious server. SDAR leveragesauxiliary data and adversarial regularization to learn a decodable simulator ofthe client's private model, which can effectively infer the client's privatefeatures under the vanilla SL, and both features and labels under the U-shapedSL. We perform extensive experiments in both configurations to validate theeffectiveness of our proposed attacks. Notably, in challenging but practicalscenarios where existing passive attacks struggle to reconstruct the client'sprivate data effectively, SDAR consistently achieves attack performancecomparable to active attacks. On CIFAR-10, at the deep split level of 7, SDARachieves private feature reconstruction with less than 0.025 mean squared errorin both the vanilla and the U-shaped SL, and attains a label inference accuracyof over 98% in the U-shaped setting, while existing attacks fail to producenon-trivial results.</description><author>Xiaochen Zhu, Xinjian Luo, Yuncheng Wu, Yangfan Jiang, Xiaokui Xiao, Beng Chin Ooi</author><pubDate>Mon, 16 Oct 2023 16:03:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10483v1</guid></item><item><title>xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection</title><link>http://arxiv.org/abs/2310.10482v1</link><description>Widely used learned metrics for machine translation evaluation, such as COMETand BLEURT, estimate the quality of a translation hypothesis by providing asingle sentence-level score. As such, they offer little insight intotranslation errors (e.g., what are the errors and what is their severity). Onthe other hand, generative large language models (LLMs) are amplifying theadoption of more granular strategies to evaluation, attempting to detail andcategorize translation errors. In this work, we introduce xCOMET, anopen-source learned metric designed to bridge the gap between these approaches.xCOMET integrates both sentence-level evaluation and error span detectioncapabilities, exhibiting state-of-the-art performance across all types ofevaluation (sentence-level, system-level, and error span detection). Moreover,it does so while highlighting and categorizing error spans, thus enriching thequality assessment. We also provide a robustness analysis with stress tests,and show that xCOMET is largely capable of identifying localized criticalerrors and hallucinations.</description><author>Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, André F. T. Martins</author><pubDate>Mon, 16 Oct 2023 16:03:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10482v1</guid></item><item><title>DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction</title><link>http://arxiv.org/abs/2310.10481v1</link><description>Most current Event Extraction (EE) methods focus on the high-resourcescenario, which requires a large amount of annotated data and can hardly beapplied to low-resource domains. To address EE more effectively with limitedresources, we propose the Demonstration-enhanced Schema-guided Generation(DemoSG) model, which benefits low-resource EE from two aspects: Firstly, wepropose the demonstration-based learning paradigm for EE to fully use theannotated data, which transforms them into demonstrations to illustrate theextraction process and help the model learn effectively. Secondly, we formulateEE as a natural language generation task guided by schema-based prompts,thereby leveraging label semantics and promoting knowledge transfer inlow-resource scenarios. We conduct extensive experiments under in-domain anddomain adaptation low-resource settings on three datasets, and study therobustness of DemoSG. The results show that DemoSG significantly outperformscurrent methods in low-resource scenarios.</description><author>Gang Zhao, Xiaocheng Gong, Xinjie Yang, Guanting Dong, Shudong Lu, Si Li</author><pubDate>Mon, 16 Oct 2023 16:02:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10481v1</guid></item><item><title>G-SPEED: General SParse Efficient Editing MoDel</title><link>http://arxiv.org/abs/2310.10480v1</link><description>Large Language Models~(LLMs) have demonstrated incredible capabilities inunderstanding, generating, and manipulating languages. Through human-modelinteractions, LLMs can automatically understand human-issued instructions andoutput the expected contents, which can significantly increase workingefficiency. In various types of real-world demands, editing-oriented tasksaccount for a considerable proportion, which involves an interactive processthat entails the continuous refinement of existing texts to meet specificcriteria. Due to the need for multi-round human-model interaction and thegeneration of complicated editing tasks, there is an emergent need forefficient general editing models. In this paper, we propose\underline{\textbf{G}}eneral \underline{\textbf{SP}}arse\underline{\textbf{E}}fficient \underline{\textbf{E}}ditingMo\underline{\textbf{D}}el~(\textbf{G-SPEED}), which can fulfill diverseediting requirements through a single model while maintaining low computationalcosts. Specifically, we first propose a novel unsupervised text editing dataclustering algorithm to deal with the data scarcity problem. Subsequently, weintroduce a sparse editing model architecture to mitigate the inherentlylimited learning capabilities of small language models. The experimentaloutcomes indicate that G-SPEED, with its 508M parameters, can surpass LLMsequipped with 175B parameters. Our code and model checkpoints are available at\url{https://github.com/Banner-Z/G-SPEED}.</description><author>Haoke Zhang, Yue Wang, Juntao Li, Xiabing Zhou, Min Zhang</author><pubDate>Mon, 16 Oct 2023 16:01:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10480v1</guid></item><item><title>Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis</title><link>http://arxiv.org/abs/2310.10477v1</link><description>The rapid advancement of large language models (LLMs) presents bothopportunities and challenges, particularly concerning unintentional generationof harmful and toxic responses. While the traditional alignment methods striveto steer LLMs towards desired performance and shield them from maliciouscontent, this study proposes a novel alignment strategy rooted in mistakeanalysis by exposing LLMs to flawed outputs purposefully and then conducting athorough assessment to fully comprehend internal reasons via natural languageanalysis. Thus, toxic responses can be transformed into instruction tuningcorpus for model alignment, and LLMs can not only be deterred from generatingflawed responses but also trained to self-criticize, leveraging its innateability to discriminate toxic content. Experimental results demonstrate thatthe proposed method outperforms conventional alignment techniques for safetyinstruction following, while maintaining superior efficiency.</description><author>Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu</author><pubDate>Mon, 16 Oct 2023 15:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10477v1</guid></item></channel></rss>