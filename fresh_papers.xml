<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 27 Oct 2025 13:45:40 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Equivariance by Contrast: Identifiable Equivariant Embeddings from Unlabeled Finite Group Actions</title><link>http://arxiv.org/abs/2510.21706v1</link><description>We propose Equivariance by Contrast (EbC) to learn equivariant embeddingsfrom observation pairs $(\mathbf{y}, g \cdot \mathbf{y})$, where $g$ is drawnfrom a finite group acting on the data. Our method jointly learns a latentspace and a group representation in which group actions correspond toinvertible linear maps -- without relying on group-specific inductive biases.We validate our approach on the infinite dSprites dataset with structuredtransformations defined by the finite group $G:= (R_m \times \mathbb{Z}_n\times \mathbb{Z}_n)$, combining discrete rotations and periodic translations.The resulting embeddings exhibit high-fidelity equivariance, with groupoperations faithfully reproduced in latent space. On synthetic data, we furthervalidate the approach on the non-abelian orthogonal group $O(n)$ and thegeneral linear group $GL(n)$. We also provide a theoretical proof foridentifiability. While broad evaluation across diverse group types onreal-world data remains future work, our results constitute the firstsuccessful demonstration of general-purpose encoder-only equivariant learningfrom group action observations alone, including non-trivial non-abelian groupsand a product group motivated by modeling affine equivariances in computervision.</description><author>Tobias Schmidt, Steffen Schneider, Matthias Bethge</author><pubDate>Fri, 24 Oct 2025 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21706v1</guid></item><item><title>Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent</title><link>http://arxiv.org/abs/2510.21704v1</link><description>When a vision model performs image recognition, which visual attributes driveits predictions? Detecting unintended reliance on specific visual features iscritical for ensuring model robustness, preventing overfitting, and avoidingspurious correlations. We introduce an automated framework for detecting suchdependencies in trained vision models. At the core of our method is aself-reflective agent that systematically generates and tests hypotheses aboutvisual attributes that a model may rely on. This process is iterative: theagent refines its hypotheses based on experimental outcomes and uses aself-evaluation protocol to assess whether its findings accurately explainmodel behavior. When inconsistencies arise, the agent self-reflects over itsfindings and triggers a new cycle of experimentation. We evaluate our approachon a novel benchmark of 130 models designed to exhibit diverse visual attributedependencies across 18 categories. Our results show that the agent'sperformance consistently improves with self-reflection, with a significantperformance increase over non-reflective baselines. We further demonstrate thatthe agent identifies real-world visual attribute dependencies instate-of-the-art models, including CLIP's vision encoder and the YOLOv8 objectdetector.</description><author>Christy Li, Josep Lopez Camu√±as, Jake Thomas Touchet, Jacob Andreas, Agata Lapedriza, Antonio Torralba, Tamar Rott Shaham</author><pubDate>Fri, 24 Oct 2025 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21704v1</guid></item><item><title>Visual Diffusion Models are Geometric Solvers</title><link>http://arxiv.org/abs/2510.21697v1</link><description>In this paper we show that visual diffusion models can serve as effectivegeometric solvers: they can directly reason about geometric problems by workingin pixel space. We first demonstrate this on the Inscribed Square Problem, along-standing problem in geometry that asks whether every Jordan curve containsfour points forming a square. We then extend the approach to two otherwell-known hard geometric problems: the Steiner Tree Problem and the SimplePolygon Problem. Our method treats each problem instance as an image and trains a standardvisual diffusion model that transforms Gaussian noise into an imagerepresenting a valid approximate solution that closely matches the exact one.The model learns to transform noisy geometric structures into correctconfigurations, effectively recasting geometric reasoning as image generation. Unlike prior work that necessitates specialized architectures anddomain-specific adaptations when applying diffusion to parametric geometricrepresentations, we employ a standard visual diffusion model that operates onthe visual representation of the problem. This simplicity highlights asurprising bridge between generative modeling and geometric problem solving.Beyond the specific problems studied here, our results point toward a broaderparadigm: operating in image space provides a general and practical frameworkfor approximating notoriously hard problems, and opens the door to tackling afar wider class of challenging geometric tasks.</description><author>Nir Goren, Shai Yehezkel, Omer Dahary, Andrey Voynov, Or Patashnik, Daniel Cohen-Or</author><pubDate>Fri, 24 Oct 2025 17:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21697v1</guid></item><item><title>Causal Climate Emulation with Bayesian Filtering</title><link>http://arxiv.org/abs/2506.09891v2</link><description>Traditional models of climate change use complex systems of coupled equationsto simulate physical processes across the Earth system. These simulations arehighly computationally expensive, limiting our predictions of climate changeand analyses of its causes and effects. Machine learning has the potential toquickly emulate data from climate models, but current approaches are not ableto incorporate physically-based causal relationships. Here, we develop aninterpretable climate model emulator based on causal representation learning.We derive a novel approach including a Bayesian filter for stable long-termautoregressive emulation. We demonstrate that our emulator learns accurateclimate dynamics, and we show the importance of each one of its components on arealistic synthetic dataset and data from two widely deployed climate models.</description><author>Sebastian Hickman, Ilija Trajkovic, Julia Kaltenborn, Francis Pelletier, Alex Archibald, Yaniv Gurwicz, Peer Nowack, David Rolnick, Julien Boussard</author><pubDate>Fri, 24 Oct 2025 17:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.09891v2</guid></item><item><title>BachVid: Training-Free Video Generation with Consistent Background and Character</title><link>http://arxiv.org/abs/2510.21696v1</link><description>Diffusion Transformers (DiTs) have recently driven significant progress intext-to-video (T2V) generation. However, generating multiple videos withconsistent characters and backgrounds remains a significant challenge. Existingmethods typically rely on reference images or extensive training, and oftenonly address character consistency, leaving background consistency toimage-to-video models. We introduce BachVid, the first training-free methodthat achieves consistent video generation without needing any reference images.Our approach is based on a systematic analysis of DiT's attention mechanism andintermediate features, revealing its ability to extract foreground masks andidentify matching points during the denoising process. Our method leveragesthis finding by first generating an identity video and caching the intermediatevariables, and then inject these cached variables into corresponding positionsin newly generated videos, ensuring both foreground and background consistencyacross multiple videos. Experimental results demonstrate that BachVid achievesrobust consistency in generated videos without requiring additional training,offering a novel and efficient solution for consistent video generation withoutrelying on reference images or additional training.</description><author>Han Yan, Xibin Song, Yifu Wang, Hongdong Li, Pan Ji, Chao Ma</author><pubDate>Fri, 24 Oct 2025 17:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21696v1</guid></item><item><title>A Knowledge-Graph Translation Layer for Mission-Aware Multi-Agent Path Planning in Spatiotemporal Dynamics</title><link>http://arxiv.org/abs/2510.21695v1</link><description>The coordination of autonomous agents in dynamic environments is hampered bythe semantic gap between high-level mission objectives and low-level plannerinputs. To address this, we introduce a framework centered on a Knowledge Graph(KG) that functions as an intelligent translation layer. The KG's two-planearchitecture compiles declarative facts into per-agent, mission-aware``worldviews" and physics-aware traversal rules, decoupling mission semanticsfrom a domain-agnostic planner. This allows complex, coordinated paths to bemodified simply by changing facts in the KG. A case study involving AutonomousUnderwater Vehicles (AUVs) in the Gulf of Mexico visually demonstrates theend-to-end process and quantitatively proves that different declarativepolicies produce distinct, high-performing outcomes. This work establishes theKG not merely as a data repository, but as a powerful, stateful orchestratorfor creating adaptive and explainable autonomous systems.</description><author>Edward Holmberg, Elias Ioup, Mahdi Abdelguerfi</author><pubDate>Fri, 24 Oct 2025 17:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21695v1</guid></item><item><title>Mechanistic Interpretability for Neural TSP Solvers</title><link>http://arxiv.org/abs/2510.21693v1</link><description>Neural networks have advanced combinatorial optimization, withTransformer-based solvers achieving near-optimal solutions on the TravelingSalesman Problem (TSP) in milliseconds. However, these models operate as blackboxes, providing no insight into the geometric patterns they learn or theheuristics they employ during tour construction. We address this opacity byapplying sparse autoencoders (SAEs), a mechanistic interpretability technique,to a Transformer-based TSP solver, representing the first application ofactivation-based interpretability methods to operations research models. Wetrain a pointer network with reinforcement learning on 100-node instances, thenfit an SAE to the encoder's residual stream to discover an overcompletedictionary of interpretable features. Our analysis reveals that the solvernaturally develops features mirroring fundamental TSP concepts: boundarydetectors that activate on convex-hull nodes, cluster-sensitive featuresresponding to locally dense regions, and separator features encoding geometricpartitions. These findings provide the first model-internal account of whatneural TSP solvers compute before node selection, demonstrate that geometricstructure emerges without explicit supervision, and suggest pathways towardtransparent hybrid systems that combine neural efficiency with algorithmicinterpretability. Interactive feature explorer:https://reubennarad.github.io/TSP_interp</description><author>Reuben Narad, Leonard Boussioux, Michael Wagner</author><pubDate>Fri, 24 Oct 2025 17:54:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21693v1</guid></item><item><title>Intrinsic Goals for Autonomous Agents: Model-Based Exploration in Virtual Zebrafish Predicts Ethological Behavior and Whole-Brain Dynamics</title><link>http://arxiv.org/abs/2506.00138v2</link><description>Autonomy is a hallmark of animal intelligence, enabling adaptive andintelligent behavior in complex environments without relying on external rewardor task structure. Existing reinforcement learning approaches to exploration inreward-free environments, including a class of methods known as model-basedintrinsic motivation, exhibit inconsistent exploration patterns and do notconverge to an exploratory policy, thus failing to capture robust autonomousbehaviors observed in animals. Moreover, systems neuroscience has largelyoverlooked the neural basis of autonomy, focusing instead on experimentalparadigms where animals are motivated by external reward rather than engagingin ethological, naturalistic and task-independent behavior. To bridge thesegaps, we introduce a novel model-based intrinsic drive explicitly designedafter the principles of autonomous exploration in animals. Our method(3M-Progress) achieves animal-like exploration by tracking divergence betweenan online world model and a fixed prior learned from an ecological niche. Tothe best of our knowledge, we introduce the first autonomous embodied agentthat predicts brain data entirely from self-supervised optimization of anintrinsic goal -- without any behavioral or neural training data --demonstrating that 3M-Progress agents capture the explainable variance inbehavioral patterns and whole-brain neural-glial dynamics recorded fromautonomously behaving larval zebrafish, thereby providing the firstgoal-driven, population-level model of neural-glial computation. Our findingsestablish a computational framework connecting model-based intrinsic motivationto naturalistic behavior, providing a foundation for building artificial agentswith animal-like autonomy.</description><author>Reece Keller, Alyn Kirsch, Felix Pei, Xaq Pitkow, Leo Kozachkov, Aran Nayebi</author><pubDate>Fri, 24 Oct 2025 17:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.00138v2</guid></item><item><title>On Uncertainty Calibration for Equivariant Functions</title><link>http://arxiv.org/abs/2510.21691v1</link><description>Data-sparse settings such as robotic manipulation, molecular physics, andgalaxy morphology classification are some of the hardest domains for deeplearning. For these problems, equivariant networks can help improve modelingacross undersampled parts of the input space, and uncertainty estimation canguard against overconfidence. However, until now, the relationships betweenequivariance and model confidence, and more generally equivariance and modelcalibration, has yet to be studied. Since traditional classification andregression error terms show up in the definitions of calibration error, it isnatural to suspect that previous work can be used to help understand therelationship between equivariance and calibration error. In this work, wepresent a theory relating equivariance to uncertainty estimation. By provinglower and upper bounds on uncertainty calibration errors (ECE and ENCE) undervarious equivariance conditions, we elucidate the generalization limits ofequivariant models and illustrate how symmetry mismatch can result inmiscalibration in both classification and regression. We complement ourtheoretical framework with numerical experiments that clarify the relationshipbetween equivariance and uncertainty using a variety of real and simulateddatasets, and we comment on trends with symmetry mismatch, group size, andaleatoric and epistemic uncertainties.</description><author>Edward Berman, Jacob Ginesin, Marco Pacini, Robin Walters</author><pubDate>Fri, 24 Oct 2025 17:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21691v1</guid></item><item><title>Knee-Deep in C-RASP: A Transformer Depth Hierarchy</title><link>http://arxiv.org/abs/2506.16055v2</link><description>It has been observed that transformers with greater depth (that is, morelayers) have more capabilities, but can we establish formally whichcapabilities are gained? We answer this question with a theoretical prooffollowed by an empirical study. First, we consider transformers that round tofixed precision except inside attention. We show that this subclass oftransformers is expressively equivalent to the programming language C-RASP andthis equivalence preserves depth. Second, we prove that deeper C-RASP programsare more expressive than shallower C-RASP programs, implying that deepertransformers are more expressive than shallower transformers (within thesubclass mentioned above). The same is also proven for transformers withpositional encodings (like RoPE and ALiBi). These results are established bystudying a temporal logic with counting operators equivalent to C-RASP.Finally, we provide empirical evidence that our theory predicts the depthrequired for transformers without positional encodings to length-generalize ona family of sequential dependency tasks.</description><author>Andy Yang, Micha√´l Cadilhac, David Chiang</author><pubDate>Fri, 24 Oct 2025 17:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.16055v2</guid></item><item><title>System-Embedded Diffusion Bridge Models</title><link>http://arxiv.org/abs/2506.23726v2</link><description>Solving inverse problems -- recovering signals from incomplete or noisymeasurements -- is fundamental in science and engineering. Score-basedgenerative models (SGMs) have recently emerged as a powerful framework for thistask. Two main paradigms have formed: unsupervised approaches that adaptpretrained generative models to inverse problems, and supervised bridge methodsthat train stochastic processes conditioned on paired clean and corrupted data.While the former typically assume knowledge of the measurement model, thelatter have largely overlooked this structural information. We introduce Systemembedded Diffusion Bridge Models (SDBs), a new class of supervised bridgemethods that explicitly embed the known linear measurement system into thecoefficients of a matrix-valued SDE. This principled integration yieldsconsistent improvements across diverse linear inverse problems and demonstratesrobust generalization under system misspecification between training anddeployment, offering a promising solution to real-world applications.</description><author>Bartlomiej Sobieski, Matthew Tivnan, Yuang Wang, Siyeop Yoon, Pengfei Jin, Dufan Wu, Quanzheng Li, Przemyslaw Biecek</author><pubDate>Fri, 24 Oct 2025 17:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.23726v2</guid></item><item><title>On Thin Ice: Towards Explainable Conservation Monitoring via Attribution and Perturbations</title><link>http://arxiv.org/abs/2510.21689v1</link><description>Computer vision can accelerate ecological research and conservationmonitoring, yet adoption in ecology lags in part because of a lack of trust inblack-box neural-network-based models. We seek to address this challenge byapplying post-hoc explanations to provide evidence for predictions and documentlimitations that are important to field deployment. Using aerial imagery fromGlacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harborseals) and generate explanations via gradient-based class activation mapping(HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME),and perturbation-based explanations. We assess explanations along three axesrelevant to field use: (i) localization fidelity: whether high-attributionregions coincide with the animal rather than background context; (ii)faithfulness: whether deletion/insertion tests produce changes in detectorconfidence; and (iii) diagnostic utility: whether explanations revealsystematic failure modes. Explanations concentrate on seal torsos and contoursrather than surrounding ice/rock, and removal of the seals reduces detectionconfidence, providing model-evidence for true positives. The analysis alsouncovers recurrent error sources, including confusion between seals and blackice and rocks. We translate these findings into actionable next steps for modeldevelopment, including more targeted data curation and augmentation. By pairingobject detection with post-hoc explainability, we can move beyond "black-box"predictions toward auditable, decision-supporting tools for conservationmonitoring.</description><author>Jiayi Zhou, G√ºnel Aghakishiyeva, Saagar Arya, Julian Dale, James David Poling, Holly R. Houliston, Jamie N. Womble, Gregory D. Larsen, David W. Johnston, Brinnae Bent</author><pubDate>Fri, 24 Oct 2025 17:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21689v1</guid></item><item><title>SimuRA: A World-Model-Driven Simulative Reasoning Architecture for General Goal-Oriented Agents</title><link>http://arxiv.org/abs/2507.23773v2</link><description>AI agents built on foundation models hold enormous promise. Current practice,however, focuses on a one-task-one-agent approach, which not only falls shortof scalability and generality, but also faces practical limitations fromblack-box autoregressive reasoning, where decisions unfold token by tokenwithout explicit simulation or counterfactual evaluation of outcomes. Humans,on the other hand, reason and plan by mentally simulating the consequences ofactions within an internal model of the world -- a capability that supportsflexible, goal-directed behavior across diverse contexts. Moving towards a moregeneral and powerful AI agent, we introduce SimuRA, a goal-orientedarchitecture for generalized agentic reasoning. Based on a principledformulation of an optimal agent in any general environment, SimuRA addressesthe limitations of black-box autoregressive reasoning by incorporating theworld model for planning via simulation. Our prototype world model isimplemented using LLMs as a substrate, leveraging the natural language as adiscrete, hierarchical representation grounded in concepts for planning, whileremaining model-agnostic. On complex web-browsing tasks such as flight search,SimuRA improves the success rate from 0% to 32.2% compared to a representativeopen-web agent baseline. Across tasks, world-model-based planning achieves upto 124% higher task completion rates than a matched black-box autoregressivebaseline, demonstrating the advantages of simulative reasoning. We releaseReasonerAgent-Web, a web-browsing agent built on SimuRA, as an open-sourceresearch demo.</description><author>Mingkai Deng, Jinyu Hou, Zhiting Hu, Eric Xing</author><pubDate>Fri, 24 Oct 2025 17:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.23773v2</guid></item><item><title>Multimodal Datasets with Controllable Mutual Information</title><link>http://arxiv.org/abs/2510.21686v1</link><description>We introduce a framework for generating highly multimodal datasets withexplicitly calculable mutual information between modalities. This enables theconstruction of benchmark datasets that provide a novel testbed for systematicstudies of mutual information estimators and multimodal self-supervisedlearning techniques. Our framework constructs realistic datasets with knownmutual information using a flow-based generative model and a structured causalframework for generating correlated latent variables.</description><author>Raheem Karim Hashmani, Garrett W. Merz, Helen Qu, Mariel Pettee, Kyle Cranmer</author><pubDate>Fri, 24 Oct 2025 17:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21686v1</guid></item><item><title>A4L: An Architecture for AI-Augmented Learning</title><link>http://arxiv.org/abs/2505.06314v2</link><description>AI promises personalized learning and scalable education. As AI agentsincreasingly permeate education in support of teaching and learning, there is acritical and urgent need for data architectures for collecting and analyzingdata on learning, and feeding the results back to teachers, learners, and theAI agents for personalization of learning at scale. At the National AIInstitute for Adult Learning and Online Education, we are developing anArchitecture for AI-Augmented Learning (A4L) for supporting adult learningthrough online education. We present the motivations, goals, requirements ofthe A4L architecture. We describe preliminary applications of A4L and discusshow it advances the goals of making learning more personalized and scalable.</description><author>Ashok Goel, Ploy Thajchayapong, Vrinda Nandan, Harshvardhan Sikka, Spencer Rugaber</author><pubDate>Fri, 24 Oct 2025 17:44:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.06314v2</guid></item><item><title>Federated Unlearning Made Practical: Seamless Integration via Negated Pseudo-Gradients</title><link>http://arxiv.org/abs/2504.05822v2</link><description>The right to be forgotten is a fundamental principle of privacy-preservingregulations and extends to Machine Learning (ML) paradigms such as FederatedLearning (FL). While FL enhances privacy by enabling collaborative modeltraining without sharing private data, trained models still retain theinfluence of training data. Federated Unlearning (FU) methods recently proposedoften rely on impractical assumptions for real-world FL deployments, such asstoring client update histories or requiring access to a publicly availabledataset. To address these constraints, this paper introduces a novel methodthat leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).Our approach only uses standard client model updates, which are employed duringregular FL rounds, and interprets them as pseudo-gradients. When a client needsto be forgotten, we apply the negation of their pseudo-gradients, appropriatelyscaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlesslyintegrates with FL workflows, incurs no additional computational andcommunication overhead beyond standard FL rounds, and supports concurrentunlearning requests. We extensively evaluated the proposed method on twowell-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) anda real-world medical imaging dataset for segmentation (ProstateMRI), usingthree different neural architectures: two residual networks and a visiontransformer. The experimental results across various settings demonstrate thatPUF achieves state-of-the-art forgetting effectiveness and recovery time,without relying on any additional assumptions.</description><author>Alessio Mora, Carlo Mazzocca, Rebecca Montanari, Paolo Bellavista</author><pubDate>Fri, 24 Oct 2025 17:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05822v2</guid></item><item><title>A Geometric Approach to Steerable Convolutions</title><link>http://arxiv.org/abs/2510.18813v2</link><description>In contrast to the somewhat abstract, group theoretical approach adopted bymany papers, our work provides a new and more intuitive derivation of steerableconvolutional neural networks in $d$ dimensions. This derivation is based ongeometric arguments and fundamental principles of pattern matching. We offer anintuitive explanation for the appearance of the Clebsch--Gordan decompositionand spherical harmonic basis functions. Furthermore, we suggest a novel way toconstruct steerable convolution layers using interpolation kernels that improveupon existing implementation, and offer greater robustness to noisy data.</description><author>Soumyabrata Kundu, Risi Kondor</author><pubDate>Fri, 24 Oct 2025 17:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.18813v2</guid></item><item><title>DynamicPAE: Generating Scene-Aware Physical Adversarial Examples in Real-Time</title><link>http://arxiv.org/abs/2412.08053v3</link><description>Physical adversarial examples (PAEs) are regarded as whistle-blowers ofreal-world risks in deep-learning applications, thus worth furtherinvestigation. However, current PAE generation studies show limited adaptiveattacking ability to diverse and varying scenes, revealing the urgentrequirement of dynamic PAEs that are generated in real time and conditioned onthe observation from the attacker. The key challenge in generating dynamic PAEsis learning the sparse relation between PAEs and the observation of attackersunder the noisy feedback of attack training. To address the challenge, wepresent DynamicPAE, the first generative framework that enables scene-awarereal-time physical attacks. Specifically, to address the noisy feedback problemthat obfuscates the exploration of scene-related PAEs, we introduce theresidual-guided adversarial pattern exploration technique. Residual-guidedtraining, which relaxes the attack training with a reconstruction task, isproposed to enrich the feedback information, thereby achieving a morecomprehensive exploration of PAEs. To address the alignment problem between thetrained generator and the real-world scenario, we introduce thedistribution-matched attack scenario alignment, consisting of theconditional-uncertainty-aligned data module and the skewness-aligned objectivere-weighting module. The former aligns the training environment with theincomplete observation of the real-world attacker. The latter facilitatesconsistent stealth control across different attack targets with the skewnesscontroller. Extensive digital and physical evaluations demonstrate the superiorattack performance of DynamicPAE, attaining a 2.07 $\times$ boost (58.8%average AP drop under attack) on representative object detectors (e.g., DETR)over state-of-the-art static PAE generating methods. Overall, our work opensthe door to end-to-end modeling of dynamic PAEs.</description><author>Jin Hu, Xianglong Liu, Jiakai Wang, Junkai Zhang, Xianqi Yang, Haotong Qin, Yuqing Ma, Ke Xu</author><pubDate>Fri, 24 Oct 2025 17:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08053v3</guid></item><item><title>WorldGrow: Generating Infinite 3D World</title><link>http://arxiv.org/abs/2510.21682v1</link><description>We tackle the challenge of generating the infinitely extendable 3D world --large, continuous environments with coherent geometry and realistic appearance.Existing methods face key challenges: 2D-lifting approaches suffer fromgeometric and appearance inconsistencies across views, 3D implicitrepresentations are hard to scale up, and current 3D foundation models aremostly object-centric, limiting their applicability to scene-level generation.Our key insight is leveraging strong generation priors from pre-trained 3Dmodels for structured scene block generation. To this end, we proposeWorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Ourmethod features three core components: (1) a data curation pipeline thatextracts high-quality scene blocks for training, making the 3D structuredlatent representations suitable for scene generation; (2) a 3D block inpaintingmechanism that enables context-aware scene extension; and (3) a coarse-to-finegeneration strategy that ensures both global layout plausibility and localgeometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,WorldGrow achieves SOTA performance in geometry reconstruction, while uniquelysupporting infinite scene generation with photorealistic and structurallyconsistent outputs. These results highlight its capability for constructinglarge-scale virtual environments and potential for building future worldmodels.</description><author>Sikuang Li, Chen Yang, Jiemin Fang, Taoran Yi, Jia Lu, Jiazhong Cen, Lingxi Xie, Wei Shen, Qi Tian</author><pubDate>Fri, 24 Oct 2025 17:39:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.21682v1</guid></item><item><title>Reinforcement Learning with Action Chunking</title><link>http://arxiv.org/abs/2507.07969v3</link><description>We present Q-chunking, a simple yet effective recipe for improvingreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.Our recipe is designed for the offline-to-online RL setting, where the goal isto leverage an offline prior dataset to maximize the sample-efficiency ofonline learning. Effective exploration and sample-efficient learning remaincentral challenges in this setting, as it is not obvious how the offline datashould be utilized to acquire a good exploratory policy. Our key insight isthat action chunking, a technique popularized in imitation learning wheresequences of future actions are predicted rather than a single action at eachtimestep, can be applied to temporal difference (TD)-based RL methods tomitigate the exploration challenge. Q-chunking adopts action chunking bydirectly running RL in a 'chunked' action space, enabling the agent to (1)leverage temporally consistent behaviors from offline data for more effectiveonline exploration and (2) use unbiased $n$-step backups for more stable andefficient TD learning. Our experimental results demonstrate that Q-chunkingexhibits strong offline performance and online sample efficiency, outperformingprior best offline-to-online methods on a range of long-horizon, sparse-rewardmanipulation tasks.</description><author>Qiyang Li, Zhiyuan Zhou, Sergey Levine</author><pubDate>Fri, 24 Oct 2025 17:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.07969v3</guid></item><item><title>HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</title><link>http://arxiv.org/abs/2510.20822v1</link><description>State-of-the-art text-to-video models excel at generating isolated clips butfall short of creating the coherent, multi-shot narratives, which are theessence of storytelling. We bridge this "narrative gap" with HoloCine, a modelthat generates entire scenes holistically to ensure global consistency from thefirst shot to the last. Our architecture achieves precise directorial controlthrough a Window Cross-Attention mechanism that localizes text prompts tospecific shots, while a Sparse Inter-Shot Self-Attention pattern (dense withinshots but sparse between them) ensures the efficiency required for minute-scalegeneration. Beyond setting a new state-of-the-art in narrative coherence,HoloCine develops remarkable emergent abilities: a persistent memory forcharacters and scenes, and an intuitive grasp of cinematic techniques. Our workmarks a pivotal shift from clip synthesis towards automated filmmaking, makingend-to-end cinematic creation a tangible future. Our code is available at:https://holo-cine.github.io/.</description><author>Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, Huamin Qu</author><pubDate>Thu, 23 Oct 2025 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20822v1</guid></item><item><title>One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling</title><link>http://arxiv.org/abs/2505.13358v3</link><description>Diffusion-based generative models have demonstrated exceptional performance,yet their iterative sampling procedures remain computationally expensive. Aprominent strategy to mitigate this cost is distillation, with offlinedistillation offering particular advantages in terms of efficiency, modularity,and flexibility. In this work, we identify two key observations that motivate aprincipled distillation framework: (1) while diffusion models have been viewedthrough the lens of dynamical systems theory, powerful and underexplored toolscan be further leveraged; and (2) diffusion models inherently imposestructured, semantically coherent trajectories in latent space. Building onthese observations, we introduce the Koopman Distillation Model (KDM), a noveloffline distillation approach grounded in Koopman theory - a classicalframework for representing nonlinear dynamics linearly in a transformed space.KDM encodes noisy inputs into an embedded space where a learned linear operatorpropagates them forward, followed by a decoder that reconstructs clean samples.This enables single-step generation while preserving semantic fidelity. Weprovide theoretical justification for our approach: (1) under mild assumptions,the learned diffusion dynamics admit a finite-dimensional Koopmanrepresentation; and (2) proximity in the Koopman latent space correlates withsemantic similarity in the generated outputs, allowing for effective trajectoryalignment. KDM achieves highly competitive performance across standard offlinedistillation benchmarks.</description><author>Nimrod Berman, Ilan Naiman, Moshe Eliasof, Hedi Zisling, Omri Azencot</author><pubDate>Thu, 23 Oct 2025 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.13358v3</guid></item><item><title>Language Models use Lookbacks to Track Beliefs</title><link>http://arxiv.org/abs/2505.14685v2</link><description>How do language models (LMs) represent characters' beliefs, especially whenthose beliefs may differ from reality? This question lies at the heart ofunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze LMs'ability to reason about characters' beliefs using causal mediation andabstraction. We construct a dataset, CausalToM, consisting of simple storieswhere two characters independently change the state of two objects, potentiallyunaware of each other's actions. Our investigation uncovers a pervasivealgorithmic pattern that we call a lookback mechanism, which enables the LM torecall important information when it becomes necessary. The LM binds eachcharacter-object-state triple together by co-locating their referenceinformation, represented as Ordering IDs (OIs), in low-rank subspaces of thestate token's residual stream. When asked about a character's beliefs regardingthe state of an object, the binding lookback retrieves the correct state OI andthen the answer lookback retrieves the corresponding state token. When weintroduce text specifying that one character is (not) visible to the other, wefind that the LM first generates a visibility ID encoding the relation betweenthe observing and the observed character OIs. In a visibility lookback, this IDis used to retrieve information about the observed character and update theobserving character's beliefs. Our work provides insights into belief trackingmechanisms, taking a step toward reverse-engineering ToM reasoning in LMs.</description><author>Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David Bau, Atticus Geiger</author><pubDate>Thu, 23 Oct 2025 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14685v2</guid></item><item><title>LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas</title><link>http://arxiv.org/abs/2510.20820v1</link><description>Despite their impressive visual fidelity, existing personalized generativemodels lack interactive control over spatial composition and scale poorly tomultiple subjects. To address these limitations, we present LayerComposer, aninteractive framework for personalized, multi-subject text-to-image generation.Our approach introduces two main contributions: (1) a layered canvas, a novelrepresentation in which each subject is placed on a distinct layer, enablingocclusion-free composition; and (2) a locking mechanism that preserves selectedlayers with high fidelity while allowing the remaining layers to adapt flexiblyto the surrounding context. Similar to professional image-editing software, theproposed layered canvas allows users to place, resize, or lock input subjectsthrough intuitive layer manipulation. Our versatile locking mechanism requiresno architectural changes, relying instead on inherent positional embeddingscombined with a new complementary data sampling strategy. Extensive experimentsdemonstrate that LayerComposer achieves superior spatial control and identitypreservation compared to the state-of-the-art methods in multi-subjectpersonalized image generation.</description><author>Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tulyakov, Kuan-Chieh Jackson Wang</author><pubDate>Thu, 23 Oct 2025 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20820v1</guid></item><item><title>Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</title><link>http://arxiv.org/abs/2510.20819v1</link><description>Recent advances in generative modeling have positioned diffusion models asstate-of-the-art tools for sampling from complex data distributions. Whilethese models have shown remarkable success across single-modality domains suchas images and audio, extending their capabilities to Modality Translation (MT),translating information across different sensory modalities, remains an openchallenge. Existing approaches often rely on restrictive assumptions, includingshared dimensionality, Gaussian source priors, and modality-specificarchitectures, which limit their generality and theoretical grounding. In thiswork, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), ageneral-purpose framework for modality translation based on a latent-variableextension of Denoising Diffusion Bridge Models. By operating in a shared latentspace, our method learns a bridge between arbitrary modalities withoutrequiring aligned dimensions. We introduce a contrastive alignment loss toenforce semantic consistency between paired samples and design adomain-agnostic encoder-decoder architecture tailored for noise prediction inlatent space. Additionally, we propose a predictive loss to guide trainingtoward accurate cross-domain translation and explore several trainingstrategies to improve stability. Our approach supports arbitrary modality pairsand performs strongly on diverse MT tasks, including multi-view to 3D shapegeneration, image super-resolution, and multi-view scene synthesis.Comprehensive experiments and ablations validate the effectiveness of ourframework, establishing a new strong baseline in general modality translation.For more information, see our project page:https://sites.google.com/view/lddbm/home.</description><author>Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</author><pubDate>Thu, 23 Oct 2025 17:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20819v1</guid></item><item><title>VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation</title><link>http://arxiv.org/abs/2510.20818v1</link><description>A fundamental challenge in robot navigation lies in learning policies thatgeneralize across diverse environments while conforming to the unique physicalconstraints and capabilities of a specific embodiment (e.g., quadrupeds canwalk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA thatdecouples semantic planning from embodiment grounding: a generalist plannerlearns from diverse, open-world data, while a specialist affordance modellearns the robot's physical constraints and capabilities in safe, low-costsimulation. We enabled this separation by carefully designing an interface thatlets a high-level planner propose candidate paths directly in image space thatthe affordance model then evaluates and re-ranks. Our real-world experimentsshow that VAMOS achieves higher success rates in both indoor and complexoutdoor navigation than state-of-the-art model-based and end-to-end learningmethods. We also show that our hierarchical design enables cross-embodiednavigation across legged and wheeled robots and is easily steerable usingnatural language. Real-world ablations confirm that the specialist model is keyto embodiment grounding, enabling a single high-level planner to be deployedacross physically distinct wheeled and legged robots. Finally, this modelsignificantly enhances single-robot reliability, achieving 3X higher successrates by rejecting physically infeasible plans. Website:https://vamos-vla.github.io/</description><author>Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta</author><pubDate>Thu, 23 Oct 2025 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20818v1</guid></item><item><title>KL-Regularized Reinforcement Learning is Designed to Mode Collapse</title><link>http://arxiv.org/abs/2510.20817v1</link><description>It is commonly believed that optimizing the reverse KL divergence results in"mode seeking", while optimizing forward KL results in "mass covering", withthe latter being preferred if the goal is to sample from multiple diversemodes. We show -- mathematically and empirically -- that this intuition doesnot necessarily transfer well to doing reinforcement learning withreverse/forward KL regularization (e.g. as commonly used with language models).Instead, the choice of reverse/forward KL determines the family of optimaltarget distributions, parameterized by the regularization coefficient. Modecoverage depends primarily on other factors, such as regularization strength,and relative scales between rewards and reference probabilities. Further, weshow commonly used settings such as low regularization strength and equalverifiable rewards tend to specify unimodal target distributions, meaning theoptimization objective is, by construction, non-diverse. We leverage theseinsights to construct a simple, scalable, and theoretically justifiedalgorithm. It makes minimal changes to reward magnitudes, yet optimizes for atarget distribution which puts high probability over all high-quality samplingmodes. In experiments, this simple modification works to post-train both LargeLanguage Models and Chemical Language Models to have higher solution qualityand diversity, without any external signals of diversity, and works with bothforward and reverse KL when using either naively fails.</description><author>Anthony GX-Chen, Jatin Prakash, Jeff Guo, Rob Fergus, Rajesh Ranganath</author><pubDate>Thu, 23 Oct 2025 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20817v1</guid></item><item><title>GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</title><link>http://arxiv.org/abs/2510.20813v1</link><description>This paper presents GSWorld, a robust, photo-realistic simulator for roboticsmanipulation that combines 3D Gaussian Splatting with physics engines. Ourframework advocates "closing the loop" of developing manipulation policies withreproducible evaluation of policies learned from real-robot data and sim2realpolicy training without using real robots. To enable photo-realistic renderingof diverse scenes, we propose a new asset format, which we term GSDF (GaussianScene Description File), that infuses Gaussian-on-Mesh representation withrobot URDF and other objects. With a streamlined reconstruction pipeline, wecurate a database of GSDF that contains 3 robot embodiments for single-arm andbimanual manipulation, as well as more than 40 objects. Combining GSDF withphysics engines, we demonstrate several immediate interesting applications: (1)learning zero-shot sim2real pixel-to-action manipulation policy withphoto-realistic rendering, (2) automated high-quality DAgger data collectionfor adapting policies to deployment environments, (3) reproducible benchmarkingof real-robot manipulation policies in simulation, (4) simulation datacollection by virtual teleoperation, and (5) zero-shot sim2real visualreinforcement learning. Website: https://3dgsworld.github.io/.</description><author>Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang</author><pubDate>Thu, 23 Oct 2025 17:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20813v1</guid></item><item><title>SpectraMorph: Structured Latent Learning for Self-Supervised Hyperspectral Super-Resolution</title><link>http://arxiv.org/abs/2510.20814v1</link><description>Hyperspectral sensors capture dense spectra per pixel but suffer from lowspatial resolution, causing blurred boundaries and mixed-pixel effects.Co-registered companion sensors such as multispectral, RGB, or panchromaticcameras provide high-resolution spatial detail, motivating hyperspectralsuper-resolution through the fusion of hyperspectral and multispectral images(HSI-MSI). Existing deep learning based methods achieve strong performance butrely on opaque regressors that lack interpretability and often fail when theMSI has very few bands. We propose SpectraMorph, a physics-guidedself-supervised fusion framework with a structured latent space. Instead ofdirect regression, SpectraMorph enforces an unmixing bottleneck: endmembersignatures are extracted from the low-resolution HSI, and a compact multilayerperceptron predicts abundance-like maps from the MSI. Spectra are reconstructedby linear mixing, with training performed in a self-supervised manner via theMSI sensor's spectral response function. SpectraMorph produces interpretableintermediates, trains in under a minute, and remains robust even with asingle-band (pan-chromatic) MSI. Experiments on synthetic and real-worlddatasets show SpectraMorph consistently outperforming state-of-the-artunsupervised/self-supervised baselines while remaining very competitive againstsupervised baselines.</description><author>Ritik Shah, Marco F Duarte</author><pubDate>Thu, 23 Oct 2025 17:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20814v1</guid></item><item><title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title><link>http://arxiv.org/abs/2510.20812v1</link><description>Large Vision-Language Models (VLMs) have achieved remarkable progress inmultimodal understanding, yet they struggle when reasoning overinformation-intensive images that densely interleave textual annotations withfine-grained graphical elements. The main challenges lie in preciselylocalizing critical cues in dense layouts and multi-hop reasoning to integratedispersed evidence. We propose Speculative Verdict (SV), a training-freeframework inspired by speculative decoding that combines multiple lightweightdraft experts with a large verdict model. In the draft stage, small VLMs act asdraft experts to generate reasoning paths that provide diverse localizationcandidates; in the verdict stage, a strong VLM synthesizes these paths toproduce the final answer, minimizing computational cost while recoveringcorrect answers. To further improve efficiency and accuracy, SV introduces aconsensus expert selection mechanism that forwards only high-agreementreasoning paths to the verdict. Empirically, SV achieves consistent gains onchallenging information-intensive and high-resolution visual question answeringbenchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.By synthesizing correct insights from multiple partially accurate reasoningpaths, SV achieves both error correction and cost-efficiency compared to largeproprietary models or training pipelines. Code is available athttps://github.com/Tinaliu0123/speculative-verdict</description><author>Yuhan Liu, Lianhui Qin, Shengjie Wang</author><pubDate>Thu, 23 Oct 2025 17:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20812v1</guid></item><item><title>On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?</title><link>http://arxiv.org/abs/2510.20810v1</link><description>With the widespread use of large language models (LLMs), many researchershave turned their attention to detecting text generated by them. However, thereis no consistent or precise definition of their target, namely "LLM-generatedtext". Differences in usage scenarios and the diversity of LLMs furtherincrease the difficulty of detection. What is commonly regarded as thedetecting target usually represents only a subset of the text that LLMs canpotentially produce. Human edits to LLM outputs, together with the subtleinfluences that LLMs exert on their users, are blurring the line betweenLLM-generated and human-written text. Existing benchmarks and evaluationapproaches do not adequately address the various conditions in real-worlddetector applications. Hence, the numerical results of detectors are oftenmisunderstood, and their significance is diminishing. Therefore, detectorsremain useful under specific conditions, but their results should beinterpreted only as references rather than decisive indicators.</description><author>Mingmeng Geng, Thierry Poibeau</author><pubDate>Thu, 23 Oct 2025 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20810v1</guid></item><item><title>Real Deep Research for AI, Robotics and Beyond</title><link>http://arxiv.org/abs/2510.20809v1</link><description>With the rapid growth of research in AI and robotics now producing over10,000 papers annually it has become increasingly difficult for researchers tostay up to date. Fast evolving trends, the rise of interdisciplinary work, andthe need to explore domains beyond one's expertise all contribute to thischallenge. To address these issues, we propose a generalizable pipeline capableof systematically analyzing any research area: identifying emerging trends,uncovering cross domain opportunities, and offering concrete starting pointsfor new inquiry. In this work, we present Real Deep Research (RDR) acomprehensive framework applied to the domains of AI and robotics, with aparticular focus on foundation models and robotics advancements. We alsobriefly extend our analysis to other areas of science. The main paper detailsthe construction of the RDR pipeline, while the appendix provides extensiveresults across each analyzed topic. We hope this work sheds light forresearchers working in the field of AI and beyond.</description><author>Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</author><pubDate>Thu, 23 Oct 2025 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20809v1</guid></item><item><title>The Reality Gap in Robotics: Challenges, Solutions, and Best Practices</title><link>http://arxiv.org/abs/2510.20808v1</link><description>Machine learning has facilitated significant advancements across variousrobotics domains, including navigation, locomotion, and manipulation. Many suchachievements have been driven by the extensive use of simulation as a criticaltool for training and testing robotic systems prior to their deployment inreal-world environments. However, simulations consist of abstractions andapproximations that inevitably introduce discrepancies between simulated andreal environments, known as the reality gap. These discrepancies significantlyhinder the successful transfer of systems from simulation to the real world.Closing this gap remains one of the most pressing challenges in robotics.Recent advances in sim-to-real transfer have demonstrated promising resultsacross various platforms, including locomotion, navigation, and manipulation.By leveraging techniques such as domain randomization, real-to-sim transfer,state and action abstractions, and sim-real co-training, many works haveovercome the reality gap. However, challenges persist, and a deeperunderstanding of the reality gap's root causes and solutions is necessary. Inthis survey, we present a comprehensive overview of the sim-to-real landscape,highlighting the causes, solutions, and evaluation metrics for the reality gapand sim-to-real transfer.</description><author>Elie Aljalbout, Jiaxu Xing, Angel Romero, Iretiayo Akinola, Caelan Reed Garrett, Eric Heiden, Abhishek Gupta, Tucker Hermans, Yashraj Narang, Dieter Fox, Davide Scaramuzza, Fabio Ramos</author><pubDate>Thu, 23 Oct 2025 17:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20808v1</guid></item><item><title>Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers</title><link>http://arxiv.org/abs/2510.20807v1</link><description>Inspired by the performance and scalability of autoregressive large languagemodels (LLMs), transformer-based models have seen recent success in the visualdomain. This study investigates a transformer adaptation for video predictionwith a simple end-to-end approach, comparing various spatiotemporalself-attention layouts. Focusing on causal modeling of physical simulationsover time; a common shortcoming of existing video-generative approaches, weattempt to isolate spatiotemporal reasoning via physical object trackingmetrics and unsupervised training on physical simulation datasets. We introducea simple yet effective pure transformer model for autoregressive videoprediction, utilizing continuous pixel-space representations for videoprediction. Without the need for complex training strategies or latentfeature-learning components, our approach significantly extends the timehorizon for physically accurate predictions by up to 50% when compared withexisting latent-space approaches, while maintaining comparable performance oncommon video quality metrics. In addition, we conduct interpretabilityexperiments to identify network regions that encode information useful toperform accurate estimations of PDE simulation parameters via probing models,and find that this generalizes to the estimation of out-of-distributionsimulation parameters. This work serves as a platform for furtherattention-based spatiotemporal modeling of videos via a simple, parameterefficient, and interpretable approach.</description><author>Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed</author><pubDate>Thu, 23 Oct 2025 17:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20807v1</guid></item><item><title>ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</title><link>http://arxiv.org/abs/2510.20803v1</link><description>We propose a novel AutoRegressive Generation-based paradigm for imageSegmentation (ARGenSeg), achieving multimodal understanding and pixel-levelperception within a unified framework. Prior works integrating imagesegmentation into multimodal large language models (MLLMs) typically employeither boundary points representation or dedicated segmentation heads. Thesemethods rely on discrete representations or semantic prompts fed intotask-specific decoders, which limits the ability of the MLLM to capturefine-grained visual details. To address these challenges, we introduce asegmentation framework for MLLM based on image generation, which naturallyproduces dense masks for target objects. We leverage MLLM to output visualtokens and detokenize them into images using an universal VQ-VAE, making thesegmentation fully dependent on the pixel-level understanding of the MLLM. Toreduce inference latency, we employ a next-scale-prediction strategy togenerate required visual tokens in parallel. Extensive experiments demonstratethat our method surpasses prior state-of-the-art approaches on multiplesegmentation datasets with a remarkable boost in inference speed, whilemaintaining strong understanding capabilities.</description><author>Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</author><pubDate>Thu, 23 Oct 2025 17:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20803v1</guid></item><item><title>DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</title><link>http://arxiv.org/abs/2510.02253v2</link><description>Drag-based image editing has long suffered from distortions in the targetregion, largely because the priors of earlier base models, Stable Diffusion,are insufficient to project optimized latents back onto the natural imagemanifold. With the shift from UNet-based DDPMs to more scalable DiT with flowmatching (e.g., SD3.5, FLUX), generative priors have become significantlystronger, enabling advances across diverse editing tasks. However, drag-basedediting has yet to benefit from these stronger priors. This work proposes thefirst framework to effectively harness FLUX's rich prior for drag-basedediting, dubbed DragFlow, achieving substantial gains over baselines. We firstshow that directly applying point-based drag editing to DiTs performs poorly:unlike the highly compressed features of UNets, DiT features are insufficientlystructured to provide reliable guidance for point-wise motion supervision. Toovercome this limitation, DragFlow introduces a region-based editing paradigm,where affine transformations enable richer and more consistent featuresupervision. Additionally, we integrate pretrained open-domain personalizationadapters (e.g., IP-Adapter) to enhance subject consistency, while preservingbackground fidelity through gradient mask-based hard constraints. Multimodallarge language models (MLLMs) are further employed to resolve task ambiguities.For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)featuring region-level dragging instructions. Extensive experiments onDragBench-DR and ReD Bench show that DragFlow surpasses both point-based andregion-based baselines, setting a new state-of-the-art in drag-based imageediting. Code and datasets will be publicly available upon publication.</description><author>Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</author><pubDate>Thu, 23 Oct 2025 17:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02253v2</guid></item><item><title>Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples</title><link>http://arxiv.org/abs/2510.20800v1</link><description>Recently, Sharma et al. suggested a method called Layer-SElective-Rankreduction (LASER) which demonstrated that pruning high-order components ofcarefully chosen LLM's weight matrices can boost downstream accuracy -- withoutany gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (eachrequiring full-dataset forward passes) makes it impractical for rapiddeployment. We demonstrate that this overhead can be removed and find that: (i)Only a small, carefully chosen subset of matrices needs to be inspected --eliminating the layer-by-layer sweep, (ii) The gradient of each matrix'ssingular values pinpoints which matrices merit reduction, (iii) Increasing thefactorization search space by allowing matrices rows to cluster around multiplesubspaces and then decomposing each cluster separately further reducesoverfitting on the original training data and further lifts accuracy by up to24.6 percentage points, and finally, (iv) we discover that evaluating on just100 samples rather than the full training data -- both for computing theindicative gradients and for measuring the final accuracy -- suffices tofurther reduce the search time; we explain that as adaptation to downstreamtasks is dominated by prompting style, not dataset size. As a result, we showthat combining these findings yields a fast and robust adaptation algorithm fordownstream tasks. Overall, with a single gradient step on 100 examples and aquick scan of the top candidate layers and factorization techniques, we canadapt LLMs to new datasets -- entirely without fine-tuning.</description><author>Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</author><pubDate>Thu, 23 Oct 2025 17:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20800v1</guid></item><item><title>Simple Context Compression: Mean-Pooling and Multi-Ratio Training</title><link>http://arxiv.org/abs/2510.20797v1</link><description>A common strategy to reduce the computational costs of using long contexts inretrieval-augmented generation (RAG) with large language models (LLMs) is softcontext compression, where the input sequence is transformed into a shortercontinuous representation. We develop a lightweight and simple mean-poolingapproach that consistently outperforms the widely used compression-tokensarchitecture, and study training the same compressor to output multiplecompression ratios. We conduct extensive experiments across in-domain andout-of-domain QA datasets, as well as across model families, scales, andcompression ratios. Overall, our simple mean-pooling approach achieves thestrongest performance, with a relatively small drop when training for multiplecompression ratios. More broadly though, across architectures and trainingregimes the trade-offs are more nuanced, illustrating the complex landscape ofcompression methods.</description><author>Yair Feldman, Yoav Artzi</author><pubDate>Thu, 23 Oct 2025 17:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20797v1</guid></item><item><title>Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks</title><link>http://arxiv.org/abs/2510.20795v1</link><description>Deep learning has emerged as a transformative methodology in moderncosmology, providing powerful tools to extract meaningful physical informationfrom complex astronomical datasets. This paper implements a novel Bayesiangraph deep learning framework for estimating key cosmological parameters in aprimordial magnetic field (PMF) cosmology directly from simulated CosmicMicrowave Background (CMB) maps. Our methodology utilizes DeepSphere, aspherical convolutional neural network architecture specifically designed torespect the spherical geometry of CMB data through HEALPix pixelization. Toadvance beyond deterministic point estimates and enable robust uncertaintyquantification, we integrate Bayesian Neural Networks (BNNs) into theframework, capturing aleatoric and epistemic uncertainties that reflect themodel confidence in its predictions. The proposed approach demonstratesexceptional performance, achieving $R^{2}$ scores exceeding 0.89 for themagnetic parameter estimation. We further obtain well-calibrated uncertaintyestimates through post-hoc training techniques including Variance Scaling andGPNormal. This integrated DeepSphere-BNNs framework not only delivers accurateparameter estimation from CMB maps with PMF contributions but also providesreliable uncertainty quantification, providing the necessary tools for robustcosmological inference in the era of precision cosmology.</description><author>Juan Alejandro Pinto Castro, H√©ctor J. Hort√∫a, Jorge Enrique Garc√≠a-Farieta, Roger Anderson Hurtado</author><pubDate>Thu, 23 Oct 2025 17:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20795v1</guid></item><item><title>Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</title><link>http://arxiv.org/abs/2510.20794v1</link><description>This paper presents a Multi-Object Tracking (MOT) framework that fuses radarand camera data to enhance tracking efficiency while minimizing manualinterventions. Contrary to many studies that underutilize radar and assign it asupplementary role--despite its capability to provide accurate range/depthinformation of targets in a world 3D coordinate system--our approach positionsradar in a crucial role. Meanwhile, this paper utilizes common features toenable online calibration to autonomously associate detections from radar andcamera. The main contributions of this work include: (1) the development of aradar-camera fusion MOT framework that exploits online radar-camera calibrationto simplify the integration of detection results from these two sensors, (2)the utilization of common features between radar and camera data to accuratelyderive real-world positions of detected objects, and (3) the adoption offeature matching and category-consistency checking to surpass the limitationsof mere position matching in enhancing sensor association accuracy. To the bestof our knowledge, we are the first to investigate the integration ofradar-camera common features and their use in online calibration for achievingMOT. The efficacy of our framework is demonstrated by its ability to streamlinethe radar-camera mapping process and improve tracking precision, as evidencedby real-world experiments conducted in both controlled environments and actualtraffic scenarios. Code is available athttps://github.com/radar-lab/Radar_Camera_MOT</description><author>Lei Cheng, Siyang Cao</author><pubDate>Thu, 23 Oct 2025 17:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20794v1</guid></item><item><title>BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</title><link>http://arxiv.org/abs/2510.20792v1</link><description>The rapid progress of graph generation has raised new security concerns,particularly regarding backdoor vulnerabilities. While prior work has exploredbackdoor attacks in image diffusion and unconditional graph generation,conditional, especially text-guided graph generation remains largelyunexamined. This paper proposes BadGraph, a backdoor attack method targetinglatent diffusion models for text-guided graph generation. BadGraph leveragestextual triggers to poison training data, covertly implanting backdoors thatinduce attacker-specified subgraphs during inference when triggers appear,while preserving normal performance on clean inputs. Extensive experiments onfour benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate theeffectiveness and stealth of the attack: less than 10% poisoning rate canachieves 50% attack success rate, while 24% suffices for over 80% success rate,with negligible performance degradation on benign samples. Ablation studiesfurther reveal that the backdoor is implanted during VAE and diffusion trainingrather than pretraining. These findings reveal the security vulnerabilities inlatent diffusion models of text-guided graph generation, highlight the seriousrisks in models' applications such as drug discovery and underscore the needfor robust defenses against the backdoor attack in such diffusion models.</description><author>Liang Ye, Shengqin Chen, Jiazhu Dai</author><pubDate>Thu, 23 Oct 2025 17:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20792v1</guid></item><item><title>Text2Mem: A Unified Memory Operation Language for Memory Operating System</title><link>http://arxiv.org/abs/2509.11145v2</link><description>Large language model agents increasingly depend on memory to sustain longhorizon interaction, but existing frameworks remain limited. Most expose only afew basic primitives such as encode, retrieve, and delete, while higher orderoperations like merge, promote, demote, split, lock, and expire are missing orinconsistently supported. Moreover, there is no formal and executablespecification for memory commands, leaving scope and lifecycle rules implicitand causing unpredictable behavior across systems. We introduce Text2Mem, aunified memory operation language that provides a standardized pathway fromnatural language to reliable execution. Text2Mem defines a compact yetexpressive operation set aligned with encoding, storage, and retrieval. Eachinstruction is represented as a JSON based schema instance with required fieldsand semantic invariants, which a parser transforms into typed operation objectswith normalized parameters. A validator ensures correctness before execution,while adapters map typed objects either to a SQL prototype backend or to realmemory frameworks. Model based services such as embeddings or summarization areintegrated when required. All results are returned through a unified executioncontract. This design ensures safety, determinism, and portability acrossheterogeneous backends. We also outline Text2Mem Bench, a planned benchmarkthat separates schema generation from backend execution to enable systematicevaluation. Together, these components establish the first standardizedfoundation for memory control in agents.</description><author>Yi Wang, Lihai Yang, Boyu Chen, Gongyi Zou, Kerun Xu, Bo Tang, Feiyu Xiong, Siheng Chen, Zhiyu Li</author><pubDate>Thu, 23 Oct 2025 17:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.11145v2</guid></item><item><title>Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction</title><link>http://arxiv.org/abs/2510.20787v1</link><description>Linear-attention models that compress the entire input sequence into afixed-size recurrent state offer an efficient alternative to Transformers, buttheir finite memory induces forgetfulness that harms retrieval-intensive tasks.To mitigate the issue, we explore a series of hybrid models that restore directaccess to past tokens. We interleave token mixers with intermediate time andspace complexity between linear and full attention, including sparse attentionwith token eviction, and the query-aware native sparse attention. Particularly,we propose a novel learnable token eviction approach. Combined withsliding-window attention, an end-to-end trainable lightweight CNN aggregatesinformation from both past and future adjacent tokens to adaptively retain alimited set of critical KV-pairs per head, maintaining linear attention'sconstant time and space complexity. Efficient Triton kernels for the sparseattention mechanisms are provided. Empirical evaluations on retrieval-intensivebenchmarks support the effectiveness of our approaches.</description><author>Mutian He, Philip N. Garner</author><pubDate>Thu, 23 Oct 2025 17:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20787v1</guid></item><item><title>A Coherence-Based Measure of AGI</title><link>http://arxiv.org/abs/2510.20784v1</link><description>Recent work by \citet{hendrycks2025agidefinition} formalized\textit{Artificial General Intelligence} (AGI) as the arithmetic mean ofproficiencies across cognitive domains derived from the Cattell--Horn--Carroll(CHC) model of human cognition. While elegant, this definition assumes\textit{compensability} -- that exceptional ability in some domains can offsetfailure in others. True general intelligence, however, should reflect\textit{coherent sufficiency}: balanced competence across all essentialdomains. We propose a coherence-aware measure of AGI based on the integral ofgeneralized means over a continuum of compensability exponents. Thisformulation spans arithmetic, geometric, and harmonic regimes, and theresulting \textit{area under the curve} (AUC) quantifies robustness undervarying compensability assumptions. Unlike the arithmetic mean, which rewardsspecialization, the AUC penalizes imbalance and captures inter-domaindependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,the coherence-adjusted AUC reveals that both systems remain far from generalcompetence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integratingthe generalized mean thus yields a principled, interpretable, and stricterfoundation for measuring genuine progress toward AGI.</description><author>Fares Fourati</author><pubDate>Thu, 23 Oct 2025 17:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20784v1</guid></item><item><title>Out-of-distribution Tests Reveal Compositionality in Chess Transformers</title><link>http://arxiv.org/abs/2510.20783v1</link><description>Chess is a canonical example of a task that requires rigorous reasoning andlong-term planning. Modern decision Transformers - trained similarly to LLMs -are able to learn competent gameplay, but it is unclear to what extent theytruly capture the rules of chess. To investigate this, we train a 270Mparameter chess Transformer and test it on out-of-distribution scenarios,designed to reveal failures of systematic generalization. Our analysis showsthat Transformers exhibit compositional generalization, as evidenced by strongrule extrapolation: they adhere to fundamental syntactic rules of the game byconsistently choosing valid moves even in situations very different from thetraining data. Moreover, they also generate high-quality moves for OOD puzzles.In a more challenging test, we evaluate the models on variants includingChess960 (Fischer Random Chess) - a variant of chess where starting positionsof pieces are randomized. We found that while the model exhibits basic strategyadaptation, they are inferior to symbolic AI algorithms that perform explicitsearch, but gap is smaller when playing against users on Lichess. Moreover, thetraining dynamics revealed that the model initially learns to move only its ownpieces, suggesting an emergent compositional understanding of the game.</description><author>Anna M√©sz√°ros, Patrik Reizinger, Ferenc Husz√°r</author><pubDate>Thu, 23 Oct 2025 17:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20783v1</guid></item><item><title>A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text</title><link>http://arxiv.org/abs/2510.20782v1</link><description>Current methods for evaluating large language models (LLMs) typically focuson high-level tasks such as text generation, without targeting a particular AIapplication. This approach is not sufficient for evaluating LLMs forResponsible AI dimensions like fairness, since protected attributes that arehighly relevant in one application may be less relevant in another. In thiswork, we construct a dataset that is driven by a real-world application(generate a plain-text product description, given a list of product features),parameterized by fairness attributes intersected with gendered adjectives andproduct categories, yielding a rich set of labeled prompts. We show how to usethe data to identify quality, veracity, safety, and fairness gaps in LLMs,contributing a proposal for LLM evaluation paired with a concrete resource forthe research community.</description><author>Alicia Sagae, Chia-Jung Lee, Sandeep Avula, Brandon Dang, Vanessa Murdock</author><pubDate>Thu, 23 Oct 2025 17:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20782v1</guid></item><item><title>Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost</title><link>http://arxiv.org/abs/2510.20780v1</link><description>Recent advancements in large reasoning models (LRMs) have introduced anintermediate "thinking" process prior to generating final answers, improvingtheir reasoning capabilities on complex downstream tasks. However, thepotential of LRMs as evaluators for machine translation (MT) quality remainsunderexplored. We provides the first systematic analysis of LRM-as-a-judge inMT evaluation. We identify key challenges, revealing LRMs require tailoredevaluation materials, tend to "overthink" simpler instances and have issueswith scoring mechanisms leading to overestimation. To address these, we proposeto calibrate LRM thinking by training them on synthetic, human-like thinkingtrajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that thisapproach largely reduces thinking budgets by ~35x while concurrently improvingevaluation performance across different LRM scales from 7B to 32B (e.g.,R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). Thesefindings highlight the potential of efficiently calibrated LRMs to advancefine-grained automatic MT evaluation.</description><author>Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang, Derek F. Wong</author><pubDate>Thu, 23 Oct 2025 17:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20780v1</guid></item><item><title>CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image</title><link>http://arxiv.org/abs/2510.20776v1</link><description>This work proposes a new generation-based 3D reconstruction method, namedCupid, that accurately infers the camera pose, 3D shape, and texture of anobject from a single 2D image. Cupid casts 3D reconstruction as a conditionalsampling process from a learned distribution of 3D objects, and it jointlygenerates voxels and pixel-voxel correspondences, enabling robust pose andshape estimation under a unified generative framework. By representing bothinput camera poses and 3D shape as a distribution in a shared 3D latent space,Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage thatproduces initial 3D geometry with associated 2D projections for pose recovery;and (2) a refinement stage that integrates pose-aligned image features toenhance structural fidelity and appearance details. Extensive experimentsdemonstrate Cupid outperforms leading 3D reconstruction methods with an over 3dB PSNR gain and an over 10% Chamfer Distance reduction, while matchingmonocular estimators on pose accuracy and delivering superior visual fidelityover baseline 3D generative models. For an immersive view of the 3D resultsgenerated by Cupid, please visit cupid3d.github.io.</description><author>Binbin Huang, Haobin Duan, Yiqun Zhao, Zibo Zhao, Yi Ma, Shenghua Gao</author><pubDate>Thu, 23 Oct 2025 17:47:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20776v1</guid></item><item><title>FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation</title><link>http://arxiv.org/abs/2510.20774v1</link><description>Large-scale and diverse datasets are vital for training robust roboticmanipulation policies, yet existing data collection methods struggle to balancescale, diversity, and quality. Simulation offers scalability but suffers fromsim-to-real gaps, while teleoperation yields high-quality demonstrations withlimited diversity and high labor cost. We introduce FieldGen, a field-guideddata generation framework that enables scalable, diverse, and high-qualityreal-world data collection with minimal human supervision. FieldGen decomposesmanipulation into two stages: a pre-manipulation phase, allowing trajectorydiversity, and a fine manipulation phase requiring expert precision. Humandemonstrations capture key contact and pose information, after which anattraction field automatically generates diverse trajectories converging tosuccessful configurations. This decoupled design combines scalable trajectorydiversity with precise supervision. Moreover, FieldGen-Reward augmentsgenerated data with reward annotations to further enhance policy learning.Experiments demonstrate that policies trained with FieldGen achieve highersuccess rates and improved stability compared to teleoperation-based baselines,while significantly reducing human effort in long-term real-world datacollection. Webpage is available at https://fieldgen.github.io/.</description><author>Wenhao Wang, Kehe Ye, Xinyu Zhou, Tianxing Chen, Cao Min, Qiaoming Zhu, Xiaokang Yang, Yongjian Shen, Yang Yang, Maoqing Yao, Yao Mu</author><pubDate>Thu, 23 Oct 2025 17:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20774v1</guid></item><item><title>AlphaFlow: Understanding and Improving MeanFlow Models</title><link>http://arxiv.org/abs/2510.20771v1</link><description>MeanFlow has recently emerged as a powerful framework for few-step generativemodeling trained from scratch, but its success is not yet fully understood. Inthis work, we show that the MeanFlow objective naturally decomposes into twoparts: trajectory flow matching and trajectory consistency. Through gradientanalysis, we find that these terms are strongly negatively correlated, causingoptimization conflict and slow convergence. Motivated by these insights, weintroduce $\alpha$-Flow, a broad family of objectives that unifies trajectoryflow matching, Shortcut Model, and MeanFlow under one formulation. By adoptinga curriculum strategy that smoothly anneals from trajectory flow matching toMeanFlow, $\alpha$-Flow disentangles the conflicting objectives, and achievesbetter convergence. When trained from scratch on class-conditional ImageNet-1K256x256 with vanilla DiT backbones, $\alpha$-Flow consistently outperformsMeanFlow across scales and settings. Our largest $\alpha$-Flow-XL/2+ modelachieves new state-of-the-art results using vanilla DiT backbones, with FIDscores of 2.58 (1-NFE) and 2.15 (2-NFE).</description><author>Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov</author><pubDate>Thu, 23 Oct 2025 17:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20771v1</guid></item><item><title>CSU-PCAST: A Dual-Branch Transformer Framework for medium-range ensemble Precipitation Forecasting</title><link>http://arxiv.org/abs/2510.20769v1</link><description>Accurate medium-range precipitation forecasting is crucial forhydrometeorological risk management and disaster mitigation, yet remainschallenging for current numerical weather prediction (NWP) systems. Traditionalensemble systems such as the Global Ensemble Forecast System (GEFS) struggle tomaintain high skill, especially for moderate and heavy rainfall at extendedlead times. This study develops a deep learning-based ensemble framework formulti-step precipitation prediction through joint modeling of a comprehensiveset of atmospheric variables. The model is trained on ERA5 reanalysis data at0.25$^{\circ}$ spatial resolution, with precipitation labels from NASA'sIntegrated Multi-satellite Retrievals for Global Precipitation Measurement(GPM) constellation (IMERG), incorporating 57 input variables, includingupper-air and surface predictors. The architecture employs a patch-based SwinTransformer backbone with periodic convolutions to handle longitudinalcontinuity and integrates time and noise embeddings through conditional layernormalization. A dual-branch decoder predicts total precipitation and othervariables, with targeted freezing of encoder-decoder pathways for specializedtraining. Training minimizes a hybrid loss combining the Continuous RankedProbability Score (CRPS) and weighted log1p mean squared error (log1pMSE),balancing probabilistic accuracy and magnitude fidelity. During inference, themodel ingests real-time Global Forecast System (GFS) initial conditions togenerate 15-day forecasts autoregressively. Evaluation against GEFS using IMERGdata demonstrates higher Critical Success Index (CSI) scores at precipitationthresholds of 0.1 mm, 1 mm, 10 mm, and 20 mm, highlighting improved performancefor moderate to heavy rainfall.</description><author>Tianyi Xiong, Haonan Chen</author><pubDate>Thu, 23 Oct 2025 17:43:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20769v1</guid></item><item><title>RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</title><link>http://arxiv.org/abs/2510.20768v1</link><description>Retrieval-Augmented Generation (RAG) has emerged as the dominantarchitectural pattern to operationalize Large Language Model (LLM) usage inCyber Threat Intelligence (CTI) systems. However, this design is susceptible topoisoning attacks, and previously proposed defenses can fail for CTI contextsas cyber threat information is often completely new for emerging attacks, andsophisticated threat actors can mimic legitimate formats, terminology, andstylistic conventions. To address this issue, we propose that the robustness ofmodern RAG defenses can be accelerated by applying source credibilityalgorithms on corpora, using PageRank as an example. In our experiments, wedemonstrate quantitatively that our algorithm applies a lower authority scoreto malicious documents while promoting trusted content, using the standardizedMS MARCO dataset. We also demonstrate proof-of-concept performance of ouralgorithm on CTI documents and feeds.</description><author>Austin Jia, Avaneesh Ramesh, Zain Shamsi, Daniel Zhang, Alex Liu</author><pubDate>Thu, 23 Oct 2025 17:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20768v1</guid></item><item><title>DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion</title><link>http://arxiv.org/abs/2510.20766v1</link><description>Diffusion Transformer models can generate images with remarkable fidelity anddetail, yet training them at ultra-high resolutions remains extremely costlydue to the self-attention mechanism's quadratic scaling with the number ofimage tokens. In this paper, we introduce Dynamic Position Extrapolation(DyPE), a novel, training-free method that enables pre-trained diffusiontransformers to synthesize images at resolutions far beyond their trainingdata, with no additional sampling cost. DyPE takes advantage of the spectralprogression inherent to the diffusion process, where low-frequency structuresconverge early, while high-frequencies take more steps to resolve.Specifically, DyPE dynamically adjusts the model's positional encoding at eachdiffusion step, matching their frequency spectrum with the current stage of thegenerative process. This approach allows us to generate images at resolutionsthat exceed the training resolution dramatically, e.g., 16 million pixels usingFLUX. On multiple benchmarks, DyPE consistently improves performance andachieves state-of-the-art fidelity in ultra-high-resolution image generation,with gains becoming even more pronounced at higher resolutions. Project page isavailable at https://noamissachar.github.io/DyPE/.</description><author>Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal</author><pubDate>Thu, 23 Oct 2025 17:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20766v1</guid></item><item><title>MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs</title><link>http://arxiv.org/abs/2510.20762v1</link><description>Decoding visual stimuli from neural population activity is crucial forunderstanding the brain and for applications in brain-machine interfaces.However, such biological data is often scarce, particularly in primates orhumans, where high-throughput recording techniques, such as two-photon imaging,remain challenging or impossible to apply. This, in turn, poses a challenge fordeep learning decoding techniques. To overcome this, we introduce MEIcoder, abiologically informed decoding method that leverages neuron-specific mostexciting inputs (MEIs), a structural similarity index measure loss, andadversarial training. MEIcoder achieves state-of-the-art performance inreconstructing visual stimuli from single-cell activity in primary visualcortex (V1), especially excelling on small datasets with fewer recordedneurons. Using ablation studies, we demonstrate that MEIs are the main driversof the performance, and in scaling experiments, we show that MEIcoder canreconstruct high-fidelity natural-looking images from as few as 1,000-2,500neurons and less than 1,000 training data points. We also propose a unifiedbenchmark with over 160,000 samples to foster future research. Our resultsdemonstrate the feasibility of reliable decoding in early visual system andprovide practical insights for neuroscience and neuroengineering applications.</description><author>Jan Sobotka, Luca Baroni, J√°n Antol√≠k</author><pubDate>Thu, 23 Oct 2025 17:35:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20762v1</guid></item><item><title>Autoencoding Random Forests</title><link>http://arxiv.org/abs/2505.21441v2</link><description>We propose a principled method for autoencoding with random forests. Ourstrategy builds on foundational results from nonparametric statistics andspectral graph theory to learn a low-dimensional embedding of the model thatoptimally represents relationships in the data. We provide exact andapproximate solutions to the decoding problem via constrained optimization,split relabeling, and nearest neighbors regression. These methods effectivelyinvert the compression pipeline, establishing a map from the embedding spaceback to the input space using splits learned by the ensemble's constituenttrees. The resulting decoders are universally consistent under commonregularity assumptions. The procedure works with supervised or unsupervisedmodels, providing a window into conditional or joint distributions. Wedemonstrate various applications of this autoencoder, including powerful newtools for visualization, compression, clustering, and denoising. Experimentsillustrate the ease and utility of our method in a wide range of settings,including tabular, image, and genomic data.</description><author>Binh Duc Vu, Jan Kapar, Marvin Wright, David S. Watson</author><pubDate>Thu, 23 Oct 2025 17:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.21441v2</guid></item><item><title>Watermarking Autoregressive Image Generation</title><link>http://arxiv.org/abs/2506.16349v2</link><description>Watermarking the outputs of generative models has emerged as a promisingapproach for tracking their provenance. Despite significant interest inautoregressive image generation models and their potential for misuse, no priorwork has attempted to watermark their outputs at the token level. In this work,we present the first such approach by adapting language model watermarkingtechniques to this setting. We identify a key challenge: the lack of reversecycle-consistency (RCC), wherein re-tokenizing generated image tokenssignificantly alters the token sequence, effectively erasing the watermark. Toaddress this and to make our method robust to common image transformations,neural compression, and removal attacks, we introduce (i) a customtokenizer-detokenizer finetuning procedure that improves RCC, and (ii) acomplementary watermark synchronization layer. As our experiments demonstrate,our approach enables reliable and robust watermark detection with theoreticallygrounded p-values. Code and models are available athttps://github.com/facebookresearch/wmar.</description><author>Nikola Jovanoviƒá, Ismail Labiad, Tom√°≈° Souƒçek, Martin Vechev, Pierre Fernandez</author><pubDate>Thu, 23 Oct 2025 17:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.16349v2</guid></item><item><title>Learning Modular Exponentiation with Transformers</title><link>http://arxiv.org/abs/2506.23679v2</link><description>Modular exponentiation is crucial to number theory and cryptography, yetremains largely unexplored from a mechanistic interpretability standpoint. Wetrain a 4-layer encoder-decoder Transformer model to perform this operation andinvestigate the emergence of numerical reasoning during training. Utilizingprincipled sampling strategies, PCA-based embedding analysis, and activationpatching, we examine how number-theoretic properties are encoded within themodel. We find that reciprocal operand training leads to strong performancegains, with sudden generalization across related moduli. These synchronizedaccuracy surges reflect grokking-like dynamics, suggesting the modelinternalizes shared arithmetic structure. We also find a subgraph consistingentirely of attention heads in the final layer sufficient to achieve fullperformance on the task of regular exponentiation. These results suggest thattransformer models learn modular arithmetic through specialized computationalcircuits, paving the way for more interpretable and efficient neural approachesto modular exponentiation.</description><author>David Demitri Africa, Sara M. Kapoor, Theo Simon Sorg, Challenger Mishra</author><pubDate>Thu, 23 Oct 2025 17:33:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.23679v2</guid></item><item><title>Tex-ViT: A Generalizable, Robust, Texture-based dual-branch cross-attention deepfake detector</title><link>http://arxiv.org/abs/2408.16892v2</link><description>Deepfakes, which employ GAN to produce highly realistic facial modification,are widely regarded as the prevailing method. Traditional CNN have been able toidentify bogus media, but they struggle to perform well on different datasetsand are vulnerable to adversarial attacks due to their lack of robustness.Vision transformers have demonstrated potential in the realm of imageclassification problems, but they require enough training data. Motivated bythese limitations, this publication introduces Tex-ViT (Texture-VisionTransformer), which enhances CNN features by combining ResNet with a visiontransformer. The model combines traditional ResNet features with a texturemodule that operates in parallel on sections of ResNet before eachdown-sampling operation. The texture module then serves as an input to the dualbranch of the cross-attention vision transformer. It specifically focuses onimproving the global texture module, which extracts feature map correlation.Empirical analysis reveals that fake images exhibit smooth textures that do notremain consistent over long distances in manipulations. Experiments wereperformed on different categories of FF++, such as DF, f2f, FS, and NT,together with other types of GAN datasets in cross-domain scenarios.Furthermore, experiments also conducted on FF++, DFDCPreview, and Celeb-DFdataset underwent several post-processing situations, such as blurring,compression, and noise. The model surpassed the most advanced models in termsof generalization, achieving a 98% accuracy in cross-domain scenarios. Thisdemonstrates its ability to learn the shared distinguishing texturalcharacteristics in the manipulated samples. These experiments provide evidencethat the proposed model is capable of being applied to various situations andis resistant to many post-processing procedures.</description><author>Deepak Dagar, Dinesh Kumar Vishwakarma</author><pubDate>Thu, 23 Oct 2025 17:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16892v2</guid></item><item><title>Controlling the Flow: Stability and Convergence for Stochastic Gradient Descent with Decaying Regularization</title><link>http://arxiv.org/abs/2505.11434v2</link><description>The present article studies the minimization of convex, L-smooth functionsdefined on a separable real Hilbert space. We analyze regularized stochasticgradient descent (reg-SGD), a variant of stochastic gradient descent that usesa Tikhonov regularization with time-dependent, vanishing regularizationparameter. We prove strong convergence of reg-SGD to the minimum-norm solutionof the original problem without additional boundedness assumptions. Moreover,we quantify the rate of convergence and optimize the interplay betweenstep-sizes and regularization decay. Our analysis reveals how vanishingTikhonov regularization controls the flow of SGD and yields stable learningdynamics, offering new insights into the design of iterative algorithms forconvex problems, including those that arise in ill-posed inverse problems. Wevalidate our theoretical findings through numerical experiments on imagereconstruction and ODE-based inverse problems.</description><author>Sebastian Kassing, Simon Weissmann, Leif D√∂ring</author><pubDate>Thu, 23 Oct 2025 17:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11434v2</guid></item><item><title>Incomplete U-Statistics of Equireplicate Designs: Berry-Esseen Bound and Efficient Construction</title><link>http://arxiv.org/abs/2510.20755v1</link><description>U-statistics are a fundamental class of estimators that generalize the samplemean and underpin much of nonparametric statistics. Although extensivelystudied in both statistics and probability, key challenges remain: their highcomputational cost - addressed partly through incomplete U-statistics - andtheir non-standard asymptotic behavior in the degenerate case, which typicallyrequires resampling methods for hypothesis testing. This paper presents a novelperspective on U-statistics, grounded in hypergraph theory and combinatorialdesigns. Our approach bypasses the traditional Hoeffding decomposition, themain analytical tool in this literature but one highly sensitive to degeneracy.By characterizing the dependence structure of a U-statistic, we derive aBerry-Esseen bound that applies to all incomplete U-statistics of deterministicdesigns, yielding conditions under which Gaussian limiting distributions can beestablished even in the degenerate case and when the order diverges. We alsointroduce efficient algorithms to construct incomplete U-statistics ofequireplicate designs, a subclass of deterministic designs that, in certaincases, achieve minimum variance. Finally, we apply our framework tokernel-based tests that use Maximum Mean Discrepancy (MMD) and Hilbert-SchmidtIndependence Criterion. In a real data example with CIFAR-10, ourpermutation-free MMD test delivers substantial computational gains whileretaining power and type I error control.</description><author>Cesare Miglioli, Jordan Awan</author><pubDate>Thu, 23 Oct 2025 17:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20755v1</guid></item><item><title>ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for Tissue Segmentation in Histopathology</title><link>http://arxiv.org/abs/2510.20754v1</link><description>Automated histopathological image analysis plays a vital role incomputer-aided diagnosis of various diseases. Among developed algorithms, deeplearning-based approaches have demonstrated excellent performance in multipletasks, including semantic tissue segmentation in histological images. In thisstudy, we propose a novel approach based on attention-driven feature fusion ofconvolutional neural networks (CNNs) and vision transformers (ViTs) within aunified dual-encoder model to improve semantic segmentation performance.Evaluation on two publicly available datasets showed that our model achieved{\mu}IoU/{\mu}Dice scores of 76.79%/86.87% on the GCPS dataset and64.93%/76.60% on the PUMA dataset, outperforming state-of-the-art and baselinebenchmarks. The implementation of our method is publicly available in a GitHubrepository: https://github.com/NimaTorbati/ACS-SegNet</description><author>Nima Torbati, Anastasia Meshcheryakova, Ramona Woitek, Diana Mechtcheriakova, Amirreza Mahbod</author><pubDate>Thu, 23 Oct 2025 17:21:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20754v1</guid></item><item><title>Two approaches to multiple canonical correlation analysis for repeated measures data</title><link>http://arxiv.org/abs/2510.04457v2</link><description>In classical canonical correlation analysis (CCA), the goal is to determinethe linear transformations of two random vectors into two new random variablesthat are most strongly correlated. Canonical variables are pairs of these newrandom variables, while canonical correlations are correlations between thesepairs. In this paper, we propose and study two generalizations of thisclassical method: (1) Instead of two random vectors we study more complex data structures thatappear in important applications. In these structures, there are $L$ features,each described by $p_l$ scalars, $1 \le l \le L$. We observe $n$ such objectsover $T$ time points. We derive a suitable analog of the CCA for such data. Ourapproach relies on embeddings into Reproducing Kernel Hilbert Spaces, andcovers several related data structures as well. (2) We develop an analogous approach for multidimensional random processes.In this case, the experimental units are multivariate continuous,square-integrable functions over a given interval. These functions are modeledas elements of a Hilbert space, so in this case, we define the multiplefunctional canonical correlation analysis, MFCCA. We justify our approaches by their application to two data sets and suitablelarge sample theory. We derive consistency rates for the related transformationand correlation estimators, and show that it is possible to relax two commonassumptions on the compactness of the underlying cross-covariance operators andthe independence of the data.</description><author>Tomasz G√≥recki, Miros≈Çaw Krzy≈õko, Felix Gnettner, Piotr Kokoszka</author><pubDate>Thu, 23 Oct 2025 17:17:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.04457v2</guid></item><item><title>Reinforcement Learning and Consumption-Savings Behavior</title><link>http://arxiv.org/abs/2510.20748v1</link><description>This paper demonstrates how reinforcement learning can explain two puzzlingempirical patterns in household consumption behavior during economic downturns.I develop a model where agents use Q-learning with neural network approximationto make consumption-savings decisions under income uncertainty, departing fromstandard rational expectations assumptions. The model replicates two keyfindings from recent literature: (1) unemployed households with previously lowliquid assets exhibit substantially higher marginal propensities to consume(MPCs) out of stimulus transfers compared to high-asset households (0.50 vs0.34), even when neither group faces borrowing constraints, consistent withGanong et al. (2024); and (2) households with more past unemploymentexperiences maintain persistently lower consumption levels after controllingfor current economic conditions, a "scarring" effect documented by Malmendierand Shen (2024). Unlike existing explanations based on belief updating aboutincome risk or ex-ante heterogeneity, the reinforcement learning mechanismgenerates both higher MPCs and lower consumption levels simultaneously throughvalue function approximation errors that evolve with experience. Simulationresults closely match the empirical estimates, suggesting that adaptivelearning through reinforcement learning provides a unifying framework forunderstanding how past experiences shape current consumption behavior beyondwhat current economic conditions would predict.</description><author>Brandon Kaplowitz</author><pubDate>Thu, 23 Oct 2025 17:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20748v1</guid></item><item><title>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</title><link>http://arxiv.org/abs/2510.08396v2</link><description>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuningmethod for foundation models, but it suffers from parameter interference,resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-basedLoRA variants show promise in mitigating intra-task correlations in single-taskinstruction tuning, they introduce additional router parameters and remainineffective in multi-task model merging where inter-task interference arises.Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicitMoE-based LoRA variant that introduces: (1) rank-wise expert activation in theup-projection matrix, and (2) an implicit router that unifies expert routingand down-projection, where a frozen sparse random projection matrix replacesthe traditional dense trainable version. This design resolves the trade-offbetween intra-task decorrelation and computational efficiency by eliminatingthe need for an explicit router, while inherently mitigating inter-taskinterference due to the orthogonality property of random matrices. Extensiveexperiments across four domains -- general knowledge understanding, scientificquestion answering, mathematical reasoning, and code generation -- demonstrateconsistent performance improvements over existing methods. Beyond empiricalgains, FlyLoRA highlights how biological structures can inspire innovations inAI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.</description><author>Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</author><pubDate>Thu, 23 Oct 2025 17:14:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.08396v2</guid></item><item><title>GenLit: Reformulating Single-Image Relighting as Video Generation</title><link>http://arxiv.org/abs/2412.11224v4</link><description>Manipulating the illumination of a 3D scene within a single image representsa fundamental challenge in computer vision and graphics. This problem hastraditionally been addressed using inverse rendering techniques, which involveexplicit 3D asset reconstruction and costly ray-tracing simulations. Meanwhile,recent advancements in visual foundation models suggest that a new paradigmcould soon be possible -- one that replaces explicit physical models withnetworks that are trained on large amounts of image and video data. In thispaper, we exploit the implicit scene understanding of a video diffusion model,particularly Stable Video Diffusion, to relight a single image. We introduceGenLit, a framework that distills the ability of a graphics engine to performlight manipulation into a video-generation model, enabling users to directlyinsert and manipulate a point light in the 3D world within a given image andgenerate results directly as a video sequence. We find that a model fine-tunedon only a small synthetic dataset generalizes to real-world scenes, enablingsingle-image relighting with plausible and convincing shadows andinter-reflections. Our results highlight the ability of video foundation modelsto capture rich information about lighting, material, and shape, and ourfindings indicate that such models, with minimal training, can be used toperform relighting without explicit asset reconstruction or ray-tracing. .Project page: https://genlit.is.tue.mpg.de/.</description><author>Shrisha Bharadwaj, Haiwen Feng, Giorgio Becherini, Victoria Fernandez Abrevaya, Michael J. Black</author><pubDate>Thu, 23 Oct 2025 17:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11224v4</guid></item><item><title>Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review</title><link>http://arxiv.org/abs/2505.02828v2</link><description>Explainable Artificial Intelligence (XAI) has emerged as a pillar ofTrustworthy AI and aims to bring transparency in complex models that are opaqueby nature. Despite the benefits of incorporating explanations in models, anurgent need is found in addressing the privacy concerns of providing thisadditional information to end users. In this article, we conduct a scopingreview of existing literature to elicit details on the conflict between privacyand explainability. Using the standard methodology for scoping review, weextracted 57 articles from 1,943 studies published from January 2019 toDecember 2024. The review addresses 3 research questions to present readerswith more understanding of the topic: (1) what are the privacy risks ofreleasing explanations in AI systems? (2) what current methods have researchersemployed to achieve privacy preservation in XAI systems? (3) what constitutes aprivacy preserving explanation? Based on the knowledge synthesized from theselected studies, we categorize the privacy risks and preservation methods inXAI and propose the characteristics of privacy preserving explanations to aidresearchers and practitioners in understanding the requirements of XAI that isprivacy compliant. Lastly, we identify the challenges in balancing privacy withother system desiderata and provide recommendations for achieving privacypreserving XAI. We expect that this review will shed light on the complexrelationship of privacy and explainability, both being the fundamentalprinciples of Trustworthy AI.</description><author>Sonal Allana, Mohan Kankanhalli, Rozita Dara</author><pubDate>Thu, 23 Oct 2025 17:10:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.02828v2</guid></item><item><title>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations</title><link>http://arxiv.org/abs/2510.20743v1</link><description>We present Empathic Prompting, a novel framework for multimodal human-AIinteraction that enriches Large Language Model (LLM) conversations withimplicit non-verbal context. The system integrates a commercial facialexpression recognition service to capture users' emotional cues and embeds themas contextual signals during prompting. Unlike traditional multimodalinterfaces, empathic prompting requires no explicit user control; instead, itunobtrusively augments textual input with affective information forconversational and smoothness alignment. The architecture is modular andscalable, allowing integration of additional non-verbal modules. We describethe system design, implemented through a locally deployed DeepSeek instance,and report a preliminary service and usability evaluation (N=5). Results showconsistent integration of non-verbal input into coherent LLM outputs, withparticipants highlighting conversational fluidity. Beyond this proof ofconcept, empathic prompting points to applications in chatbot-mediatedcommunication, particularly in domains like healthcare or education, whereusers' emotional signals are critical yet often opaque in verbal exchanges.</description><author>Lorenzo Stacchio, Andrea Ubaldi, Alessandro Galdelli, Maurizio Mauri, Emanuele Frontoni, Andrea Gaggioli</author><pubDate>Thu, 23 Oct 2025 17:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20743v1</guid></item><item><title>Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex</title><link>http://arxiv.org/abs/2504.12474v3</link><description>Text-attributed graphs (TAGs) present unique challenges in representationlearning by requiring models to capture both the semantic richness ofnode-associated texts and the structural dependencies of the graph. While graphneural networks (GNNs) excel at modeling topological information, they lack thecapacity to process unstructured text. Conversely, large language models (LLMs)are proficient in text understanding but are typically unaware of graphstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novelarchitecture that tightly integrates GNNs and LLMs through stacked Graph-TextFusion Units. Each unit allows for mutual attention between textual andstructural representations, enabling information to flow in both directions,text influencing structure and structure guiding textual interpretation. Theproposed architecture is trained using parameter-efficient fine-tuning (LoRA),keeping the LLM frozen while adapting to task-specific signals. Extensiveexperiments on five benchmark datasets demonstrate that BiGTex achievesstate-of-the-art performance in node classification and generalizes effectivelyto link prediction. An ablation study further highlights the importance of softprompting and bi-directional attention in the model's success.</description><author>Azadeh Beiranvand, Seyed Mehdi Vahidipour</author><pubDate>Thu, 23 Oct 2025 17:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.12474v3</guid></item><item><title>Position: Many generalization measures for deep learning are fragile</title><link>http://arxiv.org/abs/2510.18934v2</link><description>A wide variety of generalization measures have been applied to deep neuralnetworks (DNNs). Although obtaining tight bounds remains challenging, suchmeasures are often assumed to reproduce qualitative generalization trends. Inthis position paper, we argue that many post-mortem generalization measures --those computed on trained networks -- are \textbf{fragile}: small trainingmodifications that barely affect the underlying DNN can substantially change ameasure's value, trend, or scaling behavior. For example, minor hyperparameterchanges, such as learning rate adjustments or switching between SGD variantscan reverse the slope of a learning curve in widely used generalizationmeasures like the path norm. We also identify subtler forms of fragility. Forinstance, the PAC-Bayes origin measure is regarded as one of the most reliable,and is indeed less sensitive to hyperparameter tweaks than many other measures.However, it completely fails to capture differences in data complexity acrosslearning curves. This data fragility contrasts with the function-basedmarginal-likelihood PAC-Bayes bound, which does capture differences indata-complexity, including scaling behavior, in learning curves, but which isnot a post-mortem measure. Beyond demonstrating that many bounds -- such aspath, spectral and Frobenius norms, flatness proxies, and deterministicPAC-Bayes surrogates -- are fragile, this position paper also argues thatdevelopers of new measures should explicitly audit them for fragility.</description><author>Shuofeng Zhang, Ard Louis</author><pubDate>Thu, 23 Oct 2025 17:02:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.18934v2</guid></item><item><title>Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages</title><link>http://arxiv.org/abs/2510.20739v1</link><description>Program analysis tools often produce large volumes of candidate vulnerabilityreports that require costly manual review, creating a practical challenge: howcan security analysts prioritize the reports most likely to be truevulnerabilities? This paper investigates whether machine learning can be applied toprioritizing vulnerabilities reported by program analysis tools. We focus onNode.js packages and collect a benchmark of 1,883 Node.js packages, eachcontaining one reported ACE or ACI vulnerability. We evaluate a variety ofmachine learning approaches, including classical models, graph neural networks(GNNs), large language models (LLMs), and hybrid models that combine GNN andLLMs, trained on data based on a dynamic program analysis tool's output. Thetop LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML modelsreaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leadingmodel eliminates 66.9% of benign packages from manual review, taking around 60ms per package. If the best model is tuned to operate at a precision level of0.8 (i.e., allowing 20% false positives amongst all warnings), our approach candetect 99.2% of exploitable taint flows while missing only 0.8%, demonstratingstrong potential for real-world vulnerability triage.</description><author>Ronghao Ni, Aidan Z. H. Yang, Min-Chien Hsu, Nuno Sabino, Limin Jia, Ruben Martins, Darion Cassel, Kevin Cheang</author><pubDate>Thu, 23 Oct 2025 16:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20739v1</guid></item><item><title>Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process</title><link>http://arxiv.org/abs/2510.20736v1</link><description>Developing effective multimodal fusion approaches has become increasinglyessential in many real-world scenarios, such as health care and finance. Thekey challenge is how to preserve the feature expressiveness in each modalitywhile learning cross-modal interactions. Previous approaches primarily focus onthe cross-modal alignment, while over-emphasis on the alignment of marginaldistributions of modalities may impose excess regularization and obstructmeaningful representations within each modality. The Dirichlet process (DP)mixture model is a powerful Bayesian non-parametric method that can amplify themost prominent features by its richer-gets-richer property, which allocatesincreasing weights to them. Inspired by this unique characteristic of DP, wepropose a new DP-driven multimodal learning framework that automaticallyachieves an optimal balance between prominent intra-modal representationlearning and cross-modal alignment. Specifically, we assume that each modalityfollows a mixture of multivariate Gaussian distributions and further adopt DPto calculate the mixture weights for all the components. This paradigm allowsDP to dynamically allocate the contributions of features and select the mostprominent ones, leveraging its richer-gets-richer property, thus facilitatingmultimodal feature fusion. Extensive experiments on several multimodal datasetsdemonstrate the superior performance of our model over other competitors.Ablation analysis further validates the effectiveness of DP in aligningmodality distributions and its robustness to changes in key hyperparameters.Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git</description><author>Tsai Hor Chan, Feng Wu, Yihang Chen, Guosheng Yin, Lequan Yu</author><pubDate>Thu, 23 Oct 2025 16:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20736v1</guid></item><item><title>Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs</title><link>http://arxiv.org/abs/2506.19923v4</link><description>We present Prover Agent, a novel AI agent for automated theorem proving thatintegrates large language models (LLMs) with a formal proof assistant, Lean.Prover Agent coordinates an informal reasoning LLM, a formal prover model, andfeedback from Lean while also generating auxiliary lemmas. These auxiliarylemmas are not limited to subgoals in the formal proof but can also includespecial cases or potentially useful facts derived from the assumptions, whichhelp in discovering a viable proof strategy. It achieves an 88.1% success rateon the MiniF2F benchmark, establishing a new state-of-the-art among methodsusing small language models (SLMs) with a much lower sample budget thanprevious approaches. We also present theoretical analyses and case studies thatillustrate how these generated lemmas contribute to solving challengingproblems. Our code is publicly available at:https://github.com/kAIto47802/Prover-Agent.</description><author>Kaito Baba, Chaoran Liu, Shuhei Kurita, Akiyoshi Sannai</author><pubDate>Thu, 23 Oct 2025 16:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.19923v4</guid></item><item><title>Thought Communication in Multiagent Collaboration</title><link>http://arxiv.org/abs/2510.20733v1</link><description>Natural language has long enabled human cooperation, but its lossy,ambiguous, and indirect nature limits the potential of collective intelligence.While machines are not subject to these constraints, most LLM-based multi-agentsystems still rely solely on natural language, exchanging tokens or theirembeddings. To go beyond language, we introduce a new paradigm, thoughtcommunication, which enables agents to interact directly mind-to-mind, akin totelepathy. To uncover these latent thoughts in a principled way, we formalizethe process as a general latent variable model, where agent states aregenerated by an unknown function of underlying thoughts. We prove that, in anonparametric setting without auxiliary information, both shared and privatelatent thoughts between any pair of agents can be identified. Moreover, theglobal structure of thought sharing, including which agents share whichthoughts and how these relationships are structured, can also be recovered withtheoretical guarantees. Guided by the established theory, we develop aframework that extracts latent thoughts from all agents prior to communicationand assigns each agent the relevant thoughts, along with their sharingpatterns. This paradigm naturally extends beyond LLMs to all modalities, asmost observational data arise from hidden generative processes. Experiments onboth synthetic and real-world benchmarks validate the theory and demonstratethe collaborative advantages of thought communication. We hope this workilluminates the potential of leveraging the hidden world, as many challengesremain unsolvable through surface-level observation alone, regardless ofcompute or data scale.</description><author>Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, Kun Zhang</author><pubDate>Thu, 23 Oct 2025 16:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20733v1</guid></item><item><title>xRFM: Accurate, scalable, and interpretable feature learning models for tabular data</title><link>http://arxiv.org/abs/2508.10053v2</link><description>Inference from tabular data, collections of continuous and categoricalvariables organized into matrices, is a foundation for modern technology andscience. Yet, in contrast to the explosive changes in the rest of AI, the bestpractice for these predictive tasks has been relatively unchanged and is stillprimarily based on variations of Gradient Boosted Decision Trees (GBDTs). Veryrecently, there has been renewed interest in developing state-of-the-artmethods for tabular data based on recent developments in neural networks andfeature learning methods. In this work, we introduce xRFM, an algorithm thatcombines feature learning kernel machines with a tree structure to both adaptto the local structure of the data and scale to essentially unlimited amountsof training data. We show that compared to $31$ other methods, including recently introducedtabular foundation models (TabPFNv2) and GBDTs, xRFM achieves best performanceacross $100$ regression datasets and is competitive to the best methods across$200$ classification datasets outperforming GBDTs. Additionally, xRFM providesinterpretability natively through the Average Gradient Outer Product.</description><author>Daniel Beaglehole, David Holzm√ºller, Adityanarayanan Radhakrishnan, Mikhail Belkin</author><pubDate>Thu, 23 Oct 2025 16:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10053v2</guid></item><item><title>Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems</title><link>http://arxiv.org/abs/2510.20728v1</link><description>We present a multi-agent, human-in-the-loop workflow that co-designs quantumcodes with prescribed transversal diagonal gates. It builds on the Subset-SumLinear Programming (SSLP) framework (arXiv:2504.20847), which partitions basisstrings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL)equalities via small LPs. The workflow is powered by GPT-5 and implementedwithin TeXRA (https://texra.ai)-a multi-agent research assistant platform thatsupports an iterative tool-use loop agent and a derivation-then-edit workflowreasoning agent. We work in a LaTeX-Python environment where agents reason,edit documents, execute code, and synchronize their work to Git/Overleaf.Within this workspace, three roles collaborate: a Synthesis Agent formulatesthe problem; a Search Agent sweeps/screens candidates and exactifies numericsinto rationals; and an Audit Agent independently checks all KL equalities andthe induced logical action. As a first step we focus on distance $d=2$ withnondegenerate residues. For code dimension $K\in\{2,3,4\}$ and $n\le6$ qubits,systematic sweeps yield certificate-backed tables cataloging attainable cycliclogical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$at $n=6$. From verified instances, Synthesis Agent abstracts recurringstructures into closed-form families and proves they satisfy the KL equalitiesfor all parameters. It further demonstrates that SSLP accommodates residuedegeneracy by exhibiting a new $((6,4,2))$ code implementing the transversalcontrolled-phase $diag(1,1,1,i)$. Overall, the workflow recastsdiagonal-transversal feasibility as an analytical pipeline executed at scale,combining systematic enumeration with exact analytical reconstruction. Ityields reproducible code constructions, supports targeted extensions to larger$K$ and higher distances, and leads toward data-driven classification.</description><author>Xi He, Sirui Lu, Bei Zeng</author><pubDate>Thu, 23 Oct 2025 16:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20728v1</guid></item><item><title>Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing</title><link>http://arxiv.org/abs/2510.20727v1</link><description>Objective: Fluoropyrimidines are widely prescribed for colorectal and breastcancers, but are associated with toxicities such as hand-foot syndrome andcardiotoxicity. Since toxicity documentation is often embedded in clinicalnotes, we aimed to develop and evaluate natural language processing (NLP)methods to extract treatment and toxicity information. Materials and Methods: We constructed a gold-standard dataset of 236 clinicalnotes from 204,165 adult oncology patients. Domain experts annotated categoriesrelated to treatment regimens and toxicities. We developed rule-based, machinelearning-based (Random Forest, Support Vector Machine [SVM], LogisticRegression [LR]), deep learning-based (BERT, ClinicalBERT), and large languagemodels (LLM)-based NLP approaches (zero-shot and error-analysis prompting).Models used an 80:20 train-test split. Results: Sufficient data existed to train and evaluate 5 annotatedcategories. Error-analysis prompting achieved optimal precision, recall, and F1scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shotprompting reached F1=1.000 for treatment and F1=0.876 for toxicitiesextraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learningunderperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) andClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methodsserved as our baseline with F1 scores of 0.857 in treatment and 0.858 intoxicities. Discussion: LMM-based approaches outperformed all others, followed by machinelearning methods. Machine and deep learning approaches were limited by smalltraining data and showed limited generalizability, particularly for rarecategories. Conclusion: LLM-based NLP most effectively extracted fluoropyrimidinetreatment and toxicity information from clinical notes, and has strongpotential to support oncology research and pharmacovigilance.</description><author>Xizhi Wu, Madeline S. Kreider, Philip E. Empey, Chenyu Li, Yanshan Wang</author><pubDate>Thu, 23 Oct 2025 16:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20727v1</guid></item><item><title>AutoScape: Geometry-Consistent Long-Horizon Scene Generation</title><link>http://arxiv.org/abs/2510.20726v1</link><description>This paper proposes AutoScape, a long-horizon driving scene generationframework. At its core is a novel RGB-D diffusion model that iterativelygenerates sparse, geometrically consistent keyframes, serving as reliableanchors for the scene's appearance and geometry. To maintain long-rangegeometric consistency, the model 1) jointly handles image and depth in a sharedlatent space, 2) explicitly conditions on the existing scene geometry (i.e.,rendered point clouds) from previously generated keyframes, and 3) steers thesampling process with a warp-consistent guidance. Given high-quality RGB-Dkeyframes, a video diffusion model then interpolates between them to producedense and coherent video frames. AutoScape generates realistic andgeometrically consistent driving videos of over 20 seconds, improving thelong-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and43.0\%, respectively.</description><author>Jiacheng Chen, Ziyu Jiang, Mingfu Liang, Bingbing Zhuang, Jong-Chyi Su, Sparsh Garg, Ying Wu, Manmohan Chandraker</author><pubDate>Thu, 23 Oct 2025 16:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20726v1</guid></item><item><title>No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes</title><link>http://arxiv.org/abs/2510.20725v1</link><description>Thompson sampling (TS) is a powerful and widely used strategy for sequentialdecision-making, with applications ranging from Bayesian optimization toreinforcement learning (RL). Despite its success, the theoretical foundationsof TS remain limited, particularly in settings with complex temporal structuresuch as RL. We address this gap by establishing no-regret guarantees for TSusing models with Gaussian marginal distributions. Specifically, we consider TSin episodic RL with joint Gaussian process (GP) priors over rewards andtransitions. We prove a regret bound of$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysisaddresses several challenges, including the non-Gaussian nature of valuefunctions and the recursive structure of Bellman updates, and extends classicaltools such as the elliptical potential lemma to multi-output settings. Thiswork advances the understanding of TS in RL and highlights how structuralassumptions and model uncertainty shape its performance in finite-horizonMarkov Decision Processes.</description><author>Jasmine Bayrooti, Sattar Vakili, Amanda Prorok, Carl Henrik Ek</author><pubDate>Thu, 23 Oct 2025 16:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20725v1</guid></item><item><title>mmWalk: Towards Multi-modal Multi-view Walking Assistance</title><link>http://arxiv.org/abs/2510.11520v2</link><description>Walking assistance in extreme or complex environments remains a significantchallenge for people with blindness or low vision (BLV), largely due to thelack of a holistic scene understanding. Motivated by the real-world needs ofthe BLV community, we build mmWalk, a simulated multi-modal dataset thatintegrates multi-view sensor and accessibility-oriented features for outdoorsafe navigation. Our dataset comprises 120 manually controlled,scenario-categorized walking trajectories with 62k synchronized frames. Itcontains over 559k panoramic images across RGB, depth, and semantic modalities.Furthermore, to emphasize real-world relevance, each trajectory involvesoutdoor corner cases and accessibility-specific landmarks for BLV users.Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visualquestion-answer triplets across 9 categories tailored for safe and informedwalking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)using zero- and few-shot settings and found they struggle with our riskassessment and navigational tasks. We validate our mmWalk-finetuned model onreal-world datasets and show the effectiveness of our dataset for advancingmulti-modal walking assistance.</description><author>Kedi Ying, Ruiping Liu, Chongyan Chen, Mingzhe Tao, Hao Shi, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen</author><pubDate>Thu, 23 Oct 2025 16:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.11520v2</guid></item><item><title>User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios</title><link>http://arxiv.org/abs/2510.20721v1</link><description>Large language models (LLMs) have seen rapid adoption for tasks such asdrafting emails, summarizing meetings, and answering health questions. In suchuses, users may need to share private information (e.g., health records,contact details). To evaluate LLMs' ability to identify and redact such privateinformation, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) withreal-life scenarios. Using these benchmarks, researchers have found that LLMssometimes fail to keep secrets private when responding to complex tasks (e.g.,leaking employee salaries in meeting summaries). However, these evaluationsrely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlookingreal users' perceptions. Moreover, prior work primarily focused on theprivacy-preservation quality of responses, without investigating nuanceddifferences in helpfulness. To understand how users perceive theprivacy-preservation quality and helpfulness of LLM responses toprivacy-sensitive scenarios, we conducted a user study with 94 participantsusing 90 scenarios from PrivacyLens. We found that, when evaluating identicalresponses to the same scenario, users showed low agreement with each other onthe privacy-preservation quality and helpfulness of the LLM response. Further,we found high agreement among five proxy LLMs, while each individual LLM hadlow correlation with users' evaluations. These results indicate that theprivacy and helpfulness of LLM responses are often specific to individuals, andproxy LLMs are poor estimates of how real users would perceive these responsesin privacy-sensitive scenarios. Our results suggest the need to conductuser-centered studies on measuring LLMs' ability to help users while preservingprivacy. Additionally, future research could investigate ways to improve thealignment between proxy LLMs and users for better estimation of users'perceived privacy and utility.</description><author>Xiaoyuan Wu, Roshni Kaushik, Wenkai Li, Lujo Bauer, Koichi Onoue</author><pubDate>Thu, 23 Oct 2025 16:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20721v1</guid></item><item><title>Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding</title><link>http://arxiv.org/abs/2508.19529v2</link><description>Discrete diffusion language models have shown strong potential for textgeneration, yet standard supervised fine-tuning (SFT) misaligns with theirsemi-autoregressive inference: training randomly masks tokens across the entireresponse, while inference generates fixed-size blocks sequentially. Thismismatch introduces noisy prefixes and leaky suffixes, biasing gradients awayfrom the desired blockwise likelihood. We propose Blockwise SFT, whichpartitions responses into fixed-size blocks, selects one active block per stepfor stochastic masking, freezes all preceding tokens, and fully hides futureones. Loss is computed only over the active block, directly mirroring theblockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA showconsistent gains over classical SFT under equal compute or token budgets. Blocksize consistency studies and ablations confirm that improvements stem fromfaithful training-inference alignment rather than incidental masking effects.Our results highlight the importance of matching supervision granularity to thedecoding procedure in diffusion-based language models.</description><author>Bowen Sun, Yujun Cai, Ming-Hsuan Yang, Yiwei Wang</author><pubDate>Thu, 23 Oct 2025 16:36:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19529v2</guid></item><item><title>Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series</title><link>http://arxiv.org/abs/2510.20718v1</link><description>Semiconductor manufacturing is an extremely complex and precision-drivenprocess, characterized by thousands of interdependent parameters collectedacross diverse tools and process steps. Multi-variate time-series analysis hasemerged as a critical field for real-time monitoring and fault detection insuch environments. However, anomaly prediction in semiconductor fabricationpresents several critical challenges, including high dimensionality of sensordata and severe class imbalance due to the rarity of true faults. Furthermore,the complex interdependencies between variables complicate both anomalyprediction and root-cause-analysis. This paper proposes two novel approaches toadvance the field from anomaly detection to anomaly prediction, an essentialstep toward enabling real-time process correction and proactive faultprevention. The proposed anomaly prediction framework contains two main stages:(a) training a forecasting model on a dataset assumed to contain no anomalies,and (b) performing forecast on unseen time series data. The forecast iscompared with the forecast of the trained signal. Deviations beyond apredefined threshold are flagged as anomalies. The two approaches differ in theforecasting model employed. The first assumes independence between variables byutilizing the N-BEATS model for univariate time series forecasting. The secondlifts this assumption by utilizing a Graph Neural Network (GNN) to captureinter-variable relationships. Both models demonstrate strong forecastingperformance up to a horizon of 20 time points and maintain stable anomalyprediction up to 50 time points. The GNN consistently outperforms the N-BEATSmodel while requiring significantly fewer trainable parameters and lowercomputational cost. These results position the GNN as promising solution foronline anomaly forecasting to be deployed in manufacturing environments.</description><author>Daniel Sorensen, Bappaditya Dey, Minjin Hwang, Sandip Halder</author><pubDate>Thu, 23 Oct 2025 16:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20718v1</guid></item><item><title>Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool</title><link>http://arxiv.org/abs/2510.20714v1</link><description>In this study we aim to better align fall risk prediction from the JohnsHopkins Fall Risk Assessment Tool (JHFRAT) with additional clinicallymeaningful measures via a data-driven modelling approach. We conducted aretrospective analysis of 54,209 inpatient admissions from three Johns HopkinsHealth System hospitals between March 2022 and October 2023. A total of 20,208admissions were included as high fall risk encounters, and 13,941 were includedas low fall risk encounters. To incorporate clinical knowledge and maintaininterpretability, we employed constrained score optimization (CSO) models onJHFRAT assessment data and additional electronic health record (EHR) variables.The model demonstrated significant improvements in predictive performance overthe current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrainedscore optimization models performed similarly with and without the EHRvariables. Although the benchmark black-box model (XGBoost), improves upon theperformance metrics of the knowledge-based constrained logistic regression(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risklabelling. This evidence-based approach provides a robust foundation for healthsystems to systematically enhance inpatient fall prevention protocols andpatient safety using data-driven optimization techniques, contributing toimproved risk assessment and resource allocation in healthcare settings.</description><author>Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Kimia Ghobadi</author><pubDate>Thu, 23 Oct 2025 16:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20714v1</guid></item><item><title>Experimental differentiation and extremization with analog quantum circuits</title><link>http://arxiv.org/abs/2510.20713v1</link><description>Solving and optimizing differential equations (DEs) is ubiquitous in bothengineering and fundamental science. The promise of quantum architectures toaccelerate scientific computing thus naturally involved interest towards howefficiently quantum algorithms can solve DEs. Differentiable quantum circuits(DQC) offer a viable route to compute DE solutions using a variational approachamenable to existing quantum computers, by producing a machine-learnablesurrogate of the solution. Quantum extremal learning (QEL) complements suchapproach by finding extreme points in the output of learnable models of unknown(implicit) functions, offering a powerful tool to bypass a full DE solution, incases where the crux consists in retrieving solution extrema. In this work, weprovide the results from the first experimental demonstration of both DQC andQEL, displaying their performance on a synthetic usecase. Whilst both DQC andQEL are expected to require digital quantum hardware, we successfully challengethis assumption by running a closed-loop instance on a commercial analogquantum computer, based upon neutral atom technology.</description><author>Evan Philip, Julius de Hond, Vytautas Abramavicius, Kaonan Micadei, Mario Dagrada, Panagiotis Barkoutsos, Mourad Beji, Louis-Paul Henry, Vincent E. Elfving, Antonio A. Gentile, Savvas Varsamopoulos</author><pubDate>Thu, 23 Oct 2025 16:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20713v1</guid></item><item><title>CLEVER: A Curated Benchmark for Formally Verified Code Generation</title><link>http://arxiv.org/abs/2505.13938v4</link><description>We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of161 problems for end-to-end verified code generation in Lean. Each problemconsists of (1) the task of generating a specification that matches a held-outground-truth specification, and (2) the task of generating a Leanimplementation that provably satisfies this specification. Unlike priorbenchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generatedannotations, and specifications that leak implementation logic or allow vacuoussolutions. All outputs are verified post-hoc using Lean's type checker toensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ toevaluate several few-shot and agentic approaches based on state-of-the-artlanguage models. These methods all struggle to achieve full verification,establishing it as a challenging frontier benchmark for program synthesis andformal reasoning. Our benchmark can be found onGitHub(https://github.com/trishullab/clever) as well asHuggingFace(https://huggingface.co/datasets/amitayusht/clever). All ourevaluation code is also availableonline(https://github.com/trishullab/clever-prover).</description><author>Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzsche, Greg Durrett, Yisong Yue, Swarat Chaudhuri</author><pubDate>Thu, 23 Oct 2025 16:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.13938v4</guid></item><item><title>Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning</title><link>http://arxiv.org/abs/2504.15275v3</link><description>Process reward models (PRMs) have proven effective for test-time scaling ofLarge Language Models (LLMs) on challenging reasoning tasks. However, rewardhacking issues with PRMs limit their successful application in reinforcementfine-tuning. In this paper, we identify the main cause of PRM-induced rewardhacking: the canonical summation-form credit assignment in reinforcementlearning (RL), which defines the value as cumulative gamma-decayed futurerewards, easily induces LLMs to hack steps with high rewards. To address this,we propose PURE: Process sUpervised Reinforcement lEarning. The key innovationof PURE is a min-form credit assignment that formulates the value function asthe minimum of future rewards. This method significantly alleviates rewardhacking by limiting the value function range and distributing advantages morereasonably. Through extensive experiments on 3 base models, we show thatPRM-based approaches enabling min-form credit assignment achieve comparablereasoning performance to verifiable reward-based methods within only 30% steps.In contrast, the canonical sum-form credit assignment collapses training evenat the beginning! Additionally, when we supplement PRM-based fine-tuning withjust 10% verifiable rewards, we further alleviate reward hacking and producethe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5benchmarks. Moreover, we summarize the observed reward hacking cases andanalyze the causes of training collapse. We release our code and model weightsat https://github.com/CJReinforce/PURE.</description><author>Jie Cheng, Gang Xiong, Ruixi Qiao, Lijun Li, Chao Guo, Junle Wang, Yisheng Lv, Fei-Yue Wang</author><pubDate>Thu, 23 Oct 2025 16:28:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.15275v3</guid></item><item><title>Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning</title><link>http://arxiv.org/abs/2504.18458v2</link><description>When applying reinforcement learning--typically through GRPO--to largevision-language model reasoning struggles to effectively scale reasoning lengthor generates verbose outputs across all tasks with only marginal gains inaccuracy. To address this issue, we present FAST-GRPO, a variant of GRPO thatdynamically adapts reasoning depth based on question characteristics. Throughempirical analysis, we establish the feasibility of fast-slow thinking in LVLMsby investigating how response length and data distribution affect performance.Inspired by these observations, we introduce two complementary metrics toestimate the difficulty of the questions, guiding the model to determine whenfast or slow thinking is more appropriate. Next, we incorporate adaptivelength-based rewards and difficulty-aware KL divergence into the GRPOalgorithm. Experiments across seven reasoning benchmarks demonstrate that FASTachieves state-of-the-art accuracy with over 10\% relative improvement comparedto the base model, while reducing token usage by 32.7-67.3\% compared toprevious slow-thinking approaches, effectively balancing reasoning length andaccuracy.</description><author>Wenyi Xiao, Leilei Gan</author><pubDate>Thu, 23 Oct 2025 16:25:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.18458v2</guid></item><item><title>Separating the what and how of compositional computation to enable reuse and continual learning</title><link>http://arxiv.org/abs/2510.20709v1</link><description>The ability to continually learn, retain and deploy skills to accomplishgoals is a key feature of intelligent and efficient behavior. However, theneural mechanisms facilitating the continual learning and flexible(re-)composition of skills remain elusive. Here, we study continual learningand the compositional reuse of learned computations in recurrent neural network(RNN) models using a novel two-system approach: one system that infers whatcomputation to perform, and one that implements how to perform it. We focus ona set of compositional cognitive tasks commonly studied in neuroscience. Toconstruct the what system, we first show that a large family of tasks can besystematically described by a probabilistic generative model, wherecompositionality stems from a shared underlying vocabulary of discrete taskepochs. The shared epoch structure makes these tasks inherently compositional.We first show that this compositionality can be systematically described by aprobabilistic generative model. Furthermore, We develop an unsupervised onlinelearning approach that can learn this model on a single-trial basis, buildingits vocabulary incrementally as it is exposed to new tasks, and inferring thelatent epoch structure as a time-varying computational context within a trial.We implement the how system as an RNN whose low-rank components are composedaccording to the context inferred by the what system. Contextual inferencefacilitates the creation, learning, and reuse of low-rank RNN components as newtasks are introduced sequentially, enabling continual learning withoutcatastrophic forgetting. Using an example task set, we demonstrate the efficacyand competitive performance of this two-system learning framework, itspotential for forward and backward transfer, as well as fast compositionalgeneralization to unseen tasks.</description><author>Haozhe Shan, Sun Minni, Lea Duncker</author><pubDate>Thu, 23 Oct 2025 16:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20709v1</guid></item><item><title>TabR1: Taming GRPO for tabular reasoning LLMs</title><link>http://arxiv.org/abs/2510.17385v2</link><description>Tabular prediction has traditionally relied on gradient-boosted decisiontrees and specialized deep learning models, which excel within tasks butprovide limited interpretability and weak transfer across tables. Reasoninglarge language models (LLMs) promise cross-task adaptability with trans- parentreasoning traces, yet their potential has not been fully realized for tabulardata. This paper presents TabR1, the first reasoning LLM for tabular predictionwith multi-step reasoning. At its core is Permutation Relative PolicyOptimization (PRPO), a simple yet efficient reinforcement learning method thatencodes column-permutation invariance as a structural prior. By construct- ingmultiple label-preserving permutations per sample and estimating advantagesboth within and across permutations, PRPO transforms sparse rewards into denselearning signals and improves generalization. With limited supervision, PRPOactivates the reasoning ability of LLMs for tabular prediction, enhancingfew-shot and zero-shot performance as well as interpretability. Comprehensiveexperiments demonstrate that TabR1 achieves performance comparable to strongbaselines under full-supervision fine-tuning. In the zero-shot setting, TabR1approaches the performance of strong baselines under the 32-shot setting.Moreover, TabR1 (8B) substantially outperforms much larger LLMs across varioustasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).</description><author>Pengxiang Cai, Zihao Gao, Jintai Chen</author><pubDate>Thu, 23 Oct 2025 16:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.17385v2</guid></item><item><title>ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata</title><link>http://arxiv.org/abs/2510.20708v1</link><description>3D LiDAR sensors are essential for autonomous navigation, environmentalmonitoring, and precision mapping in remote sensing applications. Toefficiently process the massive point clouds generated by these sensors, LiDARdata is often projected into 2D range images that organize points by theirangular positions and distances. While these range image representations enableefficient processing, conventional projection methods suffer from fundamentalgeometric inconsistencies that cause irreversible information loss,compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDARIntrinsic Calibration Estimation for Lossless Range Images), the first general,sensor-agnostic method that achieves lossless range image generation fromspinning LiDAR point clouds without requiring manufacturer metadata orcalibration files. Our algorithm automatically reverse-engineers the intrinsicgeometry of any spinning LiDAR sensor by inferring critical parametersincluding laser beam configuration, angular distributions, and per-beamcalibration corrections, enabling lossless projection and complete point cloudreconstruction with zero point loss. Comprehensive evaluation across thecomplete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfectpoint preservation, with zero points lost across all point clouds. Geometricaccuracy is maintained well within sensor precision limits, establishinggeometric losslessness with real-time performance. We also present acompression case study that validates substantial downstream benefits,demonstrating significant quality improvements in practical applications. Thisparadigm shift from approximate to lossless LiDAR projections opens newpossibilities for high-precision remote sensing applications requiring completegeometric preservation.</description><author>Samuel Soutullo, Miguel Yermo, David L. Vilari√±o, √ìscar G. Lorenzo, Jos√© C. Cabaleiro, Francisco F. Rivera</author><pubDate>Thu, 23 Oct 2025 16:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20708v1</guid></item><item><title>Structured Spectral Graph Representation Learning for Multi-label Abnormality Analysis from 3D CT Scans</title><link>http://arxiv.org/abs/2510.10779v2</link><description>With the growing volume of CT examinations, there is an increasing demand forautomated tools such as organ segmentation, abnormality detection, and reportgeneration to support radiologists in managing their clinical workload.Multi-label classification of 3D Chest CT scans remains a critical yetchallenging problem due to the complex spatial relationships inherent involumetric data and the wide variability of abnormalities. Existing methodsbased on 3D convolutional neural networks struggle to capture long-rangedependencies, while Vision Transformers often require extensive pre-training onlarge-scale, domain-specific datasets to perform competitively. In this work ofacademic research, we propose a 2.5D alternative by introducing a newgraph-based framework that represents 3D CT volumes as structured graphs, whereaxial slice triplets serve as nodes processed through spectral graphconvolution, enabling the model to reason over inter-slice dependencies whilemaintaining complexity compatible with clinical deployment. Our method, trainedand evaluated on 3 datasets from independent institutions, achieves strongcross-dataset generalization, and shows competitive performance compared tostate-of-the-art visual encoders. We further conduct comprehensive ablationstudies to evaluate the impact of various aggregation strategies,edge-weighting schemes, and graph connectivity patterns. Additionally, wedemonstrate the broader applicability of our approach through transferexperiments on automated radiology report generation and abdominal CT data.</description><author>Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel</author><pubDate>Thu, 23 Oct 2025 16:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.10779v2</guid></item><item><title>Sampling from multi-modal distributions with polynomial query complexity in fixed dimension via reverse diffusion</title><link>http://arxiv.org/abs/2501.00565v3</link><description>Even in low dimensions, sampling from multi-modal distributions ischallenging. We provide the first sampling algorithm for a broad class ofdistributions -- including all Gaussian mixtures -- with a query complexitythat is polynomial in the parameters governing multi-modality, assuming fixeddimension. Our sampling algorithm simulates a time-reversed diffusion process,using a self-normalized Monte Carlo estimator of the intermediate scorefunctions. Unlike previous works, it avoids metastability, requires no priorknowledge of the mode locations, and relaxes the well-known log-smoothnessassumption which excluded general Gaussian mixtures so far.</description><author>Adrien Vacher, Omar Chehab, Anna Korba</author><pubDate>Thu, 23 Oct 2025 16:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.00565v3</guid></item><item><title>Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models</title><link>http://arxiv.org/abs/2510.20707v1</link><description>Recent large vision-language models (LVLMs) demonstrate remarkablecapabilities in processing extended multi-modal sequences, yet the resultingkey-value (KV) cache expansion creates a critical memory bottleneck thatfundamentally limits deployment scalability. While existing KV cachecompression methods focus on retaining high-importance KV pairs to minimizestorage, they often overlook the modality-specific semantic redundancy patternsthat emerge distinctively in multi-modal KV caches. In this work, we firstanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varyinglevels of redundancy across attention heads. We show that relying solely onimportance can only cover a subset of the full KV cache informationdistribution, leading to potential loss of semantic coverage. To address this,we propose \texttt{MixKV}, a novel method that mixes importance with diversityfor optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wisesemantic redundancy, selectively balancing diversity and importance whencompressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV}consistently enhances existing methods across multiple LVLMs. Under extremecompression (budget=64), \texttt{MixKV} improves baseline methods by an averageof \textbf{5.1\%} across five multi-modal understanding benchmarks and achievesremarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV onGUI grounding tasks, all while maintaining comparable inference efficiency.Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparableperformance gains. Our code is available at\href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.</description><author>Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang</author><pubDate>Thu, 23 Oct 2025 16:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20707v1</guid></item><item><title>Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning</title><link>http://arxiv.org/abs/2510.20706v1</link><description>Model-free reinforcement learning (RL) has enabled adaptable and agilequadruped locomotion; however, policies often converge to a single gait,leading to suboptimal performance. Traditionally, Model Predictive Control(MPC) has been extensively used to obtain task-specific optimal policies butlacks the ability to adapt to varying environments. To address theselimitations, we propose an optimization framework for real-time gait adaptationin a continuous gait space, combining the Model Predictive Path Integral (MPPI)algorithm with a Dreamer module to produce adaptive and optimal policies forquadruped locomotion. At each time step, MPPI jointly optimizes the actions andgait variables using a learned Dreamer reward that promotes velocity tracking,energy efficiency, stability, and smooth transitions, while penalizing abruptgait changes. A learned value function is incorporated as terminal reward,extending the formulation to an infinite-horizon planner. We evaluate ourframework in simulation on the Unitree Go1, demonstrating an average reductionof up to 36.48\% in energy consumption across varying target speeds, whilemaintaining accurate tracking and adaptive, task-appropriate gaits.</description><author>Ganga Nair B, Prakrut Kotecha, Shishir Kolathaya</author><pubDate>Thu, 23 Oct 2025 16:17:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20706v1</guid></item><item><title>On the Emergence of Linear Analogies in Word Embeddings</title><link>http://arxiv.org/abs/2505.18651v2</link><description>Models such as Word2Vec and GloVe construct word embeddings based on theco-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. Theresulting vectors $W_i$ not only group semantically similar words but alsoexhibit a striking linear analogy structure -- for example, $W_{\text{king}} -W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whosetheoretical origin remains unclear. Previous observations indicate that thisanalogy structure: (i) already emerges in the top eigenvectors of the matrix$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as moreeigenvectors of $M (i, j)$, which controls the dimension of the embeddings, areincluded, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and(iv) persists even when all word pairs involved in a specific analogy relation(e.g., king-queen, man-woman) are removed from the corpus. To explain thesephenomena, we introduce a theoretical generative model in which words aredefined by binary semantic attributes, and co-occurrence probabilities arederived from attribute-based interactions. This model analytically reproducesthe emergence of linear analogy structure and naturally accounts for properties(i)-(iv). It can be viewed as giving fine-grained resolution into the role ofeach additional embedding dimension. It is robust to various forms of noise andagrees well with co-occurrence statistics measured on Wikipedia and the analogybenchmark introduced by Mikolov et al.</description><author>Daniel J. Korchinski, Dhruva Karkada, Yasaman Bahri, Matthieu Wyart</author><pubDate>Thu, 23 Oct 2025 16:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.18651v2</guid></item><item><title>Stochastic gradient descent in high dimensions for multi-spiked tensor PCA</title><link>http://arxiv.org/abs/2410.18162v2</link><description>We study the high-dimensional dynamics of online stochastic gradient descent(SGD) for the multi-spiked tensor model. This multi-index model arises from thetensor principal component analysis (PCA) problem with multiple spikes, wherethe goal is to estimate $r$ unknown signal vectors within the $N$-dimensionalunit sphere through maximum likelihood estimation from noisy observations of a$p$-tensor. We determine the number of samples and the conditions on thesignal-to-noise ratios (SNRs) required to efficiently recover the unknownspikes from natural random initializations. We show that full recovery of allspikes is possible provided a number of sample scaling as $N^{p-2}$, matchingthe algorithmic threshold identified in the rank-one case [Ben Arous,Gheissari, Jagannath 2020, 2021]. Our results are obtained through a detailedanalysis of a low-dimensional system that describes the evolution of thecorrelations between the estimators and the spikes, while controlling the noisein the dynamics. We find that the spikes are recovered sequentially in aprocess we term "sequential elimination": once a correlation exceeds a criticalthreshold, all correlations sharing a row or column index become sufficientlysmall, allowing the next correlation to grow and become macroscopic. The orderin which correlations become macroscopic depends on their initial values andthe corresponding SNRs, leading to either exact recovery or recovery of apermutation of the spikes. In the matrix case, when $p=2$, if the SNRs aresufficiently separated, we achieve exact recovery of the spikes, whereas equalSNRs lead to recovery of the subspace spanned by them.</description><author>G√©rard Ben Arous, C√©dric Gerbelot, Vanessa Piccolo</author><pubDate>Thu, 23 Oct 2025 16:14:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18162v2</guid></item><item><title>Structure-Conditional Minimum Bayes Risk Decoding</title><link>http://arxiv.org/abs/2510.20700v1</link><description>Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternativeto traditional generation strategies. While MBR has proven effective in machinetranslation, where the variability of a language model's outcome space isnaturally constrained, it may face challenges in more open-ended tasks such asdialogue or instruction-following. We hypothesise that in such settings,applying MBR with standard similarity-based utility functions may result inselecting responses that are broadly representative of the model'sdistribution, yet sub-optimal with respect to any particular grouping ofgenerations that share an underlying latent structure. In this work, weintroduce three lightweight adaptations to the utility function, designed tomake MBR more sensitive to structural variability in the outcome space. To testour hypothesis, we curate a dataset capturing three representative types oflatent structure: dialogue act, emotion, and response structure (e.g., asentence, a paragraph, or a list). We further propose two metrics to evaluatethe structural optimality of MBR. Our analysis demonstrates that commonsimilarity-based utility functions fall short by these metrics. In contrast,our proposed adaptations considerably improve structural optimality. Finally,we evaluate our approaches on real-world instruction-following benchmarks,AlpacaEval and MT-Bench, and show that increased structural sensitivityimproves generation quality by up to 13.7 percentage points in win rate.</description><author>Bryan Eikema, Anna Rutkiewicz, Mario Giulianelli</author><pubDate>Thu, 23 Oct 2025 16:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20700v1</guid></item><item><title>Fusing Narrative Semantics for Financial Volatility Forecasting</title><link>http://arxiv.org/abs/2510.20699v1</link><description>We introduce M2VN: Multi-Modal Volatility Network, a novel deeplearning-based framework for financial volatility forecasting that unifies timeseries features with unstructured news data. M2VN leverages therepresentational power of deep neural networks to address two key challenges inthis domain: (i) aligning and fusing heterogeneous data modalities, numericalfinancial data and textual information, and (ii) mitigating look-ahead biasthat can undermine the validity of financial models. To achieve this, M2VNcombines open-source market features with news embeddings generated by TimeMachine GPT, a recently introduced point-in-time LLM, ensuring temporalintegrity. An auxiliary alignment loss is introduced to enhance the integrationof structured and unstructured data within the deep learning architecture.Extensive experiments demonstrate that M2VN consistently outperforms existingbaselines, underscoring its practical value for risk management and financialdecision-making in dynamic markets.</description><author>Yaxuan Kong, Yoontae Hwang, Marcus Kaiser, Chris Vryonides, Roel Oomen, Stefan Zohren</author><pubDate>Thu, 23 Oct 2025 16:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20699v1</guid></item><item><title>FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation</title><link>http://arxiv.org/abs/2504.15958v3</link><description>Subject-driven image generation aims to synthesize novel scenes thatfaithfully preserve subject identity from reference images while adhering totextual guidance. However, existing methods struggle with a critical trade-offbetween fidelity and efficiency. Tuning-based approaches rely on time-consumingand resource-intensive, subject-specific optimization, while zero-shot methodsoften fail to maintain adequate subject consistency. In this work, we proposeFreeGraftor, a training-free framework that addresses these limitations throughcross-image feature grafting. Specifically, FreeGraftor leverages semanticmatching and position-constrained attention fusion to transfer visual detailsfrom reference subjects to the generated images. Additionally, our frameworkintroduces a novel noise initialization strategy to preserve the geometrypriors of reference subjects, facilitating robust feature matching. Extensivequalitative and quantitative experiments demonstrate that our method enablesprecise subject identity transfer while maintaining text-aligned scenesynthesis. Without requiring model fine-tuning or additional training,FreeGraftor significantly outperforms existing zero-shot and training-freeapproaches in both subject fidelity and text alignment. Furthermore, ourframework can seamlessly extend to multi-subject generation, making itpractical for real-world deployment. Our code is available athttps://github.com/Nihukat/FreeGraftor.</description><author>Zebin Yao, Lei Ren, Huixing Jiang, Chen Wei, Xiaojie Wang, Ruifan Li, Fangxiang Feng</author><pubDate>Thu, 23 Oct 2025 16:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.15958v3</guid></item><item><title>Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward</title><link>http://arxiv.org/abs/2510.20696v1</link><description>Multimodal large language models (MLLMs) that integrate visual and textualreasoning leverage chain-of-thought (CoT) prompting to tackle complex visualtasks, yet continue to exhibit visual hallucinations and an over-reliance ontextual priors. We present a systematic diagnosis of state-of-the-artvision-language models using a three-stage evaluation framework, uncovering keyfailure modes. To address these, we propose an agent-based architecture thatcombines LLM reasoning with lightweight visual modules, enabling fine-grainedanalysis and iterative refinement of reasoning chains. Our results highlightfuture visual reasoning models should focus on integrating a broader set ofspecialized tools for analyzing visual content. Our system achieves significantgains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching orsurpassing much larger models. We will release our framework and evaluationsuite to facilitate future research.</description><author>Jing Bi, Guangyu Sun, Ali Vosoughi, Chen Chen, Chenliang Xu</author><pubDate>Thu, 23 Oct 2025 16:10:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20696v1</guid></item></channel></rss>