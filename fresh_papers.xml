<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 23 Apr 2024 06:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AutoAD III: The Prequel -- Back to the Pixels</title><link>http://arxiv.org/abs/2404.14412v1</link><description>Generating Audio Description (AD) for movies is a challenging task thatrequires fine-grained visual understanding and an awareness of the charactersand their names. Currently, visual language models for AD generation arelimited by a lack of suitable training data, and also their evaluation ishampered by using performance measures not specialized to the AD domain. Inthis paper, we make three contributions: (i) We propose two approaches forconstructing AD datasets with aligned video data, and build training andevaluation datasets using these. These datasets will be publicly released; (ii)We develop a Q-former-based architecture which ingests raw video and generatesAD, using frozen pre-trained visual encoders and large language models; and(iii) We provide new evaluation metrics to benchmark AD quality that arewell-matched to human performance. Taken together, we improve the state of theart on AD generation.</description><author>Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman</author><pubDate>Mon, 22 Apr 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14412v1</guid></item><item><title>Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses</title><link>http://arxiv.org/abs/2404.14410v1</link><description>In this paper, we present a method to reconstruct the world and multipledynamic humans in 3D from a monocular video input. As a key idea, we representboth the world and multiple humans via the recently emerging 3D GaussianSplatting (3D-GS) representation, enabling to conveniently and efficientlycompose and render them together. In particular, we address the scenarios withseverely limited and sparse observations in 3D human reconstruction, a commonchallenge encountered in the real world. To tackle this challenge, we introducea novel approach to optimize the 3D-GS representation in a canonical space byfusing the sparse cues in the common space, where we leverage a pre-trained 2Ddiffusion model to synthesize unseen views while keeping the consistency withthe observed 2D appearances. We demonstrate our method can reconstructhigh-quality animatable 3D humans in various challenging examples, in thepresence of occlusion, image crops, few-shot, and extremely sparseobservations. After reconstruction, our method is capable of not only renderingthe scene in any novel views at arbitrary time instances, but also editing the3D scene by removing individual humans or applying different motions for eachhuman. Through various experiments, we demonstrate the quality and efficiencyof our methods over alternative existing approaches.</description><author>Inhee Lee, Byungjun Kim, Hanbyul Joo</author><pubDate>Mon, 22 Apr 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14410v1</guid></item><item><title>CrossScore: Towards Multi-View Image Evaluation and Scoring</title><link>http://arxiv.org/abs/2404.14409v1</link><description>We introduce a novel cross-reference image quality assessment method thateffectively fills the gap in the image assessment landscape, complementing thearray of established evaluation schemes -- ranging from full-reference metricslike SSIM, no-reference metrics such as NIQE, to general-reference metricsincluding FID, and Multi-modal-reference metrics, e.g., CLIPScore. Utilising aneural network with the cross-attention mechanism and a unique data collectionpipeline from NVS optimisation, our method enables accurate image qualityassessment without requiring ground truth references. By comparing a queryimage against multiple views of the same scene, our method addresses thelimitations of existing metrics in novel view synthesis (NVS) and similar taskswhere direct reference images are unavailable. Experimental results show thatour method is closely correlated to the full-reference metric SSIM, while notrequiring ground truth references.</description><author>Zirui Wang, Wenjing Bian, Omkar Parkhi, Yuheng Ren, Victor Adrian Prisacariu</author><pubDate>Mon, 22 Apr 2024 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14409v1</guid></item><item><title>SpaceByte: Towards Deleting Tokenization from Large Language Modeling</title><link>http://arxiv.org/abs/2404.14408v1</link><description>Tokenization is widely used in large language models because it significantlyimproves performance. However, tokenization imposes several disadvantages, suchas performance biases, increased adversarial vulnerability, decreasedcharacter-level modeling performance, and increased modeling complexity. Toaddress these disadvantages without sacrificing performance, we proposeSpaceByte, a novel byte-level decoder architecture that closes the performancegap between byte-level and subword autoregressive language modeling. SpaceByteconsists of a byte-level Transformer model, but with extra larger transformerblocks inserted in the middle of the layers. We find that performance issignificantly improved by applying these larger blocks only after certainbytes, such as space characters, which typically denote word boundaries. Ourexperiments show that for a fixed training and inference compute budget,SpaceByte outperforms other byte-level architectures and roughly matches theperformance of tokenized Transformer architectures.</description><author>Kevin Slagle</author><pubDate>Mon, 22 Apr 2024 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14408v1</guid></item><item><title>Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular Videos</title><link>http://arxiv.org/abs/2404.12379v2</link><description>Modern 3D engines and graphics pipelines require mesh as a memory-efficientrepresentation, which allows efficient rendering, geometry processing, textureediting, and many other downstream operations. However, it is still highlydifficult to obtain high-quality mesh in terms of structure and detail frommonocular visual observations. The problem becomes even more challenging fordynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh(DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent meshgiven a single monocular video. Our work leverages the recent advancement in 3DGaussian Splatting to construct the mesh sequence with temporal consistencyfrom a video. Building on top of this representation, DG-Mesh recovershigh-quality meshes from the Gaussian points and can track the mesh verticesover time, which enables applications such as texture editing on dynamicobjects. We introduce the Gaussian-Mesh Anchoring, which encourages evenlydistributed Gaussians, resulting better mesh reconstruction through mesh-guideddensification and pruning on the deformed Gaussians. By applyingcycle-consistent deformation between the canonical and the deformed space, wecan project the anchored Gaussian back to the canonical space and optimizeGaussians across all time frames. During the evaluation on different datasets,DG-Mesh provides significantly better mesh reconstruction and rendering thanbaselines. Project page: https://www.liuisabella.com/DG-Mesh/</description><author>Isabella Liu, Hao Su, Xiaolong Wang</author><pubDate>Mon, 22 Apr 2024 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12379v2</guid></item><item><title>Hyp-OC: Hyperbolic One Class Classification for Face Anti-Spoofing</title><link>http://arxiv.org/abs/2404.14406v1</link><description>Face recognition technology has become an integral part of modern securitysystems and user authentication processes. However, these systems arevulnerable to spoofing attacks and can easily be circumvented. Most priorresearch in face anti-spoofing (FAS) approaches it as a two-classclassification task where models are trained on real samples and known spoofattacks and tested for detection performance on unknown spoof attacks. However,in practice, FAS should be treated as a one-class classification task where,while training, one cannot assume any knowledge regarding the spoof samples apriori. In this paper, we reformulate the face anti-spoofing task from aone-class perspective and propose a novel hyperbolic one-class classificationframework. To train our network, we use a pseudo-negative class sampled fromthe Gaussian distribution with a weighted running mean and propose two novelloss functions: (1) Hyp-PC: Hyperbolic Pairwise Confusion loss, and (2) Hyp-CE:Hyperbolic Cross Entropy loss, which operate in the hyperbolic space.Additionally, we employ Euclidean feature clipping and gradient clipping tostabilize the training in the hyperbolic space. To the best of our knowledge,this is the first work extending hyperbolic embeddings for face anti-spoofingin a one-class manner. With extensive experiments on five benchmark datasets:Rose-Youtu, MSU-MFSD, CASIA-MFSD, Idiap Replay-Attack, and OULU-NPU, wedemonstrate that our method significantly outperforms the state-of-the-art,achieving better spoof detection performance.</description><author>Kartik Narayan, Vishal M. Patel</author><pubDate>Mon, 22 Apr 2024 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14406v1</guid></item><item><title>GeoDiffuser: Geometry-Based Image Editing with Diffusion Models</title><link>http://arxiv.org/abs/2404.14403v1</link><description>The success of image generative models has enabled us to build methods thatcan edit images based on text or other user input. However, these methods arebespoke, imprecise, require additional information, or are limited to only 2Dimage edits. We present GeoDiffuser, a zero-shot optimization-based method thatunifies common 2D and 3D image-based object editing capabilities into a singlemethod. Our key insight is to view image editing operations as geometrictransformations. We show that these transformations can be directlyincorporated into the attention layers in diffusion models to implicitlyperform editing operations. Our training-free optimization method uses anobjective function that seeks to preserve object style but generate plausibleimages, for instance with accurate lighting and shadows. It also inpaintsdisoccluded parts of the image where the object was originally located. Given anatural image and user input, we segment the foreground object using SAM andestimate a corresponding transform which is used by our optimization approachfor editing. GeoDiffuser can perform common 2D and 3D edits like objecttranslation, 3D rotation, and removal. We present quantitative results,including a perceptual study, that shows how our approach is better thanexisting methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html formore information.</description><author>Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, Srinath Sridhar</author><pubDate>Mon, 22 Apr 2024 18:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14403v1</guid></item><item><title>A mean curvature flow arising in adversarial training</title><link>http://arxiv.org/abs/2404.14402v1</link><description>We connect adversarial training for binary classification to a geometricevolution equation for the decision boundary. Relying on a perspective thatrecasts adversarial training as a regularization problem, we introduce amodified training scheme that constitutes a minimizing movements scheme for anonlocal perimeter functional. We prove that the scheme is monotone andconsistent as the adversarial budget vanishes and the perimeter localizes, andas a consequence we rigorously show that the scheme approximates a weightedmean curvature flow. This highlights that the efficacy of adversarial trainingmay be due to locally minimizing the length of the decision boundary. In ouranalysis, we introduce a variety of tools for working with the subdifferentialof a supremal-type nonlocal total variation and its regularity properties.</description><author>Leon Bungert, Tim Laux, Kerrek Stinson</author><pubDate>Mon, 22 Apr 2024 18:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14402v1</guid></item><item><title>RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?</title><link>http://arxiv.org/abs/2404.14397v1</link><description>Large language models (LLMs) and small language models (SLMs) are beingadopted at remarkable speed, although their safety still remains a seriousconcern. With the advent of multilingual S/LLMs, the question now becomes amatter of scale: can we expand multilingual safety evaluations of these modelswith the same velocity at which they are deployed? To this end we introduceRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts andoutputs in 28 languages. RTP-LX follows participatory design practices, and aportion of the corpus is especially designed to detect culturally-specifictoxic language. We evaluate seven S/LLMs on their ability to detect toxiccontent in a culturally-sensitive, multilingual scenario. We find that,although they typically score acceptably in terms of accuracy, they have lowagreement with human judges when judging holistically the toxicity of a prompt,and have difficulty discerning harm in context-dependent scenarios,particularly with subtle-yet-harmful content (e.g. microagressions, bias). Werelease of this dataset to contribute to further reduce harmful uses of thesemodels and improve their safe deployment.</description><author>Adrian de Wynter, Ishaan Watts, Nektar Ege Altıntoprak, Tua Wongsangaroonsri, Minghui Zhang, Noura Farra, Lena Baur, Samantha Claudet, Pavel Gajdusek, Can Gören, Qilong Gu, Anna Kaminska, Tomasz Kaminski, Ruby Kuo, Akiko Kyuba, Jongho Lee, Kartik Mathur, Petter Merok, Ivana Milovanović, Nani Paananen, Vesa-Matti Paananen, Anna Pavlenko, Bruno Pereira Vidal, Luciano Strika, Yueh Tsao, Davide Turcato, Oleksandr Vakhno, Judit Velcsov, Anna Vickers, Stéphanie Visser, Herdyan Widarmanto, Andrey Zaikin, Si-Qing Chen</author><pubDate>Mon, 22 Apr 2024 18:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14397v1</guid></item><item><title>A Unified Framework for Model Editing</title><link>http://arxiv.org/abs/2403.14236v2</link><description>We introduce a unifying framework that brings two leading "locate-and-edit"model editing techniques -- ROME and MEMIT -- under a single conceptualumbrella, optimizing for the same goal, which we call thepreservation-memorization objective. ROME uses an equality constraint toperform one edit at a time, whereas MEMIT employs a more flexible least-squareconstraint that allows for batched edits. Following thepreservation-memorization objective, we present Equality-constrained Mass ModelEditing algorithm for Transformers or EMMET, a new batched memory-editingalgorithm that uses a closed-form solution for the equality-constrained versionof the preservation-memorization objective. EMMET is a batched-version of ROMEand is able to perform batched-edits up to a batch-size of 10,000 with verysimilar performance to MEMIT across multiple dimensions. With EMMET, we unifyand achieve symmetry within the "locate-and-edit" algorithms, allowingbatched-editing using both objectives.</description><author>Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli</author><pubDate>Mon, 22 Apr 2024 18:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14236v2</guid></item><item><title>SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation</title><link>http://arxiv.org/abs/2404.14396v1</link><description>The rapid evolution of multimodal foundation model has demonstratedsignificant progresses in vision-language understanding and generation, e.g.,our previous work SEED-LLaMA. However, there remains a gap between itscapability and the real-world applicability, primarily due to the model'slimited capacity to effectively respond to various user instructions andinteract with diverse visual data. In this work, we focus on bridging this gapthrough integrating two enhanced features: (1) comprehending images ofarbitrary sizes and ratios, and (2) enabling multi-granularity imagegeneration. We present a unified and versatile foundation model, namely,SEED-X, which is able to model multi-granularity visual semantics forcomprehension and generation tasks. Besides the competitive results on publicbenchmarks, SEED-X demonstrates its effectiveness in handling real-worldapplications across various domains after instruction tuning. We hope that ourwork will inspire future research into what can be achieved by versatilemultimodal foundation models in real-world applications. The models, codes, anddatasets will be released in https://github.com/AILab-CVC/SEED-X.</description><author>Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan</author><pubDate>Mon, 22 Apr 2024 18:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14396v1</guid></item><item><title>An Adversarial Approach to Evaluating the Robustness of Event Identification Models</title><link>http://arxiv.org/abs/2402.12338v2</link><description>Intelligent machine learning approaches are finding active use for eventdetection and identification that allow real-time situational awareness. Yet,such machine learning algorithms have been shown to be susceptible toadversarial attacks on the incoming telemetry data. This paper considers aphysics-based modal decomposition method to extract features for eventclassification and focuses on interpretable classifiers including logisticregression and gradient boosting to distinguish two types of events: load lossand generation loss. The resulting classifiers are then tested against anadversarial algorithm to evaluate their robustness. The adversarial attack istested in two settings: the white box setting, wherein the attacker knowsexactly the classification model; and the gray box setting, wherein theattacker has access to historical data from the same network as was used totrain the classifier, but does not know the classification model. Thoroughexperiments on the synthetic South Carolina 500-bus system highlight that arelatively simpler model such as logistic regression is more susceptible toadversarial attacks than gradient boosting.</description><author>Obai Bahwal, Oliver Kosut, Lalitha Sankar</author><pubDate>Mon, 22 Apr 2024 18:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12338v2</guid></item><item><title>PARAMANU-GANITA: Language Model with Mathematical Capabilities</title><link>http://arxiv.org/abs/2404.14395v1</link><description>In this paper, we present Paramanu-Ganita, a 208 million parameter novel AutoRegressive (AR) decoder based language model on mathematics. The model ispretrained from scratch at context size of 4096 on our curated mixedmathematical corpus. We evaluate our model on both perplexity metric and GSM8kmathematical benchmark. Paramanu-Ganita despite being 35 times smaller than 7BLLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-27B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, andmath specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0%points in GSM8k test accuracy metric respectively. Paramanu-Ganita alsooutperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8%points, LLaMa-1 33B by 3.8% points and Vicuna 13B by 11.8% points respectively.The large significant margin improvement in performance of our math model overthe existing LLMs signifies that reasoning capabilities of language model arejust not restricted to LLMs with humongous number of parameters.Paramanu-Ganita took 146 hours of A100 training whereas math specialised LLM,LLEMMA 7B, was trained for 23,000 A100 hours of training equivalent. Thus, ourapproach of pretraining powerful domain specialised language models fromscratch for domain adaptation is much more cost-effective than performingcontinual training of LLMs for domain adaptation. Hence, we conclude that forstrong mathematical reasoning abilities of language model, we do not need giantLLMs and immense computing power to our end. In the end, we want to point outthat we have only trained Paramanu-Ganita only on a part of our entiremathematical corpus and yet to explore the full potential of our model.</description><author>Mitodru Niyogi, Arnab Bhattacharya</author><pubDate>Mon, 22 Apr 2024 18:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14395v1</guid></item><item><title>A Multimodal Automated Interpretability Agent</title><link>http://arxiv.org/abs/2404.14394v1</link><description>This paper describes MAIA, a Multimodal Automated Interpretability Agent.MAIA is a system that uses neural models to automate neural model understandingtasks like feature interpretation and failure mode discovery. It equips apre-trained vision-language model with a set of tools that support iterativeexperimentation on subcomponents of other models to explain their behavior.These include tools commonly used by human interpretability researchers: forsynthesizing and editing inputs, computing maximally activating exemplars fromreal-world datasets, and summarizing and describing experimental results.Interpretability experiments proposed by MAIA compose these tools to describeand explain system behavior. We evaluate applications of MAIA to computervision models. We first characterize MAIA's ability to describe (neuron-level)features in learned representations of images. Across several trained modelsand a novel dataset of synthetic vision neurons with paired ground-truthdescriptions, MAIA produces descriptions comparable to those generated byexpert human experimenters. We then show that MAIA can aid in two additionalinterpretability tasks: reducing sensitivity to spurious features, andautomatically identifying inputs likely to be mis-classified.</description><author>Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, Antonio Torralba</author><pubDate>Mon, 22 Apr 2024 18:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14394v1</guid></item><item><title>Trends, Applications, and Challenges in Human Attention Modelling</title><link>http://arxiv.org/abs/2402.18673v2</link><description>Human attention modelling has proven, in recent years, to be particularlyuseful not only for understanding the cognitive processes underlying visualexploration, but also for providing support to artificial intelligence modelsthat aim to solve problems in various domains, including image and videoprocessing, vision-and-language applications, and language modelling. Thissurvey offers a reasoned overview of recent efforts to integrate humanattention mechanisms into contemporary deep learning models and discussesfuture research directions and challenges. For a comprehensive overview on theongoing research refer to our dedicated repository available athttps://github.com/aimagelab/awesome-human-visual-attention.</description><author>Giuseppe Cartella, Marcella Cornia, Vittorio Cuculo, Alessandro D'Amelio, Dario Zanca, Giuseppe Boccignone, Rita Cucchiara</author><pubDate>Mon, 22 Apr 2024 18:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18673v2</guid></item><item><title>Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models</title><link>http://arxiv.org/abs/2310.07712v2</link><description>Large language models (LLMs) exhibit positional bias in how they use context,which especially complicates listwise ranking. To address this, we proposepermutation self-consistency, a form of self-consistency over ranking listoutputs of black-box LLMs. Our key idea is to marginalize out different listorders in the prompt to produce an order-independent ranking with lesspositional bias. First, given some input prompt, we repeatedly shuffle the listin the prompt and pass it through the LLM while holding the instructions thesame. Next, we aggregate the resulting sample of rankings by computing thecentral ranking closest in distance to all of them, marginalizing out promptorder biases in the process. Theoretically, we prove the robustness of ourmethod, showing convergence to the true ranking in the presence of randomperturbations. Empirically, on five list-ranking datasets in sorting andpassage reranking, our approach improves scores from conventional inference byup to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previousstate of the art in passage reranking. Our code is athttps://github.com/castorini/perm-sc.</description><author>Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, Ferhan Ture</author><pubDate>Mon, 22 Apr 2024 18:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07712v2</guid></item><item><title>GeoAI Reproducibility and Replicability: a computational and spatial perspective</title><link>http://arxiv.org/abs/2404.10108v2</link><description>GeoAI has emerged as an exciting interdisciplinary research area thatcombines spatial theories and data with cutting-edge AI models to addressgeospatial problems in a novel, data-driven manner. While GeoAI research hasflourished in the GIScience literature, its reproducibility and replicability(R&amp;R), fundamental principles that determine the reusability, reliability, andscientific rigor of research findings, have rarely been discussed. This paperaims to provide an in-depth analysis of this topic from both computational andspatial perspectives. We first categorize the major goals for reproducing GeoAIresearch, namely, validation (repeatability), learning and adapting the methodfor solving a similar or new problem (reproducibility), and examining thegeneralizability of the research findings (replicability). Each of these goalsrequires different levels of understanding of GeoAI, as well as differentmethods to ensure its success. We then discuss the factors that may cause thelack of R&amp;R in GeoAI research, with an emphasis on (1) the selection and use oftraining data; (2) the uncertainty that resides in the GeoAI model design,training, deployment, and inference processes; and more importantly (3) theinherent spatial heterogeneity of geospatial data and processes. We use a deeplearning-based image analysis task as an example to demonstrate the results'uncertainty and spatial variance caused by different factors. The findingsreiterate the importance of knowledge sharing, as well as the generation of a"replicability map" that incorporates spatial autocorrelation and spatialheterogeneity into consideration in quantifying the spatial replicability ofGeoAI research.</description><author>Wenwen Li, Chia-Yu Hsu, Sizhe Wang, Peter Kedron</author><pubDate>Mon, 22 Apr 2024 18:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10108v2</guid></item><item><title>Soft-constrained Schrodinger Bridge: a Stochastic Control Approach</title><link>http://arxiv.org/abs/2403.01717v2</link><description>Schr\"{o}dinger bridge can be viewed as a continuous-time stochastic controlproblem where the goal is to find an optimally controlled diffusion processwhose terminal distribution coincides with a pre-specified target distribution.We propose to generalize this problem by allowing the terminal distribution todiffer from the target but penalizing the Kullback-Leibler divergence betweenthe two distributions. We call this new control problem soft-constrainedSchr\"{o}dinger bridge (SSB). The main contribution of this work is atheoretical derivation of the solution to SSB, which shows that the terminaldistribution of the optimally controlled process is a geometric mixture of thetarget and some other distribution. This result is further extended to a timeseries setting. One application is the development of robust generativediffusion models. We propose a score matching-based algorithm for sampling fromgeometric mixtures and showcase its use via a numerical example for the MNISTdata set.</description><author>Jhanvi Garg, Xianyang Zhang, Quan Zhou</author><pubDate>Mon, 22 Apr 2024 18:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01717v2</guid></item><item><title>Poisoning Attacks on Federated Learning-based Wireless Traffic Prediction</title><link>http://arxiv.org/abs/2404.14389v1</link><description>Federated Learning (FL) offers a distributed framework to train a globalcontrol model across multiple base stations without compromising the privacy oftheir local network data. This makes it ideal for applications like wirelesstraffic prediction (WTP), which plays a crucial role in optimizing networkresources, enabling proactive traffic flow management, and enhancing thereliability of downstream communication-aided applications, such as IoTdevices, autonomous vehicles, and industrial automation systems. Despite itspromise, the security aspects of FL-based distributed wireless systems,particularly in regression-based WTP problems, remain inadequatelyinvestigated. In this paper, we introduce a novel fake traffic injection (FTI)attack, designed to undermine the FL-based WTP system by injecting fabricatedtraffic distributions with minimal knowledge. We further propose a defensemechanism, termed global-local inconsistency detection (GLID), whichstrategically removes abnormal model parameters that deviate beyond a specificpercentile range estimated through statistical methods in each dimension.Extensive experimental evaluations, performed on real-world wireless trafficdatasets, demonstrate that both our attack and defense strategies significantlyoutperform existing baselines.</description><author>Zifan Zhang, Minghong Fang, Jiayuan Huang, Yuchen Liu</author><pubDate>Mon, 22 Apr 2024 18:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14389v1</guid></item><item><title>STROOBnet Optimization via GPU-Accelerated Proximal Recurrence Strategies</title><link>http://arxiv.org/abs/2404.14388v1</link><description>Spatiotemporal networks' observational capabilities are crucial for accuratedata gathering and informed decisions across multiple sectors. This studyfocuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network(STROOBnet), linking observational nodes (e.g., surveillance cameras) to eventswithin defined geographical regions, enabling efficient monitoring. Using datafrom Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in NewOrleans, where RTCC combats rising crime amidst reduced police presence, weaddress the network's initial observational imbalances. Aiming for uniformobservational efficacy, we propose the Proximal Recurrence approach. Itoutperformed traditional clustering methods like k-means and DBSCAN by offeringholistic event frequency and spatial consideration, enhancing observationalcoverage.</description><author>Ted Edward Holmberg, Mahdi Abdelguerfi, Elias Ioup</author><pubDate>Mon, 22 Apr 2024 18:46:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14388v1</guid></item><item><title>BAMBOO: a predictive and transferable machine learning force field framework for liquid electrolyte development</title><link>http://arxiv.org/abs/2404.07181v4</link><description>Despite the widespread applications of machine learning force field (MLFF) onsolids and small molecules, there is a notable gap in applying MLFF to complexliquid electrolytes. In this work, we introduce BAMBOO (ByteDance AI MolecularSimulation Booster), a novel framework for molecular dynamics (MD) simulations,with a demonstration of its capabilities in the context of liquid electrolytesfor lithium batteries. We design a physics-inspired graph equivarianttransformer architecture as the backbone of BAMBOO to learn from quantummechanical simulations. Additionally, we pioneer an ensemble knowledgedistillation approach and apply it on MLFFs to improve the stability of MDsimulations. Finally, we propose the density alignment algorithm to alignBAMBOO with experimental measurements. BAMBOO demonstrates state-of-the-artaccuracy in predicting key electrolyte properties such as density, viscosity,and ionic conductivity across various solvents and salt combinations. Ourcurrent model, trained on more than 15 chemical species, achieves the averagedensity error of 0.01 g/cm$^3$ on various compositions compared withexperimental data. Moreover, our model demonstrates transferability tomolecules not included in the quantum mechanical dataset. We envision this workas paving the way to a "universal MLFF" capable of simulating properties ofcommon organic liquids.</description><author>Sheng Gong, Yumin Zhang, Zhenliang Mu, Zhichen Pu, Hongyi Wang, Zhiao Yu, Mengyi Chen, Tianze Zheng, Zhi Wang, Lifei Chen, Xiaojie Wu, Shaochen Shi, Weihao Gao, Wen Yan, Liang Xiang</author><pubDate>Mon, 22 Apr 2024 18:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07181v4</guid></item><item><title>A Survey on Self-Evolution of Large Language Models</title><link>http://arxiv.org/abs/2404.14387v1</link><description>Large language models (LLMs) have significantly advanced in various fieldsand intelligent agent applications. However, current LLMs that learn from humanor external model supervision are costly and may face performance ceilings astask complexity and diversity increase. To address this issue, self-evolutionapproaches that enable LLM to autonomously acquire, refine, and learn fromexperiences generated by the model itself are rapidly growing. This newtraining paradigm inspired by the human experiential learning process offersthe potential to scale LLMs towards superintelligence. In this work, we presenta comprehensive survey of self-evolution approaches in LLMs. We first propose aconceptual framework for self-evolution and outline the evolving process asiterative cycles composed of four phases: experience acquisition, experiencerefinement, updating, and evaluation. Second, we categorize the evolutionobjectives of LLMs and LLM-based agents; then, we summarize the literature andprovide taxonomy and insights for each module. Lastly, we pinpoint existingchallenges and propose future directions to improve self-evolution frameworks,equipping researchers with critical insights to fast-track the development ofself-evolving LLMs.</description><author>Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, Jingren Zhou</author><pubDate>Mon, 22 Apr 2024 18:43:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14387v1</guid></item><item><title>Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective</title><link>http://arxiv.org/abs/2306.06615v2</link><description>Molecule discovery plays a crucial role in various scientific fields,advancing the design of tailored materials and drugs. However, most of theexisting methods heavily rely on domain experts, require excessivecomputational cost, or suffer from sub-optimal performance. On the other hand,Large Language Models (LLMs), like ChatGPT, have shown remarkable performancein various cross-modal tasks due to their powerful capabilities in naturallanguage understanding, generalization, and in-context learning (ICL), whichprovides unprecedented opportunities to advance molecule discovery. Despiteseveral previous works trying to apply LLMs in this task, the lack ofdomain-specific corpus and difficulties in training specialized LLMs stillremain challenges. In this work, we propose a novel LLM-based framework(MolReGPT) for molecule-caption translation, where an In-Context Few-ShotMolecule Learning paradigm is introduced to empower molecule discovery withLLMs like ChatGPT to perform their in-context learning capability withoutdomain-specific pre-training and fine-tuning. MolReGPT leverages the principleof molecular similarity to retrieve similar molecules and their textdescriptions from a local database to enable LLMs to learn the task knowledgefrom context examples. We evaluate the effectiveness of MolReGPT onmolecule-caption translation, including molecule understanding and text-basedmolecule generation. Experimental results show that compared to fine-tunedmodels, MolReGPT outperforms MolT5-base and is comparable to MolT5-largewithout additional training. To the best of our knowledge, MolReGPT is thefirst work to leverage LLMs via in-context learning in molecule-captiontranslation for advancing molecule discovery. Our work expands the scope of LLMapplications, as well as providing a new paradigm for molecule discovery anddesign.</description><author>Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, Qing Li</author><pubDate>Mon, 22 Apr 2024 18:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06615v2</guid></item><item><title>TAVGBench: Benchmarking Text to Audible-Video Generation</title><link>http://arxiv.org/abs/2404.14381v1</link><description>The Text to Audible-Video Generation (TAVG) task involves generating videoswith accompanying audio based on text descriptions. Achieving this requiresskillful alignment of both audio and video elements. To support research inthis field, we have developed a comprehensive Text to Audible-Video GenerationBenchmark (TAVGBench), which contains over 1.7 million clips with a totalduration of 11.8 thousand hours. We propose an automatic annotation pipeline toensure each audible video has detailed descriptions for both its audio andvideo contents. We also introduce the Audio-Visual Harmoni score (AVHScore) toprovide a quantitative measure of the alignment between the generated audio andvideo modalities. Additionally, we present a baseline model for TAVG calledTAVDiffusion, which uses a two-stream latent diffusion model to provide afundamental starting point for further research in this area. We achieve thealignment of audio and video by employing cross-attention and contrastivelearning. Through extensive experiments and evaluations on TAVGBench, wedemonstrate the effectiveness of our proposed model under both conventionalmetrics and our proposed metrics.</description><author>Yuxin Mao, Xuyang Shen, Jing Zhang, Zhen Qin, Jinxing Zhou, Mochu Xiang, Yiran Zhong, Yuchao Dai</author><pubDate>Mon, 22 Apr 2024 18:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14381v1</guid></item><item><title>Does Gaussian Splatting need SFM Initialization?</title><link>http://arxiv.org/abs/2404.12547v2</link><description>3D Gaussian Splatting has recently been embraced as a versatile and effectivemethod for scene reconstruction and novel view synthesis, owing to itshigh-quality results and compatibility with hardware rasterization. Despite itsadvantages, Gaussian Splatting's reliance on high-quality point cloudinitialization by Structure-from-Motion (SFM) algorithms is a significantlimitation to be overcome. To this end, we investigate various initializationstrategies for Gaussian Splatting and delve into how volumetric reconstructionsfrom Neural Radiance Fields (NeRF) can be utilized to bypass the dependency onSFM data. Our findings demonstrate that random initialization can perform muchbetter if carefully designed and that by employing a combination of improvedinitialization strategies and structure distillation from low-cost NeRF models,it is possible to achieve equivalent results, or at times even superior, tothose obtained from SFM initialization.</description><author>Yalda Foroutan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi</author><pubDate>Mon, 22 Apr 2024 18:35:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12547v2</guid></item><item><title>Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks</title><link>http://arxiv.org/abs/2402.12279v2</link><description>Zero-shot cross-lingual knowledge transfer enables a multilingual pretrainedlanguage model, finetuned on a task in one language, make predictions for thistask in other languages. While being broadly studied for natural languageunderstanding tasks, the described setting is understudied for generation.Previous works notice a frequent problem of generation in a wrong language andpropose approaches to address it, usually using mT5 as a backbone model. Inthis work we compare various approaches proposed from the literature in unifiedsettings, also including alternative backbone models, namely mBART andNLLB-200. We first underline the importance of tuning learning rate used forfinetuning, which helps to substantially alleviate the problem of generation inthe wrong language. Then, we show that with careful learning rate tuning, thesimple full finetuning of the model acts as a very strong baseline andalternative approaches bring only marginal improvements. Finally, we find thatmBART performs similarly to mT5 of the same size, and NLLB-200 can becompetitive in some cases. Our final zero-shot models reach the performance ofthe approach based on data translation which is usually considered as an upperbaseline for zero-shot cross-lingual transfer in generation.</description><author>Nadezhda Chirkova, Vassilina Nikoulina</author><pubDate>Mon, 22 Apr 2024 18:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12279v2</guid></item><item><title>CoGS: Controllable Gaussian Splatting</title><link>http://arxiv.org/abs/2312.05664v2</link><description>Capturing and re-animating the 3D structure of articulated objects presentsignificant barriers. On one hand, methods requiring extensively calibratedmulti-view setups are prohibitively complex and resource-intensive, limitingtheir practical applicability. On the other hand, while single-camera NeuralRadiance Fields (NeRFs) offer a more streamlined approach, they have excessivetraining and rendering costs. 3D Gaussian Splatting would be a suitablealternative but for two reasons. Firstly, existing methods for 3D dynamicGaussians require synchronized multi-view cameras, and secondly, the lack ofcontrollability in dynamic scenarios. We present CoGS, a method forControllable Gaussian Splatting, that enables the direct manipulation of sceneelements, offering real-time control of dynamic scenes without the prerequisiteof pre-computing control signals. We evaluated CoGS using both synthetic andreal-world datasets that include dynamic objects that differ in degree ofdifficulty. In our evaluations, CoGS consistently outperformed existing dynamicand controllable neural representations in terms of visual fidelity.</description><author>Heng Yu, Joel Julin, Zoltán Á. Milacski, Koichiro Niinuma, László A. Jeni</author><pubDate>Mon, 22 Apr 2024 18:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05664v2</guid></item><item><title>Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph</title><link>http://arxiv.org/abs/2404.14372v1</link><description>Model scaling is becoming the default choice for many language tasks due tothe success of large language models (LLMs). However, it can fall short inspecific scenarios where simple customized methods excel. In this paper, wedelve into the patent approval pre-diction task and unveil that simpledomain-specific graph methods outperform enlarging the model, using theintrinsic dependencies within the patent data. Specifically, we first extendthe embedding-based state-of-the-art (SOTA) by scaling up its backbone modelwith various sizes of open-source LLMs, then explore prompt-based methods toharness proprietary LLMs' potential, but find the best results close to randomguessing, underlining the ineffectiveness of model scaling-up. Hence, wepropose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulouspatent data analyses, capturing the inherent dependencies across segments ofthe patent text. As it is model-agnostic, we apply cost-effective graph modelsto our FLAN Graph to obtain representations for approval prediction. Extensiveexperiments and detailed analyses prove that incorporating FLAN Graph viavarious graph models consistently outperforms all LLM baselines significantly.We hope that our observations and analyses in this paper can bring moreattention to this challenging task and prompt further research into thelimitations of LLMs. Our source code and dataset can be obtained fromhttp://github.com/ShangDataLab/FLAN-Graph.</description><author>Xiaochen Kev Gao, Feng Yao, Kewen Zhao, Beilei He, Animesh Kumar, Vish Krishnan, Jingbo Shang</author><pubDate>Mon, 22 Apr 2024 18:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14372v1</guid></item><item><title>What AIs are not Learning (and Why): Bio-Inspired Foundation Models for Robots</title><link>http://arxiv.org/abs/2404.04267v5</link><description>It is hard to build robots (including telerobots) that are useful, and harderto build autonomous robots that are robust and general. Current robots arebuilt using manual programming, mathematical models, planning frameworks, andreinforcement learning. These methods do not lead to the leaps in performanceand generality seen with deep learning, generative AI, and foundation models(FMs). Today's robots do not learn to provide home care, to be nursingassistants, or to do household chores and other services reliably. Addressingthe aspirational opportunities of robot service applications requires improvingthe path to get there. The high cost of bipedal multi-sensory robots ("bodies")is a significant obstacle for both research and deployment. A deeper issue isthat mainstream FMs ("minds") do not support sensing and acting in the world.They do not lead to robots that experiment, communicate, or collaborate. Theydo not lead to robots that learn from and with others. They do not lead torobots that know enough to be deployed in service applications. This paperfocuses on what service robots need to know. It recommends developingexperiential FMs for bootstrapping service robots.</description><author>Mark Stefik</author><pubDate>Mon, 22 Apr 2024 18:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04267v5</guid></item><item><title>Assessing GPT-4-Vision's Capabilities in UML-Based Code Generation</title><link>http://arxiv.org/abs/2404.14370v1</link><description>The emergence of advanced neural networks has opened up new ways in automatedcode generation from conceptual models, promising to enhance softwaredevelopment processes. This paper presents a preliminary evaluation ofGPT-4-Vision, a state-of-the-art deep learning model, and its capabilities intransforming Unified Modeling Language (UML) class diagrams into fullyoperating Java class files. In our study, we used exported images of 18 classdiagrams comprising 10 single-class and 8 multi-class diagrams. We used 3different prompts for each input, and we manually evaluated the results. Wecreated a scoring system in which we scored the occurrence of elements found inthe diagram within the source code. On average, the model was able to generatesource code for 88% of the elements shown in the diagrams. Our results indicatethat GPT-4-Vision exhibits proficiency in handling single-class UML diagrams,successfully transforming them into syntactically correct class files. However,for multi-class UML diagrams, the model's performance is weaker compared tosingle-class diagrams. In summary, further investigations are necessary toexploit the model's potential completely.</description><author>Gábor Antal, Richárd Vozár, Rudolf Ferenc</author><pubDate>Mon, 22 Apr 2024 18:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14370v1</guid></item><item><title>Graphic Design with Large Multimodal Model</title><link>http://arxiv.org/abs/2404.14368v1</link><description>In the field of graphic design, automating the integration of design elementsinto a cohesive multi-layered artwork not only boosts productivity but alsopaves the way for the democratization of graphic design. One existing practiceis Graphic Layout Generation (GLG), which aims to layout sequential designelements. It has been constrained by the necessity for a predefined correctsequence of layers, thus limiting creative potential and increasing userworkload. In this paper, we present Hierarchical Layout Generation (HLG) as amore flexible and pragmatic setup, which creates graphic composition fromunordered sets of design elements. To tackle the HLG task, we introduceGraphist, the first layout generation model based on large multimodal models.Graphist efficiently reframes the HLG as a sequence generation problem,utilizing RGB-A images as input, outputs a JSON draft protocol, indicating thecoordinates, size, and order of each element. We develop new evaluation metricsfor HLG. Graphist outperforms prior arts and establishes a strong baseline forthis field. Project homepage: https://github.com/graphic-design-ai/graphist</description><author>Yutao Cheng, Zhao Zhang, Maoke Yang, Hui Nie, Chunyuan Li, Xinglong Wu, Jie Shao</author><pubDate>Mon, 22 Apr 2024 18:20:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14368v1</guid></item><item><title>Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data</title><link>http://arxiv.org/abs/2404.14367v1</link><description>Learning from preference labels plays a crucial role in fine-tuning largelanguage models. There are several distinct approaches for preferencefine-tuning, including supervised learning, on-policy reinforcement learning(RL), and contrastive learning. Different methods come with differentimplementation tradeoffs and performance differences, and existing empiricalfindings present different conclusions, for instance, some results show thatonline RL is quite important to attain good fine-tuning results, while othersfind (offline) contrastive or even purely supervised methods sufficient. Thisraises a natural question: what kind of approaches are important forfine-tuning with preference data and why? In this paper, we answer thisquestion by performing a rigorous analysis of a number of fine-tuningtechniques on didactic and full-scale LLM problems. Our main finding is that,in general, approaches that use on-policy sampling or attempt to push down thelikelihood on certain responses (i.e., employ a "negative gradient") outperformoffline and maximum likelihood objectives. We conceptualize our insights andunify methods that use on-policy sampling or negative gradient under a notionof mode-seeking objectives for categorical distributions. Mode-seekingobjectives are able to alter probability mass on specific bins of a categoricaldistribution at a fast rate compared to maximum likelihood, allowing them torelocate masses across bins more effectively. Our analysis prescribesactionable insights for preference fine-tuning of LLMs and informs how datashould be collected for maximal improvement.</description><author>Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, Aviral Kumar</author><pubDate>Mon, 22 Apr 2024 18:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14367v1</guid></item><item><title>Better Synthetic Data by Retrieving and Transforming Existing Datasets</title><link>http://arxiv.org/abs/2404.14361v1</link><description>Despite recent advances in large language models, building dependable anddeployable NLP models typically requires abundant, high-quality training data.However, task-specific data is not available for many use cases, and manuallycurating task-specific data is labor-intensive. Recent work has studiedprompt-driven synthetic data generation using large language models, but thesegenerated datasets tend to lack complexity and diversity. To address theselimitations, we introduce a method, \textit{DataTune}, to make better use ofexisting, publicly available datasets to improve automatic dataset generation.DataTune performs dataset transformation, enabling the repurposing of publiclyavailable datasets into a format that is directly aligned with the specificrequirements of target tasks. On a diverse set of language-based tasks from theBIG-Bench benchmark, we find that finetuning language models via DataTuneimproves over a few-shot prompting baseline by 49\% and improves over existingmethods that use synthetic or retrieved training data by 34\%. We find thatdataset transformation significantly increases the diversity and difficulty ofgenerated data on many tasks. We integrate DataTune into an open-sourcerepository to make this method accessible to the community:https://github.com/neulab/prompt2model.</description><author>Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, Graham Neubig</author><pubDate>Mon, 22 Apr 2024 18:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14361v1</guid></item><item><title>Learning A Physical-aware Diffusion Model Based on Transformer for Underwater Image Enhancement</title><link>http://arxiv.org/abs/2403.01497v2</link><description>Underwater visuals undergo various complex degradations, inevitablyinfluencing the efficiency of underwater vision tasks. Recently, diffusionmodels were employed to underwater image enhancement (UIE) tasks, and gainedSOTA performance. However, these methods fail to consider the physicalproperties and underwater imaging mechanisms in the diffusion process, limitinginformation completion capacity of diffusion models. In this paper, weintroduce a novel UIE framework, named PA-Diff, designed to exploiting theknowledge of physics to guide the diffusion process. PA-Diff consists of Physics Prior Generation (PPG) Branch, Implicit NeuralReconstruction (INR) Branch, and Physics-aware Diffusion Transformer (PDT)Branch. Our designed PPG branch aims to produce the prior knowledge of physics.With utilizing the physics prior knowledge to guide the diffusion process, PDTbranch can obtain underwater-aware ability and model the complex distributionin real-world underwater scenes. INR Branch can learn robust featurerepresentations from diverse underwater image via implicit neuralrepresentation, which reduces the difficulty of restoration for PDT branch.Extensive experiments prove that our method achieves best performance on UIEtasks.</description><author>Chen Zhao, Chenyu Dong, Weiling Cai</author><pubDate>Mon, 22 Apr 2024 18:13:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01497v2</guid></item><item><title>Differentially Private Kernel Inducing Points using features from ScatterNets (DP-KIP-ScatterNet) for Privacy Preserving Data Distillation</title><link>http://arxiv.org/abs/2301.13389v2</link><description>Data distillation aims to generate a small data set that closely mimics theperformance of a given learning algorithm on the original data set. Thedistilled dataset is hence useful to simplify the training process thanks toits small data size. However, distilled data samples are not necessarilyprivacy-preserving, even if they are generally humanly indiscernible. Toaddress this limitation, we introduce differentially private kernel inducingpoints (DP-KIP) for privacy-preserving data distillation. Unlike our originalintention to simply apply DP-SGD to the framework of KIP, we find that KIPusing infinitely-wide convolutional neural tangent kernels (conv-NTKs) performsbetter compared to KIP using fully-connected NTKs. However, KIP with conv-NTKs,due to its convolutional and pooling operations, introduces an unbearablecomputational complexity, requiring hundreds of V100 GPUs in parallel to train,which is impractical and more importantly, such computational resources areinaccessible to many. To overcome this issue, we propose an alternative thatdoes not require pre-training (to avoid a privacy loss) and can well capturecomplex information on images, as those features from conv-NKTs do, while thecomputational cost is manageable by a single V100 GPU. To this end, we proposeDP-KIP-ScatterNet, which uses the wavelet features from Scattering networks(ScatterNet) instead of those from conv-NTKs, to perform DP-KIP at a reasonablecomputational cost. We implement DP-KIP-ScatterNet in -- computationallyefficient -- JAX and test on several popular image datasets to show itsefficacy and its superior performance compared to state-of-the art methods inimage data distillation with differential privacy guarantees.</description><author>Margarita Vinaroz, Mi Jung Park</author><pubDate>Mon, 22 Apr 2024 18:13:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13389v2</guid></item><item><title>A General Continuous-Time Formulation of Stochastic ADMM and Its Variants</title><link>http://arxiv.org/abs/2404.14358v1</link><description>Stochastic versions of the alternating direction method of multiplier (ADMM)and its variants play a key role in many modern large-scale machine learningproblems. In this work, we introduce a unified algorithmic framework calledgeneralized stochastic ADMM and investigate their continuous-time analysis. Thegeneralized framework widely includes many stochastic ADMM variants such asstandard, linearized and gradient-based ADMM. Our continuous-time analysisprovides us with new insights into stochastic ADMM and variants, and werigorously prove that under some proper scaling, the trajectory of stochasticADMM weakly converges to the solution of a stochastic differential equationwith small noise. Our analysis also provides a theoretical explanation of whythe relaxation parameter should be chosen between 0 and 2.</description><author>Chris Junchi Li</author><pubDate>Mon, 22 Apr 2024 18:12:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14358v1</guid></item><item><title>Empirical study of pretrained multilingual language models for zero-shot cross-lingual knowledge transfer in generation</title><link>http://arxiv.org/abs/2310.09917v3</link><description>Zero-shot cross-lingual knowledge transfer enables the multilingualpretrained language model (mPLM), finetuned on a task in one language, makepredictions for this task in other languages. While being broadly studied fornatural language understanding tasks, the described setting is understudied forgeneration. Previous works notice a frequent problem of generation in a wronglanguage and propose approaches to address it, usually using mT5 as a backbonemodel. In this work, we test alternative mPLMs, such as mBART and NLLB-200,considering full finetuning and parameter-efficient finetuning with adapters.We find that mBART with adapters performs similarly to mT5 of the same size,and NLLB-200 can be competitive in some cases. We also underline the importanceof tuning learning rate used for finetuning, which helps to alleviate theproblem of generation in the wrong language.</description><author>Nadezhda Chirkova, Sheng Liang, Vassilina Nikoulina</author><pubDate>Mon, 22 Apr 2024 18:10:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09917v3</guid></item><item><title>Calc-CMU at SemEval-2024 Task 7: Pre-Calc -- Learning to Use the Calculator Improves Numeracy in Language Models</title><link>http://arxiv.org/abs/2404.14355v1</link><description>Quantitative and numerical comprehension in language is an important task inmany fields like education and finance, but still remains a challenging taskfor language models. While tool and calculator usage has shown to be helpful toimprove mathematical reasoning in large pretrained decoder-only languagemodels, this remains unexplored for smaller language models with encoders. Inthis paper, we propose Pre-Calc, a simple pre-finetuning objective of learningto use the calculator for both encoder-only and encoder-decoder architectures,formulated as a discriminative and generative task respectively. We pre-trainBERT and RoBERTa for discriminative calculator use and Flan-T5 for generativecalculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improvesperformance on downstream tasks that require numerical understanding. Our codeand data are available at https://github.com/calc-cmu/pre-calc.</description><author>Vishruth Veerendranath, Vishwa Shah, Kshitish Ghate</author><pubDate>Mon, 22 Apr 2024 18:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14355v1</guid></item><item><title>AccidentBlip2: Accident Detection With Multi-View MotionBlip2</title><link>http://arxiv.org/abs/2404.12149v3</link><description>Intelligent vehicles have demonstrated excellent capabilities in manytransportation scenarios, but the complex on-board sensors and the inferencecapabilities of on-board neural networks limit the accuracy of intelligentvehicles for accident detection in complex transportation systems. In thispaper, we present AccidentBlip2, a pure vision-based multimodal large modelBlip2 accident detection method. Our method first processes the multi-viewthrough ViT-14g and inputs the multi-view features into the cross attentionlayer of the Qformer, while our self-designed Motion Qformer replaces theself-attention layer in Blip2's Qformer with the Temporal Attention layer inthe In the inference process, the query generated in the previous frame isinput into the Temporal Attention layer to realize the inference for temporalinformation. Then we detect whether there is an accident in the surroundingenvironment by performing autoregressive inference on the query input to theMLP. We also extend our approach to a multi-vehicle cooperative system bydeploying Motion Qformer on each vehicle and simultaneously inputting theinference-generated query into the MLP for autoregressive inference. Ourapproach detects the accuracy of existing video large language models and alsoadapts to multi-vehicle systems, making it more applicable to intelligenttransportation scenarios.</description><author>Yihua Shao, Hongyi Cai, Xinwei Long, Weiyi Lang, Zhe Wang, Haoran Wu, Yan Wang, Jiayi Yin, Yang Yang, Zhen Lei</author><pubDate>Mon, 22 Apr 2024 18:07:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12149v3</guid></item><item><title>Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer</title><link>http://arxiv.org/abs/2404.14351v1</link><description>We address the task of estimating camera parameters from a set of imagesdepicting a scene. Popular feature-based structure-from-motion (SfM) toolssolve this task by incremental reconstruction: they repeat triangulation ofsparse 3D points and registration of more camera views to the sparse pointcloud. We re-interpret incremental structure-from-motion as an iteratedapplication and refinement of a visual relocalizer, that is, of a method thatregisters new views to the current state of the reconstruction. Thisperspective allows us to investigate alternative visual relocalizers that arenot rooted in local feature matching. We show that scene coordinate regression,a learning-based relocalization approach, allows us to build implicit, neuralscene representations from unposed images. Different from other learning-basedreconstruction methods, we do not require pose priors nor sequential inputs,and we optimize efficiently over thousands of images. Our method, ACE0 (ACEZero), estimates camera poses to an accuracy comparable to feature-based SfM,as demonstrated by novel view synthesis. Project page:https://nianticlabs.github.io/acezero/</description><author>Eric Brachmann, Jamie Wynn, Shuai Chen, Tommaso Cavallari, Áron Monszpart, Daniyar Turmukhambetov, Victor Adrian Prisacariu</author><pubDate>Mon, 22 Apr 2024 18:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14351v1</guid></item><item><title>Automatic Discovery of Visual Circuits</title><link>http://arxiv.org/abs/2404.14349v1</link><description>To date, most discoveries of network subcomponents that implementhuman-interpretable computations in deep vision models have involved closestudy of single units and large amounts of human labor. We explore scalablemethods for extracting the subgraph of a vision model's computational graphthat underlies recognition of a specific visual concept. We introduce a newmethod for identifying these subgraphs: specifying a visual concept using a fewexamples, and then tracing the interdependence of neuron activations acrosslayers, or their functional connectivity. We find that our approach extractscircuits that causally affect model output, and that editing these circuits candefend large pretrained models from adversarial attacks.</description><author>Achyuta Rajaram, Neil Chowdhury, Antonio Torralba, Jacob Andreas, Sarah Schwettmann</author><pubDate>Mon, 22 Apr 2024 18:00:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14349v1</guid></item><item><title>On-the-Fly Point Annotation for Fast Medical Video Labeling</title><link>http://arxiv.org/abs/2404.14344v1</link><description>Purpose: In medical research, deep learning models rely on high-qualityannotated data, a process often laborious and timeconsuming. This isparticularly true for detection tasks where bounding box annotations arerequired. The need to adjust two corners makes the process inherentlyframe-by-frame. Given the scarcity of experts' time, efficient annotationmethods suitable for clinicians are needed. Methods: We propose an on-the-flymethod for live video annotation to enhance the annotation efficiency. In thisapproach, a continuous single-point annotation is maintained by keeping thecursor on the object in a live video, mitigating the need for tedious pausingand repetitive navigation inherent in traditional annotation methods. Thisnovel annotation paradigm inherits the point annotation's ability to generatepseudo-labels using a point-to-box teacher model. We empirically evaluate thisapproach by developing a dataset and comparing on-the-fly annotation timeagainst traditional annotation method. Results: Using our method, annotationspeed was 3.2x faster than the traditional annotation technique. We achieved amean improvement of 6.51 +- 0.98 AP@50 over conventional method at equivalentannotation budgets on the developed dataset. Conclusion: Without bells andwhistles, our approach offers a significant speed-up in annotation tasks. Itcan be easily implemented on any annotation platform to accelerate theintegration of deep learning in video-based medical research.</description><author>Meyer Adrien, Mazellier Jean-Paul, Jeremy Dana, Nicolas Padoy</author><pubDate>Mon, 22 Apr 2024 17:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14344v1</guid></item><item><title>Heterogeneous Face Recognition Using Domain Invariant Units</title><link>http://arxiv.org/abs/2404.14343v1</link><description>Heterogeneous Face Recognition (HFR) aims to expand the applicability of FaceRecognition (FR) systems to challenging scenarios, enabling the matching offace images across different domains, such as matching thermal images tovisible spectra. However, the development of HFR systems is challenging becauseof the significant domain gap between modalities and the lack of availabilityof large-scale paired multi-channel data. In this work, we leverage apretrained face recognition model as a teacher network to learn domaininvariantnetwork layers called Domain-Invariant Units (DIU) to reduce the domain gap.The proposed DIU can be trained effectively even with a limited amount ofpaired training data, in a contrastive distillation framework. This proposedapproach has the potential to enhance pretrained models, making them moreadaptable to a wider range of variations in data. We extensively evaluate ourapproach on multiple challenging benchmarks, demonstrating superior performancecompared to state-of-the-art methods.</description><author>Anjith George, Sebastien Marcel</author><pubDate>Mon, 22 Apr 2024 17:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14343v1</guid></item><item><title>Zero-shot Cross-lingual Stance Detection via Adversarial Language Adaptation</title><link>http://arxiv.org/abs/2404.14339v1</link><description>Stance detection has been widely studied as the task of determining if asocial media post is positive, negative or neutral towards a specific issue,such as support towards vaccines. Research in stance detection has howeveroften been limited to a single language and, where more than one language hasbeen studied, research has focused on few-shot settings, overlooking thechallenges of developing a zero-shot cross-lingual stance detection model. Thispaper makes the first such effort by introducing a novel approach to zero-shotcross-lingual stance detection, Multilingual Translation-Augmented BERT (MTAB),aiming to enhance the performance of a cross-lingual classifier in the absenceof explicit training data for target languages. Our technique employstranslation augmentation to improve zero-shot performance and pairs it withadversarial learning to further boost model efficacy. Through experiments ondatasets labeled for stance towards vaccines in four languages English, German,French, Italian. We demonstrate the effectiveness of our proposed approach,showcasing improved results in comparison to a strong baseline model as well asablated versions of our model. Our experiments demonstrate the effectiveness ofmodel components, not least the translation-augmented data as well as theadversarial learning component, to the improved performance of the model. Wehave made our source code accessible on GitHub.</description><author>Bharathi A, Arkaitz Zubiaga</author><pubDate>Mon, 22 Apr 2024 17:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14339v1</guid></item><item><title>Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk</title><link>http://arxiv.org/abs/2403.09450v2</link><description>While diffusion models have recently demonstrated remarkable progress ingenerating realistic images, privacy risks also arise: published models or APIscould generate training images and thus leak privacy-sensitive traininginformation. In this paper, we reveal a new risk, Shake-to-Leak (S2L), thatfine-tuning the pre-trained models with manipulated data can amplify theexisting privacy risks. We demonstrate that S2L could occur in various standardfine-tuning strategies for diffusion models, including concept-injectionmethods (DreamBooth and Textual Inversion) and parameter-efficient methods(LoRA and Hypernetwork), as well as their combinations. In the worst case, S2Lcan amplify the state-of-the-art membership inference attack (MIA) on diffusionmodels by $5.4\%$ (absolute difference) AUC and can increase extracted privatesamples from almost $0$ samples to $15.8$ samples on average per target domain.This discovery underscores that the privacy risk with diffusion models is evenmore severe than previously recognized. Codes are available athttps://github.com/VITA-Group/Shake-to-Leak.</description><author>Zhangheng Li, Junyuan Hong, Bo Li, Zhangyang Wang</author><pubDate>Mon, 22 Apr 2024 17:48:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09450v2</guid></item><item><title>Full Event Particle-Level Unfolding with Variable-Length Latent Variational Diffusion</title><link>http://arxiv.org/abs/2404.14332v1</link><description>The measurements performed by particle physics experiments must account forthe imperfect response of the detectors used to observe the interactions. Oneapproach, unfolding, statistically adjusts the experimental data for detectoreffects. Recently, generative machine learning models have shown promise forperforming unbinned unfolding in a high number of dimensions. However, allcurrent generative approaches are limited to unfolding a fixed set ofobservables, making them unable to perform full-event unfolding in the variabledimensional environment of collider data. A novel modification to thevariational latent diffusion model (VLD) approach to generative unfolding ispresented, which allows for unfolding of high- and variable-dimensional featurespaces. The performance of this method is evaluated in the context ofsemi-leptonic top quark pair production at the Large Hadron Collider.</description><author>Alexander Shmakov, Kevin Greif, Michael James Fenton, Aishik Ghosh, Pierre Baldi, Daniel Whiteson</author><pubDate>Mon, 22 Apr 2024 17:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14332v1</guid></item><item><title>Neural Control: Concurrent System Identification and Control Learning with Neural ODE</title><link>http://arxiv.org/abs/2401.01836v4</link><description>Controlling continuous-time dynamical systems is generally a two stepprocess: first, identify or model the system dynamics with differentialequations, then, minimize the control objectives to achieve optimal controlfunction and optimal state trajectories. However, any inaccuracy in dynamicsmodeling will lead to sub-optimality in the resulting control function. Toaddress this, we propose a neural ODE based method for controlling unknowndynamical systems, denoted as Neural Control (NC), which combines dynamicsidentification and optimal control learning using a coupled neural ODE. Throughan intriguing interplay between the two neural networks in coupled neural ODEstructure, our model concurrently learns system dynamics as well as optimalcontrols that guides towards target states. Our experiments demonstrate theeffectiveness of our model for learning optimal control of unknown dynamicalsystems. Codes available athttps://github.com/chichengmessi/neural_ode_control/tree/main</description><author>Cheng Chi</author><pubDate>Mon, 22 Apr 2024 17:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01836v4</guid></item><item><title>Unsupervised Learning of the Total Variation Flow</title><link>http://arxiv.org/abs/2206.04406v2</link><description>The total variation (TV) flow generates a scale-space representation of animage based on the TV functional. This gradient flow observes desirablefeatures for images, such as sharp edges and enables spectral, scale, andtexture analysis. Solving the TV flow is challenging; one reason is the thenon-uniqueness of the subgradients. The standard numerical approach for TV flowrequires solving multiple non-smooth optimisation problems. Even withstate-of-the-art convex optimisation techniques, this is often prohibitivelyexpensive and strongly motivates the use of alternative, faster approaches.Inspired by and extending the framework of physics-informed neural networks(PINNs), we propose the TVflowNET, an unsupervised neural network approach, toapproximate the solution of the TV flow given an initial image and a timeinstance. The TVflowNET requires no ground truth data but rather makes use ofthe PDE for optimisation of the network parameters. We circumvent thechallenges related to the non-uniqueness of the subgradients by additionallylearning the related diffusivity term. Our approach significantly speeds up thecomputation time and we show that the TVflowNET approximates the TV flowsolution with high fidelity for different image sizes and image types.Additionally, we give a full comparison of different network architecturedesigns as well as training regimes to underscore the effectiveness of ourapproach.</description><author>Tamara G. Grossmann, Sören Dittmer, Yury Korolev, Carola-Bibiane Schönlieb</author><pubDate>Mon, 22 Apr 2024 17:41:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.04406v2</guid></item><item><title>X-Ray: A Sequential 3D Representation for Generation</title><link>http://arxiv.org/abs/2404.14329v1</link><description>In this paper, we introduce X-Ray, an innovative approach to 3D generationthat employs a new sequential representation, drawing inspiration from thedepth-revealing capabilities of X-Ray scans to meticulously capture both theexternal and internal features of objects. Central to our method is theutilization of ray casting techniques originating from the camera's viewpoint,meticulously recording the geometric and textural details encountered acrossall intersected surfaces. This process efficiently condenses complete objectsor scenes into a multi-frame format, just like videos. Such a structure ensuresthe 3D representation is composed solely of critical surface information.Highlighting the practicality and adaptability of our X-Ray representation, weshowcase its utility in synthesizing 3D objects, employing a networkarchitecture akin to that used in video diffusion models. The outcomes revealour representation's superior performance in enhancing both the accuracy andefficiency of 3D synthesis, heralding new directions for ongoing research andpractical implementations in the field.</description><author>Tao Hu, Wenhang Ge, Yuyang Zhao, Gim Hee Lee</author><pubDate>Mon, 22 Apr 2024 17:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14329v1</guid></item><item><title>Preserving linear invariants in ensemble filtering methods</title><link>http://arxiv.org/abs/2404.14328v1</link><description>Formulating dynamical models for physical phenomena is essential forunderstanding the interplay between the different mechanisms and predicting theevolution of physical states. However, a dynamical model alone is ofteninsufficient to address these fundamental tasks, as it suffers from modelerrors and uncertainties. One common remedy is to rely on data assimilation,where the state estimate is updated with observations of the true system.Ensemble filters sequentially assimilate observations by updating a set ofsamples over time. They operate in two steps: a forecast step that propagateseach sample through the dynamical model and an analysis step that updates thesamples with incoming observations. For accurate and robust predictions ofdynamical systems, discrete solutions must preserve their critical invariants.While modern numerical solvers satisfy these invariants, existinginvariant-preserving analysis steps are limited to Gaussian settings and areoften not compatible with classical regularization techniques of ensemblefilters, e.g., inflation and covariance tapering. The present work focuses onpreserving linear invariants, such as mass, stoichiometric balance of chemicalspecies, and electrical charges. Using tools from measure transport theory(Spantini et al., 2022, SIAM Review), we introduce a generic class of nonlinearensemble filters that automatically preserve desired linear invariants innon-Gaussian filtering problems. By specializing this framework to the Gaussiansetting, we recover a constrained formulation of the Kalman filter. Then, weshow how to combine existing regularization techniques for the ensemble Kalmanfilter (Evensen, 1994, J. Geophys. Res.) with the preservation of the linearinvariants. Finally, we assess the benefits of preserving linear invariants forthe ensemble Kalman filter and nonlinear ensemble filters.</description><author>Mathieu Le Provost, Jan Glaubitz, Youssef Marzouk</author><pubDate>Mon, 22 Apr 2024 17:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14328v1</guid></item><item><title>Machine Learning Techniques for MRI Data Processing at Expanding Scale</title><link>http://arxiv.org/abs/2404.14326v1</link><description>Imaging sites around the world generate growing amounts of medical scan datawith ever more versatile and affordable technology. Large-scale studies acquireMRI for tens of thousands of participants, together with metadata ranging fromlifestyle questionnaires to biochemical assays, genetic analyses and more.These large datasets encode substantial information about human health and holdconsiderable potential for machine learning training and analysis. This chapterexamines ongoing large-scale studies and the challenge of distribution shiftsbetween them. Transfer learning for overcoming such shifts is discussed,together with federated learning for safe access to distributed training datasecurely held at multiple institutions. Finally, representation learning isreviewed as a methodology for encoding embeddings that express abstractrelationships in multi-modal input formats.</description><author>Taro Langner</author><pubDate>Mon, 22 Apr 2024 17:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14326v1</guid></item><item><title>Adapting to time: why nature evolved a diverse set of neurons</title><link>http://arxiv.org/abs/2404.14325v1</link><description>Evolution has yielded a diverse set of neurons with varying morphologies andphysiological properties that impact their processing of temporal information.In addition, it is known empirically that spike timing is a significant factorin neural computations. However, despite these two observations, most neuralnetwork models deal with spatially structured inputs with synchronous timesteps, while restricting variation to parameters like weights and biases. Inthis study, we investigate the relevance of adapting temporal parameters, liketime constants and delays, in feedforward networks that map spatio-temporalspike patterns. In this context, we show that networks with richer potentialdynamics are able to more easily and robustly learn tasks with temporalstructure. Indeed, when adaptation was restricted to weights, networks wereunable to solve most problems. We also show strong interactions between thevarious parameters and the advantages of adapting temporal parameters whendealing with noise in inputs and weights, which might prove useful inneuromorphic hardware design.</description><author>Karim G. Habashy, Benjamin D. Evans, Dan F. M. Goodman, Jeffrey S. Bowers</author><pubDate>Mon, 22 Apr 2024 17:38:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14325v1</guid></item><item><title>Knowledge Graph Context-Enhanced Diversified Recommendation</title><link>http://arxiv.org/abs/2310.13253v2</link><description>The field of Recommender Systems (RecSys) has been extensively studied toenhance accuracy by leveraging users' historical interactions. Nonetheless,this persistent pursuit of accuracy frequently engenders diminished diversity,culminating in the well-recognized "echo chamber" phenomenon. DiversifiedRecSys has emerged as a countermeasure, placing diversity on par with accuracyand garnering noteworthy attention from academic circles and industrypractitioners. This research explores the realm of diversified RecSys withinthe intricate context of knowledge graphs (KG). These KGs act as repositoriesof interconnected information concerning entities and items, offering apropitious avenue to amplify recommendation diversity through the incorporationof insightful contextual information. Our contributions include introducing aninnovative metric, Entity Coverage, and Relation Coverage, which effectivelyquantifies diversity within the KG domain. Additionally, we introduce theDiversified Embedding Learning (DEL) module, meticulously designed to formulateuser representations that possess an innate awareness of diversity. In tandemwith this, we introduce a novel technique named Conditional Alignment andUniformity (CAU). It adeptly encodes KG item embeddings while preservingcontextual integrity. Collectively, our contributions signify a substantialstride towards augmenting the panorama of recommendation diversity within therealm of KG-informed RecSys paradigms.</description><author>Xiaolong Liu, Liangwei Yang, Zhiwei Liu, Mingdai Yang, Chen Wang, Hao Peng, Philip S. Yu</author><pubDate>Mon, 22 Apr 2024 17:37:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13253v2</guid></item><item><title>A Novel Approach to Chest X-ray Lung Segmentation Using U-net and Modified Convolutional Block Attention Module</title><link>http://arxiv.org/abs/2404.14322v1</link><description>Lung segmentation in chest X-ray images is of paramount importance as itplays a crucial role in the diagnosis and treatment of various lung diseases.This paper presents a novel approach for lung segmentation in chest X-rayimages by integrating U-net with attention mechanisms. The proposed methodenhances the U-net architecture by incorporating a Convolutional BlockAttention Module (CBAM), which unifies three distinct attention mechanisms:channel attention, spatial attention, and pixel attention. The channelattention mechanism enables the model to concentrate on the most informativefeatures across various channels. The spatial attention mechanism enhances themodel's precision in localization by focusing on significant spatial locations.Lastly, the pixel attention mechanism empowers the model to focus on individualpixels, further refining the model's focus and thereby improving the accuracyof segmentation. The adoption of the proposed CBAM in conjunction with theU-net architecture marks a significant advancement in the field of medicalimaging, with potential implications for improving diagnostic precision andpatient outcomes. The efficacy of this method is validated against contemporarystate-of-the-art techniques, showcasing its superiority in segmentationperformance.</description><author>Mohammad Ali Labbaf Khaniki, Mohammad Manthouri</author><pubDate>Mon, 22 Apr 2024 17:33:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14322v1</guid></item><item><title>On Prediction Feature Assignment in the Heckman Selection Model</title><link>http://arxiv.org/abs/2309.08043v2</link><description>Under missing-not-at-random (MNAR) sample selection bias, the performance ofa prediction model is often degraded. This paper focuses on one classicinstance of MNAR sample selection bias where a subset of samples havenon-randomly missing outcomes. The Heckman selection model and its variantshave commonly been used to handle this type of sample selection bias. TheHeckman model uses two separate equations to model the prediction and selectionof samples, where the selection features include all prediction features. Whenusing the Heckman model, the prediction features must be properly chosen fromthe set of selection features. However, choosing the proper prediction featuresis a challenging task for the Heckman model. This is especially the case whenthe number of selection features is large. Existing approaches that use theHeckman model often provide a manually chosen set of prediction features. Inthis paper, we propose Heckman-FA as a novel data-driven framework forobtaining prediction features for the Heckman model. Heckman-FA first trains anassignment function that determines whether or not a selection feature isassigned as a prediction feature. Using the parameters of the trained function,the framework extracts a suitable set of prediction features based on thegoodness-of-fit of the prediction model given the chosen prediction featuresand the correlation between noise terms of the prediction and selectionequations. Experimental results on real-world datasets show that Heckman-FAproduces a robust regression model under MNAR sample selection bias.</description><author>Huy Mai, Xintao Wu</author><pubDate>Mon, 22 Apr 2024 17:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08043v2</guid></item><item><title>Multi-Agent Hybrid SAC for Joint SS-DSA in CRNs</title><link>http://arxiv.org/abs/2404.14319v1</link><description>Opportunistic spectrum access has the potential to increase the efficiency ofspectrum utilization in cognitive radio networks (CRNs). In CRNs, both spectrumsensing and resource allocation (SSRA) are critical to maximizing systemthroughput while minimizing collisions of secondary users with the primarynetwork. However, many works in dynamic spectrum access do not consider theimpact of imperfect sensing information such as mis-detected channels, whichthe additional information available in joint SSRA can help remediate. In thiswork, we examine joint SSRA as an optimization which seeks to maximize a CRN'snet communication rate subject to constraints on channel sensing, channelaccess, and transmit power. Given the non-trivial nature of the problem, weleverage multi-agent reinforcement learning to enable a network of secondaryusers to dynamically access unoccupied spectrum via only local test statistics,formulated under the energy detection paradigm of spectrum sensing. In doingso, we develop a novel multi-agent implementation of hybrid soft actor critic,MHSAC, based on the QMIX mixing scheme. Through experiments, we find that ourSSRA algorithm, HySSRA, is successful in maximizing the CRN's utilization ofspectrum resources while also limiting its interference with the primarynetwork, and outperforms the current state-of-the-art by a wide margin. We alsoexplore the impact of wireless variations such as coherence time on theefficacy of the system.</description><author>David R. Nickel, Anindya Bijoy Das, David J. Love, Christopher G. Brinton</author><pubDate>Mon, 22 Apr 2024 17:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14319v1</guid></item><item><title>Automated Long Answer Grading with RiceChem Dataset</title><link>http://arxiv.org/abs/2404.14316v1</link><description>We introduce a new area of study in the field of educational Natural LanguageProcessing: Automated Long Answer Grading (ALAG). Distinguishing itself fromAutomated Short Answer Grading (ASAG) and Automated Essay Grading (AEG), ALAGpresents unique challenges due to the complexity and multifaceted nature offact-based long answers. To study ALAG, we introduce RiceChem, a datasetderived from a college chemistry course, featuring real student responses tolong-answer questions with an average word count notably higher than typicalASAG datasets. We propose a novel approach to ALAG by formulating it as arubric entailment problem, employing natural language inference models toverify whether each criterion, represented by a rubric item, is addressed inthe student's response. This formulation enables the effective use of MNLI fortransfer learning, significantly improving the performance of models on theRiceChem dataset. We demonstrate the importance of rubric-based formulation inALAG, showcasing its superiority over traditional score-based approaches incapturing the nuances of student responses. We also investigate the performanceof models in cold start scenarios, providing valuable insights into thepractical deployment considerations in educational settings. Lastly, webenchmark state-of-the-art open-sourced Large Language Models (LLMs) onRiceChem and compare their results to GPT models, highlighting the increasedcomplexity of ALAG compared to ASAG. Despite leveraging the benefits of arubric-based approach and transfer learning from MNLI, the lower performance ofLLMs on RiceChem underscores the significant difficulty posed by the ALAG task.With this work, we offer a fresh perspective on grading long, fact-basedanswers and introduce a new dataset to stimulate further research in thisimportant area. Code:\url{https://github.com/luffycodes/Automated-Long-Answer-Grading}.</description><author>Shashank Sonkar, Kangqi Ni, Lesa Tran Lu, Kristi Kincaid, John S. Hutchinson, Richard G. Baraniuk</author><pubDate>Mon, 22 Apr 2024 17:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14316v1</guid></item><item><title>Versatile Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers</title><link>http://arxiv.org/abs/2306.00816v3</link><description>Deep neural networks (DNNs) can be manipulated to exhibit specific behaviorswhen exposed to specific trigger patterns, without affecting their performanceon benign samples, dubbed \textit{backdoor attack}. Currently, implementingbackdoor attacks in physical scenarios still faces significant challenges.Physical attacks are labor-intensive and time-consuming, and the triggers areselected in a manual and heuristic way. Moreover, expanding digital attacks tophysical scenarios faces many challenges due to their sensitivity to visualdistortions and the absence of counterparts in the real world. To address thesechallenges, we define a novel trigger called the \textbf{V}isible,\textbf{S}emantic, \textbf{S}ample-Specific, and \textbf{C}ompatible (VSSC)trigger, to achieve effective, stealthy and robust simultaneously, which canalso be effectively deployed in the physical scenario using correspondingobjects. To implement the VSSC trigger, we propose an automated pipelinecomprising three modules: a trigger selection module that systematicallyidentifies suitable triggers leveraging large language models, a triggerinsertion module that employs generative models to seamlessly integratetriggers into images, and a quality assessment module that ensures the naturaland successful insertion of triggers through vision-language models. Extensiveexperimental results and analysis validate the effectiveness, stealthiness, androbustness of the VSSC trigger. It can not only maintain robustness undervisual distortions but also demonstrates strong practicality in the physicalscenario. We hope that the proposed VSSC trigger and implementation approachcould inspire future studies on designing more practical triggers in backdoorattacks.</description><author>Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Baoyuan Wu</author><pubDate>Mon, 22 Apr 2024 17:26:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00816v3</guid></item><item><title>Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels</title><link>http://arxiv.org/abs/2404.14313v1</link><description>When prompting a language model (LM), users frequently expect the model toadhere to a set of behavioral principles across diverse tasks, such asproducing insightful content while avoiding harmful or biased language.Instilling such principles into a model can be resource-intensive andtechnically challenging, generally requiring human preference labels orexamples. We introduce SAMI, a method for teaching a pretrained LM to followbehavioral principles that does not require any preference labels ordemonstrations. SAMI is an iterative algorithm that finetunes a pretrained LMto increase the conditional mutual information between constitutions andself-generated responses given queries from a datasest. On single-turn dialogueand summarization, a SAMI-trained mistral-7b outperforms the initial pretrainedmodel, with win rates between 66% and 77%. Strikingly, it also surpasses aninstruction-finetuned baseline (mistral-7b-instruct) with win rates between 55%and 57% on single-turn dialogue. SAMI requires a "principle writer" model; toavoid dependence on stronger models, we further evaluate aligning a strongpretrained model (mixtral-8x7b) using constitutions written by a weakinstruction-finetuned model (mistral-7b-instruct). The SAMI-trainedmixtral-8x7b outperforms both the initial model and the instruction-finetunedmodel, achieving a 65% win rate on summarization. Our results indicate that apretrained LM can learn to follow constitutions without using preferencelabels, demonstrations, or human oversight.</description><author>Jan-Philipp Fränken, Eric Zelikman, Rafael Rafailov, Kanishk Gandhi, Tobias Gerstenberg, Noah D. Goodman</author><pubDate>Mon, 22 Apr 2024 17:20:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14313v1</guid></item><item><title>Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data</title><link>http://arxiv.org/abs/2403.06687v2</link><description>Graph neural networks (GNNs) have proven effective in capturing relationshipsamong nodes in a graph. This study introduces a novel perspective byconsidering a graph as a simplicial complex, encompassing nodes, edges,triangles, and $k$-simplices, enabling the definition of graph-structured dataon any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneousgraph attention network (HL-HGAT), designed to learn heterogeneous signalrepresentations across $k$-simplices. The HL-HGAT incorporates three keycomponents: HL convolutional filters (HL-filters), simplicial projection (SP),and simplicial attention pooling (SAP) operators, applied to $k$-simplices.HL-filters leverage the unique topology of $k$-simplices encoded by theHodge-Laplacian (HL) operator, operating within the spectral domain of the$k$-th HL operator. To address computation challenges, we introduce apolynomial approximation for HL-filters, exhibiting spatial localizationproperties. Additionally, we propose a pooling operator to coarsen$k$-simplices, combining features through simplicial attention mechanisms ofself-attention and cross-attention via transformers and SP operators, capturingtopological interconnections across multiple dimensions of simplices. TheHL-HGAT is comprehensively evaluated across diverse graph applications,including NP-hard problems, graph multi-label and classification challenges,and graph regression tasks in logistics, computer vision, biology, chemistry,and neuroscience. The results demonstrate the model's efficacy and versatilityin handling a wide range of graph-based scenarios.</description><author>Jinghan Huang, Qiufeng Chen, Yijun Bian, Pengli Zhu, Nanguang Chen, Moo K. Chung, Anqi Qiu</author><pubDate>Mon, 22 Apr 2024 17:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06687v2</guid></item><item><title>Neuromorphic Face Analysis: a Survey</title><link>http://arxiv.org/abs/2402.11631v2</link><description>Neuromorphic sensors, also known as event cameras, are a class of imagingdevices mimicking the function of biological visual systems. Unlike traditionalframe-based cameras, which capture fixed images at discrete intervals,neuromorphic sensors continuously generate events that represent changes inlight intensity or motion in the visual field with high temporal resolution andlow latency. These properties have proven to be interesting in modeling humanfaces, both from an effectiveness and a privacy-preserving point of view.Neuromorphic face analysis however is still a raw and unstructured field ofresearch, with several attempts at addressing different tasks with no clearstandard or benchmark. This survey paper presents a comprehensive overview ofcapabilities, challenges and emerging applications in the domain ofneuromorphic face analysis, to outline promising directions and open issues.After discussing the fundamental working principles of neuromorphic vision andpresenting an in-depth overview of the related research, we explore the currentstate of available data, standard data representations, emerging challenges,and limitations that require further investigation. This paper aims tohighlight the recent process in this evolving field to provide to bothexperienced and newly come researchers an all-encompassing analysis of thestate of the art along with its problems and shortcomings.</description><author>Federico Becattini, Lorenzo Berlincioni, Luca Cultrera, Alberto Del Bimbo</author><pubDate>Mon, 22 Apr 2024 17:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11631v2</guid></item><item><title>Structure-preserving neural networks for the regularzied entropy-based closure of the Boltzmann moment system</title><link>http://arxiv.org/abs/2404.14312v1</link><description>The main challenge of large-scale numerical simulation of radiation transportis the high memory and computation time requirements of discretization methodsfor kinetic equations. In this work, we derive and investigate a neuralnetwork-based approximation to the entropy closure method to accurately computethe solution of the multi-dimensional moment system with a low memory footprintand competitive computational time. We extend methods developed for thestandard entropy-based closure to the context of regularized entropy-basedclosures. The main idea is to interpret structure-preserving neural networkapproximations of the regularized entropy closure as a two-stage approximationto the original entropy closure. We conduct a numerical analysis of thisapproximation and investigate optimal parameter choices. Our numericalexperiments demonstrate that the method has a much lower memory footprint thantraditional methods with competitive computation times and simulation accuracy.The code and all trained networks are provided onGitHub\footnote{\url{https://github.com/ScSteffen/neuralEntropyClosures}}$^,$\footnote{\url{https://github.com/CSMMLab/KiT-RT}}.</description><author>Steffen Schotthöfer, M. Paul Laiu, Martin Frank, Cory D. Hauck</author><pubDate>Mon, 22 Apr 2024 17:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14312v1</guid></item><item><title>Towards Better Adversarial Purification via Adversarial Denoising Diffusion Training</title><link>http://arxiv.org/abs/2404.14309v1</link><description>Recently, diffusion-based purification (DBP) has emerged as a promisingapproach for defending against adversarial attacks. However, previous studieshave used questionable methods to evaluate the robustness of DBP models, theirexplanations of DBP robustness also lack experimental support. We re-examineDBP robustness using precise gradient, and discuss the impact of stochasticityon DBP robustness. To better explain DBP robustness, we assess DBP robustnessunder a novel attack setting, Deterministic White-box, and pinpointstochasticity as the main factor in DBP robustness. Our results suggest thatDBP models rely on stochasticity to evade the most effective attack direction,rather than directly countering adversarial perturbations. To improve therobustness of DBP models, we propose Adversarial Denoising Diffusion Training(ADDT). This technique uses Classifier-Guided Perturbation Optimization (CGPO)to generate adversarial perturbation through guidance from a pre-trainedclassifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarialpertubation into a normal Gaussian distribution. Empirical results show thatADDT improves the robustness of DBP models. Further experiments confirm thatADDT equips DBP models with the ability to directly counter adversarialperturbations.</description><author>Yiming Liu, Kezhao Liu, Yao Xiao, Ziyi Dong, Xiaogang Xu, Pengxu Wei, Liang Lin</author><pubDate>Mon, 22 Apr 2024 17:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14309v1</guid></item><item><title>Explaining Arguments' Strength: Unveiling the Role of Attacks and Supports (Technical Report)</title><link>http://arxiv.org/abs/2404.14304v1</link><description>Quantitatively explaining the strength of arguments under gradual semanticshas recently received increasing attention. Specifically, several works in theliterature provide quantitative explanations by computing the attributionscores of arguments. These works disregard the importance of attacks andsupports, even though they play an essential role when explaining arguments'strength. In this paper, we propose a novel theory of Relation AttributionExplanations (RAEs), adapting Shapley values from game theory to offerfine-grained insights into the role of attacks and supports in quantitativebipolar argumentation towards obtaining the arguments' strength. We show thatRAEs satisfy several desirable properties. We also propose a probabilisticalgorithm to approximate RAEs efficiently. Finally, we show the applicationvalue of RAEs in fraud detection and large language models case studies.</description><author>Xiang Yin, Potyka Nico, Francesca Toni</author><pubDate>Mon, 22 Apr 2024 17:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14304v1</guid></item><item><title>Marking: Visual Grading with Highlighting Errors and Annotating Missing Bits</title><link>http://arxiv.org/abs/2404.14301v1</link><description>In this paper, we introduce "Marking", a novel grading task that enhancesautomated grading systems by performing an in-depth analysis of studentresponses and providing students with visual highlights. Unlike traditionalsystems that provide binary scores, "marking" identifies and categorizessegments of the student response as correct, incorrect, or irrelevant anddetects omissions from gold answers. We introduce a new dataset meticulouslycurated by Subject Matter Experts specifically for this task. We frame"Marking" as an extension of the Natural Language Inference (NLI) task, whichis extensively explored in the field of Natural Language Processing. The goldanswer and the student response play the roles of premise and hypothesis inNLI, respectively. We subsequently train language models to identifyentailment, contradiction, and neutrality from student response, akin to NLI,and with the added dimension of identifying omissions from gold answers. Ourexperimental setup involves the use of transformer models, specifically BERTand RoBERTa, and an intelligent training step using the e-SNLI dataset. Wepresent extensive baseline results highlighting the complexity of the "Marking"task, which sets a clear trajectory for the upcoming study. Our work not onlyopens up new avenues for research in AI-powered educational assessment tools,but also provides a valuable benchmark for the AI in education community toengage with and improve upon in the future. The code and dataset can be foundat https://github.com/luffycodes/marking.</description><author>Shashank Sonkar, Naiming Liu, Debshila B. Mallick, Richard G. Baraniuk</author><pubDate>Mon, 22 Apr 2024 17:00:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14301v1</guid></item><item><title>Let Quantum Neural Networks Choose Their Own Frequencies</title><link>http://arxiv.org/abs/2309.03279v2</link><description>Parameterized quantum circuits as machine learning models are typically welldescribed by their representation as a partial Fourier series of the inputfeatures, with frequencies uniquely determined by the feature map's generatorHamiltonians. Ordinarily, these data-encoding generators are chosen in advance,fixing the space of functions that can be represented. In this work we considera generalization of quantum models to include a set of trainable parameters inthe generator, leading to a trainable frequency (TF) quantum model. Wenumerically demonstrate how TF models can learn generators with desirableproperties for solving the task at hand, including non-regularly spacedfrequencies in their spectra and flexible spectral richness. Finally, weshowcase the real-world effectiveness of our approach, demonstrating animproved accuracy in solving the Navier-Stokes equations using a TF model withonly a single parameter added to each encoding operation. Since TF modelsencompass conventional fixed frequency models, they may offer a sensibledefault choice for variational quantum machine learning.</description><author>Ben Jaderberg, Antonio A. Gentile, Youssef Achari Berrada, Elvira Shishenina, Vincent E. Elfving</author><pubDate>Mon, 22 Apr 2024 16:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03279v2</guid></item><item><title>Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach</title><link>http://arxiv.org/abs/2404.14296v1</link><description>Recent years have witnessed significant progress in developing deeplearning-based models for automated code completion. Although using source codein GitHub has been a common practice for training deep-learning-based modelsfor code completion, it may induce some legal and ethical issues such ascopyright infringement. In this paper, we investigate the legal and ethicalissues of current neural code completion models by answering the followingquestion: Is my code used to train your neural code completion model? To thisend, we tailor a membership inference approach (termed CodeMI) that wasoriginally crafted for classification tasks to a more challenging task of codecompletion. In particular, since the target code completion models perform asopaque black boxes, preventing access to their training data and parameters, weopt to train multiple shadow models to mimic their behavior. The acquiredposteriors from these shadow models are subsequently employed to train amembership classifier. Subsequently, the membership classifier can beeffectively employed to deduce the membership status of a given code samplebased on the output of a target code completion model. We comprehensivelyevaluate the effectiveness of this adapted approach across a diverse array ofneural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, andStarCoder). Experimental results reveal that the LSTM-based and CodeGPT modelssuffer the membership leakage issue, which can be easily detected by ourproposed membership inference approach with an accuracy of 0.842, and 0.730,respectively. Interestingly, our experiments also show that the data membershipof current large language models of code, e.g., CodeGen and StarCoder, isdifficult to detect, leaving amper space for further improvement. Finally, wealso try to explain the findings from the perspective of model memorization.</description><author>Yao Wan, Guanghua Wan, Shijie Zhang, Hongyu Zhang, Yulei Sui, Pan Zhou, Hai Jin, Lichao Sun</author><pubDate>Mon, 22 Apr 2024 16:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14296v1</guid></item><item><title>Scoring Intervals using Non-Hierarchical Transformer For Automatic Piano Transcription</title><link>http://arxiv.org/abs/2404.09466v3</link><description>The neural semi-Markov Conditional Random Field (semi-CRF) framework hasdemonstrated promise for event-based piano transcription. In this framework,all events (notes or pedals) are represented as closed intervals tied tospecific event types. The neural semi-CRF approach requires an interval scoringmatrix that assigns a score for every candidate interval. However, designing anefficient and expressive architecture for scoring intervals is not trivial. Inthis paper, we introduce a simple method for scoring intervals using scaledinner product operations that resemble how attention scoring is done intransformers. We show theoretically that, due to the special structure fromencoding the non-overlapping intervals, under a mild condition, the innerproduct operations are expressive enough to represent an ideal scoring matrixthat can yield the correct transcription result. We then demonstrate that anencoder-only non-hierarchical transformer backbone, operating only on alow-time-resolution feature map, is capable of transcribing piano notes andpedals with high accuracy and time precision. The experiment shows that ourapproach achieves the new state-of-the-art performance across all subtasks interms of the F1 measure on the Maestro dataset.</description><author>Yujia Yan, Zhiyao Duan</author><pubDate>Mon, 22 Apr 2024 16:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09466v3</guid></item><item><title>Ultra-short-term multi-step wind speed prediction for wind farms based on adaptive noise reduction technology and temporal convolutional network</title><link>http://arxiv.org/abs/2311.16198v2</link><description>As an important clean and renewable kind of energy, wind power plays animportant role in coping with energy crisis and environmental pollution.However, the volatility and intermittency of wind speed restrict thedevelopment of wind power. To improve the utilization of wind power, this studyproposes a new wind speed prediction model based on data noise reductiontechnology, temporal convolutional network (TCN), and gated recurrent unit(GRU). Firstly, an adaptive data noise reduction algorithm P-SSA is proposedbased on singular spectrum analysis (SSA) and Pearson correlation coefficient.The original wind speed is decomposed into multiple subsequences by SSA andthen reconstructed. When the Pearson correlation coefficient between thereconstructed sequence and the original sequence is greater than 0.99, othernoise subsequences are deleted to complete the data denoising. Then, thereceptive field of the samples is expanded through the causal convolution anddilated convolution of TCN, and the characteristics of wind speed change areextracted. Then, the time feature information of the sequence is extracted byGRU, and then the wind speed is predicted to form the wind speed sequenceprediction model of P-SSA-TCN-GRU. The proposed model was validated on threewind farms in Shandong Province. The experimental results show that theprediction performance of the proposed model is better than that of thetraditional model and other models based on TCN, and the wind speed predictionof wind farms with high precision and strong stability is realized. The windspeed predictions of this model have the potential to become the data thatsupport the operation and management of wind farms. The code is available athttps://github.com/JethroJames/Wind-Speed-Forecast-TCN_GRU</description><author>Haojian Huang</author><pubDate>Mon, 22 Apr 2024 16:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16198v2</guid></item><item><title>A Survey on Efficient Inference for Large Language Models</title><link>http://arxiv.org/abs/2404.14294v1</link><description>Large Language Models (LLMs) have attracted extensive attention due to theirremarkable performance across various tasks. However, the substantialcomputational and memory requirements of LLM inference pose challenges fordeployment in resource-constrained scenarios. Efforts within the field havebeen directed towards developing techniques aimed at enhancing the efficiencyof LLM inference. This paper presents a comprehensive survey of the existingliterature on efficient LLM inference. We start by analyzing the primary causesof the inefficient LLM inference, i.e., the large model size, thequadratic-complexity attention operation, and the auto-regressive decodingapproach. Then, we introduce a comprehensive taxonomy that organizes thecurrent literature into data-level, model-level, and system-level optimization.Moreover, the paper includes comparative experiments on representative methodswithin critical sub-fields to provide quantitative insights. Last but notleast, we provide some knowledge summary and discuss future researchdirections.</description><author>Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, Yu Wang</author><pubDate>Mon, 22 Apr 2024 16:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14294v1</guid></item><item><title>LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots</title><link>http://arxiv.org/abs/2404.14285v1</link><description>Large language models (LLMs) have shown significant potential for roboticsapplications, particularly task planning, by harnessing their languagecomprehension and text generation capabilities. However, in applications suchas household robotics, a critical gap remains in the personalization of thesemodels to individual user preferences. We introduce LLM-Personalize, a novelframework with an optimization pipeline designed to personalize LLM plannersfor household robotics. Our LLM-Personalize framework features an LLM plannerthat performs iterative planning in multi-room, partially-observable householdscenarios, making use of a scene graph constructed with local observations. Thegenerated plan consists of a sequence of high-level actions which aresubsequently executed by a controller. Central to our approach is theoptimization pipeline, which combines imitation learning and iterativeself-training to personalize the LLM planner. In particular, the imitationlearning phase performs initial LLM alignment from demonstrations, andbootstraps the model to facilitate effective iterative self-training, whichfurther explores and aligns the model to user preferences. We evaluateLLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmarkfor household rearrangements, and show that LLM-Personalize achieves more thana 30 percent increase in success rate over existing LLM planners, showcasingsignificantly improved alignment with human preferences. Project page:https://donggehan.github.io/projectllmpersonalize/.</description><author>Dongge Han, Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Peter Bell, Amos Storkey</author><pubDate>Mon, 22 Apr 2024 16:35:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14285v1</guid></item><item><title>Fast and Robust Normal Estimation for Sparse LiDAR Scans</title><link>http://arxiv.org/abs/2404.14281v1</link><description>Light Detection and Ranging (LiDAR) technology has proven to be an importantpart of many robotics systems. Surface normals estimated from LiDAR data arecommonly used for a variety of tasks in such systems. As most of the today'smechanical LiDAR sensors produce sparse data, estimating normals from a singlescan in a robust manner poses difficulties. In this paper, we address the problem of estimating normals for sparse LiDARdata avoiding the typical issues of smoothing out the normals in high curvatureareas. Mechanical LiDARs rotate a set of rigidly mounted lasers. One firing of sucha set of lasers produces an array of points where each point's neighbor isknown due to the known firing pattern of the scanner. We use this knowledge toconnect these points to their neighbors and label them using the angles of thelines connecting them. When estimating normals at these points, we onlyconsider points with the same label as neighbors. This allows us to avoidestimating normals in high curvature areas. We evaluate our approach on various data, both self-recorded and publiclyavailable, acquired using various sparse LiDAR sensors. We show that using ourmethod for normal estimation leads to normals that are more robust in areaswith high curvature which leads to maps of higher quality. We also show thatour method only incurs a constant factor runtime overhead with respect to alightweight baseline normal estimation procedure and is therefore suited foroperation in computationally demanding environments.</description><author>Igor Bogoslavskyi, Konstantinos Zampogiannis, Raymond Phan</author><pubDate>Mon, 22 Apr 2024 16:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14281v1</guid></item><item><title>RESFM: Robust Equivariant Multiview Structure from Motion</title><link>http://arxiv.org/abs/2404.14280v1</link><description>Multiview Structure from Motion is a fundamental and challenging computervision problem. A recent deep-based approach was proposed utilizing matrixequivariant architectures for the simultaneous recovery of camera pose and 3Dscene structure from large image collections. This work however made theunrealistic assumption that the point tracks given as input are clean ofoutliers. Here we propose an architecture suited to dealing with outliers byadding an inlier/outlier classifying module that respects the modelequivariance and by adding a robust bundle adjustment step. Experimentsdemonstrate that our method can be successfully applied in realistic settingsthat include large image collections and point tracks extracted with commonheuristics and include many outliers.</description><author>Fadi Khatib, Yoni Kasten, Dror Moran, Meirav Galun, Ronen Basri</author><pubDate>Mon, 22 Apr 2024 16:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14280v1</guid></item><item><title>Co-designing a Sub-millisecond Latency Event-based Eye Tracking System with Submanifold Sparse CNN</title><link>http://arxiv.org/abs/2404.14279v1</link><description>Eye-tracking technology is integral to numerous consumer electronicsapplications, particularly in the realm of virtual and augmented reality(VR/AR). These applications demand solutions that excel in three crucialaspects: low-latency, low-power consumption, and precision. Yet, achievingoptimal performance across all these fronts presents a formidable challenge,necessitating a balance between sophisticated algorithms and efficient backendhardware implementations. In this study, we tackle this challenge through asynergistic software/hardware co-design of the system with an event camera.Leveraging the inherent sparsity of event-based input data, we integrate anovel sparse FPGA dataflow accelerator customized for submanifold sparseconvolution neural networks (SCNN). The SCNN implemented on the accelerator canefficiently extract the embedding feature vector from each representation ofevent slices by only processing the non-zero activations. Subsequently, thesevectors undergo further processing by a gated recurrent unit (GRU) and a fullyconnected layer on the host CPU to generate the eye centers. Deployment andevaluation of our system reveal outstanding performance metrics. On theEvent-based Eye-Tracking-AIS2024 dataset, our system achieves 81% p5 accuracy,99.5% p10 accuracy, and 3.71 Mean Euclidean Distance with 0.7 ms latency whileonly consuming 2.29 mJ per inference. Notably, our solution opens upopportunities for future eye-tracking systems. Code is available athttps://github.com/CASR-HKU/ESDA/tree/eye_tracking.</description><author>Baoheng Zhang, Yizhao Gao, Jingyuan Li, Hayden Kwok-Hay So</author><pubDate>Mon, 22 Apr 2024 16:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14279v1</guid></item><item><title>A Bayesian Approach for Prioritising Driving Behaviour Investigations in Telematic Auto Insurance Policies</title><link>http://arxiv.org/abs/2404.14276v1</link><description>Automotive insurers increasingly have access to telematic information viablack-box recorders installed in the insured vehicle, and wish to identifyundesirable behaviour which may signify increased risk or uninsured activities.However, identification of such behaviour with machine learning is non-trivial,and results are far from perfect, requiring human investigation to verifysuspected cases. An appropriately formed priority score, generated by automatedanalysis of GPS data, allows underwriters to make more efficient use of theirtime, improving detection of the behaviour under investigation. An example of such behaviour is the use of a privately insured vehicle forcommercial purposes, such as delivering meals and parcels. We first make use oftrip GPS and accelerometer data, augmented by geospatial information, to trainan imperfect classifier for delivery driving on a per-trip basis. We make useof a mixture of Beta-Binomial distributions to model the propensity of apolicyholder to undertake trips which result in a positive classification asbeing drawn from either a rare high-scoring or common low-scoring group, andlearn the parameters of this model using MCMC. This model provides us with aposterior probability that any policyholder will be a regular generator ofautomated alerts given any number of trips and alerts. This posteriorprobability is converted to a priority score, which was used to select the mostvaluable candidates for manual investigation. Testing over a 1-year period ranked policyholders by likelihood of commercialdriving activity on a weekly basis. The top 0.9% have been reviewed at leastonce by the underwriters at the time of writing, and of those 99.4% have beenconfirmed as correctly identified, showing the approach has achieved asignificant improvement in efficiency of human resource allocation compared tomanual searching.</description><author>Mark McLeod, Bernardo Perez-Orozco, Nika Lee, Davide Zilli</author><pubDate>Mon, 22 Apr 2024 16:26:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14276v1</guid></item><item><title>Causality Extraction from Nuclear Licensee Event Reports Using a Hybrid Framework</title><link>http://arxiv.org/abs/2404.05656v2</link><description>Industry-wide nuclear power plant operating experience is a critical sourceof raw data for performing parameter estimations in reliability and riskmodels. Much operating experience information pertains to failure events and isstored as reports containing unstructured data, such as narratives. Eventreports are essential for understanding how failures are initiated andpropagated, including the numerous causal relations involved. Causal relationextraction using deep learning represents a significant frontier in the fieldof natural language processing (NLP), and is crucial since it enables theinterpretation of intricate narratives and connections contained within vastamounts of written information. This paper proposed a hybrid framework forcausality detection and extraction from nuclear licensee event reports. Themain contributions include: (1) we compiled an LER corpus with 20,129 textsamples for causality analysis, (2) developed an interactive tool for labelingcause effect pairs, (3) built a deep-learning-based approach for causalrelation detection, and (4) developed a knowledge based cause-effect extractionapproach.</description><author>Shahidur Rahoman Sohag, Sai Zhang, Min Xian, Shoukun Sun, Fei Xu, Zhegang Ma</author><pubDate>Mon, 22 Apr 2024 16:25:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05656v2</guid></item><item><title>Random Walk in Random Permutation Set Theory</title><link>http://arxiv.org/abs/2404.03978v2</link><description>Random walk is an explainable approach for modeling natural processes at themolecular level. The Random Permutation Set Theory (RPST) serves as a frameworkfor uncertainty reasoning, extending the applicability of Dempster-ShaferTheory. Recent explorations indicate a promising link between RPST and randomwalk. In this study, we conduct an analysis and construct a random walk modelbased on the properties of RPST, with Monte Carlo simulations of such randomwalk. Our findings reveal that the random walk generated through RPST exhibitscharacteristics similar to those of a Gaussian random walk and can betransformed into a Wiener process through a specific limiting scalingprocedure. This investigation establishes a novel connection between RPST andrandom walk theory, thereby not only expanding the applicability of RPST, butalso demonstrating the potential for combining the strengths of both approachesto improve problem-solving abilities.</description><author>Jiefeng Zhou, Zhen Li, Yong Deng</author><pubDate>Mon, 22 Apr 2024 16:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03978v2</guid></item><item><title>Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents</title><link>http://arxiv.org/abs/2311.13373v5</link><description>Recent studies have uncovered the potential of Large Language Models (LLMs)in addressing complex sequential decision-making tasks through the provision ofhigh-level instructions. However, LLM-based agents lack specialization intackling specific target problems, particularly in real-time dynamicenvironments. Additionally, deploying an LLM-based agent in practical scenarioscan be both costly and time-consuming. On the other hand, reinforcementlearning (RL) approaches train agents that specialize in the target task butoften suffer from low sampling efficiency and high exploration costs. In thispaper, we introduce a novel framework that addresses these challenges bytraining a smaller, specialized student RL agent using instructions from anLLM-based teacher agent. By incorporating the guidance from the teacher agent,the student agent can distill the prior knowledge of the LLM into its ownmodel. Consequently, the student agent can be trained with significantly lessdata. Moreover, through further training with environment feedback, the studentagent surpasses the capabilities of its teacher for completing the target task.We conducted experiments on challenging MiniGrid and Habitat environments,specifically designed for embodied AI research, to evaluate the effectivenessof our framework. The results clearly demonstrate that our approach achievessuperior performance compared to strong baseline methods. Our code is availableat https://github.com/ZJLAB-AMMI/LLM4Teach.</description><author>Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang, Bin Liu</author><pubDate>Mon, 22 Apr 2024 16:17:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13373v5</guid></item><item><title>Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance Propagation</title><link>http://arxiv.org/abs/2404.14271v1</link><description>Explainability is a key component in many applications involving deep neuralnetworks (DNNs). However, current explanation methods for DNNs commonly leaveit to the human observer to distinguish relevant explanations from spuriousnoise. This is not feasible anymore when going from easily human-accessibledata such as images to more complex data such as genome sequences. Tofacilitate the accessibility of DNN outputs from such complex data and toincrease explainability, we present a modification of the widely usedexplanation method layer-wise relevance propagation. Our approach enforcessparsity directly by pruning the relevance propagation for the differentlayers. Thereby, we achieve sparser relevance attributions for the inputfeatures as well as for the intermediate layers. As the relevance propagationis input-specific, we aim to prune the relevance propagation rather than theunderlying model architecture. This allows to prune different neurons fordifferent inputs and hence, might be more appropriate to the local nature ofexplanation methods. To demonstrate the efficacy of our method, we evaluate iton two types of data, images and genomic sequences. We show that ourmodification indeed leads to noise reduction and concentrates relevance on themost important features compared to the baseline.</description><author>Paulo Yanez Sarmiento, Simon Witzke, Nadja Klein, Bernhard Y. Renard</author><pubDate>Mon, 22 Apr 2024 16:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14271v1</guid></item><item><title>What do Transformers Know about Government?</title><link>http://arxiv.org/abs/2404.14270v1</link><description>This paper investigates what insights about linguistic features and whatknowledge about the structure of natural language can be obtained from theencodings in transformer language models.In particular, we explore how BERTencodes the government relation between constituents in a sentence. We useseveral probing classifiers, and data from two morphologically rich languages.Our experiments show that information about government is encoded across alltransformer layers, but predominantly in the early layers of the model. We findthat, for both languages, a small number of attention heads encode enoughinformation about the government relations to enable us to train a classifiercapable of discovering new, previously unknown types of government, never seenin the training data. Currently, data is lacking for the research communityworking on grammatical constructions, and government in particular. We releasethe Government Bank -- a dataset defining the government relations forthousands of lemmas in the languages in our experiments.</description><author>Jue Hou, Anisia Katinskaia, Lari Kotilainen, Sathianpong Trangcasanchai, Anh-Duc Vu, Roman Yangarber</author><pubDate>Mon, 22 Apr 2024 16:15:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14270v1</guid></item><item><title>Deep Learning as Ricci Flow</title><link>http://arxiv.org/abs/2404.14265v1</link><description>Deep neural networks (DNNs) are powerful tools for approximating thedistribution of complex data. It is known that data passing through a trainedDNN classifier undergoes a series of geometric and topological simplifications.While some progress has been made toward understanding these transformations inneural networks with smooth activation functions, an understanding in the moregeneral setting of non-smooth activation functions, such as the rectifiedlinear unit (ReLU), which tend to perform better, is required. Here we proposethat the geometric transformations performed by DNNs during classificationtasks have parallels to those expected under Hamilton's Ricci flow - a toolfrom differential geometry that evolves a manifold by smoothing its curvature,in order to identify its topology. To illustrate this idea, we present acomputational framework to quantify the geometric changes that occur as datapasses through successive layers of a DNN, and use this framework to motivate anotion of `global Ricci network flow' that can be used to assess a DNN'sability to disentangle complex data geometries to solve classificationproblems. By training more than $1,500$ DNN classifiers of different widths anddepths on synthetic and real-world data, we show that the strength of globalRicci network flow-like behaviour correlates with accuracy for well-trainedDNNs, independently of depth, width and data set. Our findings motivate the useof tools from differential and discrete geometry to the problem ofexplainability in deep learning.</description><author>Anthony Baptista, Alessandro Barp, Tapabrata Chakraborti, Chris Harbron, Ben D. MacArthur, Christopher R. S. Banerji</author><pubDate>Mon, 22 Apr 2024 16:12:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14265v1</guid></item><item><title>NeLF-Pro: Neural Light Field Probes for Multi-Scale Novel View Synthesis</title><link>http://arxiv.org/abs/2312.13328v2</link><description>We present NeLF-Pro, a novel representation to model and reconstruct lightfields in diverse natural scenes that vary in extent and spatial granularity.In contrast to previous fast reconstruction methods that represent the 3D sceneglobally, we model the light field of a scene as a set of local light fieldfeature probes, parameterized with position and multi-channel 2D feature maps.Our central idea is to bake the scene's light field into spatially varyinglearnable representations and to query point features by weighted blending ofprobes close to the camera - allowing for mipmap representation and rendering.We introduce a novel vector-matrix-matrix (VMM) factorization technique thateffectively represents the light field feature probes as products of corefactors (i.e., VM) shared among local feature probes, and a basis factor (i.e.,M) - efficiently encoding internal relationships and patterns within the scene.Experimentally, we demonstrate that NeLF-Pro significantly boosts theperformance of feature grid-based representations, and achieves fastreconstruction with better rendering quality while maintaining compactmodeling. Project webpage https://sinoyou.github.io/nelf-pro/.</description><author>Zinuo You, Andreas Geiger, Anpei Chen</author><pubDate>Mon, 22 Apr 2024 16:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13328v2</guid></item><item><title>Using explainable AI to investigate electrocardiogram changes during healthy aging -- from expert features to raw signals</title><link>http://arxiv.org/abs/2310.07463v2</link><description>Cardiovascular diseases remain the leading global cause of mortality. Age isan important covariate whose effect is most easily investigated in a healthycohort to properly distinguish the former from disease-related changes.Traditionally, most of such insights have been drawn from the analysis ofelectrocardiogram (ECG) feature changes in individuals as they age. However,these features, while informative, may potentially obscure underlying datarelationships. In this paper we present the following contributions: (1) Weemploy a deep-learning model and a tree-based model to analyze ECG data from arobust dataset of healthy individuals across varying ages in both raw signalsand ECG feature format. (2) We use explainable AI methods to identify the mostdiscriminative ECG features across age groups.(3) Our analysis with tree-basedclassifiers reveals age-related declines in inferred breathing rates andidentifies notably high SDANN values as indicative of elderly individuals,distinguishing them from younger adults. (4) Furthermore, the deep-learningmodel underscores the pivotal role of the P-wave in age predictions across allage groups, suggesting potential changes in the distribution of differentP-wave types with age. These findings shed new light on age-related ECGchanges, offering insights that transcend traditional feature-based approaches.</description><author>Gabriel Ott, Yannik Schaubelt, Juan Miguel Lopez Alcaraz, Wilhelm Haverkamp, Nils Strodthoff</author><pubDate>Mon, 22 Apr 2024 16:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07463v2</guid></item><item><title>CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and View-consistent 3D Semantic Understanding</title><link>http://arxiv.org/abs/2404.14249v1</link><description>The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-timesynthesis of novel views in 3D scenes. Currently, it primarily focuses ongeometry and appearance modeling, while lacking the semantic understanding ofscenes. To bridge this gap, we present CLIP-GS, which integrates semantics fromContrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting toefficiently comprehend 3D environments without annotated semantic data. Inspecific, rather than straightforwardly learning and rendering high-dimensionalsemantic features of 3D Gaussians, which significantly diminishes theefficiency, we propose a Semantic Attribute Compactness (SAC) approach. SACexploits the inherent unified semantics within objects to learn compact yeteffective semantic representations of 3D Gaussians, enabling highly efficientrendering (&gt;100 FPS). Additionally, to address the semantic ambiguity, causedby utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, weintroduce a 3D Coherent Self-training (3DCS) strategy, resorting to themulti-view consistency originated from the 3D model. 3DCS imposes cross-viewsemantic consistency constraints by leveraging refined, self-predictedpseudo-labels derived from the trained 3D Gaussian model, thereby enhancingprecise and view-consistent segmentation results. Extensive experimentsdemonstrate that our method remarkably outperforms existing state-of-the-artapproaches, achieving improvements of 17.29% and 20.81% in mIoU metric onReplica and ScanNet datasets, respectively, while maintaining real-timerendering speed. Furthermore, our approach exhibits superior performance evenwith sparse input data, verifying the robustness of our method.</description><author>Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu</author><pubDate>Mon, 22 Apr 2024 16:01:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14249v1</guid></item><item><title>NTIRE 2024 Challenge on Low Light Image Enhancement: Methods and Results</title><link>http://arxiv.org/abs/2404.14248v1</link><description>This paper reviews the NTIRE 2024 low light image enhancement challenge,highlighting the proposed solutions and results. The aim of this challenge isto discover an effective network design or solution capable of generatingbrighter, clearer, and visually appealing results when dealing with a varietyof conditions, including ultra-high resolution (4K and beyond), non-uniformillumination, backlighting, extreme darkness, and night scenes. A notable totalof 428 participants registered for the challenge, with 22 teams ultimatelymaking valid submissions. This paper meticulously evaluates thestate-of-the-art advancements in enhancing low-light images, reflecting thesignificant progress and creativity in this field.</description><author>Xiaoning Liu, Zongwei Wu, Ao Li, Florin-Alexandru Vasluianu, Yulun Zhang, Shuhang Gu, Le Zhang, Ce Zhu, Radu Timofte, Zhi Jin, Hongjun Wu, Chenxi Wang, Haitao Ling, Yuanhao Cai, Hao Bian, Yuxin Zheng, Jing Lin, Alan Yuille, Ben Shao, Jin Guo, Tianli Liu, Mohao Wu, Yixu Feng, Shuo Hou, Haotian Lin, Yu Zhu, Peng Wu, Wei Dong, Jinqiu Sun, Yanning Zhang, Qingsen Yan, Wenbin Zou, Weipeng Yang, Yunxiang Li, Qiaomu Wei, Tian Ye, Sixiang Chen, Zhao Zhang, Suiyi Zhao, Bo Wang, Yan Luo, Zhichao Zuo, Mingshen Wang, Junhu Wang, Yanyan Wei, Xiaopeng Sun, Yu Gao, Jiancheng Huang, Hongming Chen, Xiang Chen, Hui Tang, Yuanbin Chen, Yuanbo Zhou, Xinwei Dai, Xintao Qiu, Wei Deng, Qinquan Gao, Tong Tong, Mingjia Li, Jin Hu, Xinyu He, Xiaojie Guo, Sabarinathan, K Uma, A Sasithradevi, B Sathya Bama, S. Mohamed</author><pubDate>Mon, 22 Apr 2024 16:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14248v1</guid></item><item><title>From Modalities to Styles: Rethinking the Domain Gap in Heterogeneous Face Recognition</title><link>http://arxiv.org/abs/2404.14247v1</link><description>Heterogeneous Face Recognition (HFR) focuses on matching faces from differentdomains, for instance, thermal to visible images, making Face Recognition (FR)systems more versatile for challenging scenarios. However, the domain gapbetween these domains and the limited large-scale datasets in the target HFRmodalities make it challenging to develop robust HFR models from scratch. Inour work, we view different modalities as distinct styles and propose a methodto modulate feature maps of the target modality to address the domain gap. Wepresent a new Conditional Adaptive Instance Modulation (CAIM ) module thatseamlessly fits into existing FR networks, turning them into HFR-ready systems.The CAIM block modulates intermediate feature maps, efficiently adapting to thestyle of the source modality and bridging the domain gap. Our method enablesend-to-end training using a small set of paired samples. We extensivelyevaluate the proposed approach on various challenging HFR benchmarks, showingthat it outperforms state-of-the-art methods. The source code and protocols forreproducing the findings will be made publicly available</description><author>Anjith George, Sebastien Marcel</author><pubDate>Mon, 22 Apr 2024 16:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14247v1</guid></item><item><title>AI-Generated Faces in the Real World: A Large-Scale Case Study of Twitter Profile Images</title><link>http://arxiv.org/abs/2404.14244v1</link><description>Recent advances in the field of generative artificial intelligence (AI) haveblurred the lines between authentic and machine-generated content, making italmost impossible for humans to distinguish between such media. One notableconsequence is the use of AI-generated images for fake profiles on socialmedia. While several types of disinformation campaigns and similar incidentshave been reported in the past, a systematic analysis has been lacking. In thiswork, we conduct the first large-scale investigation of the prevalence ofAI-generated profile pictures on Twitter. We tackle the challenges of areal-world measurement study by carefully integrating various data sources anddesigning a multi-stage detection pipeline. Our analysis of nearly 15 millionTwitter profile pictures shows that 0.052% were artificially generated,confirming their notable presence on the platform. We comprehensively examinethe characteristics of these accounts and their tweet content, and uncoverpatterns of coordinated inauthentic behavior. The results also reveal severalmotives, including spamming and political amplification campaigns. Our researchreaffirms the need for effective detection and mitigation strategies to copewith the potential negative effects of generative AI in the future.</description><author>Jonas Ricker, Dennis Assenmacher, Thorsten Holz, Asja Fischer, Erwin Quiring</author><pubDate>Mon, 22 Apr 2024 15:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14244v1</guid></item><item><title>Turbo-CF: Matrix Decomposition-Free Graph Filtering for Fast Recommendation</title><link>http://arxiv.org/abs/2404.14243v1</link><description>A series of graph filtering (GF)-based collaborative filtering (CF) showcasesstate-of-the-art performance on the recommendation accuracy by using a low-passfilter (LPF) without a training process. However, conventional GF-based CFapproaches mostly perform matrix decomposition on the item-item similaritygraph to realize the ideal LPF, which results in a non-trivial computationalcost and thus makes them less practical in scenarios where rapidrecommendations are essential. In this paper, we propose Turbo-CF, a GF-basedCF method that is both training-free and matrix decomposition-free. Turbo-CFemploys a polynomial graph filter to circumvent the issue of expensive matrixdecompositions, enabling us to make full use of modern computer hardwarecomponents (i.e., GPU). Specifically, Turbo-CF first constructs an item-itemsimilarity graph whose edge weights are effectively regulated. Then, our ownpolynomial LPFs are designed to retain only low-frequency signals withoutexplicit matrix decompositions. We demonstrate that Turbo-CF is extremely fastyet accurate, achieving a runtime of less than 1 second on real-world benchmarkdatasets while achieving recommendation accuracies comparable to bestcompetitors.</description><author>Jin-Duk Park, Yong-Min Shin, Won-Yong Shin</author><pubDate>Mon, 22 Apr 2024 15:56:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14243v1</guid></item><item><title>UrbanCross: Enhancing Satellite Image-Text Retrieval with Cross-Domain Adaptation</title><link>http://arxiv.org/abs/2404.14241v1</link><description>Urbanization challenges underscore the necessity for effective satelliteimage-text retrieval methods to swiftly access specific information enrichedwith geographic semantics for urban applications. However, existing methodsoften overlook significant domain gaps across diverse urban landscapes,primarily focusing on enhancing retrieval performance within single domains. Totackle this issue, we present UrbanCross, a new framework for cross-domainsatellite image-text retrieval. UrbanCross leverages a high-quality,cross-domain dataset enriched with extensive geo-tags from three countries tohighlight domain diversity. It employs the Large Multimodal Model (LMM) fortextual refinement and the Segment Anything Model (SAM) for visualaugmentation, achieving a fine-grained alignment of images, segments and texts,yielding a 10% improvement in retrieval performance. Additionally, UrbanCrossincorporates an adaptive curriculum-based source sampler and a weightedadversarial cross-domain fine-tuning module, progressively enhancingadaptability across various domains. Extensive experiments confirm UrbanCross'ssuperior efficiency in retrieval and adaptation to new urban environments,demonstrating an average performance increase of 15% over its version withoutdomain adaptation mechanisms, effectively bridging the domain gap.</description><author>Siru Zhong, Xixuan Hao, Yibo Yan, Ying Zhang, Yangqiu Song, Yuxuan Liang</author><pubDate>Mon, 22 Apr 2024 15:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14241v1</guid></item><item><title>Collaborative Filtering Based on Diffusion Models: Unveiling the Potential of High-Order Connectivity</title><link>http://arxiv.org/abs/2404.14240v1</link><description>A recent study has shown that diffusion models are well-suited for modelingthe generative process of user-item interactions in recommender systems due totheir denoising nature. However, existing diffusion model-based recommendersystems do not explicitly leverage high-order connectivities that containcrucial collaborative signals for accurate recommendations. Addressing thisgap, we propose CF-Diff, a new diffusion model-based collaborative filtering(CF) method, which is capable of making full use of collaborative signals alongwith multi-hop neighbors. Specifically, the forward-diffusion process addsrandom noise to user-item interactions, while the reverse-denoising processaccommodates our own learning model, named cross-attention-guided multi-hopautoencoder (CAM-AE), to gradually recover the original user-item interactions.CAM-AE consists of two core modules: 1) the attention-aided AE module,responsible for precisely learning latent representations of user-iteminteractions while preserving the model's complexity at manageable levels, and2) the multi-hop cross-attention module, which judiciously harnesses high-orderconnectivity information to capture enhanced collaborative signals. Throughcomprehensive experiments on three real-world datasets, we demonstrate thatCF-Diff is (a) Superior: outperforming benchmark recommendation methods,achieving remarkable gains up to 7.29% compared to the best competitor, (b)Theoretically-validated: reducing computations while ensuring that theembeddings generated by our model closely approximate those from the originalcross-attention, and (c) Scalable: proving the computational efficiency thatscales linearly with the number of users or items.</description><author>Yu Hou, Jin-Duk Park, Won-Yong Shin</author><pubDate>Mon, 22 Apr 2024 15:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14240v1</guid></item><item><title>Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning</title><link>http://arxiv.org/abs/2403.18985v2</link><description>We present a generic Reinforcement Learning (RL) framework optimized forcrafting adversarial attacks on different model types spanning from ECG signalanalysis (1D), image classification (2D), and video classification (3D). Theframework focuses on identifying sensitive regions and inducingmisclassifications with minimal distortions and various distortion types. Thenovel RL method outperforms state-of-the-art methods for all threeapplications, proving its efficiency. Our RL approach produces superiorlocalization masks, enhancing interpretability for image classification and ECGanalysis models. For applications such as ECG analysis, our platform highlightscritical ECG segments for clinicians while ensuring resilience againstprevalent distortions. This comprehensive tool aims to bolster both resiliencewith adversarial training and transparency across varied applications and datatypes.</description><author>Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Avisek Naug, Sahand Ghorbanpour</author><pubDate>Mon, 22 Apr 2024 15:49:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18985v2</guid></item><item><title>Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography</title><link>http://arxiv.org/abs/2403.16687v3</link><description>In recent years, the rapid development of artificial intelligence technology,especially the emergence of large language models (LLMs) such as ChatGPT, haspresented significant prospects for application in the field of education. LLMspossess the capability to interpret knowledge, answer questions, and considercontext, thus providing support for dialogic teaching to students. Therefore,an examination of the capacity of LLMs to effectively fulfill instructionalroles, thereby facilitating student learning akin to human educators withindialogic teaching scenarios, is an exceptionally valuable research topic. Thisresearch recruited 34 undergraduate students as participants, who were randomlydivided into two groups. The experimental group engaged in dialogic teachingusing ChatGPT, while the control group interacted with human teachers. Bothgroups learned the histogram equalization unit in the information-relatedcourse "Digital Image Processing". The research findings show comparable scoresbetween the two groups on the retention test. However, students who engaged indialogue with ChatGPT exhibited lower performance on the transfer test.Electroencephalography data revealed that students who interacted with ChatGPTexhibited higher levels of cognitive activity, suggesting that ChatGPT couldhelp students establish a knowledge foundation and stimulate cognitiveactivity. However, its strengths on promoting students. knowledge applicationand creativity were insignificant. Based upon the research findings, it isevident that ChatGPT cannot fully excel in fulfilling teaching tasks in thedialogue teaching in information related courses. Combining ChatGPT withtraditional human teachers might be a more ideal approach. The synergistic useof both can provide students with more comprehensive learning support, thuscontributing to enhancing the quality of teaching.</description><author>Jiayue Zhang, Yiheng Liu, Wenqi Cai, Lanlan Wu, Yali Peng, Jingjing Yu, Senqing Qi, Taotao Long, Bao Ge</author><pubDate>Mon, 22 Apr 2024 15:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16687v3</guid></item><item><title>MultiBooth: Towards Generating All Your Concepts in an Image from Text</title><link>http://arxiv.org/abs/2404.14239v1</link><description>This paper introduces MultiBooth, a novel and efficient technique formulti-concept customization in image generation from text. Despite thesignificant advancements in customized generation methods, particularly withthe success of diffusion models, existing methods often struggle withmulti-concept scenarios due to low concept fidelity and high inference cost.MultiBooth addresses these issues by dividing the multi-concept generationprocess into two phases: a single-concept learning phase and a multi-conceptintegration phase. During the single-concept learning phase, we employ amulti-modal image encoder and an efficient concept encoding technique to learna concise and discriminative representation for each concept. In themulti-concept integration phase, we use bounding boxes to define the generationarea for each concept within the cross-attention map. This method enables thecreation of individual concepts within their specified regions, therebyfacilitating the formation of multi-concept images. This strategy not onlyimproves concept fidelity but also reduces additional inference cost.MultiBooth surpasses various baselines in both qualitative and quantitativeevaluations, showcasing its superior performance and computational efficiency.Project Page: https://multibooth.github.io/</description><author>Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Li Xiu</author><pubDate>Mon, 22 Apr 2024 15:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14239v1</guid></item><item><title>Beyond the Edge: An Advanced Exploration of Reinforcement Learning for Mobile Edge Computing, its Applications, and Future Research Trajectories</title><link>http://arxiv.org/abs/2404.14238v1</link><description>Mobile Edge Computing (MEC) broadens the scope of computation and storagebeyond the central network, incorporating edge nodes close to end devices. Thisexpansion facilitates the implementation of large-scale "connected things"within edge networks. The advent of applications necessitating real-time,high-quality service presents several challenges, such as low latency, highdata rate, reliability, efficiency, and security, all of which demandresolution. The incorporation of reinforcement learning (RL) methodologieswithin MEC networks promotes a deeper understanding of mobile user behaviorsand network dynamics, thereby optimizing resource use in computing andcommunication processes. This paper offers an exhaustive survey of RLapplications in MEC networks, initially presenting an overview of RL from itsfundamental principles to the latest advanced frameworks. Furthermore, itoutlines various RL strategies employed in offloading, caching, andcommunication within MEC networks. Finally, it explores open issues linked withsoftware and hardware platforms, representation, RL robustness, safe RL,large-scale scheduling, generalization, security, and privacy. The paperproposes specific RL techniques to mitigate these issues and provides insightsinto their practical applications.</description><author>Ning Yang, Shuo Chen, Haijun Zhang, Randall Berry</author><pubDate>Mon, 22 Apr 2024 15:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14238v1</guid></item><item><title>Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback</title><link>http://arxiv.org/abs/2404.14233v1</link><description>The rapidly developing Large Vision Language Models (LVLMs) have shownnotable capabilities on a range of multi-modal tasks, but still face thehallucination phenomena where the generated texts do not align with the givencontexts, significantly restricting the usages of LVLMs. Most previous workdetects and mitigates hallucination at the coarse-grained level or requiresexpensive annotation (e.g., labeling by proprietary models or human experts).To address these issues, we propose detecting and mitigating hallucinations inLVLMs via fine-grained AI feedback. The basic idea is that we generate asmall-size sentence-level hallucination annotation dataset by proprietarymodels, whereby we train a hallucination detection model which can performsentence-level hallucination detection, covering primary hallucination types(i.e., object, attribute, and relationship). Then, we propose adetect-then-rewrite pipeline to automatically construct preference dataset fortraining hallucination mitigating model. Furthermore, we proposedifferentiating the severity of hallucinations, and introducing a HallucinationSeverity-Aware Direct Preference Optimization (HSA-DPO) for mitigatinghallucination in LVLMs by incorporating the severity of hallucinations intopreference learning. Extensive experiments demonstrate the effectiveness of ourmethod.</description><author>Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Hao Jiang, Fei Wu, Linchao Zhu</author><pubDate>Mon, 22 Apr 2024 15:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14233v1</guid></item><item><title>Shifting Focus with HCEye: Exploring the Dynamics of Visual Highlighting and Cognitive Load on User Attention and Saliency Prediction</title><link>http://arxiv.org/abs/2404.14232v1</link><description>Visual highlighting can guide user attention in complex interfaces. However,its effectiveness under limited attentional capacities is underexplored. Thispaper examines the joint impact of visual highlighting (permanent and dynamic)and dual-task-induced cognitive load on gaze behaviour. Our analysis, usingeye-movement data from 27 participants viewing 150 unique webpages reveals thatwhile participants' ability to attend to UI elements decreases with increasingcognitive load, dynamic adaptations (i.e., highlighting) remainattention-grabbing. The presence of these factors significantly alters whatpeople attend to and thus what is salient. Accordingly, we show thatstate-of-the-art saliency models increase their performance when accounting fordifferent cognitive loads. Our empirical insights, along with our openlyavailable dataset, enhance our understanding of attentional processes in UIsunder varying cognitive (and perceptual) loads and open the door for new modelsthat can predict user attention while multitasking.</description><author>Anwesha Das, Zekun Wu, Iza Škrjanec, Anna Maria Feit</author><pubDate>Mon, 22 Apr 2024 15:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14232v1</guid></item><item><title>SPINEPS -- Automatic Whole Spine Segmentation of T2-weighted MR images using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation</title><link>http://arxiv.org/abs/2402.16368v2</link><description>Purpose. To present SPINEPS, an open-source deep learning approach forsemantic and instance segmentation of 14 spinal structures (ten vertebrasubstructures, intervertebral discs, spinal cord, spinal canal, and sacrum) inwhole body T2w MRI. Methods. During this HIPPA-compliant, retrospective study, we utilized thepublic SPIDER dataset (218 subjects, 63% female) and a subset of the GermanNational Cohort (1423 subjects, mean age 53, 49% female) for training andevaluation. We combined CT and T2w segmentations to train models that segment14 spinal structures in T2w sagittal scans both semantically and instance-wise.Performance evaluation metrics included Dice similarity coefficient, averagesymmetrical surface distance, panoptic quality, segmentation quality, andrecognition quality. Statistical significance was assessed using the Wilcoxonsigned-rank test. An in-house dataset was used to qualitatively evaluateout-of-distribution samples. Results. On the public dataset, our approach outperformed the baseline(instance-wise vertebra dice score 0.929 vs. 0.907, p-value&lt;0.001). Training onauto-generated annotations and evaluating on manually corrected test data fromthe GNC yielded global dice scores of 0.900 for vertebrae, 0.960 forintervertebral discs, and 0.947 for the spinal canal. Incorporating the SPIDERdataset during training increased these scores to 0.920, 0.967, 0.958,respectively. Conclusions. The proposed segmentation approach offers robust segmentation of14 spinal structures in T2w sagittal images, including the spinal cord, spinalcanal, intervertebral discs, endplate, sacrum, and vertebrae. The approachyields both a semantic and instance mask as output, thus being easy to utilize.This marks the first publicly available algorithm for whole spine segmentationin sagittal T2w MR imaging.</description><author>Hendrik Möller, Robert Graf, Joachim Schmitt, Benjamin Keinert, Matan Atad, Anjany Sekuboyina, Felix Streckenbach, Hanna Schön, Florian Kofler, Thomas Kroencke, Stefanie Bette, Stefan Willich, Thomas Keil, Thoralf Niendorf, Tobias Pischon, Beate Endemann, Bjoern Menze, Daniel Rueckert, Jan S. Kirschke</author><pubDate>Mon, 22 Apr 2024 15:44:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16368v2</guid></item><item><title>Harnessing Orthogonality to Train Low-Rank Neural Networks</title><link>http://arxiv.org/abs/2401.08505v2</link><description>This study explores the learning dynamics of neural networks by analyzing thesingular value decomposition (SVD) of their weights throughout training. Ourinvestigation reveals that an orthogonal basis within each multidimensionalweight's SVD representation stabilizes during training. Building upon this, weintroduce Orthogonality-Informed Adaptive Low-Rank (OIALR) training, a noveltraining method exploiting the intrinsic orthogonality of neural networks.OIALR seamlessly integrates into existing training workflows with minimalaccuracy loss, as demonstrated by benchmarking on various datasets andwell-established network architectures. With appropriate hyperparameter tuning,OIALR can surpass conventional training setups, including those ofstate-of-the-art models.</description><author>Daniel Coquelin, Katharina Flügel, Marie Weiel, Nicholas Kiefer, Charlotte Debus, Achim Streit, Markus Götz</author><pubDate>Mon, 22 Apr 2024 15:39:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08505v2</guid></item><item><title>A Survey of Decomposition-Based Evolutionary Multi-Objective Optimization: Part II -- A Data Science Perspective</title><link>http://arxiv.org/abs/2404.14228v1</link><description>This paper presents the second part of the two-part survey series ondecomposition-based evolutionary multi-objective optimization where we mainlyfocus on discussing the literature related to multi-objective evolutionaryalgorithms based on decomposition (MOEA/D). Complementary to the first part,here we employ a series of advanced data mining approaches to provide acomprehensive anatomy of the enormous landscape of MOEA/D research, which isfar beyond the capacity of classic manual literature review protocol. In doingso, we construct a heterogeneous knowledge graph that encapsulates more than5,400 papers, 10,000 authors, 400 venues, and 1,600 institutions for MOEA/Dresearch. We start our analysis with basic descriptive statistics. Then wedelve into prominent research/application topics pertaining to MOEA/D withstate-of-the-art topic modeling techniques and interrogate theirsptial-temporal and bilateral relationships. We also explored the collaborationand citation networks of MOEA/D, uncovering hidden patterns in the growth ofliterature as well as collaboration between researchers. Our data miningresults here, combined with the expert review in Part I, together offer aholistic view of the MOEA/D research, and demonstrate the potential of anexciting new paradigm for conducting scientific surveys from a data scienceperspective.</description><author>Mingyu Huang, Ke Li</author><pubDate>Mon, 22 Apr 2024 15:38:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14228v1</guid></item><item><title>YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery</title><link>http://arxiv.org/abs/2404.00257v2</link><description>Because of its use in practice, open-world object detection (OWOD) has gottena lot of attention recently. The challenge is how can a model detect novelclasses and then incrementally learn them without forgetting previously knownclasses. Previous approaches hinge on strongly-supervised or weakly-supervisednovel-class data for novel-class detection, which may not apply to realapplications. We construct a new benchmark that novel classes are onlyencountered at the inference stage. And we propose a new OWOD detector YOLOOC,based on the YOLO architecture yet for the Open-Class setup. We introduce labelsmoothing to prevent the detector from over-confidently mapping novel classesto known classes and to discover novel classes. Extensive experiments conductedon our more realistic setup demonstrate the effectiveness of our method fordiscovering novel classes in our new benchmark.</description><author>Qian Wan, Xiang Xiang, Qinhao Zhou</author><pubDate>Mon, 22 Apr 2024 15:38:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00257v2</guid></item></channel></rss>