<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 05 Jun 2024 14:00:10 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>VHS: High-Resolution Iterative Stereo Matching with Visual Hull Priors</title><link>http://arxiv.org/abs/2406.02552v1</link><description>We present a stereo-matching method for depth estimation from high-resolutionimages using visual hulls as priors, and a memory-efficient technique for thecorrelation computation. Our method uses object masks extracted fromsupplementary views of the scene to guide the disparity estimation, effectivelyreducing the search space for matches. This approach is specifically tailoredto stereo rigs in volumetric capture systems, where an accurate depth plays akey role in the downstream reconstruction task. To enable training andregression at high resolutions targeted by recent systems, our approach extendsa sparse correlation computation into a hybrid sparse-dense scheme suitable forapplication in leading recurrent network architectures. We evaluate theperformance-efficiency trade-off of our method compared to state-of-the-artmethods, and demonstrate the efficacy of the visual hull guidance. In addition,we propose a training scheme for a further reduction of memory requirementsduring optimization, facilitating training on high-resolution data.</description><author>Markus Plack, Hannah Dröge, Leif Van Holland, Matthias B. Hullin</author><pubDate>Tue, 04 Jun 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02552v1</guid></item><item><title>Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks</title><link>http://arxiv.org/abs/2406.02550v1</link><description>Large language models can solve tasks that were not present in the trainingset. This capability is believed to be due to in-context learning and skillcomposition. In this work, we study the emergence of in-context learning andskill composition in a collection of modular arithmetic tasks. Specifically, weconsider a finite collection of linear modular functions $z = a \, x + b \, y\;\mathrm{mod}\; p$ labeled by the vector $(a, b) \in \mathbb{Z}_p^2$. We usesome of these tasks for pre-training and the rest for out-of-distributiontesting. We empirically show that a GPT-style transformer exhibits a transitionfrom in-distribution to out-of-distribution generalization as the number ofpre-training tasks increases. We find that the smallest model capable ofout-of-distribution generalization requires two transformer blocks, while fordeeper models, the out-of-distribution generalization phase is\emph{transient}, necessitating early stopping. Finally, we perform aninterpretability study of the pre-trained models, revealing the highlystructured representations in both phases; and discuss the learnt algorithm.</description><author>Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov</author><pubDate>Tue, 04 Jun 2024 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02550v1</guid></item><item><title>Dreamguider: Improved Training free Diffusion-based Conditional Generation</title><link>http://arxiv.org/abs/2406.02549v1</link><description>Diffusion models have emerged as a formidable tool for training-freeconditional generation.However, a key hurdle in inference-time guidancetechniques is the need for compute-heavy backpropagation through the diffusionnetwork for estimating the guidance direction. Moreover, these techniques oftenrequire handcrafted parameter tuning on a case-by-case basis. Although somerecent works have introduced minimal compute methods for linear inverseproblems, a generic lightweight guidance solution to both linear and non-linearguidance problems is still missing. To this end, we propose Dreamguider, amethod that enables inference-time guidance without compute-heavybackpropagation through the diffusion network. The key idea is to regulate thegradient flow through a time-varying factor. Moreover, we propose an empiricalguidance scale that works for a wide variety of tasks, hence removing the needfor handcrafted parameter tuning. We further introduce an effective lightweightaugmentation strategy that significantly boosts the performance duringinference-time guidance. We present experiments using Dreamguider on multipletasks across multiple datasets and models to show the effectiveness of theproposed modules. To facilitate further research, we will make the code publicafter the review process.</description><author>Nithin Gopalakrishnan Nair, Vishal M Patel</author><pubDate>Tue, 04 Jun 2024 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02549v1</guid></item><item><title>Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation</title><link>http://arxiv.org/abs/2406.02548v1</link><description>Recent works on open-vocabulary 3D instance segmentation show strong promise,but at the cost of slow inference speed and high computation requirements. Thishigh computation cost is typically due to their heavy reliance on 3D clipfeatures, which require computationally expensive 2D foundation models likeSegment Anything (SAM) and CLIP for multi-view aggregation into 3D. As aconsequence, this hampers their applicability in many real-world applicationsthat require both fast and accurate predictions. To this end, we propose a fastyet accurate open-vocabulary 3D instance segmentation approach, named Open-YOLO3D, that effectively leverages only 2D object detection from multi-view RGBimages for open-vocabulary 3D instance segmentation. We address this task bygenerating class-agnostic 3D masks for objects in the scene and associatingthem with text prompts. We observe that the projection of class-agnostic 3Dpoint cloud instances already holds instance information; thus, using SAM mightonly result in redundancy that unnecessarily increases the inference time. Weempirically find that a better performance of matching text prompts to 3D maskscan be achieved in a faster fashion with a 2D object detector. We validate ourOpen-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios:(i) with ground truth masks, where labels are required for given objectproposals, and (ii) with class-agnostic 3D proposals generated from a 3Dproposal network. Our Open-YOLO 3D achieves state-of-the-art performance onboth datasets while obtaining up to $\sim$16$\times$ speedup compared to thebest existing method in literature. On ScanNet200 val. set, our Open-YOLO 3Dachieves mean average precision (mAP) of 24.7\% while operating at 22 secondsper scene. Code and model are available at github.com/aminebdj/OpenYOLO3D.</description><author>Mohamed El Amine Boudjoghra, Angela Dai, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Shahbaz Khan</author><pubDate>Tue, 04 Jun 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02548v1</guid></item><item><title>Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning</title><link>http://arxiv.org/abs/2406.02547v1</link><description>Training models with longer in-context lengths is a significant challenge formultimodal model due to substantial GPU memory and computational costs. Thisexploratory study does not present state-of-the-art models; rather, itintroduces an innovative method designed to increase in-context text length inmulti-modality large language models (MLLMs) efficiently. We present VisualizedIn-Context Text Processing (VisInContext), which processes long in-context textusing visual tokens. This technique significantly reduces GPU memory usage andfloating point operations (FLOPs) for both training and inferenceing stage. Forinstance, our method expands the pre-training in-context text length from 256to 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model.Experimental results demonstrate that model trained with VisInContext deliverssuperior performance on common downstream benchmarks for in-context few-shotevaluation. Additionally, VisInContext is complementary to existing methods forincreasing in-context text length and enhances document understandingcapabilities, showing great potential in document QA tasks and sequentialdocument retrieval.</description><author>Alex Jinpeng Wang, Linjie Li, Yiqi Lin, Min Li, Lijuan Wang, Mike Zheng Shou</author><pubDate>Tue, 04 Jun 2024 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02547v1</guid></item><item><title>Robust and highly scalable estimation of directional couplings from time-shifted signals</title><link>http://arxiv.org/abs/2406.02545v1</link><description>The estimation of directed couplings between the nodes of a network fromindirect measurements is a central methodological challenge in scientificfields such as neuroscience, systems biology and economics. Unfortunately, theproblem is generally ill-posed due to the possible presence of unknown delaysin the measurements. In this paper, we offer a solution of this problem byusing a variational Bayes framework, where the uncertainty over the delays ismarginalized in order to obtain conservative coupling estimates. To overcomethe well-known overconfidence of classical variational methods, we use ahybrid-VI scheme where the (possibly flat or multimodal) posterior over themeasurement parameters is estimated using a forward KL loss while the (nearlyconvex) conditional posterior over the couplings is estimated using the highlyscalable gradient-based VI. In our ground-truth experiments, we show that thenetwork provides reliable and conservative estimates of the couplings, greatlyoutperforming similar methods such as regression DCM.</description><author>Luca Ambrogioni, Louis Rouillard, Demian Wassermann</author><pubDate>Tue, 04 Jun 2024 18:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02545v1</guid></item><item><title>To Believe or Not to Believe Your LLM</title><link>http://arxiv.org/abs/2406.02543v1</link><description>We explore uncertainty quantification in large language models (LLMs), withthe goal to identify when uncertainty in responses given a query is large. Wesimultaneously consider both epistemic and aleatoric uncertainties, where theformer comes from the lack of knowledge about the ground truth (such as aboutfacts or the language), and the latter comes from irreducible randomness (suchas multiple possible answers). In particular, we derive aninformation-theoretic metric that allows to reliably detect when only epistemicuncertainty is large, in which case the output of the model is unreliable. Thiscondition can be computed based solely on the output of the model obtainedsimply by some special iterative prompting based on the previous responses.Such quantification, for instance, allows to detect hallucinations (cases whenepistemic uncertainty is high) in both single- and multi-answer responses. Thisis in contrast to many standard uncertainty quantification strategies (such asthresholding the log-likelihood of a response) where hallucinations in themulti-answer case cannot be detected. We conduct a series of experiments whichdemonstrate the advantage of our formulation. Further, our investigations shedsome light on how the probabilities assigned to a given output by an LLM can beamplified by iterative prompting, which might be of independent interest.</description><author>Yasin Abbasi Yadkori, Ilja Kuzborskij, András György, Csaba Szepesvári</author><pubDate>Tue, 04 Jun 2024 18:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02543v1</guid></item><item><title>Loki: Low-Rank Keys for Efficient Sparse Attention</title><link>http://arxiv.org/abs/2406.02542v1</link><description>Inference on large language models can be expensive in terms of the computeand memory costs involved, especially when long sequence lengths are used. Inparticular, the self-attention mechanism used in such models contributessignificantly to these costs, which has resulted in several recent works thatpropose sparse attention approximations for inference. In this work, we proposeto approximate the self-attention computation by focusing on the dimensionalityof key vectors computed in the attention block. Our analysis reveals that thekey vectors lie in a significantly lower-dimensional space, consistently acrossseveral datasets and models. Exploiting this observation, we propose Loki, anovel sparse attention method that ranks and selects tokens in the KV-cachebased on attention scores computed in low-dimensional space. Our evaluationsshow that Loki is able to maintain the efficacy of the models better than otherpopular approximation methods, while speeding up the attention computation dueto reduced data movement (load/store) and compute costs.</description><author>Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, Abhinav Bhatele</author><pubDate>Tue, 04 Jun 2024 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02542v1</guid></item><item><title>Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2406.02541v1</link><description>Recent advancements in zero-shot video diffusion models have shown promisefor text-driven video editing, but challenges remain in achieving high temporalconsistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting(3DGS)-based video refiner designed to enhance temporal consistency inzero-shot video editors. Our approach utilizes a two-stage 3D Gaussianoptimizing process tailored for editing dynamic monocular videos. In the firststage, Video-3DGS employs an improved version of COLMAP, referred to asMC-COLMAP, which processes original videos using a Masked and Clipped approach.For each video clip, MC-COLMAP generates the point clouds for dynamicforeground objects and complex backgrounds. These point clouds are utilized toinitialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to representforeground and background views. Both foreground and background views are thenmerged with a 2D learnable parameter map to reconstruct full views. In thesecond stage, we leverage the reconstruction ability developed in the firststage to impose the temporal constraints on the video diffusion model. Todemonstrate the efficacy of Video-3DGS on both stages, we conduct extensiveexperiments across two related tasks: Video Reconstruction and Video Editing.Video-3DGS trained with 3k iterations significantly improves videoreconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency(x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methodson DAVIS dataset, respectively. Moreover, it enhances video editing by ensuringtemporal consistency across 58 dynamic monocular videos.</description><author>Inkyu Shin, Qihang Yu, Xiaohui Shen, In So Kweon, Kuk-Jin Yoon, Liang-Chieh Chen</author><pubDate>Tue, 04 Jun 2024 18:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02541v1</guid></item><item><title>ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation</title><link>http://arxiv.org/abs/2406.02540v1</link><description>Diffusion transformers (DiTs) have exhibited remarkable performance in visualgeneration tasks, such as generating realistic images or videos based ontextual instructions. However, larger model sizes and multi-frame processingfor video generation lead to increased computational and memory costs, posingchallenges for practical deployment on edge devices. Post-Training Quantization(PTQ) is an effective method for reducing memory costs and computationalcomplexity. When quantizing diffusion transformers, we find that applyingexisting diffusion quantization methods designed for U-Net faces challenges inpreserving quality. After analyzing the major challenges for quantizingdiffusion transformers, we design an improved quantization scheme: "ViDiT-Q":Video and Image Diffusion Transformer Quantization) to address these issues.Furthermore, we identify highly sensitive layers and timesteps hinderquantization for lower bit-widths. To tackle this, we improve ViDiT-Q with anovel metric-decoupled mixed-precision quantization method (ViDiT-Q-MP). Wevalidate the effectiveness of ViDiT-Q across a variety of text-to-image andvideo models. While baseline quantization methods fail at W8A8 and produceunreadable content at W4A8, ViDiT-Q achieves lossless W8A8 quantization.ViDiTQ-MP achieves W4A8 with negligible visual quality degradation, resultingin a 2.5x memory optimization and a 1.5x latency speedup.</description><author>Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, Yu Wang</author><pubDate>Tue, 04 Jun 2024 18:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02540v1</guid></item><item><title>Parrot: Multilingual Visual Instruction Tuning</title><link>http://arxiv.org/abs/2406.02539v1</link><description>The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4Vhas marked a significant step towards artificial general intelligence. Existingmethods mainly focus on aligning vision encoders with LLMs through supervisedfine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs'inherent ability to react to multiple languages progressively deteriorate asthe training process evolves. We empirically find that the imbalanced SFTdatasets, primarily composed of English-centric image-text pairs, lead tosignificantly reduced performance in non-English languages. This is due to thefailure of aligning the vision encoder and LLM with multilingual tokens duringthe SFT process. In this paper, we introduce Parrot, a novel method thatutilizes textual guidance to drive visual token alignment at the languagelevel. Parrot makes the visual tokens condition on diverse language inputs anduses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens.Specifically, to enhance non-English visual tokens alignment, we compute thecross-attention using the initial visual features and textual embeddings, theresult of which is then fed into the MoE router to select the most relevantexperts. The selected experts subsequently convert the initial visual tokensinto language-specific visual tokens. Moreover, considering the current lack ofbenchmarks for evaluating multilingual capabilities within the field, wecollect and make available a Massive Multilingual Multimodal Benchmark whichincludes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Ourmethod not only demonstrates state-of-the-art performance on multilingualMMBench and MMMB, but also excels across a broad range of multimodal tasks.Both the source code and the training dataset of Parrot will be made publiclyavailable.</description><author>Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye</author><pubDate>Tue, 04 Jun 2024 18:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02539v1</guid></item><item><title>TopViewRS: Vision-Language Models as Top-View Spatial Reasoners</title><link>http://arxiv.org/abs/2406.02537v1</link><description>Top-view perspective denotes a typical way in which humans read and reasonover different types of maps, and it is vital for localization and navigationof humans as well as of `non-human' agents, such as the ones backed by largeVision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities ofmodern VLMs remain unattested and underexplored. In this work, we thus studytheir capability to understand and reason over spatial relations from the topview. The focus on top view also enables controlled evaluations at differentgranularity of spatial reasoning; we clearly disentangle different abilities(e.g., recognizing particular objects versus understanding their relativepositions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset,consisting of 11,384 multiple-choice questions with either realistic orsemantic top-view map as visual input. We then use it to study and evaluateVLMs across 4 perception and reasoning tasks with different levels ofcomplexity. Evaluation of 10 representative open- and closed-source VLMsreveals the gap of more than 50% compared to average human performance, and itis even lower than the random baseline in some cases. Although additionalexperiments show that Chain-of-Thought reasoning can boost model capabilitiesby 5.82% on average, the overall performance of VLMs remains limited. Ourfindings underscore the critical need for enhanced model capability in top-viewspatial reasoning and set a foundation for further research towards human-levelproficiency of VLMs in real-world multimodal tasks.</description><author>Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, Ivan Vulić</author><pubDate>Tue, 04 Jun 2024 18:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02537v1</guid></item><item><title>Mitigate Position Bias in Large Language Models via Scaling a Single Dimension</title><link>http://arxiv.org/abs/2406.02536v1</link><description>Large Language Models (LLMs) are increasingly applied in various real-worldscenarios due to their excellent generalization capabilities and robustgenerative abilities. However, they exhibit position bias, also known as "lostin the middle", a phenomenon that is especially pronounced in long-contextscenarios, which indicates the placement of the key information in differentpositions of a prompt can significantly affect accuracy. This paper firstexplores the micro-level manifestations of position bias, concluding thatattention weights are a micro-level expression of position bias. It furtheridentifies that, in addition to position embeddings, causal attention mask alsocontributes to position bias by creating position-specific hidden states. Basedon these insights, we propose a method to mitigate position bias by scalingthis positional hidden states. Experiments on the NaturalQuestionsMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, usingvarious models including RoPE models, context windowextended models, and Alibimodels, demonstrate the effectiveness and generalizability of our approach. Ourmethod can improve performance by up to 15.2% by modifying just one dimensionof hidden states. Our code is available at https://aka.ms/PositionalHidden.</description><author>Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu</author><pubDate>Tue, 04 Jun 2024 18:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02536v1</guid></item><item><title>Enhancing 2D Representation Learning with a 3D Prior</title><link>http://arxiv.org/abs/2406.02535v1</link><description>Learning robust and effective representations of visual data is a fundamentaltask in computer vision. Traditionally, this is achieved by training modelswith labeled data which can be expensive to obtain. Self-supervised learningattempts to circumvent the requirement for labeled data by learningrepresentations from raw unlabeled visual data alone. However, unlike humanswho obtain rich 3D information from their binocular vision and through motion,the majority of current self-supervised methods are tasked with learning frommonocular 2D image collections. This is noteworthy as it has been demonstratedthat shape-centric visual processing is more robust compared to texture-biasedautomated methods. Inspired by this, we propose a new approach forstrengthening existing self-supervised methods by explicitly enforcing a strong3D structural prior directly into the model during training. Throughexperiments, across a range of datasets, we demonstrate that our 3D awarerepresentations are more robust compared to conventional self-supervisedbaselines.</description><author>Mehmet Aygün, Prithviraj Dhar, Zhicheng Yan, Oisin Mac Aodha, Rakesh Ranjan</author><pubDate>Tue, 04 Jun 2024 18:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02535v1</guid></item><item><title>Dsfer-Net: A Deep Supervision and Feature Retrieval Network for Bitemporal Change Detection Using Modern Hopfield Networks</title><link>http://arxiv.org/abs/2304.01101v2</link><description>Change detection, an essential application for high-resolution remote sensingimages, aims to monitor and analyze changes in the land surface over time. Dueto the rapid increase in the quantity of high-resolution remote sensing dataand the complexity of texture features, several quantitative deeplearning-based methods have been proposed. These methods outperform traditionalchange detection methods by extracting deep features and combiningspatial-temporal information. However, reasonable explanations for how deepfeatures improve detection performance are still lacking. In ourinvestigations, we found that modern Hopfield network layers significantlyenhance semantic understanding. In this paper, we propose a Deep Supervisionand FEature Retrieval network (Dsfer-Net) for bitemporal change detection.Specifically, the highly representative deep features of bitemporal images arejointly extracted through a fully convolutional Siamese network. Based on thesequential geographical information of the bitemporal images, we designed afeature retrieval module to extract difference features and leveragediscriminative information in a deeply supervised manner. Additionally, weobserved that the deeply supervised feature retrieval module providesexplainable evidence of the semantic understanding of the proposed network inits deep layers. Finally, our end-to-end network establishes a novel frameworkby aggregating retrieved features and feature pairs from different layers.Experiments conducted on three public datasets (LEVIR-CD, WHU-CD, and CDD)confirm the superiority of the proposed Dsfer-Net over other state-of-the-artmethods.</description><author>Shizhen Chang, Michael Kopp, Pedram Ghamisi, Bo Du</author><pubDate>Tue, 04 Jun 2024 18:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01101v2</guid></item><item><title>Enhancing predictive imaging biomarker discovery through treatment effect analysis</title><link>http://arxiv.org/abs/2406.02534v1</link><description>Identifying predictive biomarkers, which forecast individual treatmenteffectiveness, is crucial for personalized medicine and informs decision-makingacross diverse disciplines. These biomarkers are extracted from pre-treatmentdata, often within randomized controlled trials, and have to be distinguishedfrom prognostic biomarkers, which are independent of treatment assignment. Ourstudy focuses on the discovery of predictive imaging biomarkers, aiming toleverage pre-treatment images to unveil new causal relationships. Previousapproaches relied on labor-intensive handcrafted or manually derived features,which may introduce biases. In response, we present a new task of discoveringpredictive imaging biomarkers directly from the pre-treatment images to learnrelevant image features. We propose an evaluation protocol for this task toassess a model's ability to identify predictive imaging biomarkers anddifferentiate them from prognostic ones. It employs statistical testing and acomprehensive analysis of image feature attribution. We explore the suitabilityof deep learning models originally designed for estimating the conditionalaverage treatment effect (CATE) for this task, which previously have beenprimarily assessed for the precision of CATE estimation, overlooking theevaluation of imaging biomarker discovery. Our proof-of-concept analysisdemonstrates promising results in discovering and validating predictive imagingbiomarkers from synthetic outcomes and real-world image datasets.</description><author>Shuhan Xiao, Lukas Klein, Jens Petersen, Philipp Vollmuth, Paul F. Jaeger, Klaus H. Maier-Hein</author><pubDate>Tue, 04 Jun 2024 18:54:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02534v1</guid></item><item><title>SatSplatYOLO: 3D Gaussian Splatting-based Virtual Object Detection Ensembles for Satellite Feature Recognition</title><link>http://arxiv.org/abs/2406.02533v1</link><description>On-orbit servicing (OOS), inspection of spacecraft, and active debris removal(ADR). Such missions require precise rendezvous and proximity operations in thevicinity of non-cooperative, possibly unknown, resident space objects. Safetyconcerns with manned missions and lag times with ground-based controlnecessitate complete autonomy. In this article, we present an approach formapping geometries and high-confidence detection of components of unknown,non-cooperative satellites on orbit. We implement accelerated 3D Gaussiansplatting to learn a 3D representation of the satellite, render virtual viewsof the target, and ensemble the YOLOv5 object detector over the virtual views,resulting in reliable, accurate, and precise satellite component detections.The full pipeline capable of running on-board and stand to enable downstreammachine intelligence tasks necessary for autonomous guidance, navigation, andcontrol tasks.</description><author>Van Minh Nguyen, Emma Sandidge, Trupti Mahendrakar, Ryan T. White</author><pubDate>Tue, 04 Jun 2024 18:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02533v1</guid></item><item><title>SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices</title><link>http://arxiv.org/abs/2406.02532v1</link><description>As large language models gain widespread adoption, running them efficientlybecomes crucial. Recent works on LLM inference use speculative decoding toachieve extreme speedups. However, most of these works implicitly design theiralgorithms for high-end datacenter hardware. In this work, we ask the oppositequestion: how fast can we run LLMs on consumer machines? Consumer GPUs can nolonger fit the largest available models (50B+ parameters) and must offload themto RAM or SSD. When running with offloaded parameters, the inference engine canprocess batches of hundreds or thousands of tokens at the same time as just onetoken, making it a natural fit for speculative decoding. We propose SpecExec(Speculative Execution), a simple parallel decoding method that can generate upto 20 tokens per target model iteration for popular LLM families. It utilizesthe high spikiness of the token probabilities distribution in modern LLMs and ahigh degree of alignment between model output probabilities. SpecExec takes themost probable tokens continuation from the draft model to build a "cache" treefor the target model, which then gets validated in a single pass. UsingSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs withRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokensper second with 16-bit weights.</description><author>Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin</author><pubDate>Tue, 04 Jun 2024 18:53:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02532v1</guid></item><item><title>ReLUs Are Sufficient for Learning Implicit Neural Representations</title><link>http://arxiv.org/abs/2406.02529v1</link><description>Motivated by the growing theoretical understanding of neural networks thatemploy the Rectified Linear Unit (ReLU) as their activation function, werevisit the use of ReLU activation functions for learning implicit neuralrepresentations (INRs). Inspired by second order B-spline wavelets, weincorporate a set of simple constraints to the ReLU neurons in each layer of adeep neural network (DNN) to remedy the spectral bias. This in turn enables itsuse for various INR tasks. Empirically, we demonstrate that, contrary topopular belief, one can learn state-of-the-art INRs based on a DNN composed ofonly ReLU neurons. Next, by leveraging recent theoretical works whichcharacterize the kinds of functions ReLU neural networks learn, we provide away to quantify the regularity of the learned function. This offers aprincipled approach to selecting the hyperparameters in INR architectures. Wesubstantiate our claims through experiments in signal representation, superresolution, and computed tomography, demonstrating the versatility andeffectiveness of our method. The code for all experiments can be found athttps://github.com/joeshenouda/relu-inrs.</description><author>Joseph Shenouda, Yamin Zhou, Robert D. Nowak</author><pubDate>Tue, 04 Jun 2024 18:51:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02529v1</guid></item><item><title>Scalable MatMul-free Language Modeling</title><link>http://arxiv.org/abs/2406.02528v1</link><description>Matrix multiplication (MatMul) typically dominates the overall computationalcost of large language models (LLMs). This cost only grows as LLMs scale tolarger embedding dimensions and context lengths. In this work, we show thatMatMul operations can be completely eliminated from LLMs while maintainingstrong performance at billion-parameter scales. Our experiments show that ourproposed MatMul-free models achieve performance on-par with state-of-the-artTransformers that require far more memory during inference at a scale up to atleast 2.7B parameters. We investigate the scaling laws and find that theperformance gap between our MatMul-free models and full precision Transformersnarrows as the model size increases. We also provide a GPU-efficientimplementation of this model which reduces memory usage by up to 61% over anunoptimized baseline during training. By utilizing an optimized kernel duringinference, our model's memory consumption can be reduced by more than 10xcompared to unoptimized models. To properly quantify the efficiency of ourarchitecture, we build a custom hardware solution on an FPGA which exploitslightweight operations beyond what GPUs are capable of. We processedbillion-parameter scale models at 13W beyond human readable throughput, movingLLMs closer to brain-like efficiency. This work not only shows how far LLMs canbe stripped back while still performing effectively, but also points at thetypes of operations future accelerators should be optimized for in processingthe next generation of lightweight LLMs. Our code implementation is availableat \url{https://github.com/ridgerchu/matmulfreellm}.</description><author>Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian</author><pubDate>Tue, 04 Jun 2024 18:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02528v1</guid></item><item><title>Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective</title><link>http://arxiv.org/abs/2402.03496v4</link><description>Adaptive gradient optimizers like Adam(W) are the default training algorithmsfor many deep learning architectures, such as transformers. Their diagonalpreconditioner is based on the gradient outer product which is incorporatedinto the parameter update via a square root. While these methods are oftenmotivated as approximate second-order methods, the square root represents afundamental difference. In this work, we investigate how the behavior ofadaptive methods changes when we remove the root, i.e. strengthen theirsecond-order motivation. Surprisingly, we find that such square-root-freeadaptive methods close the generalization gap to SGD on convolutionalarchitectures, while maintaining their root-based counterpart's performance ontransformers. The second-order perspective also has practical benefits for thedevelopment of non-diagonal adaptive methods through the concept ofpreconditioner invariance. In contrast to root-based methods like Shampoo, theroot-free counterparts do not require numerically unstable matrix rootdecompositions and inversions, thus work well in half precision. Our findingsprovide new insights into the development of adaptive methods and raiseimportant questions regarding the currently overlooked role of adaptivity fortheir success.</description><author>Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E. Turner, Alireza Makhzani</author><pubDate>Tue, 04 Jun 2024 18:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03496v4</guid></item><item><title>CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks</title><link>http://arxiv.org/abs/2406.02524v1</link><description>Large Language Models (LLMs) are revolutionizing various domains, yetverifying their answers remains a significant challenge, especially forintricate open-ended tasks such as consolidation, summarization, and extractionof knowledge. In this work, we propose CheckEmbed: an accurate, scalable, andsimple LLM verification approach. CheckEmbed is driven by a straightforward yetpowerful idea: in order to compare LLM solutions to one another or to theground-truth, compare their corresponding answer-level embeddings obtained witha model such as GPT Text Embedding Large. This reduces a complex textual answerto a single embedding, facilitating straightforward, fast, and meaningfulverification. We develop a comprehensive verification pipeline implementing theCheckEmbed methodology. The CheckEmbed pipeline also comes with metrics forassessing the truthfulness of the LLM answers, such as embedding heatmaps andtheir summaries. We show how to use these metrics for deploying practicalengines that decide whether an LLM answer is satisfactory or not. We apply thepipeline to real-world document analysis tasks, including term extraction anddocument summarization, showcasing significant improvements in accuracy,cost-effectiveness, and runtime performance compared to existing token-,sentence-, and fact-level schemes such as BERTScore or SelfCheckGPT.</description><author>Maciej Besta, Lorenzo Paleari, Ales Kubicek, Piotr Nyczyk, Robert Gerstenberger, Patrick Iff, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler</author><pubDate>Tue, 04 Jun 2024 18:42:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02524v1</guid></item><item><title>RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots</title><link>http://arxiv.org/abs/2406.02523v1</link><description>Recent advancements in Artificial Intelligence (AI) have largely beenpropelled by scaling. In Robotics, scaling is hindered by the lack of access tomassive robot datasets. We advocate using realistic physical simulation as ameans to scale environments, tasks, and datasets for robot learning methods. Wepresent RoboCasa, a large-scale simulation framework for training generalistrobots in everyday environments. RoboCasa features realistic and diverse scenesfocusing on kitchen environments. We provide thousands of 3D assets across over150 object categories and dozens of interactable furniture and appliances. Weenrich the realism and diversity of our simulation with generative AI tools,such as object assets from text-to-3D models and environment textures fromtext-to-image models. We design a set of 100 tasks for systematic evaluation,including composite tasks generated by the guidance of large language models.To facilitate learning, we provide high-quality human demonstrations andintegrate automated trajectory generation methods to substantially enlarge ourdatasets with minimal human burden. Our experiments show a clear scaling trendin using synthetically generated robot data for large-scale imitation learningand show great promise in harnessing simulation data in real-world tasks.Videos and open-source code are available at https://robocasa.ai/</description><author>Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, Yuke Zhu</author><pubDate>Tue, 04 Jun 2024 18:41:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02523v1</guid></item><item><title>DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering</title><link>http://arxiv.org/abs/2406.02518v1</link><description>Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray imagesgenerated from 3D CT volumes, widely used in preoperative settings but limitedin intraoperative applications due to computational bottlenecks, especially foraccurate but heavy physics-based Monte Carlo methods. While analytical DRRrenderers offer greater efficiency, they overlook anisotropic X-ray imageformation phenomena, such as Compton scattering. We present a novel approachthat marries realistic physics-inspired X-ray simulation with efficient,differentiable DRR generation using 3D Gaussian splatting (3DGS). Ourdirection-disentangled 3DGS (DDGS) method separates the radiosity contributioninto isotropic and direction-dependent components, approximating complexanisotropic interactions without intricate runtime simulations. Additionally,we adapt the 3DGS initialization to account for tomography data properties,enhancing accuracy and efficiency. Our method outperforms state-of-the-arttechniques in image accuracy. Furthermore, our DDGS shows promise forintraoperative applications and inverse problems such as pose registration,delivering superior registration accuracy and runtime performance compared toanalytical DRR methods.</description><author>Zhongpai Gao, Benjamin Planche, Meng Zheng, Xiao Chen, Terrence Chen, Ziyan Wu</author><pubDate>Tue, 04 Jun 2024 18:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02518v1</guid></item><item><title>Deterministic Reversible Data Augmentation for Neural Machine Translation</title><link>http://arxiv.org/abs/2406.02517v1</link><description>Data augmentation is an effective way to diversify corpora in machinetranslation, but previous methods may introduce semantic inconsistency betweenoriginal and augmented data because of irreversible operations and randomsubword sampling procedures. To generate both symbolically diverse andsemantically consistent augmentation data, we propose Deterministic ReversibleData Augmentation (DRDA), a simple but effective data augmentation method forneural machine translation. DRDA adopts deterministic segmentations andreversible operations to generate multi-granularity subword representations andpulls them closer together with multi-view techniques. With no extra corpora ormodel changes required, DRDA outperforms strong baselines on severaltranslation tasks with a clear margin (up to 4.3 BLEU gain over Transformer)and exhibits good robustness in noisy, low-resource, and cross-domain datasets.</description><author>Jiashu Yao, Heyan Huang, Zeming Liu, Yuhang Guo</author><pubDate>Tue, 04 Jun 2024 18:39:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02517v1</guid></item><item><title>Uncertainty of Joint Neural Contextual Bandit</title><link>http://arxiv.org/abs/2406.02515v1</link><description>Contextual bandit learning is increasingly favored in modern large-scalerecommendation systems. To better utlize the contextual information andavailable user or item features, the integration of neural networks have beenintroduced to enhance contextual bandit learning and has triggered significantinterest from both academia and industry. However, a major challenge ariseswhen implementing a disjoint neural contextual bandit solution in large-scalerecommendation systems, where each item or user may correspond to a separatebandit arm. The huge number of items to recommend poses a significant hurdlefor real world production deployment. This paper focuses on a joint neuralcontextual bandit solution which serves all recommending items in one singlemodel. The output consists of a predicted reward $\mu$, an uncertainty $\sigma$and a hyper-parameter $\alpha$ which balances exploitation and exploration,e.g., $\mu + \alpha \sigma$. The tuning of the parameter $\alpha$ is typically heuristic and complex inpractice due to its stochastic nature. To address this challenge, we provideboth theoretical analysis and experimental findings regarding the uncertainty$\sigma$ of the joint neural contextual bandit model. Our analysis reveals that$\alpha$ demonstrates an approximate square root relationship with the size ofthe last hidden layer $F$ and inverse square root relationship with the amountof training data $N$, i.e., $\sigma \propto \sqrt{\frac{F}{N}}$. Theexperiments, conducted with real industrial data, align with the theoreticalanalysis, help understanding model behaviors and assist the hyper-parametertuning during both offline training and online deployment.</description><author>Hongbo Guo, Zheqing Zhu</author><pubDate>Tue, 04 Jun 2024 18:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02515v1</guid></item><item><title>Large Language Models Spot Phishing Emails with Surprising Accuracy: A Comparative Analysis of Performance</title><link>http://arxiv.org/abs/2404.15485v2</link><description>Phishing, a prevalent cybercrime tactic for decades, remains a significantthreat in today's digital world. By leveraging clever social engineeringelements and modern technology, cybercrime targets many individuals,businesses, and organizations to exploit trust and security. Thesecyber-attackers are often disguised in many trustworthy forms to appear aslegitimate sources. By cleverly using psychological elements like urgency,fear, social proof, and other manipulative strategies, phishers can lureindividuals into revealing sensitive and personalized information. Building onthis pervasive issue within modern technology, this paper aims to analyze theeffectiveness of 15 Large Language Models (LLMs) in detecting phishingattempts, specifically focusing on a randomized set of "419 Scam" emails. Theobjective is to determine which LLMs can accurately detect phishing emails byanalyzing a text file containing email metadata based on predefined criteria.The experiment concluded that the following models, ChatGPT 3.5,GPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detectingphishing emails.</description><author>Het Patel, Umair Rehman, Farkhund Iqbal</author><pubDate>Tue, 04 Jun 2024 18:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15485v2</guid></item><item><title>V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation</title><link>http://arxiv.org/abs/2406.02511v1</link><description>In the field of portrait video generation, the use of single images togenerate portrait videos has become increasingly prevalent. A common approachinvolves leveraging generative models to enhance adapters for controlledgeneration. However, control signals (e.g., text, audio, reference image, pose,depth map, etc.) can vary in strength. Among these, weaker conditions oftenstruggle to be effective due to interference from stronger conditions, posing achallenge in balancing these conditions. In our work on portrait videogeneration, we identified audio signals as particularly weak, oftenovershadowed by stronger signals such as facial pose and reference image.However, direct training with weak signals often leads to difficulties inconvergence. To address this, we propose V-Express, a simple method thatbalances different control signals through the progressive training and theconditional dropout operation. Our method gradually enables effective controlby weak conditions, thereby achieving generation capabilities thatsimultaneously take into account the facial pose, reference image, and audio.The experimental results demonstrate that our method can effectively generateportrait videos controlled by audio. Furthermore, a potential solution isprovided for the simultaneous and effective use of conditions of varyingstrengths.</description><author>Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, Wei Yang</author><pubDate>Tue, 04 Jun 2024 18:32:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02511v1</guid></item><item><title>Fairness-Optimized Synthetic EHR Generation for Arbitrary Downstream Predictive Tasks</title><link>http://arxiv.org/abs/2406.02510v1</link><description>Among various aspects of ensuring the responsible design of AI tools forhealthcare applications, addressing fairness concerns has been a key focusarea. Specifically, given the wide spread of electronic health record (EHR)data and their huge potential to inform a wide range of clinical decisionsupport tasks, improving fairness in this category of health AI tools is of keyimportance. While such a broad problem (that is, mitigating fairness inEHR-based AI models) has been tackled using various methods, task- andmodel-agnostic methods are noticeably rare. In this study, we aimed to targetthis gap by presenting a new pipeline that generates synthetic EHR data, whichis not only consistent with (faithful to) the real EHR data but also can reducethe fairness concerns (defined by the end-user) in the downstream tasks, whencombined with the real data. We demonstrate the effectiveness of our proposedpipeline across various downstream tasks and two different EHR datasets. Ourproposed pipeline can add a widely applicable and complementary tool to theexisting toolbox of methods to address fairness in health AI applications suchas those modifying the design of a downstream model. The codebase for ourproject is available at https://github.com/healthylaife/FairSynth</description><author>Mirza Farhan Bin Tarek, Raphael Poulain, Rahmatollah Beheshti</author><pubDate>Tue, 04 Jun 2024 18:29:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02510v1</guid></item><item><title>CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation</title><link>http://arxiv.org/abs/2406.02509v1</link><description>Recently video diffusion models have emerged as expressive generative toolsfor high-quality video content creation readily available to general users.However, these models often do not offer precise control over camera poses forvideo generation, limiting the expression of cinematic language and usercontrol. To address this issue, we introduce CamCo, which allows fine-grainedCamera pose Control for image-to-video generation. We equip a pre-trainedimage-to-video generator with accurately parameterized camera pose input usingPl\"ucker coordinates. To enhance 3D consistency in the videos produced, weintegrate an epipolar attention module in each attention block that enforcesepipolar constraints to the feature maps. Additionally, we fine-tune CamCo onreal-world videos with camera poses estimated through structure-from-motionalgorithms to better synthesize object motion. Our experiments show that CamCosignificantly improves 3D consistency and camera control capabilities comparedto previous models while effectively generating plausible object motion.Project page: https://ir1d.github.io/CamCo/</description><author>Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, Arash Vahdat</author><pubDate>Tue, 04 Jun 2024 18:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02509v1</guid></item><item><title>State-Constrained Zero-Sum Differential Games with One-Sided Information</title><link>http://arxiv.org/abs/2403.02741v2</link><description>We study zero-sum differential games with state constraints and one-sidedinformation, where the informed player (Player 1) has a categorical payoff typeunknown to the uninformed player (Player 2). The goal of Player 1 is tominimize his payoff without violating the constraints, while that of Player 2is to violate the state constraints if possible, or to maximize the payoffotherwise. One example of the game is a man-to-man matchup in football. Withoutstate constraints, Cardaliaguet (2007) showed that the value of such a gameexists and is convex to the common belief of players. Our theoreticalcontribution is an extension of this result to games with state constraints andthe derivation of the primal and dual subdynamic principles necessary forcomputing behavioral strategies. Different from existing works that areconcerned about the scalability of no-regret learning in games with discretedynamics, our study reveals the underlying structure of strategies for beliefmanipulation resulting from information asymmetry and state constraints. Thisstructure will be necessary for scalable learning on games with continuousactions and long time windows. We use a simplified football game to demonstratethe utility of this work, where we reveal player positions and belief states inwhich the attacker should (or should not) play specific random deceptive movesto take advantage of information asymmetry, and compute how the defender shouldrespond.</description><author>Mukesh Ghimire, Lei Zhang, Zhe Xu, Yi Ren</author><pubDate>Tue, 04 Jun 2024 18:26:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02741v2</guid></item><item><title>Guiding a Diffusion Model with a Bad Version of Itself</title><link>http://arxiv.org/abs/2406.02507v1</link><description>The primary axes of interest in image-generating diffusion models are imagequality, the amount of variation in the results, and how well the results alignwith a given condition, e.g., a class label or a text prompt. The popularclassifier-free guidance approach uses an unconditional model to guide aconditional model, leading to simultaneously better prompt alignment andhigher-quality images at the cost of reduced variation. These effects seeminherently entangled, and thus hard to control. We make the surprisingobservation that it is possible to obtain disentangled control over imagequality without compromising the amount of variation by guiding generationusing a smaller, less-trained version of the model itself rather than anunconditional model. This leads to significant improvements in ImageNetgeneration, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, usingpublicly available networks. Furthermore, the method is also applicable tounconditional diffusion models, drastically improving their quality.</description><author>Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine</author><pubDate>Tue, 04 Jun 2024 18:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02507v1</guid></item><item><title>VideoPoet: A Large Language Model for Zero-Shot Video Generation</title><link>http://arxiv.org/abs/2312.14125v4</link><description>We present VideoPoet, a language model capable of synthesizing high-qualityvideo, with matching audio, from a large variety of conditioning signals.VideoPoet employs a decoder-only transformer architecture that processesmultimodal inputs -- including images, videos, text, and audio. The trainingprotocol follows that of Large Language Models (LLMs), consisting of twostages: pretraining and task-specific adaptation. During pretraining, VideoPoetincorporates a mixture of multimodal generative objectives within anautoregressive Transformer framework. The pretrained LLM serves as a foundationthat can be adapted for a range of video generation tasks. We present empiricalresults demonstrating the model's state-of-the-art capabilities in zero-shotvideo generation, specifically highlighting VideoPoet's ability to generatehigh-fidelity motions. Project page: http://sites.research.google/videopoet/</description><author>Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, Lu Jiang</author><pubDate>Tue, 04 Jun 2024 18:25:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14125v4</guid></item><item><title>An Open-Source Tool for Mapping War Destruction at Scale in Ukraine using Sentinel-1 Time Series</title><link>http://arxiv.org/abs/2406.02506v1</link><description>Access to detailed war impact assessments is crucial for humanitarianorganizations to effectively assist populations most affected by armedconflicts. However, maintaining a comprehensive understanding of the situationon the ground is challenging, especially in conflicts that cover vastterritories and extend over long periods. This study presents a scalable andtransferable method for estimating war-induced damage to buildings. We firsttrain a machine learning model to output pixel-wise probability of destructionfrom Synthetic Aperture Radar (SAR) satellite image time series, leveragingexisting, manual damage assessments as ground truth and cloud-based geospatialanalysis tools for large-scale inference. We further post-process theseassessments using open building footprints to obtain a final damage estimateper building. We introduce an accessible, open-source tool that allows users toadjust the confidence interval based on their specific requirements and usecases. Our approach enables humanitarian organizations and other actors torapidly screen large geographic regions for war impacts. We provide twopublicly accessible dashboards: a Ukraine Damage Explorer to dynamically viewour pre-computed estimates, and a Rapid Damage Mapping Tool to easily run ourmethod and produce custom maps.</description><author>Olivier Dietrich, Torben Peters, Vivien Sainte Fare Garnot, Valerie Sticher, Thao Ton-That Whelan, Konrad Schindler, Jan Dirk Wegner</author><pubDate>Tue, 04 Jun 2024 18:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02506v1</guid></item><item><title>Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning</title><link>http://arxiv.org/abs/2402.04833v2</link><description>There is a consensus that instruction fine-tuning of LLMs requireshigh-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR2024) are state-of-the-art methods for selecting such high-quality examples,either via manual curation or using GPT-3.5-Turbo as a quality scorer. We showthat the extremely simple baseline of selecting the 1,000 instructions withlongest responses -- that intuitively contain more learnable information andare harder to overfit -- from standard datasets can consistently outperformthese sophisticated methods according to GPT-4 and PaLM-2 as judges, whileremaining competitive on the Open LLM benchmarks that test factual knowledge.We demonstrate this for several LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B-v0.1)and datasets (Alpaca-52k, Evol-Instruct-70k). In addition, a lightweightrefinement of such long instructions can further improve the abilities of thefine-tuned LLMs, and allows us to obtain competitive results on MT-Bench andthe 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0, while trainingon only 1,000 examples and no extra preference data. We also conduct a thoroughanalysis of our models to ensure that their enhanced performance is not simplydue to GPT-4's preference for longer responses. Overall, our findings suggestthat fine-tuning on the longest responses should be the default baseline forany work on instruction fine-tuning. We provide our code athttps://github.com/tml-epfl/long-is-more-for-alignment.</description><author>Hao Zhao, Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion</author><pubDate>Tue, 04 Jun 2024 18:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04833v2</guid></item><item><title>Demystifying the Compression of Mixture-of-Experts Through a Unified Framework</title><link>http://arxiv.org/abs/2406.02500v1</link><description>Scaling large language models has revolutionized the performance acrossdiverse domains, yet the continual growth in model size poses significantchallenges for real-world deployment. The Mixture of Experts (MoE) approachaddresses this by dynamically selecting and activating only a subset ofexperts, significantly reducing computational costs while maintaining highperformance. However, MoE introduces potential redundancy (e.g., parameters)and extra costs (e.g., communication overhead). Despite numerous compressiontechniques developed for mitigating the redundancy in dense models, thecompression of MoE remains under-explored. We first bridge this gap with acutting-edge unified framework that not only seamlessly integrates mainstreamcompression methods but also helps systematically understand MoE compression.This framework approaches compression from two perspectives: Expert Slimmingwhich compresses individual experts and Expert Trimming which removesstructured modules. Within this framework, we explore the optimization spaceunexplored by existing methods,and further introduce aggressive Expert Trimmingtechniques, i.e., Layer Drop and Block Drop, to eliminate redundancy at largerscales. Based on these insights,we present a comprehensive recipe to guidepractitioners in compressing MoE effectively. Extensive experimental resultsdemonstrate the effectiveness of the compression methods under our frameworkand the proposed recipe, achieving a 6.05x speedup and only 20.0GB memory usagewhile maintaining over 92% of performance on Mixtral-8x7B.</description><author>Shwai He, Daize Dong, Liang Ding, Ang Li</author><pubDate>Tue, 04 Jun 2024 18:18:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02500v1</guid></item><item><title>Momentum Particle Maximum Likelihood</title><link>http://arxiv.org/abs/2312.07335v3</link><description>Maximum likelihood estimation (MLE) of latent variable models is often recastas the minimization of a free energy functional over an extended space ofparameters and probability distributions. This perspective was recentlycombined with insights from optimal transport to obtain novel particle-basedalgorithms for fitting latent variable models to data. Drawing inspiration fromprior works which interpret `momentum-enriched' optimization algorithms asdiscretizations of ordinary differential equations, we propose an analogousdynamical-systems-inspired approach to minimizing the free energy functional.The result is a dynamical system that blends elements of Nesterov's AcceleratedGradient method, the underdamped Langevin diffusion, and particle methods.Under suitable assumptions, we prove that the continuous-time system minimizesthe functional. By discretizing the system, we obtain a practical algorithm forMLE in latent variable models. The algorithm outperforms existing particlemethods in numerical experiments and compares favourably with other MLEalgorithms.</description><author>Jen Ning Lim, Juan Kuntz, Samuel Power, Adam M. Johansen</author><pubDate>Tue, 04 Jun 2024 18:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07335v3</guid></item><item><title>Dropout MPC: An Ensemble Neural MPC Approach for Systems with Learned Dynamics</title><link>http://arxiv.org/abs/2406.02497v1</link><description>Neural networks are lately more and more often being used in the context ofdata-driven control, as an approximate model of the true system dynamics. ModelPredictive Control (MPC) adopts this practise leading to neural MPC strategies.This raises a question of whether the trained neural network has converged andgeneralized in a way that the learned model encapsulates an accurateapproximation of the true dynamic model of the system, thus making it areliable choice for model-based control, especially for disturbed and uncertainsystems. To tackle that, we propose Dropout MPC, a novel sampling-basedensemble neural MPC algorithm that employs the Monte-Carlo dropout technique onthe learned system model. The closed loop is based on an ensemble of predictivecontrollers, that are used simultaneously at each time-step for trajectoryoptimization. Each member of the ensemble influences the control input, basedon a weighted voting scheme, thus by employing different realizations of thelearned system dynamics, neural control becomes more reliable by design. Anadditional strength of the method is that it offers by design a way to estimatefuture uncertainty, leading to cautious control. While the method aims ingeneral at uncertain systems with complex dynamics, where models derived fromfirst principles are hard to infer, to showcase the application we utilize datagathered in the laboratory from a real mobile manipulator and employ theproposed algorithm for the navigation of the robot in simulation.</description><author>Spyridon Syntakas, Kostas Vlachos</author><pubDate>Tue, 04 Jun 2024 18:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02497v1</guid></item><item><title>Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability</title><link>http://arxiv.org/abs/2406.02496v1</link><description>Kolmogorov-Arnold Networks (KAN) is a groundbreaking model recently proposedby the MIT team, representing a revolutionary approach with the potential to bea game-changer in the field. This innovative concept has rapidly garneredworldwide interest within the AI community. Inspired by the Kolmogorov-Arnoldrepresentation theorem, KAN utilizes spline-parametrized univariate functionsin place of traditional linear weights, enabling them to dynamically learnactivation patterns and significantly enhancing interpretability. In thispaper, we explore the application of KAN to time series forecasting and proposetwo variants: T-KAN and MT-KAN. T-KAN is designed to detect concept driftwithin time series and can explain the nonlinear relationships betweenpredictions and previous time steps through symbolic regression, making ithighly interpretable in dynamically changing environments. MT-KAN, on the otherhand, improves predictive performance by effectively uncovering and leveragingthe complex relationships among variables in multivariate time series.Experiments validate the effectiveness of these approaches, demonstrating thatT-KAN and MT-KAN significantly outperform traditional methods in time seriesforecasting tasks, not only enhancing predictive accuracy but also improvingmodel interpretability. This research opens new avenues for adaptiveforecasting models, highlighting the potential of KAN as a powerful andinterpretable tool in predictive analytics.</description><author>Kunpeng Xu, Lifei Chen, Shengrui Wang</author><pubDate>Tue, 04 Jun 2024 18:14:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02496v1</guid></item><item><title>GenS: Generalizable Neural Surface Reconstruction from Multi-View Images</title><link>http://arxiv.org/abs/2406.02495v1</link><description>Combining the signed distance function (SDF) and differentiable volumerendering has emerged as a powerful paradigm for surface reconstruction frommulti-view images without 3D supervision. However, current methods are impededby requiring long-time per-scene optimizations and cannot generalize to newscenes. In this paper, we present GenS, an end-to-end generalizable neuralsurface reconstruction model. Unlike coordinate-based methods that train aseparate network for each scene, we construct a generalized multi-scale volumeto directly encode all scenes. Compared with existing solutions, ourrepresentation is more powerful, which can recover high-frequency details whilemaintaining global smoothness. Meanwhile, we introduce a multi-scalefeature-metric consistency to impose the multi-view consistency in a morediscriminative multi-scale feature space, which is robust to the failures ofthe photometric consistency. And the learnable feature can be self-enhanced tocontinuously improve the matching accuracy and mitigate aggregation ambiguity.Furthermore, we design a view contrast loss to force the model to be robust tothose regions covered by few viewpoints through distilling the geometric priorfrom dense input to sparse input. Extensive experiments on popular benchmarksshow that our model can generalize well to new scenes and outperform existingstate-of-the-art methods even those employing ground-truth depth supervision.Code is available at https://github.com/prstrive/GenS.</description><author>Rui Peng, Xiaodong Gu, Luyang Tang, Shihe Shen, Fanqi Yu, Ronggang Wang</author><pubDate>Tue, 04 Jun 2024 18:13:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02495v1</guid></item><item><title>Comparing Graph Transformers via Positional Encodings</title><link>http://arxiv.org/abs/2402.14202v2</link><description>The distinguishing power of graph transformers is closely tied to the choiceof positional encoding: features used to augment the base transformer withinformation about the graph. There are two primary types of positionalencoding: absolute positional encodings (APEs) and relative positionalencodings (RPEs). APEs assign features to each node and are given as input tothe transformer. RPEs instead assign a feature to each pair of nodes, e.g.,graph distance, and are used to augment the attention block. A priori, it isunclear which method is better for maximizing the power of the resulting graphtransformer. In this paper, we aim to understand the relationship between thesedifferent types of positional encodings. Interestingly, we show that graphtransformers using APEs and RPEs are equivalent in terms of distinguishingpower. In particular, we demonstrate how to interchange APEs and RPEs whilemaintaining their distinguishing power in terms of graph transformers. Based onour theoretical results, we provide a study on several APEs and RPEs (includingthe resistance distance and the recently introduced stable and expressivepositional encoding (SPE)) and compare their distinguishing power in terms oftransformers. We believe our work will help navigate the huge number of choicesof positional encoding and will provide guidance on the future design ofpositional encodings for graph transformers.</description><author>Mitchell Black, Zhengchao Wan, Gal Mishne, Amir Nayyeri, Yusu Wang</author><pubDate>Tue, 04 Jun 2024 18:11:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14202v2</guid></item><item><title>Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding</title><link>http://arxiv.org/abs/2401.07851v3</link><description>To mitigate the high inference latency stemming from autoregressive decodingin Large Language Models (LLMs), Speculative Decoding has emerged as a noveldecoding paradigm for LLM inference. In each decoding step, this method firstdrafts several future tokens efficiently and then verifies them in parallel.Unlike autoregressive decoding, Speculative Decoding facilitates thesimultaneous decoding of multiple tokens per step, thereby acceleratinginference. This paper presents a comprehensive overview and analysis of thispromising decoding paradigm. We begin by providing a formal definition andformulation of Speculative Decoding. Then, we organize in-depth discussions onits key facets, such as drafter selection and verification strategies.Furthermore, we present a comparative analysis of leading methods underthird-party testing environments. We aim for this work to serve as a catalystfor further research on Speculative Decoding, ultimately contributing to moreefficient LLM inference.</description><author>Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui</author><pubDate>Tue, 04 Jun 2024 18:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07851v3</guid></item><item><title>Sample Complexity of Algorithm Selection Using Neural Networks and Its Applications to Branch-and-Cut</title><link>http://arxiv.org/abs/2402.02328v3</link><description>Data-driven algorithm design is a paradigm that uses statistical and machinelearning techniques to select from a class of algorithms for a computationalproblem an algorithm that has the best expected performance with respect tosome (unknown) distribution on the instances of the problem. We build uponrecent work in this line of research by considering the setup where, instead ofselecting a single algorithm that has the best performance, we allow thepossibility of selecting an algorithm based on the instance to be solved, usingneural networks. In particular, given a representative sample of instances, welearn a neural network that maps an instance of the problem to the mostappropriate algorithm for that instance. We formalize this idea and deriverigorous sample complexity bounds for this learning problem, in the spirit ofrecent work in data-driven algorithm design. We then apply this approach to theproblem of making good decisions in the branch-and-cut framework formixed-integer optimization (e.g., which cut to add?). In other words, theneural network will take as input a mixed-integer optimization instance andoutput a decision that will result in a small branch-and-cut tree for thatinstance. Our computational results provide evidence that our particular way ofusing neural networks for cut selection can make a significant impact inreducing branch-and-cut tree sizes, compared to previous data-drivenapproaches.</description><author>Hongyu Cheng, Sammy Khalife, Barbara Fiedorowicz, Amitabh Basu</author><pubDate>Tue, 04 Jun 2024 18:05:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02328v3</guid></item><item><title>The Origin and Evolution of Information Handling</title><link>http://arxiv.org/abs/2404.04374v3</link><description>A major challenge when describing the origin of life is to explain "howinstructional information control systems emerge naturally and spontaneouslyfrom mere molecular dynamics". So far, no one has clarified how informationcontrol emerged ab initio and how primitive control mechanisms in life mighthave evolved, becoming increasingly refined. Based on recent experimentalresults showing that chemical computation does not require the presence oflife-related chemistry, we elucidate the origin and early evolution ofinformation handling by chemical automata, from information processing(computation) to information storage (memory) and information transmission(communication) and later digital messengers, covering at the same time itssyntactic, semantic and pragmatic flavors. In contrast to other theories thatassume the existence of initial complex structures, our representation startsfrom trivial self-replicators whose interaction leads to the arising of morepowerful molecular machines. By describing precisely the primordial transitionsin chemistry-based computation, our framework is capable of explaining theabove-mentioned gaps and can be translated to other models of computation,which allow us to explore biological phenomena at multiple spatial and temporalscales. Being compatible with the free energy principle, we have developed acomputational enactivist theoretical framework that could be able to describefrom the origin of life to higher-level cognition, as if it were a purelyconstructivist narrative. At the end of our manuscript, we propose some ways toextend our ideas, including experimental validation of our theory (both invitro and in silico).</description><author>Amahury Jafet López-Díaz, Hiroki Sayama, Carlos Gershenson</author><pubDate>Tue, 04 Jun 2024 18:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04374v3</guid></item><item><title>Ai-Sampler: Adversarial Learning of Markov kernels with involutive maps</title><link>http://arxiv.org/abs/2406.02490v1</link><description>Markov chain Monte Carlo methods have become popular in statistics asversatile techniques to sample from complicated probability distributions. Inthis work, we propose a method to parameterize and train transition kernels ofMarkov chains to achieve efficient sampling and good mixing. This trainingprocedure minimizes the total variation distance between the stationarydistribution of the chain and the empirical distribution of the data. Ourapproach leverages involutive Metropolis-Hastings kernels constructed fromreversible neural networks that ensure detailed balance by construction. Wefind that reversibility also implies $C_2$-equivariance of the discriminatorfunction which can be used to restrict its function space.</description><author>Evgenii Egorov, Ricardo Valperga, Efstratios Gavves</author><pubDate>Tue, 04 Jun 2024 18:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02490v1</guid></item><item><title>Language-Universal Speech Attributes Modeling for Zero-Shot Multilingual Spoken Keyword Recognition</title><link>http://arxiv.org/abs/2406.02488v1</link><description>We propose a novel language-universal approach to end-to-end automatic spokenkeyword recognition (SKR) leveraging upon (i) a self-supervised pre-trainedmodel, and (ii) a set of universal speech attributes (manner and place ofarticulation). Specifically, Wav2Vec2.0 is used to generate robust speechrepresentations, followed by a linear output layer to produce attributesequences. A non-trainable pronunciation model then maps sequences ofattributes into spoken keywords in a multilingual setting. Experiments on theMultilingual Spoken Words Corpus show comparable performances to character- andphoneme-based SKR in seen languages. The inclusion of domain adversarialtraining (DAT) improves the proposed framework, outperforming both character-and phoneme-based SKR approaches with 13.73% and 17.22% relative word errorrate (WER) reduction in seen languages, and achieves 32.14% and 19.92% WERreduction for unseen languages in zero-shot settings.</description><author>Hao Yen, Pin-Jui Ku, Sabato Marco Siniscalchi, Chin-Hui Lee</author><pubDate>Tue, 04 Jun 2024 17:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02488v1</guid></item><item><title>COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization</title><link>http://arxiv.org/abs/2403.07134v2</link><description>Post-training quantization (PTQ) has emerged as a practical approach tocompress large neural networks, making them highly efficient for deployment.However, effectively reducing these models to their low-bit counterpartswithout compromising the original accuracy remains a key challenge. In thispaper, we propose an innovative PTQ algorithm termed COMQ, which sequentiallyconducts coordinate-wise minimization of the layer-wise reconstruction errors.We consider the widely used integer quantization, where every quantized weightcan be decomposed into a shared floating-point scalar and an integer bit-code.Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes asthe variables of the reconstruction error. Every iteration improves this erroralong a single coordinate while keeping all other variables constant. COMQ iseasy to use and requires no hyper-parameter tuning. It instead involves onlydot products and rounding operations. We update these variables in a carefullydesigned greedy order, significantly enhancing the accuracy. COMQ achievesremarkable results in quantizing 4-bit Vision Transformers, with a negligibleloss of less than 1% in Top-1 accuracy. In 4-bit INT quantization ofconvolutional neural networks, COMQ maintains near-lossless accuracy with aminimal drop of merely 0.3% in Top-1 accuracy.</description><author>Aozhong Zhang, Zi Yang, Naigang Wang, Yingyong Qin, Jack Xin, Xin Li, Penghang Yin</author><pubDate>Tue, 04 Jun 2024 17:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07134v2</guid></item><item><title>A Temporal Kolmogorov-Arnold Transformer for Time Series Forecasting</title><link>http://arxiv.org/abs/2406.02486v1</link><description>Capturing complex temporal patterns and relationships within multivariatedata streams is a difficult task. We propose the Temporal Kolmogorov-ArnoldTransformer (TKAT), a novel attention-based architecture designed to addressthis task using Temporal Kolmogorov-Arnold Networks (TKANs). Inspired by theTemporal Fusion Transformer (TFT), TKAT emerges as a powerful encoder-decodermodel tailored to handle tasks in which the observed part of the features ismore important than the a priori known part. This new architecture combined thetheoretical foundation of the Kolmogorov-Arnold representation with the powerof transformers. TKAT aims to simplify the complex dependencies inherent intime series, making them more "interpretable". The use of transformerarchitecture in this framework allows us to capture long-range dependenciesthrough self-attention mechanisms.</description><author>Remi Genet, Hugo Inzirillo</author><pubDate>Tue, 04 Jun 2024 17:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02486v1</guid></item><item><title>Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation</title><link>http://arxiv.org/abs/2406.02485v1</link><description>Controllable text-to-image (T2I) diffusion models have shown impressiveperformance in generating high-quality visual content through the incorporationof various conditions. Current methods, however, exhibit limited performancewhen guided by skeleton human poses, especially in complex pose conditions suchas side or rear perspectives of human figures. To address this issue, wepresent Stable-Pose, a novel adapter model that introduces a coarse-to-fineattention masking strategy into a vision Transformer (ViT) to gain accuratepose guidance for T2I models. Stable-Pose is designed to adeptly handle poseconditions within pre-trained Stable Diffusion, providing a refined andefficient way of aligning pose representation during image synthesis. Weleverage the query-key self-attention mechanism of ViTs to explore theinterconnections among different anatomical parts in human pose skeletons.Masked pose images are used to smoothly refine the attention maps based ontarget pose-related features in a hierarchical manner, transitioning fromcoarse to fine levels. Additionally, our loss function is formulated toallocate increased emphasis to the pose region, thereby augmenting the model'sprecision in capturing intricate pose details. We assessed the performance ofStable-Pose across five public datasets under a wide range of indoor andoutdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in theLAION-Human dataset, marking around 13% improvement over the establishedtechnique ControlNet. The project link and code is available athttps://github.com/ai-med/StablePose.</description><author>Jiajun Wang, Morteza Ghahremani, Yitong Li, Björn Ommer, Christian Wachinger</author><pubDate>Tue, 04 Jun 2024 17:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02485v1</guid></item><item><title>How Do Neural Spoofing Countermeasures Detect Partially Spoofed Audio?</title><link>http://arxiv.org/abs/2406.02483v1</link><description>Partially manipulating a sentence can greatly change its meaning. Recent workshows that countermeasures (CMs) trained on partially spoofed audio caneffectively detect such spoofing. However, the current understanding of thedecision-making process of CMs is limited. We utilize Grad-CAM and introduce aquantitative analysis metric to interpret CMs' decisions. We find that CMsprioritize the artifacts of transition regions created when concatenating bonafide and spoofed audio. This focus differs from that of CMs trained on fullyspoofed audio, which concentrate on the pattern differences between bona fideand spoofed parts. Our further investigation explains the varying nature ofCMs' focus while making correct or incorrect predictions. These insightsprovide a basis for the design of CM models and the creation of datasets.Moreover, this work lays a foundation of interpretability in the field ofpartial spoofed audio detection that has not been well explored previously.</description><author>Tianchi Liu, Lin Zhang, Rohan Kumar Das, Yi Ma, Ruijie Tao, Haizhou Li</author><pubDate>Tue, 04 Jun 2024 17:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02483v1</guid></item><item><title>Hiding Text in Large Language Models: Introducing Unconditional Token Forcing Confusion</title><link>http://arxiv.org/abs/2406.02481v1</link><description>With the help of simple fine-tuning, one can artificially embed hidden textinto large language models (LLMs). This text is revealed only when triggered bya specific query to the LLM. Two primary applications are LLM fingerprintingand steganography. In the context of LLM fingerprinting, a unique textidentifier (fingerprint) is embedded within the model to verify licensingcompliance. In the context of steganography, the LLM serves as a carrier forhidden messages that can be disclosed through a designated trigger. Our work demonstrates that embedding hidden text in the LLM via fine-tuning,though seemingly secure due to the vast number of potential triggers (anysequence of characters or tokens could serve as a trigger), is susceptible toextraction through analysis of the LLM's output decoding process. We propose anovel approach to extraction called Unconditional Token Forcing. It is premisedon the hypothesis that iteratively feeding each token from the LLM's vocabularyinto the model should reveal sequences with abnormally high tokenprobabilities, indicating potential embedded text candidates. Additionally, ourexperiments show that when the first token of a hidden fingerprint is used asan input, the LLM not only produces an output sequence with high tokenprobabilities, but also repetitively generates the fingerprint itself. We alsopresent a method to hide text in such a way that it is resistant toUnconditional Token Forcing, which we named Unconditional Token ForcingConfusion.</description><author>Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki</author><pubDate>Tue, 04 Jun 2024 17:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02481v1</guid></item><item><title>Inpainting Pathology in Lumbar Spine MRI with Latent Diffusion</title><link>http://arxiv.org/abs/2406.02477v1</link><description>Data driven models for automated diagnosis in radiology suffer frominsufficient and imbalanced datasets due to low representation of pathology ina population and the cost of expert annotations. Datasets can be bolsteredthrough data augmentation. However, even when utilizing a full suite oftransformations during model training, typical data augmentations do notaddress variations in human anatomy. An alternative direction is to synthesizedata using generative models, which can potentially craft datasets withspecific attributes. While this holds promise, commonly used generative modelssuch as Generative Adversarial Networks may inadvertently produce anatomicallyinaccurate features. On the other hand, diffusion models, which offer greaterstability, tend to memorize training data, raising concerns about privacy andgenerative diversity. Alternatively, inpainting has the potential to augmentdata through directly inserting pathology in medical images. However, thisapproach introduces a new challenge: accurately merging the generatedpathological features with the surrounding anatomical context. While inpaintingis a well established method for addressing simple lesions, its application topathologies that involve complex structural changes remains relativelyunexplored. We propose an efficient method for inpainting pathological featuresonto healthy anatomy in MRI through voxelwise noise scheduling in a latentdiffusion model. We evaluate the method's ability to insert disc herniation andcentral canal stenosis in lumbar spine sagittal T2 MRI, and it achievessuperior Frechet Inception Distance compared to state-of-the-art methods.</description><author>Colin Hansen, Simas Glinskis, Ashwin Raju, Micha Kornreich, JinHyeong Park, Jayashri Pawar, Richard Herzog, Li Zhang, Benjamin Odry</author><pubDate>Tue, 04 Jun 2024 17:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02477v1</guid></item><item><title>Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding</title><link>http://arxiv.org/abs/2406.02472v1</link><description>The digital landscape is rapidly evolving with an ever-increasing volume ofonline news, emphasizing the need for swift and precise analysis of complexevents. We refer to the complex events composed of many news articles over anextended period as Temporal Complex Event (TCE). This paper proposes a novelapproach using Large Language Models (LLMs) to systematically extract andanalyze the event chain within TCE, characterized by their key points andtimestamps. We establish a benchmark, named TCELongBench, to evaluate theproficiency of LLMs in handling temporal dynamics and understanding extensivetext. This benchmark encompasses three distinct tasks - reading comprehension,temporal sequencing, and future event forecasting. In the experiment, weleverage retrieval-augmented generation (RAG) method and LLMs with long contextwindow to deal with lengthy news articles of TCE. Our findings indicate thatmodels with suitable retrievers exhibit comparable performance with thoseutilizing long context window.</description><author>Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, Tat-Seng Chua</author><pubDate>Tue, 04 Jun 2024 17:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02472v1</guid></item><item><title>Meta-Designing Quantum Experiments with Language Models</title><link>http://arxiv.org/abs/2406.02470v1</link><description>Artificial Intelligence (AI) has the potential to significantly advancescientific discovery by finding solutions beyond human capabilities. However,these super-human solutions are often unintuitive and require considerableeffort to uncover underlying principles, if possible at all. Here, we show howa code-generating language model trained on synthetic data can not only findsolutions to specific problems but can create meta-solutions, which solve anentire class of problems in one shot and simultaneously offer insight into theunderlying design principles. Specifically, for the design of new quantumphysics experiments, our sequence-to-sequence transformer architecturegenerates interpretable Python code that describes experimental blueprints fora whole class of quantum systems. We discover general and previously unknowndesign rules for infinitely large classes of quantum states. The ability toautomatically generate generalized patterns in readable computer code is acrucial step toward machines that help discover new scientific understanding --one of the central aims of physics.</description><author>Sören Arlt, Haonan Duan, Felix Li, Sang Michael Xie, Yuhuai Wu, Mario Krenn</author><pubDate>Tue, 04 Jun 2024 17:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02470v1</guid></item><item><title>Landscape-Aware Growing: The Power of a Little LAG</title><link>http://arxiv.org/abs/2406.02469v1</link><description>Recently, there has been increasing interest in efficient pretrainingparadigms for training Transformer-based models. Several recent approaches usesmaller models to initialize larger models in order to save computation (e.g.,stacking and fusion). In this work, we study the fundamental question of how toselect the best growing strategy from a given pool of growing strategies. Priorworks have extensively focused on loss- and/or function-preserving behavior atinitialization or simply performance at the end of training. Instead, weidentify that behavior at initialization can be misleading as a predictor offinal performance and present an alternative perspective based on earlytraining dynamics, which we call "landscape-aware growing (LAG)". We performextensive analysis of correlation of the final performance with performance inthe initial steps of training and find early and more accurate predictions ofthe optimal growing strategy (i.e., with only a small "lag" afterinitialization). This perspective also motivates an adaptive strategy forgradual stacking.</description><author>Stefani Karp, Nikunj Saunshi, Sobhan Miryoosefi, Sashank J. Reddi, Sanjiv Kumar</author><pubDate>Tue, 04 Jun 2024 17:38:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02469v1</guid></item><item><title>DL-KDD: Dual-Light Knowledge Distillation for Action Recognition in the Dark</title><link>http://arxiv.org/abs/2406.02468v1</link><description>Human action recognition in dark videos is a challenging task for computervision. Recent research focuses on applying dark enhancement methods to improvethe visibility of the video. However, such video processing results in the lossof critical information in the original (un-enhanced) video. Conversely,traditional two-stream methods are capable of learning information from bothoriginal and processed videos, but it can lead to a significant increase in thecomputational cost during the inference phase in the task of videoclassification. To address these challenges, we propose a novel teacher-studentvideo classification framework, named Dual-Light KnowleDge Distillation forAction Recognition in the Dark (DL-KDD). This framework enables the model tolearn from both original and enhanced video without introducing additionalcomputational cost during inference. Specifically, DL-KDD utilizes the strategyof knowledge distillation during training. The teacher model is trained withenhanced video, and the student model is trained with both the original videoand the soft target generated by the teacher model. This teacher-studentframework allows the student model to predict action using only the originalinput video during inference. In our experiments, the proposed DL-KDD frameworkoutperforms state-of-the-art methods on the ARID, ARID V1.5, and Dark-48datasets. We achieve the best performance on each dataset and up to a 4.18%improvement on Dark-48, using only original video inputs, thus avoiding the useof two-stream framework or enhancement modules for inference. We furthervalidate the effectiveness of the distillation strategy in ablativeexperiments. The results highlight the advantages of our knowledge distillationframework in dark human action recognition.</description><author>Chi-Jui Chang, Oscar Tai-Yuan Chen, Vincent S. Tseng</author><pubDate>Tue, 04 Jun 2024 17:38:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02468v1</guid></item><item><title>An Empirical Study into Clustering of Unseen Datasets with Self-Supervised Encoders</title><link>http://arxiv.org/abs/2406.02465v1</link><description>Can pretrained models generalize to new datasets without any retraining? Wedeploy pretrained image models on datasets they were not trained for, andinvestigate whether their embeddings form meaningful clusters. Our suite ofbenchmarking experiments use encoders pretrained solely on ImageNet-1k witheither supervised or self-supervised training techniques, deployed on imagedatasets that were not seen during training, and clustered with conventionalclustering algorithms. This evaluation provides new insights into theembeddings of self-supervised models, which prioritize different features tosupervised models. Supervised encoders typically offer more utility than SSLencoders within the training domain, and vice-versa far outside of it, however,fine-tuned encoders demonstrate the opposite trend. Clustering provides a wayto evaluate the utility of self-supervised learned representations orthogonalto existing methods such as kNN. Additionally, we find the silhouette scorewhen measured in a UMAP-reduced space is highly correlated with clusteringperformance, and can therefore be used as a proxy for clustering performance ondata with no ground truth labels. Our code implementation is available at\url{https://github.com/scottclowe/zs-ssl-clustering/}.</description><author>Scott C. Lowe, Joakim Bruslund Haurum, Sageev Oore, Thomas B. Moeslund, Graham W. Taylor</author><pubDate>Tue, 04 Jun 2024 17:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02465v1</guid></item><item><title>Meta-Learners for Partially-Identified Treatment Effects Across Multiple Environments</title><link>http://arxiv.org/abs/2406.02464v1</link><description>Estimating the conditional average treatment effect (CATE) from observationaldata is relevant for many applications such as personalized medicine. Here, wefocus on the widespread setting where the observational data come from multipleenvironments, such as different hospitals, physicians, or countries.Furthermore, we allow for violations of standard causal assumptions, namely,overlap within the environments and unconfoundedness. To this end, we move awayfrom point identification and focus on partial identification. Specifically, weshow that current assumptions from the literature on multiple environmentsallow us to interpret the environment as an instrumental variable (IV). Thisallows us to adapt bounds from the IV literature for partial identification ofCATE by leveraging treatment assignment mechanisms across environments. Then,we propose different model-agnostic learners (so-called meta-learners) toestimate the bounds that can be used in combination with arbitrary machinelearning models. We further demonstrate the effectiveness of our meta-learnersacross various experiments using both simulated and real-world data. Finally,we discuss the applicability of our meta-learners to partial identification ininstrumental variable settings, such as randomized controlled trials withnon-compliance.</description><author>Jonas Schweisthal, Dennis Frauen, Mihaela van der Schaar, Stefan Feuerriegel</author><pubDate>Tue, 04 Jun 2024 17:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02464v1</guid></item><item><title>Learning Image Priors through Patch-based Diffusion Models for Solving Inverse Problems</title><link>http://arxiv.org/abs/2406.02462v1</link><description>Diffusion models can learn strong image priors from underlying datadistribution and use them to solve inverse problems, but the training processis computationally expensive and requires lots of data. Such bottlenecksprevent most existing works from being feasible for high-dimensional andhigh-resolution data such as 3D images. This paper proposes a method to learnan efficient data prior for the entire image by training diffusion models onlyon patches of images. Specifically, we propose a patch-based position-awarediffusion inverse solver, called PaDIS, where we obtain the score function ofthe whole image through scores of patches and their positional encoding andutilize this as the prior for solving inverse problems. First of all, we showthat this diffusion model achieves an improved memory efficiency and dataefficiency while still maintaining the capability to generate entire images viapositional encoding. Additionally, the proposed PaDIS model is highly flexibleand can be plugged in with different diffusion inverse solvers (DIS). Wedemonstrate that the proposed PaDIS approach enables solving various inverseproblems in both natural and medical image domains, including CTreconstruction, deblurring, and superresolution, given only patch-based priors.Notably, PaDIS outperforms previous DIS methods trained on entire image priorsin the case of limited training data, demonstrating the data efficiency of ourproposed approach by learning patch-based prior.</description><author>Jason Hu, Bowen Song, Xiaojian Xu, Liyue Shen, Jeffrey A. Fessler</author><pubDate>Tue, 04 Jun 2024 17:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02462v1</guid></item><item><title>RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting</title><link>http://arxiv.org/abs/2406.02461v1</link><description>The advancement of diffusion models has pushed the boundary of text-to-3Dobject generation. While it is straightforward to composite objects into ascene with reasonable geometry, it is nontrivial to texture such a sceneperfectly due to style inconsistency and occlusions between objects. To tacklethese problems, we propose a coarse-to-fine 3D scene texturing framework,referred to as RoomTex, to generate high-fidelity and style-consistent texturesfor untextured compositional scene meshes. In the coarse stage, RoomTex firstunwraps the scene mesh to a panoramic depth map and leverages ControlNet togenerate a room panorama, which is regarded as the coarse reference to ensurethe global texture consistency. In the fine stage, based on the panoramic imageand perspective depth maps, RoomTex will refine and texture every single objectin the room iteratively along a series of selected camera views, until thisobject is completely painted. Moreover, we propose to maintain superioralignment between RGB and depth spaces via subtle edge detection methods.Extensive experiments show our method is capable of generating high-quality anddiverse room textures, and more importantly, supporting interactivefine-grained texture control and flexible scene editing thanks to ourinpainting-based framework and compositional mesh input. Our project page isavailable at https://qwang666.github.io/RoomTex/.</description><author>Qi Wang, Ruijie Lu, Xudong Xu, Jingbo Wang, Michael Yu Wang, Bo Dai, Gang Zeng, Dan Xu</author><pubDate>Tue, 04 Jun 2024 17:27:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02461v1</guid></item><item><title>CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling</title><link>http://arxiv.org/abs/2405.16433v2</link><description>Using large language models (LLMs) to assist psychological counseling is asignificant but challenging task at present. Attempts have been made onimproving empathetic conversations or acting as effective assistants in thetreatment with LLMs. However, the existing datasets lack consulting knowledge,resulting in LLMs lacking professional consulting competence. Moreover, how toautomatically evaluate multi-turn dialogues within the counseling processremains an understudied area. To bridge the gap, we propose CPsyCoun, areport-based multi-turn dialogue reconstruction and evaluation framework forChinese psychological counseling. To fully exploit psychological counselingreports, a two-phase approach is devised to construct high-quality dialogueswhile a comprehensive evaluation benchmark is developed for the effectiveautomatic evaluation of multi-turn psychological consultations. Competitiveexperimental results demonstrate the effectiveness of our proposed framework inpsychological counseling. We open-source the datasets and model for futureresearch at https://github.com/CAS-SIAT-XinHai/CPsyCoun</description><author>Chenhao Zhang, Renhao Li, Minghuan Tan, Min Yang, Jingwei Zhu, Di Yang, Jiahao Zhao, Guancheng Ye, Chengming Li, Xiping Hu, Derek F. Wong</author><pubDate>Tue, 04 Jun 2024 17:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16433v2</guid></item><item><title>Machine learning Hubbard parameters with equivariant neural networks</title><link>http://arxiv.org/abs/2406.02457v1</link><description>Density-functional theory with extended Hubbard functionals (DFT+$U$+$V$)provides a robust framework to accurately describe complex materials containingtransition-metal or rare-earth elements. It does so by mitigatingself-interaction errors inherent to semi-local functionals which areparticularly pronounced in systems with partially-filled $d$ and $f$ electronicstates. However, achieving accuracy in this approach hinges upon the accuratedetermination of the on-site $U$ and inter-site $V$ Hubbard parameters. Inpractice, these are obtained either by semi-empirical tuning, requiring priorknowledge, or, more correctly, by using predictive but expensivefirst-principles calculations. Here, we present a machine learning model basedon equivariant neural networks which uses atomic occupation matrices asdescriptors, directly capturing the electronic structure, local chemicalenvironment, and oxidation states of the system at hand. We target here theprediction of Hubbard parameters computed self-consistently with iterativelinear-response calculations, as implemented in density-functional perturbationtheory (DFPT), and structural relaxations. Remarkably, when trained on datafrom 11 materials spanning various crystal structures and compositions, ourmodel achieves mean absolute relative errors of 3% and 5% for Hubbard $U$ and$V$ parameters, respectively. By circumventing computationally expensive DFT orDFPT self-consistent protocols, our model significantly expedites theprediction of Hubbard parameters with negligible computational overhead, whileapproaching the accuracy of DFPT. Moreover, owing to its robusttransferability, the model facilitates accelerated materials discovery anddesign via high-throughput calculations, with relevance for varioustechnological applications.</description><author>Martin Uhrin, Austin Zadoks, Luca Binci, Nicola Marzari, Iurii Timrov</author><pubDate>Tue, 04 Jun 2024 17:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02457v1</guid></item><item><title>Offline Bayesian Aleatoric and Epistemic Uncertainty Quantification and Posterior Value Optimisation in Finite-State MDPs</title><link>http://arxiv.org/abs/2406.02456v1</link><description>We address the challenge of quantifying Bayesian uncertainty andincorporating it in offline use cases of finite-state Markov Decision Processes(MDPs) with unknown dynamics. Our approach provides a principled method todisentangle epistemic and aleatoric uncertainty, and a novel technique to findpolicies that optimise Bayesian posterior expected value without relying onstrong assumptions about the MDP's posterior distribution. First, we utilisestandard Bayesian reinforcement learning methods to capture the posterioruncertainty in MDP parameters based on available data. We then analyticallycompute the first two moments of the return distribution across posteriorsamples and apply the law of total variance to disentangle aleatoric andepistemic uncertainties. To find policies that maximise posterior expectedvalue, we leverage the closed-form expression for value as a function ofpolicy. This allows us to propose a stochastic gradient-based approach forsolving the problem. We illustrate the uncertainty quantification and Bayesianposterior value optimisation performance of our agent in simple, interpretablegridworlds and validate it through ground-truth evaluations on synthetic MDPs.Finally, we highlight the real-world impact and computational scalability ofour method by applying it to the AI Clinician problem, which recommendstreatment for patients in intensive care units and has emerged as a key usecase of finite-state MDPs with offline data. We discuss the challenges thatarise with Bayesian modelling of larger scale MDPs while demonstrating thepotential to apply our methods rooted in Bayesian decision theory into the realworld. We make our code available athttps://github.com/filippovaldettaro/finite-state-mdps .</description><author>Filippo Valdettaro, A. Aldo Faisal</author><pubDate>Tue, 04 Jun 2024 17:21:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02456v1</guid></item><item><title>Lay-A-Scene: Personalized 3D Object Arrangement Using Text-to-Image Priors</title><link>http://arxiv.org/abs/2406.00687v2</link><description>Generating 3D visual scenes is at the forefront of visual generative AI, butcurrent 3D generation techniques struggle with generating scenes with multiplehigh-resolution objects. Here we introduce Lay-A-Scene, which solves the taskof Open-set 3D Object Arrangement, effectively arranging unseen objects. Givena set of 3D objects, the task is to find a plausible arrangement of theseobjects in a scene. We address this task by leveraging pre-trainedtext-to-image models. We personalize the model and explain how to generateimages of a scene that contains multiple predefined objects without neglectingany of them. Then, we describe how to infer the 3D poses and arrangement ofobjects from a 2D generated image by finding a consistent projection of objectsonto the 2D scene. We evaluate the quality of Lay-A-Scene using 3D objects fromObjaverse and human raters and find that it often generates coherent andfeasible 3D object arrangements.</description><author>Ohad Rahamim, Hilit Segev, Idan Achituve, Yuval Atzmon, Yoni Kasten, Gal Chechik</author><pubDate>Tue, 04 Jun 2024 17:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00687v2</guid></item><item><title>A Generalized Apprenticeship Learning Framework for Modeling Heterogeneous Student Pedagogical Strategies</title><link>http://arxiv.org/abs/2406.02450v1</link><description>A key challenge in e-learning environments like Intelligent Tutoring Systems(ITSs) is to induce effective pedagogical policies efficiently. While DeepReinforcement Learning (DRL) often suffers from sample inefficiency and rewardfunction design difficulty, Apprenticeship Learning(AL) algorithms can overcomethem. However, most AL algorithms can not handle heterogeneity as they assumeall demonstrations are generated with a homogeneous policy driven by a singlereward function. Still, some AL algorithms which consider heterogeneity, oftencan not generalize to large continuous state space and only work with discretestates. In this paper, we propose an expectation-maximization(EM)-EDM, ageneral AL framework to induce effective pedagogical policies from givenoptimal or near-optimal demonstrations, which are assumed to be driven byheterogeneous reward functions. We compare the effectiveness of the policiesinduced by our proposed EM-EDM against four AL-based baselines and two policiesinduced by DRL on two different but related tasks that involve pedagogicalaction prediction. Our overall results showed that, for both tasks, EM-EDMoutperforms the four AL baselines across all performance metrics and the twoDRL baselines. This suggests that EM-EDM can effectively model complex studentpedagogical decision-making processes through the ability to manage a large,continuous state space and adapt to handle diverse and heterogeneous rewardfunctions with very few given demonstrations.</description><author>Md Mirajul Islam, Xi Yang, John Hostetter, Adittya Soukarjya Saha, Min Chi</author><pubDate>Tue, 04 Jun 2024 17:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02450v1</guid></item><item><title>Representations as Language: An Information-Theoretic Framework for Interpretability</title><link>http://arxiv.org/abs/2406.02449v1</link><description>Large scale neural models show impressive performance across a wide array oflinguistic tasks. Despite this they remain, largely, black-boxes - inducingvector-representations of their input that prove difficult to interpret. Thislimits our ability to understand what they learn, and when the learn it, ordescribe what kinds of representations generalise well out of distribution. Toaddress this we introduce a novel approach to interpretability that looks atthe mapping a model learns from sentences to representations as a kind oflanguage in its own right. In doing so we introduce a set ofinformation-theoretic measures that quantify how structured a model'srepresentations are with respect to its input, and when during training thatstructure arises. Our measures are fast to compute, grounded in linguistictheory, and can predict which models will generalise best based on theirrepresentations. We use these measures to describe two distinct phases oftraining a transformer: an initial phase of in-distribution learning whichreduces task loss, then a second stage where representations becoming robust tonoise. Generalisation performance begins to increase during this second phase,drawing a link between generalisation and robustness to noise. Finally we lookat how model size affects the structure of the representational space, showingthat larger models ultimately compress their representations more than theirsmaller counterparts.</description><author>Henry Conklin, Kenny Smith</author><pubDate>Tue, 04 Jun 2024 17:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02449v1</guid></item><item><title>Reducing Bias in Federated Class-Incremental Learning with Hierarchical Generative Prototypes</title><link>http://arxiv.org/abs/2406.02447v1</link><description>Federated Learning (FL) aims at unburdening the training of deep models bydistributing computation across multiple devices (clients) while safeguardingdata privacy. On top of that, Federated Continual Learning (FCL) also accountsfor data distribution evolving over time, mirroring the dynamic nature ofreal-world environments. In this work, we shed light on the Incremental andFederated biases that naturally emerge in FCL. While the former is a knownproblem in Continual Learning, stemming from the prioritization of recentlyintroduced classes, the latter (i.e., the bias towards local distributions)remains relatively unexplored. Our proposal constrains both biases in the lastlayer by efficiently fine-tuning a pre-trained backbone using learnableprompts, resulting in clients that produce less biased representations and morebiased classifiers. Therefore, instead of solely relying on parameteraggregation, we also leverage generative prototypes to effectively balance thepredictions of the global model. Our method improves on the current State OfThe Art, providing an average increase of +7.9% in accuracy.</description><author>Riccardo Salami, Pietro Buzzega, Matteo Mosconi, Mattia Verasani, Simone Calderara</author><pubDate>Tue, 04 Jun 2024 17:12:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02447v1</guid></item><item><title>A Sentiment Consolidation Framework for Meta-Review Generation</title><link>http://arxiv.org/abs/2402.18005v2</link><description>Modern natural language generation systems with Large Language Models (LLMs)exhibit the capability to generate a plausible summary of multiple documents;however, it is uncertain if they truly possess the capability of informationconsolidation to generate summaries, especially on documents with opinionatedinformation. We focus on meta-review generation, a form of sentimentsummarisation for the scientific domain. To make scientific sentimentsummarization more grounded, we hypothesize that human meta-reviewers follow athree-layer framework of sentiment consolidation to write meta-reviews. Basedon the framework, we propose novel prompting methods for LLMs to generatemeta-reviews and evaluation metrics to assess the quality of generatedmeta-reviews. Our framework is validated empirically as we find that promptingLLMs based on the framework -- compared with prompting them with simpleinstructions -- generates better meta-reviews.</description><author>Miao Li, Jey Han Lau, Eduard Hovy</author><pubDate>Tue, 04 Jun 2024 17:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18005v2</guid></item><item><title>AI-Face: A Million-Scale Demographically Annotated AI-Generated Face Dataset and Fairness Benchmark</title><link>http://arxiv.org/abs/2406.00783v2</link><description>AI-generated faces have enriched human life, such as entertainment,education, and art. However, they also pose misuse risks. Therefore, detectingAI-generated faces becomes crucial, yet current detectors show biasedperformance across different demographic groups. Mitigating biases can be doneby designing algorithmic fairness methods, which usually requiredemographically annotated face datasets for model training. However, noexisting dataset comprehensively encompasses both demographic attributes anddiverse generative methods, which hinders the development of fair detectors forAI-generated faces. In this work, we introduce the AI-Face dataset, the firstmillion-scale demographically annotated AI-generated face image dataset,including real faces, faces from deepfake videos, and faces generated byGenerative Adversarial Networks and Diffusion Models. Based on this dataset, weconduct the first comprehensive fairness benchmark to assess various AI facedetectors and provide valuable insights and findings to promote the future fairdesign of AI face detectors. Our AI-Face dataset and benchmark code arepublicly available at https://github.com/Purdue-M2/AI-Face-FairnessBench.</description><author>Li Lin, Santosh, Xin Wang, Shu Hu</author><pubDate>Tue, 04 Jun 2024 17:08:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00783v2</guid></item><item><title>Explainable Deep Learning Analysis for Raga Identification in Indian Art Music</title><link>http://arxiv.org/abs/2406.02443v1</link><description>The task of Raga Identification is a very popular research problem in MusicInformation Retrieval. Few studies that have explored this task employedvarious approaches, such as signal processing, Machine Learning (ML) methods,and more recently Deep Learning (DL) based methods. However, a key questionremains unanswered in all of these works: do these ML/DL methods learn andinterpret Ragas in a manner similar to human experts? Besides, a significantroadblock in this research is the unavailability of ample supply of rich,labeled datasets, which drives these ML/DL based methods. In this paper, weintroduce "Prasarbharti Indian Music" version-1 (PIM-v1), a novel datasetcomprising of 191 hours of meticulously labeled Hindustani Classical Music(HCM) recordings, which is the largest labeled dataset for HCM recordings tothe best of our knowledge. Our approach involves conducting ablation studies tofind the benchmark classification model for Automatic Raga Identification (ARI)using PIM-v1 dataset. We achieve a chunk-wise f1-score of 0.89 for a subset of12 Raga classes. Subsequently, we employ model explainability techniques toevaluate the classifier's predictions, aiming to ascertain whether they alignwith human understanding of Ragas or are driven by arbitrary patterns. Wevalidate the correctness of model's predictions by comparing the explanationsgiven by two ExAI models with human expert annotations. Following this, weanalyze explanations for individual test examples to understand the role ofregions highlighted by explanations in correct or incorrect predictions made bythe model.</description><author>Parampreet Singh, Vipul Arora</author><pubDate>Tue, 04 Jun 2024 17:06:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02443v1</guid></item><item><title>Automated Focused Feedback Generation for Scientific Writing Assistance</title><link>http://arxiv.org/abs/2405.20477v2</link><description>Scientific writing is a challenging task, particularly for novice researcherswho often rely on feedback from experienced peers. Recent work has primarilyfocused on improving surface form and style rather than manuscript content. Inthis paper, we propose a novel task: automated focused feedback generation forscientific writing assistance. We present SWIF$^{2}$T: a Scientific WrItingFocused Feedback Tool. It is designed to generate specific, actionable andcoherent comments, which identify weaknesses in a scientific paper and/orpropose revisions to it. Our approach consists of four components - planner,investigator, reviewer and controller - leveraging multiple Large LanguageModels (LLMs) to implement them. We compile a dataset of 300 peer reviewsciting weaknesses in scientific papers and conduct human evaluation. Theresults demonstrate the superiority in specificity, reading comprehension, andoverall helpfulness of SWIF$^{2}$T's feedback compared to other approaches. Inour analysis, we also identified cases where automatically generated reviewswere judged better than human ones, suggesting opportunities for integration ofAI-generated feedback in scientific writing.</description><author>Eric Chamoun, Michael Schlichktrull, Andreas Vlachos</author><pubDate>Tue, 04 Jun 2024 17:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20477v2</guid></item><item><title>Fast Decision Boundary based Out-of-Distribution Detector</title><link>http://arxiv.org/abs/2312.11536v2</link><description>Efficient and effective Out-of-Distribution (OOD) detection is essential forthe safe deployment of AI systems. Existing feature space methods, whileeffective, often incur significant computational overhead due to their relianceon auxiliary models built from training features. In this paper, we propose acomputationally-efficient OOD detector without using auxiliary models whilestill leveraging the rich information embedded in the feature space.Specifically, we detect OOD samples based on their feature distances todecision boundaries. To minimize computational cost, we introduce an efficientclosed-form estimation, analytically proven to tightly lower bound thedistance. Based on our estimation, we discover that In-Distribution (ID)features tend to be further from decision boundaries than OOD features.Additionally, ID and OOD samples are better separated when compared at equaldeviation levels from the mean of training features. By regularizing thedistances to decision boundaries based on feature deviation from the mean, wedevelop a hyperparameter-free, auxiliary model-free OOD detector. Our methodmatches or surpasses the effectiveness of state-of-the-art methods in extensiveexperiments while incurring negligible overhead in inference latency. Overall,our approach significantly improves the efficiency-effectiveness trade-off inOOD detection. Code is available at: https://github.com/litianliu/fDBD-OOD.</description><author>Litian Liu, Yao Qin</author><pubDate>Tue, 04 Jun 2024 17:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11536v2</guid></item><item><title>Generative Active Learning for Long-tailed Instance Segmentation</title><link>http://arxiv.org/abs/2406.02435v1</link><description>Recently, large-scale language-image generative models have gained widespreadattention and many works have utilized generated data from these models tofurther enhance the performance of perception tasks. However, not all generateddata can positively impact downstream models, and these methods do notthoroughly explore how to better select and utilize generated data. On theother hand, there is still a lack of research oriented towards active learningon generated data. In this paper, we explore how to perform active learningspecifically for generated data in the long-tailed instance segmentation task.Subsequently, we propose BSGAL, a new algorithm that online estimates thecontribution of the generated data based on gradient cache. BSGAL can handleunlimited generated data and complex downstream segmentation tasks effectively.Experiments show that BSGAL outperforms the baseline approach and effectuallyimproves the performance of long-tailed segmentation. Our code can be found athttps://github.com/aim-uofa/DiverGen.</description><author>Muzhi Zhu, Chengxiang Fan, Hao Chen, Yang Liu, Weian Mao, Xiaogang Xu, Chunhua Shen</author><pubDate>Tue, 04 Jun 2024 16:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02435v1</guid></item><item><title>Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters</title><link>http://arxiv.org/abs/2402.00828v2</link><description>Mixture of Experts (MoE) architectures have recently started burgeoning dueto their ability to scale model's capacity while maintaining the computationalcost affordable. Furthermore, they can be applied to both Transformers andState Space Models, the current state-of-the-art models in numerous fields.While MoE has been mostly investigated for the pre-training stage, its use inparameter-efficient transfer learning settings is under-explored. To narrowthis gap, this paper attempts to demystify the use of MoE forparameter-efficient fine-tuning of Audio Spectrogram Transformers to audio andspeech downstream tasks. Specifically, we propose Soft Mixture of Adapters(Soft-MoA). It exploits adapters as the experts and, leveraging the recent SoftMoE method, it relies on a soft assignment between the input tokens and expertsto keep the computational time limited. Extensive experiments across 4benchmarks demonstrate that Soft-MoA outperforms the single adapter method andperforms on par with the dense MoA counterpart. We finally present ablationstudies on key elements of Soft-MoA, showing for example that Soft-MoA achievesbetter scaling with more experts, as well as ensuring that all expertscontribute to the computation of the output tokens, thus dispensing with theexpert imbalance issue.</description><author>Umberto Cappellazzo, Daniele Falavigna, Alessio Brutti</author><pubDate>Tue, 04 Jun 2024 16:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00828v2</guid></item><item><title>How Smooth Is Attention?</title><link>http://arxiv.org/abs/2312.14820v2</link><description>Self-attention and masked self-attention are at the heart of Transformers'outstanding success. Still, our mathematical understanding of attention, inparticular of its Lipschitz properties - which are key when it comes toanalyzing robustness and expressive power - is incomplete. We provide adetailed study of the Lipschitz constant of self-attention in several practicalscenarios, discussing the impact of the sequence length $n$ and layernormalization on the local Lipschitz constant of both unmasked and maskedself-attention. In particular, we show that for inputs of length $n$ in anycompact set, the Lipschitz constant of self-attention is bounded by $\sqrt{n}$up to a constant factor and that this bound is tight for reasonable sequencelengths. When the sequence length $n$ is too large for the previous bound to betight, which we refer to as the mean-field regime, we provide an upper boundand a matching lower bound which are independent of $n$. Our mean-fieldframework for masked self-attention is novel and of independent interest. Ourexperiments on pretrained and randomly initialized BERT and GPT-2 support ourtheoretical findings.</description><author>Valérie Castin, Pierre Ablin, Gabriel Peyré</author><pubDate>Tue, 04 Jun 2024 16:51:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14820v2</guid></item><item><title>Coresets for Multiple $\ell_p$ Regression</title><link>http://arxiv.org/abs/2406.02432v1</link><description>A coreset of a dataset with $n$ examples and $d$ features is a weightedsubset of examples that is sufficient for solving downstream data analytictasks. Nearly optimal constructions of coresets for least squares and $\ell_p$linear regression with a single response are known in prior work. However, formultiple $\ell_p$ regression where there can be $m$ responses, there are noknown constructions with size sublinear in $m$. In this work, we constructcoresets of size $\tilde O(\varepsilon^{-2}d)$ for $p&lt;2$ and $\tildeO(\varepsilon^{-p}d^{p/2})$ for $p&gt;2$ independently of $m$ (i.e.,dimension-free) that approximate the multiple $\ell_p$ regression objective atevery point in the domain up to $(1\pm\varepsilon)$ relative error. If we onlyneed to preserve the minimizer subject to a subspace constraint, we improvethese bounds by an $\varepsilon$ factor for all $p&gt;1$. All of our bounds arenearly tight. We give two application of our results. First, we settle the number ofuniform samples needed to approximate $\ell_p$ Euclidean power means up to a$(1+\varepsilon)$ factor, showing that $\tilde\Theta(\varepsilon^{-2})$ samplesfor $p = 1$, $\tilde\Theta(\varepsilon^{-1})$ samples for $1 &lt; p &lt; 2$, and$\tilde\Theta(\varepsilon^{1-p})$ samples for $p&gt;2$ is tight, answering aquestion of Cohen-Addad, Saulpic, and Schwiegelshohn. Second, we show that for$1&lt;p&lt;2$, every matrix has a subset of $\tilde O(\varepsilon^{-1}k)$ rows whichspans a $(1+\varepsilon)$-approximately optimal $k$-dimensional subspace for$\ell_p$ subspace approximation, which is also nearly optimal.</description><author>David P. Woodruff, Taisuke Yasuda</author><pubDate>Tue, 04 Jun 2024 16:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02432v1</guid></item><item><title>Reweighted Solutions for Weighted Low Rank Approximation</title><link>http://arxiv.org/abs/2406.02431v1</link><description>Weighted low rank approximation (WLRA) is an important yet computationallychallenging primitive with applications ranging from statistical analysis,model compression, and signal processing. To cope with the NP-hardness of thisproblem, prior work considers heuristics, bicriteria, or fixed parametertractable algorithms to solve this problem. In this work, we introduce a newrelaxed solution to WLRA which outputs a matrix that is not necessarily lowrank, but can be stored using very few parameters and gives provableapproximation guarantees when the weight matrix has low rank. Our central ideais to use the weight matrix itself to reweight a low rank solution, which givesan extremely simple algorithm with remarkable empirical performance inapplications to model compression and on synthetic datasets. Our algorithm alsogives nearly optimal communication complexity bounds for a natural distributedproblem associated with this problem, for which we show matching communicationlower bounds. Together, our communication complexity bounds show that the rankof the weight matrix provably parameterizes the communication complexity ofWLRA. We also obtain the first relative error guarantees for feature selectionwith a weighted objective.</description><author>David P. Woodruff, Taisuke Yasuda</author><pubDate>Tue, 04 Jun 2024 16:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02431v1</guid></item><item><title>Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding</title><link>http://arxiv.org/abs/2310.07075v3</link><description>Instruction-tuned large language models (LLMs) excel at many tasks but oftenfail to use external tools due to complicated and unfamiliar syntaxconstraints. While extensive fine-tuning and prompting can mitigate the issue,these approaches are expensive and hard to generalize. Furthermore, becausesyntax constraints are only learned implicitly during fine-tuning, models stillmake frequent syntax errors. Motivated by the fact that these constraints canbe better satisfied explicitly with constrained decoding, we propose TOOLDEC, adecoding algorithm using finite state machines to force LLMs to follow toolsyntax. Our experiments show that TOOLDEC eliminates all syntax errors,achieving significantly better performance on various base models andbenchmarks. More surprisingly, when applied to generalist out-of-the-box LLMssuch as Mistral-Instruct, TOOLDEC improves its accuracy in tool use from theinitial 0% to an impressive 52%, matching the performance of specializedfine-tuned models such as ToolLLM.</description><author>Kexun Zhang, Hongqiao Chen, Lei Li, William Wang</author><pubDate>Tue, 04 Jun 2024 16:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07075v3</guid></item><item><title>Harnessing Neural Unit Dynamics for Effective and Scalable Class-Incremental Learning</title><link>http://arxiv.org/abs/2406.02428v1</link><description>Class-incremental learning (CIL) aims to train a model to learn new classesfrom non-stationary data streams without forgetting old ones. In this paper, wepropose a new kind of connectionist model by tailoring neural unit dynamicsthat adapt the behavior of neural networks for CIL. In each training session,it introduces a supervisory mechanism to guide network expansion whose growthsize is compactly commensurate with the intrinsic complexity of a newlyarriving task. This constructs a near-minimal network while allowing the modelto expand its capacity when cannot sufficiently hold new classes. At inferencetime, it automatically reactivates the required neural units to retrieveknowledge and leaves the remaining inactivated to prevent interference. We nameour model AutoActivator, which is effective and scalable. To gain insights intothe neural unit dynamics, we theoretically analyze the model's convergenceproperty via a universal approximation theorem on learning sequential mappings,which is under-explored in the CIL community. Experiments show that our methodachieves strong CIL performance in rehearsal-free and minimal-expansionsettings with different backbones.</description><author>Depeng Li, Tianqi Wang, Junwei Chen, Wei Dai, Zhigang Zeng</author><pubDate>Tue, 04 Jun 2024 16:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02428v1</guid></item><item><title>Contextual Optimization under Covariate Shift: A Robust Approach by Intersecting Wasserstein Balls</title><link>http://arxiv.org/abs/2406.02426v1</link><description>In contextual optimization, a decision-maker observes historical samples ofuncertain variables and associated concurrent covariates, without knowing theirjoint distribution. Given an additional covariate observation, the goal is tochoose a decision that minimizes some operational costs. A prevalent issue hereis covariate shift, where the marginal distribution of the new covariatediffers from historical samples, leading to decision performance variationswith nonparametric or parametric estimators. To address this, we propose adistributionally robust approach that uses an ambiguity set by the intersectionof two Wasserstein balls, each centered on typical nonparametric or parametricdistribution estimators. Computationally, we establish the tractablereformulation of this distributionally robust optimization problem.Statistically, we provide guarantees for our Wasserstein ball intersectionapproach under covariate shift by analyzing the measure concentration of theestimators. Furthermore, to reduce computational complexity, we employ asurrogate objective that maintains similar generalization guarantees. Throughsynthetic and empirical case studies on income prediction and portfoliooptimization, we demonstrate the strong empirical performance of our proposedmodels.</description><author>Tianyu Wang, Ningyuan Chen, Chun Wang</author><pubDate>Tue, 04 Jun 2024 16:46:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02426v1</guid></item><item><title>Differentially Private Decentralized Learning with Random Walks</title><link>http://arxiv.org/abs/2402.07471v2</link><description>The popularity of federated learning comes from the possibility of betterscalability and the ability for participants to keep control of their data,improving data security and sovereignty. Unfortunately, sharing model updatesalso creates a new privacy attack surface. In this work, we characterize theprivacy guarantees of decentralized learning with random walk algorithms, wherea model is updated by traveling from one node to another along the edges of acommunication graph. Using a recent variant of differential privacy tailored tothe study of decentralized algorithms, namely Pairwise Network DifferentialPrivacy, we derive closed-form expressions for the privacy loss between eachpair of nodes where the impact of the communication topology is captured bygraph theoretic quantities. Our results further reveal that random walkalgorithms tends to yield better privacy guarantees than gossip algorithms fornodes close from each other. We supplement our theoretical results withempirical evaluation on synthetic and real-world graphs and datasets.</description><author>Edwige Cyffers, Aurélien Bellet, Jalaj Upadhyay</author><pubDate>Tue, 04 Jun 2024 16:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07471v2</guid></item><item><title>CoNav: A Benchmark for Human-Centered Collaborative Navigation</title><link>http://arxiv.org/abs/2406.02425v1</link><description>Human-robot collaboration, in which the robot intelligently assists the humanwith the upcoming task, is an appealing objective. To achieve this goal, theagent needs to be equipped with a fundamental collaborative navigation ability,where the agent should reason human intention by observing human activities andthen navigate to the human's intended destination in advance of the human.However, this vital ability has not been well studied in previous literature.To fill this gap, we propose a collaborative navigation (CoNav) benchmark. OurCoNav tackles the critical challenge of constructing a 3D navigationenvironment with realistic and diverse human activities. To achieve this, wedesign a novel LLM-based humanoid animation generation framework, which isconditioned on both text descriptions and environmental context. The generatedhumanoid trajectory obeys the environmental context and can be easilyintegrated into popular simulators. We empirically find that the existingnavigation methods struggle in CoNav task since they neglect the perception ofhuman intention. To solve this problem, we propose an intention-aware agent forreasoning both long-term and short-term human intention. The agent predictsnavigation action based on the predicted intention and panoramic observation.The emergent agent behavior including observing humans, avoiding humancollision, and navigation reveals the efficiency of the proposed datasets andagents.</description><author>Changhao Li, Xinyu Sun, Peihao Chen, Jugang Fan, Zixu Wang, Yanxia Liu, Jinhui Zhu, Chuang Gan, Mingkui Tan</author><pubDate>Tue, 04 Jun 2024 16:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02425v1</guid></item><item><title>Contextual Dynamic Pricing: Algorithms, Optimality, and Local Differential Privacy Constraints</title><link>http://arxiv.org/abs/2406.02424v1</link><description>We study the contextual dynamic pricing problem where a firm sells productsto $T$ sequentially arriving consumers that behave according to an unknowndemand model. The firm aims to maximize its revenue, i.e. minimize its regretover a clairvoyant that knows the model in advance. The demand model is ageneralized linear model (GLM), allowing for a stochastic feature vector in$\mathbb R^d$ that encodes product and consumer information. We first show thatthe optimal regret upper bound is of order $\sqrt{dT}$, up to a logarithmicfactor, improving upon existing upper bounds in the literature by a $\sqrt{d}$factor. This sharper rate is materialised by two algorithms: a confidencebound-type (supCB) algorithm and an explore-then-commit (ETC) algorithm. A keyinsight of our theoretical result is an intrinsic connection between dynamicpricing and the contextual multi-armed bandit problem with many arms based on acareful discretization. We further study contextual dynamic pricing under thelocal differential privacy (LDP) constraints. In particular, we propose astochastic gradient descent based ETC algorithm that achieves an optimal regretupper bound of order $d\sqrt{T}/\epsilon$, up to a logarithmic factor, where$\epsilon&gt;0$ is the privacy parameter. The regret upper bounds with and withoutLDP constraints are accompanied by newly constructed minimax lower bounds,which further characterize the cost of privacy. Extensive numerical experimentsand a real data application on online lending are conducted to illustrate theefficiency and practical value of the proposed algorithms in dynamic pricing.</description><author>Zifeng Zhao, Feiyu Jiang, Yi Yu</author><pubDate>Tue, 04 Jun 2024 16:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02424v1</guid></item><item><title>EuSQuAD: Automatically Translated and Aligned SQuAD2.0 for Basque</title><link>http://arxiv.org/abs/2404.12177v2</link><description>The widespread availability of Question Answering (QA) datasets in Englishhas greatly facilitated the advancement of the Natural Language Processing(NLP) field. However, the scarcity of such resources for minority languages,such as Basque, poses a substantial challenge for these communities. In thiscontext, the translation and alignment of existing QA datasets plays a crucialrole in narrowing this technological gap. This work presents EuSQuAD, the firstinitiative dedicated to automatically translating and aligning SQuAD2.0 intoBasque, resulting in more than 142k QA examples. We demonstrate EuSQuAD's valuethrough extensive qualitative analysis and QA experiments supported withEuSQuAD as training data. These experiments are evaluated with a newhuman-annotated dataset.</description><author>Aitor García-Pablos, Naiara Perez, Montse Cuadros, Jaione Bengoetxea</author><pubDate>Tue, 04 Jun 2024 16:43:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12177v2</guid></item><item><title>ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport</title><link>http://arxiv.org/abs/2403.03777v2</link><description>We present a new approach for Neural Optimal Transport (NOT) trainingprocedure, capable of accurately and efficiently estimating optimaltransportation plan via specific regularization on dual Kantorovich potentials.The main bottleneck of existing NOT solvers is associated with the procedure offinding a near-exact approximation of the conjugate operator (i.e., thec-transform), which is done either by optimizing over non-convex max-minobjectives or by the computationally intensive fine-tuning of the initialapproximated prediction. We resolve both issues by proposing a new,theoretically justified loss in the form of expectile regularisation whichenforces binding conditions on the learning process of dual potentials. Such aregularization provides the upper bound estimation over the distribution ofpossible conjugate potentials and makes the learning stable, completelyeliminating the need for additional extensive fine-tuning. Proposed method,called Expectile-Regularised Neural Optimal Transport (ENOT), outperformsprevious state-of-the-art approaches on the established Wasserstein-2 benchmarktasks by a large margin (up to a 3-fold improvement in quality and up to a10-fold improvement in runtime). Moreover, we showcase performance of ENOT forvarying cost functions on different tasks such as image generation, showingrobustness of proposed algorithm.</description><author>Nazar Buzun, Maksim Bobrin, Dmitry V. Dylov</author><pubDate>Tue, 04 Jun 2024 16:41:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03777v2</guid></item><item><title>Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning</title><link>http://arxiv.org/abs/2301.00944v3</link><description>In large-scale distributed machine learning, recent works have studied theeffects of compressing gradients in stochastic optimization to alleviate thecommunication bottleneck. These works have collectively revealed thatstochastic gradient descent (SGD) is robust to structured perturbations such asquantization, sparsification, and delays. Perhaps surprisingly, despite thesurge of interest in multi-agent reinforcement learning, almost nothing isknown about the analogous question: Are common reinforcement learning (RL)algorithms also robust to similar perturbations? We investigate this questionby studying a variant of the classical temporal difference (TD) learningalgorithm with a perturbed update direction, where a general compressionoperator is used to model the perturbation. Our work makes three importanttechnical contributions. First, we prove that compressed TD algorithms, coupledwith an error-feedback mechanism used widely in optimization, exhibit the samenon-asymptotic theoretical guarantees as their SGD counterparts. Second, weshow that our analysis framework extends seamlessly to nonlinear stochasticapproximation schemes that subsume Q-learning. Third, we prove that formulti-agent TD learning, one can achieve linear convergence speedups withrespect to the number of agents while communicating just $\tilde{O}(1)$ bitsper iteration. Notably, these are the first finite-time results in RL thataccount for general compression operators and error-feedback in tandem withlinear function approximation and Markovian sampling. Our proofs hinge on theconstruction of novel Lyapunov functions that capture the dynamics of a memoryvariable introduced by error-feedback.</description><author>Aritra Mitra, George J. Pappas, Hamed Hassani</author><pubDate>Tue, 04 Jun 2024 16:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00944v3</guid></item><item><title>IterMask2: Iterative Unsupervised Anomaly Segmentation via Spatial and Frequency Masking for Brain Lesions in MRI</title><link>http://arxiv.org/abs/2406.02422v1</link><description>Unsupervised anomaly segmentation approaches to pathology segmentation traina model on images of healthy subjects, that they define as the 'normal' datadistribution. At inference, they aim to segment any pathologies in new imagesas 'anomalies', as they exhibit patterns that deviate from those in 'normal'training data. Prevailing methods follow the 'corrupt-and-reconstruct'paradigm. They intentionally corrupt an input image, reconstruct it to followthe learned 'normal' distribution, and subsequently segment anomalies based onreconstruction error. Corrupting an input image, however, inevitably leads tosuboptimal reconstruction even of normal regions, causing false positives. Toalleviate this, we propose a novel iterative spatial mask-refining strategyIterMask2. We iteratively mask areas of the image, reconstruct them, and updatethe mask based on reconstruction error. This iterative process progressivelyadds information about areas that are confidently normal as per the model. Theincreasing content guides reconstruction of nearby masked areas, improvingreconstruction of normal tissue under these areas, reducing false positives. Wealso use high-frequency image content as an auxiliary input to provideadditional structural information for masked areas. This further improvesreconstruction error of normal in comparison to anomalous areas, facilitatingsegmentation of the latter. We conduct experiments on several brain lesiondatasets and demonstrate effectiveness of our method. Code is available at:https://github.com/ZiyunLiang/IterMasks2</description><author>Ziyun Liang, Xiaoqing Guo, J. Alison Noble, Konstantinos Kamnitsas</author><pubDate>Tue, 04 Jun 2024 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02422v1</guid></item><item><title>Representing Piecewise-Linear Functions by Functions with Minimal Arity</title><link>http://arxiv.org/abs/2406.02421v1</link><description>Any continuous piecewise-linear function $F\colon \mathbb{R}^{n}\to\mathbb{R}$ can be represented as a linear combination of $\max$ functions ofat most $n+1$ affine-linear functions. In our previous paper [``Representingpiecewise linear functions by functions with small arity'', AAECC, 2023], weshowed that this upper bound of $n+1$ arguments is tight. In the present paper,we extend this result by establishing a correspondence between the function $F$and the minimal number of arguments that are needed in any such decomposition.We show that the tessellation of the input space $\mathbb{R}^{n}$ induced bythe function $F$ has a direct connection to the number of arguments in the$\max$ functions.</description><author>Christoph Koutschan, Anton Ponomarchuk, Josef Schicho</author><pubDate>Tue, 04 Jun 2024 16:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02421v1</guid></item><item><title>Approximate Nearest Neighbor Search with Window Filters</title><link>http://arxiv.org/abs/2402.00943v2</link><description>We define and investigate the problem of $\textit{c-approximate windowsearch}$: approximate nearest neighbor search where each point in the datasethas a numeric label, and the goal is to find nearest neighbors to querieswithin arbitrary label ranges. Many semantic search problems, such as image anddocument search with timestamp filters, or product search with cost filters,are natural examples of this problem. We propose and theoretically analyze amodular tree-based framework for transforming an index that solves thetraditional c-approximate nearest neighbor problem into a data structure thatsolves window search. On standard nearest neighbor benchmark datasets equippedwith random label values, adversarially constructed embeddings, and imagesearch embeddings with real timestamps, we obtain up to a $75\times$ speedupover existing solutions at the same level of recall.</description><author>Joshua Engels, Benjamin Landrum, Shangdi Yu, Laxman Dhulipala, Julian Shun</author><pubDate>Tue, 04 Jun 2024 16:38:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00943v2</guid></item><item><title>Bringing motion taxonomies to continuous domains via GPLVM on hyperbolic manifolds</title><link>http://arxiv.org/abs/2210.01672v4</link><description>Human motion taxonomies serve as high-level hierarchical abstractions thatclassify how humans move and interact with their environment. They have provenuseful to analyse grasps, manipulation skills, and whole-body support poses.Despite substantial efforts devoted to design their hierarchy and underlyingcategories, their use remains limited. This may be attributed to the lack ofcomputational models that fill the gap between the discrete hierarchicalstructure of the taxonomy and the high-dimensional heterogeneous dataassociated to its categories. To overcome this problem, we propose to modeltaxonomy data via hyperbolic embeddings that capture the associatedhierarchical structure. We achieve this by formulating a novel Gaussian processhyperbolic latent variable model that incorporates the taxonomy structurethrough graph-based priors on the latent space and distance-preserving backconstraints. We validate our model on three different human motion taxonomiesto learn hyperbolic embeddings that faithfully preserve the original graphstructure. We show that our model properly encodes unseen data from existing ornew taxonomy categories, and outperforms its Euclidean and VAE-basedcounterparts. Finally, through proof-of-concept experiments, we show that ourmodel may be used to generate realistic trajectories between the learnedembeddings.</description><author>Noémie Jaquier, Leonel Rozo, Miguel González-Duque, Viacheslav Borovitskiy, Tamim Asfour</author><pubDate>Tue, 04 Jun 2024 16:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01672v4</guid></item><item><title>Nearly Minimax Optimal Regret for Multinomial Logistic Bandit</title><link>http://arxiv.org/abs/2405.09831v3</link><description>In this paper, we study the contextual multinomial logit (MNL) bandit problemin which a learning agent sequentially selects an assortment based oncontextual information, and user feedback follows an MNL choice model. Therehas been a significant discrepancy between lower and upper regret bounds,particularly regarding the feature dimension $d$ and the maximum assortmentsize $K$. Additionally, the variation in reward structures between these boundscomplicates the quest for optimality. Under uniform rewards, where all itemshave the same expected reward, we establish a regret lower bound of$\Omega(d\sqrt{\smash[b]{T/K}})$ and propose a constant-time algorithm,OFU-MNL+, that achieves a matching upper bound of$\tilde{O}(d\sqrt{\smash[b]{T/K}})$. Under non-uniform rewards, we prove alower bound of $\Omega(d\sqrt{T})$ and an upper bound of$\tilde{O}(d\sqrt{T})$, also achievable by OFU-MNL+. Our empirical studiessupport these theoretical findings. To the best of our knowledge, this is thefirst work in the contextual MNL bandit literature to prove minimax optimality-- for either uniform or non-uniform reward setting -- and to propose acomputationally efficient algorithm that achieves this optimality up tologarithmic factors.</description><author>Joongkyu Lee, Min-hwan Oh</author><pubDate>Tue, 04 Jun 2024 16:34:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09831v3</guid></item><item><title>LLMCRIT: Teaching Large Language Models to Use Criteria</title><link>http://arxiv.org/abs/2403.01069v2</link><description>Humans follow criteria when they execute tasks, and these criteria aredirectly used to assess the quality of task completion. Therefore, havingmodels learn to use criteria to provide feedback can help humans or models toperform tasks better. However, existing research in this field tends toconsider only a limited set of criteria or quality assessment aspects. To fillthis gap, we propose a general framework that enables large language models(LLMs) to use comprehensive criteria for a task in delivering natural languagefeedback on task execution. In particular, we present a model-in-the-loopframework that semi-automatically derives criteria from collected guidelinesfor different writing tasks and constructs in-context demonstrations for eachcriterion. We choose three tasks from real-world scenarios to operationalizethis idea: paper introduction writing, Python code writing, and Reddit postwriting, and evaluate our feedback generation framework using different LLMs.The results reveal the fine-grained effects of incorporating criteria anddemonstrations and provide valuable insights on how to teach LLMs to usecriteria more effectively.</description><author>Weizhe Yuan, Pengfei Liu, Matthias Gallé</author><pubDate>Tue, 04 Jun 2024 16:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01069v2</guid></item><item><title>ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning</title><link>http://arxiv.org/abs/2402.06737v2</link><description>Self-supervised Learning (SSL) has emerged as a powerful technique inpre-training deep learning models without relying on expensive annotatedlabels, instead leveraging embedded signals in unlabeled data. While SSL hasshown remarkable success in computer vision tasks through intuitive dataaugmentation, its application to graph-structured data poses challenges due tothe semantic-altering and counter-intuitive nature of graph augmentations.Addressing this limitation, this paper introduces a novel non-contrastive SSLapproach to Explicitly Generate a compositional Relation Graph (ExGRG) insteadof relying solely on the conventional augmentation-based implicit relationgraph. ExGRG offers a framework for incorporating prior domain knowledge andonline extracted information into the SSL invariance objective, drawinginspiration from the Laplacian Eigenmap and Expectation-Maximization (EM).Employing an EM perspective on SSL, our E-step involves relation graphgeneration to identify candidates to guide the SSL invariance objective, andM-step updates the model parameters by integrating the derived relationalinformation. Extensive experimentation on diverse node classification datasetsdemonstrates the superiority of our method over state-of-the-art techniques,affirming ExGRG as an effective adoption of SSL for graph representationlearning.</description><author>Mahdi Naseri, Mahdi Biparva</author><pubDate>Tue, 04 Jun 2024 16:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06737v2</guid></item><item><title>Semi-Supervised Learning guided by the Generalized Bayes Rule under Soft Revision</title><link>http://arxiv.org/abs/2405.15294v2</link><description>We provide a theoretical and computational investigation of the Gamma-Maximinmethod with soft revision, which was recently proposed as a robust criterionfor pseudo-label selection (PLS) in semi-supervised learning. Opposed totraditional methods for PLS we use credal sets of priors ("generalized Bayes")to represent the epistemic modeling uncertainty. These latter are then updatedby the Gamma-Maximin method with soft revision. We eventually selectpseudo-labeled data that are most likely in light of the least favorabledistribution from the so updated credal set. We formalize the task of findingoptimal pseudo-labeled data w.r.t. the Gamma-Maximin method with soft revisionas an optimization problem. A concrete implementation for the class of logisticmodels then allows us to compare the predictive power of the method withcompeting approaches. It is observed that the Gamma-Maximin method with softrevision can achieve very promising results, especially when the proportion oflabeled data is low.</description><author>Stefan Dietrich, Julian Rodemann, Christoph Jansen</author><pubDate>Tue, 04 Jun 2024 16:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15294v2</guid></item><item><title>A Multi-Perspective Analysis of Memorization in Large Language Models</title><link>http://arxiv.org/abs/2405.11577v4</link><description>Large Language Models (LLMs), trained on massive corpora with billions ofparameters, show unprecedented performance in various fields. Though surprisedby their excellent performances, researchers also noticed some specialbehaviors of those LLMs. One of those behaviors is memorization, in which LLMscan generate the same content used to train them. Though previous research hasdiscussed memorization, the memorization of LLMs still lacks explanation,especially the cause of memorization and the dynamics of generating them. Inthis research, we comprehensively discussed memorization from variousperspectives and extended the discussion scope to not only just the memorizedcontent but also less and unmemorized content. Through various studies, wefound that: (1) Through experiments, we revealed the relation of memorizationbetween model size, continuation size, and context size. Further, we showed howunmemorized sentences transition to memorized sentences. (2) Through embeddinganalysis, we showed the distribution and decoding dynamics across model size inembedding space for sentences with different memorization scores. The n-gramstatistics analysis presents d (3) An analysis over n-gram and entropy decodingdynamics discovered a boundary effect when the model starts to generatememorized sentences or unmemorized sentences. (4)We trained a Transformer modelto predict the memorization of different models, showing that it is possible topredict memorizations by context.</description><author>Bowen Chen, Namgi Han, Yusuke Miyao</author><pubDate>Tue, 04 Jun 2024 16:28:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11577v4</guid></item><item><title>Improved Modelling of Federated Datasets using Mixtures-of-Dirichlet-Multinomials</title><link>http://arxiv.org/abs/2406.02416v1</link><description>In practice, training using federated learning can be orders of magnitudeslower than standard centralized training. This severely limits the amount ofexperimentation and tuning that can be done, making it challenging to obtaingood performance on a given task. Server-side proxy data can be used to runtraining simulations, for instance for hyperparameter tuning. This can greatlyspeed up the training pipeline by reducing the number of tuning runs to beperformed overall on the true clients. However, it is challenging to ensurethat these simulations accurately reflect the dynamics of the real federatedtraining. In particular, the proxy data used for simulations often comes as asingle centralized dataset without a partition into distinct clients, andpartitioning this data in a naive way can lead to simulations that poorlyreflect real federated training. In this paper we address the challenge of howto partition centralized data in a way that reflects the statisticalheterogeneity of the true federated clients. We propose a fully federated,theoretically justified, algorithm that efficiently learns the distribution ofthe true clients and observe improved server-side simulations when using theinferred distribution to create simulated clients from the centralized data.</description><author>Jonathan Scott, Áine Cahill</author><pubDate>Tue, 04 Jun 2024 16:27:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02416v1</guid></item><item><title>Motion-aware Latent Diffusion Models for Video Frame Interpolation</title><link>http://arxiv.org/abs/2404.13534v2</link><description>With the advancement of AIGC, video frame interpolation (VFI) has become acrucial component in existing video generation frameworks, attractingwidespread research interest. For the VFI task, the motion estimation betweenneighboring frames plays a crucial role in avoiding motion ambiguity. However,existing VFI methods always struggle to accurately predict the motioninformation between consecutive frames, and this imprecise estimation leads toblurred and visually incoherent interpolated frames. In this paper, we proposea novel diffusion framework, motion-aware latent diffusion models (MADiff),which is specifically designed for the VFI task. By incorporating motion priorsbetween the conditional neighboring frames with the target interpolated framepredicted throughout the diffusion sampling procedure, MADiff progressivelyrefines the intermediate outcomes, culminating in generating both visuallysmooth and realistic results. Extensive experiments conducted on benchmarkdatasets demonstrate that our method achieves state-of-the-art performancesignificantly outperforming existing approaches, especially under challengingscenarios involving dynamic textures with complex motion.</description><author>Zhilin Huang, Yijie Yu, Ling Yang, Chujun Qin, Bing Zheng, Xiawu Zheng, Zikun Zhou, Yaowei Wang, Wenming Yang</author><pubDate>Tue, 04 Jun 2024 16:23:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13534v2</guid></item><item><title>Accelerated Variance-Reduced Forward-Reflected Methods for Root-Finding Problems</title><link>http://arxiv.org/abs/2406.02413v1</link><description>We propose a novel class of Nesterov's stochastic acceleratedforward-reflected-based methods with variance reduction to solve root-findingproblems under $\frac{1}{L}$-co-coerciveness. Our algorithm is single-loop andleverages a new family of unbiased variance-reduced estimators specificallydesigned for root-finding problems. It achieves both $\mathcal{O}(L^2/k^2)$ and$o(1/k^2)$-last-iterate convergence rates in terms of expected operator squarednorm, where $k$ denotes the iteration counter. We instantiate our framework fortwo prominent estimators: SVRG and SAGA. By an appropriate choice ofparameters, both variants attain an oracle complexity of $\mathcal{O}( n +Ln^{2/3}\epsilon^{-1})$ to reach an $\epsilon$-solution, where $n$ representsthe number of summands in the finite-sum operator. Furthermore, under$\mu$-strong quasi-monotonicity, our method achieves a linear convergence rateand an oracle complexity of $\mathcal{O}(n+ \kappan^{2/3}\log(\epsilon^{-1}))$, where $\kappa := \frac{L}{\mu}$. We extend ourapproach to solve a class of finite-sum monotone inclusions, demonstrating thatour schemes retain the same theoretical guarantees as in the equation setting.Finally, numerical experiments validate our algorithms and demonstrate theirpromising performance compared to state-of-the-art methods.</description><author>Quoc Tran-Dinh</author><pubDate>Tue, 04 Jun 2024 16:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02413v1</guid></item><item><title>Fair Wasserstein Coresets</title><link>http://arxiv.org/abs/2311.05436v3</link><description>Data distillation and coresets have emerged as popular approaches to generatea smaller representative set of samples for downstream learning tasks to handlelarge-scale datasets. At the same time, machine learning is being increasinglyapplied to decision-making processes at a societal level, making it imperativefor modelers to address inherent biases towards subgroups present in the data.While current approaches focus on creating fair synthetic representativesamples by optimizing local properties relative to the original samples, theirimpact on downstream learning processes has yet to be explored. In this work,we present fair Wasserstein coresets (FWC), a novel coreset approach whichgenerates fair synthetic representative samples along with sample-level weightsto be used in downstream learning tasks. FWC uses an efficient majorityminimization algorithm to minimize the Wasserstein distance between theoriginal dataset and the weighted synthetic samples while enforcing demographicparity. We show that an unconstrained version of FWC is equivalent to Lloyd'salgorithm for k-medians and k-means clustering. Experiments conducted on bothsynthetic and real datasets show that FWC: (i) achieves a competitivefairness-utility tradeoff in downstream models compared to existing approaches,(ii) improves downstream fairness when added to the existing training data and(iii) can be used to reduce biases in predictions from large language models(GPT-3.5 and GPT-4).</description><author>Zikai Xiong, Niccolò Dalmasso, Shubham Sharma, Freddy Lecue, Daniele Magazzeni, Vamsi K. Potluru, Tucker Balch, Manuela Veloso</author><pubDate>Tue, 04 Jun 2024 16:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05436v3</guid></item><item><title>Decoupling of neural network calibration measures</title><link>http://arxiv.org/abs/2406.02411v1</link><description>A lot of effort is currently invested in safeguarding autonomous drivingsystems, which heavily rely on deep neural networks for computer vision. Weinvestigate the coupling of different neural network calibration measures witha special focus on the Area Under the Sparsification Error curve (AUSE) metric.We elaborate on the well-known inconsistency in determining optimal calibrationusing the Expected Calibration Error (ECE) and we demonstrate similar issuesfor the AUSE, the Uncertainty Calibration Score (UCS), as well as theUncertainty Calibration Error (UCE). We conclude that the current methodologiesleave a degree of freedom, which prevents a unique model calibration for thehomologation of safety-critical functionalities. Furthermore, we propose theAUSE as an indirect measure for the residual uncertainty, which is irreduciblefor a fixed network architecture and is driven by the stochasticity in theunderlying data generation process (aleatoric contribution) as well as thelimitation in the hypothesis space (epistemic contribution).</description><author>Dominik Werner Wolf, Prasannavenkatesh Balaji, Alexander Braun, Markus Ulrich</author><pubDate>Tue, 04 Jun 2024 16:21:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02411v1</guid></item></channel></rss>