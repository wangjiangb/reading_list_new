<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 24 Jul 2024 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description</title><link>http://arxiv.org/abs/2407.15850v1</link><description>Our objective is to generate Audio Descriptions (ADs) for both movies and TVseries in a training-free manner. We use the power of off-the-shelfVisual-Language Models (VLMs) and Large Language Models (LLMs), and developvisual and text prompting strategies for this task. Our contributions arethree-fold: (i) We demonstrate that a VLM can successfully name and refer tocharacters if directly prompted with character information through visualindications without requiring any fine-tuning; (ii) A two-stage process isdeveloped to generate ADs, with the first stage asking the VLM tocomprehensively describe the video, followed by a second stage utilising a LLMto summarise dense textual information into one succinct AD sentence; (iii) Anew dataset for TV audio description is formulated. Our approach, namedAutoAD-Zero, demonstrates outstanding performance (even competitive with somemodels fine-tuned on ground truth ADs) in AD generation for both movies and TVseries, achieving state-of-the-art CRITIC scores.</description><author>Junyu Xie, Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman</author><pubDate>Mon, 22 Jul 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15850v1</guid></item><item><title>WayEx: Waypoint Exploration using a Single Demonstration</title><link>http://arxiv.org/abs/2407.15849v1</link><description>We propose WayEx, a new method for learning complex goal-conditioned roboticstasks from a single demonstration. Our approach distinguishes itself fromexisting imitation learning methods by demanding fewer expert examples andeliminating the need for information about the actions taken during thedemonstration. This is accomplished by introducing a new reward function andemploying a knowledge expansion technique. We demonstrate the effectiveness ofWayEx, our waypoint exploration strategy, across six diverse tasks, showcasingits applicability in various environments. Notably, our method significantlyreduces training time by 50% as compared to traditional reinforcement learningmethods. WayEx obtains a higher reward than existing imitation learning methodsgiven only a single demonstration. Furthermore, we demonstrate its success intackling complex environments where standard approaches fall short. Moreinformation is available at: https://waypoint-ex.github.io.</description><author>Mara Levy, Nirat Saini, Abhinav Shrivastava</author><pubDate>Mon, 22 Jul 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15849v1</guid></item><item><title>BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes</title><link>http://arxiv.org/abs/2407.15848v1</link><description>While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality,their protracted training duration remains a limitation. Generalizable andMVS-based NeRFs, although capable of mitigating training time, often incurtradeoffs in quality. This paper presents a novel approach called BoostMVSNeRFsto enhance the rendering quality of MVS-based NeRFs in large-scale scenes. Wefirst identify limitations in MVS-based NeRF methods, such as restrictedviewport coverage and artifacts due to limited input views. Then, we addressthese limitations by proposing a new method that selects and combines multiplecost volumes during volume rendering. Our method does not require training andcan adapt to any MVS-based NeRF methods in a feed-forward fashion to improverendering quality. Furthermore, our approach is also end-to-end trainable,allowing fine-tuning on specific scenes. We demonstrate the effectiveness ofour method through experiments on large-scale datasets, showing significantrendering quality improvements in large-scale scenes and unbounded outdoorscenarios. We release the source code of BoostMVSNeRFs athttps://su-terry.github.io/BoostMVSNeRFs/.</description><author>Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, Yu-Lun Liu</author><pubDate>Mon, 22 Jul 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15848v1</guid></item><item><title>LLMmap: Fingerprinting For Large Language Models</title><link>http://arxiv.org/abs/2407.15847v1</link><description>We introduce LLMmap, a first-generation fingerprinting attack targeted atLLM-integrated applications. LLMmap employs an active fingerprinting approach,sending carefully crafted queries to the application and analyzing theresponses to identify the specific LLM model in use. With as few as 8interactions, LLMmap can accurately identify LLMs with over 95% accuracy. Moreimportantly, LLMmap is designed to be robust across different applicationlayers, allowing it to identify LLMs operating under various system prompts,stochastic sampling hyperparameters, and even complex generation frameworkssuch as RAG or Chain-of-Thought.</description><author>Dario Pasquini, Evgenios M. Kornaropoulos, Giuseppe Ateniese</author><pubDate>Mon, 22 Jul 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15847v1</guid></item><item><title>Multicell-Fold: geometric learning in folding multicellular life</title><link>http://arxiv.org/abs/2407.07055v2</link><description>During developmental processes such as embryogenesis, how a group of cellsfold into specific structures, is a central question in biology that defineshow living organisms form. Establishing tissue-level morphology criticallyrelies on how every single cell decides to position itself relative to itsneighboring cells. Despite its importance, it remains a major challenge tounderstand and predict the behavior of every cell within the living tissue overtime during such intricate processes. To tackle this question, we propose ageometric deep learning model that can predict multicellular folding andembryogenesis, accurately capturing the highly convoluted spatial interactionsamong cells. We demonstrate that multicellular data can be represented withboth granular and foam-like physical pictures through a unified graph datastructure, considering both cellular interactions and cell junction networks.We successfully use our model to achieve two important tasks, interpretable 4-Dmorphological sequence alignment, and predicting local cell rearrangementsbefore they occur at single-cell resolution. Furthermore, using an activationmap and ablation studies, we demonstrate that cell geometries and cell junctionnetworks together regulate local cell rearrangement which is critical forembryo morphogenesis. This approach provides a novel paradigm to studymorphogenesis, highlighting a unified data structure and harnessing the powerof geometric deep learning to accurately model the mechanisms and behaviors ofcells during development. It offers a pathway toward creating a unified dynamicmorphological atlas for a variety of developmental processes such asembryogenesis.</description><author>Haiqian Yang, Anh Q. Nguyen, Dapeng Bi, Markus J. Buehler, Ming Guo</author><pubDate>Mon, 22 Jul 2024 17:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07055v2</guid></item><item><title>Reconstructing Training Data From Real World Models Trained with Transfer Learning</title><link>http://arxiv.org/abs/2407.15845v1</link><description>Current methods for reconstructing training data from trained classifiers arerestricted to very small models, limited training set sizes, and low-resolutionimages. Such restrictions hinder their applicability to real-world scenarios.In this paper, we present a novel approach enabling data reconstruction inrealistic settings for models trained on high-resolution images. Our methodadapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios --specifically, targeting models trained via transfer learning over imageembeddings of large pre-trained models like DINO-ViT and CLIP. Our work employsdata reconstruction in the embedding space rather than in the image space,showcasing its applicability beyond visual data. Moreover, we introduce a novelclustering-based method to identify good reconstructions from thousands ofcandidates. This significantly improves on previous works that relied onknowledge of the training set to identify good reconstructed images. Ourfindings shed light on a potential privacy risk for data leakage from modelstrained using transfer learning.</description><author>Yakir Oz, Gilad Yehudai, Gal Vardi, Itai Antebi, Michal Irani, Niv Haim</author><pubDate>Mon, 22 Jul 2024 17:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15845v1</guid></item><item><title>HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning</title><link>http://arxiv.org/abs/2407.15844v1</link><description>Predicting camera-space hand meshes from single RGB images is crucial forenabling realistic hand interactions in 3D virtual and augmented worlds.Previous work typically divided the task into two stages: given a cropped imageof the hand, predict meshes in relative coordinates, followed by lifting thesepredictions into camera space in a separate and independent stage, oftenresulting in the loss of valuable contextual and scale information. To preventthe loss of these cues, we propose unifying these two stages into an end-to-endsolution that addresses the 2D-3D correspondence problem. This solution enablesback-propagation from camera space outputs to the rest of the network through anew differentiable global positioning module. We also introduce an imagerectification step that harmonizes both the training dataset and the inputimage as if they were acquired with the same camera, helping to alleviate theinherent scale-depth ambiguity of the problem. We validate the effectiveness ofour framework in evaluations against several baselines and state-of-the-artapproaches across three public benchmarks.</description><author>Eugene Valassakis, Guillermo Garcia-Hernando</author><pubDate>Mon, 22 Jul 2024 17:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15844v1</guid></item><item><title>CarFormer: Self-Driving with Learned Object-Centric Representations</title><link>http://arxiv.org/abs/2407.15843v1</link><description>The choice of representation plays a key role in self-driving. Bird's eyeview (BEV) representations have shown remarkable performance in recent years.In this paper, we propose to learn object-centric representations in BEV todistill a complex scene into more actionable information for self-driving. Wefirst learn to place objects into slots with a slot attention model on BEVsequences. Based on these object-centric representations, we then train atransformer to learn to drive as well as reason about the future of othervehicles. We found that object-centric slot representations outperform bothscene-level and object-level approaches that use the exact attributes ofobjects. Slot representations naturally incorporate information about objectsfrom their spatial and temporal context such as position, heading, and speedwithout explicitly providing it. Our model with slots achieves an increasedcompletion rate of the provided routes and, consequently, a higher drivingscore, with a lower variance across multiple runs, affirming slots as areliable alternative in object-centric approaches. Additionally, we validateour model's performance as a world model through forecasting experiments,demonstrating its capability to predict future slot representations accurately.The code and the pre-trained models can be found athttps://kuis-ai.github.io/CarFormer/.</description><author>Shadi Hamdan, Fatma Güney</author><pubDate>Mon, 22 Jul 2024 17:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15843v1</guid></item><item><title>Artist: Aesthetically Controllable Text-Driven Stylization without Training</title><link>http://arxiv.org/abs/2407.15842v1</link><description>Diffusion models entangle content and style generation during the denoisingprocess, leading to undesired content modification when directly applied tostylization tasks. Existing methods struggle to effectively control thediffusion model to meet the aesthetic-level requirements for stylization. Inthis paper, we introduce \textbf{Artist}, a training-free approach thataesthetically controls the content and style generation of a pretraineddiffusion model for text-driven stylization. Our key insight is to disentanglethe denoising of content and style into separate diffusion processes whilesharing information between them. We propose simple yet effective content andstyle control methods that suppress style-irrelevant content generation,resulting in harmonious stylization results. Extensive experiments demonstratethat our method excels at achieving aesthetic-level stylization requirements,preserving intricate details in the content image and aligning well with thestyle prompt. Furthermore, we showcase the highly controllability of thestylization strength from various perspectives. Code will be released, projecthome page: https://DiffusionArtist.github.io</description><author>Ruixiang Jiang, Changwen Chen</author><pubDate>Mon, 22 Jul 2024 17:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15842v1</guid></item><item><title>SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models</title><link>http://arxiv.org/abs/2407.15841v1</link><description>We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free videolarge language model (LLM) that can jointly capture the detailed spatialsemantics and long-range temporal context without exceeding the token budget ofcommonly used LLMs. This is realized by using a two-stream SlowFast design ofinputs for Video LLMs to aggregate features from sampled video frames in aneffective way. Specifically, the Slow pathway extracts features at a low framerate while keeping as many spatial details as possible (e.g., with 24x24tokens), and the Fast pathway operates on a high frame rate but uses a largerspatial pooling stride (e.g., downsampling 6x) to focus on the motion cues. Asa result, this design allows us to adequately capture both spatial and temporalfeatures that are beneficial for understanding details along the video.Experimental results show that SF-LLaVA outperforms existing training-freemethods on a wide range of video tasks. On some benchmarks, it achievescomparable or even better performance compared to state-of-the-art Video LLMsthat are fine-tuned on video datasets.</description><author>Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, Afshin Dehghan</author><pubDate>Mon, 22 Jul 2024 17:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15841v1</guid></item><item><title>Machine Learning Assisted Adjustment Boosts Efficiency of Exact Inference in Randomized Controlled Trials</title><link>http://arxiv.org/abs/2403.03058v2</link><description>In this work, we proposed a novel inferential procedure assisted by machinelearning based adjustment for randomized control trials. The method wasdeveloped under the Rosenbaum's framework of exact tests in randomizedexperiments with covariate adjustments. Through extensive simulationexperiments, we showed the proposed method can robustly control the type Ierror and can boost the statistical efficiency for a randomized controlledtrial (RCT). This advantage was further demonstrated in a real-world example.The simplicity, flexibility, and robustness of the proposed method makes it acompetitive candidate as a routine inference procedure for RCTs, especiallywhen nonlinear association or interaction among covariates is expected. Itsapplication may remarkably reduce the required sample size and cost of RCTs,such as phase III clinical trials.</description><author>Han Yu, Alan D. Hutson, Xiaoyi Ma</author><pubDate>Mon, 22 Jul 2024 17:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03058v2</guid></item><item><title>Importance Sampling-Guided Meta-Training for Intelligent Agents in Highly Interactive Environments</title><link>http://arxiv.org/abs/2407.15839v1</link><description>Training intelligent agents to navigate highly interactive environmentspresents significant challenges. While guided meta reinforcement learning (RL)approach that first trains a guiding policy to train the ego agent has proveneffective in improving generalizability across various levels of interaction,the state-of-the-art method tends to be overly sensitive to extreme cases,impairing the agents' performance in the more common scenarios. This studyintroduces a novel training framework that integrates guided meta RL withimportance sampling (IS) to optimize training distributions for navigatinghighly interactive driving scenarios, such as T-intersections. Unliketraditional methods that may underrepresent critical interactions oroveremphasize extreme cases during training, our approach strategically adjuststhe training distribution towards more challenging driving behaviors using ISproposal distributions and applies the importance ratio to de-bias the result.By estimating a naturalistic distribution from real-world datasets andemploying a mixture model for iterative training refinements, the frameworkensures a balanced focus across common and extreme driving scenarios.Experiments conducted with both synthetic dataset and T-intersection scenariosfrom the InD dataset demonstrate not only accelerated training but alsoimprovement in agent performance under naturalistic conditions, showcasing theefficacy of combining IS with meta RL in training reliable autonomous agentsfor highly interactive navigation tasks.</description><author>Mansur Arief, Mike Timmerman, Jiachen Li, David Isele, Mykel J Kochenderfer</author><pubDate>Mon, 22 Jul 2024 17:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15839v1</guid></item><item><title>Foundation Models for Autonomous Robots in Unstructured Environments</title><link>http://arxiv.org/abs/2407.14296v2</link><description>Automating activities through robots in unstructured environments, such asconstruction sites, has been a long-standing desire. However, the high degreeof unpredictable events in these settings has resulted in far less adoptioncompared to more structured settings, such as manufacturing, where robots canbe hard-coded or trained on narrowly defined datasets. Recently, pretrainedfoundation models, such as Large Language Models (LLMs), have demonstratedsuperior generalization capabilities by providing zero-shot solutions forproblems do not present in the training data, proposing them as a potentialsolution for introducing robots to unstructured environments. To this end, thisstudy investigates potential opportunities and challenges of pretrainedfoundation models from a multi-dimensional perspective. The studysystematically reviews application of foundation models in two field of roboticand unstructured environment and then synthesized them with deliberative actingtheory. Findings showed that linguistic capabilities of LLMs have been utilizedmore than other features for improving perception in human-robot interactions.On the other hand, findings showed that the use of LLMs demonstrated moreapplications in project management and safety in construction, and naturalhazard detection in disaster management. Synthesizing these findings, welocated the current state-of-the-art in this field on a five-level scale ofautomation, placing them at conditional automation. This assessment was thenused to envision future scenarios, challenges, and solutions toward autonomoussafe unstructured environments. Our study can be seen as a benchmark to trackour progress toward that future.</description><author>Hossein Naderi, Alireza Shojaei, Lifu Huang</author><pubDate>Mon, 22 Jul 2024 17:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14296v2</guid></item><item><title>MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with Extensive Diversity</title><link>http://arxiv.org/abs/2407.15838v1</link><description>Despite the effectiveness of vision-language supervised fine-tuning inenhancing the performance of Vision Large Language Models (VLLMs). However,existing visual instruction tuning datasets include the following limitations:(1) Instruction annotation quality: despite existing VLLMs exhibiting strongperformance, instructions generated by those advanced VLLMs may still sufferfrom inaccuracies, such as hallucinations. (2) Instructions and imagediversity: the limited range of instruction types and the lack of diversity inimage data may impact the model's ability to generate diversified and closer toreal-world scenarios outputs. To address these challenges, we construct ahigh-quality, diverse visual instruction tuning dataset MMInstruct, whichconsists of 973K instructions from 24 domains. There are four instructiontypes: Judgement, Multiple-Choice, Long Visual Question Answering and ShortVisual Question Answering. To construct MMInstruct, we propose an instructiongeneration data engine that leverages GPT-4V, GPT-3.5, and manual correction.Our instruction generation engine enables semi-automatic, low-cost, andmulti-domain instruction generation at 1/6 the cost of manual construction.Through extensive experiment validation and ablation experiments, wedemonstrate that MMInstruct could significantly improve the performance ofVLLMs, e.g., the model fine-tuning on MMInstruct achieves new state-of-the-artperformance on 10 out of 12 benchmarks. The code and data shall be available athttps://github.com/yuecao0119/MMInstruct.</description><author>Yangzhou Liu, Yue Cao, Zhangwei Gao, Weiyun Wang, Zhe Chen, Wenhai Wang, Hao Tian, Lewei Lu, Xizhou Zhu, Tong Lu, Yu Qiao, Jifeng Dai</author><pubDate>Mon, 22 Jul 2024 17:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15838v1</guid></item><item><title>Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning</title><link>http://arxiv.org/abs/2407.15837v1</link><description>Masked Image Modeling (MIM) has emerged as a promising method for derivingvisual representations from unlabeled image data by predicting missing pixelsfrom masked portions of images. It excels in region-aware learning and providesstrong initializations for various tasks, but struggles to capture high-levelsemantics without further supervised fine-tuning, likely due to the low-levelnature of its pixel reconstruction objective. A promising yet unrealizedframework is learning representations through masked reconstruction in latentspace, combining the locality of MIM with the high-level targets. However, thisapproach poses significant training challenges as the reconstruction targetsare learned in conjunction with the model, potentially leading to trivial orsuboptimal solutions.Our study is among the first to thoroughly analyze andaddress the challenges of such framework, which we refer to as Latent MIM.Through a series of carefully designed experiments and extensive analysis, weidentify the source of these challenges, including representation collapsingfor joint online/target optimization, learning objectives, the high regioncorrelation in latent space and decoding conditioning. By sequentiallyaddressing these issues, we demonstrate that Latent MIM can indeed learnhigh-level representations while retaining the benefits of MIM models.</description><author>Yibing Wei, Abhinav Gupta, Pedro Morgado</author><pubDate>Mon, 22 Jul 2024 17:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15837v1</guid></item><item><title>NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo</title><link>http://arxiv.org/abs/2405.12057v2</link><description>In this work we present a novel multi-view photometric stereo (MVPS) method.Like many works in 3D reconstruction we are leveraging neural shaperepresentations and learnt renderers. However, our work differs from thestate-of-the-art multi-view PS methods such as PS-NeRF or Supernormal in thatwe explicitly leverage per-pixel intensity renderings rather than relyingmainly on estimated normals. We model point light attenuation and explicitly raytrace cast shadows inorder to best approximate the incoming radiance for each point. The estimatedincoming radiance is used as input to a fully neural material renderer thatuses minimal prior assumptions and it is jointly optimised with the surface.Estimated normals and segmentation maps are also incorporated in order tomaximise the surface accuracy. Our method is among the first (along with Supernormal) to outperform theclassical MVPS approach proposed by the DiLiGenT-MV benchmark and achievesaverage 0.2mm Chamfer distance for objects imaged at approx 1.5m distance awaywith approximate 400x400 resolution. Moreover, our method shows high robustnessto the sparse MVPS setup (6 views, 6 lights) greatly outperforming the SOTAcompetitor (0.38mm vs 0.61mm), illustrating the importance of neural renderingin multi-view photometric stereo.</description><author>Fotios Logothetis, Ignas Budvytis, Roberto Cipolla</author><pubDate>Mon, 22 Jul 2024 17:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12057v2</guid></item><item><title>Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers</title><link>http://arxiv.org/abs/2406.18451v2</link><description>Despite extensive research on adversarial training strategies to improverobustness, the decisions of even the most robust deep learning models canstill be quite sensitive to imperceptible perturbations, creating serious riskswhen deploying them for high-stakes real-world applications. While detectingsuch cases may be critical, evaluating a model's vulnerability at aper-instance level using adversarial attacks is computationally too intensiveand unsuitable for real-time deployment scenarios. The input space margin isthe exact score to detect non-robust samples and is intractable for deep neuralnetworks. This paper introduces the concept of margin consistency -- a propertythat links the input space margins and the logit margins in robust models --for efficient detection of vulnerable samples. First, we establish that marginconsistency is a necessary and sufficient condition to use a model's logitmargin as a score for identifying non-robust samples. Next, throughcomprehensive empirical analysis of various robustly trained models on CIFAR10and CIFAR100 datasets, we show that they indicate strong margin consistencywith a strong correlation between their input space margins and the logitmargins. Then, we show that we can effectively use the logit margin toconfidently detect brittle decisions with such models and accurately estimaterobust accuracy on an arbitrarily large test set by estimating the inputmargins only on a small subset. Finally, we address cases where the model isnot sufficiently margin-consistent by learning a pseudo-margin from the featurerepresentation. Our findings highlight the potential of leveraging deeprepresentations to efficiently assess adversarial vulnerability in deploymentscenarios.</description><author>Jonas Ngnawé, Sabyasachi Sahoo, Yann Pequignot, Frédéric Precioso, Christian Gagné</author><pubDate>Mon, 22 Jul 2024 17:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18451v2</guid></item><item><title>dMel: Speech Tokenization made Simple</title><link>http://arxiv.org/abs/2407.15835v1</link><description>Large language models have revolutionized natural language processing byleveraging self-supervised pretraining on vast textual data. Inspired by thissuccess, researchers have investigated complicated speech tokenization methodsto discretize continuous speech signals so that language modeling techniquescan be applied to speech data. However, existing approaches either modelsemantic tokens, potentially losing acoustic information, or model acoustictokens, risking the loss of semantic information. Having multiple token typesalso complicates the architecture and requires additional pretraining. Here weshow that discretizing mel-filterbank channels into discrete intensity binsproduces a simple representation (dMel), that performs better than otherexisting speech tokenization methods. Using a transformer decoder-onlyarchitecture for speech-text modeling, we comprehensively evaluate differentspeech tokenization methods on speech recognition (ASR), speech synthesis(TTS). Our results demonstrate the effectiveness of dMel in achieving highperformance on both tasks within a unified framework, paving the way forefficient and effective joint modeling of speech and text.</description><author>He Bai, Tatiana Likhomanenko, Ruixiang Zhang, Zijin Gu, Zakaria Aldeneh, Navdeep Jaitly</author><pubDate>Mon, 22 Jul 2024 17:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15835v1</guid></item><item><title>NV-Retriever: Improving text embedding models with effective hard-negative mining</title><link>http://arxiv.org/abs/2407.15831v1</link><description>Text embedding models have been popular for information retrievalapplications such as semantic search and Question-Answering systems based onRetrieval-Augmented Generation (RAG). Those models are typically Transformermodels that are fine-tuned with contrastive learning objectives. Many papersintroduced new embedding model architectures and training approaches, however,one of the key ingredients, the process of mining negative passages, remainspoorly explored or described. One of the challenging aspects of fine-tuningembedding models is the selection of high quality hard-negative passages forcontrastive learning. In this paper we propose a family of positive-awaremining methods that leverage the positive relevance score for more effectivefalse negatives removal. We also provide a comprehensive ablation study onhard-negative mining methods over their configurations, exploring differentteacher and base models. We demonstrate the efficacy of our proposed methods byintroducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval(BEIR) benchmark and 0.65 points higher than previous methods. The model placed1st when it was published to MTEB Retrieval on July 07, 2024.</description><author>Gabriel de Souza P. Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, Even Oldridge</author><pubDate>Mon, 22 Jul 2024 17:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15831v1</guid></item><item><title>ACEGEN: Reinforcement learning of generative chemical agents for drug discovery</title><link>http://arxiv.org/abs/2405.04657v3</link><description>In recent years, reinforcement learning (RL) has emerged as a valuable toolin drug design, offering the potential to propose and optimize molecules withdesired properties. However, striking a balance between capabilities,flexibility, reliability, and efficiency remains challenging due to thecomplexity of advanced RL algorithms and the significant reliance onspecialized code. In this work, we introduce ACEGEN, a comprehensive andstreamlined toolkit tailored for generative drug design, built using TorchRL, amodern RL library that offers thoroughly tested reusable components. Wevalidate ACEGEN by benchmarking against other published generative modelingalgorithms and show comparable or improved performance. We also show examplesof ACEGEN applied in multiple drug discovery case studies. ACEGEN is accessibleat \url{https://github.com/acellera/acegen-open} and available for use underthe MIT license.</description><author>Albert Bou, Morgan Thomas, Sebastian Dittert, Carles Navarro Ramírez, Maciej Majewski, Ye Wang, Shivam Patel, Gary Tresadern, Mazen Ahmad, Vincent Moens, Woody Sherman, Simone Sciabola, Gianni De Fabritiis</author><pubDate>Mon, 22 Jul 2024 17:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04657v3</guid></item><item><title>J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling</title><link>http://arxiv.org/abs/2407.15828v1</link><description>Spoken dialogue plays a crucial role in human-AI interactions, necessitatingdialogue-oriented spoken language models (SLMs). To develop versatile SLMs,large-scale and diverse speech datasets are essential. Additionally, to ensurehiqh-quality speech generation, the data must be spontaneous like in-wild dataand must be acoustically clean with noise removed. Despite the critical need,no open-source corpus meeting all these criteria has been available. This studyaddresses this gap by constructing and releasing a large-scale spoken dialoguecorpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publiclyaccessible. Furthermore, this paper presents a language-independent method forcorpus construction and describes experiments on dialogue generation using SLMstrained on J-CHAT. Experimental results indicate that the collected data frommultiple domains by our method improve the naturalness and meaningfulness ofdialogue generation.</description><author>Wataru Nakata, Kentaro Seki, Hitomi Yanaka, Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari</author><pubDate>Mon, 22 Jul 2024 17:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15828v1</guid></item><item><title>Uncertainty Quantification and Propagation in Surrogate-based Bayesian Inference</title><link>http://arxiv.org/abs/2312.05153v2</link><description>Surrogate models are statistical or conceptual approximations for morecomplex simulation models. In this context, it is crucial to propagate theuncertainty induced by limited simulation budget and surrogate approximationerror to predictions, inference, and subsequent decision-relevant quantities.However, quantifying and then propagating the uncertainty of surrogates isusually limited to special analytic cases or is otherwise computationally veryexpensive. In this paper, we propose a framework enabling a scalable, Bayesianapproach to surrogate modeling with thorough uncertainty quantification,propagation, and validation. Specifically, we present three methods forBayesian inference with surrogate models given measurement data. This is a taskwhere the propagation of surrogate uncertainty is especially relevant, becausefailing to account for it may lead to biased and/or overconfident estimates ofthe parameters of interest. We showcase our approach in three detailed casestudies for linear and nonlinear real-world modeling scenarios. Uncertaintypropagation in surrogate models enables more reliable and safe approximation ofexpensive simulators and will therefore be useful in various fields ofapplications.</description><author>Philipp Reiser, Javier Enrique Aguilar, Anneli Guthke, Paul-Christian Bürkner</author><pubDate>Mon, 22 Jul 2024 17:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05153v2</guid></item><item><title>On shallow planning under partial observability</title><link>http://arxiv.org/abs/2407.15820v1</link><description>Formulating a real-world problem under the Reinforcement Learning frameworkinvolves non-trivial design choices, such as selecting a discount factor forthe learning objective (discounted cumulative rewards), which articulates theplanning horizon of the agent. This work investigates the impact of thediscount factor on the biasvariance trade-off given structural parameters ofthe underlying Markov Decision Process. Our results support the idea that ashorter planning horizon might be beneficial, especially under partialobservability.</description><author>Randy Lefebvre, Audrey Durand</author><pubDate>Mon, 22 Jul 2024 17:34:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15820v1</guid></item><item><title>Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight</title><link>http://arxiv.org/abs/2407.15819v1</link><description>This paper introduces Chain-of-Sight, a vision-language bridge module thataccelerates the pre-training of Multimodal Large Language Models (MLLMs). Ourapproach employs a sequence of visual resamplers that capture visual details atvarious spacial scales. This architecture not only leverages global and localvisual contexts effectively, but also facilitates the flexible extension ofvisual tokens through a compound token scaling strategy, allowing up to a 16xincrease in the token count post pre-training. Consequently, Chain-of-Sightrequires significantly fewer visual tokens in the pre-training phase comparedto the fine-tuning phase. This intentional reduction of visual tokens duringpre-training notably accelerates the pre-training process, cutting down thewall-clock training time by ~73%. Empirical results on a series ofvision-language benchmarks reveal that the pre-train acceleration throughChain-of-Sight is achieved without sacrificing performance, matching orsurpassing the standard pipeline of utilizing all visual tokens throughout theentire training process. Further scaling up the number of visual tokens forpre-training leads to stronger performances, competitive to existing approachesin a series of benchmarks.</description><author>Ziyuan Huang, Kaixiang Ji, Biao Gong, Zhiwu Qing, Qinglong Zhang, Kecheng Zheng, Jian Wang, Jingdong Chen, Ming Yang</author><pubDate>Mon, 22 Jul 2024 17:33:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15819v1</guid></item><item><title>Enhancing Cell Instance Segmentation in Scanning Electron Microscopy Images via a Deep Contour Closing Operator</title><link>http://arxiv.org/abs/2407.15817v1</link><description>Accurately segmenting and individualizing cells in SEM images is a highlypromising technique for elucidating tissue architecture in oncology. Whilecurrent AI-based methods are effective, errors persist, necessitatingtime-consuming manual corrections, particularly in areas where the quality ofcell contours in the image is poor and requires gap filling. This studypresents a novel AI-driven approach for refining cell boundary delineation toimprove instance-based cell segmentation in SEM images, also reducing thenecessity for residual manual correction. A CNN COp-Net is introduced toaddress gaps in cell contours, effectively filling in regions with deficient orabsent information. The network takes as input cell contour probability mapswith potentially inadequate or missing information and outputs corrected cellcontour delineations. The lack of training data was addressed by generating lowintegrity probability maps using a tailored PDE. We showcase the efficacy ofour approach in augmenting cell boundary precision using both private SEMimages from PDX hepatoblastoma tissues and publicly accessible images datasets.The proposed cell contour closing operator exhibits a notable improvement intested datasets, achieving respectively close to 50% (private data) and 10%(public data) increase in the accurately-delineated cell proportion compared tostate-of-the-art methods. Additionally, the need for manual corrections wassignificantly reduced, therefore facilitating the overall digitalizationprocess. Our results demonstrate a notable enhancement in the accuracy of cellinstance segmentation, particularly in highly challenging regions where imagequality compromises the integrity of cell boundaries, necessitating gapfilling. Therefore, our work should ultimately facilitate the study of tumourtissue bioarchitecture in onconanotomy field.</description><author>Florian Robert, Alexia Calovoulos, Laurent Facq, Fanny Decoeur, Etienne Gontier, Christophe F. Grosset, Baudouin Denis de Senneville</author><pubDate>Mon, 22 Jul 2024 17:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15817v1</guid></item><item><title>Efficient and generalizable prediction of molecular alterations in multiple cancer cohorts using H&amp;E whole slide images</title><link>http://arxiv.org/abs/2407.15816v1</link><description>Molecular testing of tumor samples for targetable biomarkers is restricted bya lack of standardization, turnaround-time, cost, and tissue availabilityacross cancer types. Additionally, targetable alterations of low prevalence maynot be tested in routine workflows. Algorithms that predict DNA alterationsfrom routinely generated hematoxylin and eosin (H&amp;E)-stained images couldprioritize samples for confirmatory molecular testing. Costs and the necessityof a large number of samples containing mutations limit approaches that trainindividual algorithms for each alteration. In this work, models were trainedfor simultaneous prediction of multiple DNA alterations from H&amp;E images using amulti-task approach. Compared to biomarker-specific models, this approachperformed better on average, with pronounced gains for rare mutations. Themodels reasonably generalized to independent temporal-holdout,externally-stained, and multi-site TCGA test sets. Additionally, whole slideimage embeddings derived using multi-task models demonstrated strongperformance in downstream tasks that were not a part of training. Overall, thisis a promising approach to develop clinically useful algorithms that providemultiple actionable predictions from a single slide.</description><author>Kshitij Ingale, Sun Hae Hong, Qiyuan Hu, Renyu Zhang, Bo Osinski, Mina Khoshdeli, Josh Och, Kunal Nagpal, Martin C. Stumpe, Rohan P. Joshi</author><pubDate>Mon, 22 Jul 2024 17:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15816v1</guid></item><item><title>Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts</title><link>http://arxiv.org/abs/2402.16822v2</link><description>As large language models (LLMs) become increasingly prevalent across manyreal-world applications, understanding and enhancing their robustness toadversarial attacks is of paramount importance. Existing methods foridentifying adversarial prompts tend to focus on specific domains, lackdiversity, or require extensive human annotations. To address theselimitations, we present Rainbow Teaming, a novel black-box approach forproducing a diverse collection of adversarial prompts. Rainbow Teaming castsadversarial prompt generation as a quality-diversity problem, and usesopen-ended search to generate prompts that are both effective and diverse.Focusing on the safety domain, we use Rainbow Teaming to target variousstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approachreveals hundreds of effective adversarial prompts, with an attack success rateexceeding 90% across all tested models. Furthermore, we demonstrate thatfine-tuning models with synthetic data generated by the Rainbow Teaming methodsignificantly enhances their safety without sacrificing general performance orhelpfulness. We additionally explore the versatility of Rainbow Teaming byapplying it to question answering and cybersecurity, showcasing its potentialto drive robust open-ended self-improvement in a wide range of applications.</description><author>Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, Roberta Raileanu</author><pubDate>Mon, 22 Jul 2024 17:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16822v2</guid></item><item><title>On the Matrix Form of the Quaternion Fourier Transform and Quaternion Convolution</title><link>http://arxiv.org/abs/2307.01836v3</link><description>We study matrix forms of quaternionic versions of the Fourier Transform andConvolution operations. Quaternions offer a powerful representation unit,however they are related to difficulties in their use that stem foremost fromnon-commutativity of quaternion multiplication, and due to that $\mu^2 = -1$possesses infinite solutions in the quaternion domain. Handling of quaternionicmatrices is consequently complicated in several aspects (definition ofeigenstructure, determinant, etc.). Our research findings clarify the relationof the Quaternion Fourier Transform matrix to the standard (complex) DiscreteFourier Transform matrix, and the extend on which well-known complex-domaintheorems extend to quaternions. We focus especially on the relation ofQuaternion Fourier Transform matrices to Quaternion Circulant matrices(representing quaternionic convolution), and the eigenstructure of the latter.A proof-of-concept application that makes direct use of our theoretical resultsis presented, where we present a method to bound the Lipschitz constant of aQuaternionic Convolutional Neural Network. Code is publicly available at:\url{https://github.com/sfikas/quaternion-fourier-convolution-matrix}.</description><author>Giorgos Sfikas, George Retsinas</author><pubDate>Mon, 22 Jul 2024 17:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01836v3</guid></item><item><title>Turing's Test, a Beautiful Thought Experiment</title><link>http://arxiv.org/abs/2401.00009v3</link><description>In the wake of the latest trends of artificial intelligence (AI), there hasbeen a resurgence of claims and questions about the Turing test and its value,which are reminiscent of decades of practical "Turing" tests. If AI werequantum physics, by now several "Schr\"odinger's" cats would have been killed.It is time for a historical reconstruction of Turing's beautiful thoughtexperiment. This paper presents a wealth of evidence, including new archivalsources, and gives original answers to several open questions about Turing's1950 paper, including its relation with early AI.</description><author>Bernardo Gonçalves</author><pubDate>Mon, 22 Jul 2024 17:29:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00009v3</guid></item><item><title>Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning</title><link>http://arxiv.org/abs/2407.15815v1</link><description>Can we endow visuomotor robots with generalization capabilities to operate indiverse open-world scenarios? In this paper, we propose \textbf{Maniwhere}, ageneralizable framework tailored for visual reinforcement learning, enablingthe trained robot policies to generalize across a combination of multiplevisual disturbance types. Specifically, we introduce a multi-viewrepresentation learning approach fused with Spatial Transformer Network (STN)module to capture shared semantic information and correspondences amongdifferent viewpoints. In addition, we employ a curriculum-based randomizationand augmentation approach to stabilize the RL training process and strengthenthe visual generalization ability. To exhibit the effectiveness of Maniwhere,we meticulously design 8 tasks encompassing articulate objects, bi-manual, anddexterous hand manipulation tasks, demonstrating Maniwhere's strong visualgeneralization and sim2real transfer abilities across 3 hardware platforms. Ourexperiments show that Maniwhere significantly outperforms existingstate-of-the-art methods. Videos are provided athttps://gemcollector.github.io/maniwhere/.</description><author>Zhecheng Yuan, Tianming Wei, Shuiqi Cheng, Gu Zhang, Yuanpei Chen, Huazhe Xu</author><pubDate>Mon, 22 Jul 2024 17:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15815v1</guid></item><item><title>Perceptions of Linguistic Uncertainty by Language Models and Humans</title><link>http://arxiv.org/abs/2407.15814v1</link><description>Uncertainty expressions such as ``probably'' or ``highly unlikely'' arepervasive in human language. While prior work has established that there ispopulation-level agreement in terms of how humans interpret these expressions,there has been little inquiry into the abilities of language models tointerpret such expressions. In this paper, we investigate how language modelsmap linguistic expressions of uncertainty to numerical responses. Our approachassesses whether language models can employ theory of mind in this setting:understanding the uncertainty of another agent about a particular statement,independently of the model's own certainty about that statement. We evaluateboth humans and 10 popular language models on a task created to assess theseabilities. Unexpectedly, we find that 8 out of 10 models are able to mapuncertainty expressions to probabilistic responses in a human-like manner.However, we observe systematically different behavior depending on whether astatement is actually true or false. This sensitivity indicates that languagemodels are substantially more susceptible to bias based on their priorknowledge (as compared to humans). These findings raise important questions andhave broad implications for human-AI alignment and AI-AI communication.</description><author>Catarina G Belem, Markelle Kelly, Mark Steyvers, Sameer Singh, Padhraic Smyth</author><pubDate>Mon, 22 Jul 2024 17:26:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15814v1</guid></item><item><title>ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models</title><link>http://arxiv.org/abs/2403.20262v2</link><description>Research on Large Language Models (LLMs) has recently witnessed an increasinginterest in extending models' context size to better capture dependencieswithin long documents. While benchmarks have been proposed to assess long-rangeabilities, existing efforts primarily considered generic tasks that are notnecessarily aligned with real-world applications. In contrast, our workproposes a new benchmark for long-context LLMs focused on a practical meetingassistant scenario. In this scenario, the long contexts consist of transcriptsobtained by automatic speech recognition, presenting unique challenges for LLMsdue to the inherent noisiness and oral nature of such data. Our benchmark,named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271manually crafted questions and their ground-truth answers. Our experiments withrecent long-context LLMs on ELITR-Bench highlight a gap between open-source andproprietary models, especially when questions are asked sequentially within aconversation. We also provide a thorough analysis of our GPT-4-based evaluationmethod, encompassing insights from a crowdsourcing study. Our findings suggestthat while GPT-4's evaluation scores are correlated with human judges', itsability to differentiate among more than three score levels may be limited.</description><author>Thibaut Thonet, Jos Rozen, Laurent Besacier</author><pubDate>Mon, 22 Jul 2024 17:24:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20262v2</guid></item><item><title>Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget</title><link>http://arxiv.org/abs/2407.15811v1</link><description>As scaling laws in generative AI push performance, they also simultaneouslyconcentrate the development of these models among actors with largecomputational resources. With a focus on text-to-image (T2I) generative models,we aim to address this bottleneck by demonstrating very low-cost training oflarge-scale T2I diffusion transformer models. As the computational cost oftransformers increases with the number of patches in each image, we propose torandomly mask up to 75% of the image patches during training. We propose adeferred masking strategy that preprocesses all patches using a patch-mixerbefore masking, thus significantly reducing the performance degradation withmasking, making it superior to model downscaling in reducing computationalcost. We also incorporate the latest improvements in transformer architecture,such as the use of mixture-of-experts layers, to improve performance andfurther identify the critical benefit of using synthetic images in micro-budgettraining. Finally, using only 37M publicly available real and synthetic images,we train a 1.16 billion parameter sparse transformer with only \$1,890economical cost and achieve a 12.7 FID in zero-shot generation on the COCOdataset. Notably, our model achieves competitive FID and high-qualitygenerations while incurring 118$\times$ lower cost than stable diffusion modelsand 14$\times$ lower cost than the current state-of-the-art approach that costs\$28,400. We aim to release our end-to-end training pipeline to furtherdemocratize the training of large-scale diffusion models on micro-budgets.</description><author>Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, Lingjuan Lyu</author><pubDate>Mon, 22 Jul 2024 17:23:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15811v1</guid></item><item><title>Breaking the Global North Stereotype: A Global South-centric Benchmark Dataset for Auditing and Mitigating Biases in Facial Recognition Systems</title><link>http://arxiv.org/abs/2407.15810v1</link><description>Facial Recognition Systems (FRSs) are being developed and deployed globallyat unprecedented rates. Most platforms are designed in a limited set ofcountries but deployed in worldwide, without adequate checkpoints. This isespecially problematic for Global South countries which lack strong legislationto safeguard persons facing disparate performance of these systems. Acombination of unavailability of datasets, lack of understanding of FRSfunctionality and low-resource bias mitigation measures accentuate the problem.In this work, we propose a new face dataset composed of 6,579 unique male andfemale sportspersons from eight countries around the world. More than 50% ofthe dataset comprises individuals from the Global South countries and isdemographically diverse. To aid adversarial audits and robust model training,each image has four adversarial variants, totaling over 40,000 images. We alsobenchmark five popular FRSs, both commercial and open-source, for the task ofgender prediction (and country prediction for one of the open-source models asan example of red-teaming). Experiments on industrial FRSs reveal accuraciesranging from 98.2%--38.1%, with a large disparity between males and females inthe Global South (max difference of 38.5%). Biases are also observed in allFRSs between females of the Global North and South (max difference of ~50%).Grad-CAM analysis identifies the nose, forehead and mouth as the regions ofinterest on one of the open-source FRSs. Utilizing this insight, we designsimple, low-resource bias mitigation solutions using few-shot and novelcontrastive learning techniques significantly improving the accuracy withdisparity between males and females reducing from 50% to 1.5% in one of thesettings. In the red-teaming experiment with the open-source Deepface model,contrastive learning proves more effective than simple fine-tuning.</description><author>Siddharth D Jaiswal, Animesh Ganai, Abhisek Dash, Saptarshi Ghosh, Animesh Mukherjee</author><pubDate>Mon, 22 Jul 2024 17:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15810v1</guid></item><item><title>FSboard: Over 3 million characters of ASL fingerspelling collected via smartphones</title><link>http://arxiv.org/abs/2407.15806v1</link><description>Progress in machine understanding of sign languages has been slow andhampered by limited data. In this paper, we present FSboard, an American SignLanguage fingerspelling dataset situated in a mobile text entry use case,collected from 147 paid and consenting Deaf signers using Pixel 4A selfiecameras in a variety of environments. Fingerspelling recognition is anincomplete solution that is only one small part of sign language translation,but it could provide some immediate benefit to Deaf/Hard of Hearing signers asmore broadly capable technology develops. At &gt;3 million characters in lengthand &gt;250 hours in duration, FSboard is the largest fingerspelling recognitiondataset to date by a factor of &gt;10x. As a simple baseline, we finetune 30 HzMediaPipe Holistic landmark inputs into ByT5-Small and achieve 11.1% CharacterError Rate (CER) on a test set with unique phrases and signers. This qualitydegrades gracefully when decreasing frame rate and excluding face/bodylandmarks: plausible optimizations to help models run on device in real time.</description><author>Manfred Georg, Garrett Tanzer, Saad Hassan, Maximus Shengelia, Esha Uboweja, Sam Sepah, Sean Forbes, Thad Starner</author><pubDate>Mon, 22 Jul 2024 17:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15806v1</guid></item><item><title>Enhancing Mass Customization Manufacturing: Multiobjective Metaheuristic Algorithms for flow shop Production in Smart Industry</title><link>http://arxiv.org/abs/2407.15802v1</link><description>The current landscape of massive production industries is undergoingsignificant transformations driven by emerging customer trends and new smartmanufacturing technologies. One such change is the imperative to implement masscustomization, wherein products are tailored to individual customerspecifications while still ensuring cost efficiency through large-scaleproduction processes. These shifts can profoundly impact various facets of theindustry. This study focuses on the necessary adaptations in shop-floorproduction planning. Specifically, it proposes the use of efficientevolutionary algorithms to tackle the flowshop with missing operations,considering different optimization objectives: makespan, weighted totaltardiness, and total completion time. An extensive computationalexperimentation is conducted across a range of realistic instances,encompassing varying numbers of jobs, operations, and probabilities of missingoperations. The findings demonstrate the competitiveness of the proposedapproach and enable the identification of the most suitable evolutionaryalgorithms for addressing this problem. Additionally, the impact of theprobability of missing operations on optimization objectives is discussed.</description><author>Diego Rossit, Daniel Rossit, Sergio Nesmachnow</author><pubDate>Mon, 22 Jul 2024 17:13:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15802v1</guid></item><item><title>DropKAN: Regularizing KANs by masking post-activations</title><link>http://arxiv.org/abs/2407.13044v2</link><description>We propose DropKAN (Dropout Kolmogorov-Arnold Networks) a regularizationmethod that prevents co-adaptation of activation function weights inKolmogorov-Arnold Networks (KANs). DropKAN operates by randomly masking some ofthe post-activations within the KANs computation graph, while scaling-up theretained post-activations. We show that this simple procedure that requireminimal coding effort has a regularizing effect and consistently lead to bettergeneralization of KANs. We analyze the adaptation of the standard Dropout with KANs and demonstratethat Dropout applied to KANs' neurons can lead to unpredictable behaviour inthe feedforward pass. We carry an empirical study with real world MachineLearning datasets to validate our findings. Our results suggest that DropKAN isconsistently a better alternative to using standard Dropout with KANs, andimproves the generalization performance of KANs. Our implementation of DropKANis available at: \url{https://github.com/Ghaith81/dropkan}.</description><author>Mohammed Ghaith Altarabichi</author><pubDate>Mon, 22 Jul 2024 17:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13044v2</guid></item><item><title>Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures</title><link>http://arxiv.org/abs/2307.15220v3</link><description>Recent advancements in surgical computer vision have been driven byvision-only models, which lack language semantics, relying on manuallyannotated videos to predict fixed object categories. This limits theirgeneralizability to unseen surgical procedures and tasks. We propose leveragingsurgical video lectures from e-learning platforms to provide effective visionand language supervisory signals for multi-modal representation learning,bypassing manual annotations. We address surgery-specific linguistic challengesusing multiple automatic speech recognition systems for text transcriptions. Weintroduce SurgVLP - Surgical Vision Language Pre-training - a novel method formulti-modal representation learning. SurgVLP employs a new contrastive learningobjective, aligning video clip embeddings with corresponding multiple textembeddings in a joint latent space. We demonstrate the representationalcapability of this space through several vision-and-language surgical tasks andvision-only tasks specific to surgery. Unlike current fully supervisedapproaches, SurgVLP adapts to different surgical procedures and tasks withoutspecific fine-tuning, achieving zero-shot adaptation to tasks such as surgicaltool, phase, and triplet recognition without manual annotation. These resultshighlight the transferability and versatility of the learned multi-modalrepresentations in surgical video analysis. The code is available athttps://github.com/CAMMA-public/SurgVLP</description><author>Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Pietro Mascagni, Nassir Navab, Nicolas Padoy</author><pubDate>Mon, 22 Jul 2024 17:12:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15220v3</guid></item><item><title>Adaptive Extensions of Unbiased Risk Estimators for Unsupervised Magnetic Resonance Image Denoising</title><link>http://arxiv.org/abs/2407.15799v1</link><description>The application of Deep Neural Networks (DNNs) to image denoising has notablychallenged traditional denoising methods, particularly within complex noisescenarios prevalent in medical imaging. Despite the effectiveness oftraditional and some DNN-based methods, their reliance on high-quality,noiseless ground truth images limits their practical utility. In response tothis, our work introduces and benchmarks innovative unsupervised learningstrategies, notably Stein's Unbiased Risk Estimator (SURE), its extension(eSURE), and our novel implementation, the Extended Poisson Unbiased RiskEstimator (ePURE), within medical imaging frameworks. This paper presents a comprehensive evaluation of these methods on MRI dataafflicted with Gaussian and Poisson noise types, a scenario typical in medicalimaging but challenging for most denoising algorithms. Our main contributionlies in the effective adaptation and implementation of the SURE, eSURE, andparticularly the ePURE frameworks for medical images, showcasing theirrobustness and efficacy in environments where traditional noiseless groundtruth cannot be obtained.</description><author>Reeshad Khan, Dr. John Gauch, Dr. Ukash Nakarmi</author><pubDate>Mon, 22 Jul 2024 17:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15799v1</guid></item><item><title>Robust Facial Reactions Generation: An Emotion-Aware Framework with Modality Compensation</title><link>http://arxiv.org/abs/2407.15798v1</link><description>The objective of the Multiple Appropriate Facial Reaction Generation (MAFRG)task is to produce contextually appropriate and diverse listener facialbehavioural responses based on the multimodal behavioural data of theconversational partner (i.e., the speaker). Current methodologies typicallyassume continuous availability of speech and facial modality data, neglectingreal-world scenarios where these data may be intermittently unavailable, whichoften results in model failures. Furthermore, despite utilising advanced deeplearning models to extract information from the speaker's multimodal inputs,these models fail to adequately leverage the speaker's emotional context, whichis vital for eliciting appropriate facial reactions from human listeners. Toaddress these limitations, we propose an Emotion-aware Modality Compensatory(EMC) framework. This versatile solution can be seamlessly integrated intoexisting models, thereby preserving their advantages while significantlyenhancing performance and robustness in scenarios with missing modalities. Ourframework ensures resilience when faced with missing modality data through theCompensatory Modality Alignment (CMA) module. It also generates moreappropriate emotion-aware reactions via the Emotion-aware Attention (EA)module, which incorporates the speaker's emotional information throughout theentire encoding and decoding process. Experimental results demonstrate that ourframework improves the appropriateness metric FRCorr by an average of 57.2\%compared to the original model structure. In scenarios where speech modalitydata is missing, the performance of appropriate generation shows animprovement, and when facial data is missing, it only exhibits minimaldegradation.</description><author>Guanyu Hu, Jie Wei, Siyang Song, Dimitrios Kollias, Xinyu Yang, Zhonglin Sun, Odysseus Kaloidas</author><pubDate>Mon, 22 Jul 2024 17:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15798v1</guid></item><item><title>MILAN: Milli-Annotations for Lidar Semantic Segmentation</title><link>http://arxiv.org/abs/2407.15797v1</link><description>Annotating lidar point clouds for autonomous driving is a notoriouslyexpensive and time-consuming task. In this work, we show that the quality ofrecent self-supervised lidar scan representations allows a great reduction ofthe annotation cost. Our method has two main steps. First, we show thatself-supervised representations allow a simple and direct selection of highlyinformative lidar scans to annotate: training a network on these selected scansleads to much better results than a random selection of scans and, moreinterestingly, to results on par with selections made by SOTA active learningmethods. In a second step, we leverage the same self-supervised representationsto cluster points in our selected scans. Asking the annotator to classify eachcluster, with a single click per cluster, then permits us to close the gap withfully-annotated training sets, while only requiring one thousandth of the pointlabels.</description><author>Nermin Samet, Gilles Puy, Oriane Siméoni, Renaud Marlet</author><pubDate>Mon, 22 Jul 2024 16:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15797v1</guid></item><item><title>AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection</title><link>http://arxiv.org/abs/2407.15795v1</link><description>Zero-shot anomaly detection (ZSAD) targets the identification of anomalieswithin images from arbitrary novel categories. This study introduces AdaCLIPfor the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP.AdaCLIP incorporates learnable prompts into CLIP and optimizes them throughtraining on auxiliary annotated anomaly detection data. Two types of learnableprompts are proposed: static and dynamic. Static prompts are shared across allimages, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamicprompts are generated for each test image, providing CLIP with dynamicadaptation capabilities. The combination of static and dynamic prompts isreferred to as hybrid prompts, and yields enhanced ZSAD performance. Extensiveexperiments conducted across 14 real-world anomaly detection datasets fromindustrial and medical domains indicate that AdaCLIP outperforms other ZSADmethods and can generalize better to different categories and even domains.Finally, our analysis highlights the importance of diverse auxiliary data andoptimized prompts for enhanced generalization capacity. Code is available athttps://github.com/caoyunkang/AdaCLIP.</description><author>Yunkang Cao, Jiangning Zhang, Luca Frittoli, Yuqi Cheng, Weiming Shen, Giacomo Boracchi</author><pubDate>Mon, 22 Jul 2024 16:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15795v1</guid></item><item><title>Disentangling spatio-temporal knowledge for weakly supervised object detection and segmentation in surgical video</title><link>http://arxiv.org/abs/2407.15794v1</link><description>Weakly supervised video object segmentation (WSVOS) enables theidentification of segmentation maps without requiring an extensive trainingdataset of object masks, relying instead on coarse video labels indicatingobject presence. Current state-of-the-art methods either require multipleindependent stages of processing that employ motion cues or, in the case ofend-to-end trainable networks, lack in segmentation accuracy, in part due tothe difficulty of learning segmentation maps from videos with transient objectpresence. This limits the application of WSVOS for semantic annotation ofsurgical videos where multiple surgical tools frequently move in and out of thefield of view, a problem that is more difficult than typically encountered inWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks(VDST-Net), a framework to disentangle spatiotemporal information usingsemi-decoupled knowledge distillation to predict high-quality class activationmaps (CAMs). A teacher network designed to resolve temporal conflicts whenspecifics about object location and timing in the video are not provided workswith a student network that integrates information over time by leveragingtemporal dependencies. We demonstrate the efficacy of our framework on a publicreference dataset and on a more challenging surgical video dataset whereobjects are, on average, present in less than 60\% of annotated frames. Ourmethod outperforms state-of-the-art techniques and generates superiorsegmentation masks under video-level weak supervision.</description><author>Guiqiu Liao, Matjaz Jogan, Sai Koushik, Eric Eaton, Daniel A. Hashimoto</author><pubDate>Mon, 22 Jul 2024 16:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15794v1</guid></item><item><title>CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning</title><link>http://arxiv.org/abs/2407.15793v1</link><description>With the emergence of Transformers and Vision-Language Models (VLMs) such asCLIP, large pre-trained models have become a common strategy to enhanceperformance in Continual Learning scenarios. This led to the development ofnumerous prompting strategies to effectively fine-tune transformer-based modelswithout succumbing to catastrophic forgetting. However, these methods struggleto specialize the model on domains significantly deviating from thepre-training and preserving its zero-shot capabilities. In this work, wepropose Continual Generative training for Incremental prompt-Learning, a novelapproach to mitigate forgetting while adapting a VLM, which exploits generativereplay to align prompts to tasks. We also introduce a new metric to evaluatezero-shot capabilities within CL benchmarks. Through extensive experiments ondifferent domains, we demonstrate the effectiveness of our framework inadapting to new tasks while improving zero-shot capabilities. Further analysisreveals that our approach can bridge the gap with joint prompt tuning. Thecodebase is available at https://github.com/aimagelab/mammoth.</description><author>Emanuele Frascaroli, Aniello Panariello, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</author><pubDate>Mon, 22 Jul 2024 16:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15793v1</guid></item><item><title>Robust Mixture Learning when Outliers Overwhelm Small Groups</title><link>http://arxiv.org/abs/2407.15792v1</link><description>We study the problem of estimating the means of well-separated mixtures whenan adversary may add arbitrary outliers. While strong guarantees are availablewhen the outlier fraction is significantly smaller than the minimum mixingweight, much less is known when outliers may crowd out low-weight clusters - asetting we refer to as list-decodable mixture learning (LD-ML). In this case,adversarial outliers can simulate additional spurious mixture components.Hence, if all means of the mixture must be recovered up to a small error in theoutput list, the list size needs to be larger than the number of (true)components. We propose an algorithm that obtains order-optimal error guaranteesfor each mixture mean with a minimal list-size overhead, significantlyimproving upon list-decodable mean estimation, the only existing method that isapplicable for LD-ML. Although improvements are observed even when the mixtureis non-separated, our algorithm achieves particularly strong guarantees whenthe mixture is separated: it can leverage the mixture structure to partiallycluster the samples before carefully iterating a base learner forlist-decodable mean estimation at different scales.</description><author>Daniil Dmitriev, Rares-Darius Buhai, Stefan Tiegel, Alexander Wolters, Gleb Novikov, Amartya Sanyal, David Steurer, Fanny Yang</author><pubDate>Mon, 22 Jul 2024 16:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15792v1</guid></item><item><title>RADA: Robust and Accurate Feature Learning with Domain Adaptation</title><link>http://arxiv.org/abs/2407.15791v1</link><description>Recent advancements in keypoint detection and descriptor extraction haveshown impressive performance in local feature learning tasks. However, existingmethods generally exhibit suboptimal performance under extreme conditions suchas significant appearance changes and domain shifts. In this study, weintroduce a multi-level feature aggregation network that incorporates twopivotal components to facilitate the learning of robust and accurate featureswith domain adaptation. First, we employ domain adaptation supervision to alignhigh-level feature distributions across different domains to achieve invariantdomain representations. Second, we propose a Transformer-based booster thatenhances descriptor robustness by integrating visual and geometric informationthrough wave position encoding concepts, effectively handling complexconditions. To ensure the accuracy and robustness of features, we adopt ahierarchical architecture to capture comprehensive information and applymeticulous targeted supervision to keypoint detection, descriptor extraction,and their coupled processing. Extensive experiments demonstrate that ourmethod, RADA, achieves excellent results in image matching, camera poseestimation, and visual localization tasks.</description><author>Jingtai He, Gehao Zhang, Tingting Liu, Songlin Du</author><pubDate>Mon, 22 Jul 2024 16:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15791v1</guid></item><item><title>Extracting Structured Insights from Financial News: An Augmented LLM Driven Approach</title><link>http://arxiv.org/abs/2407.15788v1</link><description>Financial news plays a crucial role in decision-making processes across thefinancial sector, yet the efficient processing of this information into astructured format remains challenging. This paper presents a novel approach tofinancial news processing that leverages Large Language Models (LLMs) toovercome limitations that previously prevented the extraction of structureddata from unstructured financial news. We introduce a system that extractsrelevant company tickers from raw news article content, performs sentimentanalysis at the company level, and generates summaries, all without relying onpre-structured data feeds. Our methodology combines the generative capabilitiesof LLMs, and recent prompting techniques, with a robust validation frameworkthat uses a tailored string similarity approach. Evaluation on a dataset of5530 financial news articles demonstrates the effectiveness of our approach,with 90% of articles not missing any tickers compared with current dataproviders, and 22% of articles having additional relevant tickers. In additionto this paper, the methodology has been implemented at scale with the resultingprocessed data made available through a live API endpoint, which is updated inreal-time with the latest news. To the best of our knowledge, we are the firstdata provider to offer granular, per-company sentiment analysis from newsarticles, enhancing the depth of information available to market participants.We also release the evaluation dataset of 5530 processed articles as a staticfile, which we hope will facilitate further research leveraging financial news.</description><author>Rian Dolphin, Joe Dursun, Jonathan Chow, Jarrett Blankenship, Katie Adams, Quinton Pike</author><pubDate>Mon, 22 Jul 2024 16:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15788v1</guid></item><item><title>Unsupervised Mastoidectomy for Cochlear CT Mesh Reconstruction Using Highly Noisy Data</title><link>http://arxiv.org/abs/2407.15787v1</link><description>Cochlear Implant (CI) procedures involve inserting an array of electrodesinto the cochlea located inside the inner ear. Mastoidectomy is a surgicalprocedure that uses a high-speed drill to remove part of the mastoid region ofthe temporal bone, providing safe access to the cochlea through the middle andinner ear. We aim to develop an intraoperative navigation system that registersplans created using 3D preoperative Computerized Tomography (CT) volumes withthe 2D surgical microscope view. Herein, we propose a method to synthesize themastoidectomy volume using only the preoperative CT scan, where the mastoid isintact. We introduce an unsupervised learning framework designed to synthesizemastoidectomy. For model training purposes, this method uses postoperative CTscans to avoid manual data cleaning or labeling, even when the region removedduring mastoidectomy is visible but affected by metal artifacts, lowsignal-to-noise ratio, or electrode wiring. Our approach estimatesmastoidectomy regions with a mean dice score of 70.0%. This approach representsa major step forward for CI intraoperative navigation by predicting realisticmastoidectomy-removed regions in preoperative planning that can be used toregister the pre-surgery plan to intraoperative microscopy.</description><author>Yike Zhang, Dingjie Su, Eduardo Davalos, Jack H. Noble</author><pubDate>Mon, 22 Jul 2024 16:47:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15787v1</guid></item><item><title>Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning</title><link>http://arxiv.org/abs/2310.10818v3</link><description>Sample efficiency is central to developing practical reinforcement learning(RL) for complex and large-scale decision-making problems. The ability totransfer and generalize knowledge gained from previous experiences todownstream tasks can significantly improve sample efficiency. Recent researchindicates that successor feature (SF) RL algorithms enable knowledgegeneralization between tasks with different rewards but identical transitiondynamics. It has recently been hypothesized that combining model-based (MB)methods with SF algorithms can alleviate the limitation of fixed transitiondynamics. Furthermore, uncertainty-aware exploration is widely recognized asanother appealing approach for improving sample efficiency. Putting togethertwo ideas of hybrid model-based successor feature (MB-SF) and uncertainty leadsto an approach to the problem of sample efficient uncertainty-aware knowledgetransfer across tasks with different transition dynamics or/and rewardfunctions. In this paper, the uncertainty of the value of each action isapproximated by a Kalman filter (KF)-based multiple-model adaptive estimation.This KF-based framework treats the parameters of a model as random variables.To the best of our knowledge, this is the first attempt at formulating a hybridMB-SF algorithm capable of generalizing knowledge across large or continuousstate space tasks with various transition dynamics while requiring lesscomputation at decision time than MB methods. The number of samples required tolearn the tasks was compared to recent SF and MB baselines. The results showthat our algorithm generalizes its knowledge across different transitiondynamics, learns downstream tasks with significantly fewer samples thanstarting from scratch, and outperforms existing approaches.</description><author>Parvin Malekzadeh, Ming Hou, Konstantinos N. Plataniotis</author><pubDate>Mon, 22 Jul 2024 16:47:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10818v3</guid></item><item><title>Who Shares Fake News? Uncovering Insights from Social Media Users' Post Histories</title><link>http://arxiv.org/abs/2203.10560v3</link><description>We propose that social-media users' own post histories are an underused yetvaluable resource for studying fake-news sharing. By extracting textual cuesfrom their prior posts, and contrasting their prevalence against randomsocial-media users and others (e.g., those with similar socio-demographics,political news-sharers, and fact-check sharers), researchers can identify cuesthat distinguish fake-news sharers, predict those most likely to share fakenews, and identify promising constructs to build interventions. Our researchincludes studies along these lines. In Study 1, we explore the distinctivelanguage patterns of fake-news sharers, highlighting elements such as theirhigher use of anger and power-related words. In Study 2, we show that addingtextual cues into predictive models enhances their accuracy in predictingfake-news sharers. In Study 3, we explore the contrasting role of trait andsituational anger, and show trait anger is associated with a greater propensityto share both true and fake news. In Study 4, we introduce a way toauthenticate Twitter accounts in surveys, before using it to explore howcrafting an ad copy that resonates with users' sense of power encourages theadoption of fact-checking tools. We hope to encourage the use of novel researchmethods for marketers and misinformation researchers.</description><author>Verena Schoenmueller, Simon J. Blanchard, Gita V. Johar</author><pubDate>Mon, 22 Jul 2024 16:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.10560v3</guid></item><item><title>Concept-Based Interpretable Reinforcement Learning with Limited to No Human Labels</title><link>http://arxiv.org/abs/2407.15786v1</link><description>Recent advances in reinforcement learning (RL) have predominantly leveragedneural network-based policies for decision-making, yet these models often lackinterpretability, posing challenges for stakeholder comprehension and trust.Concept bottleneck models offer an interpretable alternative by integratinghuman-understandable concepts into neural networks. However, a significantlimitation in prior work is the assumption that human annotations for theseconcepts are readily available during training, necessitating continuousreal-time input from human annotators. To overcome this limitation, weintroduce a novel training scheme that enables RL algorithms to efficientlylearn a concept-based policy by only querying humans to label a small set ofdata, or in the extreme case, without any human labels. Our algorithm,LICORICE, involves three main contributions: interleaving concept learning andRL training, using a concept ensembles to actively select informative datapoints for labeling, and decorrelating the concept data with a simple strategy.We show how LICORICE reduces manual labeling efforts to to 500 or fewer conceptlabels in three environments. Finally, we present an initial study to explorehow we can use powerful vision-language models to infer concepts from rawvisual inputs without explicit labels at minimal cost to performance.</description><author>Zhuorui Ye, Stephanie Milani, Geoffrey J. Gordon, Fei Fang</author><pubDate>Mon, 22 Jul 2024 16:46:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15786v1</guid></item><item><title>Diffusion Model Based Resource Allocation Strategy in Ultra-Reliable Wireless Networked Control Systems</title><link>http://arxiv.org/abs/2407.15784v1</link><description>Diffusion models are vastly used in generative AI, leveraging theircapability to capture complex data distributions. However, their potentialremains largely unexplored in the field of resource allocation in wirelessnetworks. This paper introduces a novel diffusion model-based resourceallocation strategy for Wireless Networked Control Systems (WNCSs) with theobjective of minimizing total power consumption through the optimization of thesampling period in the control system, and blocklength and packet errorprobability in the finite blocklength regime of the communication system. Theproblem is first reduced to the optimization of blocklength only based on thederivation of the optimality conditions. Then, the optimization theory solutioncollects a dataset of channel gains and corresponding optimal blocklengths.Finally, the Denoising Diffusion Probabilistic Model (DDPM) uses this collecteddataset to train the resource allocation algorithm that generates optimalblocklength values conditioned on the channel state information (CSI). Viaextensive simulations, the proposed approach is shown to outperform previouslyproposed Deep Reinforcement Learning (DRL) based approaches with close tooptimal performance regarding total power consumption. Moreover, an improvementof up to eighteen-fold in the reduction of critical constraint violations isobserved, further underscoring the accuracy of the solution.</description><author>Amirhassan Babazadeh Darabi, Sinem Coleri</author><pubDate>Mon, 22 Jul 2024 16:44:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15784v1</guid></item><item><title>Interrogating AI: Characterizing Emergent Playful Interactions with ChatGPT</title><link>http://arxiv.org/abs/2401.08405v2</link><description>In an era of AI's growing capabilities and influences, recent advancementsare constantly reshaping HCI and CSCW's view of AI. Playful interactions withAI systems naturally emerged as an important way for users to make sense of theever-changing technology. However, these emergent and playful interactions areunderexamined. We target this gap by investigating playful interactionsexhibited by users of an emerging AI technology, ChatGPT. Through a thematicanalysis of 372 user-generated posts on the ChatGPT subreddit, we found thatmore than half of user discourse revolves around playful interactions. Theanalysis further allowed us to construct a preliminary framework to describethese interactions, categorizing them into six types: reflecting, jesting,imitating, challenging, tricking, and contriving; each included sub-categories.This study contributes to the field of HCI and CSCW by illuminating themultifaceted nature of playful interactions with AI, underlining theirsignificance in helping users assess AI agency, shaping the human-AIrelationship, and offering rich implications to AI system design.</description><author>Mohammad Ronagh Nikghalb, Jinghui Cheng</author><pubDate>Mon, 22 Jul 2024 16:44:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08405v2</guid></item><item><title>Reducing Texture Bias of Deep Neural Networks via Edge Enhancing Diffusion</title><link>http://arxiv.org/abs/2402.09530v2</link><description>Convolutional neural networks (CNNs) for image processing tend to focus onlocalized texture patterns, commonly referred to as texture bias. While most ofthe previous works in the literature focus on the task of image classification,we go beyond this and study the texture bias of CNNs in semantic segmentation.In this work, we propose to train CNNs on pre-processed images with lesstexture to reduce the texture bias. Therein, the challenge is to suppress imagetexture while preserving shape information. To this end, we utilize edgeenhancing diffusion (EED), an anisotropic image diffusion method initiallyintroduced for image compression, to create texture reduced duplicates ofexisting datasets. Extensive numerical studies are performed with both CNNs andvision transformer models trained on original data and EED-processed data fromthe Cityscapes dataset and the CARLA driving simulator. We observe strongtexture-dependence of CNNs and moderate texture-dependence of transformers.Training CNNs on EED-processed images enables the models to become completelyignorant with respect to texture, demonstrating resilience with respect totexture re-introduction to any degree. Additionally we analyze the performancereduction in depth on a level of connected components in the semanticsegmentation and study the influence of EED pre-processing on domaingeneralization as well as adversarial robustness.</description><author>Edgar Heinert, Matthias Rottmann, Kira Maag, Karsten Kahl</author><pubDate>Mon, 22 Jul 2024 16:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09530v2</guid></item><item><title>FAIntbench: A Holistic and Precise Benchmark for Bias Evaluation in Text-to-Image Models</title><link>http://arxiv.org/abs/2405.17814v4</link><description>The rapid development and reduced barriers to entry for Text-to-Image (T2I)models have raised concerns about the biases in their outputs, but existingresearch lacks a holistic definition and evaluation framework of biases,limiting the enhancement of debiasing techniques. To address this issue, weintroduce FAIntbench, a holistic and precise benchmark for biases in T2Imodels. In contrast to existing benchmarks that evaluate bias in limitedaspects, FAIntbench evaluate biases from four dimensions: manifestation ofbias, visibility of bias, acquired attributes, and protected attributes. Weapplied FAIntbench to evaluate seven recent large-scale T2I models andconducted human evaluation, whose results demonstrated the effectiveness ofFAIntbench in identifying various biases. Our study also revealed new researchquestions about biases, including the side-effect of distillation. The findingspresented here are preliminary, highlighting the potential of FAIntbench toadvance future research aimed at mitigating the biases in T2I models. Ourbenchmark is publicly available to ensure the reproducibility.</description><author>Hanjun Luo, Ziye Deng, Ruizhe Chen, Zuozhu Liu</author><pubDate>Mon, 22 Jul 2024 16:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17814v4</guid></item><item><title>Explaining Decisions in ML Models: a Parameterized Complexity Analysis</title><link>http://arxiv.org/abs/2407.15780v1</link><description>This paper presents a comprehensive theoretical investigation into theparameterized complexity of explanation problems in various machine learning(ML) models. Contrary to the prevalent black-box perception, our study focuseson models with transparent internal mechanisms. We address two principal typesof explanation problems: abductive and contrastive, both in their local andglobal variants. Our analysis encompasses diverse ML models, including DecisionTrees, Decision Sets, Decision Lists, Ordered Binary Decision Diagrams, RandomForests, and Boolean Circuits, and ensembles thereof, each offering uniqueexplanatory challenges. This research fills a significant gap in explainable AI(XAI) by providing a foundational understanding of the complexities ofgenerating explanations for these models. This work provides insights vital forfurther research in the domain of XAI, contributing to the broader discourse onthe necessity of transparency and accountability in AI systems.</description><author>Sebastian Ordyniak, Giacomo Paesani, Mateusz Rychlicki, Stefan Szeider</author><pubDate>Mon, 22 Jul 2024 16:37:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15780v1</guid></item><item><title>In Search of Quantum Advantage: Estimating the Number of Shots in Quantum Kernel Methods</title><link>http://arxiv.org/abs/2407.15776v1</link><description>Quantum Machine Learning (QML) has gathered significant attention throughapproaches like Quantum Kernel Machines. While these methods hold considerablepromise, their quantum nature presents inherent challenges. One major challengeis the limited resolution of estimated kernel values caused by the finitenumber of circuit runs performed on a quantum device. In this study, we proposea comprehensive system of rules and heuristics for estimating the requirednumber of circuit runs in quantum kernel methods. We introduce two criticaleffects that necessitate an increased measurement precision through additionalcircuit runs: the spread effect and the concentration effect. The effects areanalyzed in the context of fidelity and projected quantum kernels. To addressthese phenomena, we develop an approach for estimating desired precision ofkernel values, which, in turn, is translated into the number of circuit runs.Our methodology is validated through extensive numerical simulations, focusingon the problem of exponential value concentration. We stress that quantumkernel methods should not only be considered from the machine learningperformance perspective, but also from the context of the resource consumption.The results provide insights into the possible benefits of quantum kernelmethods, offering a guidance for their application in quantum machine learningtasks.</description><author>Artur Miroszewski, Marco Fellous Asiani, Jakub Mielczarek, Bertrand Le Saux, Jakub Nalepa</author><pubDate>Mon, 22 Jul 2024 16:29:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15776v1</guid></item><item><title>STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay</title><link>http://arxiv.org/abs/2407.15773v1</link><description>Test-time adaptation (TTA) aims to address the distribution shift between thetraining and test data with only unlabeled data at test time. Existing TTAmethods often focus on improving recognition performance specifically for testdata associated with classes in the training set. However, during theopen-world inference process, there are inevitably test data instances fromunknown classes, commonly referred to as outliers. This paper pays attention tothe problem that conducts both sample recognition and outlier rejection duringinference while outliers exist. To address this problem, we propose a newapproach called STAble Memory rePlay (STAMP), which performs optimization overa stable memory bank instead of the risky mini-batch. In particular, the memorybank is dynamically updated by selecting low-entropy and label-consistentsamples in a class-balanced manner. In addition, we develop a self-weightedentropy minimization strategy that assigns higher weight to low-entropysamples. Extensive results demonstrate that STAMP outperforms existing TTAmethods in terms of both recognition and outlier detection performance. Thecode is released at https://github.com/yuyongcan/STAMP.</description><author>Yongcan Yu, Lijun Sheng, Ran He, Jian Liang</author><pubDate>Mon, 22 Jul 2024 16:25:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15773v1</guid></item><item><title>Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection</title><link>http://arxiv.org/abs/2407.15771v1</link><description>This paper addresses the challenge of robotic grasping of general objects.Similar to prior research, the task reads a single-view 3D observation (i.e.,point clouds) captured by a depth camera as input. Crucially, the success ofobject grasping highly demands a comprehensive understanding of the shape ofobjects within the scene. However, single-view observations often suffer fromocclusions (including both self and inter-object occlusions), which lead togaps in the point clouds, especially in complex cluttered scenes. This rendersincomplete perception of the object shape and frequently causes failures orinaccurate pose estimation during object grasping. In this paper, we tacklethis issue with an effective albeit simple solution, namely completinggrasping-related scene regions through local occupancy prediction. Followingprior practice, the proposed model first runs by proposing a number of mostlikely grasp points in the scene. Around each grasp point, a module is designedto infer any voxel in its neighborhood to be either void or occupied by someobject. Importantly, the occupancy map is inferred by fusing both local andglobal cues. We implement a multi-group tri-plane scheme for efficientlyaggregating long-distance contextual information. The model further estimates6-DoF grasp poses utilizing the local occupancy-enhanced object shapeinformation and returns the top-ranked grasp proposal. Comprehensiveexperiments on both the large-scale GraspNet-1Billion benchmark and realrobotic arm demonstrate that the proposed method can effectively complete theunobserved parts in cluttered and occluded scenes. Benefiting from theoccupancy-enhanced feature, our model clearly outstrips other competing methodsunder various performance metrics such as grasping average precision.</description><author>Kangqi Ma, Hao Dong, Yadong Mu</author><pubDate>Mon, 22 Jul 2024 16:22:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15771v1</guid></item><item><title>Towards Open-World Object-based Anomaly Detection via Self-Supervised Outlier Synthesis</title><link>http://arxiv.org/abs/2407.15763v1</link><description>Object detection is a pivotal task in computer vision that has receivedsignificant attention in previous years. Nonetheless, the capability of adetector to localise objects out of the training distribution remainsunexplored. Whilst recent approaches in object-level out-of-distribution (OoD)detection heavily rely on class labels, such approaches contradict trulyopen-world scenarios where the class distribution is often unknown. In thiscontext, anomaly detection focuses on detecting unseen instances rather thanclassifying detections as OoD. This work aims to bridge this gap by leveragingan open-world object detector and an OoD detector via virtual outliersynthesis. This is achieved by using the detector backbone features to firstlearn object pseudo-classes via self-supervision. These pseudo-classes serve asthe basis for class-conditional virtual outlier sampling of anomalous featuresthat are classified by an OoD head. Our approach empowers our overall objectdetector architecture to learn anomaly-aware feature representations withoutrelying on class labels, hence enabling truly open-world object anomalydetection. Empirical validation of our approach demonstrates its effectivenessacross diverse datasets encompassing various imaging modalities (visible,infrared, and X-ray). Moreover, our method establishes state-of-the-artperformance on object-level anomaly detection, achieving an average recallscore improvement of over 5.4% for natural images and 23.5% for a securityX-ray dataset compared to the current approaches. In addition, our methoddetects anomalies in datasets where current approaches fail. Code available athttps://github.com/KostadinovShalon/oln-ssos.</description><author>Brian K. S. Isaac-Medina, Yona Falinie A. Gaus, Neelanjan Bhowmik, Toby P. Breckon</author><pubDate>Mon, 22 Jul 2024 16:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15763v1</guid></item><item><title>Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning</title><link>http://arxiv.org/abs/2407.15762v1</link><description>Reward-based finetuning is crucial for aligning language policies withintended behaviors (e.g., creativity and safety). A key challenge here is todevelop steerable language models that trade-off multiple (conflicting)objectives in a flexible and efficient manner. This paper presents ConditionedLanguage Policy (CLP), a general framework for finetuning language models onmultiple objectives. Building on techniques from multi-task training andparameter-efficient finetuning, CLP can learn steerable models that effectivelytrade-off conflicting objectives at inference time. Notably, this does notrequire training or maintaining multiple models to achieve different trade-offsbetween the objectives. Through an extensive set of experiments and ablations,we show that the CLP framework learns steerable models that outperform andPareto-dominate the current state-of-the-art approaches for multi-objectivefinetuning.</description><author>Kaiwen Wang, Rahul Kidambi, Ryan Sullivan, Alekh Agarwal, Christoph Dann, Andrea Michi, Marco Gelmi, Yunxuan Li, Raghav Gupta, Avinava Dubey, Alexandre Ramé, Johan Ferret, Geoffrey Cideron, Le Hou, Hongkun Yu, Amr Ahmed, Aranyak Mehta, Léonard Hussenot, Olivier Bachem, Edouard Leurent</author><pubDate>Mon, 22 Jul 2024 16:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15762v1</guid></item><item><title>Condition-Invariant Semantic Segmentation</title><link>http://arxiv.org/abs/2305.17349v3</link><description>Adaptation of semantic segmentation networks to different visual conditionsis vital for robust perception in autonomous cars and robots. However, previouswork has shown that most feature-level adaptation methods, which employadversarial training and are validated on synthetic-to-real adaptation, providemarginal gains in condition-level adaptation, being outperformed by simplepixel-level adaptation via stylization. Motivated by these findings, we proposeto leverage stylization in performing feature-level adaptation by aligning theinternal network features extracted by the encoder of the network from theoriginal and the stylized view of each input image with a novel featureinvariance loss. In this way, we encourage the encoder to extract features thatare already invariant to the style of the input, allowing the decoder to focuson parsing these features and not on further abstracting from the specificstyle of the input. We implement our method, named Condition-Invariant SemanticSegmentation (CISS), on the current state-of-the-art domain adaptationarchitecture and achieve outstanding results on condition-level adaptation. Inparticular, CISS sets the new state of the art in the populardaytime-to-nighttime Cityscapes$\to$Dark Zurich benchmark. Furthermore, ourmethod achieves the second-best performance on the normal-to-adverseCityscapes$\to$ACDC benchmark. CISS is shown to generalize well to domainsunseen during training, such as BDD100K-night and ACDC-night. Code is publiclyavailable at https://github.com/SysCV/CISS .</description><author>Christos Sakaridis, David Bruggemann, Fisher Yu, Luc Van Gool</author><pubDate>Mon, 22 Jul 2024 16:12:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17349v3</guid></item><item><title>Model editing for distribution shifts in uranium oxide morphological analysis</title><link>http://arxiv.org/abs/2407.15756v1</link><description>Deep learning still struggles with certain kinds of scientific data. Notably,pretraining data may not provide coverage of relevant distribution shifts(e.g., shifts induced via the use of different measurement instruments). Weconsider deep learning models trained to classify the synthesis conditions ofuranium ore concentrates (UOCs) and show that model editing is particularlyeffective for improving generalization to distribution shifts common in thisdomain. In particular, model editing outperforms finetuning on two curateddatasets comprising of micrographs taken of U$_{3}$O$_{8}$ aged in humiditychambers and micrographs acquired with different scanning electron microscopes,respectively.</description><author>Davis Brown, Cody Nizinski, Madelyn Shapiro, Corey Fallon, Tianzhixi Yin, Henry Kvinge, Jonathan H. Tu</author><pubDate>Mon, 22 Jul 2024 16:06:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15756v1</guid></item><item><title>LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding</title><link>http://arxiv.org/abs/2407.15754v1</link><description>Large multimodal models (LMMs) are processing increasingly longer and richerinputs. Albeit the progress, few public benchmark is available to measure suchdevelopment. To mitigate this gap, we introduce LongVideoBench, aquestion-answering benchmark that features video-language interleaved inputs upto an hour long. Our benchmark includes 3,763 varying-length web-collectedvideos with their subtitles across diverse themes, designed to comprehensivelyevaluate LMMs on long-term multimodal understanding. To achieve this, weinterpret the primary challenge as to accurately retrieve and reason overdetailed multimodal information from long inputs. As such, we formulate a novelvideo question-answering task termed referring reasoning. Specifically, as partof the question, it contains a referring query that references related videocontexts, called referred context. The model is then required to reason overrelevant video details from the referred context. Following the paradigm ofreferring reasoning, we curate 6,678 human-annotated multiple-choice questionsin 17 fine-grained categories, establishing one of the most comprehensivebenchmarks for long-form video understanding. Evaluations suggest that theLongVideoBench presents significant challenges even for the most advancedproprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while theiropen-source counterparts show an even larger performance gap. In addition, ourresults indicate that model performance on the benchmark improves only whenthey are capable of processing more frames, positioning LongVideoBench as avaluable benchmark for evaluating future-generation long-context LMMs.</description><author>Haoning Wu, Dongxu Li, Bei Chen, Junnan Li</author><pubDate>Mon, 22 Jul 2024 16:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15754v1</guid></item><item><title>Robustness of Speech Separation Models for Similar-pitch Speakers</title><link>http://arxiv.org/abs/2407.15749v1</link><description>Single-channel speech separation is a crucial task for enhancing speechrecognition systems in multi-speaker environments. This paper investigates therobustness of state-of-the-art Neural Network models in scenarios where thepitch differences between speakers are minimal. Building on earlier findings byDitter and Gerkmann, which identified a significant performance drop for the2018 Chimera++ under similar-pitch conditions, our study extends the analysisto more recent and sophisticated Neural Network models. Our experiments revealthat modern models have substantially reduced the performance gap for matchedtraining and testing conditions. However, a substantial performance gappersists under mismatched conditions, with models performing well for largepitch differences but showing worse performance if the speakers' pitches aresimilar. These findings motivate further research into the generalizability ofspeech separation models to similar-pitch speakers and unseen data.</description><author>Bunlong Lay, Sebastian Zaczek, Kristina Tesch, Timo Gerkmann</author><pubDate>Mon, 22 Jul 2024 15:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15749v1</guid></item><item><title>MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2407.15748v1</link><description>In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), thefirst specialised AI chatbot for cybersecurity. MoRSE aims to providecomprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG(Retrieval Augmented Generation) systems designed to retrieve and organizeinformation from multidimensional cybersecurity contexts. MoRSE differs fromtraditional RAGs by using parallel retrievers that work together to retrievesemantically related information in different formats and structures. Unliketraditional Large Language Models (LLMs) that rely on Parametric KnowledgeBases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Basesin response to user queries. Subsequently, MoRSE uses this information togenerate accurate answers. In addition, MoRSE benefits from real-time updatesto its knowledge bases, enabling continuous knowledge enrichment withoutretraining. We have evaluated the effectiveness of MoRSE against otherstate-of-the-art LLMs, evaluating the system on 600 cybersecurity specificquestions. The experimental evaluation has shown that the improvement in termsof relevance and correctness of the answer is more than 10\% compared to knownsolutions such as GPT-4 and Mixtral 7x8.</description><author>Marco Simoni, Andrea Saracino, Vinod P., Mauro Conti</author><pubDate>Mon, 22 Jul 2024 15:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15748v1</guid></item><item><title>Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond</title><link>http://arxiv.org/abs/2407.15739v1</link><description>In recent years, research on out-of-distribution (OoD) detection for semanticsegmentation has mainly focused on road scenes -- a domain with a constrainedamount of semantic diversity. In this work, we challenge this constraint andextend the domain of this task to general natural images. To this end, weintroduce: 1. the ADE-OoD benchmark, which is based on the ADE20k dataset andincludes images from diverse domains with a high semantic diversity, and 2. anovel approach that uses Diffusion score matching for OoD detection (DOoD) andis robust to the increased semantic diversity. ADE-OoD features indoor andoutdoor images, defines 150 semantic categories as in-distribution, andcontains a variety of OoD objects. For DOoD, we train a diffusion model with anMLP architecture on semantic in-distribution embeddings and build on the scorematching interpretation to compute pixel-wise OoD scores at inference time. Oncommon road scene OoD benchmarks, DOoD performs on par or better than the stateof the art, without using outliers for training or making assumptions about thedata domain. On ADE-OoD, DOoD outperforms previous approaches, but leaves muchroom for future improvements.</description><author>Silvio Galesso, Philipp Schröppel, Hssan Driss, Thomas Brox</author><pubDate>Mon, 22 Jul 2024 15:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15739v1</guid></item><item><title>Parallel Split Learning with Global Sampling</title><link>http://arxiv.org/abs/2407.15738v1</link><description>The expansion of IoT devices and the demands of Deep Learning havehighlighted significant challenges in Distributed Deep Learning (DDL) systems.Parallel Split Learning (PSL) has emerged as a promising derivative of SplitLearning that is well suited for distributed learning on resource-constraineddevices. However, PSL faces several obstacles, such as large effective batchsizes, non-IID data distributions, and the straggler effect. We view theseissues as a sampling dilemma and propose to address them by orchestrating themini-batch sampling process on the server side. We introduce the Uniform GlobalSampling (UGS) method to decouple the effective batch size from the number ofclients and reduce mini-batch deviation in non-IID settings. To address thestraggler effect, we introduce the Latent Dirichlet Sampling (LDS) method,which generalizes UGS to balance the trade-off between batch deviation andtraining time. Our simulations reveal that our proposed methods enhance modelaccuracy by up to 34.1% in non-IID settings and reduce the training time in thepresence of stragglers by up to 62%. In particular, LDS effectively mitigatesthe straggler effect without compromising model accuracy or adding significantcomputational overhead compared to UGS. Our results demonstrate the potentialof our methods as a promising solution for DDL in real applications.</description><author>Mohammad Kohankhaki, Ahmad Ayad, Mahdi Barhoush, Anke Schmeink</author><pubDate>Mon, 22 Jul 2024 15:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15738v1</guid></item><item><title>OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a German Migration Context</title><link>http://arxiv.org/abs/2407.15736v1</link><description>When immigrating to a new country, it is easy to feel overwhelmed by the needto obtain information on financial support, housing, schooling, languagecourses, and other issues. If relocation is rushed or even forced, thenecessity for high-quality answers to such questions is all the more urgent.Official immigration counselors are usually overbooked, and online systemscould guide newcomers to the requested information or a suitable counselingservice. To this end, we present OMoS-QA, a dataset of German and English questionspaired with relevant trustworthy documents and manually annotated answers,specifically tailored to this scenario. Questions are automatically generatedwith an open-source large language model (LLM) and answer sentences areselected by crowd workers with high agreement. With our data, we conduct acomparison of 5 pretrained LLMs on the task of extractive question answering(QA) in German and English. Across all models and both languages, we find highprecision and low-to-mid recall in selecting answer sentences, which is afavorable trade-off to avoid misleading users. This performance even holds upwhen the question language does not match the document language. When it comesto identifying unanswerable questions given a context, there are largerdifferences between the two languages.</description><author>Steffen Kleinle, Jakob Prange, Annemarie Friedrich</author><pubDate>Mon, 22 Jul 2024 15:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15736v1</guid></item><item><title>TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON</title><link>http://arxiv.org/abs/2407.15734v1</link><description>TaskGen is an open-sourced agentic framework which uses an Agent to solve anarbitrary task by breaking them down into subtasks. Each subtask is mapped toan Equipped Function or another Agent to execute. In order to reduce verbosity(and hence token usage), TaskGen uses StrictJSON that ensures JSON output fromthe Large Language Model (LLM), along with additional features such as typechecking and iterative error correction. Key to the philosophy of TaskGen isthe management of information/memory on a need-to-know basis. We empiricallyevaluate TaskGen on various environments such as 40x40 dynamic maze navigationwith changing obstacle locations (100% solve rate), TextWorld escape roomsolving with dense rewards and detailed goals (96% solve rate), web browsing(69% of actions successful), solving the MATH dataset (71% solve rate over 100Level-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset(F1 score of 47.03%)</description><author>John Chong Min Tan, Prince Saroj, Bharat Runwal, Hardik Maheshwari, Brian Lim Yi Sheng, Richard Cottrill, Alankrit Chona, Ambuj Kumar, Mehul Motani</author><pubDate>Mon, 22 Jul 2024 15:37:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15734v1</guid></item><item><title>Simulation-Based Inference with Quantile Regression</title><link>http://arxiv.org/abs/2401.02413v2</link><description>We present Neural Quantile Estimation (NQE), a novel Simulation-BasedInference (SBI) method based on conditional quantile regression. NQEautoregressively learns individual one dimensional quantiles for each posteriordimension, conditioned on the data and previous posterior dimensions. Posteriorsamples are obtained by interpolating the predicted quantiles using monotoniccubic Hermite spline, with specific treatment for the tail behavior andmulti-modal distributions. We introduce an alternative definition for theBayesian credible region using the local Cumulative Density Function (CDF),offering substantially faster evaluation than the traditional Highest PosteriorDensity Region (HPDR). In case of limited simulation budget and/or known modelmisspecification, a post-processing calibration step can be integrated into NQEto ensure the unbiasedness of the posterior estimation with negligibleadditional computational cost. We demonstrate that NQE achievesstate-of-the-art performance on a variety of benchmark problems.</description><author>He Jia</author><pubDate>Mon, 22 Jul 2024 15:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02413v2</guid></item><item><title>Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language Encoders</title><link>http://arxiv.org/abs/2407.15731v1</link><description>Despite the proliferation of large vision-language foundation models,estimation of the learning and forgetting outcomes following fine-tuning ofthese models remains largely unexplored. Inspired by work highlighting thesignificance of the modality gap in contrastive dual-encoders, we propose theInter-Intra Modal Measure (IIMM). Combining terms quantifying the similaritybetween image embeddings and the similarity between incorrect image and labelembedding pairs, the IIMM functions as a strong predictor of performancechanges with fine-tuning. Our extensive empirical analysis across fourstate-of-the-art vision-language models (CLIP, SigLIP, CoCa, EVA-02-CLIP) andfive fine-tuning techniques (full fine-tuning, BitFit, attention-weight tuning,LoRA, CLIP-Adapter) demonstrates a strong, statistically significant linearrelationship: fine-tuning on tasks with higher IIMM scores produces greaterin-domain performance gains but also induces more severe out-of-domainperformance degradation, with some parameter-efficient fine-tuning (PEFT)methods showing extreme forgetting. We compare our measure against transferscores from state-of-the-art model selection methods and show that the IIMM issignificantly more predictive of accuracy gains. With only a single forwardpass of the target data, practitioners can leverage this key insight toheuristically evaluate the degree to which a model can be expected to improvefollowing fine-tuning. Given additional knowledge about the model's performanceon a few diverse tasks, this heuristic further evolves into a strong predictorof expected performance changes when training for new tasks.</description><author>Laura Niss, Kevin Vogt-Lowell, Theodoros Tsiligkaridis</author><pubDate>Mon, 22 Jul 2024 15:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15731v1</guid></item><item><title>Explorative Imitation Learning: A Path Signature Approach for Continuous Environments</title><link>http://arxiv.org/abs/2407.04856v2</link><description>Some imitation learning methods combine behavioural cloning withself-supervision to infer actions from state pairs. However, most rely on alarge number of expert trajectories to increase generalisation and humanintervention to capture key aspects of the problem, such as domain constraints.In this paper, we propose Continuous Imitation Learning from Observation(CILO), a new method augmenting imitation learning with two important features:(i) exploration, allowing for more diverse state transitions, requiring lessexpert trajectories and resulting in fewer training iterations; and (ii) pathsignatures, allowing for automatic encoding of constraints, through thecreation of non-parametric representations of agents and expert trajectories.We compared CILO with a baseline and two leading imitation learning methods infive environments. It had the best overall performance of all methods in allenvironments, outperforming the expert in two of them.</description><author>Nathan Gavenski, Juarez Monteiro, Felipe Meneguzzi, Michael Luck, Odinaldo Rodrigues</author><pubDate>Mon, 22 Jul 2024 15:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04856v2</guid></item><item><title>SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder</title><link>http://arxiv.org/abs/2402.07370v2</link><description>Face swapping has gained significant attention for its varied applications.Most previous face swapping approaches have relied on the seesaw game trainingscheme, also known as the target-oriented approach. However, this often leadsto instability in model training and results in undesired samples with blendedidentities due to the target identity leakage problem. Source-oriented methodsachieve more stable training with self-reconstruction objective but often failto accurately reflect target image's skin color and illumination. This paperintroduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, anovel self-supervised approach that combines the strengths of bothtarget-oriented and source-oriented approaches. Our training scheme addressesthe limitations of traditional training methods by circumventing theconventional seesaw game and introducing clear ground truth through itsself-reconstruction training regime. Our model effectively mitigates identityleakage and reflects target albedo and illumination through learneddisentangled identity and non-identity features. Additionally, we closelytackle the shape misalignment and volume discrepancy problems with newtechniques, including perforation confusion and random mesh scaling. SAMAEestablishes a new state-of-the-art, surpassing other baseline methods,preserving both identity and non-identity attributes without sacrificing oneither aspect.</description><author>Jaeseong Lee, Junha Hyung, Sohyun Jeong, Jaegul Choo</author><pubDate>Mon, 22 Jul 2024 15:32:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07370v2</guid></item><item><title>SAM2CLIP2SAM: Vision Language Model for Segmentation of 3D CT Scans for Covid-19 Detection</title><link>http://arxiv.org/abs/2407.15728v1</link><description>This paper presents a new approach for effective segmentation of images thatcan be integrated into any model and methodology; the paradigm that we chooseis classification of medical images (3-D chest CT scans) for Covid-19detection. Our approach includes a combination of vision-language models thatsegment the CT scans, which are then fed to a deep neural architecture, namedRACNet, for Covid-19 detection. In particular, a novel framework, namedSAM2CLIP2SAM, is introduced for segmentation that leverages the strengths ofboth Segment Anything Model (SAM) and Contrastive Language-Image Pre-Training(CLIP) to accurately segment the right and left lungs in CT scans, subsequentlyfeeding these segmented outputs into RACNet for classification of COVID-19 andnon-COVID-19 cases. At first, SAM produces multiple part-based segmentationmasks for each slice in the CT scan; then CLIP selects only the masks that areassociated with the regions of interest (ROIs), i.e., the right and left lungs;finally SAM is given these ROIs as prompts and generates the final segmentationmask for the lungs. Experiments are presented across two Covid-19 annotateddatabases which illustrate the improved performance obtained when our methodhas been used for segmentation of the CT scans.</description><author>Dimitrios Kollias, Anastasios Arsenos, James Wingate, Stefanos Kollias</author><pubDate>Mon, 22 Jul 2024 15:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15728v1</guid></item><item><title>Inferring turbulent velocity and temperature fields and their statistics from Lagrangian velocity measurements using physics-informed Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2407.15727v1</link><description>We propose the Artificial Intelligence Velocimetry-Thermometry (AIVT) methodto infer hidden temperature fields from experimental turbulent velocity data.This physics-informed machine learning method enables us to infer continuoustemperature fields using only sparse velocity data, hence eliminating the needfor direct temperature measurements. Specifically, AIVT is based onphysics-informed Kolmogorov-Arnold Networks (not neural networks) and istrained by optimizing a combined loss function that minimizes the residuals ofthe velocity data, boundary conditions, and the governing equations. We applyAIVT to a unique set of experimental volumetric and simultaneous temperatureand velocity data of Rayleigh-B\'enard convection (RBC) that we acquired bycombining Particle Image Thermometry and Lagrangian Particle Tracking. Thisallows us to compare AIVT predictions and measurements directly. We demonstratethat we can reconstruct and infer continuous and instantaneous velocity andtemperature fields from sparse experimental data at a fidelity comparable todirect numerical simulations (DNS) of turbulence. This, in turn, enables us tocompute important quantities for quantifying turbulence, such as fluctuations,viscous and thermal dissipation, and QR distribution. This paradigm shift inprocessing experimental data using AIVT to infer turbulent fields at DNS-levelfidelity is a promising avenue in breaking the current deadlock of quantitativeunderstanding of turbulence at high Reynolds numbers, where DNS iscomputationally infeasible.</description><author>Juan Diego Toscano, Theo Käufer, Martin Maxey, Christian Cierpka, George Em Karniadakis</author><pubDate>Mon, 22 Jul 2024 15:30:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15727v1</guid></item><item><title>Beyond Memorization: The Challenge of Random Memory Access in Language Models</title><link>http://arxiv.org/abs/2403.07805v3</link><description>Recent developments in Language Models (LMs) have shown their effectivenessin NLP tasks, particularly in knowledge-intensive tasks. However, themechanisms underlying knowledge storage and memory access within theirparameters remain elusive. In this paper, we investigate whether a generativeLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Throughcarefully-designed synthetic tasks, covering the scenarios of full recitation,selective recitation and grounded question answering, we reveal that LMs manageto sequentially access their memory while encountering challenges in randomlyaccessing memorized content. We find that techniques including recitation andpermutation improve the random memory access capability of LMs. Furthermore, byapplying this intervention to realistic scenarios of open-domain questionanswering, we validate that enhancing random access by recitation leads tonotable improvements in question answering. The code to reproduce ourexperiments can be found at https://github.com/sail-sg/lm-random-memory-access.</description><author>Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min Lin</author><pubDate>Mon, 22 Jul 2024 15:29:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07805v3</guid></item><item><title>Beyond Size and Class Balance: Alpha as a New Dataset Quality Metric for Deep Learning</title><link>http://arxiv.org/abs/2407.15724v1</link><description>In deep learning, achieving high performance on image classification tasksrequires diverse training sets. However, dataset diversity is incompletelyunderstood. The current best practice is to try to maximize dataset size andclass balance. Yet large, class-balanced datasets are not guaranteed to bediverse: images can still be arbitrarily similar. We hypothesized that, for agiven model architecture, better model performance can be achieved bymaximizing dataset diversity more directly. This could open a path forperformance improvement without additional computational resources orarchitectural advances. To test this hypothesis, we introduce a comprehensiveframework of diversity measures, developed in ecology, that generalizesfamiliar quantities like Shannon entropy by accounting for similarities anddifferences among images. (Dataset size and class balance emerge from thisframework as special cases.) By analyzing thousands of subsets from sevenmedical datasets representing ultrasound, X-ray, CT, and pathology images, wefound that the best correlates of performance were not size or class balancebut $A$ -- ``big alpha'' -- a set of generalized entropy measures interpretedas the effective number of image-class pairs in the dataset, after accountingfor similarities among images. One of these, $A_0$, explained 67\% of thevariance in balanced accuracy across all subsets, vs. 54\% for class balanceand just 39\% for size. The best pair was size and $A_1$ (79\%), whichoutperformed size and class balance (74\%). $A$ performed best for subsets fromindividual datasets as well as across datasets, supporting the generality ofthese results. We propose maximizing $A$ as a potential new way to improve theperformance of deep learning in medical imaging.</description><author>Josiah Couch, Ramy Arnaout, Rima Arnaout</author><pubDate>Mon, 22 Jul 2024 15:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15724v1</guid></item><item><title>DStruct2Design: Data and Benchmarks for Data Structure Driven Generative Floor Plan Design</title><link>http://arxiv.org/abs/2407.15723v1</link><description>Text conditioned generative models for images have yielded impressiveresults. Text conditioned floorplan generation as a special type of rasterimage generation task also received particular attention. However there aremany use cases in floorpla generation where numerical properties of thegenerated result are more important than the aesthetics. For instance, onemight want to specify sizes for certain rooms in a floorplan and compare thegenerated floorplan with given specifications Current approaches, datasets andcommonly used evaluations do not support these kinds of constraints. As such,an attractive strategy is to generate an intermediate data structure thatcontains numerical properties of a floorplan which can be used to generate thefinal floorplan image. To explore this setting we (1) construct a new datasetfor this data-structure to data-structure formulation of floorplan generationusing two popular image based floorplan datasets RPLAN and ProcTHOR-10k, andprovide the tools to convert further procedurally generated ProcTHOR floorplandata into our format. (2) We explore the task of floorplan generation given apartial or complete set of constraints and we design a series of metrics andbenchmarks to enable evaluating how well samples generated from models respectthe constraints. (3) We create multiple baselines by finetuning a largelanguage model (LLM), Llama3, and demonstrate the feasibility of usingfloorplan data structure conditioned LLMs for the problem of floorplangeneration respecting numerical constraints. We hope that our new datasets andbenchmarks will encourage further research on different ways to improve theperformance of LLMs and other generative modelling techniques for generatingdesigns where quantitative constraints are only partially specified, but mustbe respected.</description><author>Zhi Hao Luo, Luis Lara, Ge Ya Luo, Florian Golemo, Christopher Beckham, Christopher Pal</author><pubDate>Mon, 22 Jul 2024 15:27:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15723v1</guid></item><item><title>Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability</title><link>http://arxiv.org/abs/2407.15720v1</link><description>Large language models (LLMs) have emerged as powerful tools for many AIproblems and exhibit remarkable in-context learning (ICL) capabilities.Compositional ability, solving unseen complex tasks that combine two or moresimple tasks, is an essential reasoning ability for Artificial GeneralIntelligence. Despite LLM's tremendous success, how they approach compositetasks, especially those not encountered during the pretraining phase, remainsan open question and largely ununderstood. In this study, we delve into the ICLcapabilities of LLMs on composite tasks, with only simple tasks as in-contextexamples. We develop a test suite of composite tasks that include linguisticand logical challenges and perform empirical studies across different LLMfamilies. We observe that models exhibit divergent behaviors: (1) For simplercomposite tasks that apply distinct mapping mechanisms to different inputsegments, the models demonstrate decent compositional ability, while scaling upthe model enhances this ability; (2) for more complex composite tasks thatinvolving reasoning multiple steps, where each step represent one task, modelstypically underperform, and scaling up generally provide no improvements. Weoffer theoretical analysis in a simplified setting, explaining that modelsexhibit compositional capability when the task handles different input partsseparately. We believe our work sheds new light on the capabilities of LLMs insolving composite tasks regarding the nature of the tasks and model scale. Ourdataset and code are available at{\url{https://github.com/OliverXUZY/LLM_Compose}}.</description><author>Zhuoyan Xu, Zhenmei Shi, Yingyu Liang</author><pubDate>Mon, 22 Jul 2024 15:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15720v1</guid></item><item><title>GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI</title><link>http://arxiv.org/abs/2407.15719v1</link><description>Alzheimer's Disease (AD) is an irreversible neurodegenerative disorder thatoften progresses from Mild Cognitive Impairment (MCI), leading to memory lossand significantly impacting patients' lives. Clinical trials indicate thatearly targeted interventions for MCI patients can potentially slow or halt thedevelopment and progression of AD. Previous research has shown that accuratemedical classification requires the inclusion of extensive multimodal data,such as assessment scales and various neuroimaging techniques like MagneticResonance Imaging (MRI) and Positron Emission Tomography (PET). However,consistently tracking the diagnosis of the same individual over time andsimultaneously collecting multimodal data poses significant challenges. Toaddress this issue, we introduce GFE-Mamba, a classifier based on GenerativeFeature Extraction (GFE). This classifier effectively integrates data fromassessment scales, MRI, and PET, enabling deeper multimodal fusion. Itefficiently extracts both long and short sequence information and incorporatesadditional information beyond the pixel space. This approach not only improvesclassification accuracy but also enhances the interpretability and stability ofthe model. We constructed datasets of over 3000 samples based on theAlzheimer's Disease Neuroimaging Initiative (ADNI) for a two-step trainingprocess. Our experimental results demonstrate that the GFE-Mamba model iseffective in predicting the conversion from MCI to AD and outperforms severalstate-of-the-art methods. Our source code and ADNI dataset processing code areavailable at https://github.com/Tinysqua/GFE-Mamba.</description><author>Zhaojie Fang, Shenghao Zhu, Yifei Chen, Binfeng Zou, Fan Jia, Linwei Qiu, Chang Liu, Yiyu Huang, Xiang Feng, Feiwei Qin, Changmiao Wang, Yeru Wang, Jin Fan, Changbiao Chu, Wan-Zhen Wu, Hu Zhao</author><pubDate>Mon, 22 Jul 2024 15:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15719v1</guid></item><item><title>Harmonizing Flows: Leveraging normalizing flows for unsupervised and source-free MRI harmonization</title><link>http://arxiv.org/abs/2407.15717v1</link><description>Lack of standardization and various intrinsic parameters for magneticresonance (MR) image acquisition results in heterogeneous images acrossdifferent sites and devices, which adversely affects the generalization of deepneural networks. To alleviate this issue, this work proposes a novelunsupervised harmonization framework that leverages normalizing flows to alignMR images, thereby emulating the distribution of a source domain. The proposedstrategy comprises three key steps. Initially, a normalizing flow network istrained to capture the distribution characteristics of the source domain. Then,we train a shallow harmonizer network to reconstruct images from the sourcedomain via their augmented counterparts. Finally, during inference, theharmonizer network is updated to ensure that the output images conform to thelearned source domain distribution, as modeled by the normalizing flow network.Our approach, which is unsupervised, source-free, and task-agnostic is assessedin the context of both adults and neonatal cross-domain brain MRI segmentation,as well as neonatal brain age estimation, demonstrating its generalizabilityacross tasks and population demographics. The results underscore its superiorperformance compared to existing methodologies. The code is available athttps://github.com/farzad-bz/Harmonizing-Flows</description><author>Farzad Beizaee, Gregory A. Lodygensky, Chris L. Adamson, Deanne K. Thompso, Jeanie L. Y. Cheon, Alicia J. Spittl. Peter J. Anderso, Christian Desrosier, Jose Dolz</author><pubDate>Mon, 22 Jul 2024 15:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15717v1</guid></item><item><title>Mamba meets crack segmentation</title><link>http://arxiv.org/abs/2407.15714v1</link><description>Cracks pose safety risks to infrastructure and cannot be overlooked. Theprevailing structures in existing crack segmentation networks predominantlyconsist of CNNs or Transformers. However, CNNs exhibit a deficiency in globalmodeling capability, hindering the representation to entire crack features.Transformers can capture long-range dependencies but suffer from high andquadratic complexity. Recently, Mamba has garnered extensive attention due toits linear spatial and computational complexity and its powerful globalperception. This study explores the representation capabilities of Mamba tocrack features. Specifically, this paper uncovers the connection between Mambaand the attention mechanism, providing a profound insight, an attentionperspective, into interpreting Mamba and devising a novel Mamba modulefollowing the principles of attention blocks, namely CrackMamba. We compareCrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on twodatasets comprising asphalt pavement and concrete pavement cracks, and steelcracks, respectively. The quantitative results show that CrackMamba stands outas the sole Mamba block consistently enhancing the baseline model's performanceacross all evaluation measures, while reducing its parameters and computationalcosts. Moreover, this paper substantiates that Mamba can achieve globalreceptive fields through both theoretical analysis and visual interpretability.The discoveries of this study offer a dual contribution. First, as aplug-and-play and simple yet effective Mamba module, CrackMamba exhibitsimmense potential for integration into various crack segmentation models.Second, the proposed innovative Mamba design concept, integrating Mamba withthe attention mechanism, holds significant reference value for all Mamba-basedcomputer vision models, not limited to crack segmentation networks, asinvestigated in this study.</description><author>Zhili He, Yu-Hsing Wang</author><pubDate>Mon, 22 Jul 2024 15:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15714v1</guid></item><item><title>AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?</title><link>http://arxiv.org/abs/2407.15711v1</link><description>Language agents, built on top of language models (LMs), are systems that caninteract with complex environments, such as the open web. In this work, weexamine whether such agents can perform realistic and time-consuming tasks onthe web, e.g., monitoring real-estate markets or locating relevant nearbybusinesses. We introduce AssistantBench, a challenging new benchmark consistingof 214 realistic tasks that can be automatically evaluated, covering differentscenarios and domains. We find that AssistantBench exposes the limitations ofcurrent systems, including language models and retrieval-augmented languagemodels, as no model reaches an accuracy of more than 25 points. Whileclosed-book LMs perform well, they exhibit low precision since they tend tohallucinate facts. State-of-the-art web agents reach a score of near zero.Additionally, we introduce SeePlanAct (SPA), a new web agent that significantlyoutperforms previous agents, and an ensemble of SPA and closed-book modelsreaches the best overall performance. Moreover, we analyze failures of currentsystems and highlight that web navigation remains a major challenge.</description><author>Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, Jonathan Berant</author><pubDate>Mon, 22 Jul 2024 15:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15711v1</guid></item><item><title>SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams</title><link>http://arxiv.org/abs/2407.15708v1</link><description>The spike camera, with its high temporal resolution, low latency, and highdynamic range, addresses high-speed imaging challenges like motion blur. Itcaptures photons at each pixel independently, creating binary spike streamsrich in temporal information but challenging for image reconstruction. Currentalgorithms, both traditional and deep learning-based, still need to be improvedin the utilization of the rich temporal detail and the restoration of thedetails of the reconstructed image. To overcome this, we introduce SwinSpikeformer (SwinSF), a novel model for dynamic scene reconstruction from spikestreams. SwinSF is composed of Spike Feature Extraction, Spatial-TemporalFeature Extraction, and Final Reconstruction Module. It combines shifted windowself-attention and proposed temporal spike attention, ensuring a comprehensivefeature extraction that encapsulates both spatial and temporal dynamics,leading to a more robust and accurate reconstruction of spike streams.Furthermore, we build a new synthesized dataset for spike image reconstructionwhich matches the resolution of the latest spike camera, ensuring its relevanceand applicability to the latest developments in spike camera imaging.Experimental results demonstrate that the proposed network SwinSF sets a newbenchmark, achieving state-of-the-art performance across a series of datasets,including both real-world and synthesized data across various resolutions. Ourcodes and proposed dataset will be available soon.</description><author>Liangyan Jiang, Chuang Zhu, Yanxu Chen</author><pubDate>Mon, 22 Jul 2024 15:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15708v1</guid></item><item><title>Predicting the Best of N Visual Trackers</title><link>http://arxiv.org/abs/2407.15707v1</link><description>We observe that the performance of SOTA visual trackers surprisingly stronglyvaries across different video attributes and datasets. No single trackerremains the best performer across all tracking attributes and datasets. Tobridge this gap, for a given video sequence, we predict the "Best of the NTrackers", called the BofN meta-tracker. At its core, a Tracking PerformancePrediction Network (TP2N) selects a predicted best performing visual trackerfor the given video sequence using only a few initial frames. We also introducea frame-level BofN meta-tracker which keeps predicting best performer afterregular temporal intervals. The TP2N is based on self-supervised learningarchitectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO withViT-S as a backbone performs the best. The video-level BofN meta-trackeroutperforms, by a large margin, existing SOTA trackers on nine standardbenchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123,OTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofNmeta-tracker effectively handling variations in the tracking scenarios withinlong sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is88.7% and 91.1% with video and frame-level settings respectively. The bestperforming tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected averageoverlap is 67.88% and 70.98% with video and frame level settings, compared tothe best performing ARTrack, 64.12%. This work also presents an extensiveevaluation of competitive tracking methods on all commonly used benchmarks,following their protocols. The code, the trained models, and the results willsoon be made publicly available onhttps://github.com/BasitAlawode/Best_of_N_Trackers.</description><author>Basit Alawode, Sajid Javed, Arif Mahmood, Jiri Matas</author><pubDate>Mon, 22 Jul 2024 15:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15707v1</guid></item><item><title>Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2407.15706v1</link><description>Skeleton-based action recognition has garnered significant attention due tothe utilization of concise and resilient skeletons. Nevertheless, the absenceof detailed body information in skeletons restricts performance, while othermultimodal methods require substantial inference resources and are inefficientwhen using multimodal data during both training and inference stages. Toaddress this and fully harness the complementary multimodal features, wepropose a novel multi-modality co-learning (MMCL) framework by leveraging themultimodal large language models (LLMs) as auxiliary networks for efficientskeleton-based action recognition, which engages in multi-modality co-learningduring the training stage and keeps efficiency by employing only conciseskeletons in inference. Our MMCL framework primarily consists of two modules.First, the Feature Alignment Module (FAM) extracts rich RGB features from videoframes and aligns them with global skeleton features via contrastive learning.Second, the Feature Refinement Module (FRM) uses RGB images with temporalinformation and text instruction to generate instructive features based on thepowerful generalization of multimodal LLMs. These instructive text featureswill further refine the classification scores and the refined scores willenhance the model's robustness and generalization in a manner similar to softlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLAbenchmarks consistently verify the effectiveness of our MMCL, which outperformsthe existing skeleton-based action recognition methods. Meanwhile, experimentson UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalizationof our MMCL in zero-shot and domain-adaptive action recognition. Our code ispublicly available at: https://github.com/liujf69/MMCL-Action.</description><author>Jinfu Liu, Chen Chen, Mengyuan Liu</author><pubDate>Mon, 22 Jul 2024 15:16:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15706v1</guid></item><item><title>SEGIC: Unleashing the Emergent Correspondence for In-Context Segmentation</title><link>http://arxiv.org/abs/2311.14671v3</link><description>In-context segmentation aims at segmenting novel images using a few labeledexample images, termed as "in-context examples", exploring content similaritiesbetween examples and the target. The resulting models can be generalizedseamlessly to novel segmentation tasks, significantly reducing the labeling andtraining costs compared with conventional pipelines. However, in-contextsegmentation is more challenging than classic ones requiring the model to learnsegmentation rules conditioned on a few samples. Unlike previous work withad-hoc or non-end-to-end designs, we propose SEGIC, an end-to-endsegment-in-context framework built upon a single vision foundation model (VFM).In particular, SEGIC leverages the emergent correspondence within VFM tocapture dense relationships between target images and in-context samples. Assuch, information from in-context samples is then extracted into three types ofinstructions, i.e. geometric, visual, and meta instructions, serving asexplicit conditions for the final mask prediction. SEGIC is a straightforwardyet effective approach that yields state-of-the-art performance on one-shotsegmentation benchmarks. Notably, SEGIC can be easily generalized to diversetasks, including video object segmentation and open-vocabulary segmentation.Code will be available at https://github.com/MengLcool/SEGIC.</description><author>Lingchen Meng, Shiyi Lan, Hengduo Li, Jose M. Alvarez, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Mon, 22 Jul 2024 15:16:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14671v3</guid></item><item><title>Estimating Probability Densities with Transformer and Denoising Diffusion</title><link>http://arxiv.org/abs/2407.15703v1</link><description>Transformers are often the go-to architecture to build foundation models thatingest a large amount of training data. But these models do not estimate theprobability density distribution when trained on regression problems, yetobtaining full probabilistic outputs is crucial to many fields of science,where the probability distribution of the answer can be non-Gaussian andmultimodal. In this work, we demonstrate that training a probabilistic modelusing a denoising diffusion head on top of the Transformer provides reasonableprobability density estimation even for high-dimensional inputs. The combinedTransformer+Denoising Diffusion model allows conditioning the outputprobability density on arbitrary combinations of inputs and it is thus a highlyflexible density function emulator of all possible input/output combinations.We illustrate our Transformer+Denoising Diffusion model by training it on alarge dataset of astronomical observations and measured labels of stars withinour Galaxy and we apply it to a variety of inference tasks to show that themodel can infer labels accurately with reasonable distributions.</description><author>Henry W. Leung, Jo Bovy, Joshua S. Speagle</author><pubDate>Mon, 22 Jul 2024 15:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15703v1</guid></item><item><title>Geometric Analysis of Unconstrained Feature Models with $d=K$</title><link>http://arxiv.org/abs/2407.10702v2</link><description>Recently, interesting empirical phenomena known as Neural Collapse have beenobserved during the final phase of training deep neural networks forclassification tasks. We examine this issue when the feature dimension d isequal to the number of classes K. We demonstrate that two popular unconstrainedfeature models are strict saddle functions, with every critical point beingeither a global minimum or a strict saddle point that can be exited usingnegative curvatures. The primary findings conclusively confirm the conjectureon the unconstrained feature models in previous articles.</description><author>Yi Shen, Shao Gu</author><pubDate>Mon, 22 Jul 2024 15:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10702v2</guid></item><item><title>A Life-long Learning Intrusion Detection System for 6G-Enabled IoV</title><link>http://arxiv.org/abs/2407.15700v1</link><description>The introduction of 6G technology into the Internet of Vehicles (IoV)promises to revolutionize connectivity with ultra-high data rates and seamlessnetwork coverage. However, this technological leap also brings significantchallenges, particularly for the dynamic and diverse IoV landscape, which mustmeet the rigorous reliability and security requirements of 6G networks.Furthermore, integrating 6G will likely increase the IoV's susceptibility to aspectrum of emerging cyber threats. Therefore, it is crucial for securitymechanisms to dynamically adapt and learn new attack patterns, keeping pacewith the rapid evolution and diversification of these threats - a capabilitycurrently lacking in existing systems. This paper presents a novel intrusiondetection system leveraging the paradigm of life-long (or continual) learning.Our methodology combines class-incremental learning with federated learning, anapproach ideally suited to the distributed nature of the IoV. This strategyeffectively harnesses the collective intelligence of Connected and AutomatedVehicles (CAVs) and edge computing capabilities to train the detection system.To the best of our knowledge, this study is the first to synergizeclass-incremental learning with federated learning specifically for cyberattack detection. Through comprehensive experiments on a recent network trafficdataset, our system has exhibited a robust adaptability in learning new cyberattack patterns, while effectively retaining knowledge of previouslyencountered ones. Additionally, it has proven to maintain high accuracy and alow false positive rate.</description><author>Abdelaziz Amara korba, Souad Sebaa, Malik Mabrouki, Yacine Ghamri-Doudane, Karima Benatchba</author><pubDate>Mon, 22 Jul 2024 15:07:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15700v1</guid></item><item><title>PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans</title><link>http://arxiv.org/abs/2403.18328v3</link><description>Information from neuroimaging examinations is increasingly used to supportdiagnoses of dementia, e.g., Alzheimer's disease. While current clinicalpractice is mainly based on visual inspection and feature engineering, DeepLearning approaches can be used to automate the analysis and to discover newimage-biomarkers. Part-prototype neural networks (PP-NN) are an alternative tostandard blackbox models, and have shown promising results in general computervision. PP-NN's base their reasoning on prototypical image regions that arelearned fully unsupervised, and combined with a simple-to-understand decisionlayer. We present PIPNet3D, a PP-NN for volumetric images. We apply PIPNet3D tothe clinical diagnosis of Alzheimer's Disease from structural MagneticResonance Imaging (sMRI). We assess the quality of prototypes under asystematic evaluation framework, propose new functionally grounded metrics toevaluate brain prototypes and develop an evaluation scheme to assess theircoherency with domain experts. Our results show that PIPNet3D is aninterpretable, compact model for Alzheimer's diagnosis with its reasoning wellaligned to medical domain knowledge. Notably, PIPNet3D achieves the sameaccuracy as its blackbox counterpart; and removing the remaining clinicallyirrelevant prototypes from its decision process does not decrease predictiveperformance.</description><author>Lisa Anita De Santi, Jörg Schlötterer, Michael Scheschenja, Joel Wessendorf, Meike Nauta, Vincenzo Positano, Christin Seifert</author><pubDate>Mon, 22 Jul 2024 15:04:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18328v3</guid></item><item><title>Multimodal Explainability via Latent Shift applied to COVID-19 stratification</title><link>http://arxiv.org/abs/2212.14084v2</link><description>We are witnessing a widespread adoption of artificial intelligence inhealthcare. However, most of the advancements in deep learning in this areaconsider only unimodal data, neglecting other modalities. Their multimodalinterpretation necessary for supporting diagnosis, prognosis and treatmentdecisions. In this work we present a deep architecture, which jointly learnsmodality reconstructions and sample classifications using tabular and imagingdata. The explanation of the decision taken is computed by applying a latentshift that, simulates a counterfactual prediction revealing the features ofeach modality that contribute the most to the decision and a quantitative scoreindicating the modality importance. We validate our approach in the context ofCOVID-19 pandemic using the AIforCOVID dataset, which contains multimodal datafor the early identification of patients at risk of severe outcome. The resultsshow that the proposed method provides meaningful explanations withoutdegrading the classification performance.</description><author>Valerio Guarrasi, Lorenzo Tronchin, Domenico Albano, Eliodoro Faiella, Deborah Fazzini, Domiziana Santucci, Paolo Soda</author><pubDate>Mon, 22 Jul 2024 15:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14084v2</guid></item><item><title>Patch-based Intuitive Multimodal Prototypes Network (PIMPNet) for Alzheimer's Disease classification</title><link>http://arxiv.org/abs/2407.14277v2</link><description>Volumetric neuroimaging examinations like structural Magnetic ResonanceImaging (sMRI) are routinely applied to support the clinical diagnosis ofdementia like Alzheimer's Disease (AD). Neuroradiologists examine 3D sMRI todetect and monitor abnormalities in brain morphology due to AD, like globaland/or local brain atrophy and shape alteration of characteristic structures.There is a strong research interest in developing diagnostic systems based onDeep Learning (DL) models to analyse sMRI for AD. However, anatomicalinformation extracted from an sMRI examination needs to be interpreted togetherwith patient's age to distinguish AD patterns from the regular alteration dueto a normal ageing process. In this context, part-prototype neural networksintegrate the computational advantages of DL in an interpretable-by-designarchitecture and showed promising results in medical imaging applications. Wepresent PIMPNet, the first interpretable multimodal model for 3D images anddemographics applied to the binary classification of AD from 3D sMRI andpatient's age. Despite age prototypes do not improve predictive performancecompared to the single modality model, this lays the foundation for future workin the direction of the model's design and multimodal prototype trainingprocess</description><author>Lisa Anita De Santi, Jörg Schlötterer, Meike Nauta, Vincenzo Positano, Christin Seifert</author><pubDate>Mon, 22 Jul 2024 15:02:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14277v2</guid></item><item><title>Supporting the Digital Autonomy of Elders Through LLM Assistance</title><link>http://arxiv.org/abs/2407.15695v1</link><description>The internet offers tremendous access to services, social connections, andneeded products. However, to those without sufficient experience, engaging withbusinesses and friends across the internet can be daunting due to the everpresent danger of scammers and thieves, to say nothing of the myriad ofpotential computer viruses. Like a forest rich with both edible and poisonousplants, those familiar with the norms inhabit it safely with ease whilenewcomers need a guide. However, reliance on a human digital guide can betaxing and often impractical. We propose and pilot a simple but unexploredidea: could an LLM provide the necessary support to help the elderly who areseparated by the digital divide safely achieve digital autonomy?</description><author>Jesse Roberts, Lindsey Roberts, Alice Reed</author><pubDate>Mon, 22 Jul 2024 15:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15695v1</guid></item><item><title>Counter Turing Test ($CT^2$): Investigating AI-Generated Text Detection for Hindi -- Ranking LLMs based on Hindi AI Detectability Index ($ADI_{hi}$)</title><link>http://arxiv.org/abs/2407.15694v1</link><description>The widespread adoption of large language models (LLMs) and awareness aroundmultilingual LLMs have raised concerns regarding the potential risks andrepercussions linked to the misapplication of AI-generated text, necessitatingincreased vigilance. While these models are primarily trained for English,their extensive training on vast datasets covering almost the entire web,equips them with capabilities to perform well in numerous other languages.AI-Generated Text Detection (AGTD) has emerged as a topic that has alreadyreceived immediate attention in research, with some initial methods having beenproposed, soon followed by the emergence of techniques to bypass detection. Inthis paper, we report our investigation on AGTD for an indic language Hindi.Our major contributions are in four folds: i) examined 26 LLMs to evaluatetheir proficiency in generating Hindi text, ii) introducing the AI-generatednews article in Hindi ($AG_{hi}$) dataset, iii) evaluated the effectiveness offive recently proposed AGTD techniques: ConDA, J-Guard, RADAR, RAIDAR andIntrinsic Dimension Estimation for detecting AI-generated Hindi text, iv)proposed Hindi AI Detectability Index ($ADI_{hi}$) which shows a spectrum tounderstand the evolving landscape of eloquence of AI-generated text in Hindi.We will make the codes and datasets available to encourage further research.</description><author>Ishan Kavathekar, Anku Rani, Ashmit Chamoli, Ponnurangam Kumaraguru, Amit Sheth, Amitava Das</author><pubDate>Mon, 22 Jul 2024 15:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15694v1</guid></item><item><title>Predictive Coding Networks and Inference Learning: Tutorial and Survey</title><link>http://arxiv.org/abs/2407.04117v2</link><description>Recent years have witnessed a growing call for renewed emphasis onneuroscience-inspired approaches in artificial intelligence research, under thebanner of NeuroAI. A prime example of this is predictive coding networks(PCNs), based on the neuroscientific framework of predictive coding. Thisframework views the brain as a hierarchical Bayesian inference model thatminimizes prediction errors through feedback connections. Unlike traditionalneural networks trained with backpropagation (BP), PCNs utilize inferencelearning (IL), a more biologically plausible algorithm that explains patternsof neural activity that BP cannot. Historically, IL has been morecomputationally intensive, but recent advancements have demonstrated that itcan achieve higher efficiency than BP with sufficient parallelization.Furthermore, PCNs can be mathematically considered a superset of traditionalfeedforward neural networks (FNNs), significantly extending the range oftrainable architectures. As inherently probabilistic (graphical) latentvariable models, PCNs provide a versatile framework for both supervisedlearning and unsupervised (generative) modeling that goes beyond traditionalartificial neural networks. This work provides a comprehensive review anddetailed formal specification of PCNs, particularly situating them within thecontext of modern ML methods. Additionally, we introduce a Python library(PRECO) for practical implementation. This positions PC as a promisingframework for future ML innovations.</description><author>Björn van Zwol, Ro Jefferson, Egon L. van den Broek</author><pubDate>Mon, 22 Jul 2024 14:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04117v2</guid></item><item><title>YOLOv10 for Automated Fracture Detection in Pediatric Wrist Trauma X-rays</title><link>http://arxiv.org/abs/2407.15689v1</link><description>Wrist fractures are highly prevalent among children and can significantlyimpact their daily activities, such as attending school, participating insports, and performing basic self-care tasks. If not treated properly, thesefractures can result in chronic pain, reduced wrist functionality, and otherlong-term complications. Recently, advancements in object detection have shownpromise in enhancing fracture detection, with systems achieving accuracycomparable to, or even surpassing, that of human radiologists. The YOLO series,in particular, has demonstrated notable success in this domain. This study isthe first to provide a thorough evaluation of various YOLOv10 variants toassess their performance in detecting pediatric wrist fractures using theGRAZPEDWRI-DX dataset. It investigates how changes in model complexity, scalingthe architecture, and implementing a dual-label assignment strategy can enhancedetection performance. Experimental results indicate that our trained modelachieved mean average precision (mAP@50-95) of 51.9\% surpassing the currentYOLOv9 benchmark of 43.3\% on this dataset. This represents an improvement of8.6\%. The implementation code is publicly available athttps://github.com/ammarlodhi255/YOLOv10-Fracture-Detection</description><author>Ammar Ahmed, Abdul Manaf</author><pubDate>Mon, 22 Jul 2024 14:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15689v1</guid></item><item><title>AI-Driven Fast and Early Detection of IoT Botnet Threats: A Comprehensive Network Traffic Analysis Approach</title><link>http://arxiv.org/abs/2407.15688v1</link><description>In the rapidly evolving landscape of cyber threats targeting the Internet ofThings (IoT) ecosystem, and in light of the surge in botnet-driven DistributedDenial of Service (DDoS) and brute force attacks, this study focuses on theearly detection of IoT bots. It specifically addresses the detection of stealthbot communication that precedes and orchestrates attacks. This study proposes acomprehensive methodology for analyzing IoT network traffic, includingconsiderations for both unidirectional and bidirectional flow, as well aspacket formats. It explores a wide spectrum of network features critical forrepresenting network traffic and characterizing benign IoT traffic patternseffectively. Moreover, it delves into the modeling of traffic using varioussemi-supervised learning techniques. Through extensive experimentation with theIoT-23 dataset - a comprehensive collection featuring diverse botnet types andtraffic scenarios - we have demonstrated the feasibility of detecting botnettraffic corresponding to different operations and types of bots, specificallyfocusing on stealth command and control (C2) communications. The resultsobtained have demonstrated the feasibility of identifying C2 communication witha 100% success rate through packet-based methods and 94% via flow basedapproaches, with a false positive rate of 1.53%.</description><author>Abdelaziz Amara korba, Aleddine Diaf, Yacine Ghamri-Doudane</author><pubDate>Mon, 22 Jul 2024 14:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15688v1</guid></item><item><title>SoftCVI: contrastive variational inference with self-generated soft labels</title><link>http://arxiv.org/abs/2407.15687v1</link><description>Estimating a distribution given access to its unnormalized density is pivotalin Bayesian inference, where the posterior is generally known only up to anunknown normalizing constant. Variational inference and Markov chain MonteCarlo methods are the predominant tools for this task; however, both methodsare often challenging to apply reliably, particularly when the posterior hascomplex geometry. Here, we introduce Soft Contrastive Variational Inference(SoftCVI), which allows a family of variational objectives to be derivedthrough a contrastive estimation framework. These objectives have zero variancegradient when the variational approximation is exact, without the need forspecialized gradient estimators. The approach involves parameterizing aclassifier in terms of the variational distribution, which allows the inferencetask to be reframed as a contrastive estimation problem, aiming to identify asingle true posterior sample among a set of samples. Despite this framing, wedo not require positive or negative samples, but rather learn by sampling thevariational distribution and computing ground truth soft classification labelsfrom the unnormalized posterior itself. We empirically investigate theperformance on a variety of Bayesian inference tasks, using both using bothsimple (e.g. normal) and expressive (normalizing flow) variationaldistributions. We find that SoftCVI objectives often outperform other commonlyused variational objectives.</description><author>Daniel Ward, Mark Beaumont, Matteo Fasiolo</author><pubDate>Mon, 22 Jul 2024 14:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15687v1</guid></item></channel></rss>