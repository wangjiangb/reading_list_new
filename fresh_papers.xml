<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 29 Sep 2025 23:21:58 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing</title><link>http://arxiv.org/abs/2509.22651v1</link><description>The growing capabilities of large language models and multimodal systems havespurred interest in voice-first AI assistants, yet existing benchmarks areinadequate for evaluating the full range of these systems' capabilities. Weintroduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AIassistants across listening, speaking, and viewing. VoiceAssistant-Evalcomprises 10,497 curated examples spanning 13 task categories. These tasksinclude natural sounds, music, and spoken dialogue for listening; multi-turndialogue, role-play imitation, and various scenarios for speaking; and highlyheterogeneous images for viewing. To demonstrate its utility, we evaluate 21open-source models and GPT-4o-Audio, measuring the quality of the responsecontent and speech, as well as their consistency. The results reveal three keyfindings: (1) proprietary models do not universally outperform open-sourcemodels; (2) most models excel at speaking tasks but lag in audio understanding;and (3) well-designed smaller models can rival much larger ones. Notably, themid-sized Step-Audio-2-mini (7B) achieves more than double the listeningaccuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal(audio plus visual) input and role-play voice imitation tasks are difficult forcurrent models, and significant gaps persist in robustness and safetyalignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorousframework for evaluating and guiding the development of next-generation AIassistants. Code and data will be released athttps://mathllm.github.io/VoiceAssistantEval/ .</description><author>Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li</author><pubDate>Fri, 26 Sep 2025 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22651v1</guid></item><item><title>See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation</title><link>http://arxiv.org/abs/2509.22653v1</link><description>We present See, Point, Fly (SPF), a training-free aerial vision-and-languagenavigation (AVLN) framework built atop vision-language models (VLMs). SPF iscapable of navigating to any goal based on any type of free-form instructionsin any kind of environment. In contrast to existing VLM-based approaches thattreat action prediction as a text generation task, our key insight is toconsider action prediction for AVLN as a 2D spatial grounding task. SPFharnesses VLMs to decompose vague language instructions into iterativeannotation of 2D waypoints on the input image. Along with the predictedtraveling distance, SPF transforms predicted 2D waypoints into 3D displacementvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts thetraveling distance to facilitate more efficient navigation. Notably, SPFperforms navigation in a closed-loop control manner, enabling UAVs to followdynamic targets in dynamic environments. SPF sets a new state of the art in DRLsimulation benchmark, outperforming the previous best method by an absolutemargin of 63%. In extensive real-world evaluations, SPF outperforms strongbaselines by a large margin. We also conduct comprehensive ablation studies tohighlight the effectiveness of our design choice. Lastly, SPF shows remarkablegeneralization to different VLMs. Project page: https://spf-web.pages.dev</description><author>Chih Yao Hu, Yang-Sen Lin, Yuna Lee, Chih-Hai Su, Jie-Ying Lee, Shr-Ruei Tsai, Chin-Yang Lin, Kuan-Wen Chen, Tsung-Wei Ke, Yu-Lun Liu</author><pubDate>Fri, 26 Sep 2025 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22653v1</guid></item><item><title>Pixel Motion Diffusion is What We Need for Robot Control</title><link>http://arxiv.org/abs/2509.22652v1</link><description>We present DAWN (Diffusion is All We Need for robot control), a unifieddiffusion-based framework for language-conditioned robotic manipulation thatbridges high-level motion intent and low-level robot action via structuredpixel motion representation. In DAWN, both the high-level and low-levelcontrollers are modeled as diffusion processes, yielding a fully trainable,end-to-end system with interpretable intermediate motion abstractions. DAWNachieves state-of-the-art results on the challenging CALVIN benchmark,demonstrating strong multi-task performance, and further validates itseffectiveness on MetaWorld. Despite the substantial domain gap betweensimulation and reality and limited real-world data, we demonstrate reliablereal-world transfer with only minimal finetuning, illustrating the practicalviability of diffusion-based motion abstractions for robotic control. Ourresults show the effectiveness of combining diffusion modeling withmotion-centric representations as a strong baseline for scalable and robustrobot learning. Project page: https://nero1342.github.io/DAWN/</description><author>E-Ro Nguyen, Yichi Zhang, Kanchana Ranasinghe, Xiang Li, Michael S. Ryoo</author><pubDate>Fri, 26 Sep 2025 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22652v1</guid></item><item><title>RefAM: Attention Magnets for Zero-Shot Referral Segmentation</title><link>http://arxiv.org/abs/2509.22650v1</link><description>Most existing approaches to referring segmentation achieve strong performanceonly through fine-tuning or by composing multiple pre-trained models, often atthe cost of additional training and architectural modifications. Meanwhile,large-scale generative diffusion models encode rich semantic information,making them attractive as general-purpose feature extractors. In this work, weintroduce a new method that directly exploits features, attention scores, fromdiffusion transformers for downstream tasks, requiring neither architecturalmodifications nor additional training. To systematically evaluate thesefeatures, we extend benchmarks with vision-language grounding tasks spanningboth images and videos. Our key insight is that stop words act as attentionmagnets: they accumulate surplus attention and can be filtered to reduce noise.Moreover, we identify global attention sinks (GAS) emerging in deeper layersand show that they can be safely suppressed or redirected onto auxiliarytokens, leading to sharper and more accurate grounding maps. We further proposean attention redistribution strategy, where appended stop words partitionbackground activations into smaller clusters, yielding sharper and morelocalized heatmaps. Building on these findings, we develop RefAM, a simpletraining-free grounding framework that combines cross-attention maps, GAShandling, and redistribution. Across zero-shot referring image and videosegmentation benchmarks, our approach consistently outperforms prior methods,establishing a new state of the art without fine-tuning or additionalcomponents.</description><author>Anna Kukleva, Enis Simsar, Alessio Tonioni, Muhammad Ferjad Naeem, Federico Tombari, Jan Eric Lenssen, Bernt Schiele</author><pubDate>Fri, 26 Sep 2025 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22650v1</guid></item><item><title>Toward a Physics of Deep Learning and Brains</title><link>http://arxiv.org/abs/2509.22649v1</link><description>Deep neural networks and brains both learn and share superficialsimilarities: processing nodes are likened to neurons and adjustable weightsare likened to modifiable synapses. But can a unified theoretical framework befound to underlie them both? Here we show that the equations used to describeneuronal avalanches in living brains can also be applied to cascades ofactivity in deep neural networks. These equations are derived fromnon-equilibrium statistical physics and show that deep neural networks learnbest when poised between absorbing and active phases. Because these networksare strongly driven by inputs, however, they do not operate at a true criticalpoint but within a quasi-critical regime -- one that still approximatelysatisfies crackling noise scaling relations. By training networks withdifferent initializations, we show that maximal susceptibility is a morereliable predictor of learning than proximity to the critical point itself.This provides a blueprint for engineering improved network performance.Finally, using finite-size scaling we identify distinct universality classes,including Barkhausen noise and directed percolation. This theoretical frameworkdemonstrates that universal features are shared by both biological andartificial neural networks.</description><author>Arsham Ghavasieh, Meritxell Vila-Minana, Akanksha Khurd, John Beggs, Gerardo Ortiz, Santo Fortunato</author><pubDate>Fri, 26 Sep 2025 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22649v1</guid></item><item><title>UIP2P: Unsupervised Instruction-based Image Editing via Edit Reversibility Constraint</title><link>http://arxiv.org/abs/2412.15216v2</link><description>We propose an unsupervised instruction-based image editing approach thatremoves the need for ground-truth edited images during training. Existingmethods rely on supervised learning with triplets of input images, ground-truthedited images, and edit instructions. These triplets are typically generatedeither by existing editing methods, introducing biases, or through humanannotations, which are costly and limit generalization. Our approach addressesthese challenges by introducing a novel editing mechanism called EditReversibility Constraint (ERC), which applies forward and reverse edits in onetraining step and enforces alignment in image, text, and attention spaces. Thisallows us to bypass the need for ground-truth edited images and unlock trainingfor the first time on datasets comprising either real image-caption pairs orimage-caption-instruction triplets. We empirically show that our approachperforms better across a broader range of edits with high-fidelity andprecision. By eliminating the need for pre-existing datasets of triplets,reducing biases associated with current methods, and proposing ERC, our workrepresents a significant advancement in unblocking scaling of instruction-basedimage editing.</description><author>Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari</author><pubDate>Fri, 26 Sep 2025 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15216v2</guid></item><item><title>CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning</title><link>http://arxiv.org/abs/2509.22647v1</link><description>Image captioning is a fundamental task that bridges the visual and linguisticdomains, playing a critical role in pre-training Large Vision-Language Models(LVLMs). Current state-of-the-art captioning models are typically trained withSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalabledata annotated by humans or proprietary models. This approach often leads tomodels that memorize specific ground-truth answers, limiting their generalityand ability to generate diverse, creative descriptions. To overcome thelimitation of SFT, we propose applying the Reinforcement Learning withVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.A primary challenge, however, is designing an objective reward function for theinherently subjective nature of what constitutes a "good" caption. We introduceCaptioning Reinforcement Learning (CapRL), a novel training framework thatredefines caption quality through its utility: a high-quality caption shouldenable a non-visual language model to accurately answer questions about thecorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLMgenerates a caption, and the objective reward is derived from the accuracy of aseparate, vision-free LLM answering Multiple-Choice Questions based solely onthat caption. As the first study to apply RLVR to the subjective imagecaptioning task, we demonstrate that CapRL significantly enhances multiplesettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3Bresults in substantial gains across 12 benchmarks. Moreover, within the PrismFramework for caption quality evaluation, CapRL achieves performance comparableto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.Code is available here: https://github.com/InternLM/CapRL.</description><author>Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu, Dahua Lin</author><pubDate>Fri, 26 Sep 2025 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22647v1</guid></item><item><title>KV Cache Steering for Controlling Frozen LLMs</title><link>http://arxiv.org/abs/2507.08799v2</link><description>We propose cache steering, a lightweight method for implicit steering oflanguage models via a one-shot intervention applied directly to the key-valuecache. To validate its effectiveness, we apply cache steering to inducechain-of-thought reasoning in small language models. Our approach constructssteering vectors from reasoning traces, obtained either from teacher models(e.g., GPT-4o) or existing human annotations, that shift model behavior towardmore explicit, multi-step reasoning without fine-tuning or promptmodifications. Experimental evaluations on diverse reasoning benchmarksdemonstrate that cache steering improves both the qualitative structure ofmodel reasoning and quantitative task performance. Additional experiments showthat the method also scales to larger models and yields further gains onchallenging datasets such as GPQA and MATH. Compared to prior activationsteering techniques that require continuous interventions, our one-shot cachesteering offers substantial advantages in terms of inference latency,hyperparameter stability, and ease of integration with existing inference APIs.Beyond mere reasoning induction, we show that cache steering enablescontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),making it a practical tool for behavior-level guidance of language models.</description><author>Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb Mirza, James R. Glass, Cees G. M. Snoek, Yuki M. Asano</author><pubDate>Fri, 26 Sep 2025 17:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.08799v2</guid></item><item><title>Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs</title><link>http://arxiv.org/abs/2509.22646v1</link><description>Can humans identify AI-generated (fake) videos and provide grounded reasons?While video generation models have advanced rapidly, a critical dimension --whether humans can detect deepfake traces within a generated video, i.e.,spatiotemporal grounded visual artifacts that reveal a video as machinegenerated -- has been largely overlooked. We introduce DeeptraceReward, thefirst fine-grained, spatially- and temporally- aware benchmark that annotateshuman-perceived fake traces for video generation reward. The dataset comprises4.3K detailed annotations across 3.3K high-quality generated videos. Eachannotation provides a natural-language explanation, pinpoints a bounding-boxregion containing the perceived trace, and marks precise onset and offsettimestamps. We consolidate these annotations into 9 major categories ofdeepfake traces that lead humans to identify a video as AI-generated, and trainmultimodal language models (LMs) as reward models to mimic human judgments andlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by34.7% on average across fake clue identification, grounding, and explanation.Interestingly, we observe a consistent difficulty gradient: binary fake v.s.real classification is substantially easier than fine-grained deepfake tracedetection; within the latter, performance degrades from natural languageexplanations (easiest), to spatial grounding, to temporal labeling (hardest).By foregrounding human-perceived deepfake traces, DeeptraceReward provides arigorous testbed and training signal for socially aware and trustworthy videogeneration.</description><author>Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah, Chung Un Lee, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch</author><pubDate>Fri, 26 Sep 2025 17:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22646v1</guid></item><item><title>Hierarchical Representation Matching for CLIP-based Class-Incremental Learning</title><link>http://arxiv.org/abs/2509.22645v1</link><description>Class-Incremental Learning (CIL) aims to endow models with the ability tocontinuously adapt to evolving data streams. Recent advances in pre-trainedvision-language models (e.g., CLIP) provide a powerful foundation for thistask. However, existing approaches often rely on simplistic templates, such as"a photo of a [CLASS]", which overlook the hierarchical nature of visualconcepts. For example, recognizing "cat" versus "car" depends on coarse-grainedcues, while distinguishing "cat" from "lion" requires fine-grained details.Similarly, the current feature mapping in CLIP relies solely on therepresentation from the last layer, neglecting the hierarchical informationcontained in earlier layers. In this work, we introduce HiErarchicalRepresentation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leveragesLLMs to recursively generate discriminative textual descriptors, therebyaugmenting the semantic space with explicit hierarchical cues. Thesedescriptors are matched to different levels of the semantic hierarchy andadaptively routed based on task-specific requirements, enabling precisediscrimination while alleviating catastrophic forgetting in incremental tasks.Extensive experiments on multiple benchmarks demonstrate that our methodconsistently achieves state-of-the-art performance.</description><author>Zhen-Hao Wen, Yan Wang, Ji Feng, Han-Jia Ye, De-Chuan Zhan, Da-Wei Zhou</author><pubDate>Fri, 26 Sep 2025 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22645v1</guid></item><item><title>WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning</title><link>http://arxiv.org/abs/2509.22644v1</link><description>Agent systems powered by large language models (LLMs) have demonstratedimpressive performance on repository-level code-generation tasks. However, fortasks such as website codebase generation, which depend heavily on visualeffects and user-interaction feedback, current code agents rely only on simplecode execution for feedback and verification. This approach fails to capturethe actual quality of the generated code. In this paper, we proposeWebGen-Agent, a novel website-generation agent that leverages comprehensive andmulti-level visual feedback to iteratively generate and refine the websitecodebase. Detailed and expressive text descriptions and suggestions regardingthe screenshots and GUI-agent testing of the websites are generated by a visuallanguage model (VLM), together with scores that quantify their quality. Thescreenshot and GUI-agent scores are further integrated with a backtracking andselect-best mechanism, enhancing the performance of the agent. Utilizing theaccurate visual scores inherent in the WebGen-Agent workflow, we furtherintroduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improvethe ability of LLMs to act as the reasoning engine of WebGen-Agent. By usingthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, weprovide a dense and reliable process supervision signal, which effectivelyimproves the model's website-generation ability. On the WebGen-Bench dataset,WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%and its appearance score from 3.0 to 3.9, outperforming the previousstate-of-the-art agent system. Additionally, our Step-GRPO training approachincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% andraises the appearance score from 3.4 to 3.7.</description><author>Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li</author><pubDate>Fri, 26 Sep 2025 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22644v1</guid></item><item><title>WoW: Towards a World omniscient World model Through Embodied Interaction</title><link>http://arxiv.org/abs/2509.22642v1</link><description>Humans develop an understanding of intuitive physics through activeinteraction with the world. This approach is in stark contrast to current videomodels, such as Sora, which rely on passive observation and therefore strugglewith grasping physical causality. This observation leads to our centralhypothesis: authentic physical intuition of the world model must be grounded inextensive, causally rich interactions with the real world. To test thishypothesis, we present WoW, a 14-billion-parameter generative world modeltrained on 2 million robot interaction trajectories. Our findings reveal thatthe model's understanding of physics is a probabilistic distribution ofplausible outcomes, leading to stochastic instabilities and physicalhallucinations. Furthermore, we demonstrate that this emergent capability canbe actively constrained toward physical realism by SOPHIA, wherevision-language model agents evaluate the DiT-generated output and guide itsrefinement by iteratively evolving the language instructions. In addition, aco-trained Inverse Dynamics Model translates these refined plans intoexecutable robotic actions, thus closing the imagination-to-action loop. Weestablish WoWBench, a new benchmark focused on physical consistency and causalreasoning in video, where WoW achieves state-of-the-art performance in bothhuman and autonomous evaluation, demonstrating strong ability in physicalcausality, collision dynamics, and object permanence. Our work providessystematic evidence that large-scale, real-world interaction is a cornerstonefor developing physical intuition in AI. Models, data, and benchmarks will beopen-sourced.</description><author>Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Anthony Chen, Qiang Zhou, Yueru Jia, Jiaming Liu, Yong Dai, Qingpo Wuwu, Chengyu Bai, Yu-Kai Wang, Ying Li, Lizhang Chen, Yong Bao, Zhiyuan Jiang, Jiacheng Zhu, Kai Tang, Ruichuan An, Yulin Luo, Qiuxuan Feng, Siyuan Zhou, Chi-min Chan, Chengkai Hou, Wei Xue, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang</author><pubDate>Fri, 26 Sep 2025 17:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22642v1</guid></item><item><title>Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity</title><link>http://arxiv.org/abs/2509.22641v1</link><description>N-gram novelty is widely used to evaluate language models' ability togenerate text outside of their training data. More recently, it has also beenadopted as a metric for measuring textual creativity. However, theoretical workon creativity suggests that this approach may be inadequate, as it does notaccount for creativity's dual nature: novelty (how original the text is) andappropriateness (how sensical and pragmatic it is). We investigate therelationship between this notion of creativity and n-gram novelty through 7542expert writer annotations (n=26) of novelty, pragmaticality, and sensicalityvia close reading of human and AI-generated text. We find that while n-gramnovelty is positively associated with expert writer-judged creativity, ~91% oftop-quartile expressions by n-gram novelty are not judged as creative,cautioning against relying on n-gram novelty alone. Furthermore, unlikehuman-written text, higher n-gram novelty in open-source LLMs correlates withlower pragmaticality. In an exploratory study with frontier close-sourcemodels, we additionally confirm that they are less likely to produce creativeexpressions than humans. Using our dataset, we test whether zero-shot,few-shot, and finetuned models are able to identify creative expressions (apositive aspect of writing) and non-pragmatic ones (a negative aspect).Overall, frontier LLMs exhibit performance much higher than random but leaveroom for improvement, especially struggling to identify non-pragmaticexpressions. We further find that LLM-as-a-Judge novelty scores from thebest-performing model were predictive of expert writer preferences.</description><author>Arkadiy Saakyan, Najoung Kim, Smaranda Muresan, Tuhin Chakrabarty</author><pubDate>Fri, 26 Sep 2025 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22641v1</guid></item><item><title>Language Models Can Learn from Verbal Feedback Without Scalar Rewards</title><link>http://arxiv.org/abs/2509.22638v1</link><description>LLMs are often trained with RL from human or AI feedback, yet such methodstypically compress nuanced feedback into scalar rewards, discarding much oftheir richness and inducing scale imbalance. We propose treating verbalfeedback as a conditioning signal. Inspired by language priors in text-to-imagegeneration, which enable novel outputs from unseen prompts, we introduce thefeedback-conditional policy (FCP). FCP learns directly from response-feedbackpairs, approximating the feedback-conditional posterior through maximumlikelihood training on offline data. We further develop an online bootstrappingstage where the policy generates under positive conditions and receives freshfeedback to refine itself. This reframes feedback-driven learning asconditional generation rather than reward optimization, offering a moreexpressive way for LLMs to directly learn from verbal feedback. Our code isavailable at https://github.com/sail-sg/feedback-conditional-policy.</description><author>Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang</author><pubDate>Fri, 26 Sep 2025 17:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22638v1</guid></item><item><title>Variational Reasoning for Language Models</title><link>http://arxiv.org/abs/2509.22637v1</link><description>We introduce a variational reasoning framework for language models thattreats thinking traces as latent variables and optimizes them throughvariational inference. Starting from the evidence lower bound (ELBO), we extendit to a multi-trace objective for tighter bounds and propose a forward-KLformulation that stabilizes the training of the variational posterior. Wefurther show that rejection sampling finetuning and binary-reward RL, includingGRPO, can be interpreted as local forward-KL objectives, where an implicitweighting by model accuracy naturally arises from the derivation and reveals apreviously unnoticed bias toward easier questions. We empirically validate ourmethod on the Qwen 2.5 and Qwen 3 model families across a wide range ofreasoning tasks. Overall, our work provides a principled probabilisticperspective that unifies variational inference with RL-style methods and yieldsstable objectives for improving the reasoning ability of language models. Ourcode is available at https://github.com/sail-sg/variational-reasoning.</description><author>Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang</author><pubDate>Fri, 26 Sep 2025 17:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22637v1</guid></item><item><title>Scale-Wise VAR is Secretly Discrete Diffusion</title><link>http://arxiv.org/abs/2509.22636v1</link><description>Autoregressive (AR) transformers have emerged as a powerful paradigm forvisual generation, largely due to their scalability, computational efficiencyand unified architecture with language and vision. Among them, next scaleprediction Visual Autoregressive Generation (VAR) has recently demonstratedremarkable performance, even surpassing diffusion-based models. In this work,we revisit VAR and uncover a theoretical insight: when equipped with aMarkovian attention mask, VAR is mathematically equivalent to a discretediffusion. We term this reinterpretation as Scalable Visual Refinement withDiscrete Diffusion (SRDD), establishing a principled bridge between ARtransformers and diffusion models. Leveraging this new perspective, we show howone can directly import the advantages of diffusion such as iterativerefinement and reduce architectural inefficiencies into VAR, yielding fasterconvergence, lower inference cost, and improved zero-shot reconstruction.Across multiple datasets, we show that the diffusion based perspective of VARleads to consistent gains in efficiency and generation.</description><author>Amandeep Kumar, Nithin Gopalakrishnan Nair, Vishal M. Patel</author><pubDate>Fri, 26 Sep 2025 17:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22636v1</guid></item><item><title>Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance</title><link>http://arxiv.org/abs/2509.22635v1</link><description>Few-shot image classification remains challenging due to the limitedavailability of labeled examples. Recent approaches have explored generatingsynthetic training data using text-to-image diffusion models, but often requireextensive model fine-tuning or external information sources. We present a noveltraining-free approach, called DIPSY, that leverages IP-Adapter forimage-to-image translation to generate highly discriminative synthetic imagesusing only the available few-shot examples. DIPSY introduces three keyinnovations: (1) an extended classifier-free guidance scheme that enablesindependent control over positive and negative image conditioning; (2) a classsimilarity-based sampling strategy that identifies effective contrastiveexamples; and (3) a simple yet effective pipeline that requires no modelfine-tuning or external captioning and filtering. Experiments across tenbenchmark datasets demonstrate that our approach achieves state-of-the-art orcomparable performance, while eliminating the need for generative modeladaptation or reliance on external tools for caption generation and imagefiltering. Our results highlight the effectiveness of leveraging dual imageprompting with positive-negative guidance for generating class-discriminativefeatures, particularly for fine-grained classification tasks.</description><author>Luc Boudier, Loris Manganelli, Eleftherios Tsonis, Nicolas Dufour, Vicky Kalogeiton</author><pubDate>Fri, 26 Sep 2025 17:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22635v1</guid></item><item><title>Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback</title><link>http://arxiv.org/abs/2509.22633v1</link><description>Reinforcement learning with human feedback (RLHF), which learns a rewardmodel from human preference data and then optimizes a policy to favor preferredresponses, has emerged as a central paradigm for aligning large language models(LLMs) with human preferences. In this paper, we investigate explorationprinciples for online RLHF, where one seeks to adaptively collect newpreference data to refine both the reward model and the policy in adata-efficient manner. By examining existing optimism-based explorationalgorithms, we identify a drawback in their sampling protocol: they tend togather comparisons that fail to reduce the most informative uncertainties inreward differences, and we prove lower bounds showing that such methods canincur linear regret over exponentially long horizons. Motivated by thisinsight, we propose a new exploration scheme that directs preference queriestoward reducing uncertainty in reward differences most relevant to policyimprovement. Under a multi-armed bandit model of RLHF, we establish regretbounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta&gt;0$ is a hyperparameterthat balances reward maximization against mitigating distribution shift. To ourknowledge, this is the first online RLHF algorithm with regret scalingpolynomially in all model parameters.</description><author>Gen Li, Yuling Yan</author><pubDate>Fri, 26 Sep 2025 17:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22633v1</guid></item><item><title>Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning</title><link>http://arxiv.org/abs/2507.05418v2</link><description>Large Language Models (LLMs) have achieved strong performance in domains likemathematics, factual question answering, and code generation, yet their abilityto reason on these tasks in different languages remains underdeveloped.Especially for low-resource languages such as Swahili or Thai, LLMs can oftenmisinterpret prompts or default to reasoning in English. This implicit biastoward high-resource languages undermines factual accuracy, interpretability,and trust. We propose M2A, a novel method that combines multi-scalemultilingual alignment with language-consistency rewards on machine-translatedquestions, training models to reason directly and accurately in the targetlanguage. Furthermore, existing multilingual benchmarks only evaluate on finalanswers, overlooking whether reasoning occurs in the intended language. Toclose this gap, we introduce GeoFact-X, a geography-based multilingual factualreasoning benchmark together with reasoning traces in five languages: English,Hindi, Japanese, Swahili, and Thai. Our results show that M2A significantlyenhances multilingual reasoning fidelity in both mathematical and factualreasoning tasks, highlighting that reasoning-aware multilingual reinforcementlearning is crucial for robust cross-lingual generalization.https://jd730.github.io/projects/M2A_GeoFact-X</description><author>Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang</author><pubDate>Fri, 26 Sep 2025 17:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05418v2</guid></item><item><title>Large Pre-Training Datasets Don't Always Guarantee Robustness after Fine-Tuning</title><link>http://arxiv.org/abs/2410.21582v3</link><description>Large-scale pretrained models are widely leveraged as foundations forlearning new specialized tasks via fine-tuning, with the goal of maintainingthe general performance of the model while allowing it to gain new skills. Avaluable goal for all such models is robustness: the ability to perform well onout-of-distribution (OOD) tasks. We assess whether fine-tuning preserves theoverall robustness of the pretrained model, and observed that models pretrainedon large datasets exhibited strong catastrophic forgetting and loss of OODgeneralization. To systematically assess robustness preservation in fine-tunedmodels, we propose the Robustness Inheritance Benchmark (ImageNet-RIB). Thebenchmark, which can be applied to any pretrained model, consists of a set ofrelated but distinct OOD (downstream) tasks and involves fine-tuning on one ofthe OOD tasks in the set then testing on the rest. We find that thoughcontinual learning methods help, fine-tuning reduces robustness acrosspretrained models. Surprisingly, models pretrained on the largest and mostdiverse datasets (e.g., LAION-2B) exhibit both larger robustness losses andlower absolute robustness after fine-tuning on small datasets, relative tomodels pretrained on smaller datasets. These findings suggest that startingwith the strongest foundation model is not necessarily the best approach forperformance on specialist tasks. https://jd730.github.io/projects/ImageNet-RIB</description><author>Jaedong Hwang, Brian Cheung, Zhang-Wei Hong, Akhilan Boopathy, Pulkit Agrawal, Ila Fiete</author><pubDate>Fri, 26 Sep 2025 17:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21582v3</guid></item><item><title>LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision</title><link>http://arxiv.org/abs/2509.22631v1</link><description>Curating high-quality, domain-specific datasets is a major bottleneck fordeploying robust vision systems, requiring complex trade-offs between dataquality, diversity, and cost when researching vast, unlabeled data lakes. Weintroduce Labeling Copilot, the first data curation deep research agent forcomputer vision. A central orchestrator agent, powered by a large multimodallanguage model, uses multi-step reasoning to execute specialized tools acrossthree core capabilities: (1) Calibrated Discovery sources relevant,in-distribution data from large repositories; (2) Controllable Synthesisgenerates novel data for rare scenarios with robust filtering; and (3)Consensus Annotation produces accurate labels by orchestrating multiplefoundation models via a novel consensus mechanism incorporating non-maximumsuppression and voting. Our large-scale validation proves the effectiveness ofLabeling Copilot's components. The Consensus Annotation module excels at objectdiscovery: on the dense COCO dataset, it averages 14.2 candidate proposals perimage-nearly double the 7.4 ground-truth objects-achieving a final annotationmAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme classimbalance to discover 903 new bounding box categories, expanding its capabilityto over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a10-million sample scale, features an active learning strategy that is up to 40xmore computationally efficient than alternatives with equivalent sampleefficiency. These experiments validate that an agentic workflow with optimized,scalable tools provides a robust foundation for curating industrial-scaledatasets.</description><author>Debargha Ganguly, Sumit Kumar, Ishwar Balappanawar, Weicong Chen, Shashank Kambhatla, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam Kumaraguru, Vipin Chaudhary</author><pubDate>Fri, 26 Sep 2025 17:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22631v1</guid></item><item><title>StateX: Enhancing RNN Recall via Post-training State Expansion</title><link>http://arxiv.org/abs/2509.22630v1</link><description>While Transformer-based models have demonstrated remarkable language modelingperformance, their high complexities result in high costs when processing longcontexts. In contrast, recurrent neural networks (RNNs) such as linearattention and state space models have gained popularity due to their constantper-token complexities. However, these recurrent models struggle with tasksthat require accurate recall of contextual information from long contexts,because all contextual information is compressed into a constant-size recurrentstate. Previous works have shown that recall ability is positively correlatedwith the recurrent state size, yet directly training RNNs with larger recurrentstates results in high training costs. In this paper, we introduce StateX, atraining pipeline for efficiently expanding the states of pre-trained RNNsthrough post-training. For two popular classes of RNNs, linear attention andstate space models, we design post-training architectural modifications toscale up the state size with no or negligible increase in model parameters.Experiments on models up to 1.3B parameters demonstrate that StateX efficientlyenhances the recall and in-context learning ability of RNNs without incurringhigh post-training costs or compromising other capabilities.</description><author>Xingyu Shen, Yingfa Chen, Zhen Leng Thai, Xu Han, Zhiyuan Liu, Maosong Sun</author><pubDate>Fri, 26 Sep 2025 17:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22630v1</guid></item><item><title>UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning</title><link>http://arxiv.org/abs/2509.22628v1</link><description>Chain-of-Thought (CoT) prompting improves reasoning in large language models(LLMs), but its reliance on unstructured text limits interpretability andexecutability in embodied tasks. Prior work has explored structured CoTs usingscene or logic graphs, yet these remain fundamentally limited: they model onlylow-order relations, lack constructs like inheritance or behavioralabstraction, and provide no standardized semantics for sequential orconditional planning. We propose UML-CoT, a structured reasoning and planningframework that leverages Unified Modeling Language (UML) to generate symbolicCoTs and executable action plans. UML class diagrams capture compositionalobject semantics, while activity diagrams model procedural control flow. Ourthree-stage training pipeline combines supervised fine-tuning with GroupRelative Policy Optimization (GRPO), including reward learning from answer-onlydata. We evaluate UML-CoT on MRoom-30k, a new benchmark of clutteredroom-cleaning scenarios. UML-CoT outperforms unstructured CoTs ininterpretability, planning coherence, and execution success, highlighting UMLas a more expressive and actionable structured reasoning formalism.</description><author>Hongyu Chen, Guangrun Wang</author><pubDate>Fri, 26 Sep 2025 17:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22628v1</guid></item><item><title>CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach</title><link>http://arxiv.org/abs/2509.22627v1</link><description>Depth Estimation plays a crucial role in recent applications in robotics,autonomous vehicles, and augmented reality. These scenarios commonly operateunder constraints imposed by computational power. Stereo image pairs offer aneffective solution for depth estimation since it only needs to estimate thedisparity of pixels in image pairs to determine the depth in a known rectifiedsystem. Due to the difficulty in acquiring reliable ground-truth depth dataacross diverse scenarios, self-supervised techniques emerge as a solution,particularly when large unlabeled datasets are available. We propose a novelself-supervised convolutional approach that outperforms existingstate-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers(ViTs) while balancing computational cost. The proposed CCNeXt architectureemploys a modern CNN feature extractor with a novel windowed epipolarcross-attention module in the encoder, complemented by a comprehensive redesignof the depth estimation decoder. Our experiments demonstrate that CCNeXtachieves competitive metrics on the KITTI Eigen Split test data while being10.18$\times$ faster than the current best model and achieves state-of-the-artresults in all metrics in the KITTI Eigen Split Improved Ground Truth andDriving Stereo datasets when compared to recently proposed techniques. Toensure complete reproducibility, our project is accessible at\href{https://github.com/alelopes/CCNext}{\texttt{https://github.com/alelopes/CCNext}}.</description><author>Alexandre Lopes, Roberto Souza, Helio Pedrini</author><pubDate>Fri, 26 Sep 2025 17:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22627v1</guid></item><item><title>Learning Admissible Heuristics for A*: Theory and Practice</title><link>http://arxiv.org/abs/2509.22626v1</link><description>Heuristic functions are central to the performance of search algorithms suchas A-star, where admissibility - the property of never overestimating the trueshortest-path cost - guarantees solution optimality. Recent deep learningapproaches often disregard admissibility and provide limited guarantees ongeneralization beyond the training data. This paper addresses both of theselimitations. First, we pose heuristic learning as a constrained optimizationproblem and introduce Cross-Entropy Admissibility (CEA), a loss function thatenforces admissibility during training. On the Rubik's Cube domain, this methodyields near-admissible heuristics with significantly stronger guidance thancompressed pattern database (PDB) heuristics. Theoretically, we study thesample complexity of learning heuristics. By leveraging PDB abstractions andthe structural properties of graphs such as the Rubik's Cube, we tighten thebound on the number of training samples needed for A-star to generalize.Replacing a general hypothesis class with a ReLU neural network gives boundsthat depend primarily on the network's width and depth, rather than on graphsize. Using the same network, we also provide the first generalizationguarantees for goal-dependent heuristics.</description><author>Ehsan Futuhi, Nathan R. Sturtevant</author><pubDate>Fri, 26 Sep 2025 17:51:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22626v1</guid></item><item><title>SPARK: Synergistic Policy And Reward Co-Evolving Framework</title><link>http://arxiv.org/abs/2509.22624v1</link><description>Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)increasingly use Reinforcement Learning (RL) for post-pretraining, such as RLwith Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback(RLHF) for subjective tasks. However, RLHF incurs high costs and potentialreward-policy mismatch due to reliance on human preferences, while RLVR stillwastes supervision by discarding rollouts and correctness signals after eachupdate. To address these challenges, we introduce the Synergistic Policy AndReward Co-Evolving Framework (SPARK), an efficient, on-policy, and stablemethod that builds on RLVR. Instead of discarding rollouts and correctnessdata, SPARK recycles this valuable information to simultaneously train themodel itself as a generative reward model. This auxiliary training uses a mixof objectives, such as pointwise reward score, pairwise comparison, andevaluation conditioned on further-reflection responses, to teach the model toevaluate and improve its own responses. Our process eliminates the need for aseparate reward model and costly human preference data. SPARK creates apositive co-evolving feedback loop: improved reward accuracy yields betterpolicy gradients, which in turn produce higher-quality rollouts that furtherrefine the reward model. Our unified framework supports test-time scaling viaself-reflection without external reward models and their associated costs. Weshow that SPARK achieves significant performance gains on multiple LLM and LVLMmodels and multiple reasoning, reward models, and general benchmarks. Forexample, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over thebaselines, demonstrating robustness and broad generalization.</description><author>Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang</author><pubDate>Fri, 26 Sep 2025 17:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22624v1</guid></item><item><title>The STAR-XAI Protocol: A Framework for Inducing and Verifying Agency, Reasoning, and Reliability in AI Agents</title><link>http://arxiv.org/abs/2509.17978v2</link><description>The "black box" nature of Large Reasoning Models (LRMs) presents criticallimitations in reliability and transparency, fueling the debate around the"illusion of thinking" and the challenge of state hallucinations in agenticsystems. In response, we introduce The STAR-XAI Protocol (Socratic,Transparent, Agentic, Reasoning - for eXplainable Artificial Intelligence), anovel operational methodology for training and operating verifiably reliable AIagents. Our method reframes the human-AI interaction as a structured Socraticdialogue governed by an explicit, evolving symbolic rulebook (the ConsciousnessTransfer Package - CTP) and a suite of integrity protocols, including astate-locking Checksum that eradicates internal state corruption. Through anexhaustive case study in the complex strategic game "Caps i Caps," wedemonstrate that this "Clear Box" framework transforms an opaque LRM into adisciplined strategist. The agent not only exhibits the emergence of complextactics, such as long-term planning, but also achieves ante-hoc transparency byjustifying its intentions before acting. Crucially, it demonstratesSecond-Order Agency by identifying and correcting flaws in its ownsupervisor-approved plans, leading to empirically-proven, 100% reliable statetracking and achieving "zero hallucinations by design." The STAR-XAI Protocolthus offers a practical pathway toward building AI agents that are not justhigh-performing but intrinsically auditable, trustworthy, and reliable.</description><author>Antoni Guasch, Maria Isabel Valdez</author><pubDate>Fri, 26 Sep 2025 17:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.17978v2</guid></item><item><title>A Theoretical Analysis of Discrete Flow Matching Generative Models</title><link>http://arxiv.org/abs/2509.22623v1</link><description>We provide a theoretical analysis for end-to-end training Discrete FlowMatching (DFM) generative models. DFM is a promising discrete generativemodeling framework that learns the underlying generative dynamics by training aneural network to approximate the transformative velocity field. Our analysisestablishes a clear chain of guarantees by decomposing the final distributionestimation error. We first prove that the total variation distance between thegenerated and target distributions is controlled by the risk of the learnedvelocity field. We then bound this risk by analyzing its two primary sources:(i) Approximation Error, where we quantify the capacity of the Transformerarchitecture to represent the true velocity, and (ii) Estimation Error, wherewe derive statistical convergence rates that bound the error from training on afinite dataset. By composing these results, we provide the first formal proofthat the distribution generated by a trained DFM model provably converges tothe true data distribution as the training set size increases.</description><author>Maojiang Su, Mingcheng Lu, Jerry Yao-Chieh Hu, Shang Wu, Zhao Song, Alex Reneau, Han Liu</author><pubDate>Fri, 26 Sep 2025 17:48:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22623v1</guid></item><item><title>LongLive: Real-time Interactive Long Video Generation</title><link>http://arxiv.org/abs/2509.22622v1</link><description>We present LongLive, a frame-level autoregressive (AR) framework forreal-time and interactive long video generation. Long video generation presentschallenges in both efficiency and quality. Diffusion and Diffusion-Forcingmodels can produce high-quality videos but suffer from low efficiency due tobidirectional attention. Causal attention AR models support KV caching forfaster inference, but often degrade in quality on long videos due to memorychallenges during long-video training. In addition, beyond static prompt-basedgeneration, interactive capabilities, such as streaming prompt inputs, arecritical for dynamic content creation, enabling users to guide narratives inreal time. This interactive requirement significantly increases complexity,especially in ensuring visual consistency and semantic coherence during prompttransitions. To address these challenges, LongLive adopts a causal, frame-levelAR design that integrates a KV-recache mechanism that refreshes cached stateswith new prompts for smooth, adherent switches; streaming long tuning to enablelong video training and to align training and inference (train-long-test-long);and short window attention paired with a frame-level attention sink, shorten asframe sink, preserving long-range consistency while enabling faster generation.With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip modelto minute-long generation in just 32 GPU-days. At inference, LongLive sustains20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in bothshort and long videos. LongLive supports up to 240-second videos on a singleH100 GPU. LongLive further supports INT8-quantized inference with only marginalquality loss.</description><author>Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, Yukang Chen</author><pubDate>Fri, 26 Sep 2025 17:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22622v1</guid></item><item><title>IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning</title><link>http://arxiv.org/abs/2509.22621v1</link><description>Supervised Fine-Tuning (SFT) is used to specialize model behavior by trainingweights to produce intended target responses for queries. In contrast,In-Context Learning (ICL) adapts models during inference with instructions ordemonstrations in the prompt. ICL can offer better generalizability and morecalibrated responses compared to SFT in data scarce settings, at the cost ofmore inference compute. In this work, we ask the question: Can ICL's internalcomputations be used to improve the qualities of SFT? We first show that ICLand SFT produce distinct activation patterns, indicating that the two methodsachieve adaptation through different functional mechanisms. Motivated by thisobservation and to use ICL's rich functionality, we introduce ICL ActivationAlignment (IA2), a self-distillation technique which aims to replicate ICL'sactivation patterns in SFT models and incentivizes ICL-like internal reasoning.Performing IA2 as a priming step before SFT significantly improves the accuracyand calibration of model outputs, as shown by our extensive empirical resultson 12 popular benchmarks and 2 model families. This finding is not onlypractically useful, but also offers a conceptual window into the innermechanics of model adaptation.</description><author>Aayush Mishra, Daniel Khashabi, Anqi Liu</author><pubDate>Fri, 26 Sep 2025 17:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22621v1</guid></item><item><title>FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization of Large Language Models</title><link>http://arxiv.org/abs/2505.17967v3</link><description>Low-rank optimization has emerged as a promising direction in training largelanguage models (LLMs) to improve running time and reduce the memory usage ofadaptive optimizers by constraining learning to a lower-dimensional space.Prior work typically projects gradients of linear layers using approaches basedon Singular Value Decomposition (SVD) or QR-decomposition. Applying thesetechniques individually to each layer in large models is computationallyexpensive and incurs additional memory costs due to storing the projectionmatrices. In this work, we propose a computationally efficient and conceptuallysimple, two-step procedure to approximate SVD/QR-based gradient projectionsinto lower-dimensional spaces by using a predefined orthogonal matrix of theDiscrete Cosine Transform (DCT). We dynamically select columns from the DCTmatrix based on their alignment with the gradient of each layer. The effectiveprojection matrices are obtained via a simple matmul with the DCT matrix in$O(n^3)$ time, followed by a lightweight sorting step to identify the mostrelevant basis vectors. For large layers, DCT can be computed via Makhoul's$N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \log(n))$time. Due to the predefined nature of the orthogonal bases, they are computedonce at the start of training. Our numerical experiments on both pre-trainingand fine-tuning tasks demonstrate the effectiveness of our dual strategy inapproximating optimal low-rank projections, obtaining an approach withrank-independent running time that matches the performance of costlySVD/QR-based methods while achieving faster runtime and reduced memory usage byup to $25\%$ across different model sizes. Our code is available at\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers/tree/main/ista_daslab_optimizers/fft_low_rank}{ISTA-DASLab-Optimizers}.</description><author>Ionut-Vlad Modoranu, Mher Safaryan, Erik Schultheis, Max Ryabinin, Artem Chumachenko, Dan Alistarh</author><pubDate>Fri, 26 Sep 2025 17:42:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.17967v3</guid></item><item><title>Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting</title><link>http://arxiv.org/abs/2509.22615v1</link><description>Modern vision language pipelines are driven by RGB vision encoders trained onmassive image text corpora. While these pipelines have enabled impressive zeroshot capabilities and strong transfer across tasks, they still inherit twostructural inefficiencies from the pixel domain: (i) transmitting dense RGBimages from edge devices to the cloud is energy intensive and costly, and (ii)patch based tokenization explodes sequence length, stressing attention budgetsand context limits. We explore 2D Gaussian Splatting (2DGS) as an alternativevisual substrate for alignment: a compact, spatially adaptive representationthat parameterizes images by a set of colored anisotropic Gaussians. We developa scalable 2DGS pipeline with structured initialization, luminance awarepruning, and batched CUDA kernels, achieving over 90x faster fitting and about97% GPU utilization compared to prior implementations. We further adaptcontrastive language image pretraining (CLIP) to 2DGS by reusing a frozenRGB-based transformer backbone with a lightweight splat aware input stem and aperceiver resampler, training only about 7% of the total parameters. On largeDataComp subsets, GS encoders yield meaningful zero shot ImageNet-1Kperformance while compressing inputs 3 to 20x relative to pixels. Whileaccuracy currently trails RGB encoders, our results establish 2DGS as a viablemultimodal substrate, pinpoint architectural bottlenecks, and open a pathtoward representations that are both semantically powerful and transmissionefficient for edge cloud learning.</description><author>Yasmine Omri, Connor Ding, Tsachy Weissman, Thierry Tambe</author><pubDate>Fri, 26 Sep 2025 17:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22615v1</guid></item><item><title>CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives</title><link>http://arxiv.org/abs/2504.10823v3</link><description>Navigating dilemmas involving conflicting values is challenging even forhumans in high-stakes domains, let alone for AI, yet prior work has beenlimited to everyday scenarios. To close this gap, we introduce CLASH (Characterperspective-based LLM Assessments in Situations with High-stakes), ameticulously curated dataset consisting of 345 high-impact dilemmas along with3,795 individual perspectives of diverse values. CLASH enables the study ofcritical yet underexplored aspects of value-based decision-making processes,including understanding of decision ambivalence and psychological discomfort aswell as capturing the temporal shifts of values in the perspectives ofcharacters. By benchmarking 14 non-thinking and thinking models, we uncoverseveral key findings. (1) Even strong proprietary models, such as GPT-5 andClaude-4-Sonnet, struggle with ambivalent decisions, achieving only 24.06 and51.01 accuracy. (2) Although LLMs reasonably predict psychological discomfort,they do not adequately comprehend perspectives involving value shifts. (3)Cognitive behaviors that are effective in the math-solving and game strategydomains do not transfer to value reasoning. Instead, new failure patternsemerge, including early commitment and overcommitment. (4) The steerability ofLLMs towards a given value is significantly correlated with their valuepreferences. (5) Finally, LLMs exhibit greater steerability when reasoning froma third-party perspective, although certain values (e.g., safety) benefituniquely from first-person framing.</description><author>Ayoung Lee, Ryan Sungmo Kwon, Peter Railton, Lu Wang</author><pubDate>Fri, 26 Sep 2025 17:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.10823v3</guid></item><item><title>Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective</title><link>http://arxiv.org/abs/2509.22613v1</link><description>Recent reinforcement learning (RL) methods have substantially enhanced theplanning capabilities of Large Language Models (LLMs), yet the theoreticalbasis for their effectiveness remains elusive. In this work, we investigateRL's benefits and limitations through a tractable graph-based abstraction,focusing on policy gradient (PG) and Q-learning methods. Our theoreticalanalyses reveal that supervised fine-tuning (SFT) may introduceco-occurrence-based spurious solutions, whereas RL achieves correct planningprimarily through exploration, underscoring exploration's role in enablingbetter generalization. However, we also show that PG suffers from diversitycollapse, where output diversity decreases during training and persists evenafter perfect accuracy is attained. By contrast, Q-learning provides two keyadvantages: off-policy learning and diversity preservation at convergence. Wefurther demonstrate that careful reward design is necessary to prevent rewardhacking in Q-learning. Finally, applying our framework to the real-worldplanning benchmark Blocksworld, we confirm that these behaviors manifest inpractice.</description><author>Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong, Yaru Hao, Wei Chen</author><pubDate>Fri, 26 Sep 2025 17:39:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22613v1</guid></item><item><title>EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs</title><link>http://arxiv.org/abs/2509.15735v2</link><description>Large language models (LLMs) offer broad utility but remain prone tohallucination and out-of-distribution (OOD) errors. We propose EigenTrack, aninterpretable real-time detector that uses the spectral geometry of hiddenactivations, a compact global signature of model dynamics. By streamingcovariance-spectrum statistics such as entropy, eigenvalue gaps, and KLdivergence from random baselines into a lightweight recurrent classifier,EigenTrack tracks temporal shifts in representation structure that signalhallucination and OOD drift before surface errors appear. Unlike black- andgrey-box methods, it needs only a single forward pass without resampling.Unlike existing white-box detectors, it preserves temporal context, aggregatesglobal signals, and offers interpretable accuracy-latency trade-offs.</description><author>Davide Ettori, Nastaran Darabi, Sina Tayebati, Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo, Amit Ranjan Trivedi</author><pubDate>Fri, 26 Sep 2025 17:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.15735v2</guid></item><item><title>From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks</title><link>http://arxiv.org/abs/2509.22612v1</link><description>In this paper, we introduce a set of resampling-based methods for quantifyinguncertainty and statistical precision of evaluation metrics in multilingualand/or multitask NLP benchmarks. We show how experimental variation inperformance scores arises from both model- and data-related sources, and thataccounting for both of them is necessary to avoid substantially underestimatingthe overall variability over hypothetical replications. Using multilingualquestion answering, machine translation, and named entity recognition asexample tasks, we also demonstrate how resampling methods are useful forcomputing sampling distributions for various quantities used in leaderboardssuch as the average/median, pairwise differences between models, and rankings.</description><author>Jonne Slev, Duygu Ataman, Constantine Lignos</author><pubDate>Fri, 26 Sep 2025 17:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22612v1</guid></item><item><title>Quantile Advantage Estimation for Entropy-Safe Reasoning</title><link>http://arxiv.org/abs/2509.22611v1</link><description>Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLMreasoning, but training often oscillates between {entropy collapse} and{entropy explosion}. We trace both hazards to the mean baseline used invalue-free RL (e.g., GRPO and DAPO), which improperly penalizesnegative-advantage samples under reward outliers. We propose {QuantileAdvantage Estimation} (QAE), replacing the mean with a group-wise K-quantilebaseline. QAE induces a response-level, two-regime gate: on hard queries (p &lt;=1 - K) it reinforces rare successes, while on easy queries (p &gt; 1 - K) ittargets remaining failures. Under first-order softmax updates, we prove{two-sided entropy safety}, giving lower and upper bounds on one-step entropychange that curb explosion and prevent collapse. Empirically, this minimalmodification stabilizes entropy, sparsifies credit assignment (with tuned K,roughly 80% of responses receive zero advantage), and yields sustained pass@1gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These resultsidentify {baseline design} -- rather than token-level heuristics -- as theprimary mechanism for scaling RLVR.</description><author>Junkang Wu, Kexin Huang, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He</author><pubDate>Fri, 26 Sep 2025 17:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22611v1</guid></item><item><title>Training-Free Bayesianization for Low-Rank Adapters of Large Language Models</title><link>http://arxiv.org/abs/2412.05723v3</link><description>Estimating the uncertainty of responses from Large Language Models (LLMs)remains a critical challenge. While recent Bayesian methods have demonstratedeffectiveness in quantifying uncertainty through low-rank weight updates, theytypically require complex fine-tuning or post-training procedures. In thispaper, we propose Training-Free Bayesianization (TFB), a simple yettheoretically grounded framework that efficiently transforms trained low-rankadapters into Bayesian ones without additional training. TFB systematicallysearches for the maximally acceptable level of variance in the weightposterior, constrained within a family of low-rank isotropic Gaussiandistributions. Our theoretical analysis shows that under mild conditions, thissearch process is equivalent to KL-regularized variational optimization, ageneralized form of variational inference. Through comprehensive experiments,we show that TFB achieves superior uncertainty estimation and generalizationcompared to existing methods while eliminating the need for complexBayesianization training procedures. Code will be available athttps://github.com/Wang-ML-Lab/bayesian-peft.</description><author>Haizhou Shi, Yibin Wang, Ligong Han, Huan Zhang, Hao Wang</author><pubDate>Fri, 26 Sep 2025 17:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05723v3</guid></item><item><title>Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying</title><link>http://arxiv.org/abs/2503.21767v2</link><description>Open-vocabulary 3D scene understanding is crucial for robotics applications,such as natural language-driven manipulation, human-robot interaction, andautonomous navigation. Existing methods for querying 3D Gaussian Splattingoften struggle with inconsistent 2D mask supervision and lack a robust 3Dpoint-level retrieval mechanism. In this work, (i) we present a novelpoint-level querying framework that performs tracking on segmentation masks toestablish a semantically consistent ground-truth for distilling the languageGaussians; (ii) we introduce a GT-anchored querying approach that firstretrieves the distilled ground-truth and subsequently uses the ground-truth toquery the individual Gaussians. Extensive experiments on three benchmarkdatasets demonstrate that the proposed method outperforms state-of-the-artperformance. Our method achieves an mIoU improvement of +4.14, +20.42, and +1.7on the LERF, 3D-OVS, and Replica datasets. These results validate our frameworkas a promising step toward open-vocabulary understanding in real-world roboticsystems.</description><author>Hairong Yin, Huangying Zhan, Yi Xu, Raymond A. Yeh</author><pubDate>Fri, 26 Sep 2025 17:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.21767v2</guid></item><item><title>TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting</title><link>http://arxiv.org/abs/2506.18862v2</link><description>Temporal Change Description (TCD) and Future Satellite Image Forecasting(FSIF) are critical, yet historically disjointed tasks in Satellite Image TimeSeries (SITS) analysis. Both are fundamentally limited by the common challengeof modeling long-range temporal dynamics. To explore how to improve theperformance of methods on both tasks simultaneously by enhancing long-rangetemporal understanding capabilities, we introduce TAMMs, the first unifiedframework designed to jointly perform TCD and FSIF within a singleMLLM-diffusion architecture. TAMMs introduces two key innovations: TemporalAdaptation Modules (TAM) enhance frozen MLLM's ability to comprehend long-rangedynamics, and Semantic-Fused Control Injection (SFCI) mechanism translates thischange understanding into fine-grained generative control. This synergisticdesign makes the understanding from the TCD task to directly inform and improvethe consistency of the FSIF task. Extensive experiments demonstrate TAMMssignificantly outperforms state-of-the-art specialist baselines on both tasks.</description><author>Zhongbin Guo, Yuhao Wang, Ping Jian, Chengzhi Li, Xinyue Chen, Zhen Yang, Ertai E</author><pubDate>Fri, 26 Sep 2025 17:35:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.18862v2</guid></item><item><title>RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation</title><link>http://arxiv.org/abs/2509.15724v2</link><description>Large deep learning models such as BERT and ResNet achieve state-of-the-artperformance but are costly to deploy at the edge due to their size and computedemands. We present RMT-KD, a compression method that leverages Random MatrixTheory (RMT) for knowledge distillation to iteratively reduce network size.Instead of pruning or heuristic rank selection, RMT-KD preserves onlyinformative directions identified via the spectral properties of hiddenrepresentations. RMT-based causal reduction is applied layer by layer withself-distillation to maintain stability and accuracy. On GLUE, AG News, andCIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracyloss, delivering 2.8x faster inference and nearly halved power consumption.These results establish RMT-KD as a mathematically grounded approach to networkdistillation.</description><author>Davide Ettori, Nastaran Darabi, Sureshkumar Senthilkumar, Amit Ranjan Trivedi</author><pubDate>Fri, 26 Sep 2025 17:34:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.15724v2</guid></item><item><title>Capturing Opinion Shifts in Deliberative Discourse through Frequency-based Quantum deep learning methods</title><link>http://arxiv.org/abs/2509.22603v1</link><description>Deliberation plays a crucial role in shaping outcomes by weighing diverseperspectives before reaching decisions. With recent advancements in NaturalLanguage Processing, it has become possible to computationally modeldeliberation by analyzing opinion shifts and predicting potential outcomesunder varying scenarios. In this study, we present a comparative analysis ofmultiple NLP techniques to evaluate how effectively models interpretdeliberative discourse and produce meaningful insights. Opinions fromindividuals of varied backgrounds were collected to construct a self-sourceddataset that reflects diverse viewpoints. Deliberation was simulated usingproduct presentations enriched with striking facts, which often promptedmeasurable shifts in audience opinions. We have given comparative analysisbetween two models namely Frequency-Based Discourse Modulation andQuantum-Deliberation Framework which outperform the existing state of artmodels. The findings highlight practical applications in public policy-making,debate evaluation, decision-support frameworks, and large-scale social mediaopinion mining.</description><author>Rakesh Thakur, Harsh Chaturvedi, Ruqayya Shah, Janvi Chauhan, Ayush Sharma</author><pubDate>Fri, 26 Sep 2025 17:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22603v1</guid></item><item><title>Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection</title><link>http://arxiv.org/abs/2509.10779v2</link><description>Dense small objects in UAV imagery are often missed due to long-rangeviewpoints, occlusion, and clutter[cite: 5]. This paper presents adetector-agnostic post-processing framework that converts overlap-inducedredundancy into group evidence[cite: 6]. Overlapping tiling first recoverslow-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates groupevidence[cite: 7]. Validated groups receive controlled confidence reweightingbefore class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recallincrease from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s perimage[cite: 10]. These results indicate recall-first, precision-trade-offbehavior that benefits recall-sensitive applications such as far-field countingand monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,spatial clustering stabilizes geometry, semantic clustering enforces appearancecoherence, and reweighting provides calibrated integration with thebaseline[cite: 11]. The framework requires no retraining and integrates withmodern detectors[cite: 12]. Future work will reduce semantic gating cost andextend the approach with temporal cues[cite: 13].</description><author>Yilun Xiao</author><pubDate>Fri, 26 Sep 2025 17:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.10779v2</guid></item><item><title>How Strategic Agents Respond: Comparing Analytical Models with LLM-Generated Responses in Strategic Classification</title><link>http://arxiv.org/abs/2501.16355v2</link><description>When ML algorithms are deployed to automate human-related decisions, humanagents may learn the underlying decision policies and adapt their behavior.Strategic Classification (SC) has emerged as a framework for studying thisinteraction between agents and decision-makers to design more trustworthy MLsystems. Prior theoretical models in SC assume that agents are perfectly orapproximately rational and respond to decision policies by optimizing theirutility. However, the growing prevalence of LLMs raises the possibility thatreal-world agents may instead rely on these tools for strategic advice. Thisshift prompts two questions: (i) Can LLMs generate effective and sociallyresponsible strategies in SC settings? (ii) Can existing SC theoretical modelsaccurately capture agent behavior when agents follow LLM-generated advice? Toinvestigate these questions, we examine five critical SC scenarios: hiring,loan applications, school admissions, personal income, and public assistanceprograms. We simulate agents with diverse profiles who interact with threecommercial LLMs (GPT-4o, GPT-4.1, and GPT-5), following their suggestions oneffort allocations on features. We compare the resulting agent behaviors withthe best responses in existing SC models. Our findings show that: (i) Evenwithout access to the decision policy, LLMs can generate effective strategiesthat improve both agents' scores and qualification; (ii) At the populationlevel, LLM-guided effort allocation strategies yield similar or even higherscore improvements, qualification rates, and fairness metrics as thosepredicted by the SC theoretical model, suggesting that the theoretical modelmay still serve as a reasonable proxy for LLM-influenced behavior; and (iii) Atthe individual level, LLMs tend to produce more diverse and balanced effortallocations than theoretical models.</description><author>Tian Xie, Pavan Rauch, Xueru Zhang</author><pubDate>Fri, 26 Sep 2025 17:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.16355v2</guid></item><item><title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title><link>http://arxiv.org/abs/2509.22601v1</link><description>Reinforcement learning (RL) is the dominant paradigm for sharpening strategictool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,yet it faces a fundamental challenge of exploration-exploitation trade-off.Existing studies stimulate exploration through the lens of policy entropy, butsuch mechanical entropy maximization is prone to RL training instability due tothe multi-turn distribution shifting. In this paper, we target the progressiveexploration-exploitation balance under the guidance of the agent ownexperiences without succumbing to either entropy collapsing or runawaydivergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)recipe for training agentic LLMs. It extends the vanilla SIL framework, where areplay buffer stores self-generated promising trajectories for off-policyupdate, by gradually steering the policy evolution within a well-balanced rangeof entropy across stages. Specifically, our approach incorporates a curriculumto manage the exploration process, utilizing intrinsic rewards to fosterskill-level exploration and facilitating action-level exploration through SIL.At first, the auxiliary tool call reward plays a critical role in theaccumulation of tool-use skills, enabling broad exposure to the unfamiliardistributions of the environment feedback with an upward entropy trend. Astraining progresses, self-imitation gets strengthened to exploit existingsuccessful patterns from replayed experiences for comparative action-levelexploration, accelerating solution iteration without unbounded entropy growth.To further stabilize training, we recalibrate the advantages of experiences inthe replay buffer to address the potential policy drift. Reugularizations suchas the clipping of tokens with high covariance between probability andadvantage are introduced to the trajectory-level entropy control to curbover-confidence.</description><author>Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li, Zihan Xu, Yuchen Shi, Siqi Cai, Renting Rui, Shaofei Cai, Yuzheng Cai, Xuan Zhang, Sheng Ye, Ke Li, Xing Sun</author><pubDate>Fri, 26 Sep 2025 17:20:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22601v1</guid></item><item><title>Fused Partial Gromov-Wasserstein for Structured Objects</title><link>http://arxiv.org/abs/2502.09934v2</link><description>Structured data, such as graphs, is vital in machine learning due to itscapacity to capture complex relationships and interactions. In recent years,the Fused Gromov-Wasserstein (FGW) distance has attracted growing interestbecause it enables the comparison of structured data by jointly accounting forfeature similarity and geometric structure. However, as a variant of optimaltransport (OT), classical FGW assumes an equal mass constraint on the compareddata. In this work, we relax this mass constraint and propose the Fused PartialGromov-Wasserstein (FPGW) framework, which extends FGW to accommodateunbalanced data. Theoretically, we establish the relationship between FPGW andFGW and prove the metric properties of FPGW. Numerically, we introduceFrank-Wolfe solvers and Sinkhorn solvers for the proposed FPGW framework.Finally, we evaluate the FPGW distance through graph matching, graphclassification and graph clustering experiments, demonstrating its robustperformance.</description><author>Yikun Bai, Shuang Wang, Huy Tran, Hengrong Du, Juexin Wang, Soheil Kolouri</author><pubDate>Fri, 26 Sep 2025 17:17:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09934v2</guid></item><item><title>From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages</title><link>http://arxiv.org/abs/2509.22598v1</link><description>We prove that all standard subregular language classes are linearly separablewhen represented by their deciding predicates. This establishes finiteobservability and guarantees learnability with simple linear models. Syntheticexperiments confirm perfect separability under noise-free conditions, whilereal-data experiments on English morphology show that learned features alignwith well-known linguistic constraints. These results demonstrate that thesubregular hierarchy provides a rigorous and interpretable foundation formodeling natural language structure. Our code used in real-data experiments isavailable at https://github.com/UTokyo-HayashiLab/subregular.</description><author>Katsuhiko Hayashi, Hidetaka Kamigaito</author><pubDate>Fri, 26 Sep 2025 17:17:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22598v1</guid></item><item><title>Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives</title><link>http://arxiv.org/abs/2509.22596v1</link><description>In this paper, we present two effective policy learning algorithms formulti-agent online coordination(MA-OC) problem. The first one, \texttt{MA-SPL},not only can achieve the optimal $(1-\frac{c}{e})$-approximation guarantee forthe MA-OC problem with submodular objectives but also can handle the unexplored$\alpha$-weakly DR-submodular and $(\gamma,\beta)$-weakly submodular scenarios,where $c$ is the curvature of the investigated submodular functions, $\alpha$denotes the diminishing-return(DR) ratio and the tuple $(\gamma,\beta)$represents the submodularity ratios. Subsequently, in order to reduce thereliance on the unknown parameters $\alpha,\gamma,\beta$ inherent in the\texttt{MA-SPL} algorithm, we further introduce the second online algorithmnamed \texttt{MA-MPL}. This \texttt{MA-MPL} algorithm is entirely\emph{parameter-free} and simultaneously can maintain the same approximationratio as the first \texttt{MA-SPL} algorithm. The core of our \texttt{MA-SPL}and \texttt{MA-MPL} algorithms is a novel continuous-relaxation techniquetermed as \emph{policy-based continuous extension}. Compared with thewell-established \emph{multi-linear extension}, a notable advantage of this new\emph{policy-based continuous extension} is its ability to provide a losslessrounding scheme for any set function, thereby enabling us to tackle thechallenging weakly submodular objectives. Finally, extensive simulations areconducted to validate the effectiveness of our proposed algorithms.</description><author>Qixin Zhang, Yan Sun, Can Jin, Xikun Zhang, Yao Shu, Puning Zhao, Li Shen, Dacheng Tao</author><pubDate>Fri, 26 Sep 2025 17:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22596v1</guid></item><item><title>Transport Based Mean Flows for Generative Modeling</title><link>http://arxiv.org/abs/2509.22592v1</link><description>Flow-matching generative models have emerged as a powerful paradigm forcontinuous data generation, achieving state-of-the-art results across domainssuch as images, 3D shapes, and point clouds. Despite their success, thesemodels suffer from slow inference due to the requirement of numerous sequentialsampling steps. Recent work has sought to accelerate inference by reducing thenumber of sampling steps. In particular, Mean Flows offer a one-step generationapproach that delivers substantial speedups while retaining strong generativeperformance. Yet, in many continuous domains, Mean Flows fail to faithfullyapproximate the behavior of the original multi-step flow-matching process. Inthis work, we address this limitation by incorporating optimal transport-basedsampling strategies into the Mean Flow framework, enabling one-step generatorsthat better preserve the fidelity and diversity of the original multi-step flowprocess. Experiments on controlled low-dimensional settings and onhigh-dimensional tasks such as image generation, image-to-image translation,and point cloud generation demonstrate that our approach achieves superiorinference accuracy in one-step generative modeling.</description><author>Elaheh Akbari, Ping He, Ahmadreza Moradipari, Yikun Bai, Soheil Kolouri</author><pubDate>Fri, 26 Sep 2025 17:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22592v1</guid></item><item><title>Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?</title><link>http://arxiv.org/abs/2504.03814v5</link><description>Large language models (LLMs) are increasingly used in the creation of onlinecontent, creating feedback loops as subsequent generations of models will betrained on this synthetic data. Such loops were shown to lead to distributionshifts - models misrepresenting the true underlying distributions of human data(also called model collapse). However, how human data properties affect suchshifts remains poorly understood. In this paper, we provide the first empiricalexamination of the effect of such properties on the outcome of recursivetraining. We first confirm that using different human datasets leads todistribution shifts of different magnitudes. Through exhaustive manipulation ofdataset properties combined with regression analyses, we then identify a set ofproperties predicting distribution shift magnitudes. Lexical diversity is foundto amplify these shifts, while semantic diversity and data quality mitigatethem. Furthermore, we find that these influences are highly modular: datascrapped from a given internet domain has little influence on the contentgenerated for another domain. Finally, experiments on political bias revealthat human data properties affect whether the initial bias will be amplified orreduced. Overall, our results portray a novel view, where different parts ofinternet may undergo different types of distribution shift.</description><author>Grgur Kova, Jrmy Perez, Rmy Portelas, Peter Ford Dominey, Pierre-Yves Oudeyer</author><pubDate>Fri, 26 Sep 2025 17:11:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.03814v5</guid></item><item><title>ArabJobs: A Multinational Corpus of Arabic Job Ads</title><link>http://arxiv.org/abs/2509.22589v1</link><description>ArabJobs is a publicly available corpus of Arabic job advertisementscollected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates.Comprising over 8,500 postings and more than 550,000 words, the datasetcaptures linguistic, regional, and socio-economic variation in the Arab labourmarket. We present analyses of gender representation and occupationalstructure, and highlight dialectal variation across ads, which offersopportunities for future research. We also demonstrate applications such assalary estimation and job category normalisation using large language models,alongside benchmark tasks for gender bias detection and professionclassification. The findings show the utility of ArabJobs for fairness-awareArabic NLP and labour market research. The dataset is publicly available onGitHub: https://github.com/drelhaj/ArabJobs.</description><author>Mo El-Haj</author><pubDate>Fri, 26 Sep 2025 17:06:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22589v1</guid></item><item><title>Metric-Guided Conformal Bounds for Probabilistic Image Reconstruction</title><link>http://arxiv.org/abs/2404.15274v4</link><description>Modern deep learning reconstruction algorithms generate impressivelyrealistic scans from sparse inputs, but can often produce significantinaccuracies. This makes it difficult to provide statistically guaranteedclaims about the true state of a subject from scans reconstructed by thesealgorithms. In this study, we propose a framework for computing provably validprediction bounds on claims derived from probabilistic black-box imagereconstruction algorithms. The key insights behind our framework are torepresent reconstructed scans with a derived clinical metric of interest, andto calibrate bounds on the ground truth metric with conformal prediction (CP)using a prior calibration dataset. These bounds convey interpretable feedbackabout the subject's state, and can also be used to retrieve nearest-neighborreconstructed scans for visual inspection. We demonstrate the utility of thisframework on sparse-view computed tomography (CT) for fat mass quantificationand radiotherapy planning tasks. Results show that our framework producesbounds with better semantical interpretation than conventional pixel-basedbounding approaches. Furthermore, we can flag dangerous outlier reconstructionsthat look plausible but have statistically unlikely metric values.</description><author>Matt Y Cheung, Tucker J Netherton, Laurence E Court, Ashok Veeraraghavan, Guha Balakrishnan</author><pubDate>Fri, 26 Sep 2025 17:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15274v4</guid></item><item><title>Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs</title><link>http://arxiv.org/abs/2509.22582v1</link><description>Context-grounded hallucinations are cases where model outputs containinformation not verifiable against the source text. We study the applicabilityof LLMs for localizing such hallucinations, as a more practical alternative toexisting complex evaluation pipelines. In the absence of established benchmarksfor meta-evaluation of hallucinations localization, we construct one tailoredto LLMs, involving a challenging human annotation of over 1,000 examples. Wecomplement the benchmark with an LLM-based evaluation protocol, verifying itsquality in a human evaluation. Since existing representations of hallucinationslimit the types of errors that can be expressed, we propose a newrepresentation based on free-form textual descriptions, capturing the fullrange of possible errors. We conduct a comprehensive study, evaluating fourlarge-scale LLMs, which highlights the benchmark's difficulty, as the bestmodel achieves an F1 score of only 0.67. Through careful analysis, we offerinsights into optimal prompting strategies for the task and identify the mainfactors that make it challenging for LLMs: (1) a tendency to incorrectly flagmissing details as inconsistent, despite being instructed to check only factsin the output; and (2) difficulty with outputs containing factually correctinformation absent from the source - and thus not verifiable - due to alignmentwith the model's parametric knowledge.</description><author>Yehonatan Pesiakhovsky, Zorik Gekhman, Yosi Mass, Liat Ein-Dor, Roi Reichart</author><pubDate>Fri, 26 Sep 2025 17:03:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22582v1</guid></item><item><title>SpikeMatch: Semi-Supervised Learning with Temporal Dynamics of Spiking Neural Networks</title><link>http://arxiv.org/abs/2509.22581v1</link><description>Spiking neural networks (SNNs) have recently been attracting significantattention for their biological plausibility and energy efficiency, butsemi-supervised learning (SSL) methods for SNN-based models remainunderexplored compared to those for artificial neural networks (ANNs). In thispaper, we introduce SpikeMatch, the first SSL framework for SNNs that leveragesthe temporal dynamics through the leakage factor of SNNs for diversepseudo-labeling within a co-training framework. By utilizing agreement amongmultiple predictions from a single SNN, SpikeMatch generates reliablepseudo-labels from weakly-augmented unlabeled samples to train onstrongly-augmented ones, effectively mitigating confirmation bias by capturingdiscriminative features with limited labels. Experiments show that SpikeMatchoutperforms existing SSL methods adapted to SNN backbones across variousstandard benchmarks.</description><author>Jini Yang, Beomseok Oh, Seungryong Kim, Sunok Kim</author><pubDate>Fri, 26 Sep 2025 17:01:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22581v1</guid></item><item><title>The Lie of the Average: How Class Incremental Learning Evaluation Deceives You?</title><link>http://arxiv.org/abs/2509.22580v1</link><description>Class Incremental Learning (CIL) requires models to continuously learn newclasses without forgetting previously learned ones, while maintaining stableperformance across all possible class sequences. In real-world settings, theorder in which classes arrive is diverse and unpredictable, and modelperformance can vary substantially across different sequences. Yet mainstreamevaluation protocols calculate mean and variance from only a small set ofrandomly sampled sequences. Our theoretical analysis and empirical resultsdemonstrate that this sampling strategy fails to capture the full performancerange, resulting in biased mean estimates and a severe underestimation of thetrue variance in the performance distribution. We therefore contend that arobust CIL evaluation protocol should accurately characterize and estimate theentire performance distribution. To this end, we introduce the concept ofextreme sequences and provide theoretical justification for their crucial rolein the reliable evaluation of CIL. Moreover, we observe a consistent positivecorrelation between inter-task similarity and model performance, a relationthat can be leveraged to guide the search for extreme sequences. Building onthese insights, we propose EDGE (Extreme case-based Distribution andGeneralization Evaluation), an evaluation protocol that adaptively identifiesand samples extreme class sequences using inter-task similarity, offering acloser approximation of the ground-truth performance distribution. Extensiveexperiments demonstrate that EDGE effectively captures performance extremes andyields more accurate estimates of distributional boundaries, providingactionable insights for model selection and robustness checking. Our code isavailable at https://github.com/AIGNLAI/EDGE.</description><author>Guannan Lai, Da-Wei Zhou, Xin Yang, Han-Jia Ye</author><pubDate>Fri, 26 Sep 2025 17:00:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22580v1</guid></item><item><title>Data-driven Neural Networks for Windkessel Parameter Calibration</title><link>http://arxiv.org/abs/2509.21206v2</link><description>In this work, we propose a novel method for calibrating Windkessel (WK)parameters in a dimensionally reduced 1D-0D coupled blood flow model. To thisend, we design a data-driven neural network (NN)trained on simulated bloodpressures in the left brachial artery. Once trained, the NN emulates thepressure pulse waves across the entire simulated domain, i.e., over time, spaceand varying WK parameters, with negligible error and computational effort. Tocalibrate the WK parameters on a measured pulse wave, the NN is extended bydummy neurons and retrained only on these. The main objective of this work isto assess the effectiveness of the method in various scenarios -- particularly,when the exact measurement location is unknown or the data are affected bynoise.</description><author>Benedikt Hoock, Tobias Kppl</author><pubDate>Fri, 26 Sep 2025 16:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21206v2</guid></item><item><title>Intuition emerges in Maximum Caliber models at criticality</title><link>http://arxiv.org/abs/2508.06477v2</link><description>Whether large predictive models merely parrot their training data or producegenuine insight lacks a physical explanation. This work reports a primitiveform of intuition that emerges as a metastable phase of learning thatcritically balances next-token prediction against future path-entropy. Theintuition mechanism is discovered via mind-tuning, the minimal principle thatimposes Maximum Caliber in predictive models with a control temperature-likeparameter $\lambda$. Training on random walks in deterministic mazes reveals arich phase diagram: imitation (low $\lambda$), rule-breaking hallucination(high $\lambda$), and a fragile in-between window exhibiting strongprotocol-dependence (hysteresis) and multistability, where models spontaneouslydiscover novel goal-directed strategies. These results are captured by aneffective low-dimensional theory and frame intuition as an emergent property atthe critical balance between memorizing what is and wondering what could be.</description><author>Llus Arola-Fernndez</author><pubDate>Fri, 26 Sep 2025 16:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06477v2</guid></item><item><title>EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning</title><link>http://arxiv.org/abs/2509.22576v1</link><description>Training LLM agents in multi-turn environments with sparse rewards, wherecompleting a single task requires 30+ turns of interaction within an episode,presents a fundamental challenge for reinforcement learning. We identify acritical failure mode unique to this setting: the exploration-exploitationcascade failure. This cascade begins with early-stage policy prematureconvergence, where sparse feedback causes agents to commit to flawed,low-entropy strategies. Subsequently, agents enter late-stage policy collapse,where conventional entropy regularization becomes counterproductive, promotingchaotic exploration that destabilizes training. We propose Entropy-regularizedPolicy Optimization (EPO), a general framework that breaks this failure cyclethrough three synergistic mechanisms: (1) adopting entropy regularization inmulti-turn settings to enhance exploration, (2) an entropy smoothingregularizer that bounds policy entropy within historical averages to preventabrupt fluctuations, and (3) adaptive phase-based weighting that balancesexploration and exploitation across training. Our analysis justifies that EPOguarantees monotonically decreasing entropy variance while maintainingconvergence. EPO achieves up to 152% performance improvement on ScienceWorldand up to 19.8% on ALFWorld. Our work demonstrates that multi-turnsparse-reward settings require fundamentally different entropy control thantraditional RL, with broad implications for LLM agent training.</description><author>Xu Wujiang, Wentian Zhao, Zhenting Wang, Li Yu-Jhe, Jin Can, Jin Mingyu, Mei Kai, Wan Kun, Metaxas Dimitris</author><pubDate>Fri, 26 Sep 2025 16:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22576v1</guid></item><item><title>Machine learning approaches to seismic event classification in the Ostrava region</title><link>http://arxiv.org/abs/2509.22574v1</link><description>The northeastern region of the Czech Republic is among the most seismicallyactive areas in the country. The most frequent seismic events aremining-induced since there used to be strong mining activity in the past.However, natural tectonic events may also occur. In addition, seismic stationsoften record explosions in quarries in the region. Despite the cessation ofmining activities, mine-induced seismic events still occur. Therefore, a rapiddifferentiation between tectonic and anthropogenic events is still important. The region is currently monitored by the OKC seismic station inOstrava-Kr\'{a}sn\'{e} Pole built in 1983 which is a part of the Czech RegionalSeismic Network. The station has been providing digital continuous waveformdata at 100 Hz since 2007. In the years 1992--2002, the region was co-monitoredby the Seismic Polygon Fren\v{s}t\'{a}t (SPF) which consisted of five seismicstations using a triggered STA/LTA system. In this study, we apply and compare machine learning methods to the SPFdataset, which contains labeled records of tectonic and mining-induced events.For binary classification, a Long Short-Term Memory recurrent neural networkand XGBoost achieved an F1-score of 0.94 -- 0.95, demonstrating the potentialof modern machine learning techniques for rapid event characterization.</description><author>Marek Pecha, Michael Skotnica, Jana Ruajov, Bohdan Rieznikov, Vt Wandrol, Markta Rsnerov, Jaromr Knejzlk</author><pubDate>Fri, 26 Sep 2025 16:51:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22574v1</guid></item><item><title>MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data</title><link>http://arxiv.org/abs/2509.22573v1</link><description>Efficiently detecting human intent to interact with ubiquitous robots iscrucial for effective human-robot interaction (HRI) and collaboration. Over thepast decade, deep learning has gained traction in this field, with mostexisting approaches relying on multimodal inputs, such as RGB combined withdepth (RGB-D), to classify time-sequence windows of sensory data as interactiveor non-interactive. In contrast, we propose a novel RGB-only pipeline forpredicting human interaction intent with frame-level precision, enabling fasterrobot responses and improved service quality. A key challenge in intentprediction is the class imbalance inherent in real-world HRI datasets, whichcan hinder the model's training and generalization. To address this, weintroduce MINT-RVAE, a synthetic sequence generation method, along with newloss functions and training strategies that enhance generalization onout-of-sample data. Our approach achieves state-of-the-art performance (AUROC:0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGBinput and supporting precise frame onset prediction. Finally, to support futureresearch, we openly release our new dataset with frame-level labeling of humaninteraction intent.</description><author>Farida Mohsen, Ali Safa</author><pubDate>Fri, 26 Sep 2025 16:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22573v1</guid></item><item><title>Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time</title><link>http://arxiv.org/abs/2509.22572v1</link><description>Test-Time Scaling (TTS) enhances the reasoning ability of large languagemodels (LLMs) by allocating additional computation during inference. However,existing approaches primarily rely on output-level sampling while overlookingthe role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, weobserve that varying the number of activated experts yields complementarysolution sets with stable accuracy, revealing a new and underexplored source ofdiversity. Motivated by this observation, we propose Dynamic Experts Search(DES), a TTS strategy that elevates expert activation into a controllabledimension of the search space. DES integrates two key components: (1) DynamicMoE, which enables direct control of expert counts during inference to generatediverse reasoning trajectories without additional cost; and (2) ExpertConfiguration Inheritance, which preserves consistent expert counts within areasoning path while varying them across runs, thereby balancing stability anddiversity throughout the search. Extensive experiments across MoEarchitectures, verifiers and reasoning benchmarks (i.e., math, code andknowledge) demonstrate that DES reliably outperforms TTS baselines, enhancingaccuracy and stability without additional cost. These results highlight DES asa practical and scalable form of architecture-aware TTS, illustrating howstructural flexibility in modern LLMs can advance reasoning.</description><author>Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang</author><pubDate>Fri, 26 Sep 2025 16:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22572v1</guid></item><item><title>Multi-View Hypercomplex Learning for Breast Cancer Screening</title><link>http://arxiv.org/abs/2204.05798v4</link><description>Radiologists interpret mammography exams by jointly analyzing all four views,as correlations among them are crucial for accurate diagnosis. Recent methodsemploy dedicated fusion blocks to capture such dependencies, but these areoften hindered by view dominance, training instability, and computationaloverhead. To address these challenges, we introduce multi-view hypercomplexlearning, a novel learning paradigm for multi-view breast cancer classificationbased on parameterized hypercomplex neural networks (PHNNs). Thanks tohypercomplex algebra, our models intrinsically capture both intra- andinter-view relations. We propose PHResNets for two-view exams and twocomplementary four-view architectures: PHYBOnet, optimized for efficiency, andPHYSEnet, optimized for accuracy. Extensive experiments demonstrate that ourapproach consistently outperforms state-of-the-art multi-view models, whilealso generalizing across radiographic modalities and tasks such as diseaseclassification from chest X-rays and multimodal brain tumor segmentation. Fullcode and pretrained models are available at https://github.com/ispamm/PHBreast.</description><author>Eleonora Lopez, Eleonora Grassucci, Danilo Comminiello</author><pubDate>Fri, 26 Sep 2025 16:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.05798v4</guid></item><item><title>UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration</title><link>http://arxiv.org/abs/2509.22570v1</link><description>The rapid progress of Large Multimodal Models (LMMs) and cloud-based AIagents is transforming human-AI collaboration into bidirectional, multimodalinteraction. However, existing codecs remain optimized for unimodal, one-waycommunication, resulting in repeated degradation under conventionalcompress-transmit-reconstruct pipelines. To address this limitation, we proposeUniMIC, a Unified token-based Multimodal Interactive Coding framework thatbridges edge devices and cloud AI agents. Instead of transmitting raw pixels orplain text, UniMIC employs compact tokenized representations as thecommunication medium, enabling efficient low-bitrate transmission whilemaintaining compatibility with LMMs. To further enhance compression,lightweight Transformer-based entropy models with scenario-specificdesigns-generic, masked, and text-conditioned-effectively minimize inter-tokenredundancy. Extensive experiments on text-to-image generation, text-guidedinpainting, outpainting, and visual question answering show that UniMICachieves substantial bitrate savings and remains robust even at ultra-lowbitrates (&lt;0.05bpp), without compromising downstream task performance. Theseresults establish UniMIC as a practical and forward-looking paradigm fornext-generation multimodal interactive communication.</description><author>Qi Mao, Tinghan Yang, Jiahao Li, Bin Li, Libiao Jin, Yan Lu</author><pubDate>Fri, 26 Sep 2025 16:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22570v1</guid></item><item><title>From Parameters to Behavior: Unsupervised Compression of the Policy Space</title><link>http://arxiv.org/abs/2509.22566v1</link><description>Despite its recent successes, Deep Reinforcement Learning (DRL) isnotoriously sample-inefficient. We argue that this inefficiency stems from thestandard practice of optimizing policies directly in the high-dimensional andhighly redundant parameter space $\Theta$. This challenge is greatly compoundedin multi-task settings. In this work, we develop a novel, unsupervised approachthat compresses the policy parameter space $\Theta$ into a low-dimensionallatent space $\mathcal{Z}$. We train a generative model$g:\mathcal{Z}\to\Theta$ by optimizing a behavioral reconstruction loss, whichensures that the latent space is organized by functional similarity rather thanproximity in parameterization. We conjecture that the inherent dimensionalityof this manifold is a function of the environment's complexity, rather than thesize of the policy network. We validate our approach in continuous controldomains, showing that the parameterization of standard policy networks can becompressed up to five orders of magnitude while retaining most of itsexpressivity. As a byproduct, we show that the learned manifold enablestask-specific adaptation via Policy Gradient operating in the latent space$\mathcal{Z}$.</description><author>Davide Tenedini, Riccardo Zamboni, Mirco Mutti, Marcello Restelli</author><pubDate>Fri, 26 Sep 2025 16:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22566v1</guid></item><item><title>Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation</title><link>http://arxiv.org/abs/2509.22565v1</link><description>Asynchronous patient-clinician messaging via EHR portals is a growing sourceof clinician workload, prompting interest in large language models (LLMs) toassist with draft responses. However, LLM outputs may contain clinicalinaccuracies, omissions, or tone mismatches, making robust evaluationessential. Our contributions are threefold: (1) we introduce a clinicallygrounded error ontology comprising 5 domains and 59 granular error codes,developed through inductive coding and expert adjudication; (2) we develop aretrieval-augmented evaluation pipeline (RAEC) that leverages semanticallysimilar historical message-response pairs to improve judgment quality; and (3)we provide a two-stage prompting architecture using DSPy to enable scalable,interpretable, and hierarchical error detection. Our approach assesses thequality of drafts both in isolation and with reference to similar pastmessage-response pairs retrieved from institutional archives. Using a two-stageDSPy pipeline, we compared baseline and reference-enhanced evaluations on over1,500 patient messages. Retrieval context improved error identification indomains such as clinical completeness and workflow appropriateness. Humanvalidation on 100 messages demonstrated superior agreement (concordance = 50%vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.baseline, supporting the use of our RAEC pipeline as AI guardrails for patientmessaging.</description><author>Wenyuan Chen, Fateme Nateghi Haredasht, Kameron C. Black, Francois Grolleau, Emily Alsentzer, Jonathan H. Chen, Stephen P. Ma</author><pubDate>Fri, 26 Sep 2025 16:42:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22565v1</guid></item><item><title>MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild</title><link>http://arxiv.org/abs/2509.15548v3</link><description>In-the-wild photo collections often contain limited volumes of imagery andexhibit multiple appearances, e.g., taken at different times of day or seasons,posing significant challenges to scene reconstruction and novel view synthesis.Although recent adaptations of Neural Radiance Field (NeRF) and 3D GaussianSplatting (3DGS) have improved in these areas, they tend to oversmooth and areprone to overfitting. In this paper, we present MS-GS, a novel frameworkdesigned with Multi-appearance capabilities in Sparse-view scenarios using3DGS. To address the lack of support due to sparse initializations, ourapproach is built on the geometric priors elicited from monocular depthestimations. The key lies in extracting and utilizing local semantic regionswith a Structure-from-Motion (SfM) points anchored algorithm for reliablealignment and geometry cues. Then, to introduce multi-view constraints, wepropose a series of geometry-guided supervision at virtual views in afine-grained and coarse scheme to encourage 3D consistency and reduceoverfitting. We also introduce a dataset and an in-the-wild experiment settingto set up more realistic benchmarks. We demonstrate that MS-GS achievesphotorealistic renderings under various challenging sparse-view andmulti-appearance conditions and outperforms existing approaches significantlyacross different datasets.</description><author>Deming Li, Kaiwen Jiang, Yutao Tang, Ravi Ramamoorthi, Rama Chellappa, Cheng Peng</author><pubDate>Fri, 26 Sep 2025 16:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.15548v3</guid></item><item><title>Nearly Tight Regret Bounds for Profit Maximization in Bilateral Trade</title><link>http://arxiv.org/abs/2509.22563v1</link><description>Bilateral trade models the task of intermediating between two strategicagents, a seller and a buyer, willing to trade a good for which they holdprivate valuations. We study this problem from the perspective of a broker, ina regret minimization framework. At each time step, a new seller and buyerarrive, and the broker has to propose a mechanism that is incentive-compatibleand individually rational, with the goal of maximizing profit. We propose a learning algorithm that guarantees a nearly tight$\tilde{O}(\sqrt{T})$ regret in the stochastic setting when seller and buyervaluations are drawn i.i.d. from a fixed and possibly correlated unknowndistribution. We further show that it is impossible to achieve sublinear regretin the non-stationary scenario where valuations are generated upfront by anadversary. Our ambitious benchmark for these results is the bestincentive-compatible and individually rational mechanism. This separates usfrom previous works on efficiency maximization in bilateral trade, where thebenchmark is a single number: the best fixed price in hindsight. A particular challenge we face is that uniform convergence for allmechanisms' profits is impossible. We overcome this difficulty via a carefulchaining analysis that proves convergence for a provably near-optimal mechanismat (essentially) optimal rate. We further showcase the broader applicability ofour techniques by providing nearly optimal results for the joint ads problem.</description><author>Simone Di Gregorio, Paul Dtting, Federico Fusco, Chris Schwiegelshohn</author><pubDate>Fri, 26 Sep 2025 16:42:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22563v1</guid></item><item><title>Activation Function Design Sustains Plasticity in Continual Learning</title><link>http://arxiv.org/abs/2509.22562v1</link><description>In independent, identically distributed (i.i.d.) training regimes, activationfunctions have been benchmarked extensively, and their differences often shrinkonce model size and optimization are tuned. In continual learning, however, thepicture is different: beyond catastrophic forgetting, models can progressivelylose the ability to adapt (referred to as loss of plasticity) and the role ofthe non-linearity in this failure mode remains underexplored. We show thatactivation choice is a primary, architecture-agnostic lever for mitigatingplasticity loss. Building on a property-level analysis of negative-branch shapeand saturation behavior, we introduce two drop-in nonlinearities (Smooth-Leakyand Randomized Smooth-Leaky) and evaluate them in two complementary settings:(i) supervised class-incremental benchmarks and (ii) reinforcement learningwith non-stationary MuJoCo environments designed to induce controlleddistribution and dynamics shifts. We also provide a simple stress protocol anddiagnostics that link the shape of the activation to the adaptation underchange. The takeaway is straightforward: thoughtful activation design offers alightweight, domain-general way to sustain plasticity in continual learningwithout extra capacity or task-specific tuning.</description><author>Lute Lillo, Nick Cheney</author><pubDate>Fri, 26 Sep 2025 16:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22562v1</guid></item><item><title>StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models</title><link>http://arxiv.org/abs/2509.22558v1</link><description>Large Language Models (LLMs) have shown promising capabilities for solvingOperations Research (OR) problems. While reinforcement learning serves as apowerful paradigm for LLM training on OR problems, existing works generallyface two key limitations. First, outcome reward suffers from the creditassignment problem, where correct final answers can reinforce flawed reasoning.Second, conventional discriminative process supervision is myopic, failing toevaluate the interdependent steps of OR modeling holistically. To this end, weintroduce StepORLM, a novel self-evolving framework with generative processsupervision. At its core, StepORLM features a co-evolutionary loop where apolicy model and a generative process reward model (GenPRM) iteratively improveon each other. This loop is driven by a dual-feedback mechanism: definitive,outcome-based verification from an external solver, and nuanced, holisticprocess evaluation from the GenPRM. The combined signal is used to align thepolicy via Weighted Direct Preference Optimization (W-DPO) and simultaneouslyrefine the GenPRM. Our resulting 8B-parameter StepORLM establishes a newstate-of-the-art across six benchmarks, significantly outperforming vastlylarger generalist models, agentic methods, and specialized baselines. Moreover,the co-evolved GenPRM is able to act as a powerful and universally applicableprocess verifier, substantially boosting the inference scaling performance ofboth our own model and other existing LLMs.</description><author>Chenyu Zhou, Tianyi Xu, Jianghao Lin, Dongdong Ge</author><pubDate>Fri, 26 Sep 2025 16:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22558v1</guid></item><item><title>Learning to Price Bundles: A GCN Approach for Mixed Bundling</title><link>http://arxiv.org/abs/2509.22557v1</link><description>Bundle pricing refers to designing several product combinations (i.e.,bundles) and determining their prices in order to maximize the expected profit.It is a classic problem in revenue management and arises in many industries,such as e-commerce, tourism, and video games. However, the problem is typicallyintractable due to the exponential number of candidate bundles. In this paper,we explore the usage of graph convolutional networks (GCNs) in solving thebundle pricing problem. Specifically, we first develop a graph representationof the mixed bundling model (where every possible bundle is assigned with aspecific price) and then train a GCN to learn the latent patterns of optimalbundles. Based on the trained GCN, we propose two inference strategies toderive high-quality feasible solutions. A local-search technique is furtherproposed to improve the solution quality. Numerical experiments validate theeffectiveness and efficiency of our proposed GCN-based framework. Using a GCNtrained on instances with 5 products, our methods consistently achievenear-optimal solutions (better than 97%) with only a fraction of computationaltime for problems of small to medium size. It also achieves superior solutionsfor larger size of problems compared with other heuristic methods such asbundle size pricing (BSP). The method can also provide high quality solutionsfor instances with more than 30 products even for the challenging cases whereproduct utilities are non-additive.</description><author>Liangyu Ding, Chenghan Wu, Guokai Li, Zizhuo Wang</author><pubDate>Fri, 26 Sep 2025 16:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22557v1</guid></item><item><title>Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization</title><link>http://arxiv.org/abs/2509.20900v2</link><description>Long document summarization remains a significant challenge for current largelanguage models (LLMs), as existing approaches commonly struggle withinformation loss, factual inconsistencies, and coherence issues when processingexcessively long documents. We propose SummQ, a novel adversarial multi-agentframework that addresses these limitations through collaborative intelligencebetween specialized agents operating in two complementary domains:summarization and quizzing. Our approach employs summary generators andreviewers that work collaboratively to create and evaluate comprehensivesummaries, while quiz generators and reviewers create comprehension questionsthat serve as continuous quality checks for the summarization process. Thisadversarial dynamic, enhanced by an examinee agent that validates whether thegenerated summary contains the information needed to answer the quiz questions,enables iterative refinement through multifaceted feedback mechanisms. Weevaluate SummQ on three widely used long document summarization benchmarks.Experimental results demonstrate that our framework significantly outperformsexisting state-of-the-art methods across ROUGE and BERTScore metrics, as wellas in LLM-as-a-Judge and human evaluations. Our comprehensive analyses revealthe effectiveness of the multi-agent collaboration dynamics, the influence ofdifferent agent configurations, and the impact of the quizzing mechanism. Thiswork establishes a new approach for long document summarization that usesadversarial agentic collaboration to improve summarization quality.</description><author>Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</author><pubDate>Fri, 26 Sep 2025 16:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.20900v2</guid></item><item><title>ECHO: Toward Contextual Seq2Seq Paradigms in Large EEG Models</title><link>http://arxiv.org/abs/2509.22556v1</link><description>Electroencephalography (EEG), with its broad range of applications,necessitates models that can generalize effectively across various tasks anddatasets. Large EEG Models (LEMs) address this by pretraining encoder-centricarchitectures on large-scale unlabeled data to extract universalrepresentations. While effective, these models lack decoders of comparablecapacity, limiting the full utilization of the learned features. To addressthis issue, we introduce ECHO, a novel decoder-centric LEM paradigm thatreformulates EEG modeling as sequence-to-sequence learning. ECHO captureslayered relationships among signals, labels, and tasks within sequence space,while incorporating discrete support samples to construct contextual cues. Thisdesign equips ECHO with in-context learning, enabling dynamic adaptation toheterogeneous tasks without parameter updates. Extensive experiments acrossmultiple datasets demonstrate that, even with basic model components, ECHOconsistently outperforms state-of-the-art single-task LEMs in multi-tasksettings, showing superior generalization and adaptability.</description><author>Chenyu Liu, Yuqiu Deng, Tianyu Liu, Jinan Zhou, Xinliang Zhou, Ziyu Jia, Yi Ding</author><pubDate>Fri, 26 Sep 2025 16:37:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22556v1</guid></item><item><title>Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents</title><link>http://arxiv.org/abs/2505.22954v2</link><description>Today's AI systems have human-designed, fixed architectures and cannotautonomously and continuously improve themselves. The advance of AI coulditself be automated. If done safely, that would accelerate AI development andallow us to reap its benefits much sooner. Meta-learning can automate thediscovery of novel algorithms, but is limited by first-order improvements andthe human design of a suitable search space. The G\"odel machine proposed atheoretical alternative: a self-improving AI that repeatedly modifies itself ina provably beneficial manner. Unfortunately, proving that most changes are netbeneficial is impossible in practice. We introduce the Darwin G\"odel Machine(DGM), a self-improving system that iteratively modifies its own code (therebyalso improving its ability to modify its own codebase) and empiricallyvalidates each change using coding benchmarks. Inspired by Darwinian evolutionand open-endedness research, the DGM maintains an archive of generated codingagents. It grows the archive by sampling an agent from it and using afoundation model to create a new, interesting, version of the sampled agent.This open-ended exploration forms a growing tree of diverse, high-qualityagents and allows the parallel exploration of many different paths through thesearch space. Empirically, the DGM automatically improves its codingcapabilities (e.g., better code editing tools, long-context window management,peer-review mechanisms), increasing performance on SWE-bench from 20.0% to50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantlyoutperforms baselines without self-improvement or open-ended exploration. Allexperiments were done with safety precautions (e.g., sandboxing, humanoversight). The DGM is a significant step toward self-improving AI, capable ofgathering its own stepping stones along paths that unfold into endlessinnovation.</description><author>Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, Jeff Clune</author><pubDate>Fri, 26 Sep 2025 16:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.22954v2</guid></item><item><title>Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement</title><link>http://arxiv.org/abs/2509.22553v1</link><description>Causal representation learning (CRL) has garnered increasing interests fromthe causal inference and artificial intelligence community, due to itscapability of disentangling potentially complex data-generating mechanism intocausally interpretable latent features, by leveraging the heterogeneity ofmodern datasets. In this paper, we further contribute to the CRL literature, byfocusing on the stylized linear structural causal model over the latentfeatures and assuming a linear mixing function that maps latent features to theobserved data or measurements. Existing linear CRL methods often rely onstringent assumptions, such as accessibility to single-node interventional dataor restrictive distributional constraints on latent features and exogenousmeasurement noise. However, these prerequisites can be challenging to satisfyin certain scenarios. In this work, we propose a novel linear CRL algorithmthat, unlike most existing linear CRL methods, operates under weakerassumptions about environment heterogeneity and data-generating distributionswhile still recovering latent causal features up to an equivalence class. Wefurther validate our new algorithm via synthetic experiments and aninterpretability analysis of large language models (LLMs), demonstrating bothits superiority over competing methods in finite samples and its potential inintegrating causality into AI.</description><author>Hao Chen, Lin Liu, Yu Guang Wang</author><pubDate>Fri, 26 Sep 2025 16:35:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22553v1</guid></item><item><title>ExpertSteer: Intervening in LLMs through Expert Knowledge</title><link>http://arxiv.org/abs/2505.12313v2</link><description>Large Language Models (LLMs) exhibit remarkable capabilities across varioustasks, yet guiding them to follow desired behaviours during inference remains asignificant challenge. Activation steering offers a promising method to controlthe generation process of LLMs by modifying their internal activations.However, existing methods commonly intervene in the model's behaviour usingsteering vectors generated by the model itself, which constrains theireffectiveness to that specific model and excludes the possibility of leveragingpowerful external expert models for steering. To address these limitations, wepropose ExpertSteer, a novel approach that leverages arbitrary specializedexpert models to generate steering vectors, enabling intervention in any LLMs.ExpertSteer transfers the knowledge from an expert model to a target LLMthrough a cohesive four-step process: first aligning representation dimensionswith auto-encoders to enable cross-model transfer, then identifyingintervention layer pairs based on mutual information analysis, next generatingsteering vectors from the expert model using Recursive Feature Machines, andfinally applying these vectors on the identified layers during inference toselectively guide the target LLM without updating model parameters. We conductcomprehensive experiments using three LLMs on 15 popular benchmarks across fourdistinct domains. Experiments demonstrate that ExpertSteer significantlyoutperforms established baselines across diverse tasks at minimal cost.</description><author>Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</author><pubDate>Fri, 26 Sep 2025 16:35:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.12313v2</guid></item><item><title>Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation</title><link>http://arxiv.org/abs/2505.11235v2</link><description>Driven by the rapid growth of model parameters, parameter-efficientfine-tuning (PEFT) has become essential for adapting large models to diversedownstream tasks under constrained computational resources. Within thisparadigm, orthogonal fine-tuning and its variants preserve semanticrepresentations of pre-trained models, but struggle to achieve bothexpressiveness and efficiency in terms of parameter counts, memory, andcomputation. To overcome this limitation, we propose efficient OrthogonalFine-Tuning with Principal Subspace adaptation (PSOFT), which confinesorthogonal transformations to the principal subspace of pre-trained weights.Specifically, PSOFT constructs this subspace via matrix decomposition to enablecompatible transformations with higher effective rank, establishes atheoretical condition that strictly maintains the geometry of this subspace foressential semantic preservation, and introduces efficient tunable vectors thatgradually relax orthogonality during training to enhance adaptability.Extensive experiments on 35 NLP and CV tasks across four representative modelsdemonstrate that PSOFT offers a practical and scalable solution tosimultaneously achieve semantic preservation, expressiveness, andmulti-dimensional efficiency in PEFT. The code is publicly available athttps://github.com/fei407/PSOFT.</description><author>Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang</author><pubDate>Fri, 26 Sep 2025 16:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11235v2</guid></item><item><title>HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models</title><link>http://arxiv.org/abs/2505.12300v2</link><description>Fine-tuning large language models (LLMs) on a mixture of diverse datasetsposes challenges due to data imbalance and heterogeneity. Existing methodsoften address these issues across datasets (globally) but overlook theimbalance and heterogeneity within individual datasets (locally), which limitstheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), anovel method that enables LLMs to autonomously adjust data allocation duringfine-tuning both across datasets (globally) and within each individual dataset(locally). HBO employs a bilevel optimization strategy with two types ofactors: a Global Actor, which balances data sampling across different subsetsof the training mixture, and several Local Actors, which optimizes data usagewithin each subset based on difficulty levels. These actors are guided byreward functions derived from the LLM's training state, which measure learningprogress and relative performance improvement. We evaluate HBO on three LLMbackbones across nine diverse tasks in multilingual and multitask setups.Results show that HBO consistently outperforms existing baselines, achievingsignificant accuracy gains. Our in-depth analysis further demonstrates thatboth the global actor and local actors of HBO effectively adjust data usageduring fine-tuning. HBO provides a comprehensive solution to the challenges ofdata imbalance and heterogeneity in LLM fine-tuning, enabling more effectivetraining across diverse datasets.</description><author>Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</author><pubDate>Fri, 26 Sep 2025 16:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.12300v2</guid></item><item><title>ConQuER: Modular Architectures for Control and Bias Mitigation in IQP Quantum Generative Models</title><link>http://arxiv.org/abs/2509.22551v1</link><description>Quantum generative models based on instantaneous quantum polynomial (IQP)circuits show great promise in learning complex distributions while maintainingclassical trainability. However, current implementations suffer from two keylimitations: lack of controllability over generated outputs and severegeneration bias towards certain expected patterns. We present a ControllableQuantum Generative Framework, ConQuER, which addresses both challenges througha modular circuit architecture. ConQuER embeds a lightweight controller circuitthat can be directly combined with pre-trained IQP circuits to preciselycontrol the output distribution without full retraining. Leveraging theadvantages of IQP, our scheme enables precise control over properties such asthe Hamming Weight distribution with minimal parameter and gate overhead. Inaddition, inspired by the controller design, we extend this modular approachthrough data-driven optimization to embed implicit control paths in theunderlying IQP architecture, significantly reducing generation bias onstructured datasets. ConQuER retains efficient classical training propertiesand high scalability. We experimentally validate ConQuER on multiple quantumstate datasets, demonstrating its superior control accuracy and balancedgeneration performance, only with very low overhead cost over original IQPcircuits. Our framework bridges the gap between the advantages of quantumcomputing and the practical needs of controllable generation modeling.</description><author>Xiaocheng Zou, Shijin Duan, Charles Fleming, Gaowen Liu, Ramana Rao Kompella, Shaolei Ren, Xiaolin Xu</author><pubDate>Fri, 26 Sep 2025 16:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22551v1</guid></item><item><title>Metrics for Parametric Families of Networks</title><link>http://arxiv.org/abs/2509.22549v1</link><description>We introduce a general framework for analyzing data modeled as parameterizedfamilies of networks. Building on a Gromov-Wasserstein variant of optimaltransport, we define a family of parameterized Gromov-Wasserstein distances forcomparing such parametric data, including time-varying metric spaces induced bycollective motion, temporally evolving weighted social networks, and randomgraph models. We establish foundational properties of these distances, showingthat they subsume several existing metrics in the literature, and derivetheoretical approximation guarantees. In particular, we develop computationallytractable lower bounds and relate them to graph statistics commonly used inrandom graph theory. Furthermore, we prove that our distances can beconsistently approximated in random graph and random metric space settings viaempirical estimates from generative models. Finally, we demonstrate thepractical utility of our framework through a series of numerical experiments.</description><author>Mario Gmez, Guanqun Ma, Tom Needham, Bei Wang</author><pubDate>Fri, 26 Sep 2025 16:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22549v1</guid></item><item><title>Self-Supervised Learning of Graph Representations for Network Intrusion Detection</title><link>http://arxiv.org/abs/2509.16625v2</link><description>Detecting intrusions in network traffic is a challenging task, particularlyunder limited supervision and constantly evolving attack patterns. While recentworks have leveraged graph neural networks for network intrusion detection,they often decouple representation learning from anomaly detection, limitingthe utility of the embeddings for identifying attacks. We propose GraphIDS, aself-supervised intrusion detection model that unifies these two stages bylearning local graph representations of normal communication patterns through amasked autoencoder. An inductive graph neural network embeds each flow with itslocal topological context to capture typical network behavior, while aTransformer-based encoder-decoder reconstructs these embeddings, implicitlylearning global co-occurrence patterns via self-attention without requiringexplicit positional information. During inference, flows with unusually highreconstruction errors are flagged as potential intrusions. This end-to-endframework ensures that embeddings are directly optimized for the downstreamtask, facilitating the recognition of malicious traffic. On diverse NetFlowbenchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score,outperforming baselines by 5-25 percentage points.</description><author>Lorenzo Guerra, Thomas Chapuis, Guillaume Duc, Pavlo Mozharovskyi, Van-Tam Nguyen</author><pubDate>Fri, 26 Sep 2025 16:30:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.16625v2</guid></item><item><title>JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation</title><link>http://arxiv.org/abs/2509.22548v1</link><description>Vision-and-Language Navigation requires an embodied agent to navigate throughunseen environments, guided by natural language instructions and a continuousvideo stream. Recent advances in VLN have been driven by the powerful semanticunderstanding of Multimodal Large Language Models. However, these methodstypically rely on explicit semantic memory, such as building textual cognitivemaps or storing historical visual frames. This type of method suffers fromspatial information loss, computational redundancy, and memory bloat, whichimpede efficient navigation. Inspired by the implicit scene representation inhuman navigation, analogous to the left brain's semantic understanding and theright brain's spatial cognition, we propose JanusVLN, a novel VLN frameworkfeaturing a dual implicit neural memory that models spatial-geometric andvisual-semantic memory as separate, compact, and fixed-size neuralrepresentations. This framework first extends the MLLM to incorporate 3D priorknowledge from the spatial-geometric encoder, thereby enhancing the spatialreasoning capabilities of models based solely on RGB input. Then, thehistorical key-value caches from the spatial-geometric and visual-semanticencoders are constructed into a dual implicit memory. By retaining only the KVsof tokens in the initial and sliding window, redundant computation is avoided,enabling efficient incremental updates. Extensive experiments demonstrate thatJanusVLN outperforms over 20 recent methods to achieve SOTA performance. Forexample, the success rate improves by 10.5-35.5 compared to methods usingmultiple data types as input and by 3.6-10.8 compared to methods using more RGBtraining data. This indicates that the proposed dual implicit neural memory, asa novel paradigm, explores promising new directions for future VLN research.Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.</description><author>Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, Xing Wei</author><pubDate>Fri, 26 Sep 2025 16:29:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22548v1</guid></item><item><title>Capacity-Aware Planning and Scheduling in Budget-Constrained Multi-Agent MDPs: A Meta-RL Approach</title><link>http://arxiv.org/abs/2410.21249v2</link><description>We study capacity- and budget-constrained multi-agent MDPs (CB-MA-MDPs), aclass that captures many maintenance and scheduling tasks in which each agentcan irreversibly fail and a planner must decide (i) when to apply a restorativeaction and (ii) which subset of agents to treat in parallel. The global budgetlimits the total number of restorations, while the capacity constraint boundsthe number of simultaneous actions, turning na\"ive dynamic programming into acombinatorial search that scales exponentially with the number of agents. Wepropose a two-stage solution that remains tractable for large systems. First, aLinear Sum Assignment Problem (LSAP)-based grouping partitions the agents intor disjoint sets (r = capacity) that maximise diversity in expectedtime-to-failure, allocating budget to each set proportionally. Second, ameta-trained PPO policy solves each sub-MDP, leveraging transfer across groupsto converge rapidly. To validate our approach, we apply it to the problem ofscheduling repairs for a large team of industrial robots, constrained by alimited number of repair technicians and a total repair budget. Our resultsdemonstrate that the proposed method outperforms baseline approaches in termsof maximizing the average uptime of the robot team, particularly for large teamsizes. Lastly, we confirm the scalability of our approach through acomputational complexity analysis across varying numbers of robots and repairtechnicians.</description><author>Manav Vora, Ilan Shomorony, Melkior Ornik</author><pubDate>Fri, 26 Sep 2025 16:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21249v2</guid></item><item><title>Think Socially via Cognitive Reasoning</title><link>http://arxiv.org/abs/2509.22546v1</link><description>LLMs trained for logical reasoning excel at step-by-step deduction to reachverifiable answers. However, this paradigm is ill-suited for navigating socialsituations, which induce an interpretive process of analyzing ambiguous cuesthat rarely yield a definitive outcome. To bridge this gap, we introduceCognitive Reasoning, a paradigm modeled on human social cognition. Itformulates the interpretive process into a structured cognitive flow ofinterconnected cognitive units (e.g., observation or attribution), whichcombine adaptively to enable effective social thinking and responses. We thenpropose CogFlow, a complete framework that instills this capability in LLMs.CogFlow first curates a dataset of cognitive flows by simulating theassociative and progressive nature of human thought via tree-structuredplanning. After instilling the basic cognitive reasoning capability viasupervised fine-tuning, CogFlow adopts reinforcement learning to enable themodel to improve itself via trial and error, guided by a multi-objective rewardthat optimizes both cognitive flow and response quality. Extensive experimentsshow that CogFlow effectively enhances the social cognitive capabilities ofLLMs, and even humans, leading to more effective social decision-making.</description><author>Jinfeng Zhou, Zheyu Chen, Shuai Wang, Quanyu Dai, Zhenhua Dong, Hongning Wang, Minlie Huang</author><pubDate>Fri, 26 Sep 2025 16:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22546v1</guid></item><item><title>Learning Personalized Driving Styles via Reinforcement Learning from Human Feedback</title><link>http://arxiv.org/abs/2503.10434v2</link><description>Generating human-like and adaptive trajectories is essential for autonomousdriving in dynamic environments. While generative models have shown promise insynthesizing feasible trajectories, they often fail to capture the nuancedvariability of personalized driving styles due to dataset biases anddistributional shifts. To address this, we introduce TrajHF, a humanfeedback-driven finetuning framework for generative trajectory models, designedto align motion planning with diverse driving styles. TrajHF incorporatesmulti-conditional denoiser and reinforcement learning with human feedback torefine multi-modal trajectory generation beyond conventional imitationlearning. This enables better alignment with human driving preferences whilemaintaining safety and feasibility constraints. TrajHF achieves performancecomparable to the state-of-the-art on NavSim benchmark. TrajHF sets a newparadigm for personalized and adaptable trajectory generation in autonomousdriving.</description><author>Derun Li, Changye Li, Yue Wang, Jianwei Ren, Xin Wen, Pengxiang Li, Leimeng Xu, Kun Zhan, Peng Jia, Xianpeng Lang, Ningyi Xu, Hang Zhao</author><pubDate>Fri, 26 Sep 2025 16:25:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.10434v2</guid></item><item><title>Diffusion Curriculum: Synthetic-to-Real Data Curriculum via Image-Guided Diffusion</title><link>http://arxiv.org/abs/2410.13674v4</link><description>Low-quality or scarce data has posed significant challenges for training deepneural networks in practice. While classical data augmentation cannotcontribute very different new data, diffusion models opens up a new door tobuild self-evolving AI by generating high-quality and diverse synthetic datathrough text-guided prompts. However, text-only guidance cannot controlsynthetic images' proximity to the original images, resulting inout-of-distribution data detrimental to the model performance. To overcome thelimitation, we study image guidance to achieve a spectrum of interpolationsbetween synthetic and real images. With stronger image guidance, the generatedimages are similar to the training data but hard to learn. While with weakerimage guidance, the synthetic images will be easier for model but contribute toa larger distribution gap with the original data. The generated full spectrumof data enables us to build a novel "Diffusion Curriculum (DisCL)". DisCLadjusts the image guidance level of image synthesis for each training stage: Itidentifies and focuses on hard samples for the model and assesses the mosteffective guidance level of synthetic images to improve hard data learning. Weapply DisCL to two challenging tasks: long-tail (LT) classification andlearning from low-quality data. It focuses on lower-guidance images ofhigh-quality to learn prototypical features as a warm-up of learninghigher-guidance images that might be weak on diversity or quality. Extensiveexperiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy whenapplying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the basemodel's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02%improvement in all-class accuracy.</description><author>Yijun Liang, Shweta Bhardwaj, Tianyi Zhou</author><pubDate>Fri, 26 Sep 2025 16:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13674v4</guid></item><item><title>Benchmarking LLMs in Web API Integration Tasks</title><link>http://arxiv.org/abs/2509.20172v2</link><description>API integration is a cornerstone of our digital infrastructure, enablingsoftware systems to connect and interact. However, as shown by many studies,writing or generating correct code to invoke APIs, particularly web APIs, ischallenging. Although large language models (LLMs) have become popular insoftware development, their effectiveness in automating the generation of webAPI integration code remains unexplored. In order to address this, we present adataset and evaluation pipeline designed to assess the ability of LLMs togenerate web API invocation code. Our experiments with several open-source LLMsreveal that generating API invocations poses a significant challenge, resultingin hallucinated endpoints, incorrect argument usage, and other errors. None ofthe evaluated open-source models were able to solve more than 40% of the tasks.</description><author>Daniel Maninger, Leon Chemnitz, Amir Molzam Sharifloo, Jannis Brugger, Mira Mezini</author><pubDate>Fri, 26 Sep 2025 16:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.20172v2</guid></item><item><title>Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation</title><link>http://arxiv.org/abs/2505.24333v2</link><description>Finding the right initialisation for neural networks is crucial to ensuresmooth training and good performance. In transformers, the wrong initialisationcan lead to one of two failure modes of self-attention layers: rank collapse,where all tokens collapse into similar representations, and entropy collapse,where highly concentrated attention scores lead to training instability. Whileprevious work has studied different scaling regimes for transformers, anasymptotically exact, down-to-the constant prescription for how to initialisetransformers has so far been lacking. Here, we provide an analytical theory ofsignal propagation through deep transformers with self-attention, layernormalisation, skip connections and MLP. Our theory yields a simple algorithmto compute trainability diagrams that identify the correct choice ofinitialisation hyper-parameters for a given architecture. We overcome the keychallenge, an exact treatment of the self-attention layer, by establishing aformal parallel with the Random Energy Model from statistical physics. We alsoanalyse gradients in the backward path and determine the regime where gradientsvanish at initialisation. We demonstrate the versatility of our frameworkthrough three case studies. Our theoretical framework gives a unifiedperspective on the two failure modes of self-attention and gives quantitativepredictions on the scale of both weights and residual connections thatguarantee smooth training.</description><author>Alessio Giorlandino, Sebastian Goldt</author><pubDate>Fri, 26 Sep 2025 16:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24333v2</guid></item><item><title>Do Data Valuations Make Good Data Prices?</title><link>http://arxiv.org/abs/2504.05563v2</link><description>As large language models increasingly rely on external data sources,compensating data contributors has become a central concern. But how shouldthese payments be devised? We revisit data valuations from a$\textit{market-design perspective}$ where payments serve to compensate dataowners for the $\textit{private}$ heterogeneous costs they incur for collectingand sharing data. We show that popular valuation methods-such as Leave-One-Outand Data Shapley-make for poor payments. They fail to ensure truthful reportingof the costs, leading to $\textit{inefficient market}$ outcomes. To addressthis, we adapt well-established payment rules from mechanism design, namelyMyerson and Vickrey-Clarke-Groves (VCG), to the data market setting. We showthat Myerson payment is the minimal truthful mechanism, optimal from thebuyer's perspective. Additionally, we identify a condition under which bothdata buyers and sellers are utility-satisfied, and the market achievesefficiency. Our findings highlight the importance of incorporating incentivecompatibility into data valuation design, paving the way for more robust andefficient data markets. Our data market framework is readily applicable toreal-world scenarios. We illustrate this with simulations of contributorcompensation in an LLM based retrieval-augmented generation (RAG) marketplacetasked with challenging medical question answering.</description><author>Dongyang Fan, Tyler J. Rotello, Sai Praneeth Karimireddy</author><pubDate>Fri, 26 Sep 2025 16:21:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05563v2</guid></item><item><title>Does AI Coaching Prepare us for Workplace Negotiations?</title><link>http://arxiv.org/abs/2509.22545v1</link><description>Workplace negotiations are undermined by psychological barriers, which caneven derail well-prepared tactics. AI offers personalized and always --available negotiation coaching, yet its effectiveness for negotiationpreparedness remains unclear. We built Trucey, a prototype AI coach grounded inBrett's negotiation model. We conducted a between-subjects experiment (N=267),comparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed byin-depth interviews (N=15). While Trucey showed the strongest reductions infear relative to both comparison conditions, the Handbook outperformed both AIsin usability and psychological empowerment. Interviews revealed that theHandbook's comprehensive, reviewable content was crucial for participants'confidence and preparedness. In contrast, although participants valued AI'srehearsal capability, its guidance often felt verbose and fragmented --delivered in bits and pieces that required additional effort -- leaving themuncertain or overwhelmed. These findings challenge assumptions of AIsuperiority and motivate hybrid designs that integrate structured,theory-driven content with targeted rehearsal, clear boundaries, and adaptivescaffolds to address psychological barriers and support negotiationpreparedness.</description><author>Veda Duddu, Jash Rajesh Parekh, Andy Mao, Hanyi Min, Ziang Xiao, Vedant Das Swain, Koustuv Saha</author><pubDate>Fri, 26 Sep 2025 16:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22545v1</guid></item><item><title>Demystifying Multilingual Chain-of-Thought in Process Reward Modeling</title><link>http://arxiv.org/abs/2502.12663v2</link><description>Large language models (LLMs) are designed to perform a wide range of tasks.To improve their ability to solve complex problems requiring multi-stepreasoning, recent research leverages process reward modeling to providefine-grained feedback at each step of the reasoning process for reinforcementlearning (RL), but it predominantly focuses on English. In this paper, wetackle the critical challenge of extending process reward models (PRMs) tomultilingual settings. To achieve this, we train multilingual PRMs on a datasetspanning seven languages, which is translated from English. Throughcomprehensive evaluations on two widely used reasoning benchmarks across 11languages, we demonstrate that multilingual PRMs not only improve averageaccuracy but also reduce early-stage reasoning errors. Furthermore, our resultshighlight the sensitivity of multilingual PRMs to both the number of traininglanguages and the volume of English data, while also uncovering the benefitsarising from more candidate responses and trainable parameters. This work openspromising avenues for robust multilingual applications in complex, multi-stepreasoning tasks. In addition, we release the code to foster research along thisline.</description><author>Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</author><pubDate>Fri, 26 Sep 2025 16:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.12663v2</guid></item><item><title>HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection</title><link>http://arxiv.org/abs/2509.22544v1</link><description>Video anomaly detection (VAD) is crucial for intelligent surveillance, but asignificant challenge lies in identifying complex anomalies, which are eventsdefined by intricate relationships and temporal dependencies among multipleentities rather than by isolated actions. While self-supervised learning (SSL)methods effectively model low-level spatiotemporal patterns, they oftenstruggle to grasp the semantic meaning of these interactions. Conversely, largelanguage models (LLMs) offer powerful contextual reasoning but arecomputationally expensive for frame-by-frame analysis and lack fine-grainedspatial localization. We introduce HyCoVAD, Hybrid Complex Video AnomalyDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporalanalyzer with LLM validator. The SSL module is built upon an nnFormer backbonewhich is a transformer-based model for image segmentation. It is trained withmultiple proxy tasks, learns from video frames to identify those suspected ofanomaly. The selected frames are then forwarded to the LLM, which enriches theanalysis with semantic context by applying structured, rule-based reasoning tovalidate the presence of anomalies. Experiments on the challenging ComplexVADdataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperformingexisting baselines by 12.5% while reducing LLM computation. We release ourinteraction anomaly taxonomy, adaptive thresholding protocol, and code tofacilitate future research in complex VAD scenarios.</description><author>Mohammad Mahdi Hemmatyar, Mahdi Jafari, Mohammad Amin Yousefi, Mohammad Reza Nemati, Mobin Azadani, Hamid Reza Rastad, Amirmohammad Akbari</author><pubDate>Fri, 26 Sep 2025 16:20:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22544v1</guid></item><item><title>Position: Simulating Society Requires Simulating Thought</title><link>http://arxiv.org/abs/2506.06958v2</link><description>Simulating society with large language models (LLMs), we argue, requires morethan generating plausible behavior; it demands cognitively grounded reasoningthat is structured, revisable, and traceable. LLM-based agents are increasinglyused to emulate individual and group behavior, primarily through prompting andsupervised fine-tuning. Yet they often lack internal coherence, causalreasoning, and belief traceability, making them unreliable for simulating howpeople reason, deliberate, and respond to interventions. To address this, we present a conceptual modeling paradigm, Generative Minds(GenMinds), which draws from cognitive science to support structured beliefrepresentations in generative agents. To evaluate such agents, we introduce theRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assessreasoning fidelity via causal traceability, demographic grounding, andintervention consistency. These contributions advance a broader shift: fromsurface-level mimicry to generative agents that simulate thought -- not justlanguage -- for social simulations.</description><author>Chance Jiajie Li, Jiayi Wu, Zhenze Mo, Ao Qu, Yuhan Tang, Kaiya Ivy Zhao, Yulu Gan, Jie Fan, Jiangbo Yu, Jinhua Zhao, Paul Liang, Luis Alonso, Kent Larson</author><pubDate>Fri, 26 Sep 2025 16:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.06958v2</guid></item><item><title>Category Discovery: An Open-World Perspective</title><link>http://arxiv.org/abs/2509.22542v1</link><description>Category discovery (CD) is an emerging open-world learning task, which aimsat automatically categorizing unlabelled data containing instances from unseenclasses, given some labelled data from seen classes. This task has attractedsignificant attention over the years and leads to a rich body of literaturetrying to address the problem from different perspectives. In this survey, weprovide a comprehensive review of the literature, and offer detailed analysisand in-depth discussion on different methods. Firstly, we introduce a taxonomyfor the literature by considering two base settings, namely novel categorydiscovery (NCD) and generalized category discovery (GCD), and several derivedsettings that are designed to address the extra challenges in differentreal-world application scenarios, including continual category discovery,skewed data distribution, federated category discovery, etc. Secondly, for eachsetting, we offer a detailed analysis of the methods encompassing threefundamental components, representation learning, label assignment, andestimation of class number. Thirdly, we benchmark all the methods and distillkey insights showing that large-scale pretrained backbones, hierarchical andauxiliary cues, and curriculum-style training are all beneficial for categorydiscovery, while challenges remain in the design of label assignment, theestimation of class numbers, and scaling to complex multi-objectscenarios.Finally, we discuss the key insights from the literature so far andpoint out promising future research directions. We compile a living survey ofthe category discovery literature at\href{https://github.com/Visual-AI/Category-Discovery}{https://github.com/Visual-AI/Category-Discovery}.</description><author>Zhenqi He, Yuanpei Liu, Kai Han</author><pubDate>Fri, 26 Sep 2025 16:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22542v1</guid></item><item><title>Mobi-$$: Mobilizing Your Robot Learning Policy</title><link>http://arxiv.org/abs/2505.23692v2</link><description>Learned visuomotor policies are capable of performing increasingly complexmanipulation tasks. However, most of these policies are trained on datacollected from limited robot positions and camera viewpoints. This leads topoor generalization to novel robot positions, which limits the use of thesepolicies on mobile platforms, especially for precise tasks like pressingbuttons or turning faucets. In this work, we formulate the policy mobilizationproblem: find a mobile robot base pose in a novel environment that is indistribution with respect to a manipulation policy trained on a limited set ofcamera viewpoints. Compared to retraining the policy itself to be more robustto unseen robot base pose initializations, policy mobilization decouplesnavigation from manipulation and thus does not require additionaldemonstrations. Crucially, this problem formulation complements existingefforts to improve manipulation policy robustness to novel viewpoints andremains compatible with them. We propose a novel approach for policymobilization that bridges navigation and manipulation by optimizing the robot'sbase pose to align with an in-distribution base pose for a learned policy. Ourapproach utilizes 3D Gaussian Splatting for novel view synthesis, a scorefunction to evaluate pose suitability, and sampling-based optimization toidentify optimal robot poses. To understand policy mobilization in more depth,we also introduce the Mobi-$\pi$ framework, which includes: (1) metrics thatquantify the difficulty of mobilizing a given policy, (2) a suite of simulatedmobile manipulation tasks based on RoboCasa to evaluate policy mobilization,and (3) visualization tools for analysis. In both our developed simulation tasksuite and the real world, we show that our approach outperforms baselines,demonstrating its effectiveness for policy mobilization.</description><author>Jingyun Yang, Isabella Huang, Brandon Vu, Max Bajracharya, Rika Antonova, Jeannette Bohg</author><pubDate>Fri, 26 Sep 2025 16:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.23692v2</guid></item><item><title>The Emergence of Altruism in Large-Language-Model Agents Society</title><link>http://arxiv.org/abs/2509.22537v1</link><description>Leveraging Large Language Models (LLMs) for social simulation is a frontierin computational social science. Understanding the social logics these agentsembody is critical to this attempt. However, existing research has primarilyfocused on cooperation in small-scale, task-oriented games, overlooking howaltruism, which means sacrificing self-interest for collective benefit, emergesin large-scale agent societies. To address this gap, we introduce aSchelling-variant urban migration model that creates a social dilemma,compelling over 200 LLM agents to navigate an explicit conflict betweenegoistic (personal utility) and altruistic (system utility) goals. Our centralfinding is a fundamental difference in the social tendencies of LLMs. Weidentify two distinct archetypes: "Adaptive Egoists", which default toprioritizing self-interest but whose altruistic behaviors significantlyincrease under the influence of a social norm-setting message board; and"Altruistic Optimizers", which exhibit an inherent altruistic logic,consistently prioritizing collective benefit even at a direct cost tothemselves. Furthermore, to qualitatively analyze the cognitive underpinningsof these decisions, we introduce a method inspired by Grounded Theory tosystematically code agent reasoning. In summary, this research provides thefirst evidence of intrinsic heterogeneity in the egoistic and altruistictendencies of different LLMs. We propose that for social simulation, modelselection is not merely a matter of choosing reasoning capability, but ofchoosing an intrinsic social action logic. While "Adaptive Egoists" may offer amore suitable choice for simulating complex human societies, "AltruisticOptimizers" are better suited for modeling idealized pro-social actors orscenarios where collective welfare is the primary consideration.</description><author>Haoyang Li, Xiao Jia, Zhanzhan Zhao</author><pubDate>Fri, 26 Sep 2025 16:17:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22537v1</guid></item><item><title>InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models</title><link>http://arxiv.org/abs/2509.22536v1</link><description>The immense computational cost of training Large Language Models (LLMs)presents a major barrier to innovation. While FP8 training offers a promisingsolution with significant theoretical efficiency gains, its widespread adoptionhas been hindered by the lack of a comprehensive, open-source training recipe.To bridge this gap, we introduce an end-to-end FP8 training recipe thatseamlessly integrates continual pre-training and supervised fine-tuning. Ourmethodology employs a fine-grained, hybrid-granularity quantization strategy tomaintain numerical fidelity while maximizing computational efficiency. Throughextensive experiments, including the continue pre-training of models on a160B-token corpus, we demonstrate that our recipe is not only remarkably stablebut also essentially lossless, achieving performance on par with the BF16baseline across a suite of reasoning benchmarks. Crucially, this is achievedwith substantial efficiency improvements, including up to a 22% reduction intraining time, a 14% decrease in peak memory usage, and a 19% increase inthroughput. Our results establish FP8 as a practical and robust alternative toBF16, and we will release the accompanying code to further democratizelarge-scale model training.</description><author>Wenjun Wang, Shuo Cai, Congkai Xie, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Yuan Xie, Hongxia Yang</author><pubDate>Fri, 26 Sep 2025 16:16:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22536v1</guid></item><item><title>Debiased Front-Door Learners for Heterogeneous Effects</title><link>http://arxiv.org/abs/2509.22531v1</link><description>In observational settings where treatment and outcome share unmeasuredconfounders but an observed mediator remains unconfounded, the front-door (FD)adjustment identifies causal effects through the mediator. We study theheterogeneous treatment effect (HTE) under FD identification and introduce twodebiased learners: FD-DR-Learner and FD-R-Learner. Both attain fast,quasi-oracle rates (i.e., performance comparable to an oracle that knows thenuisances) even when nuisance functions converge as slowly as n^-1/4. Weprovide error analyses establishing debiasedness and demonstrate robustempirical performance in synthetic studies and a real-world case study ofprimary seat-belt laws using Fatality Analysis Reporting System (FARS) dataset.Together, these results indicate that the proposed learners deliver reliableand sample-efficient HTE estimates in FD scenarios. The implementation isavailable at https://github.com/yonghanjung/FD-CATE. Keywords: Front-door adjustment; Heterogeneous treatment effects; Debiasedlearning; Quasi-oracle rates; Causal inference.</description><author>Yonghan Jung</author><pubDate>Fri, 26 Sep 2025 16:11:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22531v1</guid></item><item><title>Smoothing-Based Conformal Prediction for Balancing Efficiency and Interpretability</title><link>http://arxiv.org/abs/2509.22529v1</link><description>Conformal Prediction (CP) is a distribution-free framework for constructingstatistically rigorous prediction sets. While popular variants such as CD-splitimprove CP's efficiency, they often yield prediction sets composed of multipledisconnected subintervals, which are difficult to interpret. In this paper, wepropose SCD-split, which incorporates smoothing operations into the CPframework. Such smoothing operations potentially help merge the subintervals,thus leading to interpretable prediction sets. Experimental results on bothsynthetic and real-world datasets demonstrate that SCD-split balances theinterval length and the number of disconnected subintervals. Theoretically,under specific conditions, SCD-split provably reduces the number ofdisconnected subintervals while maintaining comparable coverage guarantees andinterval length compared with CD-split.</description><author>Mingyi Zheng, Hongyu Jiang, Yizhou Lu, Jiaye Teng</author><pubDate>Fri, 26 Sep 2025 16:08:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22529v1</guid></item><item><title>Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning</title><link>http://arxiv.org/abs/2504.00907v3</link><description>Embodied agents operating in household environments must interpret ambiguousand under-specified human instructions. A capable household robot shouldrecognize ambiguity and ask relevant clarification questions to infer the userintent accurately, leading to more effective task execution. To study thisproblem, we introduce the Ask-to-Act task, where an embodied agent is taskedwith a single or multi-object rearrangement task using an under-specifiedinstruction in a home environment. The agent must strategically ask minimal,yet relevant, clarification questions to resolve ambiguity while navigatingunder partial observability. To address this challenge, we propose a novelapproach that fine-tunes multi-modal large language models (MLLMs) asvision-language-action (VLA) policies using online reinforcement learning (RL)with LLM-generated rewards. Our method eliminates the need for large-scalehuman demonstrations or manually engineered rewards for training such agents.We benchmark against strong zero-shot baselines including GPT-4o as well assupervised fine-tuned MLLMs on our task. Our results show that our RL-finetunedMLLM outperforms all baselines by a significant margin (10.4-16.5%),generalizing well to novel scenes and tasks. To the best of our knowledge, thisis the first demonstration of adapting MLLMs as VLA agents that can act and askfor help using LLM-generated rewards with online RL.</description><author>Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira, Roozbeh Mottaghi</author><pubDate>Fri, 26 Sep 2025 16:06:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.00907v3</guid></item><item><title>EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model</title><link>http://arxiv.org/abs/2509.22527v1</link><description>Monocular depth estimation (MDE) plays a pivotal role in various computervision applications, such as robotics, augmented reality, and autonomousdriving. Despite recent advancements, existing methods often fail to meet keyrequirements for 3D reconstruction and view synthesis, including geometricconsistency, fine details, robustness to real-world challenges like reflectivesurfaces, and efficiency for edge devices. To address these challenges, weintroduce a novel MDE system, called EfficientDepth, which combines atransformer architecture with a lightweight convolutional decoder, as well as abimodal density head that allows the network to estimate detailed depth maps.We train our model on a combination of labeled synthetic and real images, aswell as pseudo-labeled real images, generated using a high-performing MDEmethod. Furthermore, we employ a multi-stage optimization strategy to improvetraining efficiency and produce models that emphasize geometric consistency andfine detail. Finally, in addition to commonly used objectives, we introduce aloss function based on LPIPS to encourage the network to produce detailed depthmaps. Experimental results demonstrate that EfficientDepth achieves performancecomparable to or better than existing state-of-the-art models, withsignificantly reduced computational resources.</description><author>Andrii Litvynchuk, Ivan Livinsky, Anand Ravi, Nima Kalantari, Andrii Tsarov</author><pubDate>Fri, 26 Sep 2025 16:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.22527v1</guid></item></channel></rss>