<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 25 Oct 2025 01:25:23 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</title><link>http://arxiv.org/abs/2510.20822v1</link><description>State-of-the-art text-to-video models excel at generating isolated clips butfall short of creating the coherent, multi-shot narratives, which are theessence of storytelling. We bridge this "narrative gap" with HoloCine, a modelthat generates entire scenes holistically to ensure global consistency from thefirst shot to the last. Our architecture achieves precise directorial controlthrough a Window Cross-Attention mechanism that localizes text prompts tospecific shots, while a Sparse Inter-Shot Self-Attention pattern (dense withinshots but sparse between them) ensures the efficiency required for minute-scalegeneration. Beyond setting a new state-of-the-art in narrative coherence,HoloCine develops remarkable emergent abilities: a persistent memory forcharacters and scenes, and an intuitive grasp of cinematic techniques. Our workmarks a pivotal shift from clip synthesis towards automated filmmaking, makingend-to-end cinematic creation a tangible future. Our code is available at:https://holo-cine.github.io/.</description><author>Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, Huamin Qu</author><pubDate>Thu, 23 Oct 2025 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20822v1</guid></item><item><title>One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling</title><link>http://arxiv.org/abs/2505.13358v3</link><description>Diffusion-based generative models have demonstrated exceptional performance,yet their iterative sampling procedures remain computationally expensive. Aprominent strategy to mitigate this cost is distillation, with offlinedistillation offering particular advantages in terms of efficiency, modularity,and flexibility. In this work, we identify two key observations that motivate aprincipled distillation framework: (1) while diffusion models have been viewedthrough the lens of dynamical systems theory, powerful and underexplored toolscan be further leveraged; and (2) diffusion models inherently imposestructured, semantically coherent trajectories in latent space. Building onthese observations, we introduce the Koopman Distillation Model (KDM), a noveloffline distillation approach grounded in Koopman theory - a classicalframework for representing nonlinear dynamics linearly in a transformed space.KDM encodes noisy inputs into an embedded space where a learned linear operatorpropagates them forward, followed by a decoder that reconstructs clean samples.This enables single-step generation while preserving semantic fidelity. Weprovide theoretical justification for our approach: (1) under mild assumptions,the learned diffusion dynamics admit a finite-dimensional Koopmanrepresentation; and (2) proximity in the Koopman latent space correlates withsemantic similarity in the generated outputs, allowing for effective trajectoryalignment. KDM achieves highly competitive performance across standard offlinedistillation benchmarks.</description><author>Nimrod Berman, Ilan Naiman, Moshe Eliasof, Hedi Zisling, Omri Azencot</author><pubDate>Thu, 23 Oct 2025 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.13358v3</guid></item><item><title>Language Models use Lookbacks to Track Beliefs</title><link>http://arxiv.org/abs/2505.14685v2</link><description>How do language models (LMs) represent characters' beliefs, especially whenthose beliefs may differ from reality? This question lies at the heart ofunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze LMs'ability to reason about characters' beliefs using causal mediation andabstraction. We construct a dataset, CausalToM, consisting of simple storieswhere two characters independently change the state of two objects, potentiallyunaware of each other's actions. Our investigation uncovers a pervasivealgorithmic pattern that we call a lookback mechanism, which enables the LM torecall important information when it becomes necessary. The LM binds eachcharacter-object-state triple together by co-locating their referenceinformation, represented as Ordering IDs (OIs), in low-rank subspaces of thestate token's residual stream. When asked about a character's beliefs regardingthe state of an object, the binding lookback retrieves the correct state OI andthen the answer lookback retrieves the corresponding state token. When weintroduce text specifying that one character is (not) visible to the other, wefind that the LM first generates a visibility ID encoding the relation betweenthe observing and the observed character OIs. In a visibility lookback, this IDis used to retrieve information about the observed character and update theobserving character's beliefs. Our work provides insights into belief trackingmechanisms, taking a step toward reverse-engineering ToM reasoning in LMs.</description><author>Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David Bau, Atticus Geiger</author><pubDate>Thu, 23 Oct 2025 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14685v2</guid></item><item><title>LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas</title><link>http://arxiv.org/abs/2510.20820v1</link><description>Despite their impressive visual fidelity, existing personalized generativemodels lack interactive control over spatial composition and scale poorly tomultiple subjects. To address these limitations, we present LayerComposer, aninteractive framework for personalized, multi-subject text-to-image generation.Our approach introduces two main contributions: (1) a layered canvas, a novelrepresentation in which each subject is placed on a distinct layer, enablingocclusion-free composition; and (2) a locking mechanism that preserves selectedlayers with high fidelity while allowing the remaining layers to adapt flexiblyto the surrounding context. Similar to professional image-editing software, theproposed layered canvas allows users to place, resize, or lock input subjectsthrough intuitive layer manipulation. Our versatile locking mechanism requiresno architectural changes, relying instead on inherent positional embeddingscombined with a new complementary data sampling strategy. Extensive experimentsdemonstrate that LayerComposer achieves superior spatial control and identitypreservation compared to the state-of-the-art methods in multi-subjectpersonalized image generation.</description><author>Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tulyakov, Kuan-Chieh Jackson Wang</author><pubDate>Thu, 23 Oct 2025 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20820v1</guid></item><item><title>Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</title><link>http://arxiv.org/abs/2510.20819v1</link><description>Recent advances in generative modeling have positioned diffusion models asstate-of-the-art tools for sampling from complex data distributions. Whilethese models have shown remarkable success across single-modality domains suchas images and audio, extending their capabilities to Modality Translation (MT),translating information across different sensory modalities, remains an openchallenge. Existing approaches often rely on restrictive assumptions, includingshared dimensionality, Gaussian source priors, and modality-specificarchitectures, which limit their generality and theoretical grounding. In thiswork, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), ageneral-purpose framework for modality translation based on a latent-variableextension of Denoising Diffusion Bridge Models. By operating in a shared latentspace, our method learns a bridge between arbitrary modalities withoutrequiring aligned dimensions. We introduce a contrastive alignment loss toenforce semantic consistency between paired samples and design adomain-agnostic encoder-decoder architecture tailored for noise prediction inlatent space. Additionally, we propose a predictive loss to guide trainingtoward accurate cross-domain translation and explore several trainingstrategies to improve stability. Our approach supports arbitrary modality pairsand performs strongly on diverse MT tasks, including multi-view to 3D shapegeneration, image super-resolution, and multi-view scene synthesis.Comprehensive experiments and ablations validate the effectiveness of ourframework, establishing a new strong baseline in general modality translation.For more information, see our project page:https://sites.google.com/view/lddbm/home.</description><author>Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</author><pubDate>Thu, 23 Oct 2025 17:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20819v1</guid></item><item><title>VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation</title><link>http://arxiv.org/abs/2510.20818v1</link><description>A fundamental challenge in robot navigation lies in learning policies thatgeneralize across diverse environments while conforming to the unique physicalconstraints and capabilities of a specific embodiment (e.g., quadrupeds canwalk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA thatdecouples semantic planning from embodiment grounding: a generalist plannerlearns from diverse, open-world data, while a specialist affordance modellearns the robot's physical constraints and capabilities in safe, low-costsimulation. We enabled this separation by carefully designing an interface thatlets a high-level planner propose candidate paths directly in image space thatthe affordance model then evaluates and re-ranks. Our real-world experimentsshow that VAMOS achieves higher success rates in both indoor and complexoutdoor navigation than state-of-the-art model-based and end-to-end learningmethods. We also show that our hierarchical design enables cross-embodiednavigation across legged and wheeled robots and is easily steerable usingnatural language. Real-world ablations confirm that the specialist model is keyto embodiment grounding, enabling a single high-level planner to be deployedacross physically distinct wheeled and legged robots. Finally, this modelsignificantly enhances single-robot reliability, achieving 3X higher successrates by rejecting physically infeasible plans. Website:https://vamos-vla.github.io/</description><author>Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta</author><pubDate>Thu, 23 Oct 2025 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20818v1</guid></item><item><title>KL-Regularized Reinforcement Learning is Designed to Mode Collapse</title><link>http://arxiv.org/abs/2510.20817v1</link><description>It is commonly believed that optimizing the reverse KL divergence results in"mode seeking", while optimizing forward KL results in "mass covering", withthe latter being preferred if the goal is to sample from multiple diversemodes. We show -- mathematically and empirically -- that this intuition doesnot necessarily transfer well to doing reinforcement learning withreverse/forward KL regularization (e.g. as commonly used with language models).Instead, the choice of reverse/forward KL determines the family of optimaltarget distributions, parameterized by the regularization coefficient. Modecoverage depends primarily on other factors, such as regularization strength,and relative scales between rewards and reference probabilities. Further, weshow commonly used settings such as low regularization strength and equalverifiable rewards tend to specify unimodal target distributions, meaning theoptimization objective is, by construction, non-diverse. We leverage theseinsights to construct a simple, scalable, and theoretically justifiedalgorithm. It makes minimal changes to reward magnitudes, yet optimizes for atarget distribution which puts high probability over all high-quality samplingmodes. In experiments, this simple modification works to post-train both LargeLanguage Models and Chemical Language Models to have higher solution qualityand diversity, without any external signals of diversity, and works with bothforward and reverse KL when using either naively fails.</description><author>Anthony GX-Chen, Jatin Prakash, Jeff Guo, Rob Fergus, Rajesh Ranganath</author><pubDate>Thu, 23 Oct 2025 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20817v1</guid></item><item><title>SpectraMorph: Structured Latent Learning for Self-Supervised Hyperspectral Super-Resolution</title><link>http://arxiv.org/abs/2510.20814v1</link><description>Hyperspectral sensors capture dense spectra per pixel but suffer from lowspatial resolution, causing blurred boundaries and mixed-pixel effects.Co-registered companion sensors such as multispectral, RGB, or panchromaticcameras provide high-resolution spatial detail, motivating hyperspectralsuper-resolution through the fusion of hyperspectral and multispectral images(HSI-MSI). Existing deep learning based methods achieve strong performance butrely on opaque regressors that lack interpretability and often fail when theMSI has very few bands. We propose SpectraMorph, a physics-guidedself-supervised fusion framework with a structured latent space. Instead ofdirect regression, SpectraMorph enforces an unmixing bottleneck: endmembersignatures are extracted from the low-resolution HSI, and a compact multilayerperceptron predicts abundance-like maps from the MSI. Spectra are reconstructedby linear mixing, with training performed in a self-supervised manner via theMSI sensor's spectral response function. SpectraMorph produces interpretableintermediates, trains in under a minute, and remains robust even with asingle-band (pan-chromatic) MSI. Experiments on synthetic and real-worlddatasets show SpectraMorph consistently outperforming state-of-the-artunsupervised/self-supervised baselines while remaining very competitive againstsupervised baselines.</description><author>Ritik Shah, Marco F Duarte</author><pubDate>Thu, 23 Oct 2025 17:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20814v1</guid></item><item><title>GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</title><link>http://arxiv.org/abs/2510.20813v1</link><description>This paper presents GSWorld, a robust, photo-realistic simulator for roboticsmanipulation that combines 3D Gaussian Splatting with physics engines. Ourframework advocates "closing the loop" of developing manipulation policies withreproducible evaluation of policies learned from real-robot data and sim2realpolicy training without using real robots. To enable photo-realistic renderingof diverse scenes, we propose a new asset format, which we term GSDF (GaussianScene Description File), that infuses Gaussian-on-Mesh representation withrobot URDF and other objects. With a streamlined reconstruction pipeline, wecurate a database of GSDF that contains 3 robot embodiments for single-arm andbimanual manipulation, as well as more than 40 objects. Combining GSDF withphysics engines, we demonstrate several immediate interesting applications: (1)learning zero-shot sim2real pixel-to-action manipulation policy withphoto-realistic rendering, (2) automated high-quality DAgger data collectionfor adapting policies to deployment environments, (3) reproducible benchmarkingof real-robot manipulation policies in simulation, (4) simulation datacollection by virtual teleoperation, and (5) zero-shot sim2real visualreinforcement learning. Website: https://3dgsworld.github.io/.</description><author>Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang</author><pubDate>Thu, 23 Oct 2025 17:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20813v1</guid></item><item><title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title><link>http://arxiv.org/abs/2510.20812v1</link><description>Large Vision-Language Models (VLMs) have achieved remarkable progress inmultimodal understanding, yet they struggle when reasoning overinformation-intensive images that densely interleave textual annotations withfine-grained graphical elements. The main challenges lie in preciselylocalizing critical cues in dense layouts and multi-hop reasoning to integratedispersed evidence. We propose Speculative Verdict (SV), a training-freeframework inspired by speculative decoding that combines multiple lightweightdraft experts with a large verdict model. In the draft stage, small VLMs act asdraft experts to generate reasoning paths that provide diverse localizationcandidates; in the verdict stage, a strong VLM synthesizes these paths toproduce the final answer, minimizing computational cost while recoveringcorrect answers. To further improve efficiency and accuracy, SV introduces aconsensus expert selection mechanism that forwards only high-agreementreasoning paths to the verdict. Empirically, SV achieves consistent gains onchallenging information-intensive and high-resolution visual question answeringbenchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.By synthesizing correct insights from multiple partially accurate reasoningpaths, SV achieves both error correction and cost-efficiency compared to largeproprietary models or training pipelines. Code is available athttps://github.com/Tinaliu0123/speculative-verdict</description><author>Yuhan Liu, Lianhui Qin, Shengjie Wang</author><pubDate>Thu, 23 Oct 2025 17:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20812v1</guid></item><item><title>On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?</title><link>http://arxiv.org/abs/2510.20810v1</link><description>With the widespread use of large language models (LLMs), many researchershave turned their attention to detecting text generated by them. However, thereis no consistent or precise definition of their target, namely "LLM-generatedtext". Differences in usage scenarios and the diversity of LLMs furtherincrease the difficulty of detection. What is commonly regarded as thedetecting target usually represents only a subset of the text that LLMs canpotentially produce. Human edits to LLM outputs, together with the subtleinfluences that LLMs exert on their users, are blurring the line betweenLLM-generated and human-written text. Existing benchmarks and evaluationapproaches do not adequately address the various conditions in real-worlddetector applications. Hence, the numerical results of detectors are oftenmisunderstood, and their significance is diminishing. Therefore, detectorsremain useful under specific conditions, but their results should beinterpreted only as references rather than decisive indicators.</description><author>Mingmeng Geng, Thierry Poibeau</author><pubDate>Thu, 23 Oct 2025 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20810v1</guid></item><item><title>Real Deep Research for AI, Robotics and Beyond</title><link>http://arxiv.org/abs/2510.20809v1</link><description>With the rapid growth of research in AI and robotics now producing over10,000 papers annually it has become increasingly difficult for researchers tostay up to date. Fast evolving trends, the rise of interdisciplinary work, andthe need to explore domains beyond one's expertise all contribute to thischallenge. To address these issues, we propose a generalizable pipeline capableof systematically analyzing any research area: identifying emerging trends,uncovering cross domain opportunities, and offering concrete starting pointsfor new inquiry. In this work, we present Real Deep Research (RDR) acomprehensive framework applied to the domains of AI and robotics, with aparticular focus on foundation models and robotics advancements. We alsobriefly extend our analysis to other areas of science. The main paper detailsthe construction of the RDR pipeline, while the appendix provides extensiveresults across each analyzed topic. We hope this work sheds light forresearchers working in the field of AI and beyond.</description><author>Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang</author><pubDate>Thu, 23 Oct 2025 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20809v1</guid></item><item><title>The Reality Gap in Robotics: Challenges, Solutions, and Best Practices</title><link>http://arxiv.org/abs/2510.20808v1</link><description>Machine learning has facilitated significant advancements across variousrobotics domains, including navigation, locomotion, and manipulation. Many suchachievements have been driven by the extensive use of simulation as a criticaltool for training and testing robotic systems prior to their deployment inreal-world environments. However, simulations consist of abstractions andapproximations that inevitably introduce discrepancies between simulated andreal environments, known as the reality gap. These discrepancies significantlyhinder the successful transfer of systems from simulation to the real world.Closing this gap remains one of the most pressing challenges in robotics.Recent advances in sim-to-real transfer have demonstrated promising resultsacross various platforms, including locomotion, navigation, and manipulation.By leveraging techniques such as domain randomization, real-to-sim transfer,state and action abstractions, and sim-real co-training, many works haveovercome the reality gap. However, challenges persist, and a deeperunderstanding of the reality gap's root causes and solutions is necessary. Inthis survey, we present a comprehensive overview of the sim-to-real landscape,highlighting the causes, solutions, and evaluation metrics for the reality gapand sim-to-real transfer.</description><author>Elie Aljalbout, Jiaxu Xing, Angel Romero, Iretiayo Akinola, Caelan Reed Garrett, Eric Heiden, Abhishek Gupta, Tucker Hermans, Yashraj Narang, Dieter Fox, Davide Scaramuzza, Fabio Ramos</author><pubDate>Thu, 23 Oct 2025 17:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20808v1</guid></item><item><title>Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers</title><link>http://arxiv.org/abs/2510.20807v1</link><description>Inspired by the performance and scalability of autoregressive large languagemodels (LLMs), transformer-based models have seen recent success in the visualdomain. This study investigates a transformer adaptation for video predictionwith a simple end-to-end approach, comparing various spatiotemporalself-attention layouts. Focusing on causal modeling of physical simulationsover time; a common shortcoming of existing video-generative approaches, weattempt to isolate spatiotemporal reasoning via physical object trackingmetrics and unsupervised training on physical simulation datasets. We introducea simple yet effective pure transformer model for autoregressive videoprediction, utilizing continuous pixel-space representations for videoprediction. Without the need for complex training strategies or latentfeature-learning components, our approach significantly extends the timehorizon for physically accurate predictions by up to 50% when compared withexisting latent-space approaches, while maintaining comparable performance oncommon video quality metrics. In addition, we conduct interpretabilityexperiments to identify network regions that encode information useful toperform accurate estimations of PDE simulation parameters via probing models,and find that this generalizes to the estimation of out-of-distributionsimulation parameters. This work serves as a platform for furtherattention-based spatiotemporal modeling of videos via a simple, parameterefficient, and interpretable approach.</description><author>Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed</author><pubDate>Thu, 23 Oct 2025 17:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20807v1</guid></item><item><title>ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</title><link>http://arxiv.org/abs/2510.20803v1</link><description>We propose a novel AutoRegressive Generation-based paradigm for imageSegmentation (ARGenSeg), achieving multimodal understanding and pixel-levelperception within a unified framework. Prior works integrating imagesegmentation into multimodal large language models (MLLMs) typically employeither boundary points representation or dedicated segmentation heads. Thesemethods rely on discrete representations or semantic prompts fed intotask-specific decoders, which limits the ability of the MLLM to capturefine-grained visual details. To address these challenges, we introduce asegmentation framework for MLLM based on image generation, which naturallyproduces dense masks for target objects. We leverage MLLM to output visualtokens and detokenize them into images using an universal VQ-VAE, making thesegmentation fully dependent on the pixel-level understanding of the MLLM. Toreduce inference latency, we employ a next-scale-prediction strategy togenerate required visual tokens in parallel. Extensive experiments demonstratethat our method surpasses prior state-of-the-art approaches on multiplesegmentation datasets with a remarkable boost in inference speed, whilemaintaining strong understanding capabilities.</description><author>Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</author><pubDate>Thu, 23 Oct 2025 17:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20803v1</guid></item><item><title>DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</title><link>http://arxiv.org/abs/2510.02253v2</link><description>Drag-based image editing has long suffered from distortions in the targetregion, largely because the priors of earlier base models, Stable Diffusion,are insufficient to project optimized latents back onto the natural imagemanifold. With the shift from UNet-based DDPMs to more scalable DiT with flowmatching (e.g., SD3.5, FLUX), generative priors have become significantlystronger, enabling advances across diverse editing tasks. However, drag-basedediting has yet to benefit from these stronger priors. This work proposes thefirst framework to effectively harness FLUX's rich prior for drag-basedediting, dubbed DragFlow, achieving substantial gains over baselines. We firstshow that directly applying point-based drag editing to DiTs performs poorly:unlike the highly compressed features of UNets, DiT features are insufficientlystructured to provide reliable guidance for point-wise motion supervision. Toovercome this limitation, DragFlow introduces a region-based editing paradigm,where affine transformations enable richer and more consistent featuresupervision. Additionally, we integrate pretrained open-domain personalizationadapters (e.g., IP-Adapter) to enhance subject consistency, while preservingbackground fidelity through gradient mask-based hard constraints. Multimodallarge language models (MLLMs) are further employed to resolve task ambiguities.For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)featuring region-level dragging instructions. Extensive experiments onDragBench-DR and ReD Bench show that DragFlow surpasses both point-based andregion-based baselines, setting a new state-of-the-art in drag-based imageediting. Code and datasets will be publicly available upon publication.</description><author>Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</author><pubDate>Thu, 23 Oct 2025 17:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.02253v2</guid></item><item><title>Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples</title><link>http://arxiv.org/abs/2510.20800v1</link><description>Recently, Sharma et al. suggested a method called Layer-SElective-Rankreduction (LASER) which demonstrated that pruning high-order components ofcarefully chosen LLM's weight matrices can boost downstream accuracy -- withoutany gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (eachrequiring full-dataset forward passes) makes it impractical for rapiddeployment. We demonstrate that this overhead can be removed and find that: (i)Only a small, carefully chosen subset of matrices needs to be inspected --eliminating the layer-by-layer sweep, (ii) The gradient of each matrix'ssingular values pinpoints which matrices merit reduction, (iii) Increasing thefactorization search space by allowing matrices rows to cluster around multiplesubspaces and then decomposing each cluster separately further reducesoverfitting on the original training data and further lifts accuracy by up to24.6 percentage points, and finally, (iv) we discover that evaluating on just100 samples rather than the full training data -- both for computing theindicative gradients and for measuring the final accuracy -- suffices tofurther reduce the search time; we explain that as adaptation to downstreamtasks is dominated by prompting style, not dataset size. As a result, we showthat combining these findings yields a fast and robust adaptation algorithm fordownstream tasks. Overall, with a single gradient step on 100 examples and aquick scan of the top candidate layers and factorization techniques, we canadapt LLMs to new datasets -- entirely without fine-tuning.</description><author>Shiva Sreeram, Alaa Maalouf, Pratyusha Sharma, Daniela Rus</author><pubDate>Thu, 23 Oct 2025 17:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20800v1</guid></item><item><title>Simple Context Compression: Mean-Pooling and Multi-Ratio Training</title><link>http://arxiv.org/abs/2510.20797v1</link><description>A common strategy to reduce the computational costs of using long contexts inretrieval-augmented generation (RAG) with large language models (LLMs) is softcontext compression, where the input sequence is transformed into a shortercontinuous representation. We develop a lightweight and simple mean-poolingapproach that consistently outperforms the widely used compression-tokensarchitecture, and study training the same compressor to output multiplecompression ratios. We conduct extensive experiments across in-domain andout-of-domain QA datasets, as well as across model families, scales, andcompression ratios. Overall, our simple mean-pooling approach achieves thestrongest performance, with a relatively small drop when training for multiplecompression ratios. More broadly though, across architectures and trainingregimes the trade-offs are more nuanced, illustrating the complex landscape ofcompression methods.</description><author>Yair Feldman, Yoav Artzi</author><pubDate>Thu, 23 Oct 2025 17:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20797v1</guid></item><item><title>Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks</title><link>http://arxiv.org/abs/2510.20795v1</link><description>Deep learning has emerged as a transformative methodology in moderncosmology, providing powerful tools to extract meaningful physical informationfrom complex astronomical datasets. This paper implements a novel Bayesiangraph deep learning framework for estimating key cosmological parameters in aprimordial magnetic field (PMF) cosmology directly from simulated CosmicMicrowave Background (CMB) maps. Our methodology utilizes DeepSphere, aspherical convolutional neural network architecture specifically designed torespect the spherical geometry of CMB data through HEALPix pixelization. Toadvance beyond deterministic point estimates and enable robust uncertaintyquantification, we integrate Bayesian Neural Networks (BNNs) into theframework, capturing aleatoric and epistemic uncertainties that reflect themodel confidence in its predictions. The proposed approach demonstratesexceptional performance, achieving $R^{2}$ scores exceeding 0.89 for themagnetic parameter estimation. We further obtain well-calibrated uncertaintyestimates through post-hoc training techniques including Variance Scaling andGPNormal. This integrated DeepSphere-BNNs framework not only delivers accurateparameter estimation from CMB maps with PMF contributions but also providesreliable uncertainty quantification, providing the necessary tools for robustcosmological inference in the era of precision cosmology.</description><author>Juan Alejandro Pinto Castro, Héctor J. Hortúa, Jorge Enrique García-Farieta, Roger Anderson Hurtado</author><pubDate>Thu, 23 Oct 2025 17:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20795v1</guid></item><item><title>Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</title><link>http://arxiv.org/abs/2510.20794v1</link><description>This paper presents a Multi-Object Tracking (MOT) framework that fuses radarand camera data to enhance tracking efficiency while minimizing manualinterventions. Contrary to many studies that underutilize radar and assign it asupplementary role--despite its capability to provide accurate range/depthinformation of targets in a world 3D coordinate system--our approach positionsradar in a crucial role. Meanwhile, this paper utilizes common features toenable online calibration to autonomously associate detections from radar andcamera. The main contributions of this work include: (1) the development of aradar-camera fusion MOT framework that exploits online radar-camera calibrationto simplify the integration of detection results from these two sensors, (2)the utilization of common features between radar and camera data to accuratelyderive real-world positions of detected objects, and (3) the adoption offeature matching and category-consistency checking to surpass the limitationsof mere position matching in enhancing sensor association accuracy. To the bestof our knowledge, we are the first to investigate the integration ofradar-camera common features and their use in online calibration for achievingMOT. The efficacy of our framework is demonstrated by its ability to streamlinethe radar-camera mapping process and improve tracking precision, as evidencedby real-world experiments conducted in both controlled environments and actualtraffic scenarios. Code is available athttps://github.com/radar-lab/Radar_Camera_MOT</description><author>Lei Cheng, Siyang Cao</author><pubDate>Thu, 23 Oct 2025 17:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20794v1</guid></item><item><title>BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</title><link>http://arxiv.org/abs/2510.20792v1</link><description>The rapid progress of graph generation has raised new security concerns,particularly regarding backdoor vulnerabilities. While prior work has exploredbackdoor attacks in image diffusion and unconditional graph generation,conditional, especially text-guided graph generation remains largelyunexamined. This paper proposes BadGraph, a backdoor attack method targetinglatent diffusion models for text-guided graph generation. BadGraph leveragestextual triggers to poison training data, covertly implanting backdoors thatinduce attacker-specified subgraphs during inference when triggers appear,while preserving normal performance on clean inputs. Extensive experiments onfour benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate theeffectiveness and stealth of the attack: less than 10% poisoning rate canachieves 50% attack success rate, while 24% suffices for over 80% success rate,with negligible performance degradation on benign samples. Ablation studiesfurther reveal that the backdoor is implanted during VAE and diffusion trainingrather than pretraining. These findings reveal the security vulnerabilities inlatent diffusion models of text-guided graph generation, highlight the seriousrisks in models' applications such as drug discovery and underscore the needfor robust defenses against the backdoor attack in such diffusion models.</description><author>Liang Ye, Shengqin Chen, Jiazhu Dai</author><pubDate>Thu, 23 Oct 2025 17:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20792v1</guid></item><item><title>Text2Mem: A Unified Memory Operation Language for Memory Operating System</title><link>http://arxiv.org/abs/2509.11145v2</link><description>Large language model agents increasingly depend on memory to sustain longhorizon interaction, but existing frameworks remain limited. Most expose only afew basic primitives such as encode, retrieve, and delete, while higher orderoperations like merge, promote, demote, split, lock, and expire are missing orinconsistently supported. Moreover, there is no formal and executablespecification for memory commands, leaving scope and lifecycle rules implicitand causing unpredictable behavior across systems. We introduce Text2Mem, aunified memory operation language that provides a standardized pathway fromnatural language to reliable execution. Text2Mem defines a compact yetexpressive operation set aligned with encoding, storage, and retrieval. Eachinstruction is represented as a JSON based schema instance with required fieldsand semantic invariants, which a parser transforms into typed operation objectswith normalized parameters. A validator ensures correctness before execution,while adapters map typed objects either to a SQL prototype backend or to realmemory frameworks. Model based services such as embeddings or summarization areintegrated when required. All results are returned through a unified executioncontract. This design ensures safety, determinism, and portability acrossheterogeneous backends. We also outline Text2Mem Bench, a planned benchmarkthat separates schema generation from backend execution to enable systematicevaluation. Together, these components establish the first standardizedfoundation for memory control in agents.</description><author>Yi Wang, Lihai Yang, Boyu Chen, Gongyi Zou, Kerun Xu, Bo Tang, Feiyu Xiong, Siheng Chen, Zhiyu Li</author><pubDate>Thu, 23 Oct 2025 17:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.11145v2</guid></item><item><title>Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction</title><link>http://arxiv.org/abs/2510.20787v1</link><description>Linear-attention models that compress the entire input sequence into afixed-size recurrent state offer an efficient alternative to Transformers, buttheir finite memory induces forgetfulness that harms retrieval-intensive tasks.To mitigate the issue, we explore a series of hybrid models that restore directaccess to past tokens. We interleave token mixers with intermediate time andspace complexity between linear and full attention, including sparse attentionwith token eviction, and the query-aware native sparse attention. Particularly,we propose a novel learnable token eviction approach. Combined withsliding-window attention, an end-to-end trainable lightweight CNN aggregatesinformation from both past and future adjacent tokens to adaptively retain alimited set of critical KV-pairs per head, maintaining linear attention'sconstant time and space complexity. Efficient Triton kernels for the sparseattention mechanisms are provided. Empirical evaluations on retrieval-intensivebenchmarks support the effectiveness of our approaches.</description><author>Mutian He, Philip N. Garner</author><pubDate>Thu, 23 Oct 2025 17:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20787v1</guid></item><item><title>A Coherence-Based Measure of AGI</title><link>http://arxiv.org/abs/2510.20784v1</link><description>Recent work by \citet{hendrycks2025agidefinition} formalized\textit{Artificial General Intelligence} (AGI) as the arithmetic mean ofproficiencies across cognitive domains derived from the Cattell--Horn--Carroll(CHC) model of human cognition. While elegant, this definition assumes\textit{compensability} -- that exceptional ability in some domains can offsetfailure in others. True general intelligence, however, should reflect\textit{coherent sufficiency}: balanced competence across all essentialdomains. We propose a coherence-aware measure of AGI based on the integral ofgeneralized means over a continuum of compensability exponents. Thisformulation spans arithmetic, geometric, and harmonic regimes, and theresulting \textit{area under the curve} (AUC) quantifies robustness undervarying compensability assumptions. Unlike the arithmetic mean, which rewardsspecialization, the AUC penalizes imbalance and captures inter-domaindependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,the coherence-adjusted AUC reveals that both systems remain far from generalcompetence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integratingthe generalized mean thus yields a principled, interpretable, and stricterfoundation for measuring genuine progress toward AGI.</description><author>Fares Fourati</author><pubDate>Thu, 23 Oct 2025 17:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20784v1</guid></item><item><title>Out-of-distribution Tests Reveal Compositionality in Chess Transformers</title><link>http://arxiv.org/abs/2510.20783v1</link><description>Chess is a canonical example of a task that requires rigorous reasoning andlong-term planning. Modern decision Transformers - trained similarly to LLMs -are able to learn competent gameplay, but it is unclear to what extent theytruly capture the rules of chess. To investigate this, we train a 270Mparameter chess Transformer and test it on out-of-distribution scenarios,designed to reveal failures of systematic generalization. Our analysis showsthat Transformers exhibit compositional generalization, as evidenced by strongrule extrapolation: they adhere to fundamental syntactic rules of the game byconsistently choosing valid moves even in situations very different from thetraining data. Moreover, they also generate high-quality moves for OOD puzzles.In a more challenging test, we evaluate the models on variants includingChess960 (Fischer Random Chess) - a variant of chess where starting positionsof pieces are randomized. We found that while the model exhibits basic strategyadaptation, they are inferior to symbolic AI algorithms that perform explicitsearch, but gap is smaller when playing against users on Lichess. Moreover, thetraining dynamics revealed that the model initially learns to move only its ownpieces, suggesting an emergent compositional understanding of the game.</description><author>Anna Mészáros, Patrik Reizinger, Ferenc Huszár</author><pubDate>Thu, 23 Oct 2025 17:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20783v1</guid></item><item><title>A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text</title><link>http://arxiv.org/abs/2510.20782v1</link><description>Current methods for evaluating large language models (LLMs) typically focuson high-level tasks such as text generation, without targeting a particular AIapplication. This approach is not sufficient for evaluating LLMs forResponsible AI dimensions like fairness, since protected attributes that arehighly relevant in one application may be less relevant in another. In thiswork, we construct a dataset that is driven by a real-world application(generate a plain-text product description, given a list of product features),parameterized by fairness attributes intersected with gendered adjectives andproduct categories, yielding a rich set of labeled prompts. We show how to usethe data to identify quality, veracity, safety, and fairness gaps in LLMs,contributing a proposal for LLM evaluation paired with a concrete resource forthe research community.</description><author>Alicia Sagae, Chia-Jung Lee, Sandeep Avula, Brandon Dang, Vanessa Murdock</author><pubDate>Thu, 23 Oct 2025 17:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20782v1</guid></item><item><title>Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost</title><link>http://arxiv.org/abs/2510.20780v1</link><description>Recent advancements in large reasoning models (LRMs) have introduced anintermediate "thinking" process prior to generating final answers, improvingtheir reasoning capabilities on complex downstream tasks. However, thepotential of LRMs as evaluators for machine translation (MT) quality remainsunderexplored. We provides the first systematic analysis of LRM-as-a-judge inMT evaluation. We identify key challenges, revealing LRMs require tailoredevaluation materials, tend to "overthink" simpler instances and have issueswith scoring mechanisms leading to overestimation. To address these, we proposeto calibrate LRM thinking by training them on synthetic, human-like thinkingtrajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that thisapproach largely reduces thinking budgets by ~35x while concurrently improvingevaluation performance across different LRM scales from 7B to 32B (e.g.,R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). Thesefindings highlight the potential of efficiently calibrated LRMs to advancefine-grained automatic MT evaluation.</description><author>Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang, Derek F. Wong</author><pubDate>Thu, 23 Oct 2025 17:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20780v1</guid></item><item><title>CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image</title><link>http://arxiv.org/abs/2510.20776v1</link><description>This work proposes a new generation-based 3D reconstruction method, namedCupid, that accurately infers the camera pose, 3D shape, and texture of anobject from a single 2D image. Cupid casts 3D reconstruction as a conditionalsampling process from a learned distribution of 3D objects, and it jointlygenerates voxels and pixel-voxel correspondences, enabling robust pose andshape estimation under a unified generative framework. By representing bothinput camera poses and 3D shape as a distribution in a shared 3D latent space,Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage thatproduces initial 3D geometry with associated 2D projections for pose recovery;and (2) a refinement stage that integrates pose-aligned image features toenhance structural fidelity and appearance details. Extensive experimentsdemonstrate Cupid outperforms leading 3D reconstruction methods with an over 3dB PSNR gain and an over 10% Chamfer Distance reduction, while matchingmonocular estimators on pose accuracy and delivering superior visual fidelityover baseline 3D generative models. For an immersive view of the 3D resultsgenerated by Cupid, please visit cupid3d.github.io.</description><author>Binbin Huang, Haobin Duan, Yiqun Zhao, Zibo Zhao, Yi Ma, Shenghua Gao</author><pubDate>Thu, 23 Oct 2025 17:47:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20776v1</guid></item><item><title>FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation</title><link>http://arxiv.org/abs/2510.20774v1</link><description>Large-scale and diverse datasets are vital for training robust roboticmanipulation policies, yet existing data collection methods struggle to balancescale, diversity, and quality. Simulation offers scalability but suffers fromsim-to-real gaps, while teleoperation yields high-quality demonstrations withlimited diversity and high labor cost. We introduce FieldGen, a field-guideddata generation framework that enables scalable, diverse, and high-qualityreal-world data collection with minimal human supervision. FieldGen decomposesmanipulation into two stages: a pre-manipulation phase, allowing trajectorydiversity, and a fine manipulation phase requiring expert precision. Humandemonstrations capture key contact and pose information, after which anattraction field automatically generates diverse trajectories converging tosuccessful configurations. This decoupled design combines scalable trajectorydiversity with precise supervision. Moreover, FieldGen-Reward augmentsgenerated data with reward annotations to further enhance policy learning.Experiments demonstrate that policies trained with FieldGen achieve highersuccess rates and improved stability compared to teleoperation-based baselines,while significantly reducing human effort in long-term real-world datacollection. Webpage is available at https://fieldgen.github.io/.</description><author>Wenhao Wang, Kehe Ye, Xinyu Zhou, Tianxing Chen, Cao Min, Qiaoming Zhu, Xiaokang Yang, Yongjian Shen, Yang Yang, Maoqing Yao, Yao Mu</author><pubDate>Thu, 23 Oct 2025 17:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20774v1</guid></item><item><title>AlphaFlow: Understanding and Improving MeanFlow Models</title><link>http://arxiv.org/abs/2510.20771v1</link><description>MeanFlow has recently emerged as a powerful framework for few-step generativemodeling trained from scratch, but its success is not yet fully understood. Inthis work, we show that the MeanFlow objective naturally decomposes into twoparts: trajectory flow matching and trajectory consistency. Through gradientanalysis, we find that these terms are strongly negatively correlated, causingoptimization conflict and slow convergence. Motivated by these insights, weintroduce $\alpha$-Flow, a broad family of objectives that unifies trajectoryflow matching, Shortcut Model, and MeanFlow under one formulation. By adoptinga curriculum strategy that smoothly anneals from trajectory flow matching toMeanFlow, $\alpha$-Flow disentangles the conflicting objectives, and achievesbetter convergence. When trained from scratch on class-conditional ImageNet-1K256x256 with vanilla DiT backbones, $\alpha$-Flow consistently outperformsMeanFlow across scales and settings. Our largest $\alpha$-Flow-XL/2+ modelachieves new state-of-the-art results using vanilla DiT backbones, with FIDscores of 2.58 (1-NFE) and 2.15 (2-NFE).</description><author>Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov</author><pubDate>Thu, 23 Oct 2025 17:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20771v1</guid></item><item><title>CSU-PCAST: A Dual-Branch Transformer Framework for medium-range ensemble Precipitation Forecasting</title><link>http://arxiv.org/abs/2510.20769v1</link><description>Accurate medium-range precipitation forecasting is crucial forhydrometeorological risk management and disaster mitigation, yet remainschallenging for current numerical weather prediction (NWP) systems. Traditionalensemble systems such as the Global Ensemble Forecast System (GEFS) struggle tomaintain high skill, especially for moderate and heavy rainfall at extendedlead times. This study develops a deep learning-based ensemble framework formulti-step precipitation prediction through joint modeling of a comprehensiveset of atmospheric variables. The model is trained on ERA5 reanalysis data at0.25$^{\circ}$ spatial resolution, with precipitation labels from NASA'sIntegrated Multi-satellite Retrievals for Global Precipitation Measurement(GPM) constellation (IMERG), incorporating 57 input variables, includingupper-air and surface predictors. The architecture employs a patch-based SwinTransformer backbone with periodic convolutions to handle longitudinalcontinuity and integrates time and noise embeddings through conditional layernormalization. A dual-branch decoder predicts total precipitation and othervariables, with targeted freezing of encoder-decoder pathways for specializedtraining. Training minimizes a hybrid loss combining the Continuous RankedProbability Score (CRPS) and weighted log1p mean squared error (log1pMSE),balancing probabilistic accuracy and magnitude fidelity. During inference, themodel ingests real-time Global Forecast System (GFS) initial conditions togenerate 15-day forecasts autoregressively. Evaluation against GEFS using IMERGdata demonstrates higher Critical Success Index (CSI) scores at precipitationthresholds of 0.1 mm, 1 mm, 10 mm, and 20 mm, highlighting improved performancefor moderate to heavy rainfall.</description><author>Tianyi Xiong, Haonan Chen</author><pubDate>Thu, 23 Oct 2025 17:43:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20769v1</guid></item><item><title>RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines</title><link>http://arxiv.org/abs/2510.20768v1</link><description>Retrieval-Augmented Generation (RAG) has emerged as the dominantarchitectural pattern to operationalize Large Language Model (LLM) usage inCyber Threat Intelligence (CTI) systems. However, this design is susceptible topoisoning attacks, and previously proposed defenses can fail for CTI contextsas cyber threat information is often completely new for emerging attacks, andsophisticated threat actors can mimic legitimate formats, terminology, andstylistic conventions. To address this issue, we propose that the robustness ofmodern RAG defenses can be accelerated by applying source credibilityalgorithms on corpora, using PageRank as an example. In our experiments, wedemonstrate quantitatively that our algorithm applies a lower authority scoreto malicious documents while promoting trusted content, using the standardizedMS MARCO dataset. We also demonstrate proof-of-concept performance of ouralgorithm on CTI documents and feeds.</description><author>Austin Jia, Avaneesh Ramesh, Zain Shamsi, Daniel Zhang, Alex Liu</author><pubDate>Thu, 23 Oct 2025 17:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20768v1</guid></item><item><title>DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion</title><link>http://arxiv.org/abs/2510.20766v1</link><description>Diffusion Transformer models can generate images with remarkable fidelity anddetail, yet training them at ultra-high resolutions remains extremely costlydue to the self-attention mechanism's quadratic scaling with the number ofimage tokens. In this paper, we introduce Dynamic Position Extrapolation(DyPE), a novel, training-free method that enables pre-trained diffusiontransformers to synthesize images at resolutions far beyond their trainingdata, with no additional sampling cost. DyPE takes advantage of the spectralprogression inherent to the diffusion process, where low-frequency structuresconverge early, while high-frequencies take more steps to resolve.Specifically, DyPE dynamically adjusts the model's positional encoding at eachdiffusion step, matching their frequency spectrum with the current stage of thegenerative process. This approach allows us to generate images at resolutionsthat exceed the training resolution dramatically, e.g., 16 million pixels usingFLUX. On multiple benchmarks, DyPE consistently improves performance andachieves state-of-the-art fidelity in ultra-high-resolution image generation,with gains becoming even more pronounced at higher resolutions. Project page isavailable at https://noamissachar.github.io/DyPE/.</description><author>Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal</author><pubDate>Thu, 23 Oct 2025 17:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20766v1</guid></item><item><title>MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs</title><link>http://arxiv.org/abs/2510.20762v1</link><description>Decoding visual stimuli from neural population activity is crucial forunderstanding the brain and for applications in brain-machine interfaces.However, such biological data is often scarce, particularly in primates orhumans, where high-throughput recording techniques, such as two-photon imaging,remain challenging or impossible to apply. This, in turn, poses a challenge fordeep learning decoding techniques. To overcome this, we introduce MEIcoder, abiologically informed decoding method that leverages neuron-specific mostexciting inputs (MEIs), a structural similarity index measure loss, andadversarial training. MEIcoder achieves state-of-the-art performance inreconstructing visual stimuli from single-cell activity in primary visualcortex (V1), especially excelling on small datasets with fewer recordedneurons. Using ablation studies, we demonstrate that MEIs are the main driversof the performance, and in scaling experiments, we show that MEIcoder canreconstruct high-fidelity natural-looking images from as few as 1,000-2,500neurons and less than 1,000 training data points. We also propose a unifiedbenchmark with over 160,000 samples to foster future research. Our resultsdemonstrate the feasibility of reliable decoding in early visual system andprovide practical insights for neuroscience and neuroengineering applications.</description><author>Jan Sobotka, Luca Baroni, Ján Antolík</author><pubDate>Thu, 23 Oct 2025 17:35:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20762v1</guid></item><item><title>Autoencoding Random Forests</title><link>http://arxiv.org/abs/2505.21441v2</link><description>We propose a principled method for autoencoding with random forests. Ourstrategy builds on foundational results from nonparametric statistics andspectral graph theory to learn a low-dimensional embedding of the model thatoptimally represents relationships in the data. We provide exact andapproximate solutions to the decoding problem via constrained optimization,split relabeling, and nearest neighbors regression. These methods effectivelyinvert the compression pipeline, establishing a map from the embedding spaceback to the input space using splits learned by the ensemble's constituenttrees. The resulting decoders are universally consistent under commonregularity assumptions. The procedure works with supervised or unsupervisedmodels, providing a window into conditional or joint distributions. Wedemonstrate various applications of this autoencoder, including powerful newtools for visualization, compression, clustering, and denoising. Experimentsillustrate the ease and utility of our method in a wide range of settings,including tabular, image, and genomic data.</description><author>Binh Duc Vu, Jan Kapar, Marvin Wright, David S. Watson</author><pubDate>Thu, 23 Oct 2025 17:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.21441v2</guid></item><item><title>Watermarking Autoregressive Image Generation</title><link>http://arxiv.org/abs/2506.16349v2</link><description>Watermarking the outputs of generative models has emerged as a promisingapproach for tracking their provenance. Despite significant interest inautoregressive image generation models and their potential for misuse, no priorwork has attempted to watermark their outputs at the token level. In this work,we present the first such approach by adapting language model watermarkingtechniques to this setting. We identify a key challenge: the lack of reversecycle-consistency (RCC), wherein re-tokenizing generated image tokenssignificantly alters the token sequence, effectively erasing the watermark. Toaddress this and to make our method robust to common image transformations,neural compression, and removal attacks, we introduce (i) a customtokenizer-detokenizer finetuning procedure that improves RCC, and (ii) acomplementary watermark synchronization layer. As our experiments demonstrate,our approach enables reliable and robust watermark detection with theoreticallygrounded p-values. Code and models are available athttps://github.com/facebookresearch/wmar.</description><author>Nikola Jovanović, Ismail Labiad, Tomáš Souček, Martin Vechev, Pierre Fernandez</author><pubDate>Thu, 23 Oct 2025 17:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.16349v2</guid></item><item><title>Learning Modular Exponentiation with Transformers</title><link>http://arxiv.org/abs/2506.23679v2</link><description>Modular exponentiation is crucial to number theory and cryptography, yetremains largely unexplored from a mechanistic interpretability standpoint. Wetrain a 4-layer encoder-decoder Transformer model to perform this operation andinvestigate the emergence of numerical reasoning during training. Utilizingprincipled sampling strategies, PCA-based embedding analysis, and activationpatching, we examine how number-theoretic properties are encoded within themodel. We find that reciprocal operand training leads to strong performancegains, with sudden generalization across related moduli. These synchronizedaccuracy surges reflect grokking-like dynamics, suggesting the modelinternalizes shared arithmetic structure. We also find a subgraph consistingentirely of attention heads in the final layer sufficient to achieve fullperformance on the task of regular exponentiation. These results suggest thattransformer models learn modular arithmetic through specialized computationalcircuits, paving the way for more interpretable and efficient neural approachesto modular exponentiation.</description><author>David Demitri Africa, Sara M. Kapoor, Theo Simon Sorg, Challenger Mishra</author><pubDate>Thu, 23 Oct 2025 17:33:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.23679v2</guid></item><item><title>Tex-ViT: A Generalizable, Robust, Texture-based dual-branch cross-attention deepfake detector</title><link>http://arxiv.org/abs/2408.16892v2</link><description>Deepfakes, which employ GAN to produce highly realistic facial modification,are widely regarded as the prevailing method. Traditional CNN have been able toidentify bogus media, but they struggle to perform well on different datasetsand are vulnerable to adversarial attacks due to their lack of robustness.Vision transformers have demonstrated potential in the realm of imageclassification problems, but they require enough training data. Motivated bythese limitations, this publication introduces Tex-ViT (Texture-VisionTransformer), which enhances CNN features by combining ResNet with a visiontransformer. The model combines traditional ResNet features with a texturemodule that operates in parallel on sections of ResNet before eachdown-sampling operation. The texture module then serves as an input to the dualbranch of the cross-attention vision transformer. It specifically focuses onimproving the global texture module, which extracts feature map correlation.Empirical analysis reveals that fake images exhibit smooth textures that do notremain consistent over long distances in manipulations. Experiments wereperformed on different categories of FF++, such as DF, f2f, FS, and NT,together with other types of GAN datasets in cross-domain scenarios.Furthermore, experiments also conducted on FF++, DFDCPreview, and Celeb-DFdataset underwent several post-processing situations, such as blurring,compression, and noise. The model surpassed the most advanced models in termsof generalization, achieving a 98% accuracy in cross-domain scenarios. Thisdemonstrates its ability to learn the shared distinguishing texturalcharacteristics in the manipulated samples. These experiments provide evidencethat the proposed model is capable of being applied to various situations andis resistant to many post-processing procedures.</description><author>Deepak Dagar, Dinesh Kumar Vishwakarma</author><pubDate>Thu, 23 Oct 2025 17:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16892v2</guid></item><item><title>Controlling the Flow: Stability and Convergence for Stochastic Gradient Descent with Decaying Regularization</title><link>http://arxiv.org/abs/2505.11434v2</link><description>The present article studies the minimization of convex, L-smooth functionsdefined on a separable real Hilbert space. We analyze regularized stochasticgradient descent (reg-SGD), a variant of stochastic gradient descent that usesa Tikhonov regularization with time-dependent, vanishing regularizationparameter. We prove strong convergence of reg-SGD to the minimum-norm solutionof the original problem without additional boundedness assumptions. Moreover,we quantify the rate of convergence and optimize the interplay betweenstep-sizes and regularization decay. Our analysis reveals how vanishingTikhonov regularization controls the flow of SGD and yields stable learningdynamics, offering new insights into the design of iterative algorithms forconvex problems, including those that arise in ill-posed inverse problems. Wevalidate our theoretical findings through numerical experiments on imagereconstruction and ODE-based inverse problems.</description><author>Sebastian Kassing, Simon Weissmann, Leif Döring</author><pubDate>Thu, 23 Oct 2025 17:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11434v2</guid></item><item><title>Incomplete U-Statistics of Equireplicate Designs: Berry-Esseen Bound and Efficient Construction</title><link>http://arxiv.org/abs/2510.20755v1</link><description>U-statistics are a fundamental class of estimators that generalize the samplemean and underpin much of nonparametric statistics. Although extensivelystudied in both statistics and probability, key challenges remain: their highcomputational cost - addressed partly through incomplete U-statistics - andtheir non-standard asymptotic behavior in the degenerate case, which typicallyrequires resampling methods for hypothesis testing. This paper presents a novelperspective on U-statistics, grounded in hypergraph theory and combinatorialdesigns. Our approach bypasses the traditional Hoeffding decomposition, themain analytical tool in this literature but one highly sensitive to degeneracy.By characterizing the dependence structure of a U-statistic, we derive aBerry-Esseen bound that applies to all incomplete U-statistics of deterministicdesigns, yielding conditions under which Gaussian limiting distributions can beestablished even in the degenerate case and when the order diverges. We alsointroduce efficient algorithms to construct incomplete U-statistics ofequireplicate designs, a subclass of deterministic designs that, in certaincases, achieve minimum variance. Finally, we apply our framework tokernel-based tests that use Maximum Mean Discrepancy (MMD) and Hilbert-SchmidtIndependence Criterion. In a real data example with CIFAR-10, ourpermutation-free MMD test delivers substantial computational gains whileretaining power and type I error control.</description><author>Cesare Miglioli, Jordan Awan</author><pubDate>Thu, 23 Oct 2025 17:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20755v1</guid></item><item><title>ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for Tissue Segmentation in Histopathology</title><link>http://arxiv.org/abs/2510.20754v1</link><description>Automated histopathological image analysis plays a vital role incomputer-aided diagnosis of various diseases. Among developed algorithms, deeplearning-based approaches have demonstrated excellent performance in multipletasks, including semantic tissue segmentation in histological images. In thisstudy, we propose a novel approach based on attention-driven feature fusion ofconvolutional neural networks (CNNs) and vision transformers (ViTs) within aunified dual-encoder model to improve semantic segmentation performance.Evaluation on two publicly available datasets showed that our model achieved{\mu}IoU/{\mu}Dice scores of 76.79%/86.87% on the GCPS dataset and64.93%/76.60% on the PUMA dataset, outperforming state-of-the-art and baselinebenchmarks. The implementation of our method is publicly available in a GitHubrepository: https://github.com/NimaTorbati/ACS-SegNet</description><author>Nima Torbati, Anastasia Meshcheryakova, Ramona Woitek, Diana Mechtcheriakova, Amirreza Mahbod</author><pubDate>Thu, 23 Oct 2025 17:21:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20754v1</guid></item><item><title>Two approaches to multiple canonical correlation analysis for repeated measures data</title><link>http://arxiv.org/abs/2510.04457v2</link><description>In classical canonical correlation analysis (CCA), the goal is to determinethe linear transformations of two random vectors into two new random variablesthat are most strongly correlated. Canonical variables are pairs of these newrandom variables, while canonical correlations are correlations between thesepairs. In this paper, we propose and study two generalizations of thisclassical method: (1) Instead of two random vectors we study more complex data structures thatappear in important applications. In these structures, there are $L$ features,each described by $p_l$ scalars, $1 \le l \le L$. We observe $n$ such objectsover $T$ time points. We derive a suitable analog of the CCA for such data. Ourapproach relies on embeddings into Reproducing Kernel Hilbert Spaces, andcovers several related data structures as well. (2) We develop an analogous approach for multidimensional random processes.In this case, the experimental units are multivariate continuous,square-integrable functions over a given interval. These functions are modeledas elements of a Hilbert space, so in this case, we define the multiplefunctional canonical correlation analysis, MFCCA. We justify our approaches by their application to two data sets and suitablelarge sample theory. We derive consistency rates for the related transformationand correlation estimators, and show that it is possible to relax two commonassumptions on the compactness of the underlying cross-covariance operators andthe independence of the data.</description><author>Tomasz Górecki, Mirosław Krzyśko, Felix Gnettner, Piotr Kokoszka</author><pubDate>Thu, 23 Oct 2025 17:17:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.04457v2</guid></item><item><title>Reinforcement Learning and Consumption-Savings Behavior</title><link>http://arxiv.org/abs/2510.20748v1</link><description>This paper demonstrates how reinforcement learning can explain two puzzlingempirical patterns in household consumption behavior during economic downturns.I develop a model where agents use Q-learning with neural network approximationto make consumption-savings decisions under income uncertainty, departing fromstandard rational expectations assumptions. The model replicates two keyfindings from recent literature: (1) unemployed households with previously lowliquid assets exhibit substantially higher marginal propensities to consume(MPCs) out of stimulus transfers compared to high-asset households (0.50 vs0.34), even when neither group faces borrowing constraints, consistent withGanong et al. (2024); and (2) households with more past unemploymentexperiences maintain persistently lower consumption levels after controllingfor current economic conditions, a "scarring" effect documented by Malmendierand Shen (2024). Unlike existing explanations based on belief updating aboutincome risk or ex-ante heterogeneity, the reinforcement learning mechanismgenerates both higher MPCs and lower consumption levels simultaneously throughvalue function approximation errors that evolve with experience. Simulationresults closely match the empirical estimates, suggesting that adaptivelearning through reinforcement learning provides a unifying framework forunderstanding how past experiences shape current consumption behavior beyondwhat current economic conditions would predict.</description><author>Brandon Kaplowitz</author><pubDate>Thu, 23 Oct 2025 17:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20748v1</guid></item><item><title>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</title><link>http://arxiv.org/abs/2510.08396v2</link><description>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuningmethod for foundation models, but it suffers from parameter interference,resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-basedLoRA variants show promise in mitigating intra-task correlations in single-taskinstruction tuning, they introduce additional router parameters and remainineffective in multi-task model merging where inter-task interference arises.Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicitMoE-based LoRA variant that introduces: (1) rank-wise expert activation in theup-projection matrix, and (2) an implicit router that unifies expert routingand down-projection, where a frozen sparse random projection matrix replacesthe traditional dense trainable version. This design resolves the trade-offbetween intra-task decorrelation and computational efficiency by eliminatingthe need for an explicit router, while inherently mitigating inter-taskinterference due to the orthogonality property of random matrices. Extensiveexperiments across four domains -- general knowledge understanding, scientificquestion answering, mathematical reasoning, and code generation -- demonstrateconsistent performance improvements over existing methods. Beyond empiricalgains, FlyLoRA highlights how biological structures can inspire innovations inAI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.</description><author>Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</author><pubDate>Thu, 23 Oct 2025 17:14:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.08396v2</guid></item><item><title>GenLit: Reformulating Single-Image Relighting as Video Generation</title><link>http://arxiv.org/abs/2412.11224v4</link><description>Manipulating the illumination of a 3D scene within a single image representsa fundamental challenge in computer vision and graphics. This problem hastraditionally been addressed using inverse rendering techniques, which involveexplicit 3D asset reconstruction and costly ray-tracing simulations. Meanwhile,recent advancements in visual foundation models suggest that a new paradigmcould soon be possible -- one that replaces explicit physical models withnetworks that are trained on large amounts of image and video data. In thispaper, we exploit the implicit scene understanding of a video diffusion model,particularly Stable Video Diffusion, to relight a single image. We introduceGenLit, a framework that distills the ability of a graphics engine to performlight manipulation into a video-generation model, enabling users to directlyinsert and manipulate a point light in the 3D world within a given image andgenerate results directly as a video sequence. We find that a model fine-tunedon only a small synthetic dataset generalizes to real-world scenes, enablingsingle-image relighting with plausible and convincing shadows andinter-reflections. Our results highlight the ability of video foundation modelsto capture rich information about lighting, material, and shape, and ourfindings indicate that such models, with minimal training, can be used toperform relighting without explicit asset reconstruction or ray-tracing. .Project page: https://genlit.is.tue.mpg.de/.</description><author>Shrisha Bharadwaj, Haiwen Feng, Giorgio Becherini, Victoria Fernandez Abrevaya, Michael J. Black</author><pubDate>Thu, 23 Oct 2025 17:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11224v4</guid></item><item><title>Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review</title><link>http://arxiv.org/abs/2505.02828v2</link><description>Explainable Artificial Intelligence (XAI) has emerged as a pillar ofTrustworthy AI and aims to bring transparency in complex models that are opaqueby nature. Despite the benefits of incorporating explanations in models, anurgent need is found in addressing the privacy concerns of providing thisadditional information to end users. In this article, we conduct a scopingreview of existing literature to elicit details on the conflict between privacyand explainability. Using the standard methodology for scoping review, weextracted 57 articles from 1,943 studies published from January 2019 toDecember 2024. The review addresses 3 research questions to present readerswith more understanding of the topic: (1) what are the privacy risks ofreleasing explanations in AI systems? (2) what current methods have researchersemployed to achieve privacy preservation in XAI systems? (3) what constitutes aprivacy preserving explanation? Based on the knowledge synthesized from theselected studies, we categorize the privacy risks and preservation methods inXAI and propose the characteristics of privacy preserving explanations to aidresearchers and practitioners in understanding the requirements of XAI that isprivacy compliant. Lastly, we identify the challenges in balancing privacy withother system desiderata and provide recommendations for achieving privacypreserving XAI. We expect that this review will shed light on the complexrelationship of privacy and explainability, both being the fundamentalprinciples of Trustworthy AI.</description><author>Sonal Allana, Mohan Kankanhalli, Rozita Dara</author><pubDate>Thu, 23 Oct 2025 17:10:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.02828v2</guid></item><item><title>Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations</title><link>http://arxiv.org/abs/2510.20743v1</link><description>We present Empathic Prompting, a novel framework for multimodal human-AIinteraction that enriches Large Language Model (LLM) conversations withimplicit non-verbal context. The system integrates a commercial facialexpression recognition service to capture users' emotional cues and embeds themas contextual signals during prompting. Unlike traditional multimodalinterfaces, empathic prompting requires no explicit user control; instead, itunobtrusively augments textual input with affective information forconversational and smoothness alignment. The architecture is modular andscalable, allowing integration of additional non-verbal modules. We describethe system design, implemented through a locally deployed DeepSeek instance,and report a preliminary service and usability evaluation (N=5). Results showconsistent integration of non-verbal input into coherent LLM outputs, withparticipants highlighting conversational fluidity. Beyond this proof ofconcept, empathic prompting points to applications in chatbot-mediatedcommunication, particularly in domains like healthcare or education, whereusers' emotional signals are critical yet often opaque in verbal exchanges.</description><author>Lorenzo Stacchio, Andrea Ubaldi, Alessandro Galdelli, Maurizio Mauri, Emanuele Frontoni, Andrea Gaggioli</author><pubDate>Thu, 23 Oct 2025 17:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20743v1</guid></item><item><title>Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex</title><link>http://arxiv.org/abs/2504.12474v3</link><description>Text-attributed graphs (TAGs) present unique challenges in representationlearning by requiring models to capture both the semantic richness ofnode-associated texts and the structural dependencies of the graph. While graphneural networks (GNNs) excel at modeling topological information, they lack thecapacity to process unstructured text. Conversely, large language models (LLMs)are proficient in text understanding but are typically unaware of graphstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novelarchitecture that tightly integrates GNNs and LLMs through stacked Graph-TextFusion Units. Each unit allows for mutual attention between textual andstructural representations, enabling information to flow in both directions,text influencing structure and structure guiding textual interpretation. Theproposed architecture is trained using parameter-efficient fine-tuning (LoRA),keeping the LLM frozen while adapting to task-specific signals. Extensiveexperiments on five benchmark datasets demonstrate that BiGTex achievesstate-of-the-art performance in node classification and generalizes effectivelyto link prediction. An ablation study further highlights the importance of softprompting and bi-directional attention in the model's success.</description><author>Azadeh Beiranvand, Seyed Mehdi Vahidipour</author><pubDate>Thu, 23 Oct 2025 17:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.12474v3</guid></item><item><title>Position: Many generalization measures for deep learning are fragile</title><link>http://arxiv.org/abs/2510.18934v2</link><description>A wide variety of generalization measures have been applied to deep neuralnetworks (DNNs). Although obtaining tight bounds remains challenging, suchmeasures are often assumed to reproduce qualitative generalization trends. Inthis position paper, we argue that many post-mortem generalization measures --those computed on trained networks -- are \textbf{fragile}: small trainingmodifications that barely affect the underlying DNN can substantially change ameasure's value, trend, or scaling behavior. For example, minor hyperparameterchanges, such as learning rate adjustments or switching between SGD variantscan reverse the slope of a learning curve in widely used generalizationmeasures like the path norm. We also identify subtler forms of fragility. Forinstance, the PAC-Bayes origin measure is regarded as one of the most reliable,and is indeed less sensitive to hyperparameter tweaks than many other measures.However, it completely fails to capture differences in data complexity acrosslearning curves. This data fragility contrasts with the function-basedmarginal-likelihood PAC-Bayes bound, which does capture differences indata-complexity, including scaling behavior, in learning curves, but which isnot a post-mortem measure. Beyond demonstrating that many bounds -- such aspath, spectral and Frobenius norms, flatness proxies, and deterministicPAC-Bayes surrogates -- are fragile, this position paper also argues thatdevelopers of new measures should explicitly audit them for fragility.</description><author>Shuofeng Zhang, Ard Louis</author><pubDate>Thu, 23 Oct 2025 17:02:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.18934v2</guid></item><item><title>Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages</title><link>http://arxiv.org/abs/2510.20739v1</link><description>Program analysis tools often produce large volumes of candidate vulnerabilityreports that require costly manual review, creating a practical challenge: howcan security analysts prioritize the reports most likely to be truevulnerabilities? This paper investigates whether machine learning can be applied toprioritizing vulnerabilities reported by program analysis tools. We focus onNode.js packages and collect a benchmark of 1,883 Node.js packages, eachcontaining one reported ACE or ACI vulnerability. We evaluate a variety ofmachine learning approaches, including classical models, graph neural networks(GNNs), large language models (LLMs), and hybrid models that combine GNN andLLMs, trained on data based on a dynamic program analysis tool's output. Thetop LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML modelsreaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leadingmodel eliminates 66.9% of benign packages from manual review, taking around 60ms per package. If the best model is tuned to operate at a precision level of0.8 (i.e., allowing 20% false positives amongst all warnings), our approach candetect 99.2% of exploitable taint flows while missing only 0.8%, demonstratingstrong potential for real-world vulnerability triage.</description><author>Ronghao Ni, Aidan Z. H. Yang, Min-Chien Hsu, Nuno Sabino, Limin Jia, Ruben Martins, Darion Cassel, Kevin Cheang</author><pubDate>Thu, 23 Oct 2025 16:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20739v1</guid></item><item><title>Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process</title><link>http://arxiv.org/abs/2510.20736v1</link><description>Developing effective multimodal fusion approaches has become increasinglyessential in many real-world scenarios, such as health care and finance. Thekey challenge is how to preserve the feature expressiveness in each modalitywhile learning cross-modal interactions. Previous approaches primarily focus onthe cross-modal alignment, while over-emphasis on the alignment of marginaldistributions of modalities may impose excess regularization and obstructmeaningful representations within each modality. The Dirichlet process (DP)mixture model is a powerful Bayesian non-parametric method that can amplify themost prominent features by its richer-gets-richer property, which allocatesincreasing weights to them. Inspired by this unique characteristic of DP, wepropose a new DP-driven multimodal learning framework that automaticallyachieves an optimal balance between prominent intra-modal representationlearning and cross-modal alignment. Specifically, we assume that each modalityfollows a mixture of multivariate Gaussian distributions and further adopt DPto calculate the mixture weights for all the components. This paradigm allowsDP to dynamically allocate the contributions of features and select the mostprominent ones, leveraging its richer-gets-richer property, thus facilitatingmultimodal feature fusion. Extensive experiments on several multimodal datasetsdemonstrate the superior performance of our model over other competitors.Ablation analysis further validates the effectiveness of DP in aligningmodality distributions and its robustness to changes in key hyperparameters.Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git</description><author>Tsai Hor Chan, Feng Wu, Yihang Chen, Guosheng Yin, Lequan Yu</author><pubDate>Thu, 23 Oct 2025 16:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20736v1</guid></item><item><title>Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs</title><link>http://arxiv.org/abs/2506.19923v4</link><description>We present Prover Agent, a novel AI agent for automated theorem proving thatintegrates large language models (LLMs) with a formal proof assistant, Lean.Prover Agent coordinates an informal reasoning LLM, a formal prover model, andfeedback from Lean while also generating auxiliary lemmas. These auxiliarylemmas are not limited to subgoals in the formal proof but can also includespecial cases or potentially useful facts derived from the assumptions, whichhelp in discovering a viable proof strategy. It achieves an 88.1% success rateon the MiniF2F benchmark, establishing a new state-of-the-art among methodsusing small language models (SLMs) with a much lower sample budget thanprevious approaches. We also present theoretical analyses and case studies thatillustrate how these generated lemmas contribute to solving challengingproblems. Our code is publicly available at:https://github.com/kAIto47802/Prover-Agent.</description><author>Kaito Baba, Chaoran Liu, Shuhei Kurita, Akiyoshi Sannai</author><pubDate>Thu, 23 Oct 2025 16:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.19923v4</guid></item><item><title>Thought Communication in Multiagent Collaboration</title><link>http://arxiv.org/abs/2510.20733v1</link><description>Natural language has long enabled human cooperation, but its lossy,ambiguous, and indirect nature limits the potential of collective intelligence.While machines are not subject to these constraints, most LLM-based multi-agentsystems still rely solely on natural language, exchanging tokens or theirembeddings. To go beyond language, we introduce a new paradigm, thoughtcommunication, which enables agents to interact directly mind-to-mind, akin totelepathy. To uncover these latent thoughts in a principled way, we formalizethe process as a general latent variable model, where agent states aregenerated by an unknown function of underlying thoughts. We prove that, in anonparametric setting without auxiliary information, both shared and privatelatent thoughts between any pair of agents can be identified. Moreover, theglobal structure of thought sharing, including which agents share whichthoughts and how these relationships are structured, can also be recovered withtheoretical guarantees. Guided by the established theory, we develop aframework that extracts latent thoughts from all agents prior to communicationand assigns each agent the relevant thoughts, along with their sharingpatterns. This paradigm naturally extends beyond LLMs to all modalities, asmost observational data arise from hidden generative processes. Experiments onboth synthetic and real-world benchmarks validate the theory and demonstratethe collaborative advantages of thought communication. We hope this workilluminates the potential of leveraging the hidden world, as many challengesremain unsolvable through surface-level observation alone, regardless ofcompute or data scale.</description><author>Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, Kun Zhang</author><pubDate>Thu, 23 Oct 2025 16:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20733v1</guid></item><item><title>xRFM: Accurate, scalable, and interpretable feature learning models for tabular data</title><link>http://arxiv.org/abs/2508.10053v2</link><description>Inference from tabular data, collections of continuous and categoricalvariables organized into matrices, is a foundation for modern technology andscience. Yet, in contrast to the explosive changes in the rest of AI, the bestpractice for these predictive tasks has been relatively unchanged and is stillprimarily based on variations of Gradient Boosted Decision Trees (GBDTs). Veryrecently, there has been renewed interest in developing state-of-the-artmethods for tabular data based on recent developments in neural networks andfeature learning methods. In this work, we introduce xRFM, an algorithm thatcombines feature learning kernel machines with a tree structure to both adaptto the local structure of the data and scale to essentially unlimited amountsof training data. We show that compared to $31$ other methods, including recently introducedtabular foundation models (TabPFNv2) and GBDTs, xRFM achieves best performanceacross $100$ regression datasets and is competitive to the best methods across$200$ classification datasets outperforming GBDTs. Additionally, xRFM providesinterpretability natively through the Average Gradient Outer Product.</description><author>Daniel Beaglehole, David Holzmüller, Adityanarayanan Radhakrishnan, Mikhail Belkin</author><pubDate>Thu, 23 Oct 2025 16:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10053v2</guid></item><item><title>Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems</title><link>http://arxiv.org/abs/2510.20728v1</link><description>We present a multi-agent, human-in-the-loop workflow that co-designs quantumcodes with prescribed transversal diagonal gates. It builds on the Subset-SumLinear Programming (SSLP) framework (arXiv:2504.20847), which partitions basisstrings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL)equalities via small LPs. The workflow is powered by GPT-5 and implementedwithin TeXRA (https://texra.ai)-a multi-agent research assistant platform thatsupports an iterative tool-use loop agent and a derivation-then-edit workflowreasoning agent. We work in a LaTeX-Python environment where agents reason,edit documents, execute code, and synchronize their work to Git/Overleaf.Within this workspace, three roles collaborate: a Synthesis Agent formulatesthe problem; a Search Agent sweeps/screens candidates and exactifies numericsinto rationals; and an Audit Agent independently checks all KL equalities andthe induced logical action. As a first step we focus on distance $d=2$ withnondegenerate residues. For code dimension $K\in\{2,3,4\}$ and $n\le6$ qubits,systematic sweeps yield certificate-backed tables cataloging attainable cycliclogical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$at $n=6$. From verified instances, Synthesis Agent abstracts recurringstructures into closed-form families and proves they satisfy the KL equalitiesfor all parameters. It further demonstrates that SSLP accommodates residuedegeneracy by exhibiting a new $((6,4,2))$ code implementing the transversalcontrolled-phase $diag(1,1,1,i)$. Overall, the workflow recastsdiagonal-transversal feasibility as an analytical pipeline executed at scale,combining systematic enumeration with exact analytical reconstruction. Ityields reproducible code constructions, supports targeted extensions to larger$K$ and higher distances, and leads toward data-driven classification.</description><author>Xi He, Sirui Lu, Bei Zeng</author><pubDate>Thu, 23 Oct 2025 16:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20728v1</guid></item><item><title>Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing</title><link>http://arxiv.org/abs/2510.20727v1</link><description>Objective: Fluoropyrimidines are widely prescribed for colorectal and breastcancers, but are associated with toxicities such as hand-foot syndrome andcardiotoxicity. Since toxicity documentation is often embedded in clinicalnotes, we aimed to develop and evaluate natural language processing (NLP)methods to extract treatment and toxicity information. Materials and Methods: We constructed a gold-standard dataset of 236 clinicalnotes from 204,165 adult oncology patients. Domain experts annotated categoriesrelated to treatment regimens and toxicities. We developed rule-based, machinelearning-based (Random Forest, Support Vector Machine [SVM], LogisticRegression [LR]), deep learning-based (BERT, ClinicalBERT), and large languagemodels (LLM)-based NLP approaches (zero-shot and error-analysis prompting).Models used an 80:20 train-test split. Results: Sufficient data existed to train and evaluate 5 annotatedcategories. Error-analysis prompting achieved optimal precision, recall, and F1scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shotprompting reached F1=1.000 for treatment and F1=0.876 for toxicitiesextraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learningunderperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) andClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methodsserved as our baseline with F1 scores of 0.857 in treatment and 0.858 intoxicities. Discussion: LMM-based approaches outperformed all others, followed by machinelearning methods. Machine and deep learning approaches were limited by smalltraining data and showed limited generalizability, particularly for rarecategories. Conclusion: LLM-based NLP most effectively extracted fluoropyrimidinetreatment and toxicity information from clinical notes, and has strongpotential to support oncology research and pharmacovigilance.</description><author>Xizhi Wu, Madeline S. Kreider, Philip E. Empey, Chenyu Li, Yanshan Wang</author><pubDate>Thu, 23 Oct 2025 16:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20727v1</guid></item><item><title>AutoScape: Geometry-Consistent Long-Horizon Scene Generation</title><link>http://arxiv.org/abs/2510.20726v1</link><description>This paper proposes AutoScape, a long-horizon driving scene generationframework. At its core is a novel RGB-D diffusion model that iterativelygenerates sparse, geometrically consistent keyframes, serving as reliableanchors for the scene's appearance and geometry. To maintain long-rangegeometric consistency, the model 1) jointly handles image and depth in a sharedlatent space, 2) explicitly conditions on the existing scene geometry (i.e.,rendered point clouds) from previously generated keyframes, and 3) steers thesampling process with a warp-consistent guidance. Given high-quality RGB-Dkeyframes, a video diffusion model then interpolates between them to producedense and coherent video frames. AutoScape generates realistic andgeometrically consistent driving videos of over 20 seconds, improving thelong-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and43.0\%, respectively.</description><author>Jiacheng Chen, Ziyu Jiang, Mingfu Liang, Bingbing Zhuang, Jong-Chyi Su, Sparsh Garg, Ying Wu, Manmohan Chandraker</author><pubDate>Thu, 23 Oct 2025 16:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20726v1</guid></item><item><title>No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes</title><link>http://arxiv.org/abs/2510.20725v1</link><description>Thompson sampling (TS) is a powerful and widely used strategy for sequentialdecision-making, with applications ranging from Bayesian optimization toreinforcement learning (RL). Despite its success, the theoretical foundationsof TS remain limited, particularly in settings with complex temporal structuresuch as RL. We address this gap by establishing no-regret guarantees for TSusing models with Gaussian marginal distributions. Specifically, we consider TSin episodic RL with joint Gaussian process (GP) priors over rewards andtransitions. We prove a regret bound of$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysisaddresses several challenges, including the non-Gaussian nature of valuefunctions and the recursive structure of Bellman updates, and extends classicaltools such as the elliptical potential lemma to multi-output settings. Thiswork advances the understanding of TS in RL and highlights how structuralassumptions and model uncertainty shape its performance in finite-horizonMarkov Decision Processes.</description><author>Jasmine Bayrooti, Sattar Vakili, Amanda Prorok, Carl Henrik Ek</author><pubDate>Thu, 23 Oct 2025 16:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20725v1</guid></item><item><title>mmWalk: Towards Multi-modal Multi-view Walking Assistance</title><link>http://arxiv.org/abs/2510.11520v2</link><description>Walking assistance in extreme or complex environments remains a significantchallenge for people with blindness or low vision (BLV), largely due to thelack of a holistic scene understanding. Motivated by the real-world needs ofthe BLV community, we build mmWalk, a simulated multi-modal dataset thatintegrates multi-view sensor and accessibility-oriented features for outdoorsafe navigation. Our dataset comprises 120 manually controlled,scenario-categorized walking trajectories with 62k synchronized frames. Itcontains over 559k panoramic images across RGB, depth, and semantic modalities.Furthermore, to emphasize real-world relevance, each trajectory involvesoutdoor corner cases and accessibility-specific landmarks for BLV users.Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visualquestion-answer triplets across 9 categories tailored for safe and informedwalking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)using zero- and few-shot settings and found they struggle with our riskassessment and navigational tasks. We validate our mmWalk-finetuned model onreal-world datasets and show the effectiveness of our dataset for advancingmulti-modal walking assistance.</description><author>Kedi Ying, Ruiping Liu, Chongyan Chen, Mingzhe Tao, Hao Shi, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen</author><pubDate>Thu, 23 Oct 2025 16:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.11520v2</guid></item><item><title>User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios</title><link>http://arxiv.org/abs/2510.20721v1</link><description>Large language models (LLMs) have seen rapid adoption for tasks such asdrafting emails, summarizing meetings, and answering health questions. In suchuses, users may need to share private information (e.g., health records,contact details). To evaluate LLMs' ability to identify and redact such privateinformation, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) withreal-life scenarios. Using these benchmarks, researchers have found that LLMssometimes fail to keep secrets private when responding to complex tasks (e.g.,leaking employee salaries in meeting summaries). However, these evaluationsrely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlookingreal users' perceptions. Moreover, prior work primarily focused on theprivacy-preservation quality of responses, without investigating nuanceddifferences in helpfulness. To understand how users perceive theprivacy-preservation quality and helpfulness of LLM responses toprivacy-sensitive scenarios, we conducted a user study with 94 participantsusing 90 scenarios from PrivacyLens. We found that, when evaluating identicalresponses to the same scenario, users showed low agreement with each other onthe privacy-preservation quality and helpfulness of the LLM response. Further,we found high agreement among five proxy LLMs, while each individual LLM hadlow correlation with users' evaluations. These results indicate that theprivacy and helpfulness of LLM responses are often specific to individuals, andproxy LLMs are poor estimates of how real users would perceive these responsesin privacy-sensitive scenarios. Our results suggest the need to conductuser-centered studies on measuring LLMs' ability to help users while preservingprivacy. Additionally, future research could investigate ways to improve thealignment between proxy LLMs and users for better estimation of users'perceived privacy and utility.</description><author>Xiaoyuan Wu, Roshni Kaushik, Wenkai Li, Lujo Bauer, Koichi Onoue</author><pubDate>Thu, 23 Oct 2025 16:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20721v1</guid></item><item><title>Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding</title><link>http://arxiv.org/abs/2508.19529v2</link><description>Discrete diffusion language models have shown strong potential for textgeneration, yet standard supervised fine-tuning (SFT) misaligns with theirsemi-autoregressive inference: training randomly masks tokens across the entireresponse, while inference generates fixed-size blocks sequentially. Thismismatch introduces noisy prefixes and leaky suffixes, biasing gradients awayfrom the desired blockwise likelihood. We propose Blockwise SFT, whichpartitions responses into fixed-size blocks, selects one active block per stepfor stochastic masking, freezes all preceding tokens, and fully hides futureones. Loss is computed only over the active block, directly mirroring theblockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA showconsistent gains over classical SFT under equal compute or token budgets. Blocksize consistency studies and ablations confirm that improvements stem fromfaithful training-inference alignment rather than incidental masking effects.Our results highlight the importance of matching supervision granularity to thedecoding procedure in diffusion-based language models.</description><author>Bowen Sun, Yujun Cai, Ming-Hsuan Yang, Yiwei Wang</author><pubDate>Thu, 23 Oct 2025 16:36:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19529v2</guid></item><item><title>Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series</title><link>http://arxiv.org/abs/2510.20718v1</link><description>Semiconductor manufacturing is an extremely complex and precision-drivenprocess, characterized by thousands of interdependent parameters collectedacross diverse tools and process steps. Multi-variate time-series analysis hasemerged as a critical field for real-time monitoring and fault detection insuch environments. However, anomaly prediction in semiconductor fabricationpresents several critical challenges, including high dimensionality of sensordata and severe class imbalance due to the rarity of true faults. Furthermore,the complex interdependencies between variables complicate both anomalyprediction and root-cause-analysis. This paper proposes two novel approaches toadvance the field from anomaly detection to anomaly prediction, an essentialstep toward enabling real-time process correction and proactive faultprevention. The proposed anomaly prediction framework contains two main stages:(a) training a forecasting model on a dataset assumed to contain no anomalies,and (b) performing forecast on unseen time series data. The forecast iscompared with the forecast of the trained signal. Deviations beyond apredefined threshold are flagged as anomalies. The two approaches differ in theforecasting model employed. The first assumes independence between variables byutilizing the N-BEATS model for univariate time series forecasting. The secondlifts this assumption by utilizing a Graph Neural Network (GNN) to captureinter-variable relationships. Both models demonstrate strong forecastingperformance up to a horizon of 20 time points and maintain stable anomalyprediction up to 50 time points. The GNN consistently outperforms the N-BEATSmodel while requiring significantly fewer trainable parameters and lowercomputational cost. These results position the GNN as promising solution foronline anomaly forecasting to be deployed in manufacturing environments.</description><author>Daniel Sorensen, Bappaditya Dey, Minjin Hwang, Sandip Halder</author><pubDate>Thu, 23 Oct 2025 16:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20718v1</guid></item><item><title>Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool</title><link>http://arxiv.org/abs/2510.20714v1</link><description>In this study we aim to better align fall risk prediction from the JohnsHopkins Fall Risk Assessment Tool (JHFRAT) with additional clinicallymeaningful measures via a data-driven modelling approach. We conducted aretrospective analysis of 54,209 inpatient admissions from three Johns HopkinsHealth System hospitals between March 2022 and October 2023. A total of 20,208admissions were included as high fall risk encounters, and 13,941 were includedas low fall risk encounters. To incorporate clinical knowledge and maintaininterpretability, we employed constrained score optimization (CSO) models onJHFRAT assessment data and additional electronic health record (EHR) variables.The model demonstrated significant improvements in predictive performance overthe current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrainedscore optimization models performed similarly with and without the EHRvariables. Although the benchmark black-box model (XGBoost), improves upon theperformance metrics of the knowledge-based constrained logistic regression(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risklabelling. This evidence-based approach provides a robust foundation for healthsystems to systematically enhance inpatient fall prevention protocols andpatient safety using data-driven optimization techniques, contributing toimproved risk assessment and resource allocation in healthcare settings.</description><author>Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Kimia Ghobadi</author><pubDate>Thu, 23 Oct 2025 16:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20714v1</guid></item><item><title>Experimental differentiation and extremization with analog quantum circuits</title><link>http://arxiv.org/abs/2510.20713v1</link><description>Solving and optimizing differential equations (DEs) is ubiquitous in bothengineering and fundamental science. The promise of quantum architectures toaccelerate scientific computing thus naturally involved interest towards howefficiently quantum algorithms can solve DEs. Differentiable quantum circuits(DQC) offer a viable route to compute DE solutions using a variational approachamenable to existing quantum computers, by producing a machine-learnablesurrogate of the solution. Quantum extremal learning (QEL) complements suchapproach by finding extreme points in the output of learnable models of unknown(implicit) functions, offering a powerful tool to bypass a full DE solution, incases where the crux consists in retrieving solution extrema. In this work, weprovide the results from the first experimental demonstration of both DQC andQEL, displaying their performance on a synthetic usecase. Whilst both DQC andQEL are expected to require digital quantum hardware, we successfully challengethis assumption by running a closed-loop instance on a commercial analogquantum computer, based upon neutral atom technology.</description><author>Evan Philip, Julius de Hond, Vytautas Abramavicius, Kaonan Micadei, Mario Dagrada, Panagiotis Barkoutsos, Mourad Beji, Louis-Paul Henry, Vincent E. Elfving, Antonio A. Gentile, Savvas Varsamopoulos</author><pubDate>Thu, 23 Oct 2025 16:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20713v1</guid></item><item><title>CLEVER: A Curated Benchmark for Formally Verified Code Generation</title><link>http://arxiv.org/abs/2505.13938v4</link><description>We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of161 problems for end-to-end verified code generation in Lean. Each problemconsists of (1) the task of generating a specification that matches a held-outground-truth specification, and (2) the task of generating a Leanimplementation that provably satisfies this specification. Unlike priorbenchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generatedannotations, and specifications that leak implementation logic or allow vacuoussolutions. All outputs are verified post-hoc using Lean's type checker toensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ toevaluate several few-shot and agentic approaches based on state-of-the-artlanguage models. These methods all struggle to achieve full verification,establishing it as a challenging frontier benchmark for program synthesis andformal reasoning. Our benchmark can be found onGitHub(https://github.com/trishullab/clever) as well asHuggingFace(https://huggingface.co/datasets/amitayusht/clever). All ourevaluation code is also availableonline(https://github.com/trishullab/clever-prover).</description><author>Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzsche, Greg Durrett, Yisong Yue, Swarat Chaudhuri</author><pubDate>Thu, 23 Oct 2025 16:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.13938v4</guid></item><item><title>Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning</title><link>http://arxiv.org/abs/2504.15275v3</link><description>Process reward models (PRMs) have proven effective for test-time scaling ofLarge Language Models (LLMs) on challenging reasoning tasks. However, rewardhacking issues with PRMs limit their successful application in reinforcementfine-tuning. In this paper, we identify the main cause of PRM-induced rewardhacking: the canonical summation-form credit assignment in reinforcementlearning (RL), which defines the value as cumulative gamma-decayed futurerewards, easily induces LLMs to hack steps with high rewards. To address this,we propose PURE: Process sUpervised Reinforcement lEarning. The key innovationof PURE is a min-form credit assignment that formulates the value function asthe minimum of future rewards. This method significantly alleviates rewardhacking by limiting the value function range and distributing advantages morereasonably. Through extensive experiments on 3 base models, we show thatPRM-based approaches enabling min-form credit assignment achieve comparablereasoning performance to verifiable reward-based methods within only 30% steps.In contrast, the canonical sum-form credit assignment collapses training evenat the beginning! Additionally, when we supplement PRM-based fine-tuning withjust 10% verifiable rewards, we further alleviate reward hacking and producethe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5benchmarks. Moreover, we summarize the observed reward hacking cases andanalyze the causes of training collapse. We release our code and model weightsat https://github.com/CJReinforce/PURE.</description><author>Jie Cheng, Gang Xiong, Ruixi Qiao, Lijun Li, Chao Guo, Junle Wang, Yisheng Lv, Fei-Yue Wang</author><pubDate>Thu, 23 Oct 2025 16:28:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.15275v3</guid></item><item><title>Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning</title><link>http://arxiv.org/abs/2504.18458v2</link><description>When applying reinforcement learning--typically through GRPO--to largevision-language model reasoning struggles to effectively scale reasoning lengthor generates verbose outputs across all tasks with only marginal gains inaccuracy. To address this issue, we present FAST-GRPO, a variant of GRPO thatdynamically adapts reasoning depth based on question characteristics. Throughempirical analysis, we establish the feasibility of fast-slow thinking in LVLMsby investigating how response length and data distribution affect performance.Inspired by these observations, we introduce two complementary metrics toestimate the difficulty of the questions, guiding the model to determine whenfast or slow thinking is more appropriate. Next, we incorporate adaptivelength-based rewards and difficulty-aware KL divergence into the GRPOalgorithm. Experiments across seven reasoning benchmarks demonstrate that FASTachieves state-of-the-art accuracy with over 10\% relative improvement comparedto the base model, while reducing token usage by 32.7-67.3\% compared toprevious slow-thinking approaches, effectively balancing reasoning length andaccuracy.</description><author>Wenyi Xiao, Leilei Gan</author><pubDate>Thu, 23 Oct 2025 16:25:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.18458v2</guid></item><item><title>Separating the what and how of compositional computation to enable reuse and continual learning</title><link>http://arxiv.org/abs/2510.20709v1</link><description>The ability to continually learn, retain and deploy skills to accomplishgoals is a key feature of intelligent and efficient behavior. However, theneural mechanisms facilitating the continual learning and flexible(re-)composition of skills remain elusive. Here, we study continual learningand the compositional reuse of learned computations in recurrent neural network(RNN) models using a novel two-system approach: one system that infers whatcomputation to perform, and one that implements how to perform it. We focus ona set of compositional cognitive tasks commonly studied in neuroscience. Toconstruct the what system, we first show that a large family of tasks can besystematically described by a probabilistic generative model, wherecompositionality stems from a shared underlying vocabulary of discrete taskepochs. The shared epoch structure makes these tasks inherently compositional.We first show that this compositionality can be systematically described by aprobabilistic generative model. Furthermore, We develop an unsupervised onlinelearning approach that can learn this model on a single-trial basis, buildingits vocabulary incrementally as it is exposed to new tasks, and inferring thelatent epoch structure as a time-varying computational context within a trial.We implement the how system as an RNN whose low-rank components are composedaccording to the context inferred by the what system. Contextual inferencefacilitates the creation, learning, and reuse of low-rank RNN components as newtasks are introduced sequentially, enabling continual learning withoutcatastrophic forgetting. Using an example task set, we demonstrate the efficacyand competitive performance of this two-system learning framework, itspotential for forward and backward transfer, as well as fast compositionalgeneralization to unseen tasks.</description><author>Haozhe Shan, Sun Minni, Lea Duncker</author><pubDate>Thu, 23 Oct 2025 16:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20709v1</guid></item><item><title>TabR1: Taming GRPO for tabular reasoning LLMs</title><link>http://arxiv.org/abs/2510.17385v2</link><description>Tabular prediction has traditionally relied on gradient-boosted decisiontrees and specialized deep learning models, which excel within tasks butprovide limited interpretability and weak transfer across tables. Reasoninglarge language models (LLMs) promise cross-task adaptability with trans- parentreasoning traces, yet their potential has not been fully realized for tabulardata. This paper presents TabR1, the first reasoning LLM for tabular predictionwith multi-step reasoning. At its core is Permutation Relative PolicyOptimization (PRPO), a simple yet efficient reinforcement learning method thatencodes column-permutation invariance as a structural prior. By construct- ingmultiple label-preserving permutations per sample and estimating advantagesboth within and across permutations, PRPO transforms sparse rewards into denselearning signals and improves generalization. With limited supervision, PRPOactivates the reasoning ability of LLMs for tabular prediction, enhancingfew-shot and zero-shot performance as well as interpretability. Comprehensiveexperiments demonstrate that TabR1 achieves performance comparable to strongbaselines under full-supervision fine-tuning. In the zero-shot setting, TabR1approaches the performance of strong baselines under the 32-shot setting.Moreover, TabR1 (8B) substantially outperforms much larger LLMs across varioustasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).</description><author>Pengxiang Cai, Zihao Gao, Jintai Chen</author><pubDate>Thu, 23 Oct 2025 16:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.17385v2</guid></item><item><title>ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata</title><link>http://arxiv.org/abs/2510.20708v1</link><description>3D LiDAR sensors are essential for autonomous navigation, environmentalmonitoring, and precision mapping in remote sensing applications. Toefficiently process the massive point clouds generated by these sensors, LiDARdata is often projected into 2D range images that organize points by theirangular positions and distances. While these range image representations enableefficient processing, conventional projection methods suffer from fundamentalgeometric inconsistencies that cause irreversible information loss,compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDARIntrinsic Calibration Estimation for Lossless Range Images), the first general,sensor-agnostic method that achieves lossless range image generation fromspinning LiDAR point clouds without requiring manufacturer metadata orcalibration files. Our algorithm automatically reverse-engineers the intrinsicgeometry of any spinning LiDAR sensor by inferring critical parametersincluding laser beam configuration, angular distributions, and per-beamcalibration corrections, enabling lossless projection and complete point cloudreconstruction with zero point loss. Comprehensive evaluation across thecomplete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfectpoint preservation, with zero points lost across all point clouds. Geometricaccuracy is maintained well within sensor precision limits, establishinggeometric losslessness with real-time performance. We also present acompression case study that validates substantial downstream benefits,demonstrating significant quality improvements in practical applications. Thisparadigm shift from approximate to lossless LiDAR projections opens newpossibilities for high-precision remote sensing applications requiring completegeometric preservation.</description><author>Samuel Soutullo, Miguel Yermo, David L. Vilariño, Óscar G. Lorenzo, José C. Cabaleiro, Francisco F. Rivera</author><pubDate>Thu, 23 Oct 2025 16:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20708v1</guid></item><item><title>Structured Spectral Graph Representation Learning for Multi-label Abnormality Analysis from 3D CT Scans</title><link>http://arxiv.org/abs/2510.10779v2</link><description>With the growing volume of CT examinations, there is an increasing demand forautomated tools such as organ segmentation, abnormality detection, and reportgeneration to support radiologists in managing their clinical workload.Multi-label classification of 3D Chest CT scans remains a critical yetchallenging problem due to the complex spatial relationships inherent involumetric data and the wide variability of abnormalities. Existing methodsbased on 3D convolutional neural networks struggle to capture long-rangedependencies, while Vision Transformers often require extensive pre-training onlarge-scale, domain-specific datasets to perform competitively. In this work ofacademic research, we propose a 2.5D alternative by introducing a newgraph-based framework that represents 3D CT volumes as structured graphs, whereaxial slice triplets serve as nodes processed through spectral graphconvolution, enabling the model to reason over inter-slice dependencies whilemaintaining complexity compatible with clinical deployment. Our method, trainedand evaluated on 3 datasets from independent institutions, achieves strongcross-dataset generalization, and shows competitive performance compared tostate-of-the-art visual encoders. We further conduct comprehensive ablationstudies to evaluate the impact of various aggregation strategies,edge-weighting schemes, and graph connectivity patterns. Additionally, wedemonstrate the broader applicability of our approach through transferexperiments on automated radiology report generation and abdominal CT data.</description><author>Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel</author><pubDate>Thu, 23 Oct 2025 16:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.10779v2</guid></item><item><title>Sampling from multi-modal distributions with polynomial query complexity in fixed dimension via reverse diffusion</title><link>http://arxiv.org/abs/2501.00565v3</link><description>Even in low dimensions, sampling from multi-modal distributions ischallenging. We provide the first sampling algorithm for a broad class ofdistributions -- including all Gaussian mixtures -- with a query complexitythat is polynomial in the parameters governing multi-modality, assuming fixeddimension. Our sampling algorithm simulates a time-reversed diffusion process,using a self-normalized Monte Carlo estimator of the intermediate scorefunctions. Unlike previous works, it avoids metastability, requires no priorknowledge of the mode locations, and relaxes the well-known log-smoothnessassumption which excluded general Gaussian mixtures so far.</description><author>Adrien Vacher, Omar Chehab, Anna Korba</author><pubDate>Thu, 23 Oct 2025 16:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.00565v3</guid></item><item><title>Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models</title><link>http://arxiv.org/abs/2510.20707v1</link><description>Recent large vision-language models (LVLMs) demonstrate remarkablecapabilities in processing extended multi-modal sequences, yet the resultingkey-value (KV) cache expansion creates a critical memory bottleneck thatfundamentally limits deployment scalability. While existing KV cachecompression methods focus on retaining high-importance KV pairs to minimizestorage, they often overlook the modality-specific semantic redundancy patternsthat emerge distinctively in multi-modal KV caches. In this work, we firstanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varyinglevels of redundancy across attention heads. We show that relying solely onimportance can only cover a subset of the full KV cache informationdistribution, leading to potential loss of semantic coverage. To address this,we propose \texttt{MixKV}, a novel method that mixes importance with diversityfor optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wisesemantic redundancy, selectively balancing diversity and importance whencompressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV}consistently enhances existing methods across multiple LVLMs. Under extremecompression (budget=64), \texttt{MixKV} improves baseline methods by an averageof \textbf{5.1\%} across five multi-modal understanding benchmarks and achievesremarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV onGUI grounding tasks, all while maintaining comparable inference efficiency.Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparableperformance gains. Our code is available at\href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.</description><author>Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang</author><pubDate>Thu, 23 Oct 2025 16:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20707v1</guid></item><item><title>Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning</title><link>http://arxiv.org/abs/2510.20706v1</link><description>Model-free reinforcement learning (RL) has enabled adaptable and agilequadruped locomotion; however, policies often converge to a single gait,leading to suboptimal performance. Traditionally, Model Predictive Control(MPC) has been extensively used to obtain task-specific optimal policies butlacks the ability to adapt to varying environments. To address theselimitations, we propose an optimization framework for real-time gait adaptationin a continuous gait space, combining the Model Predictive Path Integral (MPPI)algorithm with a Dreamer module to produce adaptive and optimal policies forquadruped locomotion. At each time step, MPPI jointly optimizes the actions andgait variables using a learned Dreamer reward that promotes velocity tracking,energy efficiency, stability, and smooth transitions, while penalizing abruptgait changes. A learned value function is incorporated as terminal reward,extending the formulation to an infinite-horizon planner. We evaluate ourframework in simulation on the Unitree Go1, demonstrating an average reductionof up to 36.48\% in energy consumption across varying target speeds, whilemaintaining accurate tracking and adaptive, task-appropriate gaits.</description><author>Ganga Nair B, Prakrut Kotecha, Shishir Kolathaya</author><pubDate>Thu, 23 Oct 2025 16:17:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20706v1</guid></item><item><title>On the Emergence of Linear Analogies in Word Embeddings</title><link>http://arxiv.org/abs/2505.18651v2</link><description>Models such as Word2Vec and GloVe construct word embeddings based on theco-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. Theresulting vectors $W_i$ not only group semantically similar words but alsoexhibit a striking linear analogy structure -- for example, $W_{\text{king}} -W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whosetheoretical origin remains unclear. Previous observations indicate that thisanalogy structure: (i) already emerges in the top eigenvectors of the matrix$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as moreeigenvectors of $M (i, j)$, which controls the dimension of the embeddings, areincluded, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and(iv) persists even when all word pairs involved in a specific analogy relation(e.g., king-queen, man-woman) are removed from the corpus. To explain thesephenomena, we introduce a theoretical generative model in which words aredefined by binary semantic attributes, and co-occurrence probabilities arederived from attribute-based interactions. This model analytically reproducesthe emergence of linear analogy structure and naturally accounts for properties(i)-(iv). It can be viewed as giving fine-grained resolution into the role ofeach additional embedding dimension. It is robust to various forms of noise andagrees well with co-occurrence statistics measured on Wikipedia and the analogybenchmark introduced by Mikolov et al.</description><author>Daniel J. Korchinski, Dhruva Karkada, Yasaman Bahri, Matthieu Wyart</author><pubDate>Thu, 23 Oct 2025 16:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.18651v2</guid></item><item><title>Stochastic gradient descent in high dimensions for multi-spiked tensor PCA</title><link>http://arxiv.org/abs/2410.18162v2</link><description>We study the high-dimensional dynamics of online stochastic gradient descent(SGD) for the multi-spiked tensor model. This multi-index model arises from thetensor principal component analysis (PCA) problem with multiple spikes, wherethe goal is to estimate $r$ unknown signal vectors within the $N$-dimensionalunit sphere through maximum likelihood estimation from noisy observations of a$p$-tensor. We determine the number of samples and the conditions on thesignal-to-noise ratios (SNRs) required to efficiently recover the unknownspikes from natural random initializations. We show that full recovery of allspikes is possible provided a number of sample scaling as $N^{p-2}$, matchingthe algorithmic threshold identified in the rank-one case [Ben Arous,Gheissari, Jagannath 2020, 2021]. Our results are obtained through a detailedanalysis of a low-dimensional system that describes the evolution of thecorrelations between the estimators and the spikes, while controlling the noisein the dynamics. We find that the spikes are recovered sequentially in aprocess we term "sequential elimination": once a correlation exceeds a criticalthreshold, all correlations sharing a row or column index become sufficientlysmall, allowing the next correlation to grow and become macroscopic. The orderin which correlations become macroscopic depends on their initial values andthe corresponding SNRs, leading to either exact recovery or recovery of apermutation of the spikes. In the matrix case, when $p=2$, if the SNRs aresufficiently separated, we achieve exact recovery of the spikes, whereas equalSNRs lead to recovery of the subspace spanned by them.</description><author>Gérard Ben Arous, Cédric Gerbelot, Vanessa Piccolo</author><pubDate>Thu, 23 Oct 2025 16:14:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18162v2</guid></item><item><title>Structure-Conditional Minimum Bayes Risk Decoding</title><link>http://arxiv.org/abs/2510.20700v1</link><description>Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternativeto traditional generation strategies. While MBR has proven effective in machinetranslation, where the variability of a language model's outcome space isnaturally constrained, it may face challenges in more open-ended tasks such asdialogue or instruction-following. We hypothesise that in such settings,applying MBR with standard similarity-based utility functions may result inselecting responses that are broadly representative of the model'sdistribution, yet sub-optimal with respect to any particular grouping ofgenerations that share an underlying latent structure. In this work, weintroduce three lightweight adaptations to the utility function, designed tomake MBR more sensitive to structural variability in the outcome space. To testour hypothesis, we curate a dataset capturing three representative types oflatent structure: dialogue act, emotion, and response structure (e.g., asentence, a paragraph, or a list). We further propose two metrics to evaluatethe structural optimality of MBR. Our analysis demonstrates that commonsimilarity-based utility functions fall short by these metrics. In contrast,our proposed adaptations considerably improve structural optimality. Finally,we evaluate our approaches on real-world instruction-following benchmarks,AlpacaEval and MT-Bench, and show that increased structural sensitivityimproves generation quality by up to 13.7 percentage points in win rate.</description><author>Bryan Eikema, Anna Rutkiewicz, Mario Giulianelli</author><pubDate>Thu, 23 Oct 2025 16:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20700v1</guid></item><item><title>Fusing Narrative Semantics for Financial Volatility Forecasting</title><link>http://arxiv.org/abs/2510.20699v1</link><description>We introduce M2VN: Multi-Modal Volatility Network, a novel deeplearning-based framework for financial volatility forecasting that unifies timeseries features with unstructured news data. M2VN leverages therepresentational power of deep neural networks to address two key challenges inthis domain: (i) aligning and fusing heterogeneous data modalities, numericalfinancial data and textual information, and (ii) mitigating look-ahead biasthat can undermine the validity of financial models. To achieve this, M2VNcombines open-source market features with news embeddings generated by TimeMachine GPT, a recently introduced point-in-time LLM, ensuring temporalintegrity. An auxiliary alignment loss is introduced to enhance the integrationof structured and unstructured data within the deep learning architecture.Extensive experiments demonstrate that M2VN consistently outperforms existingbaselines, underscoring its practical value for risk management and financialdecision-making in dynamic markets.</description><author>Yaxuan Kong, Yoontae Hwang, Marcus Kaiser, Chris Vryonides, Roel Oomen, Stefan Zohren</author><pubDate>Thu, 23 Oct 2025 16:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20699v1</guid></item><item><title>FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation</title><link>http://arxiv.org/abs/2504.15958v3</link><description>Subject-driven image generation aims to synthesize novel scenes thatfaithfully preserve subject identity from reference images while adhering totextual guidance. However, existing methods struggle with a critical trade-offbetween fidelity and efficiency. Tuning-based approaches rely on time-consumingand resource-intensive, subject-specific optimization, while zero-shot methodsoften fail to maintain adequate subject consistency. In this work, we proposeFreeGraftor, a training-free framework that addresses these limitations throughcross-image feature grafting. Specifically, FreeGraftor leverages semanticmatching and position-constrained attention fusion to transfer visual detailsfrom reference subjects to the generated images. Additionally, our frameworkintroduces a novel noise initialization strategy to preserve the geometrypriors of reference subjects, facilitating robust feature matching. Extensivequalitative and quantitative experiments demonstrate that our method enablesprecise subject identity transfer while maintaining text-aligned scenesynthesis. Without requiring model fine-tuning or additional training,FreeGraftor significantly outperforms existing zero-shot and training-freeapproaches in both subject fidelity and text alignment. Furthermore, ourframework can seamlessly extend to multi-subject generation, making itpractical for real-world deployment. Our code is available athttps://github.com/Nihukat/FreeGraftor.</description><author>Zebin Yao, Lei Ren, Huixing Jiang, Chen Wei, Xiaojie Wang, Ruifan Li, Fangxiang Feng</author><pubDate>Thu, 23 Oct 2025 16:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.15958v3</guid></item><item><title>Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward</title><link>http://arxiv.org/abs/2510.20696v1</link><description>Multimodal large language models (MLLMs) that integrate visual and textualreasoning leverage chain-of-thought (CoT) prompting to tackle complex visualtasks, yet continue to exhibit visual hallucinations and an over-reliance ontextual priors. We present a systematic diagnosis of state-of-the-artvision-language models using a three-stage evaluation framework, uncovering keyfailure modes. To address these, we propose an agent-based architecture thatcombines LLM reasoning with lightweight visual modules, enabling fine-grainedanalysis and iterative refinement of reasoning chains. Our results highlightfuture visual reasoning models should focus on integrating a broader set ofspecialized tools for analyzing visual content. Our system achieves significantgains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching orsurpassing much larger models. We will release our framework and evaluationsuite to facilitate future research.</description><author>Jing Bi, Guangyu Sun, Ali Vosoughi, Chen Chen, Chenliang Xu</author><pubDate>Thu, 23 Oct 2025 16:10:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20696v1</guid></item><item><title>Superposition Yields Robust Neural Scaling</title><link>http://arxiv.org/abs/2505.10465v3</link><description>The success of today's large language models (LLMs) depends on theobservation that larger models perform better. However, the origin of thisneural scaling law, that loss decreases as a power law with model size, remainsunclear. We propose that representation superposition, meaning that LLMsrepresent more features than they have dimensions, can be a key contributor toloss and cause neural scaling. Based on Anthropic's toy model, we use weightdecay to control the degree of superposition, allowing us to systematicallystudy how loss scales with model size. When superposition is weak, the lossfollows a power law only if data feature frequencies are power-law distributed.In contrast, under strong superposition, the loss generically scales inverselywith model dimension across a broad class of frequency distributions, due togeometric overlaps between representation vectors. We confirmed thatopen-sourced LLMs operate in the strong superposition regime and have lossscaling like one over the model dimension, and that the Chinchilla scaling lawsare also consistent with this behavior. Our results identify representationsuperposition as a central driver of neural scaling laws, providing insightsinto questions like when neural scaling laws can be improved and when they willbreak down.</description><author>Yizhou Liu, Ziming Liu, Jeff Gore</author><pubDate>Thu, 23 Oct 2025 16:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.10465v3</guid></item><item><title>Exploring Large Language Models for Access Control Policy Synthesis and Summarization</title><link>http://arxiv.org/abs/2510.20692v1</link><description>Cloud computing is ubiquitous, with a growing number of services being hostedon the cloud every day. Typical cloud compute systems allow administrators towrite policies implementing access control rules which specify how access toprivate data is governed. These policies must be manually written, and due totheir complexity can often be error prone. Moreover, existing policies oftenimplement complex access control specifications and thus can be difficult toprecisely analyze in determining their behavior works exactly as intended.Recently, Large Language Models (LLMs) have shown great success in automatedcode synthesis and summarization. Given this success, they could potentially beused for automatically generating access control policies or aid inunderstanding existing policies. In this paper, we explore the effectiveness ofLLMs for access control policy synthesis and summarization. Specifically, wefirst investigate diverse LLMs for access control policy synthesis, findingthat: although LLMs can effectively generate syntactically correct policies,they have permissiveness issues, generating policies equivalent to the givenspecification 45.8% of the time for non-reasoning LLMs, and 93.7% of the timefor reasoning LLMs. We then investigate how LLMs can be used to analyzepolicies by introducing a novel semantic-based request summarization approachwhich leverages LLMs to generate a precise characterization of the requestsallowed by a policy. Our results show that while there are significant hurdlesin leveraging LLMs for automated policy generation, LLMs show promising resultswhen combined with symbolic approaches in analyzing existing policies.</description><author>Adarsh Vatsa, Bethel Hall, William Eiers</author><pubDate>Thu, 23 Oct 2025 16:06:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20692v1</guid></item><item><title>Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs</title><link>http://arxiv.org/abs/2510.20691v1</link><description>Knowledge Graph Question Answering aims to answer natural language questionsby reasoning over structured knowledge graphs. While large language models haveadvanced KGQA through their strong reasoning capabilities, existing methodscontinue to struggle to fully exploit both the rich knowledge encoded in KGsand the reasoning capabilities of LLMs, particularly in complex scenarios. Theyoften assume complete KG coverage and lack mechanisms to judge when externalinformation is needed, and their reasoning remains locally myopic, failing tomaintain coherent multi-step planning, leading to reasoning failures even whenrelevant knowledge exists. We propose Graph-RFT, a novel two-stagereinforcement fine-tuning KGQA framework with a'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs toperform autonomous planning and adaptive retrieval scheduling across KG and websources under incomplete knowledge conditions. Graph-RFT introduces achain-of-thought fine-tuning method with a customized plan-retrieval datasetactivates structured reasoning and resolves the GRPO cold-start problem. Itthen introduces a novel plan-retrieval guided reinforcement learning processintegrates explicit planning and retrieval actions with a multi-reward design,enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspiredplanning module to decompose complex questions into ordered subquestions, andlogical expression to guide tool invocation for globally consistent multi-stepreasoning. This reasoning retrieval process is optimized with a multi-rewardcombining outcome and retrieval specific signals, enabling the model to learnwhen and how to combine KG and web retrieval effectively.</description><author>Yanlin Song, Ben Liu, Víctor Gutiérrez-Basulto, Zhiwei Hu, Qianqian Xie, Min Peng, Sophia Ananiadou, Jeff Z. Pan</author><pubDate>Thu, 23 Oct 2025 16:04:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20691v1</guid></item><item><title>Neural Diversity Regularizes Hallucinations in Small Models</title><link>http://arxiv.org/abs/2510.20690v1</link><description>Language models continue to hallucinate despite increases in parameters,compute, and data. We propose neural diversity -- decorrelated parallelrepresentations -- as a principled mechanism that reduces hallucination ratesat fixed parameter and data budgets. Inspired by portfolio theory, whereuncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucinationprobability is bounded by representational correlation: $P(H) \leqf(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that languagemodels need an optimal amount of neurodiversity. To validate this, we introduceND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRAadapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduceshallucinations by up to 25.6% (and 14.6% on average) without degrading generalaccuracy. Ablations show LoRA adapters and regularization act synergistically,causal interventions prove neurodiversity as the mediating factor andcorrelational analyses indicate scale: a 0.1% neural correlation increase isassociated with a 3.8% hallucination increase. Finally, task-dependentoptimality emerges: different tasks require different amounts of optimalneurodiversity. Together, our results highlight neural diversity as a thirdaxis of scaling -- orthogonal to parameters and data -- to improve thereliability of language models at fixed budgets.</description><author>Kushal Chakrabarti, Nirmal Balachundhar</author><pubDate>Thu, 23 Oct 2025 16:03:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20690v1</guid></item><item><title>Flow based approach for Dynamic Temporal Causal models with non-Gaussian or Heteroscedastic Noises</title><link>http://arxiv.org/abs/2506.17065v2</link><description>Understanding causal relationships in multivariate time series is crucial inmany scenarios, such as those dealing with financial or neurological data. Manysuch time series exhibit multiple regimes, i.e., consecutive temporal segmentswith a priori unknown boundaries, with each regime having its own causalstructure. Inferring causal dependencies and regime shifts is critical foranalyzing the underlying processes. However, causal structure learning in thissetting is challenging due to (1) non-stationarity, i.e., each regime can haveits own causal graph and mixing function, and (2) complex noise distributions,which may be nonGaussian or heteroscedastic. Existing causal discoveryapproaches cannot address these challenges, since generally assume stationarityor Gaussian noise with constant variance. Hence, we introduce FANTOM, a unifiedframework for causal discovery that handles non-stationary processes along withnon-Gaussian and heteroscedastic noises. FANTOM simultaneously infers thenumber of regimes and their corresponding indices and learns each regime'sDirected Acyclic Graph. It uses a Bayesian Expectation Maximization algorithmthat maximizes the evidence lower bound of the data log-likelihood. On thetheoretical side, we prove, under mild assumptions, that temporalheteroscedastic causal models, introduced in FANTOM's formulation, areidentifiable in both stationary and non-stationary settings. In addition,extensive experiments on synthetic and real data show that FANTOM outperformsexisting methods.</description><author>Abdellah Rahmani, Pascal Frossard</author><pubDate>Thu, 23 Oct 2025 15:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.17065v2</guid></item><item><title>Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection</title><link>http://arxiv.org/abs/2510.10750v2</link><description>Underwater video monitoring is a promising strategy for assessing marinebiodiversity, but the vast volume of uneventful footage makes manual inspectionhighly impractical. In this work, we explore the use of visual anomalydetection (VAD) based on deep neural networks to automatically identifyinteresting or anomalous events. We introduce AURA, the first multi-annotatorbenchmark dataset for underwater VAD, and evaluate four VAD models across twomarine scenes. We demonstrate the importance of robust frame selectionstrategies to extract meaningful video segments. Our comparison againstmultiple annotators reveals that VAD performance of current models variesdramatically and is highly sensitive to both the amount of training data andthe variability in visual content that defines "normal" scenes. Our resultshighlight the value of soft and consensus labels and offer a practical approachfor supporting scientific exploration and scalable biodiversity monitoring.</description><author>Laura Weihl, Stefan H. Bengtson, Nejc Novak, Malte Pedersen</author><pubDate>Thu, 23 Oct 2025 15:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.10750v2</guid></item><item><title>A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks</title><link>http://arxiv.org/abs/2510.20683v1</link><description>Brain-computer interfaces (BCIs) promise to enable vital functions, such asspeech and prosthetic control, for individuals with neuromotor impairments.Central to their success are neural decoders, models that map neural activityto intended behavior. Current learning-based decoding approaches fall into twoclasses: simple, causal models that lack generalization, or complex, non-causalmodels that generalize and scale offline but struggle in real-time settings.Both face a common challenge, their reliance on power-hungry artificial neuralnetwork backbones, which makes integration into real-world, resource-limitedsystems difficult. Spiking neural networks (SNNs) offer a promisingalternative. Because they operate causally these models are suitable forreal-time use, and their low energy demands make them ideal forbattery-constrained environments. To this end, we introduce Spikachu: ascalable, causal, and energy-efficient neural decoding framework based on SNNs.Our approach processes binned spikes directly by projecting them into a sharedlatent space, where spiking modules, adapted to the timing of the input,extract relevant features; these latent representations are then integrated anddecoded to generate behavioral predictions. We evaluate our approach on 113recording sessions from 6 non-human primates, totaling 43 hours of recordings.Our method outperforms causal baselines when trained on single sessions usingbetween 2.26 and 418.81 times less energy. Furthermore, we demonstrate thatscaling up training to multiple sessions and subjects improves performance andenables few-shot transfer to unseen sessions, subjects, and tasks. Overall,Spikachu introduces a scalable, online-compatible neural decoding frameworkbased on SNNs, whose performance is competitive relative to state-of-the-artmodels while consuming orders of magnitude less energy.</description><author>Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale</author><pubDate>Thu, 23 Oct 2025 15:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20683v1</guid></item><item><title>CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic Learning</title><link>http://arxiv.org/abs/2503.24123v2</link><description>Many computational tasks benefit from being formulated as the composition ofneural networks followed by a discrete symbolic program. The goal ofneurosymbolic learning is to train the neural networks using end-to-endinput-output labels of the composite. We introduce CTSketch, a novel, scalableneurosymbolic learning algorithm. CTSketch uses two techniques to improve thescalability of neurosymbolic inference: decompose the symbolic program intosub-programs and summarize each sub-program with a sketched tensor. Thisstrategy allows us to approximate the output distribution of the program withsimple tensor operations over the input distributions and the sketches. Weprovide theoretical insight into the maximum approximation error. Furthermore,we evaluate CTSketch on benchmarks from the neurosymbolic learning literature,including some designed for evaluating scalability. Our results show thatCTSketch pushes neurosymbolic learning to new scales that were previouslyunattainable, with neural predictors obtaining high accuracy on tasks with onethousand inputs, despite supervision only on the final output.</description><author>Seewon Choi, Alaia Solko-Breslin, Rajeev Alur, Eric Wong</author><pubDate>Thu, 23 Oct 2025 15:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.24123v2</guid></item><item><title>R2-SVC: Towards Real-World Robust and Expressive Zero-shot Singing Voice Conversion</title><link>http://arxiv.org/abs/2510.20677v1</link><description>In real-world singing voice conversion (SVC) applications, environmentalnoise and the demand for expressive output pose significant challenges.Conventional methods, however, are typically designed without accounting forreal deployment scenarios, as both training and inference usually rely on cleandata. This mismatch hinders practical use, given the inevitable presence ofdiverse noise sources and artifacts from music separation. To tackle theseissues, we propose R2-SVC, a robust and expressive SVC framework. First, weintroduce simulation-based robustness enhancement through random fundamentalfrequency ($F_0$) perturbations and music separation artifact simulations(e.g., reverberation, echo), substantially improving performance under noisyconditions. Second, we enrich speaker representation using domain-specificsinging data: alongside clean vocals, we incorporate DNSMOS-filtered separatedvocals and public singing corpora, enabling the model to preserve speakertimbre while capturing singing style nuances. Third, we integrate the NeuralSource-Filter (NSF) model to explicitly represent harmonic and noisecomponents, enhancing the naturalness and controllability of converted singing.R2-SVC achieves state-of-the-art results on multiple SVC benchmarks under bothclean and noisy conditions.</description><author>Junjie Zheng, Gongyu Chen, Chaofan Ding, Zihao Chen</author><pubDate>Thu, 23 Oct 2025 15:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20677v1</guid></item><item><title>From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks</title><link>http://arxiv.org/abs/2502.05325v3</link><description>The advent of Machine Learning as a Service (MLaaS) has heightened thetrade-off between model explainability and security. In particular,explainability techniques, such as counterfactual explanations, inadvertentlyincrease the risk of model extraction attacks, enabling unauthorizedreplication of proprietary models. In this paper, we formalize and characterizethe risks and inherent complexity of model reconstruction, focusing on the"oracle'' queries required for faithfully inferring the underlying predictionfunction. We present the first formal analysis of model extraction attacksthrough the lens of competitive analysis, establishing a foundational frameworkto evaluate their efficiency. Focusing on models based on additive decisiontrees (e.g., decision trees, gradient boosting, and random forests), weintroduce novel reconstruction algorithms that achieve provably perfectfidelity while demonstrating strong anytime performance. Our framework providestheoretical bounds on the query complexity for extracting tree-based model,offering new insights into the security vulnerabilities of their deployment.</description><author>Awa Khouna, Julien Ferry, Thibaut Vidal</author><pubDate>Thu, 23 Oct 2025 15:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.05325v3</guid></item><item><title>Temporal-Difference Variational Continual Learning</title><link>http://arxiv.org/abs/2410.07812v4</link><description>Machine Learning models in real-world applications must continuously learnnew tasks to adapt to shifts in the data-generating distribution. Yet, forContinual Learning (CL), models often struggle to balance learning new tasks(plasticity) with retaining previous knowledge (memory stability).Consequently, they are susceptible to Catastrophic Forgetting, which degradesperformance and undermines the reliability of deployed systems. In the BayesianCL literature, variational methods tackle this challenge by employing alearning objective that recursively updates the posterior distribution whileconstraining it to stay close to its previous estimate. Nonetheless, we arguethat these methods may be ineffective due to compounding approximation errorsover successive recursions. To mitigate this, we propose new learningobjectives that integrate the regularization effects of multiple previousposterior estimations, preventing individual errors from dominating futureposterior updates and compounding over time. We reveal insightful connectionsbetween these objectives and Temporal-Difference methods, a popular learningmechanism in Reinforcement Learning and Neuroscience. Experiments onchallenging CL benchmarks show that our approach effectively mitigatesCatastrophic Forgetting, outperforming strong Variational CL methods.</description><author>Luckeciano C. Melo, Alessandro Abate, Yarin Gal</author><pubDate>Thu, 23 Oct 2025 15:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07812v4</guid></item><item><title>Analyticup E-commerce Product Search Competition Technical Report from Team Tredence_AICOE</title><link>http://arxiv.org/abs/2510.20674v1</link><description>This study presents the multilingual e-commerce search system developed bythe Tredence_AICOE team. The competition features two multilingual relevancetasks: Query-Category (QC) Relevance, which evaluates how well a user's searchquery aligns with a product category, and Query-Item (QI) Relevance, whichmeasures the match between a multilingual search query and an individualproduct listing. To ensure full language coverage, we performed dataaugmentation by translating existing datasets into languages missing from thedevelopment set, enabling training across all target languages. We fine-tunedGemma-3 12B and Qwen-2.5 14B model for both tasks using multiple strategies.The Gemma-3 12B (4-bit) model achieved the best QC performance using originaland translated data, and the best QI performance using original, translated,and minority class data creation. These approaches secured 4th place on thefinal leaderboard, with an average F1-score of 0.8857 on the private test set.</description><author>Rakshith R, Shubham Sharma, Mohammed Sameer Khan, Ankush Chopra</author><pubDate>Thu, 23 Oct 2025 15:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20674v1</guid></item><item><title>Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling</title><link>http://arxiv.org/abs/2510.20673v1</link><description>Multi-bit quantization networks enable flexible deployment of deep neuralnetworks by supporting multiple precision levels within a single model.However, existing approaches suffer from significant training overhead asfull-dataset updates are repeated for each supported bit-width, resulting in acost that scales linearly with the number of precisions. Additionally, extrafine-tuning stages are often required to support additional or intermediateprecision options, further compounding the overall training burden. To addressthis issue, we propose two techniques that greatly reduce the training overheadwithout compromising model utility: (i) Weight bias correction enables sharedbatch normalization and eliminates the need for fine-tuning by neutralizingquantization-induced bias across bit-widths and aligning activationdistributions; and (ii) Bit-wise coreset sampling strategy allows each childmodel to train on a compact, informative subset selected via gradient-basedimportance scores by exploiting the implicit knowledge transfer phenomenon.Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet andViT architectures demonstrate that our method achieves competitive or superioraccuracy while reducing training time up to 7.88x. Our code is released athttps://github.com/a2jinhee/EMQNet_jk.</description><author>Jinhee Kim, Jae Jun An, Kang Eun Jeon, Jong Hwan Ko</author><pubDate>Thu, 23 Oct 2025 15:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20673v1</guid></item><item><title>ReDit: Reward Dithering for Improved LLM Policy Optimization</title><link>http://arxiv.org/abs/2506.18631v3</link><description>DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoningcapabilities through its rule-based reward system. While it's a ''perfect''reward system that effectively mitigates reward hacking, such reward functionsare often discrete. Our experimental observations suggest that discrete rewardscan lead to gradient anomaly, unstable optimization, and slow convergence. Toaddress this issue, we propose ReDit (Reward Dithering), a method that dithersthe discrete reward signal by adding simple random noise. With this perturbedreward, exploratory gradients are continuously provided throughout the learningprocess, enabling smoother gradient updates and accelerating convergence. Theinjected noise also introduces stochasticity into flat reward regions,encouraging the model to explore novel policies and escape local optima.Experiments across diverse tasks demonstrate the effectiveness and efficiencyof ReDit. On average, ReDit achieves performance comparable to vanilla GRPOwith only approximately 10% the training steps, and furthermore, still exhibitsa 4% performance improvement over vanilla GRPO when trained for a similarduration. Visualizations confirm significant mitigation of gradient issues withReDit. Moreover, theoretical analyses are provided to further validate theseadvantages.</description><author>Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu</author><pubDate>Thu, 23 Oct 2025 15:48:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.18631v3</guid></item><item><title>GRACE: GRaph-based Addiction Care prEdiction</title><link>http://arxiv.org/abs/2510.20671v1</link><description>Determining the appropriate locus of care for addiction patients is one ofthe most critical clinical decisions that affects patient treatment outcomesand effective use of resources. With a lack of sufficient specialized treatmentresources, such as inpatient beds or staff, there is an unmet need to developan automated framework for the same. Current decision-making approaches sufferfrom severe class imbalances in addiction datasets. To address this limitation,we propose a novel graph neural network (GRACE) framework that formalizes locusof care prediction as a structured learning problem. Further, we performextensive feature engineering and propose a new approach of obtaining anunbiased meta-graph to train a GNN to overcome the class imbalance problem.Experimental results in real-world data show an improvement of 11-35% in termsof the F1 score of the minority class over competitive baselines. The codes andnote embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.</description><author>Subham Kumar, Prakrithi Shivaprakash, Koustav Rudra, Lekhansh Shukla, Animesh Mukherjee</author><pubDate>Thu, 23 Oct 2025 15:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20671v1</guid></item><item><title>\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding</title><link>http://arxiv.org/abs/2510.20670v1</link><description>Cantonese, although spoken by millions, remains under-resourced due to policyand diglossia. To address this scarcity of evaluation frameworks for Cantonese,we introduce \textsc{\textbf{CantoNLU}}, a benchmark for Cantonese naturallanguage understanding (NLU). This novel benchmark spans seven tasks coveringsyntax and semantics, including word sense disambiguation, linguisticacceptability judgment, language detection, natural language inference,sentiment analysis, part-of-speech tagging, and dependency parsing. In additionto the benchmark, we provide model baseline performance across a set of models:a Mandarin model without Cantonese training, two Cantonese-adapted modelsobtained by continual pre-training a Mandarin model on Cantonese text, and amonolingual Cantonese model trained from scratch. Results show thatCantonese-adapted models perform best overall, while monolingual models performbetter on syntactic tasks. Mandarin models remain competitive in certainsettings, indicating that direct transfer may be sufficient when Cantonesedomain data is scarce. We release all datasets, code, and model weights tofacilitate future research in Cantonese NLP.</description><author>Junghyun Min, York Hay Ng, Sophia Chan, Helena Shunhua Zhao, En-Shiun Annie Lee</author><pubDate>Thu, 23 Oct 2025 15:47:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20670v1</guid></item><item><title>HybridSOMSpikeNet: A Deep Model with Differentiable Soft Self-Organizing Maps and Spiking Dynamics for Waste Classification</title><link>http://arxiv.org/abs/2510.20669v1</link><description>Accurate waste classification is vital for achieving sustainable wastemanagement and reducing the environmental footprint of urbanization.Misclassification of recyclable materials contributes to landfill accumulation,inefficient recycling, and increased greenhouse gas emissions. To address theseissues, this study introduces HybridSOMSpikeNet, a hybrid deep learningframework that integrates convolutional feature extraction, differentiableself-organization, and spiking-inspired temporal processing to enableintelligent and energy-efficient waste classification. The proposed modelemploys a pre-trained ResNet-152 backbone to extract deep spatialrepresentations, followed by a Differentiable Soft Self-Organizing Map(Soft-SOM) that enhances topological clustering and interpretability. A spikingneural head accumulates temporal activations over discrete time steps,improving robustness and generalization. Trained on a ten-class waste dataset,HybridSOMSpikeNet achieved a test accuracy of 97.39%, outperforming severalstate-of-the-art architectures while maintaining a lightweight computationalprofile suitable for real-world deployment. Beyond its technical innovations,the framework provides tangible environmental benefits. By enabling precise andautomated waste segregation, it supports higher recycling efficiency, reducescontamination in recyclable streams, and minimizes the ecological andoperational costs of waste processing. The approach aligns with globalsustainability priorities, particularly the United Nations SustainableDevelopment Goals (SDG 11 and SDG 12), by contributing to cleaner cities,circular economy initiatives, and intelligent environmental management systems.</description><author>Debojyoti Ghosh, Adrijit Goswami</author><pubDate>Thu, 23 Oct 2025 15:47:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20669v1</guid></item><item><title>From Masks to Worlds: A Hitchhiker's Guide to World Models</title><link>http://arxiv.org/abs/2510.20668v1</link><description>This is not a typical survey of world models; it is a guide for those whowant to build worlds. We do not aim to catalog every paper that has evermentioned a ``world model". Instead, we follow one clear road: from earlymasked models that unified representation learning across modalities, tounified architectures that share a single paradigm, then to interactivegenerative models that close the action-perception loop, and finally tomemory-augmented systems that sustain consistent worlds over time. We bypassloosely related branches to focus on the core: the generative heart, theinteractive loop, and the memory system. We show that this is the mostpromising path towards true world models.</description><author>Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, Ming-Hsuan Yang</author><pubDate>Thu, 23 Oct 2025 15:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20668v1</guid></item><item><title>Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts</title><link>http://arxiv.org/abs/2510.20666v1</link><description>Global Navigation Satellite System (GNSS) signals are vulnerable to jamming,particularly in urban areas where multipath and shadowing distort receivedpower. Previous data-driven approaches achieved reasonable localization butpoorly reconstructed the received signal strength (RSS) field due to limitedspatial context. We propose a hybrid Bayesian mixture-of-experts framework thatfuses a physical path-loss (PL) model and a convolutional neural network (CNN)through log-linear pooling. The PL expert ensures physical consistency, whilethe CNN leverages building-height maps to capture urban propagation effects.Bayesian inference with Laplace approximation provides posterior uncertaintyover both the jammer position and RSS field. Experiments on urban ray-tracingdata show that localization accuracy improves and uncertainty decreases withmore training points, while uncertainty concentrates near the jammer and alongurban canyons where propagation is most sensitive.</description><author>Mariona Jaramillo-Civill, Luis González-Gudiño, Tales Imbiriba, Pau Closas</author><pubDate>Thu, 23 Oct 2025 15:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.20666v1</guid></item><item><title>X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation</title><link>http://arxiv.org/abs/2408.15172v2</link><description>Large Language Models (LLMs) have been shown to enhance the effectiveness ofenriching item descriptions, thereby improving the accuracy of recommendationsystems. However, most existing approaches either rely on text-only promptingor employ basic multimodal strategies that do not fully exploit thecomplementary information available from both textual and visual modalities.This paper introduces a novel framework, Cross-Reflection Prompting, termedX-Reflect, designed to address these limitations by prompting Multimodal LargeLanguage Models (MLLMs) to explicitly identify and reconcile supportive andconflicting information between text and images. By capturing nuanced insightsfrom both modalities, this approach generates more comprehensive andcontextually rich item representations. Extensive experiments conducted on twowidely used benchmarks demonstrate that our method outperforms existingprompting baselines in downstream recommendation accuracy. Furthermore, weidentify a U-shaped relationship between text-image dissimilarity andrecommendation performance, suggesting the benefit of applying multimodalprompting selectively. To support efficient real-time inference, we alsointroduce X-Reflect-keyword, a lightweight variant that summarizes imagecontent using keywords and replaces the base model with a smaller backbone,achieving nearly 50% reduction in input length while maintaining competitiveperformance. This work underscores the importance of integrating multimodalinformation and presents an effective solution for improving item understandingin multimodal recommendation systems.</description><author>Hanjia Lyu, Ryan Rossi, Xiang Chen, Md Mehrab Tanjim, Stefano Petrangeli, Somdeb Sarkhel, Jiebo Luo</author><pubDate>Thu, 23 Oct 2025 15:44:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15172v2</guid></item></channel></rss>