<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 17 Sep 2025 13:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>3D Aware Region Prompted Vision Language Model</title><link>http://arxiv.org/abs/2509.13317v1</link><description>We present Spatial Region 3D (SR-3D) aware vision-language model thatconnects single-view 2D images and multi-view 3D data through a shared visualtoken space. SR-3D supports flexible region prompting, allowing users toannotate regions with bounding boxes, segmentation masks on any frame, ordirectly in 3D, without the need for exhaustive multi-frame labeling. Weachieve this by enriching 2D visual features with 3D positional embeddings,which allows the 3D model to draw upon strong 2D priors for more accuratespatial reasoning across frames, even when objects of interest do not co-occurwithin the same view. Extensive experiments on both general 2D vision languageand specialized 3D spatial benchmarks demonstrate that SR-3D achievesstate-of-the-art performance, underscoring its effectiveness for unifying 2Dand 3D representation space on scene understanding. Moreover, we observeapplicability to in-the-wild videos without sensory 3D inputs or ground-truth3D annotations, where SR-3D accurately infers spatial relationships and metricmeasurements.</description><author>An-Chieh Cheng, Yang Fu, Yukang Chen, Zhijian Liu, Xiaolong Li, Subhashree Radhakrishnan, Song Han, Yao Lu, Jan Kautz, Pavlo Molchanov, Hongxu Yin, Xiaolong Wang, Sifei Liu</author><pubDate>Tue, 16 Sep 2025 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13317v1</guid></item><item><title>Do Natural Language Descriptions of Model Activations Convey Privileged Information?</title><link>http://arxiv.org/abs/2509.13316v1</link><description>Recent interpretability methods have proposed to translate LLM internalrepresentations into natural language descriptions using a second verbalizerLLM. This is intended to illuminate how the target model represents andoperates on inputs. But do such activation verbalization approaches actuallyprovide privileged knowledge about the internal workings of the target model,or do they merely convey information about its inputs? We critically evaluatepopular verbalization methods across datasets used in prior work and find thatthey succeed at benchmarks without any access to target model internals,suggesting that these datasets are not ideal for evaluating verbalizationmethods. We then run controlled experiments which reveal that verbalizationsoften reflect the parametric knowledge of the verbalizer LLM which generatedthem, rather than the activations of the target LLM being decoded. Takentogether, our results indicate a need for targeted benchmarks and experimentalcontrols to rigorously assess whether verbalization methods provide meaningfulinsights into the operations of LLMs.</description><author>Millicent Li, Alberto Mario Ceballos Arroyo, Giordano Rogers, Naomi Saphra, Byron C. Wallace</author><pubDate>Tue, 16 Sep 2025 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13316v1</guid></item><item><title>CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction</title><link>http://arxiv.org/abs/2508.04929v2</link><description>As a critical modality for structural biology, cryogenic electron microscopy(cryo-EM) facilitates the determination of macromolecular structures atnear-atomic resolution. The core computational task in single-particle cryo-EMis to reconstruct the 3D electrostatic potential of a molecule from a largecollection of noisy 2D projections acquired at unknown orientations. Gaussianmixture models (GMMs) provide a continuous, compact, and physicallyinterpretable representation for molecular density and have recently gainedinterest in cryo-EM reconstruction. However, existing methods rely on externalconsensus maps or atomic models for initialization, limiting their use inself-contained pipelines. Addressing this issue, we introduce cryoGS, aGMM-based method that integrates Gaussian splatting with the physics of cryo-EMimage formation. In particular, we develop an orthogonal projection-awareGaussian splatting, with adaptations such as a normalization term andFFT-aligned coordinate system tailored for cryo-EM imaging. All theseinnovations enable stable and efficient homogeneous reconstruction directlyfrom raw cryo-EM particle images using random initialization. Experimentalresults on real datasets validate the effectiveness and robustness of cryoGSover representative baselines. The code will be released upon publication.</description><author>Suyi Chen, Haibin Ling</author><pubDate>Tue, 16 Sep 2025 17:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.04929v2</guid></item><item><title>ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization</title><link>http://arxiv.org/abs/2509.13313v1</link><description>Large Language Model (LLM)-based web agents demonstrate strong performance onknowledge-intensive tasks but are hindered by context window limitations inparadigms like ReAct. Complex queries involving multiple entities, intertwinedrelationships, and high uncertainty demand extensive search cycles that rapidlyexhaust context budgets before reaching complete solutions. To overcome thischallenge, we introduce ReSum, a novel paradigm that enables indefiniteexploration through periodic context summarization. ReSum converts growinginteraction histories into compact reasoning states, maintaining awareness ofprior discoveries while bypassing context constraints. For paradigm adaptation,we propose ReSum-GRPO, integrating GRPO with segmented trajectory training andadvantage broadcasting to familiarize agents with summary-conditionedreasoning. Extensive experiments on web agents of varying scales across threebenchmarks demonstrate that ReSum delivers an average absolute improvement of4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPOtraining. Notably, with only 1K training samples, our WebResummer-30B (aReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 onBrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source webagents.</description><author>Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, Jingren Zhou</author><pubDate>Tue, 16 Sep 2025 17:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13313v1</guid></item><item><title>WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research</title><link>http://arxiv.org/abs/2509.13312v1</link><description>This paper tackles open-ended deep research (OEDR), a complex challenge whereAI agents must synthesize vast web-scale information into insightful reports.Current approaches are plagued by dual-fold limitations: static researchpipelines that decouple planning from evidence acquisition and one-shotgeneration paradigms that easily suffer from long-context failure issues like"loss in the middle" and hallucinations. To address these challenges, weintroduce WebWeaver, a novel dual-agent framework that emulates the humanresearch process. The planner operates in a dynamic cycle, iterativelyinterleaving evidence acquisition with outline optimization to produce acomprehensive, source-grounded outline linking to a memory bank of evidence.The writer then executes a hierarchical retrieval and writing process,composing the report section by section. By performing targeted retrieval ofonly the necessary evidence from the memory bank for each part, it effectivelymitigates long-context issues. Our framework establishes a new state-of-the-artacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, andDeepResearchGym. These results validate our human-centric, iterativemethodology, demonstrating that adaptive planning and focused synthesis arecrucial for producing high-quality, reliable, and well-structured reports.</description><author>Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, Jun Zhang, Jingren Zhou</author><pubDate>Tue, 16 Sep 2025 17:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13312v1</guid></item><item><title>Towards General Agentic Intelligence via Environment Scaling</title><link>http://arxiv.org/abs/2509.13311v1</link><description>Advanced agentic intelligence is a prerequisite for deploying Large LanguageModels in practical, real-world applications. Diverse real-world APIs demandprecise, robust function-calling intelligence, which needs agents to developthese capabilities through interaction in varied environments. The breadth offunction-calling competence is closely tied to the diversity of environments inwhich agents are trained. In this work, we scale up environments as a steptowards advancing general agentic intelligence. This gives rise to two centralchallenges: (i) how to scale environments in a principled manner, and (ii) howto effectively train agentic capabilities from experiences derived throughinteractions with these environments. To address these, we design a scalableframework that automatically constructs heterogeneous environments that arefully simulated, systematically broadening the space of function-callingscenarios. We further adapt a two-phase agent fine-tuning strategy: firstendowing agents with fundamental agentic capabilities, then specializing themfor domain-specific contexts. Extensive experiments on agentic benchmarks,tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,AgentScaler, significantly enhances the function-calling capability of models.</description><author>Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</author><pubDate>Tue, 16 Sep 2025 17:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13311v1</guid></item><item><title>Scaling Agents via Continual Pre-training</title><link>http://arxiv.org/abs/2509.13310v1</link><description>Large language models (LLMs) have evolved into agentic systems capable ofautonomous tool use and multi-step reasoning for complex problem-solving.However, post-training approaches building upon general-purpose foundationmodels consistently underperform in agentic tasks, particularly in open-sourceimplementations. We identify the root cause: the absence of robust agenticfoundation models forces models during post-training to simultaneously learndiverse agentic behaviors while aligning them to expert demonstrations, therebycreating fundamental optimization tensions. To this end, we are the first topropose incorporating Agentic Continual Pre-training (Agentic CPT) into thedeep research agents training pipeline to build powerful agentic foundationalmodels. Based on this approach, we develop a deep research agent model namedAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achievestate-of-the-art performance while retains strong tool-use ability, notably39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.</description><author>Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</author><pubDate>Tue, 16 Sep 2025 17:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13310v1</guid></item><item><title>WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents</title><link>http://arxiv.org/abs/2509.13309v1</link><description>Recent advances in deep-research systems have demonstrated the potential forAI agents to autonomously discover and synthesize knowledge from externalsources. In this paper, we introduce WebResearcher, a novel framework forbuilding such agents through two key components: (1) WebResearcher, aniterative deep-research paradigm that reformulates deep research as a MarkovDecision Process, where agents periodically consolidate findings into evolvingreports while maintaining focused workspaces, overcoming the contextsuffocation and noise contamination that plague existing mono-contextualapproaches; and (2) WebFrontier, a scalable data synthesis engine thatgenerates high-quality training data through tool-augmented complexityescalation, enabling systematic creation of research tasks that bridge the gapbetween passive knowledge recall and active knowledge construction. Notably, wefind that the training data from our paradigm significantly enhances tool-usecapabilities even for traditional mono-contextual methods. Furthermore, ourparadigm naturally scales through parallel thinking, enabling concurrentmulti-agent exploration for more comprehensive conclusions. Extensiveexperiments across 6 challenging benchmarks demonstrate that WebResearcherachieves state-of-the-art performance, even surpassing frontier proprietarysystems.</description><author>Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</author><pubDate>Tue, 16 Sep 2025 17:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13309v1</guid></item><item><title>WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning</title><link>http://arxiv.org/abs/2509.13305v1</link><description>Transcending human cognitive limitations represents a critical frontier inLLM training. Proprietary agentic systems like DeepResearch have demonstratedsuperhuman capabilities on extremely complex information-seeking benchmarkssuch as BrowseComp, a feat previously unattainable. We posit that their successhinges on a sophisticated reasoning pattern absent in open-source models: theability to systematically reduce extreme uncertainty when navigating vastinformation landscapes. Based on this insight, we introduce WebSailor, acomplete post-training methodology designed to instill this crucial capability.Our approach involves generating novel, high-uncertainty tasks throughstructured sampling and information obfuscation, RFT cold start, and anefficient agentic RL training algorithm, Duplicating Sampling PolicyOptimization (DUPO). With this integrated pipeline, WebSailor significantlyoutperforms all open-source agents in complex information-seeking tasks,matching proprietary agents' performance and closing the capability gap.</description><author>Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</author><pubDate>Tue, 16 Sep 2025 17:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13305v1</guid></item><item><title>StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance</title><link>http://arxiv.org/abs/2509.13301v1</link><description>Creating 3D assets that follow the texture and geometry style of existingones is often desirable or even inevitable in practical applications like videogaming and virtual reality. While impressive progress has been made ingenerating 3D objects from text or images, creating style-controllable 3Dassets remains a complex and challenging problem. In this work, we proposeStyleSculptor, a novel training-free approach for generating style-guided 3Dassets from a content image and one or more style images. Unlike previousworks, StyleSculptor achieves style-guided 3D generation in a zero-shot manner,enabling fine-grained 3D style control that captures the texture, geometry, orboth styles of user-provided style images. At the core of StyleSculptor is anovel Style Disentangled Attention (SD-Attn) module, which establishes adynamic interaction between the input content image and style image forstyle-guided 3D asset generation via a cross-3D attention mechanism, enablingstable feature fusion and effective style-guided generation. To alleviatesemantic content leakage, we also introduce a style-disentangled featureselection strategy within the SD-Attn module, which leverages the variance of3D feature patches to disentangle style- and content-significant channels,allowing selective feature injection within the attention framework. WithSD-Attn, the network can dynamically compute texture-, geometry-, orboth-guided features to steer the 3D generation process. Built upon this, wefurther propose the Style Guided Control (SGC) mechanism, which enablesexclusive geometry- or texture-only stylization, as well as adjustable styleintensity control. Extensive experiments demonstrate that StyleSculptoroutperforms existing baseline methods in producing high-fidelity 3D assets.</description><author>Zefan Qu, Zhenwei Wang, Haoyuan Wang, Ke Xu, Gerhard Hancke, Rynson W. H. Lau</author><pubDate>Tue, 16 Sep 2025 17:55:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13301v1</guid></item><item><title>QDFlow: A Python package for physics simulations of quantum dot devices</title><link>http://arxiv.org/abs/2509.13298v1</link><description>Recent advances in machine learning (ML) have accelerated progress incalibrating and operating quantum dot (QD) devices. However, most ML approachesrely on access to large, high-quality labeled datasets for training,benchmarking, and validation, with labels capturing key features in the data.Obtaining such datasets experimentally is challenging due to limited dataavailability and the labor-intensive nature of labeling. QDFlow is anopen-source physics simulator for multi-QD arrays that generates realisticsynthetic data with ground-truth labels. QDFlow combines a self-consistentThomas-Fermi solver, a dynamic capacitance model, and flexible noise modules toproduce charge stability diagrams and ray-based data closely resemblingexperiments. With extensive tunable parameters and customizable noise models,QDFlow supports the creation of large, diverse datasets for ML development,benchmarking, and quantum device research.</description><author>Donovan L. Buterakos, Sandesh S. Kalantre, Joshua Ziegler, Jacob M Taylor, Justyna P. Zwolak</author><pubDate>Tue, 16 Sep 2025 17:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13298v1</guid></item><item><title>Do predictability factors towards signing avatars hold across cultures?</title><link>http://arxiv.org/abs/2307.02103v2</link><description>Avatar technology can offer accessibility possibilities and improve theDeaf-and-Hard of Hearing sign language users access to communication, educationand services, such as the healthcare system. However, sign language usersacceptance of signing avatars as well as their attitudes towards them vary anddepend on many factors. Furthermore, research on avatar technology is mostlydone by researchers who are not Deaf. The study examines the extent to whichintrinsic or extrinsic factors contribute to predict the attitude towardsavatars across cultures. Intrinsic factors include the characteristics of theavatar, such as appearance, movements and facial expressions. Extrinsic factorsinclude users technology experience, their hearing status, age and their signlanguage fluency. This work attempts to answer questions such as, if lowerattitude ratings are related to poor technology experience with ASL users, forexample, is that also true for Moroccan Sign Language (MSL) users? For thepurposes of the study, we designed a questionnaire to understand MSL usersattitude towards avatars. Three groups of participants were surveyed: Deaf(57), Hearing (20) and Hard-of-Hearing (3). The results of our study were thencompared with those reported in other relevant studies.</description><author>Abdelhadi Soudi, Manal El Hakkaoui, Kristof Van Laerhoven</author><pubDate>Tue, 16 Sep 2025 17:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02103v2</guid></item><item><title>Accelerating Protein Molecular Dynamics Simulation with DeepJump</title><link>http://arxiv.org/abs/2509.13294v1</link><description>Unraveling the dynamical motions of biomolecules is essential for bridgingtheir structure and function, yet it remains a major computational challenge.Molecular dynamics (MD) simulation provides a detailed depiction ofbiomolecular motion, but its high-resolution temporal evolution comes atsignificant computational cost, limiting its applicability to timescales ofbiological relevance. Deep learning approaches have emerged as promisingsolutions to overcome these computational limitations by learning to predictlong-timescale dynamics. However, generalizable kinetics models for proteinsremain largely unexplored, and the fundamental limits of achievableacceleration while preserving dynamical accuracy are poorly understood. In thiswork, we fill this gap with DeepJump, an Euclidean-Equivariant FlowMatching-based model for predicting protein conformational dynamics acrossmultiple temporal scales. We train DeepJump on trajectories of the diverseproteins of mdCATH, systematically studying our model's performance ingeneralizing to long-term dynamics of fast-folding proteins and characterizingthe trade-off between computational acceleration and prediction accuracy. Wedemonstrate the application of DeepJump to ab initio folding, showcasingprediction of folding pathways and native states. Our results demonstrate thatDeepJump achieves significant $\approx$1000$\times$ computational accelerationwhile effectively recovering long-timescale dynamics, providing a steppingstone for enabling routine simulation of proteins.</description><author>Allan dos Santos Costa, Manvitha Ponnapati, Dana Rubin, Tess Smidt, Joseph Jacobson</author><pubDate>Tue, 16 Sep 2025 17:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13294v1</guid></item><item><title>JoPA:Explaining Large Language Model's Generation via Joint Prompt Attribution</title><link>http://arxiv.org/abs/2405.20404v3</link><description>Large Language Models (LLMs) have demonstrated impressive performances incomplex text generation tasks. However, the contribution of the input prompt tothe generated content still remains obscure to humans, underscoring thenecessity of understanding the causality between input and output pairs.Existing works for providing prompt-specific explanation often confine modeloutput to be classification or next-word prediction. Few initial attemptsaiming to explain the entire language generation often treat input prompt textsindependently, ignoring their combinatorial effects on the follow-upgeneration. In this study, we introduce a counterfactual explanation frameworkbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompttexts collaboratively influences the LLM's complete generation. Particularly,we formulate the task of prompt attribution for generation interpretation as acombinatorial optimization problem, and introduce a probabilistic algorithm tosearch for the casual input combination in the discrete space. We define andutilize multiple metrics to evaluate the produced explanations, demonstratingboth the faithfulness and efficiency of our framework.</description><author>Yurui Chang, Bochuan Cao, Yujia Wang, Jinghui Chen, Lu Lin</author><pubDate>Tue, 16 Sep 2025 17:48:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20404v3</guid></item><item><title>Image Realness Assessment and Localization with Multimodal Features</title><link>http://arxiv.org/abs/2509.13289v1</link><description>A reliable method of quantifying the perceptual realness of AI-generatedimages and identifying visually inconsistent regions is crucial for practicaluse of AI-generated images and for improving photorealism of generative AI viarealness feedback during training. This paper introduces a framework thataccomplishes both overall objective realness assessment and local inconsistencyidentification of AI-generated images using textual descriptions of visualinconsistencies generated by vision-language models trained on large datasetsthat serve as reliable substitutes for human annotations. Our resultsdemonstrate that the proposed multimodal approach improves objective realnessprediction performance and produces dense realness maps that effectivelydistinguish between realistic and unrealistic spatial regions.</description><author>Lovish Kaushik, Agnij Biswas, Somdyuti Paul</author><pubDate>Tue, 16 Sep 2025 17:42:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13289v1</guid></item><item><title>Shapes of Cognition for Computational Cognitive Modeling</title><link>http://arxiv.org/abs/2509.13288v1</link><description>Shapes of cognition is a new conceptual paradigm for the computationalcognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes areremembered constellations of sensory, linguistic, conceptual, episodic, andprocedural knowledge that allow agents to cut through the complexity of reallife the same way as people do: by expecting things to be typical, recognizingpatterns, acting by habit, reasoning by analogy, satisficing, and generallyminimizing cognitive load to the degree situations permit. Atypical outcomesare treated using shapes-based recovery methods, such as learning on the fly,asking a human partner for help, or seeking an actionable, even if imperfect,situational understanding. Although shapes is an umbrella term, it is notvague: shapes-based modeling involves particular objectives, hypotheses,modeling strategies, knowledge bases, and actual models of wide-rangingphenomena, all implemented within a particular cognitive architecture. Suchspecificity is needed both to vet our hypotheses and to achieve our practicalaims of building useful agent systems that are explainable, extensible, andworthy of our trust, even in critical domains. However, although the LEIAexample of shapes-based modeling is specific, the principles can be appliedmore broadly, giving new life to knowledge-based and hybrid AI.</description><author>Marjorie McShane, Sergei Nirenburg, Sanjay Oruganti, Jesse English</author><pubDate>Tue, 16 Sep 2025 17:39:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13288v1</guid></item><item><title>Contrastive timbre representations for musical instrument and synthesizer retrieval</title><link>http://arxiv.org/abs/2509.13285v1</link><description>Efficiently retrieving specific instrument timbres from audio mixturesremains a challenge in digital music production. This paper introduces acontrastive learning framework for musical instrument retrieval, enablingdirect querying of instrument databases using a single model for both single-and multi-instrument sounds. We propose techniques to generate realisticpositive/negative pairs of sounds for virtual musical instruments, such assamplers and synthesizers, addressing limitations in common audio dataaugmentation methods. The first experiment focuses on instrument retrieval from a dataset of 3,884instruments, using single-instrument audio as input. Contrastive approaches arecompetitive with previous works based on classification pre-training. Thesecond experiment considers multi-instrument retrieval with a mixture ofinstruments as audio input. In this case, the proposed contrastive frameworkoutperforms related works, achieving 81.7\% top-1 and 95.7\% top-5 accuraciesfor three-instrument mixtures.</description><author>Gwendal Le Vaillant, Yannick Molle</author><pubDate>Tue, 16 Sep 2025 17:38:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13285v1</guid></item><item><title>ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement</title><link>http://arxiv.org/abs/2509.13282v1</link><description>Charts are a crucial visual medium for communicating and representinginformation. While Large Vision-Language Models (LVLMs) have made progress onchart question answering (CQA), the task remains challenging, particularly whenmodels attend to irrelevant regions of the chart. In this work, we presentChartGaze, a new eye-tracking dataset that captures human gaze patterns duringchart reasoning tasks. Through a systematic comparison of human and modelattention, we find that LVLMs often diverge from human gaze, leading to reducedinterpretability and accuracy. To address this, we propose a gaze-guidedattention refinement that aligns image-text attention with human fixations. Ourapproach improves both answer accuracy and attention alignment, yielding gainsof up to 2.56 percentage points across multiple models. These resultsdemonstrate the promise of incorporating human gaze to enhance both thereasoning quality and interpretability of chart-focused LVLMs.</description><author>Ali Salamatian, Amirhossein Abaskohi, Wan-Cyuan Fan, Mir Rayat Imtiaz Hossain, Leonid Sigal, Giuseppe Carenini</author><pubDate>Tue, 16 Sep 2025 17:35:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13282v1</guid></item><item><title>RepIt: Representing Isolated Targets to Steer Language Models</title><link>http://arxiv.org/abs/2509.13281v1</link><description>While activation steering in large language models (LLMs) is a growing areaof research, methods can often incur broader effects than desired. Thismotivates isolation of purer concept vectors to enable targeted interventionsand understand LLM behavior at a more granular level. We present RepIt, asimple and data-efficient framework for isolating concept-specificrepresentations. Across five frontier LLMs, RepIt enables preciseinterventions: it selectively suppresses refusal on targeted concepts whilepreserving refusal elsewhere, producing models that answer WMD-relatedquestions while still scoring as safe on standard benchmarks. We further showthat the corrective signal localizes to just 100-200 neurons and that robusttarget representations can be extracted from as few as a dozen examples on asingle A6000. This efficiency raises a dual concern: manipulations can beperformed with modest compute and data to extend to underrepresenteddata-scarce topics while evading existing benchmarks. By disentangling refusalvectors with RepIt, this work demonstrates that targeted interventions cancounteract overgeneralization, laying the foundation for more granular controlof model behavior.</description><author>Vincent Siu, Nathan W. Henry, Nicholas Crispino, Yang Liu, Dawn Song, Chenguang Wang</author><pubDate>Tue, 16 Sep 2025 17:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13281v1</guid></item><item><title>HARMONIC: A Content-Centric Cognitive Robotic Architecture</title><link>http://arxiv.org/abs/2509.13279v1</link><description>This paper introduces HARMONIC, a cognitive-robotic architecture designed forrobots in human-robotic teams. HARMONIC supports semantic perceptioninterpretation, human-like decision-making, and intentional languagecommunication. It addresses the issues of safety and quality of results; aimsto solve problems of data scarcity, explainability, and safety; and promotestransparency and trust. Two proof-of-concept HARMONIC-based robotic systems aredemonstrated, each implemented in both a high-fidelity simulation environmentand on physical robotic platforms.</description><author>Sanjay Oruganti, Sergei Nirenburg, Marjorie McShane, Jesse English, Michael K. Roberts, Christian Arndt, Carlos Gonzalez, Mingyo Seo, Luis Sentis</author><pubDate>Tue, 16 Sep 2025 17:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13279v1</guid></item><item><title>OGF: An Online Gradient Flow Method for Optimizing the Statistical Steady-State Time Averages of Unsteady Turbulent Flows</title><link>http://arxiv.org/abs/2507.05149v2</link><description>Turbulent flows are chaotic and unsteady, but their statistical distributionconverges to a statistical steady state. Engineering quantities of interesttypically take the form of time-average statistics such as $ \frac{1}{t}\int_0^t f ( u(x,\tau; \theta) ) d\tau \overset{t \rightarrow\infty}{\rightarrow} F(x; \theta)$, where $u(x,t; \theta)$ are solutions of theNavier--Stokes equations with parameters $\theta$. Optimizing over $F(x;\theta)$ has many engineering applications including geometric optimization,flow control, and closure modeling. However, this remains an open challenge, asexisting computational approaches are incapable of scaling to physicallyrepresentative numbers of grid points. The fundamental obstacle is thechaoticity of turbulent flows: gradients calculated with the adjoint methoddiverge exponentially as $t \rightarrow \infty$. We develop a new online gradient-flow (OGF) method that is scalable to largedegree-of-freedom systems and enables optimizing for the steady-statestatistics of chaotic, unsteady, turbulence-resolving simulations. The methodforward-propagates an online estimate for the gradient of $F(x; \theta)$ whilesimultaneously performing online updates of the parameters $\theta$. A keyfeature is the fully online nature of the algorithm to facilitate fasteroptimization progress and its combination with a finite-difference estimator toavoid the divergence of gradients due to chaoticity. The proposed OGF method isdemonstrated for optimizations over three chaotic ordinary and partialdifferential equations: the Lorenz-63 equation, the Kuramoto--Sivashinskyequation, and Navier--Stokes solutions of compressible, forced, homogeneousisotropic turbulence. In each case, the OGF method successfully reduces theloss based on $F(x; \theta)$ by several orders of magnitude and accuratelyrecovers the optimal parameters.</description><author>Tom Hickling, Jonathan F. MacArt, Justin Sirignano, Den Waidmann</author><pubDate>Tue, 16 Sep 2025 17:29:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05149v2</guid></item><item><title>RadGame: An AI-Powered Platform for Radiology Education</title><link>http://arxiv.org/abs/2509.13270v1</link><description>We introduce RadGame, an AI-powered gamified platform for radiology educationthat targets two core skills: localizing findings and generating reports.Traditional radiology training is based on passive exposure to cases or activepractice with real-time input from supervising radiologists, limitingopportunities for immediate and scalable feedback. RadGame addresses this gapby combining gamification with large-scale public datasets and automated,AI-driven feedback that provides clear, structured guidance to human learners.In RadGame Localize, players draw bounding boxes around abnormalities, whichare automatically compared to radiologist-drawn annotations from publicdatasets, and visual explanations are generated by vision-language models foruser missed findings. In RadGame Report, players compose findings given a chestX-ray, patient age and indication, and receive structured AI feedback based onradiology report generation metrics, highlighting errors and omissions comparedto a radiologist's written ground truth report from public datasets, producinga final performance and style score. In a prospective evaluation, participantsusing RadGame achieved a 68% improvement in localization accuracy compared to17% with traditional passive methods and a 31% improvement in report-writingaccuracy compared to 4% with traditional methods after seeing the same cases.RadGame highlights the potential of AI-driven gamification to deliver scalable,feedback-rich radiology training and reimagines the application of medical AIresources in education.</description><author>Mohammed Baharoon, Siavash Raissi, John S. Jun, Thibault Heintz, Mahmoud Alabbad, Ali Alburkani, Sung Eun Kim, Kent Kleinschmidt, Abdulrahman O. Alhumaydhi, Mohannad Mohammed G. Alghamdi, Jeremy Francis Palacio, Mohammed Bukhaytan, Noah Michael Prudlo, Rithvik Akula, Brady Chrisler, Benjamin Galligos, Mohammed O. Almutairi, Mazeen Mohammed Alanazi, Nasser M. Alrashdi, Joel Jihwan Hwang, Sri Sai Dinesh Jaliparthi, Luke David Nelson, Nathaniel Nguyen, Sathvik Suryadevara, Steven Kim, Mohammed F. Mohammed, Yevgeniy R. Semenov, Kun-Hsing Yu, Abdulrhman Aljouie, Hassan AlOmaish, Adam Rodman, Pranav Rajpurkar</author><pubDate>Tue, 16 Sep 2025 17:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13270v1</guid></item><item><title>LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt</title><link>http://arxiv.org/abs/2509.13268v1</link><description>BACKGROUND: Most artificial intelligence tools used to estimate nutritionalcontent rely on image input. However, whether large language models (LLMs) canaccurately predict nutritional values based solely on text descriptions offoods consumed remains unknown. If effective, this approach could enablesimpler dietary monitoring without the need for photographs. METHODS: We used24-hour dietary recalls from adolescents aged 12-19 years in the NationalHealth and Nutrition Examination Survey (NHANES). An open-source quantized LLMwas prompted using a 10-shot, chain-of-thought approach to estimate energy andfive macronutrients based solely on text strings listing foods and theirquantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluatewhether predictive accuracy improved. NHANES-calculated values served as theground truth for energy, proteins, carbohydrates, total sugar, dietary fiberand total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,mean age 15.4 years), the vanilla LLM yielded poor predictions. The meanabsolute error (MAE) was 652.08 for energy and the Lin's CCC &lt;0.46 acrossendpoints. In contrast, the fine-tuned model performed substantially better,with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCCexceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using achain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposedsolely to text input can accurately predict energy and macronutrient valuesfrom 24-hour dietary recalls. This approach holds promise for low-burden,text-based dietary monitoring tools.</description><author>Rodrigo M Carrillo-Larco</author><pubDate>Tue, 16 Sep 2025 17:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13268v1</guid></item><item><title>Learning Discrete Bayesian Networks with Hierarchical Dirichlet Shrinkage</title><link>http://arxiv.org/abs/2509.13267v1</link><description>Discrete Bayesian networks (DBNs) provide a broadly useful framework formodeling dependence structures in multivariate categorical data. There is avast literature on methods for inferring conditional probabilities andgraphical structure in DBNs, but data sparsity and parametric assumptions aremajor practical issues. In this article, we detail a comprehensive Bayesianframework for learning DBNs. First, we propose a hierarchical prior for theconditional probabilities that enables complicated interactions between parentvariables and stability in sparse regimes. We give a novel Markov chain MonteCarlo (MCMC) algorithm utilizing parallel Langevin proposals to generate exactposterior samples, avoiding the pitfalls of variational approximations.Moreover, we verify that the full conditional distribution of the concentrationparameters is log-concave under mild conditions, facilitating efficientsampling. We then propose two methods for learning network structures,including parent sets, Markov blankets, and DAGs, from categorical data. Thefirst cycles through individual edges each MCMC iteration, whereas the secondupdates the entire structure as a single step. We evaluate the accuracy, power,and MCMC performance of our methods on several simulation studies. Finally, weapply our methodology to uncover prognostic network structure from primarybreast cancer samples.</description><author>Alexander Dombowsky, David B. Dunson</author><pubDate>Tue, 16 Sep 2025 17:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13267v1</guid></item><item><title>JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks</title><link>http://arxiv.org/abs/2509.13266v1</link><description>Graph Neural Networks (GNNs) have demonstrated remarkable performance acrossvarious applications, yet they are vulnerable to sophisticated adversarialattacks, particularly node injection attacks. The success of such attacksheavily relies on their stealthiness, the ability to blend in with the originalgraph and evade detection. However, existing methods often achieve stealthinessby relying on indirect proxy metrics, lacking consideration for the fundamentalcharacteristics of the injected content, or focusing only on imitating localstructures, which leads to the problem of local myopia. To overcome theselimitations, we propose a dual-constraint stealthy node injection framework,called Joint Alignment of Nodal and Universal Structures (JANUS). At the locallevel, we introduce a local feature manifold alignment strategy to achievegeometric consistency in the feature space. At the global level, we incorporatestructured latent variables and maximize the mutual information with thegenerated structures, ensuring the injected structures are consistent with thesemantic patterns of the original graph. We model the injection attack as asequential decision process, which is optimized by a reinforcement learningagent. Experiments on multiple standard datasets demonstrate that the JANUSframework significantly outperforms existing methods in terms of both attackeffectiveness and stealthiness.</description><author>Jiahao Zhang, Xiaobing Pei, Zhaokun Zhong, Wenqiang Hao, Zhenghao Tang</author><pubDate>Tue, 16 Sep 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13266v1</guid></item><item><title>Real-time, Adaptive Radiological Anomaly Detection and Isotope Identification Using Non-negative Matrix Factorization</title><link>http://arxiv.org/abs/2507.10715v2</link><description>Spectroscopic anomaly detection and isotope identification algorithms areintegral components in nuclear nonproliferation applications such as searchoperations. The task is especially challenging in the case of mobile detectorsystems due to the fact that the observed gamma-ray background changes morethan for a static detector system, and a pretrained background model can easilyfind itself out of domain. The result is that algorithms may exceed theirintended false alarm rate, or sacrifice detection sensitivity in order tomaintain the desired false alarm rate. Non-negative matrix factorization (NMF)has been shown to be a powerful tool for spectral anomaly detection andidentification, but, like many similar algorithms that rely on data-drivenbackground models, in its conventional implementation it is unable to update inreal time to account for environmental changes that affect the backgroundspectroscopic signature. We have developed a novel NMF-based algorithm thatperiodically updates its background model to accommodate changing environmentalconditions. The Adaptive NMF algorithm involves fewer assumptions about itsenvironment, making it more generalizable than existing NMF-based methods whilemaintaining or exceeding detection performance on simulated and real-worlddatasets.</description><author>Chandler Jones, Mark Bandstra, Stefan Faaland, Yue Shi Lai, Nico Abgrall, Scott Suchyta, Reynold Cooper</author><pubDate>Tue, 16 Sep 2025 17:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.10715v2</guid></item><item><title>Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning</title><link>http://arxiv.org/abs/2509.13262v1</link><description>Uncertainty quantification (UQ) is vital for trustworthy deep learning, yetexisting methods are either computationally intensive, such as Bayesian orensemble methods, or provide only partial, task-specific estimates, such assingle-forward-pass techniques. In this paper, we propose a post-hocsingle-forward-pass framework that jointly captures aleatoric and epistemicuncertainty without modifying or retraining pretrained models. Our methodapplies \emph{Split-Point Analysis} (SPA) to decompose predictive residualsinto upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs)on each side. We prove that, under ideal conditions, the total MAR equals theharmonic mean of subset MARs; deviations define a novel \emph{Self-consistencyDiscrepancy Score} (SDS) for fine-grained epistemic estimation acrossregression and classification. For regression, side-specific quantileregression yields prediction intervals with improved empirical coverage, whichare further calibrated via SDS. For classification, when calibration data areavailable, we apply SPA-based calibration identities to adjust the softmaxoutputs and then compute predictive entropy on these calibrated probabilities.Extensive experiments on diverse regression and classification benchmarksdemonstrate that our framework matches or exceeds several state-of-the-art UQmethods while incurring minimal overhead. Our source code is available at https://github.com/zzz0527/SPC-UQ.</description><author>Zhizhong Zhao, Ke Chen</author><pubDate>Tue, 16 Sep 2025 17:16:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13262v1</guid></item><item><title>ResidualViT for Efficient Temporally Dense Video Encoding</title><link>http://arxiv.org/abs/2509.13255v1</link><description>Several video understanding tasks, such as natural language temporal videogrounding, temporal activity localization, and audio description generation,require "temporally dense" reasoning over frames sampled at high temporalresolution. However, computing frame-level features for these tasks iscomputationally expensive given the temporal resolution requirements. In thispaper, we make three contributions to reduce the cost of computing features fortemporally dense tasks. First, we introduce a vision transformer (ViT)architecture, dubbed ResidualViT, that leverages the large temporal redundancyin videos to efficiently compute temporally dense frame-level features. Ourarchitecture incorporates (i) learnable residual connections that ensuretemporal consistency across consecutive frames and (ii) a token reductionmodule that enhances processing speed by selectively discarding temporallyredundant information while reusing weights of a pretrained foundation model.Second, we propose a lightweight distillation strategy to approximate theframe-level features of the original foundation model. Finally, we evaluate ourapproach across four tasks and five datasets, in both zero-shot and fullysupervised settings, demonstrating significant reductions in computational cost(up to 60%) and improvements in inference speed (up to 2.5x faster), all whileclosely approximating the accuracy of the original foundation model.</description><author>Mattia Soldan, Fabian Caba Heilbron, Bernard Ghanem, Josef Sivic, Bryan Russell</author><pubDate>Tue, 16 Sep 2025 17:12:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13255v1</guid></item><item><title>Learning from a Biased Sample</title><link>http://arxiv.org/abs/2209.01754v4</link><description>The empirical risk minimization approach to data-driven decision makingrequires access to training data drawn under the same conditions as those thatwill be faced when the decision rule is deployed. However, in a number ofsettings, we may be concerned that our training sample is biased in the sensethat some groups (characterized by either observable or unobservableattributes) may be under- or over-represented relative to the generalpopulation; and in this setting empirical risk minimization over the trainingset may fail to yield rules that perform well at deployment. We propose a modelof sampling bias called conditional $\Gamma$-biased sampling, where observedcovariates can affect the probability of sample selection arbitrarily much butthe amount of unexplained variation in the probability of sample selection isbounded by a constant factor. Applying the distributionally robust optimizationframework, we propose a method for learning a decision rule that minimizes theworst-case risk incurred under a family of test distributions that can generatethe training distribution under $\Gamma$-biased sampling. We apply a result ofRockafellar and Uryasev to show that this problem is equivalent to an augmentedconvex risk minimization problem. We give statistical guarantees for learning amodel that is robust to sampling bias via the method of sieves, and propose adeep learning algorithm whose loss function captures our robust learningtarget. We empirically validate our proposed method in a case study onprediction of mental health scores from health survey data and a case study onICU length of stay prediction.</description><author>Roshni Sahoo, Lihua Lei, Stefan Wager</author><pubDate>Tue, 16 Sep 2025 17:04:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01754v4</guid></item><item><title>Large Language Model-assisted Meta-optimizer for Automated Design of Constrained Evolutionary Algorithm</title><link>http://arxiv.org/abs/2509.13251v1</link><description>Meta-black-box optimization has been significantly advanced through the useof large language models (LLMs), yet in fancy on constrained evolutionaryoptimization. In this work, AwesomeDE is proposed that leverages LLMs as thestrategy of meta-optimizer to generate update rules for constrainedevolutionary algorithm without human intervention. On the meanwhile, $RTO^2H$framework is introduced for standardize prompt design of LLMs. Themeta-optimizer is trained on a diverse set of constrained optimizationproblems. Key components, including prompt design and iterative refinement, aresystematically analyzed to determine their impact on design quality.Experimental results demonstrate that the proposed approach outperformsexisting methods in terms of computational efficiency and solution accuracy.Furthermore, AwesomeDE is shown to generalize well across distinct problemdomains, suggesting its potential for broad applicability. This researchcontributes to the field by providing a scalable and data-driven methodologyfor automated constrained algorithm design, while also highlighting limitationsand directions for future work.</description><author>Xu Yang, Rui Wang, Kaiwen Li, Wenhua Li, Weixiong Huang</author><pubDate>Tue, 16 Sep 2025 17:02:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13251v1</guid></item><item><title>Intelligent Vacuum Thermoforming Process</title><link>http://arxiv.org/abs/2509.13250v1</link><description>Ensuring consistent quality in vacuum thermoforming presents challenges dueto variations in material properties and tooling configurations. This researchintroduces a vision-based quality control system to predict and optimiseprocess parameters, thereby enhancing part quality with minimal datarequirements. A comprehensive dataset was developed using visual data fromvacuum-formed samples subjected to various process parameters, supplemented byimage augmentation techniques to improve model training. A k-Nearest Neighbouralgorithm was subsequently employed to identify adjustments needed in processparameters by mapping low-quality parts to their high-quality counterparts. Themodel exhibited strong performance in adjusting heating power, heating time,and vacuum time to reduce defects and improve production efficiency.</description><author>Andi Kuswoyo, Christos Margadji, Sebastian W. Pattinson</author><pubDate>Tue, 16 Sep 2025 17:00:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13250v1</guid></item><item><title>Evaluating LLM Alignment on Personality Inference from Real-World Interview Data</title><link>http://arxiv.org/abs/2509.13244v1</link><description>Large Language Models (LLMs) are increasingly deployed in roles requiringnuanced psychological understanding, such as emotional support agents,counselors, and decision-making assistants. However, their ability to interprethuman personality traits, a critical aspect of such applications, remainsunexplored, particularly in ecologically valid conversational settings. Whileprior work has simulated LLM "personas" using discrete Big Five labels onsocial media data, the alignment of LLMs with continuous, ground-truthpersonality assessments derived from natural interactions is largelyunexamined. To address this gap, we introduce a novel benchmark comprisingsemi-structured interview transcripts paired with validated continuous Big Fivetrait scores. Using this dataset, we systematically evaluate LLM performanceacross three paradigms: (1) zero-shot and chain-of-thought prompting withGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMAarchitectures, and (3) regression using static embeddings from pretrained BERTand OpenAI's text-embedding-3-small. Our results reveal that all Pearsoncorrelations between model predictions and ground-truth personality traitsremain below 0.26, highlighting the limited alignment of current LLMs withvalidated psychological constructs. Chain-of-thought prompting offers minimalgains over zero-shot, suggesting that personality inference relies more onlatent semantic representation than explicit reasoning. These findingsunderscore the challenges of aligning LLMs with complex human attributes andmotivate future work on trait-specific prompting, context-aware modeling, andalignment-oriented fine-tuning.</description><author>Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin</author><pubDate>Tue, 16 Sep 2025 16:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13244v1</guid></item><item><title>SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning</title><link>http://arxiv.org/abs/2502.19668v3</link><description>Cardiovascular diseases are a leading cause of death and disabilityworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoringcardiac health, but obtaining large-scale annotated ECG datasets islabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)methods mitigate this by learning features without extensive labels but fail tocapture fine-grained clinical semantics and require extensive task-specificfine-tuning. To address these challenges, we propose $\textbf{SuPreME}$, a$\textbf{Su}$pervised $\textbf{Pre}$-training framework for$\textbf{M}$ultimodal $\textbf{E}$CG representation learning. SuPreME ispre-trained using structured diagnostic labels derived from ECG report entitiesthrough a one-time offline extraction with Large Language Models (LLMs), whichhelp denoise, standardize cardiac concepts, and improve clinical representationlearning. By fusing ECG signals with textual cardiac queries instead of fixedlabels, SuPreME enables zero-shot classification of unseen conditions withoutfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering106 cardiac conditions, achieving superior zero-shot AUC performance of$77.20\%$, surpassing state-of-the-art eSSLs by $4.98\%$. Results demonstrateSuPreME's effectiveness in leveraging structured, clinically relevant knowledgefor high-quality ECG representations.</description><author>Mingsheng Cai, Jiuming Jiang, Wenhao Huang, Che Liu, Rossella Arcucci</author><pubDate>Tue, 16 Sep 2025 16:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.19668v3</guid></item><item><title>RingMo-Aerial: An Aerial Remote Sensing Foundation Model With Affine Transformation Contrastive Learning</title><link>http://arxiv.org/abs/2409.13366v4</link><description>Aerial Remote Sensing (ARS) vision tasks present significant challenges dueto the unique viewing angle characteristics. Existing research has primarilyfocused on algorithms for specific tasks, which have limited applicability in abroad range of ARS vision applications. This paper proposes RingMo-Aerial,aiming to fill the gap in foundation model research in the field of ARS vision.A Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) mechanism is introducedto strengthen the model's capacity for small-object representation.Complementarily, an affine transformation-based contrastive learning methodimproves its adaptability to the tilted viewing angles inherent in ARS tasks.Furthermore, the ARS-Adapter, an efficient parameter fine-tuning method, isproposed to improve the model's adaptability and performance in various ARSvision tasks. Experimental results demonstrate that RingMo-Aerial achieves SOTAperformance on multiple downstream tasks. This indicates the practicality andefficacy of RingMo-Aerial in enhancing the performance of ARS vision tasks.</description><author>Wenhui Diao, Haichen Yu, Kaiyue Kang, Tong Ling, Di Liu, Yingchao Feng, Hanbo Bi, Libo Ren, Xuexue Li, Yongqiang Mao, Xian Sun</author><pubDate>Tue, 16 Sep 2025 16:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13366v4</guid></item><item><title>Is the Top Still Spinning? Evaluating Subjectivity in Narrative Understanding</title><link>http://arxiv.org/abs/2504.01132v2</link><description>Determining faithfulness of a claim to a source document is an importantproblem across many domains. This task is generally treated as a binaryjudgment of whether the claim is supported or unsupported in relation to thesource. In many cases, though, whether a claim is supported can be ambiguous.For instance, it may depend on making inferences from given evidence, anddifferent people can reasonably interpret the claim as either supported orunsupported based on their agreement with those inferences. Forcing binarylabels upon such claims lowers the reliability of evaluation. In this work, wereframe the task to manage the subjectivity involved with factuality judgmentsof ambiguous claims. We introduce LLM-generated edits of summaries as a methodof providing a nuanced evaluation of claims: how much does a summary need to beedited to be unambiguous? Whether a claim gets rewritten and how much itchanges can be used as an automatic evaluation metric, the Ambiguity RewriteMetric (ARM), with a much richer feedback signal than a binary judgment offaithfulness. We focus on the area of narrative summarization as it isparticularly rife with ambiguity and subjective interpretation. We show thatARM produces a 21% absolute improvement in annotator agreement on claimfaithfulness, indicating that subjectivity is reduced.</description><author>Melanie Subbiah, Akankshya Mishra, Grace Kim, Liyan Tang, Greg Durrett, Kathleen McKeown</author><pubDate>Tue, 16 Sep 2025 16:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.01132v2</guid></item><item><title>Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning</title><link>http://arxiv.org/abs/2509.13240v1</link><description>Existing parameter-efficient fine-tuning (PEFT) methods primarily adaptweight matrices while keeping activation functions fixed. We introduce\textbf{NoRA}, the first PEFT framework that directly adapts nonlinearactivation functions in pretrained transformer-based models. NoRA replacesfixed activations with learnable rational functions and applies structuredlow-rank updates to numerator and denominator coefficients, with a group-wisedesign that localizes adaptation and improves stability at minimal cost. Onvision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceedsfull fine-tuning while updating only 0.4\% of parameters (0.02M), achievingaccuracy gains of +0.17\% and +0.27\%. When combined with LoRA(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgetsby adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++consistently improves generation quality, yielding average MMLU gains of+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. Wefurther show that NoRA constrains adaptation to a low-dimensional functionalsubspace, implicitly regularizing update magnitude and direction. These resultsestablish activation-space tuning as a complementary and highlyparameter-efficient alternative to weight-based PEFT, positioning activationfunctions as first-class objects for model adaptation.</description><author>Bo Yin, Xingyi Yang, Xinchao Wang</author><pubDate>Tue, 16 Sep 2025 16:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13240v1</guid></item><item><title>Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors</title><link>http://arxiv.org/abs/2509.13237v1</link><description>Large language models (LLMs) now solve multi-step problems by emittingextended chains of thought. During the process, they often re-derive the sameintermediate steps across problems, inflating token usage and latency. Thissaturation of the context window leaves less capacity for exploration. We studya simple mechanism that converts recurring reasoning fragments into concise,reusable "behaviors" (name + instruction) via the model's own metacognitiveanalysis of prior traces. These behaviors are stored in a "behavior handbook"which supplies them to the model in-context at inference or distills them intoparameters via supervised fine-tuning. This approach achieves improvedtest-time reasoning across three different settings - 1) Behavior-conditionedinference: Providing the LLM relevant behaviors in-context during reasoningreduces number of reasoning tokens by up to 46% while matching or improvingbaseline accuracy; 2) Behavior-guided self-improvement: Without any parameterupdates, the model improves its own future reasoning by leveraging behaviorsfrom its own past problem solving attempts. This yields up to 10% higheraccuracy than a naive critique-and-revise baseline; and 3) Behavior-conditionedSFT: SFT on behavior-conditioned reasoning traces is more effective atconverting non-reasoning models into reasoning models as compared to vanillaSFT. Together, these results indicate that turning slow derivations into fastprocedural hints enables LLMs to remember how to reason, not just what toconclude.</description><author>Aniket Didolkar, Nicolas Ballas, Sanjeev Arora, Anirudh Goyal</author><pubDate>Tue, 16 Sep 2025 16:44:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13237v1</guid></item><item><title>Layout-Aware OCR for Black Digital Archives with Unsupervised Evaluation</title><link>http://arxiv.org/abs/2509.13236v1</link><description>Despite their cultural and historical significance, Black digital archivescontinue to be a structurally underrepresented area in AI research andinfrastructure. This is especially evident in efforts to digitize historicalBlack newspapers, where inconsistent typography, visual degradation, andlimited annotated layout data hinder accurate transcription, despite theavailability of various systems that claim to handle optical characterrecognition (OCR) well. In this short paper, we present a layout-aware OCRpipeline tailored for Black newspaper archives and introduce an unsupervisedevaluation framework suited to low-resource archival contexts. Our approachintegrates synthetic layout generation, model pretraining on augmented data,and a fusion of state-of-the-art You Only Look Once (YOLO) detectors. We usedthree annotation-free evaluation metrics, the Semantic Coherence Score (SCS),Region Entropy (RE), and Textual Redundancy Score (TRS), which quantifylinguistic fluency, informational diversity, and redundancy across OCR regions.Our evaluation on a 400-page dataset from ten Black newspaper titlesdemonstrates that layout-aware OCR improves structural diversity and reducesredundancy compared to full-page baselines, with modest trade-offs incoherence. Our results highlight the importance of respecting cultural layoutlogic in AI-driven document understanding and lay the foundation for futurecommunity-driven and ethically grounded archival AI systems.</description><author>Fitsum Sileshi Beyene, Christopher L. Dancy</author><pubDate>Tue, 16 Sep 2025 16:43:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13236v1</guid></item><item><title>A Scenario-Driven Cognitive Approach to Next-Generation AI Memory</title><link>http://arxiv.org/abs/2509.13235v1</link><description>As artificial intelligence advances toward artificial general intelligence(AGI), the need for robust and human-like memory systems has becomeincreasingly evident. Current memory architectures often suffer from limitedadaptability, insufficient multimodal integration, and an inability to supportcontinuous learning. To address these limitations, we propose a scenario-drivenmethodology that extracts essential functional requirements from representativecognitive scenarios, leading to a unified set of design principles fornext-generation AI memory systems. Based on this approach, we introduce the\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework thatintegrates cognitive scenarios, memory processes, and storage mechanisms into acohesive design. COLMA provides a structured foundation for developing AIsystems capable of lifelong learning and human-like reasoning, therebycontributing to the pragmatic development of AGI.</description><author>Linyue Cai, Yuyang Cheng, Xiaoding Shao, Huiming Wang, Yong Zhao, Wei Zhang, Kang Li</author><pubDate>Tue, 16 Sep 2025 16:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13235v1</guid></item><item><title>Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy</title><link>http://arxiv.org/abs/2509.13234v1</link><description>Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AIsystems can expand access to fundus photography screening. Current FDA-clearedsystems primarily provide binary referral outputs, where this minimal outputmay limit clinical trust and utility. Yet, determining the most effectiveoutput format to enhance clinician-AI performance is an empirical challengethat is difficult to assess at scale. We evaluated multimodal large languagemodels (MLLMs) for DR detection and their ability to simulate clinical AIassistance across different output types. Two models were tested on IDRiD andMessidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-sourcemedical model. Experiments included: (1) baseline evaluation, (2) simulated AIassistance with synthetic predictions, and (3) actual AI-to-AI collaborationwhere GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o atbaseline, achieving higher sensitivity and AUROC, while GPT-4o showednear-perfect specificity but low sensitivity. Both models adjusted predictionsbased on simulated AI inputs, but GPT-4o's performance collapsed with incorrectones, whereas MedGemma remained more stable. In actual collaboration, GPT-4oachieved strong results when guided by MedGemma's descriptive outputs, evenwithout direct image access (AUROC up to 0.96). These findings suggest MLLMsmay improve DR screening pipelines and serve as scalable simulators forstudying clinical AI assistance across varying output configurations. Open,lightweight models such as MedGemma may be especially valuable in low-resourcesettings, while descriptive outputs could enhance explainability and cliniciantrust in clinical workflows.</description><author>Nadim Barakat, William Lotter</author><pubDate>Tue, 16 Sep 2025 16:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13234v1</guid></item><item><title>Single-stream Policy Optimization</title><link>http://arxiv.org/abs/2509.13232v1</link><description>We revisit policy-gradient optimization for Large Language Models (LLMs) froma single-stream perspective. Prevailing group-based methods like GRPO reducevariance with on-the-fly baselines but suffer from critical flaws: frequentdegenerate groups erase learning signals, and synchronization barriers hinderscalability. We introduce Single-stream Policy Optimization (SPO), whicheliminates these issues by design. SPO replaces per-group baselines with apersistent, KL-adaptive value tracker and normalizes advantages globally acrossthe batch, providing a stable, low-variance learning signal for every sample.Being group-free, SPO enables higher throughput and scales effectively inlong-horizon or tool-integrated settings where generation times vary.Furthermore, the persistent value tracker naturally enables an adaptivecurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPOconverges more smoothly and attains higher accuracy than GRPO, whileeliminating computation wasted on degenerate groups. Ablation studies confirmthat SPO's gains stem from its principled approach to baseline estimation andadvantage normalization, offering a more robust and efficient path for LLMreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves theaverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantialabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gainin pass@$k$ across the evaluated $k$ values. SPO's success challenges theprevailing trend of adding incidental complexity to RL algorithms, highlightinga path where fundamental principles, not architectural workarounds, drive thenext wave of progress in LLM reasoning.</description><author>Zhongwen Xu, Zihan Ding</author><pubDate>Tue, 16 Sep 2025 16:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13232v1</guid></item><item><title>Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation</title><link>http://arxiv.org/abs/2509.13229v1</link><description>Hyperspectral imaging (HSI) captures detailed spectral signatures acrosshundreds of contiguous bands per pixel, being indispensable for remote sensingapplications such as land-cover classification, change detection, andenvironmental monitoring. Due to the high dimensionality of HSI data and theslow rate of data transfer in satellite-based systems, compact and efficientmodels are required to support onboard processing and minimize the transmissionof redundant or low-value data, e.g. cloud-covered areas. To this end, weintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)framework designed for lightweight architectures for HSI analysis. CMTSSLintegrates masked image modeling with decoupled spatial and spectral jigsawpuzzle solving, guided by a curriculum learning strategy that progressivelyincreases data complexity during self-supervision. This enables the encoder tojointly capture fine-grained spectral continuity, spatial structure, and globalsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneouslyaddresses spatial and spectral reasoning within a unified and computationallyefficient design, being particularly suitable for training lightweight modelsfor onboard satellite deployment. We validate our approach on four publicbenchmark datasets, demonstrating consistent gains in downstream segmentationtasks, using architectures that are over 16,000x lighter than somestate-of-the-art models. These results highlight the potential of CMTSSL ingeneralizable representation learning with lightweight architectures forreal-world HSI applications. Our code is publicly available athttps://github.com/hugocarlesso/CMTSSL.</description><author>Hugo Carlesso, Josiane Mothe, Radu Tudor Ionescu</author><pubDate>Tue, 16 Sep 2025 16:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13229v1</guid></item><item><title>Rich Vehicle Routing Problem with diverse Vertices allowing Hierarchical and Multimodal Time-Dependant Transhipment of multiple Node- Vehicle- compatible Cargo with Cascaded Time-Minimization Objective for Emergency Decision Support Systems</title><link>http://arxiv.org/abs/2509.13227v1</link><description>A rich vehicle routing problem is considered allowing multiple trips ofheterogeneous vehicles stationed at distributed vehicle depots spread acrossdiverse geographies having access to different modes of transportation. Theproblem arises from the real world requirement of optimizing the disasterresponse/preparedness time and minimizes the route duration of the vehicles toachieve the solution with the minimum highest-vehicle-route-duration. Multiplediversely-functional vertices are considered including the concept ofTranshipment Ports as inter-modal resource transfer stations. Both simultaneousand split pickup and transferring of different types of delivery and pickupcargo is considered, along with Vehicle-Cargo and Transhipment Port-CargoCompatibility. The superiority of the proposed cascaded minimization approachis shown over existing makespan minimization approaches through the developedMILP formulation. To solve the problem quickly for practical implementationwithin Disaster Management-specific Decision Support Systems, an extensiveHeuristic Algorithm is devised. The Heuristic utilizes Decision Tree basedstructuring of possible routes and is able to inherently consider thecompatibility issues. Preferential generation of small route elements areperformed, which are integrated into route clusters; we consider multipledifferent logical integration approaches, as well as shuffling the logics tosimultaneously produce multiple independent solutions. Finally perturbation ofthe different solutions are done to find better neighbouring solutions. Thecomputational performance of the PSR-GIP Heuristic, on our created noveldatasets, indicate that it is able to give good solutions swiftly for practicalproblems involving large integer instances which the MILP is unable to solve.</description><author>Santanu Banerjee, Goutam Sen, Siddhartha Mukhopadhyay</author><pubDate>Tue, 16 Sep 2025 16:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13227v1</guid></item><item><title>Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation</title><link>http://arxiv.org/abs/2508.00766v2</link><description>Image-to-image translation has emerged as a powerful technique in medicalimaging, enabling tasks such as image denoising and cross-modality conversion.However, it suffers from limitations in handling out-of-distribution sampleswithout causing performance degradation. To address this limitation, we proposea novel Test-Time Adaptation (TTA) framework that dynamically adjusts thetranslation process based on the characteristics of each test sample. Ourmethod introduces a Reconstruction Module to quantify the domain shift and aDynamic Adaptation Block that selectively modifies the internal features of apretrained translation model to mitigate the shift without compromising theperformance on in-distribution samples that do not require adaptation. Weevaluate our approach on two medical image-to-image translation tasks: low-doseCT denoising and T1 to T2 MRI translation, showing consistent improvements overboth the baseline translation model without TTA and prior TTA methods. Ouranalysis highlights the limitations of the state-of-the-art that uniformlyapply the adaptation to both out-of-distribution and in-distribution samples,demonstrating that dynamic, sample-specific adjustment offers a promising pathto improve model resilience in real-world scenarios. The code is available at:https://github.com/Sample-Aware-TTA/Code.</description><author>Irene Iele, Francesco Di Feola, Valerio Guarrasi, Paolo Soda</author><pubDate>Tue, 16 Sep 2025 16:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.00766v2</guid></item><item><title>A Stable Measure for Conditional Periodicity of Time Series using Persistent Homology</title><link>http://arxiv.org/abs/2501.02817v3</link><description>Given a pair of time series, we study how the periodicity of one influencesthe periodicity of the other. There are several known methods to measure thesimilarity between a pair of time series, but we have yet to find any measureswith theoretical stability results. Persistence homology has been utilized toconstruct a scoring function with theoretical guarantees of stability thatquantifies the periodicity of a single univariate time series f1, denotedscore(f1). Building on this concept, we propose a conditional periodicity scorethat quantifies the periodicity similarity of two univariate time series,denoted score(f1|f2), and derive theoretical stability results for the same. Weprove stability of score(f1|f2) under orthogonal projection of the time seriesembeddings onto their first K principal components. We show that the change inour score is bounded by a function of the eigenvalues corresponding to theremaining (unused) N-K principal components and hence is small when the first Kprincipal components capture most of the variation in the time seriesembeddings. We derive a lower bound on the embedding dimension to use in ourpipeline which guarantees that any two such embeddings produce scores that arelinearly within epsilon of each other. We present a procedure for computingconditional periodicity scores and implement it on several types of syntheticsignals. We experimentally compare our similarity measure to the most-similarstatistical measure of percent determinism (%DET) and show greater stability ofscore(f1|f2). We also compare both measures on several pairs of real timeseries describing monthly proportions of incoming calls to a police agency andhighlight the decreased stability of %DET on the same.</description><author>Bala Krishnamoorthy, Elizabeth P. Thompson</author><pubDate>Tue, 16 Sep 2025 16:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.02817v3</guid></item><item><title>On the Out-of-Distribution Backdoor Attack for Federated Learning</title><link>http://arxiv.org/abs/2509.13219v1</link><description>Traditional backdoor attacks in federated learning (FL) operate withinconstrained attack scenarios, as they depend on visible triggers and requirephysical modifications to the target object, which limits their practicality.To address this limitation, we introduce a novel backdoor attack prototype forFL called the out-of-distribution (OOD) backdoor attack ($\mathtt{OBA}$), whichuses OOD data as both poisoned samples and triggers simultaneously. Ourapproach significantly broadens the scope of backdoor attack scenarios in FL.To improve the stealthiness of $\mathtt{OBA}$, we propose $\mathtt{SoDa}$,which regularizes both the magnitude and direction of malicious local modelsduring local training, aligning them closely with their benign versions toevade detection. Empirical results demonstrate that $\mathtt{OBA}$ effectivelycircumvents state-of-the-art defenses while maintaining high accuracy on themain task. To address this security vulnerability in the FL system, we introduce$\mathtt{BNGuard}$, a new server-side defense method tailored against$\mathtt{SoDa}$. $\mathtt{BNGuard}$ leverages the observation that OOD datacauses significant deviations in the running statistics of batch normalizationlayers. This allows $\mathtt{BNGuard}$ to identify malicious model updates andexclude them from aggregation, thereby enhancing the backdoor robustness of FL.Extensive experiments across various settings show the effectiveness of$\mathtt{BNGuard}$ on defending against $\mathtt{SoDa}$. The code is availableat https://github.com/JiiahaoXU/SoDa-BNGuard.</description><author>Jiahao Xu, Zikai Zhang, Rui Hu</author><pubDate>Tue, 16 Sep 2025 16:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13219v1</guid></item><item><title>FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data</title><link>http://arxiv.org/abs/2509.13218v1</link><description>Imbalanced and small data regimes are pervasive in domains such as raredisease imaging, genomics, and disaster response, where labeled samples arescarce and naive augmentation often introduces artifacts. Existing solutionssuch as oversampling, focal loss, or meta-weighting address isolated aspects ofthis challenge but remain fragile or complex. We introduce FOSSIL (FlexibleOptimization via Sample Sensitive Importance Learning), a unified weightingframework that seamlessly integrates class imbalance correction,difficulty-aware curricula, augmentation penalties, and warmup dynamics into asingle interpretable formula. Unlike prior heuristics, the proposed frameworkprovides regret-based theoretical guarantees and achieves consistent empiricalgains over ERM, curriculum, and meta-weighting baselines on synthetic andreal-world datasets, while requiring no architectural changes.</description><author>J. Cha, J. Lee, J. Cho, J. Shin</author><pubDate>Tue, 16 Sep 2025 16:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13218v1</guid></item><item><title>Flow-Based Fragment Identification via Binding Site-Specific Latent Representations</title><link>http://arxiv.org/abs/2509.13216v1</link><description>Fragment-based drug design is a promising strategy leveraging the binding ofsmall chemical moieties that can efficiently guide drug discovery. The initialstep of fragment identification remains challenging, as fragments often bindweakly and non-specifically. We developed a protein-fragment encoder thatrelies on a contrastive learning approach to map both molecular fragments andprotein surfaces in a shared latent space. The encoder capturesinteraction-relevant features and allows to perform virtual screening as wellas generative design with our new method LatentFrag. In LatentFrag, fragmentembeddings and positions are generated conditioned on the protein surface whilebeing chemically realistic by construction. Our expressive fragment and proteinrepresentations allow location of protein-fragment interaction sites with highsensitivity and we observe state-of-the-art fragment recovery rates whensampling from the learned distribution of latent fragment embeddings. Ourgenerative method outperforms common methods such as virtual screening at afraction of its computational cost providing a valuable starting point forfragment hit discovery. We further show the practical utility of LatentFrag andextend the workflow to full ligand design tasks. Together, these approachescontribute to advancing fragment identification and provide valuable tools forfragment-based drug discovery.</description><author>Rebecca Manuela Neeser, Ilia Igashov, Arne Schneuing, Michael Bronstein, Philippe Schwaller, Bruno Correia</author><pubDate>Tue, 16 Sep 2025 16:20:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13216v1</guid></item><item><title>End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection</title><link>http://arxiv.org/abs/2509.13214v1</link><description>The powerful generative capabilities of diffusion models have significantlyadvanced the field of image synthesis, enhancing both full image generation andinpainting-based image editing. Despite their remarkable advancements,diffusion models also raise concerns about potential misuse for maliciouspurposes. However, existing approaches struggle to identify images generated bydiffusion-based inpainting models, even when similar inpainted images areincluded in their training data. To address this challenge, we propose a noveldetection method based on End-to-end denoising diffusion (End4). Specifically,End4 designs a denoising reconstruction model to improve the alignment degreebetween the latent spaces of the reconstruction and detection processes, thusreconstructing features that are more conducive to detection. Meanwhile, itleverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines localimage features under the guidance of attention pyramid layers at differentscales, enhancing feature discriminability. Additionally, to evaluate detectionperformance on inpainted images, we establish a comprehensive benchmarkcomprising images generated from five distinct masked regions. Extensiveexperiments demonstrate that our End4 effectively generalizes to unseen maskingpatterns and remains robust under various perturbations. Our code and datasetwill be released soon.</description><author>Fei Wang, Xuecheng Wu, Zheng Zhang, Danlei Huang, Yuheng Huang, BoWang</author><pubDate>Tue, 16 Sep 2025 16:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13214v1</guid></item><item><title>Density-Aware Farthest Point Sampling</title><link>http://arxiv.org/abs/2509.13213v1</link><description>We focus on training machine learning regression models in scenarios wherethe availability of labeled training data is limited due to computationalconstraints or high labeling costs. Thus, selecting suitable training sets fromunlabeled data is essential for balancing performance and efficiency. For theselection of the training data, we focus on passive and model-agnostic samplingmethods that only consider the data feature representations. We derive an upperbound for the expected prediction error of Lipschitz continuous regressionmodels that linearly depends on the weighted fill distance of the training set,a quantity we can estimate simply by considering the data features. Weintroduce "Density-Aware Farthest Point Sampling" (DA-FPS), a novel samplingmethod. We prove that DA-FPS provides approximate minimizers for a data-drivenestimation of the weighted fill distance, thereby aiming at minimizing ourderived bound. We conduct experiments using two regression models across threedatasets. The results demonstrate that DA-FPS significantly reduces the meanabsolute prediction error compared to other sampling strategies.</description><author>Paolo Climaco, Jochen Garcke</author><pubDate>Tue, 16 Sep 2025 16:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13213v1</guid></item><item><title>HAM: Hierarchical Adapter Merging for Scalable Continual Learning</title><link>http://arxiv.org/abs/2509.13211v1</link><description>Continual learning is an essential capability of human cognition, yet itposes significant challenges for current deep learning models. The primaryissue is that new knowledge can interfere with previously learned information,causing the model to forget earlier knowledge in favor of the new, a phenomenonknown as catastrophic forgetting. Although large pre-trained models canpartially mitigate forgetting by leveraging their existing knowledge andover-parameterization, they often struggle when confronted with novel datadistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,enable efficient adaptation to new knowledge. However, they still facechallenges in scaling to dynamic learning scenarios and long sequences oftasks, as maintaining one adapter per task introduces complexity and increasesthe potential for interference. In this paper, we introduce HierarchicalAdapters Merging (HAM), a novel framework that dynamically combines adaptersfrom different tasks during training. This approach enables HAM to scaleeffectively, allowing it to manage more tasks than competing baselines withimproved efficiency. To achieve this, HAM maintains a fixed set of groups thathierarchically consolidate new adapters. For each task, HAM trains a low-rankadapter along with an importance scalar, then dynamically groups tasks based onadapter similarity. Within each group, adapters are pruned, scaled and merge,facilitating transfer learning between related tasks. Extensive experiments onthree vision benchmarks show that HAM significantly outperformsstate-of-the-art methods, particularly as the number of tasks increases.</description><author>Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco</author><pubDate>Tue, 16 Sep 2025 16:18:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13211v1</guid></item><item><title>Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance</title><link>http://arxiv.org/abs/2509.13210v1</link><description>Violence detection in public surveillance is critical for public safety. Thisstudy addresses challenges such as small-scale targets, complex environments,and real-time temporal analysis. We propose Vi-SAFE, a spatial-temporalframework that integrates an enhanced YOLOv8 with a Temporal Segment Network(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 asa lightweight backbone, an exponential moving average (EMA) attentionmechanism, and pruning to reduce computational cost while maintaining accuracy.YOLOv8 and TSN are trained separately on pedestrian and violence datasets,where YOLOv8 extracts human regions and TSN performs binary classification ofviolent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFEachieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperformingexisting methods in both accuracy and efficiency, demonstrating itseffectiveness for public safety surveillance. Code is available athttps://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.</description><author>Ligang Chang, Shengkai Xu, Liangchang Shen, Binhan Xu, Junqiao Wang, Tianyu Shi, Yanhui Du</author><pubDate>Tue, 16 Sep 2025 16:16:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13210v1</guid></item><item><title>Hybrid Two-Stage Reconstruction of Multiscale Subsurface Flow with Physics-informed Residual Connected Neural Operator</title><link>http://arxiv.org/abs/2501.13271v2</link><description>The novel neural networks show great potential in solving partialdifferential equations. For single-phase flow problems in subsurface porousmedia with high-contrast coefficients, the key is to develop neural operatorswith accurate reconstruction capability and strict adherence to physical laws.In this study, we proposed a hybrid two-stage framework that uses multiscalebasis functions and physics-guided deep learning to solve the Darcy flowproblem in high-contrast fractured porous media. In the first stage, adata-driven model is used to reconstruct the multiscale basis function based onthe permeability field to achieve effective dimensionality reduction whilepreserving the necessary multiscale features. In the second stage, thephysics-informed neural network, together with Transformer-based globalinformation extractor is used to reconstruct the pressure field by integratingthe physical constraints derived from the Darcy equation, ensuring consistencywith the physical laws of the real world. The model was evaluated on datasetswith different combinations of permeability and basis functions and performedwell in terms of reconstruction accuracy. Specifically, the framework achievesR2 values above 0.9 in terms of basis function fitting and pressurereconstruction, and the residual indicator is on the order of $1\times10^{-4}$. These results validate the ability of the proposed framework toachieve accurate reconstruction while maintaining physical consistency.</description><author>Peiqi Li, Jie Chen</author><pubDate>Tue, 16 Sep 2025 16:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13271v2</guid></item><item><title>G-CSEA: A Graph-Based Conflict Set Extraction Algorithm for Identifying Infeasibility in Pseudo-Boolean Models</title><link>http://arxiv.org/abs/2509.13203v1</link><description>Workforce scheduling involves a variety of rule-based constraints-such asshift limits, staffing policies, working hour restrictions, and many similarscheduling rules-which can interact in conflicting ways, leading to infeasiblemodels. Identifying the underlying causes of such infeasibility is critical forresolving scheduling issues and restoring feasibility. A common diagnosticapproach is to compute Irreducible Infeasible Subsets (IISs): minimal sets ofconstraints that are jointly infeasible but become feasible when any one isremoved. We consider models formulated using pseudo-Boolean constraints withinequality relations over binary variables, which naturally encode schedulinglogic. Existing IIS extraction methods such as Additive Deletion andQuickXplain rely on repeated feasibility checks, often incurring large numbersof solver calls. Dual ray analysis, while effective for LP-based models, mayfail when the relaxed problem is feasible but the underlying pseudo-Booleanmodel is not. To address these limitations, we propose Graph-based Conflict SetExtraction Algorithm (G-CSEA) to extract a conflict set, an approach inspiredby Conflict-Driven Clause Learning (CDCL) in SAT solvers. Our method constructsan implication graph during constraint propagation and, upon detecting aconflict, traces all contributing constraints across both decision branches.The resulting conflict set can optionally be minimized using QuickXplain toproduce an IIS.</description><author>Kanishk Garg, Saranya D., Sanal Kumar, Saurabh Singh, Anupam Purwar</author><pubDate>Tue, 16 Sep 2025 16:09:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13203v1</guid></item><item><title>B-TGAT: A Bi-directional Temporal Graph Attention Transformer for Clustering Multivariate Spatiotemporal Data</title><link>http://arxiv.org/abs/2509.13202v1</link><description>Clustering high-dimensional multivariate spatiotemporal climate data ischallenging due to complex temporal dependencies, evolving spatialinteractions, and non-stationary dynamics. Conventional clustering methods,including recurrent and convolutional models, often struggle to capture bothlocal and global temporal relationships while preserving spatial context. Wepresent a time-distributed hybrid U-Net autoencoder that integrates aBi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficienttemporal clustering of multidimensional spatiotemporal climate datasets. Theencoder and decoder are equipped with ConvLSTM2D modules that extract jointspatial--temporal features by modeling localized dynamics and spatialcorrelations over time, and skip connections that preserve multiscale spatialdetails during feature compression and reconstruction. At the bottleneck,B-TGAT integrates graph-based spatial modeling with attention-driven temporalencoding, enabling adaptive weighting of temporal neighbors and capturing bothshort and long-range dependencies across regions. This architecture producesdiscriminative latent embeddings optimized for clustering. Experiments on threedistinct spatiotemporal climate datasets demonstrate superior clusterseparability, temporal stability, and alignment with known climate transitionscompared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Netskip connections, and B-TGAT enhances temporal clustering performance whileproviding interpretable insights into complex spatiotemporal variability,advancing both methodological development and climate science applications.</description><author>Francis Ndikum Nji, Vandana Janaja, Jianwu Wang</author><pubDate>Tue, 16 Sep 2025 16:08:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13202v1</guid></item><item><title>Podcasts as a Medium for Participation in Collective Action: A Case Study of Black Lives Matter</title><link>http://arxiv.org/abs/2509.13197v1</link><description>We study how participation in collective action is articulated in podcastdiscussions, using the Black Lives Matter (BLM) movement as a case study. Whileresearch on collective action discourse has primarily focused on text-basedcontent, this study takes a first step toward analyzing audio formats by usingpodcast transcripts. Using the Structured Podcast Research Corpus (SPoRC), weinvestigated spoken language expressions of participation in collective action,categorized as problem-solution, call-to-action, intention, and execution. Weidentified podcast episodes discussing racial justice after importantBLM-related events in May and June of 2020, and extracted participatorystatements using a layered framework adapted from prior work on social media.We examined the emotional dimensions of these statements, detecting eight keyemotions and their association with varying stages of activism. We found thatemotional profiles vary by stage, with different positive emotions standing outduring calls-to-action, intention, and execution. We detected negativeassociations between collective action and negative emotions, contrary totheoretical expectations. Our work contributes to a better understanding of howactivism is expressed in spoken digital discourse and how emotional framing maydepend on the format of the discussion.</description><author>Theodora Moldovan, Arianna Pera, Davide Vega, Luca Maria Aiello</author><pubDate>Tue, 16 Sep 2025 16:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13197v1</guid></item><item><title>The Few-shot Dilemma: Over-prompting Large Language Models</title><link>http://arxiv.org/abs/2509.13196v1</link><description>Over-prompting, a phenomenon where excessive examples in prompts lead todiminished performance in Large Language Models (LLMs), challenges theconventional wisdom about in-context few-shot learning. To investigate thisfew-shot dilemma, we outline a prompting framework that leverages threestandard few-shot selection methods - random sampling, semantic embedding, andTF-IDF vectors - and evaluate these methods across multiple LLMs, includingGPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.Our experimental results reveal that incorporating excessive domain-specificexamples into prompts can paradoxically degrade performance in certain LLMs,which contradicts the prior empirical conclusion that more relevant few-shotexamples universally benefit LLMs. Given the trend of LLM-assisted softwareengineering and requirement analysis, we experiment with two real-worldsoftware requirement classification datasets. By gradually increasing thenumber of TF-IDF-selected and stratified few-shot examples, we identify theiroptimal quantity for each LLM. This combined approach achieves superiorperformance with fewer examples, avoiding the over-prompting problem, thussurpassing the state-of-the-art by 1% in classifying functional andnon-functional requirements.</description><author>Yongjian Tang, Doruk Tuncel, Christian Koerner, Thomas Runkler</author><pubDate>Tue, 16 Sep 2025 16:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13196v1</guid></item><item><title>Implicit Neural Representations of Intramyocardial Motion and Strain</title><link>http://arxiv.org/abs/2509.09004v3</link><description>Automatic quantification of intramyocardial motion and strain from taggingMRI remains an important but challenging task. We propose a method usingimplicit neural representations (INRs), conditioned on learned latent codes, topredict continuous left ventricular (LV) displacement -- without requiringinference-time optimisation. Evaluated on 452 UK Biobank test cases, our methodachieved the best tracking accuracy (2.14 mm RMSE) and the lowest combinederror in global circumferential (2.86%) and radial (6.42%) strain compared tothree deep learning baselines. In addition, our method is $\sim$380$\times$faster than the most accurate baseline. These results highlight the suitabilityof INR-based models for accurate and scalable analysis of myocardial strain inlarge CMR datasets.</description><author>Andrew Bell, Yan Kit Choi, Steffen E Petersen, Andrew King, Muhummad Sohaib Nazir, Alistair A Young</author><pubDate>Tue, 16 Sep 2025 15:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.09004v3</guid></item><item><title>TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition</title><link>http://arxiv.org/abs/2509.05983v2</link><description>Code-switching (CS) presents a significant challenge for general Auto-SpeechRecognition (ASR) systems. Existing methods often fail to capture the subtlephonological shifts inherent in CS scenarios. The challenge is particularlydifficult for language pairs like Vietnamese and English, where both distinctphonological features and the ambiguity arising from similar sound recognitionare present. In this paper, we propose a novel architecture forVietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPCemploys a phoneme-centric approach, built upon an extended Vietnamese phonemeset as an intermediate representation to facilitate mixed-lingual modeling.Experimental results demonstrate that TSPC consistently outperforms existingbaselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving asignificantly lower word error rate of 20.8\% with reduced training resources.Furthermore, the phonetic-based two-stage architecture enables phonemeadaptation and language conversion to enhance ASR performance in complex CSVietnamese-English ASR scenarios.</description><author>Minh N. H. Nguyen, Anh Nguyen Tran, Dung Truong Dinh, Nam Van Vo</author><pubDate>Tue, 16 Sep 2025 15:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.05983v2</guid></item><item><title>TRUST-FS: Tensorized Reliable Unsupervised Multi-View Feature Selection for Incomplete Data</title><link>http://arxiv.org/abs/2509.13192v1</link><description>Multi-view unsupervised feature selection (MUFS), which selects informativefeatures from multi-view unlabeled data, has attracted increasing researchinterest in recent years. Although great efforts have been devoted to MUFS,several challenges remain: 1) existing methods for incomplete multi-view dataare limited to handling missing views and are unable to address the moregeneral scenario of missing variables, where some features have missing valuesin certain views; 2) most methods address incomplete data by first imputingmissing values and then performing feature selection, treating these twoprocesses independently and overlooking their interactions; 3) missing data canresult in an inaccurate similarity graph, which reduces the performance offeature selection. To solve this dilemma, we propose a novel MUFS method forincomplete multi-view data with missing variables, termed Tensorized ReliableUnSupervised mulTi-view Feature Selection (TRUST-FS). TRUST-FS introduces a newadaptive-weighted CP decomposition that simultaneously performs featureselection, missing-variable imputation, and view weight learning within aunified tensor factorization framework. By utilizing Subjective Logic toacquire trustworthy cross-view similarity information, TRUST-FS facilitateslearning a reliable similarity graph, which subsequently guides featureselection and imputation. Comprehensive experimental results demonstrate theeffectiveness and superiority of our method over state-of-the-art methods.</description><author>Minghui Lu, Yanyong Huang, Minbo Ma, Dongjie Wang, Xiuwen Yi, Tianrui Li</author><pubDate>Tue, 16 Sep 2025 15:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13192v1</guid></item><item><title>Textarium: Entangling Annotation, Abstraction and Argument</title><link>http://arxiv.org/abs/2509.13191v1</link><description>We present a web-based environment that connects annotation, abstraction, andargumentation during the interpretation of text. As a visual interface forscholarly reading and writing, Textarium combines human analysis withlightweight computational processing to bridge close and distant readingpractices. Readers can highlight text, group keywords into concepts, and embedthese observations as anchors in essays. The interface renders theseinterpretive actions as parameterized visualization states. Through aspeculative design process of co-creative and iterative prototyping, wedeveloped a reading-writing approach that makes interpretive processestransparent and shareable within digital narratives.</description><author>Philipp Proff, Marian Dörk</author><pubDate>Tue, 16 Sep 2025 15:46:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13191v1</guid></item><item><title>SURGIN: SURrogate-guided Generative INversion for subsurface multiphase flow with quantified uncertainty</title><link>http://arxiv.org/abs/2509.13189v1</link><description>We present a direct inverse modeling method named SURGIN, a SURrogate-guidedGenerative INversion framework tailed for subsurface multiphase flow dataassimilation. Unlike existing inversion methods that require adaptation foreach new observational configuration, SURGIN features a zero-shot conditionalgeneration capability, enabling real-time assimilation of unseen monitoringdata without task-specific retraining. Specifically, SURGIN synergisticallyintegrates a U-Net enhanced Fourier Neural Operator (U-FNO) surrogate with ascore-based generative model (SGM), framing the conditional generation as asurrogate prediction-guidance process in a Bayesian perspective. Instead ofdirectly learning the conditional generation of geological parameters, anunconditional SGM is first pretrained in a self-supervised manner to capturethe geological prior, after which posterior sampling is performed by leveraginga differentiable U-FNO surrogate to enable efficient forward evaluationsconditioned on unseen observations. Extensive numerical experiments demonstrateSURGIN's capability to decently infer heterogeneous geological fields andpredict spatiotemporal flow dynamics with quantified uncertainty across diversemeasurement settings. By unifying generative learning with surrogate-guidedBayesian inference, SURGIN establishes a new paradigm for inverse modeling anduncertainty quantification in parametric functional spaces.</description><author>Zhao Feng, Bicheng Yan, Luanxiao Zhao, Xianda Shen, Renyu Zhao, Wenhao Wang, Fengshou Zhang</author><pubDate>Tue, 16 Sep 2025 15:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13189v1</guid></item><item><title>Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy</title><link>http://arxiv.org/abs/2509.13185v1</link><description>Meta-learning is a powerful paradigm for tackling few-shot tasks. However,recent studies indicate that models trained with the whole-class trainingstrategy can achieve comparable performance to those trained with meta-learningin few-shot classification tasks. To demonstrate the value of meta-learning, weestablish an entropy-limited supervised setting for fair comparisons. Throughboth theoretical analysis and experimental validation, we establish thatmeta-learning has a tighter generalization bound compared to whole-classtraining. We unravel that meta-learning is more efficient with limited entropyand is more robust to label noise and heterogeneous tasks, making itwell-suited for unsupervised tasks. Based on these insights, We propose MINO, ameta-learning framework designed to enhance unsupervised performance. MINOutilizes the adaptive clustering algorithm DBSCAN with a dynamic head forunsupervised task construction and a stability-based meta-scaler for robustnessagainst label noise. Extensive experiments confirm its effectiveness inmultiple unsupervised few-shot and zero-shot tasks.</description><author>Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Jenq-Neng Hwang, Serge Belongie, Lei Li</author><pubDate>Tue, 16 Sep 2025 15:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13185v1</guid></item><item><title>AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations</title><link>http://arxiv.org/abs/2509.06176v2</link><description>As artificial intelligence (AI) systems permeate critical sectors, the needfor professionals who can address ethical, legal and governance challenges hasbecome urgent. Current AI ethics education remains fragmented, often siloed bydiscipline and disconnected from practice. This paper synthesizes literatureand regulatory developments to propose a modular, interdisciplinary curriculumthat integrates technical foundations with ethics, law and policy. We highlightrecurring operational failures in AI - bias, misspecified objectives,generalization errors, misuse and governance breakdowns - and link them topedagogical strategies for teaching AI governance. Drawing on perspectives fromthe EU, China and international frameworks, we outline a semester plan thatemphasizes integrated ethics, stakeholder engagement and experiential learning.The curriculum aims to prepare students to diagnose risks, navigate regulationand engage diverse stakeholders, fostering adaptive and ethically groundedprofessionals for responsible AI governance.</description><author>Raphaël Weuts, Johannes Bleher, Hannah Bleher, Rozanne Tuesday Flores, Guo Xuanyang, Paweł Pujszo, Zsolt Almási</author><pubDate>Tue, 16 Sep 2025 15:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.06176v2</guid></item><item><title>Road Obstacle Video Segmentation</title><link>http://arxiv.org/abs/2509.13181v1</link><description>With the growing deployment of autonomous driving agents, the detection andsegmentation of road obstacles have become critical to ensure safe autonomousnavigation. However, existing road-obstacle segmentation methods are applied onindividual frames, overlooking the temporal nature of the problem, leading toinconsistent prediction maps between consecutive frames. In this work, wedemonstrate that the road-obstacle segmentation task is inherently temporal,since the segmentation maps for consecutive frames are strongly correlated. Toaddress this, we curate and adapt four evaluation benchmarks for road-obstaclevideo segmentation and evaluate 11 state-of-the-art image- and video-basedsegmentation methods on these benchmarks. Moreover, we introduce two strongbaseline methods based on vision foundation models. Our approach establishes anew state-of-the-art in road-obstacle video segmentation for long-range videosequences, providing valuable insights and direction for future research.</description><author>Shyam Nandan Rai, Shyamgopal Karthik, Mariana-Iuliana Georgescu, Barbara Caputo, Carlo Masone, Zeynep Akata</author><pubDate>Tue, 16 Sep 2025 15:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13181v1</guid></item><item><title>Pitfalls of defacing whole-head MRI: re-identification risk with diffusion models and compromised research potential</title><link>http://arxiv.org/abs/2501.18834v2</link><description>Defacing is often applied to head magnetic resonance image (MRI) datasetsprior to public release to address privacy concerns. The alteration of facialand nearby voxels has provoked discussions about the true capability of thesetechniques to ensure privacy as well as their impact on downstream tasks. Withadvancements in deep generative models, the extent to which defacing canprotect privacy is uncertain. Additionally, while the altered voxels are knownto contain valuable anatomical information, their potential to support researchbeyond the anatomical regions directly affected by defacing remains uncertain.To evaluate these considerations, we develop a refacing pipeline that recoversfaces in defaced head MRIs using cascaded diffusion probabilistic models(DPMs). The DPMs are trained on images from 180 subjects and tested on imagesfrom 484 unseen subjects, 469 of whom are from a different dataset. To assesswhether the altered voxels in defacing contain universally useful information,we also predict computed tomography (CT)-derived skeletal muscle radiodensityfrom facial voxels in both defaced and original MRIs. The results show thatDPMs can generate high-fidelity faces that resemble the original faces fromdefaced images, with surface distances to the original faces significantlysmaller than those of a population average face (p &lt; 0.05). This performancealso generalizes well to previously unseen datasets. For skeletal muscleradiodensity predictions, using defaced images results in significantly weakerSpearman's rank correlation coefficients compared to using original images (p &lt;10-4). For shin muscle, the correlation is statistically significant (p &lt; 0.05)when using original images but not statistically significant (p &gt; 0.05) whenany defacing method is applied, suggesting that defacing might not only fail toprotect privacy but also eliminate valuable information.</description><author>Chenyu Gao, Kaiwen Xu, Michael E. Kim, Lianrui Zuo, Zhiyuan Li, Derek B. Archer, Timothy J. Hohman, Ann Zenobia Moore, Luigi Ferrucci, Lori L. Beason-Held, Susan M. Resnick, Christos Davatzikos, Jerry L. Prince, Bennett A. Landman</author><pubDate>Tue, 16 Sep 2025 15:33:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18834v2</guid></item><item><title>Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs General Reasoning</title><link>http://arxiv.org/abs/2505.13886v4</link><description>Real-world vision language reasoning scenarios often include diverse andcomplex tasks. However, vision language reinforcement learning has primarilyfocused on a narrow set of tasks (e.g. geometry or chart reasoning), limitingthe improvement of Vision Language Models' (VLMs) general reasoning. Therefore,we propose a novel Code2Logic approach, using Large Language Models (LLMs) tosynthesize verifiable game reasoning tasks at scale via adapting game code.Using the Code2Logic, we developed the GameQA dataset to train and evaluateVLMs. GameQA is verifiable and scalable, offers controllable difficultygradation and is diverse with 30 games and 158 tasks. Then we apply Game-RL,which is simple reinforcement learning on GameQA. Surprisingly, despitetraining solely on game tasks, VLMs demonstrated out of domain generalization,specifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diversevision-language benchmarks. Our code, dataset and models are available at theGitHub repository.</description><author>Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang</author><pubDate>Tue, 16 Sep 2025 15:33:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.13886v4</guid></item><item><title>Efficient Cold-Start Recommendation via BPE Token-Level Embedding Initialization with LLM</title><link>http://arxiv.org/abs/2509.13179v1</link><description>The cold-start issue is the challenge when we talk about recommender systems,especially in the case when we do not have the past interaction data of newusers or new items. Content-based features or hybrid solutions are common asconventional solutions, but they can only work in a sparse metadata environmentwith shallow patterns. In this paper, the efficient cold-start recommendationstrategy is presented, which is based on the sub word-level representations byapplying Byte Pair Encoding (BPE) tokenization and pre-trained Large LanguageModel (LLM) embedding in the initialization procedure. We obtain fine-grainedtoken-level vectors that are aligned with the BPE vocabulary as opposed tousing coarse-grained sentence embeddings. Together, these token embeddings canbe used as dense semantic priors on unseen entities, making immediaterecommendation performance possible without user-item interaction history. Ourmechanism can be compared to collaborative filtering systems and tested overbenchmark datasets with stringent cold-start assumptions. Experimental findingsshow that the given BPE-LLM method achieves higher Recall@k, NDCG@k, and HitRate measurements compared to the standard baseline and displays the samecapability of sufficient computational performance. Furthermore, we demonstratethat using subword-aware embeddings yields better generalizability and is moreinterpretable, especially within a multilingual and sparse input setting. Thepractical application of token-level semantic initialization as a lightweight,but nevertheless effective extension to modern recommender systems in thezero-shot setting is indicated within this work.</description><author>Yushang Zhao, Xinyue Han, Qian Leng, Qianyi Sun, Haotian Lyu, Chengrui Zhou</author><pubDate>Tue, 16 Sep 2025 15:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13179v1</guid></item><item><title>CoVariance Filters and Neural Networks over Hilbert Spaces</title><link>http://arxiv.org/abs/2509.13178v1</link><description>CoVariance Neural Networks (VNNs) perform graph convolutions on the empiricalcovariance matrix of signals defined over finite-dimensional Hilbert spaces,motivated by robustness and transferability properties. Yet, little is knownabout how these arguments extend to infinite-dimensional Hilbert spaces. Inthis work, we take a first step by introducing a novel convolutional learningframework for signals defined over infinite-dimensional Hilbert spaces,centered on the (empirical) covariance operator. We constructively defineHilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs)as stacks of HVF filterbanks with nonlinear activations. We propose aprincipled discretization procedure, and we prove that empirical HVFs canrecover the Functional PCA (FPCA) of the filtered signals. We then describe theversatility of our framework with examples ranging from multivariatereal-valued functions to reproducing kernel Hilbert spaces. Finally, wevalidate HVNs on both synthetic and real-world time-series classificationtasks, showing robust performance compared to MLP and FPCA-based classifiers.</description><author>Claudio Battiloro, Andrea Cavallo, Elvin Isufi</author><pubDate>Tue, 16 Sep 2025 15:32:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13178v1</guid></item><item><title>Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment</title><link>http://arxiv.org/abs/2509.04852v2</link><description>Estimating density ratios is a fundamental problem in machine learning, butexisting methods often trade off accuracy for efficiency. We propose\textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)},a framework that enables accurate, any-step estimation without numericalintegration. Instead of modeling infinitesimal tangents as in prior methods, ISA-DRElearns a global secant function, defined as the expectation of all tangentsover an interval, with provably lower variance, making it more suitable forneural approximation. This is made possible by the \emph{Secant AlignmentIdentity}, a self-consistency condition that formally connects the secant withits underlying tangent representations. To mitigate instability during early training, we introduce \emph{ContractionInterval Annealing}, a curriculum strategy that gradually expands the alignmentinterval during training. This process induces a contraction mapping, whichimproves convergence and training stability. Empirically, ISA-DRE achieves competitive accuracy with significantly fewerfunction evaluations compared to prior methods, resulting in much fasterinference and making it well suited for real-time and interactive applications.</description><author>Wei Chen, Shigui Li, Jiacheng Li, Jian Xu, Zhiqi Lin, Junmei Yang, Delu Zeng, John Paisley, Qibin Zhao</author><pubDate>Tue, 16 Sep 2025 15:29:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.04852v2</guid></item><item><title>More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era</title><link>http://arxiv.org/abs/2509.13175v1</link><description>The emergence of Large Language Models (LLMs) presents unprecedentedopportunities to revolutionize medical contrastive vision-languagepre-training. In this paper, we show how LLMs can facilitate large-scalesupervised pre-training, thereby advancing vision-language alignment. We beginby demonstrate that modern LLMs can automatically extract diagnostic labelsfrom radiology reports with remarkable precision (&gt;96\% AUC in our experiments)without complex prompt engineering, enabling the creation of large-scale"silver-standard" datasets at a minimal cost (~\$3 for 50k CT image-reportpairs). Further, we find that vision encoder trained on this "silver-standard"dataset achieves performance comparable to those trained on labels extracted byspecialized BERT-based models, thereby democratizing the access to large-scalesupervised pre-training. Building on this foundation, we proceed to reveal thatsupervised pre-training fundamentally improves contrastive vision-languagealignment. Our approach achieves state-of-the-art performance using only a 3DResNet-18 with vanilla CLIP training, including 83.8\% AUC for zero-shotdiagnosis on CT-RATE, 77.3\% AUC on RAD-ChestCT, and substantial improvementsin cross-modal retrieval (MAP@50=53.7\% for image-image, Recall@100=52.2\% forreport-image). These results demonstrate the potential of utilizing LLMs tofacilitate {\bf more performant and scalable} medical AI systems. Our code isavaiable at https://github.com/SadVoxel/More-performant-and-scalable.</description><author>Yingtai Li, Haoran Lai, Xiaoqian Zhou, Shuai Ming, Wenxin Ma, Wei Wei, Shaohua Kevin Zhou</author><pubDate>Tue, 16 Sep 2025 15:27:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13175v1</guid></item><item><title>Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards</title><link>http://arxiv.org/abs/2509.10407v2</link><description>Compressed video quality enhancement (CVQE) is crucial for improving userexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.While deep learning based CVQE has driven significant progress, existingsurveys still suffer from limitations: lack of systematic classificationlinking methods to specific standards and artifacts, insufficient comparativeanalysis of architectural paradigms across coding types, and underdevelopedbenchmarking practices. To address these gaps, this paper presents three keycontributions. First, it introduces a novel taxonomy classifying CVQE methodsacross architectural paradigms, coding standards, and compressed-domain featureutilization. Second, it proposes a unified benchmarking framework integratingmodern compression protocols and standard test sequences for fairmulti-criteria evaluation. Third, it provides a systematic analysis of thecritical trade-offs between reconstruction performance and computationalcomplexity observed in state-of-the-art methods and highlighting promisingdirections for future research. This comprehensive review aims to establish afoundation for consistent assessment and informed model selection in CVQEresearch and deployment.</description><author>Xiem HoangVan, Dang BuiDinh, Sang NguyenQuang, Wen-Hsiao Peng</author><pubDate>Tue, 16 Sep 2025 15:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.10407v2</guid></item><item><title>WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</title><link>http://arxiv.org/abs/2509.13172v1</link><description>Street trees are vital to urban livability, providing ecological and socialbenefits. Establishing a detailed, accurate, and dynamically updated streettree inventory has become essential for optimizing these multifunctional assetswithin space-constrained urban environments. Given that traditional fieldsurveys are time-consuming and labor-intensive, automated surveys utilizingMobile Mapping Systems (MMS) offer a more efficient solution. However, existingMMS-acquired tree datasets are limited by small-scale scene, limitedannotation, or single modality, restricting their utility for comprehensiveanalysis. To address these limitations, we introduce WHU-STree, a cross-city,richly annotated, and multi-modal urban street tree dataset. Collected acrosstwo distinct cities, WHU-STree integrates synchronized point clouds andhigh-resolution images, encompassing 21,007 annotated tree instances across 50species and 2 morphological parameters. Leveraging the unique characteristics,WHU-STree concurrently supports over 10 tasks related to street tree inventory.We benchmark representative baselines for two key tasks--tree speciesclassification and individual tree segmentation. Extensive experiments andin-depth analysis demonstrate the significant potential of multi-modal datafusion and underscore cross-domain applicability as a critical prerequisite forpractical algorithm deployment. In particular, we identify key challenges andoutline potential future works for fully exploiting WHU-STree, encompassingmulti-modal fusion, multi-task collaboration, cross-domain generalization,spatial pattern learning, and Multi-modal Large Language Model for street treeasset management. The WHU-STree dataset is accessible at:https://github.com/WHU-USI3DV/WHU-STree.</description><author>Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang</author><pubDate>Tue, 16 Sep 2025 15:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13172v1</guid></item><item><title>TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation</title><link>http://arxiv.org/abs/2411.11683v5</link><description>Robotic manipulation in the physical world is increasingly empowered by\textit{large language models} (LLMs) and \textit{vision-language models}(VLMs), leveraging their understanding and perception capabilities. Recently,various attacks against such robotic policies have been proposed, with backdoorattacks drawing considerable attention for their high stealth and strongpersistence capabilities. However, existing backdoor efforts are limited tosimulators and suffer from physical-world realization. To address this, wepropose \textit{TrojanRobot}, a highly stealthy and broadly effective roboticbackdoor attack in the physical world. Specifically, we introduce amodule-poisoning approach by embedding a backdoor module into the modularrobotic policy, enabling backdoor control over the policy's visual perceptionmodule thereby backdooring the entire robotic policy. Our vanillaimplementation leverages a backdoor-finetuned VLM to serve as the backdoormodule. To enhance its generalization in physical environments, we propose aprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developingthree types of prime attacks, \ie, \textit{permutation}, \textit{stagnation},and \textit{intentional} attacks, thus achieving finer-grained backdoors.Extensive experiments on the UR3e manipulator with 18 task instructions usingrobotic policies based on four VLMs demonstrate the broad effectiveness andphysical-world stealth of TrojanRobot. Our attack's video demonstrations areavailable via a github link https://trojanrobot.github.io.</description><author>Xianlong Wang, Hewen Pan, Hangtao Zhang, Minghui Li, Shengshan Hu, Ziqi Zhou, Lulu Xue, Aishan Liu, Yunpeng Jiang, Leo Yu Zhang, Xiaohua Jia</author><pubDate>Tue, 16 Sep 2025 15:19:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11683v5</guid></item><item><title>Concentration inequalities for semidefinite least squares based on data</title><link>http://arxiv.org/abs/2509.13166v1</link><description>We study data-driven least squares (LS) problems with semidefinite (SD)constraints and derive finite-sample guarantees on the spectrum of theiroptimal solutions when these constraints are relaxed. In particular, we providea high confidence bound allowing one to solve a simpler program in place of thefull SDLS problem, while ensuring that the eigenvalues of the resultingsolution are $\varepsilon$-close of those enforced by the SD constraints. Thedeveloped certificate, which consistently shrinks as the number of dataincreases, turns out to be easy-to-compute, distribution-free, and onlyrequires independent and identically distributed samples. Moreover, when theSDLS is used to learn an unknown quadratic function, we establish bounds on theerror between a gradient descent iterate minimizing the surrogate cost obtainedwith no SD constraints and the true minimizer.</description><author>Filippo Fabiani, Andrea Simonetto</author><pubDate>Tue, 16 Sep 2025 15:17:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13166v1</guid></item><item><title>On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models</title><link>http://arxiv.org/abs/2509.13165v1</link><description>We investigate individual fairness in generative probabilistic classifiers byanalysing the robustness of posterior inferences to perturbations in privatefeatures. Building on established results in robustness analysis, wehypothesise a correlation between robustness and predictive accuracy,specifically, instances exhibiting greater robustness are more likely to beclassified accurately. We empirically assess this hypothesis using a benchmarkof fourteen datasets with fairness concerns, employing Bayesian networks as theunderlying generative models. To address the computational complexityassociated with robustness analysis over multiple private features withBayesian networks, we reformulate the problem as a most probable explanationtask in an auxiliary Markov random field. Our experiments confirm thehypothesis about the correlation, suggesting novel directions to mitigate thetraditional trade-off between fairness and accuracy.</description><author>Alessandro Antonucci, Eric Rossetto, Ivan Duvnjak</author><pubDate>Tue, 16 Sep 2025 15:17:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13165v1</guid></item><item><title>Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)</title><link>http://arxiv.org/abs/2509.13161v1</link><description>Despite the prosperity of the video language model, the current pursuit ofcomprehensive video reasoning is thwarted by the inherent spatio-temporalincompleteness within individual videos, resulting in hallucinations andinaccuracies. A promising solution is to augment the reasoning performance withmultiple related videos. However, video tokens are numerous and containredundant information, so directly feeding the relevant video data into a largelanguage model to enhance responses could be counterproductive. To address thischallenge, we propose a multi-video collaborative framework for video languagemodels. For efficient and flexible video representation, we establish a VideoStructuring Module to represent the video's knowledge as a spatio-temporalgraph. Based on the structured video representation, we design the Graph FusionModule to fuse the structured knowledge and valuable information from relatedvideos into the augmented graph node tokens. Finally, we construct an elaboratemulti-video structured prompt to integrate the graph, visual, and textualtokens as the input to the large language model. Extensive experimentssubstantiate the effectiveness of our framework, showcasing its potential as apromising avenue for advancing video language models.</description><author>Zhihao He, Tianyao He, Tieyuan Chen, Yun Xu, Huabin Liu, Chaofan Gan, Gui Zou, Weiyao Lin</author><pubDate>Tue, 16 Sep 2025 15:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13161v1</guid></item><item><title>FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning</title><link>http://arxiv.org/abs/2509.13160v1</link><description>Search has emerged as core infrastructure for LLM-based agents and is widelyviewed as critical on the path toward more general intelligence. Finance is aparticularly demanding proving ground: analysts routinely conduct complex,multi-step searches over time-sensitive, domain-specific data, making it idealfor assessing both search proficiency and knowledge-grounded reasoning. Yet noexisting open financial datasets evaluate data searching capability ofend-to-end agents, largely because constructing realistic, complicated tasksrequires deep financial expertise and time-sensitive data is hard to evaluate.We present FinSearchComp, the first fully open-source agent benchmark forrealistic, open-domain financial search and reasoning. FinSearchComp comprisesthree tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, andComplex Historical Investigation -- closely reproduce real-world financialanalyst workflows. To ensure difficulty and reliability, we engage 70professional financial experts for annotation and implement a rigorousmulti-stage quality-assurance pipeline. The benchmark includes 635 questionsspanning global and Greater China markets, and we evaluate 21 models (products)on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.DouBao (web) leads on the Greater China subset. Experimental analyses show thatequipping agents with web search and financial plugins substantially improvesresults on FinSearchComp, and the country origin of models and tools impactperformance significantly.By aligning with realistic analyst tasks andproviding end-to-end evaluation, FinSearchComp offers a professional,high-difficulty testbed for complex financial search and reasoning.</description><author>Liang Hu, Jianpeng Jiao, Jiashuo Liu, Yanle Ren, Zhoufutu Wen, Kaiyuan Zhang, Xuanliang Zhang, Xiang Gao, Tianci He, Fei Hu, Yali Liao, Zaiyuan Wang, Chenghao Yang, Qianyu Yang, Mingren Yin, Zhiyuan Zeng, Ge Zhang, Xinyi Zhang, Xiying Zhao, Zhenwei Zhu, Hongseok Namkoong, Wenhao Huang, Yuwen Tang</author><pubDate>Tue, 16 Sep 2025 15:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13160v1</guid></item><item><title>Evaluating the Robustness of Open-Source Vision-Language Models to Domain Shift in Object Captioning</title><link>http://arxiv.org/abs/2506.19579v2</link><description>Vision-Language Models (VLMs) have emerged as powerful tools for generatingtextual descriptions from visual data. While these models excel on web-scaledatasets, their robustness to the domain shifts inherent in many real-worldapplications remains under-explored. This paper presents a systematicevaluation of VLM performance on a single-view object captioning task whenfaced with a controlled, physical domain shift. We compare captioning accuracyacross two distinct object sets: a collection of multi-material, real-worldtools and a set of single-material, 3D-printed items. The 3D-printed setintroduces a significant domain shift in texture and material properties,challenging the models' generalization capabilities. Our quantitative resultsdemonstrate that all tested VLMs show a marked performance degradation whendescribing the 3D-printed objects compared to the real-world tools. Thisunderscores a critical limitation in the ability of current models togeneralize beyond surface-level features and highlights the need for morerobust architectures for real-world signal processing applications.</description><author>Federico Tavella, Amber Drinkwater, Angelo Cangelosi</author><pubDate>Tue, 16 Sep 2025 15:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.19579v2</guid></item><item><title>References Matter: Investigating the Impact of Reference Set Variation on Summarization Evaluation</title><link>http://arxiv.org/abs/2506.14335v3</link><description>Human language production exhibits remarkable richness and variation,reflecting diverse communication styles and intents. However, this variation isoften overlooked in summarization evaluation. While having multiple referencesummaries is known to improve correlation with human judgments, the impact ofthe reference set on reference-based metrics has not been systematicallyinvestigated. This work examines the sensitivity of widely used reference-basedmetrics in relation to the choice of reference sets, analyzing three diversemulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. Wedemonstrate that many popular metrics exhibit significant instability. Thisinstability is particularly concerning for n-gram-based metrics like ROUGE,where model rankings vary depending on the reference sets, undermining thereliability of model comparisons. We also collect human judgments on LLMoutputs for genre-diverse data and examine their correlation with metrics tosupplement existing findings beyond newswire summaries, finding weak-to-nocorrelation. Taken together, we recommend incorporating reference set variationinto summarization evaluation to enhance consistency alongside correlation withhuman judgments, especially when evaluating LLMs.</description><author>Silvia Casola, Yang Janet Liu, Siyao Peng, Oliver Kraus, Albert Gatt, Barbara Plank</author><pubDate>Tue, 16 Sep 2025 15:11:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.14335v3</guid></item><item><title>New Kid in the Classroom: Exploring Student Perceptions of AI Coding Assistants</title><link>http://arxiv.org/abs/2507.22900v4</link><description>The arrival of AI coding assistants in educational settings presents aparadigm shift, introducing a "new kid in the classroom" for both students andinstructors. Thus, understanding the perceptions of these key actors about thisnew dynamic is critical. This exploratory study contributes to this area byinvestigating how these tools are shaping the experiences of novice programmersin an introductory programming course. Through a two-part exam, we investigatedstudent perceptions by first providing access to AI support for a programmingtask and then requiring an extension of the solution without it. We collectedLikert-scale and open-ended responses from 20 students to understand theirperceptions on the challenges they faced. Our findings reveal that studentsperceived AI tools as helpful for grasping code concepts and boosting theirconfidence during the initial development phase. However, a noticeabledifficulty emerged when students were asked to work unaided, pointing topotential overreliance and gaps in foundational knowledge transfer. Theseinsights highlight a critical need for new pedagogical approaches thatintegrate AI effectively while effectively enhancing core programming skills,rather than impersonating them.</description><author>Sergio Rojas-Galeano</author><pubDate>Tue, 16 Sep 2025 15:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22900v4</guid></item><item><title>LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals</title><link>http://arxiv.org/abs/2509.13154v1</link><description>Hallucination remains a critical barrier for deploying large language models(LLMs) in reliability-sensitive applications. Existing detection methodslargely fall into two categories: factuality checking, which is fundamentallyconstrained by external knowledge coverage, and static hidden-state analysis,that fails to capture deviations in reasoning dynamics. As a result, theireffectiveness and robustness remain limited. We propose HSAD (Hidden SignalAnalysis-based Detection), a novel hallucination detection framework thatmodels the temporal dynamics of hidden representations during autoregressivegeneration. HSAD constructs hidden-layer signals by sampling activations acrosslayers, applies Fast Fourier Transform (FFT) to obtain frequency-domainrepresentations, and extracts the strongest non-DC frequency component asspectral features. Furthermore, by leveraging the autoregressive nature ofLLMs, HSAD identifies optimal observation points for effective and reliabledetection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over10 percentage points improvement compared to prior state-of-the-art methods. Byintegrating reasoning-process modeling with frequency-domain analysis, HSADestablishes a new paradigm for robust hallucination detection in LLMs.</description><author>Jinxin Li, Gang Tu, ShengYu Cheng, Junjie Hu, Jinting Wang, Rui Chen, Zhilong Zhou, Dongbo Shan</author><pubDate>Tue, 16 Sep 2025 15:08:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13154v1</guid></item><item><title>Analysis of Fourier Neural Operators via Effective Field Theory</title><link>http://arxiv.org/abs/2507.21833v2</link><description>Fourier Neural Operators (FNOs) have emerged as leading surrogates for solveroperators for various functional problems, yet their stability, generalizationand frequency behavior lack a principled explanation. We present a systematiceffective field theory analysis of FNOs in an infinite dimensional functionspace, deriving closed recursion relations for the layer kernel and four pointvertex and then examining three practically important settings-analyticactivations, scale invariant cases and architectures with residual connections.The theory shows that nonlinear activations inevitably couple frequency inputsto high frequency modes that are otherwise discarded by spectral truncation,and experiments confirm this frequency transfer. For wide networks, we deriveexplicit criticality conditions on the weight initialization ensemble thatensure small input perturbations maintain a uniform scale across depth, and weconfirm experimentally that the theoretically predicted ratio of kernelperturbations matches the measurements. Taken together, our results quantifyhow nonlinearity enables neural operators to capture non-trivial features,supply criteria for hyperparameter selection via criticality analysis, andexplain why scale invariant activations and residual connections enhancefeature learning in FNOs.</description><author>Taeyoung Kim</author><pubDate>Tue, 16 Sep 2025 15:06:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.21833v2</guid></item><item><title>TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual Document Images</title><link>http://arxiv.org/abs/2509.13151v1</link><description>Recognizing textual attributes such as bold, italic, underline and strikeoutis essential for understanding text semantics, structure, and visualpresentation. These attributes highlight key information, making them crucialfor document analysis. Existing methods struggle with computational efficiencyor adaptability in noisy, multilingual settings. To address this, we introduceTexTAR, a multi-task, context-aware Transformer for Textual AttributeRecognition (TAR). Our novel data selection pipeline enhances contextawareness, and our architecture employs a 2D RoPE (Rotary PositionalEmbedding)-style mechanism to incorporate input context for more accurateattribute predictions. We also introduce MMTAD, a diverse, multilingual,multi-domain dataset annotated with text attributes across real-world documentssuch as legal records, notices, and textbooks. Extensive evaluations showTexTAR outperforms existing methods, demonstrating that contextual awarenesscontributes to state-of-the-art TAR performance.</description><author>Rohan Kumar, Jyothi Swaroopa Jinka, Ravi Kiran Sarvadevabhatla</author><pubDate>Tue, 16 Sep 2025 15:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13151v1</guid></item><item><title>MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation</title><link>http://arxiv.org/abs/2509.13149v1</link><description>4D radar super-resolution, which aims to reconstruct sparse and noisy pointclouds into dense and geometrically consistent representations, is afoundational problem in autonomous perception. However, existing methods oftensuffer from high training cost or rely on complex diffusion-based sampling,resulting in high inference latency and poor generalization, making itdifficult to balance accuracy and efficiency. To address these limitations, wepropose MSDNet, a multi-stage distillation framework that efficiently transfersdense LiDAR priors to 4D radar features to achieve both high reconstructionquality and computational efficiency. The first stage performsreconstruction-guided feature distillation, aligning and densifying thestudent's features through feature reconstruction. In the second stage, wepropose diffusion-guided feature distillation, which treats the stage-onedistilled features as a noisy version of the teacher's representations andrefines them via a lightweight diffusion network. Furthermore, we introduce anoise adapter that adaptively aligns the noise level of the feature with apredefined diffusion timestep, enabling a more precise denoising. Extensiveexperiments on the VoD and in-house datasets demonstrate that MSDNet achievesboth high-fidelity reconstruction and low-latency inference in the task of 4Dradar point cloud super-resolution, and consistently improves performance ondownstream tasks. The code will be publicly available upon publication.</description><author>Minqing Huang, Shouyi Lu, Boyuan Zheng, Ziyao Li, Xiao Tang, Guirong Zhuo</author><pubDate>Tue, 16 Sep 2025 15:05:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13149v1</guid></item><item><title>Geoff: The Generic Optimization Framework &amp; Frontend for Particle Accelerator Controls</title><link>http://arxiv.org/abs/2506.03796v3</link><description>Geoff is a collection of Python packages that form a framework for automationof particle accelerator controls. With particle accelerator laboratories aroundthe world researching machine learning techniques to improve acceleratorperformance and uptime, a multitude of approaches and algorithms have emerged.The purpose of Geoff is to harmonize these approaches and to minimize frictionwhen comparing or migrating between them. It provides standardized interfacesfor optimization problems, utility functions to speed up development, and areference GUI application that ties everything together. Geoff is anopen-source library developed at CERN and maintained and updated incollaboration between CERN and GSI as part of the EURO-LABS project. This papergives an overview over Geoff's design, features, and current usage.</description><author>Penelope Madysa, Sabrina Appel, Verena Kain, Michael Schenk</author><pubDate>Tue, 16 Sep 2025 15:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.03796v3</guid></item><item><title>Built Different: Tactile Perception to Overcome Cross-Embodiment Capability Differences in Collaborative Manipulation</title><link>http://arxiv.org/abs/2409.14896v2</link><description>Tactile sensing is a widely-studied means of implicit communication betweenrobot and human. In this paper, we investigate how tactile sensing can helpbridge differences between robotic embodiments in the context of collaborativemanipulation. For a robot, learning and executing force-rich collaborationrequire compliance to human interaction. While compliance is often achievedwith admittance control, many commercial robots lack the joint torquemonitoring needed for such control. To address this challenge, we present anapproach that uses tactile sensors and behavior cloning to transfer policiesfrom robots with these capabilities to those without. We train a single policythat demonstrates positive transfer across embodiments, including robotswithout torque sensing. We demonstrate this positive transfer on four differenttactile-enabled embodiments using the same policy trained on force-controlledrobot data. Across multiple proposed metrics, the best performance came from adecomposed tactile shear-field representation combined with a pre-trainedencoder, which improved success rates over alternative representations.</description><author>William van den Bogert, Madhavan Iyengar, Nima Fazeli</author><pubDate>Tue, 16 Sep 2025 15:03:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14896v2</guid></item><item><title>Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges</title><link>http://arxiv.org/abs/2509.13139v1</link><description>Graph heterophily poses a formidable challenge to the performance ofMessage-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filterslike Graph Convolutional Networks (GCNs) face performance degradation, whichcan be attributed to the blending of the messages from dissimilar neighboringnodes. The performance of the low-pass filters on heterophilic graphs stillrequires an in-depth analysis. In this context, we update the heterophilicgraphs by adding a number of self-loops and parallel edges. We observe thateigenvalues of the graph Laplacian decrease and increase respectively byincreasing the number of self-loops and parallel edges. We conduct severalstudies regarding the performance of GCN on various benchmark heterophilicnetworks by adding either self-loops or parallel edges. The studies reveal thatthe GCN exhibited either increasing or decreasing performance trends on addingself-loops and parallel edges. In light of the studies, we establishedconnections between the graph spectra and the performance trends of thelow-pass filters on the heterophilic graphs. The graph spectra characterize theessential intrinsic properties of the input graph like the presence ofconnected components, sparsity, average degree, cluster structures, etc. Ourwork is adept at seamlessly evaluating graph spectrum and properties byobserving the performance trends of the low-pass filters without pursuing thecostly eigenvalue decomposition. The theoretical foundations are also discussedto validate the impact of adding self-loops and parallel edges on the graphspectrum.</description><author>Kushal Bose, Swagatam Das</author><pubDate>Tue, 16 Sep 2025 14:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13139v1</guid></item><item><title>Curriculum Learning for Mesh-based simulations</title><link>http://arxiv.org/abs/2509.13138v1</link><description>Graph neural networks (GNNs) have emerged as powerful surrogates formesh-based computational fluid dynamics (CFD), but training them onhigh-resolution unstructured meshes with hundreds of thousands of nodes remainsprohibitively expensive. We study a \emph{coarse-to-fine curriculum} thataccelerates convergence by first training on very coarse meshes and thenprogressively introducing medium and high resolutions (up to \(3\times10^5\)nodes). Unlike multiscale GNN architectures, the model itself is unchanged;only the fidelity of the training data varies over time. We achieve comparablegeneralization accuracy while reducing total wall-clock time by up to 50\%.Furthermore, on datasets where our model lacks the capacity to learn theunderlying physics, using curriculum learning enables it to break throughplateaus.</description><author>Paul Garnier, Vincent Lannelongue, Elie Hachem</author><pubDate>Tue, 16 Sep 2025 14:54:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13138v1</guid></item><item><title>Agentic AI for Financial Crime Compliance</title><link>http://arxiv.org/abs/2509.13137v1</link><description>The cost and complexity of financial crime compliance (FCC) continue to rise,often without measurable improvements in effectiveness. While AI offerspotential, most solutions remain opaque and poorly aligned with regulatoryexpectations. This paper presents the design and deployment of an agentic AIsystem for FCC in digitally native financial platforms. Developed through anAction Design Research (ADR) process with a fintech firm and regulatorystakeholders, the system automates onboarding, monitoring, investigation, andreporting, emphasizing explainability, traceability, and compliance-by-design.Using artifact-centric modeling, it assigns clearly bounded roles to autonomousagents and enables task-specific model routing and audit logging. Thecontribution includes a reference architecture, a real-world prototype, andinsights into how Agentic AI can reconfigure FCC workflows under regulatoryconstraints. Our findings extend IS literature on AI-enabled compliance bydemonstrating how automation, when embedded within accountable governancestructures, can support transparency and institutional trust in high-stakes,regulated environments.</description><author>Henrik Axelsen, Valdemar Licht, Jan Damsgaard</author><pubDate>Tue, 16 Sep 2025 14:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13137v1</guid></item><item><title>Discovering Mathematical Equations with Diffusion Language Model</title><link>http://arxiv.org/abs/2509.13136v1</link><description>Discovering valid and meaningful mathematical equations from observed dataplays a crucial role in scientific discovery. While this task, symbolicregression, remains challenging due to the vast search space and the trade-offbetween accuracy and complexity. In this paper, we introduce DiffuSR, apre-training framework for symbolic regression built upon a continuous-statediffusion language model. DiffuSR employs a trainable embedding layer withinthe diffusion process to map discrete mathematical symbols into a continuouslatent space, modeling equation distributions effectively. Through iterativedenoising, DiffuSR converts an initial noisy sequence into a symbolic equation,guided by numerical data injected via a cross-attention mechanism. We alsodesign an effective inference strategy to enhance the accuracy of thediffusion-based equation generator, which injects logit priors into geneticprogramming. Experimental results on standard symbolic regression benchmarksdemonstrate that DiffuSR achieves competitive performance with state-of-the-artautoregressive methods and generates more interpretable and diversemathematical expressions.</description><author>Xiaoxu Han, Chengzhen Ning, Jinghui Zhong, Fubiao Yang, Yu Wang, Xin Mu</author><pubDate>Tue, 16 Sep 2025 14:53:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13136v1</guid></item><item><title>Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline</title><link>http://arxiv.org/abs/2509.13133v1</link><description>As automatic parking systems evolve, the accurate detection of parking slotshas become increasingly critical. This study focuses on parking slot detectionusing surround-view cameras, which offer a comprehensive bird's-eye view of theparking environment. However, the current datasets are limited in scale, andthe scenes they contain are seldom disrupted by real-world noise (e.g., light,occlusion, etc.). Moreover, manual data annotation is prone to errors andomissions due to the complexity of real-world conditions, significantlyincreasing the cost of annotating large-scale datasets. To address theseissues, we first construct a large-scale parking slot detection dataset (namedCRPS-D), which includes various lighting distributions, diverse weatherconditions, and challenging parking slot variants. Compared with existingdatasets, the proposed dataset boasts the largest data scale and consists of ahigher density of parking slots, particularly featuring more slanted parkingslots. Additionally, we develop a semi-supervised baseline for parking slotdetection, termed SS-PSD, to further improve performance by exploitingunlabeled data. To our knowledge, this is the first semi-supervised approach inparking slot detection, which is built on the teacher-student model withconfidence-guided mask consistency and adaptive feature perturbation.Experimental results demonstrate the superiority of SS-PSD over the existingstate-of-the-art (SoTA) solutions on both the proposed dataset and the existingdataset. Particularly, the more unlabeled data there is, the more significantthe gains brought by our semi-supervised scheme. The relevant source codes andthe dataset have been made publicly available athttps://github.com/zzh362/CRPS-D.</description><author>Zhihao Zhang, Chunyu Lin, Lang Nie, Jiyuan Wang, Yao Zhao</author><pubDate>Tue, 16 Sep 2025 14:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13133v1</guid></item><item><title>Optimal Transport Based Unsupervised Restoration Learning Exploiting Degradation Sparsity</title><link>http://arxiv.org/abs/2305.00273v2</link><description>Optimal transport (OT) has recently been shown as a promising criterion forunsupervised restoration when no explicit prior model is available. Despite itstheoretical appeal, OT still significantly falls short of supervised methods onchallenging tasks such as super-resolution, deraining, and dehazing. In thispaper, we propose a \emph{sparsity-aware optimal transport} (SOT) framework tobridge this gap by leveraging a key observation: the degradations in thesetasks exhibit distinct sparsity in the frequency domain. Incorporating thissparsity prior into OT can significantly reduce the ambiguity of the inversemapping for restoration and substantially boost performance. We provideanalysis to show exploiting degradation sparsity benefits unsupervisedrestoration learning. Extensive experiments on real-world super-resolution,deraining, and dehazing demonstrate that SOT offers notable performance gainsover standard OT, while achieving superior perceptual quality compared toexisting supervised and unsupervised methods. In particular, SOT consistentlyoutperforms existing unsupervised methods across all three tasks and narrowsthe performance gap to supervised counterparts.</description><author>Fei Wen, Wei Wang, Zeyu Yan, Wenbin Jiang</author><pubDate>Tue, 16 Sep 2025 14:49:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00273v2</guid></item><item><title>An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios</title><link>http://arxiv.org/abs/2509.13132v1</link><description>Autonomous driving in dense, dynamic environments requires decision-makingsystems that can exploit both spatial structure and long-horizon temporaldependencies while remaining robust to uncertainty. This work presents a novelframework that integrates multi-channel bird's-eye-view occupancy grids withtransformer-based sequence modeling for tactical driving in complex roundaboutscenarios. To address the imbalance between frequent low-risk states and raresafety-critical decisions, we propose the Uncertainty-Weighted DecisionTransformer (UWDT). UWDT employs a frozen teacher transformer to estimateper-token predictive entropy, which is then used as a weight in the studentmodel's loss function. This mechanism amplifies learning from uncertain,high-impact states while maintaining stability across common low-risktransitions. Experiments in a roundabout simulator, across varying trafficdensities, show that UWDT consistently outperforms other baselines in terms ofreward, collision rate, and behavioral stability. The results demonstrate thatuncertainty-aware, spatial-temporal transformers can deliver safer and moreefficient decision-making for autonomous driving in complex trafficenvironments.</description><author>Zhihao Zhang, Chengyang Peng, Minghao Zhu, Ekim Yurtsever, Keith A. Redmill</author><pubDate>Tue, 16 Sep 2025 14:48:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13132v1</guid></item><item><title>Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets</title><link>http://arxiv.org/abs/2509.13131v1</link><description>Recent advances in reasoning with large language models (LLMs) havedemonstrated strong performance on complex mathematical tasks, includingcombinatorial optimization. Techniques such as Chain-of-Thought and In-ContextLearning have further enhanced this capability, making LLMs both powerful andaccessible tools for a wide range of users, including non-experts. However,applying LLMs to matching problems, which require reasoning under preferentialand structural constraints, remains underexplored. To address this gap, weintroduce a novel benchmark of 369 instances of the College Admission Problem,a canonical example of a matching problem with preferences, to evaluate LLMsacross key dimensions: feasibility, stability, and optimality. We employ thisbenchmark to assess the performance of several open-weight LLMs. Our resultsfirst reveal that while LLMs can satisfy certain constraints, they struggle tomeet all evaluation criteria consistently. They also show that reasoning LLMs,like QwQ and GPT-oss, significantly outperform traditional models such asLlama, Qwen or Mistral, defined here as models used without any dedicatedreasoning mechanisms. Moreover, we observed that LLMs reacted differently tothe various prompting strategies tested, which include Chain-of-Thought,In-Context Learning and role-based prompting, with no prompt consistentlyoffering the best performance. Finally, we report the performances fromiterative prompting with auto-generated feedback and show that they are notmonotonic; they can peak early and then significantly decline in laterattempts. Overall, this work offers a new perspective on model reasoningperformance and the effectiveness of prompting strategies in combinatorialoptimization problems with preferential constraints.</description><author>Marylou Fauchard, Florian Carichon, Margarida Carvalho, Golnoosh Farnadi</author><pubDate>Tue, 16 Sep 2025 14:48:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13131v1</guid></item><item><title>WorldExplorer: Towards Generating Fully Navigable 3D Scenes</title><link>http://arxiv.org/abs/2506.01799v2</link><description>Generating 3D worlds from text is a highly anticipated goal in computervision. Existing works are limited by the degree of exploration they allowinside of a scene, i.e., produce streched-out and noisy artifacts when movingbeyond central or panoramic perspectives. To this end, we proposeWorldExplorer, a novel method based on autoregressive video trajectorygeneration, which builds fully navigable 3D scenes with consistent visualquality across a wide range of viewpoints. We initialize our scenes by creatingmulti-view consistent images corresponding to a 360 degree panorama. Then, weexpand it by leveraging video diffusion models in an iterative scene generationpipeline. Concretely, we generate multiple videos along short, pre-definedtrajectories, that explore the scene in depth, including motion around objects.Our novel scene memory conditions each video on the most relevant prior views,while a collision-detection mechanism prevents degenerate results, like movinginto objects. Finally, we fuse all generated views into a unified 3Drepresentation via 3D Gaussian Splatting optimization. Compared to priorapproaches, WorldExplorer produces high-quality scenes that remain stable underlarge camera motion, enabling for the first time realistic and unrestrictedexploration. We believe this marks a significant step toward generatingimmersive and truly explorable virtual 3D environments.</description><author>Manuel-Andreas Schneider, Lukas Höllein, Matthias Nießner</author><pubDate>Tue, 16 Sep 2025 14:47:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.01799v2</guid></item><item><title>Optimal Conformal Prediction, E-values, Fuzzy Prediction Sets and Subsequent Decisions</title><link>http://arxiv.org/abs/2509.13130v1</link><description>We make three contributions to conformal prediction. First, we propose fuzzyconformal confidence sets that offer a degree of exclusion, generalizing beyondthe binary inclusion/exclusion offered by classical confidence sets. We connectfuzzy confidence sets to e-values to show this degree of exclusion isequivalent to an exclusion at different confidence levels, capturing preciselywhat e-values bring to conformal prediction. We show that a fuzzy confidenceset is a predictive distribution with a more appropriate error guarantee.Second, we derive optimal conformal confidence sets by interpreting theminimization of the expected measure of the confidence set as an optimaltesting problem against a particular alternative. We use this to characterizeexactly in what sense traditional conformal prediction is optimal. Third, wegeneralize the inheritance of guarantees by subsequent minimax decisions fromconfidence sets to fuzzy confidence sets. All our results generalize beyond theexchangeable conformal setting to prediction sets for arbitrary models. Inparticular, we find that any valid test (e-value) for a hypothesisautomatically defines a (fuzzy) prediction confidence set.</description><author>Nick W. Koning, Sam van Meer</author><pubDate>Tue, 16 Sep 2025 14:46:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13130v1</guid></item><item><title>Second-Order Tensorial Partial Differential Equations on Graphs</title><link>http://arxiv.org/abs/2509.02015v3</link><description>Processing data on multiple interacting graphs is crucial for manyapplications, but existing approaches rely mostly on discrete filtering orfirst-order continuous models, dampening high frequencies and slow informationpropagation. In this paper, we introduce second-order tensorial partialdifferential equations on graphs (SoTPDEG) and propose the first theoreticallygrounded framework for second-order continuous product graph neural networks(GNNs). Our method exploits the separability of cosine kernels in Cartesianproduct graphs to enable efficient spectral decomposition while preservinghigh-frequency components. We further provide rigorous over-smoothing andstability analysis under graph perturbations, establishing a solid theoreticalfoundation. Experimental results on spatiotemporal traffic forecastingillustrate the superiority over the compared methods.</description><author>Aref Einizade, Fragkiskos D. Malliaros, Jhony H. Giraldo</author><pubDate>Tue, 16 Sep 2025 14:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.02015v3</guid></item><item><title>The Belief State Transformer</title><link>http://arxiv.org/abs/2410.23506v3</link><description>We introduce the "Belief State Transformer", a next-token predictor thattakes both a prefix and suffix as inputs, with a novel objective of predictingboth the next token for the prefix and the previous token for the suffix. TheBelief State Transformer effectively learns to solve challenging problems thatconventional forward-only transformers struggle with, in a domain-independentfashion. Key to this success is learning a compact belief state that capturesall relevant information necessary for accurate predictions. Empiricalablations show that each component of the model is essential in difficultscenarios where standard Transformers fall short. For the task of story writingwith known prefixes and suffixes, our approach outperforms theFill-in-the-Middle method for reaching known goals and demonstrates improvedperformance even when the goals are unknown. Altogether, the Belief StateTransformer enables more efficient goal-conditioned decoding, better test-timeinference, and high-quality text representations on small scale problems.Website: https://edwhu.github.io/bst-website</description><author>Edward S. Hu, Kwangjun Ahn, Qinghua Liu, Haoran Xu, Manan Tomar, Ada Langford, Jayden Teoh, Bryon Xu, David Yan, Dinesh Jayaraman, Alex Lamb, John Langford</author><pubDate>Tue, 16 Sep 2025 14:40:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23506v3</guid></item><item><title>Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning</title><link>http://arxiv.org/abs/2509.13127v1</link><description>Recent advancements in Large Language Models(LLMs) have led to thedevelopment of LLM-based AI agents. A key challenge is the creation of agentsthat can effectively ground themselves in complex, adversarial long-horizonenvironments. Existing methods mainly focus on (1) using LLMs as policies tointeract with the environment through generating low-level feasible actions,and (2) utilizing LLMs to generate high-level tasks or language guides tostimulate action generation. However, the former struggles to generate reliableactions, while the latter relies heavily on expert experience to translatehigh-level tasks into specific action sequences. To address these challenges,we introduce the Plan with Language, Act with Parameter (PLAP) planningframework that facilitates the grounding of LLM-based agents in long-horizonenvironments. The PLAP method comprises three key components: (1) a skilllibrary containing environment-specific parameterized skills, (2) a skillplanner powered by LLMs, and (3) a skill executor converting the parameterizedskills into executable action sequences. We implement PLAP in MicroRTS, along-horizon real-time strategy game that provides an unfamiliar andchallenging environment for LLMs. The experimental results demonstrate theeffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot settingoutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefullycrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.Additionally, we design comprehensive evaluation metrics and test 6closed-source and 2 open-source LLMs within the PLAP framework, ultimatelyreleasing an LLM leaderboard ranking long-horizon skill planning ability. Ourcode is available at https://github.com/AI-Research-TeamX/PLAP.</description><author>Sijia Cui, Shuai Xu, Aiyao He, Yanna Wang, Bo Xu</author><pubDate>Tue, 16 Sep 2025 14:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13127v1</guid></item></channel></rss>