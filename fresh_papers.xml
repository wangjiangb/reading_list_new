<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 29 Dec 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models</title><link>http://arxiv.org/abs/2412.18609v1</link><description>We present an efficient encoder-free approach for video-languageunderstanding that achieves competitive performance while significantlyreducing computational overhead. Current video-language models typically relyon heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4Bparameters), creating a substantial computational burden when processingmulti-frame videos. Our method introduces a novel Spatio-Temporal AlignmentBlock (STAB) that directly processes video inputs without requiring pre-trainedencoders while using only 45M parameters for visual processing - at least a6.5$\times$ reduction compared to traditional approaches. The STAB architecturecombines Local Spatio-Temporal Encoding for fine-grained feature extraction,efficient spatial downsampling through learned attention and separatemechanisms for modeling frame-level and video-level relationships. Our modelachieves comparable or superior performance to encoder-based approaches foropen-ended video question answering on standard benchmarks. The fine-grainedvideo question-answering evaluation demonstrates our model's effectiveness,outperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in keyaspects like correctness and temporal understanding. Extensive ablation studiesvalidate our architectural choices and demonstrate the effectiveness of ourspatio-temporal modeling approach while achieving 3-4$\times$ faster processingspeeds than previous methods. Code is available at\url{https://github.com/jh-yi/Video-Panda}.</description><author>Jinhui Yi, Syed Talal Wasim, Yanan Luo, Muzammal Naseer, Juergen Gall</author><pubDate>Tue, 24 Dec 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18609v1</guid></item><item><title>PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models</title><link>http://arxiv.org/abs/2412.18608v1</link><description>Text- or image-to-3D generators and 3D scanners can now produce 3D assetswith high-quality shapes and textures. These assets typically consist of asingle, fused representation, like an implicit neural field, a Gaussianmixture, or a mesh, without any useful structure. However, most applicationsand creative workflows require assets to be made of several meaningful partsthat can be manipulated independently. To address this gap, we introducePartGen, a novel approach that generates 3D objects composed of meaningfulparts starting from text, an image, or an unstructured 3D object. First, givenmultiple views of a 3D object, generated or rendered, a multi-view diffusionmodel extracts a set of plausible and view-consistent part segmentations,dividing the object into parts. Then, a second multi-view diffusion model takeseach part separately, fills in the occlusions, and uses those completed viewsfor 3D reconstruction by feeding them to a 3D reconstruction network. Thiscompletion process considers the context of the entire object to ensure thatthe parts integrate cohesively. The generative completion model can make up forthe information missing due to occlusions; in extreme cases, it can hallucinateentirely invisible parts based on the input 3D asset. We evaluate our method ongenerated and real 3D assets and show that it outperforms segmentation andpart-extraction baselines by a large margin. We also showcase downstreamapplications such as 3D part editing.</description><author>Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, Andrea Vedaldi</author><pubDate>Tue, 24 Dec 2024 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18608v1</guid></item><item><title>DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers</title><link>http://arxiv.org/abs/2412.18607v1</link><description>World model-based searching and planning are widely recognized as a promisingpath toward human-level physical intelligence. However, current driving worldmodels primarily rely on video diffusion models, which specialize in visualgeneration but lack the flexibility to incorporate other modalities likeaction. In contrast, autoregressive transformers have demonstrated exceptionalcapability in modeling multimodal data. Our work aims to unify both drivingmodel simulation and trajectory planning into a single sequence modelingproblem. We introduce a multimodal driving language based on interleaved imageand action tokens, and develop DrivingGPT to learn joint world modeling andplanning through standard next-token prediction. Our DrivingGPT demonstratesstrong performance in both action-conditioned video generation and end-to-endplanning, outperforming strong baselines on large-scale nuPlan and NAVSIMbenchmarks.</description><author>Yuntao Chen, Yuqi Wang, Zhaoxiang Zhang</author><pubDate>Tue, 24 Dec 2024 18:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18607v1</guid></item><item><title>Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models</title><link>http://arxiv.org/abs/2412.18605v1</link><description>Orientation is a key attribute of objects, crucial for understanding theirspatial pose and arrangement in images. However, practical solutions foraccurate orientation estimation from a single image remain underexplored. Inthis work, we introduce Orient Anything, the first expert and foundationalmodel designed to estimate object orientation in a single- and free-view image.Due to the scarcity of labeled data, we propose extracting knowledge from the3D world. By developing a pipeline to annotate the front face of 3D objects andrender images from random views, we collect 2M images with precise orientationannotations. To fully leverage the dataset, we design a robust trainingobjective that models the 3D orientation as probability distributions of threeangles and predicts the object orientation by fitting these distributions.Besides, we employ several strategies to improve synthetic-to-real transfer.Our model achieves state-of-the-art orientation estimation accuracy in bothrendered and real images and exhibits impressive zero-shot ability in variousscenarios. More importantly, our model enhances many applications, such ascomprehension and generation of complex spatial concepts and 3D object poseadjustment.</description><author>Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, Zhou Zhao</author><pubDate>Tue, 24 Dec 2024 18:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18605v1</guid></item><item><title>Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2412.18604v1</link><description>Classifiers are important components in many computer vision tasks, servingas the foundational backbone of a wide variety of models employed acrossdiverse applications. However, understanding the decision-making process ofclassifiers remains a significant challenge. We propose DiffEx, a novel methodthat leverages the capabilities of text-to-image diffusion models to explainclassifier decisions. Unlike traditional GAN-based explainability models, whichare limited to simple, single-concept analyses and typically require training anew model for each classifier, our approach can explain classifiers that focuson single concepts (such as faces or animals) as well as those that handlecomplex scenes involving multiple concepts. DiffEx employs vision-languagemodels to create a hierarchical list of semantics, allowing users to identifynot only the overarching semantic influences on classifiers (e.g., the 'beard'semantic in a facial classifier) but also their sub-types, such as 'goatee' or'Balbo' beard. Our experiments demonstrate that DiffEx is able to cover asignificantly broader spectrum of semantics compared to its GAN counterparts,providing a hierarchical tool that delivers a more detailed and fine-grainedunderstanding of classifier decisions.</description><author>Tahira Kazimi, Ritika Allada, Pinar Yanardag</author><pubDate>Tue, 24 Dec 2024 18:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18604v1</guid></item><item><title>Long-Form Speech Generation with Spoken Language Models</title><link>http://arxiv.org/abs/2412.18603v1</link><description>We consider the generative modeling of speech over multiple minutes, arequirement for long-form multimedia generation and audio-native voiceassistants. However, current spoken language models struggle to generateplausible speech past tens of seconds, from high temporal resolution of speechtokens causing loss of coherence, to architectural issues with long-sequencetraining or extrapolation, to memory costs at inference time. With theseconsiderations we propose SpeechSSM, the first speech language model to learnfrom and sample long-form spoken audio (e.g., 16 minutes of read orextemporaneous speech) in a single decoding session without text intermediates,based on recent advances in linear-time sequence modeling. Furthermore, toaddress growing challenges in spoken language evaluation, especially in thisnew long-form setting, we propose: new embedding-based and LLM-judged metrics;quality measurements over length and time; and a new benchmark for long-formspeech processing and generation, LibriSpeech-Long. Speech samples and thedataset are released athttps://google.github.io/tacotron/publications/speechssm/</description><author>Se Jin Park, Julian Salazar, Aren Jansen, Keisuke Kinoshita, Yong Man Ro, RJ Skerry-Ryan</author><pubDate>Tue, 24 Dec 2024 18:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18603v1</guid></item><item><title>Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems</title><link>http://arxiv.org/abs/2412.18601v1</link><description>In the rapidly evolving landscape of GameFi, a fusion of gaming anddecentralized finance (DeFi), there exists a critical need to enhance playerengagement and economic interaction within gaming ecosystems. Our GameFiecosystem aims to fundamentally transform this landscape by integratingadvanced embodied AI agents into GameFi platforms. These AI agents, developedusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,are capable of proactive, adaptive, and contextually rich interactions withplayers. By going beyond traditional scripted responses, these agents becomeintegral participants in the game's narrative and economic systems, directlyinfluencing player strategies and in-game economies. We address the limitationsof current GameFi platforms, which often lack immersive AI interactions andmechanisms for community engagement or creator monetization. Through the deepintegration of AI agents with blockchain technology, we establish aconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowerscreators to monetize their contributions and fosters democratic collaborationamong players and creators. Furthermore, by embedding DeFi mechanisms into thegaming experience, we enhance economic participation and provide newopportunities for financial interactions within the game. Our approach enhancesplayer immersion and retention and advances the GameFi ecosystem by bridgingtraditional gaming with Web3 technologies. By integrating sophisticated AI andDeFi elements, we contribute to the development of more engaging, economicallyrobust, and community-centric gaming environments. This project represents asignificant advancement in the state-of-the-art in GameFi, offering insightsand methodologies that can be applied throughout the gaming industry.</description><author>Fernando Jia, Jade Zheng, Florence Li</author><pubDate>Tue, 24 Dec 2024 18:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18601v1</guid></item><item><title>ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation</title><link>http://arxiv.org/abs/2412.18600v1</link><description>Human-scene interaction (HSI) generation is crucial for applications inembodied AI, virtual reality, and robotics. While existing methods cansynthesize realistic human motions in 3D scenes and generate plausiblehuman-object interactions, they heavily rely on datasets containing paired 3Dscene and motion capture data, which are expensive and time-consuming tocollect across diverse environments and interactions. We present ZeroHSI, anovel approach that enables zero-shot 4D human-scene interaction synthesis byintegrating video generation and neural human rendering. Our key insight is toleverage the rich motion priors learned by state-of-the-art video generationmodels, which have been trained on vast amounts of natural human movements andinteractions, and use differentiable rendering to reconstruct human-sceneinteractions. ZeroHSI can synthesize realistic human motions in both staticscenes and environments with dynamic objects, without requiring anyground-truth motion data. We evaluate ZeroHSI on a curated dataset of differenttypes of various indoor and outdoor scenes with different interaction prompts,demonstrating its ability to generate diverse and contextually appropriatehuman-scene interactions.</description><author>Hongjie Li, Hong-Xing Yu, Jiaman Li, Jiajun Wu</author><pubDate>Tue, 24 Dec 2024 18:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18600v1</guid></item><item><title>DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation</title><link>http://arxiv.org/abs/2412.18597v1</link><description>Sora-like video generation models have achieved remarkable progress with aMulti-Modal Diffusion Transformer MM-DiT architecture. However, the currentvideo generation models predominantly focus on single-prompt, struggling togenerate coherent scenes with multiple sequential prompts that better reflectreal-world dynamic scenarios. While some pioneering works have exploredmulti-prompt video generation, they face significant challenges includingstrict training data requirements, weak prompt following, and unnaturaltransitions. To address these problems, we propose DiTCtrl, a training-freemulti-prompt video generation method under MM-DiT architectures for the firsttime. Our key idea is to take the multi-prompt video generation task astemporal video editing with smooth transitions. To achieve this goal, we firstanalyze MM-DiT's attention mechanism, finding that the 3D full attentionbehaves similarly to that of the cross/self-attention blocks in the UNet-likediffusion models, enabling mask-guided precise semantic control acrossdifferent prompts with attention sharing for multi-prompt video generation.Based on our careful design, the video generated by DiTCtrl achieves smoothtransitions and consistent object motion given multiple sequential promptswithout additional training. Besides, we also present MPVBench, a new benchmarkspecially designed for multi-prompt video generation to evaluate theperformance of multi-prompt generation. Extensive experiments demonstrate thatour method achieves state-of-the-art performance without additional training.</description><author>Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, Xiangyu Yue</author><pubDate>Tue, 24 Dec 2024 18:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18597v1</guid></item><item><title>LatentCRF: Continuous CRF for Efficient Latent Diffusion</title><link>http://arxiv.org/abs/2412.18596v1</link><description>Latent Diffusion Models (LDMs) produce high-quality, photo-realistic images,however, the latency incurred by multiple costly inference iterations canrestrict their applicability. We introduce LatentCRF, a continuous ConditionalRandom Field (CRF) model, implemented as a neural network layer, that modelsthe spatial and semantic relationships among the latent vectors in the LDM. Byreplacing some of the computationally-intensive LDM inference iterations withour lightweight LatentCRF, we achieve a superior balance between quality, speedand diversity. We increase inference efficiency by 33% with no loss in imagequality or diversity compared to the full LDM. LatentCRF is an easy add-on,which does not require modifying the LDM.</description><author>Kanchana Ranasinghe, Sadeep Jayasumana, Andreas Veit, Ayan Chakrabarti, Daniel Glasner, Michael S Ryoo, Srikumar Ramalingam, Sanjiv Kumar</author><pubDate>Tue, 24 Dec 2024 18:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18596v1</guid></item><item><title>Structure Learning in Gaussian Graphical Models from Glauber Dynamics</title><link>http://arxiv.org/abs/2412.18594v1</link><description>Gaussian graphical model selection is an important paradigm with numerousapplications, including biological network modeling, financial networkmodeling, and social network analysis. Traditional approaches assume access toindependent and identically distributed (i.i.d) samples, which is oftenimpractical in real-world scenarios. In this paper, we address Gaussiangraphical model selection under observations from a more realistic dependentstochastic process known as Glauber dynamics. Glauber dynamics, also called theGibbs sampler, is a Markov chain that sequentially updates the variables of theunderlying model based on the statistics of the remaining model. Such models,aside from frequently being employed to generate samples from complexmultivariate distributions, naturally arise in various settings, such asopinion consensus in social networks and clearing/stock-price dynamics infinancial networks. In contrast to the extensive body of existing work, we present the firstalgorithm for Gaussian graphical model selection when data are sampledaccording to the Glauber dynamics. We provide theoretical guarantees on thecomputational and statistical complexity of the proposed algorithm's structurelearning performance. Additionally, we provide information-theoretic lowerbounds on the statistical complexity and show that our algorithm is nearlyminimax optimal for a broad class of problems.</description><author>Vignesh Tirukkonda, Anirudh Rayas, Gautam Dasarathy</author><pubDate>Tue, 24 Dec 2024 18:49:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18594v1</guid></item><item><title>ClassifyViStA:WCE Classification with Visual understanding through Segmentation and Attention</title><link>http://arxiv.org/abs/2412.18591v1</link><description>Gastrointestinal (GI) bleeding is a serious medical condition that presentssignificant diagnostic challenges, particularly in settings with limited accessto healthcare resources. Wireless Capsule Endoscopy (WCE) has emerged as apowerful diagnostic tool for visualizing the GI tract, but it requirestime-consuming manual analysis by experienced gastroenterologists, which isprone to human error and inefficient given the increasing number of patients.Toaddress this challenge, we propose ClassifyViStA, an AI-based frameworkdesigned for the automated detection and classification of bleeding andnon-bleeding frames from WCE videos. The model consists of a standardclassification path, augmented by two specialized branches: an implicitattention branch and a segmentation branch.The attention branch focuses on thebleeding regions, while the segmentation branch generates accurate segmentationmasks, which are used for classification and interpretability. The model isbuilt upon an ensemble of ResNet18 and VGG16 architectures to enhanceclassification performance. For the bleeding region detection, we implement aSoft Non-Maximum Suppression (Soft NMS) approach with YOLOv8, which improvesthe handling of overlapping bounding boxes, resulting in more accurate andnuanced detections.The system's interpretability is enhanced by using thesegmentation masks to explain the classification results, offering insightsinto the decision-making process similar to the way a gastroenterologistidentifies bleeding regions. Our approach not only automates the detection ofGI bleeding but also provides an interpretable solution that can ease theburden on healthcare professionals and improve diagnostic efficiency. Our codeis available at ClassifyViStA.</description><author>S. Balasubramanian, Ammu Abhishek, Yedu Krishna, Darshan Gera</author><pubDate>Tue, 24 Dec 2024 18:45:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18591v1</guid></item><item><title>Text-Driven Tumor Synthesis</title><link>http://arxiv.org/abs/2412.18589v1</link><description>Tumor synthesis can generate examples that AI often misses or over-detects,improving AI performance by training on these challenging cases. However,existing synthesis methods, which are typically unconditional -- generatingimages from random variables -- or conditioned only by tumor shapes, lackcontrollability over specific tumor characteristics such as texture,heterogeneity, boundaries, and pathology type. As a result, the generatedtumors may be overly similar or duplicates of existing training data, failingto effectively address AI's weaknesses. We propose a new text-driven tumorsynthesis approach, termed TextoMorph, that provides textual control over tumorcharacteristics. This is particularly beneficial for examples that confuse theAI the most, such as early tumor detection (increasing Sensitivity by +8.5%),tumor segmentation for precise radiotherapy (increasing DSC by +6.3%), andclassification between benign and malignant tumors (improving Sensitivity by+8.2%). By incorporating text mined from radiology reports into the synthesisprocess, we increase the variability and controllability of the synthetictumors to target AI's failure cases more precisely. Moreover, TextoMorph usescontrastive learning across different texts and CT scans, significantlyreducing dependence on scarce image-report pairs (only 141 pairs used in thisstudy) by leveraging a large corpus of 34,035 radiology reports. Finally, wehave developed rigorous tests to evaluate synthetic tumors, includingText-Driven Visual Turing Test and Radiomics Pattern Analysis, showing that oursynthetic tumors is realistic and diverse in texture, heterogeneity,boundaries, and pathology.</description><author>Xinran Li, Yi Shuai, Chen Liu, Qi Chen, Qilong Wu, Pengfei Guo, Dong Yang, Can Zhao, Pedro R. A. S. Bassi, Daguang Xu, Kang Wang, Yang Yang, Alan Yuille, Zongwei Zhou</author><pubDate>Tue, 24 Dec 2024 18:43:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18589v1</guid></item><item><title>A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs</title><link>http://arxiv.org/abs/2412.18588v1</link><description>Large Language Models (LLMs) are compact representations of all publicknowledge of our physical environment and animal and human behaviors. Theapplication of LLMs to robotics may offer a path to highly capable robots thatperform well across most human tasks with limited or even zero tuning. Asidefrom increasingly sophisticated reasoning and task planning, networks of(suitably designed) LLMs offer ease of upgrading capabilities and allow humansto directly observe the robot's thinking. Here we explore the advantages,limitations, and particularities of using LLMs to control physical robots. Thebasic system consists of four LLMs communicating via a human language data busimplemented via web sockets and ROS2 message passing. Surprisingly, rich robotbehaviors and good performance across different tasks could be achieved despitethe robot's data fusion cycle running at only 1Hz and the central data busrunning at the extremely limited rates of the human brain, of around 40 bits/s.The use of natural language for inter-LLM communication allowed the robot'sreasoning and decision making to be directly observed by humans and made ittrivial to bias the system's behavior with sets of rules written in plainEnglish. These rules were immutably written into Ethereum, a global, public,and censorship resistant Turing-complete computer. We suggest that by usingnatural language as the data bus among interacting AIs, and immutable publicledgers to store behavior constraints, it is possible to build robots thatcombine unexpectedly rich performance, upgradability, and durable alignmentwith humans.</description><author>OpenMind, Shaohong Zhong, Adam Zhou, Boyuan Chen, Homin Luo, Jan Liphardt</author><pubDate>Tue, 24 Dec 2024 18:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18588v1</guid></item><item><title>Refining CNN-based Heatmap Regression with Gradient-based Corner Points for Electrode Localization</title><link>http://arxiv.org/abs/2412.17105v2</link><description>We propose a method for detecting the electrode positions in lithium-ionbatteries. The process begins by identifying the region of interest (ROI) inthe battery's X-ray image through corner point detection. A convolutionalneural network is then used to regress the pole positions within this ROI.Finally, the regressed positions are optimized and corrected using corner pointpriors, significantly mitigating the loss of localization accuracy caused byoperations such as feature map down-sampling and padding during networktraining. Our findings show that combining traditional pixel gradient analysiswith CNN-based heatmap regression for keypoint extraction enhances bothaccuracy and efficiency, resulting in significant performance improvements.</description><author>Lin Wu</author><pubDate>Tue, 24 Dec 2024 18:27:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17105v2</guid></item><item><title>Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors: Diverse-Resolution Training Outperforms Interpolation</title><link>http://arxiv.org/abs/2412.18584v1</link><description>Deep learning-based 3D imaging, in particular magnetic resonance imaging(MRI), is challenging because of limited availability of 3D training data.Therefore, 2D diffusion models trained on 2D slices are starting to beleveraged for 3D MRI reconstruction. However, as we show in this paper,existing methods pertain to a fixed voxel size, and performance degrades whenthe voxel size is varied, as it is often the case in clinical practice. In thispaper, we propose and study several approaches for resolution-robust 3D MRIreconstruction with 2D diffusion priors. As a result of this investigation, weobtain a simple resolution-robust variational 3D reconstruction approach basedon diffusion-guided regularization of randomly sampled 2D slices. This methodprovides competitive reconstruction quality compared to posterior samplingbaselines. Towards resolving the sensitivity to resolution-shifts, weinvestigate state-of-the-art model-based approaches including Gaussiansplatting, neural representations, and infinite-dimensional diffusion models,as well as a simple data-centric approach of training the diffusion model onseveral resolutions. Our experiments demonstrate that the model-basedapproaches fail to close the performance gap in 3D MRI. In contrast, thedata-centric approach of training the diffusion model on various resolutionseffectively provides a resolution-robust method without compromising accuracy.</description><author>Anselm Krainovic, Stefan Ruschke, Reinhard Heckel</author><pubDate>Tue, 24 Dec 2024 18:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18584v1</guid></item><item><title>Exploring Embedding Priors in Prompt-Tuning for Improved Interpretability and Control</title><link>http://arxiv.org/abs/2412.18582v1</link><description>Prompt-Tuning is an efficient method for adapting pre-trained language modelsto new tasks with minimal computational overhead by modifying promptembeddings. In this work, we investigate how crucial the phenomenon ofembedding collapse, frequently observed in Prompt-Tuning, is for the finalperformance of the model. To address this question, we designed embeddingpriors and compared them with posteriors of the converged Soft and DeepPrompt-Tuning methods. Our findings suggest that priors strongly affect theposition of the tuned embeddings, and models can effectively work withembeddings from different parts of activation spaces, including completely newregions. As the final Prompt-Tuning capabilities are limited, we hypothesizethat controllable Prompt-Tuning posteriors may serve as a good starting pointfor tasks such as chain-of-thought (COT) distillation. Our experiments alsoshow that generated trajectories are not localized in the activation space ofthe models. However, there are distinct clusters of activations for distanttasks (e.g., NLP and arithmetic), while activations between NLP tasks (e.g.,Question-Answering and MLM) lie in the same cluster. These observations raisequestions about the importance of a single activation cluster for thegeneralization abilities of large language models.</description><author>Sergey Sedov, Sumanth Bharadwaj Hachalli Karanam, Venu Gopal Kadamba</author><pubDate>Tue, 24 Dec 2024 18:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18582v1</guid></item><item><title>ReducedLUT: Table Decomposition with "Don't Care" Conditions</title><link>http://arxiv.org/abs/2412.18579v1</link><description>Lookup tables (LUTs) are frequently used to efficiently store arrays ofprecomputed values for complex mathematical computations. When used in thecontext of neural networks, these functions exhibit a lack of recognizablepatterns which presents an unusual challenge for conventional logic synthesistechniques. Several approaches are known to break down a single large lookuptable into multiple smaller ones that can be recombined. Traditional methods,such as plain tabulation, piecewise linear approximation, and multipartitetable methods, often yield inefficient hardware solutions when applied toLUT-based NNs. This paper introduces ReducedLUT, a novel method to reduce the footprint ofthe LUTs by injecting don't cares into the compression process. This additionalfreedom introduces more self-similarities which can be exploited using knowndecomposition techniques. We then demonstrate a particular application tomachine learning; by replacing unobserved patterns within the training data ofneural network models with don't cares, we enable greater compression withminimal model accuracy degradation. In practice, we achieve up to $1.63\times$reduction in Physical LUT utilization, with a test accuracy drop of no morethan $0.01$ accuracy points.</description><author>Oliver Cassidy, Marta Andronic, Samuel Coward, George A. Constantinides</author><pubDate>Tue, 24 Dec 2024 18:11:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18579v1</guid></item><item><title>Principal Component Flow Map Learning of PDEs from Incomplete, Limited, and Noisy Data</title><link>http://arxiv.org/abs/2407.10854v2</link><description>We present a computational technique for modeling the evolution of dynamicalsystems in a reduced basis, with a focus on the challenging problem of modelingpartially-observed partial differential equations (PDEs) on high-dimensionalnon-uniform grids. We address limitations of previous work on data-driven flowmap learning in the sense that we focus on noisy and limited data to movetoward data collection scenarios in real-world applications. Leveraging recentwork on modeling PDEs in modal and nodal spaces, we present a neural networkstructure that is suitable for PDE modeling with noisy and limited dataavailable only on a subset of the state variables or computational domain. Inparticular, spatial grid-point measurements are reduced using a learned lineartransformation, after which the dynamics are learned in this reduced basisbefore being transformed back out to the nodal space. This approach yields adrastically reduced parameterization of the neural network compared withprevious flow map models for nodal space learning. This allows for rapidhigh-resolution simulations, enabled by smaller training data sets and reducedtraining times.</description><author>Victor Churchill</author><pubDate>Tue, 24 Dec 2024 18:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10854v2</guid></item><item><title>Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning</title><link>http://arxiv.org/abs/2412.10924v3</link><description>Tokenization is a necessary component within the current architecture of manylanguage models, including the transformer-based large language models (LLMs)of Generative AI, yet its impact on the model's cognition is often overlooked.We argue that LLMs demonstrate that the Distributional Hypothesis (DH) issufficient for reasonably human-like language performance, and that theemergence of human-meaningful linguistic units among tokens motivateslinguistically-informed interventions in existing, linguistically-agnostictokenization techniques, particularly with respect to their roles as (1)semantic primitives and as (2) vehicles for conveying salient distributionalpatterns from human language to the model. We explore tokenizations from a BPEtokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;and the information in exemplar token vectors as they move through the layersof a RoBERTa (large) model. Besides creating sub-optimal semantic buildingblocks and obscuring the model's access to the necessary distributionalpatterns, we describe how tokenization pretraining can be a backdoor for biasand other unwanted content, which current alignment practices may notremediate. Additionally, we relay evidence that the tokenization algorithm'sobjective function impacts the LLM's cognition, despite being meaningfullyinsulated from the main system intelligence.</description><author>Julia Witte Zimmerman, Denis Hudon, Kathryn Cramer, Alejandro J. Ruiz, Calla Beauregard, Ashley Fehr, Mikaela Irene Fudolig, Bradford Demarest, Yoshi Meke Bird, Milo Z. Trujillo, Christopher M. Danforth, Peter Sheridan Dodds</author><pubDate>Tue, 24 Dec 2024 17:56:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10924v3</guid></item><item><title>How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation</title><link>http://arxiv.org/abs/2412.18573v1</link><description>Recently, an increasing number of AI-driven programming assistants powered bycode LLMs have been integrated into various real-world software developmentenvironments, significantly boosting developer productivity. However, existingcode generation benchmarks primarily focus on general-purpose scenarios,leaving the code generation performance of LLMs for specific applicationdomains largely unknown. In this paper, we introduce a new benchmark,MultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programmingtasks, covering 12 popular software development domains and 15 programminglanguages. Specifically, we perform in-depth research to identify these 12application domains. Given that each domain may involve multiple technicalframeworks, and that different frameworks present distinct challenges in thecoding process, we categorize the commonly used frameworks and platforms withineach domain. We then sample programming problems from GitHub repositoriesrelated to these subdomains. To ensure the quality of the tasks and mitigatedata leakage issues, we invite annotators to rewrite the docstrings for eachtask in MultiCodeBench. Additionally, we build a static analysis-baseddependency parsing tool to extract the dependencies in the ground truth foreach task, enabling deeper performance analysis. Through extensive experimentson MultiCodeBench with eleven representative mainstream LLMs, we reveal thecode generation performance of the LLMs across different application domains,providing practical insights for developers in downstream fields when selectingLLMs. Furthermore, we analyze the reasons behind the models' failures incompleting software application development tasks, offering guidance for modeldevelopers to enhance domain-specific code generation capabilities.</description><author>Dewu Zheng, Yanlin Wang, Ensheng Shi, Hongyu Zhang, Zibin Zheng</author><pubDate>Tue, 24 Dec 2024 17:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18573v1</guid></item><item><title>Scalable Quantum-Inspired Optimization through Dynamic Qubit Compression</title><link>http://arxiv.org/abs/2412.18571v1</link><description>Hard combinatorial optimization problems, often mapped to Ising models,promise potential solutions with quantum advantage but are constrained bylimited qubit counts in near-term devices. We present an innovativequantum-inspired framework that dynamically compresses large Ising models tofit available quantum hardware of different sizes. Thus, we aim to bridge thegap between large-scale optimization and current hardware capabilities. Ourmethod leverages a physics-inspired GNN architecture to capture complexinteractions in Ising models and accurately predict alignments amongneighboring spins (aka qubits) at ground states. By progressively merging suchaligned spins, we can reduce the model size while preserving the underlyingoptimization structure. It also provides a natural trade-off between thesolution quality and size reduction, meeting different hardware constraints ofquantum computing devices. Extensive numerical studies on Ising instances ofdiverse topologies show that our method can reduce instance size at multiplelevels with virtually no losses in solution quality on the latest D-wavequantum annealers.</description><author>Co Tran, Quoc-Bao Tran, Hy Truong Son, Thang N Dinh</author><pubDate>Tue, 24 Dec 2024 17:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18571v1</guid></item><item><title>Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation</title><link>http://arxiv.org/abs/2412.16135v2</link><description>Malware authors often employ code obfuscations to make their malware harderto detect. Existing tools for generating obfuscated code often require accessto the original source code (e.g., C++ or Java), and adding new obfuscations isa non-trivial, labor-intensive process. In this study, we ask the followingquestion: Can Large Language Models (LLMs) potentially generate a newobfuscated assembly code? If so, this poses a risk to anti-virus engines andpotentially increases the flexibility of attackers to create new obfuscationpatterns. We answer this in the affirmative by developing the MetamorphASMbenchmark comprising MetamorphASM Dataset (MAD) along with three codeobfuscation techniques: dead code, register substitution, and control flowchange. The MetamorphASM systematically evaluates the ability of LLMs togenerate and analyze obfuscated code using MAD, which contains 328,200obfuscated assembly code samples. We release this dataset and analyze thesuccess rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assemblycode. The evaluation was performed using established information-theoreticmetrics and manual human review to ensure correctness and provide thefoundation for researchers to study and develop remediations to this risk. Thesource code can be found at the following GitHub link:https://github.com/mohammadi-ali/MetamorphASM.</description><author>Seyedreza Mohseni, Seyedali Mohammadi, Deepa Tilwani, Yash Saxena, Gerald Ndawula, Sriram Vema, Edward Raff, Manas Gaur</author><pubDate>Tue, 24 Dec 2024 17:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.16135v2</guid></item><item><title>HNCI: High-Dimensional Network Causal Inference</title><link>http://arxiv.org/abs/2412.18568v1</link><description>The problem of evaluating the effectiveness of a treatment or policy commonlyappears in causal inference applications under network interference. In thispaper, we suggest the new method of high-dimensional network causal inference(HNCI) that provides both valid confidence interval on the average directtreatment effect on the treated (ADET) and valid confidence set for theneighborhood size for interference effect. We exploit the model setting inBelloni et al. (2022) and allow certain type of heterogeneity in nodeinterference neighborhood sizes. We propose a linear regression formulation ofpotential outcomes, where the regression coefficients correspond to theunderlying true interference function values of nodes and exhibit a latenthomogeneous structure. Such a formulation allows us to leverage existingliterature from linear regression and homogeneity pursuit to conduct validstatistical inferences with theoretical guarantees. The resulting confidenceintervals for the ADET are formally justified through asymptotic normalitieswith estimable variances. We further provide the confidence set for theneighborhood size with theoretical guarantees exploiting the repro samplesapproach. The practical utilities of the newly suggested methods aredemonstrated through simulation and real data examples.</description><author>Wenqin Du, Rundong Ding, Yingying Fan, Jinchi Lv</author><pubDate>Tue, 24 Dec 2024 17:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18568v1</guid></item><item><title>Zero-resource Speech Translation and Recognition with LLMs</title><link>http://arxiv.org/abs/2412.18566v1</link><description>Despite recent advancements in speech processing, zero-resource speechtranslation (ST) and automatic speech recognition (ASR) remain challengingproblems. In this work, we propose to leverage a multilingual Large LanguageModel (LLM) to perform ST and ASR in languages for which the model has neverseen paired audio-text data. We achieve this by using a pre-trainedmultilingual speech encoder, a multilingual LLM, and a lightweight adaptationmodule that maps the audio representations to the token embedding space of theLLM. We perform several experiments both in ST and ASR to understand how tobest train the model and what data has the most impact on performance inpreviously unseen languages. In ST, our best model is capable to achieve BLEUscores over 23 in CoVoST2 for two previously unseen languages, while in ASR, weachieve WERs of up to 28.2\%. We finally show that the performance of oursystem is bounded by the ability of the LLM to output text in the desiredlanguage.</description><author>Karel Mundnich, Xing Niu, Prashant Mathur, Srikanth Ronanki, Brady Houston, Veera Raghavendra Elluru, Nilaksh Das, Zejiang Hou, Goeric Huybrechts, Anshu Bhatia, Daniel Garcia-Romero, Kyu J. Han, Katrin Kirchhoff</author><pubDate>Tue, 24 Dec 2024 17:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18566v1</guid></item><item><title>3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement</title><link>http://arxiv.org/abs/2412.18565v1</link><description>Despite advances in neural rendering, due to the scarcity of high-quality 3Ddatasets and the inherent limitations of multi-view diffusion models, viewsynthesis and 3D model generation are restricted to low resolutions withsuboptimal multi-view consistency. In this study, we present a novel 3Denhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latentdiffusion model to enhance coarse 3D inputs while preserving multi-viewconsistency. Our method includes a pose-aware encoder and a diffusion-baseddenoiser to refine low-quality multi-view images, along with data augmentationand a multi-view attention module with epipolar aggregation to maintainconsistent, high-quality 3D outputs across views. Unlike existing video-basedapproaches, our model supports seamless multi-view enhancement with improvedcoherence across diverse viewing angles. Extensive evaluations show that3DEnhancer significantly outperforms existing methods, boosting both multi-viewenhancement and per-instance 3D optimization tasks.</description><author>Yihang Luo, Shangchen Zhou, Yushi Lan, Xingang Pan, Chen Change Loy</author><pubDate>Tue, 24 Dec 2024 17:36:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18565v1</guid></item><item><title>Efficient Aircraft Design Optimization Using Multi-Fidelity Models and Multi-fidelity Physics Informed Neural Networks</title><link>http://arxiv.org/abs/2412.18564v1</link><description>Aircraft design optimization traditionally relies on computationallyexpensive simulation techniques such as Finite Element Method (FEM) and FiniteVolume Method (FVM), which, while accurate, can significantly slow down thedesign iteration process. The challenge lies in reducing the computationalcomplexity while maintaining high accuracy for quick evaluations of multipledesign alternatives. This research explores advanced methods, includingsurrogate models, reduced-order models (ROM), and multi-fidelity machinelearning techniques, to achieve more efficient aircraft design evaluations.Specifically, the study investigates the application of Multi-fidelityPhysics-Informed Neural Networks (MPINN) and autoencoders for manifoldalignment, alongside the potential of Generative Adversarial Networks (GANs)for refining design geometries. Through a proof-of-concept task, the researchdemonstrates the ability to predict high-fidelity results from low-fidelitysimulations, offering a path toward faster and more cost effective aircraftdesign iterations.</description><author>Apurba Sarker</author><pubDate>Tue, 24 Dec 2024 17:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18564v1</guid></item><item><title>Adversarial Attack Against Images Classification based on Generative Adversarial Networks</title><link>http://arxiv.org/abs/2412.16662v2</link><description>Adversarial attacks on image classification systems have always been animportant problem in the field of machine learning, and generative adversarialnetworks (GANs), as popular models in the field of image generation, have beenwidely used in various novel scenarios due to their powerful generativecapabilities. However, with the popularity of generative adversarial networks,the misuse of fake image technology has raised a series of security problems,such as malicious tampering with other people's photos and videos, and invasionof personal privacy. Inspired by the generative adversarial networks, this workproposes a novel adversarial attack method, aiming to gain insight into theweaknesses of the image classification system and improve its anti-attackability. Specifically, the generative adversarial networks are used to generateadversarial samples with small perturbations but enough to affect thedecision-making of the classifier, and the adversarial samples are generatedthrough the adversarial learning of the training generator and the classifier.From extensive experiment analysis, we evaluate the effectiveness of the methodon a classical image classification dataset, and the results show that ourmodel successfully deceives a variety of advanced classifiers while maintainingthe naturalness of adversarial samples.</description><author>Yahe Yang</author><pubDate>Tue, 24 Dec 2024 17:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.16662v2</guid></item><item><title>FedVCK: Non-IID Robust and Communication-Efficient Federated Learning via Valuable Condensed Knowledge for Medical Image Analysis</title><link>http://arxiv.org/abs/2412.18557v1</link><description>Federated learning has become a promising solution for collaboration amongmedical institutions. However, data owned by each institution would be highlyheterogeneous and the distribution is always non-independent and identicaldistribution (non-IID), resulting in client drift and unsatisfactoryperformance. Despite existing federated learning methods attempting to solvethe non-IID problems, they still show marginal advantages but rely on frequentcommunication which would incur high costs and privacy concerns. In this paper,we propose a novel federated learning method: \textbf{Fed}erated learning via\textbf{V}aluable \textbf{C}ondensed \textbf{K}nowledge (FedVCK). We enhancethe quality of condensed knowledge and select the most necessary knowledgeguided by models, to tackle the non-IID problem within limited communicationbudgets effectively. Specifically, on the client side, we condense theknowledge of each client into a small dataset and further enhance thecondensation procedure with latent distribution constraints, facilitating theeffective capture of high-quality knowledge. During each round, we specificallytarget and condense knowledge that has not been assimilated by the currentmodel, thereby preventing unnecessary repetition of homogeneous knowledge andminimizing the frequency of communications required. On the server side, wepropose relational supervised contrastive learning to provide more supervisionsignals to aid the global model updating. Comprehensive experiments acrossvarious medical tasks show that FedVCK can outperform state-of-the-art methods,demonstrating that it's non-IID robust and communication-efficient.</description><author>Guochen Yan, Luyuan Xie, Xinyi Gao, Wentao Zhang, Qingni Shen, Yuejian Fang, Zhonghai Wu</author><pubDate>Tue, 24 Dec 2024 17:20:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18557v1</guid></item><item><title>Distilling Fine-grained Sentiment Understanding from Large Language Models</title><link>http://arxiv.org/abs/2412.18552v1</link><description>Fine-grained sentiment analysis (FSA) aims to extract and summarize useropinions from vast opinionated text. Recent studies demonstrate that largelanguage models (LLMs) possess exceptional sentiment understandingcapabilities. However, directly deploying LLMs for FSA applications incurs highinference costs. Therefore, this paper investigates the distillation offine-grained sentiment understanding from LLMs into small language models(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviewsand then utilize the generated content to pretrain SLMs. Additionally, wedevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensiveexperiments on this benchmark reveal that: (1) distillation significantlyenhances the performance of SLMs in FSA tasks, achieving a 6.00\% improvementin $F_1$-score, and the distilled model can outperform Llama-2-7b with only220M parameters; (2) distillation equips SLMs with excellent zero-shotsentiment classification capabilities, enabling them to match or even exceedtheir teacher models. These results suggest that distillation from LLMs is ahighly promising direction for FSA. We will release our code, data, andpretrained model weights at\url{https://github.com/HITSZ-HLT/FSA-Distillation}.</description><author>Yice Zhang, Guangyu Xie, Hongling Xu, Kaiheng Hou, Jianzhu Bao, Qianlong Wang, Shiwei Chen, Ruifeng Xu</author><pubDate>Tue, 24 Dec 2024 17:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18552v1</guid></item><item><title>Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard of Safety and Capability</title><link>http://arxiv.org/abs/2412.18551v1</link><description>To address this gap, we introduce Libra-Leaderboard, a comprehensiveframework designed to rank LLMs through a balanced evaluation of performanceand safety. Combining a dynamic leaderboard with an interactive LLM arena,Libra-Leaderboard encourages the joint optimization of capability and safety.Unlike traditional approaches that average performance and safety metrics,Libra-Leaderboard uses a distance-to-optimal-score method to calculate theoverall rankings. This approach incentivizes models to achieve a balance ratherthan excelling in one dimension at the expense of some other ones. In the firstrelease, Libra-Leaderboard evaluates 26 mainstream LLMs from 14 leadingorganizations, identifying critical safety challenges even in state-of-the-artmodels.</description><author>Haonan Li, Xudong Han, Zenan Zhai, Honglin Mu, Hao Wang, Zhenxuan Zhang, Yilin Geng, Shom Lin, Renxi Wang, Artem Shelmanov, Xiangyu Qi, Yuxia Wang, Donghai Hong, Youliang Yuan, Meng Chen, Haoqin Tu, Fajri Koto, Tatsuki Kuribayashi, Cong Zeng, Rishabh Bhardwaj, Bingchen Zhao, Yawen Duan, Yi Liu, Emad A. Alghamdi, Yaodong Yang, Yinpeng Dong, Soujanya Poria, Pengfei Liu, Zhengzhong Liu, Xuguang Ren, Eduard Hovy, Iryna Gurevych, Preslav Nakov, Monojit Choudhury, Timothy Baldwin</author><pubDate>Tue, 24 Dec 2024 17:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18551v1</guid></item><item><title>SIGMA: Selective Gated Mamba for Sequential Recommendation</title><link>http://arxiv.org/abs/2408.11451v4</link><description>In various domains, Sequential Recommender Systems (SRS) have becomeessential due to their superior capability to discern intricate userpreferences. Typically, SRS utilize transformer-based architectures to forecastthe subsequent item within a sequence. Nevertheless, the quadraticcomputational complexity inherent in these models often leads toinefficiencies, hindering the achievement of real-time recommendations. Mamba,a recent advancement, has exhibited exceptional performance in time seriesprediction, significantly enhancing both efficiency and accuracy. However,integrating Mamba directly into SRS poses several challenges. Its inherentlyunidirectional nature may constrain the model's capacity to capture the fullcontext of user-item interactions, while its instability in state estimationcan compromise its ability to detect short-term patterns within interactionsequences. To overcome these issues, we introduce a new framework named Selective GatedMamba (SIGMA) for Sequential Recommendation. This framework leverages aPartially Flipped Mamba (PF-Mamba) to construct a bidirectional architecturespecifically tailored to improve contextual modeling. Additionally, aninput-sensitive Dense Selective Gate (DS Gate) is employed to optimizedirectional weights and enhance the processing of sequential information inPF-Mamba. For short sequence modeling, we have also developed a Feature ExtractGRU (FE-GRU) to efficiently capture short-term dependencies. Empirical resultsindicate that SIGMA outperforms current models on five real-world datasets. Ourimplementation code is available at https://github.com/ziwliu-cityu/SIMGA toease reproducibility.</description><author>Ziwei Liu, Qidong Liu, Yejing Wang, Wanyu Wang, Pengyue Jia, Maolin Wang, Zitao Liu, Yi Chang, Xiangyu Zhao</author><pubDate>Tue, 24 Dec 2024 17:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11451v4</guid></item><item><title>Token-Budget-Aware LLM Reasoning</title><link>http://arxiv.org/abs/2412.18547v1</link><description>Reasoning is critical for large language models (LLMs) to excel in a widerange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLMperformance by decomposing problems into intermediate steps, they also incursignificant overhead in token usage, leading to increased costs. We find thatthe reasoning process of current LLMs is unnecessarily lengthy and it can becompressed by including a reasonable token budget in the prompt, but the choiceof token budget plays a crucial role in the actual compression effectiveness.We then propose a token-budget-aware LLM reasoning framework, which dynamicallyestimates token budgets for different problems based on reasoning complexityand uses the estimated token budgets to guide the reasoning process.Experiments show that our method effectively reduces token costs in CoTreasoning with only a slight performance reduction, offering a practicalsolution to balance efficiency and accuracy in LLM reasoning. Code:https://github.com/GeniusHTX/TALE.</description><author>Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, Zhenting Wang</author><pubDate>Tue, 24 Dec 2024 16:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18547v1</guid></item><item><title>Advancing Deformable Medical Image Registration with Multi-axis Cross-covariance Attention</title><link>http://arxiv.org/abs/2412.18545v1</link><description>Deformable image registration is a fundamental requirement for medical imageanalysis. Recently, transformers have been widely used in deep learning-basedregistration methods for their ability to capture long-range dependency viaself-attention (SA). However, the high computation and memory loads of SA(growing quadratically with the spatial resolution) hinder transformers fromprocessing subtle textural information in high-resolution image features, e.g.,at the full and half image resolutions. This limits deformable registration asthe high-resolution textural information is crucial for finding precisepixel-wise correspondence between subtle anatomical structures.Cross-covariance Attention (XCA), as a "transposed" version of SA that operatesacross feature channels, has complexity growing linearly with the spatialresolution, providing the feasibility of capturing long-range dependency amonghigh-resolution image features. However, existing XCA-based transformers merelycapture coarse global long-range dependency, which are unsuitable fordeformable image registration relying primarily on fine-grained localcorrespondence. In this study, we propose to improve existing deeplearning-based registration methods by embedding a new XCA mechanism. To thisend, we design an XCA-based transformer block optimized for deformable medicalimage registration, named Multi-Axis XCA (MAXCA). Our MAXCA serves as a generalnetwork block that can be embedded into various registration networkarchitectures. It can capture both global and local long-range dependency amonghigh-resolution image features by applying regional and dilated XCA in parallelvia a multi-axis design. Extensive experiments on two well-benchmarkedinter-/intra-patient registration tasks with seven public medical datasetsdemonstrate that our MAXCA block enables state-of-the-art registrationperformance.</description><author>Mingyuan Meng, Michael Fulham, Lei Bi, Jinman Kim</author><pubDate>Tue, 24 Dec 2024 16:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18545v1</guid></item><item><title>Consistency Checks for Language Model Forecasters</title><link>http://arxiv.org/abs/2412.18544v1</link><description>Forecasting is a task that is difficult to evaluate: the ground truth canonly be known in the future. Recent work showing LLM forecasters rapidlyapproaching human-level performance begs the question: how can we benchmark andevaluate these forecasters instantaneously? Following the consistency checkframework, we measure the performance of forecasters in terms of theconsistency of their predictions on different logically-related questions. Wepropose a new, general consistency metric based on arbitrage: for example, if aforecasting AI illogically predicts that both the Democratic and Republicanparties have 60% probability of winning the 2024 US presidential election, anarbitrageur can trade against the forecaster's predictions and make a profit.We build an automated evaluation system that generates a set of base questions,instantiates consistency checks from these questions, elicits the predictionsof the forecaster, and measures the consistency of the predictions. We thenbuild a standard, proper-scoring-rule forecasting benchmark, and show that our(instantaneous) consistency metrics correlate with LLM forecasters' groundtruth Brier scores (which are only known in the future). We also release aconsistency benchmark that resolves in 2028, providing a long-term evaluationtool for forecasting.</description><author>Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramr</author><pubDate>Tue, 24 Dec 2024 16:51:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18544v1</guid></item><item><title>Convergence of Statistical Estimators via Mutual Information Bounds</title><link>http://arxiv.org/abs/2412.18539v1</link><description>Recent advances in statistical learning theory have revealed profoundconnections between mutual information (MI) bounds, PAC-Bayesian theory, andBayesian nonparametrics. This work introduces a novel mutual information boundfor statistical models. The derived bound has wide-ranging applications instatistical inference. It yields improved contraction rates for fractionalposteriors in Bayesian nonparametrics. It can also be used to study a widerange of estimation methods, such as variational inference or MaximumLikelihood Estimation (MLE). By bridging these diverse areas, this workadvances our understanding of the fundamental limits of statistical inferenceand the role of information in learning from data. We hope that these resultswill not only clarify connections between statistical inference and informationtheory but also help to develop a new toolbox to study a wide range ofestimators.</description><author>El Mahdi Khribch, Pierre Alquier</author><pubDate>Tue, 24 Dec 2024 16:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18539v1</guid></item><item><title>Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models</title><link>http://arxiv.org/abs/2412.12564v2</link><description>Aspect-based sentiment analysis (ABSA), a sequence labeling task, hasattracted increasing attention in multilingual contexts. While previousresearch has focused largely on fine-tuning or training models specifically forABSA, we evaluate large language models (LLMs) under zero-shot conditions toexplore their potential to tackle this challenge with minimal task-specificadaptation. We conduct a comprehensive empirical evaluation of a series of LLMson multilingual ABSA tasks, investigating various prompting strategies,including vanilla zero-shot, chain-of-thought (CoT), self-improvement,self-debate, and self-consistency, across nine different models. Resultsindicate that while LLMs show promise in handling multilingual ABSA, theygenerally fall short of fine-tuned, task-specific models. Notably, simplerzero-shot prompts often outperform more complex strategies, especially inhigh-resource languages like English. These findings underscore the need forfurther refinement of LLM-based approaches to effectively address ABSA taskacross diverse languages.</description><author>Chengyan Wu, Bolei Ma, Zheyu Zhang, Ningyuan Deng, Yanqing He, Yun Xue</author><pubDate>Tue, 24 Dec 2024 16:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12564v2</guid></item><item><title>Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation</title><link>http://arxiv.org/abs/2412.18537v1</link><description>Large Language Models (LLMs) demonstrate remarkable capabilities, yetstruggle with hallucination and outdated knowledge when tasked with complexknowledge reasoning, resulting in factually incorrect outputs. Previous studieshave attempted to mitigate it by retrieving factual knowledge from large-scaleknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction ofanswers. However, this kind of approach often introduces noise and irrelevantdata, especially in situations with extensive context from multiple knowledgeaspects. In this way, LLM attention can be potentially mislead from questionand relevant information. In our study, we introduce an Adaptive Multi-AspectRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledgeincluding entities, relations, and subgraphs, and converts each piece ofretrieved text into prompt embeddings. The Amar framework comprises two keysub-components: 1) a self-alignment module that aligns commonalities amongentities, relations, and subgraphs to enhance retrieved text, thereby reducingnoise interference; 2) a relevance gating module that employs a soft gate tolearn the relevance score between question and multi-aspect retrieved data, todetermine which information should be used to enhance LLMs' output, or evenfiltered altogether. Our method has achieved state-of-the-art performance ontwo common datasets, WebQSP and CWQ, showing a 1.9\% improvement in accuracyover its best competitor and a 6.6\% improvement in logical form generationover a method that directly uses retrieved text as context prompts. Theseresults demonstrate the effectiveness of Amar in improving the reasoning ofLLMs.</description><author>Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, Zhihong Zhu, Zhi Zheng, Xian Wu, Xiangyu Zhao, Tong Xu, Enhong Chen</author><pubDate>Tue, 24 Dec 2024 16:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18537v1</guid></item><item><title>Graph Structure Learning for Spatial-Temporal Imputation: Adapting to Node and Feature Scales</title><link>http://arxiv.org/abs/2412.18535v1</link><description>Spatial-temporal data collected across different geographic locations oftensuffer from missing values, posing challenges to data analysis. Existingmethods primarily leverage fixed spatial graphs to impute missing values, whichimplicitly assume that the spatial relationship is roughly the same for allfeatures across different locations. However, they may overlook the differentspatial relationships of diverse features recorded by sensors in differentlocations. To address this, we introduce the multi-scale Graph StructureLearning framework for spatial-temporal Imputation (GSLI) that dynamicallyadapts to the heterogeneous spatial correlations. Our framework encompassesnode-scale graph structure learning to cater to the distinct global spatialcorrelations of different features, and feature-scale graph structure learningto unveil common spatial correlation across features within all stations.Integrated with prominence modeling, our framework emphasizes nodes andfeatures with greater significance in the imputation process. Furthermore, GSLIincorporates cross-feature and cross-temporal representation learning tocapture spatial-temporal dependencies. Evaluated on six real incompletespatial-temporal datasets, GSLI showcases the improvement in data imputation.</description><author>Xinyu Yang, Yu Sun, Xinyang Chen, Ying Zhang, Xiaojie Yuan</author><pubDate>Tue, 24 Dec 2024 16:34:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18535v1</guid></item><item><title>FSDEM: Feature Selection Dynamic Evaluation Metric</title><link>http://arxiv.org/abs/2408.14234v2</link><description>Expressive evaluation metrics are indispensable for informative experimentsin all areas, and while several metrics are established in some areas, inothers, such as feature selection, only indirect or otherwise limitedevaluation metrics are found. In this paper, we propose a novel evaluationmetric to address several problems of its predecessors and allow for flexibleand reliable evaluation of feature selection algorithms. The proposed metric isa dynamic metric with two properties that can be used to evaluate both theperformance and the stability of a feature selection algorithm. We conductseveral empirical experiments to illustrate the use of the proposed metric inthe successful evaluation of feature selection algorithms. We also provide acomparison and analysis to show the different aspects involved in theevaluation of the feature selection algorithms. The results indicate that theproposed metric is successful in carrying out the evaluation task for featureselection algorithms. This paper is an extended version of a paper published at SISAP 2024.</description><author>Muhammad Rajabinasab, Anton D. Lautrup, Tobias Hyrup, Arthur Zimek</author><pubDate>Tue, 24 Dec 2024 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14234v2</guid></item><item><title>GCN-ABFT: Low-Cost Online Error Checking for Graph Convolutional Networks</title><link>http://arxiv.org/abs/2412.18534v1</link><description>Graph convolutional networks (GCNs) are popular for building machine-learningapplication for graph-structured data. This widespread adoption led to thedevelopment of specialized GCN hardware accelerators. In this work, we addressa key architectural challenge for GCN accelerators: how to detect errors in GCNcomputations arising from random hardware faults with the least computationcost. Each GCN layer performs a graph convolution, mathematically equivalent tomultiplying three matrices, computed through two separate matrixmultiplications. Existing Algorithm-based Fault Tolerance(ABFT) techniques cancheck the results of individual matrix multiplications. However, for a GCNlayer, this check should be performed twice. To avoid this overhead, this workintroduces GCN-ABFT that directly calculates a checksum for the entirethree-matrix product within a single GCN layer, providing a cost-effectiveapproach for error detection in GCN accelerators. Experimental resultsdemonstrate that GCN-ABFT reduces the number of operations needed for checksumcomputation by over 21% on average for representative GCN applications. Thesesavings are achieved without sacrificing fault-detection accuracy, as evidencedby the presented fault-injection analysis.</description><author>Christodoulos Peltekis, Giorgos Dimitrakopoulos</author><pubDate>Tue, 24 Dec 2024 16:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18534v1</guid></item><item><title>SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking State Space Models</title><link>http://arxiv.org/abs/2408.14909v2</link><description>Known as low energy consumption networks, spiking neural networks (SNNs) havegained a lot of attention within the past decades. While SNNs are increasingcompetitive with artificial neural networks (ANNs) for vision tasks, they arerarely used for long sequence tasks, despite their intrinsic temporal dynamics.In this work, we develop spiking state space models (SpikingSSMs) for longsequence learning by leveraging on the sequence learning abilities of statespace models (SSMs). Inspired by dendritic neuron structure, we hierarchicallyintegrate neuronal dynamics with the original SSM block, meanwhile realizingsparse synaptic computation. Furthermore, to solve the conflict of event-drivenneuronal dynamics with parallel computing, we propose a light-weight surrogatedynamic network which accurately predicts the after-reset membrane potentialand compatible to learnable thresholds, enabling orders of acceleration intraining speed compared with conventional iterative methods. On the long rangearena benchmark task, SpikingSSM achieves competitive performance tostate-of-the-art SSMs meanwhile realizing on average 90\% of network sparsity.On language modeling, our network significantly surpasses existing spikinglarge language models (spikingLLMs) on the WikiText-103 dataset with only athird of the model size, demonstrating its potential as backbone architecturefor low computation cost LLMs.</description><author>Shuaijie Shen, Chao Wang, Renzhuo Huang, Yan Zhong, Qinghai Guo, Zhichao Lu, Jianguo Zhang, Luziwei Leng</author><pubDate>Tue, 24 Dec 2024 16:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14909v2</guid></item><item><title>Characterizations of Language Generation With Breadth</title><link>http://arxiv.org/abs/2412.18530v1</link><description>We study language generation in the limit, introduced by Kleinberg andMullainathan [KM24], building on classical works of Gold [Gol67] and Angluin[Ang79]. [KM24] proposed an algorithm that generates strings from any countablelanguage collection in the limit. While their algorithm eventually outputsstrings from the target language $K$, it sacrifices breadth, i.e., the abilityto generate all strings in $K$. A key open question in [KM24] is whether thistrade-off between consistency and breadth is inherrent. Recent works proposed different notions of consistent generation withbreadth. Kalavasis, Mehrotra, and Velegkas [KVM24] introduced threedefinitions: generation with exact breadth, approximate breadth, andunambiguous generation. Concurrently and independently, Charikar and Pabbaraju[CP24a] proposed exhaustive generation. Both works examined when generationwith these notions of breadth is possible. Building on [CP24a, KVM24], we fully characterize language generation forthese notions and their natural combinations. For exact breadth, we provide anunconditional lower bound, removing a technical condition from [KVM24] andextending the result of [CP24a] that holds for specific collections oflanguages. We show that generation with exact breadth is characterized byAngluin's condition for identification. We further introduce a weaker versionof Angluin's condition that tightly characterizes both approximate breadth andexhaustive generation, proving their equivalence. Additionally, we show thatunambiguous generation is also characterized by Angluin's condition as aspecial case of a broader result. Finally, we strengthen [KVM24] by givingunconditional lower bounds for stable generators, showing that Angluin'scondition characterizes the previous breadth notions for stable generators.This shows a separation between stable and unstable generation with approximatebreadth.</description><author>Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas</author><pubDate>Tue, 24 Dec 2024 16:24:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18530v1</guid></item><item><title>Accelerating process control and optimization via machine learning: A review</title><link>http://arxiv.org/abs/2412.18529v1</link><description>Process control and optimization have been widely used to solvedecision-making problems in chemical engineering applications. However,identifying and tuning the best solution algorithm is challenging andtime-consuming. Machine learning tools can be used to automate these steps bylearning the behavior of a numerical solver from data. In this paper, wediscuss recent advances in (i) the representation of decision-making problemsfor machine learning tasks, (ii) algorithm selection, and (iii) algorithmconfiguration for monolithic and decomposition-based algorithms. Finally, wediscuss open problems related to the application of machine learning foraccelerating process optimization and control.</description><author>Ilias Mitrai, Prodromos Daoutidis</author><pubDate>Tue, 24 Dec 2024 16:24:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18529v1</guid></item><item><title>SMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC</title><link>http://arxiv.org/abs/2412.17707v2</link><description>The availability of challenging simulation environments is pivotal foradvancing the field of Multi-Agent Reinforcement Learning (MARL). Incooperative MARL settings, the StarCraft Multi-Agent Challenge (SMAC) hasgained prominence as a benchmark for algorithms following centralized trainingwith decentralized execution paradigm. However, with continual advancements inSMAC, many algorithms now exhibit near-optimal performance, complicating theevaluation of their true effectiveness. To alleviate this problem, in thiswork, we highlight a critical issue: the default opponent policy in theseenvironments lacks sufficient diversity, leading MARL algorithms to overfit andexploit unintended vulnerabilities rather than learning robust strategies. Toovercome these limitations, we propose SMAC-HARD, a novel benchmark designed toenhance training robustness and evaluation comprehensiveness. SMAC-HARDsupports customizable opponent strategies, randomization of adversarialpolicies, and interfaces for MARL self-play, enabling agents to generalize tovarying opponent behaviors and improve model stability. Furthermore, weintroduce a black-box testing framework wherein agents are trained withoutexposure to the edited opponent scripts but are tested against these scripts toevaluate the policy coverage and adaptability of MARL algorithms. We conductextensive evaluations of widely used and state-of-the-art algorithms onSMAC-HARD, revealing the substantial challenges posed by edited and mixedstrategy opponents. Additionally, the black-box strategy tests illustrate thedifficulty of transferring learned policies to unseen adversaries. We envisionSMAC-HARD as a critical step toward benchmarking the next generation of MARLalgorithms, fostering progress in self-play methods for multi-agent systems.Our code is available at https://github.com/devindeng94/smac-hard.</description><author>Yue Deng, Yan Yu, Weiyu Ma, Zirui Wang, Wenhui Zhu, Jian Zhao, Yin Zhang</author><pubDate>Tue, 24 Dec 2024 16:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17707v2</guid></item><item><title>The Key of Understanding Vision Tasks: Explanatory Instructions</title><link>http://arxiv.org/abs/2412.18525v1</link><description>Computer Vision (CV) has yet to fully achieve the zero-shot taskgeneralization observed in Natural Language Processing (NLP), despite followingmany of the milestones established in NLP, such as large transformer models,extensive pre-training, and the auto-regression paradigm, among others. In thispaper, we explore the idea that CV adopts discrete and terminological taskdefinitions (\eg, ``image segmentation''), which may be a key barrier tozero-shot task generalization. Our hypothesis is that without trulyunderstanding previously-seen tasks--due to these terminologicaldefinitions--deep models struggle to generalize to novel tasks. To verify this,we introduce Explanatory Instructions, which provide an intuitive way to defineCV task objectives through detailed linguistic transformations from inputimages to outputs. We create a large-scale dataset comprising 12 million``image input $\to$ explanatory instruction $\to$ output'' triplets, and trainan auto-regressive-based vision-language model (AR-based VLM) that takes bothimages and explanatory instructions as input. By learning to follow theseinstructions, the AR-based VLM achieves instruction-level zero-shotcapabilities for previously-seen tasks and demonstrates strong zero-shotgeneralization for unseen CV tasks. Code and dataset will be openly availableon our GitHub repository.</description><author>Yang Shen, Xiu-Shen Wei, Yifan Sun, Yuxin Song, Tao Yuan, Jian Jin, Heyang Xu, Yazhou Yao, Errui Ding</author><pubDate>Tue, 24 Dec 2024 16:08:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18525v1</guid></item><item><title>HTR-JAND: Handwritten Text Recognition with Joint Attention Network and Knowledge Distillation</title><link>http://arxiv.org/abs/2412.18524v1</link><description>Despite significant advances in deep learning, current Handwritten TextRecognition (HTR) systems struggle with the inherent complexity of historicaldocuments, including diverse writing styles, degraded text quality, andcomputational efficiency requirements across multiple languages and timeperiods. This paper introduces HTR-JAND (HTR-JAND: Handwritten Text Recognitionwith Joint Attention Network and Knowledge Distillation), an efficient HTRframework that combines advanced feature extraction with knowledgedistillation. Our architecture incorporates three key components: (1) a CNNarchitecture integrating FullGatedConv2d layers with Squeeze-and-Excitationblocks for adaptive feature extraction, (2) a Combined Attention mechanismfusing Multi-Head Self-Attention with Proxima Attention for robust sequencemodeling, and (3) a Knowledge Distillation framework enabling efficient modelcompression while preserving accuracy through curriculum-based training. TheHTR-JAND framework implements a multi-stage training approach combiningcurriculum learning, synthetic data generation, and multi-task learning forcross-dataset knowledge transfer. We enhance recognition accuracy throughcontext-aware T5 post-processing, particularly effective for historicaldocuments. Comprehensive evaluations demonstrate HTR-JAND's effectiveness,achieving state-of-the-art Character Error Rates (CER) of 1.23\%, 1.02\%, and2.02\% on IAM, RIMES, and Bentham datasets respectively. Our Student modelachieves a 48\% parameter reduction (0.75M versus 1.5M parameters) whilemaintaining competitive performance through efficient knowledge transfer.Source code and pre-trained models are available at\href{https://github.com/DocumentRecognitionModels/HTR-JAND}{Github}.</description><author>Mohammed Hamdan, Abderrahmane Rahiche, Mohamed Cheriet</author><pubDate>Tue, 24 Dec 2024 16:08:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18524v1</guid></item><item><title>YuLan-Mini: An Open Data-efficient Language Model</title><link>http://arxiv.org/abs/2412.17743v2</link><description>Effective pre-training of large language models (LLMs) has been challengingdue to the immense resource demands and the complexity of the technicalprocesses involved. This paper presents a detailed technical report onYuLan-Mini, a highly capable base model with 2.42B parameters that achievestop-tier performance among models of similar parameter scale. Our pre-trainingapproach focuses on enhancing training efficacy through three key technicalcontributions: an elaborate data pipeline combines data cleaning with dataschedule strategies, a robust optimization method to mitigate traininginstability, and an effective annealing approach that incorporates targeteddata selection and long context training. Remarkably, YuLan-Mini, trained on1.08T tokens, achieves performance comparable to industry-leading models thatrequire significantly more data. To facilitate reproduction, we release thefull details of the data composition for each training phase. Project detailscan be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.</description><author>Yiwen Hu, Huatong Song, Jia Deng, Jiapeng Wang, Jie Chen, Kun Zhou, Yutao Zhu, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, Ji-Rong Wen</author><pubDate>Tue, 24 Dec 2024 16:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17743v2</guid></item><item><title>On the ETHOS of AI Agents: An Ethical Technology and Holistic Oversight System</title><link>http://arxiv.org/abs/2412.17114v2</link><description>In a world increasingly defined by machine intelligence, the future dependson how we govern the development and integration of AI into society. Recentinitiatives, such as the EU AI Act, EDPB opinion, U.S. Bipartisan House TaskForce and NIST AI Risk Management Report, highlight the urgent need for robustgovernance frameworks to address the challenges posed by advancing AItechnologies. However, existing frameworks fail to adequately address the riseof AI agents or the ongoing debate between centralized and decentralizedgovernance models. To bridge these gaps, we propose the Ethical Technology andHolistic Oversight System framework, which leverages Web3 technologies,including blockchain, smart contracts, decentralized autonomous organizations,and soulbound tokens, to establish a decentralized global registry for AIagents. ETHOS incorporates the concept of AI specific legal entities, enablingthese systems to assume limited liability and ensuring accountability throughmechanisms like insurance and compliance monitoring. Additionally, theframework emphasizes the need for a collaborative, participatory approach to AIgovernance, engaging diverse stakeholders through public education,transparency, and international coordination. ETHOS balances innovation withethical accountability, providing a forward looking strategy for theresponsible integration of AI agents into society. Finally, this explorationreflects the emergence of a new interdisciplinary field we define as SystemsThinking at the Intersection of AI, Web3, and Society.</description><author>Tomer Jordi Chaffer, Justin Goldston, Bayo Okusanya, Gemach D. A. T. A. I</author><pubDate>Tue, 24 Dec 2024 16:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17114v2</guid></item><item><title>Bayesian Optimization of Bilevel Problems</title><link>http://arxiv.org/abs/2412.18518v1</link><description>Bilevel optimization, a hierarchical mathematical framework where oneoptimization problem is nested within another, has emerged as a powerful toolfor modeling complex decision-making processes in various fields such aseconomics, engineering, and machine learning. This paper focuses on bileveloptimization where both upper-level and lower-level functions are black boxesand expensive to evaluate. We propose a Bayesian Optimization framework thatmodels the upper and lower-level functions as Gaussian processes over thecombined space of upper and lower-level decisions, allowing us to exploitknowledge transfer between different sub-problems. Additionally, we propose anovel acquisition function for this model. Our experimental results demonstratethat the proposed algorithm is highly sample-efficient and outperforms existingmethods in finding high-quality solutions.</description><author>Omer Ekmekcioglu, Nursen Aydin, Juergen Branke</author><pubDate>Tue, 24 Dec 2024 15:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18518v1</guid></item><item><title>RA-RLHF: Provably Efficient Risk-Aware Reinforcement Learning Human Feedback</title><link>http://arxiv.org/abs/2410.23569v2</link><description>Reinforcement Learning Human Feedback (RLHF) studies the problem where agentsreceive only preferences over pairs of trajectories in each episode.Traditional approaches in this field have predominantly focused on the meanreward or utility criterion. However, in RLHF scenarios demanding heightenedrisk awareness, such as in AI systems, healthcare, and agriculture, risk-awaremeasures are requisite. Traditional risk-aware objectives and algorithms arenot applicable in such one-episode-reward settings. To address this, we exploreand prove the applicability of two risk-aware objectives to RLHF: nested andstatic quantile risk objectives. We also introduce Risk-Aware-RLHF (RA-RLHF),an algorithm designed to optimize both nested and static objectives.Additionally, we provide a theoretical analysis of the regret upper bounds,demonstrating that they are sublinear with respect to the number of episodes,and present empirical results to support our findings. Our code is available inhttps://github.com/aguilarjose11/pbrlNeurips.</description><author>Yujie Zhao, Jose Efraim Aguilar Escamill, Weyl Lu, Huazheng Wang</author><pubDate>Tue, 24 Dec 2024 15:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23569v2</guid></item><item><title>Subsampling, aligning, and averaging to find circular coordinates in recurrent time series</title><link>http://arxiv.org/abs/2412.18515v1</link><description>We introduce a new algorithm for finding robust circular coordinates on datathat is expected to exhibit recurrence, such as that which appears in neuronalrecordings of C. elegans. Techniques exist to create circular coordinates on asimplicial complex from a dimension 1 cohomology class, and these can beapplied to the Rips complex of a dataset when it has a prominent class in itsdimension 1 cohomology. However, it is known this approach is extremelysensitive to uneven sampling density. Our algorithm comes with a new method to correct for uneven sampling density,adapting our prior work on averaging coordinates in manifold learning. We userejection sampling to correct for inhomogeneous sampling and then applyProcrustes matching to align and average the subsamples. In addition toproviding a more robust coordinate than other approaches, this subsampling andaveraging approach has better efficiency. We validate our technique on both synthetic data sets and neuronal activityrecordings. Our results reveal a topological model of neuronal trajectories forC. elegans that is constructed from loops in which different regions of thebrain state space can be mapped to specific and interpretable macroscopicbehaviors in the worm.</description><author>Andrew J. Blumberg, Mathieu Carrire, Jun Hou Fung, Michael A. Mandell</author><pubDate>Tue, 24 Dec 2024 15:52:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18515v1</guid></item><item><title>FedGIG: Graph Inversion from Gradient in Federated Learning</title><link>http://arxiv.org/abs/2412.18513v1</link><description>Recent studies have shown that Federated learning (FL) is vulnerable toGradient Inversion Attacks (GIA), which can recover private training data fromshared gradients. However, existing methods are designed for dense, continuousdata such as images or vectorized texts, and cannot be directly applied tosparse and discrete graph data. This paper first explores GIA's impact onFederated Graph Learning (FGL) and introduces Graph Inversion from Gradient inFederated Learning (FedGIG), a novel GIA method specifically designed forgraph-structured data. FedGIG includes the adjacency matrix constrainingmodule, which ensures the sparsity and discreteness of the reconstructed graphdata, and the subgraph reconstruction module, which is designed to completemissing common subgraph structures. Extensive experiments on molecular datasetsdemonstrate FedGIG's superior accuracy over existing GIA techniques.</description><author>Tianzhe Xiao, Yichen Li, Yining Qi, Haozhao Wang, Ruixuan Li</author><pubDate>Tue, 24 Dec 2024 15:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18513v1</guid></item><item><title>An Empirical Analysis of Federated Learning Models Subject to Label-Flipping Adversarial Attack</title><link>http://arxiv.org/abs/2412.18507v1</link><description>In this paper, we empirically analyze adversarial attacks on selectedfederated learning models. The specific learning models considered areMultinominal Logistic Regression (MLR), Support Vector Classifier (SVC),Multilayer Perceptron (MLP), Convolution Neural Network (CNN), %RecurrentNeural Network (RNN), Random Forest, XGBoost, and Long Short-Term Memory(LSTM). For each model, we simulate label-flipping attacks, experimentingextensively with 10 federated clients and 100 federated clients. We vary thepercentage of adversarial clients from 10% to 100% and, simultaneously, thepercentage of labels flipped by each adversarial client is also varied from 10%to 100%. Among other results, we find that models differ in their inherentrobustness to the two vectors in our label-flipping attack, i.e., thepercentage of adversarial clients, and the percentage of labels flipped by eachadversarial client. We discuss the potential practical implications of ourresults.</description><author>Kunal Bhatnagar, Sagana Chattanathan, Angela Dang, Bhargav Eranki, Ronnit Rana, Charan Sridhar, Siddharth Vedam, Angie Yao, Mark Stamp</author><pubDate>Tue, 24 Dec 2024 15:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18507v1</guid></item><item><title>VORTEX: A Spatial Computing Framework for Optimized Drone Telemetry Extraction from First-Person View Flight Data</title><link>http://arxiv.org/abs/2412.18505v1</link><description>This paper presents the Visual Optical Recognition Telemetry EXtraction(VORTEX) system for extracting and analyzing drone telemetry data from FirstPerson View (FPV) Uncrewed Aerial System (UAS) footage. VORTEX employs MMOCR, aPyTorch-based Optical Character Recognition (OCR) toolbox, to extract telemetryvariables from drone Heads Up Display (HUD) recordings, utilizing advancedimage preprocessing techniques, including CLAHE enhancement and adaptivethresholding. The study optimizes spatial accuracy and computational efficiencythrough systematic investigation of temporal sampling rates (1s, 5s, 10s, 15s,20s) and coordinate processing methods. Results demonstrate that the 5-secondsampling rate, utilizing 4.07% of available frames, provides the optimalbalance with a point retention rate of 64% and mean speed accuracy within 4.2%of the 1-second baseline while reducing computational overhead by 80.5%.Comparative analysis of coordinate processing methods reveals that while UTMZone 33N projection and Haversine calculations provide consistently similarresults (within 0.1% difference), raw WGS84 coordinates underestimate distancesby 15-30% and speeds by 20-35%. Altitude measurements showed unexpectedresilience to sampling rate variations, with only 2.1% variation across allintervals. This research is the first of its kind, providing quantitativebenchmarks for establishing a robust framework for drone telemetry extractionand analysis using open-source tools and spatial libraries.</description><author>James E. Gallagher, Edward J. Oughton</author><pubDate>Tue, 24 Dec 2024 15:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18505v1</guid></item><item><title>Joint Adaptive OFDM and Reinforcement Learning Design for Autonomous Vehicles: Leveraging Age of Updates</title><link>http://arxiv.org/abs/2412.18500v1</link><description>Millimeter wave (mmWave)-based orthogonal frequency-division multiplexing(OFDM) stands out as a suitable alternative for high-resolution sensing andhigh-speed data transmission. To meet communication and sensing requirements,many works propose a static configuration where the wave's hyperparameters suchas the number of symbols in a frame and the number of frames in a communicationslot are already predefined. However, two facts oblige us to redefine theproblem, (1) the environment is often dynamic and uncertain, and (2) mmWave isseverely impacted by wireless environments. A striking example where thischallenge is very prominent is autonomous vehicle (AV). Such a system leveragesintegrated sensing and communication (ISAC) using mmWave to manage datatransmission and the dynamism of the environment. In this work, we consider anautonomous vehicle network where an AV utilizes its queue state information(QSI) and channel state information (CSI) in conjunction with reinforcementlearning techniques to manage communication and sensing. This enables the AV toachieve two primary objectives: establishing a stable communication link withother AVs and accurately estimating the velocities of surrounding objects withhigh resolution. The communication performance is therefore evaluated based onthe queue state, the effective data rate, and the discarded packets rate. Incontrast, the effectiveness of the sensing is assessed using the velocityresolution. In addition, we exploit adaptive OFDM techniques for dynamicmodulation, and we suggest a reward function that leverages the age of updatesto handle the communication buffer and improve sensing. The system is validatedusing advantage actor-critic (A2C) and proximal policy optimization (PPO).Furthermore, we compare our solution with the existing design and demonstrateits superior performance by computer simulations.</description><author>Mamady Delamou, Ahmed Naeem, Huseyin Arslan, El Mehdi Amhoud</author><pubDate>Tue, 24 Dec 2024 15:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18500v1</guid></item><item><title>Think or Remember? Detecting and Directing LLMs Towards Memorization or Generalization</title><link>http://arxiv.org/abs/2412.18497v1</link><description>In this paper, we explore the foundational mechanisms of memorization andgeneralization in Large Language Models (LLMs), inspired by the functionalspecialization observed in the human brain. Our investigation serves as a casestudy leveraging specially designed datasets and experimental-scale LLMs to laythe groundwork for understanding these behaviors. Specifically, we aim to firstenable LLMs to exhibit both memorization and generalization by training withthe designed dataset, then (a) examine whether LLMs exhibit neuron-levelspatial differentiation for memorization and generalization, (b) predict thesebehaviors using model internal representations, and (c) steer the behaviorsthrough inference-time interventions. Our findings reveal that neuron-wisedifferentiation of memorization and generalization is observable in LLMs, andtargeted interventions can successfully direct their behavior.</description><author>Yi-Fu Fu, Yu-Chieh Tu, Tzu-Ling Cheng, Cheng-Yu Lin, Yi-Ting Yang, Heng-Yi Liu, Keng-Te Liao, Da-Cheng Juan, Shou-De Lin</author><pubDate>Tue, 24 Dec 2024 15:28:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18497v1</guid></item><item><title>Generating event descriptions under syntactic and semantic constraints</title><link>http://arxiv.org/abs/2412.18496v1</link><description>With the goal of supporting scalable lexical semantic annotation, analysis,and theorizing, we conduct a comprehensive evaluation of different methods forgenerating event descriptions under both syntactic constraints -- e.g. desiredclause structure -- and semantic constraints -- e.g. desired verb sense. Wecompare three different methods -- (i) manual generation by experts; (ii)sampling from a corpus annotated for syntactic and semantic information; and(iii) sampling from a language model (LM) conditioned on syntactic and semanticinformation -- along three dimensions of the generated event descriptions: (a)naturalness, (b) typicality, and (c) distinctiveness. We find that all methodsreliably produce natural, typical, and distinctive event descriptions, but thatmanual generation continues to produce event descriptions that are morenatural, typical, and distinctive than the automated generation methods. Weconclude that the automated methods we consider produce event descriptions ofsufficient quality for use in downstream annotation and analysis insofar as themethods used for this annotation and analysis are robust to a small amount ofdegradation in the resulting event descriptions.</description><author>Angela Cao, Faye Holt, Jonas Chan, Stephanie Richter, Lelia Glass, Aaron Steven White</author><pubDate>Tue, 24 Dec 2024 15:28:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18496v1</guid></item><item><title>How "Real" is Your Real-Time Simultaneous Speech-to-Text Translation System?</title><link>http://arxiv.org/abs/2412.18495v1</link><description>Simultaneous speech-to-text translation (SimulST) translates source-languagespeech into target-language text concurrently with the speaker's speech,ensuring low latency for better user comprehension. Despite its intendedapplication to unbounded speech, most research has focused on humanpre-segmented speech, simplifying the task and overlooking significantchallenges. This narrow focus, coupled with widespread terminologicalinconsistencies, is limiting the applicability of research outcomes toreal-world applications, ultimately hindering progress in the field. Ourextensive literature review of 110 papers not only reveals these criticalissues in current research but also serves as the foundation for our keycontributions. We 1) define the steps and core components of a SimulST system,proposing a standardized terminology and taxonomy; 2) conduct a thoroughanalysis of community trends, and 3) offer concrete recommendations and futuredirections to bridge the gaps in existing literature, from evaluationframeworks to system architectures, for advancing the field towards morerealistic and effective SimulST solutions.</description><author>Sara Papi, Peter Polak, Ondej Bojar, Dominik Machek</author><pubDate>Tue, 24 Dec 2024 15:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18495v1</guid></item><item><title>Contextual Backpropagation Loops: Amplifying Deep Reasoning with Iterative Top-Down Feedback</title><link>http://arxiv.org/abs/2412.17737v2</link><description>Deep neural networks typically rely on a single forward pass for inference,which can limit their capacity to resolve ambiguous inputs. We introduceContextual Backpropagation Loops (CBLs) as an iterative mechanism thatincorporates top-down feedback to refine intermediate representations, therebyimproving accuracy and robustness. This repeated process mirrors how humanscontinuously re-interpret sensory information in daily life-by checking andre-checking our perceptions using contextual cues. Our results suggest thatCBLs can offer a straightforward yet powerful way to incorporate suchcontextual reasoning in modern deep learning architectures.</description><author>Jacob Fein-Ashley</author><pubDate>Tue, 24 Dec 2024 15:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17737v2</guid></item><item><title>DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion</title><link>http://arxiv.org/abs/2409.10080v2</link><description>In extreme scenarios such as nighttime or low-visibility environments,achieving reliable perception is critical for applications like autonomousdriving, robotics, and surveillance. Multi-modality image fusion, particularlyintegrating infrared imaging, offers a robust solution by combiningcomplementary information from different modalities to enhance sceneunderstanding and decision-making. However, current methods face significantlimitations: GAN-based approaches often produce blurry images that lackfine-grained details, while AE-based methods may introduce bias toward specificmodalities, leading to unnatural fusion results. To address these challenges,we propose DAE-Fuse, a novel two-phase discriminative autoencoder frameworkthat generates sharp and natural fused images. Furthermore, We pioneer theextension of image fusion techniques from static images to the video domainwhile preserving temporal consistency across frames, thus advancing theperceptual capabilities required for autonomous navigation. Extensiveexperiments on public datasets demonstrate that DAE-Fuse achievesstate-of-the-art performance on multiple benchmarks, with superiorgeneralizability to tasks like medical image fusion.</description><author>Yuchen Guo, Ruoxiang Xu, Rongcheng Li, Zhenghao Wu, Weifeng Su</author><pubDate>Tue, 24 Dec 2024 15:22:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10080v2</guid></item><item><title>An Overview and Discussion of the Suitability of Existing Speech Datasets to Train Machine Learning Models for Collective Problem Solving</title><link>http://arxiv.org/abs/2412.18489v1</link><description>This report characterized the suitability of existing datasets for devisingnew Machine Learning models, decision making methods, and analysis algorithmsto improve Collaborative Problem Solving and then enumerated requirements forfuture datasets to be devised. Problem solving was assumed to be performed inteams of about three, four members, which talked to each other. A datasetconsists of the speech recordings of such teams. The characterizationmethodology was based on metrics that capture cognitive, social, and emotionalactivities and situations. The report presented the analysis of a large groupof datasets developed for Spoken Language Understanding, a research area withsome similarity to Collaborative Problem Solving.</description><author>Gnaneswar Villuri, Alex Doboli</author><pubDate>Tue, 24 Dec 2024 15:22:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18489v1</guid></item><item><title>Segment-Based Attention Masking for GPTs</title><link>http://arxiv.org/abs/2412.18487v1</link><description>Modern Language Models (LMs) owe much of their success to masked causalattention, the backbone of Generative Pre-Trained Transformer (GPT) models.Although GPTs can process the entire user prompt at once, the causal masking isapplied to all input tokens step-by-step, mimicking the generation process.This imposes an unnecessary constraint during the initial "prefill" phase whenthe model processes the input prompt and generates the internal representationsbefore producing any output tokens. In this work, attention is masked based onthe known block structure at the prefill phase, followed by the conventionaltoken-by-token autoregressive process after that. For example, in a typicalchat prompt, the system prompt is treated as one block, and the user prompt asthe next one. Each of these is treated as a unit for the purpose of masking,such that the first tokens in each block can access the subsequent tokens in anon-causal manner. Then, the model answer is generated in the conventionalcausal manner. This Segment-by-Segment scheme entails no additionalcomputational overhead. When integrating it into models such as Llama and Qwen,state-of-the-art performance is consistently achieved.</description><author>Shahar Katz, Liran Ringel, Yaniv Romano, Lior Wolf</author><pubDate>Tue, 24 Dec 2024 15:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18487v1</guid></item><item><title>Deep Adaptive Interest Network: Personalized Recommendation with Context-Aware Learning</title><link>http://arxiv.org/abs/2409.02425v2</link><description>In personalized recommendation systems, accurately capturing users' evolvinginterests and combining them with contextual information is a critical researcharea. This paper proposes a novel model called the Deep Adaptive InterestNetwork (DAIN), which dynamically models users' interests while incorporatingcontext-aware learning mechanisms to achieve precise and adaptive personalizedrecommendations. DAIN leverages deep learning techniques to build an adaptiveinterest network structure that can capture users' interest changes inreal-time while further optimizing recommendation results by integratingcontextual information. Experiments conducted on several public datasetsdemonstrate that DAIN excels in both recommendation performance andcomputational efficiency. This research not only provides a new solution forpersonalized recommendation systems but also offers fresh insights into theapplication of context-aware learning in recommendation systems.</description><author>Shuaishuai Huang, Haowei Yang, You Yao, Xueting Lin, Yuming Tu</author><pubDate>Tue, 24 Dec 2024 15:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02425v2</guid></item><item><title>A region-wide, multi-year set of crop field boundary labels for Africa</title><link>http://arxiv.org/abs/2412.18483v1</link><description>African agriculture is undergoing rapid transformation. Annual maps of cropfields are key to understanding the nature of this transformation, but suchmaps are currently lacking and must be developed using advanced machinelearning models trained on high resolution remote sensing imagery. To enablethe development of such models, we delineated field boundaries in 33,746 Planetimages captured between 2017 and 2023 across the continent using a customlabeling platform with built-in procedures for assessing and mitigating labelerror. We collected 42,403 labels, including 7,204 labels arising from tasksdedicated to assessing label quality (Class 1 labels), 32,167 from sites mappedonce by a single labeller (Class 2) and 3,032 labels from sites where 3 or morelabellers were tasked to map the same location (Class 4). Class 1 labels wereused to calculate labeller-specific quality scores, while Class 1 and 4 sitesmapped by at least 3 labellers were used to further evaluate label uncertaintyusing a Bayesian risk metric. Quality metrics showed that label quality wasmoderately high (0.75) for measures of total field extent, but low regardingthe number of individual fields delineated (0.33), and the position of fieldedges (0.05). These values are expected when delineating small-scale fields in3-5 m resolution imagery, which can be too coarse to reliably distinguishsmaller fields, particularly in dense croplands, and therefore requiressubstantial labeller judgement. Nevertheless, previous work shows that suchlabels can train effective field mapping models. Furthermore, this large,probabilistic sample on its own provides valuable insight into regionalagricultural characteristics, highlighting variations in the median field sizeand density. The imagery and vectorized labels along with quality informationis available for download from two public repositories.</description><author>L. D. Estes, A. Wussah, M. Asipunu, M. Gathigi, P. Kovai, J. Muhando, B. V. Yeboah, F. K. Addai, E. S. Akakpo, M. K. Allotey, P. Amkoya, E. Amponsem, K. D. Donkoh, N. Ha, E. Heltzel, C. Juma, R. Mdawida, A. Miroyo, J. Mucha, J. Mugami, F. Mwawaza, D. A. Nyarko, P. Oduor, K. N. Ohemeng, S. I. D. Segbefia, T. Tumbula, F. Wambua, G. H. Xeflide, S. Ye, F. Yeboah</author><pubDate>Tue, 24 Dec 2024 15:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18483v1</guid></item><item><title>BEADs: Bias Evaluation Across Domains</title><link>http://arxiv.org/abs/2406.04220v4</link><description>Recent advancements in large language models (LLMs) have greatly enhancednatural language processing (NLP) applications. Nevertheless, these modelsoften inherit biases from their training data. Despite the availability ofvarious datasets for bias detection, most are limited to one or two NLP tasks(typically classification or evaluation) and lack comprehensive evaluationsacross a broader range of NLP tasks. To address this gap, we introduce the BiasEvaluations Across Domains BEADs dataset, designed to support a wide array ofNLP tasks, including text classification, token classification, biasquantification, and benign language generation. A key focus of this paper isthe gold label dataset that is annotated by GPT4 for scalabilty and verified byexperts to ensure high reliability. BEADs provides data for both fine-tuning,including classification and language generation tasks, and for evaluatingLLMs. Our findings indicate that BEADs effectively identifies numerous biaseswhen fine-tuned on this dataset. It also reduces biases when used forfine-tuning language generation task, while preserving language quality. Theresults also reveal some prevalent demographic biases in LLMs when BEADs isused for evaluation in demographic task. We provide the BEADs dataset fordetecting biases in various domains, and this dataset is readily usable forresponsible AI development and application. The dataset can be accessed athttps://huggingface.co/datasets/shainar/BEAD .</description><author>Shaina Raza, Mizanur Rahman, Michael R. Zhang</author><pubDate>Tue, 24 Dec 2024 15:08:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04220v4</guid></item><item><title>Security Attacks on LLM-based Code Completion Tools</title><link>http://arxiv.org/abs/2408.11006v3</link><description>The rapid development of large language models (LLMs) has significantlyadvanced code completion capabilities, giving rise to a new generation ofLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, thesetools possess unique workflows, integrating multiple information sources asinput and prioritizing code suggestions over natural language interaction,which introduces distinct security challenges. Additionally, LCCTs often relyon proprietary code datasets for training, raising concerns about the potentialexposure of sensitive data. This paper exploits these distinct characteristicsof LCCTs to develop targeted attack methodologies on two critical securityrisks: jailbreaking and training data extraction attacks. Our experimentalresults expose significant vulnerabilities within LCCTs, including a 99.4%success rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rateon Amazon Q. Furthermore, We successfully extracted sensitive user data fromGitHub Copilot, including 54 real email addresses and 314 physical addressesassociated with GitHub usernames. Our study also demonstrates that thesecode-based attack methods are effective against general-purpose LLMs, such asthe GPT series, highlighting a broader security misalignment in the handling ofcode by modern LLMs. These findings underscore critical security challengesassociated with LCCTs and suggest essential directions for strengthening theirsecurity frameworks. The example code and attack samples from our research areprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.</description><author>Wen Cheng, Ke Sun, Xinyu Zhang, Wei Wang</author><pubDate>Tue, 24 Dec 2024 15:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11006v3</guid></item><item><title>MotifGPL: Motif-Enhanced Graph Prototype Learning for Deciphering Urban Social Segregation</title><link>http://arxiv.org/abs/2412.18464v1</link><description>Social segregation in cities, spanning racial, residential, and incomedimensions, is becoming more diverse and severe. As urban spaces and socialrelations grow more complex, residents in metropolitan areas experience varyinglevels of social segregation. If left unaddressed, this could lead to increasedcrime rates, heightened social tensions, and other serious issues. Effectivelyquantifying and analyzing the structures within urban spaces and residentinteractions is crucial for addressing segregation. Previous studies havemainly focused on surface-level indicators of urban segregation, lackingcomprehensive analyses of urban structure and mobility. This limitation failsto capture the full complexity of segregation. To address this gap, we proposea framework named Motif-Enhanced Graph Prototype Learning (MotifGPL),whichconsists of three key modules: prototype-based graph structure extraction,motif distribution discovery, and urban graph structure reconstruction.Specifically, we use graph structure prototype learning to extract keyprototypes from both the urban spatial graph and the origin-destination graph,incorporating key urban attributes such as points of interest, street viewimages, and flow indices. To enhance interpretability, the motif distributiondiscovery module matches each prototype with similar motifs, representingsimpler graph structures reflecting local patterns. Finally, we use the motifdistribution results to guide the reconstruction of the two graphs. This modelenables a detailed exploration of urban spatial structures and residentmobility patterns, helping identify and analyze motif patterns that influenceurban segregation, guiding the reconstruction of urban graph structures.Experimental results demonstrate that MotifGPL effectively reveals the keymotifs affecting urban social segregation and offer robust guidance formitigating this issue.</description><author>Tengfei He, Xiao Zhou</author><pubDate>Tue, 24 Dec 2024 14:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18464v1</guid></item><item><title>TrackGo: A Flexible and Efficient Method for Controllable Video Generation</title><link>http://arxiv.org/abs/2408.11475v2</link><description>Recent years have seen substantial progress in diffusion-based controllablevideo generation. However, achieving precise control in complex scenarios,including fine-grained object parts, sophisticated motion trajectories, andcoherent background movement, remains a challenge. In this paper, we introduceTrackGo, a novel approach that leverages free-form masks and arrows forconditional video generation. This method offers users with a flexible andprecise mechanism for manipulating video content. We also propose theTrackAdapter for control implementation, an efficient and lightweight adapterdesigned to be seamlessly integrated into the temporal self-attention layers ofa pretrained video generation model. This design leverages our observation thatthe attention map of these layers can accurately activate regions correspondingto motion in videos. Our experimental results demonstrate that our newapproach, enhanced by the TrackAdapter, achieves state-of-the-art performanceon key metrics such as FVD, FID, and ObjMC scores.</description><author>Haitao Zhou, Chuang Wang, Rui Nie, Jinlin Liu, Dongdong Yu, Qian Yu, Changhu Wang</author><pubDate>Tue, 24 Dec 2024 14:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11475v2</guid></item><item><title>MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory</title><link>http://arxiv.org/abs/2411.06736v3</link><description>Significant advances have been made in developing general-purpose embodied AIin environments like Minecraft through the adoption of LLM-augmentedhierarchical approaches. While these approaches, which combine high-levelplanners with low-level controllers, show promise, low-level controllersfrequently become performance bottlenecks due to repeated failures. In thispaper, we argue that the primary cause of failure in many low-level controllersis the absence of an episodic memory system. To address this, we introduceMrSteve (Memory Recall Steve-1), a novel low-level controller equipped withPlace Event Memory (PEM), a form of episodic memory that captures what, where,and when information from episodes. This directly addresses the main limitationof the popular low-level controller, Steve-1. Unlike previous models that relyon short-term memory, PEM organizes spatial and event-based data, enablingefficient recall and navigation in long-horizon tasks. Additionally, we proposean Exploration Strategy and a Memory-Augmented Task Solving Framework, allowingagents to alternate between exploration and task-solving based on recalledevents. Our approach significantly improves task-solving and explorationefficiency compared to existing methods. We will release our code and demos onthe project page: https://sites.google.com/view/mr-steve.</description><author>Junyeong Park, Junmo Cho, Sungjin Ahn</author><pubDate>Tue, 24 Dec 2024 14:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.06736v3</guid></item><item><title>GeFL: Model-Agnostic Federated Learning with Generative Models</title><link>http://arxiv.org/abs/2412.18460v1</link><description>Federated learning (FL) is a promising paradigm in distributed learning whilepreserving the privacy of users. However, the increasing size of recent modelsmakes it unaffordable for a few users to encompass the model. It leads theusers to adopt heterogeneous models based on their diverse computingcapabilities and network bandwidth. Correspondingly, FL with heterogeneousmodels should be addressed, given that FL typically involves training a singleglobal model. In this paper, we propose Generative Model-Aided FederatedLearning (GeFL), incorporating a generative model that aggregates globalknowledge across users of heterogeneous models. Our experiments on variousclassification tasks demonstrate notable performance improvements of GeFLcompared to baselines, as well as limitations in terms of privacy andscalability. To tackle these concerns, we introduce a novel framework, GeFL-F.It trains target networks aided by feature-generative models. We empiricallydemonstrate the consistent performance gains of GeFL-F, while demonstratingbetter privacy preservation and robustness to a large number of clients. Codesare available at [1].</description><author>Honggu Kang, Seohyeon Cha, Joonhyuk Kang</author><pubDate>Tue, 24 Dec 2024 14:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18460v1</guid></item><item><title>Underwater Image Restoration via Polymorphic Large Kernel CNNs</title><link>http://arxiv.org/abs/2412.18459v1</link><description>Underwater Image Restoration (UIR) remains a challenging task in computervision due to the complex degradation of images in underwater environments.While recent approaches have leveraged various deep learning techniques,including Transformers and complex, parameter-heavy models to achievesignificant improvements in restoration effects, we demonstrate that pure CNNarchitectures with lightweight parameters can achieve comparable results. Inthis paper, we introduce UIR-PolyKernel, a novel method for underwater imagerestoration that leverages Polymorphic Large Kernel CNNs. Our approach uniquelycombines large kernel convolutions of diverse sizes and shapes to effectivelycapture long-range dependencies within underwater imagery. Additionally, weintroduce a Hybrid Domain Attention module that integrates frequency andspatial domain attention mechanisms to enhance feature importance. Byleveraging the frequency domain, we can capture hidden features that may not beperceptible to humans but are crucial for identifying patterns in bothunderwater and on-air images. This approach enhances the generalization androbustness of our UIR model. Extensive experiments on benchmark datasetsdemonstrate that UIR-PolyKernel achieves state-of-the-art performance inunderwater image restoration tasks, both quantitatively and qualitatively. Ourresults show that well-designed pure CNN architectures can effectively competewith more complex models, offering a balance between performance andcomputational efficiency. This work provides new insights into the potential ofCNN-based approaches for challenging image restoration tasks in underwaterenvironments. The code is available at\href{https://github.com/CXH-Research/UIR-PolyKernel}{https://github.com/CXH-Research/UIR-PolyKernel}.</description><author>Xiaojiao Guo, Yihang Dong, Xuhang Chen, Weiwen Chen, Zimeng Li, FuChen Zheng, Chi-Man Pun</author><pubDate>Tue, 24 Dec 2024 14:32:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18459v1</guid></item><item><title>Multi-Agent Norm Perception and Induction in Distributed Healthcare</title><link>http://arxiv.org/abs/2412.18454v1</link><description>This paper presents a Multi-Agent Norm Perception and Induction LearningModel aimed at facilitating the integration of autonomous agent systems intodistributed healthcare environments through dynamic interaction processes. Thenature of the medical norm system and its sharing channels necessitatesdistinct approaches for Multi-Agent Systems to learn two types of norms.Building on this foundation, the model enables agents to simultaneously learndescriptive norms, which capture collective tendencies, and prescriptive norms,which dictate ideal behaviors. Through parameterized mixed probability densitymodels and practice-enhanced Markov games, the multi-agent system perceivesdescriptive norms in dynamic interactions and captures emergent prescriptivenorms. We conducted experiments using a dataset from a neurological medicalcenter spanning from 2016 to 2020.</description><author>Chao Li, Olga Petruchik, Elizaveta Grishanina, Sergey Kovalchuk</author><pubDate>Tue, 24 Dec 2024 14:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18454v1</guid></item><item><title>SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance</title><link>http://arxiv.org/abs/2406.18118v4</link><description>As the development of large language models (LLMs) rapidly advances, securingthese models effectively without compromising their utility has become apivotal area of research. However, current defense strategies against jailbreakattacks (i.e., efforts to bypass security protocols) often suffer from limitedadaptability, restricted general capability, and high cost. To address thesechallenges, we introduce SafeAligner, a methodology implemented at the decodingstage to fortify defenses against jailbreak attacks. We begin by developing twospecialized models: the Sentinel Model, which is trained to foster safety, andthe Intruder Model, designed to generate riskier responses. SafeAlignerleverages the disparity in security levels between the responses from thesemodels to differentiate between harmful and beneficial tokens, effectivelyguiding the safety alignment by altering the output token distribution of thetarget model. Extensive experiments show that SafeAligner can increase thelikelihood of beneficial tokens, while reducing the occurrence of harmful ones,thereby ensuring secure alignment with minimal loss to generality.</description><author>Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Wenyu Zhan, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang</author><pubDate>Tue, 24 Dec 2024 14:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18118v4</guid></item><item><title>3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding</title><link>http://arxiv.org/abs/2412.18450v1</link><description>A 3D scene graph represents a compact scene model, storing information aboutthe objects and the semantic relationships between them, making its usepromising for robotic tasks. When interacting with a user, an embodiedintelligent agent should be capable of responding to various queries about thescene formulated in natural language. Large Language Models (LLMs) arebeneficial solutions for user-robot interaction due to their natural languageunderstanding and reasoning abilities. Recent methods for creating learnablerepresentations of 3D scenes have demonstrated the potential to improve thequality of LLMs responses by adapting to the 3D world. However, the existingmethods do not explicitly utilize information about the semantic relationshipsbetween objects, limiting themselves to information about their coordinates. Inthis work, we propose a method 3DGraphLLM for constructing a learnablerepresentation of a 3D scene graph. The learnable representation is used asinput for LLMs to perform 3D vision-language tasks. In our experiments onpopular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2capdatasets, we demonstrate the advantage of this approach over baseline methodsthat do not use information about the semantic relationships between objects.The code is publicly available athttps://github.com/CognitiveAISystems/3DGraphLLM.</description><author>Tatiana Zemskova, Dmitry Yudin</author><pubDate>Tue, 24 Dec 2024 14:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18450v1</guid></item><item><title>Guided Real Image Dehazing using YCbCr Color Space</title><link>http://arxiv.org/abs/2412.17496v2</link><description>Image dehazing, particularly with learning-based methods, has gainedsignificant attention due to its importance in real-world applications.However, relying solely on the RGB color space often fall short, frequentlyleaving residual haze. This arises from two main issues: the difficulty inobtaining clear textural features from hazy RGB images and the complexity ofacquiring real haze/clean image pairs outside controlled environments likesmoke-filled scenes. To address these issues, we first propose a novelStructure Guided Dehazing Network (SGDN) that leverages the superior structuralproperties of YCbCr features over RGB. It comprises two key modules: Bi-ColorGuidance Bridge (BGB) and Color Enhancement Module (CEM). BGB integrates aphase integration module and an interactive attention module, utilizing therich texture features of the YCbCr space to guide the RGB space, therebyrecovering clearer features in both frequency and spatial domains. To maintaintonal consistency, CEM further enhances the color perception of RGB features byaggregating YCbCr channel information. Furthermore, for effective supervisedlearning, we introduce a Real-World Well-Aligned Haze (RW$^2$AH) dataset, whichincludes a diverse range of scenes from various geographical regions andclimate conditions. Experimental results demonstrate that our method surpassesexisting state-of-the-art methods across multiple real-world smoke/hazedatasets. Code and Dataset:\textcolor{blue}{\url{https://github.com/fiwy0527/AAAI25_SGDN.}}</description><author>Wenxuan Fang, Junkai Fan, Yu Zheng, Jiangwei Weng, Ying Tai, Jun Li</author><pubDate>Tue, 24 Dec 2024 14:18:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17496v2</guid></item><item><title>Optimal-state Dynamics Estimation for Physics-based Human Motion Capture from Videos</title><link>http://arxiv.org/abs/2410.07795v3</link><description>Human motion capture from monocular videos has made significant progress inrecent years. However, modern approaches often produce temporal artifacts, e.g.in form of jittery motion and struggle to achieve smooth and physicallyplausible motions. Explicitly integrating physics, in form of internal forcesand exterior torques, helps alleviating these artifacts. Currentstate-of-the-art approaches make use of an automatic PD controller to predicttorques and reaction forces in order to re-simulate the input kinematics, i.e.the joint angles of a predefined skeleton. However, due to imperfect physicalmodels, these methods often require simplifying assumptions and extensivepreprocessing of the input kinematics to achieve good performance. To this end,we propose a novel method to selectively incorporate the physics models withthe kinematics observations in an online setting, inspired by a neuralKalman-filtering approach. We develop a control loop as a meta-PD controller topredict internal joint torques and external reaction forces, followed by aphysics-based motion simulation. A recurrent neural network is introduced torealize a Kalman filter that attentively balances the kinematics input andsimulated motion, resulting in an optimal-state dynamics prediction. We showthat this filtering step is crucial to provide an online supervision that helpsbalancing the shortcoming of the respective input motions, thus being importantfor not only capturing accurate global motion trajectories but also producingphysically plausible human poses. The proposed approach excels in thephysics-based human pose estimation task and demonstrates the physicalplausibility of the predictive dynamics, compared to state of the art. The codeis available on https://github.com/cuongle1206/OSDCap</description><author>Cuong Le, Viktor Johansson, Manon Kok, Bastian Wandt</author><pubDate>Tue, 24 Dec 2024 14:10:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07795v3</guid></item><item><title>Bridging Data Islands: Geographic Heterogeneity-Aware Federated Learning for Collaborative Remote Sensing Semantic Segmentation</title><link>http://arxiv.org/abs/2404.09292v2</link><description>Remote sensing semantic segmentation (RSS) is an essential technology inearth observation missions. Due to concerns over geographic informationsecurity, data privacy, storage bottleneck and industry competition,high-quality annotated remote sensing images are often isolated and distributedacross institutions. The issue of remote sensing data islands poses challengesfor fully utilizing isolated datasets to train a global model. Federatedlearning (FL), a privacy-preserving distributed collaborative learningtechnology, offers a potential solution to leverage isolated remote sensingdata. Typically, remote sensing images from different institutions exhibitsignificant geographic heterogeneity, characterized by coupledclass-distribution heterogeneity and object-appearance heterogeneity. However,existing FL methods lack consideration of them, leading to a decline in theperformance of the global model when FL is directly applied to RSS. We proposea novel Geographic heterogeneity-aware Federated learning (GeoFed) framework tobridge data islands in RSS. Our framework consists of three modules, includingthe Global Insight Enhancement (GIE) module, the Essential Feature Mining (EFM)module and the Local-Global Balance (LoGo) module. Through the GIE module,class distribution heterogeneity is alleviated by introducing a prior globalclass distribution vector. We design an EFM module to alleviate objectappearance heterogeneity by constructing essential features. Furthermore, theLoGo module enables the model to possess both global generalization capabilityand local adaptation. Extensive experiments on three public datasets (i.e.,FedFBP, FedCASID, FedInria) demonstrate that our GeoFed framework consistentlyoutperforms the current state-of-the-art methods.</description><author>Jieyi Tan, Yansheng Li, Sergey A. Bartalev, Shinkarenko Stanislav, Bo Dang, Yongjun Zhang, Liangqi Yuan, Wei Chen</author><pubDate>Tue, 24 Dec 2024 14:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09292v2</guid></item><item><title>3D Shape Tokenization</title><link>http://arxiv.org/abs/2412.15618v2</link><description>We introduce Shape Tokens, a 3D representation that is continuous, compact,and easy to incorporate into machine learning models. Shape Tokens act asconditioning vectors that represent shape information in a 3D flow-matchingmodel. The flow-matching model is trained to approximate probability densityfunctions corresponding to delta functions concentrated on the surfaces ofshapes in 3D. By attaching Shape Tokens to various machine learning models, wecan generate new shapes, convert images to 3D, align 3D shapes with text andimages, and render shapes directly at variable, user specified, resolution.Moreover, Shape Tokens enable a systematic analysis of geometric propertiessuch as normal, density, and deformation field. Across all tasks andexperiments, utilizing Shape Tokens demonstrate strong performance compared toexisting baselines.</description><author>Jen-Hao Rick Chang, Yuyang Wang, Miguel Angel Bautista Martin, Jiatao Gu, Josh Susskind, Oncel Tuzel</author><pubDate>Tue, 24 Dec 2024 14:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15618v2</guid></item><item><title>Is Large Language Model Good at Triple Set Prediction? An Empirical Study</title><link>http://arxiv.org/abs/2412.18443v1</link><description>The core of the Knowledge Graph Completion (KGC) task is to predict andcomplete the missing relations or nodes in a KG. Common KGC tasks are mostlyabout inferring unknown elements with one or two elements being known in atriple. In comparison, the Triple Set Prediction (TSP) task is a more realisticknowledge graph completion task. It aims to predict all elements of unknowntriples based on the information from known triples. In recent years, largelanguage models (LLMs) have exhibited significant advancements in languagecomprehension, demonstrating considerable potential for KGC tasks. However, thepotential of LLM on the TSP task has not yet to be investigated. Thus in thispaper we proposed a new framework to explore the strengths and limitations ofLLM in the TSP task. Specifically, the framework consists of LLM-based rulemining and LLM-based triple set prediction. The relation list of KG embeddedwithin rich semantic information is first leveraged to prompt LLM in thegeneration of rules. This process is both efficient and independent ofstatistical information, making it easier to mine effective and realisticrules. For each subgraph, the specified rule is applied in conjunction with therelevant triples within that subgraph to guide the LLM in predicting themissing triples. Subsequently, the predictions from all subgraphs areconsolidated to derive the complete set of predicted triples on KG. Finally,the method is evaluated on the relatively complete CFamily dataset. Theexperimental results indicate that when LLMs are required to adhere to a largeamount of factual knowledge to predict missing triples, significanthallucinations occurs, leading to a noticeable decline in performance. Tofurther explore the causes of this phenomenon, this paper presents acomprehensive analysis supported by a detailed case study.</description><author>Yuan Yuan, Yajing Xu, Wen Zhang</author><pubDate>Tue, 24 Dec 2024 14:03:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18443v1</guid></item><item><title>SoK: On the Offensive Potential of AI</title><link>http://arxiv.org/abs/2412.18442v1</link><description>Our society increasingly benefits from Artificial Intelligence (AI).Unfortunately, more and more evidence shows that AI is also used for offensivepurposes. Prior works have revealed various examples of use cases in which thedeployment of AI can lead to violation of security and privacy objectives. Noextant work, however, has been able to draw a holistic picture of the offensivepotential of AI. In this SoK paper we seek to lay the ground for a systematicanalysis of the heterogeneous capabilities of offensive AI. In particular we(i) account for AI risks to both humans and systems while (ii) consolidatingand distilling knowledge from academic literature, expert opinions, industrialvenues, as well as laymen -- all of which being valuable sources of informationon offensive AI. To enable alignment of such diverse sources of knowledge, we devise a commonset of criteria reflecting essential technological factors related to offensiveAI. With the help of such criteria, we systematically analyze: 95 researchpapers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a userstudy (N=549) entailing individuals with diverse backgrounds and expertise; andthe opinion of 12 experts. Our contributions not only reveal concerning ways(some of which overlooked by prior work) in which AI can be offensively usedtoday, but also represent a foothold to address this threat in the years tocome.</description><author>Saskia Laura Schrer, Giovanni Apruzzese, Soheil Human, Pavel Laskov, Hyrum S. Anderson, Edward W. N. Bernroider, Aurore Fass, Ben Nassi, Vera Rimmer, Fabio Roli, Samer Salam, Ashley Shen, Ali Sunyaev, Tim Wadwha-Brown, Isabel Wagner, Gang Wang</author><pubDate>Tue, 24 Dec 2024 14:02:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18442v1</guid></item><item><title>Unlocking the Potential of Multiple BERT Models for Bangla Question Answering in NCTB Textbooks</title><link>http://arxiv.org/abs/2412.18440v1</link><description>Evaluating text comprehension in educational settings is critical forunderstanding student performance and improving curricular effectiveness. Thisstudy investigates the capability of state-of-the-art language models-RoBERTaBase, Bangla-BERT, and BERT Base-in automatically assessing Banglapassage-based question-answering from the National Curriculum and TextbookBoard (NCTB) textbooks for classes 6-10. A dataset of approximately 3,000Bangla passage-based question-answering instances was compiled, and the modelswere evaluated using F1 Score and Exact Match (EM) metrics across varioushyperparameter configurations. Our findings revealed that Bangla-BERTconsistently outperformed the other models, achieving the highest F1 (0.75) andEM (0.53) scores, particularly with smaller batch sizes, the inclusion of stopwords, and a moderate learning rate. In contrast, RoBERTa Base demonstrated theweakest performance, with the lowest F1 (0.19) and EM (0.27) scores undercertain configurations. The results underscore the importance of fine-tuninghyperparameters for optimizing model performance and highlight the potential ofmachine learning models in evaluating text comprehension in educationalcontexts. However, limitations such as dataset size, spelling inconsistencies,and computational constraints emphasize the need for further research toenhance the robustness and applicability of these models. This study lays thegroundwork for the future development of automated evaluation systems ineducational institutions, providing critical insights into model performance inthe context of Bangla text comprehension.</description><author>Abdullah Khondoker, Enam Ahmed Taufik, Md Iftekhar Islam Tashik, S M Ishtiak mahmud, Antara Firoz Parsa</author><pubDate>Tue, 24 Dec 2024 13:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18440v1</guid></item><item><title>FPPL: An Efficient and Non-IID Robust Federated Continual Learning Framework</title><link>http://arxiv.org/abs/2411.01904v3</link><description>Federated continual learning (FCL) aims to learn from sequential data streamin the decentralized federated learning setting, while simultaneouslymitigating the catastrophic forgetting issue in classical continual learning.Existing FCL methods usually employ typical rehearsal mechanisms, which couldresult in privacy violations or additional onerous storage and computationalburdens. In this work, an efficient and non-IID robust federated continuallearning framework, called Federated Prototype-Augmented Prompt Learning(FPPL), is proposed. The FPPL can collaboratively learn lightweight promptsaugmented by prototypes without rehearsal. On the client side, a fusionfunction is employed to fully leverage the knowledge contained in task-specificprompts for alleviating catastrophic forgetting. Additionally, globalprototypes aggregated from the server are used to obtain unified representationthrough contrastive learning, mitigating the impact of non-IID-derived dataheterogeneity. On the server side, locally uploaded prototypes are utilized toperform debiasing on the classifier, further alleviating the performancedegradation caused by both non-IID and catastrophic forgetting. Empiricalevaluations demonstrate the effectiveness of FPPL, achieving notableperformance with an efficient design while remaining robust to diverse non-IIDdegrees. Code is available at: https://github.com/ycheoo/FPPL.</description><author>Yuchen He, Chuyun Shen, Xiangfeng Wang, Bo Jin</author><pubDate>Tue, 24 Dec 2024 13:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01904v3</guid></item><item><title>MixMAS: A Framework for Sampling-Based Mixer Architecture Search for Multimodal Fusion and Learning</title><link>http://arxiv.org/abs/2412.18437v1</link><description>Choosing a suitable deep learning architecture for multimodal data fusion isa challenging task, as it requires the effective integration and processing ofdiverse data types, each with distinct structures and characteristics. In thispaper, we introduce MixMAS, a novel framework for sampling-based mixerarchitecture search tailored to multimodal learning. Our approach automaticallyselects the optimal MLP-based architecture for a given multimodal machinelearning (MML) task. Specifically, MixMAS utilizes a sampling-basedmicro-benchmarking strategy to explore various combinations ofmodality-specific encoders, fusion functions, and fusion networks,systematically identifying the architecture that best meets the task'sperformance metrics.</description><author>Abdelmadjid Chergui, Grigor Bezirganyan, Sana Sellami, Laure Berti-quille, Sbastien Fournier</author><pubDate>Tue, 24 Dec 2024 13:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18437v1</guid></item><item><title>Data-driven decision-making under uncertainty with entropic risk measure</title><link>http://arxiv.org/abs/2409.19926v2</link><description>The entropic risk measure is widely used in high-stakes decision making toaccount for tail risks associated with an uncertain loss. With limited data,the empirical entropic risk estimator, i.e. replacing the expectation in theentropic risk measure with a sample average, underestimates the true risk. Todebias the empirical entropic risk estimator, we propose a stronglyasymptotically consistent bootstrapping procedure. The first step of theprocedure involves fitting a distribution to the data, whereas the second stepestimates the bias of the empirical entropic risk estimator usingbootstrapping, and corrects for it. We show that naively fitting a GaussianMixture Model to the data using the maximum likelihood criterion typicallyleads to an underestimation of the risk. To mitigate this issue, we considertwo alternative methods: a more computationally demanding one that fits thedistribution of empirical entropic risk, and a simpler one that fits theextreme value distribution. As an application of the approach, we study adistributionally robust entropic risk minimization problem with type-$\infty$Wasserstein ambiguity set, where debiasing the validation performance using ourtechniques significantly improves the calibration of the size of the ambiguityset. Furthermore, we propose a distributionally robust optimization model for awell-studied insurance contract design problem. The model considers multiple(potential) policyholders that have dependent risks and the insurer andpolicyholders use entropic risk measure. We show that cross validation methodscan result in significantly higher out-of-sample risk for the insurer if thebias in validation performance is not corrected for. This improvement can beexplained from the observation that our methods suggest a higher (and moreaccurate) premium to homeowners.</description><author>Utsav Sadana, Erick Delage, Angelos Georghiou</author><pubDate>Tue, 24 Dec 2024 13:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19926v2</guid></item><item><title>Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention</title><link>http://arxiv.org/abs/2410.06746v3</link><description>In the realm of graph learning, there is a category of methods thatconceptualize graphs as hierarchical structures, utilizing node clustering tocapture broader structural information. While generally effective, thesemethods often rely on a fixed graph coarsening routine, leading to overlyhomogeneous cluster representations and loss of node-level information. In thispaper, we envision the graph as a network of interconnected node sets withoutcompressing each cluster into a single embedding. To enable effectiveinformation transfer among these node sets, we propose the Node-to-ClusterAttention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from MultipleKernel Learning into the kernelized attention framework, effectively capturinginformation at both node and cluster levels. We then devise an efficient formfor N2C-Attn using the cluster-wise message-passing framework, achieving lineartime complexity. We further analyze how N2C-Attn combines bi-level feature mapsof queries and keys, demonstrating its capability to merge dual-granularityinformation. The resulting architecture, Cluster-wise Graph Transformer(Cluster-GT), which uses node clusters as tokens and employs our proposedN2C-Attn module, shows superior performance on various graph-level tasks. Codeis available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.</description><author>Siyuan Huang, Yunchong Song, Jiayue Zhou, Zhouhan Lin</author><pubDate>Tue, 24 Dec 2024 13:50:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06746v3</guid></item><item><title>Gaussian entropic optimal transport: Schrdinger bridges and the Sinkhorn algorithm</title><link>http://arxiv.org/abs/2412.18432v1</link><description>Entropic optimal transport problems are regularized versions of optimaltransport problems. These models play an increasingly important role in machinelearning and generative modelling. For finite spaces, these problems arecommonly solved using Sinkhorn algorithm (a.k.a. iterative proportional fittingprocedure). However, in more general settings the Sinkhorn iterations are basedon nonlinear conditional/conjugate transformations and exact finite-dimensionalsolutions cannot be computed. This article presents a finite-dimensionalrecursive formulation of the iterative proportional fitting procedure forgeneral Gaussian multivariate models. As expected, this recursive formulationis closely related to the celebrated Kalman filter and related Riccati matrixdifference equations, and it yields algorithms that can be implemented inpractical settings without further approximations. We extend this filteringmethodology to develop a refined and self-contained convergence analysis ofGaussian Sinkhorn algorithms, including closed form expressions of entropictransport maps and Schr\"odinger bridges.</description><author>O. Deniz Akyildiz, Pierre Del Moral, Joaqun Miguez</author><pubDate>Tue, 24 Dec 2024 13:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18432v1</guid></item><item><title>GeAR: Graph-enhanced Agent for Retrieval-augmented Generation</title><link>http://arxiv.org/abs/2412.18431v1</link><description>Retrieval-augmented generation systems rely on effective document retrievalcapabilities. By design, conventional sparse or dense retrievers facechallenges in multi-hop retrieval scenarios. In this paper, we present GeAR,which advances RAG performance through two key innovations: (i) graphexpansion, which enhances any conventional base retriever, such as BM25, and(ii) an agent framework that incorporates graph expansion. Our evaluationdemonstrates GeAR's superior retrieval performance on three multi-hop questionanswering datasets. Additionally, our system achieves state-of-the-art resultswith improvements exceeding 10% on the challenging MuSiQue dataset, whilerequiring fewer tokens and iterations compared to other multi-step retrievalsystems.</description><author>Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Damien Graux, Dandan Tu, Zeren Jiang, Ruofei Lai, Yang Ren, Jeff Z. Pan</author><pubDate>Tue, 24 Dec 2024 13:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18431v1</guid></item><item><title>Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent</title><link>http://arxiv.org/abs/2412.18428v1</link><description>International enterprises, organizations, or hospitals collect large amountsof multi-modal data stored in databases, text documents, images, and videos.While there has been recent progress in the separate fields of multi-modal dataexploration as well as in database systems that automatically translate naturallanguage questions to database query languages, the research challenge ofquerying database systems combined with other unstructured modalities such asimages in natural language is widely unexplored. In this paper, we propose XMODE - a system that enables explainable,multi-modal data exploration in natural language. Our approach is based on thefollowing research contributions: (1) Our system is inspired by a real-worlduse case that enables users to explore multi-modal information systems. (2)XMODE leverages a LLM-based agentic AI framework to decompose a naturallanguage question into subtasks such as text-to-SQL generation and imageanalysis. (3) Experimental results on multi-modal datasets over relational dataand images demonstrate that our system outperforms state-of-the-art multi-modalexploration systems, excelling not only in accuracy but also in variousperformance metrics such as query latency, API costs, planning efficiency, andexplanation quality, thanks to the more effective utilization of the reasoningcapabilities of LLMs.</description><author>Farhad Nooralahzadeh, Yi Zhang, Jonathan Furst, Kurt Stockinger</author><pubDate>Tue, 24 Dec 2024 13:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18428v1</guid></item><item><title>GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI Testing Agent</title><link>http://arxiv.org/abs/2412.18426v1</link><description>Nowadays, research on GUI agents is a hot topic in the AI community. However,current research focuses on GUI task automation, limiting the scope ofapplications in various GUI scenarios. In this paper, we propose a formalizedand comprehensive environment to evaluate the entire process of automated GUITesting (GTArena), offering a fair, standardized environment for consistentoperation of diverse multimodal large language models. We divide the testingprocess into three key subtasks: test intention generation, test taskexecution, and GUI defect detection, and construct a benchmark dataset based onthese to conduct a comprehensive evaluation. It evaluates the performance ofdifferent models using three data types: real mobile applications, mobileapplications with artificially injected defects, and synthetic data, thoroughlyassessing their capabilities in this relevant task. Additionally, we propose amethod that helps researchers explore the correlation between the performanceof multimodal language large models in specific scenarios and their generalcapabilities in standard benchmark tests. Experimental results indicate thateven the most advanced models struggle to perform well across all sub-tasks ofautomated GUI Testing, highlighting a significant gap between the currentcapabilities of Autonomous GUI Testing and its practical, real-worldapplicability. This gap provides guidance for the future direction of GUI Agentdevelopment. Our code is available athttps://github.com/ZJU-ACES-ISE/ChatUITest.</description><author>Kangjia Zhao, Jiahui Song, Leigang Sha, Haozhan Shen, Zhi Chen, Tiancheng Zhao, Xiubo Liang, Jianwei Yin</author><pubDate>Tue, 24 Dec 2024 13:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18426v1</guid></item><item><title>Physics-Based Dynamic Models Hybridisation Using Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2412.07514v2</link><description>Physics-based dynamic models (PBDMs) are simplified representations ofcomplex dynamical systems. PBDMs take specific processes within a complexsystem and assign a fragment of variables and an accompanying set of parametersto depict the processes. As this often leads to suboptimal parameterisation ofthe system, a key challenge requires refining the empirical parameters andvariables to reduce uncertainties while maintaining the model s explainabilityand enhancing its predictive accuracy. We demonstrate that a hybrid mosquitopopulation dynamics model, which integrates a PBDM with Physics-Informed NeuralNetworks (PINN), retains the explainability of the PBDM by incorporating thePINN-learned model parameters in place of its empirical counterparts.Specifically, we address the limitations of traditional PBDMs by modelling theparameters of larva and pupa development rates using a PINN that encodescomplex, learned interactions of air temperature, precipitation and humidity.Our results demonstrate improved mosquito population simulations including thedifficult-to-predict mosquito population peaks. This opens the possibility ofhybridisation concept application on other complex systems based on PBDMs suchas cancer growth to address the challenges posed by scarce and noisy data, andto numerical weather prediction and climate modelling to overcome the gapbetween physics-based and data-driven weather prediction models.</description><author>Branislava Lalic, Dinh Viet Cuong, Mina Petric, Vladimir Pavlovic, Ana Firanj Sremac, Mark Roantree</author><pubDate>Tue, 24 Dec 2024 13:40:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07514v2</guid></item><item><title>LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating</title><link>http://arxiv.org/abs/2412.18424v1</link><description>Large vision language models (LVLMs) have improved the document understandingcapabilities remarkably, enabling the handling of complex document elements,longer contexts, and a wider range of tasks. However, existing documentunderstanding benchmarks have been limited to handling only a small number ofpages and fail to provide a comprehensive analysis of layout elements locating.In this paper, we first define three primary task categories: Long DocumentUnderstanding, numerical Reasoning, and cross-element Locating, and thenpropose a comprehensive benchmark, LongDocURL, integrating above three primarytasks and comprising 20 sub-tasks categorized based on different primary tasksand answer evidences. Furthermore, we develop a semi-automated constructionpipeline and collect 2,325 high-quality question-answering pairs, covering morethan 33,000 pages of documents, significantly outperforming existingbenchmarks. Subsequently, we conduct comprehensive evaluation experiments onboth open-source and closed-source models across 26 different configurations,revealing critical performance gaps in this field.</description><author>Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-Zhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, Cheng-Lin Liu</author><pubDate>Tue, 24 Dec 2024 13:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18424v1</guid></item><item><title>Prompted Contextual Vectors for Spear-Phishing Detection</title><link>http://arxiv.org/abs/2402.08309v3</link><description>Spear-phishing attacks present a significant security challenge, with largelanguage models (LLMs) escalating the threat by generating convincing emailsand facilitating target reconnaissance. To address this, we propose a detectionapproach based on a novel document vectorization method that utilizes anensemble of LLMs to create representation vectors. By prompting LLMs to reasonand respond to human-crafted questions, we quantify the presence of commonpersuasion principles in the email's content, producing prompted contextualdocument vectors for a downstream supervised machine learning model. Weevaluate our method using a unique dataset generated by a proprietary systemthat automates target reconnaissance and spear-phishing email creation. Ourmethod achieves a 91\% F1 score in identifying LLM-generated spear-phishingemails, with the training set comprising only traditional phishing and benignemails. Key contributions include a novel document vectorization methodutilizing LLM reasoning, a publicly available dataset of high-qualityspear-phishing emails, and the demonstrated effectiveness of our method indetecting such emails. This methodology can be utilized for various documentclassification tasks, particularly in adversarial problem domains.</description><author>Daniel Nahmias, Gal Engelberg, Dan Klein, Asaf Shabtai</author><pubDate>Tue, 24 Dec 2024 13:37:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08309v3</guid></item><item><title>ASP-based Multi-shot Reasoning via DLV2 with Incremental Grounding</title><link>http://arxiv.org/abs/2412.17143v2</link><description>DLV2 is an AI tool for Knowledge Representation and Reasoning which supportsAnswer Set Programming (ASP) - a logic-based declarative formalism,successfully used in both academic and industrial applications. Given a logicprogram modelling a computational problem, an execution of DLV2 produces theso-called answer sets that correspond one-to-one to the solutions to theproblem at hand. The computational process of DLV2 relies on the typical Ground&amp; Solve approach where the grounding step transforms the input program into anew, equivalent ground program, and the subsequent solving step appliespropositional algorithms to search for the answer sets. Recently, emergingapplications in contexts such as stream reasoning and event processing createda demand for multi-shot reasoning: here, the system is expected to be reactivewhile repeatedly executed over rapidly changing data. In this work, we presenta new incremental reasoner obtained from the evolution of DLV2 towards iteratedreasoning. Rather than restarting the computation from scratch, the systemremains alive across repeated shots, and it incrementally handles the internalgrounding process. At each shot, the system reuses previous computations forbuilding and maintaining a large, more general ground program, from which asmaller yet equivalent portion is determined and used for computing answersets. Notably, the incremental process is performed in a completely transparentfashion for the user. We describe the system, its usage, its applicability andperformance in some practically relevant domains. Under consideration in Theoryand Practice of Logic Programming (TPLP).</description><author>Francesco Calimeri, Giovambattista Ianni, Francesco Pacenza, Simona Perri, Jessica Zangari</author><pubDate>Tue, 24 Dec 2024 13:37:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17143v2</guid></item><item><title>OmniHD-Scenes: A Next-Generation Multimodal Dataset for Autonomous Driving</title><link>http://arxiv.org/abs/2412.10734v2</link><description>The rapid advancement of deep learning has intensified the need forcomprehensive data for use by autonomous driving algorithms. High-qualitydatasets are crucial for the development of effective data-driven autonomousdriving solutions. Next-generation autonomous driving datasets must bemultimodal, incorporating data from advanced sensors that feature extensivedata coverage, detailed annotations, and diverse scene representation. Toaddress this need, we present OmniHD-Scenes, a large-scale multimodal datasetthat provides comprehensive omnidirectional high-definition data. TheOmniHD-Scenes dataset combines data from 128-beam LiDAR, six cameras, and six4D imaging radar systems to achieve full environmental perception. The datasetcomprises 1501 clips, each approximately 30-s long, totaling more than 450Ksynchronized frames and more than 5.85 million synchronized sensor data points.We also propose a novel 4D annotation pipeline. To date, we have annotated 200clips with more than 514K precise 3D bounding boxes. These clips also includesemantic segmentation annotations for static scene elements. Additionally, weintroduce a novel automated pipeline for generation of the dense occupancyground truth, which effectively leverages information from non-key frames.Alongside the proposed dataset, we establish comprehensive evaluation metrics,baseline models, and benchmarks for 3D detection and semantic occupancyprediction. These benchmarks utilize surround-view cameras and 4D imaging radarto explore cost-effective sensor solutions for autonomous driving applications.Extensive experiments demonstrate the effectiveness of our low-cost sensorconfiguration and its robustness under adverse conditions. Data will bereleased at https://www.2077ai.com/OmniHD-Scenes.</description><author>Lianqing Zheng, Long Yang, Qunshu Lin, Wenjin Ai, Minghao Liu, Shouyi Lu, Jianan Liu, Hongze Ren, Jingyue Mo, Xiaokai Bai, Jie Bai, Zhixiong Ma, Xichan Zhu</author><pubDate>Tue, 24 Dec 2024 13:35:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10734v2</guid></item><item><title>A Review of the Marathi Natural Language Processing</title><link>http://arxiv.org/abs/2412.15471v2</link><description>Marathi is one of the most widely used languages in the world. One mightexpect that the latest advances in NLP research in languages like English reachsuch a large community. However, NLP advancements in English didn't immediatelyreach Indian languages like Marathi. There were several reasons for this. Theyincluded diversity of scripts used, lack of (publicly available) resources liketokenization strategies, high quality datasets \&amp; benchmarks, and evaluationmetrics. In addition to this, the morphologically rich nature of Marathi, madeNLP tasks challenging. Advances in Neural Network (NN) based models and toolssince the early 2000s helped improve this situation and make NLP research moreaccessible. In the past 10 years, significant efforts were made to improvelanguage resources for all 22 scheduled languages of India. This paper presentsa broad overview of evolution of NLP research in Indic languages with a focuson Marathi and state-of-the-art resources and tools available to the researchcommunity. It also provides an overview of tools \&amp; techniques associated withMarathi NLP tasks.</description><author>Asang Dani, Shailesh R Sathe</author><pubDate>Tue, 24 Dec 2024 13:33:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15471v2</guid></item><item><title>Graph Neural Networks Are Evolutionary Algorithms</title><link>http://arxiv.org/abs/2412.17629v2</link><description>In this paper, we reveal the intrinsic duality between graph neural networks(GNNs) and evolutionary algorithms (EAs), bridging two traditionally distinctfields. Building on this insight, we propose Graph Neural Evolution (GNE), anovel evolutionary algorithm that models individuals as nodes in a graph andleverages designed frequency-domain filters to balance global exploration andlocal exploitation. Through the use of these filters, GNE aggregateshigh-frequency (diversity-enhancing) and low-frequency (stability-promoting)information, transforming EAs into interpretable and tunable mechanisms in thefrequency domain. Extensive experiments on benchmark functions demonstrate thatGNE consistently outperforms state-of-the-art algorithms such as GA, DE,CMA-ES, SDAES, and RL-SHADE, excelling in complex landscapes, optimal solutionshifts, and noisy environments. Its robustness, adaptability, and superiorconvergence highlight its practical and theoretical value. Beyond optimization,GNE establishes a conceptual and mathematical foundation linking EAs and GNNs,offering new perspectives for both fields. Its framework encourages thedevelopment of task-adaptive filters and hybrid approaches for EAs, while itsinsights can inspire advances in GNNs, such as improved global informationpropagation and mitigation of oversmoothing. GNE's versatility extends tosolving challenges in machine learning, including hyperparameter tuning andneural architecture search, as well as real-world applications in engineeringand operations research. By uniting the dynamics of EAs with the structuralinsights of GNNs, this work provides a foundation for interdisciplinaryinnovation, paving the way for scalable and interpretable solutions to complexoptimization problems.</description><author>Kaichen Ouyang, Shengwei Fu</author><pubDate>Tue, 24 Dec 2024 13:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17629v2</guid></item><item><title>Fashionability-Enhancing Outfit Image Editing with Conditional Diffusion Models</title><link>http://arxiv.org/abs/2412.18421v1</link><description>Image generation in the fashion domain has predominantly focused onpreserving body characteristics or following input prompts, but littleattention has been paid to improving the inherent fashionability of the outputimages. This paper presents a novel diffusion model-based approach thatgenerates fashion images with improved fashionability while maintaining controlover key attributes. Key components of our method include: 1) fashionabilityenhancement, which ensures that the generated images are more fashionable thanthe input; 2) preservation of body characteristics, encouraging the generatedimages to maintain the original shape and proportions of the input; and 3)automatic fashion optimization, which does not rely on manual input or externalprompts. We also employ two methods to collect training data for guidance whilegenerating and evaluating the images. In particular, we rate outfit imagesusing fashionability scores annotated by multiple fashion experts throughOpenSkill-based and five critical aspect-based pairwise comparisons. Thesemethods provide complementary perspectives for assessing and improving thefashionability of the generated images. The experimental results show that ourapproach outperforms the baseline Fashion++ in generating images with superiorfashionability, demonstrating its effectiveness in producing more stylish andappealing fashion images.</description><author>Qice Qin, Yuki Hirakawa, Ryotaro Shimizu, Takuya Furusawa, Edgar Simo-Serra</author><pubDate>Tue, 24 Dec 2024 13:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18421v1</guid></item><item><title>UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction</title><link>http://arxiv.org/abs/2411.07019v2</link><description>Beyond-triple fact representations including hyper-relational facts withauxiliary key-value pairs, temporal facts with additional timestamps, andnested facts implying relationships between facts, are gaining significantattention. However, existing link prediction models are usually designed forone specific type of facts, making it difficult to generalize to other factrepresentations. To overcome this limitation, we propose a Unified HierarchicalRepresentation learning framework (UniHR) for unified knowledge graph linkprediction. It consists of a unified Hierarchical Data Representation (HiDR)module and a unified Hierarchical Structure Learning (HiSL) module as graphencoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nestedfactual KGs into triple-based representations. Then HiSL incorporatesintra-fact and inter-fact message passing, focusing on enhancing the semanticinformation within individual facts and enriching the structural informationbetween facts. Experimental results across 7 datasets from 3 types of KGsdemonstrate that our UniHR outperforms baselines designed for one specific kindof KG, indicating strong generalization capability of HiDR form and theeffectiveness of HiSL module. Code and data are available athttps://github.com/Lza12a/UniHR.</description><author>Zhiqiang Liu, Mingyang Chen, Yin Hua, Zhuo Chen, Ziqi Liu, Lei Liang, Huajun Chen, Wen Zhang</author><pubDate>Tue, 24 Dec 2024 13:26:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07019v2</guid></item><item><title>Research on the Proximity Relationships of Psychosomatic Disease Knowledge Graph Modules Extracted by Large Language Models</title><link>http://arxiv.org/abs/2412.18419v1</link><description>As social changes accelerate, the incidence of psychosomatic disorders hassignificantly increased, becoming a major challenge in global health issues.This necessitates an innovative knowledge system and analytical methods to aidin diagnosis and treatment. Here, we establish the ontology model and entitytypes, using the BERT model and LoRA-tuned LLM for named entity recognition,constructing the knowledge graph with 9668 triples. Next, by analyzing thenetwork distances between disease, symptom, and drug modules, it was found thatcloser network distances among diseases can predict greater similarities intheir clinical manifestations, treatment approaches, and psychologicalmechanisms, and closer distances between symptoms indicate that they are morelikely to co-occur. Lastly, by comparing the proximity d and proximity z score,it was shown that symptom-disease pairs in primary diagnostic relationshipshave a stronger association and are of higher referential value than those indiagnostic relationships. The research results revealed the potentialconnections between diseases, co-occurring symptoms, and similarities intreatment strategies, providing new perspectives for the diagnosis andtreatment of psychosomatic disorders and valuable information for future mentalhealth research and practice.</description><author>Zihan Zhou, Ziyi Zeng, Wenhao Jiang, Yihui Zhu, Jiaxin Mao, Yonggui Yuan, Min Xia, Shubin Zhao, Mengyu Yao, Yunqian Chen</author><pubDate>Tue, 24 Dec 2024 13:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18419v1</guid></item></channel></rss>