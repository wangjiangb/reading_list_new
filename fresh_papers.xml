<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 10 Apr 2024 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD</title><link>http://arxiv.org/abs/2404.06512v1</link><description>The Large Vision-Language Model (LVLM) field has seen significantadvancements, yet its progression has been hindered by challenges incomprehending fine-grained visual content due to limited resolution. Recentefforts have aimed to enhance the high-resolution understanding capabilities ofLVLMs, yet they remain capped at approximately 1500 x 1500 pixels andconstrained to a relatively narrow resolution range. This paper representsInternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLMresolution capabilities up to 4K HD (3840 x 1600) and beyond. Concurrently,considering the ultra-high resolution may not be necessary in all scenarios, itsupports a wide range of diverse resolutions from 336 pixels to 4K standard,significantly broadening its scope of applicability. Specifically, thisresearch advances the patch division paradigm by introducing a novel extension:dynamic resolution with automatic patch configuration. It maintains thetraining image aspect ratios while automatically varying patch counts andconfiguring layouts based on a pre-trained Vision Transformer (ViT) (336 x336), leading to dynamic training resolution from 336 pixels to 4K standard.Our research demonstrates that scaling training resolution up to 4K HD leads toconsistent performance enhancements without hitting the ceiling of potentialimprovements. InternLM-XComposer2-4KHD shows superb capability that matches oreven surpasses GPT-4V and Gemini Pro in 10 of the 16 benchmarks. TheInternLM-XComposer2-4KHD model series with 7B parameters are publicly availableat https://github.com/InternLM/InternLM-XComposer.</description><author>Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang</author><pubDate>Tue, 09 Apr 2024 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06512v1</guid></item><item><title>MoReVQA: Exploring Modular Reasoning Models for Video Question Answering</title><link>http://arxiv.org/abs/2404.06511v1</link><description>This paper addresses the task of video question answering (videoQA) via adecomposed multi-stage, modular reasoning framework. Previous modular methodshave shown promise with a single planning stage ungrounded in visual content.However, through a simple and effective baseline, we find that such systems canlead to brittle behavior in practice for challenging videoQA settings. Thus,unlike traditional single-stage planning methods, we propose a multi-stagesystem consisting of an event parser, a grounding stage, and a final reasoningstage in conjunction with an external memory. All stages are training-free, andperformed using few-shot prompting of large models, creating interpretableintermediate outputs at each stage. By decomposing the underlying planning andtask complexity, our method, MoReVQA, improves over prior work on standardvideoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) withstate-of-the-art results, and extensions to related tasks (grounded videoQA,paragraph captioning).</description><author>Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid</author><pubDate>Tue, 09 Apr 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06511v1</guid></item><item><title>Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?</title><link>http://arxiv.org/abs/2404.06510v1</link><description>Enhancing semantic grounding abilities in Vision-Language Models (VLMs) ofteninvolves collecting domain-specific training data, refining the networkarchitectures, or modifying the training recipes. In this work, we venture intoan orthogonal direction and explore whether VLMs can improve their semanticgrounding by "receiving" feedback, without requiring in-domain data,fine-tuning, or modifications to the network architectures. We systematicallyanalyze this hypothesis using a feedback mechanism composed of a binary signal.We find that if prompted appropriately, VLMs can utilize feedback both in asingle step and iteratively, showcasing the potential of feedback as analternative technique to improve grounding in internet-scale VLMs. Furthermore,VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, wefind that this issue can be mitigated via a binary verification mechanism.Finally, we explore the potential and limitations of amalgamating thesefindings and applying them iteratively to automatically enhance VLMs' groundingperformance, showing grounding accuracy consistently improves using automatedfeedback across all models in all settings investigated. Overall, our iterativeframework improves semantic grounding in VLMs by more than 15 accuracy pointsunder noise-free feedback and up to 5 accuracy points under a simple automatedbinary verification mechanism. The project website is hosted athttps://andrewliao11.github.io/vlms_feedback</description><author>Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna</author><pubDate>Tue, 09 Apr 2024 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06510v1</guid></item><item><title>On the Effect of (Near) Duplicate Subwords in Language Modelling</title><link>http://arxiv.org/abs/2404.06508v1</link><description>Tokenisation is a core part of language models (LMs). It involves splitting acharacter sequence into subwords which are assigned arbitrary indices beforebeing served to the LM. While typically lossless, however, this process maylead to less sample efficient LM training: as it removes character-levelinformation, it could make it harder for LMs to generalise across similarsubwords, such as now and Now. We refer to such subwords as near duplicates. Inthis paper, we study the impact of near duplicate subwords on LM trainingefficiency. First, we design an experiment that gives us an upper bound to howmuch we should expect a model to improve if we could perfectly generaliseacross near duplicates. We do this by duplicating each subword in our LM'svocabulary, creating perfectly equivalent classes of subwords. Experimentally,we find that LMs need roughly 17% more data when trained in a fully duplicatedsetting. Second, we investigate the impact of naturally occurring nearduplicates on LMs. Here, we see that merging them considerably hurts LMperformance. Therefore, although subword duplication negatively impacts LMtraining efficiency, naturally occurring near duplicates may not be as similaras anticipated, limiting the potential for performance improvements.</description><author>Anton Schäfer, Thomas Hofmann, Imanol Schlag, Tiago Pimentel</author><pubDate>Tue, 09 Apr 2024 18:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06508v1</guid></item><item><title>Reconstructing Hand-Held Objects in 3D</title><link>http://arxiv.org/abs/2404.06507v1</link><description>Objects manipulated by the hand (i.e., manipulanda) are particularlychallenging to reconstruct from in-the-wild RGB images or videos. Not only doesthe hand occlude much of the object, but also the object is often only visiblein a small number of image pixels. At the same time, two strong anchors emergein this setting: (1) estimated 3D hands help disambiguate the location andscale of the object, and (2) the set of manipulanda is small relative to allpossible objects. With these insights in mind, we present a scalable paradigmfor handheld object reconstruction that builds on recent breakthroughs in largelanguage/vision models and 3D object datasets. Our model, MCC-Hand-Object(MCC-HO), jointly reconstructs hand and object geometry given a single RGBimage and inferred 3D hand as inputs. Subsequently, we use GPT-4(V) to retrievea 3D object model that matches the object in the image and rigidly align themodel to the network-inferred geometry; we call this alignmentRetrieval-Augmented Reconstruction (RAR). Experiments demonstrate that MCC-HOachieves state-of-the-art performance on lab and Internet datasets, and we showhow RAR can be used to automatically obtain 3D labels for in-the-wild images ofhand-object interactions.</description><author>Jane Wu, Georgios Pavlakos, Georgia Gkioxari, Jitendra Malik</author><pubDate>Tue, 09 Apr 2024 18:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06507v1</guid></item><item><title>Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions</title><link>http://arxiv.org/abs/2311.17048v3</link><description>Zero-shot referring expression comprehension aims at localizing boundingboxes in an image corresponding to provided textual prompts, which requires:(i) a fine-grained disentanglement of complex visual scene and textual context,and (ii) a capacity to understand relationships among disentangled entities.Unfortunately, existing large vision-language alignment (VLA) models, e.g.,CLIP, struggle with both aspects so cannot be directly used for this task. Tomitigate this gap, we leverage large foundation models to disentangle bothimages and texts into triplets in the format of (subject, predicate, object).After that, grounding is accomplished by calculating the structural similaritymatrix between visual and textual triplets with a VLA model, and subsequentlypropagate it to an instance-level similarity matrix. Furthermore, to equip VLAmodels with the ability of relationship understanding, we design atriplet-matching objective to fine-tune the VLA models on a collection ofcurated dataset containing abundant entity relationships. Experimentsdemonstrate that our visual grounding performance increase of up to 19.5% overthe SOTA zero-shot model on RefCOCO/+/g. On the more challenging Who's Waldodataset, our zero-shot approach achieves comparable accuracy to the fullysupervised model. Code is available athttps://github.com/Show-han/Zeroshot_REC.</description><author>Zeyu Han, Fangrui Zhu, Qianru Lao, Huaizu Jiang</author><pubDate>Tue, 09 Apr 2024 18:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17048v3</guid></item><item><title>Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?</title><link>http://arxiv.org/abs/2404.06503v1</link><description>Following an interaction with a patient, physicians are responsible for thesubmission of clinical documentation, often organized as a SOAP note. Aclinical note is not simply a summary of the conversation but requires the useof appropriate medical terminology. The relevant information can then beextracted and organized according to the structure of the SOAP note. In thispaper we analyze two different approaches to generate the different sections ofa SOAP note based on the audio recording of the conversation, and specificallyexamine them in terms of note consistency. The first approach generates thesections independently, while the second method generates them all together. Inthis work we make use of PEGASUS-X Transformer models and observe that bothmethods lead to similar ROUGE values (less than 1% difference) and have nodifference in terms of the Factuality metric. We perform a human evaluation tomeasure aspects of consistency and demonstrate that LLMs like Llama2 can beused to perform the same tasks with roughly the same agreement as the humanannotators. Between the Llama2 analysis and the human reviewers we observe aCohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency ofage, gender, and body part injury, respectively. With this we demonstrate theusefulness of leveraging an LLM to measure quality indicators that can beidentified by humans but are not currently captured by automatic metrics. Thisallows scaling evaluation to larger data sets, and we find that clinical noteconsistency improves by generating each new section conditioned on the outputof all previously generated sections.</description><author>Nathan Brake, Thomas Schaaf</author><pubDate>Tue, 09 Apr 2024 18:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06503v1</guid></item><item><title>Multi-person 3D pose estimation from unlabelled data</title><link>http://arxiv.org/abs/2212.08731v3</link><description>Its numerous applications make multi-human 3D pose estimation a remarkablyimpactful area of research. Nevertheless, assuming a multiple-view systemcomposed of several regular RGB cameras, 3D multi-pose estimation presentsseveral challenges. First of all, each person must be uniquely identified inthe different views to separate the 2D information provided by the cameras.Secondly, the 3D pose estimation process from the multi-view 2D information ofeach person must be robust against noise and potential occlusions in thescenario. In this work, we address these two challenges with the help of deeplearning. Specifically, we present a model based on Graph Neural Networkscapable of predicting the cross-view correspondence of the people in thescenario along with a Multilayer Perceptron that takes the 2D points to yieldthe 3D poses of each person. These two models are trained in a self-supervisedmanner, thus avoiding the need for large datasets with 3D annotations.</description><author>Daniel Rodriguez-Criado, Pilar Bachiller, George Vogiatzis, Luis J. Manso</author><pubDate>Tue, 09 Apr 2024 18:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08731v3</guid></item><item><title>Simultaneous linear connectivity of neural networks modulo permutation</title><link>http://arxiv.org/abs/2404.06498v1</link><description>Neural networks typically exhibit permutation symmetries which contribute tothe non-convexity of the networks' loss landscapes, since linearlyinterpolating between two permuted versions of a trained network tends toencounter a high loss barrier. Recent work has argued that permutationsymmetries are the only sources of non-convexity, meaning there are essentiallyno such barriers between trained networks if they are permuted appropriately.In this work, we refine these arguments into three distinct claims ofincreasing strength. We show that existing evidence only supports "weak linearconnectivity"-that for each pair of networks belonging to a set of SGDsolutions, there exist (multiple) permutations that linearly connect it withthe other networks. In contrast, the claim "strong linear connectivity"-thatfor each network, there exists one permutation that simultaneously connects itwith the other networks-is both intuitively and practically more desirable.This stronger claim would imply that the loss landscape is convex afteraccounting for permutation, and enable linear interpolation between three ormore independently trained models without increased loss. In this work, weintroduce an intermediate claim-that for certain sequences of networks, thereexists one permutation that simultaneously aligns matching pairs of networksfrom these sequences. Specifically, we discover that a single permutationaligns sequences of iteratively trained as well as iteratively pruned networks,meaning that two networks exhibit low loss barriers at each step of theiroptimization and sparsification trajectories respectively. Finally, we providethe first evidence that strong linear connectivity may be possible undercertain conditions, by showing that barriers decrease with increasing networkwidth when interpolating among three networks.</description><author>Ekansh Sharma, Devin Kwok, Tom Denton, Daniel M. Roy, David Rolnick, Gintare Karolina Dziugaite</author><pubDate>Tue, 09 Apr 2024 18:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06498v1</guid></item><item><title>Flying With Photons: Rendering Novel Views of Propagating Light</title><link>http://arxiv.org/abs/2404.06493v1</link><description>We present an imaging and neural rendering technique that seeks to synthesizevideos of light propagating through a scene from novel, moving cameraviewpoints. Our approach relies on a new ultrafast imaging setup to capture afirst-of-its kind, multi-viewpoint video dataset with picosecond-level temporalresolution. Combined with this dataset, we introduce an efficient neural volumerendering framework based on the transient field. This field is defined as amapping from a 3D point and 2D direction to a high-dimensional, discrete-timesignal that represents time-varying radiance at ultrafast timescales. Renderingwith transient fields naturally accounts for effects due to the finite speed oflight, including viewpoint-dependent appearance changes caused by lightpropagation delays to the camera. We render a range of complex effects,including scattering, specular reflection, refraction, and diffraction.Additionally, we demonstrate removing viewpoint-dependent propagation delaysusing a time warping procedure, rendering of relativistic effects, and videosynthesis of direct and global components of light transport.</description><author>Anagh Malik, Noah Juravsky, Ryan Po, Gordon Wetzstein, Kiriakos N. Kutulakos, David B. Lindell</author><pubDate>Tue, 09 Apr 2024 18:48:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06493v1</guid></item><item><title>Offline Supervised Learning V.S. Online Direct Policy Optimization: A Comparative Study and A Unified Training Paradigm for Neural Network-Based Optimal Feedback Control</title><link>http://arxiv.org/abs/2211.15930v3</link><description>This work is concerned with solving neural network-based feedback controllersefficiently for optimal control problems. We first conduct a comparative studyof two prevalent approaches: offline supervised learning and online directpolicy optimization. Albeit the training part of the supervised learningapproach is relatively easy, the success of the method heavily depends on theoptimal control dataset generated by open-loop optimal control solvers. Incontrast, direct policy optimization turns the optimal control problem into anoptimization problem directly without any requirement of pre-computing, but thedynamics-related objective can be hard to optimize when the problem iscomplicated. Our results underscore the superiority of offline supervisedlearning in terms of both optimality and training time. To overcome the mainchallenges, dataset and optimization, in the two approaches respectively, wecomplement them and propose the Pre-train and Fine-tune strategy as a unifiedtraining paradigm for optimal feedback control, which further improves theperformance and robustness significantly. Our code is accessible athttps://github.com/yzhao98/DeepOptimalControl.</description><author>Yue Zhao, Jiequn Han</author><pubDate>Tue, 09 Apr 2024 18:45:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15930v3</guid></item><item><title>Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective</title><link>http://arxiv.org/abs/2404.06492v1</link><description>Graphs are a natural representation for systems based on relations betweenconnected entities. Combinatorial optimization problems, which arise whenconsidering an objective function related to a process of interest on discretestructures, are often challenging due to the rapid growth of the solutionspace. The trial-and-error paradigm of Reinforcement Learning has recentlyemerged as a promising alternative to traditional methods, such as exactalgorithms and (meta)heuristics, for discovering better decision-makingstrategies in a variety of disciplines including chemistry, computer science,and statistics. Despite the fact that they arose in markedly different fields,these techniques share significant commonalities. Therefore, we set out tosynthesize this work in a unifying perspective that we term Graph ReinforcementLearning, interpreting it as a constructive decision-making method for graphproblems. After covering the relevant technical background, we review worksalong the dividing line of whether the goal is to optimize graph structuregiven a process of interest, or to optimize the outcome of the process itselfunder fixed graph structure. Finally, we discuss the common challenges facingthe field and open research questions. In contrast with other surveys, thepresent work focuses on non-canonical graph problems for which performantalgorithms are typically not known and Reinforcement Learning is able toprovide efficient and effective solutions.</description><author>Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi</author><pubDate>Tue, 09 Apr 2024 18:45:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06492v1</guid></item><item><title>Influencer Backdoor Attack on Semantic Segmentation</title><link>http://arxiv.org/abs/2303.12054v4</link><description>When a small number of poisoned samples are injected into the trainingdataset of a deep neural network, the network can be induced to exhibitmalicious behavior during inferences, which poses potential threats toreal-world applications. While they have been intensively studied inclassification, backdoor attacks on semantic segmentation have been largelyoverlooked. Unlike classification, semantic segmentation aims to classify everypixel within a given image. In this work, we explore backdoor attacks onsegmentation models to misclassify all pixels of a victim class by injecting aspecific trigger on non-victim pixels during inferences, which is dubbedInfluencer Backdoor Attack (IBA). IBA is expected to maintain theclassification accuracy of non-victim pixels and mislead classifications of allvictim pixels in every single inference and could be easily applied toreal-world scenes. Based on the context aggregation ability of segmentationmodels, we proposed a simple, yet effective, Nearest-Neighbor trigger injectionstrategy. We also introduce an innovative Pixel Random Labeling strategy whichmaintains optimal performance even when the trigger is placed far from thevictim pixels. Our extensive experiments reveal that current segmentationmodels do suffer from backdoor attacks, demonstrate IBA real-worldapplicability, and show that our proposed techniques can further increaseattack performance.</description><author>Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao</author><pubDate>Tue, 09 Apr 2024 18:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12054v4</guid></item><item><title>Pitfalls of Conversational LLMs on News Debiasing</title><link>http://arxiv.org/abs/2404.06488v1</link><description>This paper addresses debiasing in news editing and evaluates theeffectiveness of conversational Large Language Models in this task. We designedan evaluation checklist tailored to news editors' perspectives, obtainedgenerated texts from three popular conversational models using a subset of apublicly available dataset in media bias, and evaluated the texts according tothe designed checklist. Furthermore, we examined the models as evaluator forchecking the quality of debiased model outputs. Our findings indicate that noneof the LLMs are perfect in debiasing. Notably, some models, including ChatGPT,introduced unnecessary changes that may impact the author's style and createmisinformation. Lastly, we show that the models do not perform as proficientlyas domain experts in evaluating the quality of debiased outputs.</description><author>Ipek Baris Schlicht, Defne Altiok, Maryanne Taouk, Lucie Flek</author><pubDate>Tue, 09 Apr 2024 18:42:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06488v1</guid></item><item><title>GO4Align: Group Optimization for Multi-Task Alignment</title><link>http://arxiv.org/abs/2404.06486v1</link><description>This paper proposes \textit{GO4Align}, a multi-task optimization approachthat tackles task imbalance by explicitly aligning the optimization acrosstasks. To achieve this, we design an adaptive group risk minimization strategy,compromising two crucial techniques in implementation: (i) dynamical groupassignment, which clusters similar tasks based on task interactions; (ii)risk-guided group indicators, which exploit consistent task correlations withrisk information from previous iterations. Comprehensive experimental resultson diverse typical benchmarks demonstrate our method's performance superioritywith even lower computational costs.</description><author>Jiayi Shen, Cheems Wang, Zehao Xiao, Nanne Van Noord, Marcel Worring</author><pubDate>Tue, 09 Apr 2024 18:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06486v1</guid></item><item><title>Public-private funding models in open source software development: A case study on scikit-learn</title><link>http://arxiv.org/abs/2404.06484v1</link><description>Governments are increasingly allocating funding for open source software(OSS) development in order to address concerns related to software security,digital sovereignty, and the competitiveness of domestic software markets,amongst others. While such funding is generally welcomed by OSS practitioners,how OSS developers perceive the relative benefits and drawbacks of governmentalfunding remains an open question. This paper explores this question through acase study on scikit-learn, a Python library for machine learning, whosefunding model combines research grants, commercial sponsorship, communitydonations, and a 32 million EUR grant from the French government's artificialintelligence strategy. Through 25 interviews with scikit-learn maintainers andfunders, this study makes two key contributions with implications for researchand practice. First, it provides novel insights into the role of apublic-private funding model in a successful, community-led OSS project and howmaintainers evaluate their funding model. Furthermore, it highlights thegovernance mechanisms employed by maintainers to safeguard the community ethosof the project. Second, it offers practical implications for OSS developercommunities, companies, and governments. For OSS communities, the studyillustrates the benefits of a diversified funding model in balancing the meritsand drawbacks of different funding sources. For companies, it serves as areminder that sponsoring developers or directly funding OSS projects cansignificantly support OSS maintainers, who often struggle with limitedresources and towering workloads. For governments, the findings emphasise theimportance of funding the maintenance of existing OSS projects in addition toor exclusively funding new innovations. The paper concludes with suggestionsfor future research on OSS funding models.</description><author>Cailean Osborne</author><pubDate>Tue, 09 Apr 2024 18:35:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06484v1</guid></item><item><title>RhythmMamba: Fast Remote Physiological Measurement with Arbitrary Length Videos</title><link>http://arxiv.org/abs/2404.06483v1</link><description>Remote photoplethysmography (rPPG) is a non-contact method for detectingphysiological signals from facial videos, holding great potential in variousapplications such as healthcare, affective computing, and anti-spoofing.Existing deep learning methods struggle to address two core issues of rPPGsimultaneously: extracting weak rPPG signals from video segments with largespatiotemporal redundancy and understanding the periodic patterns of rPPG amonglong contexts. This represents a trade-off between computational complexity andthe ability to capture long-range dependencies, posing a challenge for rPPGthat is suitable for deployment on mobile devices. Based on the in-depthexploration of Mamba's comprehension of spatial and temporal information, thispaper introduces RhythmMamba, an end-to-end Mamba-based method that employsmulti-temporal Mamba to constrain both periodic patterns and short-term trends,coupled with frequency domain feed-forward to enable Mamba to robustlyunderstand the quasi-periodic patterns of rPPG. Extensive experiments show thatRhythmMamba achieves state-of-the-art performance with reduced parameters andlower computational complexity. The proposed RhythmMamba can be applied tovideo segments of any length without performance degradation. The codes areavailable at https://github.com/zizheng-guo/RhythmMamba.</description><author>Bochao Zou, Zizheng Guo, Xiaocheng Hu, Huimin Ma</author><pubDate>Tue, 09 Apr 2024 18:34:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06483v1</guid></item><item><title>GeoDirDock: Guiding Docking Along Geodesic Paths</title><link>http://arxiv.org/abs/2404.06481v1</link><description>This work introduces GeoDirDock (GDD), a novel approach to molecular dockingthat enhances the accuracy and physical plausibility of ligand dockingpredictions. GDD guides the denoising process of a diffusion model alonggeodesic paths within multiple spaces representing translational, rotational,and torsional degrees of freedom. Our method leverages expert knowledge todirect the generative modeling process, specifically targeting desiredprotein-ligand interaction regions. We demonstrate that GDD significantlyoutperforms existing blind docking methods in terms of RMSD accuracy andphysicochemical pose realism. Our results indicate that incorporating domainexpertise into the diffusion process leads to more biologically relevantdocking predictions. Additionally, we explore the potential of GDD for leadoptimization in drug discovery through angle transfer in maximal commonsubstructure (MCS) docking, showcasing its capability to predict ligandorientations for chemically similar compounds accurately.</description><author>Raúl Miñán, Javier Gallardo, Álvaro Ciudad, Alexis Molina</author><pubDate>Tue, 09 Apr 2024 18:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06481v1</guid></item><item><title>Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks</title><link>http://arxiv.org/abs/2404.06480v1</link><description>Recently, the large language model (LLM) community has shown increasinginterest in enhancing LLMs' capability to handle extremely long documents. Asvarious long-text techniques and model architectures emerge, the precise anddetailed evaluation of models' long-text capabilities has become increasinglyimportant. Existing long-text evaluation benchmarks, such as L-Eval andLongBench, construct long-text test sets based on open-source datasets,focusing mainly on QA and summarization tasks. These datasets include testsamples of varying lengths (from 2k to 32k+) entangled together, making itchallenging to assess model capabilities across different length ranges.Moreover, they do not cover the ultralong settings (100k+ tokens) that thelatest LLMs claim to achieve. In this paper, we introduce Ada-LEval, alength-adaptable benchmark for evaluating the long-context understanding ofLLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, whichenable a more reliable evaluation of LLMs' long context capabilities. Thesebenchmarks support intricate manipulation of the length of test cases, and caneasily produce text samples up to 128k tokens. We evaluate 4 state-of-the-artclosed-source API models and 6 open-source models with Ada-LEval. Theevaluation results demonstrate the limitations of current LLMs, especially inultra-long-context settings. Our code is available athttps://github.com/open-compass/Ada-LEval.</description><author>Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, Kai Chen</author><pubDate>Tue, 09 Apr 2024 18:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06480v1</guid></item><item><title>Text-Based Reasoning About Vector Graphics</title><link>http://arxiv.org/abs/2404.06479v1</link><description>While large multimodal models excel in broad vision-language benchmarks, theyoften struggle with tasks requiring precise perception of low-level visualdetails, such as comparing line lengths or solving simple mazes. In particular,this failure mode persists in question-answering tasks about vector graphics --images composed purely of 2D objects and shapes. To address this challenge, wepropose the Visually Descriptive Language Model (VDLM), which performstext-based reasoning about vector graphics. VDLM leverages Scalable VectorGraphics (SVG) for a more precise visual description and first uses anoff-the-shelf raster-to-SVG algorithm for encoding. Since existing languagemodels cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVGwith pretrained language models through a newly introduced intermediatesymbolic representation, Primal Visual Description (PVD), comprising primitiveattributes (e.g., shape, position, measurement) with their correspondingpredicted values. PVD is task-agnostic and represents visual primitives thatare universal across all vector graphics. It can be learned with procedurallygenerated (SVG, PVD) pairs and also enables the direct use of LLMs forgeneralization to complex reasoning tasks. By casting an image to a text-basedrepresentation, we can leverage the power of language models to learn alignmentfrom SVG to visual primitives and generalize to unseen question-answeringtasks. Empirical results show that VDLM achieves stronger zero-shot performancecompared to state-of-the-art LMMs, such as GPT-4V, in various low-levelmultimodal perception and reasoning tasks on vector graphics. We additionallypresent extensive analyses on VDLM's performance, demonstrating that ourframework offers better interpretability due to its disentangled perception andreasoning processes. Project page: https://mikewangwzhl.github.io/VDLM/</description><author>Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li, Jiajun Wu, Heng Ji</author><pubDate>Tue, 09 Apr 2024 18:30:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06479v1</guid></item><item><title>Autonomous Evaluation and Refinement of Digital Agents</title><link>http://arxiv.org/abs/2404.06474v1</link><description>We show that domain-general automatic evaluators can significantly improvethe performance of agents for web navigation and device control. We experimentwith multiple evaluation models that trade off between inference cost,modularity of design, and accuracy. We validate the performance of these modelsin several popular benchmarks for digital agents, finding between 74.4 and92.9% agreement with oracle evaluation metrics. Finally, we use theseevaluators to improve the performance of existing agents via fine-tuning andinference-time guidance. Without any additional supervision, we improvestate-of-the-art performance by 29% on the popular benchmark WebArena, andachieve a 75% relative improvement in a challenging domain transfer scenario.</description><author>Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, Alane Suhr</author><pubDate>Tue, 09 Apr 2024 18:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06474v1</guid></item><item><title>Learning State-Invariant Representations of Objects from Image Collections with State, Pose, and Viewpoint Changes</title><link>http://arxiv.org/abs/2404.06470v1</link><description>We add one more invariance - state invariance - to the more commonly usedother invariances for learning object representations for recognition andretrieval. By state invariance, we mean robust with respect to changes in thestructural form of the object, such as when an umbrella is folded, or when anitem of clothing is tossed on the floor. Since humans generally have nodifficulty in recognizing objects despite such state changes, we are naturallyfaced with the question of whether it is possible to devise a neuralarchitecture with similar abilities. To that end, we present a novel dataset,ObjectsWithStateChange, that captures state and pose variations in the objectimages recorded from arbitrary viewpoints. We believe that this dataset willfacilitate research in fine-grained object recognition and retrieval of objectsthat are capable of state changes. The goal of such research would be to trainmodels capable of generating object embeddings that remain invariant to statechanges while also staying invariant to transformations induced by changes inviewpoint, pose, illumination, etc. To demonstrate the usefulness of theObjectsWithStateChange dataset, we also propose a curriculum learning strategythat uses the similarity relationships in the learned embedding space aftereach epoch to guide the training process. The model learns discriminativefeatures by comparing visually similar objects within and across differentcategories, encouraging it to differentiate between objects that may bechallenging to distinguish due to changes in their state. We believe that thisstrategy enhances the model's ability to capture discriminative features forfine-grained tasks that may involve objects with state changes, leading toperformance improvements on object-level tasks not only on our new dataset, butalso on two other challenging multi-view datasets such as ModelNet40 andObjectPI.</description><author>Rohan Sarkar, Avinash Kak</author><pubDate>Tue, 09 Apr 2024 18:17:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06470v1</guid></item><item><title>Hyperparameter Selection in Continual Learning</title><link>http://arxiv.org/abs/2404.06466v1</link><description>In continual learning (CL) -- where a learner trains on a stream of data --standard hyperparameter optimisation (HPO) cannot be applied, as a learner doesnot have access to all of the data at the same time. This has prompted thedevelopment of CL-specific HPO frameworks. The most popular way to tunehyperparameters in CL is to repeatedly train over the whole data stream withdifferent hyperparameter settings. However, this end-of-training HPO isunrealistic as in practice a learner can only see the stream once. Hence, thereis an open question: what HPO framework should a practitioner use for a CLproblem in reality? This paper answers this question by evaluating severalrealistic HPO frameworks. We find that all the HPO frameworks considered,including end-of-training HPO, perform similarly. We therefore advocate usingthe realistic and most computationally efficient method: fitting thehyperparameters on the first task and then fixing them throughout training.</description><author>Thomas L. Lee, Sigrid Passano Hellan, Linus Ericsson, Elliot J. Crowley, Amos Storkey</author><pubDate>Tue, 09 Apr 2024 18:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06466v1</guid></item><item><title>An Edit Friendly DDPM Noise Space: Inversion and Manipulations</title><link>http://arxiv.org/abs/2304.06140v3</link><description>Denoising diffusion probabilistic models (DDPMs) employ a sequence of whiteGaussian noise samples to generate an image. In analogy with GANs, those noisemaps could be considered as the latent code associated with the generatedimage. However, this native noise space does not possess a convenientstructure, and is thus challenging to work with in editing tasks. Here, wepropose an alternative latent noise space for DDPM that enables a wide range ofediting operations via simple means, and present an inversion method forextracting these edit-friendly noise maps for any given image (real orsynthetically generated). As opposed to the native DDPM noise space, theedit-friendly noise maps do not have a standard normal distribution and are notstatistically independent across timesteps. However, they allow perfectreconstruction of any desired image, and simple transformations on themtranslate into meaningful manipulations of the output image (e.g. shifting,color edits). Moreover, in text-conditional models, fixing those noise mapswhile changing the text prompt, modifies semantics while retaining structure.We illustrate how this property enables text-based editing of real images viathe diverse DDPM sampling scheme (in contrast to the popular non-diverse DDIMinversion). We also show how it can be used within existing diffusion-basedediting methods to improve their quality and diversity. Webpage:https://inbarhub.github.io/DDPM_inversion</description><author>Inbar Huberman-Spiegelglas, Vladimir Kulikov, Tomer Michaeli</author><pubDate>Tue, 09 Apr 2024 18:09:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06140v3</guid></item><item><title>A Neural Framework for Generalized Causal Sensitivity Analysis</title><link>http://arxiv.org/abs/2311.16026v2</link><description>Unobserved confounding is common in many applications, making causalinference from observational data challenging. As a remedy, causal sensitivityanalysis is an important tool to draw causal conclusions under unobservedconfounding with mathematical guarantees. In this paper, we propose NeuralCSA,a neural framework for generalized causal sensitivity analysis. Unlike previouswork, our framework is compatible with (i) a large class of sensitivity models,including the marginal sensitivity model, f-sensitivity models, and Rosenbaum'ssensitivity model; (ii) different treatment types (i.e., binary andcontinuous); and (iii) different causal queries, including (conditional)average treatment effects and simultaneous effects on multiple outcomes. Thegenerality of NeuralCSA is achieved by learning a latent distribution shiftthat corresponds to a treatment intervention using two conditional normalizingflows. We provide theoretical guarantees that NeuralCSA is able to infer validbounds on the causal query of interest and also demonstrate this empiricallyusing both simulated and real-world data.</description><author>Dennis Frauen, Fergus Imrie, Alicia Curth, Valentyn Melnychuk, Stefan Feuerriegel, Mihaela van der Schaar</author><pubDate>Tue, 09 Apr 2024 18:08:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16026v2</guid></item><item><title>Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models</title><link>http://arxiv.org/abs/2403.19521v2</link><description>In this paper, we deeply explore the mechanisms employed by Transformer-basedlanguage models in factual recall tasks. In zero-shot scenarios, given a promptlike "The capital of France is," task-specific attention heads extract thetopic entity, such as "France," from the context and pass it to subsequent MLPsto recall the required answer such as "Paris." We introduce a novel analysismethod aimed at decomposing the outputs of the MLP into componentsunderstandable by humans. Through this method, we quantify the function of theMLP layer following these task-specific heads. In the residual stream, iteither erases or amplifies the information originating from individual heads.Moreover, it generates a component that redirects the residual stream towardsthe direction of its expected answer. These zero-shot mechanisms are alsoemployed in few-shot scenarios. Additionally, we observed a widely existentanti-overconfidence mechanism in the final layer of models, which suppressescorrect predictions. We mitigate this suppression by leveraging ourinterpretation to improve factual recall performance. Our interpretations havebeen evaluated across various language models, from the GPT-2 families to 1.3BOPT, and across tasks covering different domains of factual knowledge.</description><author>Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, Rui Yan</author><pubDate>Tue, 09 Apr 2024 18:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19521v2</guid></item><item><title>Learning Locally Interacting Discrete Dynamical Systems: Towards Data-Efficient and Scalable Prediction</title><link>http://arxiv.org/abs/2404.06460v1</link><description>Locally interacting dynamical systems, such as epidemic spread, rumorpropagation through crowd, and forest fire, exhibit complex global dynamicsoriginated from local, relatively simple, and often stochastic interactionsbetween dynamic elements. Their temporal evolution is often driven bytransitions between a finite number of discrete states. Despite significantadvancements in predictive modeling through deep learning, such interactionsamong many elements have rarely explored as a specific domain for predictivemodeling. We present Attentive Recurrent Neural Cellular Automata (AR-NCA), toeffectively discover unknown local state transition rules by associating thetemporal information between neighboring cells in a permutation-invariantmanner. AR-NCA exhibits the superior generalizability across various systemconfigurations (i.e., spatial distribution of states), data efficiency androbustness in extremely data-limited scenarios even in the presence ofstochastic interactions, and scalability through spatial dimension-independentprediction.</description><author>Beomseok Kang, Harshit Kumar, Minah Lee, Biswadeep Chakraborty, Saibal Mukhopadhyay</author><pubDate>Tue, 09 Apr 2024 18:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06460v1</guid></item><item><title>A comparative analysis of deep learning models for lung segmentation on X-ray images</title><link>http://arxiv.org/abs/2404.06455v1</link><description>Robust and highly accurate lung segmentation in X-rays is crucial in medicalimaging. This study evaluates deep learning solutions for this task, rankingexisting methods and analyzing their performance under diverse imagemodifications. Out of 61 analyzed papers, only nine offered implementation orpre-trained models, enabling assessment of three prominent methods: Lung VAE,TransResUNet, and CE-Net. The analysis revealed that CE-Net performs best,demonstrating the highest values in dice similarity coefficient andintersection over union metric.</description><author>Weronika Hryniewska-Guzik, Jakub Bilski, Bartosz Chrostowski, Jakub Drak Sbahi, Przemysław Biecek</author><pubDate>Tue, 09 Apr 2024 17:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06455v1</guid></item><item><title>PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits</title><link>http://arxiv.org/abs/2404.06453v1</link><description>The field of mechanistic interpretability aims to study the role ofindividual neurons in Deep Neural Networks. Single neurons, however, have thecapability to act polysemantically and encode for multiple (unrelated)features, which renders their interpretation difficult. We present a method fordisentangling polysemanticity of any Deep Neural Network by decomposing apolysemantic neuron into multiple monosemantic "virtual" neurons. This isachieved by identifying the relevant sub-graph ("circuit") for each "pure"feature. We demonstrate how our approach allows us to find and disentanglevarious polysemantic units of ResNet models trained on ImageNet. Whileevaluating feature visualizations using CLIP, our method effectivelydisentangles representations, improving upon methods based on neuronactivations. Our code is available at https://github.com/maxdreyer/PURE.</description><author>Maximilian Dreyer, Erblina Purelku, Johanna Vielhaben, Wojciech Samek, Sebastian Lapuschkin</author><pubDate>Tue, 09 Apr 2024 17:54:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06453v1</guid></item><item><title>SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions</title><link>http://arxiv.org/abs/2404.06451v1</link><description>Human visual imagination usually begins with analogies or rough sketches. Forexample, given an image with a girl playing guitar before a building, one mayanalogously imagine how it seems like if Iron Man playing guitar before Pyramidin Egypt. Nonetheless, visual condition may not be precisely aligned with theimaginary result indicated by text prompt, and existing layout-controllabletext-to-image (T2I) generation models is prone to producing degraded generatedresults with obvious artifacts. To address this issue, we present a novel T2Igeneration method dubbed SmartControl, which is designed to modify the roughvisual conditions for adapting to text prompt. The key idea of our SmartControlis to relax the visual condition on the areas that are conflicted with textprompts. In specific, a Control Scale Predictor (CSP) is designed to identifythe conflict regions and predict the local control scales, while a dataset withtext prompts and rough visual conditions is constructed for training CSP. It isworth noting that, even with a limited number (e.g., 1,000~2,000) of trainingsamples, our SmartControl can generalize well to unseen objects. Extensiveexperiments on four typical visual condition types clearly show the efficacy ofour SmartControl against state-of-the-arts. Source code, pre-trained models,and datasets are available at https://github.com/liuxiaoyu1104/SmartControl.</description><author>Xiaoyu Liu, Yuxiang Wei, Ming Liu, Xianhui Lin, Peiran Ren, Xuansong Xie, Wangmeng Zuo</author><pubDate>Tue, 09 Apr 2024 17:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06451v1</guid></item><item><title>Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models</title><link>http://arxiv.org/abs/2404.06448v1</link><description>Recently, there has been a surge in the development of advanced intelligentgenerative content (AIGC), especially large language models (LLMs). However,for many downstream tasks, it is necessary to fine-tune LLMs using privatedata. While federated learning offers a promising privacy-preserving solutionto LLM fine-tuning, the substantial size of an LLM, combined with highcomputational and communication demands, makes it hard to apply to downstreamtasks. More importantly, private edge servers often possess varying computingand network resources in real-world scenarios, introducing additionalcomplexities to LLM fine-tuning. To tackle these problems, we design andimplement an automated federated pipeline, named FedPipe, to fine-tune LLMswith minimal training cost but without adding any inference latency. FedPipefirstly identifies the weights to be fine-tuned based on their contributions tothe LLM training. It then configures a low-rank adapter for each selectedweight to train local low-rank adapters on an edge server, and aggregate localadapters of all edge servers to fine-tune the whole LLM. Finally, itappropriately quantizes the parameters of LLM to reduce memory space accordingto the requirements of edge servers. Extensive experiments demonstrate thatFedPipe expedites the model training and achieves higher accuracy thanstate-of-the-art benchmarks.</description><author>Zihan Fang, Zheng Lin, Zhe Chen, Xianhao Chen, Yue Gao, Yuguang Fang</author><pubDate>Tue, 09 Apr 2024 17:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06448v1</guid></item><item><title>The Central Spanning Tree Problem</title><link>http://arxiv.org/abs/2404.06447v1</link><description>Spanning trees are an important primitive in many data analysis tasks, when adata set needs to be summarized in terms of its "skeleton", or when atree-shaped graph over all observations is required for downstream processing.Popular definitions of spanning trees include the minimum spanning tree and theoptimum distance spanning tree, a.k.a. the minimum routing cost tree. Whensearching for the shortest spanning tree but admitting additional branchingpoints, even shorter spanning trees can be realized: Steiner trees.Unfortunately, both minimum spanning and Steiner trees are not robust withrespect to noise in the observations; that is, small perturbations of theoriginal data set often lead to drastic changes in the associated spanningtrees. In response, we make two contributions when the data lies in a Euclideanspace: on the theoretical side, we introduce a new optimization problem, the"(branched) central spanning tree", which subsumes all previously mentioneddefinitions as special cases. On the practical side, we show empirically thatthe (branched) central spanning tree is more robust to noise in the data, andas such is better suited to summarize a data set in terms of its skeleton. Wealso propose a heuristic to address the NP-hard optimization problem, andillustrate its use on single cell RNA expression data from biology and 3D pointclouds of plants.</description><author>Enrique Fita Sanmartín, Christoph Schnörr, Fred A. Hamprecht</author><pubDate>Tue, 09 Apr 2024 17:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06447v1</guid></item><item><title>Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition</title><link>http://arxiv.org/abs/2404.06443v1</link><description>Human facial action units (AUs) are mutually related in a hierarchicalmanner, as not only they are associated with each other in both spatial andtemporal domains but also AUs located in the same/close facial regions showstronger relationships than those of different facial regions. While none ofexisting approach thoroughly model such hierarchical inter-dependencies amongAUs, this paper proposes to comprehensively model multi-scale AU-relateddynamic and hierarchical spatio-temporal relationship among AUs for theiroccurrences recognition. Specifically, we first propose a novel multi-scaletemporal differencing network with an adaptive weighting block to explicitlycapture facial dynamics across frames at different spatial scales, whichspecifically considers the heterogeneity of range and magnitude in differentAUs' activation. Then, a two-stage strategy is introduced to hierarchicallymodel the relationship among AUs based on their spatial distribution (i.e.,local and cross-region AU relationship modelling). Experimental resultsachieved on BP4D and DISFA show that our approach is the new state-of-the-artin the field of AU occurrence recognition. Our code is publicly available athttps://github.com/CVI-SZU/MDHR.</description><author>Zihan Wang, Siyang Song, Cheng Luo, Songhe Deng, Weicheng Xie, Linlin Shen</author><pubDate>Tue, 09 Apr 2024 17:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06443v1</guid></item><item><title>QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding</title><link>http://arxiv.org/abs/2404.06442v1</link><description>Understanding the structural organisation of 3D indoor scenes in terms ofrooms is often accomplished via floorplan extraction. Robotic tasks such asplanning and navigation require a semantic understanding of the scene as well.This is typically achieved via object-level semantic segmentation. However,such methods struggle to segment out topological regions like "kitchen" in thescene. In this work, we introduce a two-step pipeline. First, we extract atopological map, i.e., floorplan of the indoor scene using a novelmulti-channel occupancy representation. Then, we generate CLIP-aligned featuresand semantic labels for every room instance based on the objects it containsusing a self-attention transformer. Our language-topology alignment supportsnatural language querying, e.g., a "place to cook" locates the "kitchen". Weoutperform the current state-of-the-art on room segmentation by ~20% and roomclassification by ~12%. Our detailed qualitative analysis and ablation studiesprovide insights into the problem of joint structural and semantic 3D sceneunderstanding.</description><author>Yash Mehan, Kumaraditya Gupta, Rohit Jayanti, Anirudh Govil, Sourav Garg, Madhava Krishna</author><pubDate>Tue, 09 Apr 2024 17:42:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06442v1</guid></item><item><title>Event Data Association via Robust Model Fitting for Event-based Object Tracking</title><link>http://arxiv.org/abs/2110.12962v2</link><description>Event-based approaches, which are based on bio-inspired asynchronous eventcameras, have achieved promising performance on various computer vision tasks.However, the study of the fundamental event data association problem is stillin its infancy. In this paper, we propose a novel Event Data Association(called EDA) approach to explicitly address the event association and fusionproblem. The proposed EDA seeks for event trajectories that best fit the eventdata, in order to perform unifying data association and information fusion. InEDA, we first asynchronously fuse the event data based on its informationentropy. Then, we introduce a deterministic model hypothesis generationstrategy, which effectively generates model hypotheses from the fused events,to represent the corresponding event trajectories. After that, we present atwo-stage weighting algorithm, which robustly weighs and selects true modelsfrom the generated model hypotheses, through multi-structural geometric modelfitting. Meanwhile, we also propose an adaptive model selection strategy toautomatically determine the number of the true models. Finally, we use theselected true models to associate and fuse the event data, without beingaffected by sensor noise and irrelevant structures. We evaluate the performanceof the proposed EDA on the object tracking task. The experimental results showthe effectiveness of EDA under challenging scenarios, such as high speed,motion blur, and high dynamic range conditions.</description><author>Haosheng Chen, Shuyuan Lin, Yan Yan, Hanzi Wang, Xinbo Gao</author><pubDate>Tue, 09 Apr 2024 17:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.12962v2</guid></item><item><title>Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning</title><link>http://arxiv.org/abs/2403.06108v2</link><description>This paper delves into enhancing the classification performance on theGoEmotions dataset, a large, manually annotated dataset for emotion detectionin text. The primary goal of this paper is to address the challenges ofdetecting subtle emotions in text, a complex issue in Natural LanguageProcessing (NLP) with significant practical applications. The findings offervaluable insights into addressing the challenges of emotion detection in textand suggest directions for future research, including the potential for asurvey paper that synthesizes methods and performances across various datasetsin this domain.</description><author>Kaipeng Wang, Zhi Jing, Yongye Su, Yikun Han</author><pubDate>Tue, 09 Apr 2024 17:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06108v2</guid></item><item><title>A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement</title><link>http://arxiv.org/abs/2403.02408v2</link><description>Distortions caused by low-light conditions are not only visually unpleasantbut also degrade the performance of computer vision tasks. The restoration andenhancement have proven to be highly beneficial. However, there are only alimited number of enhancement methods explicitly designed for videos acquiredin low-light conditions. We propose a Spatio-Temporal Aligned SUNet (STA-SUNet)model using a Swin Transformer as a backbone to capture low light videofeatures and exploit their spatio-temporal correlations. The STA-SUNet model istrained on a novel, fully registered dataset (BVI), which comprises dynamicscenes captured under varying light conditions. It is further analysedcomparatively against various other models over three test datasets. The modeldemonstrates superior adaptivity across all datasets, obtaining the highestPSNR and SSIM values. It is particularly effective in extreme low-lightconditions, yielding fairly good visualisation results.</description><author>Ruirui Lin, Nantheera Anantrasirichai, Alexandra Malyugina, David Bull</author><pubDate>Tue, 09 Apr 2024 17:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02408v2</guid></item><item><title>EPR-Net: Constructing non-equilibrium potential landscape via a variational force projection formulation</title><link>http://arxiv.org/abs/2301.01946v3</link><description>We present EPR-Net, a novel and effective deep learning approach that tacklesa crucial challenge in biophysics: constructing potential landscapes forhigh-dimensional non-equilibrium steady-state (NESS) systems. EPR-Net leveragesa nice mathematical fact that the desired negative potential gradient is simplythe orthogonal projection of the driving force of the underlying dynamics in aweighted inner-product space. Remarkably, our loss function has an intimateconnection with the steady entropy production rate (EPR), enabling simultaneouslandscape construction and EPR estimation. We introduce an enhanced learningstrategy for systems with small noise, and extend our framework to includedimensionality reduction and state-dependent diffusion coefficient case in aunified fashion. Comparative evaluations on benchmark problems demonstrate thesuperior accuracy, effectiveness, and robustness of EPR-Net compared toexisting methods. We apply our approach to challenging biophysical problems,such as an 8D limit cycle and a 52D multi-stability problem, which provideaccurate solutions and interesting insights on constructed landscapes. With itsversatility and power, EPR-Net offers a promising solution for diverselandscape construction problems in biophysics.</description><author>Yue Zhao, Wei Zhang, Tiejun Li</author><pubDate>Tue, 09 Apr 2024 17:34:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.01946v3</guid></item><item><title>EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management</title><link>http://arxiv.org/abs/2404.02361v2</link><description>This paper investigates the increasing roles of Renewable Energy Sources(RES) and Electric Vehicles (EVs). While indicating a new era of sustainableenergy, these also introduce complex challenges, including the need to balancesupply and demand and smooth peak consumptions amidst rising EV adoption rates.Addressing these challenges requires innovative solutions such as DemandResponse (DR), energy flexibility management, Renewable Energy Communities(RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existingV2G approaches often fall short in real-world adaptability, global RECoptimization with other flexible assets, scalability, and user engagement. Tobridge this gap, this paper introduces EnergAIze, a Multi-Agent ReinforcementLearning (MARL) energy management framework, leveraging the Multi-Agent DeepDeterministic Policy Gradient (MADDPG) algorithm. EnergAIze enablesuser-centric and multi-objective energy management by allowing each prosumer toselect from a range of personal management objectives, thus encouragingengagement. Additionally, it architects' data protection and ownership throughdecentralized computing, where each prosumer can situate an energy managementoptimization node directly at their own dwelling. The local node not onlymanages local energy assets but also fosters REC wide optimization. Theefficacy of EnergAIze was evaluated through case studies employing theCityLearn simulation framework. These simulations were instrumental indemonstrating EnergAIze's adeptness at implementing V2G technology within a RECand other energy assets. The results show reduction in peak loads, ramping,carbon emissions, and electricity costs at the REC level while optimizing forindividual prosumers objectives.</description><author>Tiago Fonseca, Luis Ferreira, Bernardo Cabral, Ricardo Severino, Isabel Praca</author><pubDate>Tue, 09 Apr 2024 17:32:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02361v2</guid></item><item><title>DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models</title><link>http://arxiv.org/abs/2307.03108v3</link><description>Recent text-to-image diffusion models have shown surprising performance ingenerating high-quality images. However, concerns have arisen regarding theunauthorized data usage during the training or fine-tuning process. One exampleis when a model trainer collects a set of images created by a particular artistand attempts to train a model capable of generating similar images withoutobtaining permission and giving credit to the artist. To address this issue, wepropose a method for detecting such unauthorized data usage by planting theinjected memorization into the text-to-image diffusion models trained on theprotected dataset. Specifically, we modify the protected images by addingunique contents on these images using stealthy image warping functions that arenearly imperceptible to humans but can be captured and memorized by diffusionmodels. By analyzing whether the model has memorized the injected content(i.e., whether the generated images are processed by the injectedpost-processing function), we can detect models that had illegally utilized theunauthorized data. Experiments on Stable Diffusion and VQ Diffusion withdifferent model training or fine-tuning methods (i.e, LoRA, DreamBooth, andstandard training) demonstrate the effectiveness of our proposed method indetecting unauthorized data usages. Code:https://github.com/ZhentingWang/DIAGNOSIS.</description><author>Zhenting Wang, Chen Chen, Lingjuan Lyu, Dimitris N. Metaxas, Shiqing Ma</author><pubDate>Tue, 09 Apr 2024 17:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03108v3</guid></item><item><title>Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks</title><link>http://arxiv.org/abs/2404.06437v1</link><description>With climate change expected to exacerbate fire weather conditions, theaccurate anticipation of wildfires on a global scale becomes increasinglycrucial for disaster mitigation. In this study, we utilize SeasFire, acomprehensive global wildfire dataset with climate, vegetation, oceanicindices, and human-related variables, to enable seasonal wildfire forecastingwith machine learning. For the predictive analysis, we train deep learningmodels with different architectures that capture the spatio-temporal contextleading to wildfires. Our investigation focuses on assessing the effectivenessof these models in predicting the presence of burned areas at varyingforecasting time horizons globally, extending up to six months into the future,and on how different spatial or/and temporal context affects the performance ofthe models. Our findings demonstrate the great potential of deep learningmodels in seasonal fire forecasting; longer input time-series leads to morerobust predictions across varying forecasting horizons, while integratingspatial information to capture wildfire spatio-temporal dynamics boostsperformance. Finally, our results hint that in order to enhance performance atlonger forecasting horizons, a larger receptive field spatially needs to beconsidered.</description><author>Dimitrios Michail, Lefki-Ioanna Panagiotou, Charalampos Davalas, Ioannis Prapas, Spyros Kondylatos, Nikolaos Ioannis Bountos, Ioannis Papoutsis</author><pubDate>Tue, 09 Apr 2024 17:28:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06437v1</guid></item><item><title>pfl-research: simulation framework for accelerating research in Private Federated Learning</title><link>http://arxiv.org/abs/2404.06430v1</link><description>Federated learning (FL) is an emerging machine learning (ML) trainingparadigm where clients own their data and collaborate to train a global model,without revealing any data to the server and other participants. Researcherscommonly perform experiments in a simulation environment to quickly iterate onideas. However, existing open-source tools do not offer the efficiency requiredto simulate FL on larger and more realistic FL datasets. We introducepfl-research, a fast, modular, and easy-to-use Python framework for simulatingFL. It supports TensorFlow, PyTorch, and non-neural network models, and istightly integrated with state-of-the-art privacy algorithms. We study the speedof open-source FL frameworks and show that pfl-research is 7-72$\times$ fasterthan alternative open-source frameworks on common cross-device setups. Suchspeedup will significantly boost the productivity of the FL research communityand enable testing hypotheses on realistic FL datasets that were previously tooresource intensive. We release a suite of benchmarks that evaluates analgorithm's overall performance on a diverse set of realistic scenarios. Thecode is available on GitHub at https://github.com/apple/pfl-research.</description><author>Filip Granqvist, Congzheng Song, Áine Cahill, Rogier van Dalen, Martin Pelikan, Yi Sheng Chan, Xiaojun Feng, Natarajan Krishnaswami, Vojta Jina, Mona Chitnis</author><pubDate>Tue, 09 Apr 2024 17:23:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06430v1</guid></item><item><title>Towards generalizing deep-audio fake detection networks</title><link>http://arxiv.org/abs/2305.13033v3</link><description>Today's generative neural networks allow the creation of high-qualitysynthetic speech at scale. While we welcome the creative use of this newtechnology, we must also recognize the risks. As synthetic speech is abused formonetary and identity theft, we require a broad set of deepfake identificationtools. Furthermore, previous work reported a limited ability of deepclassifiers to generalize to unseen audio generators. We study the frequencydomain fingerprints of current audio generators. Building on top of thediscovered frequency footprints, we train excellent lightweight detectors thatgeneralize. We report improved results on the WaveFake dataset and an extendedversion. To account for the rapid progress in the field, we extend the WaveFakedataset by additionally considering samples drawn from the novel Avocodo andBigVGAN networks. For illustration purposes, the supplementary materialcontains audio samples of generator artifacts.</description><author>Konstantin Gasenzer, Moritz Wolter</author><pubDate>Tue, 09 Apr 2024 17:22:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13033v3</guid></item><item><title>Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion</title><link>http://arxiv.org/abs/2404.06429v1</link><description>Benefiting from the rapid development of 2D diffusion models, 3D contentcreation has made significant progress recently. One promising solutioninvolves the fine-tuning of pre-trained 2D diffusion models to harness theircapacity for producing multi-view images, which are then lifted into accurate3D models via methods like fast-NeRFs or large reconstruction models. However,as inconsistency still exists and limited generated resolution, the generationresults of such methods still lack intricate textures and complex geometries.To solve this problem, we propose Magic-Boost, a multi-view conditioneddiffusion model that significantly refines coarse generative results through abrief period of SDS optimization ($\sim15$min). Compared to the previous textor single image based diffusion models, Magic-Boost exhibits a robustcapability to generate images with high consistency from pseudo synthesizedmulti-view images. It provides precise SDS guidance that well aligns with theidentity of the input images, enriching the local detail in both geometry andtexture of the initial generative results. Extensive experiments showMagic-Boost greatly enhances the coarse inputs and generates high-quality 3Dassets with rich geometric and textural details. (Project Page:https://magic-research.github.io/magic-boost/)</description><author>Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Jiashi Feng, Guosheng Lin</author><pubDate>Tue, 09 Apr 2024 17:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06429v1</guid></item><item><title>Flexible Fairness Learning via Inverse Conditional Permutation</title><link>http://arxiv.org/abs/2404.05678v2</link><description>Equalized odds, as a popular notion of algorithmic fairness, aims to ensurethat sensitive variables, such as race and gender, do not unfairly influencethe algorithm prediction when conditioning on the true outcome. Despite rapidadvancements, most of the current research focuses on the violation ofequalized odds caused by one sensitive attribute, leaving the challenge ofsimultaneously accounting for multiple attributes under-addressed. We addressthis gap by introducing a fairness learning approach that integratesadversarial learning with a novel inverse conditional permutation. Thisapproach effectively and flexibly handles multiple sensitive attributes,potentially of mixed data types. The efficacy and flexibility of our method aredemonstrated through both simulation studies and empirical analysis ofreal-world datasets.</description><author>Yuheng Lai, Leying Guan</author><pubDate>Tue, 09 Apr 2024 17:17:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05678v2</guid></item><item><title>ZeST: Zero-Shot Material Transfer from a Single Image</title><link>http://arxiv.org/abs/2404.06425v1</link><description>We propose ZeST, a method for zero-shot material transfer to an object in theinput image given a material exemplar image. ZeST leverages existing diffusionadapters to extract implicit material representation from the exemplar image.This representation is used to transfer the material using pre-trainedinpainting diffusion model on the object in the input image using depthestimates as geometry cue and grayscale object shading as illumination cues.The method works on real images without any training resulting a zero-shotapproach. Both qualitative and quantitative results on real and syntheticdatasets demonstrate that ZeST outputs photorealistic images with transferredmaterials. We also show the application of ZeST to perform multiple edits androbust material assignment under different illuminations. Project Page:https://ttchengab.github.io/zest</description><author>Ta-Ying Cheng, Prafull Sharma, Andrew Markham, Niki Trigoni, Varun Jampani</author><pubDate>Tue, 09 Apr 2024 17:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06425v1</guid></item><item><title>Deep Reinforcement Learning-Based Approach for a Single Vehicle Persistent Surveillance Problem with Fuel Constraints</title><link>http://arxiv.org/abs/2404.06423v1</link><description>This article presents a deep reinforcement learning-based approach to tacklea persistent surveillance mission requiring a single unmanned aerial vehicleinitially stationed at a depot with fuel or time-of-flight constraints torepeatedly visit a set of targets with equal priority. Owing to the vehicle'sfuel or time-of-flight constraints, the vehicle must be regularly refueled, orits battery must be recharged at the depot. The objective of the problem is todetermine an optimal sequence of visits to the targets that minimizes themaximum time elapsed between successive visits to any target while ensuringthat the vehicle never runs out of fuel or charge. We present a deepreinforcement learning algorithm to solve this problem and present the resultsof numerical experiments that corroborate the effectiveness of this approach incomparison with common-sense greedy heuristics.</description><author>Hritik Bana, Manav Mishra, Saswata Sarkar, Sujeevraja Sanjeevi, Sujit PB, Kaarthik Sundar</author><pubDate>Tue, 09 Apr 2024 17:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06423v1</guid></item><item><title>Misspecification uncertainties in near-deterministic regression</title><link>http://arxiv.org/abs/2402.01810v2</link><description>The expected loss is an upper bound to the model generalization error whichadmits robust PAC-Bayes bounds for learning. However, loss minimization isknown to ignore misspecification, where models cannot exactly reproduceobservations. This leads to significant underestimates of parameteruncertainties in the large data, or underparameterized, limit. We analyze thegeneralization error of near-deterministic, misspecified and underparametrizedsurrogate models, a regime of broad relevance in science and engineering. Weshow posterior distributions must cover every training point to avoid adivergent generalization error and derive an ensemble \textit{ansatz} thatrespects this constraint, which for linear models incurs minimal overhead. Theefficient approach is demonstrated on model problems before application to highdimensional datasets in atomistic machine learning. Parameter uncertaintiesfrom misspecification survive in the underparametrized limit, giving accurateprediction and bounding of test errors.</description><author>Thomas D Swinburne, Danny Perez</author><pubDate>Tue, 09 Apr 2024 17:11:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01810v2</guid></item><item><title>Bayesian Survival Analysis by Approximate Inference of Neural Networks</title><link>http://arxiv.org/abs/2404.06421v1</link><description>Predicting future events always comes with uncertainty, but traditionalnon-Bayesian methods cannot distinguish certain from uncertain predictions orexplain the confidence in their predictions. In survival analysis, Bayesianmethods applied to state-of-the-art solutions in the healthcare and biomedicalfield are still novel, and their implications have not been fully evaluated. Inthis paper, we study the benefits of modeling uncertainty in deep neuralnetworks for survival analysis with a focus on prediction and calibrationperformance. For this, we present a Bayesian deep learning framework thatconsists of three Bayesian network architectures, which we train by optimizingthe Cox partial likelihood and combining input-dependent aleatoric uncertaintywith model-specific epistemic uncertainty. This enables us to provideuncertainty estimates as credible intervals when predicting the survival curveor as a probability density function over the predicted median survival times.For our empirical analyses, we evaluated our proposed method on four benchmarkdatasets and found that our method demonstrates prediction performancecomparable to the state-of-the-art based on the concordance index andoutperforms all other Cox-based approaches in terms of the mean absolute error.Our work explicitly compares the extent to which different Bayesianapproximation techniques differ from each other and improves the predictionover traditional non-Bayesian alternatives.</description><author>Christian Marius Lillelund, Martin Magris, Christian Fischer Pedersen</author><pubDate>Tue, 09 Apr 2024 17:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06421v1</guid></item><item><title>Studying the Impact of Latent Representations in Implicit Neural Networks for Scientific Continuous Field Reconstruction</title><link>http://arxiv.org/abs/2404.06418v1</link><description>Learning a continuous and reliable representation of physical fields fromsparse sampling is challenging and it affects diverse scientific disciplines.In a recent work, we present a novel model called MMGN (Multiplicative andModulated Gabor Network) with implicit neural networks. In this work, we designadditional studies leveraging explainability methods to complement the previousexperiments and further enhance the understanding of latent representationsgenerated by the model. The adopted methods are general enough to be leveragedfor any latent space inspection. Preliminary results demonstrate the contextualinformation incorporated in the latent representations and their impact on themodel performance. As a work in progress, we will continue to verify ourfindings and develop novel explainability approaches.</description><author>Wei Xu, Derek Freeman DeSantis, Xihaier Luo, Avish Parmar, Klaus Tan, Balu Nadiga, Yihui Ren, Shinjae Yoo</author><pubDate>Tue, 09 Apr 2024 17:07:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06418v1</guid></item><item><title>Bias Amplification Enhances Minority Group Performance</title><link>http://arxiv.org/abs/2309.06717v2</link><description>Neural networks produced by standard training are known to suffer from pooraccuracy on rare subgroups despite achieving high accuracy on average, due tothe correlations between certain spurious features and labels. Previousapproaches based on worst-group loss minimization (e.g. Group-DRO) areeffective in improving worse-group accuracy but require expensive groupannotations for all the training samples. In this paper, we focus on the morechallenging and realistic setting where group annotations are only available ona small validation set or are not available at all. We propose BAM, a noveltwo-stage training algorithm: in the first stage, the model is trained using abias amplification scheme via introducing a learnable auxiliary variable foreach training sample; in the second stage, we upweight the samples that thebias-amplified model misclassifies, and then continue training the same modelon the reweighted dataset. Empirically, BAM achieves competitive performancecompared with existing methods evaluated on spurious correlation benchmarks incomputer vision and natural language processing. Moreover, we find a simplestopping criterion based on minimum class accuracy difference that can removethe need for group annotations, with little or no loss in worst-group accuracy.We perform extensive analyses and ablations to verify the effectiveness androbustness of our algorithm in varying class and group imbalance ratios.</description><author>Gaotang Li, Jiarui Liu, Wei Hu</author><pubDate>Tue, 09 Apr 2024 17:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06717v2</guid></item><item><title>Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems</title><link>http://arxiv.org/abs/2404.06413v1</link><description>Multi-agent robotic systems are prone to deadlocks in an obstacle environmentwhere the system can get stuck away from its desired location under a smoothlow-level control policy. Without an external intervention, often in terms of ahigh-level command, it is not possible to guarantee that just a low-levelcontrol policy can resolve such deadlocks. Utilizing the generalizability andlow data requirements of large language models (LLMs), this paper explores thepossibility of using LLMs for deadlock resolution. We propose a hierarchicalcontrol framework where an LLM resolves deadlocks by assigning a leader anddirection for the leader to move along. A graph neural network (GNN) basedlow-level distributed control policy executes the assigned plan. Wesystematically study various prompting techniques to improve LLM's performancein resolving deadlocks. In particular, as part of prompt engineering, weprovide in-context examples for LLMs. We conducted extensive experiments onvarious multi-robot environments with up to 15 agents and 40 obstacles. Ourresults demonstrate that LLM-based high-level planners are effective inresolving deadlocks in MRS.</description><author>Kunal Garg, Jacob Arkin, Songyuan Zhang, Nicholas Roy, Chuchu Fan</author><pubDate>Tue, 09 Apr 2024 17:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06413v1</guid></item><item><title>AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents</title><link>http://arxiv.org/abs/2404.06411v1</link><description>The advances made by Large Language Models (LLMs) have led to the pursuit ofLLM agents that can solve intricate, multi-step reasoning tasks. As with anyresearch pursuit, benchmarking and evaluation are key corner stones toefficient and reliable progress. However, existing benchmarks are often narrowand simply compute overall task success. To face these issues, we proposeAgentQuest -- a framework where (i) both benchmarks and metrics are modular andeasily extensible through well documented and easy-to-use APIs; (ii) we offertwo new evaluation metrics that can reliably track LLM agent progress whilesolving a task. We exemplify the utility of the metrics on two use caseswherein we identify common failure points and refine the agent architecture toobtain a significant performance increase. Together with the researchcommunity, we hope to extend AgentQuest further and therefore we make itavailable under https://github.com/nec-research/agentquest.</description><author>Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril Gashteovski, David Friede, Roberto Bifulco, Carolin Lawrence</author><pubDate>Tue, 09 Apr 2024 17:01:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06411v1</guid></item><item><title>Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak</title><link>http://arxiv.org/abs/2404.06407v1</link><description>Large language models (LLMs) have become increasingly integrated with variousapplications. To ensure that LLMs do not generate unsafe responses, they arealigned with safeguards that specify what content is restricted. However, suchalignment can be bypassed to produce prohibited content using a techniquecommonly referred to as jailbreak. Different systems have been proposed toperform the jailbreak automatically. These systems rely on evaluation methodsto determine whether a jailbreak attempt is successful. However, our analysisreveals that current jailbreak evaluation methods have two limitations. (1)Their objectives lack clarity and do not align with the goal of identifyingunsafe responses. (2) They oversimplify the jailbreak result as a binaryoutcome, successful or not. In this paper, we propose three metrics, safeguard violation,informativeness, and relative truthfulness, to evaluate language modeljailbreak. Additionally, we demonstrate how these metrics correlate with thegoal of different malicious actors. To compute these metrics, we introduce amultifaceted approach that extends the natural language generation evaluationmethod after preprocessing the response. We evaluate our metrics on a benchmarkdataset produced from three malicious intent datasets and three jailbreaksystems. The benchmark dataset is labeled by three annotators. We compare ourmultifaceted approach with three existing jailbreak evaluation methods.Experiments demonstrate that our multifaceted evaluation outperforms existingmethods, with F1 scores improving on average by 17% compared to existingbaselines. Our findings motivate the need to move away from the binary view ofthe jailbreak problem and incorporate a more comprehensive evaluation to ensurethe safety of the language model.</description><author>Hongyu Cai, Arjun Arunasalam, Leo Y. Lin, Antonio Bianchi, Z. Berkay Celik</author><pubDate>Tue, 09 Apr 2024 16:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06407v1</guid></item><item><title>Emergent Dynamics in Neural Cellular Automata</title><link>http://arxiv.org/abs/2404.06406v1</link><description>Neural Cellular Automata (NCA) models are trainable variations of traditionalCellular Automata (CA). Emergent motion in the patterns created by NCA has beensuccessfully applied to synthesize dynamic textures. However, the conditionsrequired for an NCA to display dynamic patterns remain unexplored. Here, weinvestigate the relationship between the NCA architecture and the emergentdynamics of the trained models. Specifically, we vary the number of channels inthe cell state and the number of hidden neurons in the MultiLayer Perceptron(MLP), and draw a relationship between the combination of these two variablesand the motion strength between successive frames. Our analysis reveals thatthe disparity and proportionality between these two variables have a strongcorrelation with the emergent dynamics in the NCA output. We thus propose adesign principle for creating dynamic NCA.</description><author>Yitao Xu, Ehsan Pajouheshgar, Sabine Süsstrunk</author><pubDate>Tue, 09 Apr 2024 16:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06406v1</guid></item><item><title>Wu's Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry</title><link>http://arxiv.org/abs/2404.06405v1</link><description>Proving geometric theorems constitutes a hallmark of visual reasoningcombining both intuitive and logical skills. Therefore, automated theoremproving of Olympiad-level geometry problems is considered a notable milestonein human-level automated reasoning. The introduction of AlphaGeometry, aneuro-symbolic model trained with 100 million synthetic samples, marked a majorbreakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO)problems whereas the reported baseline based on Wu's method solved only ten. Inthis note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry,and find that Wu's method is surprisingly strong. Wu's method alone can solve15 problems, and some of them are not solved by any of the other methods. Thisleads to two key findings: (i) Combining Wu's method with the classic syntheticmethods of deductive databases and angle, ratio, and distance chasing solves 21out of 30 methods by just using a CPU-only laptop with a time limit of 5minutes per problem. Essentially, this classic method solves just 4 problemsless than AlphaGeometry and establishes the first fully symbolic baselinestrong enough to rival the performance of an IMO silver medalist. (ii) Wu'smethod even solves 2 of the 5 problems that AlphaGeometry failed to solve.Thus, by combining AlphaGeometry with Wu's method we set a new state-of-the-artfor automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, thefirst AI method which outperforms an IMO gold medalist.</description><author>Shiven Sinha, Ameya Prabhu, Ponnurangam Kumaraguru, Siddharth Bhat, Matthias Bethge</author><pubDate>Tue, 09 Apr 2024 16:54:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06405v1</guid></item><item><title>Apprentices to Research Assistants: Advancing Research with Large Language Models</title><link>http://arxiv.org/abs/2404.06404v1</link><description>Large Language Models (LLMs) have emerged as powerful tools in variousresearch domains. This article examines their potential through a literaturereview and firsthand experimentation. While LLMs offer benefits likecost-effectiveness and efficiency, challenges such as prompt tuning, biases,and subjectivity must be addressed. The study presents insights fromexperiments utilizing LLMs for qualitative analysis, highlighting successes andlimitations. Additionally, it discusses strategies for mitigating challenges,such as prompt optimization techniques and leveraging human expertise. Thisstudy aligns with the 'LLMs as Research Tools' workshop's focus on integratingLLMs into HCI data work critically and ethically. By addressing bothopportunities and challenges, our work contributes to the ongoing dialogue ontheir responsible application in research.</description><author>M. Namvarpour, A. Razi</author><pubDate>Tue, 09 Apr 2024 16:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06404v1</guid></item><item><title>Online Learning of Decision Trees with Thompson Sampling</title><link>http://arxiv.org/abs/2404.06403v1</link><description>Decision Trees are prominent prediction models for interpretable MachineLearning. They have been thoroughly researched, mostly in the batch settingwith a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3and CART. Unfortunately, these methods are of heuristic nature, they rely ongreedy splits offering no guarantees of global optimality and often leading tounnecessarily complex and hard-to-interpret Decision Trees. Recentbreakthroughs addressed this suboptimality issue in the batch setting, but nosuch work has considered the online setting with data arriving in a stream. Tothis end, we devise a new Monte Carlo Tree Search algorithm, Thompson SamplingDecision Trees (TSDT), able to produce optimal Decision Trees in an onlinesetting. We analyse our algorithm and prove its almost sure convergence to theoptimal tree. Furthermore, we conduct extensive experiments to validate ourfindings empirically. The proposed TSDT outperforms existing algorithms onseveral benchmarks, all while presenting the practical advantage of beingtailored to the online setting.</description><author>Ayman Chaouki, Jesse Read, Albert Bifet</author><pubDate>Tue, 09 Apr 2024 16:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06403v1</guid></item><item><title>DiffusionLight: Light Probes for Free by Painting a Chrome Ball</title><link>http://arxiv.org/abs/2312.09168v3</link><description>We present a simple yet effective technique to estimate lighting in a singleinput image. Current techniques rely heavily on HDR panorama datasets to trainneural networks to regress an input with limited field-of-view to a fullenvironment map. However, these approaches often struggle with real-world,uncontrolled settings due to the limited diversity and size of their datasets.To address this problem, we leverage diffusion models trained on billions ofstandard images to render a chrome ball into the input image. Despite itssimplicity, this task remains challenging: the diffusion models often insertincorrect or inconsistent objects and cannot readily generate images in HDRformat. Our research uncovers a surprising relationship between the appearanceof chrome balls and the initial diffusion noise map, which we utilize toconsistently generate high-quality chrome balls. We further fine-tune an LDRdiffusion model (Stable Diffusion XL) with LoRA, enabling it to performexposure bracketing for HDR light estimation. Our method produces convincinglight estimates across diverse settings and demonstrates superiorgeneralization to in-the-wild scenarios.</description><author>Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn</author><pubDate>Tue, 09 Apr 2024 16:47:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09168v3</guid></item><item><title>Learning fast changing slow in spiking neural networks</title><link>http://arxiv.org/abs/2402.10069v2</link><description>Reinforcement learning (RL) faces substantial challenges when applied toreal-life problems, primarily stemming from the scarcity of available data dueto limited interactions with the environment. This limitation is exacerbated bythe fact that RL often demands a considerable volume of data for effectivelearning. The complexity escalates further when implementing RL in recurrentspiking networks, where inherent noise introduced by spikes adds a layer ofdifficulty. Life-long learning machines must inherently resolve theplasticity-stability paradox. Striking a balance between acquiring newknowledge and maintaining stability is crucial for artificial agents. Toaddress this challenge, we draw inspiration from machine learning technologyand introduce a biologically plausible implementation of proximal policyoptimization, referred to as lf-cs (learning fast changing slow). Our approachresults in two notable advancements: firstly, the capacity to assimilate newinformation into a new policy without requiring alterations to the currentpolicy; and secondly, the capability to replay experiences without experiencingpolicy divergence. Furthermore, when contrasted with other experience replay(ER) techniques, our method demonstrates the added advantage of beingcomputationally efficient in an online setting. We demonstrate that theproposed methodology enhances the efficiency of learning, showcasing itspotential impact on neuromorphic and real-world applications.</description><author>Cristiano Capone, Paolo Muratore</author><pubDate>Tue, 09 Apr 2024 16:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10069v2</guid></item><item><title>Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations</title><link>http://arxiv.org/abs/2404.06400v1</link><description>Using the nonlinear shallow water equations as benchmark, we demonstrate thata simulation with the ICON-O ocean model with a 20km resolution that isfrequently corrected by a U-net-type neural network can achieve discretizationerrors of a simulation with 10km resolution. The network, originally developedfor image-based super-resolution in post-processing, is trained to compute thedifference between solutions on both meshes and is used to correct the coarsemesh every 12h. Our setup is the Galewsky test case, modeling transition of abarotropic instability into turbulent flow. We show that the ML-correctedcoarse resolution run correctly maintains a balance flow and captures thetransition to turbulence in line with the higher resolution simulation. After 8day of simulation, the $L_2$-error of the corrected run is similar to asimulation run on the finer mesh. While mass is conserved in the correctedruns, we observe some spurious generation of kinetic energy.</description><author>Maximilian Witte, Fabricio Rodrigues Lapolli, Philip Freese, Sebastian Götschel, Daniel Ruprecht, Peter Korn, Christopher Kadow</author><pubDate>Tue, 09 Apr 2024 16:46:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06400v1</guid></item><item><title>Learning Local and Global Temporal Contexts for Video Semantic Segmentation</title><link>http://arxiv.org/abs/2204.03330v2</link><description>Contextual information plays a core role for video semantic segmentation(VSS). This paper summarizes contexts for VSS in two-fold: local temporalcontexts (LTC) which define the contexts from neighboring frames, and globaltemporal contexts (GTC) which represent the contexts from the whole video. Asfor LTC, it includes static and motional contexts, corresponding to static andmoving content in neighboring frames, respectively. Previously, both static andmotional contexts have been studied. However, there is no research aboutsimultaneously learning static and motional contexts (highly complementary).Hence, we propose a Coarse-to-Fine Feature Mining (CFFM) technique to learn aunified presentation of LTC. CFFM contains two parts: Coarse-to-Fine FeatureAssembling (CFFA) and Cross-frame Feature Mining (CFM). CFFA abstracts staticand motional contexts, and CFM mines useful information from nearby frames toenhance target features. To further exploit more temporal contexts, we proposeCFFM++ by additionally learning GTC from the whole video. Specifically, weuniformly sample certain frames from the video and extract global contextualprototypes by k-means. The information within those prototypes is mined by CFMto refine target features. Experimental results on popular benchmarksdemonstrate that CFFM and CFFM++ perform favorably against state-of-the-artmethods. Our code is available at https://github.com/GuoleiSun/VSS-CFFM</description><author>Guolei Sun, Yun Liu, Henghui Ding, Min Wu, Luc Van Gool</author><pubDate>Tue, 09 Apr 2024 16:44:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.03330v2</guid></item><item><title>MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies</title><link>http://arxiv.org/abs/2404.06395v1</link><description>The burgeoning interest in developing Large Language Models (LLMs) with up totrillion parameters has been met with concerns regarding resource efficiencyand practical expense, particularly given the immense cost of experimentation.This scenario underscores the importance of exploring the potential of SmallLanguage Models (SLMs) as a resource-efficient alternative. In this context, weintroduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parametervariants, not only excel in their respective categories but also demonstratecapabilities on par with 7B-13B LLMs. While focusing on SLMs, our approachexhibits scalability in both model and data dimensions for future LLM research.Regarding model scaling, we employ extensive model wind tunnel experiments forstable and optimal scaling. For data scaling, we introduce aWarmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive tocontinuous training and domain adaptation. We present an in-depth analysis ofthe intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, weare now able to efficiently study data-model scaling law without extensiveretraining experiments on both axes of model and data, from which we derive themuch higher compute optimal data-model ratio than Chinchilla Optimal.Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoEand MiniCPM-128K, whose excellent performance further cementing MiniCPM'sfoundation in diverse SLM applications. MiniCPM models are available publiclyat https://github.com/OpenBMB/MiniCPM .</description><author>Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun</author><pubDate>Tue, 09 Apr 2024 16:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06395v1</guid></item><item><title>MuPT: A Generative Symbolic Music Pretrained Transformer</title><link>http://arxiv.org/abs/2404.06393v1</link><description>In this paper, we explore the application of Large Language Models (LLMs) tothe pre-training of music. While the prevalent use of MIDI in music modeling iswell-established, our findings suggest that LLMs are inherently more compatiblewith ABC Notation, which aligns more closely with their design and strengths,thereby enhancing the model's performance in musical composition. To addressthe challenges associated with misaligned measures from different tracks duringgeneration, we propose the development of a \underline{S}ynchronized\underline{M}ulti-\underline{T}rack ABC Notation (\textbf{SMT-ABC Notation}),which aims to preserve coherence across multiple musical tracks. Ourcontributions include a series of models capable of handling up to 8192 tokens,covering 90\% of the symbolic music data in our training set. Furthermore, weexplore the implications of the \underline{S}ymbolic \underline{M}usic\underline{S}caling Law (\textbf{SMS Law}) on model performance. The resultsindicate a promising direction for future research in music generation,offering extensive resources for community-led research through our open-sourcecontributions.</description><author>Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xinrun Du, Shuyue Guo, Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma, Fengze Han, Wei Xue, Gus Xia, Emmanouil Benetos, Xiang Yue, Chenghua Lin, Xu Tan, Stephen W. Huang, Wenhu Chen, Jie Fu, Ge Zhang</author><pubDate>Tue, 09 Apr 2024 16:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06393v1</guid></item><item><title>Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis</title><link>http://arxiv.org/abs/2404.06392v1</link><description>Cross-lingual transfer-learning is widely used in Event Extraction forlow-resource languages and involves a Multilingual Language Model that istrained in a source language and applied to the target language. This paperstudies whether the typological similarity between source and target languagesimpacts the performance of cross-lingual transfer, an under-explored topic. Wefirst focus on Basque as the target language, which is an ideal target languagebecause it is typologically different from surrounding languages. Ourexperiments on three Event Extraction tasks show that the shared linguisticcharacteristic between source and target languages does have an impact ontransfer quality. Further analysis of 72 language pairs reveals that for tasksthat involve token classification such as entity and event triggeridentification, common writing script and morphological features produce higherquality cross-lingual transfer. In contrast, for tasks involving structuralprediction like argument extraction, common word order is the most relevantfeature. In addition, we show that when increasing the training size, not allthe languages scale in the same way in the cross-lingual setting. To performthe experiments we introduce EusIE, an event extraction dataset for Basque,which follows the Multilingual Event Extraction dataset (MEE). The dataset andcode are publicly available.</description><author>Mikel Zubillaga, Oscar Sainz, Ainara Estarrona, Oier Lopez de Lacalle, Eneko Agirre</author><pubDate>Tue, 09 Apr 2024 16:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06392v1</guid></item><item><title>Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity</title><link>http://arxiv.org/abs/2404.06391v1</link><description>One of the most intriguing findings in the structure of neural networklandscape is the phenomenon of mode connectivity: For two typical globalminima, there exists a path connecting them without barrier. This concept ofmode connectivity has played a crucial role in understanding importantphenomena in deep learning. In this paper, we conduct a fine-grained analysis of this connectivityphenomenon. First, we demonstrate that in the overparameterized case, theconnecting path can be as simple as a two-piece linear path, and the pathlength can be nearly equal to the Euclidean distance. This finding suggeststhat the landscape should be nearly convex in a certain sense. Second, weuncover a surprising star-shaped connectivity: For a finite number of typicalminima, there exists a center on minima manifold that connects all of themsimultaneously via linear paths. These results are provably valid for linearnetworks and two-layer ReLU networks under a teacher-student setup, and areempirically supported by models trained on MNIST and CIFAR-10.</description><author>Zhanran Lin, Puheng Li, Lei Wu</author><pubDate>Tue, 09 Apr 2024 16:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06391v1</guid></item><item><title>SGV3D:Towards Scenario Generalization for Vision-based Roadside 3D Object Detection</title><link>http://arxiv.org/abs/2401.16110v2</link><description>Roadside perception can greatly increase the safety of autonomous vehicles byextending their perception ability beyond the visual range and addressing blindspots. However, current state-of-the-art vision-based roadside detectionmethods possess high accuracy on labeled scenes but have inferior performanceon new scenes. This is because roadside cameras remain stationary afterinstallation and can only collect data from a single scene, resulting in thealgorithm overfitting these roadside backgrounds and camera poses. To addressthis issue, in this paper, we propose an innovative Scenario GeneralizationFramework for Vision-based Roadside 3D Object Detection, dubbed SGV3D.Specifically, we employ a Background-suppressed Module (BSM) to mitigatebackground overfitting in vision-centric pipelines by attenuating backgroundfeatures during the 2D to bird's-eye-view projection. Furthermore, byintroducing the Semi-supervised Data Generation Pipeline (SSDG) using unlabeledimages from new scenes, diverse instance foregrounds with varying camera posesare generated, addressing the risk of overfitting specific camera poses. Weevaluate our method on two large-scale roadside benchmarks. Our methodsurpasses all previous methods by a significant margin in new scenes, including+42.57% for vehicle, +5.87% for pedestrian, and +14.89% for cyclist compared toBEVHeight on the DAIR-V2X-I heterologous benchmark. On the larger-scale Rope3Dheterologous benchmark, we achieve notable gains of 14.48% for car and 12.41%for large vehicle. We aspire to contribute insights on the exploration ofroadside perception techniques, emphasizing their capability for scenariogeneralization. The code will be available athttps://github.com/yanglei18/SGV3D</description><author>Lei Yang, Xinyu Zhang, Jun Li, Li Wang, Chuang Zhang, Li Ju, Zhiwei Li, Yang Shen</author><pubDate>Tue, 09 Apr 2024 16:33:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16110v2</guid></item><item><title>Latent Distance Guided Alignment Training for Large Language Models</title><link>http://arxiv.org/abs/2404.06390v1</link><description>Ensuring alignment with human preferences is a crucial characteristic oflarge language models (LLMs). Presently, the primary alignment methods, RLHFand DPO, require extensive human annotation, which is expensive despite theirefficacy. The significant expenses associated with current alignment techniquesmotivate researchers to investigate the development of annotation-freealignment training methods. In pursuit of improved alignment without relying onexternal annotation, we introduce Latent Distance Guided Alignment Training(LD-Align). This approach seeks to align the model with a high-qualitysupervised fine-tune dataset using guidance from a latent space. The latentspace is generated through sample reconstruction, akin to auto-encoding.Consequently, we utilize the distance between sample pairs in the latent spaceto guide DPO-based alignment training. Extensive experimentation and evaluationshow the efficacy of our proposed method in achieving notable alignment.</description><author>Haotian Luo, Wenhao Zheng, Huaxiu Yao</author><pubDate>Tue, 09 Apr 2024 16:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06390v1</guid></item><item><title>Raster Forge: Interactive Raster Manipulation Library and GUI for Python</title><link>http://arxiv.org/abs/2404.06389v1</link><description>Raster Forge is a Python library and graphical user interface for raster datamanipulation and analysis. The tool is focused on remote sensing applications,particularly in wildfire management. It allows users to import, visualize, andprocess raster layers for tasks such as image compositing or topographicalanalysis. For wildfire management, it generates fuel maps using predefinedmodels. Its impact extends from disaster management to hydrological modeling,agriculture, and environmental monitoring. Raster Forge can be a valuable assetfor geoscientists and researchers who rely on raster data analysis, enhancinggeospatial data processing and visualization across various disciplines.</description><author>Afonso Oliveira, Nuno Fachada, João P. Matos-Carvalho</author><pubDate>Tue, 09 Apr 2024 16:31:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06389v1</guid></item><item><title>Selecting informative conformal prediction sets with false coverage rate control</title><link>http://arxiv.org/abs/2403.12295v2</link><description>In supervised learning, including regression and classification, conformalmethods provide prediction sets for the outcome/label with finite samplecoverage for any machine learning predictor. We consider here the case wheresuch prediction sets come after a selection process. The selection processrequires that the selected prediction sets be `informative' in a well definedsense. We consider both the classification and regression settings where theanalyst may consider as informative only the sample with prediction sets smallenough, excluding null values, or obeying other appropriate `monotone'constraints. We develop a unified framework for building such informativeconformal prediction sets while controlling the false coverage rate (FCR) onthe selected sample. While conformal prediction sets after selection have beenthe focus of much recent literature in the field, the new introducedprocedures, called InfoSP and InfoSCOP, are to our knowledge the first onesproviding FCR control for informative prediction sets. We show the usefulnessof our resulting procedures on real and simulated data.</description><author>Ulysse Gazin, Ruth Heller, Ariane Marandon, Etienne Roquain</author><pubDate>Tue, 09 Apr 2024 16:25:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12295v2</guid></item><item><title>Are We on the Right Way for Evaluating Large Vision-Language Models?</title><link>http://arxiv.org/abs/2403.20330v2</link><description>Large vision-language models (LVLMs) have recently achieved rapid progress,sparking numerous studies to evaluate their multi-modal capabilities. However,we dig into current evaluation works and identify two primary issues: 1) Visualcontent is unnecessary for many samples. The answers can be directly inferredfrom the questions and options, or the world knowledge embedded in LLMs. Thisphenomenon is prevalent across current benchmarks. For instance, GeminiProachieves 42.9% on the MMMU benchmark without any visual input, and outperformsthe random choice baseline across six benchmarks over 24% on average. 2)Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM couldstill answer some visual-necessary questions without visual content, indicatingthe memorizing of these samples within large-scale training data. For example,Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLMbackbone with 17.9%. Both problems lead to misjudgments of actual multi-modalgains and potentially misguide the study of LVLM. To this end, we presentMMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500samples meticulously selected by humans. MMStar benchmarks 6 core capabilitiesand 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities withcarefully balanced and purified samples. These samples are first roughlyselected from current benchmarks with an automated pipeline, human review isthen involved to ensure each curated sample exhibits visual dependency, minimaldata leakage, and requires advanced multi-modal capabilities. Moreover, twometrics are developed to measure data leakage and actual performance gain inmulti-modal training. We evaluate 16 leading LVLMs on MMStar to assess theirmulti-modal capabilities, and on 7 benchmarks with the proposed metrics toinvestigate their data leakage and actual multi-modal gain.</description><author>Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, Feng Zhao</author><pubDate>Tue, 09 Apr 2024 16:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20330v2</guid></item><item><title>The Shutdown Problem: An AI Engineering Puzzle for Decision Theorists</title><link>http://arxiv.org/abs/2403.04471v2</link><description>I explain the shutdown problem: the problem of designing artificial agentsthat (1) shut down when a shutdown button is pressed, (2) don't try to preventor cause the pressing of the shutdown button, and (3) otherwise pursue goalscompetently. I prove three theorems that make the difficulty precise. Thesetheorems show that agents satisfying some innocuous-seeming conditions willoften try to prevent or cause the pressing of the shutdown button, even incases where it's costly to do so. And patience trades off againstshutdownability: the more patient an agent, the greater the costs that agent iswilling to incur to manipulate the shutdown button. I end by noting that thesetheorems can guide our search for solutions.</description><author>Elliott Thornley</author><pubDate>Tue, 09 Apr 2024 16:09:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04471v2</guid></item><item><title>Model Generation from Requirements with LLMs: an Exploratory Study</title><link>http://arxiv.org/abs/2404.06371v1</link><description>Complementing natural language (NL) requirements with graphical models canimprove stakeholders' communication and provide directions for system design.However, creating models from requirements involves manual effort. The adventof generative large language models (LLMs), ChatGPT being a notable example,offers promising avenues for automated assistance in model generation. Thispaper investigates the capability of ChatGPT to generate a specific type ofmodel, i.e., UML sequence diagrams, from NL requirements. We conduct aqualitative study in which we examine the sequence diagrams generated byChatGPT for 28 requirements documents of various types and from differentdomains. Observations from the analysis of the generated diagrams havesystematically been captured through evaluation logs, and categorized throughthematic analysis. Our results indicate that, although the models generallyconform to the standard and exhibit a reasonable level of understandability,their completeness and correctness with respect to the specified requirementsoften present challenges. This issue is particularly pronounced in the presenceof requirements smells, such as ambiguity and inconsistency. The insightsderived from this study can influence the practical utilization of LLMs in theRE process, and open the door to novel RE-specific prompting strategiestargeting effective model generation.</description><author>Alessio Ferrari, Sallam Abualhaija, Chetan Arora</author><pubDate>Tue, 09 Apr 2024 16:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06371v1</guid></item><item><title>CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoors Object Detection from Multi-view Images</title><link>http://arxiv.org/abs/2403.04198v2</link><description>This paper introduces CN-RMA, a novel approach for 3D indoor object detectionfrom multi-view images. We observe the key challenge as the ambiguity of imageand 3D correspondence without explicit geometry to provide occlusioninformation. To address this issue, CN-RMA leverages the synergy of 3Dreconstruction networks and 3D object detection networks, where thereconstruction network provides a rough Truncated Signed Distance Function(TSDF) and guides image features to vote to 3D space correctly in an end-to-endmanner. Specifically, we associate weights to sampled points of each raythrough ray marching, representing the contribution of a pixel in an image tocorresponding 3D locations. Such weights are determined by the predicted signeddistances so that image features vote only to regions near the reconstructedsurface. Our method achieves state-of-the-art performance in 3D objectdetection from multi-view images, as measured by mAP@0.25 and mAP@0.5 on theScanNet and ARKitScenes datasets. The code and models are released athttps://github.com/SerCharles/CN-RMA.</description><author>Guanlin Shen, Jingwei Huang, Zhihua Hu, Bin Wang</author><pubDate>Tue, 09 Apr 2024 16:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04198v2</guid></item><item><title>MetaMix: Meta-state Precision Searcher for Mixed-precision Activation Quantization</title><link>http://arxiv.org/abs/2311.06798v2</link><description>Mixed-precision quantization of efficient networks often suffer fromactivation instability encountered in the exploration of bit selections. Toaddress this problem, we propose a novel method called MetaMix which consistsof bit selection and weight training phases. The bit selection phase iteratestwo steps, (1) the mixed-precision-aware weight update, and (2) the bit-searchtraining with the fixed mixed-precision-aware weights, both of which combinedreduce activation instability in mixed-precision quantization and contribute tofast and high-quality bit selection. The weight training phase exploits theweights and step sizes trained in the bit selection phase and fine-tunes themthereby offering fast training. Our experiments with efficient andhard-to-quantize networks, i.e., MobileNet v2 and v3, and ResNet-18 on ImageNetshow that our proposed method pushes the boundary of mixed-precisionquantization, in terms of accuracy vs. operations, by outperforming both mixed-and single-precision SOTA methods.</description><author>Han-Byul Kim, Joo Hyung Lee, Sungjoo Yoo, Hong-Seok Kim</author><pubDate>Tue, 09 Apr 2024 16:07:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06798v2</guid></item><item><title>Enhancing Decision Analysis with a Large Language Model: pyDecision a Comprehensive Library of MCDA Methods in Python</title><link>http://arxiv.org/abs/2404.06370v1</link><description>Purpose: Multicriteria decision analysis (MCDA) has become increasinglyessential for decision-making in complex environments. In response to thisneed, the pyDecision library, implemented in Python and available athttps://bit.ly/3tLFGtH, has been developed to provide a comprehensive andaccessible collection of MCDA methods. Methods: The pyDecision offers 70 MCDAmethods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families. Beyondoffering a vast range of techniques, the library provides visualization toolsfor more intuitive results interpretation. In addition to these features,pyDecision has integrated ChatGPT, an advanced Large Language Model, wheredecision-makers can use ChatGPT to discuss and compare the outcomes ofdifferent methods, providing a more interactive and intuitive understanding ofthe solutions. Findings: Large Language Models are undeniably potent but cansometimes be a double-edged sword. Its answers may be misleading withoutrigorous verification of its outputs, especially for researchers lacking deepdomain expertise. It's imperative to approach its insights with a discerningeye and a solid foundation in the relevant field. Originality: With theintegration of MCDA methods and ChatGPT, pyDecision is a significantcontribution to the scientific community, as it is an invaluable resource forresearchers, practitioners, and decision-makers navigating complexdecision-making problems and seeking the most appropriate solutions based onMCDA methods.</description><author>Valdecy Pereira, Marcio Pereira Basilio, Carlos Henrique Tarjano SantosCarlos Henrique Tarjano Santos</author><pubDate>Tue, 09 Apr 2024 16:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06370v1</guid></item><item><title>VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs</title><link>http://arxiv.org/abs/2404.06369v1</link><description>Automatically generating UI code from webpage design visions cansignificantly alleviate the burden of developers, enabling beginner developersor designers to directly generate Web pages from design diagrams. Currently,prior research has accomplished the objective of generating UI code fromrudimentary design visions or sketches through designing deep neural networks.Inspired by the groundbreaking advancements achieved by Multimodal LargeLanguage Models (MLLMs), the automatic generation of UI code from high-fidelitydesign images is now emerging as a viable possibility. Nevertheless, ourinvestigation reveals that existing MLLMs are hampered by the scarcity ofauthentic, high-quality, and large-scale datasets, leading to unsatisfactoryperformance in automated UI code generation. To mitigate this gap, we present anovel dataset, termed VISION2UI, extracted from real-world scenarios, augmentedwith comprehensive layout information, tailored specifically for finetuningMLLMs in UI code generation. Specifically, this dataset is derived through aseries of operations, encompassing collecting, cleaning, and filtering of theopen-source Common Crawl dataset. In order to uphold its quality, a neuralscorer trained on labeled samples is utilized to refine the data, retaininghigher-quality instances. Ultimately, this process yields a dataset comprising2,000 (Much more is coming soon) parallel samples encompassing design visionsand UI code. The dataset is available athttps://huggingface.co/datasets/xcodemind/vision2ui.</description><author>Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su, Shaoling Dong, Xing Zhou, Wenbin Jiang</author><pubDate>Tue, 09 Apr 2024 16:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06369v1</guid></item><item><title>ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish</title><link>http://arxiv.org/abs/2404.06367v1</link><description>Advances in natural language processing techniques, such as named entityrecognition and normalization to widely used standardized terminologies likeUMLS or SNOMED-CT, along with the digitalization of electronic health records,have significantly advanced clinical text analysis. This study presentsClinLinker, a novel approach employing a two-phase pipeline for medical entitylinking that leverages the potential of in-domain adapted language models forbiomedical text mining: initial candidate retrieval using a SapBERT-basedbi-encoder and subsequent re-ranking with a cross-encoder, trained by followinga contrastive-learning strategy to be tailored to medical concepts in Spanish.This methodology, focused initially on content in Spanish, substantiallyoutperforming multilingual language models designed for the same purpose. Thisis true even for complex scenarios involving heterogeneous medicalterminologies and being trained on a subset of the original data. Our results,evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate ourapproach's performance on two distinct clinical entity linking Gold Standardcorpora, DisTEMIST (diseases) and MedProcNER (clinical procedures),outperforming previous benchmarks by 40 points in DisTEMIST and 43 points inMedProcNER, both normalized to SNOMED-CT codes. These findings highlight ourapproach's ability to address language-specific nuances and set a new benchmarkin entity linking, offering a potent tool for enhancing the utility of digitalmedical records. The resulting system is of practical value, both for largescale automatic generation of structured data derived from clinical records, aswell as for exhaustive extraction and harmonization of predefined clinicalvariables of interest.</description><author>Fernando Gallego, Guillermo López-García, Luis Gasco-Sánchez, Martin Krallinger, Francisco J. Veredas</author><pubDate>Tue, 09 Apr 2024 16:04:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06367v1</guid></item><item><title>Dynamic Backtracking in GFlowNets: Enhancing Decision Steps with Reward-Dependent Adjustment Mechanisms</title><link>http://arxiv.org/abs/2404.05576v2</link><description>Generative Flow Networks (GFlowNets) are probabilistic models predicated onMarkov flows, employing specific amortization algorithms to learn stochasticpolicies that generate compositional substances including biomolecules,chemical materials, and more. Demonstrating formidable prowess in generatinghigh-performance biochemical molecules, GFlowNets accelerate the discovery ofscientific substances, effectively circumventing the time-consuming,labor-intensive, and costly shortcomings intrinsic to conventional materialdiscovery. However, previous work often struggles to accumulate exploratoryexperience and is prone to becoming disoriented within expansive samplingspaces. Attempts to address this issue, such as LS-GFN, are limited to localgreedy searches and lack broader global adjustments. This paper introduces anovel GFlowNets variant, the Dynamic Backtracking GFN (DB-GFN), which enhancesthe adaptability of decision-making steps through a reward-based dynamicbacktracking mechanism. DB-GFN permits backtracking during the networkconstruction process according to the current state's reward value, thuscorrecting disadvantageous decisions and exploring alternative pathways duringthe exploration process. Applied to generative tasks of biochemical moleculesand genetic material sequences, DB-GFN surpasses existing GFlowNets models andtraditional reinforcement learning methods in terms of sample quality,exploration sample quantity, and training convergence speed. Furthermore, theorthogonal nature of DB-GFN suggests its potential as a powerful tool forfuture improvements in GFlowNets, with the promise of integrating with otherstrategies to achieve more efficient search performance.</description><author>Shuai Guo, Jielei Chu, Lei Zhu, Tianrui Li</author><pubDate>Tue, 09 Apr 2024 16:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05576v2</guid></item><item><title>Dynamic Resolution Guidance for Facial Expression Recognition</title><link>http://arxiv.org/abs/2404.06365v1</link><description>Facial expression recognition (FER) is vital for human-computer interactionand emotion analysis, yet recognizing expressions in low-resolution imagesremains challenging. This paper introduces a practical method called DynamicResolution Guidance for Facial Expression Recognition (DRGFER) to effectivelyrecognize facial expressions in images with varying resolutions withoutcompromising FER model accuracy. Our framework comprises two main components:the Resolution Recognition Network (RRN) and the Multi-Resolution AdaptationFacial Expression Recognition Network (MRAFER). The RRN determines imageresolution, outputs a binary vector, and the MRAFER assigns images to suitablefacial expression recognition networks based on resolution. We evaluated DRGFERon widely-used datasets RAFDB and FERPlus, demonstrating that our methodretains optimal model performance at each resolution and outperformsalternative resolution approaches. The proposed framework exhibits robustnessagainst resolution variations and facial expressions, offering a promisingsolution for real-world applications.</description><author>Jie Ou, Xu Li, Tianxiang Jiang, Yuanlun Xie</author><pubDate>Tue, 09 Apr 2024 16:02:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06365v1</guid></item><item><title>SurveyAgent: A Conversational System for Personalized and Efficient Research Survey</title><link>http://arxiv.org/abs/2404.06364v1</link><description>In the rapidly advancing research fields such as AI, managing and stayingabreast of the latest scientific literature has become a significant challengefor researchers. Although previous efforts have leveraged AI to assist withliterature searches, paper recommendations, and question-answering, acomprehensive support system that addresses the holistic needs of researchershas been lacking. This paper introduces SurveyAgent, a novel conversationalsystem designed to provide personalized and efficient research surveyassistance to researchers. SurveyAgent integrates three key modules: KnowledgeManagement for organizing papers, Recommendation for discovering relevantliterature, and Query Answering for engaging with content on a deeper level.This system stands out by offering a unified platform that supports researchersthrough various stages of their literature review process, facilitated by aconversational interface that prioritizes user interaction and personalization.Our evaluation demonstrates SurveyAgent's effectiveness in streamliningresearch activities, showcasing its capability to facilitate how researchersinteract with scientific literature.</description><author>Xintao Wang, Jiangjie Chen, Nianqi Li, Lida Chen, Xinfeng Yuan, Wei Shi, Xuyang Ge, Rui Xu, Yanghua Xiao</author><pubDate>Tue, 09 Apr 2024 16:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06364v1</guid></item><item><title>NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models</title><link>http://arxiv.org/abs/2404.01306v2</link><description>Transformer-based Language Models have become ubiquitous in Natural LanguageProcessing (NLP) due to their impressive performance on various tasks. However,expensive training as well as inference remains a significant impediment totheir widespread applicability. While enforcing sparsity at various levels ofthe model architecture has found promise in addressing scaling and efficiencyissues, there remains a disconnect between how sparsity affects networktopology. Inspired by brain neuronal networks, we explore sparsity approachesthrough the lens of network topology. Specifically, we exploit mechanisms seenin biological networks, such as preferential attachment and redundant synapsepruning, and show that principled, model-agnostic sparsity approaches areperformant and efficient across diverse NLP tasks, spanning both classification(such as natural language inference) and generation (summarization, machinetranslation), despite our sole objective not being optimizing performance.NeuroPrune is competitive with (or sometimes superior to) baselines onperformance and can be up to $10$x faster in terms of training time for a givenlevel of sparsity, simultaneously exhibiting measurable improvements ininference time in many cases.</description><author>Amit Dhurandhar, Tejaswini Pedapati, Ronny Luss, Soham Dan, Aurelie Lozano, Payel Das, Georgios Kollias</author><pubDate>Tue, 09 Apr 2024 15:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01306v2</guid></item><item><title>Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation</title><link>http://arxiv.org/abs/2404.06362v1</link><description>The Segment Anything Model (SAM) and CLIP are remarkable vision foundationmodels (VFMs). SAM, a prompt driven segmentation model, excels in segmentationtasks across diverse domains, while CLIP is renowned for its zero shotrecognition capabilities. However, their unified potential has not yet beenexplored in medical image segmentation. To adapt SAM to medical imaging,existing methods primarily rely on tuning strategies that require extensivedata or prior prompts tailored to the specific task, making it particularlychallenging when only a limited number of data samples are available. This workpresents an in depth exploration of integrating SAM and CLIP into a unifiedframework for medical image segmentation. Specifically, we propose a simpleunified framework, SaLIP, for organ segmentation. Initially, SAM is used forpart based segmentation within the image, followed by CLIP to retrieve the maskcorresponding to the region of interest (ROI) from the pool of SAM generatedmasks. Finally, SAM is prompted by the retrieved ROI to segment a specificorgan. Thus, SaLIP is training and fine tuning free and does not rely on domainexpertise or labeled data for prompt engineering. Our method shows substantialenhancements in zero shot segmentation, showcasing notable improvements in DICEscores across diverse segmentation tasks like brain (63.46%), lung (50.11%),and fetal head (30.82%), when compared to un prompted SAM. Code and textprompts will be available online.</description><author>Sidra Aleem, Fangyijie Wang, Mayug Maniparambil, Eric Arazo, Julia Dietlmeier, Kathleen Curran, Noel E. O'Connor, Suzanne Little</author><pubDate>Tue, 09 Apr 2024 15:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06362v1</guid></item><item><title>Generalizable Sarcasm Detection Is Just Around The Corner, Of Course!</title><link>http://arxiv.org/abs/2404.06357v1</link><description>We tested the robustness of sarcasm detection models by examining theirbehavior when fine-tuned on four sarcasm datasets containing varyingcharacteristics of sarcasm: label source (authors vs. third-party), domain(social media/online vs. offline conversations/dialogues), style (aggressivevs. humorous mocking). We tested their prediction performance on the samedataset (intra-dataset) and across different datasets (cross-dataset). Forintra-dataset predictions, models consistently performed better when fine-tunedwith third-party labels rather than with author labels. For cross-datasetpredictions, most models failed to generalize well to the other datasets,implying that one type of dataset cannot represent all sorts of sarcasm withdifferent styles and domains. Compared to the existing datasets, modelsfine-tuned on the new dataset we release in this work showed the highestgeneralizability to other datasets. With a manual inspection of the datasetsand post-hoc analysis, we attributed the difficulty in generalization to thefact that sarcasm actually comes in different domains and styles. We argue thatfuture sarcasm research should take the broad scope of sarcasm into account.</description><author>Hyewon Jang, Diego Frassinelli</author><pubDate>Tue, 09 Apr 2024 15:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06357v1</guid></item><item><title>Policy-Guided Diffusion</title><link>http://arxiv.org/abs/2404.06356v1</link><description>In many real-world settings, agents must learn from an offline datasetgathered by some prior behavior policy. Such a setting naturally leads todistribution shift between the behavior policy and the target policy beingtrained - requiring policy conservatism to avoid instability and overestimationbias. Autoregressive world models offer a different solution to this bygenerating synthetic, on-policy experience. However, in practice, modelrollouts must be severely truncated to avoid compounding error. As analternative, we propose policy-guided diffusion. Our method uses diffusionmodels to generate entire trajectories under the behavior distribution,applying guidance from the target policy to move synthetic experience furtheron-policy. We show that policy-guided diffusion models a regularized form ofthe target distribution that balances action likelihood under both the targetand behavior policies, leading to plausible trajectories with high targetpolicy probability, while retaining a lower dynamics error than an offlineworld model baseline. Using synthetic experience from policy-guided diffusionas a drop-in substitute for real data, we demonstrate significant improvementsin performance across a range of standard offline reinforcement learningalgorithms and environments. Our approach provides an effective alternative toautoregressive offline world models, opening the door to the controllablegeneration of synthetic training data.</description><author>Matthew Thomas Jackson, Michael Tryfan Matthews, Cong Lu, Benjamin Ellis, Shimon Whiteson, Jakob Foerster</author><pubDate>Tue, 09 Apr 2024 15:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06356v1</guid></item><item><title>High Noise Scheduling is a Must</title><link>http://arxiv.org/abs/2404.06353v1</link><description>Consistency models possess high capabilities for image generation, advancingsampling steps to a single step through their advanced techniques. Currentadvancements move one step forward consistency training techniques andeliminates the limitation of distillation training. Even though the proposedcurriculum and noise scheduling in improved training techniques yield betterresults than basic consistency models, it lacks well balanced noisedistribution and its consistency between curriculum. In this study, it isinvestigated the balance between high and low noise levels in noisedistribution and offered polynomial noise distribution to maintain thestability. This proposed polynomial noise distribution is also supported with apredefined Karras noises to prevent unique noise levels arises with Karrasnoise generation algorithm. Furthermore, by elimination of learned noisy stepswith a curriculum based on sinusoidal function increase the performance of themodel in denoising. To make a fair comparison with the latest releasedconsistency model training techniques, experiments are conducted with samehyper-parameters except curriculum and noise distribution. The models utilizedduring experiments are determined with low depth to prove the robustness of ourproposed technique. The results show that the polynomial noise distributionoutperforms the model trained with log-normal noise distribution, yielding a33.54 FID score after 100,000 training steps with constant discretizationsteps. Additionally, the implementation of a sinusoidal-based curriculumenhances denoising performance, resulting in a FID score of 30.48.</description><author>Mahmut S. Gokmen, Cody Bumgardner, Jie Zhang, Ge Wang, Jin Chen</author><pubDate>Tue, 09 Apr 2024 15:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06353v1</guid></item><item><title>DaF-BEVSeg: Distortion-aware Fisheye Camera based Bird's Eye View Segmentation with Occlusion Reasoning</title><link>http://arxiv.org/abs/2404.06352v1</link><description>Semantic segmentation is an effective way to perform scene understanding.Recently, segmentation in 3D Bird's Eye View (BEV) space has become popular asits directly used by drive policy. However, there is limited work on BEVsegmentation for surround-view fisheye cameras, commonly used in commercialvehicles. As this task has no real-world public dataset and existing syntheticdatasets do not handle amodal regions due to occlusion, we create a syntheticdataset using the Cognata simulator comprising diverse road types, weather, andlighting conditions. We generalize the BEV segmentation to work with any cameramodel; this is useful for mixing diverse cameras. We implement a baseline byapplying cylindrical rectification on the fisheye images and using a standardLSS-based BEV segmentation model. We demonstrate that we can achieve betterperformance without undistortion, which has the adverse effects of increasedruntime due to pre-processing, reduced field-of-view, and resampling artifacts.Further, we introduce a distortion-aware learnable BEV pooling strategy that ismore effective for the fisheye cameras. We extend the model with an occlusionreasoning module, which is critical for estimating in BEV space. Qualitativeperformance of DaF-BEVSeg is showcased in the video athttps://streamable.com/ge4v51.</description><author>Senthil Yogamani, David Unger, Venkatraman Narayanan, Varun Ravi Kumar</author><pubDate>Tue, 09 Apr 2024 15:43:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06352v1</guid></item><item><title>HPNet: Dynamic Trajectory Forecasting with Historical Prediction Attention</title><link>http://arxiv.org/abs/2404.06351v1</link><description>Predicting the trajectories of road agents is essential for autonomousdriving systems. The recent mainstream methods follow a static paradigm, whichpredicts the future trajectory by using a fixed duration of historical frames.These methods make the predictions independently even at adjacent time steps,which leads to potential instability and temporal inconsistency. As successivetime steps have largely overlapping historical frames, their forecasting shouldhave intrinsic correlation, such as overlapping predicted trajectories shouldbe consistent, or be different but share the same motion goal depending on theroad situation. Motivated by this, in this work, we introduce HPNet, a noveldynamic trajectory forecasting method. Aiming for stable and accuratetrajectory forecasting, our method leverages not only historical framesincluding maps and agent states, but also historical predictions. Specifically,we newly design a Historical Prediction Attention module to automaticallyencode the dynamic relationship between successive predictions. Besides, italso extends the attention range beyond the currently visible windowbenefitting from the use of historical predictions. The proposed HistoricalPrediction Attention together with the Agent Attention and Mode Attention isfurther formulated as the Triple Factorized Attention module, serving as thecore design of HPNet.Experiments on the Argoverse and INTERACTION datasets showthat HPNet achieves state-of-the-art performance, and generates accurate andstable future trajectories. Our code are available athttps://github.com/XiaolongTang23/HPNet.</description><author>Xiaolong Tang, Meina Kan, Shiguang Shan, Zhilong Ji, Jinfeng Bai, Xilin Chen</author><pubDate>Tue, 09 Apr 2024 15:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06351v1</guid></item><item><title>ExIFFI and EIF+: Interpretability and Enhanced Generalizability to Extend the Extended Isolation Forest</title><link>http://arxiv.org/abs/2310.05468v2</link><description>Anomaly Detection involves identifying unusual behaviors within complexdatasets and systems. While Machine Learning algorithms and Decision SupportSystems (DSSs) offer effective solutions for this task, simply pinpointinganomalies may prove insufficient in real-world applications. Users requireinsights into the rationale behind these predictions to facilitate root causeanalysis and foster trust in the model. However, the unsupervised nature of ADpresents a challenge in developing interpretable tools. This paper addressesthis challenge by introducing ExIFFI, a novel interpretability approachspecifically designed to explain the predictions made by Extended IsolationForest. ExIFFI leverages feature importance to provide explanations at bothglobal and local levels. This work also introduces EIF+, an enhanced variant ofExtended Isolation Forest, conceived to improve its generalization capabilitiesthrough a different splitting hyperplanes design strategy. A comprehensivecomparative analysis is conducted, employing both synthetic and real-worlddatasets to evaluate various unsupervised AD approaches. The analysisdemonstrates the effectiveness of ExIFFI in providing explanations for ADpredictions. Furthermore, the paper explores the utility of ExIFFI as a featureselection technique in unsupervised settings. Finally, this work contributes tothe research community by providing open-source code, facilitating furtherinvestigation and reproducibility.</description><author>Alessio Arcudi, Davide Frizzo, Chiara Masiero, Gian Antonio Susto</author><pubDate>Tue, 09 Apr 2024 15:41:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05468v2</guid></item><item><title>Rolling Shutter Correction with Intermediate Distortion Flow Estimation</title><link>http://arxiv.org/abs/2404.06350v1</link><description>This paper proposes to correct the rolling shutter (RS) distorted images byestimating the distortion flow from the global shutter (GS) to RS directly.Existing methods usually perform correction using the undistortion flow fromthe RS to GS. They initially predict the flow from consecutive RS frames,subsequently rescaling it as the displacement fields from the RS frame to theunderlying GS image using time-dependent scaling factors. Following this,RS-aware forward warping is employed to convert the RS image into its GScounterpart. Nevertheless, this strategy is prone to two shortcomings. First,the undistortion flow estimation is rendered inaccurate by merely linearscaling the flow, due to the complex non-linear motion nature. Second, RS-awareforward warping often results in unavoidable artifacts. To address theselimitations, we introduce a new framework that directly estimates thedistortion flow and rectifies the RS image with the backward warping operation.More specifically, we first propose a global correlation-based flow attentionmechanism to estimate the initial distortion flow and GS feature jointly, whichare then refined by the following coarse-to-fine decoder layers. Additionally,a multi-distortion flow prediction strategy is integrated to mitigate the issueof inaccurate flow estimation further. Experimental results validate theeffectiveness of the proposed method, which outperforms state-of-the-artapproaches on various benchmarks while maintaining high efficiency. The projectis available at \url{https://github.com/ljzycmd/DFRSC}.</description><author>Mingdeng Cao, Sidi Yang, Yujiu Yang, Yinqiang Zheng</author><pubDate>Tue, 09 Apr 2024 15:40:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06350v1</guid></item><item><title>CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models</title><link>http://arxiv.org/abs/2404.06349v1</link><description>Causality reveals fundamental principles behind data distributions inreal-world scenarios, and the capability of large language models (LLMs) tounderstand causality directly impacts their efficacy across explaining outputs,adapting to new evidence, and generating counterfactuals. With theproliferation of LLMs, the evaluation of this capacity is increasinglygarnering attention. However, the absence of a comprehensive benchmark hasrendered existing evaluation studies being straightforward, undiversified, andhomogeneous. To address these challenges, this paper proposes a comprehensivebenchmark, namely CausalBench, to evaluate the causality understandingcapabilities of LLMs. Originating from the causal research community,CausalBench encompasses three causal learning-related tasks, which facilitate aconvenient comparison of LLMs' performance with classic causal learningalgorithms. Meanwhile, causal networks of varying scales and densities areintegrated in CausalBench, to explore the upper limits of LLMs' capabilitiesacross task scenarios of varying difficulty. Notably, background knowledge andstructured data are also incorporated into CausalBench to thoroughly unlock theunderlying potential of LLMs for long-text comprehension and prior informationutilization. Based on CausalBench, this paper evaluates nineteen leading LLMsand unveils insightful conclusions in diverse aspects. Firstly, we present thestrengths and weaknesses of LLMs and quantitatively explore the upper limits oftheir capabilities across various scenarios. Meanwhile, we further discern theadaptability and abilities of LLMs to specific structural networks and complexchain of thought structures. Moreover, this paper quantitatively presents thedifferences across diverse information sources and uncovers the gap betweenLLMs' capabilities in causal understanding within textual contexts andnumerical domains.</description><author>Yu Zhou, Xingyu Wu, Beicheng Huang, Jibin Wu, Liang Feng, Kay Chen Tan</author><pubDate>Tue, 09 Apr 2024 15:40:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06349v1</guid></item><item><title>TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese</title><link>http://arxiv.org/abs/2401.16640v2</link><description>Large language models (LLMs) have significantly advanced natural languageprocessing, but their progress has yet to be equal across languages. While mostLLMs are trained in high-resource languages like English, multilingual modelsgenerally underperform monolingual ones. Additionally, aspects of theirmultilingual foundation sometimes restrict the byproducts they produce, likecomputational demands and licensing regimes. In this study, we document thedevelopment of open-foundation models tailored for use in low-resourcesettings, their limitations, and their benefits. This is the TeenyTinyLlamapair: two compact models for Brazilian Portuguese text generation. We releasethem under the permissive Apache 2.0 license on GitHub and Hugging Face forcommunity use and further development. Seehttps://github.com/Nkluge-correa/TeenyTinyLlama</description><author>Nicholas Kluge Corrêa, Sophia Falk, Shiza Fatimah, Aniket Sen, Nythamar de Oliveira</author><pubDate>Tue, 09 Apr 2024 15:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16640v2</guid></item><item><title>RAR-b: Reasoning as Retrieval Benchmark</title><link>http://arxiv.org/abs/2404.06347v1</link><description>Semantic textual similartiy (STS) and information retrieval tasks (IR) taskshave been the two major avenues to record the progress of embedding models inthe past few years. Under the emerging Retrieval-augmented Generation (RAG)paradigm, we envision the need to evaluate next-level language understandingabilities of embedding models, and take a conscious look at the reasoningabilities stored in them. Addressing this, we pose the question: Can retrieverssolve reasoning problems? By transforming reasoning tasks into retrieval tasks,we find that without specifically trained for reasoning-level languageunderstanding, current state-of-the-art retriever models may still be far frombeing competent for playing the role of assisting LLMs, especially inreasoning-intensive tasks. Moreover, albeit trained to be aware ofinstructions, instruction-aware IR models are often better off withoutinstructions in inference time for reasoning tasks, posing an overlookedretriever-LLM behavioral gap for the research community to align. However,recent decoder-based embedding models show great promise in narrowing the gap,highlighting the pathway for embedding models to achieve reasoning-levellanguage understanding. We also show that, although current off-the-shelfre-ranker models fail on these tasks, injecting reasoning abilities into themthrough fine-tuning still appears easier than doing so to bi-encoders, and weare able to achieve state-of-the-art performance across all tasks byfine-tuning a reranking model. We release Reasoning as Retrieval Benchmark(RAR-b), a holistic suite of tasks and settings to evaluate the reasoningabilities stored in retriever models. RAR-b is available athttps://github.com/gowitheflow-1998/RAR-b.</description><author>Chenghao Xiao, G Thomas Hudson, Noura Al Moubayed</author><pubDate>Tue, 09 Apr 2024 15:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06347v1</guid></item><item><title>AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning</title><link>http://arxiv.org/abs/2404.06345v1</link><description>Connected and autonomous driving is developing rapidly in recent years.However, current autonomous driving systems, which are primarily based ondata-driven approaches, exhibit deficiencies in interpretability,generalization, and continuing learning capabilities. In addition, thesingle-vehicle autonomous driving systems lack of the ability of collaborationand negotiation with other vehicles, which is crucial for the safety andefficiency of autonomous driving systems. In order to address these issues, weleverage large language models (LLMs) to develop a novel framework,AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving.AgentsCoDriver consists of five modules: observation module, reasoning engine,cognitive memory module, reinforcement reflection module, and communicationmodule. It can accumulate knowledge, lessons, and experiences over time bycontinuously interacting with the environment, thereby making itself capable oflifelong learning. In addition, by leveraging the communication module,different agents can exchange information and realize negotiation andcollaboration in complex traffic environments. Extensive experiments areconducted and show the superiority of AgentsCoDriver.</description><author>Senkang Hu, Zhengru Fang, Zihan Fang, Xianhao Chen, Yuguang Fang</author><pubDate>Tue, 09 Apr 2024 15:33:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06345v1</guid></item><item><title>Synaptogen: A cross-domain generative device model for large-scale neuromorphic circuit design</title><link>http://arxiv.org/abs/2404.06344v1</link><description>We present a fast generative modeling approach for resistive memories thatreproduces the complex statistical properties of real-world devices. To enableefficient modeling of analog circuits, the model is implemented in Verilog-A.By training on extensive measurement data of integrated 1T1R arrays (6,000cycles of 512 devices), an autoregressive stochastic process accuratelyaccounts for the cross-correlations between the switching parameters, whilenon-linear transformations ensure agreement with both cycle-to-cycle (C2C) anddevice-to-device (D2D) variability. Benchmarks show that this statisticallycomprehensive model achieves read/write throughputs exceeding those of evenhighly simplified and deterministic compact models.</description><author>Tyler Hennen, Leon Brackmann, Tobias Ziegler, Sebastian Siegel, Stephan Menzel, Rainer Waser, Dirk J. Wouters, Daniel Bedau</author><pubDate>Tue, 09 Apr 2024 15:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06344v1</guid></item><item><title>UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation</title><link>http://arxiv.org/abs/2403.20035v2</link><description>Traditionally for improving the segmentation performance of models, mostapproaches prefer to use adding more complex modules. And this is not suitablefor the medical field, especially for mobile medical devices, wherecomputationally loaded models are not suitable for real clinical environmentsdue to computational resource constraints. Recently, state-space models (SSMs),represented by Mamba, have become a strong competitor to traditional CNNs andTransformers. In this paper, we deeply explore the key elements of parameterinfluence in Mamba and propose an UltraLight Vision Mamba UNet (UltraLightVM-UNet) based on this. Specifically, we propose a method for processingfeatures in parallel Vision Mamba, named PVM Layer, which achieves excellentperformance with the lowest computational load while keeping the overall numberof processing channels constant. We conducted comparisons and ablationexperiments with several state-of-the-art lightweight models on three skinlesion public datasets and demonstrated that the UltraLight VM-UNet exhibitsthe same strong performance competitiveness with parameters of only 0.049M andGFLOPs of 0.060. In addition, this study deeply explores the key elements ofparameter influence in Mamba, which will lay a theoretical foundation for Mambato possibly become a new mainstream module for lightweighting in the future.The code is available from https://github.com/wurenkai/UltraLight-VM-UNet .</description><author>Renkai Wu, Yinghao Liu, Pengchen Liang, Qing Chang</author><pubDate>Tue, 09 Apr 2024 15:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20035v2</guid></item><item><title>Finding fake reviews in e-commerce platforms by using hybrid algorithms</title><link>http://arxiv.org/abs/2404.06339v1</link><description>Sentiment analysis, a vital component in natural language processing, plays acrucial role in understanding the underlying emotions and opinions expressed intextual data. In this paper, we propose an innovative ensemble approach forsentiment analysis for finding fake reviews that amalgamate the predictivecapabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), andDecision Tree classifiers. Our ensemble architecture strategically combinesthese diverse models to capitalize on their strengths while mitigating inherentweaknesses, thereby achieving superior accuracy and robustness in fake reviewprediction. By combining all the models of our classifiers, the predictiveperformance is boosted and it also fosters adaptability to varied linguisticpatterns and nuances present in real-world datasets. The metrics accounted foron fake reviews demonstrate the efficacy and competitiveness of the proposedensemble method against traditional single-model approaches. Our findingsunderscore the potential of ensemble techniques in advancing thestate-of-the-art in finding fake reviews using hybrid algorithms, withimplications for various applications in different social media and e-platformsto find the best reviews and neglect the fake ones, eliminating puffery andbluffs.</description><author>Mathivanan Periasamy, Rohith Mahadevan, Bagiya Lakshmi S, Raja CSP Raman, Hasan Kumar S, Jasper Jessiman</author><pubDate>Tue, 09 Apr 2024 15:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06339v1</guid></item><item><title>Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences</title><link>http://arxiv.org/abs/2404.06337v1</link><description>Given two images, we can estimate the relative camera pose between them byestablishing image-to-image correspondences. Usually, correspondences are2D-to-2D and the pose we estimate is defined only up to scale. Someapplications, aiming at instant augmented reality anywhere, requirescale-metric pose estimates, and hence, they rely on external depth estimatorsto recover the scale. We present MicKey, a keypoint matching pipeline that isable to predict metric correspondences in 3D camera space. By learning to match3D coordinates across images, we are able to infer the metric relative posewithout depth measurements. Depth measurements are also not required fortraining, nor are scene reconstructions or image overlap information. MicKey issupervised only by pairs of images and their relative poses. MicKey achievesstate-of-the-art performance on the Map-Free Relocalisation benchmark whilerequiring less supervision than competing approaches.</description><author>Axel Barroso-Laguna, Sowmya Munukutla, Victor Adrian Prisacariu, Eric Brachmann</author><pubDate>Tue, 09 Apr 2024 15:22:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06337v1</guid></item><item><title>Quantum State Generation with Structure-Preserving Diffusion Model</title><link>http://arxiv.org/abs/2404.06336v1</link><description>This article considers the generative modeling of the states of quantumsystems, and an approach based on denoising diffusion model is proposed. Thekey contribution is an algorithmic innovation that respects the physical natureof quantum states. More precisely, the commonly used density matrixrepresentation of mixed-state has to be complex-valued Hermitian, positivesemi-definite, and trace one. Generic diffusion models, or other generativemethods, may not be able to generate data that strictly satisfy thesestructural constraints, even if all training data do. To develop a machinelearning algorithm that has physics hard-wired in, we leverage the recentdevelopment of Mirror Diffusion Model and design a previously unconsideredmirror map, to enable strict structure-preserving generation. Bothunconditional generation and conditional generation via classifier-freeguidance are experimentally demonstrated efficacious, the latter even enablingthe design of new quantum states when generated on unseen labels.</description><author>Yuchen Zhu, Tianrong Chen, Evangelos A. Theodorou, Xie Chen, Molei Tao</author><pubDate>Tue, 09 Apr 2024 15:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06336v1</guid></item><item><title>Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment</title><link>http://arxiv.org/abs/2311.04818v4</link><description>Learning from the collective knowledge of data dispersed across privatesources can provide neural networks with enhanced generalization capabilities.Federated learning, a method for collaboratively training a machine learningmodel across remote clients, achieves this by combining client models via theorchestration of a central server. However, current approaches face twocritical limitations: i) they struggle to converge when client domains aresufficiently different, and ii) current aggregation techniques produce anidentical global model for each client. In this work, we address these issuesby reformulating the typical federated learning setup: rather than learning asingle global model, we learn N models each optimized for a common objective.To achieve this, we apply a weighted distance minimization to model parametersshared in a peer-to-peer topology. The resulting framework, Iterative ParameterAlignment, applies naturally to the cross-silo setting, and has the followingproperties: (i) a unique solution for each participant, with the option toglobally converge each model in the federation, and (ii) an optionalearly-stopping mechanism to elicit fairness among peers in collaborativelearning settings. These characteristics jointly provide a flexible newframework for iteratively learning from peer models trained on disparatedatasets. We find that the technique achieves competitive results on a varietyof data partitions compared to state-of-the-art approaches. Further, we showthat the method is robust to divergent domains (i.e. disjoint classes acrosspeers) where existing approaches struggle.</description><author>Matt Gorbett, Hossein Shirazi, Indrakshi Ray</author><pubDate>Tue, 09 Apr 2024 15:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04818v4</guid></item></channel></rss>