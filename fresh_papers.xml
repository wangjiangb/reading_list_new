<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 09 Nov 2025 12:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Dark Energy Survey Year 3 results: Simulation-based $w$CDM inference from weak lensing and galaxy clustering maps with deep learning. I. Analysis design</title><link>http://arxiv.org/abs/2511.04681v1</link><description>Data-driven approaches using deep learning are emerging as powerfultechniques to extract non-Gaussian information from cosmological large-scalestructure. This work presents the first simulation-based inference (SBI)pipeline that combines weak lensing and galaxy clustering maps in a realisticDark Energy Survey Year 3 (DES Y3) configuration and serves as preparation fora forthcoming analysis of the survey data. We develop a scalable forward modelbased on the CosmoGridV1 suite of N-body simulations to generate over onemillion self-consistent mock realizations of DES Y3 at the map level.Leveraging this large dataset, we train deep graph convolutional neuralnetworks on the full survey footprint in spherical geometry to learnlow-dimensional features that approximately maximize mutual information withtarget parameters. These learned compressions enable neural density estimationof the implicit likelihood via normalizing flows in a ten-dimensional parameterspace spanning cosmological $w$CDM, intrinsic alignment, and linear galaxy biasparameters, while marginalizing over baryonic, photometric redshift, and shearbias nuisances. To ensure robustness, we extensively validate our inferencepipeline using synthetic observations derived from both systematiccontaminations in our forward model and independent Buzzard galaxy catalogs.Our forecasts yield significant improvements in cosmological parameterconstraints, achieving $2-3\times$ higher figures of merit in the $\Omega_m -S_8$ plane relative to our implementation of baseline two-point statistics andeffectively breaking parameter degeneracies through probe combination. Theseresults demonstrate the potential of SBI analyses powered by deep learning forupcoming Stage-IV wide-field imaging surveys.</description><author>A. Thomsen, J. Bucko, T. Kacprzak, V. Ajani, J. Fluri, A. Refregier, D. Anbajagane, F. J. Castander, A. Ferté, M. Gatti, N. Jeffrey, A. Alarcon, A. Amon, K. Bechtol, M. R. Becker, G. M. Bernstein, A. Campos, A. Carnero Rosell, C. Chang, R. Chen, A. Choi, M. Crocce, C. Davis, J. DeRose, S. Dodelson, C. Doux, K. Eckert, J. Elvin-Poole, S. Everett, P. Fosalba, D. Gruen, I. Harrison, K. Herner, E. M. Huff, M. Jarvis, N. Kuropatkin, P. -F. Leget, N. MacCrann, J. McCullough, J. Myles, A. Navarro-Alsina, S. Pandey, A. Porredon, J. Prat, M. Raveri, M. Rodriguez-Monroy, R. P. Rollins, A. Roodman, E. S. Rykoff, C. Sánchez, L. F. Secco, E. Sheldon, T. Shin, M. A. Troxel, I. Tutusaus, T. N. Varga, N. Weaverdyck, R. H. Wechsler, B. Yanny, B. Yin, Y. Zhang, J. Zuntz, S. Allam, F. Andrade-Oliveira, D. Ba</author><pubDate>Thu, 06 Nov 2025 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04681v1</guid></item><item><title>TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models</title><link>http://arxiv.org/abs/2505.23769v2</link><description>Image-text models excel at image-level tasks but struggle with detailedvisual understanding. While these models provide strong visual-languagealignment, segmentation models like SAM2 offer precise spatial boundaries forobjects. To this end, we propose TextRegion, a simple, effective, andtraining-free framework that combines the strengths of image-text models andSAM2 to generate powerful text-aligned region tokens. These tokens enabledetailed visual understanding while preserving open-vocabulary capabilities.They can be directly applied to various downstream tasks, including open-worldsemantic segmentation, referring expression comprehension, and grounding. Weconduct extensive evaluations and consistently achieve superior or competitiveperformance compared to state-of-the-art training-free methods. Additionally,our framework is compatible with many image-text models, making it highlypractical and easily extensible as stronger models emerge. Code is availableat: https://github.com/avaxiao/TextRegion.</description><author>Yao Xiao, Qiqian Fu, Heyi Tao, Yuqun Wu, Zhen Zhu, Derek Hoiem</author><pubDate>Thu, 06 Nov 2025 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.23769v2</guid></item><item><title>Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping</title><link>http://arxiv.org/abs/2511.04680v1</link><description>Automatic image cropping is a method for maximizing the human-perceivedquality of cropped regions in photographs. Although several works have proposedtechniques for producing singular crops, little work has addressed the problemof producing multiple, distinct crops with aesthetic appeal. In this paper, wemotivate the problem with a discussion on modern social media applications,introduce a dataset of 277 relevant images and human labels, and evaluate theefficacy of several single-crop models with an image partitioning algorithm asa pre-processing step. The dataset is available athttps://github.com/RafeLoya/carousel.</description><author>Rafe Loya, Andrew Hamara, Benjamin Estell, Benjamin Kilpatrick, Andrew C. Freeman</author><pubDate>Thu, 06 Nov 2025 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04680v1</guid></item><item><title>GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction</title><link>http://arxiv.org/abs/2511.04679v1</link><description>Humanoid robots are expected to operate in human-centered environments wheresafe and natural physical interaction is essential. However, most recentreinforcement learning (RL) policies emphasize rigid tracking and suppressexternal forces. Existing impedance-augmented approaches are typicallyrestricted to base or end-effector control and focus on resisting extremeforces rather than enabling compliance. We introduce GentleHumanoid, aframework that integrates impedance control into a whole-body motion trackingpolicy to achieve upper-body compliance. At its core is a unified spring-basedformulation that models both resistive contacts (restoring forces when pressingagainst surfaces) and guiding contacts (pushes or pulls sampled from humanmotion data). This formulation ensures kinematically consistent forces acrossthe shoulder, elbow, and wrist, while exposing the policy to diverseinteraction scenarios. Safety is further supported through task-adjustableforce thresholds. We evaluate our approach in both simulation and on theUnitree G1 humanoid across tasks requiring different levels of compliance,including gentle hugging, sit-to-stand assistance, and safe objectmanipulation. Compared to baselines, our policy consistently reduces peakcontact forces while maintaining task success, resulting in smoother and morenatural interactions. These results highlight a step toward humanoid robotsthat can safely and effectively collaborate with humans and handle objects inreal-world environments.</description><author>Qingzhou Lu, Yao Feng, Baiyu Shi, Michael Piseno, Zhenan Bao, C. Karen Liu</author><pubDate>Thu, 06 Nov 2025 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04679v1</guid></item><item><title>Residual Kolmogorov-Arnold Network for Enhanced Deep Learning</title><link>http://arxiv.org/abs/2410.05500v4</link><description>Despite their immense success, deep convolutional neural networks (CNNs) canbe difficult to optimize and costly to train due to hundreds of layers withinthe network depth. Conventional convolutional operations are fundamentallylimited by their linear nature along with fixed activations, where many layersare needed to learn meaningful patterns in data. Because of the sheer size ofthese networks, this approach is simply computationally inefficient, and posesoverfitting or gradient explosion risks, especially in small datasets. As aresult, we introduce a "plug-in" module, called Residual Kolmogorov-ArnoldNetwork (RKAN). Our module is highly compact, so it can be easily added intoany stage (level) of traditional deep networks, where it learns to integratesupportive polynomial feature transformations to existing convolutionalframeworks. RKAN offers consistent improvements over baseline models indifferent vision tasks and widely tested benchmarks, accomplishing cutting-edgeperformance on them.</description><author>Ray Congrui Yu, Sherry Wu, Jiang Gui</author><pubDate>Thu, 06 Nov 2025 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05500v4</guid></item><item><title>Tracking and Understanding Object Transformations</title><link>http://arxiv.org/abs/2511.04678v1</link><description>Real-world objects frequently undergo state transformations. From an applebeing cut into pieces to a butterfly emerging from its cocoon, tracking throughthese changes is important for understanding real-world objects and dynamics.However, existing methods often lose track of the target object aftertransformation, due to significant changes in object appearance. To addressthis limitation, we introduce the task of Track Any State: tracking objectsthrough transformations while detecting and describing state changes,accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, wepresent TubeletGraph, a zero-shot system that recovers missing objects aftertransformation and maps out how object states are evolving over time.TubeletGraph first identifies potentially overlooked tracks, and determineswhether they should be integrated based on semantic and proximity priors. Then,it reasons about the added tracks and generates a state graph describing eachobserved transformation. TubeletGraph achieves state-of-the-art trackingperformance under transformations, while demonstrating deeper understanding ofobject transformations and promising capabilities in temporal grounding andsemantic reasoning for complex object transformations. Code, additionalresults, and the benchmark dataset are available athttps://tubelet-graph.github.io.</description><author>Yihong Sun, Xinyu Yang, Jennifer J. Sun, Bharath Hariharan</author><pubDate>Thu, 06 Nov 2025 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04678v1</guid></item><item><title>InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation</title><link>http://arxiv.org/abs/2511.04675v1</link><description>We introduce InfinityStar, a unified spacetime autoregressive framework forhigh-resolution image and dynamic video synthesis. Building on the recentsuccess of autoregressive modeling in both vision and language, our purelydiscrete approach jointly captures spatial and temporal dependencies within asingle architecture. This unified design naturally supports a variety ofgeneration tasks such as text-to-image, text-to-video, image-to-video, and longinteractive video synthesis via straightforward temporal autoregression.Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,outperforming all autoregressive models by large margins, even surpassing somediffusion competitors like HunyuanVideo. Without extra optimizations, our modelgenerates a 5s, 720p video approximately 10x faster than leadingdiffusion-based methods. To our knowledge, InfinityStar is the first discreteautoregressive video generator capable of producing industrial level 720pvideos. We release all code and models to foster further research in efficient,high-quality video generation.</description><author>Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu, Xing Wang, Yi Jiang, Bingyue Peng, Zehuan Yuan</author><pubDate>Thu, 06 Nov 2025 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04675v1</guid></item><item><title>Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</title><link>http://arxiv.org/abs/2506.15680v2</link><description>Modeling the dynamics of deformable objects is challenging due to theirdiverse physical properties and the difficulty of estimating states fromlimited visual information. We address these challenges with a neural dynamicsframework that combines object particles and spatial grids in a hybridrepresentation. Our particle-grid model captures global shape and motioninformation while predicting dense particle movements, enabling the modeling ofobjects with varied shapes and materials. Particles represent object shapes,while the spatial grid discretizes the 3D space to ensure spatial continuityand enhance learning efficiency. Coupled with Gaussian Splattings for visualrendering, our framework achieves a fully learning-based digital twin ofdeformable objects and generates 3D action-conditioned videos. Throughexperiments, we demonstrate that our model learns the dynamics of diverseobjects -- such as ropes, cloths, stuffed animals, and paper bags -- fromsparse-view RGB-D recordings of robot-object interactions, while alsogeneralizing at the category level to unseen instances. Our approachoutperforms state-of-the-art learning-based and physics-based simulators,particularly in scenarios with limited camera views. Furthermore, we showcasethe utility of our learned models in model-based planning, enablinggoal-conditioned object manipulation across a range of tasks. The project pageis available at https://kywind.github.io/pgnd .</description><author>Kaifeng Zhang, Baoyu Li, Kris Hauser, Yunzhu Li</author><pubDate>Thu, 06 Nov 2025 18:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.15680v2</guid></item><item><title>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences</title><link>http://arxiv.org/abs/2509.16189v2</link><description>When do machine learning systems fail to generalize, and what mechanismscould improve their generalization? Here, we draw inspiration from cognitivescience to argue that one weakness of parametric machine learning systems istheir failure to exhibit latent learning -- learning information that is notrelevant to the task at hand, but that might be useful in a future task. Weshow how this perspective links failures ranging from the reversal curse inlanguage modeling to new findings on agent-based navigation. We then highlighthow cognitive science points to episodic memory as a potential part of thesolution to these issues. Correspondingly, we show that a system with an oracleretrieval mechanism can use learning experiences more flexibly to generalizebetter across many of these challenges. We also identify some of the essentialcomponents for effectively using retrieval, including the importance ofwithin-example in-context learning for acquiring the ability to use informationacross retrieved examples. In summary, our results illustrate one possiblecontributor to the relative data inefficiency of current machine learningsystems compared to natural intelligence, and help to understand how retrievalmethods can complement parametric learning to improve generalization. We closeby discussing some of the links between these findings and prior results incognitive science and neuroscience, and the broader implications.</description><author>Andrew Kyle Lampinen, Martin Engelcke, Yuxuan Li, Arslan Chaudhry, James L. McClelland</author><pubDate>Thu, 06 Nov 2025 18:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.16189v2</guid></item><item><title>X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations</title><link>http://arxiv.org/abs/2511.04671v1</link><description>Human videos can be recorded quickly and at scale, making them an appealingsource of training data for robot learning. However, humans and robots differfundamentally in embodiment, resulting in mismatched action execution. Directkinematic retargeting of human hand motion can therefore produce actions thatare physically infeasible for robots. Despite these low-level differences,human demonstrations provide valuable motion cues about how to manipulate andinteract with objects. Our key idea is to exploit the forward diffusionprocess: as noise is added to actions, low-level execution differences fadewhile high-level task guidance is preserved. We present X-Diffusion, aprincipled framework for training diffusion policies that maximally leverageshuman data without learning dynamically infeasible motions. X-Diffusion firsttrains a classifier to predict whether a noisy action is executed by a human orrobot. Then, a human action is incorporated into policy training only afteradding sufficient noise such that the classifier cannot discern its embodiment.Actions consistent with robot execution supervise fine-grained denoising at lownoise levels, while mismatched human actions provide only coarse guidance athigher noise levels. Our experiments show that naive co-training underexecution mismatches degrades policy performance, while X-Diffusionconsistently improves it. Across five manipulation tasks, X-Diffusion achievesa 16% higher average success rate than the best baseline. The project websiteis available at https://portal-cornell.github.io/X-Diffusion/.</description><author>Maximus A. Pace, Prithwish Dan, Chuanruo Ning, Atiksh Bhardwaj, Audrey Du, Edward W. Duan, Wei-Chiu Ma, Kushal Kedia</author><pubDate>Thu, 06 Nov 2025 18:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04671v1</guid></item><item><title>Cambrian-S: Towards Spatial Supersensing in Video</title><link>http://arxiv.org/abs/2511.04670v1</link><description>We argue that progress in true multimodal intelligence calls for a shift fromreactive, task-driven systems and brute-force long context towards a broaderparadigm of supersensing. We frame spatial supersensing as four stages beyondlinguistic-only understanding: semantic perception (naming what is seen),streaming event cognition (maintaining memory across continuous experiences),implicit 3D spatial cognition (inferring the world behind pixels), andpredictive world modeling (creating internal models that filter and organizeinformation). Current benchmarks largely test only the early stages, offeringnarrow coverage of spatial cognition and rarely challenging models in ways thatrequire true world modeling. To drive progress in spatial supersensing, wepresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatialrecall) and VSC (continual visual spatial counting). These tasks requirearbitrarily long video inputs yet are resistant to brute-force contextexpansion. We then test data scaling limits by curating VSI-590K and trainingCambrian-S, achieving +30% absolute improvement on VSI-Bench withoutsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,indicating that scale alone is insufficient for spatial supersensing. Wepropose predictive sensing as a path forward, presenting a proof-of-concept inwhich a self-supervised next-latent-frame predictor leverages surprise(prediction error) to drive memory and event segmentation. On VSI-SUPER, thisapproach substantially outperforms leading proprietary baselines, showing thatspatial supersensing requires models that not only see but also anticipate,select, and organize experience.</description><author>Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie</author><pubDate>Thu, 06 Nov 2025 18:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04670v1</guid></item><item><title>SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</title><link>http://arxiv.org/abs/2511.04668v1</link><description>Despite impressive high-level video comprehension, multimodal language modelsstruggle with spatial reasoning across time and space. While current spatialtraining approaches rely on real-world video data, obtaining diverse footagewith precise spatial annotations remains a bottleneck. To alleviate thisbottleneck, we present SIMS-V -- a systematic data-generation framework thatleverages the privileged information of 3D simulators to create spatially-richvideo training data for multimodal language models. Using this framework, weinvestigate which properties of simulated data drive effective real-worldtransfer through systematic ablations of question types, mixes, and scales. Weidentify a minimal set of three question categories (metric measurement,perspective-dependent reasoning, and temporal tracking) that prove mosteffective for developing transferable spatial intelligence, outperformingcomprehensive coverage despite using fewer question types. These insightsenable highly efficient training: our 7B-parameter video LLM fine-tuned on just25K simulated examples outperforms the larger 72B baseline and achievescompetitive performance with proprietary models on rigorous real-world spatialreasoning benchmarks. Our approach demonstrates robust generalization,maintaining performance on general video understanding while showingsubstantial improvements on embodied and real-world spatial tasks.</description><author>Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie</author><pubDate>Thu, 06 Nov 2025 18:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04668v1</guid></item><item><title>Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches</title><link>http://arxiv.org/abs/2511.04667v1</link><description>This study evaluates a 40-item mathematics placement examination administeredto 198 students using a multi-method framework combining Classical Test Theory,machine learning, and unsupervised clustering. Classical Test Theory analysisreveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$)while 30\% demonstrate poor discrimination ($D &lt; 0.20$) requiring replacement.Question 6 (Graph Interpretation) emerges as the examination's most powerfuldiscriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVAF-statistic ($F = 4609.1$), and maximum Random Forest feature importance(0.206), accounting for 20.6\% of predictive power. Machine learning algorithmsdemonstrate exceptional performance, with Random Forest and Gradient Boostingachieving 97.5\% and 96.0\% cross-validation accuracy. K-means clusteringidentifies a natural binary competency structure with a boundary at 42.5\%,diverging from the institutional threshold of 55\% and suggesting potentialoverclassification into remedial categories. The two-cluster solution exhibitsexceptional stability (bootstrap ARI = 0.855) with perfect lower-clusterpurity. Convergent evidence across methods supports specific refinements:replace poorly discriminating items, implement a two-stage assessment, andintegrate Random Forest predictions with transparency mechanisms. Thesefindings demonstrate that multi-method integration provides a robust empiricalfoundation for evidence-based mathematics placement optimization.</description><author>Julian D. Allagan, Dasia A. Singleton, Shanae N. Perry, Gabrielle C. Morgan, Essence A. Morgan</author><pubDate>Thu, 06 Nov 2025 18:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04667v1</guid></item><item><title>Forgetting is Everywhere</title><link>http://arxiv.org/abs/2511.04666v1</link><description>A fundamental challenge in developing general learning algorithms is theirtendency to forget past knowledge when adapting to new data. Addressing thisproblem requires a principled understanding of forgetting; yet, despite decadesof study, no unified definition has emerged that provides insights into theunderlying dynamics of learning. We propose an algorithm- and task-agnostictheory that characterises forgetting as a lack of self-consistency in alearner's predictive distribution over future experiences, manifesting as aloss of predictive information. Our theory naturally yields a general measureof an algorithm's propensity to forget. To validate the theory, we design acomprehensive set of experiments that span classification, regression,generative modelling, and reinforcement learning. We empirically demonstratehow forgetting is present across all learning settings and plays a significantrole in determining learning efficiency. Together, these results establish aprincipled understanding of forgetting and lay the foundation for analysing andimproving the information retention capabilities of general learningalgorithms.</description><author>Ben Sanati, Thomas L. Lee, Trevor McInroe, Aidan Scannell, Nikolay Malkin, David Abel, Amos Storkey</author><pubDate>Thu, 06 Nov 2025 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04666v1</guid></item><item><title>Distillation versus Contrastive Learning: How to Train Your Rerankers</title><link>http://arxiv.org/abs/2507.08336v3</link><description>Training effective text rerankers is crucial for information retrieval. Twostrategies are widely used: contrastive learning (optimizing directly onground-truth labels) and knowledge distillation (transferring knowledge from alarger reranker). While both have been studied extensively, a clear comparisonof their effectiveness for training cross-encoder rerankers under practicalconditions is needed. This paper empirically compares these strategies by training rerankers ofdifferent sizes (0.5B, 1.5B, 3B, 7B) and architectures (Transformer, Recurrent)using both methods on the same data, with a strong contrastive learning modelacting as the distillation teacher. Our results show that knowledgedistillation generally yields better in-domain and out-of-domain rankingperformance than contrastive learning when distilling from a more performantteacher model. This finding is consistent across student model sizes andarchitectures. However, distilling from a teacher of the same capacity does notprovide the same advantage, particularly for out-of-domain tasks. Thesefindings offer practical guidance for choosing a training strategy based onavailable teacher models. We recommend using knowledge distillation to trainsmaller rerankers if a larger, more performant teacher is accessible; in itsabsence, contrastive learning remains a robust baseline. Our codeimplementation is made available to facilitate reproducbility.</description><author>Zhichao Xu, Zhiqi Huang, Shengyao Zhuang, Vivek Srikumar</author><pubDate>Thu, 06 Nov 2025 18:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.08336v3</guid></item><item><title>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</title><link>http://arxiv.org/abs/2511.04665v1</link><description>Robotic manipulation policies are advancing rapidly, but their directevaluation in the real world remains costly, time-consuming, and difficult toreproduce, particularly for tasks involving deformable objects. Simulationprovides a scalable and systematic alternative, yet existing simulators oftenfail to capture the coupled visual and physical complexity of soft-bodyinteractions. We present a real-to-sim policy evaluation framework thatconstructs soft-body digital twins from real-world videos and renders robots,objects, and environments with photorealistic fidelity using 3D GaussianSplatting. We validate our approach on representative deformable manipulationtasks, including plush toy packing, rope routing, and T-block pushing,demonstrating that simulated rollouts correlate strongly with real-worldexecution performance and reveal key behavioral patterns of learned policies.Our results suggest that combining physics-informed reconstruction withhigh-quality rendering enables reproducible, scalable, and accurate evaluationof robotic manipulation policies. Website: https://real2sim-eval.github.io/</description><author>Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, Yunzhu Li</author><pubDate>Thu, 06 Nov 2025 18:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04665v1</guid></item><item><title>VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks</title><link>http://arxiv.org/abs/2511.04662v1</link><description>LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), butthey cannot reliably verify their own logic. Even when they reach correctanswers, the underlying reasoning may be flawed, undermining trust inhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, aneuro-symbolic method that extracts and verifies formal logical arguments fromCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-orderlogic and identifies premises that ground the argument in source context,commonsense knowledge, or prior reasoning steps. The symbolic representationenables automated solvers to verify logical validity while the NL premisesallow humans and systems to identify ungrounded or fallacious reasoning steps.Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoTeffectively identifies flawed reasoning, and serves as a strong predictor offinal answer correctness. We also leverage VeriCoT's verification signal for(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) onVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with directpreference optimization (DPO) using verification-based pairwise rewards,further improving reasoning validity and accuracy.</description><author>Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel, Sapana Chaudhary, Benjamin Kiesl-Reiter, Huzefa Rangwala</author><pubDate>Thu, 06 Nov 2025 18:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04662v1</guid></item><item><title>Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions</title><link>http://arxiv.org/abs/2509.08217v2</link><description>For machine learning datasets to accurately represent diverse opinions in apopulation, they must preserve variation in data labels while filtering outspam or low-quality responses. How can we balance annotator reliability andrepresentation? We empirically evaluate how a range of heuristics for annotatorfiltering affect the preservation of variation on subjective tasks. We findthat these methods, designed for contexts in which variation from a singleground-truth label is considered noise, often remove annotators who disagreeinstead of spam annotators, introducing suboptimal tradeoffs between accuracyand label diversity. We find that conservative settings for annotator removal(&lt;5%) are best, after which all tested methods increase the mean absolute errorfrom the true average label. We analyze performance on synthetic spam toobserve that these methods often assume spam annotators are more random thanreal spammers tend to be: most spammers are distributionally indistinguishablefrom real annotators, and the minority that are distinguishable tend to giverelatively fixed answers, not random ones. Thus, tasks requiring thepreservation of variation reverse the intuition of existing spam filteringmethods: spammers tend to be less random than non-spammers, so metrics thatassume variation is spam fare worse. These results highlight the need for spamremoval methods that account for label diversity.</description><author>Eve Fleisig, Matthias Orlikowski, Philipp Cimiano, Dan Klein</author><pubDate>Thu, 06 Nov 2025 18:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.08217v2</guid></item><item><title>CREA: A Collaborative Multi-Agent Framework for Creative Image Editing and Generation</title><link>http://arxiv.org/abs/2504.05306v2</link><description>Creativity in AI imagery remains a fundamental challenge, requiring not onlythe generation of visually compelling content but also the capacity to addnovel, expressive, and artistically rich transformations to images. Unlikeconventional editing tasks that rely on direct prompt-based modifications,creative image editing requires an autonomous, iterative approach that balancesoriginality, coherence, and artistic intent. To address this, we introduceCREA, a novel multi-agent collaborative framework that mimics the humancreative process. Our framework leverages a team of specialized AI agents whodynamically collaborate to conceptualize, generate, critique, and enhanceimages. Through extensive qualitative and quantitative evaluations, wedemonstrate that CREA significantly outperforms state-of-the-art methods indiversity, semantic alignment, and creative transformation. To the best of ourknowledge, this is the first work to introduce the task of creative editing.</description><author>Kavana Venkatesh, Connor Dunlop, Pinar Yanardag</author><pubDate>Thu, 06 Nov 2025 18:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05306v2</guid></item><item><title>Nowcast3D: Reliable precipitation nowcasting via gray-box learning</title><link>http://arxiv.org/abs/2511.04659v1</link><description>Extreme precipitation nowcasting demands high spatiotemporal fidelity andextended lead times, yet existing approaches remain limited. Numerical WeatherPrediction (NWP) and its deep-learning emulations are too slow and coarse forrapidly evolving convection, while extrapolation and purely data-driven modelssuffer from error accumulation and excessive smoothing. Hybrid 2D radar-basedmethods discard crucial vertical information, preventing accuratereconstruction of height-dependent dynamics. We introduce a gray-box, fullythree-dimensional nowcasting framework that directly processes volumetric radarreflectivity and couples physically constrained neural operators withdatadriven learning. The model learns vertically varying 3D advection fieldsunder a conservative advection operator, parameterizes spatially varyingdiffusion, and introduces a Brownian-motion--inspired stochastic term torepresent unresolved motions. A residual branch captures small-scale convectiveinitiation and microphysical variability, while a diffusion-based stochasticmodule estimates uncertainty. The framework achieves more accurate forecasts upto three-hour lead time across precipitation regimes and ranked first in 57\%of cases in a blind evaluation by 160 meteorologists. By restoring full 3Ddynamics with physical consistency, it offers a scalable and robust pathway forskillful and reliable nowcasting of extreme precipitation.</description><author>Huaguan Chen, Wei Han, Haofei Sun, Ning Lin, Xingtao Song, Yunfan Yang, Jie Tian, Yang Liu, Ji-Rong Wen, Xiaoye Zhang, Xueshun Shen, Hao Sun</author><pubDate>Thu, 06 Nov 2025 18:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04659v1</guid></item><item><title>Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts</title><link>http://arxiv.org/abs/2511.04655v1</link><description>Robust benchmarks are crucial for evaluating Multimodal Large Language Models(MLLMs). Yet we find that models can ace many multimodal benchmarks withoutstrong visual understanding, instead exploiting biases, linguistic priors, andsuperficial patterns. This is especially problematic for vision-centricbenchmarks that are meant to require visual inputs. We adopt a diagnosticprinciple for benchmark design: if a benchmark can be gamed, it will be.Designers should therefore try to ``game'' their own benchmarks first, usingdiagnostic and debiasing procedures to systematically identify and mitigatenon-visual biases. Effective diagnosis requires directly ``training on the testset'' -- probing the released test set for its intrinsic, exploitable patterns. We operationalize this standard with two components. First, we diagnosebenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.Our primary diagnostic tool involves fine-tuning a powerful Large LanguageModel via $k$-fold cross-validation on exclusively the non-visual, textualinputs of the test set to reveal shortcut performance and assign each sample abias score $s(x)$. We complement this with a lightweight Random Forest-baseddiagnostic operating on hand-crafted features for fast, interpretable auditing.Second, we debias benchmarks by filtering high-bias samples using an``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to fourbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasivenon-visual biases. As a case study, we apply our full framework to createVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a widervision-blind performance gap than the original.</description><author>Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie</author><pubDate>Thu, 06 Nov 2025 18:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04655v1</guid></item><item><title>Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2511.04654v1</link><description>Chain-of-Thought (CoT) prompting is a key technique for enabling complexreasoning in large language models. However, generating full, fixed-lengthrationales is computationally wasteful, inflating both token usage and latency.We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-freedecoding algorithm that adaptively halts rationale generation. LEASH monitorstwo intrinsic signals: the slope of token-level entropy and the improvement inthe top-logit margin. It terminates the generation once both signals plateau,indicating the model has reached a stable reasoning state. Across fourinstruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reducesaverage token generation by 30--35% and latency by 27%, while incurring a 10p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires noadditional training or supervision, offering a simple and efficient alternativeto CoT decoding.</description><author>Mohammad Atif Quamar, Mohammad Areeb</author><pubDate>Thu, 06 Nov 2025 18:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04654v1</guid></item><item><title>TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning</title><link>http://arxiv.org/abs/2511.04653v1</link><description>Federated learning (FL) offers new opportunities in machine learning,particularly in addressing data privacy concerns. In contrast to conventionalevent-based federated learning, time-triggered federated learning (TT-Fed), asa general form of both asynchronous and synchronous FL, clusters users intodifferent tiers based on fixed time intervals. However, the FL network consistsof a growing number of user devices with limited wireless bandwidth,consequently magnifying issues such as stragglers and communication overhead.In this paper, we introduce adaptive model pruning to wireless TT-Fed systemsand study the problem of jointly optimizing the pruning ratio and bandwidthallocation to minimize the training loss while ensuring minimal learninglatency. To answer this question, we perform convergence analysis on thegradient l_2 norm of the TT-Fed model based on model pruning. Based on theobtained convergence upper bound, a joint optimization problem of pruning ratioand wireless bandwidth is formulated to minimize the model training loss undera given delay threshold. Then, we derive closed-form solutions for wirelessbandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. Thesimulation results show that model pruning could reduce the communication costby 40% while maintaining the model performance at the same level.</description><author>Xinlu Zhang, Yansha Deng, Toktam Mahmoodi</author><pubDate>Thu, 06 Nov 2025 18:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04653v1</guid></item><item><title>Polarization-resolved imaging improves eye tracking</title><link>http://arxiv.org/abs/2511.04652v1</link><description>Polarization-resolved near-infrared imaging adds a useful optical contrastmechanism to eye tracking by measuring the polarization state of lightreflected by ocular tissues in addition to its intensity. In this paper wedemonstrate how this contrast can be used to enable eye tracking. Specifically,we demonstrate that a polarization-enabled eye tracking (PET) system composedof a polarization--filter--array camera paired with a linearly polarizednear-infrared illuminator can reveal trackable features across the sclera andgaze-informative patterns on the cornea, largely absent in intensity-onlyimages. Across a cohort of 346 participants, convolutional neural network basedmachine learning models trained on data from PET reduced the median95th-percentile absolute gaze error by 10--16\% relative to capacity-matchedintensity baselines under nominal conditions and in the presence of eyelidocclusions, eye-relief changes, and pupil-size variation. These results linklight--tissue polarization effects to practical gains in human--computerinteraction and position PET as a simple, robust sensing modality for futurewearable devices.</description><author>Mantas Žurauskas, Tom Bu, Sanaz Alali, Beyza Kalkanli, Derek Shi, Fernando Alamos, Gauresh Pandit, Christopher Mei, Ali Behrooz, Ramin Mirjalili, Dave Stronks, Alexander Fix, Dmitri Model</author><pubDate>Thu, 06 Nov 2025 18:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04652v1</guid></item><item><title>CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation</title><link>http://arxiv.org/abs/2509.07325v2</link><description>The National Comprehensive Cancer Network (NCCN) provides evidence-basedguidelines for cancer treatment. Translating complex patient presentations intoguideline-compliant treatment recommendations is time-intensive, requiresspecialized expertise, and is prone to error. Advances in large language model(LLM) capabilities promise to reduce the time required to generate treatmentrecommendations and improve accuracy. We present an LLM agent-based approach toautomatically generate guideline-concordant treatment trajectories for patientswith non-small cell lung cancer (NSCLC). Our contributions are threefold.First, we construct a novel longitudinal dataset of 121 cases of NSCLC patientsthat includes clinical encounters, diagnostic results, and medical histories,each expertly annotated with the corresponding NCCN guideline trajectories byboard-certified oncologists. Second, we demonstrate that existing LLMs possessdomain-specific knowledge that enables high-quality proxy benchmark generationfor both model development and evaluation, achieving strong correlation(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.Third, we develop a hybrid approach combining expensive human annotations withmodel consistency information to create both the agent framework that predictsthe relevant guidelines for a patient, as well as a meta-classifier thatverifies prediction accuracy with calibrated confidence scores for treatmentrecommendations (AUROC=0.800), a critical capability for communicating theaccuracy of outputs, custom-tailoring tradeoffs in performance, and supportingregulatory compliance. This work establishes a framework for clinically viableLLM-based guideline adherence systems that balance accuracy, interpretability,and regulatory requirements while reducing annotation costs, providing ascalable pathway toward automated clinical decision support.</description><author>Alyssa Unell, Noel C. F. Codella, Sam Preston, Peniel Argaw, Wen-wai Yim, Zelalem Gero, Cliff Wong, Rajesh Jena, Eric Horvitz, Amanda K. Hall, Ruican Rachel Zhong, Jiachen Li, Shrey Jain, Mu Wei, Matthew Lungren, Hoifung Poon</author><pubDate>Thu, 06 Nov 2025 18:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07325v2</guid></item><item><title>Optimal Inference Schedules for Masked Diffusion Models</title><link>http://arxiv.org/abs/2511.04647v1</link><description>A major bottleneck of standard auto-regressive large language models is thattheir inference process is inherently sequential, resulting in very long andcostly inference times. To circumvent this, practitioners proposed a class oflanguage models called diffusion language models, of which the masked diffusionmodel (MDM) is the most successful. The MDM is able to sample tokensout-of-order and, ostensibly, many tokens at once and in parallel. However,there is very limited rigorous understanding of how much parallel samplingthese models can perform without noticeable degradation in their samplingperformance. Prior work of Li and Cai obtained some preliminary bounds, butthese are not tight for many natural classes of distributions. In this work, wegive a new, exact characterization of the expected divergence between the truedistribution and the sampled distribution, for any distribution and anyunmasking schedule for the sampler, showing an elegant connection to the theoryof univariate function approximation. By leveraging this connection, we then attain a number of novel lower andupper bounds for this problem. While the connection to function approximationin principle gives the optimal unmasking schedule for any distribution, we showthat it is in general impossible to compete with it without strong a prioriknowledge of the distribution, even in seemingly benign settings. However, wealso demonstrate new upper bounds and new sampling schedules in terms ofwell-studied information-theoretic properties of the base distribution, namely,its total correlation and dual total correlation, which show that in somenatural settings, one can sample in $O(log n)$ steps without any visible lossin performance, where $n$ is the total sequence length.</description><author>Sitan Chen, Kevin Cong, Jerry Li</author><pubDate>Thu, 06 Nov 2025 18:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04647v1</guid></item><item><title>DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2511.04646v1</link><description>Cooperative multi-agent planning requires agents to make joint decisions withpartial information and limited communication. Coordination at the trajectorylevel often fails, as small deviations in timing or movement cascade intoconflicts. Symbolic planning mitigates this challenge by raising the level ofabstraction and providing a minimal vocabulary of actions that enablesynchronization and collective progress. We present DR. WELL, a decentralizedneurosymbolic framework for cooperative multi-agent planning. Cooperationunfolds through a two-phase negotiation protocol: agents first proposecandidate roles with reasoning and then commit to a joint allocation underconsensus and environment constraints. After commitment, each agentindependently generates and executes a symbolic plan for its role withoutrevealing detailed trajectories. Plans are grounded in execution outcomes via ashared world model that encodes the current state and is updated as agents act.By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoidsbrittle step-level alignment and enables higher-level operations that arereusable, synchronizable, and interpretable. Experiments on cooperativeblock-push tasks show that agents adapt across episodes, with the dynamic worldmodel capturing reusable patterns and improving task completion rates andefficiency. Experiments on cooperative block-push tasks show that our dynamicworld model improves task completion and efficiency through negotiation andself-refinement, trading a time overhead for evolving, more efficientcollaboration strategies.</description><author>Narjes Nourzad, Hanqing Yang, Shiyu Chen, Carlee Joe-Wong</author><pubDate>Thu, 06 Nov 2025 18:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04646v1</guid></item><item><title>When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection</title><link>http://arxiv.org/abs/2511.04643v1</link><description>The proliferation of misinformation necessitates robust yet computationallyefficient fact verification systems. While current state-of-the-art approachesleverage Large Language Models (LLMs) for generating explanatory rationales,these methods face significant computational barriers and hallucination risksin real-world deployments. We present DeReC (Dense Retrieval Classification), alightweight framework that demonstrates how general-purpose text embeddings caneffectively replace autoregressive LLM-based approaches in fact verificationtasks. By combining dense retrieval with specialized classification, our systemachieves better accuracy while being significantly more efficient. DeReCoutperforms explanation-generating LLMs in efficiency, reducing runtime by 95%on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),showcasing its effectiveness across varying dataset sizes. On the RAWFCdataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-artmethod L-Defense (61.20%). Our results demonstrate that carefully engineeredretrieval-based systems can match or exceed LLM performance in specializedtasks while being significantly more practical for real-world deployment.</description><author>Alamgir Munir Qazi, John P. McCrae, Jamal Abdul Nasir</author><pubDate>Thu, 06 Nov 2025 18:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04643v1</guid></item><item><title>Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems</title><link>http://arxiv.org/abs/2511.04641v1</link><description>This paper is concerned with probabilistic techniques for forecastingdynamical systems described by partial differential equations (such as, forexample, the Navier-Stokes equations). In particular, it is investigating andcomparing various extensions to the flow matching paradigm that reduce thenumber of sampling steps. In this regard, it compares direct distillation,progressive distillation, adversarial diffusion distillation, Wasserstein GANsand rectified flows. Moreover, experiments are conducted on a set ofchallenging systems. In particular, we also address the challenge of directlypredicting 2D slices of large-scale 3D simulations, paving the way forefficient inflow generation for solvers.</description><author>Hans Harder, Abhijeet Vishwasrao, Luca Guastoni, Ricardo Vinuesa, Sebastian Peitz</author><pubDate>Thu, 06 Nov 2025 18:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04641v1</guid></item><item><title>Addressing divergent representations from causal interventions on neural networks</title><link>http://arxiv.org/abs/2511.04638v1</link><description>A common approach to mechanistic interpretability is to causally manipulatemodel representations via targeted interventions in order to understand whatthose representations encode. Here we ask whether such interventions createout-of-distribution (divergent) representations, and whether this raisesconcerns about how faithful their resulting explanations are to the targetmodel in its natural state. First, we demonstrate empirically that commoncausal intervention techniques often do shift internal representations awayfrom the natural distribution of the target model. Then, we provide atheoretical analysis of two classes of such divergences: `harmless' divergencesthat occur in the null-space of the weights and from covariance withinbehavioral decision boundaries, and `pernicious' divergences that activatehidden network pathways and cause dormant behavioral changes. Finally, in aneffort to mitigate the pernicious cases, we modify the Counterfactual Latent(CL) loss from Grant (2025) that regularizes interventions to remain closer tothe natural distributions, reducing the likelihood of harmful divergences whilepreserving the interpretive power of interventions. Together, these resultshighlight a path towards more reliable interpretability methods.</description><author>Satchel Grant, Simon Jerome Han, Alexa Tartaglini, Christopher Potts</author><pubDate>Thu, 06 Nov 2025 18:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04638v1</guid></item><item><title>SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</title><link>http://arxiv.org/abs/2511.03092v2</link><description>The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+context length support have resulted in increasing demands for on-chip memoryto support large KV caches. Techniques such as StreamingLLM and SnapKVdemonstrate how to control KV cache size while maintaining model accuracy. Yet,these techniques are not commonly used within industrial deployments usingframeworks like vLLM or SGLang. The reason is twofold: on one hand, the staticgraphs and continuous batching methodology employed by these frameworks make itdifficult to admit modifications to the standard multi-head attentionalgorithm, while on the other hand, the accuracy implications of suchtechniques on modern instruction-following and reasoning models are not wellunderstood, obfuscating the need for implementing these techniques. In thispaper, we explore these accuracy implications on Llama-3.1-8B-Instruct andDeepSeek-R1, and develop SnapStream, a KV cache compression method that can bedeployed at scale. We demonstrate the efficacy of SnapStream in a 16-waytensor-parallel deployment of DeepSeek-671B on SambaNova SN40L acceleratorsrunning at 128k context length and up to 1832 tokens per second in a realproduction setting. SnapStream enables $4\times$ improved on-chip memory usageand introduces minimal accuracy degradation on LongBench-v2, AIME24 andLiveCodeBench. To the best of our knowledge, this is the first implementationof sparse KV attention techniques deployed in a production inference systemwith static graphs and continuous batching.</description><author>Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar</author><pubDate>Thu, 06 Nov 2025 18:27:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.03092v2</guid></item><item><title>NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment</title><link>http://arxiv.org/abs/2511.04628v1</link><description>Video quality assessment (VQA) is vital for computer vision tasks, butexisting approaches face major limitations: full-reference (FR) metrics requireclean reference videos, and most no-reference (NR) models depend on training oncostly human opinion labels. Moreover, most opinion-unaware NR methods areimage-based, ignoring temporal context critical for video object detection. Inthis work, we present a scalable, streaming-based VQA model that is bothno-reference and opinion-unaware. Our model leverages synthetic degradations ofthe DAVIS dataset, training a temporal-aware convolutional architecture topredict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, withoutreferences at inference. We show that our streaming approach outperforms ourown image-based baseline by generalizing across diverse degradations,underscoring the value of temporal modeling for scalable VQA in real-worldvision systems. Additionally, we demonstrate that our model achieves highercorrelation with full-reference metrics compared to BRISQUE, a widely-usedopinion-aware image quality assessment baseline, validating the effectivenessof our temporal, opinion-unaware approach.</description><author>Kylie Cancilla, Alexander Moore, Amar Saini, Carmen Carrano</author><pubDate>Thu, 06 Nov 2025 18:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04628v1</guid></item><item><title>ODE approximation for the Adam algorithm: General and overparametrized setting</title><link>http://arxiv.org/abs/2511.04622v1</link><description>The Adam optimizer is currently presumably the most popular optimizationmethod in deep learning. In this article we develop an ODE based method tostudy the Adam optimizer in a fast-slow scaling regime. For fixed momentumparameters and vanishing step-sizes, we show that the Adam algorithm is anasymptotic pseudo-trajectory of the flow of a particular vector field, which isreferred to as the Adam vector field. Leveraging properties of asymptoticpseudo-trajectories, we establish convergence results for the Adam algorithm.In particular, in a very general setting we show that if the Adam algorithmconverges, then the limit must be a zero of the Adam vector field, rather thana local minimizer or critical point of the objective function. In contrast, in the overparametrized empirical risk minimization setting, theAdam algorithm is able to locally find the set of minima. Specifically, we showthat in a neighborhood of the global minima, the objective function serves as aLyapunov function for the flow induced by the Adam vector field. As aconsequence, if the Adam algorithm enters a neighborhood of the global minimainfinitely often, it converges to the set of global minima.</description><author>Steffen Dereich, Arnulf Jentzen, Sebastian Kassing</author><pubDate>Thu, 06 Nov 2025 18:15:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04622v1</guid></item><item><title>Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications</title><link>http://arxiv.org/abs/2509.08604v2</link><description>Large Language Models (LLMs) have demonstrated significant potential inmedicine. To date, LLMs have been widely applied to tasks such as diagnosticassistance, medical question answering, and clinical information synthesis.However, a key open question remains: to what extent do LLMs memorize medicaltraining data. In this study, we present the first comprehensive evaluation ofmemorization of LLMs in medicine, assessing its prevalence (how frequently itoccurs), characteristics (what is memorized), volume (how much content ismemorized), and potential downstream impacts (how memorization may affectmedical applications). We systematically analyze common adaptation scenarios:(1) continued pretraining on medical corpora, (2) fine-tuning on standardmedical benchmarks, and (3) fine-tuning on real-world clinical data, includingover 13,000 unique inpatient records from Yale New Haven Health System. Theresults demonstrate that memorization is prevalent across all adaptationscenarios and significantly higher than reported in the general domain.Memorization affects both the development and adoption of LLMs in medicine andcan be categorized into three types: beneficial (e.g., accurate recall ofclinical guidelines and biomedical references), uninformative (e.g., repeateddisclaimers or templated medical document language), and harmful (e.g.,regeneration of dataset-specific or sensitive clinical content). Based on thesefindings, we offer practical recommendations to facilitate beneficialmemorization that enhances domain-specific reasoning and factual accuracy,minimize uninformative memorization to promote deeper learning beyondsurface-level patterns, and mitigate harmful memorization to prevent theleakage of sensitive or identifiable patient information.</description><author>Anran Li, Lingfei Qian, Mengmeng Du, Yu Yin, Yan Hu, Zihao Sun, Yihang Fu, Erica Stutz, Xuguang Ai, Qianqian Xie, Rui Zhu, Jimin Huang, Yifan Yang, Siru Liu, Yih-Chung Tham, Lucila Ohno-Machado, Hyunghoon Cho, Zhiyong Lu, Hua Xu, Qingyu Chen</author><pubDate>Thu, 06 Nov 2025 18:15:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.08604v2</guid></item><item><title>Dynamic causal discovery in Alzheimer's disease through latent pseudotime modelling</title><link>http://arxiv.org/abs/2511.04619v1</link><description>The application of causal discovery to diseases like Alzheimer's (AD) islimited by the static graph assumptions of most methods; such models cannotaccount for an evolving pathophysiology, modulated by a latent diseasepseudotime. We propose to apply an existing latent variable model to real-worldAD data, inferring a pseudotime that orders patients along a data-drivendisease trajectory independent of chronological age, then learning how causalrelationships evolve. Pseudotime outperformed age in predicting diagnosis (AUC0.82 vs 0.59). Incorporating minimal, disease-agnostic background knowledgesubstantially improved graph accuracy and orientation. Our framework revealsdynamic interactions between novel (NfL, GFAP) and established AD markers,enabling practical causal discovery despite violated assumptions.</description><author>Natalia Glazman, Jyoti Mangal, Pedro Borges, Sebastien Ourselin, M. Jorge Cardoso</author><pubDate>Thu, 06 Nov 2025 18:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04619v1</guid></item><item><title>Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality</title><link>http://arxiv.org/abs/2511.04615v1</link><description>Deep learning models can generate virtual immunohistochemistry (IHC) stainsfrom hematoxylin and eosin (H&amp;E) images, offering a scalable and low-costalternative to laboratory IHC. However, reliable evaluation of image qualityremains a challenge as current texture- and distribution-based metrics quantifyimage fidelity rather than the accuracy of IHC staining. Here, we introduce anautomated and accuracy grounded framework to determine image quality acrosssixteen paired or unpaired image translation models. Using color deconvolution,we generate masks of pixels stained brown (i.e., IHC-positive) as predicted byeach virtual IHC model. We use the segmented masks of real and virtual IHC tocompute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directlyquantify correct pixel - level labeling without needing expert manualannotations. Our results demonstrate that conventional image fidelity metrics,including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),and structural similarity (SSIM), correlate poorly with stain accuracy andpathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCEachieve the highest stain accuracy, whereas unpaired diffusion- and GAN-basedmodels are less reliable in providing accurate IHC positive pixel labels.Moreover, whole-slide images (WSI) reveal performance declines that areinvisible in patch-based evaluations, emphasizing the need for WSI-levelbenchmarks. Together, this framework defines a reproducible approach forassessing the quality of virtual IHC models, a critical step to acceleratetranslation towards routine use by pathologists.</description><author>Tushar Kataria, Shikha Dubey, Mary Bronner, Jolanta Jedrzkiewicz, Ben J. Brintz, Shireen Y. Elhabian, Beatrice S. Knudsen</author><pubDate>Thu, 06 Nov 2025 18:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04615v1</guid></item><item><title>Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning</title><link>http://arxiv.org/abs/2510.13865v4</link><description>We introduce the Deep Edge Filter, a novel approach that applies high-passfiltering to deep neural network features to improve model generalizability.Our method is motivated by our hypothesis that neural networks encodetask-relevant semantic information in high-frequency components while storingdomain-specific biases in low-frequency components of deep features. Bysubtracting low-pass filtered outputs from original features, our approachisolates generalizable representations while preserving architecturalintegrity. Experimental results across diverse domains such as Vision, Text,3D, and Audio demonstrate consistent performance improvements regardless ofmodel architecture and data modality. Analysis reveals that our method inducesfeature sparsification and effectively isolates high-frequency components,providing empirical validation of our core hypothesis. The code is available athttps://github.com/dongkwani/DeepEdgeFilter.</description><author>Dongkwan Lee, Junhoo Lee, Nojun Kwak</author><pubDate>Thu, 06 Nov 2025 18:08:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.13865v4</guid></item><item><title>DashCLIP: Leveraging multimodal models for generating semantic embeddings for DoorDash</title><link>http://arxiv.org/abs/2504.07110v2</link><description>Despite the success of vision-language models in various generative tasks,obtaining high-quality semantic representations for products and user intentsis still challenging due to the inability of off-the-shelf models to capturenuanced relationships between the entities. In this paper, we introduce a jointtraining framework for product and user queries by aligning uni-modal andmulti-modal encoders through contrastive learning on image-text data. Our novelapproach trains a query encoder with an LLM-curated relevance dataset,eliminating the reliance on engagement history. These embeddings demonstratestrong generalization capabilities and improve performance across applications,including product categorization and relevance prediction. For personalized adsrecommendation, a significant uplift in the click-through rate and conversionrate after the deployment further confirms the impact on key business metrics.We believe that the flexibility of our framework makes it a promising solutiontoward enriching the user experience across the e-commerce landscape.</description><author>Omkar Gurjar, Kin Sum Liu, Praveen Kolli, Utsaw Kumar, Mandar Rahurkar</author><pubDate>Thu, 06 Nov 2025 18:08:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.07110v2</guid></item><item><title>XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification</title><link>http://arxiv.org/abs/2507.14578v2</link><description>We propose XL-DURel, a finetuned, multilingual Sentence Transformer modeloptimized for ordinal Word-in-Context classification. We test several lossfunctions for regression and ranking tasks managing to outperform previousmodels on ordinal and binary data with a ranking objective based on angulardistance in complex space. We further show that binary WiC can be treated as aspecial case of ordinal WiC and that optimizing models for the general ordinaltask improves performance on the more specific binary task. This paves the wayfor a unified treatment of WiC modeling across different task formulations.</description><author>Sachin Yadav, Dominik Schlechtweg</author><pubDate>Thu, 06 Nov 2025 18:07:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14578v2</guid></item><item><title>evomap: A Toolbox for Dynamic Mapping in Python</title><link>http://arxiv.org/abs/2511.04611v1</link><description>This paper presents evomap, a Python package for dynamic mapping. Mappingmethods are widely used across disciplines to visualize relationships amongobjects as spatial representations, or maps. However, most existing statisticalsoftware supports only static mapping, which captures objects' relationships ata single point in time and lacks tools to analyze how these relationshipsevolve. evomap fills this gap by implementing the dynamic mapping frameworkEvoMap, originally proposed by Matthe, Ringel, and Skiera (2023), which adaptstraditional static mapping methods for dynamic analyses. The package supportsmultiple mapping techniques, including variants of Multidimensional Scaling(MDS), Sammon Mapping, and t-distributed Stochastic Neighbor Embedding (t-SNE).It also includes utilities for data preprocessing, exploration, and resultevaluation, offering a comprehensive toolkit for dynamic mapping applications.This paper outlines the foundations of static and dynamic mapping, describesthe architecture and functionality of evomap, and illustrates its applicationthrough an extensive usage example.</description><author>Maximilian Matthe</author><pubDate>Thu, 06 Nov 2025 18:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04611v1</guid></item><item><title>PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning</title><link>http://arxiv.org/abs/2511.04601v1</link><description>While the Contrastive Language-Image Pretraining(CLIP) model has achievedremarkable success in a variety of downstream vison language understandingtasks, enhancing its capability for fine-grained image-text alignment remainsan active research focus. To this end, most existing works adopt the strategyof explicitly increasing the granularity of visual information processing,e.g., incorporating visual prompts to guide the model focus on specific localregions within the image. Meanwhile, researches on Multimodal Large LanguageModels(MLLMs) have demonstrated that training with long and detailed textualdescriptions can effectively improve the model's fine-grained vision-languagealignment. However, the inherent token length limitation of CLIP's text encoderfundamentally limits CLIP to process more granular textual information embeddedin long text sequences. To synergistically leverage the advantages of enhancingboth visual and textual content processing granularity, we propose PixCLIP, anovel framework designed to concurrently accommodate visual prompt inputs andprocess lengthy textual descriptions. Specifically, we first establish anautomated annotation pipeline capable of generating pixel-level localized,long-form textual descriptions for images. Utilizing this pipeline, weconstruct LongGRIT, a high-quality dataset comprising nearly 1.5 millionsamples. Secondly, we replace CLIP's original text encoder with the LLM andpropose a three-branch pixel-text alignment learning framework, facilitatingfine-grained alignment between image regions and corresponding textualdescriptions at arbitrary granularity. Experiments demonstrate that PixCLIPshowcases breakthroughs in pixel-level interaction and handling long-formtexts, achieving state-of-the-art performance.</description><author>Yicheng Xiao, Yu Chen, Haoxuan Ma, Jiale Hong, Caorui Li, Lingxiang Wu, Haiyun Guo, Jinqiao Wang</author><pubDate>Thu, 06 Nov 2025 17:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04601v1</guid></item><item><title>Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures</title><link>http://arxiv.org/abs/2511.04599v1</link><description>Understanding feature-outcome associations in high-dimensional data remains challenging when relationships vary across subpopulations, yet standard methods assuming global associations miss context-dependent patterns,reducing statistical power and interpretability. We develop a geometric decomposition framework offering two strategies for partitioning inference problems into regional analyses on data-derived Riemannian graphs. Gradient flow decomposition uses path-monotonicity-validated discrete Morse theory to partition samples into basins where outcomes exhibit monotonic behavior. Co-monotonicity decomposition leverages association structure: vertex-level coefficients measuring directional concordance between outcome and features, or between feature pairs, define embeddings of samples into associationspace. These embeddings induce Riemannian k-NN graphs on which biclustering identifies co-monotonicity cells (coherent regions) and feature modules. This extends naturally to multi-modal integration across multiple feature sets. Both strategies apply independently or jointly, with Bayesian posterior sampling providing credible intervals.</description><author>Pawel Gajer, Jacques Ravel</author><pubDate>Thu, 06 Nov 2025 17:51:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04599v1</guid></item><item><title>Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning</title><link>http://arxiv.org/abs/2511.04598v1</link><description>In this paper we study how transforming regular reinforcement learningenvironments into goal-conditioned environments can let agents learn to solvetasks autonomously and reward-free. We show that an agent can learn to solvetasks by selecting its own goals in an environment-agnostic way, at trainingtimes comparable to externally guided reinforcement learning. Our method isindependent of the underlying off-policy learning algorithm. Since our methodis environment-agnostic, the agent does not value any goals higher than others,leading to instability in performance for individual goals. However, in ourexperiments, we show that the average goal success rate improves andstabilizes. An agent trained with this method can be instructed to seek anyobservations made in the environment, enabling generic training of agents priorto specific use cases.</description><author>Hampus Åström, Elin Anna Topp, Jacek Malec</author><pubDate>Thu, 06 Nov 2025 17:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04598v1</guid></item><item><title>UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction</title><link>http://arxiv.org/abs/2511.04595v1</link><description>Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,yet existing methods struggle with the joint challenges of sparse,non-overlapping camera views and complex scene dynamics. We present UniSplat, ageneral feed-forward framework that learns robust dynamic scene reconstructionthrough unified latent spatio-temporal fusion. UniSplat constructs a 3D latentscaffold, a structured representation that captures geometric and semanticscene context by leveraging pretrained foundation models. To effectivelyintegrate information across spatial views and temporal frames, we introduce anefficient fusion mechanism that operates directly within the 3D scaffold,enabling consistent spatio-temporal alignment. To ensure complete and detailedreconstructions, we design a dual-branch decoder that generates dynamic-awareGaussians from the fused scaffold by combining point-anchored refinement withvoxel-based generation, and maintain a persistent memory of static Gaussians toenable streaming scene completion beyond current camera coverage. Extensiveexperiments on real-world datasets demonstrate that UniSplat achievesstate-of-the-art performance in novel view synthesis, while providing robustand high-quality renderings even for viewpoints outside the original cameracoverage.</description><author>Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng, Bo Zhang, Li Jiang</author><pubDate>Thu, 06 Nov 2025 17:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04595v1</guid></item><item><title>Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems</title><link>http://arxiv.org/abs/2511.04594v1</link><description>Multi-agent systems (MAS) are central to applications such as swarm roboticsand traffic routing, where agents must coordinate in a decentralized manner toachieve a common objective. Stochastic Shortest Path (SSP) problems provide anatural framework for modeling decentralized control in such settings. Whilethe problem of learning in SSP has been extensively studied in single-agentsettings, the decentralized multi-agent variant remains largely unexplored. Inthis work, we take a step towards addressing that gap. We study decentralizedmulti-agent SSPs (Dec-MASSPs) under linear function approximation, where thetransition dynamics and costs are represented using linear models. Applyingnovel symmetry-based arguments, we identify the structure of optimal policies.Our main contribution is the first regret lower bound for this setting based onthe construction of hard-to-learn instances for any number of agents, $n$. Ourregret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights theinherent learning difficulty in Dec-MASSPs. These insights clarify the learningcomplexity of decentralized control and can further guide the design ofefficient learning algorithms in multi-agent systems.</description><author>Utkarsh U. Chavan, Prashant Trivedi, Nandyala Hemachandra</author><pubDate>Thu, 06 Nov 2025 17:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04594v1</guid></item><item><title>Neural Computation Without Slots: Steps Towards Biologically Plausible Memory and Attention in Natural and Artificial Intelligence</title><link>http://arxiv.org/abs/2511.04593v1</link><description>Many models used in artificial intelligence and cognitive science rely onmulti-element patterns stored in "slots" - dedicated storage locations - in adigital computer. As biological brains likely lack slots, we consider how theymight achieve similar functional outcomes without them by building on theneurally-inspired modern Hopfield network (MHN; Krotov &amp; Hopfield, 2021), whichstores patterns in the connection weights of an individual neuron. We proposeextensions of this approach to increase its biological plausibility as a modelof memory and to capture an important advantage of slot-based computation incontemporary language models. For memory, neuroscience research suggests thatthe weights of overlapping sparse ensembles of neurons, rather than a dedicatedindividual neuron, are used to store a memory. We introduce the K-winner MHN,extending the approach to ensembles, and find that within a continual learningregime, the ensemble-based MHN exhibits greater retention of older memories, asmeasured by the graded sensitivity measure d', than a standard (one-neuron)MHN. Next, we consider the powerful use of slot-based memory in contemporarylanguage models. These models use slots to store long sequences of past inputsand their learned encodings, supporting later predictions and allowing errorsignals to be transported backward in time to adjust weights underlying thelearned encodings of these past inputs. Inspired by these models' successes, weshow how the MHN can be extended to capture both of these important functionaloutcomes. Collectively, our modeling approaches constitute steps towardsunderstanding how biologically plausible mechanisms can support computationsthat have enabled AI systems to capture human-like abilities that no priormodels have been able to achieve.</description><author>Shaunak Bhandarkar, James L. McClelland</author><pubDate>Thu, 06 Nov 2025 17:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04593v1</guid></item><item><title>Projection Methods for Operator Learning and Universal Approximation</title><link>http://arxiv.org/abs/2406.12264v3</link><description>We obtain a new universal approximation theorem for continuous (possiblynonlinear) operators on arbitrary Banach spaces using the Leray-Schaudermapping. Moreover, we introduce and study a method for operator learning inBanach spaces $L^p$ of functions with multiple variables, based on orthogonalprojections on polynomial bases. We derive a universal approximation result foroperators where we learn a linear projection and a finite dimensional mappingunder some additional assumptions. For the case of $p=2$, we give somesufficient conditions for the approximation results to hold. This articleserves as the theoretical framework for a deep learning methodology in operatorlearning.</description><author>Emanuele Zappala</author><pubDate>Thu, 06 Nov 2025 17:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12264v3</guid></item><item><title>Complexity as Advantage: A Regret-Based Perspective on Emergent Structure</title><link>http://arxiv.org/abs/2511.04590v1</link><description>We introduce Complexity as Advantage (CAA), a framework that defines thecomplexity of a system relative to a family of observers. Instead of measuringcomplexity as an intrinsic property, we evaluate how much predictive regret asystem induces for different observers attempting to model it. A system iscomplex when it is easy for some observers and hard for others, creating aninformation advantage. We show that this formulation unifies several notions ofemergent behavior, including multiscale entropy, predictive information, andobserver-dependent structure. The framework suggests that "interesting" systemsare those positioned to create differentiated regret across observers,providing a quantitative grounding for why complexity can be functionallyvaluable. We demonstrate the idea through simple dynamical models and discussimplications for learning, evolution, and artificial agents.</description><author>Oshri Naparstek</author><pubDate>Thu, 06 Nov 2025 17:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04590v1</guid></item><item><title>Question the Questions: Auditing Representation in Online Deliberative Processes</title><link>http://arxiv.org/abs/2511.04588v1</link><description>A central feature of many deliberative processes, such as citizens'assemblies and deliberative polls, is the opportunity for participants toengage directly with experts. While participants are typically invited topropose questions for expert panels, only a limited number can be selected dueto time constraints. This raises the challenge of how to choose a small set ofquestions that best represent the interests of all participants. We introducean auditing framework for measuring the level of representation provided by aslate of questions, based on the social choice concept known as justifiedrepresentation (JR). We present the first algorithms for auditing JR in thegeneral utility setting, with our most efficient algorithm achieving a runtimeof $O(mn\log n)$, where $n$ is the number of participants and $m$ is the numberof proposed questions. We apply our auditing methods to historicaldeliberations, comparing the representativeness of (a) the actual questionsposed to the expert panel (chosen by a moderator), (b) participants' questionschosen via integer linear programming, (c) summary questions generated by largelanguage models (LLMs). Our results highlight both the promise and currentlimitations of LLMs in supporting deliberative processes. By integrating ourmethods into an online deliberation platform that has been used for overhundreds of deliberations across more than 50 countries, we make it easy forpractitioners to audit and improve representation in future deliberations.</description><author>Soham De, Lodewijk Gelauff, Ashish Goel, Smitha Milli, Ariel Procaccia, Alice Siu</author><pubDate>Thu, 06 Nov 2025 17:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04588v1</guid></item><item><title>VISTA Score: Verification In Sequential Turn-based Assessment</title><link>http://arxiv.org/abs/2510.27052v2</link><description>Hallucination--defined here as generating statements unsupported orcontradicted by available evidence or conversational context--remains a majorobstacle to deploying conversational AI systems in settings that demand factualreliability. Existing metrics either evaluate isolated responses or treatunverifiable content as errors, limiting their use for multi-turn dialogue. Weintroduce VISTA (Verification In Sequential Turn-based Assessment), a frameworkfor evaluating conversational factuality through claim-level verification andsequential consistency tracking. VISTA decomposes each assistant turn intoatomic factual claims, verifies them against trusted sources and dialoguehistory, and categorizes unverifiable statements (subjective, contradicted,lacking evidence, or abstaining). Across eight large language models and fourdialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTAsubstantially improves hallucination detection over FACTSCORE and LLM-as-Judgebaselines. Human evaluation confirms that VISTA's decomposition improvesannotator agreement and reveals inconsistencies in existing benchmarks. Bymodeling factuality as a dynamic property of conversation, VISTA offers a moretransparent, human-aligned measure of truthfulness in dialogue systems.</description><author>Ashley Lewis, Andrew Perrault, Eric Fosler-Lussier, Michael White</author><pubDate>Thu, 06 Nov 2025 17:44:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.27052v2</guid></item><item><title>Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off</title><link>http://arxiv.org/abs/2510.26722v3</link><description>Over-the-air (OTA) federated learning (FL) has been well recognized as ascalable paradigm that exploits the waveform superposition of the wirelessmultiple-access channel to aggregate model updates in a single use. ExistingOTA-FL designs largely enforce zero-bias model updates by either assuming\emph{homogeneous} wireless conditions (equal path loss across devices) orforcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous}wireless scenarios, however, such designs are constrained by the weakest deviceand inflate the update variance. Moreover, prior analyses of biased OTA-FLlargely address convex objectives, while most modern AI models are highlynon-convex. Motivated by these gaps, we study OTA-FL with stochastic gradientdescent (SGD) for general smooth non-convex objectives under wirelessheterogeneity. We develop novel OTA-FL SGD updates that allow a structured,time-invariant model bias while facilitating reduced variance updates. Wederive a finite-time stationarity bound (expected time average squared gradientnorm) that explicitly reveals a bias-variance trade-off. To optimize thistrade-off, we pose a non-convex joint OTA power-control design and develop anefficient successive convex approximation (SCA) algorithm that requires onlystatistical CSI at the base station. Experiments on a non-convex imageclassification task validate the approach: the SCA-based design acceleratesconvergence via an optimized bias and improves generalization over prior OTA-FLbaselines.</description><author>Muhammad Faraz Ul Abrar, Nicolò Michelusi</author><pubDate>Thu, 06 Nov 2025 17:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.26722v3</guid></item><item><title>Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis</title><link>http://arxiv.org/abs/2511.04584v1</link><description>Natural language interfaces to tabular data must handle ambiguities inherentto queries. Instead of treating ambiguity as a deficiency, we reframe it as afeature of cooperative interaction, where the responsibility of queryspecification is shared among the user and the system. We develop a principledframework distinguishing cooperative queries, i.e., queries that yield aresolvable interpretation, from uncooperative queries that cannot be resolved.Applying the framework to evaluations for tabular question answering andanalysis, we analyze the queries in 15 popular datasets, and observe anuncontrolled mixing of query types neither adequate for evaluating a system'sexecution accuracy nor for evaluating interpretation capabilities. Ourframework and analysis of queries shifts the perspective from fixing ambiguityto embracing cooperation in resolving queries. This reflection enables moreinformed design and evaluation for natural language interfaces for tabulardata, for which we outline implications and directions for future research.</description><author>Daniel Gomm, Cornelius Wolff, Madelon Hulsebos</author><pubDate>Thu, 06 Nov 2025 17:39:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04584v1</guid></item><item><title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title><link>http://arxiv.org/abs/2511.04583v1</link><description>Understanding the current capabilities and risks of AI Scientist systems isessential for ensuring trustworthy and sustainable AI-driven scientificprogress while preserving the integrity of the academic ecosystem. To this end,we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist systemthat mimics the core research workflow of a novice student researcher: Giventhe baseline paper from the human mentor, it analyzes its limitations,formulates novel hypotheses for improvement, validates them through rigorousexperimentation, and writes a paper with the results. Unlike previousapproaches that assume full automation or operate on small-scale code, Jr. AIScientist follows a well-defined research workflow and leverages modern codingagents to handle complex, multi-file implementations, leading to scientificallyvaluable contributions. For evaluation, we conducted automated assessmentsusing AI Reviewers, author-led evaluations, and submissions to Agents4Science,a venue dedicated to AI-driven scientific contributions. The findingsdemonstrate that Jr. AI Scientist generates papers receiving higher reviewscores than existing fully automated systems. Nevertheless, we identifyimportant limitations from both the author evaluation and the Agents4Sciencereviews, indicating the potential risks of directly applying current AIScientist systems and key challenges for future research. Finally, wecomprehensively report various risks identified during development. We hopethese insights will deepen understanding of current progress and risks in AIScientist development.</description><author>Atsuyuki Miyai, Mashiro Toyooka, Takashi Otonari, Zaiying Zhao, Kiyoharu Aizawa</author><pubDate>Thu, 06 Nov 2025 17:37:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04583v1</guid></item><item><title>Information-driven design of imaging systems</title><link>http://arxiv.org/abs/2405.20559v5</link><description>Imaging systems have traditionally been designed to mimic the human eye andproduce visually interpretable measurements. Modern imaging systems, however,process raw measurements computationally before or instead of human viewing. Asa result, the information content of raw measurements matters more than theirvisual interpretability. Despite the importance of measurement informationcontent, current approaches for evaluating imaging system performance do notquantify it: they instead either use alternative metrics that assess specificaspects of measurement quality or assess measurements indirectly withperformance on secondary tasks. We developed the theoretical foundations and a practical method to directlyquantify mutual information between noisy measurements and unknown objects. Byfitting probabilistic models to measurements and their noise characteristics,our method estimates information by upper bounding its true value. By applyinggradient-based optimization to these estimates, we also developed a techniquefor designing imaging systems called Information-Driven Encoder AnalysisLearning (IDEAL). Our information estimates accurately captured systemperformance differences across four imaging domains (color photography, radioastronomy, lensless imaging, and microscopy). Systems designed with IDEALmatched the performance of those designed with end-to-end optimization, theprevailing approach that jointly optimizes hardware and image processingalgorithms. These results establish mutual information as a universalperformance metric for imaging systems that enables both computationallyefficient design optimization and evaluation in real-world conditions. A video summarizing this work can be found at:https://waller-lab.github.io/EncodingInformationWebsite/</description><author>Henry Pinkard, Leyla Kabuli, Eric Markley, Tiffany Chien, Jiantao Jiao, Laura Waller</author><pubDate>Thu, 06 Nov 2025 17:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20559v5</guid></item><item><title>Physics-Informed Neural Networks and Neural Operators for Parametric PDEs: A Human-AI Collaborative Analysis</title><link>http://arxiv.org/abs/2511.04576v1</link><description>PDEs arise ubiquitously in science and engineering, where solutions depend onparameters (physical properties, boundary conditions, geometry). Traditionalnumerical methods require re-solving the PDE for each parameter, makingparameter space exploration prohibitively expensive. Recent machine learningadvances, particularly physics-informed neural networks (PINNs) and neuraloperators, have revolutionized parametric PDE solving by learning solutionoperators that generalize across parameter spaces. We critically analyze twomain paradigms: (1) PINNs, which embed physical laws as soft constraints andexcel at inverse problems with sparse data, and (2) neural operators (e.g.,DeepONet, Fourier Neural Operator), which learn mappings betweeninfinite-dimensional function spaces and achieve unprecedented generalization.Through comparisons across fluid dynamics, solid mechanics, heat transfer, andelectromagnetics, we show neural operators can achieve computational speedupsof $10^3$ to $10^5$ times faster than traditional solvers for multi-queryscenarios, while maintaining comparable accuracy. We provide practical guidancefor method selection, discuss theoretical foundations (universal approximation,convergence), and identify critical open challenges: high-dimensionalparameters, complex geometries, and out-of-distribution generalization. Thiswork establishes a unified framework for understanding parametric PDE solversvia operator learning, offering a comprehensive, incrementally updated resourcefor this rapidly evolving field</description><author>Zhuo Zhang, Xiong Xiong, Sen Zhang, Yuan Zhao, Xi Yang</author><pubDate>Thu, 06 Nov 2025 17:31:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04576v1</guid></item><item><title>SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</title><link>http://arxiv.org/abs/2511.03325v2</link><description>Video Question Answering (VideoQA) in the surgical domain aims to enhanceintraoperative understanding by enabling AI models to reason over temporallycoherent events rather than isolated frames. Current approaches are limited tostatic image features, and available datasets often lack temporal annotations,ignoring the dynamics critical for accurate procedural interpretation. Wepropose SurgViVQA, a surgical VideoQA model that extends visual reasoning fromstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoderto fuse video and question features, capturing temporal cues such as motion andtool--tissue interactions, which a fine-tuned large language model (LLM) thendecodes into coherent answers. To evaluate its performance, we curatedREAL-Colon-VQA, a colonoscopic video dataset that includes motion-relatedquestions and diagnostic attributes, as well as out-of-template questions withrephrased or semantically altered formulations to assess model robustness.Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA datasetshows that SurgViVQA outperforms existing image-based VQA benchmark models,particularly in keyword accuracy, improving over PitVQA by +11\% onREAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questionsfurther confirms improved generalizability and robustness to variations inquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a frameworkfor temporally-aware understanding in surgical VideoQA, enabling AI models tointerpret dynamic procedural contexts more effectively. Code and datasetavailable at https://github.com/madratak/SurgViVQA.</description><author>Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque</author><pubDate>Thu, 06 Nov 2025 17:28:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.03325v2</guid></item><item><title>FedQUIT: On-Device Federated Unlearning via a Quasi-Competent Virtual Teacher</title><link>http://arxiv.org/abs/2408.07587v3</link><description>Federated Learning (FL) systems enable the collaborative training of machinelearning models without requiring centralized collection of individual data. FLparticipants should have the ability to exercise their right to be forgotten,ensuring their past contributions can be removed from the learned model uponrequest. In this paper, we propose FedQUIT, a novel algorithm that usesknowledge distillation to scrub the contribution of the data to forget from anFL global model while preserving its generalization ability. FedQUIT directlyworks on client devices that request to leave the federation, and leverages ateacher-student framework. The FL global model acts as the teacher, and thelocal model works as the student. To induce forgetting, FedQUIT tailors theteacher's output on local data (the data to forget) penalizing the predictionscore of the true class. Unlike previous work, our method does not requirehardly viable assumptions for cross-device settings, such as storing historicalupdates of participants or requiring access to proxy datasets. Experimentalresults on various datasets and model architectures demonstrate that (i)FedQUIT outperforms state-of-the-art competitors in forgetting data, (ii) hasthe exact computational requirements as a regular FedAvg round, and (iii)reduces the cumulative communication costs by up to 117.6$\times$ compared toretraining from scratch to restore the initial generalization performance afterunlearning.</description><author>Alessio Mora, Lorenzo Valerio, Paolo Bellavista, Andrea Passarella</author><pubDate>Thu, 06 Nov 2025 17:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07587v3</guid></item><item><title>ARETE: an R package for Automated REtrieval from TExt with large language models</title><link>http://arxiv.org/abs/2511.04573v1</link><description>1. A hard stop for the implementation of rigorous conservation initiatives isour lack of key species data, especially occurrence data. Furthermore,researchers have to contend with an accelerated speed at which new informationmust be collected and processed due to anthropogenic activity. Publicationsranging from scientific papers to gray literature contain this crucialinformation but their data are often not machine-readable, requiring extensivehuman work to be retrieved. 2. We present the ARETE R package, an open-sourcesoftware aiming to automate data extraction of species occurrences powered bylarge language models, namely using the chatGPT Application ProgrammingInterface. This R package integrates all steps of the data extraction andvalidation process, from Optical Character Recognition to detection of outliersand output in tabular format. Furthermore, we validate ARETE through systematiccomparison between what is modelled and the work of human annotators. 3. Wedemonstrate the usefulness of the approach by comparing range maps producedusing GBIF data and with those automatically extracted for 100 species ofspiders. Newly extracted data allowed to expand the known Extent of Occurrenceby a mean three orders of magnitude, revealing new areas where the species werefound in the past, which mayhave important implications for spatialconservation planning and extinction risk assessments. 4. ARETE allows fasteraccess to hitherto untapped occurrence data, a potential game changer inprojects requiring such data. Researchers will be able to better prioritizeresources, manually verifying selected species while maintaining automatedextraction for the majority. This workflow also allows predicting availablebibliographic data during project planning.</description><author>Vasco V. Branco, Jandó Benedek, Lidia Pivovarova, Luís Correia, Pedro Cardoso</author><pubDate>Thu, 06 Nov 2025 17:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04573v1</guid></item><item><title>Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</title><link>http://arxiv.org/abs/2511.04570v1</link><description>"Thinking with Text" and "Thinking with Images" paradigm significantlyimprove the reasoning ability of large language models (LLMs) and VisionLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)Images capture only single moments and fail to represent dynamic processes orcontinuous changes, and (2) The separation of text and vision as distinctmodalities, hindering unified multimodal understanding and generation. Toovercome these limitations, we introduce "Thinking with Video", a new paradigmthat leverages video generation models, such as Sora-2, to bridge visual andtextual reasoning in a unified temporal framework. To support this exploration,we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBenchencompasses two task categories: (1) vision-centric tasks (e.g., EyeballingPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Ourevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and evensurpasses VLMs on several tasks, such as Eyeballing Games. On text-centrictasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.Furthermore, we systematically analyse the source of these abilities. We alsofind that self-consistency and in-context learning can improve Sora-2'sperformance. In summary, our findings demonstrate that the video generationmodel is the potential unified multimodal understanding and generation model,positions "thinking with video" as a unified multimodal reasoning paradigm.</description><author>Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu</author><pubDate>Thu, 06 Nov 2025 17:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04570v1</guid></item><item><title>Riesz Regression As Direct Density Ratio Estimation</title><link>http://arxiv.org/abs/2511.04568v1</link><description>Riesz regression has garnered attention as a tool in debiased machinelearning for causal and structural parameter estimation (Chernozhukov et al.,2021). This study shows that Riesz regression is closely related to directdensity-ratio estimation (DRE) in important cases, including average treat-ment effect (ATE) estimation. Specifically, the idea and objective in Rieszregression coincide with the one in least-squares importance fitting (LSIF,Kanamori et al., 2009) in direct density-ratio estimation. While Rieszregression is general in the sense that it can be applied to Riesz representerestimation in a wide class of problems, the equivalence with DRE allows us todirectly import exist- ing results in specific cases, includingconvergence-rate analyses, the selection of loss functions viaBregman-divergence minimization, and regularization techniques for flexiblemodels, such as neural networks. Conversely, insights about the Rieszrepresenter in debiased machine learning broaden the applications of directdensity-ratio estimation methods. This paper consolidates our prior results inKato (2025a) and Kato (2025b).</description><author>Masahiro Kato</author><pubDate>Thu, 06 Nov 2025 17:25:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04568v1</guid></item><item><title>Machine Learning for Electron-Scale Turbulence Modeling in W7-X</title><link>http://arxiv.org/abs/2511.04567v1</link><description>Constructing reduced models for turbulent transport is essential foraccelerating profile predictions and enabling many-query tasks such asuncertainty quantification, parameter scans, and design optimization. Thispaper presents machine-learning-driven reduced models for Electron TemperatureGradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator. Each modelpredicts the ETG heat flux as a function of three plasma parameters: thenormalized electron temperature radial gradient ($\omega_{T_e}$), the ratio ofnormalized electron temperature and density radial gradients ($\eta_e$), andthe electron-to-ion temperature ratio ($\tau$). We first construct modelsacross seven radial locations using regression and an activemachine-learning-based procedure. This process initializes models usinglow-cardinality sparse-grid training data and then iteratively refines theirtraining sets by selecting the most informative points from a pre-existingsimulation database. We evaluate the prediction capabilities of our modelsusing out-of-sample datasets with over $393$ points per location, and $95\%$prediction intervals are estimated via bootstrapping to assess predictionuncertainty. We then investigate the construction of generalized reducedmodels, including a generic, position-independent model, and assess their heatflux prediction capabilities at three additional locations. Our modelsdemonstrate robust performance and predictive accuracy comparable to theoriginal reference simulations, even when applied beyond the training domain.</description><author>Ionut-Gabriel Farcas, Don Lawrence Carl Agapito Fernando, Alejandro Banon Navarro, Gabriele Merlo, Frank Jenko</author><pubDate>Thu, 06 Nov 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04567v1</guid></item><item><title>TIMESAFE: Timing Interruption Monitoring and Security Assessment for Fronthaul Environments</title><link>http://arxiv.org/abs/2412.13049v2</link><description>5G and beyond cellular systems embrace the disaggregation of Radio AccessNetwork (RAN) components, exemplified by the evolution of the fronthaul (FH)connection between cellular baseband and radio unit equipment. Crucially,synchronization over the FH is pivotal for reliable 5G services. In recentyears, there has been a push to move these links to an Ethernet-based packetnetwork topology, leveraging existing standards and ongoing research forTime-Sensitive Networking (TSN). However, TSN standards, such as Precision TimeProtocol (PTP), focus on performance with little to no concern for security.This increases the exposure of the open FH to security risks. Attacks targetingsynchronization mechanisms pose significant threats, potentially disrupting 5Gnetworks and impairing connectivity. In this paper, we demonstrate the impact of successful spoofing and replayattacks against PTP synchronization. We show how a spoofing attack is able tocause a production-ready O-RAN and 5G-compliant private cellular base stationto catastrophically fail within 2 seconds of the attack, necessitating manualintervention to restore full network operations. To counter this, we design aMachine Learning (ML)-based monitoring solution capable of detecting variousmalicious attacks with over 97.5% accuracy.</description><author>Joshua Groen, Simone Di Valerio, Imtiaz Karim, Davide Villa, Yiewi Zhang, Leonardo Bonati, Michele Polese, Salvatore D'Oro, Tommaso Melodia, Elisa Bertino, Francesca Cuomo, Kaushik Chowdhury</author><pubDate>Thu, 06 Nov 2025 17:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13049v2</guid></item><item><title>Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI</title><link>http://arxiv.org/abs/2511.04564v1</link><description>Physics-informed machine learning (PIML) integrates partial differentialequations (PDEs) into machine learning models to solve inverse problems, suchas estimating coefficient functions (e.g., the Hamiltonian function) thatcharacterize physical systems. This framework enables data-driven understandingand prediction of complex physical phenomena. While coefficient functions inPIML are typically estimated on the basis of predictive performance, physics asa discipline does not rely solely on prediction accuracy to evaluate models.For example, Kepler's heliocentric model was favored owing to smalldiscrepancies in planetary motion, despite its similar predictive accuracy tothe geocentric model. This highlights the inherent uncertainties in data-drivenmodel inference and the scientific importance of selecting physicallymeaningful solutions. In this paper, we propose a framework to quantify andanalyze such uncertainties in the estimation of coefficient functions in PIML.We apply our framework to reduced model of magnetohydrodynamics and ourframework shows that there are uncertainties, and unique identification ispossible with geometric constraints. Finally, we confirm that we can estimatethe reduced model uniquely by incorporating these constraints.</description><author>Yoh-ichi Mototake, Makoto Sasaki</author><pubDate>Thu, 06 Nov 2025 17:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04564v1</guid></item><item><title>BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering</title><link>http://arxiv.org/abs/2511.04560v1</link><description>Developing accurate biomedical Question Answering (QA) systems inlow-resource languages remains a major challenge, limiting equitable access toreliable medical knowledge. This paper introduces BanglaMedQA andBanglaMMedBench, the first large-scale Bangla biomedical Multiple ChoiceQuestion (MCQ) datasets designed to evaluate reasoning and retrieval in medicalartificial intelligence (AI). The study applies and benchmarks severalRetrieval-Augmented Generation (RAG) strategies, including Traditional,Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combiningtextbook-based and web retrieval with generative reasoning to improve factualaccuracy. A key novelty lies in integrating a Bangla medical textbook corpusthrough Optical Character Recognition (OCR) and implementing an Agentic RAGpipeline that dynamically selects between retrieval and reasoning strategies.Experimental results show that the Agentic RAG achieved the highest accuracy89.54% with openai/gpt-oss-120b, outperforming other configurations anddemonstrating superior rationale quality. These findings highlight thepotential of RAG-based methods to enhance the reliability and accessibility ofBangla medical QA, establishing a foundation for future research inmultilingual medical artificial intelligence.</description><author>Sadia Sultana, Saiyma Sittul Muna, Mosammat Zannatul Samarukh, Ajwad Abrar, Tareque Mohmud Chowdhury</author><pubDate>Thu, 06 Nov 2025 17:15:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04560v1</guid></item><item><title>Are Minimal Radial Distortion Solvers Necessary for Relative Pose Estimation?</title><link>http://arxiv.org/abs/2410.05984v2</link><description>Estimating the relative pose between two cameras is a fundamental step inmany applications such as Structure-from-Motion. The common approach torelative pose estimation is to apply a minimal solver inside a RANSAC loop.Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all camerasexhibit radial distortion. Not modeling radial distortion leads to(significantly) worse results. However, minimal radial distortion solvers aresignificantly more complex than pinhole solvers, both in terms of run-time andimplementation efforts. This paper compares radial distortion solvers with asimple-to-implement approach that combines an efficient pinhole solver withsampled radial distortion parameters. Extensive experiments on multipledatasets and RANSAC variants show that this simple approach performs similarlyor better than the most accurate minimal distortion solvers at faster run-timeswhile being significantly more accurate than faster non-minimal solvers. Weclearly show that complex radial distortion solvers are not necessary inpractice. Code and benchmark are available at https://github.com/kocurvik/rd.</description><author>Charalambos Tzamos, Viktor Kocur, Yaqing Ding, Torsten Sattler, Zuzana Kukelova</author><pubDate>Thu, 06 Nov 2025 17:12:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05984v2</guid></item><item><title>Measure-Theoretic Time-Delay Embedding</title><link>http://arxiv.org/abs/2409.08768v2</link><description>The celebrated Takens' embedding theorem provides a theoretical foundationfor reconstructing the full state of a dynamical system from partialobservations. However, the classical theorem assumes that the underlying systemis deterministic and that observations are noise-free, limiting itsapplicability in real-world scenarios. Motivated by these limitations, weformulate a measure-theoretic generalization that adopts an Euleriandescription of the dynamics and recasts the embedding as a pushforward mapbetween spaces of probability measures. Our mathematical results leveragerecent advances in optimal transport. Building on the proposedmeasure-theoretic time-delay embedding theory, we develop a computationalprocedure that aims to reconstruct the full state of a dynamical system fromtime-lagged partial observations, engineered with robustness to handle sparseand noisy data. We evaluate our measure-based approach across several numericalexamples, ranging from the classic Lorenz-63 system to real-world applicationssuch as NOAA sea surface temperature reconstruction and ERA5 wind fieldreconstruction.</description><author>Jonah Botvinick-Greenhouse, Maria Oprea, Romit Maulik, Yunan Yang</author><pubDate>Thu, 06 Nov 2025 17:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08768v2</guid></item><item><title>Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs</title><link>http://arxiv.org/abs/2506.05410v2</link><description>Recent advances in Large Language Models (LLMs) have highlighted the criticalimportance of extending context length, yet the quadratic complexity ofattention mechanisms poses significant challenges for efficient long-contextmodeling. KV cache compression has emerged as a key approach to address thischallenge. Through extensive empirical analysis, we reveal a fundamental yetpreviously overlooked asymmetry in KV caches: while adjacent keys receivesimilar attention weights ({\it local homogeneity}), adjacent valuesdemonstrate distinct {\it heterogeneous} distributions. This key-valueasymmetry reveals a critical limitation in existing compression methods thattreat keys and values uniformly. To address the limitation, we propose atraining-free compression framework (AsymKV) that combines homogeneity-basedkey merging with a mathematically proven lossless value compression. Extensiveexperiments demonstrate that AsymKV consistently outperforms existinglong-context methods across various tasks and base models. For example, onLLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassingSOTA methods like H$_2$O (38.89) by a large margin.Our code can be found inthis link:https://github.com/the-scale-lab/Asymkv.</description><author>Wanyun Cui, Mingwei Xu</author><pubDate>Thu, 06 Nov 2025 17:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05410v2</guid></item><item><title>Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning</title><link>http://arxiv.org/abs/2511.04557v1</link><description>In domains such as healthcare, finance, and e-commerce, the temporal dynamicsof relational data emerge from complex interactions-such as those betweenpatients and providers, or users and products across diverse categories. To bebroadly useful, models operating on these data must integrate long-rangespatial and temporal dependencies across diverse types of entities, while alsosupporting multiple predictive tasks. However, existing graph models forrelational data primarily focus on spatial structure, treating temporalinformation merely as a filtering constraint to exclude future events ratherthan a modeling signal, and are typically designed for single-task prediction.To address these gaps, we introduce a temporal subgraph sampler that enhancesglobal context by retrieving nodes beyond the immediate neighborhood to capturetemporally relevant relationships. In addition, we propose the Relational GraphPerceiver (RGP), a graph transformer architecture for relational deep learningthat leverages a cross-attention-based latent bottleneck to efficientlyintegrate information from both structural and temporal contexts. This latentbottleneck integrates signals from different node and edge types into a commonlatent space, enabling the model to build global context across the entirerelational system. RGP also incorporates a flexible cross-attention decoderthat supports joint learning across tasks with disjoint label spaces within asingle model. Experiments on RelBench, SALT, and CTU show that RGP deliversstate-of-the-art performance, offering a general and scalable solution forrelational deep learning with support for diverse predictive tasks.</description><author>Divyansha Lachi, Mahmoud Mohammadi, Joe Meyer, Vinam Arora, Tom Palczewski, Eva L. Dyer</author><pubDate>Thu, 06 Nov 2025 17:08:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04557v1</guid></item><item><title>Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach</title><link>http://arxiv.org/abs/2511.04556v1</link><description>Urban surface water flooding, triggered by intense rainfall overwhelmingdrainage systems, is increasingly frequent and widespread. While floodprediction and monitoring in high spatial-temporal resolution are desired,practical constraints in time, budget, and technology hinder its fullimplementation. How to monitor urban drainage networks and predict flowconditions under constrained resource is a major challenge. This study presentsa data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, tooptimize sensor placement and reconstruct peak flowrates in a stormwatersystem, using the Woodland Avenue catchment in Duluth, Minnesota, as a casestudy. We utilized a SWMM model to generate a training dataset of peak flowrateprofiles across the stormwater network. Furthermore, we applied DSS -leveraging singular value decomposition for dimensionality reduction and QRfactorization for sensor allocation - to identify the optimal monitoring nodesbased on the simulated training dataset. We then validated therepresentativeness of these identified monitoring nodes by comparing theDSS-reconstructed peak flowrate profiles with those obtained from SWMM. Threeoptimally placed sensors among 77 nodes achieved satisfactory reconstructionperformance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to75th percentiles). In addition, the model showed good robustness to uncertaintyin measurements. Its robustness to sensor failures is location-dependent andimproves with the number of sensors deployed. The framework balancescomputational efficiency and physical interpretability, enabling high-accuracyflow reconstruction with minimal sensors. This DSS framework can be furtherintegrated with predictive models to realize flood early warning and real-timecontrol under limited sensing and monitoring resource.</description><author>Zihang Ding, Kun Zhang</author><pubDate>Thu, 06 Nov 2025 17:08:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04556v1</guid></item><item><title>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</title><link>http://arxiv.org/abs/2511.04555v1</link><description>Vision-Language-Action (VLA) models have emerged as a powerful framework thatunifies perception, language, and control, enabling robots to perform diversetasks through multimodal understanding. However, current VLA models typicallycontain massive parameters and rely heavily on large-scale robot datapretraining, leading to high computational costs during training, as well aslimited deployability for real-time inference. Moreover, most trainingparadigms often degrade the perceptual representations of the vision-languagebackbone, resulting in overfitting and poor generalization to downstream tasks.In this work, we present Evo-1, a lightweight VLA model that reducescomputation and improves deployment efficiency, while maintaining strongperformance without pretraining on robot data. Evo-1 builds on a nativemultimodal Vision-Language model (VLM), incorporating a novel cross-modulateddiffusion transformer along with an optimized integration module, togetherforming an effective architecture. We further introduce a two-stage trainingparadigm that progressively aligns action with perception, preserving therepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1achieves state-of-the-art results on the Meta-World and RoboTwin suite,surpassing the previous best models by 12.4% and 6.9%, respectively, and alsoattains a competitive result of 94.8% on LIBERO. In real-world evaluations,Evo-1 attains a 78% success rate with high inference frequency and low memoryoverhead, outperforming all baseline methods. We release code, data, and modelweights to facilitate future research on lightweight and efficient VLA models.</description><author>Tao Lin, Yilei Zhong, Yuxin Du, Jingjing Zhang, Jiting Liu, Yinxinyu Chen, Encheng Gu, Ziyan Liu, Hongyi Cai, Yanwen Zou, Lixing Zou, Zhaoye Zhou, Gen Li, Bo Zhao</author><pubDate>Thu, 06 Nov 2025 17:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04555v1</guid></item><item><title>Generative Bayesian Filtering and Parameter Learning</title><link>http://arxiv.org/abs/2511.04552v1</link><description>Generative Bayesian Filtering (GBF) provides a powerful and flexibleframework for performing posterior inference in complex nonlinear andnon-Gaussian state-space models. Our approach extends Generative BayesianComputation (GBC) to dynamic settings, enabling recursive posterior inferenceusing simulation-based methods powered by deep neural networks. GBF does notrequire explicit density evaluations, making it particularly effective whenobservation or transition distributions are analytically intractable. Toaddress parameter learning, we introduce the Generative-Gibbs sampler, whichbypasses explicit density evaluation by iteratively sampling each variable fromits implicit full conditional distribution. Such technique is broadlyapplicable and enables inference in hierarchical Bayesian models withintractable densities, including state-space models. We assess the performanceof the proposed methodologies through both simulated and empirical studies,including the estimation of $\alpha$-stable stochastic volatility models. Ourfindings indicate that GBF significantly outperforms existing likelihood-freeapproaches in accuracy and robustness when dealing with intractable state-spacemodels.</description><author>Edoardo Marcelli, Sean O'Hagan, Veronika Rockova</author><pubDate>Thu, 06 Nov 2025 17:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04552v1</guid></item><item><title>Confidential Computing for Cloud Security: Exploring Hardware based Encryption Using Trusted Execution Environments</title><link>http://arxiv.org/abs/2511.04550v1</link><description>The growth of cloud computing has revolutionized data processing and storagecapacities to another levels of scalability and flexibility. But in theprocess, it has created a huge challenge of security, especially in terms ofsafeguarding sensitive data. Classical security practices, including encryptionat rest and during transit, fail to protect data in use and expose it tovarious possible breaches. In response to this problem , Confidential Computinghas been a tool ,seeking to secure data in processing by usage ofhardware-based Trusted Execution Environments (TEEs). TEEs, including Intel'sSoftware Guard Extensions (SGX) and ARM's TrustZone, offers protected contextswithin the processor, where data is kept confidential ,intact and secure , evenwith malicious software or compromised operating systems. In this research, wehave explored the architecture and security features of TEEs like Intel SGX andARM TrustZone, and their effectiveness in improving cloud data security. From athorough literature survey ,we have analyzed the deployment strategies,performance indicators, and practical uses of these TEEs for the same purpose.In addition, we have discussed the issues regarding deployment, possibleweaknesses, scalability issues, and integration issues. Our results focuses onthe central position of TEEs in strengthening and advancing cloud securityinfrastructures, pointing towards their ability to create a secure foundationfor Confidential Computing.</description><author>Dhruv Deepak Agarwal, Aswani Kumar Cherukuri</author><pubDate>Thu, 06 Nov 2025 17:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04550v1</guid></item><item><title>Practical solutions to the relative pose of three calibrated cameras</title><link>http://arxiv.org/abs/2303.16078v5</link><description>We study the challenging problem of estimating the relative pose of threecalibrated cameras from four point correspondences. We propose novel efficientsolutions to this problem that are based on the simple idea of using fourcorrespondences to estimate an approximate geometry of the first two views. Wemodel this geometry either as an affine or a fully perspective geometryestimated using one additional approximate correspondence. We generate such anapproximate correspondence using a very simple and efficient strategy, wherethe new point is the mean point of three corresponding input points. The newsolvers are efficient and easy to implement, since they are based on existingefficient minimal solvers, i.e., the 4-point affine fundamental matrix, thewell-known 5-point relative pose solver, and the P3P solver. Extensiveexperiments on real data show that the proposed solvers, when properly coupledwith local optimization, achieve state-of-the-art results, with the novelsolver based on approximate mean-point correspondences being more robust andaccurate than the affine-based solver.</description><author>Charalambos Tzamos, Viktor Kocur, Yaqing Ding, Daniel Barath, Zuzana Berger Haladova, Torsten Sattler, Zuzana Kukelova</author><pubDate>Thu, 06 Nov 2025 17:00:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16078v5</guid></item><item><title>Revisiting Federated Fine-Tuning: A Single Communication Round is Enough for Foundation Models</title><link>http://arxiv.org/abs/2412.04650v2</link><description>The recent advancement of foundation models (FMs) has increased the demandfor fine-tuning these models on large-scale cross-domain datasets. To addressthis, federated fine-tuning has emerged, allowing FMs to be fine-tuned ondistributed datasets across multiple devices while ensuring data privacy.However, the substantial parameter size and the multi-round communication infederated learning algorithms result in prohibitively high communication costs,challenging the practicality of federated fine-tuning. In this paper, weidentify and analyze, both theoretically and empirically, that the traditionalmulti-round aggregation algorithms may not be necessary for federatedfine-tuning large FMs. Our experiments reveal that a single round ofaggregation (i.e., one-shot federated fine-tuning) yields a global modelperformance comparable to that achieved through multiple rounds of aggregation.Through rigorous mathematical and empirical analyses, we demonstrate that largeFMs, due to their extensive parameter sizes and pre-training on general tasks,achieve significantly lower training loss in one-shot federated fine-tuningcompared to smaller models. Our extensive experiments show that one-shotfederated fine-tuning significantly reduces communication costs. It also hasthe potential to enable asynchronous aggregation, enhances privacy, andmaintains performance consistency with multi-round federated fine-tuning onboth text generation and text-to-image generation tasks. Our findings provideinsights to revolutionize federated fine-tuning in practice, enhancingefficiency, reducing costs, and expanding accessibility for FMs.</description><author>Ziyao Wang, Bowei Tian, Yexiao He, Zheyu Shen, Guoheng Sun, Yuhan Liu, Luyang Liu, Meng Liu, Ang Li</author><pubDate>Thu, 06 Nov 2025 16:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04650v2</guid></item><item><title>LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems</title><link>http://arxiv.org/abs/2511.04541v1</link><description>Modeling user preferences across domains remains a key challenge in slaterecommendation (i.e. recommending an ordered sequence of items) research. Weinvestigate how Large Language Models (LLM) can effectively act as world modelsof user preferences through pairwise reasoning over slates. We conduct anempirical study involving several LLMs on three tasks spanning differentdatasets. Our results reveal relationships between task performance andproperties of the preference function captured by LLMs, hinting towards areasfor improvement and highlighting the potential of LLMs as world models inrecommender systems.</description><author>Baptiste Bonin, Maxime Heuillet, Audrey Durand</author><pubDate>Thu, 06 Nov 2025 16:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04541v1</guid></item><item><title>Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework</title><link>http://arxiv.org/abs/2511.03179v2</link><description>The engineering design process often demands expertise from multiple domains,leading to complex collaborations and iterative refinements. Traditionalmethods can be resource-intensive and prone to inefficiencies. To address this,we formalize the engineering design process through a multi-agent AI frameworkthat integrates structured design and review loops. The framework introducesspecialized knowledge-driven agents that collaborate to generate and refinedesign candidates. As an exemplar, we demonstrate its application to theaerodynamic optimization of 4-digit NACA airfoils. The framework consists ofthree key AI agents: a Graph Ontologist, a Design Engineer, and a SystemsEngineer. The Graph Ontologist employs a Large Language Model (LLM) toconstruct two domain-specific knowledge graphs from airfoil design literature.The Systems Engineer, informed by a human manager, formulates technicalrequirements that guide design generation and evaluation. The Design Engineerleverages the design knowledge graph and computational tools to proposecandidate airfoils meeting these requirements. The Systems Engineer reviews andprovides feedback both qualitative and quantitative using its own knowledgegraph, forming an iterative feedback loop until a design is validated by themanager. The final design is then optimized to maximize performance metricssuch as the lift-to-drag ratio. Overall, this work demonstrates howcollaborative AI agents equipped with structured knowledge representations canenhance efficiency, consistency, and quality in the engineering design process.</description><author>Varun Kumar, George Em Karniadakis</author><pubDate>Thu, 06 Nov 2025 16:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.03179v2</guid></item><item><title>OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights</title><link>http://arxiv.org/abs/2511.01019v2</link><description>Artificial intelligence is transforming the sciences, yet generalconversational AI systems often generate unverified "hallucinations"undermining scientific rigor. We present OceanAI, a conversational platformthat integrates the natural-language fluency of open-source large languagemodels (LLMs) with real-time, parameterized access to authoritativeoceanographic data streams hosted by the National Oceanic and AtmosphericAdministration (NOAA). Each query such as "What was Boston Harbor's highestwater level in 2024?" triggers real-time API calls that identify, parse, andsynthesize relevant datasets into reproducible natural-language responses anddata visualizations. In a blind comparison with three widely used AIchat-interface products, only OceanAI produced NOAA-sourced values withoriginal data references; others either declined to answer or providedunsupported results. Designed for extensibility, OceanAI connects to multipleNOAA data products and variables, supporting applications in marine hazardforecasting, ecosystem assessment, and water-quality monitoring. By groundingoutputs and verifiable observations, OceanAI advances transparency,reproducibility, and trust, offering a scalable framework for AI-enableddecision support within the oceans. A public demonstration is available athttps://oceanai.ai4ocean.xyz.</description><author>Bowen Chen, Jayesh Gajbhar, Gregory Dusek, Rob Redmon, Patrick Hogan, Paul Liu, DelWayne Bohnenstiehl, Dongkuan Xu, Ruoying He</author><pubDate>Thu, 06 Nov 2025 16:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.01019v2</guid></item><item><title>Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction</title><link>http://arxiv.org/abs/2409.07055v3</link><description>Legal judgment prediction (LJP), which enables litigants and their lawyers toforecast judgment outcomes and refine litigation strategies, has emerged as acrucial legal NLP task. Existing studies typically utilize legal facts, i.e.,facts that have been established by evidence and determined by the judge, topredict the judgment. However, legal facts are often difficult to obtain in theearly stages of litigation, significantly limiting the practical applicabilityof fact-based LJP. To address this limitation, we propose a novel legal NLPtask: legal fact prediction (LFP), which takes the evidence submitted bylitigants for trial as input to predict legal facts, thereby empoweringfact-based LJP technologies to make predictions in the absence of ground-truthlegal facts. We also propose the first benchmark dataset, LFPBench, forevaluating the LFP task. Our extensive experiments on LFPBench demonstrate theeffectiveness of LFP-empowered LJP and highlight promising research directionsfor LFP.</description><author>Junkai Liu, Yujie Tong, Hui Huang, Bowen Zheng, Yiran Hu, Peicheng Wu, Chuan Xiao, Makoto Onizuka, Muyun Yang, Shuyuan Zheng</author><pubDate>Thu, 06 Nov 2025 16:53:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07055v3</guid></item><item><title>Rater Equivalence: Evaluating Classifiers in Human Judgment Settings</title><link>http://arxiv.org/abs/2106.01254v2</link><description>In many decision settings, the definitive ground truth is either non-existentor inaccessible. We introduce a framework for evaluating classifiers basedsolely on human judgments. In such cases, it is helpful to compare automatedclassifiers to human judgment. We quantify a classifier's performance by itsrater equivalence: the smallest number of human raters whose combined judgmentmatches the classifier's performance. Our framework uses human-generated labelsboth to construct benchmark panels and to evaluate performance. We distinguishbetween two models of utility: one based on agreement with the assumed butinaccessible ground truth, and one based on matching individual humanjudgments. Using case studies and formal analysis, we demonstrate how thisframework can inform the evaluation and deployment of AI systems in practice.</description><author>Paul Resnick, Yuqing Kong, Grant Schoenebeck, Tim Weninger</author><pubDate>Thu, 06 Nov 2025 16:52:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.01254v2</guid></item><item><title>Unified Generative Latent Representation for Functional Brain Graphs</title><link>http://arxiv.org/abs/2511.04539v1</link><description>Functional brain graphs are often characterized with separate graph-theoreticor spectral descriptors, overlooking how these properties covary and partiallyoverlap across brains and conditions. We anticipate that dense, weightedfunctional connectivity graphs occupy a low-dimensional latent geometry alongwhich both topological and spectral structures display graded variations. Here,we estimated this unified graph representation and enabled generation of densefunctional brain graphs through a graph transformer autoencoder with latentdiffusion, with spectral geometry providing an inductive bias to guidelearning. This geometry-aware latent representation, although unsupervised,meaningfully separated working-memory states and decoded visual stimuli, withperformance further enhanced by incorporating neural dynamics. From thediffusion modeled distribution, we were able to sample biologically plausibleand structurally grounded synthetic dense graphs.</description><author>Subati Abulikemu, Tiago Azevedo, Michail Mamalakis, John Suckling</author><pubDate>Thu, 06 Nov 2025 16:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04539v1</guid></item><item><title>From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting</title><link>http://arxiv.org/abs/2511.04538v1</link><description>As the role of Large Language Models (LLM)-based coding assistants insoftware development becomes more critical, so does the role of the bugs theygenerate in the overall cybersecurity landscape. While a number of LLM codesecurity benchmarks have been proposed alongside approaches to improve thesecurity of generated code, it remains unclear to what extent they haveimpacted widely used coding LLMs. Here, we show that even the latestopen-weight models are vulnerable in the earliest reported vulnerabilityscenarios in a realistic use setting, suggesting that the safety-functionalitytrade-off has until now prevented effective patching of vulnerabilities. Tohelp address this issue, we introduce a new severity metric that reflects therisk posed by an LLM-generated vulnerability, accounting for vulnerabilityseverity, generation chance, and the formulation of the prompt that inducesvulnerable code generation - Prompt Exposure (PE). To encourage the mitigationof the most serious and prevalent vulnerabilities, we use PE to define theModel Exposure (ME) score, which indicates the severity and prevalence ofvulnerabilities a model generates.</description><author>Cyril Vallez, Alexander Sternfeld, Andrei Kucharavy, Ljiljana Dolamic</author><pubDate>Thu, 06 Nov 2025 16:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04538v1</guid></item><item><title>Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics</title><link>http://arxiv.org/abs/2511.04534v1</link><description>Reduced-order models (ROMs) can efficiently simulate high-dimensionalphysical systems, but lack robust uncertainty quantification methods. Existingapproaches are frequently architecture- or training-specific, which limitsflexibility and generalization. We introduce a post hoc, model-agnosticframework for predictive uncertainty quantification in latent space ROMs thatrequires no modification to the underlying architecture or training procedure.Using conformal prediction, our approach estimates statistical predictionintervals for multiple components of the ROM pipeline: latent dynamics,reconstruction, and end-to-end predictions. We demonstrate the method on alatent space dynamical model for cloud microphysics, where it accuratelypredicts the evolution of droplet-size distributions and quantifies uncertaintyacross the ROM pipeline.</description><author>Jonas E. Katona, Emily K. de Jong, Nipun Gunawardena</author><pubDate>Thu, 06 Nov 2025 16:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04534v1</guid></item><item><title>IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection</title><link>http://arxiv.org/abs/2511.04528v1</link><description>We present IntelliProof, an interactive system for analyzing argumentativeessays through LLMs. IntelliProof structures an essay as an argumentationgraph, where claims are represented as nodes, supporting evidence is attachedas node properties, and edges encode supporting or attacking relations. Unlikeexisting automated essay scoring systems, IntelliProof emphasizes the userexperience: each relation is initially classified and scored by an LLM, thenvisualized for enhanced understanding. The system provides justifications forclassifications and produces quantitative measures for essay coherence. Itenables rapid exploration of argumentative quality while retaining humanoversight. In addition, IntelliProof provides a set of tools for a betterunderstanding of an argumentative essay and its corresponding graph in naturallanguage, bridging the gap between the structural semantics of argumentativeessays and the user's understanding of a given text. A live demo and the systemare available here to try: \textbf{https://intelliproof.vercel.app}</description><author>Kaveh Eskandari Miandoab, Katharine Kowalyshyn, Kabir Pamnani, Anesu Gavhera, Vasanth Sarathy, Matthias Scheutz</author><pubDate>Thu, 06 Nov 2025 16:43:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04528v1</guid></item><item><title>Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics</title><link>http://arxiv.org/abs/2511.04527v1</link><description>When a language model generates text, the selection of individual tokensmight lead it down very different reasoning paths, making uncertainty difficultto quantify. In this work, we consider whether reasoning language modelsrepresent the alternate paths that they could take during generation. To testthis hypothesis, we use hidden activations to control and predict a languagemodel's uncertainty during chain-of-thought reasoning. In our experiments, wefind a clear correlation between how uncertain a model is at different tokens,and how easily the model can be steered by controlling its activations. Thissuggests that activation interventions are most effective when there arealternate paths available to the model -- in other words, when it has not yetcommitted to a particular final answer. We also find that hidden activationscan predict a model's future outcome distribution, demonstrating that modelsimplicitly represent the space of possible paths.</description><author>Amir Zur, Atticus Geiger, Ekdeep Singh Lubana, Eric Bigelow</author><pubDate>Thu, 06 Nov 2025 16:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04527v1</guid></item><item><title>Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy</title><link>http://arxiv.org/abs/2511.04525v1</link><description>Purpose: Accurate assessment of surgical complexity is essential inLaparoscopic Cholecystectomy (LC), where severe inflammation is associated withlonger operative times and increased risk of postoperative complications. TheParkland Grading Scale (PGS) provides a clinically validated framework forstratifying inflammation severity; however, its automation in surgical videosremains largely unexplored, particularly in realistic scenarios where completevideos must be analyzed without prior manual curation. Methods: In this work,we introduce STC-Net, a novel framework for SingleTimestamp-based Complexityestimation in LC via the PGS, designed to operate under weak temporalsupervision. Unlike prior methods limited to static images or manually trimmedclips, STC-Net operates directly on full videos. It jointly performs temporallocalization and grading through a localization, window proposal, and gradingmodule. We introduce a novel loss formulation combining hard and softlocalization objectives and background-aware grading supervision. Results:Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracyof 62.11% and an F1-score of 61.42%, outperforming non-localized baselines byover 10% in both metrics and highlighting the effectiveness of weak supervisionfor surgical complexity assessment. Conclusion: STC-Net demonstrates a scalableand effective approach for automated PGS-based surgical complexity estimationfrom full LC videos, making it promising for post-operative analysis andsurgical training.</description><author>Dimitrios Anastasiou, Santiago Barbarisi, Lucy Culshaw, Jayna Patel, Evangelos B. Mazomenos, Imanol Luengo, Danail Stoyanov</author><pubDate>Thu, 06 Nov 2025 16:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04525v1</guid></item><item><title>A Criminology of Machines</title><link>http://arxiv.org/abs/2511.02895v2</link><description>While the possibility of reaching human-like Artificial Intelligence (AI)remains controversial, the likelihood that the future will be characterized bya society with a growing presence of autonomous machines is high. Autonomous AIagents are already deployed and active across several industries and digitalenvironments and alongside human-human and human-machine interactions,machine-machine interactions are poised to become increasingly prevalent. Giventhese developments, I argue that criminology must begin to address theimplications of this transition for crime and social control. Drawing onActor-Network Theory and Woolgar's decades-old call for a sociology of machines-- frameworks that acquire renewed relevance with the rise of generative AIagents -- I contend that criminologists should move beyond conceiving AI solelyas a tool. Instead, AI agents should be recognized as entities with agencyencompassing computational, social, and legal dimensions. Building on theliterature on AI safety, I thus examine the risks associated with the rise ofmulti-agent AI systems, proposing a dual taxonomy to characterize the channelsthrough which interactions among AI agents may generate deviant, unlawful, orcriminal outcomes. I then advance and discuss four key questions that warranttheoretical and empirical attention: (1) Can we assume that machines willsimply mimic humans? (2) Will crime theories developed for humans suffice toexplain deviant or criminal behaviors emerging from interactions betweenautonomous AI agents? (3) What types of criminal behaviors will be affectedfirst? (4) How might this unprecedented societal shift impact policing? Thesequestions underscore the urgent need for criminologists to theoretically andempirically engage with the implications of multi-agent AI systems for thestudy of crime and play a more active role in debates on AI safety andgovernance.</description><author>Gian Maria Campedelli</author><pubDate>Thu, 06 Nov 2025 16:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.02895v2</guid></item><item><title>Artificial Intelligence in Elementary STEM Education: A Systematic Review of Current Applications and Future Challenges</title><link>http://arxiv.org/abs/2511.00105v2</link><description>Artificial intelligence (AI) is transforming elementary STEM education, yetevidence remains fragmented. This systematic review synthesizes 258 studies(2020-2025) examining AI applications across eight categories: intelligenttutoring systems (45% of studies), learning analytics (18%), automatedassessment (12%), computer vision (8%), educational robotics (7%), multimodalsensing (6%), AI-enhanced extended reality (XR) (4%), and adaptive contentgeneration. The analysis shows that most studies focus on upper elementarygrades (65%) and mathematics (38%), with limited cross-disciplinary STEMintegration (15%). While conversational AI demonstrates moderate effectiveness(d = 0.45-0.70 where reported), only 34% of studies include standardized effectsizes. Eight major gaps limit real-world impact: fragmented ecosystems,developmental inappropriateness, infrastructure barriers, lack of privacyframeworks, weak STEM integration, equity disparities, teacher marginalization,and narrow assessment scopes. Geographic distribution is also uneven, with 90%of studies originating from North America, East Asia, and Europe. Futuredirections call for interoperable architectures that support authentic STEMintegration, grade-appropriate design, privacy-preserving analytics, andteacher-centered implementations that enhance rather than replace humanexpertise.</description><author>Majid Memari, Krista Ruggles</author><pubDate>Thu, 06 Nov 2025 16:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.00105v2</guid></item><item><title>End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit</title><link>http://arxiv.org/abs/2511.04522v1</link><description>With our recently proposed method based on reinforcement learning (Mayfranket al. (2024), Comput. Chem. Eng. 190), Koopman surrogate models can be trainedfor optimal performance in specific (economic) nonlinear model predictivecontrol ((e)NMPC) applications. So far, our method has exclusively beendemonstrated on a small-scale case study. Herein, we show that our methodscales well to a more challenging demand response case study built on alarge-scale model of a single-product (nitrogen) air separation unit. Acrossall numerical experiments, we assume observability of only a few realisticallymeasurable plant variables. Compared to a purely system identification-basedKoopman eNMPC, which generates small economic savings but frequently violatesconstraints, our method delivers similar economic performance while avoidingconstraint violations.</description><author>Daniel Mayfrank, Kayra Dernek, Laura Lang, Alexander Mitsos, Manuel Dahmen</author><pubDate>Thu, 06 Nov 2025 16:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04522v1</guid></item><item><title>THEval. Evaluation Framework for Talking Head Video Generation</title><link>http://arxiv.org/abs/2511.04520v1</link><description>Video generation has achieved remarkable progress, with generated videosincreasingly resembling real ones. However, the rapid advance in generation hasoutpaced the development of adequate evaluation metrics. Currently, theassessment of talking head generation primarily relies on limited metrics,evaluating general video quality, lip synchronization, and on conducting userstudies. Motivated by this, we propose a new evaluation framework comprising 8metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)synchronization. In selecting the metrics, we place emphasis on efficiency, aswell as alignment with human preferences. Based on this considerations, westreamline to analyze fine-grained dynamics of head, mouth, and eyebrows, aswell as face quality. Our extensive experiments on 85,000 videos generated by17 state-of-the-art models suggest that while many algorithms excel in lipsynchronization, they face challenges with generating expressiveness andartifact-free details. These videos were generated based on a novel realdataset, that we have curated, in order to mitigate bias of training data. Ourproposed benchmark framework is aimed at evaluating the improvement ofgenerative methods. Original code, dataset and leaderboards will be publiclyreleased and regularly updated with new methods, in order to reflect progressin the field.</description><author>Nabyl Quignon, Baptiste Chopin, Yaohui Wang, Antitza Dantcheva</author><pubDate>Thu, 06 Nov 2025 16:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04520v1</guid></item><item><title>Approximate non-linear model predictive control with safety-augmented neural networks</title><link>http://arxiv.org/abs/2304.09575v3</link><description>Model predictive control (MPC) achieves stability and constraint satisfactionfor general nonlinear systems, but requires computationally expensive onlineoptimization. This paper studies approximations of such MPC controllers vianeural networks (NNs) to achieve fast online evaluation. We propose safetyaugmentation that yields deterministic guarantees for convergence andconstraint satisfaction despite approximation inaccuracies. We approximate theentire input sequence of the MPC with NNs, which allows us to verify online ifit is a feasible solution to the MPC problem. We replace the NN solution by asafe candidate based on standard MPC techniques whenever it is infeasible orhas worse cost. Our method requires a single evaluation of the NN and forwardintegration of the input sequence online, which is fast to compute onresource-constrained systems. The proposed control framework is illustratedusing two numerical non-linear MPC benchmarks of different complexity,demonstrating computational speedups that are orders of magnitude higher thanonline optimization. In the examples, we achieve deterministic safety throughthe safety-augmented NNs, where a naive NN implementation fails.</description><author>Henrik Hose, Johannes Köhler, Melanie N. Zeilinger, Sebastian Trimpe</author><pubDate>Thu, 06 Nov 2025 16:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09575v3</guid></item><item><title>Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity</title><link>http://arxiv.org/abs/2511.04518v1</link><description>We present a new benchmarking study comparing a boundary-constrainedEhrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classicalfinite element method combined with Crank--Nicolson time stepping (CN-FEM) forsolving the two-dimensional wave equation with homogeneous Dirichlet boundaryconditions. The B-EPGP construction leverages exponential-polynomial basesderived from the characteristic variety to enforce the PDE and boundaryconditions exactly and employs penalized least squares to estimate thecoefficients. To ensure fairness across paradigms, we introduce adegrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGPconsistently attains lower space-time $L^2$-error and maximum-in-time$L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders ofmagnitude.</description><author>Obed Amo, Samit Ghosh, Markus Lange-Hegermann, Bogdan Raiţă, Michael Pokojovy</author><pubDate>Thu, 06 Nov 2025 16:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04518v1</guid></item><item><title>Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers</title><link>http://arxiv.org/abs/2511.04514v1</link><description>The phenomenon of linear mode connectivity (LMC) links several aspects ofdeep learning, including training stability under noisy stochastic gradients,the smoothness and generalization of local minima (basins), the similarity andfunctional diversity of sampled models, and architectural effects on dataprocessing. In this work, we experimentally study LMC under data shifts andidentify conditions that mitigate their impact. We interpret data shifts as anadditional source of stochastic gradient noise, which can be reduced throughsmall learning rates and large batch sizes. These parameters influence whethermodels converge to the same local minimum or to regions of the loss landscapewith varying smoothness and generalization. Although models sampled via LMCtend to make similar errors more frequently than those converging to differentbasins, the benefit of LMC lies in balancing training efficiency against thegains achieved from larger, more diverse ensembles. Code and supplementarymaterials will be made publicly available at https://github.com/DLR-KI/LMC indue course.</description><author>C. Hepburn, T. Zielke, A. P. Raulf</author><pubDate>Thu, 06 Nov 2025 16:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04514v1</guid></item><item><title>Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning</title><link>http://arxiv.org/abs/2511.02818v2</link><description>Tabular data remain the predominant format for real-world applications. Yet,developing effective neural models for tabular data remains challenging due toheterogeneous feature types and complex interactions occurring at multiplescales. Recent advances in tabular in-context learning (ICL), such as TabPFNand TabICL, have achieved state-of-the-art performance comparable togradient-boosted trees (GBTs) without task-specific fine-tuning. However,current architectures exhibit key limitations: (1) single-scale featureprocessing that overlooks hierarchical dependencies, (2) dense attention withquadratic scaling in table width, and (3) strictly sequential componentprocessing that prevents iterative representation refinement andcross-component communication. To address these challenges, we introduceOrion-MSP, a tabular ICL architecture featuring three key innovations: (1)multi-scale processing to capture hierarchical feature interactions; (2)block-sparse attention combining windowed, global, and random patterns forscalable efficiency and long-range connectivity; and (3) a Perceiver-stylememory enabling safe bidirectional information flow across components. Acrossdiverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performancewhile scaling effectively to high-dimensional tables, establishing a newstandard for efficient tabular in-context learning. The model is publiclyavailable at https://github.com/Lexsi-Labs/Orion-MSP .</description><author>Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu</author><pubDate>Thu, 06 Nov 2025 16:29:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.02818v2</guid></item><item><title>$μ$NeuFMT: Optical-Property-Adaptive Fluorescence Molecular Tomography via Implicit Neural Representation</title><link>http://arxiv.org/abs/2511.04510v1</link><description>Fluorescence Molecular Tomography (FMT) is a promising technique fornon-invasive 3D visualization of fluorescent probes, but its reconstructionremains challenging due to the inherent ill-posedness and reliance oninaccurate or often-unknown tissue optical properties. While deep learningmethods have shown promise, their supervised nature limits generalizationbeyond training data. To address these problems, we propose $\mu$NeuFMT, aself-supervised FMT reconstruction framework that integrates implicitneural-based scene representation with explicit physical modeling of photonpropagation. Its key innovation lies in jointly optimize both the fluorescencedistribution and the optical properties ($\mu$) during reconstruction,eliminating the need for precise prior knowledge of tissue optics orpre-conditioned training data. We demonstrate that $\mu$NeuFMT robustlyrecovers accurate fluorophore distributions and optical coefficients even withseverely erroneous initial values (0.5$\times$ to 2$\times$ of ground truth).Extensive numerical, phantom, and in vivo validations show that $\mu$NeuFMToutperforms conventional and supervised deep learning approaches across diverseheterogeneous scenarios. Our work establishes a new paradigm for robust andaccurate FMT reconstruction, paving the way for more reliable molecular imagingin complex clinically related scenarios, such as fluorescence guided surgery.</description><author>Shihan Zhao, Jianru Zhang, Yanan Wu, Linlin Li, Siyuan Shen, Xingjun Zhu, Guoyan Zheng, Jiahua Jiang, Wuwei Ren</author><pubDate>Thu, 06 Nov 2025 16:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04510v1</guid></item><item><title>GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units</title><link>http://arxiv.org/abs/2507.18989v2</link><description>As AI workloads proliferate, optimizing arithmetic units is becomingincreasingly important for reducing the footprint of digital systems.Conventional design flows, which often rely on manual or heuristic-basedoptimization, are limited in their ability to thoroughly explore the vastdesign space. In this paper, we introduce GENIAL, a machine learning-basedframework for the automatic generation and optimization of arithmetic units,with a focus on multipliers. At the core of GENIAL is a Transformer-based surrogate model trained in twostages, involving self-supervised pretraining followed by supervisedfinetuning, to robustly forecast key hardware metrics such as power and areafrom abstracted design representations. By inverting the surrogate model,GENIAL efficiently searches for new operand encodings that directly minimizepower consumption in arithmetic units for specific input data distributions.Extensive experiments on large datasets demonstrate that GENIAL is consistentlymore sample efficient than other methods, and converges faster towardsoptimized designs. This enables deployment of a high-effort logic synthesisoptimization flow in the loop, improving the accuracy of the surrogate model.Notably, GENIAL automatically discovers encodings that achieve up to 18%switching activity savings within multipliers on representative AI workloadscompared with the conventional two's complement. We also demonstrate theversatility of our approach by achieving significant improvements on FiniteState Machines, highlighting GENIAL's applicability for a wide spectrum oflogic functions. Together, these advances mark a significant step towardautomated Quality-of-Results-optimized combinational circuit generation fordigital systems.</description><author>Maxence Bouvier, Ryan Amaudruz, Felix Arnold, Renzo Andri, Lukas Cavigelli</author><pubDate>Thu, 06 Nov 2025 16:26:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.18989v2</guid></item><item><title>Understanding Adam Requires Better Rotation Dependent Assumptions</title><link>http://arxiv.org/abs/2410.19964v3</link><description>Despite its widespread adoption, Adam's advantage over Stochastic GradientDescent (SGD) lacks a comprehensive theoretical explanation. This paperinvestigates Adam's sensitivity to rotations of the parameter space. We observethat Adam's performance in training transformers degrades under randomrotations of the parameter space, indicating a crucial sensitivity to thechoice of basis in practice. This reveals that conventional rotation-invariantassumptions are insufficient to capture Adam's advantages theoretically. Tobetter understand the rotation-dependent properties that benefit Adam, we alsoidentify structured rotations that preserve or even enhance its empiricalperformance. We then examine the rotation-dependent assumptions in theliterature and find that they fall short in explaining Adam's behaviour acrossvarious rotation types. In contrast, we verify the orthogonality of the updateas a promising indicator of Adam's basis sensitivity, suggesting it may be thekey quantity for developing rotation-dependent theoretical frameworks thatbetter explain its empirical success.</description><author>Tianyue H. Zhang, Lucas Maes, Alan Milligan, Alexia Jolicoeur-Martineau, Ioannis Mitliagkas, Damien Scieur, Simon Lacoste-Julien, Charles Guille-Escuret</author><pubDate>Thu, 06 Nov 2025 16:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19964v3</guid></item><item><title>Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways</title><link>http://arxiv.org/abs/2511.04506v1</link><description>Radiology reports are invaluable for clinical decision-making and hold greatpotential for automated analysis when structured into machine-readable formats.These reports often contain uncertainty, which we categorize into two distincttypes: (i) Explicit uncertainty reflects doubt about the presence or absence offindings, conveyed through hedging phrases. These vary in meaning depending onthe context, making rule-based systems insufficient to quantify the level ofuncertainty for specific findings; (ii) Implicit uncertainty arises whenradiologists omit parts of their reasoning, recording only key findings ordiagnoses. Here, it is often unclear whether omitted findings are truly absentor simply unmentioned for brevity. We address these challenges with a two-partframework. We quantify explicit uncertainty by creating an expert-validated,LLM-based reference ranking of common hedging phrases, and mapping each findingto a probability value based on this reference. In addition, we model implicituncertainty through an expansion framework that systematically addscharacteristic sub-findings derived from expert-defined diagnostic pathways for14 common diagnoses. Using these methods, we release Lunguage++, an expanded,uncertainty-aware version of the Lunguage benchmark of fine-grained structuredradiology reports. This enriched resource enables uncertainty-aware imageclassification, faithful diagnostic reasoning, and new investigations into theclinical impact of diagnostic uncertainty.</description><author>Paloma Rabaey, Jong Hak Moon, Jung-Oh Lee, Min Gwan Kim, Hangyul Yoon, Thomas Demeester, Edward Choi</author><pubDate>Thu, 06 Nov 2025 16:24:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04506v1</guid></item><item><title>Alternative Fairness and Accuracy Optimization in Criminal Justice</title><link>http://arxiv.org/abs/2511.04505v1</link><description>Algorithmic fairness has grown rapidly as a research area, yet key conceptsremain unsettled, especially in criminal justice. We review group, individual,and process fairness and map the conditions under which they conflict. We thendevelop a simple modification to standard group fairness. Rather than exactparity across protected groups, we minimize a weighted error loss while keepingdifferences in false negative rates within a small tolerance. This makessolutions easier to find, can raise predictive accuracy, and surfaces theethical choice of error costs. We situate this proposal within three classes ofcritique: biased and incomplete data, latent affirmative action, and theexplosion of subgroup constraints. Finally, we offer a practical framework fordeployment in public decision systems built on three pillars: need-baseddecisions, Transparency and accountability, and narrowly tailored definitionsand solutions. Together, these elements link technical design to legitimacy andprovide actionable guidance for agencies that use risk assessment and relatedtools.</description><author>Shaolong Wu, James Blume, Geshi Yeung</author><pubDate>Thu, 06 Nov 2025 16:24:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04505v1</guid></item><item><title>RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG</title><link>http://arxiv.org/abs/2511.04502v1</link><description>Retrieval-Augmented Generation (RAG) is a critical technique for groundingLarge Language Models (LLMs) in factual evidence, yet evaluating RAG systems inspecialized, safety-critical domains remains a significant challenge. Existingevaluation frameworks often rely on heuristic-based metrics that fail tocapture domain-specific nuances and other works utilize LLM-as-a-Judgeapproaches that lack validated alignment with human judgment. This paperintroduces RAGalyst, an automated, human-aligned agentic framework designed forthe rigorous evaluation of domain-specific RAG systems. RAGalyst features anagentic pipeline that generates high-quality, synthetic question-answering (QA)datasets from source documents, incorporating an agentic filtering step toensure data fidelity. The framework refines two key LLM-as-a-Judgemetrics-Answer Correctness and Answerability-using prompt optimization toachieve a strong correlation with human annotations. Applying this framework toevaluate various RAG components across three distinct domains (militaryoperations, cybersecurity, and bridge engineering), we find that performance ishighly context-dependent. No single embedding model, LLM, or hyperparameterconfiguration proves universally optimal. Additionally, we provide an analysison the most common low Answer Correctness reasons in RAG. These findingshighlight the necessity of a systematic evaluation framework like RAGalyst,which empowers practitioners to uncover domain-specific trade-offs and makeinformed design choices for building reliable and effective RAG systems.RAGalyst is available on our Github.</description><author>Joshua Gao, Quoc Huy Pham, Subin Varghese, Silwal Saurav, Vedhus Hoskere</author><pubDate>Thu, 06 Nov 2025 16:22:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04502v1</guid></item><item><title>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</title><link>http://arxiv.org/abs/2503.22879v4</link><description>State Space Models (SSMs) are emerging as a compelling alternative toTransformers because of their consistent memory usage and high performance.Despite this, scaling up SSMs on cloud services or limited-resource devices ischallenging due to their storage requirements and computational power. Toovercome this, quantizing SSMs with low bit-width data formats can reduce modelsize and benefit from hardware acceleration. As SSMs are prone toquantization-induced errors, recent efforts have focused on optimizing aparticular model or bit-width for efficiency without sacrificing performance.However, distinct bit-width configurations are essential for differentscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 forenhancing generation speed in short prompt applications for a single user. Tothis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for bothMamba1 and Mamba2 backbones, addressing the growing demand for SSM deploymenton various platforms. Based on the channel order preserving and activationpersistence of SSMs, we propose an offline approach to quantize inputs of alinear recurrence in 8-bit by sorting and clustering for input $x$, combinedwith a per-state-group quantization for input-dependent parameters $B$ and $C$.To ensure compute-invariance in the SSM output, we rearrange weights offlineaccording to the clustering sequence. The experiments show that Quamba2-8Boutperforms two state-of-the-art SSM quantization methods and delivers1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages,respectively, while offering 4$\times$ memory reduction with only a $1.6\%$average accuracy drop. The evaluation on MMLU shows the generalizability androbustness of our framework. The code and quantized models will be released at:https://github.com/enyac-group/Quamba.</description><author>Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu</author><pubDate>Thu, 06 Nov 2025 16:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.22879v4</guid></item></channel></rss>