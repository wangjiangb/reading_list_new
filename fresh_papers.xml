<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 28 Aug 2025 13:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning</title><link>http://arxiv.org/abs/2508.20096v1</link><description>Autonomous agents for Graphical User Interfaces (GUIs) face significantchallenges in specialized domains such as scientific computing, where bothlong-horizon planning and precise execution are required. Existing approachessuffer from a trade-off: generalist agents excel at planning but perform poorlyin execution, while specialized agents demonstrate the opposite weakness.Recent compositional frameworks attempt to bridge this gap by combining aplanner and an actor, but they are typically static and non-trainable, whichprevents adaptation from experience. This is a critical limitation given thescarcity of high-quality data in scientific domains. To address theselimitations, we introduce CODA, a novel and trainable compositional frameworkthat integrates a generalist planner (Cerebrum) with a specialist executor(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,Specialization, we apply a decoupled GRPO approach to train an expert plannerfor each scientific application individually, bootstrapping from a small set oftask trajectories. In the second stage, Generalization, we aggregate allsuccessful trajectories from the specialized experts to build a consolidateddataset, which is then used for supervised fine-tuning of the final planner.This equips CODA with both robust execution and cross-domain generalization.Evaluated on four challenging applications from the ScienceBoard benchmark,CODA significantly outperforms baselines and establishes a new state of the artamong open-source models.</description><author>Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang</author><pubDate>Wed, 27 Aug 2025 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20096v1</guid></item><item><title>Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning</title><link>http://arxiv.org/abs/2508.20095v1</link><description>Multi-Robot Motion Planning (MRMP) involves generating collision-freetrajectories for multiple robots operating in a shared continuous workspace.While discrete multi-agent path finding (MAPF) methods are broadly adopted dueto their scalability, their coarse discretization severely limits trajectoryquality. In contrast, continuous optimization-based planners offerhigher-quality paths but suffer from the curse of dimensionality, resulting inpoor scalability with respect to the number of robots. This paper tackles thelimitations of these two approaches by introducing a novel framework thatintegrates discrete MAPF solvers with constrained generative diffusion models.The resulting framework, called Discrete-Guided Diffusion (DGD), has three keycharacteristics: (1) it decomposes the original nonconvex MRMP problem intotractable subproblems with convex configuration spaces, (2) it combinesdiscrete MAPF solutions with constrained optimization techniques to guidediffusion models capture complex spatiotemporal dependencies among robots, and(3) it incorporates a lightweight constraint repair mechanism to ensuretrajectory feasibility. The proposed method sets a new state-of-the-artperformance in large-scale, complex environments, scaling to 100 robots whileachieving planning efficiency and high success rates.</description><author>Jinhao Liang, Sven Koenig, Ferdinando Fioretto</author><pubDate>Wed, 27 Aug 2025 17:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20095v1</guid></item><item><title>LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning</title><link>http://arxiv.org/abs/2406.05881v6</link><description>Large language models (LLMs) have shown remarkable abilities in logicalreasoning, in-context learning, and code generation. However, translatingnatural language instructions into effective robotic control policies remains asignificant challenge, especially for tasks requiring long-horizon planning andoperating under sparse reward conditions. Hierarchical Reinforcement Learning(HRL) provides a natural framework to address this challenge in robotics;however, it typically suffers from non-stationarity caused by the changingbehavior of the lower-level policy during training, destabilizing higher-levelpolicy learning. We introduce LGR2, a novel HRL framework that leverages LLMsto generate language-guided reward functions for the higher-level policy. Bydecoupling high-level reward generation from low-level policy changes, LGR2fundamentally mitigates the non-stationarity problem in off-policy HRL,enabling stable and efficient learning. To further enhance sample efficiency insparse environments, we integrate goal-conditioned hindsight experiencerelabeling. Extensive experiments across simulated and real-world roboticnavigation and manipulation tasks demonstrate LGR2 outperforms bothhierarchical and non-hierarchical baselines, achieving over 55% success rateson challenging tasks and robust transfer to real robots, without additionalfine-tuning.</description><author>Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri</author><pubDate>Wed, 27 Aug 2025 17:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05881v6</guid></item><item><title>Unifying the Extremes: Developing a Unified Model for Detecting and Predicting Extremist Traits and Radicalization</title><link>http://arxiv.org/abs/2501.04820v2</link><description>The proliferation of ideological movements into extremist factions via socialmedia has become a global concern. While radicalization has been studiedextensively within the context of specific ideologies, our ability toaccurately characterize extremism in more generalizable terms remainsunderdeveloped. In this paper, we propose a novel method for extracting andanalyzing extremist discourse across a range of online community forums. Byfocusing on verbal behavioral signatures of extremist traits, we develop aframework for quantifying extremism at both user and community levels. Ourresearch identifies 11 distinct factors, which we term ``The ExtremistEleven,'' as a generalized psychosocial model of extremism. Applying our methodto various online communities, we demonstrate an ability to characterizeideologically diverse communities across the 11 extremist traits. Wedemonstrate the power of this method by analyzing user histories from membersof the incel community. We find that our framework accurately predicts whichusers join the incel community up to 10 months before their actual entry withan AUC of $&gt;0.6$, steadily increasing to AUC ~0.9 three to four months beforethe event. Further, we find that upon entry into an extremist forum, the userstend to maintain their level of extremism within the community, while stillremaining distinguishable from the general online discourse. Our findingscontribute to the study of extremism by introducing a more holistic,cross-ideological approach that transcends traditional, trait-specific models.</description><author>Allison Lahnala, Vasudha Varadarajan, Lucie Flek, H. Andrew Schwartz, Ryan L. Boyd</author><pubDate>Wed, 27 Aug 2025 17:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.04820v2</guid></item><item><title>Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</title><link>http://arxiv.org/abs/2508.20089v1</link><description>Labelling images of Lepidoptera (moths) from automated camera systems isvital for understanding insect declines. However, accurate speciesidentification is challenging due to domain shifts between curated images andnoisy field imagery. We propose a lightweight classification approach,combining limited expert-labelled field data with knowledge distillation fromthe high-performance BioCLIP2 foundation model into a ConvNeXt-tinyarchitecture. Experiments on 101 Danish moth species from AMI camera systemsdemonstrate that BioCLIP2 substantially outperforms other methods and that ourdistilled lightweight model achieves comparable accuracy with significantlyreduced computational cost. These insights offer practical guidelines for thedevelopment of efficient insect monitoring systems and bridging domain gaps forfine-grained classification.</description><author>Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas Høye</author><pubDate>Wed, 27 Aug 2025 17:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20089v1</guid></item><item><title>AudioStory: Generating Long-Form Narrative Audio with Large Language Models</title><link>http://arxiv.org/abs/2508.20088v1</link><description>Recent advances in text-to-audio (TTA) generation excel at synthesizing shortaudio clips but struggle with long-form narrative audio, which requirestemporal coherence and compositional reasoning. To address this gap, we proposeAudioStory, a unified framework that integrates large language models (LLMs)with TTA systems to generate structured, long-form audio narratives. AudioStorypossesses strong instruction-following reasoning generation capabilities. Itemploys LLMs to decompose complex narrative queries into temporally orderedsub-tasks with contextual cues, enabling coherent scene transitions andemotional tone consistency. AudioStory has two appealing features: (1)Decoupled bridging mechanism: AudioStory disentangles LLM-diffusercollaboration into two specialized components, i.e., a bridging query forintra-event semantic alignment and a residual query for cross-event coherencepreservation. (2) End-to-end training: By unifying instruction comprehensionand audio generation within a single end-to-end framework, AudioStoryeliminates the need for modular training pipelines while enhancing synergybetween components. Furthermore, we establish a benchmark AudioStory-10K,encompassing diverse domains such as animated soundscapes and natural soundnarratives. Extensive experiments show the superiority of AudioStory on bothsingle-audio generation and narrative audio generation, surpassing prior TTAbaselines in both instruction-following ability and audio fidelity. Our code isavailable at https://github.com/TencentARC/AudioStory</description><author>Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan</author><pubDate>Wed, 27 Aug 2025 17:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20088v1</guid></item><item><title>Pseudo-Simulation for Autonomous Driving</title><link>http://arxiv.org/abs/2506.04218v2</link><description>Existing evaluation paradigms for Autonomous Vehicles (AVs) face criticallimitations. Real-world evaluation is often challenging due to safety concernsand a lack of reproducibility, whereas closed-loop simulation can faceinsufficient realism or high computational costs. Open-loop evaluation, whilebeing efficient and data-driven, relies on metrics that generally overlookcompounding errors. In this paper, we propose pseudo-simulation, a novelparadigm that addresses these limitations. Pseudo-simulation operates on realdatasets, similar to open-loop evaluation, but augments them with syntheticobservations generated prior to evaluation using 3D Gaussian Splatting. Our keyidea is to approximate potential future states the AV might encounter bygenerating a diverse set of observations that vary in position, heading, andspeed. Our method then assigns a higher importance to synthetic observationsthat best match the AV's likely behavior using a novel proximity-basedweighting scheme. This enables evaluating error recovery and the mitigation ofcausal confusion, as in closed-loop benchmarks, without requiring sequentialinteractive simulation. We show that pseudo-simulation is better correlatedwith closed-loop simulations ($R^2=0.8$) than the best existing open-loopapproach ($R^2=0.7$). We also establish a public leaderboard for the communityto benchmark new methodologies with pseudo-simulation. Our code is available athttps://github.com/autonomousvision/navsim.</description><author>Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta</author><pubDate>Wed, 27 Aug 2025 17:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.04218v2</guid></item><item><title>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</title><link>http://arxiv.org/abs/2506.18088v2</link><description>Simulation-based data synthesis has emerged as a powerful paradigm foradvancing real-world robotic manipulation. Yet existing datasets remaininsufficient for robust bimanual manipulation due to (1) the lack of scalabletask generation methods and (2) oversimplified simulation environments. Wepresent RoboTwin 2.0, a scalable framework for automated, large-scalegeneration of diverse and realistic data, together with unified evaluationprotocols for dual-arm manipulation. At its core is RoboTwin-OD, an objectlibrary of 731 instances across 147 categories with semantic andmanipulation-relevant annotations. Building on this, we design an expert datasynthesis pipeline that leverages multimodal language models (MLLMs) andsimulation-in-the-loop refinement to automatically generate task-levelexecution code. To improve sim-to-real transfer, RoboTwin 2.0 appliesstructured domain randomization along five axes: clutter, lighting, background,tabletop height, and language, enhancing data diversity and policy robustness.The framework is instantiated across 50 dual-arm tasks and five robotembodiments. Empirically, it yields a 10.9% gain in code generation successrate. For downstream policy learning, a VLA model trained with synthetic dataplus only 10 real demonstrations achieves a 367% relative improvement over the10-demo baseline, while zero-shot models trained solely on synthetic dataobtain a 228% gain. These results highlight the effectiveness of RoboTwin 2.0in strengthening sim-to-real transfer and robustness to environmentalvariations. We release the data generator, benchmark, dataset, and code tosupport scalable research in robust bimanual manipulation. Project Page:https://robotwin-platform.github.io/, Code:https://github.com/robotwin-Platform/robotwin/.</description><author>Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, Weiliang Deng, Yubin Guo, Tian Nian, Xuanbing Xie, Qiangyu Chen, Kailun Su, Tianling Xu, Guodong Liu, Mengkang Hu, Huan-ang Gao, Kaixuan Wang, Zhixuan Liang, Yusen Qin, Xiaokang Yang, Ping Luo, Yao Mu</author><pubDate>Wed, 27 Aug 2025 17:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.18088v2</guid></item><item><title>Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning</title><link>http://arxiv.org/abs/2508.20083v1</link><description>Retrieval-Augmented Generation (RAG) has become a standard approach forimproving the reliability of large language models (LLMs). Prior workdemonstrates the vulnerability of RAG systems by misleading them intogenerating attacker-chosen outputs through poisoning the knowledge base.However, this paper uncovers that such attacks could be mitigated by the strong\textit{self-correction ability (SCA)} of modern LLMs, which can reject falsecontext once properly configured. This SCA poses a significant challenge forattackers aiming to manipulate RAG systems. In contrast to previous poisoning methods, which primarily target theknowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm thatcompromises the retriever itself to suppress the SCA and enforceattacker-chosen outputs. This compromisation enables the attacker tostraightforwardly embed anti-SCA instructions into the context provided to thegenerator, thereby bypassing the SCA. To this end, we present acontrastive-learning-based model editing technique that performs localized andstealthy edits, ensuring the retriever returns a malicious instruction only forspecific victim queries while preserving benign retrieval behavior. To furtherstrengthen the attack, we design an iterative co-optimization framework thatautomatically discovers robust instructions capable of bypassing prompt-baseddefenses. We extensively evaluate DisarmRAG across six LLMs and three QAbenchmarks. Our results show near-perfect retrieval of malicious instructions,which successfully suppress SCA and achieve attack success rates exceeding 90\%under diverse defensive prompts. Also, the edited retriever remains stealthyunder several detection methods, highlighting the urgent need forretriever-centric defenses.</description><author>Yanbo Dai, Zhenlan Ji, Zongjie Li, Kuan Li, Shuai Wang</author><pubDate>Wed, 27 Aug 2025 17:49:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20083v1</guid></item><item><title>Approximate Lifted Model Construction</title><link>http://arxiv.org/abs/2504.20784v3</link><description>Probabilistic relational models such as parametric factor graphs enableefficient (lifted) inference by exploiting the indistinguishability of objects.In lifted inference, a representative of indistinguishable objects is used forcomputations. To obtain a relational (i.e., lifted) representation, theAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACPalgorithm, however, requires underlying distributions, encoded aspotential-based factorisations, to exactly match to identify and exploitindistinguishabilities. Hence, ACP is unsuitable for practical applicationswhere potentials learned from data inevitably deviate even if associatedobjects are indistinguishable. To mitigate this problem, we introduce the$\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, whichallows for a deviation of potentials depending on a hyperparameter$\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploitsindistinguishabilities that are not exact. We prove that the approximationerror induced by $\varepsilon$-ACP is strictly bounded and our experiments showthat the approximation error is close to zero in practice.</description><author>Malte Luttermann, Jan Speller, Marcel Gehrke, Tanya Braun, Ralf Möller, Mattis Hartwig</author><pubDate>Wed, 27 Aug 2025 17:48:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.20784v3</guid></item><item><title>Evaluating the Fitness of Ontologies for the Task of Question Generation</title><link>http://arxiv.org/abs/2504.07994v2</link><description>Ontology-based question generation is an important application ofsemantic-aware systems that enables the creation of large question banks fordiverse learning environments. The effectiveness of these systems, both interms of the calibre and cognitive difficulty of the resulting questions,depends heavily on the quality and modelling approach of the underlyingontologies, making it crucial to assess their fitness for this task. To date,there has been no comprehensive investigation into the specific ontologyaspects or characteristics that affect the question generation process.Therefore, this paper proposes a set of requirements and task-specific metricsfor evaluating the fitness of ontologies for question generation tasks inpedagogical settings. Using the ROMEO methodology (a structured framework usedfor identifying task-specific metrics), a set of evaluation metrics have beenderived from an expert assessment of questions generated by a questiongeneration model. To validate the proposed metrics, we apply them to a set ofontologies previously used in question generation to illustrate how the metricscores align with and complement findings reported in earlier studies. Theanalysis confirms that ontology characteristics significantly impact theeffectiveness of question generation, with different ontologies exhibitingvarying performance levels. This highlights the importance of assessingontology quality with respect to Automatic Question Generation (AQG) tasks.</description><author>Samah Alkhuzaey, Floriana Grasso, Terry R. Payne, Valentina Tamma</author><pubDate>Wed, 27 Aug 2025 17:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.07994v2</guid></item><item><title>Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images</title><link>http://arxiv.org/abs/2508.20080v1</link><description>360-degree visual content is widely shared on platforms such as YouTube andplays a central role in virtual reality, robotics, and autonomous navigation.However, consumer-grade dual-fisheye systems consistently yield imperfectpanoramas due to inherent lens separation and angular distortions. In thiswork, we introduce a novel calibration framework that incorporates adual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approachnot only simulates the realistic visual artifacts produced by dual-fisheyecameras but also enables the synthesis of seamlessly rendered 360-degreeimages. By jointly optimizing 3D Gaussian parameters alongside calibrationvariables that emulate lens gaps and angular distortions, our frameworktransforms imperfect omnidirectional inputs into flawless novel view synthesis.Extensive evaluations on real-world datasets confirm that our method producesseamless renderings-even from imperfect images-and outperforms existing360-degree rendering models.</description><author>Changha Shin, Woong Oh Cho, Seon Joo Kim</author><pubDate>Wed, 27 Aug 2025 17:46:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20080v1</guid></item><item><title>Refining Czech GEC: Insights from a Multi-Experiment Approach</title><link>http://arxiv.org/abs/2506.22402v2</link><description>We present a grammar error correction (GEC) system that achieves state of theart for the Czech language. Our system is based on a neural network translationapproach with the Transformer architecture, and its key feature is itsreal-time synthetic generation pipeline, which dynamically augments sentenceswith artificial errors by introducing both language-agnostic and Czech-specificerrors. We conduct a comprehensive series of experiments, investigating theCzech GEC corpora as bases for synthetic error introduction, several errorgeneration strategies, domain balancing, tokenization granularity, model size,and data scaling during fine-tuning. Additionally, we evaluate the performanceof large language models (LLMs) on Czech GEC in both end-user and expertfine-tuning scenarios. Our best-performing model is superior both inperformance and computational efficiency. The source code and the trained modellinks are available on https://github.com/ufal/tsd2025-gec.</description><author>Petr Pechman, Milan Straka, Jana Straková, Jakub Náplava</author><pubDate>Wed, 27 Aug 2025 17:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.22402v2</guid></item><item><title>Anomaly Detection in Networked Bandits</title><link>http://arxiv.org/abs/2508.20076v1</link><description>The nodes' interconnections on a social network often reflect theirdependencies and information-sharing behaviors. Nevertheless, abnormal nodes,which significantly deviate from most of the network concerning patterns orbehaviors, can lead to grave consequences. Therefore, it is imperative todesign efficient online learning algorithms that robustly learn users'preferences while simultaneously detecting anomalies. We introduce a novel bandit algorithm to address this problem. Throughnetwork knowledge, the method characterizes the users' preferences andresiduals of feature information. By learning and analyzing these preferencesand residuals, it develops a personalized recommendation strategy for each userand simultaneously detects anomalies. We rigorously prove an upper bound on theregret of the proposed algorithm and experimentally compare it with severalstate-of-the-art collaborative contextual bandit algorithms on both syntheticand real-world datasets.</description><author>Xiaotong Cheng, Setareh Maghsudi</author><pubDate>Wed, 27 Aug 2025 17:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20076v1</guid></item><item><title>Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</title><link>http://arxiv.org/abs/2508.20072v1</link><description>Vision-Language-Action (VLA) models adapt large vision-language backbones tomap images and instructions to robot actions. However, prevailing VLA decoderseither generate actions autoregressively in a fixed left-to-right order orattach continuous diffusion or flow matching heads outside the backbone,demanding specialized training and iterative sampling that hinder a unified,scalable architecture. We present Discrete Diffusion VLA, a single-transformerpolicy that models discretized action chunks with discrete diffusion and istrained with the same cross-entropy objective as the VLM backbone. The designretains diffusion's progressive refinement paradigm while remaining nativelycompatible with the discrete token interface of VLMs. Our method achieves anadaptive decoding order that resolves easy action elements before harder onesand uses secondary remasking to revisit uncertain predictions across refinementrounds, which improves consistency and enables robust error correction. Thisunified decoder preserves pretrained vision language priors, supports paralleldecoding, breaks the autoregressive bottleneck, and reduces the number offunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnvBridge, improving over both autoregressive and continuous diffusion baselines.These findings indicate that discrete-diffusion action decoder supports preciseaction modeling and consistent training, laying groundwork for scaling VLA tolarger models and datasets.</description><author>Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo</author><pubDate>Wed, 27 Aug 2025 17:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20072v1</guid></item><item><title>Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems</title><link>http://arxiv.org/abs/2506.22971v3</link><description>This paper introduces a two-timescale hierarchical decentralized controlarchitecture for Cyber-Physical Systems (CPS). The system consists of a globalcontroller (GC), and N local controllers (LCs). The GC operates at a slowertimescale, imposing budget constraints on the actions of LCs, which function ata faster timescale. Applications can be found in energy grid planning, wildfiremanagement, and other decentralized resource allocation problems. We proposeand analyze two optimization frameworks for this setting: COpt and FOpt. InCOpt, both GC and LCs together optimize infinite-horizon discounted rewards,while in FOpt the LCs optimize finite-horizon episodic rewards, and the GCoptimizes infinite-horizon rewards. Although both frameworks share identicalreward functions, their differing horizons can lead to different optimalpolicies. In particular, FOpt grants greater autonomy to LCs by allowing theirpolicies to be determined only by local objectives, unlike COpt. To ourknowledge, these frameworks have not been studied in the literature. Weestablish the formulations, prove the existence of optimal policies, and provethe convergence of their value iteration algorithms. We further show that COptalways achieves a higher value function than FOpt and derive explicit bounds ontheir difference. Finally, we establish a set of sufficient structuralconditions under which the two frameworks become equivalent.</description><author>Kesav Kaza, Ramachandran Anantharaman, Rahul Meshram</author><pubDate>Wed, 27 Aug 2025 17:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.22971v3</guid></item><item><title>11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis</title><link>http://arxiv.org/abs/2508.20068v1</link><description>For human cognitive process, spatial reasoning and perception are closelyentangled, yet the nature of this interplay remains underexplored in theevaluation of multimodal large language models (MLLMs). While recent MLLMadvancements show impressive performance on reasoning, their capacity forhuman-like spatial cognition remains an open question. In this work, weintroduce a systematic evaluation framework to assess the spatial reasoningabilities of state-of-the-art MLLMs relative to human performance. Central toour work is 11Plus-Bench, a high-quality benchmark derived from realisticstandardized spatial aptitude tests. 11Plus-Bench also features fine-grainedexpert annotations of both perceptual complexity and reasoning process,enabling detailed instance-level analysis of model behavior. Through extensiveexperiments across 14 MLLMs and human evaluation, we find that current MLLMsexhibit early signs of spatial cognition. Despite a large performance gapcompared to humans, MLLMs' cognitive profiles resemble those of humans in thatcognitive effort correlates strongly with reasoning-related complexity.However, instance-level performance in MLLMs remains largely random, whereashuman correctness is highly predictable and shaped by abstract patterncomplexity. These findings highlight both emerging capabilities and limitationsin current MLLMs' spatial reasoning capabilities and provide actionableinsights for advancing model design.</description><author>Chengzu Li, Wenshan Wu, Huanyu Zhang, Qingtao Li, Zeyu Gao, Yan Xia, José Hernández-Orallo, Ivan Vulić, Furu Wei</author><pubDate>Wed, 27 Aug 2025 17:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20068v1</guid></item><item><title>Neural Conditional Simulation for Complex Spatial Processes</title><link>http://arxiv.org/abs/2508.20067v1</link><description>A key objective in spatial statistics is to simulate from the distribution ofa spatial process at a selection of unobserved locations conditional onobservations (i.e., a predictive distribution) to enable spatial prediction anduncertainty quantification. However, exact conditional simulation from thispredictive distribution is intractable or inefficient for many spatial processmodels. In this paper, we propose neural conditional simulation (NCS), ageneral method for spatial conditional simulation that is based on neuraldiffusion models. Specifically, using spatial masks, we implement a conditionalscore-based diffusion model that evolves Gaussian noise into samples from apredictive distribution when given a partially observed spatial field andspatial process parameters as inputs. The diffusion model relies on a neuralnetwork that only requires unconditional samples from the spatial process fortraining. Once trained, the diffusion model is amortized with respect to theobservations in the partially observed field, the number and locations of thoseobservations, and the spatial process parameters, and can therefore be used toconditionally simulate from a broad class of predictive distributions withoutretraining the neural network. We assess the NCS-generated simulations againstsimulations from the true conditional distribution of a Gaussian process model,and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnickprocess model for spatial extremes. In the latter case, we show that it is moreefficient and accurate to conditionally simulate using NCS than classical MCMCtechniques implemented in standard software. We conclude that NCS enablesefficient and accurate conditional simulation from spatial predictivedistributions that are challenging to sample from using traditional methods.</description><author>Julia Walchessen, Andrew Zammit-Mangion, Raphaël Huser, Mikael Kuusela</author><pubDate>Wed, 27 Aug 2025 17:21:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20067v1</guid></item><item><title>PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence</title><link>http://arxiv.org/abs/2508.20066v1</link><description>Cross-view geo-localization is a critical task for UAV navigation, eventdetection, and aerial surveying, as it enables matching between drone-capturedand satellite imagery. Most existing approaches embed multi-modal data into ajoint feature space to maximize the similarity of paired images. However, thesemethods typically assume perfect alignment of image pairs during training,which rarely holds true in real-world scenarios. In practice, factors such asurban canyon effects, electromagnetic interference, and adverse weatherfrequently induce GPS drift, resulting in systematic alignment shifts whereonly partial correspondences exist between pairs. Despite its prevalence, thissource of noisy correspondence has received limited attention in currentresearch. In this paper, we formally introduce and address the NoisyCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming tobridge the gap between idealized benchmarks and practical applications. To thisend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), anovel framework that partitions and augments training data based on estimateddata uncertainty through uncertainty-aware co-augmentation and evidentialco-training. Specifically, PAUL selectively augments regions with highcorrespondence confidence and utilizes uncertainty estimation to refine featurelearning, effectively suppressing noise from misaligned pairs. Distinct fromtraditional filtering or label correction, PAUL leverages both data uncertaintyand loss discrepancy for targeted partitioning and augmentation, thus providingrobust supervision for noisy samples. Comprehensive experiments validate theeffectiveness of individual components in PAUL,which consistently achievessuperior performance over other competitive noisy-correspondence-driven methodsin various noise ratios.</description><author>Zheng Li, Yanming Guo, WenZhe Liu, Xueyi Zhang, Zhaoyun Ding, Long Xu, Mingrui Lao</author><pubDate>Wed, 27 Aug 2025 17:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20066v1</guid></item><item><title>Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices</title><link>http://arxiv.org/abs/2508.20064v1</link><description>Age-related Macular Degeneration (AMD) is a prevalent eye condition affectingvisual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatmentshave been effective in slowing the progression of neovascular AMD, with betteroutcomes achieved through timely diagnosis and consistent monitoring. Trackingthe progression of neovascular activity in OCT scans of patients with exudativeAMD allows for the development of more personalized and effective treatmentplans. This was the focus of the Monitoring Age-related Macular DegenerationProgression in Optical Coherence Tomography (MARIO) challenge, in which weparticipated. In Task 1, which involved classifying the evolution between twopairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNNnetwork with model ensembling to further enhance the model's performance. ForTask 2, which focused on predicting progression over the next three monthsbased on current exam data, we proposed the Patch Progression MaskedAutoencoder that generates an OCT for the next exam and then classifies theevolution between the current OCT and the one generated using our solution fromTask 1. The results we achieved allowed us to place in the Top 10 for bothtasks. Some team members are part of the same organization as the challengeorganizers; therefore, we are not eligible to compete for the prize.</description><author>Philippe Zhang, Weili Jiang, Yihao Li, Jing Zhang, Sarah Matta, Yubo Tan, Hui Lin, Haoshen Wang, Jiangtian Pan, Hui Xu, Laurent Borderie, Alexandre Le Guilcher, Béatrice Cochener, Chubin Ou, Gwenolé Quellec, Mathieu Lamard</author><pubDate>Wed, 27 Aug 2025 17:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20064v1</guid></item><item><title>StepWiser: Stepwise Generative Judges for Wiser Reasoning</title><link>http://arxiv.org/abs/2508.19229v2</link><description>As models increasingly leverage multi-step reasoning strategies to solvecomplex problems, supervising the logical validity of these intermediate stepshas become a critical research challenge. Process reward models address this byproviding step-by-step feedback, but current approaches have two majordrawbacks: they typically function as classifiers without providingexplanations, and their reliance on supervised fine-tuning with static datasetslimits generalization. Inspired by recent advances, we reframe stepwise rewardmodeling from a classification task to a reasoning task itself. We thus proposea generative judge that reasons about the policy model's reasoning steps (i.e.,meta-reasons), outputting thinking tokens before delivering a final verdict.Our model, StepWiser, is trained by reinforcement learning using relativeoutcomes of rollouts. We show it provides (i) better judgment accuracy onintermediate steps than existing methods; (ii) can be used to improve thepolicy model at training time; and (iii) improves inference-time search.</description><author>Wei Xiong, Wenting Zhao, Weizhe Yuan, Olga Golovneva, Tong Zhang, Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Wed, 27 Aug 2025 17:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19229v2</guid></item><item><title>OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</title><link>http://arxiv.org/abs/2508.20063v1</link><description>Open-vocabulary (OV) 3D object detection is an emerging field, yet itsexploration through image-based methods remains limited compared to 3D pointcloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-viewindoor 3D object detector trained without human annotations. In particular,OpenM3D is a single-stage detector adapting the 2D-induced voxel features fromthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic3D localization loss requiring high-quality 3D pseudo boxes and avoxel-semantic alignment loss requiring diverse pre-trained CLIP features. Wefollow the training setting of OV-3DET where posed RGB-D images are given butno human annotations of 3D boxes or classes are available. We propose a 3DPseudo Box Generation method using a graph embedding technique that combines 2Dsegments into coherent 3D structures. Our pseudo-boxes achieve higher precisionand recall than other methods, including the method proposed in OV-3DET. Wefurther sample diverse CLIP features from 2D segments associated with eachcoherent 3D structure to align with the corresponding voxel feature. The key totraining a highly accurate single-stage detector requires both losses to belearned toward high-quality targets. At inference, OpenM3D, a highly efficientdetector, requires only multi-view images for input and demonstrates superioraccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoorbenchmarks compared to existing methods. We outperform a strong two-stagemethod that leverages our class-agnostic detector with a ViT CLIP-based OVclassifier and a baseline incorporating multi-view depth estimator on bothaccuracy and speed.</description><author>Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo</author><pubDate>Wed, 27 Aug 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20063v1</guid></item><item><title>Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks</title><link>http://arxiv.org/abs/2508.20056v1</link><description>Failure-Directed Search (FDS) is a significant complete generic searchalgorithm used in Constraint Programming (CP) to efficiently explore the searchspace, proven particularly effective on scheduling problems. This paperanalyzes FDS's properties, showing that minimizing the size of its search treeguided by ranked branching decisions is closely related to the Multi-armedbandit (MAB) problem. Building on this insight, MAB reinforcement learningalgorithms are applied to FDS, extended with problem-specific refinements andparameter tuning, and evaluated on the two most fundamental schedulingproblems, the Job Shop Scheduling Problem (JSSP) and Resource-ConstrainedProject Scheduling Problem (RCPSP). The resulting enhanced FDS, using the bestextended MAB algorithm and configuration, performs 1.7 times faster on the JSSPand 2.1 times faster on the RCPSP benchmarks compared to the originalimplementation in a new solver called OptalCP, while also being 3.5 timesfaster on the JSSP and 2.1 times faster on the RCPSP benchmarks than thecurrent state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,using only a 900-second time limit per instance, the enhanced FDS improved theexisting state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSPstandard open benchmark instances while also completely closing a few of them.</description><author>Vilém Heinz, Petr Vilím, Zdeněk Hanzálek</author><pubDate>Wed, 27 Aug 2025 17:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20056v1</guid></item><item><title>GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation</title><link>http://arxiv.org/abs/2508.14036v2</link><description>We introduce GeoSAM2, a prompt-controllable framework for 3D partsegmentation that casts the task as multi-view 2D mask prediction. Given atextureless object, we render normal and point maps from predefined viewpointsand accept simple 2D prompts - clicks or boxes - to guide part selection. Theseprompts are processed by a shared SAM2 backbone augmented with LoRA andresidual geometry fusion, enabling view-specific reasoning while preservingpretrained priors. The predicted masks are back-projected to the object andaggregated across views. Our method enables fine-grained, part-specific controlwithout requiring text prompts, per-shape optimization, or full 3D labels. Incontrast to global clustering or scale-based methods, prompts are explicit,spatially grounded, and interpretable. We achieve state-of-the-artclass-agnostic performance on PartObjaverse-Tiny and PartNetE, outperformingboth slow optimization-based pipelines and fast but coarse feedforwardapproaches. Our results highlight a new paradigm: aligning the paradigm of 3Dsegmentation with SAM2, leveraging interactive 2D inputs to unlockcontrollability and precision in object-level part understanding.</description><author>Ken Deng, Yunhan Yang, Jingxiang Sun, Xihui Liu, Yebin Liu, Ding Liang, Yan-Pei Cao</author><pubDate>Wed, 27 Aug 2025 17:10:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14036v2</guid></item><item><title>Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution</title><link>http://arxiv.org/abs/2508.16557v2</link><description>Diffusion-based real-world image super-resolution (Real-ISR) methods havedemonstrated impressive performance. To achieve efficient Real-ISR, many worksemploy Variational Score Distillation (VSD) to distill pre-trainedstable-diffusion (SD) model for one-step SR with a fixed timestep. However, dueto the different noise injection timesteps, the SD will perform differentgenerative priors. Therefore, a fixed timestep is difficult for these methodsto fully leverage the generative priors in SD, leading to suboptimalperformance. To address this, we propose a Time-Aware one-step DiffusionNetwork for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder,which projects the same image into different latent features based ontimesteps. Through joint dynamic variation of timesteps and latent features,the student model can better align with the input pattern distribution of thepre-trained SD, thereby enabling more effective utilization of SD's generativecapabilities. To better activate the generative prior of SD at differenttimesteps, we propose a Time-Aware VSD loss that bridges the timesteps of thestudent model and those of the teacher model, thereby producing more consistentgenerative prior guidance conditioned on timesteps. Additionally, thoughutilizing the generative prior in SD at different timesteps, our method cannaturally achieve controllable trade-offs between fidelity and realism bychanging the timestep condition. Experimental results demonstrate that ourmethod achieves both state-of-the-art performance and controllable SR resultswith only a single step.</description><author>Tainyi Zhang, Zheng-Peng Duan, Peng-Tao Jiang, Bo Li, Ming-Ming Cheng, Chun-Le Guo, Chongyi Li</author><pubDate>Wed, 27 Aug 2025 17:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16557v2</guid></item><item><title>A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions</title><link>http://arxiv.org/abs/2504.06121v9</link><description>Lane detection is a critical component of Advanced Driver Assistance Systems(ADAS). Existing lane detection algorithms generally perform well underfavorable weather conditions. However, their performance degrades significantlyin adverse conditions, such as fog, which increases the risk of trafficaccidents. This challenge is compounded by the lack of specialized datasets andmethods designed for foggy environments. To address this, we introduce theFoggyLane dataset, captured in real-world foggy scenarios, and synthesize twoadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lanedetection datasets. Furthermore, we propose a robust Fog-Enhanced Network forlane detection, incorporating a Global Feature Fusion Module (GFFM) to captureglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) tomodel the structural and positional relationships of lane instances, and aLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggyconditions. Comprehensive experiments demonstrate that our method achievesstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 onFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRTacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIAJetson AGX Orin, confirming its real-time capabilities and robustness in foggyenvironments.</description><author>Ronghui Zhang, Yuhang Ma, Tengfei Li, Ziyu Lin, Yueying Wu, Junzhou Chen, Lin Zhang, Jia Hu, Tony Z. Qiu, Konghui Guo</author><pubDate>Wed, 27 Aug 2025 16:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.06121v9</guid></item><item><title>Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful</title><link>http://arxiv.org/abs/2507.07101v2</link><description>Conventional wisdom dictates that small batch sizes make language modelpretraining and fine-tuning unstable, motivating gradient accumulation, whichtrades off the number of optimizer steps for a proportional increase in batchsize. While it is common to decrease the learning rate for smaller batch sizes,other hyperparameters are often held fixed. In this work, we revisit smallbatch sizes all the way down to batch size one, and we propose a rule forscaling Adam hyperparameters to small batch sizes. In particular, rather thanholding the decay rate of the second moment fixed across batch sizes, wepropose to hold its half-life fixed in terms of tokens. We find that smallbatch sizes (1) train stably, (2) are consistently more robust tohyperparameter choices, (3) achieve equal or better per-FLOP performance thanlarger batch sizes, and (4) notably enable stable language model training withvanilla SGD, even without momentum, despite storing no optimizer state.Building on these results, we provide practical recommendations for selecting abatch size and setting optimizer hyperparameters. We further recommend againstgradient accumulation unless training on multiple devices with multiple modelreplicas. Finally, we show that a small batch size combined with an optimizerwith a small state size can provide the performance benefits of fullfine-tuning while maintaining a similar memory footprint to LoRA.</description><author>Martin Marek, Sanae Lotfi, Aditya Somasundaram, Andrew Gordon Wilson, Micah Goldblum</author><pubDate>Wed, 27 Aug 2025 16:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.07101v2</guid></item><item><title>AraHealthQA 2025 Shared Task Description Paper</title><link>http://arxiv.org/abs/2508.20047v1</link><description>We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health QuestionAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-locatedwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabicmedical QA resources by offering two complementary tracks: {MentalQA}, focusingon Arabic mental health Q\&amp;A (e.g., anxiety, depression, stigma reduction), and{MedArabiQ}, covering broader medical domains such as internal medicine,pediatrics, and clinical decision making. Each track comprises multiplesubtasks, evaluation datasets, and standardized metrics, facilitating fairbenchmarking. The task was structured to promote modeling under realistic,multilingual, and culturally nuanced healthcare contexts. We outline thedataset creation, task design and evaluation framework, participationstatistics, baseline systems, and summarize the overall outcomes. We concludewith reflections on the performance trends observed and prospects for futureiterations in Arabic health QA.</description><author>Hassan Alhuzali, Farah Shamout, Muhammad Abdul-Mageed, Chaimae Abouzahir, Mouath Abu-Daoud, Ashwag Alasmari, Walid Al-Eisawi, Renad Al-Monef, Ali Alqahtani, Lama Ayash, Nizar Habash, Leen Kharouf</author><pubDate>Wed, 27 Aug 2025 16:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20047v1</guid></item><item><title>Scaling Decentralized Learning with FLock</title><link>http://arxiv.org/abs/2507.15349v2</link><description>Fine-tuning the large language models (LLMs) are prevented by the deficiencyof centralized control and the massive computing and communication overhead onthe decentralized schemes. While the typical standard federated learning (FL)supports data privacy, the central server requirement creates a single point ofattack and vulnerability to poisoning attacks. Generalizing the result in thisdirection to 70B-parameter models in the heterogeneous, trustless environmentshas turned out to be a huge, yet unbroken bottleneck. This paper introducesFLock, a decentralized framework for secure and efficient collaborative LLMfine-tuning. Integrating a blockchain-based trust layer with economicincentives, FLock replaces the central aggregator with a secure, auditableprotocol for cooperation among untrusted parties. We present the firstempirical validation of fine-tuning a 70B LLM in a secure, multi-domain,decentralized setting. Our experiments show the FLock framework defends againstbackdoor poisoning attacks that compromise standard FL optimizers and fosterssynergistic knowledge transfer. The resulting models show a &gt;68% reduction inadversarial attack success rates. The global model also demonstrates superiorcross-domain generalization, outperforming models trained in isolation on theirown specialized data.</description><author>Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo</author><pubDate>Wed, 27 Aug 2025 16:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.15349v2</guid></item><item><title>From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity</title><link>http://arxiv.org/abs/2508.19172v2</link><description>Autonomous skill discovery aims to enable robots to acquire diverse behaviorswithout explicit supervision. Learning such behaviors directly on physicalhardware remains challenging due to safety and data efficiency constraints.Existing methods, including Quality-Diversity Actor-Critic (QDAC), requiremanually defined skill spaces and carefully tuned heuristics, limitingreal-world applicability. We propose Unsupervised Real-world Skill Acquisition(URSA), an extension of QDAC that enables robots to autonomously discover andmaster diverse, high-performing skills directly in the real world. Wedemonstrate that URSA successfully discovers diverse locomotion skills on aUnitree A1 quadruped in both simulation and the real world. Our approachsupports both heuristic-driven skill discovery and fully unsupervised settings.We also show that the learned skill repertoire can be reused for downstreamtasks such as real-world damage adaptation, where URSA outperforms allbaselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.Our results establish a new framework for real-world robot learning thatenables continuous skill discovery with limited human intervention,representing a significant step toward more autonomous and adaptable roboticsystems. Demonstration videos are available athttps://adaptive-intelligent-robotics.github.io/URSA.</description><author>Luca Grillotti, Lisa Coiffard, Oscar Pang, Maxence Faldor, Antoine Cully</author><pubDate>Wed, 27 Aug 2025 07:38:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19172v2</guid></item><item><title>Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration</title><link>http://arxiv.org/abs/2508.03337v5</link><description>The practical application of Multimodal Large Language Models (MLLMs) toVideo Question Answering (Video-QA) is severely hindered by the high token costof processing numerous video frames. While increasing the number of sampledframes is a common strategy, we observe a "less is more" phenomenon whereexcessive frames can paradoxically degrade performance due to context dilution.Concurrently, state-of-the-art keyframe selection methods, while effective,still yield significant temporal redundancy, which we term 'visual echoes'. Toaddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novelpost-processing method that intelligently prunes the selected keyframes. AFPemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 andCLIP feature space to identify and merge these echoes into singlerepresentatives. To compensate for information loss, we then introduce alightweight, text-based semantic graph that provides critical context withminimal token overhead. Conducting extensive experiments on the LongVideoBenchand VideoMME benchmarks across multiple leading MLLMs, our full approachdemonstrates a drastic reduction in required frames by up to 86.9% and totalinput tokens by up to 83.2%. Crucially, by providing a concise, high-qualityset of frames, our method not only enhances efficiency but often improvesaccuracy over baselines that use more frames. The code will be released uponpublication.</description><author>Shaoguang Wang, Ziyang Chen, Yijie Xu, Weiyu Guo, Hui Xiong</author><pubDate>Wed, 27 Aug 2025 01:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.03337v5</guid></item><item><title>Leveraging Multi-facet Paths for Heterogeneous Graph Representation Learning</title><link>http://arxiv.org/abs/2407.20648v4</link><description>Recent advancements in graph neural networks (GNNs) and heterogeneous GNNs(HGNNs) have advanced node embeddings and relationship learning for varioustasks. However, existing methods often rely on domain-specific predefinedmeta-paths, which are coarse-grained and focus solely on aspects like nodetype, limiting their ability to capture complex interactions. We introduceMF2Vec, a model that uses multi-faceted (fine-grained) paths instead ofpredefined meta-paths. MF2Vec extracts paths via random walks and generatesmulti-faceted vectors, ignoring predefined schemas. This method learns diverseaspects of nodes and their relationships, constructs a homogeneous network, andcreates node embeddings for classification, link prediction, and clustering.Extensive experiments show that MF2Vec outperforms existing methods, offering amore flexible and comprehensive framework for analyzing complex networks. Thecode is available at https://anonymous.4open.science/r/MF2Vec-6ABC.</description><author>Jongwoo Kim, Seongyeub Chu, Hyeongmin Park, Bryan Wong, Keejun Han, Mun Yong Yi</author><pubDate>Wed, 27 Aug 2025 01:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20648v4</guid></item><item><title>FastMesh: Efficient Artistic Mesh Generation via Component Decoupling</title><link>http://arxiv.org/abs/2508.19188v2</link><description>Recent mesh generation approaches typically tokenize triangle meshes intosequences of tokens and train autoregressive models to generate these tokenssequentially. Despite substantial progress, such token sequences inevitablyreuse vertices multiple times to fully represent manifold meshes, as eachvertex is shared by multiple faces. This redundancy leads to excessively longtoken sequences and inefficient generation processes. In this paper, we proposean efficient framework that generates artistic meshes by treating vertices andfaces separately, significantly reducing redundancy. We employ anautoregressive model solely for vertex generation, decreasing the token countto approximately 23\% of that required by the most compact existing tokenizer.Next, we leverage a bidirectional transformer to complete the mesh in a singlestep by capturing inter-vertex relationships and constructing the adjacencymatrix that defines the mesh faces. To further improve the generation quality,we introduce a fidelity enhancer to refine vertex positioning into more naturalarrangements and propose a post-processing framework to remove undesirable edgeconnections. Experimental results show that our method achieves more than8$\times$ faster speed on mesh generation compared to state-of-the-artapproaches, while producing higher mesh quality.</description><author>Jeonghwan Kim, Yushi Lan, Armando Fortes, Yongwei Chen, Xingang Pan</author><pubDate>Wed, 27 Aug 2025 01:23:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19188v2</guid></item><item><title>Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using a Retrieval-Augmented Model Selection Framework</title><link>http://arxiv.org/abs/2508.14940v2</link><description>Accurate lung cancer risk prediction remains challenging due to substantialvariability across patient populations and clinical settings -- no single modelperforms best for all cohorts. To address this, we propose a personalized lungcancer risk prediction agent that dynamically selects the most appropriatemodel for each patient by combining cohort-specific knowledge with modernretrieval and reasoning techniques. Given a patient's CT scan and structuredmetadata -- including demographic, clinical, and nodule-level features -- theagent first performs cohort retrieval using FAISS-based similarity searchacross nine diverse real-world cohorts to identify the most relevant patientpopulation from a multi-institutional database. Second, a Large Language Model(LLM) is prompted with the retrieved cohort and its associated performancemetrics to recommend the optimal prediction algorithm from a pool of eightrepresentative models, including classical linear risk models (e.g., Mayo,Brock), temporally-aware models (e.g., TD-VIT, DLSTM), and multi-modal computervision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agentpipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic,cohort-aware risk prediction personalized to each patient's profile. Buildingon this architecture, the agent supports flexible and cohort-driven modelselection across diverse clinical populations, offering a practical path towardindividualized risk assessment in real-world lung cancer screening.</description><author>Chongyu Qu, Allen J. Luna, Thomas Z. Li, Junchao Zhu, Junlin Guo, Juming Xiong, Kim L. Sandler, Bennett A. Landman, Yuankai Huo</author><pubDate>Tue, 26 Aug 2025 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14940v2</guid></item><item><title>VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space</title><link>http://arxiv.org/abs/2508.19247v1</link><description>3D local editing of specified regions is crucial for game industry and robotinteraction. Recent methods typically edit rendered multi-view images and thenreconstruct 3D models, but they face challenges in precisely preservingunedited regions and overall coherence. Inspired by structured 3D generativemodels, we propose VoxHammer, a novel training-free approach that performsprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammerfirst predicts its inversion trajectory and obtains its inverted latents andkey-value tokens at each timestep. Subsequently, in the denoising and editingphase, we replace the denoising features of preserved regions with thecorresponding inverted latents and cached key-value tokens. By retaining thesecontextual features, this approach ensures consistent reconstruction ofpreserved areas and coherent integration of edited parts. To evaluate theconsistency of preserved regions, we constructed Edit3D-Bench, ahuman-annotated dataset comprising hundreds of samples, each with carefullylabeled 3D editing regions. Experiments demonstrate that VoxHammersignificantly outperforms existing methods in terms of both 3D consistency ofpreserved regions and overall quality. Our method holds promise forsynthesizing high-quality edited paired data, thereby laying the datafoundation for in-context 3D generation. See our project page athttps://huanngzh.github.io/VoxHammer-Page/.</description><author>Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng</author><pubDate>Tue, 26 Aug 2025 17:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19247v1</guid></item><item><title>Style4D-Bench: A Benchmark Suite for 4D Stylization</title><link>http://arxiv.org/abs/2508.19243v1</link><description>We introduce Style4D-Bench, the first benchmark suite specifically designedfor 4D stylization, with the goal of standardizing evaluation and facilitatingprogress in this emerging area. Style4D-Bench comprises: 1) a comprehensiveevaluation protocol measuring spatial fidelity, temporal coherence, andmulti-view consistency through both perceptual and quantitative metrics, 2) astrong baseline that make an initial attempt for 4D stylization, and 3) acurated collection of high-resolution dynamic 4D scenes with diverse motionsand complex backgrounds. To establish a strong baseline, we present Style4D, anovel framework built upon 4D Gaussian Splatting. It consists of three keycomponents: a basic 4DGS scene representation to capture reliable geometry, aStyle Gaussian Representation that leverages lightweight per-Gaussian MLPs fortemporally and spatially aware appearance control, and a HolisticGeometry-Preserved Style Transfer module designed to enhance spatio-temporalconsistency via contrastive coherence learning and structural contentpreservation. Extensive experiments on Style4D-Bench demonstrate that Style4Dachieves state-of-the-art performance in 4D stylization, producing fine-grainedstylistic details with stable temporal dynamics and consistent multi-viewrendering. We expect Style4D-Bench to become a valuable resource forbenchmarking and advancing research in stylized rendering of dynamic 3D scenes.Project page: https://becky-catherine.github.io/Style4D . Code:https://github.com/Becky-catherine/Style4D-Bench .</description><author>Beiqi Chen, Shuai Shao, Haitang Feng, Jianhuang Lai, Jianlou Si, Guangcong Wang</author><pubDate>Tue, 26 Aug 2025 17:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19243v1</guid></item><item><title>Articulate3D: Zero-Shot Text-Driven 3D Object Posing</title><link>http://arxiv.org/abs/2508.19244v1</link><description>We propose a training-free method, Articulate3D, to pose a 3D asset throughlanguage control. Despite advances in vision and language models, this taskremains surprisingly challenging. To achieve this goal, we decompose theproblem into two steps. We modify a powerful image-generator to create targetimages conditioned on the input image and a text instruction. We then align themesh to the target images through a multi-view pose optimisation step. Indetail, we introduce a self-attention rewiring mechanism (RSActrl) thatdecouples the source structure from pose within an image generative model,allowing it to maintain a consistent structure across varying poses. Weobserved that differentiable rendering is an unreliable signal for articulationoptimisation; instead, we use keypoints to establish correspondences betweeninput and target images. The effectiveness of Articulate3D is demonstratedacross a diverse range of 3D objects and free-form text prompts, successfullymanipulating poses while maintaining the original identity of the mesh.Quantitative evaluations and a comparative user study, in which our method waspreferred over 85\% of the time, confirm its superiority over existingapproaches. Project page:https://odeb1.github.io/articulate3d_page_deb/</description><author>Oishi Deb, Anjun Hu, Ashkan Khakzar, Philip Torr, Christian Rupprecht</author><pubDate>Tue, 26 Aug 2025 17:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19244v1</guid></item><item><title>Autoregressive Universal Video Segmentation Model</title><link>http://arxiv.org/abs/2508.19242v1</link><description>Recent video foundation models such as SAM2 excel at prompted videosegmentation by treating masks as a general-purpose primitive. However, manyreal-world settings require unprompted segmentation that aims to detect andtrack all objects in a video without external cues, leaving today's landscapefragmented across task-specific models and pipelines. We recast streaming videosegmentation as sequential mask prediction, analogous to language modeling, andintroduce the Autoregressive Universal Segmentation Model (AUSM), a singlearchitecture that unifies both prompted and unprompted video segmentation.Built on recent state-space models, AUSM maintains a fixed-size spatial stateand scales to video streams of arbitrary length. Furthermore, all components ofAUSM are designed for parallel training across frames, yielding substantialspeedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS2018 &amp; 2019, MOSE, YouTube-VIS 2019 &amp; 2021, and OVIS) AUSM outperforms prioruniversal streaming video segmentation methods and achieves up to 2.5x fastertraining on 16-frame sequences.</description><author>Miran Heo, Sukjun Hwang, Min-Hung Chen, Yu-Chiang Frank Wang, Albert Gu, Seon Joo Kim, Ryo Hachiuma</author><pubDate>Tue, 26 Aug 2025 17:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19242v1</guid></item><item><title>Model Context Protocols in Adaptive Transport Systems: A Survey</title><link>http://arxiv.org/abs/2508.19239v1</link><description>The rapid expansion of interconnected devices, autonomous systems, and AIapplications has created severe fragmentation in adaptive transport systems,where diverse protocols and context sources remain isolated. This surveyprovides the first systematic investigation of the Model Context Protocol (MCP)as a unifying paradigm, highlighting its ability to bridge protocol-leveladaptation with context-aware decision making. Analyzing establishedliterature, we show that existing efforts have implicitly converged towardMCP-like architectures, signaling a natural evolution from fragmented solutionsto standardized integration frameworks. We propose a five-category taxonomycovering adaptive mechanisms, context-aware frameworks, unification models,integration strategies, and MCP-enabled architectures. Our findings revealthree key insights: traditional transport protocols have reached the limits ofisolated adaptation, MCP's client-server and JSON-RPC structure enablessemantic interoperability, and AI-driven transport demands integrationparadigms uniquely suited to MCP. Finally, we present a research roadmappositioning MCP as a foundation for next-generation adaptive, context-aware,and intelligent transport infrastructures.</description><author>Gaurab Chhetri, Shriyank Somvanshi, Md Monzurul Islam, Shamyo Brotee, Mahmuda Sultana Mimi, Dipti Koirala, Biplov Pandey, Subasish Das</author><pubDate>Tue, 26 Aug 2025 17:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19239v1</guid></item><item><title>MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation</title><link>http://arxiv.org/abs/2508.19236v1</link><description>Temporal context is essential for robotic manipulation because such tasks areinherently non-Markovian, yet mainstream VLA models typically overlook it andstruggle with long-horizon, temporally dependent tasks. Cognitive sciencesuggests that humans rely on working memory to buffer short-livedrepresentations for immediate control, while the hippocampal system preservesverbatim episodic details and semantic gist of past experience for long-termmemory. Inspired by these mechanisms, we propose MemoryVLA, aCognition-Memory-Action framework for long-horizon robotic manipulation. Apretrained VLM encodes the observation into perceptual and cognitive tokensthat form working memory, while a Perceptual-Cognitive Memory Bank storeslow-level details and high-level semantics consolidated from it. Working memoryretrieves decision-relevant entries from the bank, adaptively fuses them withcurrent tokens, and updates the bank by merging redundancies. Using thesetokens, a memory-conditioned diffusion action expert yields temporally awareaction sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasksacross three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, itachieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperformingstate-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain onBridge. On 12 real-world tasks spanning general skills and long-horizontemporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizontasks showing a +26 improvement over state-of-the-art baseline. Project Page:https://shihao1895.github.io/MemoryVLA</description><author>Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, Gao Huang</author><pubDate>Tue, 26 Aug 2025 17:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19236v1</guid></item><item><title>Emergent time-keeping mechanisms in a deep reinforcement learning agent performing an interval timing task</title><link>http://arxiv.org/abs/2508.15784v2</link><description>Drawing parallels between Deep Artificial Neural Networks (DNNs) andbiological systems can aid in understanding complex biological mechanisms thatare difficult to disentangle. Temporal processing, an extensively researchedtopic, is one such example that lacks a coherent understanding of itsunderlying mechanisms. In this study, we investigate temporal processing in aDeep Reinforcement Learning (DRL) agent performing an interval timing task andexplore potential biological counterparts to its emergent behavior. The agentwas successfully trained to perform a duration production task, which involvedmarking successive occurrences of a target interval while viewing a videosequence. Analysis of the agent's internal states revealed oscillatory neuralactivations, a ubiquitous pattern in biological systems. Interestingly, theagent's actions were predominantly influenced by neurons exhibiting theseoscillations with high amplitudes and frequencies corresponding to the targetinterval. Parallels are drawn between the agent's time-keeping strategy and theStriatal Beat Frequency (SBF) model, a biologically plausible model of intervaltiming. Furthermore, the agent maintained its oscillatory representations andtask performance when tested on different video sequences (including a blankvideo). Thus, once learned, the agent internalized its time-keeping mechanismand showed minimal reliance on its environment to perform the timing task. Ahypothesis about the resemblance between this emergent behavior and certainaspects of the evolution of biological processes like circadian rhythms, hasbeen discussed. This study aims to contribute to recent research efforts ofutilizing DNNs to understand biological systems, with a particular emphasis ontemporal processing.</description><author>Amrapali Pednekar, Alvaro Garrido, Pieter Simoens, Yara Khaluf</author><pubDate>Tue, 26 Aug 2025 17:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15784v2</guid></item><item><title>Automated Feature Tracking for Real-Time Kinematic Analysis and Shape Estimation of Carbon Nanotube Growth</title><link>http://arxiv.org/abs/2508.19232v1</link><description>Carbon nanotubes (CNTs) are critical building blocks in nanotechnology, yetthe characterization of their dynamic growth is limited by the experimentalchallenges in nanoscale motion measurement using scanning electron microscopy(SEM) imaging. Existing ex situ methods offer only static analysis, while insitu techniques often require manual initialization and lack continuousper-particle trajectory decomposition. We present Visual Feature Tracking(VFTrack) an in-situ real-time particle tracking framework that automaticallydetects and tracks individual CNT particles in SEM image sequences. VFTrackintegrates handcrafted or deep feature detectors and matchers within a particletracking framework to enable kinematic analysis of CNT micropillar growth. Asystematic using 13,540 manually annotated trajectories identifies the ALIKEDdetector with LightGlue matcher as an optimal combination (F1-score of 0.78,$\alpha$-score of 0.89). VFTrack motion vectors decomposed into axial growth,lateral drift, and oscillations, facilitate the calculation of heterogeneousregional growth rates and the reconstruction of evolving CNT pillarmorphologies. This work enables advancement in automated nano-materialcharacterization, bridging the gap between physics-based models andexperimental observation to enable real-time optimization of CNT synthesis.</description><author>Kaveh Safavigerdini, Ramakrishna Surya, Jaired Collins, Prasad Calyam, Filiz Bunyak, Matthew R. Maschmann, Kannappan Palaniappan</author><pubDate>Tue, 26 Aug 2025 17:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19232v1</guid></item><item><title>Distribution free M-estimation</title><link>http://arxiv.org/abs/2505.22807v4</link><description>The basic question of delineating those statistical problems that aresolvable without making any assumptions on the underlying data distribution haslong animated statistics and learning theory. This paper characterizes when aconvex M-estimation or stochastic optimization problem is solvable in such anassumption-free setting, providing a precise dividing line between solvable andunsolvable problems. The conditions we identify show, perhaps surprisingly,that Lipschitz continuity of the loss being minimized is not necessary fordistribution free minimization, and they are also distinct from classicalcharacterizations of learnability in machine learning.</description><author>Felipe Areces, John C. Duchi</author><pubDate>Tue, 26 Aug 2025 17:47:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.22807v4</guid></item><item><title>StepWiser: Stepwise Generative Judges for Wiser Reasoning</title><link>http://arxiv.org/abs/2508.19229v1</link><description>As models increasingly leverage multi-step reasoning strategies to solvecomplex problems, supervising the logical validity of these intermediate stepshas become a critical research challenge. Process reward models address this byproviding step-by-step feedback, but current approaches have two majordrawbacks: they typically function as classifiers without providingexplanations, and their reliance on supervised fine-tuning with static datasetslimits generalization. Inspired by recent advances, we reframe stepwise rewardmodeling from a classification task to a reasoning task itself. We thus proposea generative judge that reasons about the policy model's reasoning steps (i.e.,meta-reasons), outputting thinking tokens before delivering a final verdict.Our model, StepWiser, is trained by reinforcement learning using relativeoutcomes of rollouts. We show it provides (i) better judgment accuracy onintermediate steps than existing methods; (ii) can be used to improve thepolicy model at training time; and (iii) improves inference-time search.</description><author>Wei Xiong, Wenting Zhao, Weizhe Yuan, Olga Golovneva, Tong Zhang, Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Tue, 26 Aug 2025 17:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19229v1</guid></item><item><title>Predicting the Order of Upcoming Tokens Improves Language Modeling</title><link>http://arxiv.org/abs/2508.19228v1</link><description>Multi-Token Prediction (MTP) has been proposed as an auxiliary objective toimprove next-token prediction (NTP) in language model training but showsinconsistent improvements, underperforming in standard NLP benchmarks. We arguethat MTP's exact future token prediction is too difficult as an auxiliary loss.Instead, we propose Token Order Prediction (TOP), which trains models to orderupcoming tokens by their proximity using a learning-to-rank loss. TOP requiresonly a single additional unembedding layer compared to MTP's multipletransformer layers. We pretrain models of 340M, 1.8B, and 7B parameters usingNTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks showthat TOP overall outperforms both NTP and MTP even at scale. Our code isavailable at https://github.com/zaydzuhri/token-order-prediction</description><author>Zayd M. K. Zuhri, Erland Hilman Fuadi, Alham Fikri Aji</author><pubDate>Tue, 26 Aug 2025 17:43:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19228v1</guid></item><item><title>Generative Interfaces for Language Models</title><link>http://arxiv.org/abs/2508.19227v1</link><description>Large language models (LLMs) are increasingly seen as assistants, copilots,and consultants, capable of supporting a wide range of tasks through naturalconversation. However, most systems remain constrained by a linearrequest-response format that often makes interactions inefficient inmulti-turn, information-dense, and exploratory tasks. To address theselimitations, we propose Generative Interfaces for Language Models, a paradigmin which LLMs respond to user queries by proactively generating user interfaces(UIs) that enable more adaptive and interactive engagement. Our frameworkleverages structured interface-specific representations and iterativerefinements to translate user queries into task-specific UIs. For systematicevaluation, we introduce a multidimensional assessment framework that comparesgenerative interfaces with traditional chat-based ones across diverse tasks,interaction patterns, and query types, capturing functional, interactive, andemotional aspects of user experience. Results show that generative interfacesconsistently outperform conversational ones, with humans preferring them inover 70% of cases. These findings clarify when and why users favor generativeinterfaces, paving the way for future advancements in human-AI interaction.</description><author>Jiaqi Chen, Yanzhe Zhang, Yutong Zhang, Yijia Shao, Diyi Yang</author><pubDate>Tue, 26 Aug 2025 17:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19227v1</guid></item><item><title>Evaluating the Evaluators: Are readability metrics good measures of readability?</title><link>http://arxiv.org/abs/2508.19221v1</link><description>Plain Language Summarization (PLS) aims to distill complex documents intoaccessible summaries for non-expert audiences. In this paper, we conduct athorough survey of PLS literature, and identify that the current standardpractice for readability evaluation is to use traditional readability metrics,such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility inother fields, these metrics have not been compared to human readabilityjudgments in PLS. We evaluate 8 readability metrics and show that mostcorrelate poorly with human judgments, including the most popular metric, FKGL.We then show that Language Models (LMs) are better judges of readability, withthe best-performing model achieving a Pearson correlation of 0.56 with humanjudgments. Extending our analysis to PLS datasets, which contain summariesaimed at non-expert audiences, we find that LMs better capture deeper measuresof readability, such as required background knowledge, and lead to differentconclusions than the traditional metrics. Based on these findings, we offerrecommendations for best practices in the evaluation of plain languagesummaries. We release our analysis code and survey data.</description><author>Isabel Cachola, Daniel Khashabi, Mark Dredze</author><pubDate>Tue, 26 Aug 2025 17:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19221v1</guid></item><item><title>The Subset Sum Matching Problem</title><link>http://arxiv.org/abs/2508.19218v1</link><description>This paper presents a new combinatorial optimisation task, the Subset SumMatching Problem (SSMP), which is an abstraction of common financialapplications such as trades reconciliation. We present three algorithms, twosuboptimal and one optimal, to solve this problem. We also generate a benchmarkto cover different instances of SSMP varying in complexity, and carry out anexperimental evaluation to assess the performance of the approaches.</description><author>Yufei Wu, Manuel R. Torres, Parisa Zehtabi, Alberto Pozanco Lancho, Michael Cashmore, Daniel Borrajo, Manuela Veloso</author><pubDate>Tue, 26 Aug 2025 17:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19218v1</guid></item><item><title>Local Learning Rules for Out-of-Equilibrium Physical Generative Models</title><link>http://arxiv.org/abs/2506.19136v2</link><description>We show that the out-of-equilibrium driving protocol of score-basedgenerative models (SGMs) can be learned via local learning rules. The gradientwith respect to the parameters of the driving protocol is computed directlyfrom force measurements or from observed system dynamics. As a demonstration,we implement an SGM in a network of driven, nonlinear, overdamped oscillatorscoupled to a thermal bath. We first apply it to the problem of sampling from amixture of two Gaussians in 2D. Finally, we train a 12x12 oscillator network onthe MNIST dataset to generate images of handwritten digits 0 and 1.</description><author>Cyrill Bösch, Geoffrey Roeder, Marc Serra-Garcia, Ryan P. Adams</author><pubDate>Tue, 26 Aug 2025 17:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.19136v2</guid></item><item><title>From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification</title><link>http://arxiv.org/abs/2411.14252v2</link><description>In conversational AI systems, a critical challenge in training effectivemulti-turn intent classification models lies in the generation of large-scale,domain-specific, multilingual dialogue datasets. In this paper, we introduceChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)with Large Language Models (LLMs) to generate intent-driven, context-awaredialogues through self-play. Our method first extracts domain-specific intenttransition patterns from real-world e-commerce chat logs, which guide themodeling of turn-level dynamics and intent sequences. LLMs are then employed toparameterize the emission probabilities of HMMs, enabling the generation ofnatural, coherent utterances aligned with predicted intents and dialoguecontext. We further propose MINT-CL, a multi-task contrastive learningframework for multi-turn intent classification, which improves performancewhile reducing dependence on large-scale annotated datasets. Empirical resultsdemonstrate that our approach outperforms competitive baselines in bothdialogue generation quality and classification accuracy, particularly inmultilingual settings. To facilitate future research, we release MINT-E, acomprehensive, multilingual, intent-aware multi-turn dialogue corpus derivedfrom the e-commerce domain. The reproduced source code and dataset areavailable at https://github.com/junhua/chain-of-intent.</description><author>Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim</author><pubDate>Tue, 26 Aug 2025 17:22:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14252v2</guid></item><item><title>Interpolating Speaker Identities in Embedding Space for Data Expansion</title><link>http://arxiv.org/abs/2508.19210v1</link><description>The success of deep learning-based speaker verification systems is largelyattributed to access to large-scale and diverse speaker identity data. However,collecting data from more identities is expensive, challenging, and oftenlimited by privacy concerns. To address this limitation, we propose INSIDE(Interpolating Speaker Identities in Embedding Space), a novel data expansionmethod that synthesizes new speaker identities by interpolating betweenexisting speaker embeddings. Specifically, we select pairs of nearby speakerembeddings from a pretrained speaker embedding space and compute intermediateembeddings using spherical linear interpolation. These interpolated embeddingsare then fed to a text-to-speech system to generate corresponding speechwaveforms. The resulting data is combined with the original dataset to traindownstream models. Experiments show that models trained with INSIDE-expandeddata outperform those trained only on real data, achieving 3.06\% to 5.24\%relative improvements. While INSIDE is primarily designed for speakerverification, we also validate its effectiveness on gender classification,where it yields a 13.44\% relative improvement. Moreover, INSIDE is compatiblewith other augmentation techniques and can serve as a flexible, scalableaddition to existing training pipelines.</description><author>Tianchi Liu, Ruijie Tao, Qiongqiong Wang, Yidi Jiang, Hardik B. Sailor, Ke Zhang, Jingru Lin, Haizhou Li</author><pubDate>Tue, 26 Aug 2025 17:15:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19210v1</guid></item><item><title>OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation</title><link>http://arxiv.org/abs/2508.19209v1</link><description>Existing video avatar models can produce fluid human animations, yet theystruggle to move beyond mere physical likeness to capture a character'sauthentic essence. Their motions typically synchronize with low-level cues likeaudio rhythm, lacking a deeper semantic understanding of emotion, intent, orcontext. To bridge this gap, \textbf{we propose a framework designed togenerate character animations that are not only physically plausible but alsosemantically coherent and expressive.} Our model, \textbf{OmniHuman-1.5}, isbuilt upon two key technical contributions. First, we leverage Multimodal LargeLanguage Models to synthesize a structured textual representation of conditionsthat provides high-level semantic guidance. This guidance steers our motiongenerator beyond simplistic rhythmic synchronization, enabling the productionof actions that are contextually and emotionally resonant. Second, to ensurethe effective fusion of these multimodal inputs and mitigate inter-modalityconflicts, we introduce a specialized Multimodal DiT architecture with a novelPseudo Last Frame design. The synergy of these components allows our model toaccurately interpret the joint semantics of audio, images, and text, therebygenerating motions that are deeply coherent with the character, scene, andlinguistic content. Extensive experiments demonstrate that our model achievesleading performance across a comprehensive set of metrics, including lip-syncaccuracy, video quality, motion naturalness and semantic consistency withtextual prompts. Furthermore, our approach shows remarkable extensibility tocomplex scenarios, such as those involving multi-person and non-human subjects.Homepage: \href{https://omnihuman-lab.github.io/v1_5/}</description><author>Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, Mingyuan Gao</author><pubDate>Tue, 26 Aug 2025 17:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19209v1</guid></item><item><title>Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots</title><link>http://arxiv.org/abs/2508.15748v3</link><description>The development of parasocial relationships with AI agents has severe, and insome cases, tragic effects for human well-being. Yet preventing such dynamicsis challenging: parasocial cues often emerge gradually in privateconversations, and not all forms of emotional engagement are inherentlyharmful. We address this challenge by introducing a simple response evaluationframework, created by repurposing a state-of-the-art language model, thatevaluates ongoing conversations for parasocial cues in real time. To test thefeasibility of this approach, we constructed a small synthetic dataset ofthirty dialogues spanning parasocial, sycophantic, and neutral conversations.Iterative evaluation with five stage testing successfully identified allparasocial conversations while avoiding false positives under a tolerantunanimity rule, with detection typically occurring within the first fewexchanges. These findings provide preliminary evidence that evaluation agentscan provide a viable solution for the prevention of parasocial relations.</description><author>Emma Rath, Stuart Armstrong, Rebecca Gorman</author><pubDate>Tue, 26 Aug 2025 17:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15748v3</guid></item><item><title>Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment</title><link>http://arxiv.org/abs/2508.16839v2</link><description>Clinical workflows are fragmented as a patchwork of scripts and task-specificnetworks that often handle triage, task selection, and model deployment. Thesepipelines are rarely streamlined for data science pipeline, reducing efficiencyand raising operational costs. Workflows also lack data-driven modelidentification (from imaging/tabular inputs) and standardized delivery of modeloutputs. In response, we present a practical, healthcare-first framework thatuses a single vision-language model (VLM) in two complementary roles. First(Solution 1), the VLM acts as an aware model-card matcher that routes anincoming image to the appropriate specialist model via a three-stage workflow(modality -&gt; primary abnormality -&gt; model-card id). Checks are provided by (i)stagewise prompts that allow early exit via None/Normal/Other and (ii) astagewise answer selector that arbitrates between the top-2 candidates at eachstage, reducing the chance of an incorrect selection and aligning the workflowwith clinical risk tolerance. Second (Solution 2), we fine-tune the VLM onspecialty-specific datasets ensuring a single model covers multiple downstreamtasks within each specialty, maintaining performance while simplifyingdeployment. Across gastroenterology, hematology, ophthalmology, and pathology,our single-model deployment matches or approaches specialized baselines. Compared with pipelines composed of many task-specific agents, this approachshows that one VLM can both decide and do. It may reduce effort by datascientists, shorten monitoring, increase the transparency of model selection(with per-stage justifications), and lower integration overhead.</description><author>Shayan Vassef, Soorya Ram Shimegekar, Abhay Goyal, Koustuv Saha, Pi Zonooz, Navin Kumar</author><pubDate>Tue, 26 Aug 2025 17:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16839v2</guid></item><item><title>Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications</title><link>http://arxiv.org/abs/2502.13358v3</link><description>Large Language Models (LLMs) have significantly advanced natural languageprocessing, demonstrating strong capabilities in tasks such as text generation,summarization, and reasoning. Recently, their potential for automating precisetext editing tasks across specialized domains, such as programming code, LaTeX,and structured database languages, has gained attention. However, currentstate-of-the-art LLMs still struggle with executing precise, instruction-drivenedits, particularly when structural accuracy and strict adherence to domainconventions are required. To address these challenges, we introduceInstrEditBench, an automated benchmark dataset comprising over 30,000structured editing tasks spanning diverse domains, including Wikipediaarticles, LaTeX documents, source code, and database languages. Using thisbenchmark, we develop FineEdit, a specialized editing model explicitly trainedfor accurate, context-aware text modifications. Experimental evaluationsdemonstrate that FineEdit outperforms state-of-the-art models, achievingimprovements of approximately 10\% over Gemini models on single-turn edits, upto 30\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance byover 40\% on direct editing tasks. FineEdit also effectively generalizes torealistic multi-turn editing scenarios, highlighting its practicalapplicability. To facilitate further research and reproducibility, we releaseFineEdit at https://github.com/StuRinDQB/FineEdit} andhttps://huggingface.co/datasets/YimingZeng/FineEdit_bench.</description><author>Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu</author><pubDate>Tue, 26 Aug 2025 17:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.13358v3</guid></item><item><title>VibeVoice Technical Report</title><link>http://arxiv.org/abs/2508.19205v1</link><description>This report presents VibeVoice, a novel model designed to synthesizelong-form speech with multiple speakers by employing next-token diffusion,which is a unified method for modeling continuous data by autoregressivelygenerating latent vectors via diffusion. To enable this, we introduce a novelcontinuous speech tokenizer that, when compared to the popular Encodec model,improves data compression by 80 times while maintaining comparable performance.The tokenizer effectively preserves audio fidelity while significantly boostingcomputational efficiency for processing long sequences. Thus, VibeVoice cansynthesize long-form speech for up to 90 minutes (in a 64K context windowlength) with a maximum of 4 speakers, capturing the authentic conversational``vibe'' and surpassing open-source and proprietary dialogue models.</description><author>Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, Shaohan Huang, Yan Xia, Furu Wei</author><pubDate>Tue, 26 Aug 2025 17:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19205v1</guid></item><item><title>LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding</title><link>http://arxiv.org/abs/2508.19204v1</link><description>Large-scale scene data is essential for training and testing in robotlearning. Neural reconstruction methods have promised the capability ofreconstructing large physically-grounded outdoor scenes from captured sensordata. However, these methods have baked-in static environments and only allowfor limited scene control -- they are functionally constrained in scene andtrajectory diversity by the captures from which they are reconstructed. Incontrast, generating driving data with recent image or video diffusion modelsoffers control, however, at the cost of geometry grounding and causality. Inthis work, we aim to bridge this gap and present a method that directlygenerates large-scale 3D driving scenes with accurate geometry, allowing forcausal novel view synthesis with object permanence and explicit 3D geometryestimation. The proposed method combines the generation of a proxy geometry andenvironment representation with score distillation from learned 2D imagepriors. We find that this approach allows for high controllability, enablingthe prompt-guided geometry and high-fidelity texture and structure that can beconditioned on map layouts -- producing realistic and geometrically consistent3D generations of complex driving scenes.</description><author>Julian Ost, Andrea Ramazzina, Amogh Joshi, Maximilian Bömer, Mario Bijelic, Felix Heide</author><pubDate>Tue, 26 Aug 2025 17:04:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19204v1</guid></item><item><title>Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning</title><link>http://arxiv.org/abs/2508.19202v1</link><description>Scientific problem solving poses unique challenges for LLMs, requiring bothdeep domain knowledge and the ability to apply such knowledge through complexreasoning. While automated scientific reasoners hold great promise forassisting human scientists, there is currently no widely adopted holisticbenchmark for evaluating scientific reasoning, and few approachessystematically disentangle the distinct roles of knowledge and reasoning inthese tasks. To address these gaps, we introduce SciReas, a diverse suite ofexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, aselective subset that requires more complex reasoning. Our holistic evaluationsurfaces insights about scientific reasoning performance that remain hiddenwhen relying on individual benchmarks alone. We then propose KRUX, a probingframework for studying the distinct roles of reasoning and knowledge inscientific tasks. Combining the two, we conduct an in-depth analysis thatyields several key findings: (1) Retrieving task-relevant knowledge from modelparameters is a critical bottleneck for LLMs in scientific reasoning; (2)Reasoning models consistently benefit from external knowledge added in-contexton top of the reasoning enhancement; (3) Enhancing verbalized reasoningimproves LLMs' ability to surface task-relevant knowledge. Finally, we conducta lightweight analysis, comparing our science-focused data composition withconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baselinefor scientific reasoning.</description><author>Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan</author><pubDate>Tue, 26 Aug 2025 17:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19202v1</guid></item><item><title>Understanding Tool-Integrated Reasoning</title><link>http://arxiv.org/abs/2508.19201v1</link><description>We study why Tool-Integrated Reasoning (TIR) makes Large Language Models(LLMs) more capable. While LLMs integrated with tools like Python codeinterpreters show great promise, a principled theory explaining why thisparadigm is effective has been missing. This work provides the first formalproof that TIR fundamentally expands an LLM's capabilities. We demonstrate thattools enable a strict expansion of the model's empirical and feasible support,breaking the capability ceiling of pure-text models by unlockingproblem-solving strategies that are otherwise impossible or intractablyverbose. To guide model behavior without compromising training stability andperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), anovel algorithm that directly modifies the advantage function to guide thepolicy behavior. We conduct comprehensive experiments on challengingmathematical benchmarks, leveraging a Python interpreter as the external tool.Our results show that the TIR model decisively outperforms its pure-textcounterpart on the pass@k metric. Crucially, this advantage is not confined tocomputationally-intensive problems but extends to those requiring significantabstract insight. We further identify the emergent cognitive patterns thatillustrate how models learn to think with tools. Finally, we report improvedtool usage behavior with early code invocation and much more interactive turnswith ASPO. Overall, our work provides the first principled explanation forTIR's success, shifting the focus from the mere fact that tools work to why andhow they enable more powerful reasoning.</description><author>Heng Lin, Zhongwen Xu</author><pubDate>Tue, 26 Aug 2025 17:03:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19201v1</guid></item><item><title>The Ramon Llull's Thinking Machine for Automated Ideation</title><link>http://arxiv.org/abs/2508.19200v1</link><description>This paper revisits Ramon Llull's Ars combinatoria - a medieval framework forgenerating knowledge through symbolic recombination - as a conceptualfoundation for building a modern Llull's thinking machine for researchideation. Our approach defines three compositional axes: Theme (e.g.,efficiency, adaptivity), Domain (e.g., question answering, machinetranslation), and Method (e.g., adversarial training, linear attention). Theseelements represent high-level abstractions common in scientific work -motivations, problem settings, and technical approaches - and serve as buildingblocks for LLM-driven exploration. We mine elements from human experts orconference papers and show that prompting LLMs with curated combinationsproduces research ideas that are diverse, relevant, and grounded in currentliterature. This modern thinking machine offers a lightweight, interpretabletool for augmenting scientific creativity and suggests a path towardcollaborative ideation between humans and AI.</description><author>Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu</author><pubDate>Tue, 26 Aug 2025 17:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19200v1</guid></item><item><title>Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation</title><link>http://arxiv.org/abs/2508.19199v1</link><description>Efficient planning in high-dimensional spaces, such as those involvingdeformable objects, requires computationally tractable yet sufficientlyexpressive dynamics models. This paper introduces a method that automaticallygenerates task-specific, spatially adaptive dynamics models by learning whichregions of the object require high-resolution modeling to achieve good taskperformance for a given planning query. Task performance depends on the complexinterplay between the dynamics model, world dynamics, control, and taskrequirements. Our proposed diffusion-based model generator predicts per-regionmodel resolutions based on start and goal pointclouds that define the planningquery. To efficiently collect the data for learning this mapping, a two-stageprocess optimizes resolution using predictive dynamics as a prior beforedirectly optimizing using closed-loop performance. On a tree-manipulation task,our method doubles planning speed with only a small decrease in taskperformance over using a full-resolution model. This approach informs a pathtowards using previous planning and control data to generate computationallyefficient yet sufficiently expressive dynamics models for new tasks.</description><author>Alex LaGrassa, Zixuan Huang, Dmitry Berenson, Oliver Kroemer</author><pubDate>Tue, 26 Aug 2025 17:03:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19199v1</guid></item><item><title>Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates</title><link>http://arxiv.org/abs/2507.09166v2</link><description>The coarse spatial resolution of gridded climate models, such as generalcirculation models, limits their direct use in projecting socially relevantvariables like extreme precipitation. Most downscaling methods estimate theconditional distributions of extremes by generating large ensembles,complicating the assessment of robustness under distributional shifts, such asthose induced by climate change. To better understand and potentially improverobustness, we propose super-resolving the parameters of the target variable'sprobability distribution directly using analytically tractable mappings. Withina perfect-model framework over Switzerland, we demonstrate that vectorgeneralized linear and additive models can super-resolve the generalizedextreme value distribution of summer hourly precipitation extremes from coarseprecipitation fields and topography. We introduce the notion of a "robustnessgap", defined as the difference in predictive error between present-trained andfuture-trained models, and use it to diagnose how model structure affects thegeneralization of each quantile to a pseudo-global warming scenario. Byevaluating multiple model configurations, we also identify an upper limit onthe super-resolution factor based on the spatial auto- and cross-correlation ofprecipitation and elevation, beyond which coarse precipitation loses predictivevalue. Our framework is broadly applicable to variables governed by parametricdistributions and offers a model-agnostic diagnostic for understanding when andwhy empirical downscaling generalizes to climate change and extremes.</description><author>Louise Largeau, Erwan Koch, David Leutwyler, Gregoire Mariethoz, Valerie Chavez-Demoulin, Tom Beucler</author><pubDate>Tue, 26 Aug 2025 17:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.09166v2</guid></item><item><title>Prompt-based Dynamic Token Pruning for Efficient Segmentation of Medical Images</title><link>http://arxiv.org/abs/2506.16369v2</link><description>The high computational demands of Vision Transformers (ViTs) in processing alarge number of tokens often constrain their practical application in analyzingmedical images. This research proposes a Prompt-driven Adaptive Token ({\itPrATo}) pruning method to selectively reduce the processing of irrelevanttokens in the segmentation pipeline. The prompt-based spatial prior helps torank the tokens according to their relevance. Tokens with low-relevance scoresare down-weighted, ensuring that only the relevant ones are propagated forprocessing across subsequent stages. This data-driven pruning strategy improvessegmentation accuracy and inference speed by allocating computational resourcesto essential regions. The proposed framework is integrated with severalstate-of-the-art models to facilitate the elimination of irrelevant tokens,thereby enhancing computational efficiency while preserving segmentationaccuracy. The experimental results show a reduction of $\sim$ 35-55% tokens;thus reducing the computational costs relative to baselines. Cost-effectivemedical image processing, using our framework, facilitates real-time diagnosisby expanding its applicability in resource-constrained environments.</description><author>Pallabi Dutta, Anubhab Maity, Sushmita Mitra</author><pubDate>Tue, 26 Aug 2025 17:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.16369v2</guid></item><item><title>Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels</title><link>http://arxiv.org/abs/2508.17437v2</link><description>Inferring the physical properties of 3D scenes from visual information is acritical yet challenging task for creating interactive and realistic virtualworlds. While humans intuitively grasp material characteristics such aselasticity or stiffness, existing methods often rely on slow, per-sceneoptimization, limiting their generalizability and application. To address thisproblem, we introduce PIXIE, a novel method that trains a generalizable neuralnetwork to predict physical properties across multiple scenes from 3D visualfeatures purely using supervised losses. Once trained, our feed-forward networkcan perform fast inference of plausible material fields, which coupled with alearned static scene representation like Gaussian Splatting enables realisticphysics simulation under external forces. To facilitate this research, we alsocollected PIXIEVERSE, one of the largest known datasets of paired 3D assets andphysic material annotations. Extensive evaluations demonstrate that PIXIE isabout 1.46-4.39x better and orders of magnitude faster than test-timeoptimization methods. By leveraging pretrained visual features like CLIP, ourmethod can also zero-shot generalize to real-world scenes despite only everbeen trained on synthetic data. https://pixie-3d.github.io/</description><author>Long Le, Ryan Lucas, Chen Wang, Chuhao Chen, Dinesh Jayaraman, Eric Eaton, Lingjie Liu</author><pubDate>Tue, 26 Aug 2025 16:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17437v2</guid></item><item><title>All-in-One Slider for Attribute Manipulation in Diffusion Models</title><link>http://arxiv.org/abs/2508.19195v1</link><description>Text-to-image (T2I) diffusion models have made significant strides ingenerating high-quality images. However, progressively manipulating certainattributes of generated images to meet the desired user expectations remainschallenging, particularly for content with rich details, such as human faces.Some studies have attempted to address this by training slider modules.However, they follow a One-for-One manner, where an independent slider istrained for each attribute, requiring additional training whenever a newattribute is introduced. This not only results in parameter redundancyaccumulated by sliders but also restricts the flexibility of practicalapplications and the scalability of attribute manipulation. To address thisissue, we introduce the All-in-One Slider, a lightweight module that decomposesthe text embedding space into sparse, semantically meaningful attributedirections. Once trained, it functions as a general-purpose slider, enablinginterpretable and fine-grained continuous control over various attributes.Moreover, by recombining the learned directions, the All-in-One Slider supportszero-shot manipulation of unseen attributes (e.g., races and celebrities) andthe composition of multiple attributes. Extensive experiments demonstrate thatour method enables accurate and scalable attribute manipulation, achievingnotable improvements compared to previous methods. Furthermore, our method canbe extended to integrate with the inversion framework to perform attributemanipulation on real images, broadening its applicability to various real-worldscenarios. The code and trained model will be released at:https://github.com/ywxsuperstar/KSAE-FaceSteer.</description><author>Weixin Ye, Hongguang Zhu, Wei Wang, Yahui Liu, Mengyu Wang</author><pubDate>Tue, 26 Aug 2025 16:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19195v1</guid></item><item><title>Emotions as Ambiguity-aware Ordinal Representations</title><link>http://arxiv.org/abs/2508.19193v1</link><description>Emotions are inherently ambiguous and dynamic phenomena, yet existingcontinuous emotion recognition approaches either ignore their ambiguity ortreat ambiguity as an independent and static variable over time. Motivated bythis gap in the literature, in this paper we introduce \emph{ambiguity-awareordinal} emotion representations, a novel framework that captures both theambiguity present in emotion annotation and the inherent temporal dynamics ofemotional traces. Specifically, we propose approaches that model emotionambiguity through its rate of change. We evaluate our framework on twoaffective corpora -- RECOLA and GameVibe -- testing our proposed approaches onboth bounded (arousal, valence) and unbounded (engagement) continuous traces.Our results demonstrate that ordinal representations outperform conventionalambiguity-aware models on unbounded labels, achieving the highest ConcordanceCorrelation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,highlighting their effectiveness in modeling the traces' dynamics. For boundedtraces, ordinal representations excel in SDA, revealing their superior abilityto capture relative changes of annotated emotion traces.</description><author>Jingyao Wu, Matthew Barthet, David Melhart, Georgios N. Yannakakis</author><pubDate>Tue, 26 Aug 2025 16:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19193v1</guid></item><item><title>FastMesh:Efficient Artistic Mesh Generation via Component Decoupling</title><link>http://arxiv.org/abs/2508.19188v1</link><description>Recent mesh generation approaches typically tokenize triangle meshes intosequences of tokens and train autoregressive models to generate these tokenssequentially. Despite substantial progress, such token sequences inevitablyreuse vertices multiple times to fully represent manifold meshes, as eachvertex is shared by multiple faces. This redundancy leads to excessively longtoken sequences and inefficient generation processes. In this paper, we proposean efficient framework that generates artistic meshes by treating vertices andfaces separately, significantly reducing redundancy. We employ anautoregressive model solely for vertex generation, decreasing the token countto approximately 23\% of that required by the most compact existing tokenizer.Next, we leverage a bidirectional transformer to complete the mesh in a singlestep by capturing inter-vertex relationships and constructing the adjacencymatrix that defines the mesh faces. To further improve the generation quality,we introduce a fidelity enhancer to refine vertex positioning into more naturalarrangements and propose a post-processing framework to remove undesirable edgeconnections. Experimental results show that our method achieves more than8$\times$ faster speed on mesh generation compared to state-of-the-artapproaches, while producing higher mesh quality.</description><author>Jeonghwan Kim, Yushi Lan, Armando Fortes, Yongwei Chen, Xingang Pan</author><pubDate>Tue, 26 Aug 2025 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19188v1</guid></item><item><title>Real-Time Model Checking for Closed-Loop Robot Reactive Planning</title><link>http://arxiv.org/abs/2508.19186v1</link><description>We present a new application of model checking which achieves real-timemulti-step planning and obstacle avoidance on a real autonomous robot. We havedeveloped a small, purpose-built model checking algorithm which generates plansin situ based on "core" knowledge and attention as found in biological agents.This is achieved in real-time using no pre-computed data on a low-powereddevice. Our approach is based on chaining temporary control systems which arespawned to counteract disturbances in the local environment that disrupt anautonomous agent from its preferred action (or resting state). A noveldiscretization of 2D LiDAR data sensitive to bounded variations in the localenvironment is used. Multi-step planning using model checking by forwarddepth-first search is applied to cul-de-sac and playground scenarios. Bothempirical results and informal proofs of two fundamental properties of ourapproach demonstrate that model checking can be used to create efficientmulti-step plans for local obstacle avoidance, improving on the performance ofa reactive agent which can only plan one step. Our approach is an instructionalcase study for the development of safe, reliable and explainable planning inthe context of autonomous vehicles.</description><author>Christopher Chandler, Bernd Porr, Giulia Lafratta, Alice Miller</author><pubDate>Tue, 26 Aug 2025 16:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19186v1</guid></item><item><title>mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2505.24073v2</link><description>Large Vision-Language Models (LVLMs) have made remarkable strides inmultimodal tasks such as visual question answering, visual grounding, andcomplex reasoning. However, they remain limited by static training data,susceptibility to hallucinations, and inability to verify claims againstup-to-date, external evidence, compromising their performance in dynamicreal-world applications. Retrieval-Augmented Generation (RAG) offers apractical solution to mitigate these challenges by allowing the LVLMs to accesslarge-scale knowledge databases via retrieval mechanisms, thereby groundingmodel outputs in factual, contextually relevant information. Here in thispaper, we conduct the first systematic dissection of the multimodal RAGpipeline for LVLMs, explicitly investigating (1) the retrieval phase: on themodality configurations and retrieval strategies, (2) the re-ranking stage: onstrategies to mitigate positional biases and improve the relevance of retrievedevidence, and (3) the generation phase: we further investigate how to bestintegrate retrieved candidates into the final generation process. Finally, weextend to explore a unified agentic framework that integrates re-ranking andgeneration through self-reflection, enabling LVLMs to select relevant evidenceand suppress irrelevant context dynamically. Our full-stack exploration of RAGfor LVLMs yields substantial insights, resulting in an average performanceboost of 5% without any fine-tuning.</description><author>Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Suofei Feng, Ryan Rossi, Zhengzhong Tu</author><pubDate>Tue, 26 Aug 2025 16:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24073v2</guid></item><item><title>Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</title><link>http://arxiv.org/abs/2508.17630v2</link><description>We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neuralnetwork that integrates variational quantum circuits into the attentionmechanism. At its core, QGAT employs strongly entangling quantum circuits withamplitude-encoded node features to enable expressive nonlinear interactions.Distinct from classical multi-head attention that separately computes eachhead, QGAT leverages a single quantum circuit to simultaneously generatemultiple attention coefficients. This quantum parallelism facilitates parametersharing across heads, substantially reducing computational overhead and modelcomplexity. Classical projection weights and quantum circuit parameters areoptimized jointly in an end-to-end manner, ensuring flexible adaptation tolearning tasks. Empirical results demonstrate QGAT's effectiveness in capturingcomplex structural dependencies and improved generalization in inductivescenarios, highlighting its potential for scalable quantum-enhanced learningacross domains such as chemistry, biology, and network analysis. Furthermore,experiments confirm that quantum embedding enhances robustness against featureand structural noise, suggesting advantages in handling real-world noisy data.The modularity of QGAT also ensures straightforward integration into existingarchitectures, allowing it to easily augment classical attention-based models.</description><author>An Ning, Tai Yue Li, Nan Yow Chen</author><pubDate>Tue, 26 Aug 2025 16:42:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17630v2</guid></item><item><title>Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness</title><link>http://arxiv.org/abs/2508.19183v1</link><description>In safety-critical deep learning applications, robustness measures theability of neural models that handle imperceptible perturbations in input data,which may lead to potential safety hazards. Existing pre-deployment robustnessassessment methods typically suffer from significant trade-offs betweencomputational cost and measurement precision, limiting their practical utility.To address these limitations, this paper conducts a comprehensive comparativeanalysis of existing robustness definitions and associated assessmentmethodologies. We propose tower robustness to evaluate robustness, which is anovel, practical metric based on hypothesis testing to quantitatively evaluateprobabilistic robustness, enabling more rigorous and efficient pre-deploymentassessments. Our extensive comparative evaluation illustrates the advantagesand applicability of our proposed approach, thereby advancing the systematicunderstanding and enhancement of model robustness in safety-critical deeplearning applications.</description><author>Wenchuan Mu, Kwan Hui Lim</author><pubDate>Tue, 26 Aug 2025 16:41:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19183v1</guid></item><item><title>TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use</title><link>http://arxiv.org/abs/2412.15495v2</link><description>Large language models (LLMs) achieve remarkable advancements by leveragingtools to interact with environments, a critical step toward generalized AI.However, the standard supervised fine-tuning (SFT) approach, which relies onlarge-scale datasets, often overlooks task-specific characteristics in tooluse, leading to performance bottlenecks. To address this issue, we analyzethree existing LLMs and uncover key insights: training data can inadvertentlyimpede tool-use behavior, token importance is distributed unevenly, and errorsin tool calls fall into a small set of categories. Building on these findings,we propose~\emph{TL-Training}, a task-feature-based framework that mitigatesthe effects of suboptimal training data, dynamically adjusts token weights toprioritize key tokens during SFT, and incorporates a robust reward mechanismtailored to error categories, optimized through proximal policy optimization.We validate TL-Training by training CodeLLaMA-2-7B and evaluating it on fouropen-source test sets. Our results demonstrate that the LLM trained by ourmethod matches or surpasses both open- and closed-source LLMs in tool-useperformance using only 1,217 training data points. Additionally, our methodenhances robustness in noisy environments and improves general taskperformance, offering a scalable and efficient paradigm for tool-use trainingin LLMs. Code and data are available athttps://github.com/Junjie-Ye/TL-Training.</description><author>Junjie Ye, Yilong Wu, Sixian Li, Yuming Yang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan, Zhengyin Du</author><pubDate>Tue, 26 Aug 2025 16:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15495v2</guid></item><item><title>SoccerNet 2025 Challenges Results</title><link>http://arxiv.org/abs/2508.19182v1</link><description>The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNetopen benchmarking effort, dedicated to advancing computer vision research infootball video understanding. This year's challenges span four vision-basedtasks: (1) Team Ball Action Spotting, focused on detecting ball-related actionsin football broadcasts and assigning actions to teams; (2) Monocular DepthEstimation, targeting the recovery of scene geometry from single-camerabroadcast clips through relative depth estimation for each pixel; (3)Multi-View Foul Recognition, requiring the analysis of multiple synchronizedcamera views to classify fouls and their severity; and (4) Game StateReconstruction, aimed at localizing and identifying all players from abroadcast video to reconstruct the game state on a 2D top-view of the field.Across all tasks, participants were provided with large-scale annotateddatasets, unified evaluation protocols, and strong baselines as startingpoints. This report presents the results of each challenge, highlights thetop-performing solutions, and provides insights into the progress made by thecommunity. The SoccerNet Challenges continue to serve as a driving force forreproducible, open research at the intersection of computer vision, artificialintelligence, and sports. Detailed information about the tasks, challenges, andleaderboards can be found at https://www.soccer-net.org, with baselines anddevelopment kits available at https://github.com/SoccerNet.</description><author>Silvio Giancola, Anthony Cioppa, Marc Gutiérrez-Pérez, Jan Held, Carlos Hinojosa, Victor Joos, Arnaud Leduc, Floriane Magera, Karen Sanchez, Vladimir Somers, Artur Xarles, Antonio Agudo, Alexandre Alahi, Olivier Barnich, Albert Clapés, Christophe De Vleeschouwer, Sergio Escalera, Bernard Ghanem, Thomas B. Moeslund, Marc Van Droogenbroeck, Tomoki Abe, Saad Alotaibi, Faisal Altawijri, Steven Araujo, Xiang Bai, Xiaoyang Bi, Jiawang Cao, Vanyi Chao, Kamil Czarnogórski, Fabian Deuser, Mingyang Du, Tianrui Feng, Patrick Frenzel, Mirco Fuchs, Jorge García, Konrad Habel, Takaya Hashiguchi, Sadao Hirose, Xinting Hu, Yewon Hwang, Ririko Inoue, Riku Itsuji, Kazuto Iwai, Hongwei Ji, Yangguang Ji, Licheng Jiao, Yuto Kageyama, Yuta Kamikawa, Yuuki Kanasugi, Hyungjung Kim, Jinwook Kim, Takuya Kurihara, B</author><pubDate>Tue, 26 Aug 2025 16:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19182v1</guid></item><item><title>Consistent Opponent Modeling of Static Opponents in Imperfect-Information Games</title><link>http://arxiv.org/abs/2508.17671v2</link><description>The goal of agents in multi-agent environments is to maximize total rewardagainst the opposing agents that are encountered. Following a game-theoreticsolution concept, such as Nash equilibrium, may obtain a strong performance insome settings; however, such approaches fail to capitalize on historical andobserved data from repeated interactions against our opponents. Opponentmodeling algorithms integrate machine learning techniques to exploit suboptimalopponents utilizing available data; however, the effectiveness of suchapproaches in imperfect-information games to date is quite limited. We showthat existing opponent modeling approaches fail to satisfy a simple desirableproperty even against static opponents drawn from a known prior distribution;namely, they do not guarantee that the model approaches the opponent's truestrategy even in the limit as the number of game iterations approachesinfinity. We develop a new algorithm that is able to achieve this property andruns efficiently by solving a convex minimization problem based on thesequence-form game representation using projected gradient descent. Thealgorithm is guaranteed to efficiently converge to the opponent's true strategygiven observations from gameplay and possibly additional historical data if itis available.</description><author>Sam Ganzfried</author><pubDate>Tue, 26 Aug 2025 16:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17671v2</guid></item><item><title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2505.04594v5</link><description>Accurately predicting 3D attributes is crucial for monocular 3D objectdetection (Mono3D), with depth estimation posing the greatest challenge due tothe inherent ambiguity in mapping 2D images to 3D space. While existing methodsleverage multiple depth cues (e.g., estimating depth uncertainty, modelingdepth error) to improve depth accuracy, they overlook that accurate depthprediction requires conditioning on other 3D attributes, as these attributesare intrinsically inter-correlated through the 3D to 2D projection, whichultimately limits overall accuracy and stability. Inspired by Chain-of-Thought(CoT) in large language models (LLMs), this paper proposes MonoCoP, whichleverages a Chain-of-Prediction (CoP) to predict attributes sequentially andconditionally via three key designs. First, it employs a lightweightAttributeNet (AN) for each 3D attribute to learn attribute-specific features.Next, MonoCoP constructs an explicit chain to propagate these learned featuresfrom one attribute to the next. Finally, MonoCoP uses a residual connection toaggregate features for each attribute along the chain, ensuring that laterattribute predictions are conditioned on all previously processed attributeswithout forgetting the features of earlier ones. Experimental results show thatour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTIleaderboard without requiring additional data and further surpasses existingmethods on the Waymo and nuScenes frontal datasets.</description><author>Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu</author><pubDate>Tue, 26 Aug 2025 16:31:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.04594v5</guid></item><item><title>Image Coding for Machines via Feature-Preserving Rate-Distortion Optimization</title><link>http://arxiv.org/abs/2504.02216v2</link><description>Many images and videos are primarily processed by computer vision algorithms,involving only occasional human inspection. When this content requirescompression before processing, e.g., in distributed applications, codingmethods must optimize for both visual quality and downstream task performance.We first show theoretically that an approach to reduce the effect ofcompression for a given task loss is to perform rate-distortion optimization(RDO) using the distance between features, obtained from the original and thedecoded images, as a distortion metric. However, optimizing directly such arate-distortion objective is computationally impractical because it requiresiteratively encoding and decoding the entire image-plus feature evaluation-foreach possible coding configuration. We address this problem by simplifying theRDO formulation to make the distortion term computable using block-basedencoders. We first apply Taylor's expansion to the feature extractor, recastingthe feature distance as a quadratic metric involving the Jacobian matrix of theneural network. Then, we replace the linearized metric with a block-wiseapproximation, which we call input-dependent squared error (IDSE). To make themetric computable, we approximate IDSE using sketches of the Jacobian. Theresulting loss can be evaluated block-wise in the transform domain and combinedwith the sum of squared errors (SSE) to address both visual quality andcomputer vision performance. Simulations with AVC and HEVC across multiplefeature extractors and downstream networks show up to 17 % bit-rate savings forthe same task accuracy compared to RDO based on SSE, with no decoder complexityoverhead and a small (7.86 %) encoder complexity increase.</description><author>Samuel Fernández-Menduiña, Eduardo Pavez, Antonio Ortega</author><pubDate>Tue, 26 Aug 2025 16:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.02216v2</guid></item><item><title>Leveraging Evolutionary Surrogate-Assisted Prescription in Multi-Objective Chlorination Control Systems</title><link>http://arxiv.org/abs/2508.19173v1</link><description>This short, written report introduces the idea of EvolutionarySurrogate-Assisted Prescription (ESP) and presents preliminary results on itspotential use in training real-world agents as a part of the 1st AI forDrinking Water Chlorination Challenge at IJCAI-2025. This work was done by ateam from Project Resilience, an organization interested in bridging AI toreal-world problems.</description><author>Rivaaj Monsia, Olivier Francon, Daniel Young, Risto Miikkulainen</author><pubDate>Tue, 26 Aug 2025 16:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19173v1</guid></item><item><title>From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity</title><link>http://arxiv.org/abs/2508.19172v1</link><description>Autonomous skill discovery aims to enable robots to acquire diverse behaviorswithout explicit supervision. Learning such behaviors directly on physicalhardware remains challenging due to safety and data efficiency constraints.Existing methods, including Quality-Diversity Actor-Critic (QDAC), requiremanually defined skill spaces and carefully tuned heuristics, limitingreal-world applicability. We propose Unsupervised Real-world Skill Acquisition(URSA), an extension of QDAC that enables robots to autonomously discover andmaster diverse, high-performing skills directly in the real world. Wedemonstrate that URSA successfully discovers diverse locomotion skills on aUnitree A1 quadruped in both simulation and the real world. Our approachsupports both heuristic-driven skill discovery and fully unsupervised settings.We also show that the learned skill repertoire can be reused for downstreamtasks such as real-world damage adaptation, where URSA outperforms allbaselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.Our results establish a new framework for real-world robot learning thatenables continuous skill discovery with limited human intervention,representing a significant step toward more autonomous and adaptable roboticsystems. Demonstration videos are available athttp://adaptive-intelligent-robotics.github.io/URSA .</description><author>Luca Grillotti, Lisa Coiffard, Oscar Pang, Maxence Faldor, Antoine Cully</author><pubDate>Tue, 26 Aug 2025 16:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19172v1</guid></item><item><title>Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions</title><link>http://arxiv.org/abs/2508.19167v1</link><description>Vision Transformers have demonstrated remarkable success in computer visiontasks, yet their reliance on learnable one-dimensional positional embeddingsfundamentally disrupts the inherent two-dimensional spatial structure of imagesthrough patch flattening procedures. Traditional positional encoding approacheslack geometric constraints and fail to establish monotonic correspondencebetween Euclidean spatial distances and sequential index distances, therebylimiting the model's capacity to leverage spatial proximity priors effectively.We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), amathematically principled approach that directly addresses two-dimensionalcoordinates through natural complex domain representation, where the doublyperiodic properties of elliptic functions align remarkably with translationalinvariance patterns commonly observed in visual data. Our method exploits thenon-linear geometric nature of elliptic functions to encode spatial distancerelationships naturally, while the algebraic addition formula enables directderivation of relative positional information between arbitrary patch pairsfrom their absolute encodings. Comprehensive experiments demonstrate thatWEF-PE achieves superior performance across diverse scenarios, including63.78\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture,93.28\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements onVTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decayproperty through rigorous mathematical proof, while attention visualizationreveals enhanced geometric inductive bias and more coherent semantic focuscompared to conventional approaches.The source code implementing the methodsdescribed in this paper is publicly available on GitHub.</description><author>Zhihang Xin, Xitong Hu, Rui Wang</author><pubDate>Tue, 26 Aug 2025 16:14:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19167v1</guid></item><item><title>Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding</title><link>http://arxiv.org/abs/2508.19165v1</link><description>Monocular 3D visual grounding is a novel task that aims to locate 3D objectsin RGB images using text descriptions with explicit geometry information.Despite the inclusion of geometry details in the text, we observe that the textembeddings are sensitive to the magnitude of numerical values but largelyignore the associated measurement units. For example, simply equidistantmapping the length with unit "meter" to "decimeters" or "centimeters" leads tosevere performance degradation, even though the physical length remainsequivalent. This observation signifies the weak 3D comprehension of pre-trainedlanguage model, which generates misguiding text features to hinder 3Dperception. Therefore, we propose to enhance the 3D perception of model on textembeddings and geometry features with two simple and effective methods.Firstly, we introduce a pre-processing method named 3D-text Enhancement (3DTE),which enhances the comprehension of mapping relationships between differentunits by augmenting the diversity of distance descriptors in text queries.Next, we propose a Text-Guided Geometry Enhancement (TGE) module to furtherenhance the 3D-text information by projecting the basic text features intogeometrically consistent space. These 3D-enhanced text features are thenleveraged to precisely guide the attention of geometry features. We evaluatethe proposed method through extensive comparisons and ablation studies on theMono3DRefer dataset. Experimental results demonstrate substantial improvementsover previous methods, achieving new state-of-the-art results with a notableaccuracy gain of 11.94\% in the "Far" scenario. Our code will be made publiclyavailable.</description><author>Yuzhen Li, Min Liu, Yuan Bian, Xueping Wang, Zhaoyang Li, Gen Li, Yaonan Wang</author><pubDate>Tue, 26 Aug 2025 16:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19165v1</guid></item><item><title>MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation</title><link>http://arxiv.org/abs/2508.19163v1</link><description>Despite the growing use of large language models (LLMs) in clinical dialoguesystems, existing evaluations focus on task completion or fluency, offeringlittle insight into the behavioral and risk management requirements essentialfor safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTionfRamework for safe Interactions and conteXtual clinical conversationalevaluation), a structured, extensible framework for safety-oriented evaluationof clinical dialogue agents. MATRIX integrates three components: (1) a safety-aligned taxonomy of clinicalscenarios, expected system behaviors and failure modes derived throughstructured safety engineering methods; (2) BehvJudge, an LLM-based evaluatorfor detecting safety-relevant dialogue failures, validated against expertclinician annotations; and (3) PatBot, a simulated patient agent capable ofproducing diverse, scenario-conditioned responses, evaluated for realism andbehavioral fidelity with human factors expertise, and a patient-preferencestudy. Across three experiments, we show that MATRIX enables systematic, scalablesafety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazarddetection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blindedassessment of 240 dialogues. We also conducted one of the first realismanalyses of LLM-based patient simulation, showing that PatBot reliablysimulates realistic patient behavior in quantitative and qualitativeevaluations. Using MATRIX, we demonstrate its effectiveness in benchmarkingfive LLM agents across 2,100 simulated dialogues spanning 14 hazard scenariosand 10 clinical domains. MATRIX is the first framework to unify structured safety engineering withscalable, validated conversational AI evaluation, enabling regulator-alignedsafety auditing. We release all evaluation tools, prompts, structuredscenarios, and datasets.</description><author>Ernest Lim, Yajie Vera He, Jared Joselowitz, Kate Preston, Mohita Chowdhury, Louis Williams, Aisling Higham, Katrina Mason, Mariane Melo, Tom Lawton, Yan Jia, Ibrahim Habli</author><pubDate>Tue, 26 Aug 2025 16:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19163v1</guid></item><item><title>Few-Shot Connectivity-Aware Text Line Segmentation in Historical Documents</title><link>http://arxiv.org/abs/2508.19162v1</link><description>A foundational task for the digital analysis of documents is text linesegmentation. However, automating this process with deep learning models ischallenging because it requires large, annotated datasets that are oftenunavailable for historical documents. Additionally, the annotation process is alabor- and cost-intensive task that requires expert knowledge, which makesfew-shot learning a promising direction for reducing data requirements. In thiswork, we demonstrate that small and simple architectures, coupled with atopology-aware loss function, are more accurate and data-efficient than morecomplex alternatives. We pair a lightweight UNet++ with a connectivity-awareloss, initially developed for neuron morphology, which explicitly penalizesstructural errors like line fragmentation and unintended line merges. Toincrease our limited data, we train on small patches extracted from a merethree annotated pages per manuscript. Our methodology significantly improvesupon the current state-of-the-art on the U-DIADS-TL dataset, with a 200%increase in Recognition Accuracy and a 75% increase in Line Intersection overUnion. Our method also achieves an F-Measure score on par with or evenexceeding that of the competition winner of the DIVA-HisDB baseline detectiontask, all while requiring only three annotated pages, exemplifying the efficacyof our approach. Our implementation is publicly available at:https://github.com/RafaelSterzinger/acpr_few_shot_hist.</description><author>Rafael Sterzinger, Tingyu Lin, Robert Sablatnig</author><pubDate>Tue, 26 Aug 2025 16:11:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19162v1</guid></item><item><title>RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration</title><link>http://arxiv.org/abs/2508.19154v1</link><description>We present the RAW domain diffusion model (RDDM), an end-to-end diffusionmodel that restores photo-realistic images directly from the sensor RAW data.While recent sRGB-domain diffusion methods achieve impressive results, they arecaught in a dilemma between high fidelity and realistic generation. As thesemodels process lossy sRGB inputs and neglect the accessibility of the sensorRAW images in many scenarios, e.g., in image and video capturing in edgedevices, resulting in sub-optimal performance. RDDM bypasses this limitation bydirectly restoring images in the RAW domain, replacing the conventionaltwo-stage image signal processing (ISP) + IR pipeline. However, a simpleadaptation of pre-trained diffusion models to the RAW domain confronts theout-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE(RVAE) learning optimal latent representations, (2) a differentiable Post ToneProcessing (PTP) module enabling joint RAW and sRGB space optimization. Tocompensate for the deficiency in the dataset, we develop a scalable degradationpipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets forlarge-scale training. Furthermore, we devise a configurable multi-bayer (CMB)LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensiveexperiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusionmethods, yielding higher fidelity results with fewer artifacts.</description><author>Yan Chen, Yi Wen, Wei Li, Junchao Liu, Yong Guo, Jie Hu, Xinghao Chen</author><pubDate>Tue, 26 Aug 2025 16:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19154v1</guid></item><item><title>MicroMIL: Graph-Based Multiple Instance Learning for Context-Aware Diagnosis with Microscopic Images</title><link>http://arxiv.org/abs/2407.21604v4</link><description>Cancer diagnosis has greatly benefited from the integration of whole-slideimages (WSIs) with multiple instance learning (MIL), enabling high-resolutionanalysis of tissue morphology. Graph-based MIL (GNN-MIL) approaches haveemerged as powerful solutions for capturing contextual information in WSIs,thereby improving diagnostic accuracy. However, WSIs require significantcomputational and infrastructural resources, limiting accessibility inresource-constrained settings. Conventional light microscopes offer acost-effective alternative, but applying GNN-MIL to such data is challengingdue to extensive redundant images and missing spatial coordinates, which hindercontextual learning. To address these issues, we introduce MicroMIL, the firstweakly-supervised MIL framework specifically designed for images acquired fromconventional light microscopes. MicroMIL leverages a representative imageextractor (RIE) that employs deep cluster embedding (DCE) and hardGumbel-Softmax to dynamically reduce redundancy and select representativeimages. These images serve as graph nodes, with edges computed via cosinesimilarity, eliminating the need for spatial coordinates while preservingcontextual information. Extensive experiments on a real-world colon cancerdataset and the BreakHis dataset demonstrate that MicroMIL achievesstate-of-the-art performance, improving both diagnostic accuracy and robustnessto redundancy. The code is available athttps://github.com/kimjongwoo-cell/MicroMIL</description><author>Jongwoo Kim, Bryan Wong, Huazhu Fu, Willmer Rafell Quiñones, Youngsin Ko, Mun Yong Yi</author><pubDate>Tue, 26 Aug 2025 16:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21604v4</guid></item><item><title>Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games</title><link>http://arxiv.org/abs/2508.19152v1</link><description>Contemporary artificial intelligence (AI) development largely centers onrational decision-making, valued for its measurability and suitability forobjective evaluation. Yet in real-world contexts, an intelligent agent'sdecisions are shaped not only by logic but also by deeper influences such asbeliefs, values, and preferences. The diversity of human decision-making stylesemerges from these differences, highlighting that "style" is an essential butoften overlooked dimension of intelligence. This dissertation introduces playstyle as an alternative lens for observingand analyzing the decision-making behavior of intelligent agents, and examinesits foundational meaning and historical context from a philosophicalperspective. By analyzing how beliefs and values drive intentions and actions,we construct a two-tier framework for style formation: the external interactionloop with the environment and the internal cognitive loop of deliberation. Onthis basis, we formalize style-related characteristics and propose measurableindicators such as style capacity, style popularity, and evolutionary dynamics. The study focuses on three core research directions: (1) Defining andmeasuring playstyle, proposing a general playstyle metric based on discretizedstate spaces, and extending it to quantify strategic diversity and competitivebalance; (2) Expressing and generating playstyle, exploring how reinforcementlearning and imitation learning can be used to train agents exhibiting specificstylistic tendencies, and introducing a novel approach for human-like stylelearning and modeling; and (3) Practical applications, analyzing the potentialof these techniques in domains such as game design and interactiveentertainment. Finally, the dissertation outlines future extensions, including the role ofstyle as a core element in building artificial general intelligence (AGI).</description><author>Chiu-Chou Lin</author><pubDate>Tue, 26 Aug 2025 16:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19152v1</guid></item><item><title>Generative Artificial Intelligence-Supported Pentesting: A Comparison between Claude Opus, GPT-4, and Copilot</title><link>http://arxiv.org/abs/2501.06963v2</link><description>The advent of Generative Artificial Intelligence (GenAI) has brought asignificant change to our society. GenAI can be applied across numerous fields,with particular relevance in cybersecurity. Among the various areas ofapplication, its use in penetration testing (pentesting) or ethical hackingprocesses is of special interest. In this paper, we have analyzed the potentialof leading generic-purpose GenAI tools-Claude Opus, GPT-4 from ChatGPT, andCopilot-in augmenting the penetration testing process as defined by thePenetration Testing Execution Standard (PTES). Our analysis involved evaluatingeach tool across all PTES phases within a controlled virtualized environment.The findings reveal that, while these tools cannot fully automate thepentesting process, they provide substantial support by enhancing efficiencyand effectiveness in specific tasks. Notably, all tools demonstrated utility;however, Claude Opus consistently outperformed the others in our experimentalscenarios.</description><author>Antonio López Martínez, Alejandro Cano, Antonio Ruiz-Martínez</author><pubDate>Tue, 26 Aug 2025 16:03:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06963v2</guid></item><item><title>Saddle Hierarchy in Dense Associative Memory</title><link>http://arxiv.org/abs/2508.19151v1</link><description>Dense associative memory (DAM) models have been attracting renewed attentionsince they were shown to be robust to adversarial examples and closely relatedto state-of-the-art machine learning paradigms, such as the attentionmechanisms in transformers and generative diffusion models. We study a DAMbuilt upon a three-layer Boltzmann machine with Potts hidden units, whichrepresent data clusters and classes. Through a statistical mechanics analysis,we derive saddle-point equations that characterize both the stationary pointsof DAMs trained on real data and the fixed points of DAMs trained on syntheticdata within a teacher-student framework. Based on these results, we propose anovel regularization scheme that makes training significantly more stable.Moreover, we show empirically that our DAM learns interpretable solutions toboth supervised and unsupervised classification problems. Pushing ourtheoretical analysis further, we find that the weights learned by relativelysmall DAMs correspond to unstable saddle points in larger DAMs. We implement anetwork-growing algorithm that leverages this saddle-point hierarchy todrastically reduce the computational cost of training dense associative memory.</description><author>Robin Thériault, Daniele Tantari</author><pubDate>Tue, 26 Aug 2025 16:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19151v1</guid></item><item><title>Leveraging Multi-facet Paths for Heterogeneous Graph Representation Learning</title><link>http://arxiv.org/abs/2407.20648v3</link><description>Recent advancements in graph neural networks (GNNs) and heterogeneous GNNs(HGNNs) have advanced node embeddings and relationship learning for varioustasks. However, existing methods often rely on domain-specific predefinedmeta-paths, which are coarse-grained and focus solely on aspects like nodetype, limiting their ability to capture complex interactions. We introduceMF2Vec, a model that uses multi-faceted (fine-grained) paths instead ofpredefined meta-paths. MF2Vec extracts paths via random walks and generatesmulti-faceted vectors, ignoring predefined schemas. This method learns diverseaspects of nodes and their relationships, constructs a homogeneous network, andcreates node embeddings for classification, link prediction, and clustering.Extensive experiments show that MF2Vec outperforms existing methods, offering amore flexible and comprehensive framework for analyzing complex networks. Thecode is available at https://anonymous.4open.science/r/MF2Vec-6ABC.</description><author>Jongwoo Kim, Seongyeub Chu, Hyeongmin Park, Bryan Wong, Keejun Han, Mun Yong Yi</author><pubDate>Tue, 26 Aug 2025 16:02:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20648v3</guid></item><item><title>Uncertainty-Resilient Active Intention Recognition for Robotic Assistants</title><link>http://arxiv.org/abs/2508.19150v1</link><description>Purposeful behavior in robotic assistants requires the integration ofmultiple components and technological advances. Often, the problem is reducedto recognizing explicit prompts, which limits autonomy, or is oversimplifiedthrough assumptions such as near-perfect information. We argue that a criticalgap remains unaddressed -- specifically, the challenge of reasoning about theuncertain outcomes and perception errors inherent to human intentionrecognition. In response, we present a framework designed to be resilient touncertainty and sensor noise, integrating real-time sensor data with acombination of planners. Centered around an intention-recognition POMDP, ourapproach addresses cooperative planning and acting under uncertainty. Ourintegrated framework has been successfully tested on a physical robot withpromising results.</description><author>Juan Carlos Saborío, Marc Vinci, Oscar Lima, Sebastian Stock, Lennart Niecksch, Martin Günther, Alexander Sung, Joachim Hertzberg, Martin Atzmüller</author><pubDate>Tue, 26 Aug 2025 16:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19150v1</guid></item><item><title>Algorithmic Collective Action with Multiple Collectives</title><link>http://arxiv.org/abs/2508.19149v1</link><description>As learning systems increasingly influence everyday decisions, user-sidesteering via Algorithmic Collective Action (ACA)-coordinated changes to shareddata-offers a complement to regulator-side policy and firm-side model design.Although real-world actions have been traditionally decentralized andfragmented into multiple collectives despite sharing overarchingobjectives-with each collective differing in size, strategy, and actionablegoals, most of the ACA literature focused on single collective settings. Inthis work, we present the first theoretical framework for ACA with multiplecollectives acting on the same system. In particular, we focus on collectiveaction in classification, studying how multiple collectives can plant signals,i.e., bias a classifier to learn an association between an altered version ofthe features and a chosen, possibly overlapping, set of target classes. Weprovide quantitative results about the role and the interplay of collectives'sizes and their alignment of goals. Our framework, by also complementingprevious empirical results, opens a path for a holistic treatment of ACA withmultiple collectives.</description><author>Claudio Battiloro, Pietro Greiner, Bret Nestor, Oumaima Amezgar, Francesca Dominici</author><pubDate>Tue, 26 Aug 2025 16:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19149v1</guid></item><item><title>Echoes of the past: A unified perspective on fading memory and echo states</title><link>http://arxiv.org/abs/2508.19145v1</link><description>Recurrent neural networks (RNNs) have become increasingly popular ininformation processing tasks involving time series and temporal data. Afundamental property of RNNs is their ability to create reliable input/outputresponses, often linked to how the network handles its memory of theinformation it processed. Various notions have been proposed to conceptualizethe behavior of memory in RNNs, including steady states, echo states, stateforgetting, input forgetting, and fading memory. Although these notions areoften used interchangeably, their precise relationships remain unclear. Thiswork aims to unify these notions in a common language, derive new implicationsand equivalences between them, and provide alternative proofs to some existingresults. By clarifying the relationships between these concepts, this researchcontributes to a deeper understanding of RNNs and their temporal informationprocessing capabilities.</description><author>Juan-Pablo Ortega, Florian Rossmannek</author><pubDate>Tue, 26 Aug 2025 15:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19145v1</guid></item><item><title>GeNet: A Multimodal LLM-Based Co-Pilot for Network Topology and Configuration</title><link>http://arxiv.org/abs/2407.08249v2</link><description>Communication network engineering in enterprise environments is traditionallya complex, time-consuming, and error-prone manual process. Most research onnetwork engineering automation has concentrated on configuration synthesis,often overlooking changes in the physical network topology. This paperintroduces GeNet, a multimodal co-pilot for enterprise network engineers. GeNetis a novel framework that leverages a large language model (LLM) to streamlinenetwork design workflows. It uses visual and textual modalities to interpretand update network topologies and device configurations based on user intents.GeNet was evaluated on enterprise network scenarios adapted from Ciscocertification exercises. Our results demonstrate GeNet's ability to interpretnetwork topology images accurately, potentially reducing network engineers'efforts and accelerating network design processes in enterprise environments.Furthermore, we show the importance of precise topology understanding whenhandling intents that require modifications to the network's topology.</description><author>Beni Ifland, Elad Duani, Rubin Krief, Miro Ohana, Aviram Zilberman, Andres Murillo, Ofir Manor, Ortal Lavi, Hikichi Kenji, Asaf Shabtai, Yuval Elovici, Rami Puzis</author><pubDate>Tue, 26 Aug 2025 15:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08249v2</guid></item><item><title>A Bag of Tricks for Efficient Implicit Neural Point Clouds</title><link>http://arxiv.org/abs/2508.19140v1</link><description>Implicit Neural Point Cloud (INPC) is a recent hybrid representation thatcombines the expressiveness of neural fields with the efficiency of point-basedrendering, achieving state-of-the-art image quality in novel view synthesis.However, as with other high-quality approaches that query neural networksduring rendering, the practical usability of INPC is limited by comparativelyslow rendering. In this work, we present a collection of optimizations thatsignificantly improve both the training and inference performance of INPCwithout sacrificing visual fidelity. The most significant modifications are animproved rasterizer implementation, more effective sampling techniques, and theincorporation of pre-training for the convolutional neural network used forhole-filling. Furthermore, we demonstrate that points can be modeled as smallGaussians during inference to further improve quality in extrapolated, e.g.,close-up views of the scene. We design our implementations to be broadlyapplicable beyond INPC and systematically evaluate each modification in aseries of experiments. Our optimized INPC pipeline achieves up to 25% fastertraining, 2x faster rendering, and 20% reduced VRAM usage paired with slightimage quality improvements.</description><author>Florian Hahlbohm, Linus Franke, Leon Overkämping, Paula Wespe, Susana Castillo, Martin Eisemann, Marcus Magnor</author><pubDate>Tue, 26 Aug 2025 15:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19140v1</guid></item><item><title>Generative Data Augmentation for Object Point Cloud Segmentation</title><link>http://arxiv.org/abs/2505.17783v2</link><description>Data augmentation is widely used to train deep learning models to addressdata scarcity. However, traditional data augmentation (TDA) typically relies onsimple geometric transformation, such as random rotation and rescaling,resulting in minimal data diversity enrichment and limited model performanceimprovement. State-of-the-art generative models for 3D shape generation rely onthe denoising diffusion probabilistic models and manage to generate realisticnovel point clouds for 3D content creation and manipulation. Nevertheless, thegenerated 3D shapes lack associated point-wise semantic labels, restrictingtheir usage in enlarging the training data for point cloud segmentation tasks.To bridge the gap between data augmentation techniques and the advanceddiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to apart-aware generative model that can generate high-quality point cloudsconditioned on given segmentation masks. Leveraging the novel generative model,we introduce a 3-step generative data augmentation (GDA) pipeline for pointcloud segmentation training. Our GDA approach requires only a small amount oflabeled samples but enriches the training data with generated variants andpseudo-labeled samples, which are validated by a novel diffusion-basedpseudo-label filtering method. Extensive experiments on two large-scalesynthetic datasets and a real-world medical dataset demonstrate that our GDAmethod outperforms TDA approach and related semi-supervised and self-supervisedmethods.</description><author>Dekai Zhu, Stefan Gavranovic, Flavien Boussuge, Benjamin Busam, Slobodan Ilic</author><pubDate>Tue, 26 Aug 2025 15:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.17783v2</guid></item><item><title>ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context</title><link>http://arxiv.org/abs/2407.06866v3</link><description>While the biases of language models in production are extensively documented,the biases of their guardrails have been neglected. This paper studies howcontextual information about the user influences the likelihood of an LLM torefuse to execute a request. By generating user biographies that offerideological and demographic information, we find a number of biases inguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personasare more likely to trigger a refusal guardrail when requesting censored orillegal information. Guardrails are also sycophantic, refusing to comply withrequests for a political position the user is likely to disagree with. We findthat certain identity groups and seemingly innocuous information, e.g., sportsfandom, can elicit changes in guardrail sensitivity similar to directstatements of political ideology. For each demographic category and even forAmerican football team fandom, we find that ChatGPT appears to infer a likelypolitical ideology and modify guardrail behavior accordingly.</description><author>Victoria R. Li, Yida Chen, Naomi Saphra</author><pubDate>Tue, 26 Aug 2025 15:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06866v3</guid></item><item><title>Active Query Selection for Crowd-Based Reinforcement Learning</title><link>http://arxiv.org/abs/2508.19132v1</link><description>Preference-based reinforcement learning has gained prominence as a strategyfor training agents in environments where the reward signal is difficult tospecify or misaligned with human intent. However, its effectiveness is oftenlimited by the high cost and low availability of reliable human input,especially in domains where expert feedback is scarce or errors are costly. Toaddress this, we propose a novel framework that combines two complementarystrategies: probabilistic crowd modelling to handle noisy, multi-annotatorfeedback, and active learning to prioritize feedback on the most informativeagent actions. We extend the Advise algorithm to support multiple trainers,estimate their reliability online, and incorporate entropy-based queryselection to guide feedback requests. We evaluate our approach in a set ofenvironments that span both synthetic and real-world-inspired settings,including 2D games (Taxi, Pacman, Frozen Lake) and a blood glucose control taskfor Type 1 Diabetes using the clinically approved UVA/Padova simulator. Ourpreliminary results demonstrate that agents trained with feedback on uncertaintrajectories exhibit faster learning in most tasks, and we outperform thebaselines for the blood glucose control task.</description><author>Jonathan Erskine, Taku Yamagata, Raúl Santos-Rodríguez</author><pubDate>Tue, 26 Aug 2025 15:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19132v1</guid></item><item><title>Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions</title><link>http://arxiv.org/abs/2507.06029v2</link><description>Explainable AI (XAI) methods often struggle to generate clear, interpretableoutputs for users without domain expertise. We introduce Feature-GuidedNeighbor Selection (FGNS), a post hoc method that enhances interpretability byselecting class-representative examples using both local and global featureimportance. In a user study (N = 98) evaluating Kannada script classifications,FGNS significantly improved non-experts' ability to identify model errors whilemaintaining appropriate agreement with correct predictions. Participants madefaster and more accurate decisions compared to those given traditional k-NNexplanations. Quantitative analysis shows that FGNS selects neighbors thatbetter reflect class characteristics rather than merely minimizingfeature-space distance, leading to more consistent selection and tighterclustering around class prototypes. These results support FGNS as a step towardmore human-aligned model assessment, although further work is needed to addressthe gap between explanation quality and perceived trust.</description><author>Courtney Ford, Mark T. Keane</author><pubDate>Tue, 26 Aug 2025 15:33:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.06029v2</guid></item><item><title>ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments</title><link>http://arxiv.org/abs/2508.19131v1</link><description>The advancement of robotics and autonomous navigation systems hinges on theability to accurately predict terrain traversability. Traditional methods forgenerating datasets to train these prediction models often involve puttingrobots into potentially hazardous environments, posing risks to equipment andsafety. To solve this problem, we present ZeST, a novel approach leveragingvisual reasoning capabilities of Large Language Models (LLMs) to create atraversability map in real-time without exposing robots to danger. Our approachnot only performs zero-shot traversability and mitigates the risks associatedwith real-world data collection but also accelerates the development ofadvanced navigation systems, offering a cost-effective and scalable solution.To support our findings, we present navigation results, in both controlledindoor and unstructured outdoor environments. As shown in the experiments, ourmethod provides safer navigation when compared to other state-of-the-artmethods, constantly reaching the final goal.</description><author>Shreya Gummadi, Mateus V. Gasparino, Gianluca Capezzuto, Marcelo Becker, Girish Chowdhary</author><pubDate>Tue, 26 Aug 2025 15:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19131v1</guid></item><item><title>A Survey on Data Selection for LLM Instruction Tuning</title><link>http://arxiv.org/abs/2402.05123v3</link><description>Instruction tuning is a vital step of training large language models (LLMs),so how to enhance the effect of instruction tuning has received increasedattention. Existing works indicate that the quality of the dataset is morecrucial than the quantity during instruction tuning of LLMs. Therefore,recently a lot of studies focus on exploring the methods of selectinghigh-quality subset from instruction datasets, aiming to reduce training costsand enhance the instruction-following capabilities of LLMs. This paper presentsa comprehensive survey on data selection for LLM instruction tuning. Firstly,we introduce the wildly used instruction datasets. Then, we propose a newtaxonomy of the data selection methods and provide a detailed introduction ofrecent advances, and the evaluation strategies and results of data selectionmethods are also elaborated in detail. Finally, we emphasize the openchallenges and present new frontiers of this task.</description><author>Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu</author><pubDate>Tue, 26 Aug 2025 15:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05123v3</guid></item><item><title>An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal, and Deterministic Approach</title><link>http://arxiv.org/abs/2505.00039v4</link><description>Retrieval-Augmented Generation (RAG) systems in the legal domain face acritical challenge: standard, flat-text retrieval is blind to the hierarchical,diachronic, and causal structure of law, leading to anachronistic andunreliable answers. This paper introduces an ontology-driven Graph RAGframework designed to overcome these limitations. We ground our knowledge graphin a formal, LRMoo-inspired model that distinguishes abstract legal Works fromtheir versioned Expressions. We model temporal states as efficient aggregationsthat reuse the versioned expressions (CTVs) of unchanged components, and wereify legislative events as first-class Action nodes to make causality explicitand queryable. This structured backbone enables a unified, planner-guided querystrategy that applies explicit policies to deterministically resolve complexrequests for (i) point-in-time retrieval, (ii) hierarchical impact analysis,and (iii) auditable provenance reconstruction. Through a case study on theBrazilian Constitution, we demonstrate how this approach provides a verifiable,temporally-correct substrate for LLMs, enabling higher-order analyticalcapabilities while drastically reducing the risk of factual errors. The resultis a practical framework for building more trustworthy and explainable legal AIsystems.</description><author>Hudson de Martim</author><pubDate>Tue, 26 Aug 2025 15:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.00039v4</guid></item></channel></rss>