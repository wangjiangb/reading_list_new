<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 24 Oct 2023 06:00:24 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions</title><link>http://arxiv.org/abs/2310.15171v1</link><description>Depth estimation from monocular images is pivotal for real-world visualperception systems. While current learning-based depth estimation models trainand test on meticulously curated data, they often overlook out-of-distribution(OoD) situations. Yet, in practical settings -- especially safety-critical oneslike autonomous driving -- common corruptions can arise. Addressing thisoversight, we introduce a comprehensive robustness test suite, RoboDepth,encompassing 18 corruptions spanning three categories: i) weather and lightingconditions; ii) sensor failures and movement; and iii) data processinganomalies. We subsequently benchmark 42 depth estimation models across indoorand outdoor scenes to assess their resilience to these corruptions. Ourfindings underscore that, in the absence of a dedicated robustness evaluationframework, many leading depth estimation models may be susceptible to typicalcorruptions. We delve into design considerations for crafting more robust depthestimation models, touching upon pre-training, augmentation, modality, modelcapacity, and learning paradigms. We anticipate our benchmark will establish afoundational platform for advancing robust OoD depth estimation.</description><author>Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Lai Xing Ng, Benoit R. Cottereau, Wei Tsang Ooi</author><pubDate>Mon, 23 Oct 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15171v1</guid></item><item><title>FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling</title><link>http://arxiv.org/abs/2310.15169v1</link><description>With the availability of large-scale video datasets and the advances ofdiffusion models, text-driven video generation has achieved substantialprogress. However, existing video generation models are typically trained on alimited number of frames, resulting in the inability to generate high-fidelitylong videos during inference. Furthermore, these models only supportsingle-text conditions, whereas real-life scenarios often require multi-textconditions as the video content changes over time. To tackle these challenges,this study explores the potential of extending the text-driven capability togenerate longer videos conditioned on multiple texts. 1) We first analyze theimpact of initial noise in video diffusion models. Then building upon theobservation of noise, we propose FreeNoise, a tuning-free and time-efficientparadigm to enhance the generative capabilities of pretrained video diffusionmodels while preserving content consistency. Specifically, instead ofinitializing noises for all frames, we reschedule a sequence of noises forlong-range correlation and perform temporal attention over them by window-basedfunction. 2) Additionally, we design a novel motion injection method to supportthe generation of videos conditioned on multiple text prompts. Extensiveexperiments validate the superiority of our paradigm in extending thegenerative capabilities of video diffusion models. It is noteworthy thatcompared with the previous best-performing method which brought about 255%extra time cost, our method incurs only negligible time cost of approximately17%. Generated video samples are available at our website:http://haonanqiu.com/projects/FreeNoise.html.</description><author>Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, Ziwei Liu</author><pubDate>Mon, 23 Oct 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15169v1</guid></item><item><title>Ghost on the Shell: An Expressive Representation of General 3D Shapes</title><link>http://arxiv.org/abs/2310.15168v1</link><description>The creation of photorealistic virtual worlds requires the accurate modelingof 3D surface geometry for a wide range of objects. For this, meshes areappealing since they 1) enable fast physics-based rendering with realisticmaterial and lighting, 2) support physical simulation, and 3) arememory-efficient for modern graphics pipelines. Recent work on reconstructingand statistically modeling 3D shape, however, has critiqued meshes as beingtopologically inflexible. To capture a wide range of object shapes, any 3Drepresentation must be able to model solid, watertight, shapes as well as thin,open, surfaces. Recent work has focused on the former, and methods forreconstructing open surfaces do not support fast reconstruction with materialand lighting or unconditional generative modelling. Inspired by the observationthat open surfaces can be seen as islands floating on watertight surfaces, weparameterize open surfaces by defining a manifold signed distance field onwatertight templates. With this parameterization, we further develop agrid-based and differentiable representation that parameterizes both watertightand non-watertight meshes of arbitrary topology. Our new representation, calledGhost-on-the-Shell (G-Shell), enables two important applications:differentiable rasterization-based reconstruction from multiview images andgenerative modelling of non-watertight meshes. We empirically demonstrate thatG-Shell achieves state-of-the-art performance on non-watertight meshreconstruction and generation tasks, while also performing effectively forwatertight meshes.</description><author>Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, Bernhard Schölkopf</author><pubDate>Mon, 23 Oct 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15168v1</guid></item><item><title>Large Language Models are Visual Reasoning Coordinators</title><link>http://arxiv.org/abs/2310.15166v1</link><description>Visual reasoning requires multimodal perception and commonsense cognition ofthe world. Recently, multiple vision-language models (VLMs) have been proposedwith excellent commonsense reasoning ability in various domains. However, howto harness the collective power of these complementary VLMs is rarely explored.Existing methods like ensemble still struggle to aggregate these models withthe desired higher-order communications. In this work, we propose Cola, a novelparadigm that coordinates multiple VLMs for visual reasoning. Our key insightis that a large language model (LLM) can efficiently coordinate multiple VLMsby facilitating natural language communication that leverages their distinctand complementary capabilities. Extensive experiments demonstrate that ourinstruction tuning variant, Cola-FT, achieves state-of-the-art performance onvisual question answering (VQA), outside knowledge VQA, visual entailment, andvisual spatial reasoning tasks. Moreover, we show that our in-context learningvariant, Cola-Zero, exhibits competitive performance in zero and few-shotsettings, without finetuning. Through systematic ablation studies andvisualizations, we validate that a coordinator LLM indeed comprehends theinstruction prompts as well as the separate functionalities of VLMs; it thencoordinates them to enable impressive visual reasoning capabilities.</description><author>Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, Ziwei Liu</author><pubDate>Mon, 23 Oct 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15166v1</guid></item><item><title>Handling Data Heterogeneity via Architectural Design for Federated Visual Recognition</title><link>http://arxiv.org/abs/2310.15165v1</link><description>Federated Learning (FL) is a promising research paradigm that enables thecollaborative training of machine learning models among various parties withoutthe need for sensitive information exchange. Nonetheless, retaining data inindividual clients introduces fundamental challenges to achieving performanceon par with centrally trained models. Our study provides an extensive review offederated learning applied to visual recognition. It underscores the criticalrole of thoughtful architectural design choices in achieving optimalperformance, a factor often neglected in the FL literature. Many existing FLsolutions are tested on shallow or simple networks, which may not accuratelyreflect real-world applications. This practice restricts the transferability ofresearch findings to large-scale visual recognition models. Through an in-depthanalysis of diverse cutting-edge architectures such as convolutional neuralnetworks, transformers, and MLP-mixers, we experimentally demonstrate thatarchitectural choices can substantially enhance FL systems' performance,particularly when handling heterogeneous data. We study 19 visual recognitionmodels from five different architectural families on four challenging FLdatasets. We also re-investigate the inferior performance of convolution-basedarchitectures in the FL setting and analyze the influence of normalizationlayers on the FL performance. Our findings emphasize the importance ofarchitectural design for computer vision tasks in practical scenarios,effectively narrowing the performance gap between federated and centralizedlearning. Our source code is available athttps://github.com/sarapieri/fed_het.git.</description><author>Sara Pieri, Jose Renato Restom, Samuel Horvath, Hisham Cholakkal</author><pubDate>Mon, 23 Oct 2023 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15165v1</guid></item><item><title>LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers</title><link>http://arxiv.org/abs/2310.15164v1</link><description>Logical reasoning, i.e., deductively inferring the truth value of aconclusion from a set of premises, is an important task for artificialintelligence with wide potential impacts on science, mathematics, and society.While many prompting-based strategies have been proposed to enable LargeLanguage Models (LLMs) to do such reasoning more effectively, they still appearunsatisfactory, often failing in subtle and unpredictable ways. In this work,we investigate the validity of instead reformulating such tasks as modularneurosymbolic programming, which we call LINC: Logical Inference viaNeurosymbolic Computation. In LINC, the LLM acts as a semantic parser,translating premises and conclusions from natural language to expressions infirst-order logic. These expressions are then offloaded to an external theoremprover, which symbolically performs deductive inference. Leveraging thisapproach, we observe significant performance gains on FOLIO and a balancedsubset of ProofWriter for three different models in nearly all experimentalconditions we evaluate. On ProofWriter, augmenting the comparatively smallopen-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%,respectively. When used with GPT-4, LINC scores 26% higher than CoT onProofWriter while performing comparatively on FOLIO. Further analysis revealsthat although both methods on average succeed roughly equally often on thisdataset, they exhibit distinct and complementary failure modes. We thus providepromising evidence for how logical reasoning over natural language can betackled through jointly leveraging LLMs alongside symbolic provers. Allcorresponding code is publicly available at https://github.com/benlipkin/linc</description><author>Theo X. Olausson, Alex Gu, Benjamin Lipkin, Cedegao E. Zhang, Armando Solar-Lezama, Joshua B. Tenenbaum, Roger Levy</author><pubDate>Mon, 23 Oct 2023 18:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15164v1</guid></item><item><title>SAM-Med3D</title><link>http://arxiv.org/abs/2310.15161v1</link><description>Although the Segment Anything Model (SAM) has demonstrated impressiveperformance in 2D natural image segmentation, its application to 3D volumetricmedical images reveals significant shortcomings, namely suboptimal performanceand unstable prediction, necessitating an excessive number of prompt points toattain the desired outcomes. These issues can hardly be addressed byfine-tuning SAM on medical data because the original 2D structure of SAMneglects 3D spatial information. In this paper, we introduce SAM-Med3D, themost comprehensive study to modify SAM for 3D medical images. Our approach ischaracterized by its comprehensiveness in two primary aspects: firstly, bycomprehensively reformulating SAM to a thorough 3D architecture trained on acomprehensively processed large-scale volumetric medical dataset; and secondly,by providing a comprehensive evaluation of its performance. Specifically, wetrain SAM-Med3D with over 131K 3D masks and 247 categories. Our SAM-Med3Dexcels at capturing 3D spatial information, exhibiting competitive performancewith significantly fewer prompt points than the top-performing fine-tuned SAMin the medical domain. We then evaluate its capabilities across 15 datasets andanalyze it from multiple perspectives, including anatomical structures,modalities, targets, and generalization abilities. Our approach, compared withSAM, showcases pronouncedly enhanced efficiency and broad segmentationcapabilities for 3D volumetric medical images. Our code is released athttps://github.com/uni-medical/SAM-Med3D.</description><author>Haoyu Wang, Sizheng Guo, Jin Ye, Zhongying Deng, Junlong Cheng, Tianbin Li, Jianpin Chen, Yanzhou Su, Ziyan Huang, Yiqing Shen, Bin Fu, Shaoting Zhang, Junjun He, Yu Qiao</author><pubDate>Mon, 23 Oct 2023 18:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15161v1</guid></item><item><title>FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models</title><link>http://arxiv.org/abs/2310.15160v1</link><description>Semantic segmentation has witnessed tremendous progress due to the proposalof various advanced network architectures. However, they are extremely hungryfor delicate annotations to train, and the acquisition is laborious andunaffordable. Therefore, we present FreeMask in this work, which resorts tosynthetic images from generative models to ease the burden of both datacollection and annotation procedures. Concretely, we first synthesize abundanttraining images conditioned on the semantic masks provided by realisticdatasets. This yields extra well-aligned image-mask training pairs for semanticsegmentation models. We surprisingly observe that, solely trained withsynthetic images, we already achieve comparable performance with real ones(e.g., 48.3 vs. 48.5 mIoU on ADE20K, and 49.3 vs. 50.5 on COCO-Stuff). Then, weinvestigate the role of synthetic images by joint training with real images, orpre-training for real images. Meantime, we design a robust filtering principleto suppress incorrectly synthesized regions. In addition, we propose toinequally treat different semantic masks to prioritize those harder ones andsample more corresponding synthetic images for them. As a result, eitherjointly trained or pre-trained with our filtered and re-sampled synthesizedimages, segmentation models can be greatly enhanced, e.g., from 48.7 to 52.0 onADE20K. Code is available at https://github.com/LiheYoung/FreeMask.</description><author>Lihe Yang, Xiaogang Xu, Bingyi Kang, Yinghuan Shi, Hengshuang Zhao</author><pubDate>Mon, 23 Oct 2023 18:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15160v1</guid></item><item><title>Linear Representations of Sentiment in Large Language Models</title><link>http://arxiv.org/abs/2310.15154v1</link><description>Sentiment is a pervasive feature in natural language text, yet it is an openquestion how sentiment is represented within Large Language Models (LLMs). Inthis study, we reveal that across a range of models, sentiment is representedlinearly: a single direction in activation space mostly captures the featureacross a range of tasks with one extreme for positive and the other fornegative. Through causal interventions, we isolate this direction and show itis causally relevant in both toy tasks and real world datasets such as StanfordSentiment Treebank. Through this case study we model a thorough investigationof what a single direction means on a broad data distribution. We further uncover the mechanisms that involve this direction, highlightingthe roles of a small subset of attention heads and neurons. Finally, wediscover a phenomenon which we term the summarization motif: sentiment is notsolely represented on emotionally charged words, but is additionally summarizedat intermediate positions without inherent sentiment, such as punctuation andnames. We show that in Stanford Sentiment Treebank zero-shot classification,76% of above-chance classification accuracy is lost when ablating the sentimentdirection, nearly half of which (36%) is due to ablating the summarizedsentiment direction exclusively at comma positions.</description><author>Curt Tigges, Oskar John Hollinsworth, Atticus Geiger, Neel Nanda</author><pubDate>Mon, 23 Oct 2023 18:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15154v1</guid></item><item><title>Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number</title><link>http://arxiv.org/abs/2310.15151v1</link><description>Deep architectures such as Transformers are sometimes criticized for havinguninterpretable "black-box" representations. We use causal interventionanalysis to show that, in fact, some linguistic features are represented in alinear, interpretable format. Specifically, we show that BERT's ability toconjugate verbs relies on a linear encoding of subject number that can bemanipulated with predictable effects on conjugation accuracy. This encoding isfound in the subject position at the first layer and the verb position at thelast layer, but distributed across positions at middle layers, particularlywhen there are multiple cues to subject number.</description><author>Sophie Hao, Tal Linzen</author><pubDate>Mon, 23 Oct 2023 18:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15151v1</guid></item><item><title>Online Detection of AI-Generated Images</title><link>http://arxiv.org/abs/2310.15150v1</link><description>With advancements in AI-generated images coming on a continuous basis, it isincreasingly difficult to distinguish traditionally-sourced images (e.g.,photos, artwork) from AI-generated ones. Previous detection methods study thegeneralization from a single generator to another in isolation. However, inreality, new generators are released on a streaming basis. We studygeneralization in this setting, training on N models and testing on the next(N+k), following the historical release dates of well-known generation methods.Furthermore, images increasingly consist of both real and generated components,for example through image inpainting. Thus, we extend this approach to pixelprediction, demonstrating strong performance using automatically-generatedinpainted data. In addition, for settings where commercial models are notpublicly available for automatic data generation, we evaluate if pixeldetectors can be trained solely on whole synthetic images.</description><author>David C. Epstein, Ishan Jain, Oliver Wang, Richard Zhang</author><pubDate>Mon, 23 Oct 2023 18:53:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15150v1</guid></item><item><title>Unlocking the Transferability of Tokens in Deep Models for Tabular Data</title><link>http://arxiv.org/abs/2310.15149v1</link><description>Fine-tuning a pre-trained deep neural network has become a successfulparadigm in various machine learning tasks. However, such a paradigm becomesparticularly challenging with tabular data when there are discrepancies betweenthe feature sets of pre-trained models and the target tasks. In this paper, wepropose TabToken, a method aims at enhancing the quality of feature tokens(i.e., embeddings of tabular features). TabToken allows for the utilization ofpre-trained models when the upstream and downstream tasks share overlappingfeatures, facilitating model fine-tuning even with limited training examples.Specifically, we introduce a contrastive objective that regularizes the tokens,capturing the semantics within and across features. During the pre-trainingstage, the tokens are learned jointly with top-layer deep models such astransformer. In the downstream task, tokens of the shared features are keptfixed while TabToken efficiently fine-tunes the remaining parts of the model.TabToken not only enables knowledge transfer from a pre-trained model to taskswith heterogeneous features, but also enhances the discriminative ability ofdeep tabular models in standard classification and regression tasks.</description><author>Qi-Le Zhou, Han-Jia Ye, Le-Ye Wang, De-Chuan Zhan</author><pubDate>Mon, 23 Oct 2023 18:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15149v1</guid></item><item><title>S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models</title><link>http://arxiv.org/abs/2310.15147v1</link><description>The rapid development of Large Language Models (LLMs) has led to greatstrides in model capabilities like reasoning and long-context understanding.However, as LLMs are able to process longer contexts, it becomes morechallenging to evaluate whether they have acquired certain capabilities, sincethe length of text (e.g., 100K tokens) they can process far exceeds what humanscan reliably assess in a reasonable duration. In this paper, we propose usingcomplex synthetic tasks as a proxy evaluation method, and present S3Eval, aSynthetic, Scalable, Systematic evaluation suite for LLMs evaluation. As asynthetic benchmark, S3Eval enables the creation of any number of evaluationexamples that are theoretically invisible to LLMs, mitigating the test setcontamination issue. The synthetic nature of S3Eval provides users full controlover the dataset, allowing them to systematically probe LLM capabilities byscaling text length and varying task difficulty across diverse scenarios. Thestrong correlation between S3Eval performance and scores of real-worldbenchmarks like Big-Bench Hard (BBH) demonstrates the soundness of using S3Evalfor evaluation of LLMs. The in-depth analysis also uncover additional insights,including performance drop when the answer is sparsely distributed or locatedin the middle context, as well as some counter-intuitive trends of modelperformance.</description><author>Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao, Kang Liu</author><pubDate>Mon, 23 Oct 2023 18:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15147v1</guid></item><item><title>GRASP: Accelerating Shortest Path Attacks via Graph Attention</title><link>http://arxiv.org/abs/2310.07980v2</link><description>Recent advances in machine learning (ML) have shown promise in aiding andaccelerating classical combinatorial optimization algorithms. ML-based speedups that aim to learn in an end to end manner (i.e., directly output thesolution) tend to trade off run time with solution quality. Therefore,solutions that are able to accelerate existing solvers while maintaining theirperformance guarantees, are of great interest. We consider an APX-hard problem,where an adversary aims to attack shortest paths in a graph by removing theminimum number of edges. We propose the GRASP algorithm: Graph AttentionAccelerated Shortest Path Attack, an ML aided optimization algorithm thatachieves run times up to 10x faster, while maintaining the quality of solutiongenerated. GRASP uses a graph attention network to identify a smaller subgraphcontaining the combinatorial solution, thus effectively reducing the inputproblem size. Additionally, we demonstrate how careful representation of theinput graph, including node features that correlate well with the optimizationtask, can highlight important structure in the optimization solution.</description><author>Zohair Shafi, Benjamin A. Miller, Ayan Chatterjee, Tina Eliassi-Rad, Rajmonda S. Caceres</author><pubDate>Mon, 23 Oct 2023 18:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07980v2</guid></item><item><title>Robot Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning</title><link>http://arxiv.org/abs/2310.15145v1</link><description>The pre-train and fine-tune paradigm in machine learning has had dramaticsuccess in a wide range of domains because the use of existing data orpre-trained models on the internet enables quick and easy learning of newtasks. We aim to enable this paradigm in robotic reinforcement learning,allowing a robot to learn a new task with little human effort by leveragingdata and models from the Internet. However, reinforcement learning oftenrequires significant human effort in the form of manual reward specification orenvironment resets, even if the policy is pre-trained. We introduce RoboFuME, areset-free fine-tuning system that pre-trains a multi-task manipulation policyfrom diverse datasets of prior experiences and self-improves online to learn atarget task with minimal human intervention. Our insights are to utilizecalibrated offline reinforcement learning techniques to ensure efficient onlinefine-tuning of a pre-trained policy in the presence of distribution shifts andleverage pre-trained vision language models (VLMs) to build a robust rewardclassifier for autonomously providing reward signals during the onlinefine-tuning process. In a diverse set of five real robot manipulation tasks, weshow that our method can incorporate data from an existing robot datasetcollected at a different institution and improve on a target task within aslittle as 3 hours of autonomous real-world experience. We also demonstrate insimulation experiments that our method outperforms prior works that usedifferent RL algorithms or different approaches for predicting rewards. Projectwebsite: https://robofume.github.io</description><author>Jingyun Yang, Max Sobol Mark, Brandon Vu, Archit Sharma, Jeannette Bohg, Chelsea Finn</author><pubDate>Mon, 23 Oct 2023 18:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15145v1</guid></item><item><title>DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design</title><link>http://arxiv.org/abs/2310.15144v1</link><description>We introduce DEsignBench, a text-to-image (T2I) generation benchmark tailoredfor visual design scenarios. Recent T2I models like DALL-E 3 and others, havedemonstrated remarkable capabilities in generating photorealistic images thatalign closely with textual inputs. While the allure of creating visuallycaptivating images is undeniable, our emphasis extends beyond mere aestheticpleasure. We aim to investigate the potential of using these powerful models inauthentic design contexts. In pursuit of this goal, we develop DEsignBench,which incorporates test samples designed to assess T2I models on both "designtechnical capability" and "design application scenario." Each of these twodimensions is supported by a diverse set of specific design categories. Weexplore DALL-E 3 together with other leading T2I models on DEsignBench,resulting in a comprehensive visual gallery for side-by-side comparisons. ForDEsignBench benchmarking, we perform human evaluations on generated images inDEsignBench gallery, against the criteria of image-text alignment, visualaesthetic, and design creativity. Our evaluation also considers otherspecialized design capabilities, including text rendering, layout composition,color harmony, 3D design, and medium style. In addition to human evaluations,we introduce the first automatic image generation evaluator powered by GPT-4V.This evaluator provides ratings that align well with human judgments, whilebeing easily replicable and cost-efficient. A high-resolution version isavailable athttps://github.com/design-bench/design-bench.github.io/raw/main/designbench.pdf?download=</description><author>Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Lijuan Wang</author><pubDate>Mon, 23 Oct 2023 18:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15144v1</guid></item><item><title>Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR Decomposition</title><link>http://arxiv.org/abs/2305.02996v2</link><description>Cross-encoder models, which jointly encode and score a query-item pair, areprohibitively expensive for direct k-nearest neighbor (k-NN) search.Consequently, k-NN search typically employs a fast approximate retrieval (e.g.using BM25 or dual-encoder vectors), followed by reranking with across-encoder; however, the retrieval approximation often has detrimentalrecall regret. This problem is tackled by ANNCUR (Yadav et al., 2022), a recentwork that employs a cross-encoder only, making search efficient using arelatively small number of anchor items, and a CUR matrix factorization. WhileANNCUR's one-time selection of anchors tends to approximate the cross-encoderdistances on average, doing so forfeits the capacity to accurately estimatedistances to items near the query, leading to regret in the crucial end-task:recall of top-k items. In this paper, we propose ADACUR, a method thatadaptively, iteratively, and efficiently minimizes the approximation error forthe practically important top-k neighbors. It does so by iteratively performingk-NN search using the anchors available so far, then adding these retrievednearest neighbors to the anchor set for the next round. Empirically, onmultiple datasets, in comparison to previous traditional and state-of-the-artmethods such as ANNCUR and dual-encoder-based retrieve-and-rerank, our proposedapproach ADACUR consistently reduces recall error-by up to 70% on the importantk = 1 setting-while using no more compute than its competitors.</description><author>Nishant Yadav, Nicholas Monath, Manzil Zaheer, Andrew McCallum</author><pubDate>Mon, 23 Oct 2023 18:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02996v2</guid></item><item><title>Hyperparameter optimization of hp-greedy reduced basis for gravitational wave surrogates</title><link>http://arxiv.org/abs/2310.15143v1</link><description>In a previous work we introduced, in the context of gravitational wavescience, an initial study on an automated domain-decomposition approach forreduced basis through hp-greedy refinement. The approach constructs localreduced bases of lower dimensionality than global ones, with the same or higheraccuracy. These ``light'' local bases should imply both faster evaluations whenpredicting new waveforms and faster data analysis, in particular fasterstatistical inference (the forward and inverse problems, respectively). In thisapproach, however, we have previously found important dependence on severalhyperparameters, which do not appear in global reduced basis. This naturallyleads to the problem of hyperparameter optimization (HPO), which is the subjectof this paper. We tackle the problem through a Bayesian optimization, and showits superiority when compared to grid or random searches. We find that forgravitational waves from the collision of two spinning but non-precessing blackholes, for the same accuracy, local hp-greedy reduced bases with HPO have alower dimensionality of up to $4 \times$ for the cases here studied, dependingon the desired accuracy. This factor should directly translate in a parameterestimation speedup, for instance. Such acceleration might help in the nearreal-time requirements for electromagnetic counterparts of gravitational wavesfrom compact binary coalescences. In addition, we find that the Bayesianapproach used in this paper for HPO is two orders of magnitude faster than, forexample, a grid search, with about a $100 \times$ acceleration. The codedeveloped for this project is available as open source from publicrepositories.</description><author>Franco Cerino, Andrés Diaz-Pace, Emmanuel Tassone, Manuel Tiglio, Atuel Villegas</author><pubDate>Mon, 23 Oct 2023 18:48:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15143v1</guid></item><item><title>Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries</title><link>http://arxiv.org/abs/2310.13132v2</link><description>Large language models (LLMs) are transforming the ways the general publicaccesses and consumes information. Their influence is particularly pronouncedin pivotal sectors like healthcare, where lay individuals are increasinglyappropriating LLMs as conversational agents for everyday queries. While LLMsdemonstrate impressive language understanding and generation proficiencies,concerns regarding their safety remain paramount in these high-stake domains.Moreover, the development of LLMs is disproportionately focused on English. Itremains unclear how these LLMs perform in the context of non-English languages,a gap that is critical for ensuring equity in the real-world use of thesesystems.This paper provides a framework to investigate the effectiveness ofLLMs as multi-lingual dialogue systems for healthcare queries. Ourempirically-derived framework XlingEval focuses on three fundamental criteriafor evaluating LLM responses to naturalistic human-authored health-relatedquestions: correctness, consistency, and verifiability. Through extensiveexperiments on four major global languages, including English, Spanish,Chinese, and Hindi, spanning three expert-annotated large health Q&amp;A datasets,and through an amalgamation of algorithmic and human-evaluation strategies, wefound a pronounced disparity in LLM responses across these languages,indicating a need for enhanced cross-lingual capabilities. We further proposeXlingHealth, a cross-lingual benchmark for examining the multilingualcapabilities of LLMs in the healthcare context. Our findings underscore thepressing need to bolster the cross-lingual capacities of these models, and toprovide an equitable information ecosystem accessible to all.</description><author>Yiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu, Munmun De Choudhury, Srijan Kumar</author><pubDate>Mon, 23 Oct 2023 18:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13132v2</guid></item><item><title>SpecTr: Fast Speculative Decoding via Optimal Transport</title><link>http://arxiv.org/abs/2310.15141v1</link><description>Autoregressive sampling from large language models has led tostate-of-the-art results in several natural language tasks. However,autoregressive sampling generates tokens one at a time making it slow, and evenprohibitive in certain tasks. One way to speed up sampling is$\textit{speculative decoding}$: use a small model to sample a $\textit{draft}$(block or sequence of tokens), and then score all tokens in the draft by thelarge language model in parallel. A subset of the tokens in the draft areaccepted (and the rest rejected) based on a statistical method to guaranteethat the final output follows the distribution of the large model. In thiswork, we provide a principled understanding of speculative decoding through thelens of optimal transport (OT) with $\textit{membership cost}$. This frameworkcan be viewed as an extension of the well-known $\textit{maximal-coupling}$problem. This new formulation enables us to generalize the speculative decodingmethod to allow for a set of $k$ candidates at the token-level, which leads toan improved optimal membership cost. We show that the optimal draft selectionalgorithm (transport plan) can be computed via linear programming, whosebest-known runtime is exponential in $k$. We then propose a valid draftselection algorithm whose acceptance probability is $(1-1/e)$-optimalmultiplicatively. Moreover, it can be computed in time almost linear with sizeof domain of a single token. Using this $new draft selection$ algorithm, wedevelop a new autoregressive sampling algorithm called $\textit{SpecTr}$, whichprovides speedup in decoding while ensuring that there is no qualitydegradation in the decoded output. We experimentally demonstrate that forstate-of-the-art large language models, the proposed approach achieves a wallclock speedup of 2.13X, a further 1.37X speedup over speculative decoding onstandard benchmarks.</description><author>Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu</author><pubDate>Mon, 23 Oct 2023 18:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15141v1</guid></item><item><title>AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models</title><link>http://arxiv.org/abs/2310.15140v1</link><description>Safety alignment of Large Language Models (LLMs) can be compromised withmanual jailbreak attacks and (automatic) adversarial attacks. Recent worksuggests that patching LLMs against these attacks is possible: manual jailbreakattacks are human-readable but often limited and public, making them easy toblock; adversarial attacks generate gibberish prompts that can be detectedusing perplexity-based filters. In this paper, we show that these solutions maybe too optimistic. We propose an interpretable adversarial attack,\texttt{AutoDAN}, that combines the strengths of both types of attacks. Itautomatically generates attack prompts that bypass perplexity-based filterswhile maintaining a high attack success rate like manual jailbreak attacks.These prompts are interpretable and diverse, exhibiting strategies commonlyused in manual jailbreak attacks, and transfer better than their non-readablecounterparts when using limited training data or a single proxy model. We alsocustomize \texttt{AutoDAN}'s objective to leak system prompts, anotherjailbreak application not addressed in the adversarial attack literature. %,demonstrating the versatility of the approach. We can also customize theobjective of \texttt{AutoDAN} to leak system prompts, beyond the ability toelicit harmful content from the model, demonstrating the versatility of theapproach. Our work provides a new way to red-team LLMs and to understand themechanism of jailbreak attacks.</description><author>Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, Tong Sun</author><pubDate>Mon, 23 Oct 2023 18:46:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15140v1</guid></item><item><title>Fusion-Driven Tree Reconstruction and Fruit Localization: Advancing Precision in Agriculture</title><link>http://arxiv.org/abs/2310.15138v1</link><description>Fruit distribution is pivotal in shaping the future of both agriculture andagricultural robotics, paving the way for a streamlined supply chain. Thisstudy introduces an innovative methodology that harnesses the synergy of RGBimagery, LiDAR, and IMU data, to achieve intricate tree reconstructions and thepinpoint localization of fruits. Such integration not only offers insights intothe fruit distribution, which enhances the precision of guidance foragricultural robotics and automation systems, but also sets the stage forsimulating synthetic fruit patterns across varied tree architectures. Tovalidate this approach, experiments have been carried out in both a controlledenvironment and an actual peach orchard. The results underscore the robustnessand efficacy of this fusion-driven methodology, highlighting its potential as atransformative tool for future agricultural robotics and precision farming.</description><author>Kaiming Fu, Peng Wei, Juan Villacres, Zhaodan Kong, Stavros G. Vougioukas, Brian N. Bailey</author><pubDate>Mon, 23 Oct 2023 18:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15138v1</guid></item><item><title>Diverse Offline Imitation Learning</title><link>http://arxiv.org/abs/2307.11373v2</link><description>There has been significant recent progress in the area of unsupervised skilldiscovery, utilizing various information-theoretic objectives as measures ofdiversity. Despite these advances, challenges remain: current methods requiresignificant online interaction, fail to leverage vast amounts of availabletask-agnostic data and typically lack a quantitative measure of skill utility.We address these challenges by proposing a principled offline algorithm forunsupervised skill discovery that, in addition to maximizing diversity, ensuresthat each learned skill imitates state-only expert demonstrations to a certaindegree. Our main analytical contribution is to connect Fenchel duality,reinforcement learning, and unsupervised skill discovery to maximize a mutualinformation objective subject to KL-divergence state occupancy constraints.Furthermore, we demonstrate the effectiveness of our method on the standardoffline benchmark D4RL and on a custom offline dataset collected from a 12-DoFquadruped robot for which the policies trained in simulation transfer well tothe real robotic system.</description><author>Marin Vlastelica, Jin Cheng, Georg Martius, Pavel Kolev</author><pubDate>Mon, 23 Oct 2023 18:44:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11373v2</guid></item><item><title>Quantifying the Dialect Gap and its Correlates Across Languages</title><link>http://arxiv.org/abs/2310.15135v1</link><description>Historically, researchers and consumers have noticed a decrease in qualitywhen applying NLP tools to minority variants of languages (i.e. Puerto RicanSpanish or Swiss German), but studies exploring this have been limited to aselect few languages. Additionally, past studies have mainly been conducted ina monolingual context, so cross-linguistic trends have not been identified andtied to external factors. In this work, we conduct a comprehensive evaluationof the most influential, state-of-the-art large language models (LLMs) acrosstwo high-use applications, machine translation and automatic speechrecognition, to assess their functionality on the regional dialects of severalhigh- and low-resource languages. Additionally, we analyze how the regionaldialect gap is correlated with economic, social, and linguistic factors. Theimpact of training data, including related factors like dataset size and itsconstruction procedure, is shown to be significant but not consistent acrossmodels or languages, meaning a one-size-fits-all approach cannot be taken insolving the dialect gap. This work will lay the foundation for furthering thefield of dialectal NLP by laying out evident disparities and identifyingpossible pathways for addressing them through mindful data collection.</description><author>Anjali Kantharuban, Ivan Vulić, Anna Korhonen</author><pubDate>Mon, 23 Oct 2023 18:42:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15135v1</guid></item><item><title>Self-Supervised One-Shot Learning for Automatic Segmentation of StyleGAN Images</title><link>http://arxiv.org/abs/2303.05639v3</link><description>We propose a framework for the automatic one-shot segmentation of syntheticimages generated by a StyleGAN. Our framework is based on the observation thatthe multi-scale hidden features in the GAN generator hold useful semanticinformation that can be utilized for automatic on-the-fly segmentation of thegenerated images. Using these features, our framework learns to segmentsynthetic images using a self-supervised contrastive clustering algorithm thatprojects the hidden features into a compact space for per-pixel classification.This contrastive learner is based on using a novel data augmentation strategyand a pixel-wise swapped prediction loss that leads to faster learning of thefeature vectors for one-shot segmentation. We have tested our implementation onfive standard benchmarks to yield a segmentation performance that not onlyoutperforms the semi-supervised baselines by an average wIoU margin of 1.02 %but also improves the inference speeds by a factor of 4.5. Finally, we alsoshow the results of using the proposed one-shot learner in implementing BagGAN,a framework for producing annotated synthetic baggage X-ray scans for threatdetection. This framework was trained and tested on the PIDRay baggagebenchmark to yield a performance comparable to its baseline segmenter based onmanual annotations.</description><author>Ankit Manerikar, Avinash C. Kak</author><pubDate>Mon, 23 Oct 2023 18:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05639v3</guid></item><item><title>A Zero-Shot Language Agent for Computer Control with Structured Reflection</title><link>http://arxiv.org/abs/2310.08740v3</link><description>Large language models (LLMs) have shown increasing capacity at planning andexecuting a high-level goal in a live computer environment (e.g. MiniWoB++). Toperform a task, recent works often require a model to learn from trace examplesof the task via either supervised learning or few/many-shot prompting. Withoutthese trace examples, it remains a challenge how an agent can autonomouslylearn and improve its control on a computer, which limits the ability of anagent to perform a new task. We approach this problem with a zero-shot agentthat requires no given expert traces. Our agent plans for executable actions ona partially observed environment, and iteratively progresses a task byidentifying and learning from its mistakes via self-reflection and structuredthought management. On the easy tasks of MiniWoB++, we show that our zero-shotagent often outperforms recent SoTAs, with more efficient reasoning. For taskswith more complexity, our reflective agent performs on par with prior bestmodels, even though previous works had the advantages of accessing experttraces or additional screen information.</description><author>Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, Yang Li</author><pubDate>Mon, 23 Oct 2023 18:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08740v3</guid></item><item><title>Novel-View Acoustic Synthesis from 3D Reconstructed Rooms</title><link>http://arxiv.org/abs/2310.15130v1</link><description>We investigate the benefit of combining blind audio recordings with 3D sceneinformation for novel-view acoustic synthesis. Given audio recordings from 2-4microphones and the 3D geometry and material of a scene containing multipleunknown sound sources, we estimate the sound anywhere in the scene. We identifythe main challenges of novel-view acoustic synthesis as sound sourcelocalization, separation, and dereverberation. While naively training anend-to-end network fails to produce high-quality results, we show thatincorporating room impulse responses (RIRs) derived from 3D reconstructed roomsenables the same network to jointly tackle these tasks. Our method outperformsexisting methods designed for the individual tasks, demonstrating itseffectiveness at utilizing 3D visual information. In a simulated study on theMatterport3D-NVAS dataset, our model achieves near-perfect accuracy on sourcelocalization, a PSNR of 26.44 dB and a SDR of 14.23 dB for source separationand dereverberation, resulting in a PSNR of 25.55 dB and a SDR of 14.20 dB onnovel-view acoustic synthesis. Code, pretrained model, and video results areavailable on the project webpage (https://github.com/apple/ml-nvas3d).</description><author>Byeongjoo Ahn, Karren Yang, Brian Hamilton, Jonathan Sheaffer, Anurag Ranjan, Miguel Sarabia, Oncel Tuzel, Jen-Hao Rick Chang</author><pubDate>Mon, 23 Oct 2023 18:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15130v1</guid></item><item><title>Location-Aware Visual Question Generation with Lightweight Models</title><link>http://arxiv.org/abs/2310.15129v1</link><description>This work introduces a novel task, location-aware visual question generation(LocaVQG), which aims to generate engaging questions from data relevant to aparticular geographical location. Specifically, we represent suchlocation-aware information with surrounding images and a GPS coordinate. Totackle this task, we present a dataset generation pipeline that leverages GPT-4to produce diverse and sophisticated questions. Then, we aim to learn alightweight model that can address the LocaVQG task and fit on an edge device,such as a mobile phone. To this end, we propose a method which can reliablygenerate engaging questions from location-aware information. Our proposedmethod outperforms baselines regarding human evaluation (e.g., engagement,grounding, coherence) and automatic evaluation metrics (e.g., BERTScore,ROUGE-2). Moreover, we conduct extensive ablation studies to justify ourproposed techniques for both generating the dataset and solving the task.</description><author>Nicholas Collin Suwono, Justin Chih-Yao Chen, Tun Min Hung, Ting-Hao Kenneth Huang, I-Bin Liao, Yung-Hui Li, Lun-Wei Ku, Shao-Hua Sun</author><pubDate>Mon, 23 Oct 2023 18:33:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15129v1</guid></item><item><title>Projected Stochastic Gradient Descent with Quantum Annealed Binary Gradients</title><link>http://arxiv.org/abs/2310.15128v1</link><description>We present, QP-SBGD, a novel layer-wise stochastic optimiser tailored towardstraining neural networks with binary weights, known as binary neural networks(BNNs), on quantum hardware. BNNs reduce the computational requirements andenergy consumption of deep learning models with minimal loss in accuracy.However, training them in practice remains to be an open challenge. Most knownBNN-optimisers either rely on projected updates or binarise weightspost-training. Instead, QP-SBGD approximately maps the gradient onto binaryvariables, by solving a quadratic constrained binary optimisation. Underpractically reasonable assumptions, we show that this update rule convergeswith a rate of $\mathcal{O}(1 / \sqrt{T})$. Moreover, we show how the$\mathcal{NP}$-hard projection can be effectively executed on an adiabaticquantum annealer, harnessing recent advancements in quantum computation. Wealso introduce a projected version of this update rule and prove that if afixed point exists in the binary variable space, the modified updates willconverge to it. Last but not least, our algorithm is implemented layer-wise,making it suitable to train larger networks on resource-limited quantumhardware. Through extensive evaluations, we show that QP-SBGD outperforms or ison par with competitive and well-established baselines such as BinaryConnect,signSGD and ProxQuant when optimising the Rosenbrock function, training BNNs aswell as binary graph neural networks.</description><author>Maximilian Krahn, Michelle Sasdelli, Fengyi Yang, Vladislav Golyanik, Juho Kannala, Tat-Jun Chin, Tolga Birdal</author><pubDate>Mon, 23 Oct 2023 18:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15128v1</guid></item><item><title>Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models</title><link>http://arxiv.org/abs/2310.15127v1</link><description>Pre-trained and frozen LLMs can effectively map simple scene re-arrangementinstructions to programs over a robot's visuomotor functions throughappropriate few-shot example prompting. To parse open-domain natural languageand adapt to a user's idiosyncratic procedures, not known during promptengineering time, fixed prompts fall short. In this paper, we introduce HELPER,an embodied agent equipped with an external memory of language-program pairsthat parses free-form human-robot dialogue into action programs throughretrieval-augmented LLM prompting: relevant memories are retrieved based on thecurrent dialogue, instruction, correction or VLM description, and used asin-context prompt examples for LLM querying. The memory is expanded duringdeployment to include pairs of user's language and action plans, to assistfuture inferences and personalize them to the user's language and routines.HELPER sets a new state-of-the-art in the TEACh benchmark in both Executionfrom Dialog History (EDH) and Trajectory from Dialogue (TfD), with 1.7ximprovement over the previous SOTA for TfD. Our models, code and video resultscan be found in our project's website: https://helper-agent-llm.github.io.</description><author>Gabriel Sarch, Yue Wu, Michael J. Tarr, Katerina Fragkiadaki</author><pubDate>Mon, 23 Oct 2023 18:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15127v1</guid></item><item><title>Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT</title><link>http://arxiv.org/abs/2310.10903v2</link><description>The rapid proliferation of ChatGPT has incited debates regarding its impacton human writing. Amid concerns about declining writing standards, this studyinvestigates the role of ChatGPT in facilitating academic writing, especiallyamong language learners. Using a case study approach, this study examines theexperiences of Kailing, a doctoral student, who integrates ChatGPT throughouttheir academic writing process. The study employs activity theory as a lens forunderstanding writing with generative AI tools and data analyzed includessemi-structured interviews, writing samples, and GPT logs. Results indicatethat Kailing effectively collaborates with ChatGPT across various writingstages while preserving her distinct authorial voice and agency. Thisunderscores the potential of AI tools such as ChatGPT to enhance academicwriting for language learners without overshadowing individual authenticity.This case study offers a critical exploration of how ChatGPT is utilized in theacademic writing process and the preservation of a student's authentic voicewhen engaging with the tool.</description><author>Sharin Jacob, Tamara Tate, Mark Warschauer</author><pubDate>Mon, 23 Oct 2023 18:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10903v2</guid></item><item><title>Mixed-Variable Global Sensitivity Analysis For Knowledge Discovery And Efficient Combinatorial Materials Design</title><link>http://arxiv.org/abs/2310.15124v1</link><description>Global Sensitivity Analysis (GSA) is the study of the influence of any giveninputs on the outputs of a model. In the context of engineering design, GSA hasbeen widely used to understand both individual and collective contributions ofdesign variables on the design objectives. So far, global sensitivity studieshave often been limited to design spaces with only quantitative (numerical)design variables. However, many engineering systems also contain, if not only,qualitative (categorical) design variables in addition to quantitative designvariables. In this paper, we integrate Latent Variable Gaussian Process (LVGP)with Sobol' analysis to develop the first metamodel-based mixed-variable GSAmethod. Through numerical case studies, we validate and demonstrate theeffectiveness of our proposed method for mixed-variable problems. Furthermore,while the proposed GSA method is general enough to benefit various engineeringdesign applications, we integrate it with multi-objective Bayesian optimization(BO) to create a sensitivity-aware design framework in accelerating the Paretofront design exploration for metal-organic framework (MOF) materials withmany-level combinatorial design spaces. Although MOFs are constructed only fromqualitative variables that are notoriously difficult to design, our method canutilize sensitivity analysis to navigate the optimization in the many-levellarge combinatorial design space, greatly expediting the exploration of novelMOF candidates.</description><author>Yigitcan Comlek, Liwei Wang, Wei Chen</author><pubDate>Mon, 23 Oct 2023 18:29:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15124v1</guid></item><item><title>Branch-Solve-Merge Improves Large Language Model Evaluation and Generation</title><link>http://arxiv.org/abs/2310.15123v1</link><description>Large Language Models (LLMs) are frequently used for multi-faceted languagegeneration and evaluation tasks that involve satisfying intricate userconstraints or taking into account multiple aspects and criteria. However,their performance can fall short, due to the model's lack of coherence andinability to plan and decompose the problem. We propose Branch-Solve-Merge(BSM), a Large Language Model program (Schlag et al., 2023) for tackling suchchallenging natural language tasks. It consists of branch, solve, and mergemodules that are parameterized with specific prompts to the base LLM. Thesethree modules plan a decomposition of the task into multiple parallelsub-tasks, independently solve them, and fuse the solutions to the sub-tasks.We apply our method to the tasks of LLM response evaluation and constrainedtext generation and evaluate its effectiveness with multiple LLMs, includingVicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness andconsistency for each LLM by enhancing human-LLM agreement by up to 26%,reducing length and pairwise position biases by up to 50%, and allowingLLaMA-2-chat to match or outperform GPT-4 on most domains. On the constraintstory generation task, BSM improves the coherence of the stories while alsoimproving constraint satisfaction by 12%.</description><author>Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li</author><pubDate>Mon, 23 Oct 2023 18:29:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15123v1</guid></item><item><title>Zero-shot Query Reformulation for Conversational Search</title><link>http://arxiv.org/abs/2307.09384v2</link><description>As the popularity of voice assistants continues to surge, conversationalsearch has gained increased attention in Information Retrieval. However, datasparsity issues in conversational search significantly hinder the progress ofsupervised conversational search methods. Consequently, researchers arefocusing more on zero-shot conversational search approaches. Nevertheless,existing zero-shot methods face three primary limitations: they are notuniversally applicable to all retrievers, their effectiveness lacks sufficientexplainability, and they struggle to resolve common conversational ambiguitiescaused by omission. To address these limitations, we introduce a novelZero-shot Query Reformulation (ZeQR) framework that reformulates queries basedon previous dialogue contexts without requiring supervision from conversationalsearch data. Specifically, our framework utilizes language models designed formachine reading comprehension tasks to explicitly resolve two commonambiguities: coreference and omission, in raw queries. In comparison toexisting zero-shot methods, our approach is universally applicable to anyretriever without additional adaptation or indexing. It also provides greaterexplainability and effectively enhances query intent understanding becauseambiguities are explicitly and proactively resolved. Through extensiveexperiments on four TREC conversational datasets, we demonstrate theeffectiveness of our method, which consistently outperforms state-of-the-artbaselines.</description><author>Dayu Yang, Yue Zhang, Hui Fang</author><pubDate>Mon, 23 Oct 2023 18:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09384v2</guid></item><item><title>Causal Inference Using LLM-Guided Discovery</title><link>http://arxiv.org/abs/2310.15117v1</link><description>At the core of causal inference lies the challenge of determining reliablecausal graphs solely based on observational data. Since the well-known backdoorcriterion depends on the graph, any errors in the graph can propagatedownstream to effect inference. In this work, we initially show that completegraph information is not necessary for causal effect inference; the topologicalorder over graph variables (causal order) alone suffices. Further, given a nodepair, causal order is easier to elicit from domain experts compared to graphedges since determining the existence of an edge can depend extensively onother variables. Interestingly, we find that the same principle holds for LargeLanguage Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automatedmethod to obtain causal order (and hence causal effect) with LLMs acting asvirtual domain experts. To this end, we employ different prompting strategiesand contextual cues to propose a robust technique of obtaining causal orderfrom LLMs. Acknowledging LLMs' limitations, we also study possible techniquesto integrate LLMs with established causal discovery algorithms, includingconstraint-based and score-based methods, to enhance their performance.Extensive experiments demonstrate that our approach significantly improvescausal ordering accuracy as compared to discovery algorithms, highlighting thepotential of LLMs to enhance causal inference across diverse fields.</description><author>Aniket Vashishtha, Abbavaram Gowtham Reddy, Abhinav Kumar, Saketh Bachu, Vineeth N Balasubramanian, Amit Sharma</author><pubDate>Mon, 23 Oct 2023 18:23:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15117v1</guid></item><item><title>SpVOS: Efficient Video Object Segmentation with Triple Sparse Convolution</title><link>http://arxiv.org/abs/2310.15115v1</link><description>Semi-supervised video object segmentation (Semi-VOS), which requires onlyannotating the first frame of a video to segment future frames, has receivedincreased attention recently. Among existing pipelines, thememory-matching-based one is becoming the main research stream, as it can fullyutilize the temporal sequence information to obtain high-quality segmentationresults. Even though this type of method has achieved promising performance,the overall framework still suffers from heavy computation overhead, mainlycaused by the per-frame dense convolution operations between high-resolutionfeature maps and each kernel filter. Therefore, we propose a sparse baseline ofVOS named SpVOS in this work, which develops a novel triple sparse convolutionto reduce the computation costs of the overall VOS framework. The designedtriple gate, taking full consideration of both spatial and temporal redundancybetween adjacent video frames, adaptively makes a triple decision to decide howto apply the sparse convolution on each pixel to control the computationoverhead of each layer, while maintaining sufficient discrimination capabilityto distinguish similar objects and avoid error accumulation. A mixed sparsetraining strategy, coupled with a designed objective considering the sparsityconstraint, is also developed to balance the VOS segmentation performance andcomputation costs. Experiments are conducted on two mainstream VOS datasets,including DAVIS and Youtube-VOS. Results show that, the proposed SpVOS achievessuperior performance over other state-of-the-art sparse methods, and evenmaintains comparable performance, e.g., an 83.04% (79.29%) overall score on theDAVIS-2017 (Youtube-VOS) validation set, with the typical non-sparse VOSbaseline (82.88% for DAVIS-2017 and 80.36% for Youtube-VOS) while saving up to42% FLOPs, showing its application potential for resource-constrainedscenarios.</description><author>Weihao Lin, Tao Chen, Chong Yu</author><pubDate>Mon, 23 Oct 2023 18:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15115v1</guid></item><item><title>How To Build Competitive Multi-gender Speech Translation Models For Controlling Speaker Gender Translation</title><link>http://arxiv.org/abs/2310.15114v1</link><description>When translating from notional gender languages (e.g., English) intogrammatical gender languages (e.g., Italian), the generated translationrequires explicit gender assignments for various words, including thosereferring to the speaker. When the source sentence does not convey thespeaker's gender, speech translation (ST) models either rely on thepossibly-misleading vocal traits of the speaker or default to the masculinegender, the most frequent in existing training corpora. To avoid such biasedand not inclusive behaviors, the gender assignment of speaker-relatedexpressions should be guided by externally-provided metadata about thespeaker's gender. While previous work has shown that the most effectivesolution is represented by separate, dedicated gender-specific models, the goalof this paper is to achieve the same results by integrating the speaker'sgender metadata into a single "multi-gender" neural ST model, easier tomaintain. Our experiments demonstrate that a single multi-gender modeloutperforms gender-specialized ones when trained from scratch (with genderaccuracy gains up to 12.9 for feminine forms), while fine-tuning from existingST models does not lead to competitive results.</description><author>Marco Gaido, Dennis Fucci, Matteo Negri, Luisa Bentivogli</author><pubDate>Mon, 23 Oct 2023 18:21:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15114v1</guid></item><item><title>Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model</title><link>http://arxiv.org/abs/2310.15113v1</link><description>Large language models (LLMs) have recently reached an impressive level oflinguistic capability, prompting comparisons with human language skills.However, there have been relatively few systematic inquiries into thelinguistic capabilities of the latest generation of LLMs, and those studiesthat do exist (i) ignore the remarkable ability of humans to generalize, (ii)focus only on English, and (iii) investigate syntax or semantics and overlookother capabilities that lie at the heart of human language, like morphology.Here, we close these gaps by conducting the first rigorous analysis of themorphological capabilities of ChatGPT in four typologically varied languages(specifically, English, German, Tamil, and Turkish). We apply a version ofBerko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets forthe four examined languages. We find that ChatGPT massively underperformspurpose-built systems, particularly in English. Overall, our results -- throughthe lens of morphology -- cast a new light on the linguistic capabilities ofChatGPT, suggesting that claims of human-like language skills are premature andmisleading.</description><author>Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Schütze, Kemal Oflazer, David R. Mortensen</author><pubDate>Mon, 23 Oct 2023 18:21:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15113v1</guid></item><item><title>The Self 2.0: How AI-Enhanced Self-Clones Transform Self-Perception and Improve Presentation Skills</title><link>http://arxiv.org/abs/2310.15112v1</link><description>This study explores the impact of AI-generated digital self-clones onimproving online presentation skills. We carried out a mixed-design experimentinvolving 44 international students, comparing self-recorded videos (control)with self-clone videos (AI group) for English presentation practice. The AIvideos utilized voice cloning, face swapping, lip-sync, and body-languagesimulation to refine participants' original presentations in terms ofrepetition, filler words, and pronunciation. Machine-rated scores indicatedenhancements in speech performance for both groups. Though the groups didn'tsignificantly differ, the AI group exhibited a heightened depth of reflection,self-compassion, and a meaningful transition from a corrective to an enhanciveapproach to self-critique. Within the AI group, congruence betweenself-perception and AI self-clones resulted in diminished speech anxiety andincreased enjoyment. Our findings recommend the ethical employment of digitalself-clones to enhance the emotional and cognitive facets of skill development.</description><author>Qingxiao Zheng, Yun Huang</author><pubDate>Mon, 23 Oct 2023 18:20:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15112v1</guid></item><item><title>Matryoshka Diffusion Models</title><link>http://arxiv.org/abs/2310.15111v1</link><description>Diffusion models are the de facto approach for generating high-quality imagesand videos, but learning high-dimensional models remains a formidable task dueto computational and optimization challenges. Existing methods often resort totraining cascaded models in pixel space or using a downsampled latent space ofa separately trained auto-encoder. In this paper, we introduce MatryoshkaDiffusion Models(MDM), an end-to-end framework for high-resolution image andvideo synthesis. We propose a diffusion process that denoises inputs atmultiple resolutions jointly and uses a NestedUNet architecture where featuresand parameters for small-scale inputs are nested within those of large scales.In addition, MDM enables a progressive training schedule from lower to higherresolutions, which leads to significant improvements in optimization forhigh-resolution generation. We demonstrate the effectiveness of our approach onvarious benchmarks, including class-conditioned image generation,high-resolution text-to-image, and text-to-video applications. Remarkably, wecan train a single pixel-space model at resolutions of up to 1024x1024 pixels,demonstrating strong zero-shot generalization using the CC12M dataset, whichcontains only 12 million images.</description><author>Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind, Navdeep Jaitly</author><pubDate>Mon, 23 Oct 2023 18:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15111v1</guid></item><item><title>Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model</title><link>http://arxiv.org/abs/2310.15110v1</link><description>We report Zero123++, an image-conditioned diffusion model for generating3D-consistent multi-view images from a single input view. To take fulladvantage of pretrained 2D generative priors, we develop various conditioningand training schemes to minimize the effort of finetuning from off-the-shelfimage diffusion models such as Stable Diffusion. Zero123++ excels in producinghigh-quality, consistent multi-view images from a single image, overcomingcommon issues like texture degradation and geometric misalignment. Furthermore,we showcase the feasibility of training a ControlNet on Zero123++ for enhancedcontrol over the generation process. The code is available athttps://github.com/SUDO-AI-3D/zero123plus.</description><author>Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, Hao Su</author><pubDate>Mon, 23 Oct 2023 18:18:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15110v1</guid></item><item><title>Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems</title><link>http://arxiv.org/abs/2310.05280v4</link><description>Recent advancements in Large Language Models empower them to follow freeforminstructions, including imitating generic or specific demographic personas inconversations. We define generic personas to represent demographic groups, suchas "an Asian person", whereas specific personas may take the form of specificpopular Asian names like "Yumi". While the adoption of personas enriches userexperiences by making dialogue systems more engaging and approachable, it alsocasts a shadow of potential risk by exacerbating social biases within modelresponses, thereby causing societal harm through interactions with users. Inthis paper, we systematically study "persona biases", which we define to be thesensitivity of dialogue models' harmful behaviors contingent upon the personasthey adopt. We categorize persona biases into biases in harmful expression andharmful agreement, and establish a comprehensive evaluation framework tomeasure persona biases in five aspects: Offensiveness, Toxic Continuation,Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose toinvestigate persona biases by experimenting with UNIVERSALPERSONA, asystematically constructed persona dataset encompassing various types of bothgeneric and specific model personas. Through benchmarking on four differentmodels -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncoverssignificant persona biases in dialogue systems. Our findings also underscorethe pressing need to revisit the use of personas in dialogue agents to ensuresafe application.</description><author>Yixin Wan, Jieyu Zhao, Aman Chadha, Nanyun Peng, Kai-Wei Chang</author><pubDate>Mon, 23 Oct 2023 18:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05280v4</guid></item><item><title>GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs</title><link>http://arxiv.org/abs/2310.15109v1</link><description>Self-supervised representation learning on text-attributed graphs, which aimsto create expressive and generalizable representations for various downstreamtasks, has received increasing research attention lately. However, existingmethods either struggle to capture the full extent of structural contextinformation or rely on task-specific training labels, which largely hamperstheir effectiveness and generalizability in practice. To solve the problem ofself-supervised representation learning on text-attributed graphs, we develop anovel Graph-Centric Language model -- GRENADE. Specifically, GRENADE exploitsthe synergistic effect of both pre-trained language model and graph neuralnetwork by optimizing with two specialized self-supervised learning algorithms:graph-centric contrastive learning and graph-centric knowledge alignment. Theproposed graph-centric self-supervised learning algorithms effectively helpGRENADE to capture informative textual semantics as well as structural contextinformation on text-attributed graphs. Through extensive experiments, GRENADEshows its superiority over state-of-the-art methods. Implementation isavailable at \url{https://github.com/bigheiniu/GRENADE}.</description><author>Yichuan Li, Kaize Ding, Kyumin Lee</author><pubDate>Mon, 23 Oct 2023 18:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15109v1</guid></item><item><title>Simplifying Momentum-based Positive-definite Submanifold Optimization with Applications to Deep Learning</title><link>http://arxiv.org/abs/2302.09738v8</link><description>Riemannian submanifold optimization with momentum is computationallychallenging because, to ensure that the iterates remain on the submanifold, weoften need to solve difficult differential equations. Here, we simplify suchdifficulties for a class of sparse or structured symmetric positive-definitematrices with the affine-invariant metric. We do so by proposing a generalizedversion of the Riemannian normal coordinates that dynamically orthonormalizesthe metric and locally converts the problem into an unconstrained problem inthe Euclidean space. We use our approach to simplify existing approaches forstructured covariances and develop matrix-inverse-free $2^\text{nd}$-orderoptimizers for deep learning with low precision by using only matrixmultiplications. Code: https://github.com/yorkerlin/StructuredNGD-DL</description><author>Wu Lin, Valentin Duruisseaux, Melvin Leok, Frank Nielsen, Mohammad Emtiyaz Khan, Mark Schmidt</author><pubDate>Mon, 23 Oct 2023 18:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09738v8</guid></item><item><title>Improving Dialogue Management: Quality Datasets vs Models</title><link>http://arxiv.org/abs/2310.01339v2</link><description>Task-oriented dialogue systems (TODS) have become crucial for users tointeract with machines and computers using natural language. One of its keycomponents is the dialogue manager, which guides the conversation towards agood goal for the user by providing the best possible response. Previous workshave proposed rule-based systems (RBS), reinforcement learning (RL), andsupervised learning (SL) as solutions for the correct dialogue management; inother words, select the best response given input by the user. However, thiswork argues that the leading cause of DMs not achieving maximum performanceresides in the quality of the datasets rather than the models employed thusfar; this means that dataset errors, like mislabeling, originate a largepercentage of failures in dialogue management. We studied the main errors inthe most widely used datasets, Multiwoz 2.1 and SGD, to demonstrate thishypothesis. To do this, we have designed a synthetic dialogue generator tofully control the amount and type of errors introduced in the dataset. Usingthis generator, we demonstrated that errors in the datasets contributeproportionally to the performance of the models</description><author>Miguel Ángel Medina-Ramírez, Cayetano Guerra-Artal, Mario Hernández-Tejera</author><pubDate>Mon, 23 Oct 2023 18:15:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01339v2</guid></item><item><title>Evaluating machine learning models in non-standard settings: An overview and new findings</title><link>http://arxiv.org/abs/2310.15108v1</link><description>Estimating the generalization error (GE) of machine learning models isfundamental, with resampling methods being the most common approach. However,in non-standard settings, particularly those where observations are notindependently and identically distributed, resampling using simple random datadivisions may lead to biased GE estimates. This paper strives to presentwell-grounded guidelines for GE estimation in various such non-standardsettings: clustered data, spatial data, unequal sampling probabilities, conceptdrift, and hierarchically structured outcomes. Our overview combineswell-established methodologies with other existing methods that, to ourknowledge, have not been frequently considered in these particular settings. Aunifying principle among these techniques is that the test data used in eachiteration of the resampling procedure should reflect the new observations towhich the model will be applied, while the training data should berepresentative of the entire data set used to obtain the final model. Beyondproviding an overview, we address literature gaps by conducting simulationstudies. These studies assess the necessity of using GE-estimation methodstailored to the respective setting. Our findings corroborate the concern thatstandard resampling methods often yield biased GE estimates in non-standardsettings, underscoring the importance of tailored GE estimation.</description><author>Roman Hornung, Malte Nalenz, Lennart Schneider, Andreas Bender, Ludwig Bothmann, Bernd Bischl, Thomas Augustin, Anne-Laure Boulesteix</author><pubDate>Mon, 23 Oct 2023 18:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15108v1</guid></item><item><title>Improving day-ahead Solar Irradiance Time Series Forecasting by Leveraging Spatio-Temporal Context</title><link>http://arxiv.org/abs/2306.01112v2</link><description>Solar power harbors immense potential in mitigating climate change bysubstantially reducing CO$_{2}$ emissions. Nonetheless, the inherentvariability of solar irradiance poses a significant challenge for seamlesslyintegrating solar power into the electrical grid. While the majority of priorresearch has centered on employing purely time series-based methodologies forsolar forecasting, only a limited number of studies have taken into accountfactors such as cloud cover or the surrounding physical context. In this paper,we put forth a deep learning architecture designed to harness spatio-temporalcontext using satellite data, to attain highly accurate \textit{day-ahead}time-series forecasting for any given station, with a particular emphasis onforecasting Global Horizontal Irradiance (GHI). We also suggest a methodologyto extract a distribution for each time step prediction, which can serve as avery valuable measure of uncertainty attached to the forecast. When evaluatingmodels, we propose a testing scheme in which we separate particularly difficultexamples from easy ones, in order to capture the model performances in crucialsituations, which in the case of this study are the days suffering from varyingcloudy conditions. Furthermore, we present a new multi-modal dataset gatheringsatellite imagery over a large zone and time series for solar irradiance andother related physical variables from multiple geographically diverse solarstations. Our approach exhibits robust performance in solar irradianceforecasting, including zero-shot generalization tests at unobserved solarstations, and holds great promise in promoting the effective integration ofsolar power into the grid.</description><author>Oussama Boussif, Ghait Boukachab, Dan Assouline, Stefano Massaroli, Tianle Yuan, Loubna Benabbou, Yoshua Bengio</author><pubDate>Mon, 23 Oct 2023 18:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01112v2</guid></item><item><title>FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning</title><link>http://arxiv.org/abs/2310.15105v1</link><description>Due to the limited availability of data, existing few-shot learning methodstrained from scratch fail to achieve satisfactory performance. In contrast,large-scale pre-trained models such as CLIP demonstrate remarkable few-shot andzero-shot capabilities. To enhance the performance of pre-trained models fordownstream tasks, fine-tuning the model on downstream data is frequentlynecessary. However, fine-tuning the pre-trained model leads to a decrease inits generalizability in the presence of distribution shift, while the limitednumber of samples in few-shot learning makes the model highly susceptible tooverfitting. Consequently, existing methods for fine-tuning few-shot learningprimarily focus on fine-tuning the model's classification head or introducingadditional structure. In this paper, we introduce a fine-tuning approach termedFeature Discrimination Alignment (FD-Align). Our method aims to bolster themodel's generalizability by preserving the consistency of spurious featuresacross the fine-tuning process. Extensive experimental results validate theefficacy of our approach for both ID and OOD tasks. Once fine-tuned, the modelcan seamlessly integrate with existing methods, leading to performanceimprovements. Our code can be found in https://github.com/skingorz/FD-Align.</description><author>Kun Song, Huimin Ma, Bochao Zou, HuiShuai Zhang, Weiran Huang</author><pubDate>Mon, 23 Oct 2023 18:12:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15105v1</guid></item><item><title>Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting</title><link>http://arxiv.org/abs/2305.14755v2</link><description>Most existing stylistic text rewriting methods and evaluation metrics operateon a sentence level, but ignoring the broader context of the text can lead topreferring generic, ambiguous, and incoherent rewrites. In this paper, weinvestigate integrating the preceding textual context into both the$\textit{rewriting}$ and $\textit{evaluation}$ stages of stylistic textrewriting, and introduce a new composite contextual evaluation metric$\texttt{CtxSimFit}$ that combines similarity to the original sentence withcontextual cohesiveness. We comparatively evaluate non-contextual andcontextual rewrites in formality, toxicity, and sentiment transfer tasks. Ourexperiments show that humans significantly prefer contextual rewrites as morefitting and natural over non-contextual ones, yet existing sentence-levelautomatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences($\rho$=0--0.3). In contrast, human preferences are much better reflected byboth our novel $\texttt{CtxSimFit}$ ($\rho$=0.7--0.9) as well as proposedcontext-infused versions of common metrics ($\rho$=0.4--0.7). Overall, ourfindings highlight the importance of integrating context into the generationand especially the evaluation stages of stylistic text rewriting.</description><author>Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap</author><pubDate>Mon, 23 Oct 2023 18:11:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14755v2</guid></item><item><title>CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages</title><link>http://arxiv.org/abs/2310.13683v2</link><description>This work introduces CAPIVARA, a cost-efficient framework designed to enhancethe performance of multilingual CLIP models in low-resource languages. WhileCLIP has excelled in zero-shot vision-language tasks, the resource-intensivenature of model training remains challenging. Many datasets lack linguisticdiversity, featuring solely English descriptions for images. CAPIVARA addressesthis by augmenting text data using image captioning and machine translation togenerate multiple synthetic captions in low-resource languages. We optimize thetraining pipeline with LiT, LoRA, and gradient checkpointing to alleviate thecomputational cost. Through extensive experiments, CAPIVARA emerges as state ofthe art in zero-shot tasks involving images and Portuguese texts. We show thepotential for significant improvements in other low-resource languages,achieved by fine-tuning the pre-trained multilingual CLIP using CAPIVARA on asingle GPU for 2 hours. Our model and code is available athttps://github.com/hiaac-nlp/CAPIVARA.</description><author>Gabriel Oliveira dos Santos, Diego A. B. Moreira, Alef Iury Ferreira, Jhessica Silva, Luiz Pereira, Pedro Bueno, Thiago Sousa, Helena Maia, Nádia Da Silva, Esther Colombini, Helio Pedrini, Sandra Avila</author><pubDate>Mon, 23 Oct 2023 18:06:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13683v2</guid></item><item><title>LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis</title><link>http://arxiv.org/abs/2310.15100v1</link><description>Thematic analysis (TA) has been widely used for analyzing qualitative data inmany disciplines and fields. To ensure reliable analysis, the same piece ofdata is typically assigned to at least two human coders. Moreover, to producemeaningful and useful analysis, human coders develop and deepen their datainterpretation and coding over multiple iterations, making TA labor-intensiveand time-consuming. Recently the emerging field of large language models (LLMs)research has shown that LLMs have the potential replicate human-like behaviorin various tasks: in particular, LLMs outperform crowd workers ontext-annotation tasks, suggesting an opportunity to leverage LLMs on TA. Wepropose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conductTA with in-context learning (ICL). This framework provides the prompt to framediscussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA.We demonstrate the utility of this framework using survey datasets on theaspects of the music listening experience and the usage of a password manager.Results of the two case studies show that the proposed framework yields similarcoding quality to that of human coders but reduces TA's labor and time demands.</description><author>Shih-Chieh Dai, Aiping Xiong, Lun-Wei Ku</author><pubDate>Mon, 23 Oct 2023 18:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15100v1</guid></item><item><title>Dual-path convolutional neural network using micro-FTIR imaging to predict breast cancer subtypes and biomarkers levels: estrogen receptor, progesterone receptor, HER2 and Ki67</title><link>http://arxiv.org/abs/2310.15099v1</link><description>Breast cancer molecular subtypes classification plays an import role to sortpatients with divergent prognosis. The biomarkers used are Estrogen Receptor(ER), Progesterone Receptor (PR), HER2, and Ki67. Based on these biomarkersexpression levels, subtypes are classified as Luminal A (LA), Luminal B (LB),HER2 subtype, and Triple-Negative Breast Cancer (TNBC). Immunohistochemistry isused to classify subtypes, although interlaboratory and interobservervariations can affect its accuracy, besides being a time-consuming technique.The Fourier transform infrared micro-spectroscopy may be coupled with deeplearning for cancer evaluation, where there is still a lack of studies forsubtypes and biomarker levels prediction. This study presents a novel 2D deeplearning approach to achieve these predictions. Sixty micro-FTIR images of320x320 pixels were collected from a human breast biopsies microarray. Datawere clustered by K-means, preprocessed and 32x32 patches were generated usinga fully automated approach. CaReNet-V2, a novel convolutional neural network,was developed to classify breast cancer (CA) vs adjacent tissue (AT) andmolecular subtypes, and to predict biomarkers level. The clustering methodenabled to remove non-tissue pixels. Test accuracies for CA vs AT and subtypewere above 0.84. The model enabled the prediction of ER, PR, and HER2 levels,where borderline values showed lower performance (minimum accuracy of 0.54).Ki67 percentage regression demonstrated a mean error of 3.6%. Thus, CaReNet-V2is a potential technique for breast cancer biopsies evaluation, standing out asa screening analysis technique and helping to prioritize patients.</description><author>Matheus del-Valle, Emerson Soares Bernardes, Denise Maria Zezell</author><pubDate>Mon, 23 Oct 2023 18:05:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15099v1</guid></item><item><title>Topics, Authors, and Networks in Large Language Model Research: Trends from a Survey of 17K arXiv Papers</title><link>http://arxiv.org/abs/2307.10700v2</link><description>Large language model (LLM) research is dramatically impacting society, makingit essential to understand the topics and values it prioritizes, the authorsand institutions driving it, and its networks of collaboration. Due to therecent growth of the field, many of these fundamental attributes lacksystematic description. We gather, annotate, and analyze a new dataset of16,979 LLM-related arXiv papers, focusing on changes in 2023 vs. 2018-2022. Weshow that LLM research increasingly focuses on societal impacts: the Computersand Society sub-arXiv has seen 20x growth in its proportion of LLM-relatedpapers in 2023. This change is driven in part by an influx of new authors: amajority of 2023 papers are first-authored by researchers who have notpreviously written an LLM-related paper, and these papers focus particularly onapplications and societal considerations. While a handful of companies holdoutsize influence, academia publishes a much larger fraction of papers thanindustry overall, and this gap widens in 2023. LLM research is also beingshaped by social dynamics: there are gender and academic/industry differencesin the topics authors prioritize, and a stark U.S./China schism in thecollaboration network. Overall, our analysis documents how LLM research bothshapes and is shaped by society, attesting to the necessity of sociotechnicallenses; we discuss implications for researchers and policymakers.</description><author>Rajiv Movva, Sidhika Balachandar, Kenny Peng, Gabriel Agostini, Nikhil Garg, Emma Pierson</author><pubDate>Mon, 23 Oct 2023 18:04:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10700v2</guid></item><item><title>The Geometry of Neural Nets' Parameter Spaces Under Reparametrization</title><link>http://arxiv.org/abs/2302.07384v3</link><description>Model reparametrization, which follows the change-of-variable rule ofcalculus, is a popular way to improve the training of neural nets. But it canalso be problematic since it can induce inconsistencies in, e.g., Hessian-basedflatness measures, optimization trajectories, and modes of probabilitydensities. This complicates downstream analyses: e.g. one cannot definitivelyrelate flatness with generalization since arbitrary reparametrization changestheir relationship. In this work, we study the invariance of neural nets underreparametrization from the perspective of Riemannian geometry. From this pointof view, invariance is an inherent property of any neural net if one explicitlyrepresents the metric and uses the correct associated transformation rules.This is important since although the metric is always present, it is oftenimplicitly assumed as identity, and thus dropped from the notation, then lostunder reparametrization. We discuss implications for measuring the flatness ofminima, optimization, and for probability-density maximization. Finally, weexplore some interesting directions where invariance is useful.</description><author>Agustinus Kristiadi, Felix Dangel, Philipp Hennig</author><pubDate>Mon, 23 Oct 2023 18:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07384v3</guid></item><item><title>Acquiring Weak Annotations for Tumor Localization in Temporal and Volumetric Data</title><link>http://arxiv.org/abs/2310.15098v1</link><description>Creating large-scale and well-annotated datasets to train AI algorithms iscrucial for automated tumor detection and localization. However, with limitedresources, it is challenging to determine the best type of annotations whenannotating massive amounts of unlabeled data. To address this issue, we focuson polyps in colonoscopy videos and pancreatic tumors in abdominal CT scans;both applications require significant effort and time for pixel-wise annotationdue to the high dimensional nature of the data, involving either temporary orspatial dimensions. In this paper, we develop a new annotation strategy, termedDrag&amp;Drop, which simplifies the annotation process to drag and drop. Thisannotation strategy is more efficient, particularly for temporal and volumetricimaging, than other types of weak annotations, such as per-pixel, boundingboxes, scribbles, ellipses, and points. Furthermore, to exploit our Drag&amp;Dropannotations, we develop a novel weakly supervised learning method based on thewatershed algorithm. Experimental results show that our method achieves betterdetection and localization performance than alternative weak annotations and,more importantly, achieves similar performance to that trained on detailedper-pixel annotations. Interestingly, we find that, with limited resources,allocating weak annotations from a diverse patient population can foster modelsmore robust to unseen images than allocating per-pixel annotations for a smallset of images. In summary, this research proposes an efficient annotationstrategy for tumor detection and localization that is less accurate thanper-pixel annotations but useful for creating large-scale datasets forscreening tumors in various medical modalities.</description><author>Yu-Cheng Chou, Bowen Li, Deng-Ping Fan, Alan Yuille, Zongwei Zhou</author><pubDate>Mon, 23 Oct 2023 18:03:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15098v1</guid></item><item><title>A Canonical Data Transformation for Achieving Inter- and Within-group Fairness</title><link>http://arxiv.org/abs/2310.15097v1</link><description>Increases in the deployment of machine learning algorithms for applicationsthat deal with sensitive data have brought attention to the issue of fairnessin machine learning. Many works have been devoted to applications that requiredifferent demographic groups to be treated fairly. However, algorithms that aimto satisfy inter-group fairness (also called group fairness) may inadvertentlytreat individuals within the same demographic group unfairly. To address thisissue, we introduce a formal definition of within-group fairness that maintainsfairness among individuals from within the same group. We propose apre-processing framework to meet both inter- and within-group fairness criteriawith little compromise in accuracy. The framework maps the feature vectors ofmembers from different groups to an inter-group-fair canonical domain beforefeeding them into a scoring function. The mapping is constructed to preservethe relative relationship between the scores obtained from the unprocessedfeature vectors of individuals from the same demographic group, guaranteeingwithin-group fairness. We apply this framework to the COMPAS risk assessmentand Law School datasets and compare its performance in achieving inter-groupand within-group fairness to two regularization-based methods.</description><author>Zachary McBride Lazri, Ivan Brugere, Xin Tian, Dana Dachman-Soled, Antigoni Polychroniadou, Danial Dervovic, Min Wu</author><pubDate>Mon, 23 Oct 2023 18:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15097v1</guid></item><item><title>One-dimensional convolutional neural network model for breast cancer subtypes classification and biochemical content evaluation using micro-FTIR hyperspectral images</title><link>http://arxiv.org/abs/2310.15094v1</link><description>Breast cancer treatment still remains a challenge, where molecular subtypesclassification plays a crucial role in selecting appropriate and specifictherapy. The four subtypes are Luminal A (LA), Luminal B (LB), HER2 subtype,and Triple-Negative Breast Cancer (TNBC). Immunohistochemistry is thegold-standard evaluation, although interobserver variations are reported andmolecular signatures identification is time-consuming. Fourier transforminfrared micro-spectroscopy with machine learning approaches have been used toevaluate cancer samples, presenting biochemical-related explainability.However, this explainability is harder when using deep learning. This studycreated a 1D deep learning tool for breast cancer subtype evaluation andbiochemical contribution. Sixty hyperspectral images were acquired from a humanbreast cancer microarray. K-Means clustering was applied to select tissue andparaffin spectra. CaReNet-V1, a novel 1D convolutional neural network, wasdeveloped to classify breast cancer (CA) and adjacent tissue (AT), andmolecular subtypes. A 1D adaptation of Grad-CAM was applied to assess thebiochemical impact to the classifications. CaReNet-V1 effectively classified CAand AT (test accuracy of 0.89), as well as HER2 and TNBC subtypes (0.83 and0.86), with greater difficulty for LA and LB (0.74 and 0.68). The model enabledthe evaluation of the most contributing wavenumbers to the predictions,providing a direct relationship with the biochemical content. Therefore,CaReNet-V1 and hyperspectral images is a potential approach for breast cancerbiopsies assessment, providing additional information to the pathology report.Biochemical content impact feature may be used for other studies, such astreatment efficacy evaluation and development new diagnostics and therapeuticmethods.</description><author>Matheus del-Valle, Emerson Soares Bernardes, Denise Maria Zezell</author><pubDate>Mon, 23 Oct 2023 17:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15094v1</guid></item><item><title>Evaluating LLMs for Privilege-Escalation Scenarios</title><link>http://arxiv.org/abs/2310.11409v2</link><description>Penetration testing, an essential component of cybersecurity, allowsorganizations to proactively identify and remediate vulnerabilities in theirsystems, thus bolstering their defense mechanisms against potentialcyberattacks. One recent advancement in the realm of penetration testing is theutilization of Language Models (LLMs). We explore the intersection of LLMs andpenetration testing to gain insight into their capabilities and challenges inthe context of privilige escalation. We create an automated Linuxprivilege-escalation benchmark utilizing local virtual machines. We introducean LLM-guided privilege-escalation tool designed for evaluating different LLMsand prompt strategies against our benchmark. We analyze the impact of differentprompt designs, the benefits of in-context learning, and the advantages ofoffering high-level guidance to LLMs. We discuss challenging areas for LLMs,including maintaining focus during testing, coping with errors, and finallycomparing them with both stochastic parrots as well as with human hackers.</description><author>Andreas Happe, Aaron Kaplan, Jürgen Cito</author><pubDate>Mon, 23 Oct 2023 17:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11409v2</guid></item><item><title>On the Detection of Image-Scaling Attacks in Machine Learning</title><link>http://arxiv.org/abs/2310.15085v1</link><description>Image scaling is an integral part of machine learning and computer visionsystems. Unfortunately, this preprocessing step is vulnerable to so-calledimage-scaling attacks where an attacker makes unnoticeable changes to an imageso that it becomes a new image after scaling. This opens up new ways forattackers to control the prediction or to improve poisoning and backdoorattacks. While effective techniques exist to prevent scaling attacks, theirdetection has not been rigorously studied yet. Consequently, it is currentlynot possible to reliably spot these attacks in practice. This paper presents the first in-depth systematization and analysis ofdetection methods for image-scaling attacks. We identify two general detectionparadigms and derive novel methods from them that are simple in design yetsignificantly outperform previous work. We demonstrate the efficacy of thesemethods in a comprehensive evaluation with all major learning platforms andscaling algorithms. First, we show that image-scaling attacks modifying theentire scaled image can be reliably detected even under an adaptive adversary.Second, we find that our methods provide strong detection performance even ifonly minor parts of the image are manipulated. As a result, we can introduce anovel protection layer against image-scaling attacks.</description><author>Erwin Quiring, Andreas Müller, Konrad Rieck</author><pubDate>Mon, 23 Oct 2023 17:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15085v1</guid></item><item><title>Quantum Federated Learning With Quantum Networks</title><link>http://arxiv.org/abs/2310.15084v1</link><description>A major concern of deep learning models is the large amount of data that isrequired to build and train them, much of which is reliant on sensitive andpersonally identifiable information that is vulnerable to access by thirdparties. Ideas of using the quantum internet to address this issue have beenpreviously proposed, which would enable fast and completely secure onlinecommunications. Previous work has yielded a hybrid quantum-classical transferlearning scheme for classical data and communication with a hub-spoke topology.While quantum communication is secure from eavesdrop attacks and nomeasurements from quantum to classical translation, due to no cloning theorem,hub-spoke topology is not ideal for quantum communication without quantummemory. Here we seek to improve this model by implementing a decentralized ringtopology for the federated learning scheme, where each client is given aportion of the entire dataset and only performs training on that set. We alsodemonstrate the first successful use of quantum weights for quantum federatedlearning, which allows us to perform our training entirely in quantum.</description><author>Tyler Wang, Huan-Hsin Tseng, Shinjae Yoo</author><pubDate>Mon, 23 Oct 2023 17:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15084v1</guid></item><item><title>Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture</title><link>http://arxiv.org/abs/2305.12710v2</link><description>Real-world domain experts (e.g., doctors) rarely annotate only a decisionlabel in their day-to-day workflow without providing explanations. Yet,existing low-resource learning techniques, such as Active Learning (AL), thataim to support human annotators mostly focus on the label while neglecting thenatural language explanation of a data point. This work proposes a novel ALarchitecture to support experts' real-world need for label and explanationannotations in low-resource scenarios. Our AL architecture leverages anexplanation-generation model to produce explanations guided by humanexplanations, a prediction model that utilizes generated explanations towardprediction faithfully, and a novel data diversity-based AL sampling strategythat benefits from the explanation annotations. Automated and human evaluationsdemonstrate the effectiveness of incorporating explanations into AL samplingand the improved human annotation efficiency and trustworthiness with our ALarchitecture. Additional ablation studies illustrate the potential of our ALarchitecture for transfer learning, generalizability, and integration withlarge language models (LLMs). While LLMs exhibit exceptionalexplanation-generation capabilities for relatively simple tasks, theireffectiveness in complex real-world tasks warrants further in-depth study.</description><author>Bingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank Srivastava, Yunyao Li, James Hendler, Dakuo Wang</author><pubDate>Mon, 23 Oct 2023 17:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12710v2</guid></item><item><title>E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion</title><link>http://arxiv.org/abs/2310.15081v1</link><description>This paper proposes a novel approach to face swapping from the perspective offine-grained facial editing, dubbed "editing for swapping" (E4S). Thetraditional face swapping methods rely on global feature extraction and oftenfail to preserve the source identity. In contrast, our framework proposes aRegional GAN Inversion (RGI) method, which allows the explicit disentanglementof shape and texture. Specifically, our E4S performs face swapping in thelatent space of a pretrained StyleGAN, where a multi-scale mask-guided encoderis applied to project the texture of each facial component into regional stylecodes and a mask-guided injection module then manipulates feature maps with thestyle codes. Based on this disentanglement, face swapping can be simplified asstyle and mask swapping. Besides, since reconstructing the source face in thetarget image may lead to disharmony lighting, we propose to train a re-coloringnetwork to make the swapped face maintain the lighting condition on the targetface. Further, to deal with the potential mismatch area during mask exchange,we designed a face inpainting network as post-processing. The extensivecomparisons with state-of-the-art methods demonstrate that our E4S outperformsexisting methods in preserving texture, shape, and lighting. Our implementationis available at https://github.com/e4s2023/E4S2023.</description><author>Maomao Li, Ge Yuan, Cairong Wang, Zhian Liu, Yong Zhang, Yongwei Nie, Jue Wang, Dong Xu</author><pubDate>Mon, 23 Oct 2023 17:41:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15081v1</guid></item><item><title>Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization</title><link>http://arxiv.org/abs/2310.15080v1</link><description>Federated learning (FL) is a promising paradigm to enable collaborative modeltraining with decentralized data. However, the training process of LargeLanguage Models (LLMs) generally incurs the update of significant parameters,which limits the applicability of FL techniques to tackle the LLMs in realscenarios. Prompt tuning can significantly reduce the number of parameters toupdate, but it either incurs performance degradation or low trainingefficiency. The straightforward utilization of prompt tuning in the FL oftenraises non-trivial communication costs and dramatically degrades performance.In addition, the decentralized data is generally non-Independent andIdentically Distributed (non-IID), which brings client drift problems and thuspoor performance. This paper proposes a Parameter-efficient prompt Tuningapproach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient andeffective FL of LLMs. First, an efficient partial prompt tuning approach isproposed to improve performance and efficiency simultaneously. Second, a noveladaptive optimization method is developed to address the client drift problemson both the device and server sides to enhance performance further. Extensiveexperiments based on 10 datasets demonstrate the superb performance (up to60.8\% in terms of accuracy) and efficiency (up to 97.59\% in terms of trainingtime) of FedPepTAO compared with 9 baseline approaches. Our code is availableat https://github.com/llm-eff/FedPepTAO.</description><author>Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S. Sheng, Huaiyu Dai, Dejing Dou</author><pubDate>Mon, 23 Oct 2023 17:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15080v1</guid></item><item><title>Affective and Dynamic Beam Search for Story Generation</title><link>http://arxiv.org/abs/2310.15079v1</link><description>Storytelling's captivating potential makes it a fascinating research area,with implications for entertainment, education, therapy, and cognitive studies.In this paper, we propose Affective Story Generator (AffGen) for generatinginteresting narratives. AffGen introduces "intriguing twists" in narratives byemploying two novel techniques-Dynamic Beam Sizing and Affective Reranking.Dynamic Beam Sizing encourages less predictable, more captivating word choicesusing a contextual multi-arm bandit model. Affective Reranking prioritizessentence candidates based on affect intensity. Our empirical evaluations, bothautomatic and human, demonstrate AffGen's superior performance over existingbaselines in generating affectively charged and interesting narratives. Ourablation study and analysis provide insights into the strengths and weaknessesof AffGen.</description><author>Tenghao Huang, Ehsan Qasemi, Bangzheng Li, He Wang, Faeze Brahman, Muhao Chen, Snigdha Chaturvedi</author><pubDate>Mon, 23 Oct 2023 17:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15079v1</guid></item><item><title>'Don't Get Too Technical with Me': A Discourse Structure-Based Framework for Science Journalism</title><link>http://arxiv.org/abs/2310.15077v1</link><description>Science journalism refers to the task of reporting technical findings of ascientific paper as a less technical news article to the general publicaudience. We aim to design an automated system to support this real-world task(i.e., automatic science journalism) by 1) introducing a newly-constructed andreal-world dataset (SciTechNews), with tuples of a publicly-availablescientific paper, its corresponding news article, and an expert-written shortsummary snippet; 2) proposing a novel technical framework that integrates apaper's discourse structure with its metadata to guide generation; and, 3)demonstrating with extensive automatic and human experiments that our frameworkoutperforms other baseline methods (e.g. Alpaca and ChatGPT) in elaborating acontent plan meaningful for the target audience, simplifying the informationselected, and producing a coherent final report in a layman's style.</description><author>Ronald Cardenas, Bingsheng Yao, Dakuo Wang, Yufang Hou</author><pubDate>Mon, 23 Oct 2023 17:35:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15077v1</guid></item><item><title>TableQAKit: A Comprehensive and Practical Toolkit for Table-based Question Answering</title><link>http://arxiv.org/abs/2310.15075v1</link><description>Table-based question answering (TableQA) is an important task in naturallanguage processing, which requires comprehending tables and employing variousreasoning ways to answer the questions. This paper introduces TableQAKit, thefirst comprehensive toolkit designed specifically for TableQA. The toolkitdesigns a unified platform that includes plentiful TableQA datasets andintegrates popular methods of this task as well as large language models(LLMs). Users can add their datasets and methods according to the friendlyinterface. Also, pleasantly surprised using the modules in this toolkitachieves new SOTA on some datasets. Finally, \tableqakit{} also provides anLLM-based TableQA Benchmark for evaluating the role of LLMs in TableQA.TableQAKit is open-source with an interactive interface that includes visualoperations, and comprehensive data for ease of use.</description><author>Fangyu Lei, Tongxu Luo, Pengqi Yang, Weihao Liu, Hanwen Liu, Jiahe Lei, Yiming Huang, Yifan Wei, Shizhu He, Jun Zhao, Kang Liu</author><pubDate>Mon, 23 Oct 2023 17:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15075v1</guid></item><item><title>MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks</title><link>http://arxiv.org/abs/2310.15074v1</link><description>Differentiable architecture search (DAS) has become the prominent approach inthe field of neural architecture search (NAS) due to its time-efficientautomation of neural network design. It shifts the traditional paradigm ofdiscrete architecture sampling and evaluation to differentiable super-netoptimization and discretization. However, existing DAS methods either onlyconduct coarse-grained operation-level search, or restrictively explorefine-grained filter-level and weight-level units using manually-definedremaining ratios, which fail to simultaneously achieve small model size andsatisfactory model performance. Additionally, they address the high memoryconsumption of the search process at the expense of search quality. To tacklethese issues, we introduce multi-granularity architecture search (MGAS), aunified framework which aims to comprehensively and memory-efficiently explorethe multi-granularity search space to discover both effective and efficientneural networks. Specifically, we learn discretization functions specific toeach granularity level to adaptively determine the remaining ratios accordingto the evolving architecture. This ensures an optimal balance among units ofdifferent granularity levels for different target model sizes. Considering thememory demands, we break down the super-net optimization and discretizationinto multiple sub-net stages. By allowing re-pruning and regrowing of units inprevious sub-nets during subsequent stages, we compensate for potential bias inearlier stages. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNetdemonstrate that MGAS outperforms other state-of-the-art methods in achieving abetter trade-off between model performance and model size.</description><author>Xiaoyun Liu, Divya Saxena, Jiannong Cao, Yuqing Zhao, Penghui Ruan</author><pubDate>Mon, 23 Oct 2023 17:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15074v1</guid></item><item><title>RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in Dynamic Environments</title><link>http://arxiv.org/abs/2310.15072v1</link><description>It is typically challenging for visual or visual-inertial odometry systems tohandle the problems of dynamic scenes and pure rotation. In this work, wedesign a novel visual-inertial odometry (VIO) system called RD-VIO to handleboth of these two problems. Firstly, we propose an IMU-PARSAC algorithm whichcan robustly detect and match keypoints in a two-stage process. In the firststate, landmarks are matched with new keypoints using visual and IMUmeasurements. We collect statistical information from the matching and thenguide the intra-keypoint matching in the second stage. Secondly, to handle theproblem of pure rotation, we detect the motion type and adapt thedeferred-triangulation technique during the data-association process. We makethe pure-rotational frames into the special subframes. When solving thevisual-inertial bundle adjustment, they provide additional constraints to thepure-rotational motion. We evaluate the proposed VIO system on public datasets.Experiments show the proposed RD-VIO has obvious advantages over other methodsin dynamic environments.</description><author>Jinyu Li, Xiaokun Pan, Gan Huang, Ziyang Zhang, Nan Wang, Hujun Bao, Guofeng Zhang</author><pubDate>Mon, 23 Oct 2023 17:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15072v1</guid></item><item><title>Content-Based Search for Deep Generative Models</title><link>http://arxiv.org/abs/2210.03116v3</link><description>The growing proliferation of customized and pretrained generative models hasmade it infeasible for a user to be fully cognizant of every model inexistence. To address this need, we introduce the task of content-based modelsearch: given a query and a large set of generative models, finding the modelsthat best match the query. As each generative model produces a distribution ofimages, we formulate the search task as an optimization problem to select themodel with the highest probability of generating similar content as the query.We introduce a formulation to approximate this probability given the query fromdifferent modalities, e.g., image, sketch, and text. Furthermore, we propose acontrastive learning framework for model retrieval, which learns to adaptfeatures for various query modalities. We demonstrate that our methodoutperforms several baselines on Generative Model Zoo, a new benchmark wecreate for the model retrieval task.</description><author>Daohan Lu, Sheng-Yu Wang, Nupur Kumari, Rohan Agarwal, David Bau, Jun-Yan Zhu</author><pubDate>Mon, 23 Oct 2023 17:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03116v3</guid></item><item><title>Edge-aware Hard Clustering Graph Pooling for Brain Imaging</title><link>http://arxiv.org/abs/2308.11909v5</link><description>Graph Convolutional Networks (GCNs) can capture non-Euclidean spatialdependence between different brain regions. The graph pooling operator, acrucial element of GCNs, enhances the representation learning capability andfacilitates the acquisition of abnormal brain maps. However, most existingresearch designs graph pooling operators solely from the perspective of nodeswhile disregarding the original edge features, in a way that not only confinesgraph pooling application scenarios, but also diminishes its ability to capturecritical substructures. To design a graph clustering pooling operator that istailored to dominant edge features, we proposed the edge-aware hard clusteringgraph pool (EHCPool) and redefined the graph clustering process. Specifically,the 'Edge-to-node' criterion was proposed to evaluate the significance of bothedge and node features. Guided by edge scores, we designed a revolutionaryIteration n-top strategy, aimed at adaptively learning sparse hard clusteringassignments for graphs. Subsequently, a novel N-E Aggregation strategy isintroduced to aggregate node and edge information in each independent subgraph.Extensive experiments on the multi-site public datasets demonstrate thesuperiority and robustness of the proposed model. More notably, EHCPool has thepotential to probe different types of dysfunctional brain networks from adata-driven perspective. Core code is at: https://github.com/swfen/EHCPool.</description><author>Cheng Zhu, Jiayi Zhu, Lijuan Zhang, Xi Wu, Shuqi Yang, Ping Liang, Honghan Chen, Ying Tan</author><pubDate>Mon, 23 Oct 2023 17:23:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11909v5</guid></item><item><title>Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation</title><link>http://arxiv.org/abs/2310.12953v2</link><description>Thanks to their generative capabilities, large language models (LLMs) havebecome an invaluable tool for creative processes. These models have thecapacity to produce hundreds and thousands of visual and textual outputs,offering abundant inspiration for creative endeavors. But are we harnessingtheir full potential? We argue that current interaction paradigms fall short,guiding users towards rapid convergence on a limited set of ideas, rather thanempowering them to explore the vast latent design space in generative models.To address this limitation, we propose a framework that facilitates thestructured generation of design space in which users can seamlessly explore,evaluate, and synthesize a multitude of responses. We demonstrate thefeasibility and usefulness of this framework through the design and developmentof an interactive system, Luminate, and a user study with 8 professionalwriters. Our work advances how we interact with LLMs for creative tasks,introducing a way to harness the creative potential of LLMs.</description><author>Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, Haijun Xia</author><pubDate>Mon, 23 Oct 2023 17:16:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12953v2</guid></item><item><title>Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge</title><link>http://arxiv.org/abs/2310.15066v1</link><description>The ability to actively ground task instructions from an egocentric view iscrucial for AI agents to accomplish tasks or assist humans virtually. Oneimportant step towards this goal is to localize and track key active objectsthat undergo major state change as a consequence of human actions/interactionsto the environment without being told exactly what/where to ground (e.g.,localizing and tracking the `sponge` in video from the instruction "Dip the`sponge` into the bucket."). While existing works approach this problem from apure vision perspective, we investigate to which extent the textual modality(i.e., task instructions) and their interaction with visual modality can bebeneficial. Specifically, we propose to improve phrase grounding models'ability on localizing the active objects by: (1) learning the role of `objectsundergoing change` and extracting them accurately from the instructions, (2)leveraging pre- and post-conditions of the objects during actions, and (3)recognizing the objects more robustly with descriptional knowledge. We leveragelarge language models (LLMs) to extract the aforementioned action-objectknowledge, and design a per-object aggregation masking technique to effectivelyperform joint inference on object phrases and symbolic knowledge. We evaluateour framework on Ego4D and Epic-Kitchens datasets. Extensive experimentsdemonstrate the effectiveness of our proposed framework, which leads to&gt;54%improvements in all standard metrics on the TREK-150-OPE-Det localization +tracking task, &gt;7% improvements in all standard metrics on the TREK-150-OPEtracking task, and &gt;3% improvements in average precision (AP) on the Ego4D SCODtask.</description><author>Te-Lin Wu, Yu Zhou, Nanyun Peng</author><pubDate>Mon, 23 Oct 2023 17:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15066v1</guid></item><item><title>The Crucial Role of Normalization in Sharpness-Aware Minimization</title><link>http://arxiv.org/abs/2305.15287v2</link><description>Sharpness-Aware Minimization (SAM) is a recently proposed gradient-basedoptimizer (Foret et al., ICLR 2021) that greatly improves the predictionperformance of deep neural networks. Consequently, there has been a surge ofinterest in explaining its empirical success. We focus, in particular, onunderstanding the role played by normalization, a key component of the SAMupdates. We theoretically and empirically study the effect of normalization inSAM for both convex and non-convex functions, revealing two key roles played bynormalization: i) it helps in stabilizing the algorithm; and ii) it enables thealgorithm to drift along a continuum (manifold) of minima -- a propertyidentified by recent theoretical works that is the key to better performance.We further argue that these two properties of normalization make SAM robustagainst the choice of hyper-parameters, supporting the practicality of SAM. Ourconclusions are backed by various experiments.</description><author>Yan Dai, Kwangjun Ahn, Suvrit Sra</author><pubDate>Mon, 23 Oct 2023 17:12:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15287v2</guid></item><item><title>Generative Flow Networks as Entropy-Regularized RL</title><link>http://arxiv.org/abs/2310.12934v2</link><description>The recently proposed generative flow networks (GFlowNets) are a method oftraining a policy to sample compositional discrete objects with probabilitiesproportional to a given reward via a sequence of actions. GFlowNets exploit thesequential nature of the problem, drawing parallels with reinforcement learning(RL). Our work extends the connection between RL and GFlowNets to a generalcase. We demonstrate how the task of learning a generative flow network can beefficiently redefined as an entropy-regularized RL problem with a specificreward and regularizer structure. Furthermore, we illustrate the practicalefficiency of this reformulation by applying standard soft RL algorithms toGFlowNet training across several probabilistic modeling tasks. Contrary topreviously reported results, we show that entropic RL approaches can becompetitive against established GFlowNet training methods. This perspectiveopens a direct path for integrating reinforcement learning principles into therealm of generative flow networks.</description><author>Daniil Tiapkin, Nikita Morozov, Alexey Naumov, Dmitry Vetrov</author><pubDate>Mon, 23 Oct 2023 17:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12934v2</guid></item><item><title>Synergizing Human-AI Agency: A Guide of 23 Heuristics for Service Co-Creation with LLM-Based Agents</title><link>http://arxiv.org/abs/2310.15065v1</link><description>This empirical study serves as a primer for interested service providers todetermine if and how Large Language Models (LLMs) technology will be integratedfor their practitioners and the broader community. We investigate the mutuallearning journey of non-AI experts and AI through CoAGent, a serviceco-creation tool with LLM-based agents. Engaging in a three-stage participatorydesign processes, we work with with 23 domain experts from public librariesacross the U.S., uncovering their fundamental challenges of integrating AI intohuman workflows. Our findings provide 23 actionable "heuristics for serviceco-creation with AI", highlighting the nuanced shared responsibilities betweenhumans and AI. We further exemplar 9 foundational agency aspects for AI,emphasizing essentials like ownership, fair treatment, and freedom ofexpression. Our innovative approach enriches the participatory design model byincorporating AI as crucial stakeholders and utilizing AI-AI interaction toidentify blind spots. Collectively, these insights pave the way for synergisticand ethical human-AI co-creation in service contexts, preparing for workforceecosystems where AI coexists.</description><author>Qingxiao Zheng, Zhongwei Xu, Abhinav Choudhary, Yuting Chen, Yongming Li, Yun Huang</author><pubDate>Mon, 23 Oct 2023 17:11:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15065v1</guid></item><item><title>The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models</title><link>http://arxiv.org/abs/2310.15061v1</link><description>Despite the impressive performance achieved by pre-trainedlanguage-and-vision models in downstream tasks, it remains an open questionwhether this reflects a proper understanding of image-text interaction. In thiswork, we explore to what extent they handle basic linguistic constructions --active-passive voice, coordination, and relative clauses -- that even preschoolchildren can typically master. We present BLA, a novel, automaticallyconstructed benchmark to evaluate multimodal models on these Basic LanguageAbilities. We show that different types of Transformer-based systems, such asCLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting,in line with previous findings. Our experiments, in particular, show that mostof the tested models only marginally benefit when fine-tuned or prompted withconstruction-specific samples. Yet, the generative BLIP2 shows promisingtrends, especially in an in-context learning setting. This opens the door tousing BLA not only as an evaluation benchmark but also to improve models' basiclanguage abilities.</description><author>Xinyi Chen, Raquel Fernández, Sandro Pezzelle</author><pubDate>Mon, 23 Oct 2023 17:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15061v1</guid></item><item><title>Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation</title><link>http://arxiv.org/abs/2309.08138v2</link><description>The task of Visual Object Navigation (VON) involves an agent's ability tolocate a particular object within a given scene. In order to successfullyaccomplish the VON task, two essential conditions must be fulfilled:1) the usermust know the name of the desired object; and 2) the user-specified object mustactually be present within the scene. To meet these conditions, a simulator canincorporate pre-defined object names and positions into the metadata of thescene. However, in real-world scenarios, it is often challenging to ensure thatthese conditions are always met. Human in an unfamiliar environment may notknow which objects are present in the scene, or they may mistakenly specify anobject that is not actually present. Nevertheless, despite these challenges,human may still have a demand for an object, which could potentially befulfilled by other objects present within the scene in an equivalent manner.Hence, we propose Demand-driven Navigation (DDN), which leverages the user'sdemand as the task instruction and prompts the agent to find the object matchesthe specified demand. DDN aims to relax the stringent conditions of VON byfocusing on fulfilling the user's demand rather than relying solely onpredefined object categories or names. We propose a method first acquiretextual attribute features of objects by extracting common knowledge from alarge language model. These textual attribute features are subsequently alignedwith visual attribute features using Contrastive Language-Image Pre-training(CLIP). By incorporating the visual attribute features as prior knowledge, weenhance the navigation process. Experiments on AI2Thor with the ProcThordataset demonstrate the visual attribute features improve the agent'snavigation performance and outperform the baseline methods commonly used inVON.</description><author>Hongcheng Wang, Andy Guan Hong Chen, Xiaoqi Li, Mingdong Wu, Hao Dong</author><pubDate>Mon, 23 Oct 2023 17:04:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08138v2</guid></item><item><title>Robot Skill Generalization via Keypoint Integrated Soft Actor-Critic Gaussian Mixture Models</title><link>http://arxiv.org/abs/2310.15059v1</link><description>A long-standing challenge for a robotic manipulation system operating inreal-world scenarios is adapting and generalizing its acquired motor skills tounseen environments. We tackle this challenge employing hybrid skill modelsthat integrate imitation and reinforcement paradigms, to explore how thelearning and adaptation of a skill, along with its core grounding in the scenethrough a learned keypoint, can facilitate such generalization. To that end, wedevelop Keypoint Integrated Soft Actor-Critic Gaussian Mixture Models (KIS-GMM)approach that learns to predict the reference of a dynamical system within thescene as a 3D keypoint, leveraging visual observations obtained by the robot'sphysical interactions during skill learning. Through conducting comprehensiveevaluations in both simulated and real-world environments, we show that ourmethod enables a robot to gain a significant zero-shot generalization to novelenvironments and to refine skills in the target environments faster thanlearning from scratch. Importantly, this is achieved without the need for newground truth data. Moreover, our method effectively copes with scenedisplacements.</description><author>Iman Nematollahi, Kirill Yankov, Wolfram Burgard, Tim Welschehold</author><pubDate>Mon, 23 Oct 2023 17:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15059v1</guid></item><item><title>Towards Conceptualization of "Fair Explanation": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators</title><link>http://arxiv.org/abs/2310.15055v1</link><description>Recent research at the intersection of AI explainability and fairness hasfocused on how explanations can improve human-plus-AI task performance asassessed by fairness measures. We propose to characterize what constitutes anexplanation that is itself "fair" -- an explanation that does not adverselyimpact specific populations. We formulate a novel evaluation method of "fairexplanations" using not just accuracy and label time, but also psychologicalimpact of explanations on different user groups across many metrics (mentaldiscomfort, stereotype activation, and perceived workload). We apply thismethod in the context of content moderation of potential hate speech, and itsdifferential impact on Asian vs. non-Asian proxy moderators, across explanationapproaches (saliency map and counterfactual explanation). We find that saliencymaps generally perform better and show less evidence of disparate impact(group) and individual unfairness than counterfactual explanations. Content warning: This paper contains examples of hate speech and raciallydiscriminatory language. The authors do not support such content. Pleaseconsider your risk of discomfort carefully before continuing reading!</description><author>Tin Nguyen, Jiannan Xu, Aayushi Roy, Hal Daumé III, Marine Carpuat</author><pubDate>Mon, 23 Oct 2023 16:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15055v1</guid></item><item><title>Coordinated Replay Sample Selection for Continual Federated Learning</title><link>http://arxiv.org/abs/2310.15054v1</link><description>Continual Federated Learning (CFL) combines Federated Learning (FL), thedecentralized learning of a central model on a number of client devices thatmay not communicate their data, and Continual Learning (CL), the learning of amodel from a continual stream of data without keeping the entire history. InCL, the main challenge is \textit{forgetting} what was learned from past data.While replay-based algorithms that keep a small pool of past training data areeffective to reduce forgetting, only simple replay sample selection strategieshave been applied to CFL in prior work, and no previous work has exploredcoordination among clients for better sample selection. To bridge this gap, weadapt a replay sample selection objective based on loss gradient diversity toCFL and propose a new relaxation-based selection of samples to optimize theobjective. Next, we propose a practical algorithm to coordinate gradient-basedreplay sample selection across clients without communicating private data. Webenchmark our coordinated and uncoordinated replay sample selection algorithmsagainst random sampling-based baselines with language models trained on a largescale de-identified real-world text dataset. We show that gradient-based sampleselection methods both boost performance and reduce forgetting compared torandom sampling methods, with our coordination method showing gains early inthe low replay size regime (when the budget for storing past data is small).</description><author>Jack Good, Jimit Majmudar, Christophe Dupuy, Jixuan Wang, Charith Peris, Clement Chung, Richard Zemel, Rahul Gupta</author><pubDate>Mon, 23 Oct 2023 16:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15054v1</guid></item><item><title>DREAM+: Efficient Dataset Distillation by Bidirectional Representative Matching</title><link>http://arxiv.org/abs/2310.15052v1</link><description>Dataset distillation plays a crucial role in creating compact datasets withsimilar training performance compared with original large-scale ones. This isessential for addressing the challenges of data storage and training costs.Prevalent methods facilitate knowledge transfer by matching the gradients,embedding distributions, or training trajectories of synthetic images withthose of the sampled original images. Although there are various matchingobjectives, currently the strategy for selecting original images is limited tonaive random sampling. We argue that random sampling overlooks the evenness ofthe selected sample distribution, which may result in noisy or biased matchingtargets. Besides, the sample diversity is also not constrained by randomsampling. Additionally, current methods predominantly focus onsingle-dimensional matching, where information is not fully utilized. Toaddress these challenges, we propose a novel matching strategy called DatasetDistillation by Bidirectional REpresentAtive Matching (DREAM+), which selectsrepresentative original images for bidirectional matching. DREAM+ is applicableto a variety of mainstream dataset distillation frameworks and significantlyreduces the number of distillation iterations by more than 15 times withoutaffecting performance. Given sufficient training time, DREAM+ can furtherimprove the performance and achieve state-of-the-art results. We have releasedthe code at github.com/NUS-HPC-AI-Lab/DREAM+.</description><author>Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Zhu, Kaipeng Zhang, Wei Jiang, Yang You</author><pubDate>Mon, 23 Oct 2023 16:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15052v1</guid></item><item><title>TeleQnA: A Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge</title><link>http://arxiv.org/abs/2310.15051v1</link><description>We introduce TeleQnA, the first benchmark dataset designed to evaluate theknowledge of Large Language Models (LLMs) in telecommunications. Comprising10,000 questions and answers, this dataset draws from diverse sources,including standards and research articles. This paper outlines the automatedquestion generation framework responsible for creating this dataset, along withhow human input was integrated at various stages to ensure the quality of thequestions. Afterwards, using the provided dataset, an evaluation is conductedto assess the capabilities of LLMs, including GPT-3.5 and GPT-4. The resultshighlight that these models struggle with complex standards related questionsbut exhibit proficiency in addressing general telecom-related inquiries.Additionally, our results showcase how incorporating telecom knowledge contextsignificantly enhances their performance, thus shedding light on the need for aspecialized telecom foundation model. Finally, the dataset is shared withactive telecom professionals, whose performance is subsequently benchmarkedagainst that of the LLMs. The findings illustrate that LLMs can rival theperformance of active professionals in telecom knowledge, thanks to theircapacity to process vast amounts of information, underscoring the potential ofLLMs within this domain. The dataset has been made publicly accessible onGitHub.</description><author>Ali Maatouk, Fadhel Ayed, Nicola Piovesan, Antonio De Domenico, Merouane Debbah, Zhi-Quan Luo</author><pubDate>Mon, 23 Oct 2023 16:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15051v1</guid></item><item><title>Quantum Advantage Seeker with Kernels (QuASK): a software framework to speed up the research in quantum machine learning</title><link>http://arxiv.org/abs/2206.15284v2</link><description>Exploiting the properties of quantum information to the benefit of machinelearning models is perhaps the most active field of research in quantumcomputation. This interest has supported the development of a multitude ofsoftware frameworks (e.g. Qiskit, Pennylane, Braket) to implement, simulate,and execute quantum algorithms. Most of them allow us to define quantumcircuits, run basic quantum algorithms, and access low-level primitivesdepending on the hardware such software is supposed to run. For mostexperiments, these frameworks have to be manually integrated within a largermachine learning software pipeline. The researcher is in charge of knowingdifferent software packages, integrating them through the development of longcode scripts, analyzing the results, and generating the plots. Long code oftenleads to erroneous applications, due to the average number of bugs growingproportional with respect to the program length. Moreover, other researcherswill struggle to understand and reproduce the experiment, due to the need to befamiliar with all the different software frameworks involved in the codescript. We propose QuASK, an open-source quantum machine learning frameworkwritten in Python that aids the researcher in performing their experiments,with particular attention to quantum kernel techniques. QuASK can be used as acommand-line tool to download datasets, pre-process them, quantum machinelearning routines, analyze and visualize the results. QuASK implements moststate-of-the-art algorithms to analyze the data through quantum kernels, withthe possibility to use projected kernels, (gradient-descent) trainable quantumkernels, and structure-optimized quantum kernels. Our framework can also beused as a library and integrated into pre-existing software, maximizing codereuse.</description><author>Francesco Di Marcantonio, Massimiliano Incudini, Davide Tezza, Michele Grossi</author><pubDate>Mon, 23 Oct 2023 16:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.15284v2</guid></item><item><title>Meta- (out-of-context) learning in neural networks</title><link>http://arxiv.org/abs/2310.15047v1</link><description>Brown et al. (2020) famously introduced the phenomenon of in-context learningin large language models (LLMs). We establish the existence of a phenomenon wecall $\textbf{meta-out-of-context learning (meta-OCL)}$ via carefully designedsynthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMsto more readily "internalize" the semantic content of text that is, or appearsto be, broadly useful (such as true statements, or text from authoritativesources) and use it in appropriate circumstances. We further demonstratemeta-OCL in a synthetic computer vision setting, and propose two hypotheses forthe emergence of meta-OCL: one relying on the way models store knowledge intheir parameters, and another suggesting that the implicit gradient alignmentbias of gradient-descent-based optimizers may be responsible. Finally, wereflect on what our results might imply about capabilities of future AIsystems, and discuss potential risks. Our code can be found athttps://github.com/krasheninnikov/internalization .</description><author>Dmitrii Krasheninnikov, Egor Krasheninnikov, Bruno Mlodozeniec, David Krueger</author><pubDate>Mon, 23 Oct 2023 16:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15047v1</guid></item><item><title>A Universal Anti-Spoofing Approach for Contactless Fingerprint Biometric Systems</title><link>http://arxiv.org/abs/2310.15044v1</link><description>With the increasing integration of smartphones into our daily lives,fingerphotos are becoming a potential contactless authentication method. Whileit offers convenience, it is also more vulnerable to spoofing using variouspresentation attack instruments (PAI). The contactless fingerprint is anemerging biometric authentication but has not yet been heavily investigated foranti-spoofing. While existing anti-spoofing approaches demonstrated fairresults, they have encountered challenges in terms of universality andscalability to detect any unseen/unknown spoofed samples. To address thisissue, we propose a universal presentation attack detection method forcontactless fingerprints, despite having limited knowledge of presentationattack samples. We generated synthetic contactless fingerprints using StyleGANfrom live finger photos and integrating them to train a semi-supervisedResNet-18 model. A novel joint loss function, combining the Arcface and Centerloss, is introduced with a regularization to balance between the two lossfunctions and minimize the variations within the live samples while enhancingthe inter-class variations between the deepfake and live samples. We alsoconducted a comprehensive comparison of different regularizations' impact onthe joint loss function for presentation attack detection (PAD) and exploredthe performance of a modified ResNet-18 architecture with different activationfunctions (i.e., leaky ReLU and RelU) in conjunction with Arcface and centerloss. Finally, we evaluate the performance of the model using unseen types ofspoof attacks and live data. Our proposed method achieves a Bona FideClassification Error Rate (BPCER) of 0.12\%, an Attack PresentationClassification Error Rate (APCER) of 0.63\%, and an Average ClassificationError Rate (ACER) of 0.37\%.</description><author>Banafsheh Adami, Sara Tehranipoor, Nasser Nasrabadi, Nima Karimian</author><pubDate>Mon, 23 Oct 2023 16:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15044v1</guid></item><item><title>CalibrationPhys: Self-supervised Video-based Heart and Respiratory Rate Measurements by Calibrating Between Multiple Cameras</title><link>http://arxiv.org/abs/2310.15043v1</link><description>Video-based heart and respiratory rate measurements using facial videos aremore useful and user-friendly than traditional contact-based sensors. However,most of the current deep learning approaches require ground-truth pulse andrespiratory waves for model training, which are expensive to collect. In thispaper, we propose CalibrationPhys, a self-supervised video-based heart andrespiratory rate measurement method that calibrates between multiple cameras.CalibrationPhys trains deep learning models without supervised labels by usingfacial videos captured simultaneously by multiple cameras. Contrastive learningis performed so that the pulse and respiratory waves predicted from thesynchronized videos using multiple cameras are positive and those fromdifferent videos are negative. CalibrationPhys also improves the robustness ofthe models by means of a data augmentation technique and successfully leveragesa pre-trained model for a particular camera. Experimental results utilizing twodatasets demonstrate that CalibrationPhys outperforms state-of-the-art heartand respiratory rate measurement methods. Since we optimize camera-specificmodels using only videos from multiple cameras, our approach makes it easy touse arbitrary cameras for heart and respiratory rate measurements.</description><author>Yusuke Akamatsu, Terumi Umematsu, Hitoshi Imaoka</author><pubDate>Mon, 23 Oct 2023 16:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15043v1</guid></item><item><title>Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing</title><link>http://arxiv.org/abs/2306.06599v6</link><description>Existing regression models tend to fall short in both accuracy anduncertainty estimation when the label distribution is imbalanced. In thispaper, we propose a probabilistic deep learning model, dubbed variationalimbalanced regression (VIR), which not only performs well in imbalancedregression but naturally produces reasonable uncertainty estimation as abyproduct. Different from typical variational autoencoders assuming I.I.D.representations (a data point's representation is not directly affected byother data points), our VIR borrows data with similar regression labels tocompute the latent representation's variational distribution; furthermore,different from deterministic regression models producing point estimates, VIRpredicts the entire normal-inverse-gamma distributions and modulates theassociated conjugate distributions to impose probabilistic reweighting on theimbalanced data, thereby providing better uncertainty estimation. Experimentsin several real-world datasets show that our VIR can outperformstate-of-the-art imbalanced regression models in terms of both accuracy anduncertainty estimation. Code will soon be available athttps://github.com/Wang-ML-Lab/variational-imbalanced-regression.</description><author>Ziyan Wang, Hao Wang</author><pubDate>Mon, 23 Oct 2023 16:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06599v6</guid></item><item><title>Manipulation Mask Generator: High-Quality Image Manipulation Mask Generation Method Based on Modified Total Variation Noise Reduction</title><link>http://arxiv.org/abs/2310.15041v1</link><description>In artificial intelligence, any model that wants to achieve a good result isinseparable from a large number of high-quality data. It is especially true inthe field of tamper detection. This paper proposes a modified total variationnoise reduction method to acquire high-quality tampered images. Weautomatically crawl original and tampered images from the Baidu PS Bar. BaiduPS Bar is a website where net friends post countless tampered images.Subtracting the original image with the tampered image can highlight thetampered area. However, there is also substantial noise on the final print, sothese images can't be directly used in the deep learning model. Our modifiedtotal variation noise reduction method is aimed at solving this problem.Because a lot of text is slender, it is easy to lose text information after theopening and closing operation. We use MSER (Maximally Stable Extremal Regions)and NMS (Non-maximum Suppression) technology to extract text information. Andthen use the modified total variation noise reduction technology to process thesubtracted image. Finally, we can obtain an image with little noise by addingthe image and text information. And the idea also largely retains the textinformation. Datasets generated in this way can be used in deep learningmodels, and they will help the model achieve better results.</description><author>Xinyu Yang, Jizhe Zhou</author><pubDate>Mon, 23 Oct 2023 16:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15041v1</guid></item><item><title>SLOG: A Structural Generalization Benchmark for Semantic Parsing</title><link>http://arxiv.org/abs/2310.15040v1</link><description>The goal of compositional generalization benchmarks is to evaluate how wellmodels generalize to new complex linguistic expressions. Existing benchmarksoften focus on lexical generalization, the interpretation of novel lexicalitems in syntactic structures familiar from training; structural generalizationtasks, where a model needs to interpret syntactic structures that arethemselves unfamiliar from training, are often underrepresented, resulting inoverly optimistic perceptions of how well models can generalize. We introduceSLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with17 structural generalization cases. In our experiments, the generalizationaccuracy of Transformer models, including pretrained ones, only reaches 40.6%,while a structure-aware parser only achieves 70.8%. These results are far fromthe near-perfect accuracy existing models achieve on COGS, demonstrating therole of SLOG in foregrounding the large discrepancy between models' lexical andstructural generalization capacities.</description><author>Bingzhi Li, Lucia Donatelli, Alexander Koller, Tal Linzen, Yuekun Yao, Najoung Kim</author><pubDate>Mon, 23 Oct 2023 16:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15040v1</guid></item><item><title>Understanding and Modeling the Effects of Task and Context on Drivers' Gaze Allocation</title><link>http://arxiv.org/abs/2310.09275v2</link><description>Understanding what drivers look at is important for many applications,including driver training, monitoring, and assistance, as well as self-driving.Traditionally, factors affecting human visual attention have been divided intobottom-up (involuntary attraction to salient regions) and top-down (task- andcontext-driven). Although both play a role in drivers' gaze allocation, most ofthe existing modeling approaches apply techniques developed for bottom-upsaliency and do not consider task and context influences explicitly. Likewise,common driving attention benchmarks lack relevant task and context annotations.Therefore, to enable analysis and modeling of these factors for drivers' gazeprediction, we propose the following: 1) address some shortcomings of thepopular DR(eye)VE dataset and extend it with per-frame annotations for drivingtask and context; 2) benchmark a number of baseline and SOTA models forsaliency and driver gaze prediction and analyze them w.r.t. the newannotations; and finally, 3) a novel model that modulates drivers' gazeprediction with explicit action and context information, and as a resultsignificantly improves SOTA performance on DR(eye)VE overall (by 24\% KLD and89\% NSS) and on a subset of action and safety-critical intersection scenarios(by 10--30\% KLD). Extended annotations, code for model and evaluation will bemade publicly available.</description><author>Iuliia Kotseruba, John K. Tsotsos</author><pubDate>Mon, 23 Oct 2023 16:37:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09275v2</guid></item><item><title>CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents</title><link>http://arxiv.org/abs/2306.10376v5</link><description>In this paper, we focus on inferring whether the given user command is clear,ambiguous, or infeasible in the context of interactive robotic agents utilizinglarge language models (LLMs). To tackle this problem, we first present anuncertainty estimation method for LLMs to classify whether the command iscertain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the commandis classified as uncertain, we further distinguish it between ambiguous orinfeasible commands leveraging LLMs with situational aware context in azero-shot manner. For ambiguous commands, we disambiguate the command byinteracting with users via question generation with LLMs. We believe thatproper recognition of the given commands could lead to a decrease inmalfunction and undesired actions of the robot, enhancing the reliability ofinteractive robot agents. We present a dataset for robotic situationalawareness, consisting pair of high-level commands, scene descriptions, andlabels of command type (i.e., clear, ambiguous, or infeasible). We validate theproposed method on the collected dataset, pick-and-place tabletop simulation.Finally, we demonstrate the proposed approach in real-world human-robotinteraction experiments, i.e., handover scenarios.</description><author>Jeongeun Park, Seungwon Lim, Joonhyung Lee, Sangbeom Park, Minsuk Chang, Youngjae Yu, Sungjoon Choi</author><pubDate>Mon, 23 Oct 2023 16:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10376v5</guid></item><item><title>UWB Based Static Gesture Classification</title><link>http://arxiv.org/abs/2310.15036v1</link><description>Our paper presents a robust framework for UWB-based static gesturerecognition, leveraging proprietary UWB radar sensor technology. Extensive datacollection efforts were undertaken to compile datasets containing five commonlyused gestures. Our approach involves a comprehensive data pre-processingpipeline that encompasses outlier handling, aspect ratio-preserving resizing,and false-color image transformation. Both CNN and MobileNet models weretrained on the processed images. Remarkably, our best-performing model achievedan accuracy of 96.78%. Additionally, we developed a user-friendly GUI frameworkto assess the model's system resource usage and processing times, whichrevealed low memory utilization and real-time task completion in under onesecond. This research marks a significant step towards enhancing static gesturerecognition using UWB technology, promising practical applications in variousdomains.</description><author>Abhishek Sebastian</author><pubDate>Mon, 23 Oct 2023 16:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15036v1</guid></item><item><title>An Attribution Method for Siamese Encoders</title><link>http://arxiv.org/abs/2310.05703v2</link><description>Despite the success of Siamese encoder models such as sentence transformers(ST), little is known about the aspects of inputs they pay attention to. Abarrier is that their predictions cannot be attributed to individual features,as they compare two inputs rather than processing a single one. This paperderives a local attribution method for Siamese encoders by generalizing theprinciple of integrated gradients to models with multiple inputs. The solutiontakes the form of feature-pair attributions, and can be reduced to atoken-token matrix for STs. Our method involves the introduction of integratedJacobians and inherits the advantageous formal properties of integratedgradients: it accounts for the model's full computation graph and is guaranteedto converge to the actual prediction. A pilot study shows that in an ST fewtoken-pairs can often explain large fractions of predictions, and it focuses onnouns and verbs. For accurate predictions, it however needs to attend to themajority of tokens and parts of speech.</description><author>Lucas Möller, Dmitry Nikolaev, Sebastian Padó</author><pubDate>Mon, 23 Oct 2023 16:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05703v2</guid></item><item><title>Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models</title><link>http://arxiv.org/abs/2310.13678v2</link><description>One challenge in speech translation is that plenty of spoken content islong-form, but short units are necessary for obtaining high-qualitytranslations. To address this mismatch, we adapt large language models (LLMs)to split long ASR transcripts into segments that can be independentlytranslated so as to maximize the overall translation quality. We overcome thetendency of hallucination in LLMs by incorporating finite-state constraintsduring decoding; these eliminate invalid outputs without requiring additionaltraining. We discover that LLMs are adaptable to transcripts containing ASRerrors through prompt-tuning or fine-tuning. Relative to a state-of-the-artautomatic punctuation baseline, our best LLM improves the average BLEU by 2.9points for English-German, English-Spanish, and English-Arabic TED talktranslation in 9 test sets, just by improving segmentation.</description><author>Arya D. McCarthy, Hao Zhang, Shankar Kumar, Felix Stahlberg, Ke Wu</author><pubDate>Mon, 23 Oct 2023 16:25:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13678v2</guid></item><item><title>The Benefits of Label-Description Training for Zero-Shot Text Classification</title><link>http://arxiv.org/abs/2305.02239v2</link><description>Pretrained language models have improved zero-shot text classification byallowing the transfer of semantic knowledge from the training data in order toclassify among specific label sets in downstream tasks. We propose a simple wayto further improve zero-shot accuracies with minimal effort. We curate smallfinetuning datasets intended to describe the labels for a task. Unlike typicalfinetuning data, which has texts annotated with labels, our data simplydescribes the labels in language, e.g., using a few related terms,dictionary/encyclopedia entries, and short templates. Across a range of topicand sentiment datasets, our method is more accurate than zero-shot by 17-19%absolute. It is also more robust to choices required for zero-shotclassification, such as patterns for prompting the model to classify andmappings from labels to tokens in the model's vocabulary. Furthermore, sinceour data merely describes the labels but does not use input texts, finetuningon it yields a model that performs strongly on multiple text domains for agiven label set, even improving over few-shot out-of-domain classification inmultiple settings.</description><author>Lingyu Gao, Debanjan Ghosh, Kevin Gimpel</author><pubDate>Mon, 23 Oct 2023 16:24:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02239v2</guid></item><item><title>Deep Autoencoder-based Z-Interference Channels with Perfect and Imperfect CSI</title><link>http://arxiv.org/abs/2310.15027v1</link><description>A deep autoencoder (DAE)-based structure for endto-end communication over thetwo-user Z-interference channel (ZIC) with finite-alphabet inputs is designedin this paper. The proposed structure jointly optimizes the two encoder/decoderpairs and generates interference-aware constellations that dynamically adapttheir shape based on interference intensity to minimize the bit error rate(BER). An in-phase/quadrature-phase (I/Q) power allocation layer is introducedin the DAE to guarantee an average power constraint and enable the architectureto generate constellations with nonuniform shapes. This brings further gaincompared to standard uniform constellations such as quadrature amplitudemodulation. The proposed structure is then extended to work with imperfectchannel state information (CSI). The CSI imperfection due to both theestimation and quantization errors are examined. The performance of the DAEZICis compared with two baseline methods, i.e., standard and rotatedconstellations. The proposed structure significantly enhances the performanceof the ZIC both for the perfect and imperfect CSI. Simulation results show thatthe improvement is achieved in all interference regimes (weak, moderate, andstrong) and consistently increases with the signal-to-noise ratio (SNR). Forexample, more than an order of magnitude BER reduction is obtained with respectto the most competitive conventional method at weak interference when SNR&gt;15dBand two bits per symbol are transmitted. The improvements reach about twoorders of magnitude when quantization error exists, indicating that the DAE-ZICis more robust to the interference compared to the conventional methods.</description><author>Xinliang Zhang, Mojtaba Vaezi</author><pubDate>Mon, 23 Oct 2023 16:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15027v1</guid></item><item><title>Fast 2D Bicephalous Convolutional Autoencoder for Compressing 3D Time Projection Chamber Data</title><link>http://arxiv.org/abs/2310.15026v1</link><description>High-energy large-scale particle colliders produce data at high speed in theorder of 1 terabytes per second in nuclear physics and petabytes per second inhigh-energy physics. Developing real-time data compression algorithms to reducesuch data at high throughput to fit permanent storage has drawn increasingattention. Specifically, at the newly constructed sPHENIX experiment at theRelativistic Heavy Ion Collider (RHIC), a time projection chamber is used asthe main tracking detector, which records particle trajectories in a volume ofa three-dimensional (3D) cylinder. The resulting data are usually very sparsewith occupancy around 10.8%. Such sparsity presents a challenge to conventionallearning-free lossy compression algorithms, such as SZ, ZFP, and MGARD. The 3Dconvolutional neural network (CNN)-based approach, Bicephalous ConvolutionalAutoencoder (BCAE), outperforms traditional methods both in compression rateand reconstruction accuracy. BCAE can also utilize the computation power ofgraphical processing units suitable for deployment in a modern heterogeneoushigh-performance computing environment. This work introduces two BCAE variants:BCAE++ and BCAE-2D. BCAE++ achieves a 15% better compression ratio and a 77%better reconstruction accuracy measured in mean absolute error compared withBCAE. BCAE-2D treats the radial direction as the channel dimension of an image,resulting in a 3x speedup in compression throughput. In addition, wedemonstrate an unbalanced autoencoder with a larger decoder can improvereconstruction accuracy without significantly sacrificing throughput. Lastly,we observe both the BCAE++ and BCAE-2D can benefit more from usinghalf-precision mode in throughput (76-79% increase) without loss inreconstruction accuracy. The source code and links to data and pretrainedmodels can be found at https://github.com/BNL-DAQ-LDRD/NeuralCompression_v2.</description><author>Yi Huang, Yihui Ren, Shinjae Yoo, Jin Huang</author><pubDate>Mon, 23 Oct 2023 16:23:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15026v1</guid></item><item><title>P2AT: Pyramid Pooling Axial Transformer for Real-time Semantic Segmentation</title><link>http://arxiv.org/abs/2310.15025v1</link><description>Recently, Transformer-based models have achieved promising results in variousvision tasks, due to their ability to model long-range dependencies. However,transformers are computationally expensive, which limits their applications inreal-time tasks such as autonomous driving. In addition, an efficient local andglobal feature selection and fusion are vital for accurate dense prediction,especially driving scene understanding tasks. In this paper, we propose areal-time semantic segmentation architecture named Pyramid Pooling AxialTransformer (P2AT). The proposed P2AT takes a coarse feature from the CNNencoder to produce scale-aware contextual features, which are then combinedwith the multi-level feature aggregation scheme to produce enhanced contextualfeatures. Specifically, we introduce a pyramid pooling axial transformer tocapture intricate spatial and channel dependencies, leading to improvedperformance on semantic segmentation. Then, we design a Bidirectional Fusionmodule (BiF) to combine semantic information at different levels. Meanwhile, aGlobal Context Enhancer is introduced to compensate for the inadequacy ofconcatenating different semantic levels. Finally, a decoder block is proposedto help maintain a larger receptive field. We evaluate P2AT variants on threechallenging scene-understanding datasets. In particular, our P2AT variantsachieve state-of-art results on the Camvid dataset 80.5%, 81.0%, 81.1% forP2AT-S, P2ATM, and P2AT-L, respectively. Furthermore, our experiment onCityscapes and Pascal VOC 2012 have demonstrated the efficiency of the proposedarchitecture, with results showing that P2AT-M, achieves 78.7% on Cityscapes.The source code will be available at</description><author>Mohammed A. M. Elhassan, Changjun Zhou, Amina Benabid, Abuzar B. M. Adam</author><pubDate>Mon, 23 Oct 2023 16:23:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15025v1</guid></item><item><title>Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems</title><link>http://arxiv.org/abs/2305.15017v2</link><description>Despite outstanding performance in many tasks, language models arenotoriously inclined to make factual errors in tasks requiring arithmeticcomputation. We address this deficiency by creating Calc-X, a collection ofdatasets that demonstrates the appropriate use of a calculator in reasoningchains. Calc-X is suitable for teaching language models to offload computationsto a symbolic system. We survey and unify several existing chain-of-thoughtdatasets into a proposed format, resulting in a standard collection of over300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-Xcollection to train open-source calculator-using models we call Calcformers andshow that these models approximately double the accuracy of generating correctresults compared to vanilla language model baselines. We make all Calc-Xdatasets, source code and Calcformers models publicly available.</description><author>Marek Kadlčík, Michal Štefánik, Ondřej Sotolář, Vlastimil Martinek</author><pubDate>Mon, 23 Oct 2023 16:23:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15017v2</guid></item><item><title>SONIC: Sonar Image Correspondence using Pose Supervised Learning for Imaging Sonars</title><link>http://arxiv.org/abs/2310.15023v1</link><description>In this paper, we address the challenging problem of data association forunderwater SLAM through a novel method for sonar image correspondence usinglearned features. We introduce SONIC (SONar Image Correspondence), apose-supervised network designed to yield robust feature correspondence capableof withstanding viewpoint variations. The inherent complexity of the underwaterenvironment stems from the dynamic and frequently limited visibilityconditions, restricting vision to a few meters of often featureless expanses.This makes camera-based systems suboptimal in most open water applicationscenarios. Consequently, multibeam imaging sonars emerge as the preferredchoice for perception sensors. However, they too are not without theirlimitations. While imaging sonars offer superior long-range visibility comparedto cameras, their measurements can appear different from varying viewpoints.This inherent variability presents formidable challenges in data association,particularly for feature-based methods. Our method demonstrates significantlybetter performance in generating correspondences for sonar images which willpave the way for more accurate loop closure constraints and sonar-based placerecognition. Code as well as simulated and real-world datasets will be madepublic to facilitate further development in the field.</description><author>Samiran Gode, Akshay Hinduja, Michael Kaess</author><pubDate>Mon, 23 Oct 2023 16:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15023v1</guid></item></channel></rss>