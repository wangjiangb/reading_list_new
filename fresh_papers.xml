<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 02 Sep 2025 13:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DriveQA: Passing the Driving Knowledge Test</title><link>http://arxiv.org/abs/2508.21824v1</link><description>If a Large Language Model (LLM) were to take a driving knowledge test today,would it pass? Beyond standard spatial and visual question-answering (QA) taskson current autonomous driving benchmarks, driving knowledge tests require acomplete understanding of all traffic rules, signage, and right-of-wayprinciples. To pass this test, human drivers must discern various edge casesthat rarely appear in real-world datasets. In this work, we present DriveQA, anextensive open-source text and vision-based benchmark that exhaustively coverstraffic regulations and scenarios. Through our experiments using DriveQA, weshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well onbasic traffic rules but exhibit significant weaknesses in numerical reasoningand complex right-of-way scenarios, traffic sign variations, and spatiallayouts, (2) fine-tuning on DriveQA improves accuracy across multiplecategories, particularly in regulatory sign recognition and intersectiondecision-making, (3) controlled variations in DriveQA-V provide insights intomodel sensitivity to environmental factors such as lighting, perspective,distance, and weather conditions, and (4) pretraining on DriveQA enhancesdownstream driving task performance, leading to improved results on real-worlddatasets such as nuScenes and BDD, while also demonstrating that models caninternalize text and synthetic traffic knowledge to generalize effectivelyacross downstream QA tasks.</description><author>Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar</author><pubDate>Fri, 29 Aug 2025 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21824v1</guid></item><item><title>SAGA: A Security Architecture for Governing AI Agentic Systems</title><link>http://arxiv.org/abs/2504.21034v2</link><description>Large Language Model (LLM)-based agents increasingly interact, collaborate,and delegate tasks to one another autonomously with minimal human interaction.Industry guidelines for agentic system governance emphasize the need for usersto maintain comprehensive control over their agents, mitigating potentialdamage from malicious agents. Several proposed agentic system designs addressagent identity, authorization, and delegation, but remain purely theoretical,without concrete implementation and evaluation. Most importantly, they do notprovide user-controlled agent management. To address this gap, we propose SAGA, a scalable Security Architecture forGoverning Agentic systems, that offers user oversight over their agents'lifecycle. In our design, users register their agents with a central entity,the Provider, that maintains agent contact information, user-defined accesscontrol policies, and helps agents enforce these policies on inter-agentcommunication. We introduce a cryptographic mechanism for deriving accesscontrol tokens, that offers fine-grained control over an agent's interactionwith other agents, providing formal security guarantees. We evaluate SAGA onseveral agentic tasks, using agents in different geolocations, and multipleon-device and cloud LLMs, demonstrating minimal performance overhead with noimpact on underlying task utility in a wide range of conditions. Ourarchitecture enables secure and trustworthy deployment of autonomous agents,accelerating the responsible adoption of this technology in sensitiveenvironments.</description><author>Georgios Syros, Anshuman Suri, Jacob Ginesin, Cristina Nita-Rotaru, Alina Oprea</author><pubDate>Fri, 29 Aug 2025 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.21034v2</guid></item><item><title>ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning</title><link>http://arxiv.org/abs/2412.00631v2</link><description>Instruction tuning has underscored the significant potential of largelanguage models (LLMs) in producing more human controllable and effectiveoutputs in various domains. In this work, we focus on the data selectionproblem for task-specific instruction tuning of LLMs. Prevailing methodsprimarily rely on the crafted similarity metrics to select training data thataligns with the test data distribution. The goal is to minimize instructiontuning loss on the test data, ultimately improving performance on the targettask. However, it has been widely observed that instruction tuning loss (i.e.,cross-entropy loss for next token prediction) in LLMs often fails to exhibit amonotonic relationship with actual task performance. This misalignmentundermines the effectiveness of current data selection methods fortask-specific instruction tuning. To address this issue, we introduce ROSE, anovel Reward-Oriented inStruction data sElection method which leveragespairwise preference loss as a reward signal to optimize data selection fortask-specific instruction tuning. Specifically, ROSE adapts an influenceformulation to approximate the influence of training data points relative to afew-shot preference validation set to select the most task-related trainingdata points. Experimental results show that by selecting just 5\% of thetraining data using ROSE, our approach can achieve competitive results comparedto fine-tuning with the full training dataset, and it surpasses otherstate-of-the-art data selection methods for task-specific instruction tuning.Our qualitative analysis further confirms the robust generalizability of ourmethod across multiple benchmark datasets and diverse model architectures.</description><author>Yang Wu, Huayi Zhang, Yizheng Jiao, Lin Ma, Xiaozhong Liu, Jinhong Yu, Dongyu Zhang, Dezhi Yu, Wei Xu</author><pubDate>Fri, 29 Aug 2025 17:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00631v2</guid></item><item><title>The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning</title><link>http://arxiv.org/abs/2508.21816v1</link><description>Context recognition (SR) is a fundamental task in computer vision that aimsto extract structured semantic summaries from images by identifying key eventsand their associated entities. Specifically, given an input image, the modelmust first classify the main visual events (verb classification), then identifythe participating entities and their semantic roles (semantic role labeling),and finally localize these entities in the image (semantic role localization).Existing methods treat verb classification as a single-label problem, but weshow through a comprehensive analysis that this formulation fails to addressthe inherent ambiguity in visual event recognition, as multiple verb categoriesmay reasonably describe the same image. This paper makes three keycontributions: First, we reveal through empirical analysis that verbclassification is inherently a multi-label problem due to the ubiquitoussemantic overlap between verb categories. Second, given the impracticality offully annotating large-scale datasets with multiple labels, we propose toreformulate verb classification as a single positive multi-label learning(SPMLL) problem - a novel perspective in SR research. Third, we design acomprehensive multi-label evaluation benchmark for SR that is carefullydesigned to fairly evaluate model performance in a multi-label setting. Toaddress the challenges of SPMLL, we futher develop the Graph Enhanced VerbMultilayer Perceptron (GE-VerbMLP), which combines graph neural networks tocapture label correlations and adversarial training to optimize decisionboundaries. Extensive experiments on real-world datasets show that our approachachieves more than 3\% MAP improvement while remaining competitive ontraditional top-1 and top-5 accuracy metrics.</description><author>Yiming Lin, Yuchen Niu, Shang Wang, Kaizhu Huang, Qiufeng Wang, Xiao-Bo Jin</author><pubDate>Fri, 29 Aug 2025 17:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21816v1</guid></item><item><title>Achieving Hilbert-Schmidt Independence Under RÃ©nyi Differential Privacy for Fair and Private Data Generation</title><link>http://arxiv.org/abs/2508.21815v1</link><description>As privacy regulations such as the GDPR and HIPAA and responsibilityframeworks for artificial intelligence such as the AI Act gain traction, theethical and responsible use of real-world data faces increasing constraints.Synthetic data generation has emerged as a promising solution to risk-awaredata sharing and model development, particularly for tabular datasets that arefoundational to sensitive domains such as healthcare. To address both privacyand fairness concerns in this setting, we propose FLIP (Fair LatentIntervention under Privacy guarantees), a transformer-based variationalautoencoder augmented with latent diffusion to generate heterogeneous tabulardata. Unlike the typical setup in fairness-aware data generation, we assume atask-agnostic setup, not reliant on a fixed, defined downstream task, thusoffering broader applicability. To ensure privacy, FLIP employs R\'enyidifferential privacy (RDP) constraints during training and addresses fairnessin the input space with RDP-compatible balanced sampling that accounts forgroup-specific noise levels across multiple sampling rates. In the latentspace, we promote fairness by aligning neuron activation patterns acrossprotected groups using Centered Kernel Alignment (CKA), a similarity measureextending the Hilbert-Schmidt Independence Criterion (HSIC). This alignmentencourages statistical independence between latent representations and theprotected feature. Empirical results demonstrate that FLIP effectively providessignificant fairness improvements for task-agnostic fairness and across diversedownstream tasks under differential privacy constraints.</description><author>Tobias Hyrup, Emmanouil Panagiotou, Arjun Roy, Arthur Zimek, Eirini Ntoutsi, Peter Schneider-Kamp</author><pubDate>Fri, 29 Aug 2025 17:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21815v1</guid></item><item><title>QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models</title><link>http://arxiv.org/abs/2508.21810v1</link><description>The growing scale of Large Language Models (LLMs) has necessitated thedevelopment of parameter-efficient fine-tuning techniques. Low-Rank Adaptation(LoRA) has emerged as a promising approach, reducing the number of trainableparameters by applying low-rank updates to pretrained weights. While standardLoRA learns both update factors directly, several recent variants firstinitialize those matrices via an SVD of the pretrained weights -- an operationthat can be expensive on large models and yields singular vectors that are notalways easy to interpret. In this work, we extract an orthonormal basis fromthe pretrained weight matrix using QR decomposition with column pivoting, andthen express the LoRA update as a linear combination of these basis vectors --training only the scalar coefficients, which imposes clear structure onadaptation and drastically reduces parameter count. Experiments across GLUEtasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,standard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singularvalue decomposition) with as few as 601 parameters -- a reduction of over 1000xcompared to full fine-tuning and 77x fewer than typical LoRA setups.</description><author>Jessica Liang, Anirudh Bharadwaj</author><pubDate>Fri, 29 Aug 2025 17:47:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21810v1</guid></item><item><title>Active Domain Knowledge Acquisition with 100-Dollar Budget: Enhancing LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains</title><link>http://arxiv.org/abs/2508.17202v2</link><description>Large Language Models (LLMs) have demonstrated an impressive level of generalknowledge. However, they often struggle in highly specialized andcost-sensitive domains such as drug discovery and rare disease research due tothe lack of expert knowledge. In this paper, we propose a novel framework(PU-ADKA) designed to efficiently enhance domain-specific LLMs by activelyengaging domain experts within a fixed budget. Unlike traditional fine-tuningapproaches, PU-ADKA selectively identifies and queries the most appropriateexpert from a team, taking into account each expert's availability, knowledgeboundaries, and consultation costs. We train PU-ADKA using simulations onPubMed data and validate it through both controlled expert interactions andreal-world deployment with a drug development team, demonstrating itseffectiveness in enhancing LLM performance in specialized domains under strictbudget constraints. In addition to outlining our methodological innovations andexperimental results, we introduce a new benchmark dataset, CKAD, forcost-effective LLM domain knowledge acquisition to foster further research inthis challenging area.</description><author>Yang Wu, Raha Moraffah, Rujing Yao, Jinhong Yu, Zhimin Tao, Xiaozhong Liu</author><pubDate>Fri, 29 Aug 2025 17:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17202v2</guid></item><item><title>VoCap: Video Object Captioning and Segmentation from Any Prompt</title><link>http://arxiv.org/abs/2508.21809v1</link><description>Understanding objects in videos in terms of fine-grained localization masksand detailed semantic properties is a fundamental task in video understanding.In this paper, we propose VoCap, a flexible video model that consumes a videoand a prompt of various modalities (text, box or mask), and produces aspatio-temporal masklet with a corresponding object-centric caption. As suchour model addresses simultaneously the tasks of promptable video objectsegmentation, referring expression segmentation, and object captioning. Sinceobtaining data for this task is tedious and expensive, we propose to annotatean existing large-scale segmentation dataset (SAV) with pseudo object captions.We do so by preprocessing videos with their ground-truth masks to highlight theobject of interest and feed this to a large Vision Language Model (VLM). For anunbiased evaluation, we collect manual annotations on the validation set. Wecall the resulting dataset SAV-Caption. We train our VoCap model at scale on aSAV-Caption together with a mix of other image and video datasets. Our modelyields state-of-the-art results on referring expression video objectsegmentation, is competitive on semi-supervised video object segmentation, andestablishes a benchmark for video object captioning. Our dataset will be madeavailable at https://github.com/google-deepmind/vocap.</description><author>Jasper Uijlings, Xingyi Zhou, Xiuye Gu, Arsha Nagrani, Anurag Arnab, Alireza Fathi, David Ross, Cordelia Schmid</author><pubDate>Fri, 29 Aug 2025 17:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21809v1</guid></item><item><title>Considerations for Estimating Causal Effects of Informatively Timed Treatments</title><link>http://arxiv.org/abs/2508.21804v1</link><description>Epidemiological studies are often concerned with estimating causal effects ofa sequence of treatment decisions on survival outcomes. In many settings,treatment decisions do not occur at fixed, pre-specified followup times.Rather, timing varies across subjects in ways that may be informative ofsubsequent treatment decisions and potential outcomes. Awareness of the issueand its potential solutions is lacking in the literature, which motivate thiswork. Here, we formalize the issue of informative timing, problems associatedwith ignoring it, and show how g-methods can be used to analyze sequentialtreatments that are informatively timed. As we describe, in such settings, thewaiting times between successive treatment decisions may be properly viewed asa time-varying confounders. Using synthetic examples, we illustrate howg-methods that do not adjust for these waiting times may be biased and howadjustment can be done in scenarios where patients may die or be censored inbetween treatments. We draw connections between adjustment and identificationwith discrete-time versus continuous-time models. Finally, we provideimplementation guidance and examples using publicly available software. Ourconcluding message is that 1) considering timing is important for validinference and 2) correcting for informative timing can be done with g-methodsthat adjust for waiting times between treatments as time-varying confounders.</description><author>Arman Oganisian</author><pubDate>Fri, 29 Aug 2025 17:32:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21804v1</guid></item><item><title>Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture</title><link>http://arxiv.org/abs/2508.21803v1</link><description>Accurate interpretation of clinical narratives is critical for patient care,but the complexity of these notes makes automation challenging. While LargeLanguage Models (LLMs) show promise, single-model approaches can lack therobustness required for high-stakes clinical tasks. We introduce acollaborative multi-agent system (MAS) that models a clinical consultation teamto address this gap. The system is tasked with identifying clinical problems byanalyzing only the Subjective (S) and Objective (O) sections of SOAP notes,simulating the diagnostic reasoning process of synthesizing raw data into anassessment. A Manager agent orchestrates a dynamically assigned team ofspecialist agents who engage in a hierarchical, iterative debate to reach aconsensus. We evaluated our MAS against a single-agent baseline on a curateddataset of 420 MIMIC-III notes. The dynamic multi-agent configurationdemonstrated consistently improved performance in identifying congestive heartfailure, acute kidney injury, and sepsis. Qualitative analysis of the agentdebates reveals that this structure effectively surfaces and weighs conflictingevidence, though it can occasionally be susceptible to groupthink. By modelinga clinical team's reasoning process, our system offers a promising path towardmore accurate, robust, and interpretable clinical decision support tools.</description><author>Yeawon Lee, Xiaoyang Wang, Christopher C. Yang</author><pubDate>Fri, 29 Aug 2025 17:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21803v1</guid></item><item><title>Tree-Guided Diffusion Planner</title><link>http://arxiv.org/abs/2508.21800v1</link><description>Planning with pretrained diffusion models has emerged as a promising approachfor solving test-time guided control problems. However, standard gradientguidance typically performs optimally under convex and differentiable rewardlandscapes, showing substantially reduced effectiveness in real-world scenariosinvolving non-convex objectives, non-differentiable constraints, andmulti-reward structures. Furthermore, recent supervised planning approachesrequire task-specific training or value estimators, which limits test-timeflexibility and zero-shot generalization. We propose a Tree-guided DiffusionPlanner (TDP), a zero-shot test-time planning framework that balancesexploration and exploitation through structured trajectory generation. We frametest-time planning as a tree search problem using a bi-level sampling process:(1) diverse parent trajectories are produced via training-free particleguidance to encourage broad exploration, and (2) sub-trajectories are refinedthrough fast conditional denoising guided by task objectives. TDP addresses thelimitations of gradient guidance by exploring diverse trajectory regions andharnessing gradient information across this expanded solution space using onlypretrained models and test-time reward signals. We evaluate TDP on threediverse tasks: maze gold-picking, robot arm block manipulation, and AntMazemulti-goal exploration. TDP consistently outperforms state-of-the-artapproaches on all tasks. The project page can be found at:tree-diffusion-planner.github.io.</description><author>Hyeonseong Jeon, Cheolhong Min, Jaesik Park</author><pubDate>Fri, 29 Aug 2025 17:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21800v1</guid></item><item><title>DynaMark: A Reinforcement Learning Framework for Dynamic Watermarking in Industrial Machine Tool Controllers</title><link>http://arxiv.org/abs/2508.21797v1</link><description>Industry 4.0's highly networked Machine Tool Controllers (MTCs) are primetargets for replay attacks that use outdated sensor data to manipulateactuators. Dynamic watermarking can reveal such tampering, but current schemesassume linear-Gaussian dynamics and use constant watermark statistics, makingthem vulnerable to the time-varying, partly proprietary behavior of MTCs. Weclose this gap with DynaMark, a reinforcement learning framework that modelsdynamic watermarking as a Markov decision process (MDP). It learns an adaptivepolicy online that dynamically adapts the covariance of a zero-mean Gaussianwatermark using available measurements and detector feedback, without needingsystem knowledge. DynaMark maximizes a unique reward function balancing controlperformance, energy consumption, and detection confidence dynamically. Wedevelop a Bayesian belief updating mechanism for real-time detection confidencein linear systems. This approach, independent of specific system assumptions,underpins the MDP for systems with linear dynamics. On a Siemens Sinumerik 828Dcontroller digital twin, DynaMark achieves a reduction in watermark energy by70% while preserving the nominal trajectory, compared to constant variancebaselines. It also maintains an average detection delay equivalent to onesampling interval. A physical stepper-motor testbed validates these findings,rapidly triggering alarms with less control performance decline and exceedingexisting benchmarks.</description><author>Navid Aftabi, Abhishek Hanchate, Satish Bukkapatnam, Dan Li</author><pubDate>Fri, 29 Aug 2025 17:24:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21797v1</guid></item><item><title>Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement</title><link>http://arxiv.org/abs/2507.08340v2</link><description>Deep learning has shown remarkable performance in integrating multimodal datafor survival prediction. However, existing multimodal methods mainly focus onsingle cancer types and overlook the challenge of generalization acrosscancers. In this work, we are the first to reveal that multimodal prognosismodels often generalize worse than unimodal ones in cross-cancer scenarios,despite the critical need for such robustness in clinical practice. To addressthis, we propose a new task: Cross-Cancer Single Domain Generalization forMultimodal Prognosis, which evaluates whether models trained on a single cancertype can generalize to unseen cancers. We identify two key challenges: degradedfeatures from weaker modalities and ineffective multimodal integration. Totackle these, we introduce two plug-and-play modules: Sparse Dirac InformationRebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIRmitigates the dominance of strong features by applying Bernoulli-basedsparsification and Dirac-inspired stabilization to enhance weaker modalitysignals. CADE, designed to synthesize the target domain distribution, fuseslocal morphological cues and global gene expression in latent space.Experiments on a four-cancer-type benchmark demonstrate superiorgeneralization, laying the foundation for practical, robust cross-cancermultimodal prognosis. Code is available athttps://github.com/HopkinsKwong/MCCSDG</description><author>Jia-Xuan Jiang, Jiashuai Liu, Hongtao Wu, Yifeng Wu, Zhong Wang, Qi Bi, Yefeng Zheng</author><pubDate>Fri, 29 Aug 2025 17:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.08340v2</guid></item><item><title>TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank</title><link>http://arxiv.org/abs/2508.21795v1</link><description>Anomaly detection, which aims to identify anomalies deviating from normalpatterns, is challenging due to the limited amount of normal data available.Unlike most existing unified methods that rely on carefully designed imagefeature extractors and memory banks to capture logical relationships betweenobjects, we introduce a text memory bank to enhance the detection of logicalanomalies. Specifically, we propose a Three-Memory framework for Unifiedstructural and logical Anomaly Detection (TMUAD). First, we build a class-leveltext memory bank for logical anomaly detection by the proposed logic-aware textextractor, which can capture rich logical descriptions of objects from inputimages. Second, we construct an object-level image memory bank that preservescomplete object contours by extracting features from segmented objects. Third,we employ visual encoders to extract patch-level image features forconstructing a patch-level memory bank for structural anomaly detection. Thesethree complementary memory banks are used to retrieve and compare normal imagesthat are most similar to the query image, compute anomaly scores at multiplelevels, and fuse them into a final anomaly score. By unifying structural andlogical anomaly detection through collaborative memory banks, TMUAD achievesstate-of-the-art performance across seven publicly available datasets involvingindustrial and medical domains. The model and code are available athttps://github.com/SIA-IDE/TMUAD.</description><author>Jiawei Liu, Jiahe Hou, Wei Wang, Jinsong Du, Yang Cong, Huijie Fan</author><pubDate>Fri, 29 Aug 2025 17:22:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21795v1</guid></item><item><title>COBRA-PPM: A Causal Bayesian Reasoning Architecture Using Probabilistic Programming for Robot Manipulation Under Uncertainty</title><link>http://arxiv.org/abs/2403.14488v4</link><description>Manipulation tasks require robots to reason about cause and effect wheninteracting with objects. Yet, many data-driven approaches lack causalsemantics and thus only consider correlations. We introduce COBRA-PPM, a novelcausal Bayesian reasoning architecture that combines causal Bayesian networksand probabilistic programming to perform interventional inference for robotmanipulation under uncertainty. We demonstrate its capabilities throughhigh-fidelity Gazebo-based experiments on an exemplar block stacking task,where it predicts manipulation outcomes with high accuracy (Pred Acc: 88.6%)and performs greedy next-best action selection with a 94.2% task success rate.We further demonstrate sim2real transfer on a domestic robot, showingeffectiveness in handling real-world uncertainty from sensor noise andstochastic actions. Our generalised and extensible framework supports a widerange of manipulation scenarios and lays a foundation for future work at theintersection of robotics and causality.</description><author>Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze</author><pubDate>Fri, 29 Aug 2025 17:17:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14488v4</guid></item><item><title>MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction</title><link>http://arxiv.org/abs/2508.21793v1</link><description>Healthcare systems generate diverse multimodal data, including ElectronicHealth Records (EHR), clinical notes, and medical images. Effectivelyleveraging this data for clinical prediction is challenging, particularly asreal-world samples often present with varied or incomplete modalities. Existingapproaches typically require complete modality data or rely on manual selectionstrategies, limiting their applicability in real-world clinical settings wheredata availability varies across patients and institutions. To address theselimitations, we propose MoE-Health, a novel Mixture of Experts frameworkdesigned for robust multimodal fusion in healthcare prediction. MoE-Healtharchitecture is specifically developed to handle samples with differingmodalities and improve performance on critical clinical tasks. By leveragingspecialized expert networks and a dynamic gating mechanism, our approachdynamically selects and combines relevant experts based on available datamodalities, enabling flexible adaptation to varying data availabilityscenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three criticalclinical prediction tasks: in-hospital mortality prediction, long length ofstay, and hospital readmission prediction. Experimental results demonstratethat MoE-Health achieves superior performance compared to existing multimodalfusion methods while maintaining robustness across different modalityavailability patterns. The framework effectively integrates multimodalinformation, offering improved predictive performance and robustness inhandling heterogeneous and incomplete healthcare data, making it particularlysuitable for deployment in diverse healthcare environments with heterogeneousdata availability.</description><author>Xiaoyang Wang, Christopher C. Yang</author><pubDate>Fri, 29 Aug 2025 17:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21793v1</guid></item><item><title>Strategic resource allocation in memory encoding: An efficiency principle shaping language processing</title><link>http://arxiv.org/abs/2503.14728v2</link><description>How is the limited capacity of working memory efficiently used to supporthuman linguistic behaviors? In this paper, we propose Strategic ResourceAllocation (SRA) as an efficiency principle for memory encoding in sentenceprocessing. The idea is that working memory resources are dynamically andstrategically allocated to prioritize novel and unexpected information. From aresource-rational perspective, we argue that SRA is the principled solution toa computational problem posed by two functional assumptions about workingmemory, namely its limited capacity and its noisy representation. Specifically,working memory needs to minimize the retrieval error of past inputs under theconstraint of limited memory resources, an optimization problem whose solutionis to allocate more resources to encode more surprising inputs with higherprecision. One of the critical consequences of SRA is that surprising inputsare encoded with enhanced representations, and therefore are less susceptibleto memory decay and interference. Empirically, through naturalistic corpusdata, we find converging evidence for SRA in the context of dependency localityfrom both production and comprehension, where non-local dependencies with lesspredictable antecedents are associated with reduced locality effect. However,our results also reveal considerable cross-linguistic variability, suggestingthe need for a closer examination of how SRA, as a domain-general memoryefficiency principle, interacts with language-specific phrase structures. SRAhighlights the critical role of representational uncertainty in understandingmemory encoding. It also reimages the effects of surprisal and entropy onprocessing difficulty from the perspective of efficient memory encoding.</description><author>Weijie Xu, Richard Futrell</author><pubDate>Fri, 29 Aug 2025 17:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.14728v2</guid></item><item><title>DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding</title><link>http://arxiv.org/abs/2508.07313v2</link><description>Understanding multi-page documents poses a significant challenge formultimodal large language models (MLLMs), as it requires fine-grained visualcomprehension and multi-hop reasoning across pages. While prior work hasexplored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,its application to multi-page document understanding remains underexplored. Inthis paper, we introduce DocR1, an MLLM trained with a novel RL framework,Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-awarereward mechanism that promotes a coarse-to-fine reasoning strategy, guiding themodel to first retrieve relevant pages before generating answers. This trainingparadigm enables us to build high-quality models with limited supervision. Tosupport this, we design a two-stage annotation pipeline and a curriculumlearning strategy, based on which we construct two datasets: EviBench, ahigh-quality training set with 4.8k examples, and ArxivFullQA, an evaluationbenchmark with 8.6k QA pairs based on scientific papers. Extensive experimentsacross a wide range of benchmarks demonstrate that DocR1 achievesstate-of-the-art performance on multi-page tasks, while consistentlymaintaining strong results on single-page benchmarks.</description><author>Junyu Xiong, Yonghui Wang, Weichao Zhao, Chenyu Liu, Bing Yin, Wengang Zhou, Houqiang Li</author><pubDate>Fri, 29 Aug 2025 17:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07313v2</guid></item><item><title>Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models</title><link>http://arxiv.org/abs/2412.06748v2</link><description>A key component of building safe and reliable language models is enabling themodels to appropriately refuse to follow certain instructions or answer certainquestions. We may want models to output refusal messages for various categoriesof user queries, for example, ill-posed questions, instructions for committingillegal acts, or queries which require information past the model's knowledgehorizon. Engineering models that refuse to answer such questions is complicatedby the fact that an individual may want their model to exhibit varying levelsof sensitivity for refusing queries of various categories, and different usersmay want different refusal rates. The current default approach involvestraining multiple models with varying proportions of refusal messages from eachcategory to achieve the desired refusal rates, which is computationallyexpensive and may require training a new model to accommodate each user'sdesired preference over refusal rates. To address these challenges, we proposerefusal tokens, one such token for each refusal category or a single refusaltoken, which are prepended to the model's responses during training. We thenshow how to increase or decrease the probability of generating the refusaltoken for each category during inference to steer the model's refusal behavior.Refusal tokens enable controlling a single model's refusal rates without theneed of any further fine-tuning, but only by selectively intervening duringgeneration.</description><author>Neel Jain, Aditya Shrivastava, Chenyang Zhu, Daben Liu, Alfy Samuel, Ashwinee Panda, Anoop Kumar, Micah Goldblum, Tom Goldstein</author><pubDate>Fri, 29 Aug 2025 17:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06748v2</guid></item><item><title>Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval</title><link>http://arxiv.org/abs/2508.21788v1</link><description>Large language models (LLMs) rely heavily on web-scale datasets like CommonCrawl, which provides over 80\% of training data for some modern models.However, the indiscriminate nature of web crawling raises challenges in dataquality, safety, and ethics. Despite the critical importance of training dataquality, prior research on harmful content has been limited to small samplesdue to computational constraints. This project presents a framework forindexing and analyzing LLM training datasets using an ElasticSearch-basedpipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),achieving fast query performance--most searches in milliseconds, all under 2seconds. Our work demonstrates real-time dataset analysis, offering practicaltools for safer, more accountable AI systems.</description><author>InÃ©s Altemir Marinas, Anastasiia Kucherenko, Andrei Kucharavy</author><pubDate>Fri, 29 Aug 2025 17:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21788v1</guid></item><item><title>PiCSAR: Probabilistic Confidence Selection And Ranking</title><link>http://arxiv.org/abs/2508.21787v1</link><description>Best-of-n sampling improves the accuracy of large language models (LLMs) andlarge reasoning models (LRMs) by generating multiple candidate solutions andselecting the one with the highest reward. The key challenge for reasoningtasks is designing a scoring function that can identify correct reasoningchains without access to ground-truth answers. We propose ProbabilisticConfidence Selection And Ranking (PiCSAR): a simple, training-free method thatscores each candidate generation using the joint log-likelihood of thereasoning and final answer. The joint log-likelihood of the reasoning and finalanswer naturally decomposes into reasoning confidence and answer confidence.PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in16 out of 20 comparisons. Our analysis reveals that correct reasoning chainsexhibit significantly higher reasoning and answer confidence, justifying theeffectiveness of PiCSAR.</description><author>Joshua Ong Jun Leang, Zheng Zhao, Aryo Pradipta Gema, Sohee Yang, Wai-Chung Kwan, Xuanli He, Wenda Li, Pasquale Minervini, Eleonora Giunchiglia, Shay B. Cohen</author><pubDate>Fri, 29 Aug 2025 17:03:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21787v1</guid></item><item><title>Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling</title><link>http://arxiv.org/abs/2508.21785v1</link><description>Heart rate prediction is vital for personalized health monitoring andfitness, while it frequently faces a critical challenge when deploying inreal-world: data heterogeneity. We classify it in two key dimensions: sourceheterogeneity from fragmented device markets with varying feature sets, anduser heterogeneity reflecting distinct physiological patterns acrossindividuals and activities. Existing methods either discard device-specificinformation, or fail to model user-specific differences, limiting theirreal-world performance. To address this, we propose a framework that learnslatent representations agnostic to both heterogeneity, enabling downstreampredictors to work consistently under heterogeneous data patterns.Specifically, we introduce a random feature dropout strategy to handle sourceheterogeneity, making the model robust to various feature sets. To manage userheterogeneity, we employ a time-aware attention module to capture long-termphysiological traits and use a contrastive learning objective to build adiscriminative representation space. To reflect the heterogeneous nature ofreal-world data, we created and publicly released a new benchmark dataset,ParroTao. Evaluations on both ParroTao and the public FitRec dataset show thatour model significantly outperforms existing baselines by 17% and 15%,respectively. Furthermore, analysis of the learned representations demonstratestheir strong discriminative power, and one downstream application task confirmthe practical value of our model.</description><author>Peng Yang, Zhengdong Huang, Zicheng Xie, Wentao Tian, Jingyu Liu, Lunhong Dong</author><pubDate>Fri, 29 Aug 2025 17:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21785v1</guid></item><item><title>FORGE: Foundational Optimization Representations from Graph Embeddings</title><link>http://arxiv.org/abs/2508.20330v2</link><description>Combinatorial optimization problems are ubiquitous in science andengineering, yet learning-based approaches to accelerate their solution oftenrequire solving a large number of hard-to-solve optimization instances tocollect training data, incurring significant computational overhead. Existingmethods require training dedicated models for each problem distribution foreach downstream task, severely limiting their scalability and generalization.In this work, we introduce Forge, a method of pre-training a vector-quantizedgraph autoencoder on a large and diverse collection of mixed-integerprogramming (MIP) instances in an unsupervised fashion without dependency ontheir solution. The vector quantization process creates discrete codeassignments that act as a vocabulary to represent optimization instances. Weevaluate our approach under both supervised and unsupervised settings. For theunsupervised setting, we demonstrate that Forge embeddings effectivelydifferentiate and cluster unseen instances. For the supervised setting, wefine-tuneForge embeddings and show that a single model predicts both thevariables for warm-starts and integrality gaps for cut-generation acrossmultiple problem type distributions. Both predictions help improve performanceof a state-of-the-art, commercial optimization solver. Finally, we release ourcode and pre-trained Forge weights to encourage further research and practicaluse of instance-level MIP embeddings at https://github.com/skadio/forge/.</description><author>Zohair Shafi, Serdar Kadioglu</author><pubDate>Fri, 29 Aug 2025 16:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20330v2</guid></item><item><title>Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight</title><link>http://arxiv.org/abs/2508.21777v1</link><description>Introduction: Large language models (LLM) have shown great potential inclinical decision support. GPT-5 is a novel LLM system that has beenspecifically marketed towards oncology use. Methods: Performance was assessed using two complementary benchmarks: (i) theACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300multiple-choice items, and (ii) a curated set of 60 authentic radiationoncologic vignettes representing diverse disease sites and treatmentindications. For the vignette evaluation, GPT-5 was instructed to generateconcise therapeutic plans. Four board-certified radiation oncologists ratedcorrectness, comprehensiveness, and hallucinations. Inter-rater reliability wasquantified using Fleiss' \k{appa}. Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains weremost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5'streatment recommendations were rated highly for correctness (mean 3.24/4, 95%CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).Hallucinations were rare with no case reaching majority consensus for theirpresence. Inter-rater agreement was low (Fleiss' \k{appa} 0.083 forcorrectness), reflecting inherent variability in clinical judgment. Errorsclustered in complex scenarios requiring precise trial knowledge or detailedclinical adaptation. Discussion: GPT-5 clearly outperformed prior model variants on the radiationoncology multiple-choice benchmark. Although GPT-5 exhibited favorableperformance in generating real-world radiation oncology treatmentrecommendations, correctness ratings indicate room for further improvement.While hallucinations were infrequent, the presence of substantive errorsunderscores that GPT-5-generated recommendations require rigorous expertoversight before clinical implementation.</description><author>Ugur Dinc, Jibak Sarkar, Philipp Schubert, Sabine Semrau, Thomas Weissmann, Andre Karius, Johann Brand, Bernd-Niklas Axer, Ahmed Gomaa, Pluvio Stephan, Ishita Sheth, Sogand Beirami, Annette Schwarz, Udo Gaipl, Benjamin Frey, Christoph Bert, Stefanie Corradini, Rainer Fietkau, Florian Putz</author><pubDate>Fri, 29 Aug 2025 16:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21777v1</guid></item><item><title>A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI</title><link>http://arxiv.org/abs/2508.21775v1</link><description>Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI iscritical for clinical workflows but is hindered by poor tumor-tissue contrastand a scarcity of annotated data. This paper details our submission to thePANTHER challenge, addressing both diagnostic T1-weighted (Task 1) andtherapeutic T2-weighted (Task 2) segmentation. Our approach is built upon thennU-Net framework and leverages a deep, multi-stage cascaded pre-trainingstrategy, starting from a general anatomical foundation model and sequentiallyfine-tuning on CT pancreatic lesion datasets and the target MRI modalities.Through extensive five-fold cross-validation, we systematically evaluated dataaugmentation schemes and training schedules. Our analysis revealed a criticaltrade-off, where aggressive data augmentation produced the highest volumetricaccuracy, while default augmentations yielded superior boundary precision(achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1).For our final submission, we exploited this finding by constructing custom,heterogeneous ensembles of specialist models, essentially creating a mix ofexperts. This metric-aware ensembling strategy proved highly effective,achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523for Task 2. Our work presents a robust methodology for developing specialized,high-performance models in the context of limited data and complex medicalimaging tasks (Team MIC-DKFZ).</description><author>Omer Faruk Durugol, Maximilian Rokuss, Yannick Kirchhoff, Klaus H. Maier-Hein</author><pubDate>Fri, 29 Aug 2025 16:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21775v1</guid></item><item><title>Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering</title><link>http://arxiv.org/abs/2508.21773v1</link><description>We propose a realistic scenario for the unsupervised video learning whereneither task boundaries nor labels are provided when learning a succession oftasks. We also provide a non-parametric learning solution for theunder-explored problem of unsupervised video continual learning. Videosrepresent a complex and rich spatio-temporal media information, widely used inmany applications, but which have not been sufficiently explored inunsupervised continual learning. Prior studies have only focused on supervisedcontinual learning, relying on the knowledge of labels and task boundaries,while having labeled data is costly and not practical. To address this gap, westudy the unsupervised video continual learning (uVCL). uVCL raises morechallenges due to the additional computational and memory requirements ofprocessing videos when compared to images. We introduce a general benchmarkexperimental protocol for uVCL by considering the learning of unstructuredvideo data categories during each task. We propose to use the Kernel DensityEstimation (KDE) of deep embedded video features extracted by unsupervisedvideo transformer networks as a non-parametric probabilistic representation ofthe data. We introduce a novelty detection criterion for the incoming new taskdata, dynamically enabling the expansion of memory clusters, aiming to capturenew knowledge when learning a succession of tasks. We leverage the use oftransfer learning from the previous tasks as an initial state for the knowledgetransfer to the current learning task. We found that the proposed methodologysubstantially enhances the performance of the model when successively learningmany tasks. We perform in-depth evaluations on three standard video actionrecognition datasets, including UCF101, HMDB51, and Something-to-Something V2,without using any labels or class boundaries.</description><author>Nattapong Kurpukdee, Adrian G. Bors</author><pubDate>Fri, 29 Aug 2025 16:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21773v1</guid></item><item><title>CHaRM: Conditioned Heatmap Regression Methodology for Accurate and Fast Dental Landmark Localization</title><link>http://arxiv.org/abs/2501.13073v5</link><description>Identifying anatomical landmarks in 3D dental models is essential fororthodontic treatment, yet manual placement is labor-intensive and requiresexpert knowledge. While machine learning methods have been proposed forautomatic landmark detection in 3D Intraoral Scans (IOS), none provide a fullyend-to-end solution that avoids costly tooth segmentation. We present CHaRM (Conditioned Heatmap Regression Methodology), the firstfully end-to-end deep learning approach for tooth landmark detection in 3D IOS.CHaRM integrates four components: a point cloud encoder, a decoder with aheatmap regression head, a teeth-presence classification head, and the novelCHaR module. The CHaR module leverages teeth-presence information to adapt tomissing teeth, improving detection accuracy in complex dental cases. Unliketwo-stage workflows that segment teeth before landmarking, CHaRM operatesdirectly on IOS point clouds, reducing complexity, avoiding error propagation,and lowering computational cost. We evaluated CHaRM with five point cloud learning backbones onIOSLandmarks-1k, a new dataset of 1,214 annotated 3D dental models. Both thedataset and code will be publicly released to address the scarcity of open datain orthodontics and foster reproducible research. CHaRM with PointMLP, named CHaRNet, achieved the best accuracy andefficiency. Compared to state-of-the-art methods (TSMDL and ALIIOS), CHaRNetreduced mean Euclidean distance error to 0.56 mm on standard dental models and1.12 mm across all dentition type, while delivering up to 14.8x fasterinference on GPU. This end-to-end approach streamlines orthodontic workflows,enhances the precision of 3D IOS analysis, and enables efficientcomputer-assisted treatment planning.</description><author>JosÃ© RodrÃ­guez-Ortega, Francisco PÃ©rez-HernÃ¡ndez, Siham Tabik</author><pubDate>Fri, 29 Aug 2025 16:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13073v5</guid></item><item><title>UniMLR: Modeling Implicit Class Significance for Multi-Label Ranking</title><link>http://arxiv.org/abs/2508.21772v1</link><description>Existing multi-label ranking (MLR) frameworks only exploit informationdeduced from the bipartition of labels into positive and negative sets.Therefore, they do not benefit from ranking among positive labels, which is thenovel MLR approach we introduce in this paper. We propose UniMLR, a new MLRparadigm that models implicit class relevance/significance values asprobability distributions using the ranking among positive labels, rather thantreating them as equally important. This approach unifies ranking andclassification tasks associated with MLR. Additionally, we address thechallenges of scarcity and annotation bias in MLR datasets by introducing eightsynthetic datasets (Ranked MNISTs) generated with varyingsignificance-determining factors, providing an enriched and controllableexperimental environment. We statistically demonstrate that our methodaccurately learns a representation of the positive rank order, which isconsistent with the ground truth and proportional to the underlyingsignificance values. Finally, we conduct comprehensive empirical experiments onboth real-world and synthetic datasets, demonstrating the value of our proposedframework.</description><author>V. Bugra Yesilkaynak, Emine Dari, Alican Mertan, Gozde Unal</author><pubDate>Fri, 29 Aug 2025 16:44:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21772v1</guid></item><item><title>What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge</title><link>http://arxiv.org/abs/2508.08344v2</link><description>Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is anincreasingly explored approach for combining the reasoning capabilities oflarge language models with the structured evidence of knowledge graphs.However, current evaluation practices fall short: existing benchmarks ofteninclude questions that can be directly answered using existing triples in KG,making it unclear whether models perform reasoning or simply retrieve answersdirectly. Moreover, inconsistent evaluation metrics and lenient answer matchingcriteria further obscure meaningful comparisons. In this work, we introduce ageneral method for constructing benchmarks, together with an evaluationprotocol, to systematically assess KG-RAG methods under knowledgeincompleteness. Our empirical results show that current KG-RAG methods havelimited reasoning ability under missing knowledge, often rely on internalmemorization, and exhibit varying degrees of generalization depending on theirdesign.</description><author>Dongzhuoran Zhou, Yuqicheng Zhu, Xiaxia Wang, Hongkuan Zhou, Yuan He, Jiaoyan Chen, Steffen Staab, Evgeny Kharlamov</author><pubDate>Fri, 29 Aug 2025 16:43:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.08344v2</guid></item><item><title>What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos</title><link>http://arxiv.org/abs/2508.21770v1</link><description>Humans usually show exceptional generalisation and discovery ability in theopen world, when being shown uncommon new concepts. Whereas most existingstudies in the literature focus on common typical data from closed sets,open-world novel discovery is under-explored in videos. In this paper, we areinterested in asking: \textit{What if atypical unusual videos are exposed inthe learning process?} To this end, we collect a new video dataset consistingof various types of unusual atypical data (\eg sci-fi, animation, \etc). Tostudy how such atypical data may benefit open-world learning, we feed them intothe model training process for representation learning. Focusing on three keytasks in open-world learning: out-of-distribution (OOD) detection, novelcategory discovery (NCD), and zero-shot action recognition (ZSAR), we foundthat even straightforward learning approaches with atypical data consistentlyimprove performance across various settings. Furthermore, we found thatincreasing the categorical diversity of the atypical samples further boosts OODdetection performance. Additionally, in the NCD task, using a smaller yet moresemantically diverse set of atypical samples leads to better performancecompared to using a larger but more typical dataset. In the ZSAR setting, thesemantic diversity of atypical videos helps the model generalise better tounseen action classes. These observations in our extensive experimentalevaluations reveal the benefits of atypical videos for visual representationlearning in the open world, together with the newly proposed dataset,encouraging further studies in this direction.</description><author>Qiyue Sun, Qiming Huang, Yang Yang, Hongjun Wang, Jianbo Jiao</author><pubDate>Fri, 29 Aug 2025 16:43:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21770v1</guid></item><item><title>A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Disease Detection from Retinal Fundus Images</title><link>http://arxiv.org/abs/2504.08481v3</link><description>In many medical imaging tasks, convolutional neural networks (CNNs)efficiently extract local features hierarchically. More recently, visiontransformers (ViTs) have gained popularity, using self-attention mechanisms tocapture global dependencies, but lacking the inherent spatial localization ofconvolutions. Therefore, hybrid models combining CNNs and ViTs have beendeveloped to combine the strengths of both architectures. However, such hybridmodels are difficult to interpret, which hinders their application in medicalimaging. In this work, we introduce an interpretable-by-design hybrid fullyconvolutional CNN-Transformer architecture for retinal disease detection.Unlike widely used post-hoc saliency methods for ViTs, our approach generatesfaithful and localized evidence maps that directly reflect the mode's decisionprocess. We evaluated our method on two medical tasks focused on diseasedetection using color fundus images. Our model achieves state-of-the-artpredictive performance compared to black-box and interpretable models andprovides class-specific sparse evidence maps in a single forward pass. The codeis available at:https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer.</description><author>Kerol Djoumessi, Samuel Ofosu Mensah, Philipp Berens</author><pubDate>Fri, 29 Aug 2025 16:43:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.08481v3</guid></item><item><title>Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</title><link>http://arxiv.org/abs/2508.21769v1</link><description>Evaluating domain generalization (DG) for foundational models like CLIP ischallenging, as web-scale pretraining data potentially covers many existingbenchmarks. Consequently, current DG evaluation may neither be sufficientlychallenging nor adequately test genuinely unseen data scenarios. To betterassess the performance of CLIP on DG in-the-wild, a scenario where CLIPencounters challenging unseen data, we consider two approaches: (1) evaluatingon 33 diverse datasets with quantified out-of-distribution (OOD) scores afterfine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'some domains as an approximation. We observe that CLIP's performancedeteriorates significantly on more OOD datasets. To address this, we presentCLIP-DCA (Disentangling Classification from enhanced domain Awarerepresentations). Our approach is motivated by the observation that whilestandard domain invariance losses aim to make representations domain-invariant,this can be harmful to foundation models by forcing the discarding ofdomain-aware representations beneficial for generalization. We insteadhypothesize that enhancing domain awareness is a prerequisite for effectivedomain-invariant classification in foundation models. CLIP-DCA identifies andenhances domain awareness within CLIP's encoders using a separate domain headand synthetically generated diverse domain data. Simultaneously, it encouragesdomain-invariant classification through disentanglement from the domainfeatures. CLIP-DCA shows significant improvements within this challengingevaluation compared to existing methods, particularly on datasets that are moreOOD.</description><author>Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu</author><pubDate>Fri, 29 Aug 2025 16:43:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21769v1</guid></item><item><title>UItron: Foundational GUI Agent with Advanced Perception and Planning</title><link>http://arxiv.org/abs/2508.21767v1</link><description>GUI agent aims to enable automated operations on Mobile/PC devices, which isan important task toward achieving artificial general intelligence. The rapidadvancement of VLMs accelerates the development of GUI agents, owing to theirpowerful capabilities in visual understanding and task planning. However,building a GUI agent remains a challenging task due to the scarcity ofoperation trajectories, the availability of interactive infrastructure, and thelimitation of initial capabilities in foundation models. In this work, weintroduce UItron, an open-source foundational model for automatic GUI agents,featuring advanced GUI perception, grounding, and planning capabilities. UItronhighlights the necessity of systemic data engineering and interactiveinfrastructure as foundational components for advancing GUI agent development.It not only systematically studies a series of data engineering strategies toenhance training effects, but also establishes an interactive environmentconnecting both Mobile and PC devices. In training, UItron adopts supervisedfinetuning over perception and planning tasks in various GUI scenarios, andthen develop a curriculum reinforcement learning framework to enable complexreasoning and exploration for online environments. As a result, UItron achievessuperior performance in benchmarks of GUI perception, grounding, and planning.In particular, UItron highlights the interaction proficiency with top-tierChinese mobile APPs, as we identified a general lack of Chinese capabilitieseven in state-of-the-art solutions. To this end, we manually collect over onemillion steps of operation trajectories across the top 100 most popular apps,and build the offline and online agent evaluation environments. Experimentalresults demonstrate that UItron achieves significant progress in Chinese appscenarios, propelling GUI agents one step closer to real-world application.</description><author>Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang, Yingjie Chu, Yuzhi He, Lin Ma</author><pubDate>Fri, 29 Aug 2025 16:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21767v1</guid></item><item><title>Documenting Deployment with Fabric: A Repository of Real-World AI Governance</title><link>http://arxiv.org/abs/2508.14119v4</link><description>Artificial intelligence (AI) is increasingly integrated into society, fromfinancial services and traffic management to creative writing. Academicliterature on the deployment of AI has mostly focused on the risks and harmsthat result from the use of AI. We introduce Fabric, a publicly availablerepository of deployed AI use cases to outline their governance mechanisms.Through semi-structured interviews with practitioners, we collect an initialset of 20 AI use cases. In addition, we co-design diagrams of the AI workflowwith the practitioners. We discuss the oversight mechanisms and guardrails usedin practice to safeguard AI use. The Fabric repository includes visual diagramsof AI use cases and descriptions of the deployed systems. Using the repository,we surface gaps in governance and find common patterns in human oversight ofdeployed AI systems. We intend for Fabric to serve as an extendable, evolvingtool for researchers to study the effectiveness of AI governance.</description><author>Mackenzie Jorgensen, Kendall Brogle, Katherine M. Collins, Lujain Ibrahim, Arina Shah, Petra Ivanovic, Noah Broestl, Gabriel Piles, Paul Dongha, Hatim Abdulhussein, Adrian Weller, Jillian Powers, Umang Bhatt</author><pubDate>Fri, 29 Aug 2025 16:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14119v4</guid></item><item><title>Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods under Knowledge Incompleteness</title><link>http://arxiv.org/abs/2504.05163v2</link><description>Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a techniquethat enhances Large Language Model (LLM) inference in tasks like QuestionAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).However, real-world KGs are often incomplete, meaning that essentialinformation for answering questions may be missing. Existing benchmarks do notadequately capture the impact of KG incompleteness on KG-RAG performance. Inthis paper, we systematically evaluate KG-RAG methods under incomplete KGs byremoving triples using different methods and analyzing the resulting effects.We demonstrate that KG-RAG methods are sensitive to KG incompleteness,highlighting the need for more robust approaches in realistic settings.</description><author>Dongzhuoran Zhou, Yuqicheng Zhu, Xiaxia Wang, Yuan He, Jiaoyan Chen, Steffen Staab, Evgeny Kharlamov</author><pubDate>Fri, 29 Aug 2025 16:38:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05163v2</guid></item><item><title>Reasoning-Intensive Regression</title><link>http://arxiv.org/abs/2508.21762v1</link><description>AI researchers and practitioners increasingly apply large language models(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducingsubtle numerical properties from text. Unlike standard language regressiontasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hocproblems like rubric-based scoring or domain-specific retrieval, where muchdeeper analysis of text is required while only limited task-specific trainingdata and computation are available. We cast three realistic problems as RiRtasks to establish an initial benchmark, and use that to test our hypothesisthat prompting frozen LLMs and finetuning Transformer encoders via gradientdescent will both often struggle in RiR. We then propose MENTAT, a simple andlightweight method that combines batch-reflective prompt optimization withneural ensemble learning. MENTAT achieves up to 65% improvement over bothbaselines, though substantial room remains for future advances in RiR.</description><author>Diane Tchuindjo, Omar Khattab</author><pubDate>Fri, 29 Aug 2025 16:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21762v1</guid></item><item><title>Learning from Silence and Noise for Visual Sound Source Localization</title><link>http://arxiv.org/abs/2508.21761v1</link><description>Visual sound source localization is a fundamental perception task that aimsto detect the location of sounding sources in a video given its audio. Despiterecent progress, we identify two shortcomings in current methods: 1) mostapproaches perform poorly in cases with low audio-visual semanticcorrespondence such as silence, noise, and offscreen sounds, i.e. in thepresence of negative audio; and 2) most prior evaluations are limited topositive cases, where both datasets and metrics convey scenarios with a singlevisible sound source in the scene. To address this, we introduce three keycontributions. First, we propose a new training strategy that incorporatessilence and noise, which improves performance in positive cases, while beingmore robust against negative sounds. Our resulting self-supervised model,SSL-SaN, achieves state-of-the-art performance compared to otherself-supervised models, both in sound localization and cross-modal retrieval.Second, we propose a new metric that quantifies the trade-off between alignmentand separability of auditory and visual features across positive and negativeaudio-visual pairs. Third, we present IS3+, an extended and improved version ofthe IS3 synthetic dataset with negative audio. Our data, metrics and code are available on thehttps://xavijuanola.github.io/SSL-SaN/.</description><author>Xavier Juanola, Giovana Morais, Magdalena Fuentes, Gloria Haro</author><pubDate>Fri, 29 Aug 2025 16:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21761v1</guid></item><item><title>Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning</title><link>http://arxiv.org/abs/2507.08730v4</link><description>Modern configurable software systems need to learn models that correlateconfiguration and performance. However, when the system operates in dynamicenvironments, the workload variations, hardware changes, and system updateswill inevitably introduce concept drifts at different levels - global drifts,which reshape the performance landscape of the entire configuration space; andlocal drifts, which only affect certain sub-regions of that space. As such,existing offline and transfer learning approaches can struggle to adapt tothese implicit and unpredictable changes in real-time, rendering configurationperformance learning challenging. To address this, we propose DHDA, an onlineconfiguration performance learning framework designed to capture and adapt tothese drifts at different levels. The key idea is that DHDA adapts to both thelocal and global drifts using dually hierarchical adaptation: at the upperlevel, we redivide the data into different divisions, within each of which thelocal model is retrained, to handle global drifts only when necessary. At thelower level, the local models of the divisions can detect local drifts andadapt themselves asynchronously. To balance responsiveness and efficiency, DHDAcombines incremental updates with periodic full retraining to minimizeredundant computation when no drifts are detected. Through evaluating eightsoftware systems and against state-of-the-art approaches, we show that DHDAachieves considerably better accuracy and can effectively adapt to drifts withup to 2x improvements, while incurring reasonable overhead and is able toimprove different local models in handling concept drift.</description><author>Zezhen Xiang, Jingzhi Gong, Tao Chen</author><pubDate>Fri, 29 Aug 2025 16:25:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.08730v4</guid></item><item><title>Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective</title><link>http://arxiv.org/abs/2506.19028v4</link><description>Large Language Models (LLMs) often generate responses with inherent biases,undermining their reliability in real-world applications. Existing evaluationmethods often overlook biases in long-form responses and the intrinsicvariability of LLM outputs. To address these challenges, we propose FiSCo(Fine-grained Semantic Comparison), a novel statistical framework to evaluategroup-level fairness in LLMs by detecting subtle semantic differences inlong-form responses across demographic groups. Unlike prior work focusing onsentiment or token-level comparisons, FiSCo goes beyond surface-level analysisby operating at the claim level, leveraging entailment checks to assess theconsistency of meaning across responses. We decompose model outputs intosemantically distinct claims and apply statistical hypothesis testing tocompare inter- and intra-group similarities, enabling robust detection ofsubtle biases. We formalize a new group counterfactual fairness definition andvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,race, and age. Experiments show that FiSCo more reliably identifies nuancedbiases while reducing the impact of stochastic LLM variability, outperformingvarious evaluation metrics.</description><author>Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, Chandan K. Reddy</author><pubDate>Fri, 29 Aug 2025 16:20:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.19028v4</guid></item><item><title>Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications</title><link>http://arxiv.org/abs/2508.15008v2</link><description>The deployment of Quantized Neural Networks (QNNs) on resource-constraineddevices, such as microcontrollers, has introduced significant challenges inbalancing model performance, computational complexity, and memory constraints.Tiny Machine Learning (TinyML) addresses these issues by integratingadvancements across machine learning algorithms, hardware acceleration, andsoftware optimization to efficiently run deep neural networks on embeddedsystems. This survey presents a hardware-centric introduction to quantization,systematically reviewing essential quantization techniques employed toaccelerate deep learning models for embedded applications. In particular,further emphasis is placed on the critical trade-offs between model performanceand hardware capabilities. The survey further evaluates existing softwareframeworks and hardware platforms designed specifically for supporting QNNexecution on microcontrollers. Moreover, we provide an analysis of the currentchallenges and an outline of promising future directions in the rapidlyevolving domain of QNN deployment.</description><author>Hamza A. Abushahla, Dara Varam, Ariel J. N. Panopio, Mohamed I. AlHajri</author><pubDate>Fri, 29 Aug 2025 16:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15008v2</guid></item><item><title>WebInject: Prompt Injection Attack to Web Agents</title><link>http://arxiv.org/abs/2505.11717v3</link><description>Multi-modal large language model (MLLM)-based web agents interact withwebpage environments by generating actions based on screenshots of thewebpages. In this work, we propose WebInject, a prompt injection attack thatmanipulates the webpage environment to induce a web agent to perform anattacker-specified action. Our attack adds a perturbation to the raw pixelvalues of the rendered webpage. After these perturbed pixels are mapped into ascreenshot, the perturbation induces the web agent to perform theattacker-specified action. We formulate the task of finding the perturbation asan optimization problem. A key challenge in solving this problem is that themapping between raw pixel values and screenshot is non-differentiable, makingit difficult to backpropagate gradients to the perturbation. To overcome this,we train a neural network to approximate the mapping and apply projectedgradient descent to solve the reformulated optimization problem. Extensiveevaluation on multiple datasets shows that WebInject is highly effective andsignificantly outperforms baselines.</description><author>Xilong Wang, John Bloch, Zedian Shao, Yuepeng Hu, Shuyan Zhou, Neil Zhenqiang Gong</author><pubDate>Fri, 29 Aug 2025 16:16:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11717v3</guid></item><item><title>Scientifically-Interpretable Reasoning Network (ScIReN): Discovering Hidden Relationships in the Carbon Cycle and Beyond</title><link>http://arxiv.org/abs/2506.14054v3</link><description>Understanding how carbon flows through the soil is crucial for mitigating theeffects of climate change. While soils have potential to sequester carbon fromthe atmosphere, the soil carbon cycle remains poorly understood. Scientistshave developed mathematical process-based models of the soil carbon cycle basedon existing knowledge, but they contain numerous unknown parameters that mustbe set in an ad-hoc manner, and often fit observations poorly. On the otherhand, neural networks can learn patterns from data, but do not respect knownscientific laws, nor can they reveal novel scientific relationships due totheir black-box nature. We thus propose Scientifically-Interpretable ReasoningNetwork (ScIReN), a fully-transparent framework that combines interpretableneural and process-based reasoning. An interpretable encoder predictsscientifically-meaningful latent parameters, which are then passed through adifferentiable process-based decoder to predict labeled output variables.ScIReN leverages Kolmogorov-Arnold networks (KAN) to ensure the encoder isfully interpretable and reveals relationships between input features and latentparameters; it uses novel smoothness penalties to balance expressivity andsimplicity. ScIReN also uses a novel hard-sigmoid constraint layer to restrictlatent parameters to meaningful ranges defined by scientific prior knowledge.While the process-based decoder enforces established scientific knowledge, theKAN-based encoder reveals new scientific relationships hidden in conventionalblack-box models. We apply ScIReN on two tasks: simulating the flow of organiccarbon through soils, and modeling ecosystem respiration from plants. In bothtasks, ScIReN outperforms black-box networks in predictive accuracy whileproviding substantial scientific interpretability -- it can infer latentscientific mechanisms and their relationships with input features.</description><author>Joshua Fan, Haodi Xu, Feng Tao, Md Nasim, Marc Grimson, Yiqi Luo, Carla P. Gomes</author><pubDate>Fri, 29 Aug 2025 16:14:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.14054v3</guid></item><item><title>Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions</title><link>http://arxiv.org/abs/2508.21742v1</link><description>Understanding causal relations between temporal variables is a centralchallenge in time series analysis, particularly when the full causal structureis unknown. Even when the full causal structure cannot be fully specified,experts often succeed in providing a high-level abstraction of the causalgraph, known as a summary causal graph, which captures the main causalrelations between different time series while abstracting away micro-leveldetails. In this work, we present conditions that guarantee the orientabilityof micro-level edges between temporal variables given the background knowledgeencoded in a summary causal graph and assuming having access to a faithful andcausally sufficient distribution with respect to the true unknown graph. Ourresults provide theoretical guarantees for edge orientation at the micro-level,even in the presence of cycles or bidirected edges at the macro-level. Thesefindings offer practical guidance for leveraging SCGs to inform causaldiscovery in complex temporal systems and highlight the value of incorporatingexpert knowledge to improve causal inference from observational time seriesdata.</description><author>TimothÃ©e Loranchet, Charles K. Assaad</author><pubDate>Fri, 29 Aug 2025 16:08:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21742v1</guid></item><item><title>Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance</title><link>http://arxiv.org/abs/2508.21741v1</link><description>Supervised fine-tuning (SFT) is a pivotal approach to adapting large languagemodels (LLMs) for downstream tasks; however, performance often suffers from the``seesaw phenomenon'', where indiscriminate parameter updates yield progress oncertain tasks at the expense of others. To address this challenge, we propose anovel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.Specifically, we first independently fine-tune the LLM on each task to identifyits core parameter regions by quantifying parameter update magnitudes. Taskswith similar core regions are then grouped based on region overlap, formingclusters for joint modeling. We further introduce a parameter fusion technique:for each task, core parameters from its individually fine-tuned model aredirectly transplanted into a unified backbone, while non-core parameters fromdifferent tasks are smoothly integrated via Spherical Linear Interpolation(SLERP), mitigating destructive interference. A lightweight, pipelined SFTtraining phase using mixed-task data is subsequently employed, while freezingcore regions from prior tasks to prevent catastrophic forgetting. Extensiveexperiments on multiple public benchmarks demonstrate that our approachsignificantly alleviates task interference and forgetting, consistentlyoutperforming vanilla multi-task and multi-stage fine-tuning baselines.</description><author>Yao Wang, Di Liang, Minlong Peng</author><pubDate>Fri, 29 Aug 2025 16:07:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21741v1</guid></item><item><title>Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL</title><link>http://arxiv.org/abs/2508.21739v1</link><description>The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamlineexperiments at rates of up to 1~MHz, with detectors producing data throughputsexceeding 1 TB/s. Managing such massive data streams presents significantchallenges, as transmission and storage infrastructures become prohibitivelyexpensive. Machine learning (ML) offers a promising solution for real-time datareduction, but conventional implementations introduce excessive latency, makingthem unsuitable for high-speed experimental environments. To address thesechallenges, SLAC developed the SLAC Neural Network Library (SNL), a specializedframework designed to deploy real-time ML inference models onField-Programmable Gate Arrays (FPGA). SNL's key feature is the ability todynamically update model weights without requiring FPGA resynthesis, enhancingflexibility for adaptive learning applications. To further enhance usabilityand accessibility, we introduce Auto-SNL, a Python extension that streamlinesthe process of converting Python-based neural network models intoSNL-compatible high-level synthesis code. This paper presents a benchmarkcomparison against hls4ml, the current state-of-the-art tool, across multipleneural network architectures, fixed-point precisions, and synthesisconfigurations targeting a Xilinx ZCU102 FPGA. The results showed that SNLachieves competitive or superior latency in most tested architectures, while insome cases also offering FPGA resource savings. This adaptation demonstratesSNL's versatility, opening new opportunities for researchers and academics infields such as high-energy physics, medical imaging, robotics, and many more.</description><author>Hamza Ezzaoui Rahali, Abhilasha Dave, Larry Ruckman, Mohammad Mehdi Rahimifar, Audrey C. Therrien, James J. Russel, Ryan T. Herbst</author><pubDate>Fri, 29 Aug 2025 16:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21739v1</guid></item><item><title>From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China</title><link>http://arxiv.org/abs/2508.21738v1</link><description>With the deepening of poverty alleviation and rural revitalizationstrategies, improving the rural living environment and enhancing the quality oflife have become key priorities. Rural livability is a key indicator formeasuring the effectiveness of these efforts. Current measurement approachesface significant limitations, as questionnaire-based methods are difficult toscale, while urban-oriented visual perception methods are poorly suited forrural contexts. In this paper, a rural-specific livability assessment frameworkwas proposed based on drone imagery and multimodal large language models(MLLMs). To comprehensively assess village livability, this study first used atop-down approach to collect large-scale drone imagery of 1,766 villages in 146counties across China. In terms of the model framework, an efficient imagecomparison mechanism was developed, incorporating binary search interpolationto determine effective image pairs while reducing comparison iterations.Building on expert knowledge, a chain-of-thought prompting suitable fornationwide rural livability measurement was constructed, considering bothliving quality and ecological habitability dimensions. This approach enhancedthe rationality and reliability of the livability assessment. Finally, thisstudy characterized the spatial heterogeneity of rural livability across Chinaand thoroughly analyzed its influential factors. The results show that: (1) Therural livability in China demonstrates a dual-core-periphery spatial pattern,radiating outward from Sichuan and Zhejiang provinces with declining gradients;(2) Among various influential factors, government fiscal expenditure emerged asthe core determinant, with each unit increase corresponding to a 3.9 - 4.9 unitenhancement in livability. The findings provide valuable insights for ruralconstruction policy-making.</description><author>Weihuan Deng, Yaofu Huang, Luan Chen, Xun Li, Yao Yao</author><pubDate>Fri, 29 Aug 2025 16:04:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21738v1</guid></item><item><title>Developer Insights into Designing AI-Based Computer Perception Tools</title><link>http://arxiv.org/abs/2508.21733v1</link><description>Artificial intelligence (AI)-based computer perception (CP) technologies usemobile sensors to collect behavioral and physiological data for clinicaldecision-making. These tools can reshape how clinical knowledge is generatedand interpreted. However, effective integration of these tools into clinicalworkflows depends on how developers balance clinical utility with useracceptability and trustworthiness. Our study presents findings from 20 in-depthinterviews with developers of AI-based CP tools. Interviews were transcribedand inductive, thematic analysis was performed to identify 4 key designpriorities: 1) to account for context and ensure explainability for bothpatients and clinicians; 2) align tools with existing clinical workflows; 3)appropriately customize to relevant stakeholders for usability andacceptability; and 4) push the boundaries of innovation while aligning withestablished paradigms. Our findings highlight that developers view themselvesas not merely technical architects but also ethical stewards, designing toolsthat are both acceptable by users and epistemically responsible (prioritizingobjectivity and pushing clinical knowledge forward). We offer the followingsuggestions to help achieve this balance: documenting how design choices aroundcustomization are made, defining limits for customization choices,transparently conveying information about outputs, and investing in usertraining. Achieving these goals will require interdisciplinary collaborationbetween developers, clinicians, and ethicists.</description><author>Maya Guhan, Meghan E. Hurley, Eric A. Storch, John Herrington, Casey Zampella, Julia Parish-Morris, Gabriel LÃ¡zaro-MuÃ±oz, Kristin Kostick-Quenet</author><pubDate>Fri, 29 Aug 2025 16:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21733v1</guid></item><item><title>CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</title><link>http://arxiv.org/abs/2508.21732v1</link><description>Recent advancements in Large Vision-Language Models (LVLMs) have demonstratedimpressive capabilities across various multimodal tasks. They continue,however, to struggle with trivial scenarios such as reading values from DigitalMeasurement Devices (DMDs), particularly in real-world conditions involvingclutter, occlusions, extreme viewpoints, and motion blur; common inhead-mounted cameras and Augmented Reality (AR) applications. Motivated bythese limitations, this work introduces CAD2DMD-SET, a synthetic datageneration tool designed to support visual question answering (VQA) tasksinvolving DMDs. By leveraging 3D CAD models, advanced rendering, andhigh-fidelity image composition, our tool produces diverse, VQA-labelledsynthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we presentDMDBench, a curated validation set of 1,000 annotated real-world imagesdesigned to evaluate model performance under practical constraints.Benchmarking three state-of-the-art LVLMs using Average Normalised LevenshteinSimilarity (ANLS) and further fine-tuning LoRA's of these models withCAD2DMD-SET's generated dataset yielded substantial improvements, with InternVLshowcasing a score increase of 200% without degrading on other tasks. Thisdemonstrates that the CAD2DMD-SET training dataset substantially improves therobustness and performance of LVLMs when operating under the previously statedchallenging conditions. The CAD2DMD-SET tool is expected to be released asopen-source once the final version of this manuscript is prepared, allowing thecommunity to add different measurement devices and generate their own datasets.</description><author>JoÃ£o Valente, Atabak Dehban, Rodrigo Ventura</author><pubDate>Fri, 29 Aug 2025 15:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21732v1</guid></item><item><title>Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem</title><link>http://arxiv.org/abs/2508.21730v1</link><description>In this paper we present a variational algorithm for the Traveling SalesmanProblem (TSP) that combines (i) a compact encoding of permutations, whichreduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:where the circuit topology (``Ansatz'') is first optimized on a traininginstance by Simulated Annealing (SA), then ``frozen'' and re-used on novelinstances, limited to a rapid re-optimization of only the circuit parameters.This pipeline eliminates costly structural research in testing, making theprocedure immediately implementable on NISQ hardware. On a set of $40$ randomly generated symmetric instances that span $4 - 7$cities, the resulting Ansatz achieves an average optimal trip samplingprobability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for6 city cases. With 7 cities the success rate drops markedly to an average of$\sim 20\%$, revealing the onset of scalability limitations of the proposedmethod. The results show robust generalization ability for moderate problem sizes andindicate how freezing the Ansatz can dramatically reduce time-to-solutionwithout degrading solution quality. The paper also discusses scalabilitylimitations, the impact of ``warm-start'' initialization of parameters, andprospects for extension to more complex problems, such as Vehicle Routing andJob-Shop Scheduling.</description><author>Fabrizio Fagiolo, Nicolo' Vescera</author><pubDate>Fri, 29 Aug 2025 15:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21730v1</guid></item><item><title>Control of Rayleigh-BÃ©nard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime</title><link>http://arxiv.org/abs/2504.12000v2</link><description>Data-driven flow control has significant potential for industry, energysystems, and climate science. In this work, we study the effectiveness ofReinforcement Learning (RL) for reducing convective heat transfer in the 2DRayleigh-B\'enard Convection (RBC) system under increasing turbulence. Weinvestigate the generalizability of control across varying initial conditionsand turbulence levels and introduce a reward shaping technique to acceleratethe training. RL agents trained via single-agent Proximal Policy Optimization(PPO) are compared to linear proportional derivative (PD) controllers fromclassical control theory. The RL agents reduced convection, measured by theNusselt Number, by up to 33% in moderately turbulent systems and 10% in highlyturbulent settings, clearly outperforming PD control in all settings. Theagents showed strong generalization performance across different initialconditions and to a significant extent, generalized to higher degrees ofturbulence. The reward shaping improved sample efficiency and consistentlystabilized the Nusselt Number to higher turbulence levels.</description><author>Thorben Markmann, Michiel Straat, Sebastian Peitz, Barbara Hammer</author><pubDate>Fri, 29 Aug 2025 15:52:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.12000v2</guid></item><item><title>OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization</title><link>http://arxiv.org/abs/2508.21727v1</link><description>Watermarking diffusion-generated images is crucial for copyright protectionand user tracking. However, current diffusion watermarking methods facesignificant limitations: zero-bit watermarking systems lack the capacity forlarge-scale user tracking, while multi-bit methods are highly sensitive tocertain image transformations or generative attacks, resulting in a lack ofcomprehensive robustness. In this paper, we propose OptMark, anoptimization-based approach that embeds a robust multi-bit watermark into theintermediate latents of the diffusion denoising process. OptMark strategicallyinserts a structural watermark early to resist generative attacks and a detailwatermark late to withstand image transformations, with tailored regularizationterms to preserve image quality and ensure imperceptibility. To address thechallenge of memory consumption growing linearly with the number of denoisingsteps during optimization, OptMark incorporates adjoint gradient methods,reducing memory usage from O(N) to O(1). Experimental results demonstrate thatOptMark achieves invisible multi-bit watermarking while ensuring robustresilience against valuemetric transformations, geometric transformations,editing, and regeneration attacks.</description><author>Jiazheng Xing, Hai Ci, Hongbin Xu, Hangjie Yuan, Yong Liu, Mike Zheng Shou</author><pubDate>Fri, 29 Aug 2025 15:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21727v1</guid></item><item><title>Inducing Programmatic Skills for Agentic Tasks</title><link>http://arxiv.org/abs/2504.06821v2</link><description>To succeed in common digital tasks such as web navigation, agents must carryout a variety of specialized tasks such as searching for products or planning atravel route. To tackle these tasks, agents can bootstrap themselves bylearning task-specific skills online through interaction with the webenvironment. In this work, we demonstrate that programs are an effectiverepresentation for skills. We propose agent skill induction (ASI), which allowsagents to adapt themselves by inducing, verifying, and utilizing program-basedskills on the fly. We start with an evaluation on the WebArena agent benchmarkand show that ASI outperforms the static baseline agent and its text-skillcounterpart by 23.5% and 11.3% in success rate, mainly thanks to theprogrammatic verification guarantee during the induction phase. ASI alsoimproves efficiency by reducing 10.7-15.3% of the steps over baselines, bycomposing primitive actions (e.g., click) into higher-level skills (e.g.,search product). We then highlight the efficacy of ASI in remaining efficientand accurate under scaled-up web activities. Finally, we examine thegeneralizability of induced skills when transferring between websites, and findthat ASI can effectively reuse common skills, while also updating incompatibleskills to versatile website changes.</description><author>Zora Zhiruo Wang, Apurva Gandhi, Graham Neubig, Daniel Fried</author><pubDate>Fri, 29 Aug 2025 15:46:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.06821v2</guid></item><item><title>Consistent and Invariant Generalization Learning for Short-video Misinformation Detection</title><link>http://arxiv.org/abs/2507.04061v3</link><description>Short-video misinformation detection has attracted wide attention in themulti-modal domain, aiming to accurately identify the misinformation in thevideo format accompanied by the corresponding audio. Despite significantadvancements, current models in this field, trained on particular domains(source domains), often exhibit unsatisfactory performance on unseen domains(target domains) due to domain gaps. To effectively realize such domaingeneralization on the short-video misinformation detection task, we proposedeep insights into the characteristics of different domains: (1) The detectionon various domains may mainly rely on different modalities (i.e., mainlyfocusing on videos or audios). To enhance domain generalization, it is crucialto achieve optimal model performance on all modalities simultaneously. (2) Forsome domains focusing on cross-modal joint fraud, a comprehensive analysisrelying on cross-modal fusion is necessary. However, domain biases located ineach modality (especially in each frame of videos) will be accumulated in thisfusion process, which may seriously damage the final identification ofmisinformation. To address these issues, we propose a new DOmain generalizationmodel via ConsisTency and invariance learning for shORt-video misinformationdetection (named DOCTOR), which contains two characteristic modules: (1) Weinvolve the cross-modal feature interpolation to map multiple modalities into ashared space and the interpolation distillation to synchronize multi-modallearning; (2) We design the diffusion model to add noise to retain corefeatures of multi modal and enhance domain invariant features throughcross-modal guided denoising. Extensive experiments demonstrate theeffectiveness of our proposed DOCTOR model. Our code is public available athttps://github.com/ghh1125/DOCTOR.</description><author>Hanghui Guo, Weijie Shi, Mengze Li, Juncheng Li, Hao Chen, Yue Cui, Jiajie Xu, Jia Zhu, Jiawei Shen, Zhangze Chen, Sirui Han</author><pubDate>Fri, 29 Aug 2025 15:39:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.04061v3</guid></item><item><title>Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety</title><link>http://arxiv.org/abs/2508.21722v1</link><description>Estimating community-specific mental health effects of local events is vitalfor public health policy. While forecasting mental health scores alone offerslimited insights into the impact of events on community well-being,quasi-experimental designs like the Longitudinal Regression DiscontinuityDesign (LRDD) from econometrics help researchers derive more effects that aremore likely to be causal from observational data. LRDDs aim to extrapolate thesize of changes in an outcome (e.g. a discontinuity in running scores foranxiety) due to a time-specific event. Here, we propose adapting LRDDs beyondtraditional forecasting into a statistical learning framework whereby futurediscontinuities (i.e. time-specific shifts) and changes in slope (i.e. lineartrajectories) are estimated given a location's history of the score, dynamiccovariates (other running assessments), and exogenous variables (staticrepresentations). Applying our framework to predict discontinuities in theanxiety of US counties from COVID-19 events, we found the task was difficultbut more achievable as the sophistication of models was increased, with thebest results coming from integrating exogenous and dynamic covariates. Ourapproach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$for slope) over traditional static community representations. Discontinuityforecasting raises new possibilities for estimating the idiosyncratic effectsof potential future or hypothetical events on specific communities.</description><author>Siddharth Mangalik, Ojas Deshpande, Adithya V. Ganesan, Sean A. P. Clouston, H. Andrew Schwartz</author><pubDate>Fri, 29 Aug 2025 15:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21722v1</guid></item><item><title>PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster Generation</title><link>http://arxiv.org/abs/2508.21720v1</link><description>We present a novel training-free framework, \textit{PosterForest}, forautomated scientific poster generation. Unlike prior approaches, which largelyneglect the hierarchical structure of scientific documents and the semanticintegration of textual and visual elements, our method addresses bothchallenges directly. We introduce the \textit{Poster Tree}, a hierarchicalintermediate representation that jointly encodes document structure andvisual-textual relationships at multiple levels. Our framework employs amulti-agent collaboration strategy, where agents specializing in contentsummarization and layout planning iteratively coordinate and provide mutualfeedback. This approach enables the joint optimization of logical consistency,content fidelity, and visual coherence. Extensive experiments on multipleacademic domains show that our method outperforms existing baselines in bothqualitative and quantitative evaluations. The resulting posters achieve qualityclosest to expert-designed ground truth and deliver superior informationpreservation, structural clarity, and user preference.</description><author>Jiho Choi, Seojeong Park, Seongjong Song, Hyunjung Shim</author><pubDate>Fri, 29 Aug 2025 15:36:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21720v1</guid></item><item><title>Entropy-Based Non-Invasive Reliability Monitoring of Convolutional Neural Networks</title><link>http://arxiv.org/abs/2508.21715v1</link><description>Convolutional Neural Networks (CNNs) have become the foundation of moderncomputer vision, achieving unprecedented accuracy across diverse imagerecognition tasks. While these networks excel on in-distribution data, theyremain vulnerable to adversarial perturbations imperceptible inputmodifications that cause misclassification with high confidence. However,existing detection methods either require expensive retraining, modify networkarchitecture, or degrade performance on clean inputs. Here we show thatadversarial perturbations create immediate, detectable entropy signatures inCNN activations that can be monitored without any model modification. Usingparallel entropy monitoring on VGG-16, we demonstrate that adversarial inputsconsistently shift activation entropy by 7% in early convolutional layers,enabling 90% detection accuracy with false positives and false negative ratesbelow 20%. The complete separation between clean and adversarial entropydistributions reveals that CNNs inherently encode distribution shifts in theiractivation patterns. This work establishes that CNN reliability can be assessedthrough activation entropy alone, enabling practical deployment ofself-diagnostic vision systems that detect adversarial inputs in real-timewithout compromising original model performance.</description><author>Amirhossein Nazeri, Wael Hafez</author><pubDate>Fri, 29 Aug 2025 15:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21715v1</guid></item><item><title>FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</title><link>http://arxiv.org/abs/2508.21712v1</link><description>Recent advances in diffusion-based generative models have demonstratedsignificant potential in augmenting scarce datasets for object detection tasks.Nevertheless, most recent models rely on resource-intensive full fine-tuning oflarge-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIAV100) and thousands of synthetic images. To address these limitations, wepropose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generationpipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tunedexclusively through Low-Rank Adaptation (LoRA). This dramatically reducescomputational requirements, enabling synthetic dataset generation with aconsumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate ourapproach on seven diverse object detection datasets. Our results demonstratethat training object detectors with just 500 synthetic images generated by ourapproach yields superior detection performance compared to models trained on5000 synthetic images from the ODGEN baseline, achieving improvements of up to21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpassstate-of-the-art performance with far greater efficiency, as FLORA achievessuperior results using only 10% of the data and a fraction of the computationalcost. This work demonstrates that a quality and efficiency-focused approach ismore effective than brute-force generation, making advanced synthetic datacreation more practical and accessible for real-world scenarios.</description><author>Alvaro Patricio, Atabak Dehban, Rodrigo Ventura</author><pubDate>Fri, 29 Aug 2025 15:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21712v1</guid></item><item><title>Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization</title><link>http://arxiv.org/abs/2507.05137v2</link><description>Learning Japanese vocabulary is a challenge for learners from Roman alphabetbackgrounds due to script differences. Japanese combines syllabaries likehiragana with kanji, which are logographic characters of Chinese origin. Kanjiare also complicated due to their complexity and volume. Keyword mnemonics area common strategy to aid memorization, often using the compositional structureof kanji to form vivid associations. Despite recent efforts to use largelanguage models (LLMs) to assist learners, existing methods for LLM-basedkeyword mnemonic generation function as a black box, offering limitedinterpretability. We propose a generative framework that explicitly models themnemonic construction process as driven by a set of common rules, and learnthem using a novel Expectation-Maximization-type algorithm. Trained onlearner-authored mnemonics from an online platform, our method learns latentstructures and compositional rules, enabling interpretable and systematicmnemonics generation. Experiments show that our method performs well in thecold-start setting for new learners while providing insight into the mechanismsbehind effective mnemonic creation.</description><author>Jaewook Lee, Alexander Scarlatos, Andrew Lan</author><pubDate>Fri, 29 Aug 2025 15:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05137v2</guid></item><item><title>Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization</title><link>http://arxiv.org/abs/2507.09823v2</link><description>In this paper, we focus on the problem of minimizing a continuouslydifferentiable convex objective function, $\min_x f(x)$. Recently, Malitsky(2020); Alacaoglu et al.(2023) developed an adaptive first-order method, GRAAL.This algorithm computes stepsizes by estimating the local curvature of theobjective function without any line search procedures or hyperparameter tuning,and attains the standard iteration complexity $\mathcal{O}(L\lVertx_0-x^*\rVert^2/\epsilon)$ of fixed-stepsize gradient descent for $L$-smoothfunctions. However, a natural question arises: is it possible to accelerate theconvergence of GRAAL to match the optimal complexity $\mathcal{O}(\sqrt{L\lVertx_0-x^*\rVert^2/\epsilon})$ of the accelerated gradient descent of Nesterov(1983)? Although some attempts have been made by Li and Lan (2025); Suh and Ma(2025), the ability of existing accelerated algorithms to adapt to the localcurvature of the objective function is highly limited. We resolve this issueand develop GRAAL with Nesterov acceleration, which can adapt its stepsize tothe local curvature at a geometric, or linear, rate just like non-acceleratedGRAAL. We demonstrate the adaptive capabilities of our algorithm by provingthat it achieves near-optimal iteration complexities for $L$-smooth functions,as well as under a more general $(L_0,L_1)$-smoothness assumption (Zhang etal., 2019).</description><author>Ekaterina Borodich, Dmitry Kovalev</author><pubDate>Fri, 29 Aug 2025 15:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.09823v2</guid></item><item><title>Compression versus Accuracy: A Hierarchy of Lifted Models</title><link>http://arxiv.org/abs/2505.22288v2</link><description>Probabilistic graphical models that encode indistinguishable objects andrelations among them use first-order logic constructs to compress apropositional factorised model for more efficient (lifted) inference. To obtaina lifted representation, the state-of-the-art algorithm Advanced Colour Passing(ACP) groups factors that represent matching distributions. In an approximateversion using $\varepsilon$ as a hyperparameter, factors are grouped thatdiffer by a factor of at most $(1\pm \varepsilon)$. However, finding a suitable$\varepsilon$ is not obvious and may need a lot of exploration, possiblyrequiring many ACP runs with different $\varepsilon$ values. Additionally,varying $\varepsilon$ can yield wildly different models, leading to decreasedinterpretability. Therefore, this paper presents a hierarchical approach tolifted model construction that is hyperparameter-free. It efficiently computesa hierarchy of $\varepsilon$ values that ensures a hierarchy of models, meaningthat once factors are grouped together given some $\varepsilon$, these factorswill be grouped together for larger $\varepsilon$ as well. The hierarchy of$\varepsilon$ values also leads to a hierarchy of error bounds. This allows forexplicitly weighing compression versus accuracy when choosing specific$\varepsilon$ values to run ACP with and enables interpretability between thedifferent models.</description><author>Jan Speller, Malte Luttermann, Marcel Gehrke, Tanya Braun</author><pubDate>Fri, 29 Aug 2025 15:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.22288v2</guid></item><item><title>Activation Subspaces for Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2508.21695v1</link><description>To ensure the reliability of deep models in real-world applications,out-of-distribution (OOD) detection methods aim to distinguish samples close tothe training distribution (in-distribution, ID) from those farther away (OOD).In this work, we propose a novel OOD detection method that utilizes singularvalue decomposition of the weight matrix of the classification head todecompose the model's activations into decisive and insignificant components,which contribute maximally, respectively minimally, to the final classifieroutput. We find that the subspace of insignificant components more effectivelydistinguishes ID from OOD data than raw activations in regimes of largedistribution shifts (Far-OOD). This occurs because the classification objectiveleaves the insignificant subspace largely unaffected, yielding features thatare ''untainted'' by the target classification task. Conversely, in regimes ofsmaller distribution shifts (Near-OOD), we find that activation shaping methodsprofit from only considering the decisive subspace, as the insignificantcomponent can cause interference in the activation space. By combining twofindings into a single approach, termed ActSub, we achieve state-of-the-artresults in various standard OOD benchmarks.</description><author>BarÄ±Å ZÃ¶ngÃ¼r, Robin Hesse, Stefan Roth</author><pubDate>Fri, 29 Aug 2025 15:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21695v1</guid></item><item><title>Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR</title><link>http://arxiv.org/abs/2508.21693v1</link><description>Conventional optical character recognition (OCR) techniques segmented eachcharacter and then recognized. This made them prone to error in charactersegmentation, and devoid of context to exploit language models. Advances insequence to sequence translation in last decade led to modern techniques firstdetecting words and then inputting one word at a time to a model to directlyoutput full words as sequence of characters. This allowed better utilization oflanguage models and bypass error-prone character segmentation step. We observethat the above transition in style has moved the bottleneck in accuracy to wordsegmentation. Hence, in this paper, we propose a natural and logicalprogression from word level OCR to line-level OCR. The proposal allows tobypass errors in word detection, and provides larger sentence context forbetter utilization of language models. We show that the proposed technique notonly improves the accuracy but also efficiency of OCR. Despite our thoroughliterature survey, we did not find any public dataset to train and benchmarksuch shift from word to line-level OCR. Hence, we also contribute ameticulously curated dataset of 251 English page images with line-levelannotations. Our experimentation revealed a notable end-to-end accuracyimprovement of 5.4%, underscoring the potential benefits of transitioningtowards line-level OCR, especially for document images. We also report a 4times improvement in efficiency compared to word-based pipelines. Withcontinuous improvements in large language models, our methodology also holdspotential to exploit such advances. Project Website:https://nishitanand.github.io/line-level-ocr-website</description><author>Shashank Vempati, Nishit Anand, Gaurav Talebailkar, Arpan Garai, Chetan Arora</author><pubDate>Fri, 29 Aug 2025 15:02:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21693v1</guid></item><item><title>Federated Diffusion Modeling with Differential Privacy for Tabular Data Synthesis</title><link>http://arxiv.org/abs/2412.16083v2</link><description>The increasing demand for privacy-preserving data analytics in variousdomains necessitates solutions for synthetic data generation that rigorouslyuphold privacy standards. We introduce the DP-FedTabDiff framework, a novelintegration of Differential Privacy, Federated Learning and Denoising DiffusionProbabilistic Models designed to generate high-fidelity synthetic tabular data.This framework ensures compliance with privacy regulations while maintainingdata utility. We demonstrate the effectiveness of DP-FedTabDiff on multiplereal-world mixed-type tabular datasets, achieving significant improvements inprivacy guarantees without compromising data quality. Our empirical evaluationsreveal the optimal trade-offs between privacy budgets, client configurations,and federated optimization strategies. The results affirm the potential ofDP-FedTabDiff to enable secure data sharing and analytics in highly regulateddomains, paving the way for further advances in federated learning andprivacy-preserving data synthesis.</description><author>Timur Sattarov, Marco Schreyer, Damian Borth</author><pubDate>Fri, 29 Aug 2025 15:00:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.16083v2</guid></item><item><title>Bayesian Double Descent</title><link>http://arxiv.org/abs/2507.07338v2</link><description>Double descent is a phenomenon of over-parameterized statistical models. Ourgoal is to view double descent from a Bayesian perspective. Over-parameterizedmodels such as deep neural networks have an interesting re-descending propertyin their risk characteristics. This is a recent phenomenon in machine learningand has been the subject of many studies. As the complexity of the modelincreases, there is a U-shaped region corresponding to the traditionalbias-variance trade-off, but then as the number of parameters equals the numberof observations and the model becomes one of interpolation, the risk can becomeinfinite and then, in the over-parameterized region, it re-descends -- thedouble descent effect. We show that this has a natural Bayesian interpretation.Moreover, we show that it is not in conflict with the traditional Occam's razorthat Bayesian models possess, in that they tend to prefer simpler models whenpossible. We develop comprehensive theoretical foundations including Dawid'smodel comparison theory, Dickey-Savage results, and connections to generalizedridge regression and shrinkage methods. We illustrate the approach withexamples of Bayesian model selection in neural networks and provide detailedtreatments of infinite Gaussian means models and non-parametric regression.Finally, we conclude with directions for future research.</description><author>Nick Polson, Vadim Sokolov</author><pubDate>Fri, 29 Aug 2025 14:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.07338v2</guid></item><item><title>Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</title><link>http://arxiv.org/abs/2508.21689v1</link><description>Constructing high-definition (HD) maps from sensory input requires accuratelymapping the road elements in image space to the Bird's Eye View (BEV) space.The precision of this mapping directly impacts the quality of the finalvectorized HD map. Existing HD mapping approaches outsource the projection tostandard mapping techniques, such as attention-based ones. However, thesemethods struggle with accuracy due to generalization problems, oftenhallucinating non-existent road elements. Our key idea is to start with ageometric mapping based on camera parameters and adapt it to the scene toextract relevant map information from camera images. To implement this, wepropose a novel probabilistic projection mechanism with confidence scores to(i) refine the mapping to better align with the scene and (ii) filter outirrelevant elements that should not influence HD map generation. In addition,we improve temporal processing by using confidence scores to selectivelyaccumulate reliable information over time. Experiments on new splits of thenuScenes and Argoverse2 datasets demonstrate improved performance overstate-of-the-art approaches, indicating better generalization. The improvementsare particularly pronounced on nuScenes and in the challenging long perceptionrange. Our code and model checkpoints are available athttps://github.com/Fatih-Erdogan/mapping-like-skeptic .</description><author>Fatih ErdoÄan, Merve Rabia BarÄ±n, Fatma GÃ¼ney</author><pubDate>Fri, 29 Aug 2025 14:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21689v1</guid></item><item><title>Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation</title><link>http://arxiv.org/abs/2505.04619v2</link><description>Vision is well-known for its use in manipulation, especially using visualservoing. Due to the 3D nature of the world, using multiple camera views andmerging them creates better representations for Q-learning and in turn, trainsmore sample efficient policies. Nevertheless, these multi-view policies aresensitive to failing cameras and can be burdensome to deploy. To mitigate theseissues, we introduce a Merge And Disentanglement (MAD) algorithm thatefficiently merges views to increase sample efficiency while simultaneouslydisentangling views by augmenting multi-view feature inputs with single-viewfeatures. This produces robust policies and allows lightweight deployment. Wedemonstrate the efficiency and robustness of our approach using Meta-World andManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad</description><author>Abdulaziz Almuzairee, Rohan Patil, Dwait Bhatt, Henrik I. Christensen</author><pubDate>Fri, 29 Aug 2025 14:54:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.04619v2</guid></item><item><title>Mixed membership estimation for categorical data with weighted responses</title><link>http://arxiv.org/abs/2310.10989v2</link><description>The Grade of Membership (GoM) model, which allows subjects to belong tomultiple latent classes, is a powerful tool for inferring latent classes incategorical data. However, its application is limited to categorical data withnonnegative integer responses, as it assumes that the response matrix isgenerated from Bernoulli or Binomial distributions, making it inappropriate fordatasets with continuous or negative weighted responses. To address this, thispaper proposes a novel model named the Weighted Grade of Membership (WGoM)model. Our WGoM is more general than GoM because it relaxes GoM's distributionconstraint by allowing the response matrix to be generated from distributionslike Bernoulli, Binomial, Normal, and Uniform as long as the expected responsematrix has a block structure related to subjects' mixed memberships under thedistribution. We show that WGoM can describe any response matrix with finitedistinct elements. We then propose an algorithm to estimate the latent mixedmemberships and other WGoM parameters. We derive the error bounds of theestimated parameters and show that the algorithm is statistically consistent.We also propose an efficient method for determining the number of latentclasses $K$ for categorical data with weighted responses by maximizing fuzzyweighted modularity. The performance of our methods is validated through bothsynthetic and real-world datasets. The results demonstrate the accuracy andefficiency of our algorithm for estimating latent mixed memberships, as well asthe high accuracy of our method for estimating $K$, indicating their highpotential for practical applications.</description><author>Huan Qing</author><pubDate>Fri, 29 Aug 2025 14:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10989v2</guid></item><item><title>Towards Interactive Lesion Segmentation in Whole-Body PET/CT with Promptable Models</title><link>http://arxiv.org/abs/2508.21680v1</link><description>Whole-body PET/CT is a cornerstone of oncological imaging, yet accuratelesion segmentation remains challenging due to tracer heterogeneity,physiological uptake, and multi-center variability. While fully automatedmethods have advanced substantially, clinical practice benefits from approachesthat keep humans in the loop to efficiently refine predicted masks. TheautoPET/CT IV challenge addresses this need by introducing interactivesegmentation tasks based on simulated user prompts. In this work, we presentour submission to Task 1. Building on the winning autoPET III nnU-Net pipeline,we extend the framework with promptable capabilities by encoding user-providedforeground and background clicks as additional input channels. Wesystematically investigate representations for spatial prompts and demonstratethat Euclidean Distance Transform (EDT) encodings consistently outperformGaussian kernels. Furthermore, we propose online simulation of userinteractions and a custom point sampling strategy to improve robustness underrealistic prompting conditions. Our ensemble of EDT-based models, trained withand without external data, achieves the strongest cross-validation performance,reducing both false positives and false negatives compared to baseline models.These results highlight the potential of promptable models to enable efficient,user-guided segmentation workflows in multi-tracer, multi-center PET/CT. Codeis publicly available at https://github.com/MIC-DKFZ/autoPET-interactive</description><author>Maximilian Rokuss, Yannick Kirchhoff, Fabian Isensee, Klaus H. Maier-Hein</author><pubDate>Fri, 29 Aug 2025 14:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21680v1</guid></item><item><title>Studying Evolutionary Solution Adaption Using a Flexibility Benchmark Based on a Metal Cutting Process</title><link>http://arxiv.org/abs/2305.19775v2</link><description>We consider optimizing for different production requirements from theviewpoint of a bio-inspired framework for system flexibility that allows us tostudy the ability of an algorithm to transfer solutions from previousoptimization tasks, which also relates to dynamic evolutionary optimization. Optimizing manufacturing process parameters is typically a multi-objectiveproblem with often contradictory objectives such as production quality andproduction time. If production requirements change, process parameters have tobe optimized again. Since optimization usually requires costly simulationsbased on, for example, the Finite Element method, it is of great interest tohave means to reduce the number of evaluations needed for optimization. Basedon the extended Oxley model for orthogonal metal cutting, we introduce amulti-objective optimization benchmark where different materials define relatedoptimization tasks. We use the benchmark to study the flexibility of NSGA-II, which we extend bytwo variants: 1) varying goals, which optimizes solutions for two taskssimultaneously to obtain in-between source solutions expected to be moreadaptable, and 2) active-inactive genotype, which accommodates differentpossibilities that can be activated or deactivated. Results show that adaption,i.e. transferring a solution from a previous optimization task, with standardNSGA-II greatly reduces the number of evaluations required for optimization toa target goal in comparison to starting from scratch. The proposed variantsfurther improve the adaption costs, although further work is needed towardsmaking the methods advantageous for real applications.</description><author>Leo Francoso Dal Piccol Sotto, Sebastian Mayer, Hemanth Janarthanam, Alexander Butz, Jochen Garcke</author><pubDate>Fri, 29 Aug 2025 14:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19775v2</guid></item><item><title>BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens</title><link>http://arxiv.org/abs/2508.17196v2</link><description>Recent advancements in Large Language Models (LLMs) have leveraged increasedtest-time computation to enhance reasoning capabilities, a strategy that, whileeffective, incurs significant latency and resource costs, limiting theirapplicability in real-world time-constrained or cost-sensitive scenarios. Thispaper introduces BudgetThinker, a novel framework designed to empower LLMs withbudget-aware reasoning, enabling precise control over the length of theirthought processes. We propose a methodology that periodically inserts specialcontrol tokens during inference to continuously inform the model of itsremaining token budget. This approach is coupled with a comprehensive two-stagetraining pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarizethe model with budget constraints, followed by a curriculum-based ReinforcementLearning (RL) phase that utilizes a length-aware reward function to optimizefor both accuracy and budget adherence. We demonstrate that BudgetThinkersignificantly surpasses strong baselines in maintaining performance across avariety of reasoning budgets on challenging mathematical benchmarks. Our methodprovides a scalable and effective solution for developing efficient andcontrollable LLM reasoning, making advanced models more practical fordeployment in resource-constrained and real-time environments.</description><author>Hao Wen, Xinrui Wu, Yi Sun, Feifei Zhang, Liye Chen, Jie Wang, Yunxin Liu, Yunhao Liu, Ya-Qin Zhang, Yuanchun Li</author><pubDate>Fri, 29 Aug 2025 14:42:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17196v2</guid></item><item><title>Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback</title><link>http://arxiv.org/abs/2507.15066v3</link><description>Time series anomaly detection is critical across various domains, yet currentapproaches often limit analysis to mere binary anomaly classification withoutdetailed categorization or further explanatory reasoning. To address theselimitations, we propose a novel task, Time-series Reasoning for Anomaly(Time-RA) that transforms classical time series anomaly detection from adiscriminative into a generative, reasoning-intensive task leveraging LargeLanguage Models (LLMs). Also, we introduce the first real-world multimodalbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,comprising approximately 40,000 samples across 10 real-world domains. Eachsample includes numeric time series data, contextual text information, andvisual representations, each annotated with fine-grained categories (14 typesfor univariate anomalies and 6 for multivariate anomalies) and structuredexplanatory reasoning. We develop a sophisticated annotation frameworkutilizing ensemble-generated labels refined through GPT-4-driven feedback,ensuring accuracy and interpretability. Extensive benchmarking of LLMs andmultimodal LLMs demonstrates the capabilities and limitations of currentmodels, highlighting the critical role of supervised fine-tuning. Our datasetand task pave the way for significant advancements in interpretable time seriesanomaly detection and reasoning. The code(https://github.com/yyysjz1997/Time-RA) and dataset(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourcedto support and accelerate future research in this area.</description><author>Yiyuan Yang, Zichuan Liu, Lei Song, Kai Ying, Zhiguang Wang, Tom Bamford, Svitlana Vyetrenko, Jiang Bian, Qingsong Wen</author><pubDate>Fri, 29 Aug 2025 14:41:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.15066v3</guid></item><item><title>Conditional Hierarchical Bayesian Tucker Decomposition for Genetic Data Analysis</title><link>http://arxiv.org/abs/1911.12426v9</link><description>We analyze large, multi-dimensional, sparse counting data sets, findingunsupervised groups to provide unique insights into genetic data. We creategene and biological pathway groups based on patients' variants to find commonrisk factors for four common types of cancer (breast, lung, prostate, andcolorectal) and autism spectrum disorder. To accomplish this, we extend latentDirichlet allocation to multiple dimensions and design distinct methods forhierarchical topic modeling. We find that our conditional hierarchical BayesianTucker decomposition models are more coherent than baseline models.</description><author>Adam Sandler, Diego Klabjan, Yuan Luo</author><pubDate>Fri, 29 Aug 2025 14:37:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1911.12426v9</guid></item><item><title>Is this chart lying to me? Automating the detection of misleading visualizations</title><link>http://arxiv.org/abs/2508.21675v1</link><description>Misleading visualizations are a potent driver of misinformation on socialmedia and the web. By violating chart design principles, they distort data andlead readers to draw inaccurate conclusions. Prior work has shown that bothhumans and multimodal large language models (MLLMs) are frequently deceived bysuch visualizations. Automatically detecting misleading visualizations andidentifying the specific design rules they violate could help protect readersand reduce the spread of misinformation. However, the training and evaluationof AI models has been limited by the absence of large, diverse, and openlyavailable datasets. In this work, we introduce Misviz, a benchmark of 2,604real-world visualizations annotated with 12 types of misleaders. To supportmodel training, we also release Misviz-synth, a synthetic dataset of 81,814visualizations generated using Matplotlib and based on real-world data tables.We perform a comprehensive evaluation on both datasets using state-of-the-artMLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal thatthe task remains highly challenging. We release Misviz, Misviz-synth, and theaccompanying code.</description><author>Jonathan Tonglet, Jan Zimny, Tinne Tuytelaars, Iryna Gurevych</author><pubDate>Fri, 29 Aug 2025 14:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21675v1</guid></item><item><title>A Soft Inducement Framework for Incentive-Aided Steering of No-Regret Players</title><link>http://arxiv.org/abs/2508.21672v1</link><description>In this work, we investigate a steering problem in a mediator-augmentedtwo-player normal-form game, where the mediator aims to guide players toward aspecific action profile through information and incentive design. We firstcharacterize the games for which successful steering is possible. Moreover, weestablish that steering players to any desired action profile is not alwaysachievable with information design alone, nor when accompanied with sublinearpayment schemes. Consequently, we derive a lower bound on the constant paymentsrequired per round to achieve this goal. To address these limitations incurredwith information design, we introduce an augmented approach that involves aone-shot information design phase before the start of the repeated game,transforming the prior interaction into a Stackelberg game. Finally, wetheoretically demonstrate that this approach improves the convergence rate ofplayers' action profiles to the target point by a constant factor with highprobability, and support it with empirical results.</description><author>Asrin Efe Yorulmaz, Raj Kiriti Velicheti, Melih Bastopcu, Tamer BaÅar</author><pubDate>Fri, 29 Aug 2025 14:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21672v1</guid></item><item><title>Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land</title><link>http://arxiv.org/abs/2404.17625v3</link><description>Neural networks surround us, in the form of large language models, speechtranscription systems, molecular discovery algorithms, robotics, and much more.Stripped of anything else, neural networks are compositions of differentiableprimitives, and studying them means learning how to program and how to interactwith these models, a particular example of what is called differentiableprogramming. This primer is an introduction to this fascinating field imagined forsomeone, like Alice, who has just ventured into this strange differentiablewonderland. I overview the basics of optimizing a function via automaticdifferentiation, and a selection of the most common designs for handlingsequences, graphs, texts, and audios. The focus is on a intuitive,self-contained introduction to the most important design techniques, includingconvolutional, attentional, and recurrent blocks, hoping to bridge the gapbetween theory and code (PyTorch and JAX) and leaving the reader capable ofunderstanding some of the most advanced models out there, such as largelanguage models (LLMs) and multimodal architectures.</description><author>Simone Scardapane</author><pubDate>Fri, 29 Aug 2025 14:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17625v3</guid></item><item><title>Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education</title><link>http://arxiv.org/abs/2508.21666v1</link><description>This paper introduces the Future Atmospheric Conditions Training System(FACTS), a novel platform that advances climate resilience education throughplace-based, adaptive learning experiences. FACTS combines real-timeatmospheric data collected by IoT sensors with curated resources from aKnowledge Base to dynamically generate localized learning challenges. Learnerresponses are analyzed by a Generative AI powered server, which deliverspersonalized feedback and adaptive support. Results from a user evaluationindicate that participants found the system both easy to use and effective forbuilding knowledge related to climate resilience. These findings suggest thatintegrating IoT and Generative AI into atmospherically adaptive learningtechnologies holds significant promise for enhancing educational engagement andfostering climate awareness.</description><author>Imran S. A. Khan, Emmanuel G. Blanchard, SÃ©bastien George</author><pubDate>Fri, 29 Aug 2025 14:30:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21666v1</guid></item><item><title>CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics</title><link>http://arxiv.org/abs/2508.18124v3</link><description>We introduce CMPhysBench, designed to assess the proficiency of LargeLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.CMPhysBench is composed of more than 520 graduate-level meticulously curatedquestions covering both representative subfields and foundational theoreticalframeworks of condensed matter physics, such as magnetism, superconductivity,strongly correlated systems, etc. To ensure a deep understanding of theproblem-solving process,we focus exclusively on calculation problems, requiringLLMs to independently generate comprehensive solutions. Meanwhile, leveragingtree-based representations of expressions, we introduce the Scalable ExpressionEdit Distance (SEED) score, which provides fine-grained (non-binary) partialcredit and yields a more accurate assessment of similarity between predictionand ground-truth. Our results show that even the best models, Grok-4, reachonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring asignificant capability gap, especially for this practical and frontier domainrelative to traditional physics. The code anddataset are publicly available athttps://github.com/CMPhysBench/CMPhysBench.</description><author>Weida Wang, Dongchen Huang, Jiatong Li, Tengchao Yang, Ziyang Zheng, Di Zhang, Dong Han, Benteng Chen, Binzhao Luo, Zhiyu Liu, Kunling Liu, Zhiyuan Gao, Shiqi Geng, Wei Ma, Jiaming Su, Xin Li, Shuchen Pu, Yuhan Shui, Qianjia Cheng, Zhihao Dou, Dongfei Cui, Changyong He, Jin Zeng, Zeke Xie, Mao Su, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang, Yunqi Cai, Xi Dai, Shufei Zhang, Lei Bai, Jinguang Cheng, Zhong Fang, Hongming Weng</author><pubDate>Fri, 29 Aug 2025 14:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18124v3</guid></item><item><title>Trajectory learning for ensemble forecasts via the continuous ranked probability score: a Lorenz '96 case study</title><link>http://arxiv.org/abs/2508.21664v1</link><description>This paper demonstrates the feasibility of trajectory learning for ensembleforecasts by employing the continuous ranked probability score (CRPS) as a lossfunction. Using the two-scale Lorenz '96 system as a case study, we develop andtrain both additive and multiplicative stochastic parametrizations to generateensemble predictions. Results indicate that CRPS-based trajectory learningproduces parametrizations that are both accurate and sharp. The resultingparametrizations are straightforward to calibrate and outperformderivative-fitting-based parametrizations in short-term forecasts. Thisapproach is particularly promising for data assimilation applications due toits accuracy over short lead times.</description><author>Sagy Ephrati, James Woodfield</author><pubDate>Fri, 29 Aug 2025 14:25:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21664v1</guid></item><item><title>Surface Stability Modeling with Universal Machine Learning Interatomic Potentials: A Comprehensive Cleavage Energy Benchmarking Study</title><link>http://arxiv.org/abs/2508.21663v1</link><description>Machine learning interatomic potentials (MLIPs) have revolutionizedcomputational materials science by bridging the gap between quantum mechanicalaccuracy and classical simulation efficiency, enabling unprecedentedexploration of materials properties across the periodic table. Despite theirremarkable success in predicting bulk properties, no systematic evaluation hasassessed how well these universal MLIPs (uMLIPs) can predict cleavage energies,a critical property governing fracture, catalysis, surface stability, andinterfacial phenomena. Here, we present a comprehensive benchmark of 19state-of-the-art uMLIPs for cleavage energy prediction using our previouslyestablished density functional theory (DFT) database of 36,718 slab structuresspanning elemental, binary, and ternary metallic compounds. We evaluate diversearchitectural paradigms, analyzing their performance across chemicalcompositions, crystal systems, thickness, and surface orientations. Our resultsreveal that training data composition dominates architectural sophistication:models trained on the Open Materials 2024 (OMat24) dataset, which emphasizesnon-equilibrium configurations, achieve mean absolute percentage errors below6% and correctly identify the thermodynamically most stable surfaceterminations in 87% of cases, without any explicit surface energy training. Incontrast, architecturally identical models trained on equilibrium-only datasetsshow five-fold higher errors, while models trained on surface-adsorbate datafail catastrophically with a 17-fold degradation. Remarkably, simplerarchitectures trained on appropriate data achieve comparable accuracy tocomplex transformers while offering 10-100x computational speedup. Thesefindings show that the community should focus on strategic training datageneration that captures the relevant physical phenomena.</description><author>Ardavan Mehdizadeh, Peter Schindler</author><pubDate>Fri, 29 Aug 2025 14:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21663v1</guid></item><item><title>Unfolding Framework with Complex-Valued Deformable Attention for High-Quality Computer-Generated Hologram Generation</title><link>http://arxiv.org/abs/2508.21657v1</link><description>Computer-generated holography (CGH) has gained wide attention with deeplearning-based algorithms. However, due to its nonlinear and ill-posed nature,challenges remain in achieving accurate and stable reconstruction.Specifically, ($i$) the widely used end-to-end networks treat thereconstruction model as a black box, ignoring underlying physicalrelationships, which reduces interpretability and flexibility. ($ii$) CNN-basedCGH algorithms have limited receptive fields, hindering their ability tocapture long-range dependencies and global context. ($iii$) Angular spectrummethod (ASM)-based models are constrained to finite near-fields.In this paper,we propose a Deep Unfolding Network (DUN) that decomposes gradient descent intotwo modules: an adaptive bandwidth-preserving model (ABPM) and a phase-domaincomplex-valued denoiser (PCD), providing more flexibility. ABPM allows forwider working distances compared to ASM-based methods. At the same time, PCDleverages its complex-valued deformable self-attention module to capture globalfeatures and enhance performance, achieving a PSNR over 35 dB. Experiments onsimulated and real data show state-of-the-art results.</description><author>Haomiao Zhang, Zhangyuan Li, Yanling Piao, Zhi Li, Xiaodong Wang, Miao Cao, Xiongfei Su, Qiang Song, Xin Yuan</author><pubDate>Fri, 29 Aug 2025 14:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21657v1</guid></item><item><title>From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling</title><link>http://arxiv.org/abs/2505.14177v2</link><description>We consider the problem of sampling distributions stemming from non-convexpotentials with Unadjusted Langevin Algorithm (ULA). We prove the stability ofthe discrete-time ULA to drift approximations under the assumption that thepotential is strongly convex at infinity. In many context, e.g. imaging inverseproblems, potentials are non-convex and non-smooth. Proximal StochasticGradient Langevin Algorithm (PSGLA) is a popular algorithm to handle suchpotentials. It combines the forward-backward optimization algorithm with a ULAstep. Our main stability result combined with properties of the Moreau envelopeallows us to derive the first proof of convergence of the PSGLA for non-convexpotentials. We empirically validate our methodology on synthetic data and inthe context of imaging inverse problems. In particular, we observe that PSGLAexhibits faster convergence rates than Stochastic Gradient Langevin Algorithmfor posterior sampling while preserving its restoration properties.</description><author>Marien Renaud, Valentin De Bortoli, Arthur Leclaire, Nicolas Papadakis</author><pubDate>Fri, 29 Aug 2025 14:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14177v2</guid></item><item><title>I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks</title><link>http://arxiv.org/abs/2508.21654v1</link><description>Model stealing attacks endanger the confidentiality of machine learningmodels offered as a service. Although these models are kept secret, a maliciousparty can query a model to label data samples and train their own substitutemodel, violating intellectual property. While novel attacks in the field arecontinually being published, their design and evaluations are not standardised,making it challenging to compare prior works and assess progress in the field.This paper is the first to address this gap by providing recommendations fordesigning and evaluating model stealing attacks. To this end, we study thelargest group of attacks that rely on training a substitute model -- thoseattacking image classification models. We propose the first comprehensivethreat model and develop a framework for attack comparison. Further, we analyseattack setups from related works to understand which tasks and models have beenstudied the most. Based on our findings, we present best practices for attackdevelopment before, during, and beyond experiments and derive an extensive listof open research questions regarding the evaluation of model stealing attacks.Our findings and recommendations also transfer to other problem domains, henceestablishing the first generic evaluation methodology for model stealingattacks.</description><author>Daryna Oliynyk, Rudolf Mayer, Kathrin Grosse, Andreas Rauber</author><pubDate>Fri, 29 Aug 2025 14:16:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21654v1</guid></item><item><title>Machine Intelligence on the Edge: Interpretable Cardiac Pattern Localisation Using Reinforcement Learning</title><link>http://arxiv.org/abs/2508.21652v1</link><description>Matched filters are widely used to localise signal patterns due to their highefficiency and interpretability. However, their effectiveness deteriorates forlow signal-to-noise ratio (SNR) signals, such as those recorded on edgedevices, where prominent noise patterns can closely resemble the target withinthe limited length of the filter. One example is the ear-electrocardiogram(ear-ECG), where the cardiac signal is attenuated and heavily corrupted byartefacts. To address this, we propose the Sequential Matched Filter (SMF), aparadigm that replaces the conventional single matched filter with a sequenceof filters designed by a Reinforcement Learning agent. By formulating filterdesign as a sequential decision-making process, SMF adaptively designsignal-specific filter sequences that remain fully interpretable by revealingkey patterns driving the decision-making. The proposed SMF framework has strongpotential for reliable and interpretable clinical decision support, asdemonstrated by its state-of-the-art R-peak detection and physiological stateclassification performance on two challenging real-world ECG datasets. Theproposed formulation can also be extended to a broad range of applications thatrequire accurate pattern localisation from noise-corrupted signals.</description><author>Haozhe Tian, Qiyu Rao, Nina Moutonnet, Pietro Ferraro, Danilo Mandic</author><pubDate>Fri, 29 Aug 2025 14:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21652v1</guid></item><item><title>Predicting Social Media Engagement from Emotional and Temporal Features</title><link>http://arxiv.org/abs/2508.21650v1</link><description>We present a machine learning approach for predicting social media engagement(comments and likes) from emotional and temporal features. The dataset contains600 songs with annotations for valence, arousal, and related sentiment metrics.A multi target regression model based on HistGradientBoostingRegressor istrained on log transformed engagement ratios to address skewed targets.Performance is evaluated with both a custom order of magnitude accuracy andstandard regression metrics, including the coefficient of determination (R^2).Results show that emotional and temporal metadata, together with existing viewcounts, predict future engagement effectively. The model attains R^2 = 0.98 forlikes but only R^2 = 0.41 for comments. This gap indicates that likes arelargely driven by readily captured affective and exposure signals, whereascomments depend on additional factors not represented in the current featureset.</description><author>Yunwoo Kim, Junhyuk Hwang</author><pubDate>Fri, 29 Aug 2025 14:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21650v1</guid></item><item><title>Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI</title><link>http://arxiv.org/abs/2508.21648v1</link><description>Bias in medical artificial intelligence is conventionally viewed as a defectrequiring elimination. However, human reasoning inherently incorporates biasesshaped by education, culture, and experience, suggesting their presence may beinevitable and potentially valuable. We propose MEDLEY (Medical EnsembleDiagnostic system with Leveraged diversitY), a conceptual framework thatorchestrates multiple AI models while preserving their diverse outputs ratherthan collapsing them into a consensus. Unlike traditional approaches thatsuppress disagreement, MEDLEY documents model-specific biases as potentialstrengths and treats hallucinations as provisional hypotheses for clinicianverification. A proof-of-concept demonstrator was developed using over 30 largelanguage models, creating a minimum viable product that preserved bothconsensus and minority views in synthetic cases, making diagnostic uncertaintyand latent biases transparent for clinical oversight. While not yet a validatedclinical tool, the demonstration illustrates how structured diversity canenhance medical reasoning under clinician supervision. By reframing AIimperfection as a resource, MEDLEY offers a paradigm shift that opens newregulatory, ethical, and innovation pathways for developing trustworthy medicalAI systems.</description><author>Farhad Abtahi, Mehdi Astaraki, Fernando Seoane</author><pubDate>Fri, 29 Aug 2025 14:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21648v1</guid></item><item><title>A-MHA*: Anytime Multi-Heuristic A*</title><link>http://arxiv.org/abs/2508.21637v1</link><description>Designing good heuristic functions for graph search requires adequate domainknowledge. It is often easy to design heuristics that perform well andcorrelate with the underlying true cost-to-go values in certain parts of thesearch space but these may not be admissible throughout the domain therebyaffecting the optimality guarantees of the search. Bounded suboptimal searchusing several such partially good but inadmissible heuristics was developed inMulti-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissibleheuristics to potentially generate a faster suboptimal solution, the originalversion does not improve the solution over time. It is a one shot algorithmthat requires careful setting of inflation factors to obtain a desired one timesolution. In this work, we tackle this issue by extending MHA* to an anytimeversion that finds a feasible suboptimal solution quickly and continuallyimproves it until time runs out. Our work is inspired from the AnytimeRepairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*concepts in the MHA* framework preserves the original suboptimal andcompleteness guarantees and enhances MHA* to perform in an anytime fashion.Furthermore, we report the performance of A-MHA* in 3-D path planning domainand sliding tiles puzzle and compare against MHA* and other anytime algorithms.</description><author>Ramkumar Natarajan, Muhammad Suhail Saleem, William Xiao, Sandip Aine, Howie Choset, Maxim Likhachev</author><pubDate>Fri, 29 Aug 2025 14:00:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21637v1</guid></item><item><title>The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</title><link>http://arxiv.org/abs/2508.21635v1</link><description>We present a multi-modal dataset collected in a soybean crop field,comprising over two hours of recorded data from sensors such as stereo infraredcamera, color camera, accelerometer, gyroscope, magnetometer, GNSS (SinglePoint Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheelodometry. This dataset captures key challenges inherent to robotics inagricultural environments, including variations in natural lighting, motionblur, rough terrain, and long, perceptually aliased sequences. By addressingthese complexities, the dataset aims to support the development andbenchmarking of advanced algorithms for localization, mapping, perception, andnavigation in agricultural robotics. The platform and data collection system isdesigned to meet the key requirements for evaluating multi-modal SLAM systems,including hardware synchronization of sensors, 6-DOF ground truth and loops onlong trajectories. We run multimodal state-of-the art SLAM methods on the dataset, showcasingthe existing limitations in their application on agricultural settings. Thedataset and utilities to work with it are released onhttps://cifasis.github.io/rosariov2/.</description><author>Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano GarcÃ­a, GastÃ³n Castro, TaihÃº Pire</author><pubDate>Fri, 29 Aug 2025 13:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21635v1</guid></item><item><title>QZhou-Embedding Technical Report</title><link>http://arxiv.org/abs/2508.21632v1</link><description>We present QZhou-Embedding, a general-purpose contextual text embedding modelwith exceptional text representation capabilities. Built upon theQwen2.5-7B-Instruct foundation model, we designed a unified multi-taskframework comprising specialized data transformation and training strategies.The data transformation scheme enables the incorporation of more diversetextual training datasets, while the task-specific training strategies enhancemodel learning efficiency. We developed a data synthesis pipeline leveragingLLM API, incorporating techniques such as paraphrasing, augmentation, and hardnegative example generation to improve the semantic richness and sampledifficulty of the training set. Additionally, we employ a two-stage trainingstrategy, comprising initial retrieval-focused pretraining followed byfull-task fine-tuning, enabling the embedding model to extend its capabilitiesbased on robust retrieval performance. Our model achieves state-of-the-artresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards(August 27 2025), and simultaneously achieves state-of-the-art performance ontasks including reranking, clustering, etc. Our findings demonstrate thathigher-quality, more diverse data is crucial for advancing retrieval modelperformance, and that leveraging LLMs generative capabilities can furtheroptimize data quality for embedding model breakthroughs. Our model weights arereleased on HuggingFace under Apache 2.0 license. For reproducibility, weprovide evaluation code and instructions on GitHub.</description><author>Peng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu</author><pubDate>Fri, 29 Aug 2025 13:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21632v1</guid></item><item><title>Robustness is Important: Limitations of LLMs for Data Fitting</title><link>http://arxiv.org/abs/2508.19563v2</link><description>Large Language Models (LLMs) are being applied in a wide array of settings,well beyond the typical language-oriented use cases. In particular, LLMs areincreasingly used as a plug-and-play method for fitting data and generatingpredictions. Prior work has shown that LLMs, via in-context learning orsupervised fine-tuning, can perform competitively with many tabular supervisedlearning techniques in terms of predictive performance. However, we identify acritical vulnerability of using LLMs for data fitting -- making changes to datarepresentation that are completely irrelevant to the underlying learning taskcan drastically alter LLMs' predictions on the same data. For example, simplychanging variable names can sway the size of prediction error by as much as 82%in certain settings. Such prediction sensitivity with respect totask-irrelevant variations manifests under both in-context learning andsupervised fine-tuning, for both close-weight and open-weight general-purposeLLMs. Moreover, by examining the attention scores of an open-weight LLM, wediscover a non-uniform attention pattern: training examples and variablenames/values which happen to occupy certain positions in the prompt receivemore attention when output tokens are generated, even though differentpositions are expected to receive roughly the same attention. This partiallyexplains the sensitivity in the presence of task-irrelevant variations. We alsoconsider a state-of-the-art tabular foundation model (TabPFN) trainedspecifically for data fitting. Despite being explicitly designed to achieveprediction robustness, TabPFN is still not immune to task-irrelevantvariations. Overall, despite LLMs' impressive predictive capabilities,currently they lack even the basic level of robustness to be used as aprincipled data-fitting tool.</description><author>Hejia Liu, Mochen Yang, Gediminas Adomavicius</author><pubDate>Fri, 29 Aug 2025 13:46:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19563v2</guid></item><item><title>Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks</title><link>http://arxiv.org/abs/2508.21628v1</link><description>As Large Language Models (LLMs) increasingly integrate into everydayworkflows, where users shape outcomes through multi-turn collaboration, acritical question emerges: do users with different personality traitssystematically prefer certain LLMs over others? We conducted a study with 32participants evenly distributed across four Keirsey personality types,evaluating their interactions with GPT-4 and Claude 3.5 across fourcollaborative tasks: data analysis, creative writing, information retrieval,and writing assistance. Results revealed significant personality-drivenpreferences: Rationals strongly preferred GPT-4, particularly for goal-orientedtasks, while idealists favored Claude 3.5, especially for creative andanalytical tasks. Other personality types showed task-dependent preferences.Sentiment analysis of qualitative feedback confirmed these patterns. Notably,aggregate helpfulness ratings were similar across models, showing howpersonality-based analysis reveals LLM differences that traditional evaluationsmiss.</description><author>Sarfaroz Yunusov, Kaige Chen, Kazi Nishat Anwar, Ali Emami</author><pubDate>Fri, 29 Aug 2025 13:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21628v1</guid></item><item><title>Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study</title><link>http://arxiv.org/abs/2508.21622v1</link><description>This paper presents an integrated framework that combines traditional networkoptimization models with large language models (LLMs) to deliver interactive,explainable, and role-aware decision support for supply chain planning. Theproposed system bridges the gap between complex operations research outputs andbusiness stakeholder understanding by generating natural language summaries,contextual visualizations, and tailored key performance indicators (KPIs). Thecore optimization model addresses tactical inventory redistribution across anetwork of distribution centers for multi-period and multi-item, using amixed-integer formulation. The technical architecture incorporates AI agents,RESTful APIs, and a dynamic user interface to support real-time interaction,configuration updates, and simulation-based insights. A case study demonstrateshow the system improves planning outcomes by preventing stockouts, reducingcosts, and maintaining service levels. Future extensions include integratingprivate LLMs, transfer learning, reinforcement learning, and Bayesian neuralnetworks to enhance explainability, adaptability, and real-timedecision-making.</description><author>Saravanan Venkatachalam</author><pubDate>Fri, 29 Aug 2025 13:34:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21622v1</guid></item><item><title>Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning</title><link>http://arxiv.org/abs/2505.17464v3</link><description>Retrieval-augmented generation (RAG) enhances large language models (LLMs) byincorporating external knowledge. Current hybrid RAG system retrieves evidencefrom both knowledge graphs (KGs) and text documents to support LLM reasoning.However, it faces challenges like handling multi-hop reasoning, multi-entityquestions, multi-source verification, and effective graph utilization. Toaddress these limitations, we present Hydra, a training-free framework thatunifies graph topology, document semantics, and source reliability to supportdeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entityproblems through agent-driven exploration that combines structured andunstructured retrieval, increasing both diversity and precision of evidence. Totackle multi-source verification, Hydra uses a tri-factor cross-sourceverification (source trustworthiness assessment, cross-source corroboration,and entity-path alignment), to balance topic relevance with cross-modalagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,guides efficient exploration, and prunes noise early. Comprehensive experimentson seven benchmark datasets show that Hydra achieves overall state-of-the-artresults on all benchmarks with GPT-3.5, outperforming the strong hybridbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydraenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performancecomparable to that of GPT-4-Turbo. The source code is available onhttps://stevetantan.github.io/Hydra/.</description><author>Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang</author><pubDate>Fri, 29 Aug 2025 13:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.17464v3</guid></item><item><title>Introduction to the Analysis of Probabilistic Decision-Making Algorithms</title><link>http://arxiv.org/abs/2508.21620v1</link><description>Decision theories offer principled methods for making choices under varioustypes of uncertainty. Algorithms that implement these theories have beensuccessfully applied to a wide range of real-world problems, includingmaterials and drug discovery. Indeed, they are desirable since they canadaptively gather information to make better decisions in the future, resultingin data-efficient workflows. In scientific discovery, where experiments arecostly, these algorithms can thus significantly reduce the cost ofexperimentation. Theoretical analyses of these algorithms are crucial forunderstanding their behavior and providing valuable insights for developingnext-generation algorithms. However, theoretical analyses in the literature areoften inaccessible to non-experts. This monograph aims to provide anaccessible, self-contained introduction to the theoretical analysis of commonlyused probabilistic decision-making algorithms, including bandit algorithms,Bayesian optimization, and tree search algorithms. Only basic knowledge ofprobability theory and statistics, along with some elementary knowledge aboutGaussian processes, is assumed.</description><author>Agustinus Kristiadi</author><pubDate>Fri, 29 Aug 2025 13:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21620v1</guid></item><item><title>Physics-Informed Spectral Modeling for Hyperspectral Imaging</title><link>http://arxiv.org/abs/2508.21618v1</link><description>We present PhISM, a physics-informed deep learning architecture that learnswithout supervision to explicitly disentangle hyperspectral observations andmodel them with continuous basis functions. \mname outperforms prior methods onseveral classification and regression benchmarks, requires limited labeleddata, and provides additional insights thanks to interpretable latentrepresentation.</description><author>Zuzanna Gawrysiak, Krzysztof Krawiec</author><pubDate>Fri, 29 Aug 2025 13:32:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21618v1</guid></item><item><title>Adapting to Change: A Comparison of Continual and Transfer Learning for Modeling Building Thermal Dynamics under Concept Drifts</title><link>http://arxiv.org/abs/2508.21615v1</link><description>Transfer Learning (TL) is currently the most effective approach for modelingbuilding thermal dynamics when only limited data are available. TL uses apretrained model that is fine-tuned to a specific target building. However, itremains unclear how to proceed after initial fine-tuning, as more operationalmeasurement data are collected over time. This challenge becomes even morecomplex when the dynamics of the building change, for example, after a retrofitor a change in occupancy. In Machine Learning literature, Continual Learning(CL) methods are used to update models of changing systems. TL approaches canalso address this challenge by reusing the pretrained model at each update stepand fine-tuning it with new measurement data. A comprehensive study on how toincorporate new measurement data over time to improve prediction accuracy andaddress the challenges of concept drifts (changes in dynamics) for buildingthermal dynamics is still missing. Therefore, this study compares several CL and TL strategies, as well as amodel trained from scratch, for thermal dynamics modeling during buildingoperation. The methods are evaluated using 5--7 years of simulated datarepresentative of single-family houses in Central Europe, including scenarioswith concept drifts from retrofits and changes in occupancy. We propose a CLstrategy (Seasonal Memory Learning) that provides greater accuracy improvementsthan existing CL and TL methods, while maintaining low computational effort.SML outperformed the benchmark of initial fine-tuning by 28.1\% without conceptdrifts and 34.9\% with concept drifts.</description><author>Fabian Raisch, Max Langtry, Felix Koch, Ruchi Choudhary, Christoph Goebel, Benjamin Tischler</author><pubDate>Fri, 29 Aug 2025 13:29:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21615v1</guid></item><item><title>Guiding a diffusion model using sliding windows</title><link>http://arxiv.org/abs/2411.10257v3</link><description>Guidance is a widely used technique for diffusion models to enhance samplequality. Technically, guidance is realised by using an auxiliary model thatgeneralises more broadly than the primary model. Using a 2D toy example, wefirst show that it is highly beneficial when the auxiliary model exhibitssimilar but stronger generalisation errors than the primary model. Based onthis insight, we introduce \emph{masked sliding window guidance (M-SWG)}, anovel, training-free method. M-SWG upweights long-range spatial dependencies byguiding the primary model with itself by selectively restricting its receptivefield. M-SWG requires neither access to model weights from previous iterations,additional training, nor class conditioning. M-SWG achieves a superiorInception score (IS) compared to previous state-of-the-art training-freeapproaches, without introducing sample oversaturation. In conjunction withexisting guidance methods, M-SWG reaches state-of-the-art Frechet DINOv2distance on ImageNet using EDM2-XXL and DiT-XL. The code is available athttps://github.com/HHU-MMBS/swg_bmvc2025_official.</description><author>Nikolas Adaloglou, Tim Kaiser, Damir Iagudin, Markus Kollmann</author><pubDate>Fri, 29 Aug 2025 13:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.10257v3</guid></item><item><title>Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge</title><link>http://arxiv.org/abs/2508.19637v2</link><description>Flexible Electronics (FE) offer a promising alternative to rigidsilicon-based hardware for wearable healthcare devices, enabling lightweight,conformable, and low-cost systems. However, their limited integration densityand large feature sizes impose strict area and power constraints, makingML-based healthcare systems-integrating analog frontend, feature extraction andclassifier-particularly challenging. Existing FE solutions often neglectpotential system-wide solutions and focus on the classifier, overlooking thesubstantial hardware cost of feature extraction and Analog-to-DigitalConverters (ADCs)-both major contributors to area and power consumption. Inthis work, we present a holistic mixed-signal feature-to-classifier co-designframework for flexible smart wearable systems. To the best of our knowledge, wedesign the first analog feature extractors in FE, significantly reducingfeature extraction cost. We further propose an hardware-aware NAS-inspiredfeature selection strategy within ML training, enabling efficient,application-specific designs. Our evaluation on healthcare benchmarks shows ourapproach delivers highly accurate, ultra-area-efficient flexible systems-idealfor disposable, low-power wearable monitoring.</description><author>Maha Shatta, Konstantinos Balaskas, Paula Carolina Lozano Duarte, Georgios Panagopoulos, Mehdi B. Tahoori, Georgios Zervakis</author><pubDate>Fri, 29 Aug 2025 13:09:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19637v2</guid></item><item><title>Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation</title><link>http://arxiv.org/abs/2508.19660v2</link><description>Printed electronics offer a promising alternative for applications beyondsilicon-based systems, requiring properties like flexibility, stretchability,conformality, and ultra-low fabrication costs. Despite the large feature sizesin printed electronics, printed neural networks have attracted attention formeeting target application requirements, though realizing complex circuitsremains challenging. This work bridges the gap between classification accuracyand area efficiency in printed neural networks, covering the entireprocessing-near-sensor system design and co-optimization from theanalog-to-digital interface-a major area and power bottleneck-to the digitalclassifier. We propose an automated framework for designing printed TernaryNeural Networks with arbitrary input precision, utilizing multi-objectiveoptimization and holistic approximation. Our circuits outperform existingapproximate printed neural networks by 17x in area and 59x in power on average,being the first to enable printed-battery-powered operation with under 5%accuracy loss while accounting for analog-to-digital interfacing costs.</description><author>Vojtech Mrazek, Konstantinos Balaskas, Paula Carolina Lozano Duarte, Zdenek Vasicek, Mehdi B. Tahoori, Georgios Zervakis</author><pubDate>Fri, 29 Aug 2025 13:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19660v2</guid></item><item><title>Adaptive Visual Navigation Assistant in 3D RPGs</title><link>http://arxiv.org/abs/2508.18539v2</link><description>In complex 3D game environments, players rely on visual affordances to spotmap transition points. Efficient identification of such points is important toclient-side auto-mapping, and provides an objective basis for evaluating mapcue presentation. In this work, we formalize the task of detecting traversableSpatial Transition Points (STPs)-connectors between two sub regions-andselecting the singular Main STP (MSTP), the unique STP that lies on thedesigner-intended critical path toward the player's current macro-objective,from a single game frame, proposing this as a new research focus. We introducea two-stage deep-learning pipeline that first detects potential STPs usingFaster R-CNN and then ranks them with a lightweight MSTP selector that fuseslocal and global visual features. Both stages benefit from parameter-efficientadapters, and we further introduce an optional retrieval-augmented fusion step.Our primary goal is to establish the feasibility of this problem and setbaseline performance metrics. We validate our approach on a custom-built,diverse dataset collected from five Action RPG titles. Our experiments reveal akey trade-off: while full-network fine-tuning produces superior STP detectionwith sufficient data, adapter-only transfer is significantly more robust andeffective in low-data scenarios and for the MSTP selection task. By definingthis novel problem, providing a baseline pipeline and dataset, and offeringinitial insights into efficient model adaptation, we aim to contribute tofuture AI-driven navigation aids and data-informed level-design tools.</description><author>Kaijie Xu, Clark Verbrugge</author><pubDate>Fri, 29 Aug 2025 13:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18539v2</guid></item><item><title>Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics</title><link>http://arxiv.org/abs/2508.21595v1</link><description>Many high-level multi-agent planning problems, including multi-robotnavigation and path planning, can be effectively modeled using deterministicactions and observations. In this work, we focus on such domains and introduce the class ofDeterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass ofDec-POMDPs characterized by deterministic transitions and observationsconditioned on the state and joint actions. We then propose a practical solver called Iterative Deterministic POMDPPlanning (IDPP). This method builds on the classic Joint Equilibrium Search forPolicies framework and is specifically optimized to handle large-scaleDet-Dec-POMDPs that current Dec-POMDP solvers are unable to addressefficiently.</description><author>Yang You, Alex Schutz, Zhikun Li, Bruno Lacerda, Robert Skilton, Nick Hawes</author><pubDate>Fri, 29 Aug 2025 12:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21595v1</guid></item></channel></rss>