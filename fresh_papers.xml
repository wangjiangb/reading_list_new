<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 14 Aug 2024 01:00:46 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The Distributional Uncertainty of the SHAP score in Explainable Machine Learning</title><link>http://arxiv.org/abs/2401.12731v4</link><description>Attribution scores reflect how important the feature values in an inputentity are for the output of a machine learning model. One of the most popularattribution scores is the SHAP score, which is an instantiation of the generalShapley value used in coalition game theory. The definition of this scorerelies on a probability distribution on the entity population. Since the exactdistribution is generally unknown, it needs to be assigned subjectively or beestimated from data, which may lead to misleading feature scores. In thispaper, we propose a principled framework for reasoning on SHAP scores underunknown entity population distributions. In our framework, we consider anuncertainty region that contains the potential distributions, and the SHAPscore of a feature becomes a function defined over this region. We study thebasic problems of finding maxima and minima of this function, which allows usto determine tight ranges for the SHAP scores of all features. In particular,we pinpoint the complexity of these problems, and other related ones, showingthem to be NP-complete. Finally, we present experiments on a real-worlddataset, showing that our framework may contribute to a more robust featurescoring.</description><author>Santiago Cifuentes, Leopoldo Bertossi, Nina Pardal, Sergio Abriola, Maria Vanina Martinez, Miguel Romero</author><pubDate>Tue, 13 Aug 2024 16:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12731v4</guid></item><item><title>FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data</title><link>http://arxiv.org/abs/2408.06273v2</link><description>Large language models (LLMs) have demonstrated prowess in a wide range oftasks. However, many LLMs exhibit significant performance discrepancies betweenhigh- and low-resource languages. To mitigate this challenge, we presentFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy theneed of the research community for balanced and high-performing multilingualcapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, istrained from scratch on a meticulously balanced multilingual data repositorythat contains 600 billion tokens covering 43 natural languages and 16programming languages. In addition to the base model, we also develop twoinstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diversemultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refinedwith DPO on a preference dataset for enhanced alignment ability. Extensiveexperiments on a wide range of multilingual benchmarks demonstrate thecompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,BLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretabilityanalyses at both the neuron and representation level suggest that FuxiTranyu isable to learn consistent multilingual representations across differentlanguages. To promote further research into multilingual LLMs and their workingmechanisms, we release both the base and instruction-tuned FuxiTranyu modelstogether with 58 pretraining checkpoints at HuggingFace and Github.</description><author>Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Du, Yikun Lei, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, Deyi Xiong</author><pubDate>Tue, 13 Aug 2024 14:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06273v2</guid></item><item><title>A Universal Flexible Near-sensor Neuromorphic Tactile System with Multi-threshold strategy for Pressure Characteristic Detection</title><link>http://arxiv.org/abs/2408.05846v2</link><description>Constructing the new generation information processing system by mimickingbiological nervous system is a feasible way for implement of high-efficientintelligent sensing device and bionic robot. However, most biological nervoussystem, especially the tactile system, have various powerful functions. This isa big challenge for bionic system design. Here we report a universal fullyflexible neuromorphic tactile perception system with strong compatibility and amultithreshold signal processing strategy. Like nervous system, signal in oursystem is transmitted as pulses and processed as threshold information. Forfeasibility verification, recognition of three different type pressure signals(continuous changing signal, Morse code signal and symbol pattern) is testedrespectively. Our system can output trend of these signals accurately and havea high accuracy in the recognition of symbol pattern and Morse code. Comparingto conventional system, consumption of our system significantly decreases in asame recognition task. Meanwhile, we give the detail introduction anddemonstration of our system universality.</description><author>Jialin Liu, Diansheng Liao</author><pubDate>Tue, 13 Aug 2024 14:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05846v2</guid></item><item><title>Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts</title><link>http://arxiv.org/abs/2408.05905v2</link><description>Current weakly supervised video anomaly detection (WSVAD) task aims toachieve frame-level anomalous event detection with only coarse video-levelannotations available. Existing works typically involve extracting globalfeatures from full-resolution video frames and training frame-level classifiersto detect anomalies in the temporal dimension. However, most anomalous eventstend to occur in localized spatial regions rather than the entire video frames,which implies existing frame-level feature based works may be misled by thedominant background information and lack the interpretation of the detectedanomalies. To address this dilemma, this paper introduces a novel method calledSTPrompt that learns spatio-temporal prompt embeddings for weakly supervisedvideo anomaly detection and localization (WSVADL) based on pre-trainedvision-language models (VLMs). Our proposed method employs a two-stream networkstructure, with one stream focusing on the temporal dimension and the otherprimarily on the spatial dimension. By leveraging the learned knowledge frompre-trained VLMs and incorporating natural motion priors from raw videos, ourmodel learns prompt embeddings that are aligned with spatio-temporal regions ofvideos (e.g., patches of individual frames) for identify specific local regionsof anomalies, enabling accurate video anomaly detection while mitigating theinfluence of background information. Without relying on detailedspatio-temporal annotations or auxiliary object detection/tracking, our methodachieves state-of-the-art performance on three public benchmarks for the WSVADLtask.</description><author>Peng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, Peng Wang, Yanning Zhang</author><pubDate>Tue, 13 Aug 2024 13:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05905v2</guid></item><item><title>Decentralized Intelligence Network (DIN)</title><link>http://arxiv.org/abs/2407.02461v4</link><description>Decentralized Intelligence Network (DIN) is a theoretical frameworkaddressing data fragmentation and siloing challenges, enabling scalable AIthrough data sovereignty. It facilitates effective AI utilization withinsovereign networks by overcoming barriers to accessing diverse data sources,leveraging: 1) personal data stores to ensure data sovereignty, where dataremains securely within Participants' control; 2) a scalable federated learningprotocol implemented on a public blockchain for decentralized AI training,where only model parameter updates are shared, keeping data within the personaldata stores; and 3) a scalable, trustless cryptographic rewards mechanism on apublic blockchain to incentivize participation and ensure fair rewarddistribution through a decentralized auditing protocol. This approachguarantees that no entity can prevent or control access to training data orinfluence financial benefits, as coordination and reward distribution aremanaged on the public blockchain with an immutable record. The frameworksupports effective AI training by allowing Participants to maintain controlover their data, benefit financially, and contribute to a decentralized,scalable ecosystem that leverages collective AI to develop beneficialalgorithms.</description><author>Abraham Nash</author><pubDate>Tue, 13 Aug 2024 13:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02461v4</guid></item><item><title>kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually Expanding Large Vocabularies</title><link>http://arxiv.org/abs/2404.09447v3</link><description>Continual segmentation has not yet tackled the challenge of improvingopen-vocabulary segmentation models with training data for accuratesegmentation across large, continually expanding vocabularies. We discover thattraditional continual training results in severe catastrophic forgetting,failing to outperform a zero-shot segmentation baseline. We introduce a noveltraining-free strategy, kNN-CLIP, which augments the model with a database ofinstance embeddings for semantic and panoptic segmentation that achieves zeroforgetting. We demonstrate that kNN-CLIP can adapt to continually growingvocabularies without the need for retraining or large memory costs. kNN-CLIPenables open-vocabulary segmentation methods to expand their vocabularies onany domain with a single pass through the data, while only storing compactembeddings. This approach minimizes both compute and memory costs. kNN-CLIPachieves state-of-the-art performance across large-vocabulary semantic andpanoptic segmentation datasets. We hope kNN-CLIP represents a significant stepforward in enabling more efficient and adaptable continual segmentation, pavingthe way for advances in real-world large-vocabulary continual segmentationmethods.</description><author>Zhongrui Gui, Shuyang Sun, Runjia Li, Jianhao Yuan, Zhaochong An, Karsten Roth, Ameya Prabhu, Philip Torr</author><pubDate>Tue, 13 Aug 2024 13:24:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09447v3</guid></item><item><title>Neural Architecture Search based Global-local Vision Mamba for Palm-Vein Recognition</title><link>http://arxiv.org/abs/2408.05743v2</link><description>Due to the advantages such as high security, high privacy, and livenessrecognition, vein recognition has been received more and more attention in pastyears. Recently, deep learning models, e.g., Mamba has shown robust featurerepresentation with linear computational complexity and successfully appliedfor visual tasks. However, vision Manba can capture long-distance featuredependencies but unfortunately deteriorate local feature details. Besides,manually designing a Mamba architecture based on human priori knowledge is verytime-consuming and error-prone. In this paper, first, we propose a hybridnetwork structure named Global-local Vision Mamba (GLVM), to learn the localcorrelations in images explicitly and global dependencies among tokens for veinfeature representation. Secondly, we design a Multi-head Mamba to learn thedependencies along different directions, so as to improve the featurerepresentation ability of vision Mamba. Thirdly, to learn the complementaryfeatures, we propose a ConvMamba block consisting of three branches, namedMulti-head Mamba branch (MHMamba), Feature Iteration Unit branch (FIU), andConvolutional Neural Network (CNN) branch, where the Feature Iteration Unitbranch aims to fuse convolutional local features with Mamba-based globalrepresentations. Finally, a Globallocal Alternate Neural Architecture Search(GLNAS) method is proposed to search the optimal architecture of GLVMalternately with the evolutionary algorithm, thereby improving the recognitionperformance for vein recognition tasks. We conduct rigorous experiments onthree public palm-vein databases to estimate the performance. The experimentalresults demonstrate that the proposed method outperforms the representativeapproaches and achieves state-of-the-art recognition accuracy.</description><author>Huafeng Qin, Yuming Fu, Jing Chen, Mounim A. El-Yacoubi, Xinbo Gao, Jun Wang</author><pubDate>Tue, 13 Aug 2024 13:02:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05743v2</guid></item><item><title>Federated Smoothing Proximal Gradient for Quantile Regression with Non-Convex Penalties</title><link>http://arxiv.org/abs/2408.05640v2</link><description>Distributed sensors in the internet-of-things (IoT) generate vast amounts ofsparse data. Analyzing this high-dimensional data and identifying relevantpredictors pose substantial challenges, especially when data is preferred toremain on the device where it was collected for reasons such as data integrity,communication bandwidth, and privacy. This paper introduces a federatedquantile regression algorithm to address these challenges. Quantile regressionprovides a more comprehensive view of the relationship between variables thanmean regression models. However, traditional approaches face difficulties whendealing with nonconvex sparse penalties and the inherent non-smoothness of theloss function. For this purpose, we propose a federated smoothing proximalgradient (FSPG) algorithm that integrates a smoothing mechanism with theproximal gradient framework, thereby enhancing both precision and computationalspeed. This integration adeptly handles optimization over a network of devices,each holding local data samples, making it particularly effective in federatedlearning scenarios. The FSPG algorithm ensures steady progress and reliableconvergence in each iteration by maintaining or reducing the value of theobjective function. By leveraging nonconvex penalties, such as the minimaxconcave penalty (MCP) and smoothly clipped absolute deviation (SCAD), theproposed method can identify and preserve key predictors within sparse models.Comprehensive simulations validate the robust theoretical foundations of theproposed algorithm and demonstrate improved estimation precision and reliableconvergence.</description><author>Reza Mirzaeifard, Diyako Ghaderyan, Stefan Werner</author><pubDate>Tue, 13 Aug 2024 11:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05640v2</guid></item><item><title>MLAAN: Scaling Supervised Local Learning with Multilaminar Leap Augmented Auxiliary Network</title><link>http://arxiv.org/abs/2406.16633v3</link><description>Deep neural networks (DNNs) typically employ an end-to-end (E2E) trainingparadigm which presents several challenges, including high GPU memoryconsumption, inefficiency, and difficulties in model parallelization duringtraining. Recent research has sought to address these issues, with onepromising approach being local learning. This method involves partitioning thebackbone network into gradient-isolated modules and manually designingauxiliary networks to train these local modules. Existing methods often neglectthe interaction of information between local modules, leading to myopic issuesand a performance gap compared to E2E training. To address these limitations,we propose the Multilaminar Leap Augmented Auxiliary Network (MLAAN).Specifically, MLAAN comprises Multilaminar Local Modules (MLM) and LeapAugmented Modules (LAM). MLM captures both local and global features throughindependent and cascaded auxiliary networks, alleviating performance issuescaused by insufficient global features. However, overly simplistic auxiliarynetworks can impede MLM's ability to capture global information. To addressthis, we further design LAM, an enhanced auxiliary network that uses theExponential Moving Average (EMA) method to facilitate information exchangebetween local modules, thereby mitigating the shortsightedness resulting frominadequate interaction. The synergy between MLM and LAM has demonstratedexcellent performance. Our experiments on the CIFAR-10, STL-10, SVHN, andImageNet datasets show that MLAAN can be seamlessly integrated into existinglocal learning frameworks, significantly enhancing their performance and evensurpassing end-to-end (E2E) training methods, while also reducing GPU memoryconsumption.</description><author>Yuming Zhang, Shouxin Zhang, Peizhe Wang, Feiyu Zhu, Dongzhi Guan, Junhao Su, Jiabin Liu, Changpeng Cai</author><pubDate>Tue, 13 Aug 2024 11:35:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16633v3</guid></item><item><title>Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation</title><link>http://arxiv.org/abs/2408.06276v2</link><description>Recent advancements in Large Language Models (LLMs) have demonstratedexceptional performance across a wide range of tasks, generating significantinterest in their application to recommendation systems. However, existingmethods have not fully capitalized on the potential of LLMs, often constrainedby limited input information or failing to fully utilize their advancedreasoning capabilities. To address these limitations, we introduce EXP3RT, anovel LLM-based recommender designed to leverage rich preference informationcontained in user and item reviews. EXP3RT is basically fine-tuned throughdistillation from a teacher LLM to perform three key tasks in order: EXP3RTfirst extracts and encapsulates essential subjective preferences from rawreviews, aggregates and summarizes them according to specific criteria tocreate user and item profiles. It then generates detailed step-by-stepreasoning followed by predicted rating, i.e., reasoning-enhanced ratingprediction, by considering both subjective and objective information fromuser/item profiles and item descriptions. This personalized preferencereasoning from EXP3RT enhances rating prediction accuracy and also providesfaithful and reasonable explanations for recommendation. Extensive experimentsshow that EXP3RT outperforms existing methods on both rating prediction andcandidate item reranking for top-k recommendation, while significantlyenhancing the explainability of recommendation systems.</description><author>Jieyong Kim, Hyunseo Kim, Hyunjin Cho, SeongKu Kang, Buru Chang, Jinyoung Yeo, Dongha Lee</author><pubDate>Tue, 13 Aug 2024 11:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06276v2</guid></item><item><title>SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning</title><link>http://arxiv.org/abs/2408.05517v2</link><description>Recent development in Large Language Models (LLMs) and Multi-modal LargeLanguage Models (MLLMs) have leverage Attention-based Transformer architecturesand achieved superior performance and generalization capabilities. They havesince covered extensive areas of traditional learning tasks. For instance,text-based tasks such as text-classification and sequence-labeling, as well asmulti-modal tasks like Visual Question Answering (VQA) and Optical CharacterRecognition (OCR), which were previously addressed using different models, cannow be tackled based on one foundation model. Consequently, the training andlightweight fine-tuning of LLMs and MLLMs, especially those based onTransformer architecture, has become particularly important. In recognition ofthese overwhelming needs, we develop SWIFT, a customizable one-stopinfrastructure for large models. With support of over $300+$ LLMs and $50+$MLLMs, SWIFT stands as the open-source framework that provide the \textit{mostcomprehensive support} for fine-tuning large models. In particular, it is thefirst training framework that provides systematic support for MLLMs. Inaddition to the core functionalities of fine-tuning, SWIFT also integratespost-training processes such as inference, evaluation, and model quantization,to facilitate fast adoptions of large models in various application scenarios.With a systematic integration of various training techniques, SWIFT offershelpful utilities such as benchmark comparisons among different trainingtechniques for large models. For fine-tuning models specialized in agentframework, we show that notable improvements on the ToolBench leader-board canbe achieved by training with customized dataset on SWIFT, with an increase of5.2%-21.8% in the Act.EM metric over various baseline models, a reduction inhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.</description><author>Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, Yingda Chen</author><pubDate>Tue, 13 Aug 2024 09:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05517v2</guid></item><item><title>Performance Evaluation of YOLOv8 Model Configurations, for Instance Segmentation of Strawberry Fruit Development Stages in an Open Field Environment</title><link>http://arxiv.org/abs/2408.05661v2</link><description>Accurate identification of strawberries during their maturing stages iscrucial for optimizing yield management, and pest control, and making informeddecisions related to harvest and post-harvest logistics. This study evaluatesthe performance of YOLOv8 model configurations for instance segmentation ofstrawberries into ripe and unripe stages in an open field environment. TheYOLOv8n model demonstrated superior segmentation accuracy with a mean AveragePrecision (mAP) of 80.9\%, outperforming other YOLOv8 configurations. In termsof inference speed, YOLOv8n processed images at 12.9 milliseconds, whileYOLOv8s, the least-performing model, processed at 22.2 milliseconds. Over 86test images with 348 ground truth labels, YOLOv8n detected 235 ripe fruitclasses and 51 unripe fruit classes out of 251 ground truth ripe fruits and 97unripe ground truth labels, respectively. In comparison, YOLOv8s detected 204ripe fruits and 37 unripe fruits. Overall, YOLOv8n achieved the fastestinference speed of 24.2 milliseconds, outperforming YOLOv8s, YOLOv8m, YOLOv8l,and YOLOv8x, which processed images at 33.0 milliseconds, 44.3 milliseconds,53.6 milliseconds, and 62.5 milliseconds, respectively. These resultsunderscore the potential of advanced object segmentation algorithms to addresscomplex visual recognition tasks in open-field agriculture effectively toaddress complex visual recognition tasks in open-field agriculture effectively.</description><author>Abdul-Razak Alhassan Gamani, Ibrahim Arhin, Adrena Kyeremateng Asamoah</author><pubDate>Tue, 13 Aug 2024 08:41:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05661v2</guid></item><item><title>EasyInv: Toward Fast and Better DDIM Inversion</title><link>http://arxiv.org/abs/2408.05159v2</link><description>This paper introduces EasyInv, an easy yet novel approach that significantlyadvances the field of DDIM Inversion by addressing the inherent inefficienciesand performance limitations of traditional iterative optimization methods. Atthe core of our EasyInv is a refined strategy for approximating inversionnoise, which is pivotal for enhancing the accuracy and reliability of theinversion process. By prioritizing the initial latent state, which encapsulatesrich information about the original images, EasyInv steers clear of theiterative refinement of noise items. Instead, we introduce a methodicalaggregation of the latent state from the preceding time step with the currentstate, effectively increasing the influence of the initial latent state andmitigating the impact of noise. We illustrate that EasyInv is capable ofdelivering results that are either on par with or exceed those of theconventional DDIM Inversion approach, especially under conditions where themodel's precision is limited or computational resources are scarce.Concurrently, our EasyInv offers an approximate threefold enhancement regardinginference efficiency over off-the-shelf iterative optimization techniques.</description><author>Ziyue Zhang, Mingbao Lin, Shuicheng Yan, Rongrong Ji</author><pubDate>Tue, 13 Aug 2024 08:23:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05159v2</guid></item><item><title>mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models</title><link>http://arxiv.org/abs/2408.04840v2</link><description>Multi-modal Large Language Models (MLLMs) have demonstrated remarkablecapabilities in executing instructions for a variety of single-image tasks.Despite this progress, significant challenges remain in modeling long imagesequences. In this work, we introduce the versatile multi-modal large languagemodel, mPLUG-Owl3, which enhances the capability for long image-sequenceunderstanding in scenarios that incorporate retrieved image-text knowledge,interleaved image-text, and lengthy videos. Specifically, we propose novelhyper attention blocks to efficiently integrate vision and language into acommon language-guided semantic space, thereby facilitating the processing ofextended multi-image scenarios. Extensive experimental results suggest thatmPLUG-Owl3 achieves state-of-the-art performance among models with a similarsize on single-image, multi-image, and video benchmarks. Moreover, we propose achallenging long visual sequence evaluation named Distractor Resistance toassess the ability of models to maintain focus amidst distractions. Finally,with the proposed architecture, mPLUG-Owl3 demonstrates outstanding performanceon ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute tothe development of more efficient and powerful multimodal large languagemodels.</description><author>Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou</author><pubDate>Tue, 13 Aug 2024 08:10:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04840v2</guid></item><item><title>The Misclassification Likelihood Matrix: Some Classes Are More Likely To Be Misclassified Than Others</title><link>http://arxiv.org/abs/2407.07818v3</link><description>This study introduces the Misclassification Likelihood Matrix (MLM) as anovel tool for quantifying the reliability of neural network predictions underdistribution shifts. The MLM is obtained by leveraging softmax outputs andclustering techniques to measure the distances between the predictions of atrained neural network and class centroids. By analyzing these distances, theMLM provides a comprehensive view of the model's misclassification tendencies,enabling decision-makers to identify the most common and critical sources oferrors. The MLM allows for the prioritization of model improvements and theestablishment of decision thresholds based on acceptable risk levels. Theapproach is evaluated on the MNIST dataset using a Convolutional Neural Network(CNN) and a perturbed version of the dataset to simulate distribution shifts.The results demonstrate the effectiveness of the MLM in assessing thereliability of predictions and highlight its potential in enhancing theinterpretability and risk mitigation capabilities of neural networks. Theimplications of this work extend beyond image classification, with ongoingapplications in autonomous systems, such as self-driving cars, to improve thesafety and reliability of decision-making in complex, real-world environments.</description><author>Daniel Sikar, Artur Garcez, Robin Bloomfield, Tillman Weyde, Kaleem Peeroo, Naman Singh, Maeve Hutchinson, Dany Laksono, Mirela Reljan-Delaney</author><pubDate>Tue, 13 Aug 2024 07:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07818v3</guid></item><item><title>etuner: Redundancy-Aware Efficient Continual Learning on Edge Devices</title><link>http://arxiv.org/abs/2401.16694v4</link><description>Many emerging applications, such as robot-assisted eldercare and objectrecognition, generally employ deep learning neural networks (DNNs) and requirethe deployment of DNN models on edge devices. These applications naturallyrequire i) handling streaming-in inference requests and ii) fine-tuning thedeployed models to adapt to possible deployment scenario changes. Continuallearning (CL) is widely adopted to satisfy these needs. CL is a popular deeplearning paradigm that handles both continuous model fine-tuning and overtimeinference requests. However, an inappropriate model fine-tuning scheme couldinvolve significant redundancy and consume considerable time and energy, makingit challenging to apply CL on edge devices. In this paper, we propose ETuner,an efficient edge continual learning framework that optimizes inferenceaccuracy, fine-tuning execution time, and energy efficiency through bothinter-tuning and intra-tuning optimizations. Experimental results show that, onaverage, ETuner reduces overall fine-tuning execution time by 64%, energyconsumption by 56%, and improves average inference accuracy by 1.75% over theimmediate model fine-tuning approach.</description><author>Sheng Li, Geng Yuan, Yawen Wu, Yue Dai, Tianyu Wang, Chao Wu, Alex K. Jones, Jingtong Hu, Yanzhi Wang, Xulong Tang</author><pubDate>Tue, 13 Aug 2024 07:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16694v4</guid></item><item><title>A Comprehensive Survey on EEG-Based Emotion Recognition: A Graph-Based Perspective</title><link>http://arxiv.org/abs/2408.06027v2</link><description>Compared to other modalities, electroencephalogram (EEG) based emotionrecognition can intuitively respond to emotional patterns in the human brainand, therefore, has become one of the most focused tasks in affectivecomputing. The nature of emotions is a physiological and psychological statechange in response to brain region connectivity, making emotion recognitionfocus more on the dependency between brain regions instead of specific brainregions. A significant trend is the application of graphs to encapsulate suchdependency as dynamic functional connections between nodes across temporal andspatial dimensions. Concurrently, the neuroscientific underpinnings behind thisdependency endow the application of graphs in this field with a distinctivesignificance. However, there is neither a comprehensive review nor a tutorialfor constructing emotion-relevant graphs in EEG-based emotion recognition. Inthis paper, we present a comprehensive survey of these studies, delivering asystematic review of graph-related methods in this field from a methodologicalperspective. We propose a unified framework for graph applications in thisfield and categorize these methods on this basis. Finally, based on previousstudies, we also present several open challenges and future directions in thisfield.</description><author>Chenyu Liu, Xinliang Zhou, Yihao Wu, Yi Ding, Liming Zhai, Kun Wang, Ziyu Jia, Yang Liu</author><pubDate>Tue, 13 Aug 2024 06:22:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06027v2</guid></item><item><title>Spb3DTracker: A Robust LiDAR-Based Person Tracker for Noisy Environment</title><link>http://arxiv.org/abs/2408.05940v2</link><description>Person detection and tracking (PDT) has seen significant advancements with 2Dcamera-based systems in the autonomous vehicle field, leading to widespreadadoption of these algorithms. However, growing privacy concerns have recentlyemerged as a major issue, prompting a shift towards LiDAR-based PDT as a viablealternative. Within this domain, "Tracking-by-Detection" (TBD) has become aprominent methodology. Despite its effectiveness, LiDAR-based PDT has not yetachieved the same level of performance as camera-based PDT. This paper examineskey components of the LiDAR-based PDT framework, including detectionpost-processing, data association, motion modeling, and lifecycle management.Building upon these insights, we introduce SpbTrack, a robust person trackerdesigned for diverse environments. Our method achieves superior performance onnoisy datasets and state-of-the-art results on KITTI Dataset benchmarks andcustom office indoor dataset among LiDAR-based trackers.</description><author>Eunsoo Im, Changhyun Jee, Jung Kwon Lee</author><pubDate>Tue, 13 Aug 2024 05:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05940v2</guid></item><item><title>Long working distance portable smartphone microscopy for metallic mesh defect detection</title><link>http://arxiv.org/abs/2408.05518v2</link><description>Metallic mesh is a transparent electromagnetic shielding film with a finemetal line structure. However, it can develop defects that affect theoptoelectronic performance whether in the production preparation or in actualuse. The development of in-situ non-destructive testing (NDT) devices formetallic mesh requires long working distances, reflective optical path design,and miniaturization. To address the limitations of existing smartphonemicroscopes, which feature short working distances and inadequate transmissionimaging for industrial in-situ inspection, we propose a novel long-workingdistance reflective smartphone microscopy system (LD-RSM). LD-RSM builds a 4foptical imaging system with external optical components and a smartphone,utilizing a beam splitter to achieve reflective imaging with the illuminationsystem and imaging system on the same side of the sample. It achieves anoptical resolution of 4.92$\mu$m and a working distance of up to 22.23 mm.Additionally, we introduce a dual prior weighted Robust Principal ComponentAnalysis (DW-RPCA) for defect detection. This approach leverages spectralfilter fusion and Hough transform to model different defect types, enhancingthe accuracy and efficiency of defect identification. Coupled with an optimizedthreshold segmentation algorithm, DW-RPCA method achieves a pixel-levelaccuracy of 84.8%. Our work showcases strong potential for growth in the fieldof in-situ on-line inspection of industrial products.</description><author>Zhengang Lu, Hongsheng Qin, Jing Li, Ming Sun, Jiubin Tan</author><pubDate>Tue, 13 Aug 2024 05:16:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05518v2</guid></item><item><title>A Laplacian-based Quantum Graph Neural Network for Semi-Supervised Learning</title><link>http://arxiv.org/abs/2408.05498v2</link><description>Laplacian learning method is a well-established technique in classicalgraph-based semi-supervised learning, but its potential in the quantum domainremains largely unexplored. This study investigates the performance of theLaplacian-based Quantum Semi-Supervised Learning (QSSL) method across fourbenchmark datasets -- Iris, Wine, Breast Cancer Wisconsin, and Heart Disease.Further analysis explores the impact of increasing Qubit counts, revealing thatadding more Qubits to a quantum system doesn't always improve performance. Theeffectiveness of additional Qubits depends on the quantum algorithm and howwell it matches the dataset. Additionally, we examine the effects of varyingentangling layers on entanglement entropy and test accuracy. The performance ofLaplacian learning is highly dependent on the number of entangling layers, withoptimal configurations varying across different datasets. Typically, moderatelevels of entanglement offer the best balance between model complexity andgeneralization capabilities. These observations highlight the crucial need forprecise hyperparameter tuning tailored to each dataset to achieve optimalperformance in Laplacian learning methods.</description><author>Hamed Gholipour, Farid Bozorgnia, Kailash Hambarde, Hamzeh Mohammadigheymasi, Javier Mancilla, Andre Sequeira, Joao Neves, Hugo Proen√ßa</author><pubDate>Tue, 13 Aug 2024 04:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05498v2</guid></item><item><title>Decentralized Intelligence Health Network (DIHN)</title><link>http://arxiv.org/abs/2408.06240v2</link><description>Decentralized Health Intelligence Network (DHIN) is a theoretical frameworkaddressing significant challenges of health data sovereignty and AI utilizationin healthcare caused by data fragmentation across providers and institutions.It establishes a sovereign architecture for healthcare provision as aprerequisite to a sovereign health network, then facilitates effective AIutilization by overcoming barriers to accessing diverse medical data sources.This comprehensive framework leverages: 1) self-sovereign identity architecturecoupled with a personal health record (PHR) as a prerequisite for health datasovereignty; 2) a scalable federated learning (FL) protocol implemented on apublic blockchain for decentralized AI training in healthcare, where healthdata remains with participants and only model parameter updates are shared; and3) a scalable, trustless rewards mechanism to incentivize participation andensure fair reward distribution. This framework ensures that no entity canprevent or control access to training on health data offered by participants ordetermine financial benefits, as these processes operate on a public blockchainwith an immutable record and without a third party. It supports effective AItraining in healthcare, allowing patients to maintain control over their healthdata, benefit financially, and contribute to a decentralized, scalableecosystem that leverages collective AI to develop beneficial healthcarealgorithms. Patients receive rewards into their digital wallets as an incentiveto opt-in to the FL protocol, with a long-term roadmap to funding decentralizedinsurance solutions. This approach introduces a novel, self-financed healthcaremodel that adapts to individual needs, complements existing systems, andredefines universal coverage. It highlights the potential to transformhealthcare data management and AI utilization while empowering patients.</description><author>Abraham Nash</author><pubDate>Tue, 13 Aug 2024 02:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06240v2</guid></item><item><title>Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs</title><link>http://arxiv.org/abs/2406.07791v3</link><description>LLM-as-a-Judge offers a promising alternative to human judges across varioustasks, yet inherent biases, particularly position bias - a systematicpreference for answers based on their position in the prompt - compromise itseffectiveness. Our study investigates this issue by developing a framework tosystematically study and quantify position bias using metrics such asrepetitional consistency, positional consistency, and positional fairness. Weconduct experiments with 9 judge models across 22 tasks from the MTBench andDevBench benchmarks and nearly 40 answer-generating models, generatingapproximately 80,000 evaluation instances. This comprehensive assessmentreveals significant variations in bias across judges and tasks. Although GPT-4often excels in positional consistency and fairness, some more cost-effectivemodels perform comparably or even better in specific tasks, highlightingessential trade-offs between consistency, fairness, and cost. Our results alsodemonstrate high consistency of judgment across repetitions, confirming thatposition bias is not due to random variations. This research significantlycontributes to the field by introducing new concepts for understanding positionbias and providing a multi-dimensional framework for evaluation. These insightsguide the selection of optimal judge models, enhance benchmark design, and laythe foundation for future research into effective debiasing strategies,ultimately enhancing the reliability of LLM evaluators.</description><author>Lin Shi, Chiyu Ma, Weicheng Ma, Soroush Vosoughi</author><pubDate>Tue, 13 Aug 2024 02:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07791v3</guid></item><item><title>ClickAttention: Click Region Similarity Guided Interactive Segmentation</title><link>http://arxiv.org/abs/2408.06021v2</link><description>Interactive segmentation algorithms based on click points have garneredsignificant attention from researchers in recent years. However, existingstudies typically use sparse click maps as model inputs to segment specifictarget objects, which primarily affect local regions and have limited abilitiesto focus on the whole target object, leading to increased times of clicks. Inaddition, most existing algorithms can not balance well between highperformance and efficiency. To address this issue, we propose a click attentionalgorithm that expands the influence range of positive clicks based on thesimilarity between positively-clicked regions and the whole input. We alsopropose a discriminative affinity loss to reduce the attention coupling betweenpositive and negative click regions to avoid an accuracy decrease caused bymutual interference between positive and negative clicks. Extensive experimentsdemonstrate that our approach is superior to existing methods and achievescutting-edge performance in fewer parameters. An interactive demo and allreproducible codes will be released athttps://github.com/hahamyt/ClickAttention.</description><author>Long Xu, Shanghong Li, Yongquan Chen, Junkang Chen, Rui Huang, Feng Wu</author><pubDate>Tue, 13 Aug 2024 02:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06021v2</guid></item><item><title>HcNet: Image Modeling with Heat Conduction Equation</title><link>http://arxiv.org/abs/2408.05901v2</link><description>Foundation models, such as CNNs and ViTs, have powered the development ofimage modeling. However, general guidance to model architecture design is stillmissing. The design of many modern model architectures, such as residualstructures, multiplicative gating signal, and feed-forward networks, can beinterpreted in terms of the heat conduction equation. This finding inspired usto model images by the heat conduction equation, where the essential idea is toconceptualize image features as temperatures and model their informationinteraction as the diffusion of thermal energy. We can take advantage of therich knowledge in the heat conduction equation to guide us in designing new andmore interpretable models. As an example, we propose Heat Conduction Layer andRefine Approximation Layer inspired by solving the heat conduction equationusing Finite Difference Method and Fourier series, respectively. This paperdoes not aim to present a state-of-the-art model; instead, it seeks tointegrate the overall architectural design of the model into the heatconduction theory framework. Nevertheless, our Heat Conduction Network (HcNet)still shows competitive performance. Code available at\url{https://github.com/ZheminZhang1/HcNet}.</description><author>Zhemin Zhang, Xun Gong</author><pubDate>Tue, 13 Aug 2024 02:23:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05901v2</guid></item><item><title>PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models</title><link>http://arxiv.org/abs/2402.07867v3</link><description>Large language models (LLMs) have achieved remarkable success due to theirexceptional generative capabilities. Despite their success, they also haveinherent limitations such as a lack of up-to-date knowledge and hallucination.Retrieval-Augmented Generation (RAG) is a state-of-the-art technique tomitigate these limitations. The key idea of RAG is to ground the answergeneration of an LLM on external knowledge retrieved from a knowledge database.Existing studies mainly focus on improving the accuracy or efficiency of RAG,leaving its security largely unexplored. We aim to bridge the gap in this work.We find that the knowledge database in a RAG system introduces a new andpractical attack surface. Based on this attack surface, we propose PoisonedRAG,the first knowledge corruption attack to RAG, where an attacker could inject afew malicious texts into the knowledge database of a RAG system to induce anLLM to generate an attacker-chosen target answer for an attacker-chosen targetquestion. We formulate knowledge corruption attacks as an optimization problem,whose solution is a set of malicious texts. Depending on the backgroundknowledge (e.g., black-box and white-box settings) of an attacker on a RAGsystem, we propose two solutions to solve the optimization problem,respectively. Our results show PoisonedRAG could achieve a 90% attack successrate when injecting five malicious texts for each target question into aknowledge database with millions of texts. We also evaluate several defensesand our results show they are insufficient to defend against PoisonedRAG,highlighting the need for new defenses.</description><author>Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia</author><pubDate>Tue, 13 Aug 2024 01:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07867v3</guid></item><item><title>On the Impact of Calibration Data in Post-training Quantization and Pruning</title><link>http://arxiv.org/abs/2311.09755v2</link><description>Quantization and pruning form the foundation of compression for neuralnetworks, enabling efficient inference for large language models (LLMs).Recently, various quantization and pruning techniques have demonstratedremarkable performance in a post-training setting. They rely upon calibrationdata, a small set of unlabeled examples that are used to generate layeractivations. However, no prior work has systematically investigated how thecalibration data impacts the effectiveness of model compression methods. Inthis paper, we present the first extensive empirical study on the effect ofcalibration data upon LLM performance. We trial a variety of quantization andpruning methods, datasets, tasks, and models. Surprisingly, we find substantialvariations in downstream task performance, contrasting existing work thatsuggests a greater level of robustness to the calibration data. Finally, wemake a series of recommendations for the effective use of calibration data inLLM quantization and pruning.</description><author>Miles Williams, Nikolaos Aletras</author><pubDate>Mon, 12 Aug 2024 17:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09755v2</guid></item><item><title>Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents</title><link>http://arxiv.org/abs/2402.00798v4</link><description>Recent advancements on Large Language Models (LLMs) enable AI Agents toautomatically generate and execute multi-step plans to solve complex tasks.However, since LLM's content generation process is hardly controllable, currentLLM-based agents frequently generate invalid or non-executable plans, whichjeopardizes the performance of the generated plans and corrupts users' trust inLLM-based agents. In response, this paper proposes a novel "Formal-LLM"framework for LLM-based agents by integrating the expressiveness of naturallanguage and the precision of formal language. Specifically, the frameworkallows agent developers to express their requirements or constraints for theplanning process as an automaton. A stack-based LLM plan generation process isthen conducted under the supervision of the automaton to ensure that thegenerated plan satisfies the constraints, making the planning processcontrollable. We conduct experiments on both benchmark tasks and practicalreal-life tasks, and our framework achieves over 50% overall performanceincrease, which validates the feasibility and effectiveness of employingFormal-LLM to guide the plan generation of agents, preventing the agents fromgenerating invalid and unsuccessful plans. Further, more controllable LLM-basedagents can facilitate the broader utilization of LLM in application scenarioswhere high validity of planning is essential. The source code of this work isavailable at https://github.com/agiresearch/Formal-LLM.</description><author>Zelong Li, Wenyue Hua, Hao Wang, He Zhu, Yongfeng Zhang</author><pubDate>Mon, 12 Aug 2024 17:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00798v4</guid></item><item><title>Benchmarking Cognitive Biases in Large Language Models as Evaluators</title><link>http://arxiv.org/abs/2309.17012v2</link><description>Large Language Models (LLMs) have recently been shown to be effective asautomatic evaluators with simple prompting and in-context learning. In thiswork, we assemble 15 LLMs of four different size ranges and evaluate theiroutput responses by preference ranking from the other LLMs as evaluators, suchas System Star is better than System Square. We then evaluate the quality ofranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators(CoBBLEr), a benchmark to measure six different cognitive biases in LLMevaluation outputs, such as the Egocentric bias where a model prefers to rankits own outputs highly in evaluation. We find that LLMs are biased text qualityevaluators, exhibiting strong indications on our bias benchmark (average of 40%of comparisons across all models) within each of their evaluations thatquestion their robustness as evaluators. Furthermore, we examine thecorrelation between human and machine preferences and calculate the averageRank-Biased Overlap (RBO) score to be 49.6%, indicating that machinepreferences are misaligned with humans. According to our findings, LLMs maystill be unable to be utilized for automatic annotation aligned with humanpreferences. Our project page is at: https://minnesotanlp.github.io/cobbler.</description><author>Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang</author><pubDate>Mon, 12 Aug 2024 17:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17012v2</guid></item><item><title>Moo-ving Beyond Tradition: Revolutionizing Cattle Behavioural Phenotyping with Pose Estimation Techniques</title><link>http://arxiv.org/abs/2408.06336v1</link><description>The cattle industry has been a major contributor to the economy of manycountries, including the US and Canada. The integration of ArtificialIntelligence (AI) has revolutionized this sector, mirroring its transformativeimpact across all industries by enabling scalable and automated monitoring andintervention practices. AI has also introduced tools and methods that automatemany tasks previously performed by human labor with the help of computervision, including health inspections. Among these methods, pose estimation hasa special place; pose estimation is the process of finding the position ofjoints in an image of animals. Analyzing the pose of animal subjects enablesprecise identification and tracking of the animal's movement and the movementsof its body parts. By summarizing the video and imagery data into movement andjoint location using pose estimation and then analyzing this information, wecan address the scalability challenge in cattle management, focusing on healthmonitoring, behavioural phenotyping and welfare concerns. Our study reviewsrecent advancements in pose estimation methodologies, their applicability inimproving the cattle industry, existing challenges, and gaps in this field.Furthermore, we propose an initiative to enhance open science frameworks withinthis field of study by launching a platform designed to connect industry andacademia.</description><author>Navid Ghassemi, Ali Goldani, Ian Q. Whishaw, Majid H. Mohajerani</author><pubDate>Mon, 12 Aug 2024 17:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06336v1</guid></item><item><title>LOLgorithm: Integrating Semantic,Syntactic and Contextual Elements for Humor Classification</title><link>http://arxiv.org/abs/2408.06335v1</link><description>This paper explores humor detection through a linguistic lens, prioritizingsyntactic, semantic, and contextual features over computational methods inNatural Language Processing. We categorize features into syntactic, semantic,and contextual dimensions, including lexicons, structural statistics, Word2Vec,WordNet, and phonetic style. Our proposed model, Colbert, utilizes BERTembeddings and parallel hidden layers to capture sentence congruity. Bycombining syntactic, semantic, and contextual features, we train Colbert forhumor detection. Feature engineering examines essential syntactic and semanticfeatures alongside BERT embeddings. SHAP interpretations and decision treesidentify influential features, revealing that a holistic approach improveshumor detection accuracy on unseen data. Integrating linguistic cues fromdifferent dimensions enhances the model's ability to understand humorcomplexity beyond traditional computational methods.</description><author>Tanisha Khurana, Kaushik Pillalamarri, Vikram Pande, Munindar Singh</author><pubDate>Mon, 12 Aug 2024 17:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06335v1</guid></item><item><title>FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection</title><link>http://arxiv.org/abs/2408.06333v1</link><description>Open Domain Question Answering (ODQA) has been advancing rapidly in recenttimes, driven by significant developments in dense passage retrieval andpretrained language models. Current models typically incorporate the FiDframework, which is composed by a neural retriever alongside an encoder-decoderneural reader. In the answer generation process, the retriever will retrievenumerous passages (around 100 for instance), each of which is then individuallyencoded by the encoder. Subsequently, the decoder makes predictions based onthese encoded passages. Nevertheless, this framework can be relativelytime-consuming, particularly due to the extensive length of the gatheredpassages. To address this, we introduce FastFiD in this paper, a novel approachthat executes sentence selection on the encoded passages. This aids inretaining valuable sentences while reducing the context length required forgenerating answers. Experiments on three commonly used datasets (NaturalQuestions, TriviaQA and ASQA) demonstrate that our method can enhance theinference speed by 2.3X-5.7X, while simultaneously maintaining the model'sperformance. Moreover, an in-depth analysis of the model's attention revealsthat the selected sentences indeed hold a substantial contribution towards thefinal answer. The codes are publicly available athttps://github.com/thunlp/FastFiD.</description><author>Yufei Huang, Xu Han, Maosong Sun</author><pubDate>Mon, 12 Aug 2024 17:50:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06333v1</guid></item><item><title>Animate, or Inanimate, That is the Question for Large Language Models</title><link>http://arxiv.org/abs/2408.06332v1</link><description>The cognitive essence of humans is deeply intertwined with the concept ofanimacy, which plays an essential role in shaping their memory, vision, andmulti-layered language understanding. Although animacy appears in language vianuanced constraints on verbs and adjectives, it is also learned and refinedthrough extralinguistic information. Similarly, we assume that the LLMs'limited abilities to understand natural language when processing animacy aremotivated by the fact that these models are trained exclusively on text. Hence, the question this paper aims to answer arises: can LLMs, in theirdigital wisdom, process animacy in a similar way to what humans would do? Wethen propose a systematic analysis via prompting approaches. In particular, weprobe different LLMs by prompting them using animate, inanimate, usual, andstranger contexts. Results reveal that, although LLMs have been trainedpredominantly on textual data, they exhibit human-like behavior when faced withtypical animate and inanimate entities in alignment with earlier studies.Hence, LLMs can adapt to understand unconventional situations by recognizingoddities as animated without needing to interface with unspoken cognitivetriggers humans rely on to break down animations.</description><author>Leonardo Ranaldi, Giulia Pucci, Fabio Massimo Zanzotto</author><pubDate>Mon, 12 Aug 2024 17:48:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06332v1</guid></item><item><title>HeLiMOS: A Dataset for Moving Object Segmentation in 3D Point Clouds From Heterogeneous LiDAR Sensors</title><link>http://arxiv.org/abs/2408.06328v1</link><description>Moving object segmentation (MOS) using a 3D light detection and ranging(LiDAR) sensor is crucial for scene understanding and identification of movingobjects. Despite the availability of various types of 3D LiDAR sensors in themarket, MOS research still predominantly focuses on 3D point clouds frommechanically spinning omnidirectional LiDAR sensors. Thus, we are, for example,lacking a dataset with MOS labels for point clouds from solid-state LiDARsensors which have irregular scanning patterns. In this paper, we present alabeled dataset, called \textit{HeLiMOS}, that enables to test MOS approacheson four heterogeneous LiDAR sensors, including two solid-state LiDAR sensors.Furthermore, we introduce a novel automatic labeling method to substantiallyreduce the labeling effort required from human annotators. To this end, ourframework exploits an instance-aware static map building approach andtracking-based false label filtering. Finally, we provide experimental resultsregarding the performance of commonly used state-of-the-art MOS approaches onHeLiMOS that suggest a new direction for a sensor-agnostic MOS, which generallyworks regardless of the type of LiDAR sensors used to capture 3D point clouds.Our dataset is available at https://sites.google.com/view/helimos.</description><author>Hyungtae Lim, Seoyeon Jang, Benedikt Mersch, Jens Behley, Hyun Myung, Cyrill Stachniss</author><pubDate>Mon, 12 Aug 2024 17:44:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06328v1</guid></item><item><title>VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents</title><link>http://arxiv.org/abs/2408.06327v1</link><description>Large Multimodal Models (LMMs) have ushered in a new era in artificialintelligence, merging capabilities in both language and vision to form highlycapable Visual Foundation Agents. These agents are postulated to excel across amyriad of tasks, potentially approaching general artificial intelligence.However, existing benchmarks fail to sufficiently challenge or showcase thefull potential of LMMs in complex, real-world environments. To address thisgap, we introduce VisualAgentBench (VAB), a comprehensive and pioneeringbenchmark specifically designed to train and evaluate LMMs as visual foundationagents across diverse scenarios, including Embodied, Graphical User Interface,and Visual Design, with tasks formulated to probe the depth of LMMs'understanding and interaction capabilities. Through rigorous testing acrossnine proprietary LMM APIs and eight open models, we demonstrate theconsiderable yet still developing agent capabilities of these models.Additionally, VAB constructs a trajectory training set constructed throughhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, andHuman Demonstrations, promoting substantial performance improvements in LMMsthrough behavior cloning. Our work not only aims to benchmark existing modelsbut also provides a solid foundation for future development into visualfoundation agents. Code, train \&amp; test data, and part of fine-tuned open LMMsare available at \url{https://github.com/THUDM/VisualAgentBench}.</description><author>Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, Jiadai Sun, Xinyue Yang, Yu Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qinkai Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, Jie Tang</author><pubDate>Mon, 12 Aug 2024 17:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06327v1</guid></item><item><title>EqNIO: Subequivariant Neural Inertial Odometry</title><link>http://arxiv.org/abs/2408.06321v1</link><description>Presently, neural networks are widely employed to accurately estimate 2Ddisplacements and associated uncertainties from Inertial Measurement Unit (IMU)data that can be integrated into stochastic filter networks like the ExtendedKalman Filter (EKF) as measurements and uncertainties for the update step inthe filter. However, such neural approaches overlook symmetry which is acrucial inductive bias for model generalization. This oversight is notablebecause (i) physical laws adhere to symmetry principles when considering thegravity axis, meaning there exists the same transformation for both thephysical entity and the resulting trajectory, and (ii) displacements shouldremain equivariant to frame transformations when the inertial frame changes. Toaddress this, we propose a subequivariant framework by: (i) derivingfundamental layers such as linear and nonlinear layers for a subequivariantnetwork, designed to handle sequences of vectors and scalars, (ii) employingthe subequivariant network to predict an equivariant frame for the sequence ofinertial measurements. This predicted frame can then be utilized for extractinginvariant features through projection, which are integrated with arbitrarynetwork architectures, (iii) transforming the invariant output by frametransformation to obtain equivariant displacements and covariances. Wedemonstrate the effectiveness and generalization of our Equivariant Frameworkon a filter-based approach with TLIO architecture for TLIO and Aria datasets,and an end-to-end deep learning approach with RONIN architecture for RONIN,RIDI and OxIOD datasets.</description><author>Royina Karegoudra Jayanth, Yinshuang Xu, Ziyun Wang, Evangelos Chatzipantazis, Daniel Gehrig, Kostas Daniilidis</author><pubDate>Mon, 12 Aug 2024 17:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06321v1</guid></item><item><title>Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example</title><link>http://arxiv.org/abs/2408.06318v1</link><description>Large language models (LLMs) have brought autonomous agents closer toartificial general intelligence (AGI) due to their promising generalization andemergent capabilities. There is, however, a lack of studies on how LLM-basedagents behave, why they could potentially fail, and how to improve them,particularly in demanding real-world planning tasks. In this paper, as aneffort to fill the gap, we present our study using a realistic benchmark,TravelPlanner, where an agent must meet multiple constraints to generateaccurate plans. We leverage this benchmark to address four key researchquestions: (1) are LLM agents robust enough to lengthy and noisy contexts whenit comes to reasoning and planning? (2) can few-shot prompting adversely impactthe performance of LLM agents in scenarios with long context? (3) can we relyon refinement to improve plans, and (4) can fine-tuning LLMs with both positiveand negative feedback lead to further improvement? Our comprehensiveexperiments indicate that, firstly, LLMs often fail to attend to crucial partsof a long context, despite their ability to handle extensive referenceinformation and few-shot examples; secondly, they still struggle with analyzingthe long plans and cannot provide accurate feedback for refinement; thirdly, wepropose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive andnegative feedback, resulting in substantial gains over Supervised Fine-Tuning(SFT). Our findings offer in-depth insights to the community on various aspectsrelated to real-world planning applications.</description><author>Yanan Chen, Ali Pesaranghader, Tanmana Sadhu, Dong Hoon Yi</author><pubDate>Mon, 12 Aug 2024 17:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06318v1</guid></item><item><title>Body Transformer: Leveraging Robot Embodiment for Policy Learning</title><link>http://arxiv.org/abs/2408.06316v1</link><description>In recent years, the transformer architecture has become the de factostandard for machine learning algorithms applied to natural language processingand computer vision. Despite notable evidence of successful deployment of thisarchitecture in the context of robot learning, we claim that vanillatransformers do not fully exploit the structure of the robot learning problem.Therefore, we propose Body Transformer (BoT), an architecture that leveragesthe robot embodiment by providing an inductive bias that guides the learningprocess. We represent the robot body as a graph of sensors and actuators, andrely on masked attention to pool information throughout the architecture. Theresulting architecture outperforms the vanilla transformer, as well as theclassical multilayer perceptron, in terms of task completion, scalingproperties, and computational efficiency when representing either imitation orreinforcement learning policies. Additional material including the open-sourcecode is available at https://sferrazza.cc/bot_site.</description><author>Carmelo Sferrazza, Dun-Ming Huang, Fangchen Liu, Jongmin Lee, Pieter Abbeel</author><pubDate>Mon, 12 Aug 2024 17:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06316v1</guid></item><item><title>OWL2Vec4OA: Tailoring Knowledge Graph Embeddings for Ontology Alignment</title><link>http://arxiv.org/abs/2408.06310v1</link><description>Ontology alignment is integral to achieving semantic interoperability as thenumber of available ontologies covering intersecting domains is increasing.This paper proposes OWL2Vec4OA, an extension of the ontology embedding systemOWL2Vec*. While OWL2Vec* has emerged as a powerful technique for ontologyembedding, it currently lacks a mechanism to tailor the embedding to theontology alignment task. OWL2Vec4OA incorporates edge confidence values fromseed mappings to guide the random walk strategy. We present the theoreticalfoundations, implementation details, and experimental evaluation of ourproposed extension, demonstrating its potential effectiveness for ontologyalignment tasks.</description><author>Sevinj Teymurova, Ernesto Jim√©nez-Ruiz, Tillman Weyde, Jiaoyan Chen</author><pubDate>Mon, 12 Aug 2024 17:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06310v1</guid></item><item><title>Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models</title><link>http://arxiv.org/abs/2403.18957v2</link><description>Online user generated content games (UGCGs) are increasingly popular amongchildren and adolescents for social interaction and more creative onlineentertainment. However, they pose a heightened risk of exposure to explicitcontent, raising growing concerns for the online safety of children andadolescents. Despite these concerns, few studies have addressed the issue ofillicit image-based promotions of unsafe UGCGs on social media, which caninadvertently attract young users. This challenge arises from the difficulty ofobtaining comprehensive training data for UGCG images and the unique nature ofthese images, which differ from traditional unsafe content. In this work, wetake the first step towards studying the threat of illicit promotions of unsafeUGCGs. We collect a real-world dataset comprising 2,924 images that displaydiverse sexually explicit and violent content used to promote UGCGs by theirgame creators. Our in-depth studies reveal a new understanding of this problemand the urgent need for automatically flagging illicit UGCG promotions. Weadditionally create a cutting-edge system, UGCG-Guard, designed to aid socialmedia platforms in effectively identifying images used for illicit UGCGpromotions. This system leverages recently introduced large vision-languagemodels (VLMs) and employs a novel conditional prompting strategy for zero-shotdomain adaptation, along with chain-of-thought (CoT) reasoning for contextualidentification. UGCG-Guard achieves outstanding results, with an accuracy rateof 94% in detecting these images used for the illicit promotion of such gamesin real-world scenarios.</description><author>Keyan Guo, Ayush Utkarsh, Wenbo Ding, Isabelle Ondracek, Ziming Zhao, Guo Freeman, Nishant Vishwamitra, Hongxin Hu</author><pubDate>Mon, 12 Aug 2024 17:20:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18957v2</guid></item><item><title>KIX: A Knowledge and Interaction-Centric Metacognitive Framework for Task Generalization</title><link>http://arxiv.org/abs/2402.05346v2</link><description>People aptly exhibit general intelligence behaviors in solving a variety oftasks with flexibility and ability to adapt to novel situations by reusing andapplying high-level knowledge acquired over time. But artificial agents aremore like specialists, lacking such generalist behaviors. Artificial agentswill require understanding and exploiting critical structured knowledgerepresentations. We present a metacognitive generalization framework,Knowledge-Interaction-eXecution (KIX), and argue that interactions with objectsleveraging type space facilitate the learning of transferable interactionconcepts and generalization. It is a natural way of integrating knowledge intoreinforcement learning and is promising to act as an enabler for autonomous andgeneralist behaviors in artificial intelligence systems.</description><author>Arun Kumar, Paul Schrater</author><pubDate>Mon, 12 Aug 2024 17:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05346v2</guid></item><item><title>From SAM to SAM 2: Exploring Improvements in Meta's Segment Anything Model</title><link>http://arxiv.org/abs/2408.06305v1</link><description>The Segment Anything Model (SAM), introduced to the computer vision communityby Meta in April 2023, is a groundbreaking tool that allows automatedsegmentation of objects in images based on prompts such as text, clicks, orbounding boxes. SAM excels in zero-shot performance, segmenting unseen objectswithout additional training, stimulated by a large dataset of over one billionimage masks. SAM 2 expands this functionality to video, leveraging memory frompreceding and subsequent frames to generate accurate segmentation across entirevideos, enabling near real-time performance. This comparison shows how SAM hasevolved to meet the growing need for precise and efficient segmentation invarious applications. The study suggests that future advancements in modelslike SAM will be crucial for improving computer vision technology.</description><author>Athulya Sundaresan Geetha, Muhammad Hussain</author><pubDate>Mon, 12 Aug 2024 17:17:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06305v1</guid></item><item><title>ReLU-KAN: New Kolmogorov-Arnold Networks that Only Need Matrix Addition, Dot Multiplication, and ReLU</title><link>http://arxiv.org/abs/2406.02075v2</link><description>Limited by the complexity of basis function (B-spline) calculations,Kolmogorov-Arnold Networks (KAN) suffer from restricted parallel computingcapability on GPUs. This paper proposes a novel ReLU-KAN implementation thatinherits the core idea of KAN. By adopting ReLU (Rectified Linear Unit) andpoint-wise multiplication, we simplify the design of KAN's basis function andoptimize the computation process for efficient CUDA computing. The proposedReLU-KAN architecture can be readily implemented on existing deep learningframeworks (e.g., PyTorch) for both inference and training. Experimentalresults demonstrate that ReLU-KAN achieves a 20x speedup compared totraditional KAN with 4-layer networks. Furthermore, ReLU-KAN exhibits a morestable training process with superior fitting ability while preserving the"catastrophic forgetting avoidance" property of KAN. You can get the code inhttps://github.com/quiqi/relu_kan</description><author>Qi Qiu, Tao Zhu, Helin Gong, Liming Chen, Huansheng Ning</author><pubDate>Mon, 12 Aug 2024 17:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02075v2</guid></item><item><title>Long-Form Answers to Visual Questions from Blind and Low Vision People</title><link>http://arxiv.org/abs/2408.06303v1</link><description>Vision language models can now generate long-form answers to questions aboutimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, adataset of long-form answers to visual questions posed by blind and low vision(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,collected from human expert describers and six VQA models. We develop andannotate functional roles of sentences of LFVQA and demonstrate that long-formanswers contain information beyond the question answer such as explanations andsuggestions. We further conduct automatic and human evaluations with BLV andsighted people to evaluate long-form answers. BLV people perceive bothhuman-written and generated long-form answers to be plausible, but generatedanswers often hallucinate incorrect visual details, especially for unanswerablevisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,we evaluate the ability of VQA models to abstain from answering unanswerablequestions across multiple prompting strategies.</description><author>Mina Huh, Fangyuan Xu, Yi-Hao Peng, Chongyan Chen, Hansika Murugu, Danna Gurari, Eunsol Choi, Amy Pavel</author><pubDate>Mon, 12 Aug 2024 17:15:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06303v1</guid></item><item><title>Finding Patterns in Ambiguity: Interpretable Stress Testing in the Decision~Boundary</title><link>http://arxiv.org/abs/2408.06302v1</link><description>The increasing use of deep learning across various domains highlights theimportance of understanding the decision-making processes of these black-boxmodels. Recent research focusing on the decision boundaries of deepclassifiers, relies on generated synthetic instances in areas of lowconfidence, uncovering samples that challenge both models and humans. Wepropose a novel approach to enhance the interpretability of deep binaryclassifiers by selecting representative samples from the decision boundary -prototypes - and applying post-model explanation algorithms. We evaluate theeffectiveness of our approach through 2D visualizations and GradientSHAPanalysis. Our experiments demonstrate the potential of the proposed method,revealing distinct and compact clusters and diverse prototypes that captureessential features that lead to low-confidence decisions. By offering a moreaggregated view of deep classifiers' decision boundaries, our work contributesto the responsible development and deployment of reliable machine learningsystems.</description><author>In√™s Gomes, Lu√≠s F. Teixeira, Jan N. van Rijn, Carlos Soares, Andr√© Restivo, Lu√≠s Cunha, Mois√©s Santos</author><pubDate>Mon, 12 Aug 2024 17:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06302v1</guid></item><item><title>RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems</title><link>http://arxiv.org/abs/2403.09040v2</link><description>Retrieval-augmented generation (RAG) can significantly improve theperformance of language models (LMs) by providing additional context for taskssuch as document-based question answering (DBQA). However, the effectiveness ofRAG is highly dependent on its configuration. To systematically find theoptimal configuration, we introduce RAGGED, a framework for analyzing RAGconfigurations across various DBQA tasks. Using the framework, we discoverdistinct LM behaviors in response to varying context quantities, contextqualities, and retrievers. For instance, while some models are robust to noisycontexts, monotonically performing better with more contexts, others are morenoise-sensitive and can effectively use only a few contexts before declining inperformance. This framework also provides a deeper analysis of thesedifferences by evaluating the LMs' sensitivity to signal and noise underspecific context quality conditions. Using RAGGED, researchers andpractitioners can derive actionable insights about how to optimally configuretheir RAG systems for their specific question-answering tasks.</description><author>Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, Graham Neubig</author><pubDate>Mon, 12 Aug 2024 17:12:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09040v2</guid></item><item><title>Inverse designing metamaterials with programmable nonlinear functional responses in graph space</title><link>http://arxiv.org/abs/2408.06300v1</link><description>Material responses to static and dynamic stimuli, represented as nonlinearcurves, are design targets for engineering functionalities like structuralsupport, impact protection, and acoustic and photonic bandgaps.Three-dimensional metamaterials offer significant tunability due to theirinternal structure, yet existing methods struggle to capture their complexbehavior-to-structure relationships. We present GraphMetaMat, a graph-basedframework capable of designing three-dimensional metamaterials withprogrammable responses and arbitrary manufacturing constraints. Integratinggraph networks, physics biases, reinforcement learning, and tree search,GraphMetaMat can target stress-strain curves spanning four orders of magnitudeand complex behaviors, as well as viscoelastic transmission responses withvarying attenuation gaps. GraphMetaMat can create cushioning materials forprotective equipment and vibration-damping panels for electric vehicles,outperforming commercial materials, and enabling the automatic design ofmaterials with on-demand functionalities.</description><author>Marco Maurizi, Derek Xu, Yu-Tong Wang, Desheng Yao, David Hahn, Mourad Oudich, Anish Satpati, Mathieu Bauchy, Wei Wang, Yizhou Sun, Yun Jing, Xiaoyu Rayne Zheng</author><pubDate>Mon, 12 Aug 2024 17:09:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06300v1</guid></item><item><title>LEARN: An Invex Loss for Outlier Oblivious Robust Online Optimization</title><link>http://arxiv.org/abs/2408.06297v1</link><description>We study a robust online convex optimization framework, where an adversarycan introduce outliers by corrupting loss functions in an arbitrary number ofrounds k, unknown to the learner. Our focus is on a novel setting allowingunbounded domains and large gradients for the losses without relying on aLipschitz assumption. We introduce the Log Exponential Adjusted Robust andiNvex (LEARN) loss, a non-convex (invex) robust loss function to mitigate theeffects of outliers and develop a robust variant of the online gradient descentalgorithm by leveraging the LEARN loss. We establish tight regret guarantees(up to constants), in a dynamic setting, with respect to the uncorrupted roundsand conduct experiments to validate our theory. Furthermore, we present aunified analysis framework for developing online optimization algorithms fornon-convex (invex) losses, utilizing it to provide regret bounds with respectto the LEARN loss, which may be of independent interest.</description><author>Adarsh Barik, Anand Krishna, Vincent Y. F. Tan</author><pubDate>Mon, 12 Aug 2024 17:08:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06297v1</guid></item><item><title>An Experimental Comparison of Partitioning Strategies for Distributed Graph Neural Network Training</title><link>http://arxiv.org/abs/2308.15602v2</link><description>Recently, graph neural networks (GNNs) have gained much attention as agrowing area of deep learning capable of learning on graph-structured data.However, the computational and memory requirements for training GNNs onlarge-scale graphs make it necessary to distribute the training. A prerequisitefor distributed GNN training is to partition the input graph into smaller partsthat are distributed among multiple machines of a compute cluster. Althoughgraph partitioning has been studied with regard to graph analytics and graphdatabases, its effect on GNN training performance is largely unexplored. As aconsequence, it is unclear whether investing computational efforts intohigh-quality graph partitioning would pay off in GNN training scenarios. In this paper, we study the effectiveness of graph partitioning fordistributed GNN training. Our study aims to understand how different factorssuch as GNN parameters, mini-batch size, graph type, features size, andscale-out factor influence the effectiveness of graph partitioning. We conductexperiments with two different GNN systems using vertex and edge partitioning.We found that high-quality graph partitioning is a very effective optimizationto speed up GNN training and to reduce memory consumption. Furthermore, ourresults show that invested partitioning time can quickly be amortized byreduced GNN training time, making it a relevant optimization for most GNNscenarios. Compared to research on distributed graph processing, our studyreveals that graph partitioning plays an even more significant role indistributed GNN training, which motivates further research on the graphpartitioning problem.</description><author>Nikolai Merkel, Daniel Stoll, Ruben Mayer, Hans-Arno Jacobsen</author><pubDate>Mon, 12 Aug 2024 17:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15602v2</guid></item><item><title>Non-Stationary Latent Auto-Regressive Bandits</title><link>http://arxiv.org/abs/2402.03110v2</link><description>We consider the stochastic multi-armed bandit problem with non-stationaryrewards. We present a novel formulation of non-stationarity in the environmentwhere changes in the mean reward of the arms over time are due to some unknown,latent, auto-regressive (AR) state of order $k$. We call this new environmentthe latent AR bandit. Different forms of the latent AR bandit appear in manyreal-world settings, especially in emerging scientific fields such asbehavioral health or education where there are few mechanistic models of theenvironment. If the AR order $k$ is known, we propose an algorithm thatachieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, ouralgorithm outperforms standard UCB across multiple non-stationary environments,even if $k$ is mis-specified.</description><author>Anna L. Trella, Walter Dempsey, Finale Doshi-Velez, Susan A. Murphy</author><pubDate>Mon, 12 Aug 2024 16:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03110v2</guid></item><item><title>OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation</title><link>http://arxiv.org/abs/2309.00616v5</link><description>In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3Dopen-vocabulary scene understanding. The OpenIns3D framework employs a"Mask-Snap-Lookup" scheme. The "Mask" module learns class-agnostic maskproposals in 3D point clouds, the "Snap" module generates synthetic scene-levelimages at multiple scales and leverages 2D vision-language models to extractinteresting objects, and the "Lookup" module searches through the outcomes of"Snap" to assign category names to the proposed masks. This approach, yetsimple, achieves state-of-the-art performance across a wide range of 3Dopen-vocabulary tasks, including recognition, object detection, and instancesegmentation, on both indoor and outdoor datasets. Moreover, OpenIns3Dfacilitates effortless switching between different 2D detectors withoutrequiring retraining. When integrated with powerful 2D open-world models, itachieves excellent results in scene understanding tasks. Furthermore, whencombined with LLM-powered 2D models, OpenIns3D exhibits an impressivecapability to comprehend and process highly complex text queries that demandintricate reasoning and real-world knowledge. Project page:https://zheninghuang.github.io/OpenIns3D/</description><author>Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, Joan Lasenby</author><pubDate>Mon, 12 Aug 2024 16:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00616v5</guid></item><item><title>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</title><link>http://arxiv.org/abs/2408.06292v1</link><description>One of the grand challenges of artificial general intelligence is developingagents capable of conducting scientific research and discovering new knowledge.While frontier models have already been used as aids to human scientists, e.g.for brainstorming ideas, writing code, or prediction tasks, they still conductonly a small part of the scientific process. This paper presents the firstcomprehensive framework for fully automatic scientific discovery, enablingfrontier large language models to perform research independently andcommunicate their findings. We introduce The AI Scientist, which generatesnovel research ideas, writes code, executes experiments, visualizes results,describes its findings by writing a full scientific paper, and then runs asimulated review process for evaluation. In principle, this process can berepeated to iteratively develop ideas in an open-ended fashion, acting like thehuman scientific community. We demonstrate its versatility by applying it tothree distinct subfields of machine learning: diffusion modeling,transformer-based language modeling, and learning dynamics. Each idea isimplemented and developed into a full paper at a cost of less than $15 perpaper. To evaluate the generated papers, we design and validate an automatedreviewer, which we show achieves near-human performance in evaluating paperscores. The AI Scientist can produce papers that exceed the acceptancethreshold at a top machine learning conference as judged by our automatedreviewer. This approach signifies the beginning of a new era in scientificdiscovery in machine learning: bringing the transformative benefits of AIagents to the entire research process of AI itself, and taking us closer to aworld where endless affordable creativity and innovation can be unleashed onthe world's most challenging problems. Our code is open-sourced athttps://github.com/SakanaAI/AI-Scientist</description><author>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha</author><pubDate>Mon, 12 Aug 2024 16:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06292v1</guid></item><item><title>Mambular: A Sequential Model for Tabular Deep Learning</title><link>http://arxiv.org/abs/2408.06291v1</link><description>The analysis of tabular data has traditionally been dominated bygradient-boosted decision trees (GBDTs), known for their proficiency with mixedcategorical and numerical features. However, recent deep learning innovationsare challenging this dominance. We introduce Mambular, an adaptation of theMamba architecture optimized for tabular data. We extensively benchmarkMambular against state-of-the-art models, including neural networks andtree-based methods, and demonstrate its competitive performance across diversedatasets. Additionally, we explore various adaptations of Mambular tounderstand its effectiveness for tabular data. We investigate different poolingstrategies, feature interaction mechanisms, and bi-directional processing. Ouranalysis shows that interpreting features as a sequence and passing themthrough Mamba layers results in surprisingly performant models. The resultshighlight Mambulars potential as a versatile and powerful architecture fortabular data analysis, expanding the scope of deep learning applications inthis domain. The source code is available at https://github.com/basf/mamba-tabular.</description><author>Anton Frederik Thielmann, Manish Kumar, Christoph Weisser, Arik Reuter, Benjamin S√§fken, Soheila Samiee</author><pubDate>Mon, 12 Aug 2024 16:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06291v1</guid></item><item><title>Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials</title><link>http://arxiv.org/abs/2402.17003v2</link><description>Online reinforcement learning (RL) algorithms offer great potential forpersonalizing treatment for participants in clinical trials. However, deployingan online, autonomous algorithm in the high-stakes healthcare setting makesquality control and data quality especially difficult to achieve. This paperproposes algorithm fidelity as a critical requirement for deploying online RLalgorithms in clinical trials. It emphasizes the responsibility of thealgorithm to (1) safeguard participants and (2) preserve the scientific utilityof the data for post-trial analyses. We also present a framework forpre-deployment planning and real-time monitoring to help algorithm developersand clinical researchers ensure algorithm fidelity. To illustrate ourframework's practical application, we present real-world examples from theOralytics clinical trial. Since Spring 2023, this trial successfully deployedan autonomous, online RL algorithm to personalize behavioral interventions forparticipants at risk for dental disease.</description><author>Anna L. Trella, Kelly W. Zhang, Inbal Nahum-Shani, Vivek Shetty, Iris Yan, Finale Doshi-Velez, Susan A. Murphy</author><pubDate>Mon, 12 Aug 2024 16:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17003v2</guid></item><item><title>Fair Column Subset Selection</title><link>http://arxiv.org/abs/2306.04489v4</link><description>The problem of column subset selection asks for a subset of columns from aninput matrix such that the matrix can be reconstructed as accurately aspossible within the span of the selected columns. A natural extension is toconsider a setting where the matrix rows are partitioned into two groups, andthe goal is to choose a subset of columns that minimizes the maximumreconstruction error of both groups, relative to their respective best rank-kapproximation. Extending the known results of column subset selection to thisfair setting is not straightforward: in certain scenarios it is unavoidable tochoose columns separately for each group, resulting in double the expectedcolumn count. We propose a deterministic leverage-score sampling strategy forthe fair setting and show that sampling a column subset of minimum size becomesNP-hard in the presence of two groups. Despite these negative results, we givean approximation algorithm that guarantees a solution within 1.5 times theoptimal solution size. We also present practical heuristic algorithms based onrank-revealing QR factorization. Finally, we validate our methods through anextensive set of experiments using real-world data.</description><author>Antonis Matakos, Bruno Ordozgoiti, Suhas Thejaswi</author><pubDate>Mon, 12 Aug 2024 16:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04489v4</guid></item><item><title>Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning</title><link>http://arxiv.org/abs/2311.17693v3</link><description>Robot-assisted surgical systems have demonstrated significant potential inenhancing surgical precision and minimizing human errors. However, existingsystems cannot accommodate individual surgeons' unique preferences andrequirements. Additionally, they primarily focus on general surgeries (e.g.,laparoscopy) and are unsuitable for highly precise microsurgeries, such asophthalmic procedures. Thus, we propose an image-guided approach forsurgeon-centered autonomous agents that can adapt to the individual surgeon'sskill level and preferred surgical techniques during ophthalmic cataractsurgery. Our approach trains reinforcement and imitation learning agentssimultaneously using curriculum learning approaches guided by image data toperform all tasks of the incision phase of cataract surgery. By integrating thesurgeon's actions and preferences into the training process, our approachenables the robot to implicitly learn and adapt to the individual surgeon'sunique techniques through surgeon-in-the-loop demonstrations. This results in amore intuitive and personalized surgical experience for the surgeon whileensuring consistent performance for the autonomous robotic apprentice. Wedefine and evaluate the effectiveness of our approach in a simulatedenvironment using our proposed metrics and highlight the trade-off between ageneric agent and a surgeon-centered adapted agent. Finally, our approach hasthe potential to extend to other ophthalmic and microsurgical procedures,opening the door to a new generation of surgeon-in-the-loop autonomous surgicalrobots. We provide an open-source simulation framework for future developmentand reproducibility athttps://github.com/amrgomaaelhady/CataractAdaptSurgRobot.</description><author>Amr Gomaa, Bilal Mahdy, Niko Kleer, Antonio Kr√ºger</author><pubDate>Mon, 12 Aug 2024 16:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17693v3</guid></item><item><title>Reinforcement Learning in High-frequency Market Making</title><link>http://arxiv.org/abs/2407.21025v2</link><description>This paper establishes a new and comprehensive theoretical analysis for theapplication of reinforcement learning (RL) in high-frequency market making. Webridge the modern RL theory and the continuous-time statistical models inhigh-frequency financial economics. Different with most existing literature onmethodological research about developing various RL methods for market makingproblem, our work is a pilot to provide the theoretical analysis. We target theeffects of sampling frequency, and find an interesting tradeoff between errorand complexity of RL algorithm when tweaking the values of the time increment$\Delta$ $-$ as $\Delta$ becomes smaller, the error will be smaller but thecomplexity will be larger. We also study the two-player case under thegeneral-sum game framework and establish the convergence of Nash equilibrium tothe continuous-time game equilibrium as $\Delta\rightarrow0$. The NashQ-learning algorithm, which is an online multi-agent RL method, is applied tosolve the equilibrium. Our theories are not only useful for practitioners tochoose the sampling frequency, but also very general and applicable to otherhigh-frequency financial decision making problems, e.g., optimal executions, aslong as the time-discretization of a continuous-time markov decision process isadopted. Monte Carlo simulation evidence support all of our theories.</description><author>Yuheng Zheng, Zihan Ding</author><pubDate>Mon, 12 Aug 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21025v2</guid></item><item><title>Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for Anti-aliasing Rendering</title><link>http://arxiv.org/abs/2408.06286v1</link><description>3D Gaussian Splatting (3DGS) has attracted great attention in novel viewsynthesis because of its superior rendering efficiency and high fidelity.However, the trained Gaussians suffer from severe zooming degradation due tonon-adjustable representation derived from single-scale training. Though somemethods attempt to tackle this problem via post-processing techniques such asselective rendering or filtering techniques towards primitives, thescale-specific information is not involved in Gaussians. In this paper, wepropose a unified optimization method to make Gaussians adaptive for arbitraryscales by self-adjusting the primitive properties (e.g., color, shape and size)and distribution (e.g., position). Inspired by the mipmap technique, we designpseudo ground-truth for the target scale and propose a scale-consistencyguidance loss to inject scale information into 3D Gaussians. Our method is aplug-in module, applicable for any 3DGS models to solve the zoom-in andzoom-out aliasing. Extensive experiments demonstrate the effectiveness of ourmethod. Notably, our method outperforms 3DGS in PSNR by an average of 9.25 dBfor zoom-in and 10.40 dB for zoom-out on the NeRF Synthetic dataset.</description><author>Jiameng Li, Yue Shi, Jiezhang Cao, Bingbing Ni, Wenjun Zhang, Kai Zhang, Luc Van Gool</author><pubDate>Mon, 12 Aug 2024 16:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06286v1</guid></item><item><title>Synthetic Patient-Physician Dialogue Generation from Clinical Notes Using LLM</title><link>http://arxiv.org/abs/2408.06285v1</link><description>Medical dialogue systems (MDS) enhance patient-physician communication,improve healthcare accessibility, and reduce costs. However, acquiring suitabledata to train these systems poses significant challenges. Privacy concernsprevent the use of real conversations, necessitating synthetic alternatives.Synthetic dialogue generation from publicly available clinical notes offers apromising solution to this issue, providing realistic data while safeguardingprivacy. Our approach, SynDial, uses a single LLM iteratively with zero-shotprompting and a feedback loop to generate and refine high-quality syntheticdialogues. The feedback consists of weighted evaluation scores for similarityand extractiveness. The iterative process ensures dialogues meet predefinedthresholds, achieving superior extractiveness as a result of the feedback loop.Additionally, evaluation shows that the generated dialogues excel in factualitymetric compared to the baselines and has comparable diversity scores with GPT4.</description><author>Trisha Das, Dina Albassam, Jimeng Sun</author><pubDate>Mon, 12 Aug 2024 16:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06285v1</guid></item><item><title>MovieSum: An Abstractive Summarization Dataset for Movie Screenplays</title><link>http://arxiv.org/abs/2408.06281v1</link><description>Movie screenplay summarization is challenging, as it requires anunderstanding of long input contexts and various elements unique to movies.Large language models have shown significant advancements in documentsummarization, but they often struggle with processing long input contexts.Furthermore, while television transcripts have received attention in recentstudies, movie screenplay summarization remains underexplored. To stimulateresearch in this area, we present a new dataset, MovieSum, for abstractivesummarization of movie screenplays. This dataset comprises 2200 moviescreenplays accompanied by their Wikipedia plot summaries. We manuallyformatted the movie screenplays to represent their structural elements.Compared to existing datasets, MovieSum possesses several distinctive features:(1) It includes movie screenplays, which are longer than scripts of TVepisodes. (2) It is twice the size of previous movie screenplay datasets. (3)It provides metadata with IMDb IDs to facilitate access to additional externalknowledge. We also show the results of recently released large language modelsapplied to summarization on our dataset to provide a detailed baseline.</description><author>Rohit Saxena, Frank Keller</author><pubDate>Mon, 12 Aug 2024 16:43:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06281v1</guid></item><item><title>Multi-marginal Schr√∂dinger Bridges with Iterative Reference</title><link>http://arxiv.org/abs/2408.06277v1</link><description>Practitioners frequently aim to infer an unobserved population trajectoryusing sample snapshots at multiple time points. For instance, in single-cellsequencing, scientists would like to learn how gene expression evolves overtime. But sequencing any cell destroys that cell. So we cannot access anycell's full trajectory, but we can access snapshot samples from many cells.Stochastic differential equations are commonly used to analyze systems withfull individual-trajectory access; since here we have only sample snapshots,these methods are inapplicable. The deep learning community has recentlyexplored using Schr\"odinger bridges (SBs) and their extensions to estimatethese dynamics. However, these methods either (1) interpolate between just twotime points or (2) require a single fixed reference dynamic within the SB,which is often just set to be Brownian motion. But learning piecewise fromadjacent time points can fail to capture long-term dependencies. Andpractitioners are typically able to specify a model class for the referencedynamic but not the exact values of the parameters within it. So we propose anew method that (1) learns the unobserved trajectories from sample snapshotsacross multiple time points and (2) requires specification only of a class ofreference dynamics, not a single fixed one. In particular, we suggest aniterative projection method inspired by Schr\"odinger bridges; we alternatebetween learning a piecewise SB on the unobserved trajectories and using thelearned SB to refine our best guess for the dynamics within the referenceclass. We demonstrate the advantages of our method via a well-known simulatedparametric model from ecology, simulated and real data from systems biology,and real motion-capture data.</description><author>Yunyi Shen, Renato Berlinghieri, Tamara Broderick</author><pubDate>Mon, 12 Aug 2024 16:39:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06277v1</guid></item><item><title>Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation</title><link>http://arxiv.org/abs/2408.06276v1</link><description>Recent advancements in Large Language Models (LLMs) have demonstratedexceptional performance across a wide range of tasks, generating significantinterest in their application to recommendation systems. However, existingmethods have not fully capitalized on the potential of LLMs, often constrainedby limited input information or failing to fully utilize their advancedreasoning capabilities. To address these limitations, we introduce EXP3RT, anovel LLM-based recommender designed to leverage rich preference informationcontained in user and item reviews. EXP3RT is basically fine-tuned throughdistillation from a teacher LLM to perform three key tasks in order: EXP3RTfirst extracts and encapsulates essential subjective preferences from rawreviews, aggregates and summarizes them according to specific criteria tocreate user and item profiles. It then generates detailed step-by-stepreasoning followed by predicted rating, i.e., reasoning-enhanced ratingprediction, by considering both subjective and objective information fromuser/item profiles and item descriptions. This personalized preferencereasoning from EXP3RT enhances rating prediction accuracy and also providesfaithful and reasonable explanations for recommendation. Extensive experimentsshow that EXP3RT outperforms existing methods on both rating prediction andcandidate item reranking for top-k recommendation, while significantlyenhancing the explainability of recommendation systems.</description><author>Jieyong Kim, Hyunseo Kim, Hyunjin Cho, SeongKu Kang, Buru Chang, Jinyoung Yeo, Dongha Lee</author><pubDate>Mon, 12 Aug 2024 16:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06276v1</guid></item><item><title>CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions</title><link>http://arxiv.org/abs/2403.08042v2</link><description>This research embarked on a comparative exploration of the holisticsegmentation capabilities of Convolutional Neural Networks (CNNs) in both 2Dand 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilizeddata from two CF reference centers, covering five major CF structural changes.Initially, it compared the 2D and 3D models, highlighting the 3D model'ssuperior capability in capturing complex features like mucus plugs andconsolidations. To improve the 2D model's performance, a loss adapted to finestructures segmentation was implemented and evaluated, significantly enhancingits accuracy, though not surpassing the 3D model's performance. The modelsunderwent further validation through external evaluation against pulmonaryfunction tests (PFTs), confirming the robustness of the findings. Moreover,this study went beyond comparing metrics; it also included comprehensiveassessments of the models' interpretability and reliability, providing valuableinsights for their clinical application.</description><author>Amel Imene Hadj Bouzid, Baudouin Denis de Senneville, Fabien Baldacci, Pascal Desbarats, Patrick Berger, Ilyes Benlala, Ga√´l Dournes</author><pubDate>Mon, 12 Aug 2024 16:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08042v2</guid></item><item><title>FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data</title><link>http://arxiv.org/abs/2408.06273v1</link><description>Large language models (LLMs) have demonstrated prowess in a wide range oftasks. However, many LLMs exhibit significant performance discrepancies betweenhigh- and low-resource languages. To mitigate this challenge, we presentFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy theneed of the research community for balanced and high-performing multilingualcapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, istrained from scratch on a meticulously balanced multilingual data repositorythat contains 600 billion tokens covering 43 natural languages and 16programming languages. In addition to the base model, we also develop twoinstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diversemultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refinedwith DPO on a preference dataset for enhanced alignment ability. Extensiveexperiments on a wide range of multilingual benchmarks demonstrate thecompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,BLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretabilityanalyses at both the neuron and representation level suggest that FuxiTranyu isable to learn consistent multilingual representations across differentlanguages. To promote further research into multilingual LLMs and their workingmechanisms, we release both the base and instruction-tuned FuxiTranyu modelstogether with 58 pretraining checkpoints at HuggingFace and Github.</description><author>Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Dui, Yikun Lei, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, Deyi Xiong</author><pubDate>Mon, 12 Aug 2024 16:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06273v1</guid></item><item><title>Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment</title><link>http://arxiv.org/abs/2408.06266v1</link><description>Large Language Models (LLMs) are often aligned using contrastive alignmentobjectives and preference pair datasets. The interaction between model, paireddata, and objective makes alignment a complicated procedure, sometimesproducing subpar results. We study this and find that (i) preference data givesa better learning signal when the underlying responses are contrastive, and(ii) alignment objectives lead to better performance when they specify morecontrol over the model during training. Based on these insights, we introduceContrastive Learning from AI Revisions (CLAIR), a data-creation method whichleads to more contrastive preference pairs, and Anchored PreferenceOptimization (APO), a controllable and more stable alignment objective. Wealign Llama-3-8B-Instruct using various comparable datasets and alignmentobjectives and measure MixEval-Hard scores, which correlate highly with humanjudgments. The CLAIR preferences lead to the strongest performance out of alldatasets, and APO consistently outperforms less controllable objectives. Ourbest model, trained on 32K CLAIR preferences with APO, improvesLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our codeis available at https://github.com/ContextualAI/CLAIR_and_APO.</description><author>Karel D'Oosterlinck, Winnie Xu, Chris Develder, Thomas Demeester, Amanpreet Singh, Christopher Potts, Douwe Kiela, Shikib Mehri</author><pubDate>Mon, 12 Aug 2024 16:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06266v1</guid></item><item><title>Audio Enhancement for Computer Audition -- An Iterative Training Paradigm Using Sample Importance</title><link>http://arxiv.org/abs/2408.06264v1</link><description>Neural network models for audio tasks, such as automatic speech recognition(ASR) and acoustic scene classification (ASC), are susceptible to noisecontamination for real-life applications. To improve audio quality, anenhancement module, which can be developed independently, is explicitly used atthe front-end of the target audio applications. In this paper, we present anend-to-end learning solution to jointly optimise the models for audioenhancement (AE) and the subsequent applications. To guide the optimisation ofthe AE module towards a target application, and especially to overcomedifficult samples, we make use of the sample-wise performance measure as anindication of sample importance. In experiments, we consider fourrepresentative applications to evaluate our training paradigm, i.e., ASR,speech command recognition (SCR), speech emotion recognition (SER), and ASC.These applications are associated with speech and non-speech tasks concerningsemantic and non-semantic features, transient and global information, and theexperimental results indicate that our proposed approach can considerably boostthe noise robustness of the models, especially at low signal-to-noise ratios(SNRs), for a wide range of computer audition tasks in everyday-life noisyenvironments.</description><author>Manuel Milling, Shuo Liu, Andreas Triantafyllopoulos, Ilhan Aslan, Bj√∂rn W. Schuller</author><pubDate>Mon, 12 Aug 2024 16:23:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06264v1</guid></item><item><title>Unified Discrete Diffusion for Categorical Data</title><link>http://arxiv.org/abs/2402.03701v2</link><description>Discrete diffusion models have seen a surge of attention with applications onnaturally discrete data such as language and graphs. Although discrete-timediscrete diffusion has been established for a while, only recently Campbell etal. (2022) introduced the first framework for continuous-time discretediffusion. However, their training and sampling processes differ significantlyfrom the discrete-time version, necessitating nontrivial approximations fortractability. In this paper, we first present a series of mathematicalsimplifications of the variational lower bound that enable more accurate andeasy-to-optimize training for discrete diffusion. In addition, we derive asimple formulation for backward denoising that enables exact and acceleratedsampling, and importantly, an elegant unification of discrete-time andcontinuous-time discrete diffusion. Thanks to simpler analytical formulations,both forward and now also backward probabilities can flexibly accommodate anynoise distribution, including different noise distributions for multi-elementobjects. Experiments show that our proposed USD3 (for Unified SimplifiedDiscrete Denoising Diffusion) outperform all SOTA baselines on establisheddatasets. We open-source our unified code athttps://github.com/LingxiaoShawn/USD3.</description><author>Lingxiao Zhao, Xueying Ding, Lijun Yu, Leman Akoglu</author><pubDate>Mon, 12 Aug 2024 16:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03701v2</guid></item><item><title>DUNE: A Machine Learning Deep UNet++ based Ensemble Approach to Monthly, Seasonal and Annual Climate Forecasting</title><link>http://arxiv.org/abs/2408.06262v1</link><description>Capitalizing on the recent availability of ERA5 monthly averaged long-termdata records of mean atmospheric and climate fields based on high-resolutionreanalysis, deep-learning architectures offer an alternative to physics-baseddaily numerical weather predictions for subseasonal to seasonal (S2S) andannual means. A novel Deep UNet++-based Ensemble (DUNE) neural architecture isintroduced, employing multi-encoder-decoder structures with residual blocks.When initialized from a prior month or year, this architecture produced thefirst AI-based global monthly, seasonal, or annual mean forecast of 2-metertemperatures (T2m) and sea surface temperatures (SST). ERA5 monthly mean datais used as input for T2m over land, SST over oceans, and solar radiation at thetop of the atmosphere for each month of 40 years to train the model. Validationforecasts are performed for an additional two years, followed by five years offorecast evaluations to account for natural annual variability. AI-trainedinference forecast weights generate forecasts in seconds, enabling ensembleseasonal forecasts. Root Mean Squared Error (RMSE), Anomaly CorrelationCoefficient (ACC), and Heidke Skill Score (HSS) statistics are presentedglobally and over specific regions. These forecasts outperform persistence,climatology, and multiple linear regression for all domains. DUNE forecastsdemonstrate comparable statistical accuracy to NOAA's operational monthly andseasonal probabilistic outlook forecasts over the US but at significantlyhigher resolutions. RMSE and ACC error statistics for other recent AI-baseddaily forecasts also show superior performance for DUNE-based forecasts. TheDUNE model's application to an ensemble data assimilation cycle showscomparable forecast accuracy with a single high-resolution model, potentiallyeliminating the need for retraining on extrapolated datasets.</description><author>Pratik Shukla, Milton Halem</author><pubDate>Mon, 12 Aug 2024 16:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06262v1</guid></item><item><title>Open-Source Molecular Processing Pipeline for Generating Molecules</title><link>http://arxiv.org/abs/2408.06261v1</link><description>Generative models for molecules have shown considerable promise for use incomputational chemistry, but remain difficult to use for non-experts. For thisreason, we introduce open-source infrastructure for easily building generativemolecular models into the widely used DeepChem [Ramsundar et al., 2019] librarywith the aim of creating a robust and reusable molecular generation pipeline.In particular, we add high quality PyTorch [Paszke et al., 2019]implementations of the Molecular Generative Adversarial Networks (MolGAN) [Caoand Kipf, 2022] and Normalizing Flows [Papamakarios et al., 2021]. Ourimplementations show strong performance comparable with past work [Kuznetsovand Polykovskiy, 2021, Cao and Kipf, 2022].</description><author>Shreyas V, Jose Siguenza, Karan Bania, Bharath Ramsundar</author><pubDate>Mon, 12 Aug 2024 16:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06261v1</guid></item><item><title>MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts</title><link>http://arxiv.org/abs/2407.21770v3</link><description>We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)architecture designed for pre-training mixed-modal, early-fusion languagemodels. MoMa processes images and text in arbitrary sequences by dividingexpert modules into modality-specific groups. These groups exclusively processdesignated tokens while employing learned routing within each group to maintainsemantically informed adaptivity. Our empirical results reveal substantialpre-training efficiency gains through this modality-specific parameterallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,featuring 4 text experts and 4 image experts, achieves impressive FLOPssavings: 3.7x overall, with 2.6x for text and 5.2x for image processingcompared to a compute-equivalent dense baseline, measured by pre-training loss.This outperforms the standard expert-choice MoE with 8 mixed-modal experts,which achieves 3x overall FLOPs savings (3x for text, 2.8x for image).Combining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPssavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combinationhurts performance in causal inference due to increased sensitivity to routeraccuracy. These results demonstrate MoMa's potential to significantly advancethe efficiency of mixed-modal, early-fusion language model pre-training, pavingthe way for more resource-efficient and capable multimodal AI systems.</description><author>Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke Zettlemoyer, Armen Aghajanyan</author><pubDate>Mon, 12 Aug 2024 16:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21770v3</guid></item><item><title>Context-aware Visual Storytelling with Visual Prefix Tuning and Contrastive Learning</title><link>http://arxiv.org/abs/2408.06259v1</link><description>Visual storytelling systems generate multi-sentence stories from imagesequences. In this task, capturing contextual information and bridging visualvariation bring additional challenges. We propose a simple yet effectiveframework that leverages the generalization capabilities of pretrainedfoundation models, only training a lightweight vision-language mapping networkto connect modalities, while incorporating context to enhance coherence. Weintroduce a multimodal contrastive objective that also improves visualrelevance and story informativeness. Extensive experimental results, acrossboth automatic metrics and human evaluations, demonstrate that the storiesgenerated by our framework are diverse, coherent, informative, and interesting.</description><author>Yingjin Song, Denis Paperno, Albert Gatt</author><pubDate>Mon, 12 Aug 2024 16:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06259v1</guid></item><item><title>Deep Learning System Boundary Testing through Latent Space Style Mixing</title><link>http://arxiv.org/abs/2408.06258v1</link><description>Evaluating the behavioral frontier of deep learning (DL) systems is crucialfor understanding their generalizability and robustness. However, boundarytesting is challenging due to their high-dimensional input space. Generativeartificial intelligence offers a promising solution by modeling datadistribution within compact latent space representations, thereby facilitatingfiner-grained explorations. In this work, we introduce MIMICRY, a novelblack-box system-agnostic test generator that leverages these latentrepresentations to generate frontier inputs for the DL systems under test.Specifically, MIMICRY uses style-based generative adversarial networks trainedto learn the representation of inputs with disentangled features. Thisrepresentation enables embedding style-mixing operations between a source and atarget input, combining their features to explore the boundary between them. Weevaluated the effectiveness of different MIMICRY configurations in generatingboundary inputs for four popular DL image classification systems. Our resultsshow that manipulating the latent space allows for effective and efficientexploration of behavioral frontiers. As opposed to a model-based baseline,MIMICRY generates a higher quality frontier of behaviors which includes moreand closer inputs. Additionally, we assessed the validity of these inputs,revealing a high validity rate according to human assessors.</description><author>Amr Abdellatif, Xingcheng Chen, Vincenzo Riccio, Andrea Stocco</author><pubDate>Mon, 12 Aug 2024 16:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06258v1</guid></item><item><title>Reciprocal Learning</title><link>http://arxiv.org/abs/2408.06257v1</link><description>We demonstrate that a wide array of machine learning algorithms are specificinstances of one single paradigm: reciprocal learning. These instances rangefrom active learning over multi-armed bandits to self-training. We show thatall these algorithms do not only learn parameters from data but also viceversa: They iteratively alter training data in a way that depends on thecurrent model fit. We introduce reciprocal learning as a generalization ofthese algorithms using the language of decision theory. This allows us to studyunder what conditions they converge. The key is to guarantee that reciprocallearning contracts such that the Banach fixed-point theorem applies. In thisway, we find that reciprocal learning algorithms converge at linear rates to anapproximately optimal model under relatively mild assumptions on the lossfunction, if their predictions are probabilistic and the sample adaption isboth non-greedy and either randomized or regularized. We interpret thesefindings and provide corollaries that relate them to specific active learning,self-training, and bandit algorithms.</description><author>Julian Rodemann, Christoph Jansen, Georg Schollmeyer</author><pubDate>Mon, 12 Aug 2024 16:14:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06257v1</guid></item><item><title>A Text-guided Protein Design Framework</title><link>http://arxiv.org/abs/2302.04611v3</link><description>Current AI-assisted protein design mainly utilizes protein sequential andstructural information. Meanwhile, there exists tremendous knowledge curated byhumans in the text format describing proteins' high-level functionalities. Yet,whether the incorporation of such text data can help protein design tasks hasnot been explored. To bridge this gap, we propose ProteinDT, a multi-modalframework that leverages textual descriptions for protein design. ProteinDTconsists of three subsequent steps: ProteinCLAP which aligns the representationof two modalities, a facilitator that generates the protein representation fromthe text modality, and a decoder that creates the protein sequences from therepresentation. To train ProteinDT, we construct a large dataset,SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify theeffectiveness of ProteinDT on three challenging tasks: (1) over 90\% accuracyfor text-guided protein generation; (2) best hit ratio on 12 zero-shottext-guided protein editing tasks; (3) superior performance on four out of sixprotein property prediction benchmarks.</description><author>Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Arvind Ramanathan, Chaowei Xiao, Jian Tang, Hongyu Guo, Anima Anandkumar</author><pubDate>Mon, 12 Aug 2024 16:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04611v3</guid></item><item><title>Rethinking Video with a Universal Event-Based Representation</title><link>http://arxiv.org/abs/2408.06248v1</link><description>Traditionally, video is structured as a sequence of discrete image frames.Recently, however, a novel video sensing paradigm has emerged which eschewsvideo frames entirely. These "event" sensors aim to mimic the human visionsystem with asynchronous sensing, where each pixel has an independent, sparsedata stream. While these cameras enable high-speed and high-dynamic-rangesensing, researchers often revert to a framed representation of the event datafor existing applications, or build bespoke applications for a particularcamera's event data type. At the same time, classical video systems havesignificant computational redundancy at the application layer, since pixelsamples are repeated across frames in the uncompressed domain. To address the shortcomings of existing systems, I introduce Address,Decimation, {\Delta}t Event Representation (AD{\Delta}ER, pronounced "adder"),a novel intermediate video representation and system framework. The frameworktranscodes a variety of framed and event camera sources into a singleevent-based representation, which supports source-modeled lossy compression andbackward compatibility with traditional frame-based applications. I demonstratethat AD{\Delta}ER achieves state-of-the-art application speed and compressionperformance for scenes with high temporal redundancy. Crucially, I describe howAD{\Delta}ER unlocks an entirely new control mechanism for computer vision:application speed can correlate with both the scene content and the level oflossy compression. Finally, I discuss the implications for event-based video onlarge-scale video surveillance and resource-constrained sensing.</description><author>Andrew Freeman</author><pubDate>Mon, 12 Aug 2024 16:00:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06248v1</guid></item><item><title>Latent Disentanglement for Low Light Image Enhancement</title><link>http://arxiv.org/abs/2408.06245v1</link><description>Many learning-based low-light image enhancement (LLIE) algorithms are basedon the Retinex theory. However, the Retinex-based decomposition techniques insuch models introduce corruptions which limit their enhancement performance. Inthis paper, we propose a Latent Disentangle-based Enhancement Network (LDE-Net)for low light vision tasks. The latent disentanglement module disentangles theinput image in latent space such that no corruption remains in the disentangledContent and Illumination components. For LLIE task, we design a Content-AwareEmbedding (CAE) module that utilizes Content features to direct the enhancementof the Illumination component. For downstream tasks (e.g. nighttime UAVtracking and low-light object detection), we develop an effective light-weightenhancer based on the latent disentanglement framework. Comprehensivequantitative and qualitative experiments demonstrate that our LDE-Netsignificantly outperforms state-of-the-art methods on various LLIE benchmarks.In addition, the great results obtained by applying our framework on thedownstream tasks also demonstrate the usefulness of our latent disentanglementdesign.</description><author>Zhihao Zheng, Mooi Choo Chuah</author><pubDate>Mon, 12 Aug 2024 15:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06245v1</guid></item><item><title>Decentralized Intelligence Network (DIN)</title><link>http://arxiv.org/abs/2407.02461v3</link><description>Decentralized Intelligence Network (DIN) is a theoretical frameworkaddressing data fragmentation and siloing challenges, enabling scalable AIthrough data sovereignty. It facilitates effective AI utilization withinsovereign networks by overcoming barriers to accessing diverse data sources,leveraging: 1) personal data stores to ensure data sovereignty, where dataremains securely within Participants' control; 2) a scalable federated learningprotocol implemented on a public blockchain for decentralized AI training,where only model parameter updates are shared, keeping data within the personaldata stores; and 3) a scalable, trustless cryptographic rewards mechanism on apublic blockchain to incentivize participation and ensure fair rewarddistribution through a decentralized auditing protocol. This approachguarantees that no entity can prevent or control access to training data orinfluence financial benefits, as coordination and reward distribution aremanaged on the public blockchain with an immutable record. The frameworksupports effective AI training by allowing Participants to maintain controlover their data, benefit financially, and contribute to a decentralized,scalable ecosystem that leverages collective AI to develop beneficialalgorithms.</description><author>Abraham Nash</author><pubDate>Mon, 12 Aug 2024 15:54:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02461v3</guid></item><item><title>3D Reconstruction of Protein Structures from Multi-view AFM Images using Neural Radiance Fields (NeRFs)</title><link>http://arxiv.org/abs/2408.06244v1</link><description>Recent advancements in deep learning for predicting 3D protein structureshave shown promise, particularly when leveraging inputs like protein sequencesand Cryo-Electron microscopy (Cryo-EM) images. However, these techniques oftenfall short when predicting the structures of protein complexes (PCs), whichinvolve multiple proteins. In our study, we investigate using atomic forcemicroscopy (AFM) combined with deep learning to predict the 3D structures ofPCs. AFM generates height maps that depict the PCs in various randomorientations, providing a rich information for training a neural network topredict the 3D structures. We then employ the pre-trained UpFusion model (whichutilizes a conditional diffusion model for synthesizing novel views) to trainan instance-specific NeRF model for 3D reconstruction. The performance ofUpFusion is evaluated through zero-shot predictions of 3D protein structuresusing AFM images. The challenge, however, lies in the time-intensive andimpractical nature of collecting actual AFM images. To address this, we use avirtual AFM imaging process that transforms a `PDB' protein file intomulti-view 2D virtual AFM images via volume rendering techniques. Weextensively validate the UpFusion architecture using both virtual and actualmulti-view AFM images. Our results include a comparison of structures predictedwith varying numbers of views and different sets of views. This novel approachholds significant potential for enhancing the accuracy of protein complexstructure predictions with further fine-tuning of the UpFusion network.</description><author>Jaydeep Rade, Ethan Herron, Soumik Sarkar, Anwesha Sarkar, Adarsh Krishnamurthy</author><pubDate>Mon, 12 Aug 2024 15:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06244v1</guid></item><item><title>Decentralized Intelligence Health Network (DIHN)</title><link>http://arxiv.org/abs/2408.06240v1</link><description>Decentralized Intelligence Health Network (DIHN) is a theoretical frameworkaddressing significant challenges of health data sovereignty and AI utilizationin healthcare caused by data fragmentation across providers and institutions.It establishes a sovereign architecture for healthcare provision as aprerequisite to a sovereign health network, then facilitates effective AIutilization by overcoming barriers to accessing diverse medical data sources.This comprehensive framework leverages: 1) self-sovereign identity architecturecoupled with a personal health record (PHR) as a prerequisite for health datasovereignty; 2) a scalable federated learning (FL) protocol implemented on apublic blockchain for decentralized AI training in healthcare, where healthdata remains with participants and only model parameter updates are shared; and3) a scalable, trustless rewards mechanism to incentivize participation andensure fair reward distribution. This framework ensures that no entity canprevent or control access to training on health data offered by participants ordetermine financial benefits, as these processes operate on a public blockchainwith an immutable record and without a third party. It supports effective AItraining in healthcare, allowing patients to maintain control over their healthdata, benefit financially, and contribute to a decentralized, scalableecosystem that leverages collective AI to develop beneficial healthcarealgorithms. Patients receive rewards into their digital wallets as an incentiveto opt-in to the FL protocol, with a long-term roadmap to funding decentralizedinsurance solutions. This approach introduces a novel, self-financed healthcaremodel that adapts to individual needs, complements existing systems, andredefines universal coverage. It highlights the potential to transformhealthcare data management and AI utilization while empowering patients.</description><author>Abraham Nash</author><pubDate>Mon, 12 Aug 2024 15:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06240v1</guid></item><item><title>Across Platforms and Languages: Dutch Influencers and Legal Disclosures on Instagram, YouTube and TikTok</title><link>http://arxiv.org/abs/2407.12451v2</link><description>Content monetization on social media fuels a growing influencer economy.Influencer marketing remains largely undisclosed or inappropriately disclosedon social media. Non-disclosure issues have become a priority for national andsupranational authorities worldwide, who are starting to impose increasinglyharsher sanctions on them. This paper proposes a transparent methodology formeasuring whether and how influencers comply with disclosures based on legalstandards. We introduce a novel distinction between disclosures that arelegally sufficient (green) and legally insufficient (yellow). We apply thismethodology to an original dataset reflecting the content of 150 Dutchinfluencers publicly registered with the Dutch Media Authority based onrecently introduced registration obligations. The dataset consists of 292,315posts and is multi-language (English and Dutch) and cross-platform (Instagram,YouTube and TikTok). We find that influencer marketing remains generallyunderdisclosed on social media, and that bigger influencers are not necessarilymore compliant with disclosure standards.</description><author>Haoyang Gui, Thales Bertaglia, Catalina Goanta, Sybe de Vries, Gerasimos Spanakis</author><pubDate>Mon, 12 Aug 2024 15:44:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12451v2</guid></item><item><title>Correlation Weighted Prototype-based Self-Supervised One-Shot Segmentation of Medical Images</title><link>http://arxiv.org/abs/2408.06235v1</link><description>Medical image segmentation is one of the domains where sufficient annotateddata is not available. This necessitates the application of low-data frameworkslike few-shot learning. Contemporary prototype-based frameworks often do notaccount for the variation in features within the support and query images,giving rise to a large variance in prototype alignment. In this work, we adopta prototype-based self-supervised one-way one-shot learning framework usingpseudo-labels generated from superpixels to learn the semantic segmentationtask itself. We use a correlation-based probability score to generate a dynamicprototype for each query pixel from the bag of prototypes obtained from thesupport feature map. This weighting scheme helps to give a higher weightage tocontextually related prototypes. We also propose a quadrant masking strategy inthe downstream segmentation task by utilizing prior domain information todiscard unwanted false positives. We present extensive experimentations andevaluations on abdominal CT and MR datasets to show that the proposed simplebut potent framework performs at par with the state-of-the-art methods.</description><author>Siladittya Manna, Saumik Bhattacharya, Umapada Pal</author><pubDate>Mon, 12 Aug 2024 15:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06235v1</guid></item><item><title>A Comprehensive Case Study on the Performance of Machine Learning Methods on the Classification of Solar Panel Electroluminescence Images</title><link>http://arxiv.org/abs/2408.06229v1</link><description>Photovoltaics (PV) are widely used to harvest solar energy, an important formof renewable energy. Photovoltaic arrays consist of multiple solar panelsconstructed from solar cells. Solar cells in the field are vulnerable tovarious defects, and electroluminescence (EL) imaging provides effective andnon-destructive diagnostics to detect those defects. We use multipletraditional machine learning and modern deep learning models to classify ELsolar cell images into different functional/defective categories. Because ofthe asymmetry in the number of functional vs. defective cells, an imbalancedlabel problem arises in the EL image data. The current literature lacksinsights on which methods and metrics to use for model training and prediction.In this paper, we comprehensively compare different machine learning and deeplearning methods under different performance metrics on the classification ofsolar cell EL images from monocrystalline and polycrystalline modules. Weprovide a comprehensive discussion on different metrics. Our results provideinsights and guidelines for practitioners in selecting prediction methods andperformance metrics.</description><author>Xinyi Song, Kennedy Odongo, Francis G. Pascual, Yili Hong</author><pubDate>Mon, 12 Aug 2024 15:29:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06229v1</guid></item><item><title>FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks</title><link>http://arxiv.org/abs/2408.06227v1</link><description>This paper introduces FLEURS-R, a speech restoration applied version of theFew-shot Learning Evaluation of Universal Representations of Speech (FLEURS)corpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages asFLEURS, with improved audio quality and fidelity by applying the speechrestoration model Miipher. The aim of FLEURS-R is to advance speech technologyin more languages and catalyze research including text-to-speech (TTS) andother speech generation tasks in low-resource languages. Comprehensiveevaluations with the restored speech and TTS baseline models trained from thenew corpus show that the new corpus obtained significantly improved speechquality while maintaining the semantic contents of the speech. The corpus ispublicly released via Hugging Face.</description><author>Min Ma, Yuma Koizumi, Shigeki Karita, Heiga Zen, Jason Riesa, Haruko Ishikawa, Michiel Bacchiani</author><pubDate>Mon, 12 Aug 2024 15:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06227v1</guid></item><item><title>A Large-Scale Study of Model Integration in ML-Enabled Software Systems</title><link>http://arxiv.org/abs/2408.06226v1</link><description>The rise of machine learning (ML) and its embedding in systems hasdrastically changed the engineering of software-intensive systems.Traditionally, software engineering focuses on manually created artifacts suchas source code and the process of creating them, as well as best practices forintegrating them, i.e., software architectures. In contrast, the development ofML artifacts, i.e. ML models, comes from data science and focuses on the MLmodels and their training data. However, to deliver value to end users, theseML models must be embedded in traditional software, often forming complextopologies. In fact, ML-enabled software can easily incorporate many differentML models. While the challenges and practices of building ML-enabled systemshave been studied to some extent, beyond isolated examples, little is knownabout the characteristics of real-world ML-enabled systems. Properly embeddingML models in systems so that they can be easily maintained or reused is farfrom trivial. We need to improve our empirical understanding of such systems,which we address by presenting the first large-scale study of real ML-enabledsoftware systems, covering over 2,928 open source systems on GitHub. Weclassified and analyzed them to determine their characteristics, as well astheir practices for reusing ML models and related code, and the architecture ofthese systems. Our findings provide practitioners and researchers with insightinto practices for embedding and integrating ML models, bringing data scienceand software engineering closer together.</description><author>Yorick Sens, Henriette Knopp, Sven Peldszus, Thorsten Berger</author><pubDate>Mon, 12 Aug 2024 15:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06226v1</guid></item><item><title>On Effects of Steering Latent Representation for Large Language Model Unlearning</title><link>http://arxiv.org/abs/2408.06223v1</link><description>Representation Misdirection for Unlearning (RMU), which steers modelrepresentation in the intermediate layer to a target random representation, isan effective method for large language model (LLM) unlearning. Despite its highperformance, the underlying cause and explanation remain underexplored. In thispaper, we first theoretically demonstrate that steering forget representationsin the intermediate layer reduces token confidence, causing LLMs to generatewrong or nonsense responses. Second, we investigate how the coefficientinfluences the alignment of forget-sample representations with the randomdirection and hint at the optimal coefficient values for effective unlearningacross different network layers. Third, we show that RMU unlearned models arerobust against adversarial jailbreak attacks. Last, our empirical analysisshows that RMU is less effective when applied to the middle and later layers inLLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yeteffective alternative method that makes unlearning effective with most layers.Extensive experiments demonstrate that Adaptive RMU significantly improves theunlearning performance compared to prior art while incurring no additionalcomputational cost.</description><author>Dang Huu-Tien, Trung-Tin Pham, Hoang Thanh-Tung, Naoya Inoue</author><pubDate>Mon, 12 Aug 2024 15:24:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06223v1</guid></item><item><title>A Digital Twin Framework Utilizing Machine Learning for Robust Predictive Maintenance: Enhancing Tire Health Monitoring</title><link>http://arxiv.org/abs/2408.06220v1</link><description>We introduce a novel digital twin framework for predictive maintenance oflong-term physical systems. Using monitoring tire health as an application, weshow how the digital twin framework can be used to enhance automotive safetyand efficiency, and how the technical challenges can be overcome using athree-step approach. Firstly, for managing the data complexity over a longoperation span, we employ data reduction techniques to concisely representphysical tires using historical performance and usage data. Relying on thesedata, for fast real-time prediction, we train a transformer-based model offlineon our concise dataset to predict future tire health over time, represented asRemaining Casing Potential (RCP). Based on our architecture, our modelquantifies both epistemic and aleatoric uncertainty, providing reliableconfidence intervals around predicted RCP. Secondly, to incorporate real-timedata, we update the predictive model in the digital twin framework, ensuringits accuracy throughout its life span with the aid of hybrid modeling and theuse of discrepancy function. Thirdly, to assist decision making in predictivemaintenance, we implement a Tire State Decision Algorithm, which strategicallydetermines the optimal timing for tire replacement based on RCP forecasted byour transformer model. This approach ensures our digital twin accuratelypredicts system health, continually refines its digital representation, andsupports predictive maintenance decisions. Our framework effectively embodies aphysical system, leveraging big data and machine learning for predictivemaintenance, model updates, and decision-making.</description><author>Vispi Karkaria, Jie Chen, Christopher Luey, Chase Siuta, Damien Lim, Robert Radulescu, Wei Chen</author><pubDate>Mon, 12 Aug 2024 15:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06220v1</guid></item><item><title>Detecting Android Malware: From Neural Embeddings to Hands-On Validation with BERTroid</title><link>http://arxiv.org/abs/2405.03620v2</link><description>As cyber threats and malware attacks increasingly alarm both individuals andbusinesses, the urgency for proactive malware countermeasures intensifies. Thishas driven a rising interest in automated machine learning solutions.Transformers, a cutting-edge category of attention-based deep learning methods,have demonstrated remarkable success. In this paper, we present BERTroid, aninnovative malware detection model built on the BERT architecture. Overall,BERTroid emerged as a promising solution for combating Android malware. Itsability to outperform state-of-the-art solutions demonstrates its potential asa proactive defense mechanism against malicious software attacks. Additionally,we evaluate BERTroid on multiple datasets to assess its performance acrossdiverse scenarios. In the dynamic landscape of cybersecurity, our approach hasdemonstrated promising resilience against the rapid evolution of malware onAndroid systems. While the machine learning model captures broad patterns, weemphasize the role of manual validation for deeper comprehension and insightinto these behaviors. This human intervention is critical for discerningintricate and context-specific behaviors, thereby validating and reinforcingthe model's findings.</description><author>Meryam Chaieb, Mostafa Anouar Ghorab, Mohamed Aymen Saied</author><pubDate>Mon, 12 Aug 2024 15:16:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03620v2</guid></item><item><title>Semisupervised Neural Proto-Language Reconstruction</title><link>http://arxiv.org/abs/2406.05930v2</link><description>Existing work implementing comparative reconstruction of ancestral languages(proto-languages) has usually required full supervision. However, historicalreconstruction models are only of practical value if they can be trained with alimited amount of labeled data. We propose a semisupervised historicalreconstruction task in which the model is trained on only a small amount oflabeled data (cognate sets with proto-forms) and a large amount of unlabeleddata (cognate sets without proto-forms). We propose a neural architecture forcomparative reconstruction (DPD-BiReconstructor) incorporating an essentialinsight from linguists' comparative method: that reconstructed words should notonly be reconstructable from their daughter words, but also deterministicallytransformable back into their daughter words. We show that this architecture isable to leverage unlabeled cognate sets to outperform strong semisupervisedbaselines on this novel task.</description><author>Liang Lu, Peirong Xie, David R. Mortensen</author><pubDate>Mon, 12 Aug 2024 15:10:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05930v2</guid></item><item><title>Private Fine-tuning of Large Language Models with Zeroth-order Optimization</title><link>http://arxiv.org/abs/2401.04343v2</link><description>Differentially private stochastic gradient descent (DP-SGD) allows models tobe trained in a privacy-preserving manner, but has proven difficult to scale tothe era of foundation models. We introduce DP-ZO, a private fine-tuningframework for large language models by privatizing zeroth order optimizationmethods. A key insight into the design of our method is that the direction ofthe gradient in the zeroth-order optimization we use is random and the onlyinformation from training data is the step size, i.e., a scalar. Therefore, weonly need to privatize the scalar step size, which is memory-efficient. DP-ZOprovides a strong privacy-utility trade-off across different tasks, and modelsizes that are comparable to DP-SGD in $(\varepsilon,\delta)$-DP. Notably,DP-ZO possesses significant advantages over DP-SGD in memory efficiency, andobtains higher utility in $\varepsilon$-DP when using the Laplace mechanism.</description><author>Xinyu Tang, Ashwinee Panda, Milad Nasr, Saeed Mahloujifar, Prateek Mittal</author><pubDate>Mon, 12 Aug 2024 15:07:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04343v2</guid></item><item><title>Leveraging KANs For Enhanced Deep Koopman Operator Discovery</title><link>http://arxiv.org/abs/2406.02875v3</link><description>Multi-layer perceptrons (MLP's) have been extensively utilized in discoveringDeep Koopman operators for linearizing nonlinear dynamics. With the emergenceof Kolmogorov-Arnold Networks (KANs) as a more efficient and accuratealternative to the MLP Neural Network, we propose a comparison of theperformance of each network type in the context of learning Koopman operatorswith control. In this work, we propose a KANs-based deep Koopman framework withapplications to an orbital Two-Body Problem (2BP) and the pendulum fordata-driven discovery of linear system dynamics. KANs were found to be superiorin nearly all aspects of training; learning 31 times faster, being 15 timesmore parameter efficiency, and predicting 1.25 times more accurately ascompared to the MLP Deep Neural Networks (DNNs) in the case of the 2BP. Thus,KANs shows potential for being an efficient tool in the development of DeepKoopman Theory.</description><author>George Nehma, Madhur Tiwari</author><pubDate>Mon, 12 Aug 2024 15:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02875v3</guid></item><item><title>Computability of Classification and Deep Learning: From Theoretical Limits to Practical Feasibility through Quantization</title><link>http://arxiv.org/abs/2408.06212v1</link><description>The unwavering success of deep learning in the past decade led to theincreasing prevalence of deep learning methods in various application fields.However, the downsides of deep learning, most prominently its lack oftrustworthiness, may not be compatible with safety-critical orhigh-responsibility applications requiring stricter performance guarantees.Recently, several instances of deep learning applications have been shown to besubject to theoretical limitations of computability, undermining thefeasibility of performance guarantees when employed on real-world computers. Weextend the findings by studying computability in the deep learning frameworkfrom two perspectives: From an application viewpoint in the context ofclassification problems and a general limitation viewpoint in the context oftraining neural networks. In particular, we show restrictions on thealgorithmic solvability of classification problems that also render thealgorithmic detection of failure in computations in a general settinginfeasible. Subsequently, we prove algorithmic limitations in training deepneural networks even in cases where the underlying problem is well-behaved.Finally, we end with a positive observation, showing that in quantized versionsof classification and deep network training, computability restrictions do notarise or can be overcome to a certain degree.</description><author>Holger Boche, Vit Fojtik, Adalbert Fono, Gitta Kutyniok</author><pubDate>Mon, 12 Aug 2024 15:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06212v1</guid></item><item><title>A mathematical perspective on Transformers</title><link>http://arxiv.org/abs/2312.10794v4</link><description>Transformers play a central role in the inner workings of large languagemodels. We develop a mathematical framework for analyzing Transformers based ontheir interpretation as interacting particle systems, which reveals thatclusters emerge in long time. Our study explores the underlying theory andoffers new perspectives for mathematicians as well as computer scientists.</description><author>Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, Philippe Rigollet</author><pubDate>Mon, 12 Aug 2024 14:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10794v4</guid></item><item><title>ControlNet-XS: Rethinking the Control of Text-to-Image Diffusion Models as Feedback-Control Systems</title><link>http://arxiv.org/abs/2312.06573v2</link><description>The field of image synthesis has made tremendous strides forward in the lastyears. Besides defining the desired output image with text-prompts, anintuitive approach is to additionally use spatial guidance in form of an image,such as a depth map. In state-of-the-art approaches, this guidance is realizedby a separate controlling model that controls a pre-trained image generationnetwork, such as a latent diffusion model. Understanding this process from acontrol system perspective shows that it forms a feedback-control system, wherethe control module receives a feedback signal from the generation process andsends a corrective signal back. When analysing existing systems, we observethat the feedback signals are timely sparse and have a small number of bits. Asa consequence, there can be long delays between newly generated features andthe respective corrective signals for these features. It is known that thisdelay is the most unwanted aspect of any control system. In this work, we takean existing controlling network (ControlNet) and change the communicationbetween the controlling network and the generation process to be ofhigh-frequency and with large-bandwidth. By doing so, we are able toconsiderably improve the quality of the generated images, as well as thefidelity of the control. Also, the controlling network needs noticeably fewerparameters and hence is about twice as fast during inference and training time.Another benefit of small-sized models is that they help to democratise ourfield and are likely easier to understand. We call our proposed networkControlNet-XS. When comparing with the state-of-the-art approaches, weoutperform them for pixel-level guidance, such as depth, canny-edges, andsemantic segmentation, and are on a par for loose keypoint-guidance of humanposes. All code and pre-trained models will be made publicly available.</description><author>Denis Zavadski, Johann-Friedrich Feiden, Carsten Rother</author><pubDate>Mon, 12 Aug 2024 14:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06573v2</guid></item><item><title>Strategy Game-Playing with Size-Constrained State Abstraction</title><link>http://arxiv.org/abs/2408.06202v1</link><description>Playing strategy games is a challenging problem for artificial intelligence(AI). One of the major challenges is the large search space due to a diverseset of game components. In recent works, state abstraction has been applied tosearch-based game AI and has brought significant performance improvements.State abstraction techniques rely on reducing the search space, e.g., byaggregating similar states. However, the application of these abstractions ishindered because the quality of an abstraction is difficult to evaluate.Previous works hence abandon the abstraction in the middle of the search to notbias the search to a local optimum. This mechanism introduces a hyper-parameterto decide the time to abandon the current state abstraction. In this work, wepropose a size-constrained state abstraction (SCSA), an approach that limitsthe maximum number of nodes being grouped together. We found that with SCSA,the abstraction is not required to be abandoned. Our empirical results on $3$strategy games show that the SCSA agent outperforms the previous methods andyields robust performance over different games. Codes are open-sourced at\url{https://github.com/GAIGResearch/Stratega}.</description><author>Linjie Xu, Diego Perez-Liebana, Alexander Dockhorn</author><pubDate>Mon, 12 Aug 2024 14:50:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06202v1</guid></item><item><title>Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery</title><link>http://arxiv.org/abs/2407.14499v2</link><description>Concept Bottleneck Models (CBMs) have recently been proposed to address the'black-box' problem of deep neural networks, by first mapping images to ahuman-understandable concept space and then linearly combining concepts forclassification. Such models typically require first coming up with a set ofconcepts relevant to the task and then aligning the representations of afeature extractor to map to these concepts. However, even with powerfulfoundational feature extractors like CLIP, there are no guarantees that thespecified concepts are detectable. In this work, we leverage recent advances inmechanistic interpretability and propose a novel CBM approach -- calledDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: insteadof pre-selecting concepts based on the downstream classification task, we usesparse autoencoders to first discover concepts learnt by the model, and thenname them and train linear probes for classification. Our concept extractionstrategy is efficient, since it is agnostic to the downstream task, and usesconcepts already known to the model. We perform a comprehensive evaluationacross multiple datasets and CLIP architectures and show that our method yieldssemantically meaningful concepts, assigns appropriate names to them that makethem easy to interpret, and yields performant and interpretable CBMs. Codeavailable at https://github.com/neuroexplicit-saar/discover-then-name.</description><author>Sukrut Rao, Sweta Mahajan, Moritz B√∂hle, Bernt Schiele</author><pubDate>Mon, 12 Aug 2024 14:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14499v2</guid></item><item><title>Dynamic Blocked Clause Elimination for Projected Model Counting</title><link>http://arxiv.org/abs/2408.06199v1</link><description>In this paper, we explore the application of blocked clause elimination forprojected model counting. This is the problem of determining the number ofmodels ||\exists X.{\Sigma}|| of a propositional formula {\Sigma} aftereliminating a given set X of variables existentially. Although blocked clauseelimination is a well-known technique for SAT solving, its direct applicationto model counting is challenging as in general it changes the number of models.However, we demonstrate, by focusing on projected variables during the blockedclause search, that blocked clause elimination can be leveraged whilepreserving the correct model count. To take advantage of blocked clauseelimination in an efficient way during model counting, a novel data structureand associated algorithms are introduced. Our proposed approach is implementedin the model counter d4. Our experiments demonstrate the computational benefitsof our new method of blocked clause elimination for projected model counting.</description><author>Jean-Marie Lagniez, Pierre Marquis, Armin Biere</author><pubDate>Mon, 12 Aug 2024 14:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06199v1</guid></item><item><title>On the Generalization of Preference Learning with DPO</title><link>http://arxiv.org/abs/2408.03459v2</link><description>Large language models (LLMs) have demonstrated remarkable capabilities butoften struggle to align with human preferences, leading to harmful orundesirable outputs. Preference learning, which trains models to distinguishbetween preferred and non-preferred responses based on human feedback, hasbecome a crucial component for ensuring that LLMs align with human values.Despite the widespread adoption in real-world systems, a thorough theoreticalunderstanding of the generalization guarantees for these models remain lacking.This paper bridges that gap by introducing a new theoretical framework toanalyze the generalization guarantees of models trained with direct preferenceoptimization (DPO). While existing generalization theory often focuses onoverparameterized models achieving near-optimal loss or models independent ofthe training process, our framework rigorously assesses how well modelsgeneralize after a finite number of gradient steps, reflecting real-world LLMtraining practices. By analyzing the reward margin associated with each sampleand its trajectory throughout training, we can effectively bound thegeneralization error. We derive learning guarantees showing that, underspecific conditions, models trained with DPO can correctly discern preferredresponses on unseen data with high probability. These insights are empiricallyvalidated on contemporary LLMs, underscoring the practical relevance of ourtheoretical findings.</description><author>Shawn Im, Yixuan Li</author><pubDate>Mon, 12 Aug 2024 14:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03459v2</guid></item><item><title>A secure and private ensemble matcher using multi-vault obfuscated templates</title><link>http://arxiv.org/abs/2404.05205v2</link><description>Generative AI has revolutionized modern machine learning by providingunprecedented realism, diversity, and efficiency in data generation. Thistechnology holds immense potential for biometrics, including for securingsensitive and personally identifiable information. Given the irrevocability ofbiometric samples and mounting privacy concerns, biometric template securityand secure matching are among the most sought-after features of modernbiometric systems. This paper proposes a novel obfuscation method usingGenerative AI to enhance biometric template security. Our approach utilizessynthetic facial images generated by a Generative Adversarial Network (GAN) as"random chaff points" within a secure vault system. Our method creates nsub-templates from the original template, each obfuscated with m GAN chaffpoints. During verification, s closest vectors to the biometric query areretrieved from each vault and combined to generate hash values, which are thencompared with the stored hash value. Thus, our method safeguards useridentities during the training and deployment phases by employing theGAN-generated synthetic images. Our protocol was tested using the AT&amp;T, GT, andLFW face datasets, achieving ROC areas under the curve of 0.99, 0.99, and 0.90,respectively. Our results demonstrate that the proposed method can maintainhigh accuracy and reasonable computational complexity comparable to thoseunprotected template methods while significantly enhancing security andprivacy, underscoring the potential of Generative AI in developing proactivedefensive strategies for biometric systems.</description><author>Babak Poorebrahim Gilkalaye, Shubhabrata Mukherjee, Reza Derakhshani</author><pubDate>Mon, 12 Aug 2024 14:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05205v2</guid></item><item><title>Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers</title><link>http://arxiv.org/abs/2408.06195v1</link><description>This paper introduces rStar, a self-play mutual reasoning approach thatsignificantly improves reasoning capabilities of small language models (SLMs)without fine-tuning or superior models. rStar decouples reasoning into aself-play mutual generation-discrimination process. First, a target SLMaugments the Monte Carlo Tree Search (MCTS) with a rich set of human-likereasoning actions to construct higher quality reasoning trajectories. Next,another SLM, with capabilities similar to the target SLM, acts as adiscriminator to verify each trajectory generated by the target SLM. Themutually agreed reasoning trajectories are considered mutual consistent, thusare more likely to be correct. Extensive experiments across five SLMsdemonstrate rStar can effectively solve diverse reasoning problems, includingGSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8Kaccuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% forMistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will beavailable at https://github.com/zhentingqi/rStar.</description><author>Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang</author><pubDate>Mon, 12 Aug 2024 14:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06195v1</guid></item><item><title>FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework</title><link>http://arxiv.org/abs/2408.06190v1</link><description>We introduce FruitNeRF, a unified novel fruit counting framework thatleverages state-of-the-art view synthesis methods to count any fruit typedirectly in 3D. Our framework takes an unordered set of posed images capturedby a monocular camera and segments fruit in each image. To make our systemindependent of the fruit type, we employ a foundation model that generatesbinary segmentation masks for any fruit. Utilizing both modalities, RGB andsemantic, we train a semantic neural radiance field. Through uniform volumesampling of the implicit Fruit Field, we obtain fruit-only point clouds. Byapplying cascaded clustering on the extracted point cloud, our approachachieves precise fruit count.The use of neural radiance fields providessignificant advantages over conventional methods such as object tracking oroptical flow, as the counting itself is lifted into 3D. Our method preventsdouble counting fruit and avoids counting irrelevant fruit.We evaluate ourmethodology using both real-world and synthetic datasets. The real-worlddataset consists of three apple trees with manually counted ground truths, abenchmark apple dataset with one row and ground truth fruit location, while thesynthetic dataset comprises various fruit types including apple, plum, lemon,pear, peach, and mango.Additionally, we assess the performance of fruitcounting using the foundation model compared to a U-Net.</description><author>Lukas Meyer, Andreas Gilson, Ute Schmidt, Marc Stamminger</author><pubDate>Mon, 12 Aug 2024 14:40:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06190v1</guid></item><item><title>Time is Not Enough: Time-Frequency based Explanation for Time-Series Black-Box Models</title><link>http://arxiv.org/abs/2408.03636v2</link><description>Despite the massive attention given to time-series explanations due to theirextensive applications, a notable limitation in existing approaches is theirprimary reliance on the time-domain. This overlooks the inherent characteristicof time-series data containing both time and frequency features. In this work,we present Spectral eXplanation (SpectralX), an XAI framework that providestime-frequency explanations for time-series black-box classifiers. This easilyadaptable framework enables users to "plug-in" various perturbation-based XAImethods for any pre-trained time-series classification models to assess theirimpact on the explanation quality without having to modify the frameworkarchitecture. Additionally, we introduce Feature Importance Approximations(FIA), a new perturbation-based XAI method. These methods consist of featureinsertion, deletion, and combination techniques to enhance computationalefficiency and class-specific explanations in time-series classification tasks.We conduct extensive experiments in the generated synthetic dataset and variousUCR Time-Series datasets to first compare the explanation performance of FIAand other existing perturbation-based XAI methods in both time-domain andtime-frequency domain, and then show the superiority of our FIA in thetime-frequency domain with the SpectralX framework. Finally, we conduct a userstudy to confirm the practicality of our FIA in SpectralX framework forclass-specific time-frequency based time-series explanations. The source codeis available in https://github.com/gustmd0121/Time_is_not_Enough</description><author>Hyunseung Chung, Sumin Jo, Yeonsu Kwon, Edward Choi</author><pubDate>Mon, 12 Aug 2024 14:39:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03636v2</guid></item></channel></rss>