<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 19 Aug 2025 13:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>4DNeX: Feed-Forward 4D Generative Modeling Made Easy</title><link>http://arxiv.org/abs/2508.13154v1</link><description>We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,dynamic 3D) scene representations from a single image. In contrast to existingmethods that rely on computationally intensive optimization or requiremulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4Dgeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scaledataset with high-quality 4D annotations generated using advancedreconstruction approaches. 2) we introduce a unified 6D video representationthat jointly models RGB and XYZ sequences, facilitating structured learning ofboth appearance and geometry. 3) we propose a set of simple yet effectiveadaptation strategies to repurpose pretrained video diffusion models for 4Dmodeling. 4DNeX produces high-quality dynamic point clouds that enablenovel-view video synthesis. Extensive experiments demonstrate that 4DNeXoutperforms existing 4D generation methods in efficiency and generalizability,offering a scalable solution for image-to-4D modeling and laying the foundationfor generative 4D world models that simulate dynamic scene evolution.</description><author>Zhaoxi Chen, Tianqi Liu, Long Zhuo, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, Ziwei Liu</author><pubDate>Mon, 18 Aug 2025 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13154v1</guid></item><item><title>IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion</title><link>http://arxiv.org/abs/2508.13153v1</link><description>Reconstructing complete and interactive 3D scenes remains a fundamentalchallenge in computer vision and robotics, particularly due to persistentobject occlusions and limited sensor coverage. Multiview observations from asingle scene scan often fail to capture the full structural details. Existingapproaches typically rely on multi stage pipelines, such as segmentation,background completion, and inpainting or require per-object dense scanning,both of which are error-prone, and not easily scalable. We propose IGFuse, anovel framework that reconstructs interactive Gaussian scene by fusingobservations from multiple scans, where natural object rearrangement betweencaptures reveal previously occluded regions. Our method constructs segmentationaware Gaussian fields and enforces bi-directional photometric and semanticconsistency across scans. To handle spatial misalignments, we introduce apseudo-intermediate scene state for unified alignment, alongside collaborativeco-pruning strategies to refine geometry. IGFuse enables high fidelityrendering and object level scene manipulation without dense observations orcomplex pipelines. Extensive experiments validate the framework's stronggeneralization to novel scene configurations, demonstrating its effectivenessfor real world 3D reconstruction and real-to-simulation transfer. Our projectpage is available online.</description><author>Wenhao Hu, Zesheng Li, Haonan Zhou, Liu Liu, Xuexiang Wen, Zhizhong Su, Xi Li, Gaoang Wang</author><pubDate>Mon, 18 Aug 2025 17:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13153v1</guid></item><item><title>RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns</title><link>http://arxiv.org/abs/2508.13152v1</link><description>Detecting content generated by large language models (LLMs) is crucial forpreventing misuse and building trustworthy AI systems. Although existingdetection methods perform well, their robustness in out-of-distribution (OOD)scenarios is still lacking. In this paper, we hypothesize that, compared tofeatures used by existing detection methods, the internal representations ofLLMs contain more comprehensive and raw features that can more effectivelycapture and distinguish the statistical pattern differences betweenLLM-generated texts (LGT) and human-written texts (HWT). We validated thishypothesis across different LLMs and observed significant differences in neuralactivation patterns when processing these two types of texts. Based on this, wepropose RepreGuard, an efficient statistics-based detection method.Specifically, we first employ a surrogate model to collect representation ofLGT and HWT, and extract the distinct activation feature that can betteridentify LGT. We can classify the text by calculating the projection score ofthe text representations along this feature direction and comparing with aprecomputed threshold. Experimental results show that RepreGuard outperformsall baselines with average 94.92% AUROC on both in-distribution (ID) and OODscenarios, while also demonstrating robust resilience to various text sizes andmainstream attacks. Data and code are publicly available at:https://github.com/NLP2CT/RepreGuard</description><author>Xin Chen, Junchao Wu, Shu Yang, Runzhe Zhan, Zeyu Wu, Ziyang Luo, Di Wang, Min Yang, Lidia S. Chao, Derek F. Wong</author><pubDate>Mon, 18 Aug 2025 17:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13152v1</guid></item><item><title>MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models</title><link>http://arxiv.org/abs/2508.13148v1</link><description>Diffusion language models, as a promising alternative to traditionalautoregressive (AR) models, enable faster generation and richer conditioning onbidirectional context. However, they suffer from a key discrepancy betweentraining and inference: during inference, MDLMs progressively reveal thestructure of the generated sequence by producing fewer and fewer masked tokens,whereas this structure is ignored in training as tokens are masked at random.Although this discrepancy between training and inference can lead to suboptimalperformance, it has been largely overlooked by previous works, leaving closingthis gap between the two stages an open problem. To address this, we frame theproblem of learning effective denoising trajectories as a sequentialdecision-making problem and use the resulting framework to apply reinforcementlearning. We propose a novel Masked Diffusion Policy Optimization (MDPO) toexploit the Markov property diffusion possesses and explicitly train the modelunder the same progressive refining schedule used at inference. MDPO matchesthe performance of the previous state-of-the-art (SOTA) method with 60x fewergradient updates, while achieving average improvements of 9.6% on MATH500 and54.2% on Countdown over SOTA when trained within the same number of weightupdates. Additionally, we improve the remasking strategy of MDLMs as a plug-ininference replacement to overcome the limitation that the model cannot refinetokens flexibly. This simple yet effective training-free strategy, what werefer to as RCR, consistently improves performance and yields additional gainswhen combined with MDPO. Our findings establish great potential forinvestigating the discrepancy between pre-training and inference of MDLMs.Code: https://github.com/autonomousvision/mdpo. Project Page:https://cli212.github.io/MDPO/.</description><author>Haoyu He, Katrin Renz, Yong Cao, Andreas Geiger</author><pubDate>Mon, 18 Aug 2025 17:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13148v1</guid></item><item><title>New Interaction Paradigm for Complex EDA Software Leveraging GPT</title><link>http://arxiv.org/abs/2307.14740v2</link><description>Electronic Design Automation (EDA) tools such as KiCad offer powerfulfunctionalities but remain difficult to use, particularly for beginners, due totheir steep learning curves and fragmented documentation. To address thischallenge, we present SmartonAI, an AI-assisted interaction system thatintegrates large language models into the EDA workflow, enabling naturallanguage communication, intelligent task decomposition, and contextual pluginexecution. SmartonAI consists of two main components: a Chat Plugin that breaksdown user instructions into subtasks and retrieves tailored documentation, anda OneCommandLine Plugin that recommends and executes relevant plugins based onuser intent. The system supports multilingual interaction and adapts to userfeedback through incremental learning. Preliminary results suggest thatSmartonAI significantly reduces onboarding time and enhances productivity,representing a promising step toward generalizable AI-assisted interactionparadigms for complex software systems.</description><author>Xinyu Wang, Boyu Han, Zhenghan Tai, Jingrui Tian, Yifan Wang, Junyu Yan, Yidong Tian</author><pubDate>Mon, 18 Aug 2025 17:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14740v2</guid></item><item><title>Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation</title><link>http://arxiv.org/abs/2508.13144v1</link><description>Developing large language models is expensive and involves making decisionswith small experiments, typically by evaluating on large, multi-task evaluationsuites. In this work, we analyze specific properties which make a benchmarkmore reliable for such decisions, and interventions to design higher-qualityevaluation benchmarks. We introduce two key metrics that show differences incurrent benchmarks: signal, a benchmark's ability to separate better modelsfrom worse models, and noise, a benchmark's sensitivity to random variabilitybetween training steps. We demonstrate that benchmarks with a bettersignal-to-noise ratio are more reliable when making decisions at small scale,and those with less noise have lower scaling law prediction error. Theseresults suggest that improving signal or noise will lead to more usefulbenchmarks, so we introduce three interventions designed to directly affectsignal or noise. For example, we propose that switching to a metric that hasbetter signal and noise (e.g., perplexity rather than accuracy) leads to betterreliability and improved scaling law error. We also find that filtering noisysubtasks, to improve an aggregate signal-to-noise ratio, leads to more reliablemulti-task evaluations. We also find that averaging the output of a model'sintermediate checkpoints to reduce noise leads to consistent improvements. Weconclude by recommending that those creating new benchmarks, or selecting whichexisting benchmarks to use, aim for high signal and low noise. We use 30benchmarks for these experiments, and 375 open-weight language models from 60Mto 32B parameters, resulting in a new, publicly available dataset of 900Kevaluation benchmark results, totaling 200M instances.</description><author>David Heineman, Valentin Hofmann, Ian Magnusson, Yuling Gu, Noah A. Smith, Hannaneh Hajishirzi, Kyle Lo, Jesse Dodge</author><pubDate>Mon, 18 Aug 2025 17:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13144v1</guid></item><item><title>Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks</title><link>http://arxiv.org/abs/2508.13143v1</link><description>Autonomous agent systems powered by Large Language Models (LLMs) havedemonstrated promising capabilities in automating complex tasks. However,current evaluations largely rely on success rates without systematicallyanalyzing the interactions, communication mechanisms, and failure causes withinthese systems. To bridge this gap, we present a benchmark of 34 representativeprogrammable tasks designed to rigorously assess autonomous agents. Using thisbenchmark, we evaluate three popular open-source agent frameworks combined withtwo LLM backbones, observing a task completion rate of approximately 50%.Through in-depth failure analysis, we develop a three-tier taxonomy of failurecauses aligned with task phases, highlighting planning errors, task executionissues, and incorrect response generation. Based on these insights, we proposeactionable improvements to enhance agent planning and self-diagnosiscapabilities. Our failure taxonomy, together with mitigation advice, providesan empirical foundation for developing more robust and effective autonomousagent systems in the future.</description><author>Ruofan Lu, Yichen Li, Yintong Huo</author><pubDate>Mon, 18 Aug 2025 17:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13143v1</guid></item><item><title>Has GPT-5 Achieved Spatial Intelligence? An Empirical Study</title><link>http://arxiv.org/abs/2508.13142v1</link><description>Multi-modal models have achieved remarkable progress in recent years.Nevertheless, they continue to exhibit notable limitations in spatialunderstanding and reasoning, which are fundamental capabilities to achievingartificial general intelligence. With the recent release of GPT-5, allegedlythe most powerful AI model to date, it is timely to examine where the leadingmodels stand on the path toward spatial intelligence. First, we propose acomprehensive taxonomy of spatial tasks that unifies existing benchmarks anddiscuss the challenges in ensuring fair evaluation. We then evaluatestate-of-the-art proprietary and open-source models on eight key benchmarks, ata cost exceeding one billion total tokens. Our empirical study reveals that (1)GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)still falls short of human performance across a broad spectrum of tasks.Moreover, we (3) identify the more challenging spatial intelligence problemsfor multi-modal models, and (4) proprietary models do not exhibit a decisiveadvantage when facing the most difficult problems. In addition, we conduct aqualitative evaluation across a diverse set of scenarios that are intuitive forhumans yet fail even the most advanced multi-modal models.</description><author>Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Jiaqi Li, Xiangyu Fan, Hanming Deng, Lewei Lu, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang</author><pubDate>Mon, 18 Aug 2025 17:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13142v1</guid></item><item><title>OptimalThinkingBench: Evaluating Over and Underthinking in LLMs</title><link>http://arxiv.org/abs/2508.13141v1</link><description>Thinking LLMs solve complex tasks at the expense of increased compute andoverthinking on simpler problems, while non-thinking LLMs are faster andcheaper but underthink on harder reasoning problems. This has led to thedevelopment of separate thinking and non-thinking LLM variants, leaving theonus of selecting the optimal model for each query on the end user. In thiswork, we introduce OptimalThinkingBench, a unified benchmark that jointlyevaluates overthinking and underthinking in LLMs and also encourages thedevelopment of optimally-thinking models that balance performance andefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,featuring simple queries in 72 domains, and UnderthinkingBench, containing 11challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, weperform extensive evaluation of 33 different thinking and non-thinking modelsand show that no model is able to optimally think on our benchmark. Thinkingmodels often overthink for hundreds of tokens on the simplest user querieswithout improving performance. In contrast, large non-thinking modelsunderthink, often falling short of much smaller thinking models. We furtherexplore several methods to encourage optimal thinking, but find that theseapproaches often improve on one sub-benchmark at the expense of the other,highlighting the need for better unified and optimal models in the future.</description><author>Pranjal Aggarwal, Seungone Kim, Jack Lanchantin, Sean Welleck, Jason Weston, Ilia Kulikov, Swarnadeep Saha</author><pubDate>Mon, 18 Aug 2025 17:53:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13141v1</guid></item><item><title>Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence</title><link>http://arxiv.org/abs/2508.13139v1</link><description>This work studies the challenge of transfer animations between characterswhose skeletal topologies differ substantially. While many techniques haveadvanced retargeting techniques in decades, transfer motions across diversetopologies remains less-explored. The primary obstacle lies in the inherenttopological inconsistency between source and target skeletons, which restrictsthe establishment of straightforward one-to-one bone correspondences. Besides,the current lack of large-scale paired motion datasets spanning differenttopological structures severely constrains the development of data-drivenapproaches. To address these limitations, we introduce Motion2Motion, a novel,training-free framework. Simply yet effectively, Motion2Motion works with onlyone or a few example motions on the target skeleton, by accessing a sparse setof bone correspondences between the source and target skeletons. Throughcomprehensive qualitative and quantitative evaluations, we demonstrate thatMotion2Motion achieves efficient and reliable performance in bothsimilar-skeleton and cross-species skeleton transfer scenarios. The practicalutility of our approach is further evidenced by its successful integration indownstream applications and user interfaces, highlighting its potential forindustrial applications. Code and data are available athttps://lhchen.top/Motion2Motion.</description><author>Ling-Hao Chen, Yuhong Zhang, Zixin Yin, Zhiyang Dou, Xin Chen, Jingbo Wang, Taku Komura, Lei Zhang</author><pubDate>Mon, 18 Aug 2025 17:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13139v1</guid></item><item><title>Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]</title><link>http://arxiv.org/abs/2508.13135v1</link><description>Individual-level human mobility prediction has emerged as a significant topicof research with applications in infectious disease monitoring, child, andelderly care. Existing studies predominantly focus on the microscopic aspectsof human trajectories: such as predicting short-term trajectories or the nextlocation visited, while offering limited attention to macro-level mobilitypatterns and the corresponding life routines. In this paper, we focus on anunderexplored problem in human mobility prediction: determining the bestpractices to train a machine learning model using historical data to forecastan individuals complete trajectory over the next days and weeks. In thisexperiment paper, we undertake a comprehensive experimental analysis of diversemodels, parameter configurations, and training strategies, accompanied by anin-depth examination of the statistical distribution inherent in human mobilitypatterns. Our empirical evaluations encompass both Long Short-Term Memory andTransformer-based architectures, and further investigate how incorporatingindividual life patterns can enhance the effectiveness of the prediction. Weshow that explicitly including semantic information such as day-of-the-week anduser-specific historical information can help the model better understandindividual patterns of life and improve predictions. Moreover, since theabsence of explicit user information is often missing due to user privacy, weshow that the sampling of users may exacerbate data skewness and result in asubstantial loss in predictive accuracy. To mitigate data imbalance andpreserve diversity, we apply user semantic clustering with stratified samplingto ensure that the sampled dataset remains representative. Our results furthershow that small-batch stochastic gradient optimization improves modelperformance, especially when human mobility training data is limited.</description><author>Yueyang Liu, Lance Kennedy, Ruochen Kong, Joon-Seok Kim, Andreas ZÃ¼fle</author><pubDate>Mon, 18 Aug 2025 17:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13135v1</guid></item><item><title>Improving Detection of Watermarked Language Models</title><link>http://arxiv.org/abs/2508.13131v1</link><description>Watermarking has recently emerged as an effective strategy for detecting thegenerations of large language models (LLMs). The strength of a watermarktypically depends strongly on the entropy afforded by the language model andthe set of input prompts. However, entropy can be quite limited in practice,especially for models that are post-trained, for example via instruction tuningor reinforcement learning from human feedback (RLHF), which makes detectionbased on watermarking alone challenging. In this work, we investigate whetherdetection can be improved by combining watermark detectors with non-watermarkones. We explore a number of hybrid schemes that combine the two, observingperformance gains over either class of detector under a wide range ofexperimental conditions.</description><author>Dara Bahri, John Wieting</author><pubDate>Mon, 18 Aug 2025 17:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13131v1</guid></item><item><title>MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation</title><link>http://arxiv.org/abs/2508.13130v1</link><description>Commonsense validation evaluates whether a sentence aligns with everydayhuman understanding, a critical capability for developing robust naturallanguage understanding systems. While substantial progress has been made inEnglish, the task remains underexplored in Arabic, particularly given its richlinguistic diversity. Existing Arabic resources have primarily focused onModern Standard Arabic (MSA), leaving regional dialects underrepresenteddespite their prevalence in spoken contexts. To bridge this gap, we present twokey contributions: (i) we introduce MuDRiC, an extended Arabic commonsensedataset incorporating multiple dialects, and (ii) a novel method adapting GraphConvolutional Networks (GCNs) to Arabic commonsense reasoning, which enhancessemantic relationship modeling for improved commonsense validation. Ourexperimental results demonstrate that this approach achieves superiorperformance in Arabic commonsense validation. Our work enhances Arabic naturallanguage understanding by providing both a foundational dataset and a novelmethod for handling its complex variations. To the best of our knowledge, werelease the first Arabic multi-dialect commonsense reasoning dataset.</description><author>Kareem Elozeiri, Mervat Abassy, Preslav Nakov, Yuxia Wang</author><pubDate>Mon, 18 Aug 2025 17:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13130v1</guid></item><item><title>A polynomial formula for the perspective four points problem</title><link>http://arxiv.org/abs/2501.13058v2</link><description>We present a fast and accurate solution to the perspective $n$-pointsproblem, by way of a new approach to the n=4 case. Our solution hinges on anovel separation of variables: given four 3D points and four corresponding 2Dpoints on the camera canvas, we start by finding another set of 3D points,sitting on the rays connecting the camera to the 2D canvas points, so that thesix pair-wise distances between these 3D points are as close as possible to thesix distances between the original 3D points. This step reduces the perspectiveproblem to an absolute orientation problem, which has a solution via explicitformula. To solve the first problem we set coordinates which are asorientation-free as possible: on the 3D points side our coordinates are thesquared distances between the points. On the 2D canvas-points side ourcoordinates are the dot products of the points after rotating one of them tosit on the optical axis. We then derive the solution with the help of acomputer algebra system. Our solution is an order of magnitude faster thanstate of the art algorithms, while offering similar accuracy under realisticnoise. Moreover, our reduction to the absolute orientation problem runs twoorders of magnitude faster than other perspective problem solvers, allowingextremely efficient seed rejection when implementing RANSAC.</description><author>David Lehavi, Brian Osserman</author><pubDate>Mon, 18 Aug 2025 17:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13058v2</guid></item><item><title>Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries</title><link>http://arxiv.org/abs/2508.13124v1</link><description>Abstractive summarization is a core application in contact centers, whereLarge Language Models (LLMs) generate millions of summaries of call transcriptsdaily. Despite their apparent quality, it remains unclear whether LLMssystematically under- or over-attend to specific aspects of the transcript,potentially introducing biases in the generated summary. While prior work hasexamined social and positional biases, the specific forms of bias pertinent tocontact center operations - which we term Operational Bias - have remainedunexplored. To address this gap, we introduce BlindSpot, a framework built upona taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)for the identification and quantification of these biases. BlindSpot leveragesan LLM as a zero-shot classifier to derive categorical distributions for eachbias dimension in a pair of transcript and its summary. The bias is thenquantified using two metrics: Fidelity Gap (the JS Divergence betweendistributions) and Coverage (the percentage of source labels omitted). UsingBlindSpot, we conducted an empirical study with 2500 real call transcripts andtheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,Llama, Claude). Our analysis reveals that biases are systemic and presentacross all evaluated models, regardless of size or family.</description><author>Kawin Mayilvaghanan, Siddhant Gupta, Ayush Kumar</author><pubDate>Mon, 18 Aug 2025 17:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13124v1</guid></item><item><title>Bayesian Optimization-based Search for Agent Control in Automated Game Testing</title><link>http://arxiv.org/abs/2508.13121v1</link><description>This work introduces an automated testing approach that employs agentscontrolling game characters to detect potential bugs within a game level.Harnessing the power of Bayesian Optimization (BO) to execute sample-efficientsearch, the method determines the next sampling point by analyzing the datacollected so far and calculates the data point that will maximize informationacquisition. To support the BO process, we introduce a game testing-specificmodel built on top of a grid map, that features the smoothness and uncertaintyestimation required by BO, however and most importantly, it does not suffer thescalability issues that traditional models carry. The experiments demonstratethat the approach significantly improves map coverage capabilities in both timeefficiency and exploration distribution.</description><author>Carlos Celemin</author><pubDate>Mon, 18 Aug 2025 17:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13121v1</guid></item><item><title>AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2508.13118v1</link><description>Incident response (IR) requires fast, coordinated, and well-informeddecision-making to contain and mitigate cyber threats. While large languagemodels (LLMs) have shown promise as autonomous agents in simulated IR settings,their reasoning is often limited by a lack of access to external knowledge. Inthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework thatincorporates retrieval-augmented generation (RAG) into multi-agent incidentresponse simulations. Built on the Backdoors &amp; Breaches (B&amp;B) tabletop gameenvironment, AutoBnB-RAG enables agents to issue retrieval queries andincorporate external evidence during collaborative investigations. We introducetwo retrieval settings: one grounded in curated technical documentation(RAG-Wiki), and another using narrative-style incident reports (RAG-News). Weevaluate performance across eight team structures, including newly introducedargumentative configurations designed to promote critical reasoning. Tovalidate practical utility, we also simulate real-world cyber incidents basedon public breach reports, demonstrating AutoBnB-RAG's ability to reconstructcomplex multi-stage attacks. Our results show that retrieval augmentationimproves decision quality and success rates across diverse organizationalmodels. This work demonstrates the value of integrating retrieval mechanismsinto LLM-based multi-agent systems for cybersecurity decision-making.</description><author>Zefang Liu, Arman Anwar</author><pubDate>Mon, 18 Aug 2025 17:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13118v1</guid></item><item><title>Contrastive Representations for Temporal Reasoning</title><link>http://arxiv.org/abs/2508.13113v1</link><description>In classical AI, perception relies on learning state-based representations,while planning, which can be thought of as temporal reasoning over actionsequences, is typically achieved through search. We study whether suchreasoning can instead emerge from representations that capture both perceptualand temporal structure. We show that standard temporal contrastive learning,despite its popularity, often fails to capture temporal structure due to itsreliance on spurious features. To address this, we introduce CombinatorialRepresentations for Temporal Reasoning (CRTR), a method that uses a negativesampling scheme to provably remove these spurious features and facilitatetemporal reasoning. CRTR achieves strong results on domains with complextemporal structure, such as Sokoban and Rubik's Cube. In particular, for theRubik's Cube, CRTR learns representations that generalize across all initialstates and allow it to solve the puzzle using fewer search steps than BestFS,though with longer solutions. To our knowledge, this is the first method thatefficiently solves arbitrary Cube states using only learned representations,without relying on an external search algorithm.</description><author>Alicja Ziarko, Michal Bortkiewicz, Michal Zawalski, Benjamin Eysenbach, Piotr Milos</author><pubDate>Mon, 18 Aug 2025 17:20:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13113v1</guid></item><item><title>Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry</title><link>http://arxiv.org/abs/2508.13111v1</link><description>Foundational modelling of multi-dimensional time-series data in industrialsystems presents a central trade-off: channel-dependent (CD) models capturespecific cross-variable dynamics but lack robustness and adaptability as modellayers are commonly bound to the data dimensionality of the tackled use-case,while channel-independent (CI) models offer generality at the cost of modellingthe explicit interactions crucial for system-level predictive regression tasks.To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), anovel architecture that integrates a known causal graph as an inductive bias.The core of CGPT is built around a pairwise modeling paradigm, tackling theCD/CI conflict by decomposing the multidimensional data into pairs. The modeluses channel-agnostic learnable layers where all parameter dimensions areindependent of the number of variables. CGPT enforces a CD information flow atthe pair-level and CI-like generalization across pairs. This approachdisentangles complex system dynamics and results in a highly flexiblearchitecture that ensures scalability and any-variate adaptability. We validateCGPT on a suite of synthetic and real-world industrial datasets on long-termand one-step forecasting tasks designed to simulate common industrialcomplexities. Results demonstrate that CGPT significantly outperforms both CIand CD baselines in predictive accuracy and shows competitive performance withend-to-end trained CD models while remaining agnostic to the problemdimensionality.</description><author>Michael Mayr, Georgios C. Chasparis</author><pubDate>Mon, 18 Aug 2025 17:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13111v1</guid></item><item><title>TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection</title><link>http://arxiv.org/abs/2503.24115v4</link><description>The detection of telecom fraud faces significant challenges due to the lackof high-quality multimodal training data that integrates audio signals withreasoning-oriented textual analysis. To address this gap, we presentTeleAntiFraud-28k, the first open-source audio-text slow-thinking datasetspecifically designed for automated telecom fraud analysis. Our dataset isconstructed through three strategies: (1) Privacy-preserved text-truth samplegeneration using automatically speech recognition (ASR)-transcribed callrecordings (with anonymized original audio), ensuring real-world consistencythrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement vialarge language model (LLM)-based self-instruction sampling on authentic ASRoutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis thatsimulates emerging fraud tactics through predefined communication scenarios andfraud typologies. The generated dataset contains 28,511 rigorously processedspeech-text pairs, complete with detailed annotations for fraud reasoning. Thedataset is divided into three tasks: scenario classification, fraud detection,fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, astandardized evaluation benchmark comprising proportionally sampled instancesfrom the dataset, to facilitate systematic testing of model performance ontelecom fraud detection tasks. We also contribute a production-optimizedsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, whileopen-sourcing the data processing framework to enable community-driven datasetexpansion. This work establishes a foundational framework for multimodalanti-fraud research while addressing critical challenges in data privacy andscenario diversity. The project will be released athttps://github.com/JimmyMa99/TeleAntiFraud.</description><author>Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang</author><pubDate>Mon, 18 Aug 2025 17:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.24115v4</guid></item><item><title>All for law and law for all: Adaptive RAG Pipeline for Legal Research</title><link>http://arxiv.org/abs/2508.13107v1</link><description>Retrieval-Augmented Generation (RAG) mitigates hallucinations by groundinglarge language model outputs in cited sources, a capability that is especiallycritical in the legal domain. We present an end-to-end RAG pipeline thatrevisits and extends the LegalBenchRAG baseline with three targetedenhancements: (i) a context-aware query translator that disentangles documentreferences from natural-language questions and adapts retrieval depth andresponse style based on expertise and specificity, (ii) open-source retrievalstrategies using SBERT and GTE embeddings that achieve substantial performancegains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for$K&gt;4$) while remaining cost-efficient, and (iii) a comprehensive evaluation andgeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall toassess semantic alignment and faithfulness across models and prompt designs.Our results show that carefully designed open-source pipelines can rival oroutperform proprietary approaches in retrieval quality, while a customlegal-grounded prompt consistently produces more faithful and contextuallyrelevant answers than baseline prompting. Taken together, these contributionsdemonstrate the potential of task-aware, component-level tuning to deliverlegally grounded, reproducible, and cost-effective RAG systems for legalresearch assistance.</description><author>Figarri Keisha, Prince Singh, Pallavi, Dion Fernandes, Aravindh Manivannan, Ilham Wicaksono, Faisal Ahmad</author><pubDate>Mon, 18 Aug 2025 17:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13107v1</guid></item><item><title>Precise Action-to-Video Generation Through Visual Action Prompts</title><link>http://arxiv.org/abs/2508.13104v1</link><description>We present visual action prompts, a unified action representation foraction-to-video generation of complex high-DoF interactions while maintainingtransferable visual dynamics across domains. Action-driven video generationfaces a precision-generality trade-off: existing methods using text, primitiveactions, or coarse masks offer generality but lack precision, whileagent-centric action signals provide precision at the cost of cross-domaintransferability. To balance action precision and dynamic transferability, wepropose to "render" actions into precise visual prompts as domain-agnosticrepresentations that preserve both geometric precision and cross-domainadaptability for complex actions; specifically, we choose visual skeletons fortheir generality and accessibility. We propose robust pipelines to constructskeletons from two interaction-rich data sources - human-object interactions(HOI) and dexterous robotic manipulation - enabling cross-domain training ofaction-driven generative models. By integrating visual skeletons intopretrained video generation models via lightweight fine-tuning, we enableprecise action control of complex interaction while preserving the learning ofcross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate theeffectiveness of our proposed approach. Project page:https://zju3dv.github.io/VAP/.</description><author>Yuang Wang, Chao Wen, Haoyu Guo, Sida Peng, Minghan Qin, Hujun Bao, Xiaowei Zhou, Ruizhen Hu</author><pubDate>Mon, 18 Aug 2025 17:12:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13104v1</guid></item><item><title>High-Fidelity And Complex Test Data Generation For Real-World SQL Code Generation Services</title><link>http://arxiv.org/abs/2504.17203v2</link><description>The demand for high-fidelity test data is paramount in industrial settingswhere access to production data is largely restricted. Traditional datageneration methods often fall short, struggling with low-fidelity and theability to model complex data structures and semantic relationships that arecritical for testing complex SQL code generation services like Natural Languageto SQL (NL2SQL). In this paper, we address the critical need for generatingsyntactically correct and semantically ``meaningful'' mock data for complexschema that includes columns with nested structures that we frequentlyencounter in Google SQL code generation workloads. We highlight the limitationsof existing approaches used in production, particularly their inability tohandle large and complex schema, as well as the lack of semantically coherenttest data that lead to limited test coverage. We demonstrate that by leveragingLarge Language Models (LLMs) and incorporating strategic pre- andpost-processing steps, we can generate realistic high-fidelity test data thatadheres to complex structural constraints and maintains semantic integrity tothe test targets (SQL queries/functions). This approach supports comprehensivetesting of complex SQL queries involving joins, aggregations, and even deeplynested subqueries, ensuring robust evaluation of SQL code generation services,like NL2SQL and SQL Code Assistant services. Our results demonstrate thepractical utility of an out-of-the-box LLM (\textit{gemini}) based test datageneration for industrial SQL code generation services where generatingrealistic test data is essential due to the frequent unavailability ofproduction datasets.</description><author>Shivasankari Kannan, Yeounoh Chung, Amita Gondi, Tristan Swadell, Fatma Ozcan</author><pubDate>Mon, 18 Aug 2025 17:11:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.17203v2</guid></item><item><title>Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy</title><link>http://arxiv.org/abs/2508.13103v1</link><description>Vision-Language-Action (VLA) models frequently encounter challenges ingeneralizing to real-world environments due to inherent discrepancies betweenobservation and action spaces. Although training data are collected fromdiverse camera perspectives, the models typically predict end-effector poseswithin the robot base coordinate frame, resulting in spatial inconsistencies.To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)framework, which grounds action predictions directly in the camera observationspace. Leveraging the camera's extrinsic calibration matrix, OC-VLA transformsend-effector poses from the robot base coordinate system into the cameracoordinate system, thereby unifying prediction targets across heterogeneousviewpoints. This lightweight, plug-and-play strategy ensures robust alignmentbetween perception and action, substantially improving model resilience tocamera viewpoint variations. The proposed approach is readily compatible withexisting VLA architectures, requiring no substantial modifications.Comprehensive evaluations on both simulated and real-world robotic manipulationtasks demonstrate that OC-VLA accelerates convergence, enhances task successrates, and improves cross-view generalization. The code will be publiclyavailable.</description><author>Tianyi Zhang, Haonan Duan, Haoran Hao, Yu Qiao, Jifeng Dai, Zhi Hou</author><pubDate>Mon, 18 Aug 2025 17:10:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13103v1</guid></item><item><title>Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants</title><link>http://arxiv.org/abs/2508.13101v1</link><description>Coastal pollution is a pressing global environmental issue, necessitatingscalable and automated solutions for monitoring and management. This studyinvestigates the efficacy of the Real-Time Detection Transformer (RT-DETR), astate-of-the-art, end-to-end object detection model, for the automateddetection and counting of beach litter. A rigorous comparative analysis isconducted between two model variants, RT-DETR-Large (RT-DETR-L) andRT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset ofcoastal debris. The evaluation reveals that the RT-DETR-X model achievesmarginally superior accuracy, with a mean Average Precision at 50\% IoU(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's0.810 and 0.606, respectively. However, this minor performance gain is realizedat a significant computational cost; the RT-DETR-L model demonstrates asubstantially faster inference time of 20.1 ms versus 34.5 ms for theRT-DETR-X. The findings suggest that the RT-DETR-L model offers a morepractical and efficient solution for real-time, in-field deployment due to itssuperior balance of processing speed and detection accuracy. This researchprovides valuable insights into the application of advanced Transformer-baseddetectors for environmental conservation, highlighting the critical trade-offsbetween model complexity and operational viability.</description><author>Miftahul Huda, Arsyiah Azahra, Putri Maulida Chairani, Dimas Rizky Ramadhani, Nabila Azhari, Ade Lailani</author><pubDate>Mon, 18 Aug 2025 17:10:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13101v1</guid></item><item><title>A Perfectly Truthful Calibration Measure</title><link>http://arxiv.org/abs/2508.13100v1</link><description>Calibration requires that predictions are conditionally unbiased and,therefore, reliably interpretable as probabilities. Calibration measuresquantify how far a predictor is from perfect calibration. As introduced byHaghtalab et al. (2024), a calibration measure is truthful if it is minimizedin expectation when a predictor outputs the ground-truth probabilities.Although predicting the true probabilities guarantees perfect calibration, inreality, when calibration is evaluated on a finite sample, predicting the truthis not guaranteed to minimize any known calibration measure. All knowncalibration measures incentivize predictors to lie in order to appear morecalibrated on a finite sample. Such lack of truthfulness motivated Haghtalab etal. (2024) and Qiao and Zhao (2025) to construct approximately truthfulcalibration measures in the sequential prediction setting, but no perfectlytruthful calibration measure was known to exist even in the more basic batchsetting. We design a perfectly truthful calibration measure in the batch setting:averaged two-bin calibration error (ATB). In addition to being truthful, ATB issound, complete, continuous, and quadratically related to two existingcalibration measures: the smooth calibration error (smCal) and the (lower)distance to calibration (distCal). The simplicity in our definition of ATBmakes it efficient and straightforward to compute. ATB allows faster estimationalgorithms with significantly easier implementations than smCal and distCal,achieving improved running time and simplicity for the calibration testingproblem studied by Hu et al. (2024). We also introduce a general recipe forconstructing truthful measures, which proves the truthfulness of ATB as aspecial case and allows us to construct other truthful calibration measuressuch as quantile-binned l_2-ECE.</description><author>Jason Hartline, Lunjia Hu, Yifan Wu</author><pubDate>Mon, 18 Aug 2025 17:09:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13100v1</guid></item><item><title>Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network</title><link>http://arxiv.org/abs/2508.13099v1</link><description>This paper presents a framework for classifying and detecting spatialcommission outliers in maritime environments using seabed acoustic sensornetworks and log Gaussian Cox processes (LGCPs). By modeling target arrivals asa mixture of normal and outlier processes, we estimate the probability that anewly observed event is an outlier. We propose a second-order approximation ofthis probability that incorporates both the mean and variance of the normalintensity function, providing improved classification accuracy compared tomean-only approaches. We analytically show that our method yields a tighterbound to the true probability using Jensen's inequality. To enhance detection,we integrate a real-time, near-optimal sensor placement strategy thatdynamically adjusts sensor locations based on the evolving outlier intensity.The proposed framework is validated using real ship traffic data near Norfolk,Virginia, where numerical results demonstrate the effectiveness of our approachin improving both classification performance and outlier detection throughsensor deployment.</description><author>Mingyu Kim, Daniel Stilwell, Jorge Jimenez</author><pubDate>Mon, 18 Aug 2025 17:08:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13099v1</guid></item><item><title>Denoising diffusion models for inverse design of inflatable structures with programmable deformations</title><link>http://arxiv.org/abs/2508.13097v1</link><description>Programmable structures are systems whose undeformed geometries and materialproperty distributions are deliberately designed to achieve prescribed deformedconfigurations under specific loading conditions. Inflatable structures are aprominent example, using internal pressurization to realize large, nonlineardeformations in applications ranging from soft robotics and deployableaerospace systems to biomedical devices and adaptive architecture. We present agenerative design framework based on denoising diffusion probabilistic models(DDPMs) for the inverse design of elastic structures undergoing large,nonlinear deformations under pressure-driven actuation. The method formulatesthe inverse design as a conditional generation task, using geometricdescriptors of target deformed states as inputs and outputting image-basedrepresentations of the undeformed configuration. Representing theseconfigurations as simple images is achieved by establishing a pre- andpostprocessing pipeline that involves a fixed image processing, simulationsetup, and descriptor extraction methods. Numerical experiments with scalar andhigher-dimensional descriptors show that the framework can quickly producediverse undeformed configurations that achieve the desired deformations wheninflated, enabling parallel exploration of viable design candidates whileaccommodating complex constraints.</description><author>Sara Karimi, Nikolaos N. Vlassis</author><pubDate>Mon, 18 Aug 2025 17:07:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13097v1</guid></item><item><title>VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog</title><link>http://arxiv.org/abs/2508.13092v1</link><description>Timely detection of hardware vulnerabilities during the early design stage iscritical for reducing remediation costs. Existing early detection techniquesoften require specialized security expertise, limiting their usability. Recentefforts have explored the use of large language models (LLMs) for Verilogvulnerability detection. However, LLMs struggle to capture the structure inVerilog code, resulting in inconsistent detection results. To this end, wepropose VerilogLAVD, the first LLM-aided graph traversal rule generationapproach for Verilog vulnerability detection. Our approach introduces theVerilog Property Graph (VeriPG), a unified representation of Verilog code. Itcombines syntactic features extracted from the abstract syntax tree (AST) withsemantic information derived from control flow and data dependency graphs. Weleverage LLMs to generate VeriPG-based detection rules from Common WeaknessEnumeration (CWE) descriptions. These rules guide the rule executor thattraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, webuild a dataset collected from open-source repositories and synthesized data.In our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,VerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM withexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,respectively.</description><author>Xiang Long, Yingjie Xia, Xiyuan Chen, Li Kuang</author><pubDate>Mon, 18 Aug 2025 17:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13092v1</guid></item><item><title>DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation</title><link>http://arxiv.org/abs/2508.13091v1</link><description>While supervised stereo matching and monocular depth estimation have advancedsignificantly with learning-based algorithms, self-supervised methods usingstereo images as supervision signals have received relatively less focus andrequire further investigation. A primary challenge arises from ambiguityintroduced during photometric reconstruction, particularly due to missingcorresponding pixels in ill-posed regions of the target view, such asocclusions and out-of-frame areas. To address this and establish explicitphotometric correspondences, we propose DMS, a model-agnostic approach thatutilizes geometric priors from diffusion models to synthesize novel views alongthe epipolar direction, guided by directional prompts. Specifically, wefinetune a Stable Diffusion model to simulate perspectives at key positions:left-left view shifted from the left camera, right-right view shifted from theright camera, along with an additional novel view between the left and rightcameras. These synthesized views supplement occluded pixels, enabling explicitphotometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''method that seamlessly enhances self-supervised stereo matching and monoculardepth estimation, and relies solely on unlabeled stereo image pairs for bothtraining and synthesizing. Extensive experiments demonstrate the effectivenessof our approach, with up to 35% outlier reduction and state-of-the-artperformance across multiple benchmark datasets.</description><author>Zihua Liu, Yizhou Li, Songyan Zhang, Masatoshi Okutomi</author><pubDate>Mon, 18 Aug 2025 17:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13091v1</guid></item><item><title>Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates</title><link>http://arxiv.org/abs/2508.13088v1</link><description>Recently, neural surrogate models have emerged as a compelling alternative totraditional simulation workflows. This is accomplished by modeling theunderlying function of scientific simulations, removing the need to runexpensive simulations. Beyond just mapping from input parameter to output,surrogates have also been shown useful for inverse problems: output to inputparameters. Inverse problems can be understood as search, where we aim to findparameters whose surrogate outputs contain a specified feature. Yet findingthese parameters can be costly, especially for high-dimensional parameterspaces. Thus, existing surrogate-based solutions primarily focus on finding asmall set of matching parameters, in the process overlooking the broaderpicture of plausible parameters. Our work aims to model and visualize thedistribution of possible input parameters that produce a given output feature.To achieve this goal, we aim to address two challenges: (1) the approximationerror inherent in the surrogate model and (2) forming the parameterdistribution in an interactive manner. We model error via density estimation,reporting high density only if a given parameter configuration is close totraining parameters, measured both over the input and output space. Our densityestimate is used to form a prior belief on parameters, and when combined with alikelihood on features, gives us an efficient way to sample plausible parameterconfigurations that generate a target output feature. We demonstrate theusability of our solution through a visualization interface by performingfeature-driven parameter analysis over the input parameter space of threesimulation datasets. Source code is available athttps://github.com/matthewberger/seeing-the-many</description><author>Xiaohan Wang, Zhimin Li, Joshua A. Levine, Matthew Berger</author><pubDate>Mon, 18 Aug 2025 17:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13088v1</guid></item><item><title>WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction</title><link>http://arxiv.org/abs/2505.04813v2</link><description>In this work we present WIR3D, a technique for abstracting 3D shapes througha sparse set of visually meaningful curves in 3D. We optimize the parameters ofBezier curves such that they faithfully represent both the geometry and salientvisual features (e.g. texture) of the shape from arbitrary viewpoints. Weleverage the intermediate activations of a pre-trained foundation model (CLIP)to guide our optimization process. We divide our optimization into two phases:one for capturing the coarse geometry of the shape, and the other forrepresenting fine-grained features. Our second phase supervision is spatiallyguided by a novel localized keypoint loss. This spatial guidance enables usercontrol over abstracted features. We ensure fidelity to the original surfacethrough a neural SDF loss, which allows the curves to be used as intuitivedeformation handles. We successfully apply our method for shape abstractionover a broad dataset of shapes with varying complexity, geometric structure,and texture, and demonstrate downstream applications for feature control andshape deformation.</description><author>Richard Liu, Daniel Fu, Noah Tan, Itai Lang, Rana Hanocka</author><pubDate>Mon, 18 Aug 2025 17:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.04813v2</guid></item><item><title>Checkmate: interpretable and explainable RSVQA is the endgame</title><link>http://arxiv.org/abs/2508.13086v1</link><description>Remote Sensing Visual Question Answering (RSVQA) presents unique challengesin ensuring that model decisions are both understandable and grounded in visualcontent. Current models often suffer from a lack of interpretability andexplainability, as well as from biases in dataset distributions that lead toshortcut learning. In this work, we tackle these issues by introducing a novelRSVQA dataset, Chessboard, designed to minimize biases through 3'123'253questions and a balanced answer distribution. Each answer is linked to one ormore cells within the image, enabling fine-grained visual reasoning. Building on this dataset, we develop an explainable and interpretable modelcalled Checkmate that identifies the image cells most relevant to itsdecisions. Through extensive experiments across multiple model architectures,we show that our approach improves transparency and supports more trustworthydecision-making in RSVQA systems.</description><author>Lucrezia Tosato, Christel Tartini Chappuis, Syrielle Montariol, Flora Weissgerber, Sylvain Lobry, Devis Tuia</author><pubDate>Mon, 18 Aug 2025 16:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13086v1</guid></item><item><title>Deliberate Planning in Language Models with Symbolic Representation</title><link>http://arxiv.org/abs/2505.01479v2</link><description>Planning remains a core challenge for language models (LMs), particularly indomains that require coherent multi-step action sequences grounded in externalconstraints. We introduce SymPlanner, a novel framework that equips LMs withstructured planning capabilities by interfacing them with a symbolicenvironment that serves as an explicit world model. Rather than relying purelyon natural language reasoning, SymPlanner grounds the planning process in asymbolic state space, where a policy model proposes actions and a symbolicenvironment deterministically executes and verifies their effects. To enhanceexploration and improve robustness, we introduce Iterative Correction (IC),which refines previously proposed actions by leveraging feedback from thesymbolic environment to eliminate invalid decisions and guide the model towardvalid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grainedcomparison of candidate plans by evaluating them jointly. We evaluateSymPlanner on PlanBench, demonstrating that it produces more coherent, diverse,and verifiable plans than pure natural language baselines.</description><author>Siheng Xiong, Zhangding Liu, Jieyu Zhou, Yusen Su</author><pubDate>Mon, 18 Aug 2025 16:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.01479v2</guid></item><item><title>AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up</title><link>http://arxiv.org/abs/2505.24584v3</link><description>Recent advances in generative AI have accelerated the discovery of novelchemicals and materials. However, scaling these discoveries to industrialproduction remains a major bottleneck due to the synthesis gap -- the need todevelop entirely new manufacturing processes. This challenge requires detailedengineering blueprints: PFDs for equipment layouts and material/energy flows,and PIDs for process plant operations. Current AI systems cannot yet reliablygenerate these critical engineering schematics, creating a fundamental obstacleto manufacturing scale-up of novel discoveries. We present a closed-loop,physics-aware framework for automated generation of industrially viable PFDsand PIDs. The framework integrates three key components: (1) domain-specializedsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)a hierarchical knowledge graph containing process flow and instrumentationdescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation(GRAG), and (3) an open-source chemical process simulator for modeling,simulation, optimization, and analysis of novel chemical processes. The SLMsare trained through a multi-stage pipeline on synthetic datasets, with processsimulator-in-the-loop validation ensuring feasibility. To enhance computationalefficiency, the framework implements structural pruning (width and depth)guided by importance heuristics to reduce language model size while preservingaccuracy, followed by advanced inference optimizations includingFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,and Test-Time Inference Scaling. Experimental results demonstrate that ourframework generates simulator-validated process descriptions with highfidelity.</description><author>Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana</author><pubDate>Mon, 18 Aug 2025 16:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24584v3</guid></item><item><title>DocHPLT: A Massively Multilingual Document-Level Translation Dataset</title><link>http://arxiv.org/abs/2508.13079v1</link><description>Existing document-level machine translation resources are only available fora handful of languages, mostly high-resourced ones. To facilitate the trainingand evaluation of document-level translation and, more broadly, long-contextmodeling for global communities, we create DocHPLT, the largest publiclyavailable document-level translation dataset to date. It contains 124 millionaligned document pairs across 50 languages paired with English, comprising 4.26billion sentences, with further possibility to provide 2500 bonus pairs notinvolving English. Unlike previous reconstruction-based approaches that piecetogether documents from sentence-level data, we modify an existing webextraction pipeline to preserve complete document integrity from the source,retaining all content including unaligned portions. After our preliminaryexperiments identify the optimal training context strategy for document-leveltranslation, we demonstrate that LLMs fine-tuned on DocHPLT substantiallyoutperform off-the-shelf instruction-tuned baselines, with particularlydramatic improvements for under-resourced languages. We open-source the datasetunder a permissive license, providing essential infrastructure for advancingmultilingual document-level translation.</description><author>DayyÃ¡n O'Brien, Bhavitvya Malik, Ona de Gibert, Pinzhen Chen, Barry Haddow, JÃ¶rg Tiedemann</author><pubDate>Mon, 18 Aug 2025 16:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13079v1</guid></item><item><title>ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset</title><link>http://arxiv.org/abs/2508.13078v1</link><description>Nowadays, the development of a Presentation Attack Detection (PAD) system forID cards presents a challenge due to the lack of images available to train arobust PAD system and the increase in diversity of possible attack instrumentspecies. Today, most algorithms focus on generating attack samples and do nottake into account the limited number of bona fide images. This work is one ofthe first to propose a method for mimicking bona fide images by generatingsynthetic versions of them using Stable Diffusion, which may help improve thegeneralisation capabilities of the detector. Furthermore, the new imagesgenerated are evaluated in a system trained from scratch and in a commercialsolution. The PAD system yields an interesting result, as it identifies ourimages as bona fide, which has a positive impact on detection performance anddata restrictions.</description><author>Qingwen Zeng, Juan E. Tapia, Izan Garcia, Juan M. Espin, Christoph Busch</author><pubDate>Mon, 18 Aug 2025 16:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13078v1</guid></item><item><title>From Transthoracic to Transesophageal: Cross-Modality Generation using LoRA Diffusion</title><link>http://arxiv.org/abs/2508.13077v1</link><description>Deep diffusion models excel at realistic image synthesis but demand largetraining sets-an obstacle in data-scarce domains like transesophagealechocardiography (TEE). While synthetic augmentation has boosted performance intransthoracic echo (TTE), TEE remains critically underrepresented, limiting thereach of deep learning in this high-impact modality. We address this gap by adapting a TTE-trained, mask-conditioned diffusionbackbone to TEE with only a limited number of new cases and adapters as smallas $10^5$ parameters. Our pipeline combines Low-Rank Adaptation with MaskR$^2$,a lightweight remapping layer that aligns novel mask formats with thepretrained model's conditioning channels. This design lets users adapt modelsto new datasets with a different set of anatomical structures to the basemodel's original set. Through a targeted adaptation strategy, we find that adapting only MLP layerssuffices for high-fidelity TEE synthesis. Finally, mixing less than 200 realTEE frames with our synthetic echoes improves the dice score on a multiclasssegmentation task, particularly boosting performance on underrepresentedright-heart structures. Our results demonstrate that (1) semanticallycontrolled TEE images can be generated with low overhead, (2) MaskR$^2$effectively transforms unseen mask formats into compatible formats withoutdamaging downstream task performance, and (3) our method generates images thatare effective for improving performance on a downstream task of multiclasssegmentation.</description><author>Emmanuel Oladokun, Yuxuan Ou, Anna Novikova, Daria Kulikova, Sarina Thomas, Jurica Å prem, Vicente Grau</author><pubDate>Mon, 18 Aug 2025 16:48:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13077v1</guid></item><item><title>CaRL: Learning Scalable Planning Policies with Simple Rewards</title><link>http://arxiv.org/abs/2504.17838v2</link><description>We investigate reinforcement learning (RL) for privileged planning inautonomous driving. State-of-the-art approaches for this task are rule-based,but these methods do not scale to the long tail. RL, on the other hand, isscalable and does not suffer from compounding errors like imitation learning.Contemporary RL approaches for driving use complex shaped rewards that summultiple individual rewards, \eg~progress, position, or orientation rewards. Weshow that PPO fails to optimize a popular version of these rewards when themini-batch size is increased, which limits the scalability of these approaches.Instead, we propose a new reward design based primarily on optimizing a singleintuitive reward term: route completion. Infractions are penalized byterminating the episode or multiplicatively reducing route completion. We findthat PPO scales well with higher mini-batch sizes when trained with our simplereward, even improving performance. Training with large mini-batch sizesenables efficient scaling via distributed data parallelism. We scale PPO to300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. Theresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,outperforming other RL methods with more complex rewards by a large margin.Requiring only minimal adaptations from its use in CARLA, the same method isthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and90.6 in reactive traffic on the Val14 benchmark while being an order ofmagnitude faster than prior work.</description><author>Bernhard Jaeger, Daniel Dauner, Jens BeiÃwenger, Simon Gerstenecker, Kashyap Chitta, Andreas Geiger</author><pubDate>Mon, 18 Aug 2025 16:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.17838v2</guid></item><item><title>GraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data</title><link>http://arxiv.org/abs/2409.14500v3</link><description>Although data that can be naturally represented as graphs is widespread inreal-world applications across diverse industries, popular graph ML benchmarksfor node property prediction only cover a surprisingly narrow set of datadomains, and graph neural networks (GNNs) are often evaluated on just a fewacademic citation networks. This issue is particularly pressing in light of therecent growing interest in designing graph foundation models. These models aresupposed to be able to transfer to diverse graph datasets from differentdomains, and yet the proposed graph foundation models are often evaluated on avery limited set of datasets from narrow applications. To alleviate this issue,we introduce GraphLand: a benchmark of 14 diverse graph datasets for nodeproperty prediction from a range of different industrial applications.GraphLand allows evaluating graph ML models on a wide range of graphs withdiverse sizes, structural characteristics, and feature sets, all in a unifiedsetting. Further, GraphLand allows investigating such previously underexploredresearch questions as how realistic temporal distributional shifts undertransductive and inductive settings influence graph ML model performance. Tomimic realistic industrial settings, we use GraphLand to compare GNNs withgradient-boosted decision trees (GBDT) models that are popular in industrialapplications and show that GBDTs provided with additional graph-based inputfeatures can sometimes be very strong baselines. Further, we evaluate currentlyavailable general-purpose graph foundation models and find that they fail toproduce competitive results on our proposed datasets.</description><author>Gleb Bazhenov, Oleg Platonov, Liudmila Prokhorenkova</author><pubDate>Mon, 18 Aug 2025 16:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14500v3</guid></item><item><title>A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis</title><link>http://arxiv.org/abs/2508.13072v1</link><description>Contemporary cardiovascular management involves complex consideration andintegration of multimodal cardiac datasets, where each modality providesdistinct but complementary physiological characteristics. While the effectiveintegration of multiple modalities could yield a holistic clinical profile thataccurately models the true clinical situation with respect to data modalitiesand their relatives weightings, current methodologies remain limited by: 1) thescarcity of patient- and time-aligned multimodal data; 2) reliance on isolatedsingle-modality or rigid multimodal input combinations; 3) alignment strategiesthat prioritize cross-modal similarity over complementarity; and 4) a narrowsingle-task focus. In response to these limitations, a comprehensive multimodaldataset was curated for immediate application, integrating laboratory testresults, electrocardiograms, and echocardiograms with clinical outcomes.Subsequently, a unified framework, Textual Guidance Multimodal fusion forMultiple cardiac tasks (TGMM), was proposed. TGMM incorporated three keycomponents: 1) a MedFlexFusion module designed to capture the unique andcomplementary characteristics of medical modalities and dynamically integratedata from diverse cardiac sources and their combinations; 2) a textual guidancemodule to derive task-relevant representations tailored to diverse clinicalobjectives, including heart disease diagnosis, risk stratification andinformation retrieval; and 3) a response module to produce final decisions forall these tasks. Furthermore, this study systematically explored key featuresacross multiple modalities and elucidated their synergistic contributions inclinical decision-making. Extensive experiments showed that TGMM outperformedstate-of-the-art methods across multiple clinical tasks, with additionalvalidation confirming its robustness on another public dataset.</description><author>Yuting Zhang, Tiantian Geng, Luoying Hao, Xinxing Cheng, Alexander Thorley, Xiaoxia Wang, Wenqi Lu, Sandeep S Hothi, Lei Wei, Zhaowen Qiu, Dipak Kotecha, Jinming Duan</author><pubDate>Mon, 18 Aug 2025 16:43:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13072v1</guid></item><item><title>Reinforced Context Order Recovery for Adaptive Reasoning and Planning</title><link>http://arxiv.org/abs/2508.13070v1</link><description>Modern causal language models, followed by rapid developments in discretediffusion models, can now produce a wide variety of interesting and usefulcontent. However, these families of models are predominantly trained to outputtokens with a fixed (left-to-right) or random order, which may deviate from thelogical order in which tokens are generated originally. In this paper, weobserve that current causal and diffusion models encounter difficulties inproblems that require adaptive token generation orders to solve tractably,which we characterize with the $\mathcal{V}$-information framework. Motivatedby this, we propose Reinforced Context Order Recovery (ReCOR), areinforcement-learning-based framework to extract adaptive, data-dependenttoken generation orders from text data without annotations. Self-supervised bytoken prediction statistics, ReCOR estimates the hardness of predicting everyunfilled token and adaptively selects the next token during both training andinference. Experiments on challenging reasoning and planning datasetsdemonstrate the superior performance of ReCOR compared with baselines,sometimes outperforming oracle models supervised with the ground-truth order.</description><author>Long Ma, Fangwei Zhong, Yizhou Wang</author><pubDate>Mon, 18 Aug 2025 16:42:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13070v1</guid></item><item><title>Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation</title><link>http://arxiv.org/abs/2508.13068v1</link><description>We propose a two-stage multimodal framework that enhances diseaseclassification and region-aware radiology report generation from chest X-rays,leveraging the MIMIC-Eye dataset. In the first stage, we introduce agaze-guided contrastive learning architecture for disease classification. Itintegrates visual features, clinical labels, bounding boxes, and radiologisteye-tracking signals and is equipped with a novel multi-term gaze-attentionloss combining MSE, KL divergence, correlation, and center-of-mass alignment.Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUCfrom 0.821 to 0.849 (+3.41%), while also improving precision and recall,highlighting the effectiveness of gaze-informed attention supervision. In thesecond stage, we present a modular report generation pipeline that extractsconfidence-weighted diagnostic keywords, maps them to anatomical regions usinga curated dictionary constructed from domain-specific priors, and generatesregion-aligned sentences via structured prompts. This pipeline improves reportquality as measured by clinical keyword recall and ROUGE overlap. Our resultsdemonstrate that integrating gaze data improves both classification performanceand the interpretability of generated medical reports.</description><author>Tanjim Islam Riju, Shuchismita Anwar, Saman Sarker Joy, Farig Sadeque, Swakkhar Shatabda</author><pubDate>Mon, 18 Aug 2025 16:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13068v1</guid></item><item><title>LLMs Are In-Context Bandit Reinforcement Learners</title><link>http://arxiv.org/abs/2410.05362v3</link><description>Large Language Models (LLMs) excel at in-context learning (ICL), a supervisedlearning technique that relies on adding annotated examples to the modelcontext. We investigate a contextual bandit version of in-context reinforcementlearning (ICRL), where models learn in-context, online, from external reward,instead of supervised data. We show that LLMs effectively demonstrate suchlearning, and provide a detailed study of the phenomena, experimenting withchallenging classification tasks and models of sizes from 500M to 70Bparameters. This includes identifying and addressing the instability of theprocess, demonstrating learning with both semantic and abstract labels, andshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, whilealso underscoring fundamental limitations in their implicit reasoning abouterrors.</description><author>Giovanni Monea, Antoine Bosselut, KiantÃ© Brantley, Yoav Artzi</author><pubDate>Mon, 18 Aug 2025 16:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05362v3</guid></item><item><title>DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition</title><link>http://arxiv.org/abs/2503.18830v2</link><description>Gait recognition is emerging as a promising and innovative area within thefield of computer vision, widely applied to remote person identification.Although existing gait recognition methods have achieved substantial success incontrolled laboratory datasets, their performance often declines significantlywhen transitioning to wild datasets.We argue that the performance gap can beprimarily attributed to the spatio-temporal distribution inconsistenciespresent in wild datasets, where subjects appear at varying angles, positions,and distances across the frames. To achieve accurate gait recognition in thewild, we propose a skeleton-guided silhouette alignment strategy, which usesprior knowledge of the skeletons to perform affine transformations on thecorresponding silhouettes.To the best of our knowledge, this is the first studyto explore the impact of data alignment on gait recognition. We conductedextensive experiments across multiple datasets and network architectures, andthe results demonstrate the significant advantages of our proposed alignmentstrategy.Specifically, on the challenging Gait3D dataset, our method achievedan average performance improvement of 7.9% across all evaluated networks.Furthermore, our method achieves substantial improvements on cross-domaindatasets, with accuracy improvements of up to 24.0%.</description><author>Zhengxian Wu, Chuanrui Zhang, Hangrui Xu, Peng Jiao, Haoqian Wang</author><pubDate>Mon, 18 Aug 2025 16:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.18830v2</guid></item><item><title>Degradation-Agnostic Statistical Facial Feature Transformation for Blind Face Restoration in Adverse Weather Conditions</title><link>http://arxiv.org/abs/2507.07464v3</link><description>With the increasing deployment of intelligent CCTV systems in outdoorenvironments, there is a growing demand for face recognition systems optimizedfor challenging weather conditions. Adverse weather significantly degradesimage quality, which in turn reduces recognition accuracy. Although recent faceimage restoration (FIR) models based on generative adversarial networks (GANs)and diffusion models have shown progress, their performance remains limited dueto the lack of dedicated modules that explicitly address weather-induceddegradations. This leads to distorted facial textures and structures. Toaddress these limitations, we propose a novel GAN-based blind FIR frameworkthat integrates two key components: local Statistical Facial FeatureTransformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). Thelocal SFFT module enhances facial structure and color fidelity by aligning thelocal statistical distributions of low-quality (LQ) facial regions with thoseof high-quality (HQ) counterparts. Complementarily, the DAFE module enablesrobust statistical facial feature extraction under adverse weather conditionsby aligning LQ and HQ encoder representations, thereby making the restorationprocess adaptive to severe weather-induced degradations. Experimental resultsdemonstrate that the proposed degradation-agnostic SFFT model outperformsexisting state-of-the-art FIR methods based on GAN and diffusion models,particularly in suppressing texture distortions and accurately reconstructingfacial structures. Furthermore, both the SFFT and DAFE modules are empiricallyvalidated in enhancing structural fidelity and perceptual quality in facerestoration under challenging weather scenarios.</description><author>Chang-Hwan Son</author><pubDate>Mon, 18 Aug 2025 16:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.07464v3</guid></item><item><title>Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping</title><link>http://arxiv.org/abs/2508.13065v1</link><description>Human shape editing enables controllable transformation of a person's bodyshape, such as thin, muscular, or overweight, while preserving pose, identity,clothing, and background. Unlike human pose editing, which has advancedrapidly, shape editing remains relatively underexplored. Current approachestypically rely on 3D morphable models or image warping, often introducingunrealistic body proportions, texture distortions, and backgroundinconsistencies due to alignment errors and deformations. A key limitation isthe lack of large-scale, publicly available datasets for training andevaluating body shape manipulation methods. In this work, we introduce thefirst large-scale dataset of 18,573 images across 1523 subjects, specificallydesigned for controlled human shape editing. It features diverse variations inbody shape, including fat, muscular and thin, captured under consistentidentity, clothing, and background conditions. Using this dataset, we proposeOdo, an end-to-end diffusion-based method that enables realistic and intuitivebody reshaping guided by simple semantic attributes. Our approach combines afrozen UNet that preserves fine-grained appearance and background details fromthe input image with a ControlNet that guides shape transformation using targetSMPL depth maps. Extensive experiments demonstrate that our method outperformsprior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,significantly lower than the 13.6mm observed in baseline methods, whileproducing realistic results that accurately match the desired target shapes.</description><author>Siddharth Khandelwal, Sridhar Kamath, Arjun Jain</author><pubDate>Mon, 18 Aug 2025 16:37:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13065v1</guid></item><item><title>Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation</title><link>http://arxiv.org/abs/2508.13064v1</link><description>Personalized news recommendation aims to deliver news articles aligned withusers' interests, serving as a key solution to alleviate the problem ofinformation overload on online news platforms. While prior work has improvedinterest matching through refined representations of news and users, thefollowing time-related challenges remain underexplored: (C1) leveraging the ageof clicked news to infer users' interest persistence, and (C2) modeling thevarying lifetime of news across topics and users. To jointly address thesechallenges, we propose a novel Lifetime-aware Interest Matching framework fornEws recommendation, named LIME, which incorporates three key strategies: (1)User-Topic lifetime-aware age representation to capture the relative age ofnews with respect to a user-topic pair, (2) Candidate-aware lifetime attentionfor generating temporally aligned user representation, and (3) Freshness-guidedinterest refinement for prioritizing valid candidate news at prediction time.Extensive experiments on two real-world datasets demonstrate that LIMEconsistently outperforms a wide range of state-of-the-art news recommendationmethods, and its model agnostic strategies significantly improve recommendationaccuracy.</description><author>Seongeun Ryu, Yunyong Ko, Sang-Wook Kim</author><pubDate>Mon, 18 Aug 2025 16:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13064v1</guid></item><item><title>Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database</title><link>http://arxiv.org/abs/2508.13060v1</link><description>The Simon Fraser University Speech Error Database (SFUSED) is a public datacollection developed for linguistic and psycholinguistic research. Here wedemonstrate how its design and annotations can be used to test and evaluatespeech recognition models. The database comprises systematically annotatedspeech errors from spontaneous English speech, with each error tagged forintended and actual error productions. The annotation schema incorporatesmultiple classificatory dimensions that are of some value to model assessment,including linguistic hierarchical level, contextual sensitivity, degradedwords, word corrections, and both word-level and syllable-level errorpositioning. To assess the value of these classificatory variables, weevaluated the transcription accuracy of WhisperX across 5,300 documented wordand phonological errors. This analysis demonstrates the atabase's effectivenessas a diagnostic tool for ASR system performance.</description><author>John Alderete, Macarious Kin Fung Hui, Aanchan Mohan</author><pubDate>Mon, 18 Aug 2025 16:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13060v1</guid></item><item><title>Towards Multimodal Social Conversations with Robots: Using Vision-Language Models</title><link>http://arxiv.org/abs/2507.19196v2</link><description>Large language models have given social robots the ability to autonomouslyengage in open-domain conversations. However, they are still missing afundamental social skill: making use of the multiple modalities that carrysocial interactions. While previous work has focused on task-orientedinteractions that require referencing the environment or specific phenomena insocial interactions such as dialogue breakdowns, we outline the overall needsof a multimodal system for social conversations with robots. We then argue thatvision-language models are able to process this wide range of visualinformation in a sufficiently general manner for autonomous social robots. Wedescribe how to adapt them to this setting, which technical challenges remain,and briefly discuss evaluation practices.</description><author>Ruben Janssens, Tony Belpaeme</author><pubDate>Mon, 18 Aug 2025 16:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.19196v2</guid></item><item><title>DoÄal Dil Ä°Ålemede Tokenizasyon StandartlarÄ± ve ÃlÃ§Ã¼mÃ¼: TÃ¼rkÃ§e Ãzerinden BÃ¼yÃ¼k Dil Modellerinin KarÅÄ±laÅtÄ±rmalÄ± Analizi</title><link>http://arxiv.org/abs/2508.13058v1</link><description>Tokenization is a fundamental preprocessing step in Natural LanguageProcessing (NLP), significantly impacting the capability of large languagemodels (LLMs) to capture linguistic and semantic nuances. This study introducesa novel evaluation framework addressing tokenization challenges specific tomorphologically-rich and low-resource languages such as Turkish. Utilizing theTurkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions fromthe Turkish education system, we assessed tokenizers based on vocabulary size,token count, processing time, language-specific token percentages (\%TR), andtoken purity (\%Pure). These newly proposed metrics measure how effectivelytokenizers preserve linguistic structures. Our analysis reveals thatlanguage-specific token percentages exhibit a stronger correlation withdownstream performance (e.g., MMLU scores) than token purity. Furthermore,increasing model parameters alone does not necessarily enhance linguisticperformance, underscoring the importance of tailored, language-specifictokenization methods. The proposed framework establishes robust and practicaltokenization standards for morphologically complex languages.</description><author>M. Ali Bayram, Ali Arda Fincan, Ahmet Semih GÃ¼mÃ¼Å, Sercan KarakaÅ, Banu Diri, SavaÅ YÄ±ldÄ±rÄ±m</author><pubDate>Mon, 18 Aug 2025 16:26:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13058v1</guid></item><item><title>Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models</title><link>http://arxiv.org/abs/2508.13057v1</link><description>Demand forecasting is essential for strategic planning in competitiveenvironments, enabling resource optimization and improved responsiveness tomarket dynamics. However, multivariate time series modeling faces challengesdue to data complexity, uncertainty, and frequent regime shifts. Traditionalevaluation metrics can introduce biases and limit generalization. This workcompares two custom evaluation functions: FMAE (Focused Mean Absolute Error),focused on minimizing absolute errors, and HEF (Hierarchical EvaluationFunction), designed to weight global metrics and penalize large deviations.Experiments were conducted under different data splits (91:9, 80:20, 70:30)using three optimizers (Grid Search, PSO, Optuna), assessing fit, relativeaccuracy, robustness, and computational efficiency. Results show that HEFconsistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,RMSSE), enhancing model robustness and explanatory power. These findings wereconfirmed via visualizations and statistical tests. Conversely, FMAE offersadvantages in local metrics (MAE, MASE) and execution time, making it suitablefor short-term scenarios. The study highlights a methodological trade-off: HEFis ideal for strategic planning, while FMAE is better suited for operationalefficiency. A replicable framework is proposed for optimizing predictive modelsin dynamic environments.</description><author>Adolfo GonzÃ¡lez, VÃ­ctor Parada</author><pubDate>Mon, 18 Aug 2025 16:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13057v1</guid></item><item><title>When can in-context learning generalize out of task distribution?</title><link>http://arxiv.org/abs/2506.05574v2</link><description>In-context learning (ICL) is a remarkable capability of pretrainedtransformers that allows models to generalize to unseen tasks after seeing onlya few examples. We investigate empirically the conditions necessary on thepretraining distribution for ICL to emerge and generalize\emph{out-of-distribution}. Previous work has focused on the number of distincttasks necessary in the pretraining dataset. Here, we use a different notion oftask diversity to study the emergence of ICL in transformers trained on linearfunctions. We find that as task diversity increases, transformers undergo atransition from a specialized solution, which exhibits ICL only within thepretraining task distribution, to a solution which generalizes out ofdistribution to the entire task space. We also investigate the nature of thesolutions learned by the transformer on both sides of the transition, andobserve similar transitions in nonlinear regression problems. We construct aphase diagram to characterize how our concept of task diversity interacts withthe number of pretraining tasks. In addition, we explore how factors such asthe depth of the model and the dimensionality of the regression probleminfluence the transition.</description><author>Chase Goddard, Lindsay M. Smith, Vudtiwat Ngampruetikorn, David J. Schwab</author><pubDate>Mon, 18 Aug 2025 16:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05574v2</guid></item><item><title>STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning</title><link>http://arxiv.org/abs/2412.15182v2</link><description>Robot learning is witnessing a significant increase in the size, diversity,and complexity of pre-collected datasets, mirroring trends in domains such asnatural language processing and computer vision. Many robot learning methodstreat such datasets as multi-task expert data and learn a multi-task,generalist policy by training broadly across them. Notably, while thesegeneralist policies can improve the average performance across many tasks, theperformance of generalist policies on any one task is often suboptimal due tonegative transfer between partitions of the data, compared to task-specificspecialist policies. In this work, we argue for the paradigm of trainingpolicies during deployment given the scenarios they encounter: rather thandeploying pre-trained policies to unseen problems in a zero-shot manner, wenon-parametrically retrieve and train models directly on relevant data at testtime. Furthermore, we show that many robotics tasks share considerable amountsof low-level behaviors and that retrieval at the "sub"-trajectory granularityenables significantly improved data utilization, generalization, and robustnessin adapting policies to novel problems. In contrast, existing full-trajectoryretrieval methods tend to underutilize the data and miss out on sharedcross-task content. This work proposes STRAP, a technique for leveragingpre-trained vision foundation models and dynamic time warping to retrievesub-sequences of trajectories from large training corpora in a robust fashion.STRAP outperforms both prior retrieval algorithms and multi-task learningmethods in simulated and real experiments, showing the ability to scale to muchlarger offline datasets in the real world as well as the ability to learnrobust control policies with just a handful of real-world demonstrations.</description><author>Marius Memmel, Jacob Berg, Bingqing Chen, Abhishek Gupta, Jonathan Francis</author><pubDate>Mon, 18 Aug 2025 16:14:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15182v2</guid></item><item><title>XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads</title><link>http://arxiv.org/abs/2508.13049v1</link><description>This work proposes XR-NPE, a high-throughput Mixed-precision SIMD NeuralProcessing Engine, designed for extended reality (XR) perception workloads likevisual inertial odometry (VIO), object classification, and eye gaze extraction.XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)formats, with layer adaptive hybrid-algorithmic implementation supportingultra-low bit precision to significantly reduce memory bandwidth requirements,and accompanied by quantization-aware training for minimal accuracy loss. Theproposed Reconfigurable Mantissa Multiplication and Exponent processingCircuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assistedby selective power gating to reduce energy consumption, providing 2.85ximproved arithmetic intensity. XR-NPE achieves a maximum operating frequency of1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,reducing 42% area, 38% power compared to the best of state-of-the-art MACapproaches. The proposed XR-NPE based AXI-enabled Matrix-multiplicationco-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2xbetter energy efficiency compared to SoTA accelerators on VCU129. The proposedco-processor provides 23% better energy efficiency and 4% better computedensity for VIO workloads. XR-NPE establishes itself as a scalable,precision-adaptive compute engine for future resource-constrained XR devices.The complete set for codes for results reproducibility are released publicly,enabling designers and researchers to readily adopt and build upon them.https://github.com/mukullokhande99/XR-NPE.</description><author>Tejas Chaudhari, Akarsh J., Tanushree Dewangan, Mukul Lokhande, Santosh Kumar Vishvakarma</author><pubDate>Mon, 18 Aug 2025 16:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13049v1</guid></item><item><title>Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</title><link>http://arxiv.org/abs/2504.05410v2</link><description>The dominant approach to generating from language models subject to someconstraint is locally constrained decoding (LCD), incrementally sampling tokensat each time step such that the constraint is never violated. Typically, thisis achieved through token masking: looping over the vocabulary and excludingnon-conforming tokens. There are two important problems with this approach. (i)Evaluating the constraint on every token can be prohibitively expensive -- LMvocabularies often exceed $100,000$ tokens. (ii) LCD can distort the globaldistribution over strings, sampling tokens based only on local information,even if they lead down dead-end paths. This work introduces a new algorithmthat addresses both these problems. First, to avoid evaluating a constraint onthe full vocabulary at each step of generation, we propose an adaptiverejection sampling algorithm that typically requires orders of magnitude fewerconstraint evaluations. Second, we show how this algorithm can be extended toproduce low-variance, unbiased estimates of importance weights at a very smalladditional cost -- estimates that can be soundly used within previouslyproposed sequential Monte Carlo algorithms to correct for the myopic behaviorof local constraint enforcement. Through extensive empirical evaluation intext-to-SQL, molecular synthesis, goal inference, pattern matching, and JSONdomains, we show that our approach is superior to state-of-the-art baselines,supporting a broader class of constraints and improving both runtime andperformance. Additional theoretical and empirical analyses show that ourmethod's runtime efficiency is driven by its dynamic use of computation,scaling with the divergence between the unconstrained and constrained LM, andas a consequence, runtime improvements are greater for better models.</description><author>Benjamin Lipkin, Benjamin LeBrun, Jacob Hoover Vigly, JoÃ£o Loula, David R. MacIver, Li Du, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Timothy J. O'Donnell, Alexander K. Lew, Tim Vieira</author><pubDate>Mon, 18 Aug 2025 16:10:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05410v2</guid></item><item><title>Using AI for User Representation: An Analysis of 83 Persona Prompts</title><link>http://arxiv.org/abs/2508.13047v1</link><description>We analyzed 83 persona prompts from 27 research articles that used largelanguage models (LLMs) to generate user personas. Findings show that theprompts predominantly generate single personas. Several prompts express adesire for short or concise persona descriptions, which deviates from thetradition of creating rich, informative, and rounded persona profiles. Text isthe most common format for generated persona attributes, followed by numbers.Text and numbers are often generated together, and demographic attributes areincluded in nearly all generated personas. Researchers use up to 12 prompts ina single study, though most research uses a small number of prompts. Comparisonand testing multiple LLMs is rare. More than half of the prompts require thepersona output in a structured format, such as JSON, and 74% of the promptsinsert data or dynamic variables. We discuss the implications of increased useof computational personas for user representation.</description><author>Joni Salminen, Danial Amin, Bernard Jansen</author><pubDate>Mon, 18 Aug 2025 16:09:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13047v1</guid></item><item><title>Quantformer: from attention to profit with a quantitative transformer trading strategy</title><link>http://arxiv.org/abs/2404.00424v3</link><description>In traditional quantitative trading practice, navigating the complicated anddynamic financial market presents a persistent challenge. Fully capturingvarious market variables, including long-term information, as well as essentialsignals that may lead to profit remains a difficult task for learningalgorithms. In order to tackle this challenge, this paper introducesquantformer, an enhanced neural network architecture based on transformer, tobuild investment factors. By transfer learning from sentiment analysis,quantformer not only exploits its original inherent advantages in capturinglong-range dependencies and modeling complex data relationships, but is alsoable to solve tasks with numerical inputs and accurately forecast futurereturns over a given period. This work collects more than 5,000,000 rollingdata of 4,601 stocks in the Chinese capital market from 2010 to 2023. Theresults of this study demonstrate the model's superior performance inpredicting stock trends compared with other 100-factor-based quantitativestrategies. Notably, the model's innovative use of transformer-like model toestablish factors, in conjunction with market sentiment information, has beenshown to enhance the accuracy of trading signals significantly, therebyoffering promising implications for the future of quantitative tradingstrategies.</description><author>Zhaofeng Zhang, Banghao Chen, Shengxin Zhu, Nicolas LangrenÃ©</author><pubDate>Mon, 18 Aug 2025 16:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00424v3</guid></item><item><title>Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models</title><link>http://arxiv.org/abs/2504.04823v2</link><description>Recent advancements in reasoning language models have demonstrated remarkableperformance in complex tasks, but their extended chain-of-thought reasoningprocess increases inference overhead. While quantization has been widelyadopted to reduce the inference cost of large language models, its impact onreasoning models remains understudied. In this paper, we conduct the firstsystematic study on quantized reasoning models, evaluating the open-sourcedDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70Bparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,and activation quantization using state-of-the-art algorithms at varyingbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Ourfindings reveal that while lossless quantization can be achieved with W8A8 orW4A16 quantization, lower bit-widths introduce significant accuracy risks. Wefurther identify model size, model origin, and task difficulty as criticaldeterminants of performance. Contrary to expectations, quantized models do notexhibit increased output lengths. In addition, strategically scaling the modelsizes or reasoning steps can effectively enhance the performance. All quantizedmodels and codes are open-sourced inhttps://github.com/ruikangliu/Quantized-Reasoning-Models.</description><author>Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou</author><pubDate>Mon, 18 Aug 2025 16:06:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.04823v2</guid></item><item><title>INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization</title><link>http://arxiv.org/abs/2505.24269v2</link><description>In-network computation represents a transformative approach to addressing theescalating demands of Artificial Intelligence (AI) workloads on networkinfrastructure. By leveraging the processing capabilities of network devicessuch as switches, routers, and Network Interface Cards (NICs), this paradigmenables AI computations to be performed directly within the network fabric,significantly reducing latency, enhancing throughput, and optimizing resourceutilization. This paper provides a comprehensive analysis of optimizingin-network computation for AI, exploring the evolution of programmable networkarchitectures, such as Software-Defined Networking (SDN) and Programmable DataPlanes (PDPs), and their convergence with AI. It examines methodologies formapping AI models onto resource-constrained network devices, addressingchallenges like limited memory and computational capabilities through efficientalgorithm design and model compression techniques. The paper also highlightsadvancements in distributed learning, particularly in-network aggregation, andthe potential of federated learning to enhance privacy and scalability.Frameworks like Planter and Quark are discussed for simplifying development,alongside key applications such as intelligent network monitoring, intrusiondetection, traffic management, and Edge AI. Future research directions,including runtime programmability, standardized benchmarks, and newapplications paradigms, are proposed to advance this rapidly evolving field.This survey underscores the potential of in-network AI to create intelligent,efficient, and responsive networks capable of meeting the demands ofnext-generation AI applications.</description><author>Aleksandr Algazinov, Joydeep Chandra, Matt Laing</author><pubDate>Mon, 18 Aug 2025 16:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24269v2</guid></item><item><title>BÃ¼yÃ¼k Dil Modelleri iÃ§in TR-MMLU BenchmarkÄ±: Performans DeÄerlendirmesi, Zorluklar ve Ä°yileÅtirme FÄ±rsatlarÄ±</title><link>http://arxiv.org/abs/2508.13044v1</link><description>Language models have made significant advancements in understanding andgenerating human language, achieving remarkable success in variousapplications. However, evaluating these models remains a challenge,particularly for resource-limited languages like Turkish. To address thisissue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensiveevaluation framework designed to assess the linguistic and conceptualcapabilities of large language models (LLMs) in Turkish. TR-MMLU is based on ameticulously curated dataset comprising 6,200 multiple-choice questions across62 sections within the Turkish education system. This benchmark provides astandard framework for Turkish NLP research, enabling detailed analyses ofLLMs' capabilities in processing Turkish text. In this study, we evaluatedstate-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in modeldesign. TR-MMLU sets a new standard for advancing Turkish NLP research andinspiring future innovations.</description><author>M. Ali Bayram, Ali Arda Fincan, Ahmet Semih GÃ¼mÃ¼Å, Banu Diri, SavaÅ YÄ±ldÄ±rÄ±m, Ãner AytaÅ</author><pubDate>Mon, 18 Aug 2025 16:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13044v1</guid></item><item><title>IntelliCap: Intelligent Guidance for Consistent View Sampling</title><link>http://arxiv.org/abs/2508.13043v1</link><description>Novel view synthesis from images, for example, with 3D Gaussian splatting,has made great progress. Rendering fidelity and speed are now ready even fordemanding virtual reality applications. However, the problem of assistinghumans in collecting the input images for these rendering algorithms hasreceived much less attention. High-quality view synthesis requires uniform anddense view sampling. Unfortunately, these requirements are not easily addressedby human camera operators, who are in a hurry, impatient, or lack understandingof the scene structure and the photographic process. Existing approaches toguide humans during image acquisition concentrate on single objects or neglectview-dependent material characteristics. We propose a novel situatedvisualization technique for scanning at multiple scales. During the scanning ofa scene, our method identifies important objects that need extended imagecoverage to properly represent view-dependent appearance. To this end, weleverage semantic segmentation and category identification, ranked by avision-language model. Spherical proxies are generated around highly rankedobjects to guide the user during scanning. Our results show superiorperformance in real scenes compared to conventional view sampling strategies.</description><author>Ayaka Yasunaga, Hideo Saito, Dieter Schmalstieg, Shohei Mori</author><pubDate>Mon, 18 Aug 2025 16:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13043v1</guid></item><item><title>Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data</title><link>http://arxiv.org/abs/2508.13040v1</link><description>Ensuring fairness in AI systems is critical, especially in high-stakesdomains such as lending, hiring, and healthcare. This urgency is reflected inemerging global regulations that mandate fairness assessments and independentbias audits. However, procuring the necessary complete data for fairnesstesting remains a significant challenge. In industry settings, legal andprivacy concerns restrict the collection of demographic data required to assessgroup disparities, and auditors face practical and cultural challenges ingaining access to data. In practice, data relevant for fairness testing isoften split across separate sources: internal datasets held by institutionswith predictive attributes, and external public datasets such as census datacontaining protected attributes, each providing only partial, marginalinformation. Our work seeks to leverage such available separate data toestimate model fairness when complete data is inaccessible. We proposeutilising the available separate data to estimate a set of feasible jointdistributions and then compute the set plausible fairness metrics. Throughsimulation and real experiments, we demonstrate that we can derive meaningfulbounds on fairness metrics and obtain reliable estimates of the true metric.Our results demonstrate that this approach can serve as a practical andeffective solution for fairness testing in real-world settings where access tocomplete data is restricted.</description><author>Varsha Ramineni, Hossein A. Rahmani, Emine Yilmaz, David Barber</author><pubDate>Mon, 18 Aug 2025 15:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13040v1</guid></item><item><title>Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction</title><link>http://arxiv.org/abs/2508.13037v1</link><description>Recent studies have demonstrated that Large Language Models (LLMs) havestrong mathematical reasoning abilities but rely on hundreds of billions ofparameters. To tackle the challenge of poor reasoning in Small Language Models(SLMs), existing methods typically leverage LLMs to generate massive amounts ofdata for cramming training. In psychology, they are akin to System 1 thinking,which resolves reasoning problems rapidly based on experience and intuition.However, human learning also requires System 2 thinking, where knowledge isfirst acquired and then reinforced through practice. Inspired by such twodistinct modes of thinking, we propose a novel method based on the multi-LoRAInteraction for mathematical reasoning Distillation (LoRID). First, we inputthe question and reasoning of each sample into an LLM to createknowledge-enhanced datasets. Subsequently, we train a LoRA block on the studentmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughtsfor problem-solving. Then, to imitate System 2 thinking, we train the KnowledgeGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs onlyknowledge after receiving problems, while the latter uses that knowledge toperform reasoning. Finally, to address the randomness in the generation of IRand DR, we evaluate whether their outputs are consistent, and the inferenceprocess needs to be iterated if not. This step can enhance the mathematicalreasoning ability of SLMs through mutual feedback. Experimental results showthat LoRID achieves state-of-the-art performance, especially on the GSM8Kdataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,12.3%, and 1.8% accuracy across the five base models, respectively.</description><author>Xinhe Li, Jiajun Liu, Peng Wang</author><pubDate>Mon, 18 Aug 2025 15:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13037v1</guid></item><item><title>NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</title><link>http://arxiv.org/abs/2508.10711v2</link><description>Prevailing autoregressive (AR) models for text-to-image generation eitherrely on heavy, computationally-intensive diffusion models to process continuousimage tokens, or employ vector quantization (VQ) to obtain discrete tokens withquantization loss. In this paper, we push the autoregressive paradigm forwardwith NextStep-1, a 14B autoregressive model paired with a 157M flow matchinghead, training on discrete text tokens and continuous image tokens withnext-token prediction objectives. NextStep-1 achieves state-of-the-artperformance for autoregressive models in text-to-image generation tasks,exhibiting strong capabilities in high-fidelity image synthesis. Furthermore,our method shows strong performance in image editing, highlighting the powerand versatility of our unified approach. To facilitate open research, we willrelease our code and models to the community.</description><author>NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, Hongyu Zhou, Kenkun Liu, Ailin Huang, Bin Wang, Changxin Miao, Deshan Sun, En Yu, Fukun Yin, Gang Yu, Hao Nie, Haoran Lv, Hanpeng Hu, Jia Wang, Jian Zhou, Jianjian Sun, Kaijun Tan, Kang An, Kangheng Lin, Liang Zhao, Mei Chen, Peng Xing, Rui Wang, Shiyu Liu, Shutao Xia, Tianhao You, Wei Ji, Xianfang Zeng, Xin Han, Xuelin Zhang, Yana Wei, Yanming Xu, Yimin Jiang, Yingming Wang, Yu Zhou, Yucheng Han, Ziyang Meng, Binxing Jiao, Daxin Jiang, Xiangyu Zhang, Yibo Zhu</author><pubDate>Mon, 18 Aug 2025 15:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10711v2</guid></item><item><title>Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process</title><link>http://arxiv.org/abs/2504.12488v2</link><description>As generative AI tools like ChatGPT become integral to everyday writing,critical questions arise about how to preserve writers' sense of agency andownership when using these tools. Yet, a systematic understanding of how AIassistance affects different aspects of the writing process - and how thisshapes writers' agency - remains underexplored. To address this gap, weconducted a systematic review of 109 HCI papers using the PRISMA approach. Fromthis literature, we identify four overarching design strategies for AI writingsupport: structured guidance, guided exploration, active co-writing, andcritical feedback - mapped across the four key cognitive processes in writing:planning, translating, reviewing, and monitoring. We complement this analysiswith interviews of 15 writers across diverse domains. Our findings reveal thatwriters' desired levels of AI intervention vary across the writing process:content-focused writers (e.g., academics) prioritize ownership during planning,while form-focused writers (e.g., creatives) value control over translating andreviewing. Writers' preferences are also shaped by contextual goals, values,and notions of originality and authorship. By examining when ownership matters,what writers want to own, and how AI interactions shape agency, we surface bothalignment and gaps between research and user needs. Our findings offeractionable design guidance for developing human-centered writing tools forco-writing with AI, on human terms.</description><author>Mohi Reza, Jeb Thomas-Mitchell, Peter Dushniku, Nathan Laundry, Joseph Jay Williams, Anastasia Kuzminykh</author><pubDate>Mon, 18 Aug 2025 15:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.12488v2</guid></item><item><title>Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC</title><link>http://arxiv.org/abs/2508.06309v2</link><description>In recent years, concerns about intellectual property (IP) in large languagemodels (LLMs) have grown significantly. Plagiarizing other LLMs (through directweight copying, upcycling, pruning, or continual pretraining) and claimingauthorship without properly attributing to the original license, is a seriousmisconduct that can lead to significant financial and reputational harm to theoriginal developers. However, existing methods for detecting LLM plagiarismfall short in key areas. They fail to accurately reconstruct weightcorrespondences, lack the ability to compute statistical significance measuressuch as $p$-values, and may mistakenly flag models trained on similar data asbeing related. To address these limitations, we propose Matrix-Driven InstantReview (MDIR), a novel method that leverages matrix analysis and LargeDeviation Theory. MDIR achieves accurate reconstruction of weightrelationships, provides rigorous $p$-value estimation, and focuses exclusivelyon weight similarity without requiring full model inference. Experimentalresults demonstrate that MDIR reliably detects plagiarism even after extensivetransformations, such as random permutations and continual pretraining withtrillions of tokens. Moreover, all detections can be performed on a single PCwithin an hour, making MDIR both efficient and accessible.</description><author>Ruichong Zhang</author><pubDate>Mon, 18 Aug 2025 15:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06309v2</guid></item><item><title>The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks</title><link>http://arxiv.org/abs/2508.13030v1</link><description>Cyberattacks are increasing, and securing against such threats is costingindustries billions of dollars annually. Threat Modeling, that is,comprehending the consequences of these attacks, can provide critical supportto cybersecurity professionals, enabling them to take timely action andallocate resources that could be used elsewhere. Cybersecurity is heavilydependent on threat modeling, as it assists security experts in assessing andmitigating risks related to identifying vulnerabilities and threats. Recently,there has been a pressing need for automated methods to assess attackdescriptions and forecast the future consequences of the increasing complexityof cyberattacks. This study examines how Natural Language Processing (NLP) anddeep learning can be applied to analyze the potential impact of cyberattacks byleveraging textual descriptions from the MITRE Common Weakness Enumeration(CWE) database. We emphasize classifying attack consequences into fiveprincipal categories: Availability, Access Control, Confidentiality, Integrity,and Other. This paper investigates the use of Bidirectional EncoderRepresentations from Transformers (BERT) in combination with HierarchicalAttention Networks (HANs) for Multi-label classification, evaluating theirperformance in comparison with conventional CNN and LSTM-based models.Experimental findings show that BERT achieves an overall accuracy of $0.972$,far higher than conventional deep learning models in multi-labelclassification. HAN outperforms baseline forms of CNN and LSTM-based models onspecific cybersecurity labels. However, BERT consistently achieves betterprecision and recall, making it more suitable for predicting the consequencesof a cyberattack.</description><author>Bipin Chhetri, Akbar Siami Namin</author><pubDate>Mon, 18 Aug 2025 15:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13030v1</guid></item><item><title>Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis</title><link>http://arxiv.org/abs/2508.13028v1</link><description>Sarcastic speech synthesis, which involves generating speech that effectivelyconveys sarcasm, is essential for enhancing natural interactions inapplications such as entertainment and human-computer interaction. However,synthesizing sarcastic speech remains a challenge due to the nuanced prosodythat characterizes sarcasm, as well as the limited availability of annotatedsarcastic speech data. To address these challenges, this study introduces anovel approach that integrates feedback loss from a bi-modal sarcasm detectionmodel into the TTS training process, enhancing the model's ability to captureand convey sarcasm. In addition, by leveraging transfer learning, a speechsynthesis model pre-trained on read speech undergoes a two-stage fine-tuningprocess. First, it is fine-tuned on a diverse dataset encompassing variousspeech styles, including sarcastic speech. In the second stage, the model isfurther refined using a dataset focused specifically on sarcastic speech,enhancing its ability to generate sarcasm-aware speech. Objective andsubjective evaluations demonstrate that our proposed methods improve thequality, naturalness, and sarcasm-awareness of synthesized speech.</description><author>Zhu Li, Yuqing Zhang, Xiyuan Gao, Devraj Raghuvanshi, Nagendra Kumar, Shekhar Nayak, Matt Coler</author><pubDate>Mon, 18 Aug 2025 15:44:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13028v1</guid></item><item><title>HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters</title><link>http://arxiv.org/abs/2508.13026v1</link><description>Deep learning-based cardiac MRI reconstruction faces significant domain shiftchallenges when deployed across multiple clinical centers with heterogeneousscanner configurations and imaging protocols. We propose HierAdaptMR, ahierarchical feature adaptation framework that addresses multi-level domainvariations through parameter-efficient adapters. Our method employsProtocol-Level Adapters for sequence-specific characteristics and Center-LevelAdapters for scanner-dependent variations, built upon a variational unrollingbackbone. A Universal Adapter enables generalization to entirely unseen centersthrough stochastic training that learns center-invariant adaptations. Theframework utilizes multi-scale SSIM loss with frequency domain enhancement andcontrast-adaptive weighting for robust optimization. Comprehensive evaluationon the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9modalities demonstrates superior cross-center generalization while maintainingreconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR</description><author>Ruru Xu, Ilkay Oksuz</author><pubDate>Mon, 18 Aug 2025 15:43:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13026v1</guid></item><item><title>Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling</title><link>http://arxiv.org/abs/2508.10995v2</link><description>Masked diffusion language models (MDMs) have recently gained traction as aviable generative framework for natural language. This can be attributed to itsscalability and ease of training compared to other diffusion model paradigmsfor discrete data, establishing itself as the state-of-the-artnon-autoregressive generator for discrete data. Diffusion models, in general,have shown excellent ability to improve the generation quality by leveraginginference-time scaling either by increasing the number of denoising steps or byusing external verifiers on top of the outputs of each step to guide thegeneration. In this work, we propose a verifier-based inference-time scalingmethod that aids in finding a better candidate generation during the denoisingprocess of the MDM. Our experiments demonstrate the application of MDMs forstandard text-style transfer tasks and establish MDMs as a better alternativeto autoregressive language models. Additionally, we show that a simplesoft-value-based verifier setup for MDMs using off-the-shelf pre-trainedembedding models leads to significant gains in generation quality even whenused on top of typical classifier-free guidance setups in the existingliterature.</description><author>Tejomay Kishor Padole, Suyash P Awate, Pushpak Bhattacharyya</author><pubDate>Mon, 18 Aug 2025 15:41:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10995v2</guid></item><item><title>WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents</title><link>http://arxiv.org/abs/2508.13024v1</link><description>LLM-based web agents have the potential to automate long-running web tasks,such as finding offers for specific products in multiple online shops andsubsequently ordering the cheapest products that meet the users needs. Thispaper introduces WebMall, a multi-shop online shopping benchmark for evaluatingthe effectiveness and efficiency of web agents for comparison-shopping. WebMallconsists of four simulated online shops populated with authentic product offerssourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. Thesetasks include basic tasks such as finding specific products in multiple shops,performing price comparisons, adding items to the shopping cart, and completingcheckout. Advanced tasks involve searching for products based on vaguerequirements, identifying suitable substitutes, and finding compatibleproducts. Compared to existing e-commerce benchmarks, such as WebShop orShoppingBench, WebMall introduces comparison-shopping tasks across multipleshops. Furthermore, the product offers are more heterogeneous, as theyoriginate from hundreds of distinct real-world shops. The tasks in WebMallrequire longer interaction trajectories than those in WebShop, while remainingrepresentative of real-world shopping behaviors. We evaluate eight baselineagents on WebMall, varying in observation modality, memory utilization, andunderlying large language model (GPT 4.1 and Claude Sonnet 4). Thebest-performing configurations achieve completion rates of 75% and 53%, and F1scores of 87% and 63%, on the basic and advanced task sets, respectively.WebMall is publicly released to facilitate research on web agents and topromote advancements in navigation, reasoning, and efficiency within e-commercescenarios.</description><author>Ralph Peeters, Aaron Steiner, Luca Schwarz, Julian Yuya Caspary, Christian Bizer</author><pubDate>Mon, 18 Aug 2025 15:41:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13024v1</guid></item><item><title>G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</title><link>http://arxiv.org/abs/2508.13023v1</link><description>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhancedthe reasoning abilities of large language models (LLMs). Its success, however,largely depends on strong base models with rich world knowledge, yielding onlymodest improvements for small-size language models (SLMs). To address thislimitation, we investigate Guided GRPO, which injects ground-truth reasoningsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.Through a comprehensive study of various guidance configurations, we find thatnaively adding guidance delivers limited gains. These insights motivateG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strengthin response to the model's evolving training dynamics. Experiments onmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-Asubstantially outperforms vanilla GRPO. Our code and models are available athttps://github.com/T-Lab-CUHKSZ/G2RPO-A.</description><author>Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang</author><pubDate>Mon, 18 Aug 2025 15:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13023v1</guid></item><item><title>PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models</title><link>http://arxiv.org/abs/2508.13021v1</link><description>Recent advances in masked diffusion models (MDMs) have established them aspowerful non-autoregressive alternatives for sequence generation. Nevertheless,our preliminary experiments reveal that the generation quality of MDMs is stillhighly sensitive to the choice of decoding strategy. In particular, widelyadopted uncertainty-based samplers suffer from two key limitations: a lack ofglobal trajectory control and a pronounced bias toward trivial tokens in theearly stages of decoding. These shortcomings restrict the full potential ofMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling(PC-Sampler), a novel decoding strategy that unifies global trajectory planningwith content-aware informativeness maximization. PC-Sampler incorporates aposition-aware weighting mechanism to regulate the decoding path and acalibrated confidence score to suppress the premature selection of trivialtokens. Extensive experiments on three advanced MDMs across seven challengingbenchmarks-including logical reasoning and planning tasks-demonstrate thatPC-Sampler consistently outperforms existing MDM decoding strategies by morethan 10% on average, significantly narrowing the performance gap withstate-of-the-art autoregressive models. All codes are available athttps://github.com/NEUIR/PC-Sampler.</description><author>Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Tong Xiao</author><pubDate>Mon, 18 Aug 2025 15:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13021v1</guid></item><item><title>e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving</title><link>http://arxiv.org/abs/2508.13020v1</link><description>E-graphs have attracted growing interest in many fields, particularly inlogic synthesis and formal verification. E-graph extraction is a challengingNP-hard combinatorial optimization problem. It requires identifying optimalterms from exponentially many equivalent expressions, serving as the primaryperformance bottleneck in e-graph based optimization tasks. However,traditional extraction methods face a critical trade-off: heuristic approachesoffer speed but sacrifice optimality, while exact methods provide optimalsolutions but face prohibitive computational costs on practical problems. Wepresent e-boost, a novel framework that bridges this gap through three keyinnovations: (1) parallelized heuristic extraction that leverages weak datadependence to compute DAG costs concurrently, enabling efficient multi-threadedperformance without sacrificing extraction quality; (2) adaptive search spacepruning that employs a parameterized threshold mechanism to retain onlypromising candidates, dramatically reducing the solution space while preservingnear-optimal solutions; and (3) initialized exact solving that formulates thereduced problem as an Integer Linear Program with warm-start capabilities,guiding solvers toward high-quality solutions faster. Across the diverse benchmarks in formal verification and logic synthesisfields, e-boost demonstrates 558x runtime speedup over traditional exactapproaches (ILP) and 19.04% performance improvement over the state-of-the-artextraction framework (SmoothE). In realistic logic synthesis tasks, e-boostproduces 7.6% and 8.1% area improvements compared to conventional synthesistools with two different technology mapping libraries. e-boost is available athttps://github.com/Yu-Maryland/e-boost.</description><author>Jiaqi Yin, Zhan Song, Chen Chen, Yaohui Cai, Zhiru Zhang, Cunxi Yu</author><pubDate>Mon, 18 Aug 2025 15:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13020v1</guid></item><item><title>Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control</title><link>http://arxiv.org/abs/2508.13018v1</link><description>In this work, we propose a robust adaptive filtering approach for activenoise control applications in the presence of impulsive noise. In particular,we develop the filtered-x hyperbolic tangent exponential generalized KernelM-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysisof the proposed FXHEKM algorithm is carried out along with a study of itscomputational cost. {In order to evaluate the proposed FXHEKM algorithm, themean-square error (MSE) and the average noise reduction (ANR) performancemetrics have been adopted.} Numerical results show the efficiency of theproposed FXHEKM algorithm to cancel the presence of the additive spurioussignals, such as \textbf{$\alpha$}-stable noises against competing algorithms.</description><author>Iam Kim de S. Hermont, Andre R. Flores, Rodrigo C. de Lamare</author><pubDate>Mon, 18 Aug 2025 15:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13018v1</guid></item><item><title>Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming</title><link>http://arxiv.org/abs/2409.11041v3</link><description>While there has been a lot of research recently on robots in householdenvironments, at the present time, most robots in existence can be found onshop floors, and most interactions between humans and robots happen there.``Collaborative robots'' (cobots) designed to work alongside humans on assemblylines traditionally require expert programming, limiting ability to makechanges, or manual guidance, limiting expressivity of the resulting programs.To address these limitations, we explore using Large Language Models (LLMs),and in particular, their abilities of doing in-context learning, forconversational code generation. As a first step, we define RATS, the``Repetitive Assembly Task'', a 2D building task designed to lay the foundationfor simulating industry assembly scenarios. In this task, a `programmer'instructs a cobot, using natural language, on how a certain assembly is to bebuilt; that is, the programmer induces a program, through natural language. Wecreate a dataset that pairs target structures with various example instructions(human-authored, template-based, and model-generated) and example code. Withthis, we systematically evaluate the capabilities of state-of-the-art LLMs forsynthesising this kind of code, given in-context examples. Evaluating in asimulated environment, we find that LLMs are capable of generating accurate`first order code' (instruction sequences), but have problems producing`higher-order code' (abstractions such as functions, or use of loops).</description><author>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</author><pubDate>Mon, 18 Aug 2025 15:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11041v3</guid></item><item><title>EgoTwin: Dreaming Body and View in First Person</title><link>http://arxiv.org/abs/2508.13013v1</link><description>While exocentric video synthesis has achieved great progress, egocentricvideo generation remains largely underexplored, which requires modelingfirst-person view content along with camera motion patterns induced by thewearer's body movements. To bridge this gap, we introduce a novel task of jointegocentric video and human motion generation, characterized by two keychallenges: 1) Viewpoint Alignment: the camera trajectory in the generatedvideo must accurately align with the head trajectory derived from human motion;2) Causal Interplay: the synthesized human motion must causally align with theobserved visual dynamics across adjacent video frames. To address thesechallenges, we propose EgoTwin, a joint video-motion generation framework builton the diffusion transformer architecture. Specifically, EgoTwin introduces ahead-centric motion representation that anchors the human motion to the headjoint and incorporates a cybernetics-inspired interaction mechanism thatexplicitly captures the causal interplay between video and motion withinattention operations. For comprehensive evaluation, we curate a large-scalereal-world dataset of synchronized text-video-motion triplets and design novelmetrics to assess video-motion consistency. Extensive experiments demonstratethe effectiveness of the EgoTwin framework.</description><author>Jingqiao Xiu, Fangzhou Hong, Yicong Li, Mengze Li, Wentao Wang, Sirui Han, Liang Pan, Ziwei Liu</author><pubDate>Mon, 18 Aug 2025 15:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13013v1</guid></item><item><title>Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model</title><link>http://arxiv.org/abs/2508.13009v1</link><description>Recent advances in interactive video generations have demonstrated diffusionmodel's potential as world models by capturing complex physical dynamics andinteractive behaviors. However, existing interactive world models depend onbidirectional attention and lengthy inference steps, severely limitingreal-time performance. Consequently, they are hard to simulate real-worlddynamics, where outcomes must update instantaneously based on historicalcontext and current actions. To address this, we present Matrix-Game 2.0, aninteractive world model generates long videos on-the-fly via few-stepauto-regressive diffusion. Our framework consists of three key components: (1)A scalable data production pipeline for Unreal Engine and GTA5 environments toeffectively produce massive amounts (about 1200 hours) of video data withdiverse interaction annotations; (2) An action injection module that enablesframe-level mouse and keyboard inputs as interactive conditions; (3) A few-stepdistillation based on the casual architecture for real-time and streaming videogeneration. Matrix Game 2.0 can generate high-quality minute-level videosacross diverse scenes at an ultra-fast speed of 25 FPS. We open-source ourmodel weights and codebase to advance research in interactive world modeling.</description><author>Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, Baixin Xu, Hao-Xiang Guo, Kaixiong Gong, Cyrus Wu, Wei Li, Xuchen Song, Yang Liu, Eric Li, Yahui Zhou</author><pubDate>Mon, 18 Aug 2025 15:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13009v1</guid></item><item><title>A Comprehensive Review of Datasets for Clinical Mental Health AI Systems</title><link>http://arxiv.org/abs/2508.09809v2</link><description>Mental health disorders are rising worldwide. However, the availability oftrained clinicians has not scaled proportionally, leaving many people withoutadequate or timely support. To bridge this gap, recent studies have shown thepromise of Artificial Intelligence (AI) to assist mental health diagnosis,monitoring, and intervention. However, the development of efficient, reliable,and ethical AI to assist clinicians is heavily dependent on high-qualityclinical training datasets. Despite growing interest in data curation fortraining clinical AI assistants, existing datasets largely remain scattered,under-documented, and often inaccessible, hindering the reproducibility,comparability, and generalizability of AI models developed for clinical mentalhealth care. In this paper, we present the first comprehensive survey ofclinical mental health datasets relevant to the training and development ofAI-powered clinical assistants. We categorize these datasets by mentaldisorders (e.g., depression, schizophrenia), data modalities (e.g., text,speech, physiological signals), task types (e.g., diagnosis prediction, symptomseverity estimation, intervention generation), accessibility (public,restricted or private), and sociocultural context (e.g., language and culturalbackground). Along with these, we also investigate synthetic clinical mentalhealth datasets. Our survey identifies critical gaps such as a lack oflongitudinal data, limited cultural and linguistic representation, inconsistentcollection and annotation standards, and a lack of modalities in syntheticdata. We conclude by outlining key challenges in curating and standardizingfuture datasets and provide actionable recommendations to facilitate thedevelopment of more robust, generalizable, and equitable mental health AIsystems.</description><author>Aishik Mandal, Prottay Kumar Adhikary, Hiba Arnaout, Iryna Gurevych, Tanmoy Chakraborty</author><pubDate>Mon, 18 Aug 2025 15:27:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09809v2</guid></item><item><title>SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception</title><link>http://arxiv.org/abs/2508.13007v1</link><description>Collaborative perception allows connected autonomous vehicles (CAVs) toovercome occlusion and limited sensor range by sharing intermediate features.Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm thebandwidth available for inter-vehicle communication. We present SlimComm, acommunication-efficient framework that integrates 4D radar Doppler with aquery-driven sparse scheme. SlimComm builds a motion-centric dynamic map todistinguish moving from static objects and generates two query types: (i)reference queries on dynamic and high-confidence regions, and (ii) exploratoryqueries probing occluded areas via a two-stage offset. Only query-specific BEVfeatures are exchanged and fused through multi-scale gated deformableattention, reducing payload while preserving accuracy. For evaluation, werelease OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Dopplerradar. SlimComm achieves up to 90% lower bandwidth than full-map sharing whilematching or surpassing prior baselines across varied traffic densities andocclusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.</description><author>Melih Yazgan, Qiyuan Wu, Iramm Hamdard, Shiqi Li, J. Marius Zoellner</author><pubDate>Mon, 18 Aug 2025 15:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13007v1</guid></item><item><title>Monte Carlo Functional Regularisation for Continual Learning</title><link>http://arxiv.org/abs/2508.13006v1</link><description>Continual learning (CL) is crucial for the adaptation of neural networkmodels to new environments. Although outperforming weight-space regularisationapproaches, the functional regularisation-based CL methods suffer from highcomputational costs and large linear approximation errors. In this work, wepresent a new functional regularisation CL framework, called MCFRCL, whichapproximates model prediction distributions by Monte Carlo (MC) sampling.Moreover, three continuous distributions are leveraged to capture thestatistical characteristics of the MC samples via moment-based methods.Additionally, both the Wasserstein distance and the Kullback-Leibler (KL)distance are employed to construct the regularisation function. The proposedMCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFARdatasets, with simulation results highlighting its effectiveness in bothprediction accuracy and training efficiency.</description><author>Pengcheng Hao, Menghao Waiyan William Zhu, Ercan Engin Kuruoglu</author><pubDate>Mon, 18 Aug 2025 15:25:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13006v1</guid></item><item><title>Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning</title><link>http://arxiv.org/abs/2508.13005v1</link><description>Open set recognition (OSR) and continual learning are two critical challengesin machine learning, focusing respectively on detecting novel classes atinference time and updating models to incorporate the new classes. While manyrecent approaches have addressed these problems, particularly OSR, byheuristically promoting feature diversity, few studies have directly examinedthe role that feature diversity plays in tackling them. In this work, weprovide empirical evidence that enhancing feature diversity improves therecognition of open set samples. Moreover, increased feature diversity alsofacilitates both the retention of previously learned data and the integrationof new data in continual learning. We hope our findings can inspire furtherresearch into both practical methods and theoretical understanding in thesedomains.</description><author>Jiawen Xu, Odej Kao</author><pubDate>Mon, 18 Aug 2025 15:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13005v1</guid></item><item><title>EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing</title><link>http://arxiv.org/abs/2508.13003v1</link><description>The rapid advancement of LLMs poses a significant challenge to existingmathematical reasoning benchmarks. These benchmarks commonly suffer from issuessuch as score saturation, temporal decay, and data contamination. To addressthis challenge, this paper introduces EvolMathEval, an automated mathematicalbenchmark generation and evolution framework based on evolutionary testing. Bydynamically generating unique evaluation instances ab initio, the frameworkfundamentally eliminates the risk of data contamination, and ensuring thebenchmark remains perpetually challenging for future models.The core mechanismsof EvolMathEval include: seed problem generation based on reverse engineeringwith algebraic guarantees; multi-dimensional genetic operators designed toinject diverse cognitive challenges; and a composite fitness function that canrapidly and accurately assess problem difficulty. Experimental resultsdemonstrate that the proposed composite fitness function can efficiently andprecisely quantify the difficulty of mathematical problems. Furthermore,EvolMathEval can not only generate a large volume of high-difficulty problemsthrough continuous self-iteration, but it can also significantly enhance thecomplexity of public datasets like GSM8K through evolution, reducing modelaccuracy by an average of 48%. Deeper investigation reveals that when solvingthese evolved, complex problems, LLMs tend to employ non-rigorous heuristics tobypass complex multi-step logical reasoning, consequently leading to incorrectsolutions. We define this phenomenon as "Pseudo Aha Moment". This findinguncovers a cognitive shortcut-taking behavior in the deep reasoning processesof current LLMs, which we find accounts for 77% to 100% of errors on targetedproblems. Code and resources are availableat:https://github.com/SYSUSELab/EvolMathEval.</description><author>Shengbo Wang, Mingwei Liu, Zike Li, Anji Li, Yanlin Wang, Xin Peng, Zibin Zheng</author><pubDate>Mon, 18 Aug 2025 15:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13003v1</guid></item><item><title>TopoMortar: A dataset to evaluate image segmentation methods focused on topology accuracy</title><link>http://arxiv.org/abs/2503.03365v2</link><description>We present TopoMortar, a brick wall dataset that is the first datasetspecifically designed to evaluate topology-focused image segmentation methods,such as topology loss functions. Motivated by the known sensitivity of methodsto dataset challenges, such as small training sets, noisy labels, andout-of-distribution test-set images, TopoMortar is created to enable in twoways investigating methods' effectiveness at improving topology accuracy.First, by eliminating dataset challenges that, as we show, impact theeffectiveness of topology loss functions. Second, by allowing to representdifferent dataset challenges in the same dataset, isolating methods'performance from dataset challenges. TopoMortar includes three types of labels(accurate, pseudo-labels, and noisy labels), two fixed training sets (large andsmall), and in-distribution and out-of-distribution test-set images. Wecompared eight loss functions on TopoMortar, and we found that clDice achievedthe most topologically accurate segmentations, and that the relativeadvantageousness of the other loss functions depends on the experimentalsetting. Additionally, we show that data augmentation and self-distillation canelevate Cross entropy Dice loss to surpass most topology loss functions, andthat those simple methods can enhance topology loss functions as well.TopoMortar and our code can be found at https://jmlipman.github.io/TopoMortar</description><author>Juan Miguel Valverde, Motoya Koga, Nijihiko Otsuka, Anders Bjorholm Dahl</author><pubDate>Mon, 18 Aug 2025 15:23:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.03365v2</guid></item><item><title>An MRP Formulation for Supervised Learning: Generalized Temporal Difference Learning Models</title><link>http://arxiv.org/abs/2404.15518v4</link><description>In traditional statistical learning, data points are usually assumed to beindependently and identically distributed (i.i.d.) following an unknownprobability distribution. This paper presents a contrasting viewpoint,perceiving data points as interconnected and employing a Markov reward process(MRP) for data modeling. We reformulate the typical supervised learning as anon-policy policy evaluation problem within reinforcement learning (RL),introducing a generalized temporal difference (TD) learning algorithm as aresolution. Theoretically, our analysis establishes connections between thesolutions of linear TD learning and ordinary least squares (OLS). Underspecific conditions -- particularly when the noise is correlated -- the TDsolution serves as a more effective estimator than OLS. Furthermore, we showthat when our algorithm is applied with many commonly used loss functions --such as those found in generalized linear models -- it corresponds to theapplication of a novel and generalized Bellman operator. We prove that thisoperator admits a unique fixed point, and based on this, we establishconvergence guarantees for our generalized TD algorithm under linear functionapproximation. Empirical studies verify our theoretical results, examine thevital design of our TD algorithm and show practical utility across variousdatasets, encompassing tasks such as regression and image classification withdeep learning.</description><author>Yangchen Pan, Junfeng Wen, Chenjun Xiao, Philip Torr</author><pubDate>Mon, 18 Aug 2025 15:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15518v4</guid></item><item><title>Omni Survey for Multimodality Analysis in Visual Object Tracking</title><link>http://arxiv.org/abs/2508.13000v1</link><description>The development of smart cities has led to the generation of massive amountsof multi-modal data in the context of a range of tasks that enable acomprehensive monitoring of the smart city infrastructure and services. Thispaper surveys one of the most critical tasks, multi-modal visual objecttracking (MMVOT), from the perspective of multimodality analysis. Generally,MMVOT differs from single-modal tracking in four key aspects, data collection,modality alignment and annotation, model designing, and evaluation.Accordingly, we begin with an introduction to the relevant data modalities,laying the groundwork for their integration. This naturally leads to adiscussion of challenges of multi-modal data collection, alignment, andannotation. Subsequently, existing MMVOT methods are categorised, based ondifferent ways to deal with visible (RGB) and X modalities: programming theauxiliary X branch with replicated or non-replicated experimentalconfigurations from the RGB branch. Here X can be thermal infrared (T), depth(D), event (E), near infrared (NIR), language (L), or sonar (S). The final partof the paper addresses evaluation and benchmarking. In summary, we undertake anomni survey of all aspects of multi-modal visual object tracking (VOT),covering six MMVOT tasks and featuring 338 references in total. In addition, wediscuss the fundamental rhetorical question: Is multi-modal tracking alwaysguaranteed to provide a superior solution to unimodal tracking with the help ofinformation fusion, and if not, in what circumstances its application isbeneficial. Furthermore, for the first time in this field, we analyse thedistributions of the object categories in the existing MMVOT datasets,revealing their pronounced long-tail nature and a noticeable lack of animalcategories when compared with RGB datasets.</description><author>Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Hui Li, Shaochuan Zhao, Tao Zhou, Chunyang Cheng, Xiaojun Wu, Josef Kittler</author><pubDate>Mon, 18 Aug 2025 15:18:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13000v1</guid></item><item><title>From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning</title><link>http://arxiv.org/abs/2505.14425v2</link><description>Instruction-tuned large language models (LLMs) have shown strong performanceon a variety of tasks; however, generalizing from synthetic to human-authoredinstructions in grounded environments remains a challenge for them. In thiswork, we study generalization challenges in spatial grounding tasks wheremodels interpret and translate instructions for building object arrangements ona $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluatetheir performance on a benchmark dataset containing both synthetic andhuman-written instructions. Our results reveal that while models generalizewell on simple tasks, their performance degrades significantly on more complextasks. We present a detailed error analysis of the gaps in instructiongeneralization.</description><author>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</author><pubDate>Mon, 18 Aug 2025 15:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14425v2</guid></item><item><title>Vitamin N: Benefits of Different Forms of Public Greenery for Urban Health</title><link>http://arxiv.org/abs/2508.12998v1</link><description>Urban greenery is often linked to better health, yet findings from pastresearch have been inconsistent. One reason is that official greenery metricsmeasure the amount or nearness of greenery but ignore how often people actuallymay potentially see or use it in daily life. To address this gap, we introduceda new classification that separates on-road greenery, which people see whilewalking through streets, from off-road greenery, which requires planned visits.We did so by combining aerial imagery of Greater London and greenery data fromOpenStreetMap with quantified greenery from over 100,000 Google Street Viewimages and accessibility estimates based on 160,000 road segments. We linkedthese measures to 7.45 billion medical prescriptions issued by the NationalHealth Service and processed through our methodology. These prescriptions coverfive conditions: diabetes, hypertension, asthma, depression, and anxiety, aswell as opioid use. As hypothesized, we found that green on-road was morestrongly linked to better health than four widely used official measures. Forexample, hypertension prescriptions dropped by 3.68% in wards with on-roadgreenery above the median citywide level compared to those below it. If allbelow-median wards reached the citywide median in on-road greenery,prescription costs could fall by up to {\pounds}3.15 million each year. Theseresults suggest that greenery seen in daily life may be more relevant thanpublic yet secluded greenery, and that official metrics commonly used in theliterature have important limitations.</description><author>Sanja Å ÄepanoviÄ, Sagar Joglekar, Stephen Law, Daniele Quercia, Ke Zhou, Alice Battiston, Rossano Schifanella</author><pubDate>Mon, 18 Aug 2025 15:17:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12998v1</guid></item><item><title>Fairness-Aware Multi-view Evidential Learning with Adaptive Prior</title><link>http://arxiv.org/abs/2508.12997v1</link><description>Multi-view evidential learning aims to integrate information from multipleviews to improve prediction performance and provide trustworthy uncertaintyesitimation. Most previous methods assume that view-specific evidence learningis naturally reliable. However, in practice, the evidence learning processtends to be biased. Through empirical analysis on real-world data, we revealthat samples tend to be assigned more evidence to support data-rich classes,thereby leading to unreliable uncertainty estimation in predictions. Thismotivates us to delve into a new Biased Evidential Multi-view Learning (BEML)problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning(FAML). FAML first introduces an adaptive prior based on training trajectory,which acts as a regularization strategy to flexibly calibrate the biasedevidence learning process. Furthermore, we explicitly incorporate a fairnessconstraint based on class-wise evidence variance to promote balanced evidenceallocation. In the multi-view fusion stage, we propose an opinion alignmentmechanism to mitigate view-specific bias across views, thereby encouraging theintegration of consistent and mutually supportive evidence. Extensiveexperiments on five real-world multi-view datasets demonstrate that FAMLachieves more balanced evidence allocation and improves both predictionperformance and the reliability of uncertainty estimation compared tostate-of-the-art methods.</description><author>Haishun Chen, Cai Xu, Jinlong Yu, Yilin Zhang, Ziyu Guan, Wei Zhao</author><pubDate>Mon, 18 Aug 2025 15:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12997v1</guid></item><item><title>Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair</title><link>http://arxiv.org/abs/2508.12996v1</link><description>Transformer neural networks are increasingly used for physics-based problems.In data-driven PDE surrogates, training samples from varying boundary andinitial conditions can cause erratic losses and spiky gradients; inphysics-informed neural networks (PINNs), stiff composite losses amplify thiseffect. We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixedsecond-moment discount beta2 is replaced by a layer-wise dynamic value drivenby a bounded ``sunspike'' ratio: the current pooled gradient norm divided by anexponential moving average (EMA) of past norms, squashed to the interval [0,1).Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio),adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',``exact'). With all features off and bias_correction=``none'', the method isexactly Adam. We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3DPINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task withjitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MBof enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final lossversus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smallervariance. The method remains drop-in, with runtime overhead comparable to Adamin testbeds A-C and within single-digit percent in testbed D. It preservesAdam-style convergence guarantees while improving robustness under spikygradients.</description><author>Stavros C. Kassinos</author><pubDate>Mon, 18 Aug 2025 15:16:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12996v1</guid></item><item><title>USAD: Universal Speech and Audio Representation via Distillation</title><link>http://arxiv.org/abs/2506.18843v2</link><description>Self-supervised learning (SSL) has revolutionized audio representations, yetmodels often remain domain-specific, focusing on either speech or non-speechtasks. In this work, we present Universal Speech and Audio Distillation (USAD),a unified approach to audio representation learning that integrates diverseaudio types - speech, sound, and music - into a single model. USAD employsefficient layer-to-layer distillation from domain-specific SSL models to traina student on a comprehensive audio dataset. USAD offers competitive performanceacross various benchmarks and datasets, including frame and instance-levelspeech processing tasks, audio tagging, and sound classification, achievingnear state-of-the-art results with a single encoder on SUPERB and HEARbenchmarks.</description><author>Heng-Jui Chang, Saurabhchand Bhati, James Glass, Alexander H. Liu</author><pubDate>Mon, 18 Aug 2025 15:16:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.18843v2</guid></item><item><title>CCDM: Continuous Conditional Diffusion Models for Image Generation</title><link>http://arxiv.org/abs/2405.03546v4</link><description>Continuous Conditional Generative Modeling (CCGM) estimates high-dimensionaldata distributions, such as images, conditioned on scalar continuous variables(aka regression labels). While Continuous Conditional Generative AdversarialNetworks (CcGANs) were designed for this task, their instability duringadversarial learning often leads to suboptimal results. Conditional DiffusionModels (CDMs) offer a promising alternative, generating more realistic images,but their diffusion processes, label conditioning, and model fitting proceduresare either not optimized for or incompatible with CCGM, making it difficult tointegrate CcGANs' vicinal approach. To address these issues, we introduceContinuous Conditional Diffusion Models (CCDMs), the first CDM specificallytailored for CCGM. CCDMs address existing limitations with specially designedconditional diffusion processes, a novel hard vicinal image denoising loss, acustomized label embedding method, and efficient conditional samplingprocedures. Through comprehensive experiments on four datasets with resolutionsranging from 64x64 to 192x192, we demonstrate that CCDMs outperformstate-of-the-art CCGM models, establishing a new benchmark. Ablation studiesfurther validate the model design and implementation, highlighting that somewidely used CDM implementations are ineffective for the CCGM task. Our code ispublicly available at https://github.com/UBCDingXin/CCDM.</description><author>Xin Ding, Yongwei Wang, Kao Zhang, Z. Jane Wang</author><pubDate>Mon, 18 Aug 2025 15:15:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03546v4</guid></item><item><title>Two-sample comparison through additive tree models for density ratios</title><link>http://arxiv.org/abs/2508.03059v2</link><description>The ratio of two densities characterizes their differences. We considerlearning the density ratio given i.i.d. observations from each of the twodistributions. We propose additive tree models for the density ratio along withefficient algorithms for training these models using a new loss function calledthe balancing loss. With this loss, additive tree models for the density ratiocan be trained using algorithms original designed for supervised learning.Specifically, they can be trained from both an optimization perspective thatparallels tree boosting and from a (generalized) Bayesian perspective thatparallels Bayesian additive regression trees (BART). For the former, we presenttwo boosting algorithms -- one based on forward-stagewise fitting and the otherbased on gradient boosting, both of which produce a point estimate for thedensity ratio function. For the latter, we show that due to the loss function'sresemblance to an exponential family kernel, the new loss can serve as apseudo-likelihood for which conjugate priors exist, thereby enabling effectivegeneralized Bayesian inference on the density ratio using backfitting samplersdesigned for BART. The resulting uncertainty quantification on the inferreddensity ratio is critical for applications involving high-dimensional andcomplex distributions in which uncertainty given limited data can often besubstantial. We provide insights on the balancing loss through its closeconnection to the exponential loss in binary classification and to thevariational form of f-divergence, in particular that of the squared Hellingerdistance. Our numerical experiments demonstrate the accuracy of the proposedapproach while providing unique capabilities in uncertainty quantification. Wedemonstrate the application of our method in a case study involving assessingthe quality of generative models for microbiome compositional data.</description><author>Naoki Awaya, Yuliang Xu, Li Ma</author><pubDate>Mon, 18 Aug 2025 15:14:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.03059v2</guid></item><item><title>Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian</title><link>http://arxiv.org/abs/2508.12993v1</link><description>A common observation in the Graph Convolutional Network (GCN) literature isthat stacking GCN layers may or may not result in better performance on taskslike node classification and edge prediction. We have found empirically that agraph's algebraic connectivity, which is known as the Fiedler value, is a goodpredictor of GCN performance. Intuitively, graphs with similar Fiedler valueshave analogous structural properties, suggesting that the same filters andhyperparameters may yield similar results when used with GCNs, and thattransfer learning may be more effective between graphs with similar algebraicconnectivity. We explore this theoretically and empirically with experiments onsynthetic and real graph data, including the Cora, CiteSeer and Polblogsdatasets. We explore multiple ways of aggregating the Fiedler value forconnected components in the graphs to arrive at a value for the entire graph,and show that it can be used to predict GCN performance. We also presenttheoretical arguments as to why the Fiedler value is a good predictor.</description><author>Shalima Binta Manir, Tim Oates</author><pubDate>Mon, 18 Aug 2025 15:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12993v1</guid></item><item><title>A locally statistical active contour model for SAR image segmentation can be solved by denoising algorithms</title><link>http://arxiv.org/abs/2401.10083v2</link><description>In this paper, we propose a novel locally statistical variational activecontour model based on I-divergence-TV denoising model, which hybrides geodesicactive contour (GAC) model with active contours without edges (ACWE) model, andcan be used to segment images corrupted by multiplicative gamma noise. Byadding a diffusion term into the level set evolution (LSE) equation of theproposed model, we construct a reaction-diffusion (RD) equation, which cangradually regularize the level set function (LSF) to be piecewise constant ineach segment domain and gain the stable solution. We further transform theproposed model into classic ROF model by adding a proximity term. [27] issubmitted on 29-Aug-2013, and our early edition ever submitted to TGRS on12-Jun-2012, Venkatakrishnan et al. [31] proposed their 'pnp algorithm' on29-May-2013, so Venkatakrishnan and we proposed the 'pnp algorithm' almostsimultaneously. Inspired by a fast denoising algorithm proposed by Jia-Zhaorecently, we propose two fast fixed point algorithms to solve SAR imagesegmentation question. Experimental results for real SAR images show that theproposed image segmentation model can efficiently stop the contours at weak orblurred edges, and can automatically detect the exterior and interiorboundaries of images with multiplicative gamma noise. The proposed FPRD1/FPRD2models are about 1/2 (or less than) of the time required for the SBRD modelbased on the Split Bregman technique.</description><author>Guangming Liu</author><pubDate>Mon, 18 Aug 2025 15:10:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10083v2</guid></item><item><title>Transfer Learning for Neutrino Scattering: Domain Adaptation with GANs</title><link>http://arxiv.org/abs/2508.12987v1</link><description>We utilize transfer learning to extrapolate the physics knowledge encoded ina Generative Adversarial Network (GAN) model trained on syntheticcharged-current (CC) neutrino-carbon inclusive scattering data. This base modelis adapted to generate CC inclusive scattering events (lepton kinematics only)for neutrino-argon and antineutrino-carbon interactions. Furthermore, we assessthe effectiveness of transfer learning in re-optimizing a custom model when newdata comes from a different neutrino-nucleus interaction model. Our resultsdemonstrate that transfer learning significantly outperforms traininggenerative models from scratch. To study this, we consider two training datasets: one with 10,000 and another with 100,000 events. The models obtained viatransfer learning perform well even with smaller training data. The proposedmethod provides a promising approach for constructing neutrino scattering eventgenerators in scenarios where experimental data is sparse.</description><author>Jose L. Bonilla, Krzysztof M. Graczyk, Artur M. Ankowski, Rwik Dharmapal Banerjee, Beata E. Kowal, Hemant Prasad, Jan T. Sobczyk</author><pubDate>Mon, 18 Aug 2025 15:08:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12987v1</guid></item><item><title>Point upsampling networks for single-photon sensing</title><link>http://arxiv.org/abs/2508.12986v1</link><description>Single-photon sensing has generated great interest as a prominent techniqueof long-distance and ultra-sensitive imaging, however, it tends to yield sparseand spatially biased point clouds, thus limiting its practical utility. In thiswork, we propose using point upsampling networks to increase point density andreduce spatial distortion in single-photon point cloud. Particularly, ournetwork is built on the state space model which integrates a multi-pathscanning mechanism to enrich spatial context, a bidirectional Mamba backbone tocapture global geometry and local details, and an adaptive upsample shiftmodule to correct offset-induced distortions. Extensive experiments areimplemented on commonly-used datasets to confirm its high reconstructionaccuracy and strong robustness to the distortion noise, and also on real-worlddata to demonstrate that our model is able to generate visually consistent,detail-preserving, and noise suppressed point clouds. Our work is the first toestablish the upsampling framework for single-photon sensing, and hence opens anew avenue for single-photon sensing and its practical applications in thedownstreaming tasks.</description><author>Jinyi Liu, Guoyang Zhao, Lijun Liu, Yiguang Hong, Weiping Zhang, Shuming Cheng</author><pubDate>Mon, 18 Aug 2025 15:05:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12986v1</guid></item><item><title>SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression</title><link>http://arxiv.org/abs/2508.12984v1</link><description>The increasing complexity of neural networks poses a significant barrier tothe deployment of distributed machine learning (ML) on resource-constraineddevices, such as federated learning (FL). Split learning (SL) offers apromising solution by offloading the primary computing load from edge devicesto a server via model partitioning. However, as the number of participatingdevices increases, the transmission of excessive smashed data (i.e.,activations and gradients) becomes a major bottleneck for SL, slowing down themodel training. To tackle this challenge, we propose a communication-efficientSL framework, named SL-ACC, which comprises two key components: adaptivechannel importance identification (ACII) and channel grouping compression(CGC). ACII first identifies the contribution of each channel in the smasheddata to model training using Shannon entropy. Following this, CGC groups thechannels based on their entropy and performs group-wise adaptive compression toshrink the transmission volume without compromising training accuracy.Extensive experiments across various datasets validate that our proposed SL-ACCframework takes considerably less time to achieve a target accuracy thanstate-of-the-art benchmarks.</description><author>Zehang Lin, Zheng Lin, Miao Yang, Jianhao Huang, Yuxin Zhang, Zihan Fang, Xia Du, Zhe Chen, Shunzhi Zhu, Wei Ni</author><pubDate>Mon, 18 Aug 2025 15:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12984v1</guid></item><item><title>Analyzing Information Sharing and Coordination in Multi-Agent Planning</title><link>http://arxiv.org/abs/2508.12981v1</link><description>Multi-agent systems (MASs) have pushed the boundaries of large language model(LLM) agents in domains such as web research and software engineering. However,long-horizon, multi-constraint planning tasks involve conditioning on detailedinformation and satisfying complex interdependent constraints, which can pose achallenge for these systems. In this study, we construct an LLM-based MAS for atravel planning task which is representative of these challenges. We evaluatethe impact of a notebook to facilitate information sharing, and evaluate anorchestrator agent to improve coordination in free form conversation betweenagents. We find that the notebook reduces errors due to hallucinated details by18%, while an orchestrator directs the MAS to focus on and further reduceerrors by up to 13.5% within focused sub-areas. Combining both mechanismsachieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absoluteimprovement over the single-agent baseline's 7.5% pass rate. These resultshighlight the potential of structured information sharing and reflectiveorchestration as key components in MASs for long horizon planning with LLMs.</description><author>Tianyue Ou, Saujas Vaduguru, Daniel Fried</author><pubDate>Mon, 18 Aug 2025 14:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12981v1</guid></item></channel></rss>