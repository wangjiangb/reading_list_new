<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 10 Sep 2025 13:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CAViAR: Critic-Augmented Video Agentic Reasoning</title><link>http://arxiv.org/abs/2509.07680v1</link><description>Video understanding has seen significant progress in recent years, withmodels' performance on perception from short clips continuing to rise. Yet,multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, showperformance wanes for tasks requiring complex reasoning on videos as queriesgrow more complex and videos grow longer. In this work, we ask: can existingperception capabilities be leveraged to successfully perform more complex videoreasoning? In particular, we develop a large language model agent given accessto video modules as subagents or tools. Rather than following a fixed procedureto solve queries as in previous work such as Visual Programming, ViperGPT, andMoReVQA, the agent uses the results of each call to a module to determinesubsequent steps. Inspired by work in the textual reasoning domain, weintroduce a critic to distinguish between instances of successful andunsuccessful sequences from the agent. We show that the combination of ouragent and critic achieve strong performance on the previously-mentioneddatasets.</description><author>Sachit Menon, Ahmet Iscen, Arsha Nagrani, Tobias Weyand, Carl Vondrick, Cordelia Schmid</author><pubDate>Tue, 09 Sep 2025 17:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07680v1</guid></item><item><title>Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</title><link>http://arxiv.org/abs/2509.07980v1</link><description>Parallel thinking has emerged as a novel approach for enhancing the reasoningcapabilities of large language models (LLMs) by exploring multiple reasoningpaths concurrently. However, activating such capabilities through trainingremains challenging, as existing methods predominantly rely on supervisedfine-tuning (SFT) over synthetic data, which encourages teacher-forcedimitation rather than exploration and generalization. Different from them, wepropose \textbf{Parallel-R1}, the first reinforcement learning (RL) frameworkthat enables parallel thinking behaviors for complex real-world reasoningtasks. Our framework employs a progressive curriculum that explicitly addressesthe cold-start problem in training parallel thinking with RL. We first use SFTon prompt-generated trajectories from easier tasks to instill the parallelthinking ability, then transition to RL to explore and generalize this skill onharder problems. Experiments on various math benchmarks, including MATH, AMC23,and AIME, show that Parallel-R1 successfully instills parallel thinking,leading to 8.4% accuracy improvements over the sequential thinking modeltrained directly on challenging tasks with RL. Further analysis reveals a clearshift in the model's thinking behavior: at an early stage, it uses parallelthinking as an exploration strategy, while in a later stage, it uses the samecapability for multi-perspective verification. Most significantly, we validateparallel thinking as a \textbf{mid-training exploration scaffold}, where thistemporary exploratory phase unlocks a higher performance ceiling after RL,yielding a 42.9% improvement over the baseline on AIME25. Our model, data, andcode will be open-source at https://github.com/zhengkid/Parallel-R1.</description><author>Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu</author><pubDate>Tue, 09 Sep 2025 17:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07980v1</guid></item><item><title>Visual Representation Alignment for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2509.07979v1</link><description>Multimodal large language models (MLLMs) trained with visual instructiontuning have achieved strong performance across diverse tasks, yet they remainlimited in vision-centric tasks such as object counting or spatial reasoning.We attribute this gap to the prevailing text-only supervision paradigm, whichprovides only indirect guidance for the visual pathway and often leads MLLMs todiscard fine-grained visual details during training. In this paper, we presentVIsual Representation ALignment (VIRAL), a simple yet effective regularizationstrategy that aligns the internal visual representations of MLLMs with those ofpre-trained vision foundation models (VFMs). By explicitly enforcing thisalignment, VIRAL enables the model not only to retain critical visual detailsfrom the input vision encoder but also to complement additional visualknowledge from VFMs, thereby enhancing its ability to reason over complexvisual inputs. Our experiments demonstrate consistent improvements across alltasks on widely adopted multimodal benchmarks. Furthermore, we conductcomprehensive ablation studies to validate the key design choices underlyingour framework. We believe this simple finding opens up an important directionfor the effective integration of visual information in training MLLMs.</description><author>Heeji Yoon, Jaewoo Jung, Junwan Kim, Hyungyu Choi, Heeseong Shin, Sangbeom Lim, Honggyu An, Chaehyun Kim, Jisang Han, Donghyun Kim, Chanho Eom, Sunghwan Hong, Seungryong Kim</author><pubDate>Tue, 09 Sep 2025 17:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07979v1</guid></item><item><title>One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</title><link>http://arxiv.org/abs/2509.07978v1</link><description>Estimating the 6D pose of arbitrary unseen objects from a single referenceimage is critical for robotics operating in the long-tail of real-worldinstances. However, this setting is notoriously challenging: 3D models arerarely available, single-view reconstructions lack metric scale, and domaingaps between generated models and real-world images undermine robustness. Wepropose OnePoseViaGen, a pipeline that tackles these challenges through two keycomponents. First, a coarse-to-fine alignment module jointly refines scale andpose by combining multi-view feature matching with render-and-comparerefinement. Second, a text-guided generative domain randomization strategydiversifies textures, enabling effective fine-tuning of pose estimators withsynthetic data. Together, these steps allow high-fidelity single-view 3Dgeneration to support reliable one-shot 6D pose estimation. On challengingbenchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achievesstate-of-the-art performance far surpassing prior approaches. We furtherdemonstrate robust dexterous grasping with a real robot hand, validating thepracticality of our method in real-world manipulation. Project page:https://gzwsama.github.io/OnePoseviaGen.github.io/</description><author>Zheng Geng, Nan Wang, Shaocong Xu, Chongjie Ye, Bohan Li, Zhaoxi Chen, Sida Peng, Hao Zhao</author><pubDate>Tue, 09 Sep 2025 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07978v1</guid></item><item><title>Counterfactual Cocycles: A Framework for Robust and Coherent Counterfactual Transports</title><link>http://arxiv.org/abs/2405.13844v3</link><description>Estimating joint distributions (a.k.a. couplings) over counterfactualoutcomes is central to personalized decision-making and treatment riskassessment. Two emergent frameworks with identifiability guarantees are: (i)bijective structural causal models (SCMs), which are flexible but brittle tomis-specified latent noise; and (ii) optimal-transport (OT) methods, whichavoid latent noise assumptions but can produce incoherent counterfactualtransports which fail to identify higher-order couplings. In this work, webridge the gap with \emph{counterfactual cocycles}: a framework forcounterfactual transports that use algebraic structure to provide coherence andidentifiability guarantees. Every counterfactual cocycle corresponds to anequivalence class of SCMs, however the cocycle is invariant to the latent noisedistribution, enabling us to sidestep various mis-specification problems. Wecharacterize the structure of all identifiable counterfactual cocycles; proposeflexible model parameterizations; introduce a novel cocycle estimator thatavoids any distributional assumptions; and derive mis-specification robustnessproperties of the resulting counterfactual inference method. We demonstratestate-of-the-art performance and noise-robustness of counterfactual cocyclesacross synthetic benchmarks and a 401(k) eligibility study.</description><author>Hugh Dance, Benjamin Bloem-Reddy</author><pubDate>Tue, 09 Sep 2025 17:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13844v3</guid></item><item><title>Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence</title><link>http://arxiv.org/abs/2509.07972v1</link><description>Learning rate warmup is a popular and practical technique in traininglarge-scale deep neural networks. Despite the huge success in practice, thetheoretical advantages of this strategy of gradually increasing the learningrate at the beginning of the training process have not been fully understood.To resolve this gap between theory and practice, we first propose a novelfamily of generalized smoothness assumptions, and validate its applicabilityboth theoretically and empirically. Under the novel smoothness assumption, westudy the convergence properties of gradient descent (GD) in both deterministicand stochastic settings. It is shown that learning rate warmup consistentlyaccelerates GD, and GD with warmup can converge at most $\Theta(T)$ timesfaster than with a non-increasing learning rate schedule in some specificcases, providing insights into the benefits of this strategy from anoptimization theory perspective.</description><author>Yuxing Liu, Yuze Ge, Rui Pan, An Kang, Tong Zhang</author><pubDate>Tue, 09 Sep 2025 17:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07972v1</guid></item><item><title>Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</title><link>http://arxiv.org/abs/2509.07969v1</link><description>Recent advances in large multimodal models have leveraged image-based toolswith reinforcement learning to tackle visual problems. However, existingopen-source approaches often exhibit monotonous reasoning patterns and allowonly a limited number of interaction turns, making them inadequate fordifficult tasks that require trial-and-error exploration. In this work, weaddress this limitation by scaling up tool-based interactions and introduceMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens ofsteps -- and achieves state-of-the-art performance on challenging visual searchtasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three keycomponents. First, we construct the Visual Probe Dataset, a collection ofthousands of challenging visual search problems designed for exploratoryreasoning. Second, we develop an iterative data collection pipeline to obtaincold-start trajectories that exhibit diverse reasoning patterns, includingdepth-first search, trial-and-error, and goal maintenance. Third, we propose anover-turn masking strategy that prevents penalization of over-turn responses(those that hit the maximum number of turns) during reinforcement learning,thereby balancing training-time efficiency with test-time scalability. Despitetraining with an upper bound of only six interaction turns, our model generatestrajectories that naturally scale to tens of turns at inference time, withaccuracy improving as the number of turns increases. Extensive experimentsdemonstrate that Mini-o3 produces rich reasoning patterns and deep thinkingpaths, effectively solving challenging visual search problems.</description><author>Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao</author><pubDate>Tue, 09 Sep 2025 17:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07969v1</guid></item><item><title>SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge</title><link>http://arxiv.org/abs/2509.07968v1</link><description>We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating LargeLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. Itaddresses critical limitations in OpenAI's benchmark, including noisy andincorrect labels, topical biases, and question redundancy. SimpleQA Verifiedwas created through a rigorous multi-stage filtering process involvingde-duplication, topic balancing, and source reconciliation to produce a morereliable and challenging evaluation set, alongside improvements in theautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves astate-of-the-art F1-score of 55.6, outperforming other frontier models,including GPT-5. This work provides the research community with ahigher-fidelity tool to track genuine progress in parametric model factualityand to mitigate hallucinations. The benchmark dataset, evaluation code, andleaderboard are available at:https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.</description><author>Lukas Haas, Gal Yona, Giovanni D'Antonio, Sasha Goldshtein, Dipanjan Das</author><pubDate>Tue, 09 Sep 2025 17:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07968v1</guid></item><item><title>Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</title><link>http://arxiv.org/abs/2509.07966v1</link><description>Visual reasoning over structured data such as tables is a critical capabilityfor modern vision-language models (VLMs), yet current benchmarks remain limitedin scale, diversity, or reasoning depth, especially when it comes to renderedtable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,open-domain multimodal dataset specifically designed to evaluate and enhancevisual reasoning over complex tabular data. Our generation pipeline is modular,scalable, and fully autonomous, involving multiple reasoning LLMs collaboratingacross distinct roles: generation, validation, and inspiration. Visual-TableQAcomprises 2.5k richly structured LaTeX-rendered tables and 6kreasoning-intensive QA pairs, all produced at a cost of under USD 100. Topromote diversity and creativity, our pipeline performs multi-modelcollaborative data generation via cross-model prompting ('inspiration') andLLM-jury filtering. Stronger models seed layouts and topics that weaker modelselaborate, collectively distilling diverse reasoning patterns and visualstructures into the dataset. Empirical results show that models fine-tuned onVisual-TableQA generalize robustly to external benchmarks, outperformingseveral proprietary models despite the dataset's synthetic nature. The fullpipeline and resources are publicly available athttps://github.com/AI-4-Everyone/Visual-TableQA.</description><author>Boammani Aser Lompo, Marc Haraoui</author><pubDate>Tue, 09 Sep 2025 17:52:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07966v1</guid></item><item><title>Customizing the Inductive Biases of Softmax Attention using Structured Matrices</title><link>http://arxiv.org/abs/2509.07963v1</link><description>The core component of attention is the scoring function, which transforms theinputs into low-dimensional queries and keys and takes the dot product of eachpair. While the low-dimensional projection improves efficiency, it causesinformation loss for certain tasks that have intrinsically high-dimensionalinputs. Additionally, attention uses the same scoring function for all inputpairs, without imposing a distance-dependent compute bias for neighboringtokens in the sequence. In this work, we address these shortcomings byproposing new scoring functions based on computationally efficient structuredmatrices with high ranks, including Block Tensor-Train (BTT) and Multi-LevelLow Rank (MLR) matrices. On in-context regression tasks with high-dimensionalinputs, our proposed scoring functions outperform standard attention for anyfixed compute budget. On language modeling, a task that exhibits localitypatterns, our MLR-based attention method achieves improved scaling lawscompared to both standard attention and variants of sliding window attention.Additionally, we show that both BTT and MLR fall under a broader family ofefficient structured matrices capable of encoding either full-rank ordistance-dependent compute biases, thereby addressing significant shortcomingsof standard attention. Finally, we show that MLR attention has promisingresults for long-range time-series forecasting.</description><author>Yilun Kuang, Noah Amsel, Sanae Lotfi, Shikai Qiu, Andres Potapczynski, Andrew Gordon Wilson</author><pubDate>Tue, 09 Sep 2025 17:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07963v1</guid></item><item><title>Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare</title><link>http://arxiv.org/abs/2509.07961v1</link><description>We develop new experimental paradigms for measuring welfare in languagemodels. We compare verbal reports of models about their preferences withpreferences expressed through behavior when navigating a virtual environmentand selecting conversation topics. We also test how costs and rewards affectbehavior and whether responses to an eudaimonic welfare scale - measuringstates such as autonomy and purpose in life - are consistent acrosssemantically equivalent prompts. Overall, we observed a notable degree ofmutual support between our measures. The reliable correlations observed betweenstated preferences and behavior across conditions suggest that preferencesatisfaction can, in principle, serve as an empirically measurable welfareproxy in some of today's AI systems. Furthermore, our design offered anilluminating setting for qualitative observation of model behavior. Yet, theconsistency between measures was more pronounced in some models and conditionsthan others and responses were not consistent across perturbations. Due tothis, and the background uncertainty about the nature of welfare and thecognitive states (and welfare subjecthood) of language models, we are currentlyuncertain whether our methods successfully measure the welfare state oflanguage models. Nevertheless, these findings highlight the feasibility ofwelfare measurement in language models, inviting further exploration.</description><author>Valen Tagliabue, Leonard Dung</author><pubDate>Tue, 09 Sep 2025 17:48:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07961v1</guid></item><item><title>ACE and Diverse Generalization via Selective Disagreement</title><link>http://arxiv.org/abs/2509.07955v1</link><description>Deep neural networks are notoriously sensitive to spurious correlations -where a model learns a shortcut that fails out-of-distribution. Existing workon spurious correlations has often focused on incompletecorrelations,leveraging access to labeled instances that break the correlation.But in cases where the spurious correlations are complete, the correctgeneralization is fundamentally \textit{underspecified}. To resolve thisunderspecification, we propose learning a set of concepts that are consistentwith training data but make distinct predictions on a subset of novel unlabeledinputs. Using a self-training approach that encourages \textit{confident} and\textit{selective} disagreement, our method ACE matches or outperforms existingmethods on a suite of complete-spurious correlation benchmarks, while remainingrobust to incomplete spurious correlations. ACE is also more configurable thanprior approaches, allowing for straight-forward encoding of prior knowledge andprincipled unsupervised model selection. In an early application tolanguage-model alignment, we find that ACE achieves competitive performance onthe measurement tampering detection benchmark \textit{without} access tountrusted measurements. While still subject to important limitations, ACErepresents significant progress towards overcoming underspecification.</description><author>Oliver Daniels, Stuart Armstrong, Alexandre Maranh√£o, Mahirah Fairuz Rahman, Benjamin M. Marlin, Rebecca Gorman</author><pubDate>Tue, 09 Sep 2025 17:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07955v1</guid></item><item><title>RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and Correction</title><link>http://arxiv.org/abs/2509.07953v1</link><description>Modern paradigms for robot imitation train expressive policy architectures onlarge amounts of human demonstration data. Yet performance on contact-rich,deformable-object, and long-horizon tasks plateau far below perfect execution,even with thousands of expert demonstrations. This is due to the inefficiencyof existing ``expert'' data collection procedures based on human teleoperation.To address this issue, we introduce RaC, a new phase of training onhuman-in-the-loop rollouts after imitation learning pre-training. In RaC, wefine-tune a robotic policy on human intervention trajectories that illustraterecovery and correction behaviors. Specifically, during a policy rollout, humanoperators intervene when failure appears imminent, first rewinding the robotback to a familiar, in-distribution state and then providing a correctivesegment that completes the current sub-task. Training on this data compositionexpands the robotic skill repertoire to include retry and adaptation behaviors,which we show are crucial for boosting both efficiency and robustness onlong-horizon tasks. Across three real-world bimanual control tasks: shirthanging, airtight container lid sealing, takeout box packing, and a simulatedassembly task, RaC outperforms the prior state-of-the-art using 10$\times$ lessdata collection time and samples. We also show that RaC enables test-timescaling: the performance of the trained RaC policy scales linearly in thenumber of recovery maneuvers it exhibits. Videos of the learned policy areavailable at https://rac-scaling-robot.github.io/.</description><author>Zheyuan Hu, Robyn Wu, Naveen Enock, Jasmine Li, Riya Kadakia, Zackory Erickson, Aviral Kumar</author><pubDate>Tue, 09 Sep 2025 17:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07953v1</guid></item><item><title>Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives</title><link>http://arxiv.org/abs/2505.21627v2</link><description>State-of-the-art large language models require specialized hardware andsubstantial energy to operate. As a consequence, cloud-based services thatprovide access to large language models have become very popular. In theseservices, the price users pay for an output provided by a model depends on thenumber of tokens the model uses to generate it -- they pay a fixed price pertoken. In this work, we show that this pricing mechanism creates a financialincentive for providers to strategize and misreport the (number of) tokens amodel used to generate an output, and users cannot prove, or even know, whethera provider is overcharging them. However, we also show that, if an unfaithfulprovider is obliged to be transparent about the generative process used by themodel, misreporting optimally without raising suspicion is hard. Nevertheless,as a proof-of-concept, we develop an efficient heuristic algorithm that allowsproviders to significantly overcharge users without raising suspicion.Crucially, we demonstrate that the cost of running the algorithm is lower thanthe additional revenue from overcharging users, highlighting the vulnerabilityof users under the current pay-per-token pricing mechanism. Further, we showthat, to eliminate the financial incentive to strategize, a pricing mechanismmust price tokens linearly on their character count. While this makes aprovider's profit margin vary across tokens, we introduce a simple prescriptionunder which the provider who adopts such an incentive-compatible pricingmechanism can maintain the average profit margin they had under thepay-per-token pricing mechanism. Along the way, to illustrate and complementour theoretical results, we conduct experiments with several large languagemodels from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$families, and input prompts from the LMSYS Chatbot Arena platform.</description><author>Ander Artola Velasco, Stratis Tsirtsis, Nastaran Okati, Manuel Gomez-Rodriguez</author><pubDate>Tue, 09 Sep 2025 17:37:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.21627v2</guid></item><item><title>Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges</title><link>http://arxiv.org/abs/2509.07946v1</link><description>Multi-modal multi-task (M3T) foundation models (FMs) have recently showntransformative potential in artificial intelligence, with emerging applicationsin education. However, their deployment in real-world educational settings ishindered by privacy regulations, data silos, and limited domain-specific dataavailability. We introduce M3T Federated Foundation Models (FedFMs) foreducation: a paradigm that integrates federated learning (FL) with M3T FMs toenable collaborative, privacy-preserving training across decentralizedinstitutions while accommodating diverse modalities and tasks. Subsequently,this position paper aims to unveil M3T FedFMs as a promising yet underexploredapproach to the education community, explore its potentials, and reveal itsrelated future research directions. We outline how M3T FedFMs can advance threecritical pillars of next-generation intelligent education systems: (i) privacypreservation, by keeping sensitive multi-modal student and institutional datalocal; (ii) personalization, through modular architectures enabling tailoredmodels for students, instructors, and institutions; and (iii) equity andinclusivity, by facilitating participation from underrepresented andresource-constrained entities. We finally identify various open researchchallenges, including studying of (i) inter-institution heterogeneous privacyregulations, (ii) the non-uniformity of data modalities' characteristics, (iii)the unlearning approaches for M3T FedFMs, (iv) the continual learningframeworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which mustbe collectively addressed for practical deployment.</description><author>Kasra Borazjani, Naji Khosravan, Rajeev Sahay, Bita Akram, Seyyedali Hosseinalipour</author><pubDate>Tue, 09 Sep 2025 17:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07946v1</guid></item><item><title>One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning</title><link>http://arxiv.org/abs/2509.07945v1</link><description>In heterogeneous multi-task learning, tasks not only exhibit diverseobservation and action spaces but also vary substantially in intrinsicdifficulty. While conventional multi-task world models like UniZero excel insingle-task settings, we find that when handling large-scale heterogeneousenvironments, gradient conflicts and the loss of model plasticity oftenconstrain their sample and computational efficiency. In this work, we addressthese challenges from two perspectives: the single learning iteration and theoverall learning process. First, we investigate the impact of key design spaceson extending UniZero to multi-task planning. We find that a Mixture-of-Experts(MoE) architecture provides the most substantial performance gains bymitigating gradient conflicts, leading to our proposed model,\textit{ScaleZero}. Second, to dynamically balance the computational loadacross the learning process, we introduce an online, LoRA-based \textit{dynamicparameter scaling} (DPS) strategy. This strategy progressively integrates LoRAadapters in response to task-specific progress, enabling adaptive knowledgeretention and parameter expansion. Empirical evaluations on standard benchmarkssuch as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relyingexclusively on online reinforcement learning with one model, attainsperformance on par with specialized single-task baselines. Furthermore, whenaugmented with our dynamic parameter scaling strategy, our method achievescompetitive performance while requiring only 80\% of the single-taskenvironment interaction steps. These findings underscore the potential ofScaleZero for effective large-scale multi-task learning. Our code is availableat \textcolor{magenta}{https://github.com/opendilab/LightZero}.</description><author>Yuan Pu, Yazhe Niu, Jia Tang, Junyu Xiong, Shuai Hu, Hongsheng Li</author><pubDate>Tue, 09 Sep 2025 17:27:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07945v1</guid></item><item><title>ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation</title><link>http://arxiv.org/abs/2509.07941v1</link><description>Code generation has emerged as a pivotal capability of Large LanguageModels(LLMs), revolutionizing development efficiency for programmers of allskill levels. However, the complexity of data structures and algorithmic logicoften results in functional deficiencies and security vulnerabilities ingenerated code, reducing it to a prototype requiring extensive manualdebugging. While Retrieval-Augmented Generation (RAG) can enhance correctnessand security by leveraging external code manuals, it simultaneously introducesnew attack surfaces. In this paper, we pioneer the exploration of attack surfaces inRetrieval-Augmented Code Generation (RACG), focusing on malicious dependencyhijacking. We demonstrate how poisoned documentation containing hiddenmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploitingdual trust chains: LLM reliance on RAG and developers' blind trust in LLMsuggestions. To construct poisoned documents, we propose ImportSnare, a novelattack framework employing two synergistic strategies: 1)Position-aware beamsearch optimizes hidden ranking sequences to elevate poisoned documents inretrieval results, and 2)Multilingual inductive suggestions generatejailbreaking sequences to manipulate LLMs into recommending maliciousdependencies. Through extensive experiments across Python, Rust, andJavaScript, ImportSnare achieves significant attack success rates (over 50% forpopular libraries such as matplotlib and seaborn) in general, and is also ableto succeed even when the poisoning ratio is as low as 0.01%, targeting bothcustom and real-world malicious packages. Our findings reveal critical supplychain risks in LLM-powered development, highlighting inadequate securityalignment for code generation tasks. To support future research, we willrelease the multilingual benchmark suite and datasets. The project homepage ishttps://importsnare.github.io.</description><author>Kai Ye, Liangcai Su, Chenxiong Qian</author><pubDate>Tue, 09 Sep 2025 17:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07941v1</guid></item><item><title>Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees</title><link>http://arxiv.org/abs/2509.07939v1</link><description>Recent advances in Large Language Models (LLMs) have driven interest inautomating cybersecurity penetration testing workflows, offering the promise offaster and more consistent vulnerability assessment for enterprise systems.Existing LLM agents for penetration testing primarily rely on self-guidedreasoning, which can produce inaccurate or hallucinated procedural steps. As aresult, the LLM agent may undertake unproductive actions, such as exploitingunused software libraries or generating cyclical responses that repeat priortactics. In this work, we propose a guided reasoning pipeline for penetrationtesting LLM agents that incorporates a deterministic task tree built from theMITRE ATT&amp;CK Matrix, a proven penetration testing kll chain, to constrain theLLM's reaoning process to explicitly defined tactics, techniques, andprocedures. This anchors reasoning in proven penetration testing methodologiesand filters out ineffective actions by guiding the agent towards moreproductive attack procedures. To evaluate our approach, we built an automatedpenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, andGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with103 discrete subtasks representing real-world cyberattack scenarios. Ourproposed reasoning pipeline guided the LLM agent through 71.8\%, 72.8\%, and78.6\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.Comparatively, the state-of-the-art LLM penetration testing tool usingself-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks andrequired 86.2\%, 118.7\%, and 205.9\% more model queries. This suggests thatincorporating a deterministic task tree into LLM reasoning pipelines canenhance the accuracy and efficiency of automated cybersecurity assessments</description><author>Katsuaki Nakano, Reza Feyyazi, Shanchieh Jay Yang, Michael Zuzak</author><pubDate>Tue, 09 Sep 2025 17:19:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07939v1</guid></item><item><title>Feature Space Analysis by Guided Diffusion Model</title><link>http://arxiv.org/abs/2509.07936v1</link><description>One of the key issues in Deep Neural Networks (DNNs) is the black-box natureof their internal feature extraction process. Targeting vision-related domains,this paper focuses on analysing the feature space of a DNN by proposing adecoder that can generate images whose features are guaranteed to closely matcha user-specified feature. Owing to this guarantee that is missed in paststudies, our decoder allows us to evidence which of various attributes in animage are encoded into a feature by the DNN, by generating images whosefeatures are in proximity to that feature. Our decoder is implemented as aguided diffusion model that guides the reverse image generation of apre-trained diffusion model to minimise the Euclidean distance between thefeature of a clean image estimated at each step and the user-specified feature.One practical advantage of our decoder is that it can analyse feature spaces ofdifferent DNNs with no additional training and run on a single COTS GPU. Theexperimental results targeting CLIP's image encoder, ResNet-50 and visiontransformer demonstrate that images generated by our decoder have featuresremarkably similar to the user-specified ones and reveal valuable insights intothese DNNs' feature spaces.</description><author>Kimiaki Shirahama, Miki Yanobu, Kaduki Yamashita, Miho Ohsaki</author><pubDate>Tue, 09 Sep 2025 17:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07936v1</guid></item><item><title>Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation</title><link>http://arxiv.org/abs/2509.07933v1</link><description>The rapid evolution of Artificial Intelligence (AI) and Large Language Models(LLMs) has opened up new opportunities in the area of cybersecurity, especiallyin the exploitation automation landscape and penetration testing. This studyexplores Android penetration testing automation using LLM-based tools,especially PentestGPT, to identify and execute rooting techniques. Through acomparison of the traditional manual rooting process and exploitation methodsproduced using AI, this study evaluates the efficacy, reliability, andscalability of automated penetration testing in achieving high-level privilegeaccess on Android devices. With the use of an Android emulator (Genymotion) asthe testbed, we fully execute both traditional and exploit-based rootingmethods, automating the process using AI-generated scripts. Secondly, we createa web application by integrating OpenAI's API to facilitate automated scriptgeneration from LLM-processed responses. The research focuses on theeffectiveness of AI-enabled exploitation by comparing automated and manualpenetration testing protocols, by determining LLM weaknesses and strengthsalong the way. We also provide security suggestions of AI-enabled exploitation,including ethical factors and potential misuse. The findings exhibit that whileLLMs can significantly streamline the workflow of exploitation, they need to becontrolled by humans to ensure accuracy and ethical application. This studyadds to the increasing body of literature on AI-powered cybersecurity and itseffect on ethical hacking, security research, and mobile device security.</description><author>Wanni Vidulige Ishan Perera, Xing Liu, Fan liang, Junyi Zhang</author><pubDate>Tue, 09 Sep 2025 17:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07933v1</guid></item><item><title>Dynamic Scene 3D Reconstruction of an Uncooperative Resident Space Object</title><link>http://arxiv.org/abs/2509.07932v1</link><description>Characterization of uncooperative Resident Space Objects (RSO) play a crucialrole in On-Orbit Servicing (OOS) and Active Debris Removal (ADR) missions toassess the geometry and motion properties. To address the challenges ofreconstructing tumbling uncooperative targets, this study evaluates theperformance of existing state-of-the-art 3D reconstruction algorithms fordynamic scenes, focusing on their ability to generate geometrically accuratemodels with high-fidelity. To support our evaluation, we developed a simulationenvironment using Isaac Sim to generate physics-accurate 2D image sequences oftumbling satellite under realistic orbital lighting conditions. Our preliminaryresults on static scenes using Neuralangelo demonstrate promisingreconstruction quality. The generated 3D meshes closely match the original CADmodels with minimal errors and artifacts when compared using Cloud Compare(CC). The reconstructed models were able to capture critical fine details formission planning. This provides a baseline for our ongoing evaluation ofdynamic scene reconstruction.</description><author>Bala Prenith Reddy Gopu, Timothy Jacob Huber, George M. Nehma, Patrick Quinn, Madhur Tiwari, Matt Ueckermann, David Hinckley, Christopher McKenna</author><pubDate>Tue, 09 Sep 2025 17:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07932v1</guid></item><item><title>Smart Fast Finish: Preventing Overdelivery via Daily Budget Pacing at DoorDash</title><link>http://arxiv.org/abs/2509.07929v1</link><description>We present a budget pacing feature called Smart Fast Finish (SFF). SFF buildsupon the industry standard Fast Finish (FF) feature in budget pacing systemsthat depletes remaining advertising budget as quickly as possible towards theend of some fixed time period. SFF dynamically updates system parameters suchas start time and throttle rate depending on historical ad-campaign data. SFFis currently in use at DoorDash, one of the largest delivery platforms in theUS, and is part of its budget pacing system. We show via online budget-splitexperimentation data and offline simulations that SFF is a robust solution foroverdelivery mitigation when pacing budget.</description><author>Rohan Garg, Yongjin Xiao, Jason, Yang, Mandar Rahurkar</author><pubDate>Tue, 09 Sep 2025 17:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07929v1</guid></item><item><title>Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s</title><link>http://arxiv.org/abs/2509.07928v1</link><description>As local AI grows in popularity, there is a critical gap between thebenchmark performance of object detectors and their practical viability onconsumer-grade hardware. While models like YOLOv10s promise real-time speeds,these metrics are typically achieved on high-power, desktop-class GPUs. Thispaper reveals that on resource-constrained systems, such as laptops with RTX4060 GPUs, performance is not compute-bound but is instead dominated bysystem-level bottlenecks, as illustrated by a simple bottleneck test. Toovercome this hardware-level constraint, we introduce a Two-Pass AdaptiveInference algorithm, a model-independent approach that requires noarchitectural changes. This study mainly focuses on adaptive inferencestrategies and undertakes a comparative analysis of architectural early-exitand resolution-adaptive routing, highlighting their respective trade-offswithin a unified evaluation framework. The system uses a fast, low-resolutionpass and only escalates to a high-resolution model pass when detectionconfidence is low. On a 5000-image COCO dataset, our method achieves a 1.85xspeedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%.This work provides a practical and reproducible blueprint for deployinghigh-performance, real-time AI on consumer-grade devices by shifting the focusfrom pure model optimization to hardware-aware inference strategies thatmaximize throughput.</description><author>Mahmudul Islam Masum, Miad Islam, Arif I. Sarwat</author><pubDate>Tue, 09 Sep 2025 17:13:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07928v1</guid></item><item><title>GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models</title><link>http://arxiv.org/abs/2509.07925v1</link><description>Uncertainty estimation is essential for enhancing the reliability of LargeLanguage Models (LLMs), particularly in high-stakes applications. Existingmethods often overlook semantic dependencies, relying on token-levelprobability measures that fail to capture structural relationships within thegenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINtyEstimation for Large Language Models, a structure-aware framework thatleverages dependency parse trees and hierarchical graph pooling to refineuncertainty quantification. By incorporating supervised learning, GENUINEeffectively models semantic and structural relationships, improving confidenceassessments. Extensive experiments across NLP tasks show that GENUINE achievesup to 29% higher AUROC than semantic entropy-based approaches and reducescalibration errors by over 15%, demonstrating the effectiveness of graph-baseduncertainty modeling. The code is available athttps://github.com/ODYSSEYWT/GUQ.</description><author>Tuo Wang, Adithya Kulkarni, Tyler Cody, Peter A. Beling, Yujun Yan, Dawei Zhou</author><pubDate>Tue, 09 Sep 2025 17:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07925v1</guid></item><item><title>Automatic Reward Shaping from Confounded Offline Data</title><link>http://arxiv.org/abs/2505.11478v2</link><description>A key task in Artificial Intelligence is learning effective policies forcontrolling agents in unknown environments to optimize performance measures.Off-policy learning methods, like Q-learning, allow learners to make optimaldecisions based on past experiences. This paper studies off-policy learningfrom biased data in complex and high-dimensional domains where \emph{unobservedconfounding} cannot be ruled out a priori. Building on the well-celebrated DeepQ-Network (DQN), we propose a novel deep reinforcement learning algorithmrobust to confounding biases in observed data. Specifically, our algorithmattempts to find a safe policy for the worst-case environment compatible withthe observations. We apply our method to twelve confounded Atari games, andfind that it consistently dominates the standard DQN in all games where theobserved input to the behavioral and target policies mismatch and unobservedconfounders exist.</description><author>Mingxuan Li, Junzhe Zhang, Elias Bareinboim</author><pubDate>Tue, 09 Sep 2025 17:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11478v2</guid></item><item><title>Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation</title><link>http://arxiv.org/abs/2509.07923v1</link><description>Digital dentistry represents a transformative shift in modern dentalpractice. The foundational step in this transformation is the accurate digitalrepresentation of the patient's dentition, which is obtained from segmentedCone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite thegrowing interest in digital dental technologies, existing segmentationmethodologies frequently lack rigorous validation and demonstrate limitedperformance and clinical applicability. To the best of our knowledge, this isthe first work to introduce a multimodal pretraining framework for toothsegmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning forpretraining that integrates volumetric (CBCT) and surface-based (IOS)modalities. By capturing modality-invariant representations through multimodalcontrastive learning, our approach effectively models fine-grained anatomicalfeatures, enabling precise multi-class segmentation and accurate identificationof F\'ed\'eration Dentaire Internationale (FDI) tooth numbering. Along with theframework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset todate, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensivecollection of independent datasets, representing the largest and most diverseevaluation to date. Our method achieves state-of-the-art performance in bothinternal and external testing, with an increase of 12\% for CBCT segmentationand 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC).Furthermore, ToothMCL consistently surpasses existing approaches in toothgroups and demonstrates robust generalizability across varying imagingconditions and clinical scenarios.</description><author>Moo Hyun Son, Juyoung Bae, Zelin Qiu, Jiale Peng, Kai Xin Li, Yifan Lin, Hao Chen</author><pubDate>Tue, 09 Sep 2025 17:05:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07923v1</guid></item><item><title>Llama-Nemotron: Efficient Reasoning Models</title><link>http://arxiv.org/abs/2505.00949v5</link><description>We introduce the Llama-Nemotron series of models, an open family ofheterogeneous reasoning models that deliver exceptional reasoning capabilities,inference efficiency, and an open license for enterprise use. The family comesin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performscompetitively with state-of-the-art reasoning models such as DeepSeek-R1 whileoffering superior inference throughput and memory efficiency. In this report,we discuss the training procedure for these models, which entails using neuralarchitecture search from Llama 3 models for accelerated inference, knowledgedistillation, and continued pretraining, followed by a reasoning-focusedpost-training stage consisting of two main parts: supervised fine-tuning andlarge scale reinforcement learning. Llama-Nemotron models are the firstopen-source models to support a dynamic reasoning toggle, allowing users toswitch between standard chat and reasoning modes during inference. To furthersupport open research and facilitate model development, we provide thefollowing resources: 1. We release the Llama-Nemotron reasoning models --LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIAOpen Model License Agreement. 2. We release the complete post-training dataset:Llama-Nemotron-Post-Training-Dataset. 3. We also release our trainingcodebases: NeMo, NeMo-Aligner, and Megatron-LM.</description><author>Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Prasoon Varshney, Makesh Narsimhan, Jane Polak Scowcroft, John Kam</author><pubDate>Tue, 09 Sep 2025 17:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.00949v5</guid></item><item><title>ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion</title><link>http://arxiv.org/abs/2509.07920v1</link><description>Joint reconstruction of human-object interaction marks a significantmilestone in comprehending the intricate interrelations between humans andtheir surrounding environment. Nevertheless, previous optimization methodsoften struggle to achieve physically plausible reconstruction results due tothe lack of prior knowledge about human-object interactions. In this paper, weintroduce ScoreHOI, an effective diffusion-based optimizer that introducesdiffusion priors for the precise recovery of human-object interactions. Byharnessing the controllability within score-guided sampling, the diffusionmodel can reconstruct a conditional distribution of human and object pose giventhe image observation and object feature. During inference, the ScoreHOIeffectively improves the reconstruction results by guiding the denoisingprocess with specific physical constraints. Furthermore, we propose acontact-driven iterative refinement approach to enhance the contactplausibility and improve the reconstruction accuracy. Extensive evaluations onstandard benchmarks demonstrate ScoreHOI's superior performance overstate-of-the-art methods, highlighting its ability to achieve a precise androbust improvement in joint human-object interaction reconstruction.</description><author>Ao Li, Jinpeng Liu, Yixuan Zhu, Yansong Tang</author><pubDate>Tue, 09 Sep 2025 17:00:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07920v1</guid></item><item><title>Object-level Correlation for Few-Shot Segmentation</title><link>http://arxiv.org/abs/2509.07917v1</link><description>Few-shot semantic segmentation (FSS) aims to segment objects of novelcategories in the query images given only a few annotated support samples.Existing methods primarily build the image-level correlation between thesupport target object and the entire query image. However, this correlationcontains the hard pixel noise, \textit{i.e.}, irrelevant background objects,that is intractable to trace and suppress, leading to the overfitting of thebackground. To address the limitation of this correlation, we imitate thebiological vision process to identify novel objects in the object-levelinformation. Target identification in the general objects is more valid than inthe entire image, especially in the low-data regime. Inspired by this, wedesign an Object-level Correlation Network (OCNet) by establishing theobject-level correlation between the support target object and query generalobjects, which is mainly composed of the General Object Mining Module (GOMM)and Correlation Construction Module (CCM). Specifically, GOMM constructs thequery general object feature by learning saliency and high-level similaritycues, where the general objects include the irrelevant background objects andthe target foreground object. Then, CCM establishes the object-levelcorrelation by allocating the target prototypes to match the general objectfeature. The generated object-level correlation can mine the query targetfeature and suppress the hard pixel noise for the final prediction. Extensiveexperiments on PASCAL-${5}^{i}$ and COCO-${20}^{i}$ show that our modelachieves the state-of-the-art performance.</description><author>Chunlin Wen, Yu Zhang, Jie Fan, Hongyuan Zhu, Xiu-Shen Wei, Yijun Wang, Zhiqiang Kou, Shuzhou Sun</author><pubDate>Tue, 09 Sep 2025 16:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07917v1</guid></item><item><title>Uncovering Scaling Laws for Large Language Models via Inverse Problems</title><link>http://arxiv.org/abs/2509.07909v1</link><description>Large Language Models (LLMs) are large-scale pretrained models that haveachieved remarkable success across diverse domains. These successes have beendriven by unprecedented complexity and scale in both data and computations.However, due to the high costs of training such models, brute-forcetrial-and-error approaches to improve LLMs are not feasible. Inspired by thesuccess of inverse problems in uncovering fundamental scientific laws, thisposition paper advocates that inverse problems can also efficiently uncoverscaling laws that guide the building of LLMs to achieve the desirableperformance with significantly better cost-effectiveness.</description><author>Arun Verma, Zhaoxuan Wu, Zijian Zhou, Xiaoqiang Lin, Zhiliang Chen, Rachael Hwee Ling Sim, Rui Qiao, Jingtan Wang, Nhung Bui, Xinyuan Niu, Wenyang Hu, Gregory Kang Ruey Lau, Zi-Yu Khoo, Zitong Zhao, Xinyi Xu, Apivich Hemachandra, See-Kiong Ng, Bryan Kian Hsiang Low</author><pubDate>Tue, 09 Sep 2025 16:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07909v1</guid></item><item><title>Biased Tales: Cultural and Topic Bias in Generating Children's Stories</title><link>http://arxiv.org/abs/2509.07908v1</link><description>Stories play a pivotal role in human communication, shaping beliefs andmorals, particularly in children. As parents increasingly rely on largelanguage models (LLMs) to craft bedtime stories, the presence of cultural andgender stereotypes in these narratives raises significant concerns. To addressthis issue, we present Biased Tales, a comprehensive dataset designed toanalyze how biases influence protagonists' attributes and story elements inLLM-generated stories. Our analysis uncovers striking disparities. When theprotagonist is described as a girl (as compared to a boy), appearance-relatedattributes increase by 55.26%. Stories featuring non-Western childrendisproportionately emphasize cultural heritage, tradition, and family themesfar more than those for Western children. Our findings highlight the role ofsociocultural bias in making creative AI use more equitable and diverse.</description><author>Donya Rooein, Vil√©m Zouhar, Debora Nozza, Dirk Hovy</author><pubDate>Tue, 09 Sep 2025 16:51:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07908v1</guid></item><item><title>Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts</title><link>http://arxiv.org/abs/2504.21117v2</link><description>Evaluating natural language generation systems is challenging due to thediversity of valid outputs. While human evaluation is the gold standard, itsuffers from inconsistencies, lack of standardisation, and demographic biases,limiting reproducibility. LLM-based evaluators offer a scalable alternative butare highly sensitive to prompt design, where small variations can lead tosignificant discrepancies. In this work, we propose an inversion learningmethod that learns effective reverse mappings from model outputs back to theirinput instructions, enabling the automatic generation of highly effective,model-specific evaluation prompts. Our method requires only a single evaluationsample and eliminates the need for time-consuming manual prompt engineering,thereby improving both efficiency and robustness. Our work contributes toward anew direction for more robust and efficient LLM-based evaluation.</description><author>Hanhua Hong, Chenghao Xiao, Yang Wang, Yiqi Liu, Wenge Rong, Chenghua Lin</author><pubDate>Tue, 09 Sep 2025 16:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.21117v2</guid></item><item><title>MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</title><link>http://arxiv.org/abs/2508.17180v2</link><description>A key frontier for Multimodal Large Language Models (MLLMs) is the ability toperform deep mathematical and spatial reasoning directly from images, movingbeyond their established success in semantic description. Mathematical surfaceplots provide a rigorous testbed for this capability, as they isolate the taskof reasoning from the semantic noise common in natural images. To measureprogress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning overVisual Landscapes), a new benchmark designed to quantitatively evaluate thesecore reasoning skills. The benchmark comprises two novel tasks: TopologicalCounting, identifying and enumerating features like local maxima; andTransformation Recognition, recognizing applied geometric transformations.Generated from a curated library of functions with rigorous ambiguityfiltering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMsstruggle significantly, often resorting to superficial heuristics instead ofrobust spatial reasoning. MaRVL-QA provides a challenging new tool for theresearch community to measure progress, expose model limitations, and guide thedevelopment of MLLMs with more profound reasoning abilities.</description><author>Nilay Pande, Sahiti Yerramilli, Jayant Sravan Tamarapalli, Rynaa Grover</author><pubDate>Tue, 09 Sep 2025 16:48:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17180v2</guid></item><item><title>Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph Embeddings</title><link>http://arxiv.org/abs/2509.07905v1</link><description>Knowledge graphs and ontologies represent entities and their relationships ina structured way, having gained significance in the development of modern AIapplications. Integrating these semantic resources with machine learning modelsoften relies on knowledge graph embedding models to transform graph data intonumerical representations. Therefore, pre-trained models for popular knowledgegraphs and ontologies are increasingly valuable, as they spare the need toretrain models for different tasks using the same data, thereby helping todemocratize AI development and enabling sustainable computing. In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API,designed to generate and serve knowledge graph embeddings for widely usedbiomedical ontologies. Given the dynamic nature of these ontologies,Bio-KGvec2go also supports regular updates aligned with ontology versionreleases. By offering up-to-date embeddings with minimal computational effortrequired from users, Bio-KGvec2go facilitates efficient and timely biomedicalresearch.</description><author>Hamid Ahmad, Heiko Paulheim, Rita T. Sousa</author><pubDate>Tue, 09 Sep 2025 16:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07905v1</guid></item><item><title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title><link>http://arxiv.org/abs/2509.05685v2</link><description>Transforming road network data into vector representations using deeplearning has proven effective for road network analysis. However, urban roadnetworks' heterogeneous and hierarchical nature poses challenges for accuraterepresentation learning. Graph neural networks, which aggregate features fromneighboring nodes, often struggle due to their homogeneity assumption and focuson a single structural scale. To address these issues, this paper presentsMSRFormer, a novel road network representation learning framework thatintegrates multi-scale spatial interactions by addressing their flowheterogeneity and long-distance dependencies. It uses spatial flow convolutionto extract small-scale features from large trajectory datasets, and identifiesscale-dependent spatial interaction regions to capture the spatial structure ofroad networks and flow heterogeneity. By employing a graph transformer,MSRFormer effectively captures complex spatial dependencies across multiplescales. The spatial interaction features are fused using residual connections,which are fed to a contrastive learning algorithm to derive the final roadnetwork representation. Validation on two real-world datasets demonstrates thatMSRFormer outperforms baseline methods in two road network analysis tasks. Theperformance gains of MSRFormer suggest the traffic-related task benefits morefrom incorporating trajectory data, also resulting in greater improvements incomplex road network structures with up to 16% improvements compared to themost competitive baseline method. This research provides a practical frameworkfor developing task-agnostic road network representation models and highlightsdistinct association patterns of the interplay between scale effects and flowheterogeneity of spatial interactions.</description><author>Jian Yang, Jiahui Wu, Li Fang, Hongchao Fan, Bianying Zhang, Huijie Zhao, Guangyi Yang, Rui Xin, Xiong You</author><pubDate>Tue, 09 Sep 2025 16:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.05685v2</guid></item><item><title>Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning</title><link>http://arxiv.org/abs/2109.03445v7</link><description>We begin by briefly surveying some results on the convergence of theStochastic Gradient Descent (SGD) Method, proved in a companion paper by thepresent authors. These results are based on viewing SGD as a version ofStochastic Approximation (SA). Ever since its introduction in the classic paperof Robbins and Monro in 1951, SA has become a standard tool for finding asolution of an equation of the form $f(\theta) = 0$, when only noisymeasurements of $f(\cdot)$ are available. In most situations, \textit{everycomponent} of the putative solution $\theta_t$ is updated at each step $t$. Insome applications in Reinforcement Learning (RL), \textit{only one component}of $\theta_t$ is updated at each $t$. This is known as \textbf{asynchronous}SA. In this paper, we study \textbf{Block Asynchronous SA (BASA)}, in which, ateach step $t$, \textit{some but not necessarily all} components of $\theta_t$are updated. The theory presented here embraces both conventional (synchronous)SA as well as asynchronous SA, and all in-between possibilities. We providesufficient conditions for the convergence of BASA, and also prove bounds on the\textit{rate} of convergence of $\theta_t$ to the solution. For the case ofconventional SGD, these results reduce to those proved in our companion paper.Then we apply these results to the problem of finding a fixed point of a mapwith only noisy measurements. This problem arises frequently in RL. We provesufficient conditions for convergence as well as estimates for the rate ofconvergence.</description><author>Rajeeva L. Karandikar, M. Vidyasagar</author><pubDate>Tue, 09 Sep 2025 16:45:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.03445v7</guid></item><item><title>Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs</title><link>http://arxiv.org/abs/2506.07824v2</link><description>Multi-digit addition is a clear probe of the computational power of largelanguage models. To dissect the internal arithmetic processes inLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.Inspired by the step-by-step manner in which humans perform addition, wepropose and analyze a coherent four-stage trajectory in the forwardpass:Formula-structure representations become linearly decodable first, whilethe answer token is still far down the candidate list.Core computationalfeatures then emerge prominently.At deeper activation layers, numericalabstractions of the result become clearer, enabling near-perfect detection anddecoding of the individual digits in the sum.Near the output, the modelorganizes and generates the final content, with the correct token reliablyoccupying the top rank.This trajectory suggests a hierarchical process thatfavors internal computation over rote memorization. We release our code anddata to facilitate reproducibility.</description><author>Yao Yan</author><pubDate>Tue, 09 Sep 2025 16:35:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.07824v2</guid></item><item><title>A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges</title><link>http://arxiv.org/abs/2508.06401v3</link><description>This systematic review of the research literature on retrieval-augmentedgeneration (RAG) provides a focused analysis of the most highly cited studiespublished between 2020 and May 2025. A total of 128 articles met our inclusioncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).RAG couples a neural retriever with a generative language model, groundingoutput in up-to-date, non-parametric memory while retaining the semanticgeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we(i) specify explicit inclusion and exclusion criteria based on citation countand research questions, (ii) catalogue datasets, architectures, and evaluationpractices, and (iii) synthesise empirical evidence on the effectiveness andlimitations of RAG. To mitigate citation-lag bias, we applied a lowercitation-count threshold to papers published in 2025 so that emergingbreakthroughs with naturally fewer citations were still captured. This reviewclarifies the current research landscape, highlights methodological gaps, andcharts priority directions for future research.</description><author>Andrew Brown, Muhammad Roman, Barry Devereux</author><pubDate>Tue, 09 Sep 2025 16:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06401v3</guid></item><item><title>A Modular Algorithm for Non-Stationary Online Convex-Concave Optimization</title><link>http://arxiv.org/abs/2509.07901v1</link><description>This paper investigates the problem of Online Convex-Concave Optimization,which extends Online Convex Optimization to two-player time-varyingconvex-concave games. The goal is to minimize the dynamic duality gap (D-DGap),a critical performance measure that evaluates players' strategies againstarbitrary comparator sequences. Existing algorithms fail to deliver optimalperformance, particularly in stationary or predictable environments. To addressthis, we propose a novel modular algorithm with three core components: anAdaptive Module that dynamically adjusts to varying levels of non-stationarity,a Multi-Predictor Aggregator that identifies the best predictor among multiplecandidates, and an Integration Module that effectively combines theirstrengths. Our algorithm achieves a minimax optimal D-DGap upper bound, up to alogarithmic factor, while also ensuring prediction error-driven D-DGap bounds.The modular design allows for the seamless replacement of components thatregulate adaptability to dynamic environments, as well as the incorporation ofcomponents that integrate ``side knowledge'' from multiple predictors.Empirical results further demonstrate the effectiveness and adaptability of theproposed method.</description><author>Qing-xin Meng, Xia Lei, Jian-wei Liu</author><pubDate>Tue, 09 Sep 2025 16:33:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07901v1</guid></item><item><title>Convergence of Momentum-Based Optimization Algorithms with Time-Varying Parameters</title><link>http://arxiv.org/abs/2506.11904v2</link><description>In this paper, we present a unified algorithm for stochastic optimizationthat makes use of a "momentum" term; in other words, the stochastic gradientdepends not only on the current true gradient of the objective function, butalso on the true gradient at the previous iteration. Our formulation includesthe Stochastic Heavy Ball (SHB) and the Stochastic Nesterov AcceleratedGradient (SNAG) algorithms as special cases. In addition, in our formulation,the momentum term is allowed to vary as a function of time (i.e., the iterationcounter). The assumptions on the stochastic gradient are the most general inthe literature, in that it can be biased, and have a conditional variance thatgrows in an unbounded fashion as a function of time. This last feature iscrucial in order to make the theory applicable to "zero-order" methods, wherethe gradient is estimated using just two function evaluations. We present a set of sufficient conditions for the convergence of the unifiedalgorithm. These conditions are natural generalizations of the familiarRobbins-Monro and Kiefer-Wolfowitz-Blum conditions for standard stochasticgradient descent. We also analyze another method from the literature for theSHB algorithm with a time-varying momentum parameter, and show that it isimpracticable.</description><author>Mathukumalli Vidyasagar</author><pubDate>Tue, 09 Sep 2025 16:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.11904v2</guid></item><item><title>Feasibility of In-Ear Single-Channel ExG for Wearable Sleep~Monitoring in Real-World Settings</title><link>http://arxiv.org/abs/2509.07896v1</link><description>Automatic sleep staging typically relies on gold-standard EEG setups, whichare accurate but obtrusive and impractical for everyday use outside sleeplaboratories. This limits applicability in real-world settings, such as homeenvironments, where continuous, long-term monitoring is needed. Detecting sleeponset is particularly relevant, enabling consumer applications (e.g.automatically pausing media playback when the user falls asleep). Recentresearch has shown correlations between in-ear EEG and full-scalp EEG forvarious phenomena, suggesting wearable, in-ear devices could allow unobtrusivesleep monitoring. We investigated the feasibility of using single-channelin-ear electrophysiological (ExG) signals for automatic sleep staging in awearable device by conducting a sleep study with 11~participants (mean age:24), using a custom earpiece with a dry eartip electrode (D\"atwyler SoftPulse)as a measurement electrode in one ear and a reference in the other. Groundtruth sleep stages were obtained from an Apple Watch Ultra, validated for sleepstaging. Our system achieved 90.5% accuracy for binary sleep detection (Awakevs. Asleep) and 65.1% accuracy for four-class staging (Awake, REM, Core, Deep)using leave-one-subject-out validation. These findings demonstrate thepotential of in-ear electrodes as a low-effort, comfortable approach to sleepmonitoring, with applications such as stopping podcasts when users fall asleep.</description><author>Philipp Lepold, Jonas Leichtle, Tobias R√∂ddiger, Michael Beigl</author><pubDate>Tue, 09 Sep 2025 16:27:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07896v1</guid></item><item><title>HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?</title><link>http://arxiv.org/abs/2509.07894v1</link><description>Recently, the physical capabilities of (M)LLMs have garnered increasingattention. However, existing benchmarks for physics suffer from two major gaps:they neither provide systematic and up-to-date coverage of real-world physicscompetitions such as physics Olympiads, nor enable direct performancecomparison with humans. To bridge these gaps, we present HiPhO, the firstbenchmark dedicated to high school physics Olympiads with human-alignedevaluation. Specifically, HiPhO highlights three key innovations. (1)Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,spanning both international and regional competitions, and covering mixedmodalities that encompass problems spanning text-only to diagram-based. (2)Professional Evaluation: We adopt official marking schemes to performfine-grained grading at both the answer and step level, fully aligned withhuman examiners to ensure high-quality and domain-specific evaluation. (3)Comparison with Human Contestants: We assign gold, silver, and bronze medals tomodels based on official medal thresholds, thereby enabling direct comparisonbetween (M)LLMs and human contestants. Our large-scale evaluation of 30state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostlyremain at or below the bronze level; open-source LLMs show promising progresswith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 goldmedals; and most models still have a significant gap from full marks. Theseresults highlight a substantial performance gap between open-source models andtop students, the strong physical reasoning capabilities of closed-sourcereasoning models, and the fact that there is still significant room forimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focusedbenchmark for advancing multimodal physical reasoning, is open-source andavailable at https://github.com/SciYu/HiPhO.</description><author>Fangchen Yu, Haiyuan Wan, Qianjia Cheng, Yuchen Zhang, Jiacheng Chen, Fujun Han, Yulun Wu, Junchi Yao, Ruilizhen Hu, Ning Ding, Yu Cheng, Tao Chen, Lei Bai, Dongzhan Zhou, Yun Luo, Ganqu Cui, Peng Ye</author><pubDate>Tue, 09 Sep 2025 16:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07894v1</guid></item><item><title>Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers</title><link>http://arxiv.org/abs/2508.15792v2</link><description>Antonym vs synonym distinction across multiple languages presents uniquecomputational challenges due to the paradoxical nature of antonymousrelationships words that share semantic domains while expressing oppositemeanings. This work introduces Bhav-Net, a novel dual-space architecture thatenables effective knowledge transfer from complex multilingual models tosimpler, language-specific architectures while maintaining robust cross-lingualantonym--synonym distinction capabilities. Our approach combineslanguage-specific BERT encoders with graph transformer networks, creatingdistinct semantic projections where synonymous pairs cluster in one space whileantonymous pairs exhibit high similarity in a complementary space. Throughcomprehensive evaluation across eight languages (English, German, French,Spanish, Italian, Portuguese, Dutch, and Russian), we demonstrate that semanticrelationship modeling transfers effectively across languages. The dual-encoderdesign achieves competitive performance against state-of-the-art baselineswhile providing interpretable semantic representations and effectivecross-lingual generalization.</description><author>Samyak S. Sanghvi</author><pubDate>Tue, 09 Sep 2025 16:24:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15792v2</guid></item><item><title>Expected Signature Kernels for L√©vy Rough Paths</title><link>http://arxiv.org/abs/2509.07893v1</link><description>The expected signature kernel arises in statistical learning tasks as asimilarity measure of probability measures on path space. Computing this kernelfor known classes of stochastic processes is an important problem that, inparticular, can help reduce computational costs. Building on the representationof the expected signature of (inhomogeneous) L\'evy processes with absolutelycontinuous characteristics as the development of an absolutely continuous pathin the extended tensor algebra [F.-H.-Tapia, Forum of Mathematics: Sigma(2022), "Unified signature cumulants and generalized Magnus expansions"], weextend the arguments developed for smooth rough paths in[Lemercier-Lyons-Salvi, "Log-PDE Methods for Rough Signature Kernels"] toderive a PDE system for the expected signature of inhomogeneous L\'evyprocesses. As a specific example, we see that the expected signature kernel ofGaussian martingales satisfies a Goursat PDE.</description><author>Peter K. Friz, Paul P. Hager</author><pubDate>Tue, 09 Sep 2025 16:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07893v1</guid></item><item><title>Cardiverse: Harnessing LLMs for Novel Card Game Prototyping</title><link>http://arxiv.org/abs/2502.07128v2</link><description>The prototyping of computer games, particularly card games, requiresextensive human effort in creative ideation and gameplay evaluation. Recentadvances in Large Language Models (LLMs) offer opportunities to automate andstreamline these processes. However, it remains challenging for LLMs to designnovel game mechanics beyond existing databases, generate consistent gameplayenvironments, and develop scalable gameplay AI for large-scale evaluations.This paper addresses these challenges by introducing a comprehensive automatedcard game prototyping framework. The approach highlights a graph-based indexingmethod for generating novel game variations, an LLM-driven system forconsistent game code generation validated by gameplay records, and a gameplayAI constructing method that uses an ensemble of LLM-generated heuristicfunctions optimized through self-play. These contributions aim to acceleratecard game prototyping, reduce human labor, and lower barriers to entry for gamedevelopers. For code repo visit this http URLhttps://github.com/danruili/Cardiverse</description><author>Danrui Li, Sen Zhang, Sam S. Sohn, Kaidong Hu, Muhammad Usman, Mubbasir Kapadia</author><pubDate>Tue, 09 Sep 2025 16:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.07128v2</guid></item><item><title>Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning</title><link>http://arxiv.org/abs/2509.06213v2</link><description>We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)environment, a complex puzzle in which an agent must infer and execute hiddenrules to clear a 6$\times$6 board by placing game pieces into buckets. Weexplore two state representation strategies, namely Feature-Centric (FC) andObject-Centric (OC), and employ a Transformer-based Advantage Actor-Critic(A2C) algorithm for training. The agent has access only to partial observationsand must simultaneously infer the governing rule and learn the optimal policythrough experience. We evaluate our models across multiple rule-based andtrial-list-based experimental setups, analyzing transfer effects and the impactof representation on learning efficiency.</description><author>Christo Mathew, Wentian Wang, Jacob Feldman, Lazaros K. Gallos, Paul B. Kantor, Vladimir Menkov, Hao Wang</author><pubDate>Tue, 09 Sep 2025 16:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.06213v2</guid></item><item><title>JoPA:Explaining Large Language Model's Generation via Joint Prompt Attribution</title><link>http://arxiv.org/abs/2405.20404v2</link><description>Large Language Models (LLMs) have demonstrated impressive performances incomplex text generation tasks. However, the contribution of the input prompt tothe generated content still remains obscure to humans, underscoring thenecessity of understanding the causality between input and output pairs.Existing works for providing prompt-specific explanation often confine modeloutput to be classification or next-word prediction. Few initial attemptsaiming to explain the entire language generation often treat input prompt textsindependently, ignoring their combinatorial effects on the follow-upgeneration. In this study, we introduce a counterfactual explanation frameworkbased on Joint Prompt Attribution, JoPA, which aims to explain how a few prompttexts collaboratively influences the LLM's complete generation. Particularly,we formulate the task of prompt attribution for generation interpretation as acombinatorial optimization problem, and introduce a probabilistic algorithm tosearch for the casual input combination in the discrete space. We define andutilize multiple metrics to evaluate the produced explanations, demonstratingboth the faithfulness and efficiency of our framework.</description><author>Yurui Chang, Bochuan Cao, Yujia Wang, Jinghui Chen, Lu Lin</author><pubDate>Tue, 09 Sep 2025 16:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20404v2</guid></item><item><title>From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing</title><link>http://arxiv.org/abs/2509.07889v1</link><description>This paper presents our team's solution to Shared Task 7 of NLPCC-2025, whichfocuses on sentence-level gender bias detection and mitigation in Chinese. Thetask aims to promote fairness and controllability in natural languagegeneration by automatically detecting, classifying, and mitigating gender bias.To address this challenge, we adopt a fine-tuning approach based on largelanguage models (LLMs), efficiently adapt to the bias detection task viaLow-Rank Adaptation (LoRA). In terms of data processing, we construct a morebalanced training set to alleviate class imbalance and introduce heterogeneoussamples from multiple sources to enhance model generalization. For thedetection and classification sub-tasks, we employ a majority voting strategythat integrates outputs from multiple expert models to boost performance.Additionally, to improve bias generation detection and mitigation, we design amulti-temperature sampling mechanism to capture potential variations in biasexpression styles. Experimental results demonstrate the effectiveness of ourapproach in bias detection, classification, and mitigation. Our methodultimately achieves an average score of 47.90%, ranking fourth in the sharedtask.</description><author>Chengyan Wu, Yiqiang Cai, Yufei Cheng, Yun Xue</author><pubDate>Tue, 09 Sep 2025 16:12:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07889v1</guid></item><item><title>A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges</title><link>http://arxiv.org/abs/2509.07887v1</link><description>Graph Neural Networks (GNNs) have gained traction in the complex domain ofdrug discovery because of their ability to process graph-structured data suchas drug molecule models. This approach has resulted in a myriad of methods andmodels in published literature across several categories of drug discoveryresearch. This paper covers the research categories comprehensively with recentpapers, namely molecular property prediction, including drug-target bindingaffinity prediction, drug-drug interaction study, microbiome interactionprediction, drug repositioning, retrosynthesis, and new drug design, andprovides guidance for future work on GNNs for drug discovery.</description><author>Katherine Berry, Liang Cheng</author><pubDate>Tue, 09 Sep 2025 16:09:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07887v1</guid></item><item><title>Audio-centric Video Understanding Benchmark without Text Shortcut</title><link>http://arxiv.org/abs/2503.19951v2</link><description>Audio often serves as an auxiliary modality in video understanding tasks ofaudio-visual large language models (LLMs), merely assisting in thecomprehension of visual information. However, a thorough understanding ofvideos significantly depends on auditory information, as audio offers criticalcontext, emotional cues, and semantic meaning that visual data alone oftenlacks. This paper proposes an audio-centric video understanding benchmark(AVUT) to evaluate the video comprehension capabilities of multimodal LLMs witha particular focus on auditory information. AVUT introduces a suite ofcarefully designed audio-centric tasks, holistically testing the understandingof both audio content and audio-visual interactions in videos. Moreover, thiswork points out the text shortcut problem that largely exists in otherbenchmarks where the correct answer can be found from question text alonewithout needing videos. AVUT addresses this problem by proposing a answerpermutation-based filtering mechanism. A thorough evaluation across a diverserange of open-source and proprietary multimodal LLMs is performed, followed bythe analyses of deficiencies in audio-visual LLMs. Demos and data are availableat https://github.com/lark-png/AVUT.</description><author>Yudong Yang, Jimin Zhuang, Guangzhi Sun, Changli Tang, Yixuan Li, Peihan Li, Yifan Jiang, Wei Li, Zejun Ma, Chao Zhang</author><pubDate>Tue, 09 Sep 2025 16:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.19951v2</guid></item><item><title>Visualizing Thought: Conceptual Diagrams Enable Robust Combinatorial Planning in LMMs</title><link>http://arxiv.org/abs/2503.11790v2</link><description>Human reasoning relies on constructing and manipulating mental models --simplified internal representations of situations that we use to understand andsolve problems. Conceptual diagrams (e.g., a sketch drawn by a human to aidreasoning) externalize these mental models, abstracting irrelevant details toefficiently capture how entities interact with each other. In contrast, LargeLanguage Models (LLMs) and Large MultiModal Models (LMMs) predominantly reasonthrough text, limiting their effectiveness in complex multi-step tasks. In thispaper, we propose Visual Thinking, a zero-shot framework that enables LMMs toreason through multiple chains of (self-generated) conceptual diagrams,significantly enhancing their combinatorial planning capabilities. Our approachdoes not require any human initialization beyond the natural languagedescription of the task. It integrates both textual and diagrammatic reasoningwithin an optimized Graph-of-Thought inference framework, enhanced by beamsearch and depth-wise backtracking. Evaluated on multiple challenging PDDLplanning domains, our method substantially improves LMMs' performance (e.g.,GPT-4o: 35.5% -&gt; 90.2% in Blocksworld) and consistently outperforms othertext-only search-based inference methods. On more difficult planning domainswith solution depths up to 40, our approach outperforms even the o1-previewreasoning model (e.g., 16 percentage points improvement in Floor Tiles). Theseresults highlight the value of conceptual diagrams as a reasoning medium inLMMs.</description><author>Nasim Borazjanizadeh, Roei Herzig, Eduard Oks, Trevor Darrell, Rogerio Feris, Leonid Karlinsky</author><pubDate>Tue, 09 Sep 2025 16:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.11790v2</guid></item><item><title>Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference</title><link>http://arxiv.org/abs/2509.06942v2</link><description>Recent studies have demonstrated the effectiveness of directly aligningdiffusion models with human preferences using differentiable reward. However,they exhibit two primary challenges: (1) they rely on multistep denoising withgradient computation for reward scoring, which is computationally expensive,thus restricting optimization to only a few diffusion steps; (2) they oftenneed continuous offline adaptation of reward models in order to achieve desiredaesthetic quality, such as photorealism or precise lighting effects. To addressthe limitation of multistep denoising, we propose Direct-Align, a method thatpredefines a noise prior to effectively recover original images from any timesteps via interpolation, leveraging the equation that diffusion states areinterpolations between noise and target images, which effectively avoidsover-optimization in late timesteps. Furthermore, we introduce SemanticRelative Preference Optimization (SRPO), in which rewards are formulated astext-conditioned signals. This approach enables online adjustment of rewards inresponse to positive and negative prompt augmentation, thereby reducing thereliance on offline reward fine-tuning. By fine-tuning the FLUX model withoptimized denoising and online reward adjustment, we improve itshuman-evaluated realism and aesthetic quality by over 3x.</description><author>Xiangwei Shen, Zhimin Li, Zhantao Yang, Shiyi Zhang, Yingfang Zhang, Donghao Li, Chunyu Wang, Qinglin Lu, Yansong Tang</author><pubDate>Tue, 09 Sep 2025 16:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.06942v2</guid></item><item><title>Hybrid-Regularized Magnitude Pruning for Robust Federated Learning under Covariate Shift</title><link>http://arxiv.org/abs/2412.15010v2</link><description>Federated Learning offers a solution for decentralised model training,addressing the difficulties associated with distributed data and privacy inmachine learning. However, the fact of data heterogeneity in federated learningfrequently hinders the global model's generalisation, leading to lowperformance and adaptability to unseen data. This problem is particularlycritical for specialised applications such as medical imaging, where both thedata and the number of clients are limited. In this paper, we empiricallydemonstrate that inconsistencies in client-side training distributionssubstantially degrade the performance of federated learning models acrossmultiple benchmark datasets. We propose a novel FL framework using acombination of pruning and regularisation of clients' training to improve thesparsity, redundancy, and robustness of neural connections, and thereby theresilience to model aggregation. To address a relatively unexplored dimensionof data heterogeneity, we further introduce a novel benchmark dataset,CelebA-Gender, specifically designed to control for within-class distributionalshifts across clients based on attribute variations, thereby complementing thepredominant focus on inter-class imbalance in prior federated learningresearch. Comprehensive experiments on many datasets like CIFAR-10, MNIST, andthe newly introduced CelebA-Gender dataset demonstrate that our methodconsistently outperforms standard FL baselines, yielding more robust andgeneralizable models in heterogeneous settings.</description><author>Ozgu Goksu, Nicolas Pugeault</author><pubDate>Tue, 09 Sep 2025 16:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15010v2</guid></item><item><title>Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning</title><link>http://arxiv.org/abs/2509.07879v1</link><description>Active Membership Inference Test (aMINT) is a method designed to detectwhether given data were used during the training of machine learning models. InActive MINT, we propose a novel multitask learning process that involvestraining simultaneously two models: the original or Audited Model, and asecondary model, referred to as the MINT Model, responsible for identifying thedata used for training the Audited Model. This novel multi-task learningapproach has been designed to incorporate the auditability of the model as anoptimization objective during the training process of neural networks. Theproposed approach incorporates intermediate activation maps as inputs to theMINT layers, which are trained to enhance the detection of training data. Wepresent results using a wide range of neural networks, from lighterarchitectures such as MobileNet to more complex ones such as VisionTransformers, evaluated in 5 public benchmarks. Our proposed Active MINTachieves over 80% accuracy in detecting if given data was used for training,significantly outperforming previous approaches in the literature. Our aMINTand related methodological developments contribute to increasing transparencyin AI models, facilitating stronger safeguards in AI deployments to achieveproper security, privacy, and copyright protection.</description><author>Daniel DeAlcala, Aythami Morales, Julian Fierrez, Gonzalo Mancera, Ruben Tolosana, Javier Ortega-Garcia</author><pubDate>Tue, 09 Sep 2025 16:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07879v1</guid></item><item><title>Missing Fine Details in Images: Last Seen in High Frequencies</title><link>http://arxiv.org/abs/2509.05441v2</link><description>Latent generative models have shown remarkable progress in high-fidelityimage synthesis, typically using a two-stage training process that involvescompressing images into latent embeddings via learned tokenizers in the firststage. The quality of generation strongly depends on how expressive andwell-optimized these latent embeddings are. While various methods have beenproposed to learn effective latent representations, generated images often lackrealism, particularly in textured regions with sharp transitions, due to lossof fine details governed by high frequencies. We conduct a detailed frequencydecomposition of existing state-of-the-art (SOTA) latent tokenizers and showthat conventional objectives inherently prioritize low-frequencyreconstruction, often at the expense of high-frequency fidelity. Our analysisreveals these latent tokenizers exhibit a bias toward low-frequency informationduring optimization, leading to over-smoothed outputs and visual artifacts thatdiminish perceptual quality. To address this, we propose a wavelet-based,frequency-aware variational autoencoder (FA-VAE) framework that explicitlydecouples the optimization of low- and high-frequency components. Thisdecoupling enables improved reconstruction of fine textures while preservingglobal structure. Moreover, we integrate our frequency-preserving latentembeddings into a SOTA latent diffusion model, resulting in sharper and morerealistic image generation. Our approach bridges the fidelity gap in currentlatent tokenizers and emphasizes the importance of frequency-aware optimizationfor realistic image synthesis, with broader implications for applications incontent creation, neural rendering, and medical imaging.</description><author>Tejaswini Medi, Hsien-Yi Wang, Arianna Rampini, Margret Keuper</author><pubDate>Tue, 09 Sep 2025 15:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.05441v2</guid></item><item><title>Leveraging Support Vector Regression for Outcome Prediction in Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy</title><link>http://arxiv.org/abs/2509.07872v1</link><description>Personalized ultra-fractionated stereotactic adaptive radiotherapy (PULSAR)is a novel treatment that delivers radiation in pulses of protracted intervals.Accurate prediction of gross tumor volume (GTV) changes through regressionmodels has substantial prognostic value. This study aims to develop amulti-omics based support vector regression (SVR) model for predicting GTVchange. A retrospective cohort of 39 patients with 69 brain metastases wasanalyzed, based on radiomics (MRI images) and dosiomics (dose maps) features.Delta features were computed to capture relative changes between two timepoints. A feature selection pipeline using least absolute shrinkage andselection operator (Lasso) algorithm with weight- or frequency-based rankingcriterion was implemented. SVR models with various kernels were evaluated usingthe coefficient of determination (R2) and relative root mean square error(RRMSE). Five-fold cross-validation with 10 repeats was employed to mitigatethe limitation of small data size. Multi-omics models that integrate radiomics,dosiomics, and their delta counterparts outperform individual-omics models.Delta-radiomic features play a critical role in enhancing prediction accuracyrelative to features at single time points. The top-performing model achievesan R2 of 0.743 and an RRMSE of 0.022. The proposed multi-omics SVR model showspromising performance in predicting continuous change of GTV. It provides amore quantitative and personalized approach to assist patient selection andtreatment adjustment in PULSAR.</description><author>Yajun Yu, Steve Jiang, Robert Timmerman, Hao Peng</author><pubDate>Tue, 09 Sep 2025 15:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07872v1</guid></item><item><title>Are Humans as Brittle as Large Language Models?</title><link>http://arxiv.org/abs/2509.07869v1</link><description>The output of large language models (LLM) is unstable, due to bothnon-determinism of the decoding process as well as to prompt brittleness. Whilethe intrinsic non-determinism of LLM generation may mimic existing uncertaintyin human annotations through distributional shifts in outputs, it is largelyassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.This raises the question: do human annotators show similar sensitivity toinstruction changes? If so, should prompt brittleness in LLMs be consideredproblematic? One may alternatively hypothesize that prompt brittlenesscorrectly reflects human annotation variances. To fill this research gap, wesystematically compare the effects of prompt modifications on LLMs andidentical instruction modifications for human annotators, focusing on thequestion of whether humans are similarly sensitive to prompt perturbations. Tostudy this, we prompt both humans and LLMs for a set of text classificationtasks conditioned on prompt variations. Our findings indicate that both humansand LLMs exhibit increased brittleness in response to specific types of promptmodifications, particularly those involving the substitution of alternativelabel sets or label formats. However, the distribution of human judgments isless affected by typographical errors and reversed label order than that ofLLMs.</description><author>Jiahui Li, Sean Papay, Roman Klinger</author><pubDate>Tue, 09 Sep 2025 15:56:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07869v1</guid></item><item><title>CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models</title><link>http://arxiv.org/abs/2509.07867v1</link><description>Constraint Programming and its high-level modeling languages have long beenrecognized for their potential to achieve the holy grail of problem-solving.However, the complexity of modeling languages, the large number of globalconstraints, and the art of creating good models have often hinderednon-experts from choosing CP to solve their combinatorial problems. Whilegenerating an expert-level model from a natural-language description of aproblem would be the dream, we are not yet there. We propose a tutoring systemcalled CP-Model-Zoo, exploiting expert-written models accumulated through theyears. CP-Model-Zoo retrieves the closest source code model from a databasebased on a user's natural language description of a combinatorial problem. Itensures that expert-validated models are presented to the user whileeliminating the need for human data labeling. Our experiments show excellentaccuracy in retrieving the correct model based on a user-input description of aproblem simulated with different levels of expertise.</description><author>Augustin Crespin, Ioannis Kostis, H√©l√®ne Verhaeghe, Pierre Schaus</author><pubDate>Tue, 09 Sep 2025 15:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07867v1</guid></item><item><title>Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection</title><link>http://arxiv.org/abs/2502.13061v3</link><description>Hateful memes have become a significant concern on the Internet,necessitating robust automated detection systems. While Large Multimodal Models(LMMs) have shown promise in hateful meme detection, they face notablechallenges like sub-optimal performance and limited out-of-domaingeneralization capabilities. Recent studies further reveal the limitations ofboth supervised fine-tuning (SFT) and in-context learning when applied to LMMsin this setting. To address these issues, we propose a robust adaptationframework for hateful meme detection that enhances in-domain accuracy andcross-domain generalization while preserving the general vision-languagecapabilities of LMMs. Analysis reveals that our approach achieves improvedrobustness under adversarial attacks compared to SFT models. Experiments on sixmeme classification datasets show that our approach achieves state-of-the-artperformance, outperforming larger agentic systems. Moreover, our methodgenerates higher-quality rationales for explaining hateful content compared tostandard SFT, enhancing model interpretability. Code available athttps://github.com/JingbiaoMei/RGCL</description><author>Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, Bill Byrne</author><pubDate>Tue, 09 Sep 2025 15:54:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.13061v3</guid></item><item><title>D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics</title><link>http://arxiv.org/abs/2509.07864v1</link><description>Multimodal Large Language Models (MLLMs) achieve strong performance on taskslike image captioning and visual question answering, but remain prone tohallucinations, where generated text conflicts with the visual input. Priorwork links this partly to insufficient visual attention, but existingattention-based detectors and mitigation typically apply uniform adjustmentsacross layers and heads, obscuring where errors originate. In this paper, wefirst show these methods fail to accurately localize problematic layers. Then,we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flagsanomalous layers, and Image Attention Focus (IAF) which scores attention headswithin those layers. Analysis shows that LIAE pinpoints faulty layers and IAFreliably ranks heads that warrant correction. Guided by these signals, wepropose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), atask-agnostic, attention-guided method that dynamically localizes and correctserrors during inference with negligible overhead. Results show our D-LEAFdelivers a 53% relative improvement on standard captioning benchmarks, and onVQA both accuracy and F1-score improve by approximately 4%, substantiallysuppressing hallucinations while preserving efficiency.</description><author>Tiancheng Yang, Lin Zhang, Jiaye Lin, Guimin Hu, Di Wang, Lijie Hu</author><pubDate>Tue, 09 Sep 2025 15:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07864v1</guid></item><item><title>Hunyuan-MT Technical Report</title><link>http://arxiv.org/abs/2509.05209v2</link><description>In this report, we introduce Hunyuan-MT-7B, our first open-sourcemultilingual translation model, which supports bidirectional translation across33 major languages and places a special emphasis on translation betweenMandarin and several ethnic minority languages as well as dialects.Furthermore, to serve and address diverse translation scenarios and enhancemodel performance at test time, we introduce Hunyuan-MT-Chimera-7B, atranslation model inspired by the slow thinking mode. This model integratesmultiple outputs generated by the Hunyuan-MT-7B model under varying parametersettings, thereby achieving performance superior to that of conventionalslow-thinking models based on Chain-of-Thought (CoT). The development of ourmodels follows a holistic training process specifically engineered formultilingual translation, which begins with general and MT-orientedpre-training to build foundational capabilities, proceeds to SupervisedFine-Tuning (SFT) for task-specific adaptation, and culminates in advancedalignment through Reinforcement Learning (RL) and weak-to-strong RL. Throughcomprehensive experimentation, we demonstrate that both Hunyuan-MT-7B andHunyuan-MT-Chimera-7B significantly outperform all translation-specific modelsof comparable parameter size and most of the SOTA large models, particularly onthe task of translation between Mandarin and minority languages as well asdialects. In the WMT2025 shared task (General Machine Translation), our modelsdemonstrate state-of-the-art performance, ranking first in 30 out of 31language pairs. This result highlights the robustness of our models across adiverse linguistic spectrum, encompassing high-resource languages such asChinese, English, and Japanese, as well as low-resource languages includingCzech, Marathi, Estonian, and Icelandic.</description><author>Mao Zheng, Zheng Li, Bingxin Qu, Mingyang Song, Yang Du, Mingrui Sun, Di Wang</author><pubDate>Tue, 09 Sep 2025 15:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.05209v2</guid></item><item><title>SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs</title><link>http://arxiv.org/abs/2509.07858v1</link><description>Existing code large language models (LLMs) often rely on large-scaleinstruction data distilled from proprietary LLMs for fine-tuning, whichtypically incurs high costs. In this paper, we explore the potential ofsmall-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality codeinstruction data construction. We first observe that the data synthesiscapability of small-scale LLMs can be enhanced by training on a few superiordata synthesis samples from proprietary LLMs. Building on this, we propose anovel iterative self-distillation approach to bootstrap small-scale LLMs,transforming them into powerful synthesizers that reduce reliance onproprietary LLMs and minimize costs. Concretely, in each iteration, to obtaindiverse and high-quality self-distilled data, we design multi-checkpointsampling and multi-aspect scoring strategies for initial data selection.Furthermore, to identify the most influential samples, we introduce agradient-based influence estimation method for final data filtering. Based onthe code instruction datasets from the small-scale synthesizers, we developSCoder, a family of code generation models fine-tuned from DeepSeek-Coder.SCoder models achieve state-of-the-art code generation capabilities,demonstrating the effectiveness of our method.</description><author>Xinyu Zhang, Changzhi Zhou, Linmei Hu, Luhao Zhang, Xiancai Chen, Haomin Fu, Yang Yang, Mengdi Zhang</author><pubDate>Tue, 09 Sep 2025 15:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07858v1</guid></item><item><title>Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer</title><link>http://arxiv.org/abs/2508.10587v2</link><description>To bridge the temporal granularity gap in energy network design and operationbased on Energy System Models, resampling of time series is required. Whileconventional upsampling methods are computationally efficient, they oftenresult in significant information loss or increased noise. Advanced models suchas time series generation models, Super-Resolution models and imputation modelsshow potential, but also face fundamental challenges. The goal of time seriesgenerative models is to learn the distribution of the original data to generatehigh-resolution series with similar statistical characteristics. This is notentirely consistent with the definition of upsampling. Time seriesSuper-Resolution models or imputation models can degrade the accuracy ofupsampling because the input low-resolution time series are sparse and may haveinsufficient context. Moreover, such models usually rely on supervised learningparadigms. This presents a fundamental application paradox: their trainingrequires the high-resolution time series that is intrinsically absent inupsampling application scenarios. To address the mentioned upsampling issue,this paper introduces a new method utilizing Generative AdversarialTransformers (GATs), which can be trained without access to any ground-truthhigh-resolution data. Compared with conventional interpolation methods, theintroduced method can reduce the root mean square error (RMSE) of upsamplingtasks by 9%, and the accuracy of a model predictive control (MPC) applicationscenario is improved by 13%.</description><author>Xuanhao Mu, G√∂khan Demirel, Yuzhe Zhang, Jianlei Liu, Thorsten Schlachter, Veit Hagenmeyer</author><pubDate>Tue, 09 Sep 2025 15:37:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10587v2</guid></item><item><title>P3-SAM: Native 3D Part Segmentation</title><link>http://arxiv.org/abs/2509.06784v2</link><description>Segmenting 3D assets into their constituent parts is crucial for enhancing 3Dunderstanding, facilitating model reuse, and supporting various applicationssuch as part generation. However, current methods face limitations such as poorrobustness when dealing with complex objects and cannot fully automate theprocess. In this paper, we propose a native 3D point-promptable partsegmentation model termed P3-SAM, designed to fully automate the segmentationof any 3D objects into components. Inspired by SAM, P3-SAM consists of afeature extractor, multiple segmentation heads, and an IoU predictor, enablinginteractive segmentation for users. We also propose an algorithm toautomatically select and merge masks predicted by our model for part instancesegmentation. Our model is trained on a newly built dataset containing nearly3.7 million models with reasonable segmentation labels. Comparisons show thatour method achieves precise segmentation results and strong robustness on anycomplex objects, attaining state-of-the-art performance. Our code will bereleased soon.</description><author>Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, Chunchao Guo</author><pubDate>Tue, 09 Sep 2025 15:32:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.06784v2</guid></item><item><title>One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</title><link>http://arxiv.org/abs/2509.04444v2</link><description>Driven by the demand for spatial intelligence and holistic scene perception,omnidirectional images (ODIs), which provide a complete 360\textdegree{} fieldof view, are receiving growing attention across diverse applications such asvirtual reality, autonomous driving, and embodied robotics. Despite theirunique characteristics, ODIs exhibit remarkable differences from perspectiveimages in geometric projection, spatial distribution, and boundary continuity,making it challenging for direct domain adaption from perspective methods. Thissurvey reviews recent panoramic vision techniques with a particular emphasis onthe perspective-to-panorama adaptation. We first revisit the panoramic imagingpipeline and projection methods to build the prior knowledge required foranalyzing the structural disparities. Then, we summarize three challenges ofdomain adaptation: severe geometric distortions near the poles, non-uniformsampling in Equirectangular Projection (ERP), and periodic boundary continuity.Building on this, we cover 20+ representative tasks drawn from more than 300research papers in two dimensions. On one hand, we present a cross-methodanalysis of representative strategies for addressing panoramic specificchallenges across different tasks. On the other hand, we conduct a cross-taskcomparison and classify panoramic vision into four major categories: visualquality enhancement and assessment, visual understanding, multimodalunderstanding, and visual generation. In addition, we discuss open challengesand future directions in data, models, and applications that will drive theadvancement of panoramic vision research. We hope that our work can provide newinsight and forward looking perspectives to advance the development ofpanoramic vision technologies. Our project page ishttps://insta360-research-team.github.io/Survey-of-Panorama</description><author>Xin Lin, Xian Ge, Dizhe Zhang, Zhaoliang Wan, Xianshun Wang, Xiangtai Li, Wenjie Jiang, Bo Du, Dacheng Tao, Ming-Hsuan Yang, Lu Qi</author><pubDate>Tue, 09 Sep 2025 15:29:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.04444v2</guid></item><item><title>Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets</title><link>http://arxiv.org/abs/2509.07852v1</link><description>Accurate and timely mapping of burned areas is crucial for environmentalmonitoring, disaster management, and assessment of climate change. This studypresents a novel approach to automated burned area mapping using the AlphaEArthdataset combined with the Siamese U-Net deep learning architecture. TheAlphaEArth Dataset, comprising high-resolution optical and thermal infraredimagery with comprehensive ground-truth annotations, provides an unprecedentedresource for training robust burned area detection models. We trained our modelwith the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous USand evaluated it with 17 regions cross in Europe. Our experimental resultsdemonstrate that the proposed ensemble approach achieves superior performancewith an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the testdataset. The model successfully identifies burned areas across diverseecosystems with complex background, showing particular strength in detectingpartially burned vegetation and fire boundaries and its transferability andhigh generalization in burned area mapping. This research contributes to theadvancement of automated fire damage assessment and provides a scalablesolution for global burn area monitoring using the AlphaEarth dataset.</description><author>Seyd Teymoor Seydi</author><pubDate>Tue, 09 Sep 2025 15:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07852v1</guid></item><item><title>Addressing the Cold-Start Problem for Personalized Combination Drug Screening</title><link>http://arxiv.org/abs/2509.07850v1</link><description>Personalizing combination therapies in oncology requires navigating animmense space of possible drug and dose combinations, a task that remainslargely infeasible through exhaustive experimentation. Recent developments inpatient-derived models have enabled high-throughput ex vivo screening, but thenumber of feasible experiments is limited. Further, a tight therapeutic windowmakes gathering molecular profiling information (e.g. RNA-seq) impractical as ameans of guiding drug response prediction. This leads to a challengingcold-start problem: how do we select the most informative combinations to testearly, when no prior information about the patient is available? We propose astrategy that leverages a pretrained deep learning model built on historicaldrug response data. The model provides both embeddings for drug combinationsand dose-level importance scores, enabling a principled selection of initialexperiments. We combine clustering of drug embeddings to ensure functionaldiversity with a dose-weighting mechanism that prioritizes doses based on theirhistorical informativeness. Retrospective simulations on large-scale drugcombination datasets show that our method substantially improves initialscreening efficiency compared to baselines, offering a viable path for moreeffective early-phase decision-making in personalized combination drug screens.</description><author>Antoine de Mathelin, Christopher Tosh, Wesley Tansey</author><pubDate>Tue, 09 Sep 2025 15:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07850v1</guid></item><item><title>Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study</title><link>http://arxiv.org/abs/2509.07846v1</link><description>Large language models like ChatGPT are increasingly used in classrooms, butthey often provide outdated or fabricated information that can misleadstudents. Retrieval Augmented Generation (RAG) improves reliability of LLMs bygrounding responses in external resources. We investigate two accessible RAGparadigms, vector-based retrieval and graph-based retrieval to identify bestpractices for classroom question answering (QA). Existing comparative studiesfail to account for pedagogical factors such as educational disciplines,question types, and practical deployment costs. Using a novel dataset,EduScopeQA, of 3,176 questions across academic subjects, we measure performanceon various educational query types, from specific facts to broad thematicdiscussions. We also evaluate system alignment with a dataset of systematicallyaltered textbooks that contradict the LLM's latent knowledge. We find thatOpenAI Vector Search RAG (representing vector-based RAG) performs well as alow-cost generalist, especially for quick fact retrieval. On the other hand,GraphRAG Global excels at providing pedagogically rich answers to thematicqueries, and GraphRAG Local achieves the highest accuracy with the dense,altered textbooks when corpus integrity is critical. Accounting for the 10-20xhigher resource usage of GraphRAG (representing graph-based RAG), we show thata dynamic branching framework that routes queries to the optimal retrievalmethod boosts fidelity and efficiency. These insights provide actionableguidelines for educators and system designers to integrate RAG-augmented LLMsinto learning environments effectively.</description><author>Amay Jain, Liu Cui, Si Chen</author><pubDate>Tue, 09 Sep 2025 15:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07846v1</guid></item><item><title>Predicting person-level injury severity using crash narratives: A balanced approach with roadway classification and natural language process techniques</title><link>http://arxiv.org/abs/2509.07845v1</link><description>Predicting injuries and fatalities in traffic crashes plays a critical rolein enhancing road safety, improving emergency response, and guiding publichealth interventions. This study investigates the added value of unstructuredcrash narratives (written by police officers at the scene) when combined withstructured crash data to predict injury severity. Two widely used NaturalLanguage Processing (NLP) techniques, Term Frequency-Inverse Document Frequency(TF-IDF) and Word2Vec, were employed to extract semantic meaning from thenarratives, and their effectiveness was compared. To address the challenge ofclass imbalance, a K-Nearest Neighbors-based oversampling method was applied tothe training data prior to modeling. The dataset consists of crash records fromKentucky spanning 2019 to 2023. To account for roadway heterogeneity, threeroad classification schemes were used: (1) eight detailed functional classes(e.g., Urban Two-Lane, Rural Interstate, Urban Multilane Divided), (2) fourbroader paired categories (e.g., Urban vs. Rural, Freeway vs. Non-Freeway), and(3) a unified dataset without classification. A total of 102 machine learningmodels were developed by combining structured features and narrative-basedfeatures using the two NLP techniques alongside three ensemble algorithms:XGBoost, Random Forest, and AdaBoost. Results demonstrate that modelsincorporating narrative data consistently outperform those relying solely onstructured data. Among all combinations, TF-IDF coupled with XGBoost yieldedthe most accurate predictions in most subgroups. The findings highlight thepower of integrating textual and structured crash information to enhanceperson-level injury prediction. This work offers a practical and adaptableframework for transportation safety professionals to improve crash severitymodeling, guide policy decisions, and design more effective countermeasures.</description><author>Mohammad Zana Majidi, Sajjad Karimi, Teng Wang, Robert Kluger, Reginald Souleyrette</author><pubDate>Tue, 09 Sep 2025 15:22:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07845v1</guid></item><item><title>zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs</title><link>http://arxiv.org/abs/2508.21393v2</link><description>Fine-tuning large language models (LLMs) is crucial for adapting them tospecific tasks, yet it remains computationally demanding and raises concernsabout correctness and privacy, particularly in untrusted environments. Althoughparameter-efficient methods like Low-Rank Adaptation (LoRA) significantlyreduce resource requirements, ensuring the security and verifiability offine-tuning under zero-knowledge constraints remains an unresolved challenge.To address this, we introduce zkLoRA, the first framework to integrate LoRAfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security andcorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookuparguments, sumcheck protocols, and polynomial commitments -- to verify botharithmetic and non-arithmetic operations in Transformer-based architectures.The framework provides end-to-end verifiability for forward propagation,backward propagation, and parameter updates during LoRA fine-tuning, whilesafeguarding the privacy of model parameters and training data. LeveragingGPU-based implementations, zkLoRA demonstrates practicality and efficiencythrough experimental validation on open-source LLMs like LLaMA, scaling up to13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,zkLoRA bridges a critical gap, enabling secure and trustworthy deployment ofLLMs in sensitive or untrusted environments.</description><author>Guofu Liao, Taotao Wang, Shengli Zhang, Jiqun Zhang, Shi Long, Dacheng Tao</author><pubDate>Tue, 09 Sep 2025 15:20:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21393v2</guid></item><item><title>Modeling the Diachronic Evolution of Legal Norms: An LRMoo-Based, Component-Level, Event-Centric Approach to Legal Knowledge Graphs</title><link>http://arxiv.org/abs/2506.07853v3</link><description>Effectively representing legal norms for automated processing is a criticalchallenge, particularly in tracking the temporal evolution of theirhierarchical components. While foundational conceptual frameworks like IFLALRMoo provide a generic toolkit for bibliographic data, and encoding standardslike Akoma Ntoso offer a robust syntax for legal documents, a dedicated, formalmodeling pattern for granular, component-level versioning is still required.This limitation hinders the deterministic point-intime reconstruction of legaltexts, a fundamental capability for reliable Legal Tech and AI applications.This paper proposes a structured, temporal modeling pattern grounded in theLRMoo ontology to address this need. Our approach models the evolution of alegal norm as a diachronic chain of F2 Expressions. We introduce a keydistinction between a language-agnostic Temporal Version (TV)-a semanticsnapshot of the norm's structure-and its concrete monolingual realizations, theLanguage Versions (LV). Both are modeled as F2 Expressions linked by thecanonical R76 is derivative of property. This paradigm is applied recursivelyto the legal text's internal structure, representing it as a parallel hierarchyof abstract Component Works (F1) and their versioned Component Expressions(F2). Furthermore, we formalize the legislative amendment process using the F28Expression Creation event, allowing changes to be traced from an amending actto its precise effect on the amended norm. Using the Brazilian FederalConstitution as a case study, we demonstrate how this event-centricarchitecture enables the precise, deterministic retrieval and reconstruction ofany part of a legal text as it existed on a specific date. The model provides arobust foundation for building verifiable knowledge graphs and advanced AItools, overcoming the limitations of current generative models.</description><author>Hudson de Martim</author><pubDate>Tue, 09 Sep 2025 15:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.07853v3</guid></item><item><title>ASP-FZN: A Translation-based Constraint Answer Set Solver</title><link>http://arxiv.org/abs/2507.22774v2</link><description>We present the solver asp-fzn for Constraint Answer Set Programming (CASP),which extends ASP with linear constraints. Our approach is based on translatingCASP programs into the solver-independent FlatZinc language that supportsseveral Constraint Programming and Integer Programming backend solvers. Oursolver supports a rich language of linear constraints, including some commonglobal constraints. As for evaluation, we show that asp-fzn is competitive withstate-of-the-art ASP solvers on benchmarks taken from past ASP competitions.Furthermore, we evaluate it on several CASP problems from the literature andcompare its performance with clingcon, which is a prominent CASP solver thatsupports most of the asp-fzn language. The performance of asp-fzn is verypromising as it is already competitive on plain ASP and even outperformsclingcon on some CASP benchmarks.</description><author>Thomas Eiter, Tobias Geibinger, Tobias Kaminski, Nysret Musliu, Johannes Oetsch</author><pubDate>Tue, 09 Sep 2025 15:10:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22774v2</guid></item><item><title>Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost</title><link>http://arxiv.org/abs/2509.07829v1</link><description>Literary translation has recently gained attention as a distinct and complextask in machine translation research. However, the translation by small openmodels remains an open problem. We contribute to this ongoing research byintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework fordataset creation, fine tuning, and evaluation in English-Romanian literarytranslations, centred on the creation and open release of both a compact, finetuned language model (TF2-12B) and large scale synthetic parallel datasets(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), thelargest collection of synthetic English fables to date, we address the need forrich, high quality literary datasets in low resource languages such asRomanian. Our pipeline first generates 15k high quality Romanian referencesfrom the TF1 pool using a high performing LLM. We then apply a two stage finetuning process to a 12B parameter open weight model: (i) instruction tuning tocapture genre specific narrative style, and (ii) adapter compression forefficient deployment. Evaluation combines corpus level BLEU and a fivedimension LLM based rubric (accuracy, fluency, coherence, style, culturaladaptation) to provide a nuanced assessment of translation quality. Resultsshow that our fine tuned model achieves fluency and adequacy competitive withtop performing large proprietary models, while being open, accessible, andsignificantly more cost effective. Alongside the fine tuned model and bothdatasets, we publicly release all scripts and evaluation prompts. TF2 thusprovides an end-to-end, reproducible pipeline for research on cost efficienttranslation, cross lingual narrative generation, and the broad adoption of openmodels for culturally significant literary content in low resource settings.</description><author>Mihai Nadas, Laura Diosan, Andreea Tomescu, Andrei Piscoran</author><pubDate>Tue, 09 Sep 2025 15:07:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07829v1</guid></item><item><title>Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model</title><link>http://arxiv.org/abs/2509.07825v1</link><description>3D object segmentation with Large Language Models (LLMs) has become aprevailing paradigm due to its broad semantics, task flexibility, and stronggeneralization. However, this paradigm is hindered by representationmisalignment: LLMs process high-level semantic tokens, whereas 3D point cloudsconvey only dense geometric structures. In prior methods, misalignment limitsboth input and output. At the input stage, dense point patches require heavypre-alignment, weakening object-level semantics and confusing similardistractors. At the output stage, predictions depend only on dense featureswithout explicit geometric cues, leading to a loss of fine-grained accuracy. Toaddress these limitations, we present the Point Linguist Model (PLM), a generalframework that bridges the representation gap between LLMs and dense 3D pointclouds without requiring large-scale pre-alignment between 3D-text or3D-images. Specifically, we introduce Object-centric DiscriminativeRepresentation (OcDR), which learns object-centric tokens that capture targetsemantics and scene relations under a hard negative-aware training objective.This mitigates the misalignment between LLM tokens and 3D points, enhancesresilience to distractors, and facilitates semantic-level reasoning withinLLMs. For accurate segmentation, we introduce the Geometric ReactivationDecoder (GRD), which predicts masks by combining OcDR tokens carryingLLM-inferred geometry with corresponding dense features, preservingcomprehensive dense features throughout the pipeline. Extensive experimentsshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gainsacross 7 benchmarks spanning 4 different tasks, demonstrating the effectivenessof comprehensive object-centric reasoning for robust 3D understanding.</description><author>Zhuoxu Huang, Mingqi Gao, Jungong Han</author><pubDate>Tue, 09 Sep 2025 15:01:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07825v1</guid></item><item><title>Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach</title><link>http://arxiv.org/abs/2509.07820v1</link><description>The rise of large reasoning language models (LRLMs) has unlocked newpotential for solving complex tasks. These models operate with a thinkingbudget, that is, a predefined number of reasoning tokens used to arrive at asolution. We propose a novel approach, inspired by the generator/discriminatorframework in generative adversarial networks, in which a critic modelperiodically probes its own reasoning to assess whether it has reached aconfident conclusion. If not, reasoning continues until a target certaintythreshold is met. This mechanism adaptively balances efficiency and reliabilityby allowing early termination when confidence is high, while encouragingfurther reasoning when uncertainty persists. Through experiments on theAIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)improves baseline accuracy while reducing token usage. Importantly, extendedmulti-seed evaluations over 64 runs demonstrate that CGR is stable, reducingvariance across seeds and improving exam-like performance under penalty-basedgrading. Additionally, our token savings analysis shows that CGR can eliminatemillions of tokens in aggregate, with tunable trade-offs between certaintythresholds and efficiency. Together, these findings highlight certainty as apowerful signal for reasoning sufficiency. By integrating confidence into thereasoning process, CGR makes large reasoning language models more adaptive,trustworthy, and resource efficient, paving the way for practical deployment indomains where both accuracy and computational cost matter.</description><author>Jo√£o Paulo Nogueira, Wentao Sun, Alonso Silva, Laith Zumot</author><pubDate>Tue, 09 Sep 2025 14:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07820v1</guid></item><item><title>Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems</title><link>http://arxiv.org/abs/2509.07817v1</link><description>Textual response generation is pivotal for multimodal \mbox{task-oriented}dialog systems, which aims to generate proper textual responses based on themultimodal context. While existing efforts have demonstrated remarkableprogress, there still exist the following limitations: 1) \textit{neglect ofunstructured review knowledge} and 2) \textit{underutilization of largelanguage models (LLMs)}. Inspired by this, we aim to fully utilize dualknowledge (\textit{i.e., } structured attribute and unstructured reviewknowledge) with LLMs to promote textual response generation in multimodaltask-oriented dialog systems. However, this task is non-trivial due to two keychallenges: 1) \textit{dynamic knowledge type selection} and 2)\textit{intention-response decoupling}. To address these challenges, we proposea novel dual knowledge-enhanced two-stage reasoner by adapting LLMs formultimodal dialog systems (named DK2R). To be specific, DK2R first extractsboth structured attribute and unstructured review knowledge from externalknowledge base given the dialog context. Thereafter, DK2R uses an LLM toevaluate each knowledge type's utility by analyzing LLM-generated provisionalprobe responses. Moreover, DK2R separately summarizes the intention-orientedkey clues via dedicated reasoning, which are further used as auxiliary signalsto enhance LLM-based textual response generation. Extensive experimentsconducted on a public dataset verify the superiority of DK2R. We have releasedthe codes and parameters.</description><author>Xiaolin Chen, Xuemeng Song, Haokun Wen, Weili Guan, Xiangyu Zhao, Liqiang Nie</author><pubDate>Tue, 09 Sep 2025 14:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07817v1</guid></item><item><title>Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models</title><link>http://arxiv.org/abs/2509.07813v1</link><description>This study applies a range of forecasting techniques,including ARIMA,Prophet, Long Short Term Memory networks (LSTM), Temporal ConvolutionalNetworks (TCN), and XGBoost, to model and predict Russian equipment lossesduring the ongoing war in Ukraine. Drawing on daily and monthly open-sourceintelligence (OSINT) data from WarSpotting, we aim to assess trends inattrition, evaluate model performance, and estimate future loss patternsthrough the end of 2025. Our findings show that deep learning models,particularly TCN and LSTM, produce stable and consistent forecasts, especiallyunder conditions of high temporal granularity. By comparing different modelarchitectures and input structures, this study highlights the importance ofensemble forecasting in conflict modeling, and the value of publicly availableOSINT data in quantifying material degradation over time.</description><author>Jonathan Teagan</author><pubDate>Tue, 09 Sep 2025 14:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07813v1</guid></item><item><title>SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting</title><link>http://arxiv.org/abs/2509.07809v1</link><description>3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3Dscene representations from sets of multi-view images. However, inpaintingmissing regions, whether due to occlusion or scene editing, remains achallenging task, often leading to blurry details, artifacts, and inconsistentgeometry. In this work, we introduce SplatFill, a novel depth-guided approachfor 3DGS scene inpainting that achieves state-of-the-art perceptual quality andimproved efficiency. Our method combines two key ideas: (1) joint depth-basedand object-based supervision to ensure inpainted Gaussians are accuratelyplaced in 3D space and aligned with surrounding geometry, and (2) we propose aconsistency-aware refinement scheme that selectively identifies and correctsinconsistent regions without disrupting the rest of the scene. Evaluations onthe SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existingNeRF-based and 3DGS-based inpainting methods in visual fidelity but alsoreduces training time by 24.5%. Qualitative results show our method deliverssharper details, fewer artifacts, and greater coherence across challengingviewpoints.</description><author>Mahtab Dahaghin, Milind G. Padalkar, Matteo Toso, Alessio Del Bue</author><pubDate>Tue, 09 Sep 2025 14:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07809v1</guid></item><item><title>Feature Understanding and Sparsity Enhancement via 2-Layered kernel machines (2L-FUSE)</title><link>http://arxiv.org/abs/2509.07806v1</link><description>We propose a novel sparsity enhancement strategy for regression tasks, basedon learning a data-adaptive kernel metric, i.e., a shape matrix, through2-Layered kernel machines. The resulting shape matrix, which defines aMahalanobis-type deformation of the input space, is then factorized via aneigen-decomposition, allowing us to identify the most informative directions inthe space of features. This data-driven approach provides a flexible,interpretable and accurate feature reduction scheme. Numerical experiments onsynthetic and applications to real datasets of geomagnetic storms demonstratethat our approach achieves minimal yet highly informative feature sets withoutlosing predictive performance.</description><author>Fabiana Camattari, Sabrina Guastavino, Francesco Marchetti, Emma Perracchione</author><pubDate>Tue, 09 Sep 2025 14:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07806v1</guid></item><item><title>TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review</title><link>http://arxiv.org/abs/2506.07642v3</link><description>While Large Language Models (LLMs) have shown significant potential inassisting peer review, current methods often struggle to generate thorough andinsightful reviews while maintaining efficiency. In this paper, we proposeTreeReview, a novel framework that models paper review as a hierarchical andbidirectional question-answering process. TreeReview first constructs a tree ofreview questions by recursively decomposing high-level questions intofine-grained sub-questions and then resolves the question tree by iterativelyaggregating answers from leaf to root to get the final review. Crucially, weincorporate a dynamic question expansion mechanism to enable deeper probing bygenerating follow-up questions when needed. We construct a benchmark derivedfrom ICLR and NeurIPS venues to evaluate our method on full review generationand actionable feedback comments generation tasks. Experimental results of bothLLM-based and human evaluation show that TreeReview outperforms strongbaselines in providing comprehensive, in-depth, and expert-aligned reviewfeedback, while reducing LLM token usage by up to 80% compared tocomputationally intensive approaches. Our code and benchmark dataset areavailable at https://github.com/YuanChang98/tree-review.</description><author>Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Hayden Kwok-Hay So, Zhijiang Guo, Liya Zhu, Ngai Wong</author><pubDate>Tue, 09 Sep 2025 14:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.07642v3</guid></item><item><title>Bootstrapping Task Spaces for Self-Improvement</title><link>http://arxiv.org/abs/2509.04575v2</link><description>Progress in many task domains emerges from repeated revisions to previoussolution attempts. Training agents that can reliably self-improve over suchsequences at inference-time is a natural target for reinforcement learning(RL), yet the naive approach assumes a fixed maximum iteration depth, which canbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a familyof autocurriculum RL methods that directly exploits the recurrent structure ofself-improvement tasks to train LLMs to perform multi-step self-improvement atinference-time while only training on the most informative single-stepiterations. ExIt grows a task space by selectively sampling the mostinformative intermediate, partial histories encountered during an episode forcontinued iteration, treating these starting points as new self-iteration taskinstances to train a self-improvement policy. ExIt can further pair withexplicit exploration mechanisms to sustain greater task diversity. Acrossseveral domains, encompassing competition math, multi-turn tool-use, andmachine learning engineering, we demonstrate that ExIt strategies, startingfrom either a single or many task instances, can produce policies exhibitingstrong inference-time self-improvement on held-out task instances, and theability to iterate towards higher performance over a step budget extendingbeyond the average iteration depth encountered during training.</description><author>Minqi Jiang, Andrei Lupu, Yoram Bachrach</author><pubDate>Tue, 09 Sep 2025 14:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.04575v2</guid></item><item><title>UPLex: Fine-Grained Personality Control in Large Language Models via Unsupervised Lexical Modulation</title><link>http://arxiv.org/abs/2310.16582v3</link><description>Personality is a crucial factor that shapes human communication patterns,thereby regulating the personalities of large language models (LLMs) holdssignificant potential in enhancing their user experiences. Previous approacheseither relied on fine-tuning LLMs on specific corpora or required manuallycrafted prompts to evoke specific personalities from LLMs. However, the formeris inefficient and costly, while the latter cannot precisely manipulatepersonality traits at a fine-grained level. To address these challenges, wepropose UPLex, a method that uses an Unsupervisedly-Built Personalized Lexicon(UPL) during the decoding phase to manipulate LLM's personality traits. UPL canbe constructed from a newly built situational judgment test dataset in anunsupervised fashion, and used to modulate the personality expression of LLMsby dynamically altering their predicted probability of upcoming words in apluggable fashion. Extensive experimentation demonstrates the remarkableeffectiveness and pluggability of our method for fine-grained manipulation ofLLMs' personalities.</description><author>Tianlong Li, Wenhao Liu, Muling Wu, Shihan Dou, Zhenghua Wang, Changze Lv, Xiaohua Wang, Xiaoqing Zheng, Xuanjing Huang</author><pubDate>Tue, 09 Sep 2025 14:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16582v3</guid></item><item><title>SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP</title><link>http://arxiv.org/abs/2509.07801v1</link><description>Structured information extraction from scientific literature is crucial forcapturing core concepts and emerging trends in specialized fields. Whileexisting datasets aid model development, most focus on specific publicationsections due to domain complexity and the high cost of annotating scientifictexts. To address this limitation, we introduce SciNLP - a specializedbenchmark for full-text entity and relation extraction in the Natural LanguageProcessing (NLP) domain. The dataset comprises 60 manually annotated full-textNLP publications, covering 7,072 entities and 1,826 relations. Compared toexisting research, SciNLP is the first dataset providing full-text annotationsof entities and their relationships in the NLP domain. To validate theeffectiveness of SciNLP, we conducted comparative experiments with similardatasets and evaluated the performance of state-of-the-art supervised models onthis dataset. Results reveal varying extraction capabilities of existing modelsacross academic texts of different lengths. Cross-comparisons with existingdatasets show that SciNLP achieves significant performance improvements oncertain baseline models. Using models trained on SciNLP, we implementedautomatic construction of a fine-grained knowledge graph for the NLP domain.Our KG has an average node degree of 3.2 per entity, indicating rich semantictopological information that enhances downstream applications. The dataset ispublicly available at https://github.com/AKADDC/SciNLP.</description><author>Decheng Duan, Yingyi Zhang, Jitong Peng, Chengzhi Zhang</author><pubDate>Tue, 09 Sep 2025 14:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07801v1</guid></item><item><title>Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI Using a Sparse Coordinate Loss</title><link>http://arxiv.org/abs/2509.07798v1</link><description>Acquiring images in high resolution is often a challenging task. Especiallyin the medical sector, image quality has to be balanced with acquisition timeand patient comfort. To strike a compromise between scan time and quality forMagnetic Resonance (MR) imaging, two anisotropic scans with differentlow-resolution (LR) orientations can be acquired. Typically, LR scans areanalyzed individually by radiologists, which is time consuming and can lead toinaccurate interpretation. To tackle this, we propose a novel approach forfusing two orthogonal anisotropic LR MR images to reconstruct anatomicaldetails in a unified representation. Our multi-view neural network is trainedin a self-supervised manner, without requiring corresponding high-resolution(HR) data. To optimize the model, we introduce a sparse coordinate-based loss,enabling the integration of LR images with arbitrary scaling. We evaluate ourmethod on MR images from two independent cohorts. Our results demonstratecomparable or even improved super-resolution (SR) performance compared tostate-of-the-art (SOTA) self-supervised SR methods for different upsamplingscales. By combining a patient-agnostic offline and a patient-specific onlinephase, we achieve a substantial speed-up of up to ten times forpatient-specific reconstruction while achieving similar or better SR quality.Code is available at https://github.com/MajaSchle/tripleSR.</description><author>Maja Schlereth, Moritz Schillinger, Katharina Breininger</author><pubDate>Tue, 09 Sep 2025 14:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07798v1</guid></item><item><title>EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control</title><link>http://arxiv.org/abs/2508.21112v3</link><description>The human ability to seamlessly perform multimodal reasoning and physicalinteraction in the open world is a core goal for general-purpose embodiedintelligent systems. Recent vision-language-action (VLA) models, which areco-trained on large-scale robot and visual-text data, have demonstrated notableprogress in general robot control. However, they still fail to achievehuman-level flexibility in interleaved reasoning and interaction. In this work,introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 isa unified embodied foundation model that achieves superior performance inmultimodal embodied reasoning and robot control through interleavedvision-text-action pre-training. The development of EO-1 is based on two keypillars: (i) a unified architecture that processes multimodal inputsindiscriminately (image, text, video, and action), and (ii) a massive,high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which containsover 1.5 million samples with emphasis on interleaved vision-text-actioncomprehension. EO-1 is trained through synergies between auto-regressivedecoding and flow matching denoising on EO-Data1.5M, enabling seamless robotaction generation and multimodal embodied reasoning. Extensive experimentsdemonstrate the effectiveness of interleaved vision-text-action learning foropen-world understanding and generalization, validated through a variety oflong-horizon, dexterous manipulation tasks across multiple embodiments. Thispaper details the architecture of EO-1, the data construction strategy ofEO-Data1.5M, and the training methodology, offering valuable insights fordeveloping advanced embodied foundation models.</description><author>Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang</author><pubDate>Tue, 09 Sep 2025 14:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21112v3</guid></item><item><title>Enhanced SegNet with Integrated Grad-CAM for Interpretable Retinal Layer Segmentation in OCT Images</title><link>http://arxiv.org/abs/2509.07795v1</link><description>Optical Coherence Tomography (OCT) is essential for diagnosing conditionssuch as glaucoma, diabetic retinopathy, and age-related macular degeneration.Accurate retinal layer segmentation enables quantitative biomarkers criticalfor clinical decision-making, but manual segmentation is time-consuming andvariable, while conventional deep learning models often lack interpretability.This work proposes an improved SegNet-based deep learning framework forautomated and interpretable retinal layer segmentation. Architecturalinnovations, including modified pooling strategies, enhance feature extractionfrom noisy OCT images, while a hybrid loss function combining categoricalcross-entropy and Dice loss improves performance for thin and imbalancedretinal layers. Gradient-weighted Class Activation Mapping (Grad-CAM) isintegrated to provide visual explanations, allowing clinical validation ofmodel decisions. Trained and validated on the Duke OCT dataset, the frameworkachieved 95.77% validation accuracy, a Dice coefficient of 0.9446, and aJaccard Index (IoU) of 0.8951. Class-wise results confirmed robust performanceacross most layers, with challenges remaining for thinner boundaries. Grad-CAMvisualizations highlighted anatomically relevant regions, aligning segmentationwith clinical biomarkers and improving transparency. By combining architecturalimprovements, a customized hybrid loss, and explainable AI, this study deliversa high-performing SegNet-based framework that bridges the gap between accuracyand interpretability. The approach offers strong potential for standardizingOCT analysis, enhancing diagnostic efficiency, and fostering clinical trust inAI-driven ophthalmic tools.</description><author>S M Asiful Islam Saky, Ugyen Tshering</author><pubDate>Tue, 09 Sep 2025 14:31:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07795v1</guid></item><item><title>Individual utilities of life satisfaction reveal inequality aversion unrelated to political alignment</title><link>http://arxiv.org/abs/2509.07793v1</link><description>How should well-being be prioritised in society, and what trade-offs arepeople willing to make between fairness and personal well-being? We investigatethese questions using a stated preference experiment with a nationallyrepresentative UK sample (n = 300), in which participants evaluated lifesatisfaction outcomes for both themselves and others under conditions ofuncertainty. Individual-level utility functions were estimated using anExpected Utility Maximisation (EUM) framework and tested for sensitivity to theoverweighting of small probabilities, as characterised by Cumulative ProspectTheory (CPT). A majority of participants displayed concave (risk-averse)utility curves and showed stronger aversion to inequality in societal lifesatisfaction outcomes than to personal risk. These preferences were unrelatedto political alignment, suggesting a shared normative stance on fairness inwell-being that cuts across ideological boundaries. The results challenge useof average life satisfaction as a policy metric, and support the development ofnonlinear utility-based alternatives that more accurately reflect collectivehuman values. Implications for public policy, well-being measurement, and thedesign of value-aligned AI systems are discussed.</description><author>Crispin Cooper, Ana Friedrich, Tommaso Reggiani, Wouter Poortinga</author><pubDate>Tue, 09 Sep 2025 14:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07793v1</guid></item><item><title>Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference</title><link>http://arxiv.org/abs/2412.18934v2</link><description>With the continuous advancement in the performance of large language models(LLMs), their demand for computational resources and memory has significantlyincreased, which poses major challenges for efficient inference onconsumer-grade devices and legacy servers. These devices typically featurerelatively weaker GPUs and stronger CPUs. Although techniques such as parameteroffloading and partial offloading can alleviate GPU memory pressure to someextent, their effectiveness is limited due to communication latency andsuboptimal hardware resource utilization. To address this issue, we proposeDovetail, a lossless inference acceleration method that leverages thecomplementary characteristics of heterogeneous devices and the advantages ofspeculative decoding. Dovetail deploys a draft model on the GPU to performpreliminary predictions, while a target model running on the CPU validatesthese outputs. By reducing the granularity of data transfer, Dovetailsignificantly minimizes communication overhead. To further improve efficiency,we optimize the draft model specifically for heterogeneous hardwareenvironments by reducing the number of draft tokens to lower parallelverification latency, increasing model depth to enhance predictivecapabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism toimprove the integration of feature and embedding information. We conductcomprehensive evaluations of Dovetail across various consumer-grade GPUs,covering multiple tasks and mainstream models. Experimental results on 13Bmodels demonstrate that Dovetail achieves inference speedups ranging from 1.79xto 10.1x across different devices, while maintaining consistency and stabilityin the distribution of generated texts.</description><author>Libo Zhang, Zhaoning Zhang, Baizhou Xu, Rui Li, Zhiliang Tian, Songzhu Mei, Dongsheng Li</author><pubDate>Tue, 09 Sep 2025 14:27:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18934v2</guid></item><item><title>Closing the Gap between TD Learning and Supervised Learning with $Q$-Conditioned Maximization</title><link>http://arxiv.org/abs/2506.00795v2</link><description>Recently, supervised learning (SL) methodology has emerged as an effectiveapproach for offline reinforcement learning (RL) due to their simplicity,stability, and efficiency. However, recent studies show that SL methods lackthe trajectory stitching capability, typically associated with temporaldifference (TD)-based approaches. A question naturally surfaces: \textit{Howcan we endow SL methods with stitching capability and close its performance gapwith TD learning?} To answer this question, we introduce $Q$-conditionedmaximization supervised learning for offline goal-conditioned RL, whichenhances SL with the stitching capability through $Q$-conditioned policy and$Q$-conditioned maximization. Concretely, we propose\textbf{G}oal-\textbf{C}onditioned \textbf{\textit{Rein}}forced\textbf{S}upervised \textbf{L}earning (\textbf{GC\textit{Rein}SL}), whichconsists of (1) estimating the $Q$-function by Normalizing Flows from theoffline dataset and (2) finding the maximum $Q$-value within the data supportby integrating $Q$-function maximization with Expectile Regression. Ininference time, our policy chooses optimal actions based on such a maximum$Q$-value. Experimental results from stitching evaluations on offline RLdatasets demonstrate that our method outperforms prior SL approaches withstitching capabilities and goal data augmentation techniques.</description><author>Xing Lei, Zifeng Zhuang, Shentao Yang, Sheng Xu, Yunhao Luo, Fei Shen, Wenyan Yang, Xuetao Zhang, Donglin Wang</author><pubDate>Tue, 09 Sep 2025 14:25:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.00795v2</guid></item><item><title>MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering</title><link>http://arxiv.org/abs/2502.18993v2</link><description>Multi-entity question answering (MEQA) represents significant challenges forlarge language models (LLM) and retrieval-augmented generation (RAG) systems,which frequently struggle to consolidate scattered information across diversedocuments. While existing methods excel at single-document comprehension, theyoften struggle with cross-document aggregation, particularly when resolvingentity-dense questions like "What is the distribution of ACM Fellows amongvarious fields of study?", which require integrating entity-centric insightsfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, weintroduce MEBench, a novel multi-document, multi-entity benchmark designed tosystematically evaluate LLMs' capacity to retrieve, consolidate, and reasonover fragmented information. Our benchmark comprises 4,780 questions which aresystematically categorized into three primary categories, further divided intoeight distinct types, ensuring broad coverage of real-world multi-entityreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,Llama-3) and RAG pipelines reveal critical limitations: even advanced modelsachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importanceof completeness and factual precision of information extraction in MEQA tasks,using Entity-Attributed F1 (EA-F1) metric for granular evaluation ofentity-level correctness and attribution validity. MEBench not only highlightssystemic weaknesses in current LLM frameworks but also provides a foundationfor advancing robust, entity-aware QA architectures.</description><author>Teng Lin, Yuyu Luo, Nan Tang</author><pubDate>Tue, 09 Sep 2025 14:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.18993v2</guid></item><item><title>Nuclear Data Adjustment for Nonlinear Applications in the OECD/NEA WPNCS SG14 Benchmark -- A Bayesian Inverse UQ-based Approach for Data Assimilation</title><link>http://arxiv.org/abs/2509.07790v1</link><description>The Organization for Economic Cooperation and Development (OECD) WorkingParty on Nuclear Criticality Safety (WPNCS) proposed a benchmark exercise toassess the performance of current nuclear data adjustment techniques applied tononlinear applications and experiments with low correlation to applications.This work introduces Bayesian Inverse Uncertainty Quantification (IUQ) as amethod for nuclear data adjustments in this benchmark, and compares IUQ to themore traditional methods of Generalized Linear Least Squares (GLLS) and MonteCarlo Bayes (MOCABA). Posterior predictions from IUQ showed agreement with GLLSand MOCABA for linear applications. When comparing GLLS, MOCABA, and IUQposterior predictions to computed model responses using adjusted parameters, weobserve that GLLS predictions fail to replicate computed response distributionsfor nonlinear applications, while MOCABA shows near agreement, and IUQ usescomputed model responses directly. We also discuss observations on whyexperiments with low correlation to applications can be informative to nucleardata adjustments and identify some properties useful in selecting experimentsfor inclusion in nuclear data adjustment. Performance in this benchmarkindicates potential for Bayesian IUQ in nuclear data adjustments.</description><author>Christopher Brady, Xu Wu</author><pubDate>Tue, 09 Sep 2025 14:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07790v1</guid></item><item><title>Closed-Loop Unsupervised Representation Disentanglement with $Œ≤$-VAE Distillation and Diffusion Probabilistic Feedback</title><link>http://arxiv.org/abs/2402.02346v2</link><description>Representation disentanglement may help AI fundamentally understand the realworld and thus benefit both discrimination and generation tasks. It currentlyhas at least three unresolved core issues: (i) heavy reliance on labelannotation and synthetic data -- causing poor generalization on naturalscenarios; (ii) heuristic/hand-craft disentangling constraints make it hard toadaptively achieve an optimal training trade-off; (iii) lacking reasonableevaluation metric, especially for the real label-free data. To address thesechallenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervisedrepresentation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}.Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone whileresorting to $\beta$-VAE as a co-pilot to extract semantically disentangledrepresentations. The strong generation ability of diffusion model and the gooddisentanglement ability of VAE model are complementary. To strengthendisentangling, VAE-latent distillation and diffusion-wise feedback areinterconnected in a closed-loop system for a further mutual promotion. Then, aself-supervised \textbf{Navigation} strategy is introduced to identifyinterpretable semantic directions in the disentangled latent space. Finally, anew metric based on content tracking is designed to evaluate thedisentanglement effect. Experiments demonstrate the superiority of CL-Dis onapplications like real image manipulation and visual analysis.</description><author>Xin Jin, Bohan Li, BAAO Xie, Wenyao Zhang, Jinming Liu, Ziqiang Li, Tao Yang, Wenjun Zeng</author><pubDate>Tue, 09 Sep 2025 14:21:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02346v2</guid></item><item><title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title><link>http://arxiv.org/abs/2509.06465v2</link><description>Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, anovel Cross-modality Attention framework with a Mixture-of-Experts (MoE)backbone for robust antibody binding site prediction. CAME-AB integrates fivebiologically grounded modalities, including raw amino acid encodings, BLOSUMsubstitution profiles, pretrained language model embeddings, structure-awarefeatures, and GCN-refined biochemical graphs, into a unified multimodalrepresentation. To enhance adaptive cross-modal reasoning, we propose an\emph{adaptive modality fusion} module that learns to dynamically weight eachmodality based on its global relevance and input-specific contribution. ATransformer encoder combined with an MoE module further promotes featurespecialization and capacity expansion. We additionally incorporate a supervisedcontrastive learning objective to explicitly shape the latent space geometry,encouraging intra-class compactness and inter-class separability. To improveoptimization stability and generalization, we apply stochastic weight averagingduring training. Extensive experiments on benchmark antibody-antigen datasetsdemonstrate that CAME-AB consistently outperforms strong baselines on multiplemetrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablationstudies further validate the effectiveness of each architectural component andthe benefit of multimodal feature integration. The model implementation detailsand the codes are available on https://anonymous.4open.science/r/CAME-AB-C525</description><author>Hongzong Li, Jiahao Ma, Zhanpeng Shi, Rui Xiao, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang</author><pubDate>Tue, 09 Sep 2025 14:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.06465v2</guid></item><item><title>RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis</title><link>http://arxiv.org/abs/2509.07782v1</link><description>RayGauss has achieved state-of-the-art rendering quality for novel-viewsynthesis on synthetic and indoor scenes by representing radiance and densityfields with irregularly distributed elliptical basis functions, rendered viavolume ray casting using a Bounding Volume Hierarchy (BVH). However, itscomputational cost prevents real-time rendering on real-world scenes. Ourapproach, RayGaussX, builds on RayGauss by introducing key contributions thataccelerate both training and inference. Specifically, we incorporate volumetricrendering acceleration strategies such as empty-space skipping and adaptivesampling, enhance ray coherence, and introduce scale regularization to reducefalse-positive intersections. Additionally, we propose a new densificationcriterion that improves density distribution in distant regions, leading toenhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5xto 12x faster training and 50x to 80x higher rendering speeds (FPS) onreal-world datasets while improving visual quality by up to +0.56 dB in PSNR.Project page with videos and code: https://raygaussx.github.io/.</description><author>Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic</author><pubDate>Tue, 09 Sep 2025 14:19:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07782v1</guid></item><item><title>Decentralized Online Riemannian Optimization Beyond Hadamard Manifolds</title><link>http://arxiv.org/abs/2509.07779v1</link><description>We study decentralized online Riemannian optimization over manifolds withpossibly positive curvature, going beyond the Hadamard manifold setting.Decentralized optimization techniques rely on a consensus step that is wellunderstood in Euclidean spaces because of their linearity. However, inpositively curved Riemannian spaces, a main technical challenge is thatgeodesic distances may not induce a globally convex structure. In this work, wefirst analyze a curvature-aware Riemannian consensus step that enables a linearconvergence beyond Hadamard manifolds. Building on this step, we establish a$O(\sqrt{T})$ regret bound for the decentralized online Riemannian gradientdescent algorithm. Then, we investigate the two-point bandit feedback setup,where we employ computationally efficient gradient estimators using smoothingtechniques, and we demonstrate the same $O(\sqrt{T})$ regret bound through thesubconvexity analysis of smoothed objectives.</description><author>Emre Sahinoglu, Shahin Shahrampour</author><pubDate>Tue, 09 Sep 2025 14:14:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07779v1</guid></item><item><title>Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models</title><link>http://arxiv.org/abs/2503.21929v2</link><description>Advances in hardware and language model architecture have spurred arevolution in natural language generation. However, autoregressive modelscompute probability distributions over next-token choices, and sampling fromthese distributions, known as decoding, has received significantly lessattention than other design choices. Existing decoding strategies are largelybased on heuristics, resulting in methods that are difficult to apply orimprove in a principled manner. We develop the theory of decoding strategiesfor language models by expressing popular decoding algorithms as equilibriumstates in the language of ergodic theory and stating the objective functionsthey optimize. Using this, we analyze the effect of the local normalizationstep required to make probabilities sum to one in top-k, nucleus, andtemperature sampling. We argue that local normalization distortion is afundamental defect of decoding strategies and quantify the size of thisdistortion and its effect on mathematical proxies for the quality and diversityof generated text. This yields conclusions for the design of decodingalgorithms and the detection of machine-generated text.</description><author>Tom Kempton, Stuart Burrell</author><pubDate>Tue, 09 Sep 2025 14:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.21929v2</guid></item><item><title>HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2509.07774v1</link><description>Human hair reconstruction is a challenging problem in computer vision, withgrowing importance for applications in virtual reality and digital humanmodeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficientand explicit scene representations that naturally align with the structure ofhair strands. In this work, we extend the 3DGS framework to enable strand-levelhair geometry reconstruction from multi-view images. Our multi-stage pipelinefirst reconstructs detailed hair geometry using a differentiable Gaussianrasterizer, then merges individual Gaussian segments into coherent strandsthrough a novel merging scheme, and finally refines and grows the strands underphotometric supervision. While existing methods typically evaluate reconstruction quality at thegeometric level, they often neglect the connectivity and topology of hairstrands. To address this, we propose a new evaluation metric that serves as aproxy for assessing topological accuracy in strand reconstruction. Extensiveexperiments on both synthetic and real-world datasets demonstrate that ourmethod robustly handles a wide range of hairstyles and achieves efficientreconstruction, typically completing within one hour. The project page can be found at: https://yimin-pan.github.io/hair-gs/</description><author>Yimin Pan, Matthias Nie√üner, Tobias Kirschstein</author><pubDate>Tue, 09 Sep 2025 14:08:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07774v1</guid></item><item><title>Quantum Computing for Large-scale Network Optimization: Opportunities and Challenges</title><link>http://arxiv.org/abs/2509.07773v1</link><description>The complexity of large-scale 6G-and-beyond networks demands innovativeapproaches for multi-objective optimization over vast search spaces, a taskoften intractable. Quantum computing (QC) emerges as a promising technology forefficient large-scale optimization. We present our vision of leveraging QC totackle key classes of problems in future mobile networks. By analyzing andidentifying common features, particularly their graph-centric representation,we propose a unified strategy involving QC algorithms. Specifically, we outlinea methodology for optimization using quantum annealing as well as quantumreinforcement learning. Additionally, we discuss the main challenges that QCalgorithms and hardware must overcome to effectively optimize future networks.</description><author>Sebastian Macaluso, Giovanni Geraci, El√≠as F. Combarro, Sergi Abadal, Ioannis Arapakis, Sofia Vallecorsa, Eduard Alarc√≥n</author><pubDate>Tue, 09 Sep 2025 14:06:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07773v1</guid></item><item><title>XSRD-Net: EXplainable Stroke Relapse Detection</title><link>http://arxiv.org/abs/2509.07772v1</link><description>Stroke is the second most frequent cause of death world wide with an annualmortality of around 5.5 million. Recurrence rates of stroke are between 5 and25% in the first year. As mortality rates for relapses are extraordinarily high(40%) it is of utmost importance to reduce the recurrence rates. We addressthis issue by detecting patients at risk of stroke recurrence at an early stagein order to enable appropriate therapy planning. To this end we collected 3Dintracranial CTA image data and recorded concomitant heart diseases, the ageand the gender of stroke patients between 2010 and 2024. We trained single- andmultimodal deep learning based neural networks for binary relapse detection(Task 1) and for relapse free survival (RFS) time prediction together with asubsequent classification (Task 2). The separation of relapse from non-relapsepatients (Task 1) could be solved with tabular data (AUC on test dataset:0.84). However, for the main task, the regression (Task 2), our multimodalXSRD-net processed the modalities vision:tabular with 0.68:0.32 according tomodality contribution measures. The c-index with respect to relapses for themultimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final,deeper interpretability analysis results could highlight a link between bothheart diseases (tabular) and carotid arteries (vision) for the detection ofrelapses and the prediction of the RFS time. This is a central outcome that westrive to strengthen with ongoing data collection and model retraining.</description><author>Christian Gapp, Elias Tappeiner, Martin Welk, Karl Fritscher, Stephanie Mangesius, Constantin Eisenschink, Philipp Deisl, Michael Knoflach, Astrid E. Grams, Elke R. Gizewski, Rainer Schubert</author><pubDate>Tue, 09 Sep 2025 14:06:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07772v1</guid></item><item><title>Signal-Based Malware Classification Using 1D CNNs</title><link>http://arxiv.org/abs/2509.06548v2</link><description>Malware classification is a contemporary and ongoing challenge incyber-security: modern obfuscation techniques are able to evade traditionalstatic analysis, while dynamic analysis is too resource intensive to bedeployed at a large scale. One prominent line of research addresses theselimitations by converting malware binaries into 2D images by heuristicallyreshaping them into a 2D grid before resizing using Lanczos resampling. Theseimages can then be classified based on their textural information usingcomputer vision approaches. While this approach can detect obfuscated malwaremore effectively than static analysis, the process of converting files into 2Dimages results in significant information loss due to both quantisation noise,caused by rounding to integer pixel values, and the introduction of 2Ddependencies which do not exist in the original data. This loss of signallimits the classification performance of the downstream model. This workaddresses these weaknesses by instead resizing the files into 1D signals whichavoids the need for heuristic reshaping, and additionally these signals do notsuffer from quantisation noise due to being stored in a floating-point format.It is shown that existing 2D CNN architectures can be readily adapted toclassify these 1D signals for improved performance. Furthermore, a bespoke 1Dconvolutional neural network, based on the ResNet architecture andsqueeze-and-excitation layers, was developed to classify these signals andevaluated on the MalNet dataset. It was found to achieve state-of-the-artperformance on binary, type, and family level classification with F1 scores of0.874, 0.503, and 0.507, respectively, paving the way for future models tooperate on the proposed signal modality.</description><author>Jack Wilkie, Hanan Hindy, Ivan Andonovic, Christos Tachtatzis, Robert Atkinson</author><pubDate>Tue, 09 Sep 2025 14:05:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.06548v2</guid></item></channel></rss>