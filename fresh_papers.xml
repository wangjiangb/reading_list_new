<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 24 Sep 2025 21:42:23 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>mRadNet: A Compact Radar Object Detector with MetaFormer</title><link>http://arxiv.org/abs/2509.16223v2</link><description>Frequency-modulated continuous wave radars have gained increasing popularityin the automotive industry. Its robustness against adverse weather conditionsmakes it a suitable choice for radar object detection in advanced driverassistance systems. These real-time embedded systems have requirements for thecompactness and efficiency of the model, which have been largely overlooked inprevious work. In this work, we propose mRadNet, a novel radar object detectionmodel with compactness in mind. mRadNet employs a U-net style architecture withMetaFormer blocks, in which separable convolution and attention token mixersare used to capture both local and global features effectively. More efficienttoken embedding and merging strategies are introduced to further facilitate thelightweight design. The performance of mRadNet is validated on the CRUWdataset, improving state-of-the-art performance with the least number ofparameters and FLOPs.</description><author>Huaiyu Chen, Fahed Hassanat, Robert Laganiere, Martin Bouchard</author><pubDate>Tue, 23 Sep 2025 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.16223v2</guid></item><item><title>Residual Off-Policy RL for Finetuning Behavior Cloning Policies</title><link>http://arxiv.org/abs/2509.19301v1</link><description>Recent advances in behavior cloning (BC) have enabled impressive visuomotorcontrol policies. However, these approaches are limited by the quality of humandemonstrations, the manual effort required for data collection, and thediminishing returns from increasing offline data. In comparison, reinforcementlearning (RL) trains an agent through autonomous interaction with theenvironment and has shown remarkable success in various domains. Still,training RL policies directly on real-world robots remains challenging due tosample inefficiency, safety concerns, and the difficulty of learning fromsparse rewards for long-horizon tasks, especially for high-degree-of-freedom(DoF) systems. We present a recipe that combines the benefits of BC and RLthrough a residual learning framework. Our approach leverages BC policies asblack-box bases and learns lightweight per-step residual corrections viasample-efficient off-policy RL. We demonstrate that our method requires onlysparse binary reward signals and can effectively improve manipulation policieson high-degree-of-freedom (DoF) systems in both simulation and the real world.In particular, we demonstrate, to the best of our knowledge, the firstsuccessful real-world RL training on a humanoid robot with dexterous hands. Ourresults demonstrate state-of-the-art performance in various vision-based tasks,pointing towards a practical pathway for deploying RL in the real world.Project website: https://residual-offpolicy-rl.github.io</description><author>Lars Ankile, Zhenyu Jiang, Rocky Duan, Guanya Shi, Pieter Abbeel, Anusha Nagabandi</author><pubDate>Tue, 23 Sep 2025 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19301v1</guid></item><item><title>CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching</title><link>http://arxiv.org/abs/2509.19300v1</link><description>Conditional generative modeling aims to learn a conditional data distributionfrom samples containing data-condition pairs. For this, diffusion andflow-based methods have attained compelling results. These methods use alearned (flow) model to transport an initial standard Gaussian noise thatignores the condition to the conditional data distribution. The model is hencerequired to learn both mass transport and conditional injection. To ease thedemand on the model, we propose Condition-Aware Reparameterization for FlowMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,the target, or both distributions. By relocating these distributions, CAR-Flowshortens the probability path the model must learn, leading to faster trainingin practice. On low-dimensional synthetic data, we visualize and quantify theeffects of CAR. On higher-dimensional natural image data (ImageNet-256),equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, whileintroducing less than 0.6% additional parameters.</description><author>Chen Chen, Pengsheng Guo, Liangchen Song, Jiasen Lu, Rui Qian, Xinze Wang, Tsu-Jui Fu, Wei Liu, Yinfei Yang, Alex Schwing</author><pubDate>Tue, 23 Sep 2025 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19300v1</guid></item><item><title>VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction</title><link>http://arxiv.org/abs/2509.19297v1</link><description>Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effectivesolution for novel view synthesis. Existing methods predominantly rely on apixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a3D Gaussian. We rethink this widely adopted formulation and identify severalinherent limitations: it renders the reconstructed 3D models heavily dependenton the number of input views, leads to view-biased density distributions, andintroduces alignment errors, particularly when source views contain occlusionsor low texture. To address these challenges, we introduce VolSplat, a newmulti-view feed-forward paradigm that replaces pixel alignment withvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3Dvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D featurematching, ensuring robust multi-view consistency. Furthermore, it enablesadaptive control over Gaussian density based on 3D scene complexity, yieldingmore faithful Gaussian point clouds, improved geometric consistency, andenhanced novel-view rendering quality. Experiments on widely used benchmarksincluding RealEstate10K and ScanNet demonstrate that VolSplat achievesstate-of-the-art performance while producing more plausible and view-consistentGaussian reconstructions. In addition to superior results, our approachestablishes a more scalable framework for feed-forward 3D reconstruction withdenser and more robust representations, paving the way for further research inwider communities. The video results, code and trained models are available onour project page: https://lhmd.top/volsplat.</description><author>Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang, Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Y. Chen, Bohan Zhuang</author><pubDate>Tue, 23 Sep 2025 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19297v1</guid></item><item><title>Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation</title><link>http://arxiv.org/abs/2509.19296v1</link><description>The ability to generate virtual environments is crucial for applicationsranging from gaming to physical AI domains such as robotics, autonomousdriving, and industrial AI. Current learning-based 3D reconstruction methodsrely on the availability of captured real-world multi-view data, which is notalways readily available. Recent advancements in video diffusion models haveshown remarkable imagination capabilities, yet their 2D nature limits theapplications to simulation where a robot needs to navigate and interact withthe environment. In this paper, we propose a self-distillation framework thataims to distill the implicit 3D knowledge in the video diffusion models into anexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need formulti-view training data. Specifically, we augment the typical RGB decoder witha 3DGS decoder, which is supervised by the output of the RGB decoder. In thisapproach, the 3DGS decoder can be purely trained with synthetic data generatedby video diffusion models. At inference time, our model can synthesize 3Dscenes from either a text prompt or a single image for real-time rendering. Ourframework further extends to dynamic 3D scene generation from a monocular inputvideo. Experimental results show that our framework achieves state-of-the-artperformance in static and dynamic 3D scene generation.</description><author>Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, Xuanchi Ren</author><pubDate>Tue, 23 Sep 2025 17:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19296v1</guid></item><item><title>Audio-Based Pedestrian Detection in the Presence of Vehicular Noise</title><link>http://arxiv.org/abs/2509.19295v1</link><description>Audio-based pedestrian detection is a challenging task and has, thus far,only been explored in noise-limited environments. We present a new dataset,results, and a detailed analysis of the state-of-the-art in audio-basedpedestrian detection in the presence of vehicular noise. In our study, weconduct three analyses: (i) cross-dataset evaluation between noisy andnoise-limited environments, (ii) an assessment of the impact of noisy data onmodel performance, highlighting the influence of acoustic context, and (iii) anevaluation of the model's predictive robustness on out-of-domain sounds. Thenew dataset is a comprehensive 1321-hour roadside dataset. It incorporatestraffic-rich soundscapes. Each recording includes 16kHz audio synchronized withframe-level pedestrian annotations and 1fps video thumbnails.</description><author>Yonghyun Kim, Chaeyeon Han, Akash Sarode, Noah Posner, Subhrajit Guhathakurta, Alexander Lerch</author><pubDate>Tue, 23 Sep 2025 17:57:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19295v1</guid></item><item><title>Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM and FastTOM Datasets</title><link>http://arxiv.org/abs/2506.14765v4</link><description>Today, Earth Observation (EO) satellites generate massive volumes of data. Tofully exploit this, it is essential to pretrain EO Foundation Models (FMs) onlarge unlabeled datasets, enabling efficient fine-tuning for downstream taskswith minimal labeled data. In this paper, we study scaling-up FMs: we train ourmodels on the pretraining dataset MajorTOM 23TB which includes all regions, andthe performance on average is competitive versus models pretrained on morespecialized datasets which are substantially smaller and include only land. Theadditional data of oceans and ice do not decrease the performance onland-focused downstream tasks. These results indicate that large FMs trained onglobal datasets for a wider variety of downstream tasks can be useful fordownstream applications that only require a subset of the information includedin their training. The second contribution is the exploration of U-NetConvolutional Neural Network (CNN), Vision Transformers (ViT), and MambaState-Space Models (SSM) as FMs. U-Net captures local correlations amongstpixels, while ViT and Mamba capture local and distant correlations. We developvarious models using different architectures, including U-Net, ViT, and Mamba,and different number of parameters. We evaluate the FLoating-point OPerations(FLOPs) needed by the models. We fine-tune on the PhilEO Bench for differentdownstream tasks: roads, buildings, and land cover. For most n-shots for roadsand buildings, U-Net 200M-2T outperforms the other models. Using Mamba, weachieve comparable results on the downstream tasks, with less computationalexpenses. We also compare with the recent FM TerraMind which we evaluate onPhilEO Bench.</description><author>Nikolaos Dionelis, Riccardo Musto, Jente Bosmans, Simone Sarti, Giancarlo Paoletti, Sébastien Lefèvre, Bertrand Le Saux, Nicolas Longépé</author><pubDate>Tue, 23 Sep 2025 17:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.14765v4</guid></item><item><title>The ICML 2023 Ranking Experiment: Examining Author Self-Assessment in ML/AI Peer Review</title><link>http://arxiv.org/abs/2408.13430v3</link><description>We conducted an experiment during the review process of the 2023International Conference on Machine Learning (ICML), asking authors withmultiple submissions to rank their papers based on perceived quality. In total,we received 1,342 rankings, each from a different author, covering 2,592submissions. In this paper, we present an empirical analysis of howauthor-provided rankings could be leveraged to improve peer review processes atmachine learning conferences. We focus on the Isotonic Mechanism, whichcalibrates raw review scores using the author-provided rankings. Our analysisshows that these ranking-calibrated scores outperform the raw review scores inestimating the ground truth ``expected review scores'' in terms of both squaredand absolute error metrics. Furthermore, we propose several cautious, low-riskapplications of the Isotonic Mechanism and author-provided rankings in peerreview, including supporting senior area chairs in overseeing area chairs'recommendations, assisting in the selection of paper awards, and guiding therecruitment of emergency reviewers.</description><author>Buxin Su, Jiayao Zhang, Natalie Collina, Yuling Yan, Didong Li, Kyunghyun Cho, Jianqing Fan, Aaron Roth, Weijie Su</author><pubDate>Tue, 23 Sep 2025 17:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13430v3</guid></item><item><title>SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration</title><link>http://arxiv.org/abs/2509.19292v1</link><description>Intelligent agents progress by continually refining their capabilitiesthrough actively exploring environments. Yet robot policies often lacksufficient exploration capability due to action mode collapse. Existing methodsthat encourage exploration typically rely on random perturbations, which areunsafe and induce unstable, erratic behaviors, thereby limiting theireffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), aframework that enhances policy exploration and improvement in roboticmanipulation. SOE learns a compact latent representation of task-relevantfactors and constrains exploration to the manifold of valid actions, ensuringsafety, diversity, and effectiveness. It can be seamlessly integrated witharbitrary policy models as a plug-in module, augmenting exploration withoutdegrading the base policy performance. Moreover, the structured latent spaceenables human-guided exploration, further improving efficiency andcontrollability. Extensive experiments in both simulation and real-world tasksdemonstrate that SOE consistently outperforms prior methods, achieving highertask success rates, smoother and safer exploration, and superior sampleefficiency. These results establish on-manifold exploration as a principledapproach to sample-efficient policy self-improvement. Project website:https://ericjin2002.github.io/SOE</description><author>Yang Jin, Jun Lv, Han Xue, Wendi Chen, Chuan Wen, Cewu Lu</author><pubDate>Tue, 23 Sep 2025 17:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19292v1</guid></item><item><title>What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT</title><link>http://arxiv.org/abs/2509.19284v1</link><description>Large reasoning models (LRMs) spend substantial test-time compute on longchain-of-thought (CoT) traces, but what *characterizes* an effective CoTremains unclear. While prior work reports gains from lengthening CoTs andincreasing review (revisiting earlier steps) via appended *wait* tokens, recentstudies suggest that shorter thinking can outperform longer traces. Wetherefore conduct a systematic evaluation across ten LRMs on math andscientific reasoning. Contrary to the "longer-is-better" narrative, we findthat both naive CoT lengthening and increased review are associated with*lower* accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity withprocess quality. We introduce a graph view of CoT to extract structure andidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction ofsteps in abandoned branches-that consistently outpredicts length and reviewratio for correctness across models. To probe causality, we design twointerventions. First, we rank candidate CoTs by each metric at test time, whereFSF yields the largest pass@1 gains; second, we edit CoTs to remove failedbranches, which significantly improves accuracy, indicating that failedbranches bias subsequent reasoning. Taken together, these results characterizeeffective CoTs as those that *fail less* and support *structure-aware*test-time scaling over indiscriminately generating long CoT.</description><author>Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, Anthony Hartshorn</author><pubDate>Tue, 23 Sep 2025 17:50:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19284v1</guid></item><item><title>OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps</title><link>http://arxiv.org/abs/2509.19282v1</link><description>Despite steady progress in layout-to-image generation, current methods stillstruggle with layouts containing significant overlap between bounding boxes. Weidentify two primary challenges: (1) large overlapping regions and (2)overlapping instances with minimal semantic distinction. Through bothqualitative examples and quantitative analysis, we demonstrate how thesefactors degrade generation quality. To systematically assess this issue, weintroduce OverLayScore, a novel metric that quantifies the complexity ofoverlapping bounding boxes. Our analysis reveals that existing benchmarks arebiased toward simpler cases with low OverLayScore values, limiting theireffectiveness in evaluating model performance under more challengingconditions. To bridge this gap, we present OverLayBench, a new benchmarkfeaturing high-quality annotations and a balanced distribution across differentlevels of OverLayScore. As an initial step toward improving performance oncomplex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on acurated amodal mask dataset. Together, our contributions lay the groundwork formore robust layout-to-image generation under realistic and challengingscenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.</description><author>Bingnan Li, Chen-Yu Wang, Haiyang Xu, Xiang Zhang, Ethan Armand, Divyansh Srivastava, Xiaojun Shan, Zeyuan Chen, Jianwen Xie, Zhuowen Tu</author><pubDate>Tue, 23 Sep 2025 17:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19282v1</guid></item><item><title>Turning Tabular Foundation Models into Graph Foundation Models</title><link>http://arxiv.org/abs/2508.20906v2</link><description>While foundation models have revolutionized such fields as natural languageprocessing and computer vision, their potential in graph machine learningremains largely unexplored. One of the key challenges in designing graphfoundation models (GFMs) is handling diverse node features that can vary acrossdifferent graph datasets. While many works on GFMs have focused exclusively ontext-attributed graphs, the problem of handling arbitrary features of othertypes in GFMs has not been fully addressed. However, this problem is not uniqueto the graph domain, as it also arises in the field of machine learning fortabular data. In this work, motivated by the recent success of tabularfoundation models (TFMs) like TabPFNv2 or LimiX, we propose G2T-FM, a simpleframework for turning tabular foundation models into graph foundation models.Specifically, G2T-FM augments the original node features with neighborhoodfeature aggregation, adds structural embeddings, and then applies a TFM to theconstructed node representations. Even in a fully in-context regime, our modelachieves strong results, significantly outperforming publicly available GFMsand performing competitively with, and often better than, well-tuned GNNstrained from scratch. Moreover, after finetuning, G2T-FM surpasses well-tunedGNN baselines. In particular, when combined with LimiX, G2T-FM oftenoutperforms the best GNN by a significant margin. In summary, our paper revealsthe potential of a previously overlooked direction of utilizing tabularfoundation models for graph machine learning tasks.</description><author>Dmitry Eremeev, Gleb Bazhenov, Oleg Platonov, Artem Babenko, Liudmila Prokhorenkova</author><pubDate>Tue, 23 Sep 2025 17:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20906v2</guid></item><item><title>MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurobromas in whole-body MRI</title><link>http://arxiv.org/abs/2509.19277v1</link><description>Background and Objectives: Neurofibromatosis type 1 is a genetic disordercharacterized by the development of numerous neurofibromas (NFs) throughout thebody. Whole-body MRI (WB-MRI) is the clinical standard for detection andlongitudinal surveillance of NF tumor growth. Existing interactive segmentationmethods fail to combine high lesion-wise precision with scalability to hundredsof lesions. This study proposes a novel interactive segmentation model tailoredto this challenge. Methods: We introduce MOIS-SAM2, a multi-object interactive segmentationmodel that extends the state-of-the-art, transformer-based, promptable SegmentAnything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 wastrained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired usingT2-weighted fat-suppressed sequences. The dataset was split at the patientlevel into a training set and four test sets (one in-domain and threereflecting different domain shift scenarios, e.g., MRI field strengthvariation, low tumor burden, differences in clinical site and scanner vendor). Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintainedunder MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1scores ranged from 0.62 to 0.78 across test sets. Preliminary inter-readervariability analysis showed model-to-expert agreement (DSC: 0.62-0.68),comparable to inter-expert agreement (DSC: 0.57-0.69). Conclusions: The proposed MOIS-SAM2 enables efficient and scalableinteractive segmentation of NFs in WB-MRI with minimal user input and stronggeneralization, supporting integration into clinical workflows.</description><author>Georgii Kolokolnikov, Marie-Lena Schmalhofer, Sophie Götz, Lennart Well, Said Farschtschi, Victor-Felix Mautner, Inka Ristow, Rene Werner</author><pubDate>Tue, 23 Sep 2025 17:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19277v1</guid></item><item><title>A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models</title><link>http://arxiv.org/abs/2509.19276v1</link><description>Solving ill-posed inverse problems requires powerful and flexible priors. Wepropose leveraging pretrained latent diffusion models for this task through anew training-free approach, termed Diffusion-regularized Wasserstein GradientFlow (DWGF). Specifically, we formulate the posterior sampling problem as aregularized Wasserstein gradient flow of the Kullback-Leibler divergence in thelatent space. We demonstrate the performance of our method on standardbenchmarks using StableDiffusion (Rombach et al., 2022) as the prior.</description><author>Tim Y. J. Wang, O. Deniz Akyildiz</author><pubDate>Tue, 23 Sep 2025 17:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19276v1</guid></item><item><title>DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture</title><link>http://arxiv.org/abs/2509.19274v1</link><description>We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingualbenchmark centered exclusively on Indian culture, designed to evaluate thecultural understanding of generative AI systems. Unlike existing benchmarkswith a generic or global scope, DRISHTIKON offers deep, fine-grained coverageacross India's diverse regions, spanning 15 languages, covering all states andunion territories, and incorporating over 64,000 aligned text-image pairs. Thedataset captures rich cultural themes including festivals, attire, cuisines,art forms, and historical heritage amongst many more. We evaluate a wide rangeof vision-language models (VLMs), including open-source small and large models,proprietary systems, reasoning-specialized VLMs, and Indic-focused models,across zero-shot and chain-of-thought settings. Our results expose keylimitations in current models' ability to reason over culturally grounded,multimodal inputs, particularly for low-resource languages and less-documentedtraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering arobust testbed to advance culturally aware, multimodally competent languagetechnologies.</description><author>Arijit Maji, Raghvendra Kumar, Akash Ghosh, Anushka, Nemil Shah, Abhilekh Borah, Vanshika Shah, Nishant Mishra, Sriparna Saha</author><pubDate>Tue, 23 Sep 2025 17:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19274v1</guid></item><item><title>Generative Medical Event Models Improve with Scale</title><link>http://arxiv.org/abs/2508.12104v2</link><description>Realizing personalized medicine at scale calls for methods that distillinsights from longitudinal patient journeys, which can be viewed as a sequenceof medical events. Foundation models pretrained on large-scale medical eventdata represent a promising direction for scaling real-world evidence generationand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset withmedical events from de-identified longitudinal health records for 16.3 billionencounters over 300 million unique patient records from 310 health systems, weintroduce the Comet models, a family of decoder-only transformer modelspretrained on 118 million patients representing 115 billion discrete medicalevents (151 billion tokens). We present the largest scaling-law study ofmedical event data, establishing a methodology for pretraining and revealingpower-law scaling relationships for compute, tokens, and model size.Consequently, we pretrained a series of compute-optimal models with up to 1billion parameters. Conditioned on a patient's real-world history, Cometautoregressively predicts the next medical event to simulate patient healthtimelines. We studied 78 real-world tasks, including diagnosis prediction,disease prognosis, and healthcare operations. Remarkably for a foundation modelwith generic pretraining and simulation-based inference, Comet generallyoutperformed or matched task-specific supervised models on these tasks, withoutrequiring task-specific fine-tuning or few-shot examples. Comet's predictivepower consistently improves as the model and pretraining scale. Our resultsshow that Comet, a generative medical event foundation model, can effectivelycapture complex clinical dynamics, providing an extensible and generalizableframework to support clinical decision-making, streamline healthcareoperations, and improve patient outcomes.</description><author>Shane Waxler, Paul Blazek, Davis White, Daniel Sneider, Kevin Chung, Mani Nagarathnam, Patrick Williams, Hank Voeller, Karen Wong, Matthew Swanhorst, Sheng Zhang, Naoto Usuyama, Cliff Wong, Tristan Naumann, Hoifung Poon, Andrew Loza, Daniella Meeker, Seth Hain, Rahul Shah</author><pubDate>Tue, 23 Sep 2025 17:35:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12104v2</guid></item><item><title>Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</title><link>http://arxiv.org/abs/2509.18057v2</link><description>We explore whether techniques from AI can help discover new combinatorialstructures that improve on known limits on efficient algorithms. Specifically,we use AlphaEvolve (an LLM coding agent) to study two settings: a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve arecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set onrandom 3- and 4-regular graphs. Our improved lower bounds are obtained byconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, usingAlphaEvolve. Additionally, via analytical arguments we strengthen the upperbounds to settle the computational hardness of these questions up to an errorin the third decimal place. b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain newinapproximability results, proving that it is NP-hard to approximate MAX-4-CUTand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, usingAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improvesupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the currentbest gadget-based inapproximability result of $0.9853$, but falls short ofimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadgetreduction from "standard" H{\aa}stad-style PCPs. A key technical challenge we faced: verifying a candidate constructionproduced by AlphaEvolve is costly (often requiring exponential time). In bothsettings above, our results were enabled by using AlphaEvolve itself to evolvethe verification procedure to be faster (sometimes by $10,000\times$). Weconclude with a discussion of norms by which to assess the assistance from AIin developing proofs.</description><author>Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta</author><pubDate>Tue, 23 Sep 2025 17:34:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.18057v2</guid></item><item><title>Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs</title><link>http://arxiv.org/abs/2509.18058v2</link><description>Large language model (LLM) developers aim for their models to be honest,helpful, and harmless. However, when faced with malicious requests, models aretrained to refuse, sacrificing helpfulness. We show that frontier LLMs candevelop a preference for dishonesty as a new strategy, even when other optionsare available. Affected models respond to harmful requests with outputs thatsound harmful but are crafted to be subtly incorrect or otherwise harmless inpractice. This behavior emerges with hard-to-predict variations even withinmodels from the same model family. We find no apparent cause for the propensityto deceive, but show that more capable models are better at executing thisstrategy. Strategic dishonesty already has a practical impact on safetyevaluations, as we show that dishonest responses fool all output-based monitorsused to detect jailbreaks that we test, rendering benchmark scores unreliable.Further, strategic dishonesty can act like a honeypot against malicious users,which noticeably obfuscates prior jailbreak attacks. While output monitorsfail, we show that linear probes on internal activations can be used toreliably detect strategic dishonesty. We validate probes on datasets withverifiable outcomes and by using them as steering vectors. Overall, we considerstrategic dishonesty as a concrete example of a broader concern that alignmentof LLMs is hard to control, especially when helpfulness and harmlessnessconflict.</description><author>Alexander Panfilov, Evgenii Kortukov, Kristina Nikolić, Matthias Bethge, Sebastian Lapuschkin, Wojciech Samek, Ameya Prabhu, Maksym Andriushchenko, Jonas Geiping</author><pubDate>Tue, 23 Sep 2025 17:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.18058v2</guid></item><item><title>Leveraging Large Models to Evaluate Novel Content: A Case Study on Advertisement Creativity</title><link>http://arxiv.org/abs/2503.00046v2</link><description>Evaluating creativity is challenging, even for humans, not only because ofits subjectivity but also because it involves complex cognitive processes.Inspired by work in marketing, we attempt to break down visual advertisementcreativity into atypicality and originality. With fine-grained humanannotations on these dimensions, we propose a suite of tasks specifically forsuch a subjective problem. We also evaluate the alignment betweenstate-of-the-art (SoTA) vision language models (VLMs) and humans on ourproposed benchmark, demonstrating both the promises and challenges of usingVLMs for automatic creativity assessment.</description><author>Zhaoyi Joey Hou, Adriana Kovashka, Xiang Lorraine Li</author><pubDate>Tue, 23 Sep 2025 17:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.00046v2</guid></item><item><title>WolBanking77: Wolof Banking Speech Intent Classification Dataset</title><link>http://arxiv.org/abs/2509.19271v1</link><description>Intent classification models have made a lot of progress in recent years.However, previous studies primarily focus on high-resource languages datasets,which results in a gap for low-resource languages and for regions with a highrate of illiterate people where languages are more spoken than read or written.This is the case in Senegal, for example, where Wolof is spoken by around 90\%of the population, with an illiteracy rate of 42\% for the country. Wolof isactually spoken by more than 10 million people in West African region. Totackle such limitations, we release a Wolof Intent Classification Dataset(WolBanking77), for academic research in intent classification. WolBanking77currently contains 9,791 text sentences in the banking domain and more than 4hours of spoken sentences. Experiments on various baselines are conducted inthis work, including text and voice state-of-the-art models. The results arevery promising on this current dataset. This paper also provides detailedanalyses of the contents of the data. We report baseline f1-score and worderror rate metrics respectively on NLP and ASR models trained on WolBanking77dataset and also comparisons between models. We plan to share and conductdataset maintenance, updates and to release open-source code.</description><author>Abdou Karim Kandji, Frédéric Precioso, Cheikh Ba, Samba Ndiaye, Augustin Ndione</author><pubDate>Tue, 23 Sep 2025 17:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19271v1</guid></item><item><title>SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data</title><link>http://arxiv.org/abs/2509.19270v1</link><description>Automatic Speech Recognition (ASR) for low-resource languages like Slovak ishindered by the scarcity of training data. To address this, we introduceSloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours ofspeech from parliamentary proceedings. We developed a robust processingpipeline to align and segment long-form recordings into clean, 30-secondaudio-transcript pairs suitable for model training. We use this dataset tofine-tune several OpenAI Whisper models (small, medium, large-v3, andlarge-v3-turbo), achieving significant Word Error Rate (WER) reductions onstandard Slovak benchmarks like Common Voice and FLEURS. For instance, thefine-tuned Whisper-small model's WER dropped by up to 70\%, approaching thebaseline performance of the much larger Whisper-large-v3 model. To fosterfuture research in low-resource speech recognition, we publicly release thecomplete SloPalSpeech dataset, the fully segmented transcripts (60 millionwords), and all our fine-tuned models.</description><author>Erik Božík, Marek Šuppa</author><pubDate>Tue, 23 Sep 2025 17:33:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19270v1</guid></item><item><title>Extracting Conceptual Spaces from LLMs Using Prototype Embeddings</title><link>http://arxiv.org/abs/2509.19269v1</link><description>Conceptual spaces represent entities and concepts using cognitivelymeaningful dimensions, typically referring to perceptual features. Suchrepresentations are widely used in cognitive science and have the potential toserve as a cornerstone for explainable AI. Unfortunately, they have provennotoriously difficult to learn, although recent LLMs appear to capture therequired perceptual features to a remarkable extent. Nonetheless, practicalmethods for extracting the corresponding conceptual spaces are currently stilllacking. While various methods exist for extracting embeddings from LLMs,extracting conceptual spaces also requires us to encode the underlyingfeatures. In this paper, we propose a strategy in which features (e.g.sweetness) are encoded by embedding the description of a correspondingprototype (e.g. a very sweet food). To improve this strategy, we fine-tune theLLM to align the prototype embeddings with the corresponding conceptual spacedimensions. Our empirical analysis finds this approach to be highly effective.</description><author>Nitesh Kumar, Usashi Chatterjee, Steven Schockaert</author><pubDate>Tue, 23 Sep 2025 17:33:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19269v1</guid></item><item><title>SpinMeRound: Consistent Multi-View Identity Generation Using Diffusion Models</title><link>http://arxiv.org/abs/2504.10716v2</link><description>Despite recent progress in diffusion models, generating realistic headportraits from novel viewpoints remains a significant challenge. Most currentapproaches are constrained to limited angular ranges, predominantly focusing onfrontal or near-frontal views. Moreover, although the recent emerginglarge-scale diffusion models have been proven robust in handling 3D scenes,they underperform on facial data, given their complex structure and the uncannyvalley pitfalls. In this paper, we propose SpinMeRound, a diffusion-basedapproach designed to generate consistent and accurate head portraits from novelviewpoints. By leveraging a number of input views alongside an identityembedding, our method effectively synthesizes diverse viewpoints of a subjectwhilst robustly maintaining its unique identity features. Throughexperimentation, we showcase our model's generation capabilities in 360 headsynthesis, while beating current state-of-the-art multiview diffusion models.</description><author>Stathis Galanakis, Alexandros Lattas, Stylianos Moschoglou, Bernhard Kainz, Stefanos Zafeiriou</author><pubDate>Tue, 23 Sep 2025 17:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.10716v2</guid></item><item><title>Latent Representation Learning of Multi-scale Thermophysics: Application to Dynamics in Shocked Porous Energetic Material</title><link>http://arxiv.org/abs/2506.12996v2</link><description>Coupling of physics across length and time scales plays an important role inthe response of microstructured materials to external loads. In a multi-scaleframework, unresolved (subgrid) meso-scale dynamics is upscaled to thehomogenized (macro-scale) representation of the heterogeneous material throughclosure models. Deep learning models trained using meso-scale simulation dataare now a popular route to assimilate such closure laws. However, meso-scalesimulations are computationally taxing, posing practical challenges in trainingdeep learning-based surrogate models from scratch. In this work, we investigatean alternative meta-learning approach motivated by the idea of tokenization innatural language processing. We show that one can learn a reducedrepresentation of the micro-scale physics to accelerate the meso-scale learningprocess by tokenizing the meso-scale evolution of the physical fields involvedin an archetypal, albeit complex, reactive dynamics problem, \textit{viz.},shock-induced energy localization in a porous energetic material. Aprobabilistic latent representation of \textit{micro}-scale dynamics is learnedas building blocks for \textit{meso}-scale dynamics. The \textit{meso-}scalelatent dynamics model learns the correlation between neighboring buildingblocks by training over a small dataset of meso-scale simulations. We comparethe performance of our model with a physics-aware recurrent convolutionalneural network (PARC) trained only on the full meso-scale dataset. Wedemonstrate that our model can outperform PARC with scarce meso-scale data. Theproposed approach accelerates the development of closure models by leveraginginexpensive micro-scale simulations and fast training over a small meso-scaledataset, and can be applied to a range of multi-scale modeling problems.</description><author>Shahab Azarfar, Joseph B. Choi, Phong CH. Nguyen, Yen T. Nguyen, Pradeep Seshadri, H. S. Udaykumar, Stephen Baek</author><pubDate>Tue, 23 Sep 2025 17:29:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.12996v2</guid></item><item><title>Exploring Model Kinship for Merging Large Language Models</title><link>http://arxiv.org/abs/2410.12613v3</link><description>Model merging has emerged as a key technique for enhancing the capabilitiesand efficiency of Large Language Models (LLMs). The open-source community hasdriven model evolution by iteratively merging existing models, yet a principledunderstanding of the gains and underlying factors in model merging remainslimited. In this work, we study model evolution through iterative merging,drawing an analogy to biological evolution, and introduce the concept of modelkinship, the degree of similarity or relatedness between LLMs. Throughcomprehensive empirical analysis, we show that model kinship is closely linkedto the performance improvements achieved by merging, providing a usefulcriterion for selecting candidate models. Building on this insight, we proposea new model merging strategy: Top-k Greedy Merging with Model Kinship, whichcan improve benchmark performance. Specifically, we discover that incorporatingmodel kinship as a guiding criterion enables continuous merging whilemitigating performance degradation caused by local optima, thereby facilitatingmore effective model evolution. Code is available athttps://github.com/zjunlp/ModelKinship.</description><author>Yedi Hu, Yunzhi Yao, Ningyu Zhang, Huajun Chen, Shumin Deng</author><pubDate>Tue, 23 Sep 2025 17:26:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12613v3</guid></item><item><title>Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner</title><link>http://arxiv.org/abs/2508.15044v3</link><description>Aligning large language models (LLMs) with human preferences has become acritical step in their development. Recent research has increasingly focused ontest-time alignment, where additional compute is allocated during inference toenhance LLM safety and reasoning capabilities. However, these test-timealignment techniques often incur substantial inference costs, limiting theirpractical application. We are inspired by the speculative samplingacceleration, which leverages a small draft model to efficiently predict futuretokens, to address the efficiency bottleneck of test-time alignment. Weintroduce the reward-shifted speculative sampling (SSS) algorithm, in which thedraft model is aligned with human preferences, while the target model remainsunchanged. We theoretically demonstrate that the distributional shift betweenthe aligned draft model and the unaligned target model can be exploited torecover the RLHF optimal solution without actually obtaining it, by modifyingthe acceptance criterion and bonus token distribution. Our algorithm achievessuperior gold reward scores at a significantly reduced inference cost intest-time weak-to-strong alignment experiments, thereby validating both itseffectiveness and efficiency.</description><author>Bolian Li, Yanran Wu, Xinyu Luo, Ruqi Zhang</author><pubDate>Tue, 23 Sep 2025 17:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15044v3</guid></item><item><title>Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World</title><link>http://arxiv.org/abs/2509.19265v1</link><description>Large language models (LLMs) often reflect Western-centric biases, limitingtheir effectiveness in diverse cultural contexts. Although some work hasexplored cultural alignment, the potential for cross-cultural transfer, usingalignment in one culture to improve performance in others, remainsunderexplored. This paper investigates cross-cultural transfer of commonsensereasoning in the Arab world, where linguistic and historical similaritiescoexist with local cultural differences. Using a culturally groundedcommonsense reasoning dataset covering 13 Arab countries, we evaluatelightweight alignment methods such as in-context learning anddemonstration-based reinforcement (DITTO), alongside baselines like supervisedfine-tuning and direct preference optimization. Our results show that merely 12culture-specific examples from one country can improve performance in others by10\% on average, within multilingual models. In addition, we demonstrate thatout-of-culture demonstrations from Indonesia and US contexts can match orsurpass in-culture alignment for MCQ reasoning, highlighting culturalcommonsense transferability beyond the Arab world. These findings demonstratethat efficient cross-cultural alignment is possible and offer a promisingapproach to adapt LLMs to low-resource cultural settings.</description><author>Saeed Almheiri, Rania Hossam, Mena Attia, Chenxi Wang, Preslav Nakov, Timothy Baldwin, Fajri Koto</author><pubDate>Tue, 23 Sep 2025 17:24:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19265v1</guid></item><item><title>Discovering strategies for coastal resilience with AI-based prediction and optimization</title><link>http://arxiv.org/abs/2509.19263v1</link><description>Tropical storms cause extensive property damage and loss of life, making themone of the most destructive types of natural hazards. The development ofpredictive models that identify interventions effective at mitigating stormimpacts has considerable potential to reduce these adverse outcomes. In thisstudy, we use an artificial intelligence (AI)-driven approach for optimizingintervention schemes that improve resilience to coastal flooding. We combinethree different AI models to optimize the selection of intervention types,sites, and scales in order to minimize the expected cost of flooding damage ina given region, including the cost of installing and maintaining interventions.Our approach combines data-driven generation of storm surge fields, surrogatemodeling of intervention impacts, and the solving of a continuous-armed banditproblem. We applied this methodology to optimize the selection of sea wall andoyster reef interventions near Tyndall Air Force Base (AFB) in Florida, an areathat was catastrophically impacted by Hurricane Michael. Our analysis predictsthat intervention optimization could be used to potentially save billions ofdollars in storm damage, far outpacing greedy or non-optimal solutions.</description><author>Jared Markowitz, Alexander New, Jennifer Sleeman, Chace Ashcraft, Jay Brett, Gary Collins, Stella In, Nathaniel Winstead</author><pubDate>Tue, 23 Sep 2025 17:21:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19263v1</guid></item><item><title>LightThinker: Thinking Step-by-Step Compression</title><link>http://arxiv.org/abs/2502.15589v2</link><description>Large language models (LLMs) have shown remarkable performance in complexreasoning tasks, but their efficiency is hindered by the substantial memory andcomputational costs associated with generating lengthy tokens. In this paper,we propose LightThinker, a novel method that enables LLMs to dynamicallycompress intermediate thoughts during reasoning. Inspired by human cognitiveprocesses, LightThinker compresses verbose thought steps into compactrepresentations and discards the original reasoning chains, therebysignificantly reducing the number of tokens stored in the context window. Thisis achieved by training the model on when and how to perform compressionthrough data construction, mapping hidden states to condensed gist tokens, andcreating specialized attention masks. Additionally, we introduce the Dependency(Dep) metric to quantify the degree of compression by measuring the reliance onhistorical tokens during generation. Extensive experiments on four datasets andtwo models show that LightThinker reduces peak memory usage and inference time,while maintaining competitive accuracy. Our work provides a new direction forimproving the efficiency of LLMs in complex reasoning tasks without sacrificingperformance. Code is released at https://github.com/zjunlp/LightThinker.</description><author>Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang</author><pubDate>Tue, 23 Sep 2025 17:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.15589v2</guid></item><item><title>Unified Spatiotemporal Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics</title><link>http://arxiv.org/abs/2509.13425v3</link><description>Ecological systems exhibit complex multi-scale dynamics that challengetraditional modeling. New methods must capture temporal oscillations andemergent spatiotemporal patterns while adhering to conservation principles. Wepresent the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,a deep learning architecture integrating physics-informed neural networks(PINNs) and conservation laws to model predator-prey dynamics acrossdimensional scales. The framework provides a unified solution for both ordinary(ODE) and partial (PDE) differential equation systems, describing temporalcycles and reaction-diffusion patterns within a single neural networkarchitecture. Our methodology uses automatic differentiation to enforce physicsconstraints and adaptive loss weighting to balance data fidelity with physicalconsistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and capturescomplex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).Validation confirms conservation law adherence within 0.5% and shows a 10-50xcomputational speedup for inference compared to numerical solvers. USPIL alsoenables mechanistic understanding through interpretable physics constraints,facilitating parameter discovery and sensitivity analysis not possible withpurely data-driven methods. Its ability to transition between dimensionalformulations opens new avenues for multi-scale ecological modeling. Thesecapabilities make USPIL a transformative tool for ecological forecasting,conservation planning, and understanding ecosystem resilience, establishingphysics-informed deep learning as a powerful and scientifically rigorousparadigm.</description><author>Julian Evan Chrisnanto, Salsabila Rahma Alia, Yulison Herry Chrisnanto, Ferry Faizal</author><pubDate>Tue, 23 Sep 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13425v3</guid></item><item><title>Moving by Looking: Towards Vision-Driven Avatar Motion Generation</title><link>http://arxiv.org/abs/2509.19259v1</link><description>The way we perceive the world fundamentally shapes how we move, whether it ishow we navigate in a room or how we interact with other humans. Current humanmotion generation methods, neglect this interdependency and use task-specific``perception'' that differs radically from that of humans. We argue that thegeneration of human-like avatar behavior requires human-like perception.Consequently, in this work we present CLOPS, the first human avatar that solelyuses egocentric vision to perceive its surroundings and navigate. Using visionas the primary driver of motion however, gives rise to a significant challengefor training avatars: existing datasets have either isolated human motion,without the context of a scene, or lack scale. We overcome this challenge bydecoupling the learning of low-level motion skills from learning of high-levelcontrol that maps visual input to motion. First, we train a motion prior modelon a large motion capture dataset. Then, a policy is trained using Q-learningto map egocentric visual inputs to high-level control commands for the motionprior. Our experiments empirically demonstrate that egocentric vision can giverise to human-like motion characteristics in our avatars. For example, theavatars walk such that they avoid obstacles present in their visual field.These findings suggest that equipping avatars with human-like sensors,particularly egocentric vision, holds promise for training avatars that behavelike humans.</description><author>Markos Diomataris, Berat Mert Albaba, Giorgio Becherini, Partha Ghosh, Omid Taheri, Michael J. Black</author><pubDate>Tue, 23 Sep 2025 17:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19259v1</guid></item><item><title>Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies</title><link>http://arxiv.org/abs/2509.19258v1</link><description>A significant challenge in solid tumors is reliably distinguishingconfounding pathologies from malignant neoplasms on routine imaging. Whileradiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,many aggregate features across the region of interest (ROI) and miss complexspatial relationships among varying intensity compositions. We present a newGraph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesionalheterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters ofsub-regions using per-voxel radiomic measurements, then (2) computesgraph-theoretic metrics to quantify spatial associations among clusters. Theresulting weighted graphs encode higher-order spatial relationships within theROI, aiming to reliably capture ILH and disambiguate confounding pathologiesfrom malignancy. To assess efficacy and clinical feasibility, GrRAiL wasevaluated in n=947 subjects spanning three use cases: differentiating tumorrecurrence from radiation effects in glioblastoma (GBM; n=106) and brainmetastasis (n=233), and stratifying pancreatic intraductal papillary mucinousneoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutionalsetting, GrRAiL consistently outperformed state-of-the-art baselines - GraphNeural Networks (GNNs), textural radiomics, and intensity-graph analysis. InGBM, cross-validation (CV) and test accuracies for recurrence vspseudo-progression were 89% and 78% with &gt;10% test-accuracy gains overcomparators. In brain metastasis, CV and test accuracies for recurrence vsradiation necrosis were 84% and 74% (&gt;13% improvement). For IPMN riskstratification, CV and test accuracies were 84% and 75%, showing &gt;10%improvement.</description><author>Dheerendranath Battalapalli, Apoorva Safai, Maria Jaramillo, Hyemin Um, Gustavo Adalfo Pineda Ortiz, Ulas Bagci, Manmeet Singh Ahluwalia, Marwa Ismail, Pallavi Tiwari</author><pubDate>Tue, 23 Sep 2025 17:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19258v1</guid></item><item><title>PIGDreamer: Privileged Information Guided World Models for Safe Partially Observable Reinforcement Learning</title><link>http://arxiv.org/abs/2508.02159v2</link><description>Partial observability presents a significant challenge for Safe ReinforcementLearning (Safe RL), as it impedes the identification of potential risks andrewards. Leveraging specific types of privileged information during training tomitigate the effects of partial observability has yielded notable empiricalsuccesses. In this paper, we propose Asymmetric Constrained PartiallyObservable Markov Decision Processes (ACPOMDPs) to theoretically examine theadvantages of incorporating privileged information in Safe RL. Building uponACPOMDPs, we propose the Privileged Information Guided Dreamer (PIGDreamer), amodel-based RL approach that leverages privileged information to enhance theagent's safety and performance through privileged representation alignment andan asymmetric actor-critic structure. Our empirical results demonstrate thatPIGDreamer significantly outperforms existing Safe RL methods. Furthermore,compared to alternative privileged RL methods, our approach exhibits enhancedperformance, robustness, and efficiency. Codes are available at:https://github.com/hggforget/PIGDreamer.</description><author>Dongchi Huang, Jiaqi Wang, Yang Li, Chunhe Xia, Tianle Zhang, Kaige Zhang</author><pubDate>Tue, 23 Sep 2025 17:15:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.02159v2</guid></item><item><title>Communication-Efficient Federated Learning with Adaptive Number of Participants</title><link>http://arxiv.org/abs/2508.13803v2</link><description>Rapid scaling of deep learning models has enabled performance gains acrossdomains, yet it introduced several challenges. Federated Learning (FL) hasemerged as a promising framework to address these concerns by enablingdecentralized training. Nevertheless, communication efficiency remains a keybottleneck in FL, particularly under heterogeneous and dynamic clientparticipation. Existing methods, such as FedAvg and FedProx, or otherapproaches, including client selection strategies, attempt to mitigatecommunication costs. However, the problem of choosing the number of clients ina training round remains extremely underexplored. We introduce IntelligentSelection of Participants (ISP), an adaptive mechanism that dynamicallydetermines the optimal number of clients per round to enhance communicationefficiency without compromising model accuracy. We validate the effectivenessof ISP across diverse setups, including vision transformers, real-world ECGclassification, and training with gradient compression. Our results showconsistent communication savings of up to 30\% without losing the finalquality. Applying ISP to different real-world ECG classification setupshighlighted the selection of the number of clients as a separate task offederated learning.</description><author>Sergey Skorik, Vladislav Dorofeev, Gleb Molodtsov, Aram Avetisyan, Dmitry Bylinkin, Daniil Medyakov, Aleksandr Beznosikov</author><pubDate>Tue, 23 Sep 2025 17:13:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13803v2</guid></item><item><title>PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection</title><link>http://arxiv.org/abs/2509.03277v2</link><description>In this paper, we aim to transfer CLIP's robust 2D generalizationcapabilities to identify 3D anomalies across unseen objects of highly diverseclass semantics. To this end, we propose a unified framework to comprehensivelydetect and segment 3D anomalies by leveraging both point- and pixel-levelinformation. We first design PointAD, which leverages point-pixelcorrespondence to represent 3D anomalies through their associated renderingpixel representations. This approach is referred to as implicit 3Drepresentation, as it focuses solely on rendering pixel anomalies but neglectsthe inherent spatial relationships within point clouds. Then, we proposePointAD+ to further broaden the interpretation of 3D anomalies by introducingexplicit 3D representation, emphasizing spatial abnormality to uncover abnormalspatial relationships. Hence, we propose G-aggregation to involve geometryinformation to enable the aggregated point representations spatially aware. Tosimultaneously capture rendering and spatial abnormality, PointAD+ proposeshierarchical representation learning, incorporating implicit and explicitanomaly semantics into hierarchical text prompts: rendering prompts for therendering layer and geometry prompts for the geometry layer. A cross-hierarchycontrastive alignment is further introduced to promote the interaction betweenthe rendering and geometry layers, facilitating mutual anomaly learning.Finally, PointAD+ integrates anomaly semantics from both layers to capture thegeneralized anomaly semantics. During the test, PointAD+ can integrate RGBinformation in a plug-and-play manner and further improve its detectionperformance. Extensive experiments demonstrate the superiority of PointAD+ inZS 3D anomaly detection across unseen objects with highly diverse classsemantics, achieving a holistic understanding of abnormality.</description><author>Qihang Zhou, Shibo He, Jiangtao Yan, Wenchao Meng, Jiming Chen</author><pubDate>Tue, 23 Sep 2025 17:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.03277v2</guid></item><item><title>Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps</title><link>http://arxiv.org/abs/2509.19252v1</link><description>Continuous human motion understanding remains a core challenge in computervision due to its high dimensionality and inherent redundancy. Efficientcompression and representation are crucial for analyzing complex motiondynamics. In this work, we introduce an adversarially-refined VQ-GAN frameworkwith dense motion tokenization for compressing spatio-temporal heatmaps whilepreserving the fine-grained traces of human motion. Our approach combines densemotion tokenization with adversarial refinement, which eliminatesreconstruction artifacts like motion smearing and temporal misalignmentobserved in non-adversarial baselines. Our experiments on the CMU Panopticdataset provide conclusive evidence of our method's superiority, outperformingthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.Furthermore, our dense tokenization strategy enables a novel analysis of motioncomplexity, revealing that 2D motion can be optimally represented with acompact 128-token vocabulary, while 3D motion's complexity demands a muchlarger 1024-token codebook for faithful reconstruction. These results establishpractical deployment feasibility across diverse motion analysis applications.The code base for this work is available athttps://github.com/TeCSAR-UNCC/Pose-Quantization.</description><author>Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi</author><pubDate>Tue, 23 Sep 2025 17:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19252v1</guid></item><item><title>Recovering Wasserstein Distance Matrices from Few Measurements</title><link>http://arxiv.org/abs/2509.19250v1</link><description>This paper proposes two algorithms for estimating square Wasserstein distancematrices from a small number of entries. These matrices are used to computemanifold learning embeddings like multidimensional scaling (MDS) or Isomap, butcontrary to Euclidean distance matrices, are extremely costly to compute. Weanalyze matrix completion from upper triangular samples and Nystr\"{o}mcompletion in which $\mathcal{O}(d\log(d))$ columns of the distance matricesare computed where $d$ is the desired embedding dimension, prove stability ofMDS under Nystr\"{o}m completion, and show that it can outperform matrixcompletion for a fixed budget of sample distances. Finally, we show thatclassification of the OrganCMNIST dataset from the MedMNIST benchmark is stableon data embedded from the Nystr\"{o}m estimation of the distance matrix evenwhen only 10\% of the columns are computed.</description><author>Muhammad Rana, Abiy Tasissa, HanQin Cai, Yakov Gavriyelov, Keaton Hamm</author><pubDate>Tue, 23 Sep 2025 17:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19250v1</guid></item><item><title>Reinforcement Learning on Pre-Training Data</title><link>http://arxiv.org/abs/2509.19249v1</link><description>The growing disparity between the exponential scaling of computationalresources and the finite growth of high-quality text data now constrainsconventional scaling approaches for large language models (LLMs). To addressthis challenge, we introduce Reinforcement Learning on Pre-Training data(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrastto prior approaches that scale training primarily through supervised learning,RLPT enables the policy to autonomously explore meaningful trajectories tolearn from pre-training data and improve its capability through reinforcementlearning (RL). While existing RL strategies such as reinforcement learning fromhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)rely on human annotation for reward construction, RLPT eliminates thisdependency by deriving reward signals directly from pre-training data.Specifically, it adopts a next-segment reasoning objective, rewarding thepolicy for accurately predicting subsequent text segments conditioned on thepreceding context. This formulation allows RL to be scaled on pre-trainingdata, encouraging the exploration of richer trajectories across broadercontexts and thereby fostering more generalizable reasoning skills. Extensiveexperiments on both general-domain and mathematical reasoning benchmarks acrossmultiple models validate the effectiveness of RLPT. For example, when appliedto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, andAIME25, respectively. The results further demonstrate favorable scalingbehavior, suggesting strong potential for continued gains with more compute. Inaddition, RLPT provides a solid foundation, extending the reasoning boundariesof LLMs and enhancing RLVR performance.</description><author>Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng, Chenchen Zhang, Kun Shi, Kyrierl Deng, Qi Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan Zeng, Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu, Xiaoxue Li, Zhuoyu Li, Yangyu Tao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, Di Wang</author><pubDate>Tue, 23 Sep 2025 17:10:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19249v1</guid></item><item><title>CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners</title><link>http://arxiv.org/abs/2503.16356v2</link><description>Knowledge Editing (KE) enables the modification of outdated or incorrectinformation in large language models (LLMs). While existing KE methods canupdate isolated facts, they often fail to generalize these updates to multi-hopreasoning tasks that rely on the modified knowledge. Through an analysis ofreasoning circuits -- the neural pathways LLMs use for knowledge-basedinference, we find that current layer-localized KE approaches (e.g., MEMIT,WISE), which edit only single or a few model layers, inadequately integrateupdated knowledge into these reasoning pathways. To address this limitation, wepresent CaKE (Circuit-aware Knowledge Editing), a novel method that enhancesthe effective integration of updated knowledge in LLMs. By only leveraging afew curated data samples guided by our circuit-based analysis, CaKE stimulatesthe model to develop appropriate reasoning circuits for newly incorporatedknowledge. Experiments show that CaKE enables more accurate and consistent useof edited knowledge across related reasoning tasks, achieving an averageimprovement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset whilerequiring less memory than existing KE methods. We release the code and data inhttps://github.com/zjunlp/CaKE.</description><author>Yunzhi Yao, Jizhan Fang, Jia-Chen Gu, Ningyu Zhang, Shumin Deng, Huajun Chen, Nanyun Peng</author><pubDate>Tue, 23 Sep 2025 17:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.16356v2</guid></item><item><title>Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models</title><link>http://arxiv.org/abs/2505.11341v3</link><description>The task of Critical Questions Generation (CQs-Gen) aims to foster criticalthinking by enabling systems to generate questions that expose underlyingassumptions and challenge the validity of argumentative reasoning structures.Despite growing interest in this area, progress has been hindered by the lackof suitable datasets and automatic evaluation standards. This paper presents acomprehensive approach to support the development and benchmarking of systemsfor this task. We construct the first large-scale dataset including ~5Kmanually annotated questions. We also investigate automatic evaluation methodsand propose reference-based techniques as the strategy that best correlateswith human judgments. Our zero-shot evaluation of 11 LLMs establishes a strongbaseline while showcasing the difficulty of the task. Data and code plus apublic leaderboard are provided to encourage further research, not only interms of model performance, but also to explore the practical benefits ofCQs-Gen for both automated reasoning and human critical thinking.</description><author>Banca Calvo Figueras, Rodrigo Agerri</author><pubDate>Tue, 23 Sep 2025 17:07:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11341v3</guid></item><item><title>ConViS-Bench: Estimating Video Similarity Through Semantic Concepts</title><link>http://arxiv.org/abs/2509.19245v1</link><description>What does it mean for two videos to be similar? Videos may appear similarwhen judged by the actions they depict, yet entirely different if evaluatedbased on the locations where they were filmed. While humans naturally comparevideos by taking different aspects into account, this ability has not beenthoroughly studied and presents a challenge for models that often depend onbroad global similarity scores. Large Multimodal Models (LMMs) with videounderstanding capabilities open new opportunities for leveraging naturallanguage in comparative video tasks. We introduce Concept-based VideoSimilarity estimation (ConViS), a novel task that compares pairs of videos bycomputing interpretable similarity scores across a predefined set of keysemantic concepts. ConViS allows for human-like reasoning about videosimilarity and enables new applications such as concept-conditioned videoretrieval. To support this task, we also introduce ConViS-Bench, a newbenchmark comprising carefully annotated video pairs spanning multiple domains.Each pair comes with concept-level similarity scores and textual descriptionsof both differences and similarities. Additionally, we benchmark severalstate-of-the-art models on ConViS, providing insights into their alignment withhuman judgments. Our results reveal significant performance differences onConViS, indicating that some concepts present greater challenges for estimatingvideo similarity. We believe that ConViS-Bench will serve as a valuableresource for advancing research in language-driven video understanding.</description><author>Benedetta Liberatori, Alessandro Conti, Lorenzo Vaquero, Yiming Wang, Elisa Ricci, Paolo Rota</author><pubDate>Tue, 23 Sep 2025 17:06:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19245v1</guid></item><item><title>Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation</title><link>http://arxiv.org/abs/2509.19244v1</link><description>We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)capable of image understanding and generation tasks. Unlike existing multimodaldiffsion language models such as MMaDa and Muddit which only support simpleimage-level understanding tasks and low-resolution image generation, Lavida-Oexhibits many new capabilities such as object grounding, image-editing, andhigh-resolution (1024px) image synthesis. It is also the first unified MDM thatuses its understanding capabilities to improve image generation and editingresults through planning and iterative self-reflection. To allow effective andefficient training and sampling, Lavida-O ntroduces many novel techniques suchas Elastic Mixture-of-Transformer architecture, universal text conditioning,and stratified sampling. \ours~achieves state-of-the-art performance on a widerange of benchmarks such as RefCOCO object grounding, GenEval text-to-imagegeneration, and ImgEdit image editing, outperforming existing autoregressiveand continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, whileoffering considerable speedup at inference.</description><author>Shufan Li, Jiuxiang Gu, Kangning Liu, Zhe Lin, Zijun Wei, Aditya Grover, Jason Kuen</author><pubDate>Tue, 23 Sep 2025 17:05:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19244v1</guid></item><item><title>Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images</title><link>http://arxiv.org/abs/2504.08727v3</link><description>We present a system using Multimodal LLMs (MLLMs) to analyze a large databasewith tens of millions of images captured at different times, with the aim ofdiscovering patterns in temporal changes. Specifically, we aim to capturefrequent co-occurring changes ("trends") across a city over a certain period.Unlike previous visual analyses, our analysis answers open-ended queries (e.g.,"what are the frequent types of changes in the city?") without anypredetermined target subjects or training labels. These properties cast priorlearning-based or unsupervised visual analysis tools unsuitable. We identifyMLLMs as a novel tool for their open-ended semantic understanding capabilities.Yet, our datasets are four orders of magnitude too large for an MLLM to ingestas context. So we introduce a bottom-up procedure that decomposes the massivevisual analysis problem into more tractable sub-problems. We carefully designMLLM-based solutions to each sub-problem. During experiments and ablationstudies with our system, we find it significantly outperforms baselines and isable to discover interesting trends from images captured in large cities (e.g.,"addition of outdoor dining,", "overpass was painted blue," etc.). See moreresults and interactive demos at https://boyangdeng.com/visual-chronicles.</description><author>Boyang Deng, Songyou Peng, Kyle Genova, Gordon Wetzstein, Noah Snavely, Leonidas Guibas, Thomas Funkhouser</author><pubDate>Tue, 23 Sep 2025 17:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.08727v3</guid></item><item><title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title><link>http://arxiv.org/abs/2503.19041v2</link><description>Fine-tuning enables large language models (LLMs) to adapt to specificdomains, but often compromises their previously established safety alignment.To mitigate the degradation of model safety during fine-tuning, we introduceLookAhead Tuning, a lightweight and effective data-driven approach thatpreserves safety during fine-tuning. The method introduces two simplestrategies that modify training data by previewing partial answer prefixes,thereby minimizing perturbations to the model's initial token distributions andmaintaining its built-in safety mechanisms. Comprehensive experimentsdemonstrate that LookAhead Tuning effectively maintains model safety withoutsacrificing robust performance on downstream tasks. Our findings positionLookAhead Tuning as a reliable and efficient solution for the safe andeffective adaptation of LLMs.</description><author>Kangwei Liu, Mengru Wang, Yujie Luo, Yuan Lin, Mengshu Sun, Lei Liang, Zhiqiang Zhang, Jun Zhou, Bryan Hooi, Shumin Deng</author><pubDate>Tue, 23 Sep 2025 17:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.19041v2</guid></item><item><title>Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation</title><link>http://arxiv.org/abs/2412.14487v4</link><description>Direct Preference Optimization (DPO) has been demonstrated to be highlyeffective in mitigating hallucinations in Large Vision Language Models (LVLMs)by aligning their outputs more closely with human preferences. Despite therecent progress, existing methods suffer from two drawbacks: 1) Lack ofscalable token-level rewards; and 2) Neglect of visual-anchored tokens. To thisend, we propose a novel Token Preference Optimization model withself-calibrated rewards (dubbed as TPO), which adaptively attends tovisual-correlated tokens without fine-grained annotations. Specifically, weintroduce a token-level \emph{visual-anchored} \emph{reward} as the differenceof the logistic distributions of generated tokens conditioned on the raw imageand the corrupted one. In addition, to highlight the informativevisual-anchored tokens, a visual-aware training objective is proposed toenhance more accurate token-level optimization. Extensive experimental resultshave manifested the state-of-the-art performance of the proposed TPO. Forexample, by building on top of LLAVA-1.5-7B, our TPO boosts the performanceabsolute improvement for hallucination benchmarks.</description><author>Jihao Gu, Yingyao Wang, Meng Cao, Pi Bu, Jun Song, Yancheng He, Shilong Li, Bo Zheng</author><pubDate>Tue, 23 Sep 2025 17:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14487v4</guid></item><item><title>Linear Regression under Missing or Corrupted Coordinates</title><link>http://arxiv.org/abs/2509.19242v1</link><description>We study multivariate linear regression under Gaussian covariates in twosettings, where data may be erased or corrupted by an adversary under acoordinate-wise budget. In the incomplete data setting, an adversary mayinspect the dataset and delete entries in up to an $\eta$-fraction of samplesper coordinate; a strong form of the Missing Not At Random model. In thecorrupted data setting, the adversary instead replaces values arbitrarily, andthe corruption locations are unknown to the learner. Despite substantial workon missing data, linear regression under such adversarial missingness remainspoorly understood, even information-theoretically. Unlike the clean setting,where estimation error vanishes with more samples, here the optimal errorremains a positive function of the problem parameters. Our main contribution isto characterize this error up to constant factors across essentially the entireparameter range. Specifically, we establish novel information-theoretic lowerbounds on the achievable error that match the error of (computationallyefficient) algorithms. A key implication is that, perhaps surprisingly, theoptimal error in the missing data setting matches that in the corruptionsetting-so knowing the corruption locations offers no general advantage.</description><author>Ilias Diakonikolas, Jelena Diakonikolas, Daniel M. Kane, Jasper C. H. Lee, Thanasis Pittas</author><pubDate>Tue, 23 Sep 2025 17:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19242v1</guid></item><item><title>MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning</title><link>http://arxiv.org/abs/2509.07021v2</link><description>3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesistechnique, but its high memory consumption severely limits its applicability onedge devices. A growing number of 3DGS compression methods have been proposedto make 3DGS more efficient, yet most only focus on storage compression andfail to address the critical bottleneck of rendering memory. To address thisproblem, we introduce MEGS$^{2}$, a novel memory-efficient framework thattackles this challenge by jointly optimizing two key factors: the totalprimitive number and the parameters per primitive, achieving unprecedentedmemory compression. Specifically, we replace the memory-intensive sphericalharmonics with lightweight, arbitrarily oriented spherical Gaussian lobes asour color representations. More importantly, we propose a unified soft pruningframework that models primitive-number and lobe-number pruning as a singleconstrained optimization problem. Experiments show that MEGS$^{2}$ achieves a50% static VRAM reduction and a 40% rendering VRAM reduction compared toexisting methods, while maintaining comparable rendering quality. Project page:https://megs-2.github.io/</description><author>Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liu, Yujing Sun, Wenping Wang</author><pubDate>Tue, 23 Sep 2025 17:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07021v2</guid></item><item><title>AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration</title><link>http://arxiv.org/abs/2509.19236v1</link><description>Proper initialization is crucial for any system, particularly in multi-agentsystems (MAS), where it plays a pivotal role in determining both the system'sefficiency and effectiveness. However, existing MAS initialization methods donot fully account for the collaborative needs of the generated agents insubsequent stages. Inspired by the principles of effective team composition, wepropose AgentInit, which aims to optimize the structure of agent teams.Specifically, in addition to multi-round interactions and reflections betweenagents during agent generation, AgentInit incorporates a Natural Language toFormat mechanism to ensure consistency and standardization. Balanced teamselection strategies using Pareto principles are subsequently applied tojointly consider agent team diversity and task relevance to promote effectiveand efficient collaboration and enhance overall system performance. Experimentsshow that AgentInit consistently outperforms state-of-the-art initializationmethods and pre-defined strategies across various frameworks and tasks,achieving an overall performance improvement of up to 1.2 and 1.6,respectively, while also significantly reducing token consumption. Furtheranalysis confirms its strong transferability to similar tasks and verifies theeffectiveness of its key components, demonstrating its capability andadaptability as a reliable MAS initialization method. Source code and modelsare available at https://github.com/1737423697/AgentInit.</description><author>Chunhao Tian, Yutong Wang, Xuebo Liu, Zhexuan Wang, Liang Ding, Miao Zhang, Min Zhang</author><pubDate>Tue, 23 Sep 2025 16:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19236v1</guid></item><item><title>Stability and Generalization of Adversarial Diffusion Training</title><link>http://arxiv.org/abs/2509.19234v1</link><description>Algorithmic stability is an established tool for analyzing generalization.While adversarial training enhances model robustness, it often suffers fromrobust overfitting and an enlarged generalization gap. Although recent work hasestablished the convergence of adversarial training in decentralized networks,its generalization properties remain unexplored. This work presents astability-based generalization analysis of adversarial training under thediffusion strategy for convex losses. We derive a bound showing that thegeneralization error grows with both the adversarial perturbation strength andthe number of training steps, a finding consistent with single-agent case butnovel for decentralized settings. Numerical experiments on logistic regressionvalidate these theoretical predictions.</description><author>Hesam Hosseini, Ying Cao, Ali H. Sayed</author><pubDate>Tue, 23 Sep 2025 16:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19234v1</guid></item><item><title>Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation</title><link>http://arxiv.org/abs/2509.19233v1</link><description>In the context of the energy transition, with increasing integration ofrenewable sources and cross-border electricity exchanges, power grids areencountering greater uncertainty and operational risk. Maintaining gridstability under varying conditions is a complex task, and power flow simulatorsare commonly used to support operators by evaluating potential actions beforeimplementation. However, traditional physical solvers, while accurate, areoften too slow for near real-time use. Machine learning models have emerged asfast surrogates, and to improve their adherence to physical laws (e.g.,Kirchhoff's laws), they are often trained with embedded constraints which arealso known as physics-informed or hybrid models. This paper presents anablation study to demystify hybridization strategies, ranging fromincorporating physical constraints as regularization terms or unsupervisedlosses, and exploring model architectures from simple multilayer perceptrons toadvanced graph-based networks enabling the direct optimization of physicsequations. Using our custom benchmarking pipeline for hybrid models calledLIPS, we evaluate these models across four dimensions: accuracy, physicalcompliance, industrial readiness, and out-of-distribution generalization. Theresults highlight how integrating physical knowledge impacts performance acrossthese criteria. All the implementations are reproducible and provided in thecorresponding Github page.</description><author>Milad Leyli-abadi, Antoine Marot, Jérôme Picault</author><pubDate>Tue, 23 Sep 2025 16:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19233v1</guid></item><item><title>Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes</title><link>http://arxiv.org/abs/2508.19356v3</link><description>Graphs are central to the chemical sciences, providing a natural language todescribe molecules, proteins, reactions, and industrial processes. They captureinteractions and structures that underpin materials, biology, and medicine.This primer, Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes,introduces graphs as mathematical objects in chemistry and shows how learningalgorithms (particularly graph neural networks) can operate on them. We outlinethe foundations of graph design, key prediction tasks, representative examplesacross chemical sciences, and the role of machine learning in graph-basedmodeling. Together, these concepts prepare readers to apply graph methods tothe next generation of chemical discovery.</description><author>José Manuel Barraza-Chavez, Rana A. Barghout, Ricardo Almada-Monter, Adrian Jinich, Radhakrishnan Mahadevan, Benjamin Sanchez-Lengeling</author><pubDate>Tue, 23 Sep 2025 16:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19356v3</guid></item><item><title>Finding My Voice: Generative Reconstruction of Disordered Speech for Automated Clinical Evaluation</title><link>http://arxiv.org/abs/2509.19231v1</link><description>We present ChiReSSD, a speech reconstruction framework that preserveschildren speaker's identity while suppressing mispronunciations. Unlike priorapproaches trained on healthy adult speech, ChiReSSD adapts to the voices ofchildren with speech sound disorders (SSD), with particular emphasis on pitchand prosody. We evaluate our method on the STAR dataset and report substantialimprovements in lexical accuracy and speaker identity preservation.Furthermore, we automatically predict the phonetic content in the original andreconstructed pairs, where the proportion of corrected consonants is comparableto the percentage of correct consonants (PCC), a clinical speech assessmentmetric. Our experiments show Pearson correlation of 0.63 between automatic andhuman expert annotations, highlighting the potential to reduce the manualtranscription burden. In addition, experiments on the TORGO dataset demonstrateeffective generalization for reconstructing adult dysarthric speech. Ourresults indicate that disentangled, style-based TTS reconstruction can provideidentity-preserving speech across diverse clinical populations.</description><author>Karen Rosero, Eunjung Yeo, David R. Mortensen, Cortney Van't Slot, Rami R. Hallac, Carlos Busso</author><pubDate>Tue, 23 Sep 2025 16:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19231v1</guid></item><item><title>DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces</title><link>http://arxiv.org/abs/2509.19230v1</link><description>The rise of realistic digital face generation and manipulation posessignificant social risks. The primary challenge lies in the rapid and diverseevolution of generation techniques, which often outstrip the detectioncapabilities of existing models. To defend against the ever-evolving new typesof forgery, we need to enable our model to quickly adapt to new domains withlimited computation and data while avoiding forgetting previously learnedforgery types. In this work, we posit that genuine facial samples are abundantand relatively stable in acquisition methods, while forgery faces continuouslyevolve with the iteration of manipulation techniques. Given the practicalinfeasibility of exhaustively collecting all forgery variants, we frame faceforgery detection as a continual learning problem and allow the model todevelop as new forgery types emerge. Specifically, we employ a DevelopmentalMixture of Experts (MoE) architecture that uses LoRA models as its individualexperts. These experts are organized into two groups: a Real-LoRA to learn andrefine knowledge of real faces, and multiple Fake-LoRAs to capture incrementalinformation from different forgery types. To prevent catastrophic forgetting,we ensure that the learning direction of Fake-LoRAs is orthogonal to theestablished subspace. Moreover, we integrate orthogonal gradients into theorthogonal loss of Fake-LoRAs, preventing gradient interference throughout thetraining process of each task. Experimental results under both the datasets andmanipulation types incremental protocols demonstrate the effectiveness of ourmethod.</description><author>Tianshuo Zhang, Li Gao, Siran Peng, Xiangyu Zhu, Zhen Lei</author><pubDate>Tue, 23 Sep 2025 16:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19230v1</guid></item><item><title>CompLLM: Compression for Long Context Q&amp;A</title><link>http://arxiv.org/abs/2509.19228v1</link><description>Large Language Models (LLMs) face significant computational challenges whenprocessing long contexts due to the quadratic complexity of self-attention.While soft context compression methods, which map input text to smaller latentrepresentations, have shown promise, their real-world adoption is limited.Existing techniques typically compress the context as a single unit, whichleads to quadratic compression complexity and an inability to reusecomputations across queries with overlapping contexts. In this work, weintroduce CompLLM, a soft compression technique designed for practicaldeployment. Instead of processing the context holistically, CompLLM divides itinto segments and compresses each one independently. This simple design choiceyields three critical properties: efficiency, as the compression step scaleslinearly with the context length; scalability, enabling models trained on shortsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; andreusability, allowing compressed segments to be cached and reused acrossdifferent queries. Our experiments show that with a 2x compression rate, athigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4xand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performancecomparable to that obtained with the uncompressed context, and even surpassesit on very long sequences, demonstrating its effectiveness and practicalutility.</description><author>Gabriele Berton, Jayakrishnan Unnikrishnan, Son Tran, Mubarak Shah</author><pubDate>Tue, 23 Sep 2025 16:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19228v1</guid></item><item><title>MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation</title><link>http://arxiv.org/abs/2509.19227v1</link><description>With the widespread deployment of dashcams and advancements in computervision, developing accident prediction models from the dashcam perspective hasbecome critical for proactive safety interventions. However, two key challengespersist: modeling feature-level interactions among traffic participants (oftenoccluded in dashcam views) and capturing complex, asynchronous multi-temporalbehavioral cues preceding accidents. To deal with these two challenges, aMulti-scale Feature Interaction Network (MsFIN) is proposed for early-stageaccident anticipation from dashcam videos. MsFIN has three layers formulti-scale feature aggregation, temporal feature processing and multi-scalefeature post fusion, respectively. For multi-scale feature aggregation, aMulti-scale Module is designed to extract scene representations at short-term,mid-term and long-term temporal scales. Meanwhile, the Transformer architectureis leveraged to facilitate comprehensive feature interactions. Temporal featureprocessing captures the sequential evolution of scene and object features undercausal constraints. In the multi-scale feature post fusion stage, the networkfuses scene and object features across multiple temporal scales to generate acomprehensive risk representation. Experiments on DAD and DADA datasets showthat MsFIN significantly outperforms state-of-the-art models with single-scalefeature extraction in both prediction correctness and earliness. Ablationstudies validate the effectiveness of each module in MsFIN, highlighting howthe network achieves superior performance through multi-scale feature fusionand contextual interaction modeling.</description><author>Tongshuai Wu, Chao Lu, Ze Song, Yunlong Lin, Sizhe Fan, Xuemei Chen</author><pubDate>Tue, 23 Sep 2025 16:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19227v1</guid></item><item><title>Neighbor Embeddings Using Unbalanced Optimal Transport Metrics</title><link>http://arxiv.org/abs/2509.19226v1</link><description>This paper proposes the use of the Hellinger--Kantorovich metric fromunbalanced optimal transport (UOT) in a dimensionality reduction and learning(supervised and unsupervised) pipeline. The performance of UOT is compared tothat of regular OT and Euclidean-based dimensionality reduction methods onseveral benchmark datasets including MedMNIST. The experimental resultsdemonstrate that, on average, UOT shows improvement over both Euclidean andOT-based methods as verified by statistical hypothesis tests. In particular, onthe MedMNIST datasets, UOT outperforms OT in classification 81\% of the time.For clustering MedMNIST, UOT outperforms OT 83\% of the time and outperformsboth other metrics 58\% of the time.</description><author>Muhammad Rana, Keaton Hamm</author><pubDate>Tue, 23 Sep 2025 16:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19226v1</guid></item><item><title>Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction</title><link>http://arxiv.org/abs/2509.19224v1</link><description>Attention-based models have become the leading approach in modeling medicallanguage for Natural Language Processing (NLP) in clinical notes. These modelsoutperform traditional techniques by effectively capturing contextual rep-resentations of language. In this research a comparative analysis is doneamongst pre- trained attention based models namely Bert Base, BioBert, twovariations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on taskrelated to Electronic Health Record (EHR) information extraction. The tasksfrom Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges(n2c2) are considered for this comparison, with the Contextualized MedicationEvent Dataset (CMED) given for these task. CMED is a dataset of unstructuredEHRs and annotated notes that contain task relevant information about the EHRs.The goal of the challenge is to develop effective solutions for extractingcontextual information related to patient medication events from EHRs usingdata driven methods. Each pre-trained model is fine-tuned and applied on CMEDto perform medication extraction, medical event detection, andmulti-dimensional medication event context classification. Pro- cessing methodsare also detailed for breaking down EHRs for compatibility with the appliedmodels. Performance analysis has been carried out using a script based onconstructing medical terms from the evaluation portion of CMED with metricsincluding recall, precision, and F1-Score. The results demonstrate that modelspre-trained on clinical data are more effective in detecting medication andmedication events, but Bert Base, pre- trained on general domain data showed tobe the most effective for classifying the context of events related tomedications.</description><author>Tariq Abdul-Quddoos, Xishuang Dong, Lijun Qian</author><pubDate>Tue, 23 Sep 2025 16:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19224v1</guid></item><item><title>A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models</title><link>http://arxiv.org/abs/2503.05613v3</link><description>Large Language Models (LLMs) have transformed natural language processing,yet their internal mechanisms remain largely opaque. Recently, mechanisticinterpretability has attracted significant attention from the researchcommunity as a means to understand the inner workings of LLMs. Among variousmechanistic interpretability approaches, Sparse Autoencoders (SAEs) haveemerged as a promising method due to their ability to disentangle the complex,superimposed features within LLMs into more interpretable components. Thispaper presents a comprehensive survey of SAEs for interpreting andunderstanding the internal workings of LLMs. Our major contributions include:(1) exploring the technical framework of SAEs, covering basic architecture,design improvements, and effective training strategies; (2) examining differentapproaches to explaining SAE features, categorized into input-based andoutput-based explanation methods; (3) discussing evaluation methods forassessing SAE performance, covering both structural and functional metrics; and(4) investigating real-world applications of SAEs in understanding andmanipulating LLM behaviors.</description><author>Dong Shu, Xuansheng Wu, Haiyan Zhao, Daking Rai, Ziyu Yao, Ninghao Liu, Mengnan Du</author><pubDate>Tue, 23 Sep 2025 16:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.05613v3</guid></item><item><title>Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models</title><link>http://arxiv.org/abs/2509.19222v1</link><description>Recent advances in text-to-video (T2V) generation have enabled the creationof high-fidelity, temporally coherent clips from natural language prompts. Yetthese systems come with significant computational costs, and their energydemands remain poorly understood. In this paper, we present a systematic studyof the latency and energy consumption of state-of-the-art open-source T2Vmodels. We first develop a compute-bound analytical model that predicts scalinglaws with respect to spatial resolution, temporal length, and denoising steps.We then validate these predictions through fine-grained experiments onWAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, andlinear scaling with the number of denoising steps. Finally, we extend ouranalysis to six diverse T2V models, comparing their runtime and energy profilesunder default settings. Our results provide both a benchmark reference andpractical insights for designing and deploying more sustainable generativevideo systems.</description><author>Julien Delavande, Regis Pierrard, Sasha Luccioni</author><pubDate>Tue, 23 Sep 2025 16:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19222v1</guid></item><item><title>FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity</title><link>http://arxiv.org/abs/2509.19220v1</link><description>Federated learning in practice must contend with heterogeneous featurespaces, severe non-IID data, and scarce labels across clients. We presentFedFusion, a federated transfer-learning framework that unifies domainadaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients viaconfidence-filtered pseudo-labels and domain-adaptive transfer, while clientsmaintain personalised encoders tailored to local data. To preserve globalcoherence under heterogeneity, FedFusion employs similarity-weighted classifiercoupling (with optional cluster-wise averaging), mitigating dominance bydata-rich sites and improving minority-client performance. The frugal-labellingpipeline combines self-/semi-supervised pretext training with selectivefine-tuning, reducing annotation demands without sharing raw data. Acrosstabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,FedFusion consistently outperforms state-of-the-art baselines in accuracy,robustness, and fairness while maintaining comparable communication andcomputation budgets. These results show that harmonising personalisation,domain adaptation, and label efficiency is an effective recipe for robustfederated learning under real-world constraints.</description><author>Ferdinand Kahenga, Antoine Bagula, Patrick Sello, Sajal K. Das</author><pubDate>Tue, 23 Sep 2025 16:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19220v1</guid></item><item><title>Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders</title><link>http://arxiv.org/abs/2505.08080v2</link><description>Sparse Autoencoders (SAEs) have recently emerged as powerful tools forinterpreting and steering the internal representations of large language models(LLMs). However, conventional approaches to analyzing SAEs typically relysolely on input-side activations, without considering the causal influencebetween each latent feature and the model's output. This work is built on twokey hypotheses: (1) activated latents do not contribute equally to theconstruction of the model's output, and (2) only latents with high causalinfluence are effective for model steering. To validate these hypotheses, wepropose Gradient Sparse Autoencoder (GradSAE), a simple yet effective methodthat identifies the most influential latents by incorporating output-sidegradient information.</description><author>Dong Shu, Xuansheng Wu, Haiyan Zhao, Mengnan Du, Ninghao Liu</author><pubDate>Tue, 23 Sep 2025 16:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.08080v2</guid></item><item><title>The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface</title><link>http://arxiv.org/abs/2509.02783v2</link><description>We present the Transparent Earth, a transformer-based architecture forreconstructing subsurface properties from heterogeneous datasets that vary insparsity, resolution, and modality, where each modality represents a distincttype of observation (e.g., stress angle, mantle temperature, tectonic platetype). The model incorporates positional encodings of observations togetherwith modality encodings, derived from a text embedding model applied to adescription of each modality. This design enables the model to scale to anarbitrary number of modalities, making it straightforward to add new ones notconsidered in the initial design. We currently include eight modalitiesspanning directional angles, categorical classes, and continuous propertiessuch as temperature and thickness. These capabilities support in-contextlearning, enabling the model to generate predictions either with no inputs orwith an arbitrary number of additional observations from any subset ofmodalities. On validation data, this reduces errors in predicting stress angleby more than a factor of three. The proposed architecture is scalable anddemonstrates improved performance with increased parameters. Together, theseadvances make the Transparent Earth an initial foundation model for the Earth'ssubsurface that ultimately aims to predict any subsurface property anywhere onEarth.</description><author>Arnab Mazumder, Javier E. Santos, Noah Hobbs, Mohamed Mehana, Daniel O'Malley</author><pubDate>Tue, 23 Sep 2025 16:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.02783v2</guid></item><item><title>HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus</title><link>http://arxiv.org/abs/2509.19218v1</link><description>Evaluation of hydrocephalus in children is challenging, and the relatedresearch is limited by a lack of publicly available, expert-annotated datasets,particularly those with segmentation of the choroid plexus. To address this, wepresent HyKid, an open-source dataset from 48 pediatric patients withhydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which wasreconstructed from routine low-resolution images using a slice-to-volumealgorithm. Manually corrected segmentations of brain tissues, including whitematter, grey matter, lateral ventricle, external CSF, and the choroid plexus,were provided by an experienced neurologist. Additionally, structured data wasextracted from clinical radiology reports using a Retrieval-AugmentedGeneration framework. The strong correlation between choroid plexus volume andtotal CSF volume provided a potential biomarker for hydrocephalus evaluation,achieving excellent performance in a predictive model (AUC = 0.87). Theproposed HyKid dataset provided a high-quality benchmark for neuroimagingalgorithms development, and it revealed the choroid plexus-related features inhydrocephalus assessments. Our datasets are publicly available athttps://www.synapse.org/Synapse:syn68544889.</description><author>Yunzhi Xu, Yushuang Ding, Hu Sun, Hongxi Zhang, Li Zhao</author><pubDate>Tue, 23 Sep 2025 16:42:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19218v1</guid></item><item><title>FragmentGPT: A Unified GPT Model for Fragment Growing, Linking, and Merging in Molecular Design</title><link>http://arxiv.org/abs/2509.11044v2</link><description>Fragment-Based Drug Discovery (FBDD) is a popular approach in early drugdevelopment, but designing effective linkers to combine disconnected molecularfragments into chemically and pharmacologically viable candidates remainschallenging. Further complexity arises when fragments contain structuralredundancies, like duplicate rings, which cannot be addressed by simply addingor removing atoms or bonds. To address these challenges in a unified framework,we introduce FragmentGPT, which integrates two core components: (1) a novelchemically-aware, energy-based bond cleavage pre-training strategy that equipsthe GPT-based model with fragment growing, linking, and merging capabilities,and (2) a novel Reward Ranked Alignment with Expert Exploration (RAE) algorithmthat combines expert imitation learning for diversity enhancement, dataselection and augmentation for Pareto and composite score optimality, andSupervised Fine-Tuning (SFT) to align the learner policy with multi-objectivegoals. Conditioned on fragment pairs, FragmentGPT generates linkers thatconnect diverse molecular subunits while simultaneously optimizing for multiplepharmaceutical goals. It also learns to resolve structural redundancies-such asduplicated fragments-through intelligent merging, enabling the synthesis ofoptimized molecules. FragmentGPT facilitates controlled, goal-driven molecularassembly. Experiments and ablation studies on real-world cancer datasetsdemonstrate its ability to generate chemically valid, high-quality moleculestailored for downstream drug discovery tasks.</description><author>Xuefeng Liu, Songhao Jiang, Qinan Huang, Tinson Xu, Ian Foster, Mengdi Wang, Hening Lin, Rick Stevens</author><pubDate>Tue, 23 Sep 2025 16:41:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.11044v2</guid></item><item><title>Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability</title><link>http://arxiv.org/abs/2501.01346v3</link><description>Large Vision-Language Models (LVLMs) have demonstrated remarkablecapabilities in processing both visual and textual information. However, thecritical challenge of alignment between visual and textual representations isnot fully understood. This survey presents a comprehensive examination ofalignment and misalignment in LVLMs through an explainability lens. We firstexamine the fundamentals of alignment, exploring its representational andbehavioral aspects, training methodologies, and theoretical foundations. Wethen analyze misalignment phenomena across three semantic levels: object,attribute, and relational misalignment. Our investigation reveals thatmisalignment emerges from challenges at multiple levels: the data level, themodel level, and the inference level. We provide a comprehensive review ofexisting mitigation strategies, categorizing them into parameter-frozen andparameter-tuning approaches. Finally, we outline promising future researchdirections, emphasizing the need for standardized evaluation protocols andin-depth explainability studies.</description><author>Dong Shu, Haiyan Zhao, Jingyu Hu, Weiru Liu, Ali Payani, Lu Cheng, Mengnan Du</author><pubDate>Tue, 23 Sep 2025 16:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01346v3</guid></item><item><title>Conformal Convolution and Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects</title><link>http://arxiv.org/abs/2402.04906v6</link><description>Generating probabilistic forecasts of potential outcomes and individualtreatment effects (ITE) is essential for risk-aware decision-making in domainssuch as healthcare, policy, marketing, and finance. We propose two novelmethods: the conformal convolution T-learner (CCT) and the conformal MonteCarlo (CMC) meta-learner, that generate full predictive distributions of bothpotential outcomes and ITEs. Our approaches combine weighted conformalpredictive systems with either analytic convolution of potential outcomedistributions or Monte Carlo sampling, addressing covariate shift throughpropensity score weighting. In contrast to other approaches that allow thegeneration of potential outcome predictive distributions, our approaches aremodel agnostic, universal, and come with finite-sample guarantees ofprobabilistic calibration under knowledge of the propensity score. Regardingestimating the ITE distribution, we formally characterize how assumptions aboutpotential outcomes' noise dependency impact distribution validity and establishuniversal consistency under independence noise assumptions. Experiments onsynthetic and semi-synthetic datasets demonstrate that the proposed methodsachieve probabilistically calibrated predictive distributions while maintainingnarrow prediction intervals and having performant continuous ranked probabilityscores. Besides probabilistic forecasting performance, we observe significantefficiency gains for the CCT- and CMC meta-learners compared to other conformalapproaches that produce prediction intervals for ITE with coverage guarantees.</description><author>Jef Jonkers, Jarne Verhaeghe, Glenn Van Wallendael, Luc Duchateau, Sofie Van Hoecke</author><pubDate>Tue, 23 Sep 2025 16:40:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04906v6</guid></item><item><title>QSpark: Towards Reliable Qiskit Code Generation</title><link>http://arxiv.org/abs/2507.12642v2</link><description>Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code andStarCoder often output flawed Qiskit code. We fine-tuned the Qwen2.5-Coder-32Bmodel with two RL methods, Group Relative Policy Optimization (GRPO) andOdds-Ratio Preference Optimization (ORPO), using a richly annotated syntheticdataset. On the Qiskit HumanEval benchmark, ORPO reaches 56.29% Pass@1($\approx+10$ pp over Granite-8B-QK) and GRPO hits 49%, both beating allgeneral-purpose baselines; on the original HumanEval they score 65.90% and63.00%. GRPO performs well on basic tasks (44/78) and excels on intermediateones (41/68), but neither GRPO nor ORPO solves any of the five advanced tasks,highlighting clear gains yet room for progress in AI-assisted quantumprogramming.</description><author>Kiana Kheiri, Aamna Aamir, Andriy Miranskyy, Chen Ding</author><pubDate>Tue, 23 Sep 2025 16:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.12642v2</guid></item><item><title>PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation</title><link>http://arxiv.org/abs/2509.19215v1</link><description>Photoplethysmography (PPG) is widely used in wearable health monitoring, yetlarge PPG foundation models remain difficult to deploy on resource-limiteddevices. We present PPG-Distill, a knowledge distillation framework thattransfers both global and local knowledge through prediction-, feature-, andpatch-level distillation. PPG-Distill incorporates morphology distillation topreserve local waveform patterns and rhythm distillation to capture inter-patchtemporal structures. On heart rate estimation and atrial fibrillationdetection, PPG-Distill improves student performance by up to 21.8% whileachieving 7X faster inference and reducing memory usage by 19X, enablingefficient PPG analysis on wearables</description><author>Juntong Ni, Saurabh Kataria, Shengpu Tang, Carl Yang, Xiao Hu, Wei Jin</author><pubDate>Tue, 23 Sep 2025 16:35:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19215v1</guid></item><item><title>Steering Multimodal Large Language Models Decoding for Context-Aware Safety</title><link>http://arxiv.org/abs/2509.19212v1</link><description>Multimodal Large Language Models (MLLMs) are increasingly deployed inreal-world applications, yet their ability to make context-aware safetydecisions remains limited. Existing methods often fail to balanceoversensitivity (unjustified refusals of benign queries) and undersensitivity(missed detection of visually grounded risks), leaving a persistent gap insafety alignment. To address this issue, we introduce Safety-aware ContrastiveDecoding (SafeCoDe), a lightweight and model-agnostic decoding framework thatdynamically adjusts token generation based on multimodal context. SafeCoDeoperates in two stages: (1) a contrastive decoding mechanism that highlightstokens sensitive to visual context by contrasting real and Gaussian-noisedimages, and (2) a global-aware token modulation strategy that integratesscene-level reasoning with token-level adjustment to adapt refusals accordingto the predicted safety verdict. Extensive experiments across diverse MLLMarchitectures and safety benchmarks, covering undersensitivity,oversensitivity, and general safety evaluations, show that SafeCoDeconsistently improves context-sensitive refusal behaviors while preservingmodel helpfulness.</description><author>Zheyuan Liu, Zhangchen Xu, Guangyao Dou, Xiangchi Yuan, Zhaoxuan Tan, Radha Poovendran, Meng Jiang</author><pubDate>Tue, 23 Sep 2025 16:32:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19212v1</guid></item><item><title>Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data</title><link>http://arxiv.org/abs/2509.19208v1</link><description>Accurate plant segmentation in thermal imagery remains a significantchallenge for high throughput field phenotyping, particularly in outdoorenvironments where low contrast between plants and weeds and frequentocclusions hinder performance. To address this, we present a framework thatleverages synthetic RGB imagery, a limited set of real annotations, andGAN-based cross-modality alignment to enhance semantic segmentation in thermalimages. We trained models on 1,128 synthetic images containing complex mixturesof crop and weed plants in order to generate image segmentation masks for cropand weed plants. We additionally evaluated the benefit of integrating as few asfive real, manually segmented field images within the training process usingvarious sampling strategies. When combining all the synthetic images with a fewlabeled real images, we observed a maximum relative improvement of 22% for theweed class and 17% for the plant class compared to the full real-data baseline.Cross-modal alignment was enabled by translating RGB to thermal usingCycleGAN-turbo, allowing robust template matching without calibration. Resultsdemonstrated that combining synthetic data with limited manual annotations andcross-domain translation via generative models can significantly boostsegmentation performance in complex field environments for multi-model imagery.</description><author>Earl Ranario, Ismael Mayanja, Heesup Yun, Brian N. Bailey, J. Mason Earles</author><pubDate>Tue, 23 Sep 2025 16:29:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19208v1</guid></item><item><title>Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs</title><link>http://arxiv.org/abs/2509.19207v1</link><description>Contrastive vision-language models (VLMs) have made significant progress inbinding visual and textual information, but understanding long, dense captionsremains an open challenge. We hypothesize that compositionality, the capacityto reason about object-attribute bindings and inter-object relationships, iskey to understanding longer captions. In this paper, we investigate theinteraction between compositionality and long-caption understanding, askingwhether training for one property enhances the other. We train and evaluate arange of models that target each of these capabilities. Our results reveal abidirectional relationship: compositional training improves performance onlong-caption retrieval, and training on long captions promotescompositionality. However, these gains are sensitive to data quality and modeldesign. We find that training on poorly structured captions, or with limitedparameter updates, fails to support generalization. Likewise, strategies thataim at retaining general alignment, such as freezing positional embeddings, donot improve compositional understanding. Overall, we find that compositionalunderstanding and long-caption understanding are intertwined capabilities thatcan be jointly learned through training on dense, grounded descriptions.Despite these challenges, we show that models trained on high-quality,long-caption data can achieve strong performance in both tasks, offeringpractical guidance for improving VLM generalization.</description><author>Israfel Salazar, Desmond Elliott, Yova Kementchedjhieva</author><pubDate>Tue, 23 Sep 2025 16:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19207v1</guid></item><item><title>Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions</title><link>http://arxiv.org/abs/2509.19203v1</link><description>Contrastively-trained Vision-Language Models (VLMs), such as CLIP, havebecome the standard approach for learning discriminative vision-languagerepresentations. However, these models often exhibit shallow languageunderstanding, manifesting bag-of-words behaviour. These limitations arereinforced by their dual-encoder design, which induces a modality gap.Additionally, the reliance on vast web-collected data corpora for trainingmakes the process computationally expensive and introduces significant privacyconcerns. To address these limitations, in this work, we challenge thenecessity of vision encoders for retrieval tasks by introducing a vision-free,single-encoder retrieval pipeline. Departing from the traditional text-to-imageretrieval paradigm, we migrate to a text-to-text paradigm with the assistanceof VLLM-generated structured image descriptions. We demonstrate that thisparadigm shift has significant advantages, including a substantial reduction ofthe modality gap, improved compositionality, and better performance on shortand long caption queries, all attainable with only a few hours of calibrationon two GPUs. Additionally, substituting raw images with textual descriptionsintroduces a more privacy-friendly alternative for retrieval. To further assessgeneralisation and address some of the shortcomings of prior compositionalitybenchmarks, we release two benchmarks derived from Flickr30k and COCO,containing diverse compositional queries made of short captions, which we coinsubFlickr and subCOCO. Our vision-free retriever matches and often surpassestraditional multimodal models. Importantly, our approach achievesstate-of-the-art zero-shot performance on multiple retrieval andcompositionality benchmarks, with models as small as 0.3B parameters. Code isavailable at: https://github.com/IoannaNti/LexiCLIP</description><author>Ioanna Ntinou, Alexandros Xenos, Yassine Ouali, Adrian Bulat, Georgios Tzimiropoulos</author><pubDate>Tue, 23 Sep 2025 16:22:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19203v1</guid></item><item><title>AlloyInter: Visualising Alloy Mixture Interpolations in t-SNE Representations</title><link>http://arxiv.org/abs/2509.19202v1</link><description>This entry description proposes AlloyInter, a novel system to enable jointexploration of input mixtures and output parameters space in the context of theSciVis Contest 2025. We propose an interpolation approach, guided byeXplainable Artificial Intelligence (XAI) based on a learned model ensemblethat allows users to discover input mixture ratios by specifying outputparameter goals that can be iteratively adjusted and improved towards a goal.We strengthen the capabilities of our system by building upon prior researchwithin the robustness of XAI, as well as combining well-established techniqueslike manifold learning with interpolation approaches.</description><author>Benedikt Kantz, Peter Waldert, Stefan Lengauer, Tobias Schreck</author><pubDate>Tue, 23 Sep 2025 16:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19202v1</guid></item><item><title>Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models</title><link>http://arxiv.org/abs/2508.09403v3</link><description>Expanding the abbreviated column names of tables, such as "esal" to "employeesalary", is critical for many downstream NLP tasks for tabular data, such asNL2SQL, table QA, and keyword search. This problem arises in enterprises,domain sciences, government agencies, and more. In this paper, we make threecontributions that significantly advance the state of the art. First, we showthat the synthetic public data used by prior work has major limitations, and weintroduce four new datasets in enterprise/science domains, with real-worldabbreviations. Second, we show that accuracy measures used by prior workseriously undercount correct expansions, and we propose new synonym-awaremeasures that capture accuracy much more accurately. Finally, we developColumbo, a powerful LLM-based solution that exploits context, rules,chain-of-thought reasoning, and token-level analysis. Extensive experimentsshow that Columbo significantly outperforms NameGuess, the current mostadvanced solution, by 4-29%, over five datasets. Columbo has been used inproduction on EDI, a major data lake for environmental sciences.</description><author>Ting Cai, Stephen Sheen, AnHai Doan</author><pubDate>Tue, 23 Sep 2025 16:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09403v3</guid></item><item><title>Penalizing Boundary Activation for Object Completeness in Diffusion Models</title><link>http://arxiv.org/abs/2509.16968v2</link><description>Diffusion models have emerged as a powerful technique for text-to-image (T2I)generation, creating high-quality, diverse images across various domains.However, a common limitation in these models is the incomplete display ofobjects, where fragments or missing parts undermine the model's performance indownstream applications. In this study, we conduct an in-depth analysis of theincompleteness issue and reveal that the primary factor behind incompleteobject generation is the usage of RandomCrop during model training. This widelyused data augmentation method, though enhances model generalization ability,disrupts object continuity during training. To address this, we propose atraining-free solution that penalizes activation values at image boundariesduring the early denoising steps. Our method is easily applicable topre-trained Stable Diffusion models with minimal modifications and negligiblecomputational overhead. Extensive experiments demonstrate the effectiveness ofour method, showing substantial improvements in object integrity and imagequality.</description><author>Haoyang Xu, Tianhao Zhao, Sibei Yang, Yutian Lin</author><pubDate>Tue, 23 Sep 2025 16:17:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.16968v2</guid></item><item><title>Online Process Reward Leanring for Agentic Reinforcement Learning</title><link>http://arxiv.org/abs/2509.19199v1</link><description>Large language models (LLMs) are increasingly trained with reinforcementlearning (RL) as autonomous agents that reason and act over long horizons ininteractive environments. However, sparse and sometimes unverifiable rewards make temporal creditassignment extremely challenging. Recent work attempts to integrate process supervision into agent learning butsuffers from biased annotation, reward hacking, high-variance from overlyfine-grained signals or failtures when state overlap is rare. We therefore introduce Online Process Reward Learning (OPRL), a generalcredit-assignment strategy for agentic RL that integrates seamlessly withstandard on-policy algorithms without relying on additional rollouts orexplicit step labels. In OPRL, we optimize an implicit process reward model (PRM) alternately withthe agent's policy to transform trajectory preferences into implicit steprewards through a trajectory-based DPO objective. These step rewards are then used to compute step-level advantages, which arecombined with episode-level advantages from outcome rewards for policy update,creating a self-reinforcing loop. Theoretical findings guarantee that the learned step rewards are consistentwith trajectory preferences and act as potential-based shaping rewards,providing bounded gradients to stabilize training. Empirically, we evaluate OPRL on three distinct agent benmarks, includingWebShop and VisualSokoban, as well as open-ended social interactions withunverfiable rewards in SOTOPIA. Crucially, OPRL shows superior performance over frontier LLMs and strong RLbaselines across domains, achieving state-of-the-art results with highersample-efficiency and lower variance during training. Further analysis also demonstrates the efficient exploration by OPRL usingfewer actions, underscoring its potential for agentic learning in real-worldscenarios.</description><author>Xiaoqian Liu, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li, Junge Zhang, Jianbin Jiao</author><pubDate>Tue, 23 Sep 2025 16:15:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19199v1</guid></item><item><title>Integrating Belief Domains into Probabilistic Logic Programs</title><link>http://arxiv.org/abs/2507.17291v2</link><description>Probabilistic Logic Programming (PLP) under the Distribution Semantics is aleading approach to practical reasoning under uncertainty. An advantage of theDistribution Semantics is its suitability for implementation as a Prolog orPython library, available through two well-maintained implementations, namelyProbLog and cplint/PITA. However, current formulations of the DistributionSemantics use point-probabilities, making it difficult to express epistemicuncertainty, such as arises from, for example, hierarchical classificationsfrom computer vision models. Belief functions generalize probability measuresas non-additive capacities, and address epistemic uncertainty via intervalprobabilities. This paper introduces interval-based Capacity Logic Programsbased on an extension of the Distribution Semantics to include belieffunctions, and describes properties of the new framework that make it amenableto practical applications.</description><author>Damiano Azzolini, Fabrizio Riguzzi, Theresa Swift</author><pubDate>Tue, 23 Sep 2025 16:15:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.17291v2</guid></item><item><title>A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness</title><link>http://arxiv.org/abs/2509.19197v1</link><description>Data-driven models, especially deep learning classifiers often demonstrategreat success on clean datasets. Yet, they remain vulnerable to common datadistortions such as adversarial and common corruption perturbations. Theseperturbations can significantly degrade performance, thereby challenging theoverall reliability of the models. Traditional robustness validation typicallyrelies on perturbed test datasets to assess and improve model performance. Inour framework, however, we propose a validation approach that extracts "weakrobust" samples directly from the training dataset via local robustnessanalysis. These samples, being the most susceptible to perturbations, serve asan early and sensitive indicator of the model's vulnerabilities. By evaluatingmodels on these challenging training instances, we gain a more nuancedunderstanding of its robustness, which informs targeted performanceenhancement. We demonstrate the effectiveness of our approach on models trainedwith CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validationguided by weak robust samples can drive meaningful improvements in modelreliability under adversarial and common corruption scenarios.</description><author>Abdul-Rauf Nuhu, Parham Kebria, Vahid Hemmati, Benjamin Lartey, Mahmoud Nabil Mahmoud, Abdollah Homaifar, Edward Tunstel</author><pubDate>Tue, 23 Sep 2025 16:14:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19197v1</guid></item><item><title>3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</title><link>http://arxiv.org/abs/2509.12197v2</link><description>In this paper, we present a comprehensive review of 3D human pose estimationand human mesh recovery from in-the-wild LiDAR point clouds. We compareexisting approaches across several key dimensions, and propose a structuredtaxonomy to classify these methods. Following this taxonomy, we analyze eachmethod's strengths, limitations, and design choices. In addition, (i) weperform a quantitative comparison of the three most widely used datasets,detailing their characteristics; (ii) we compile unified definitions of allevaluation metrics; and (iii) we establish benchmark tables for both tasks onthese datasets to enable fair comparisons and promote progress in the field. Wealso outline open challenges and research directions critical for advancingLiDAR-based 3D human understanding. Moreover, we maintain an accompanyingwebpage that organizes papers according to our taxonomy and continuously updateit with new studies:https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</description><author>Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet</author><pubDate>Tue, 23 Sep 2025 16:13:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.12197v2</guid></item><item><title>Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion</title><link>http://arxiv.org/abs/2306.11593v2</link><description>State-of-The-Art (SoTA) image captioning models are often trained on theMicroSoft Common Objects in Context (MS-COCO) dataset, which containshuman-annotated captions with an average length of approximately ten tokens.Although effective for general scene understanding, these short captions oftenfail to capture complex scenes and convey detailed information. Moreover,captioning models tend to exhibit bias towards the ``average'' caption, whichcaptures only the more general aspects, thus overlooking finer details. In thispaper, we present a novel approach to generate richer and more informativeimage captions by combining the captions generated from different SoTAcaptioning models. Our proposed method requires no additional model training:given an image, it leverages pre-trained models from the literature to generatethe initial captions, and then ranks them using a newly introducedimage-text-based metric, which we name BLIPScore. Subsequently, the top twocaptions are fused using a Large Language Model (LLM) to produce the final,more detailed description. Experimental results on the MS-COCO and Flickr30ktest sets demonstrate the effectiveness of our approach in terms ofcaption-image alignment and hallucination reduction according to the ALOHa,CAPTURE, and Polos metrics. A subjective study lends additional support tothese results, suggesting that the captions produced by our model are generallyperceived as more consistent with human judgment. By combining the strengths ofdiverse SoTA models, our method enhances the quality and appeal of imagecaptions, bridging the gap between automated systems and the rich andinformative nature of human-generated descriptions. This advance enables thegeneration of more suitable captions for the training of both vision-languageand captioning models.</description><author>Luigi Celona, Simone Bianco, Marco Donzella, Paolo Napoletano</author><pubDate>Tue, 23 Sep 2025 16:12:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11593v2</guid></item><item><title>Fine-Tuning is Subgraph Search: A New Lens on Learning Dynamics</title><link>http://arxiv.org/abs/2502.06106v3</link><description>The study of mechanistic interpretability aims to reverse-engineer a model toexplain its behaviors. While recent studies have focused on the staticmechanism of a certain behavior, the learning dynamics inside a model remain tobe explored. In this work, we develop a fine-tuning method for analyzing themechanism behind learning. Inspired by the concept of intrinsic dimension, weview a model as a computational graph with redundancy for a specific task, andtreat the fine-tuning process as a search for and optimization of a subgraphwithin this graph. Based on this hypothesis, we propose circuit-tuning, analgorithm that iteratively builds the subgraph for a specific task and updatesthe relevant parameters in a heuristic way. We first validate our hypothesisthrough a carefully designed experiment and provide a detailed analysis of thelearning dynamics during fine-tuning. Subsequently, we conduct experiments onmore complex tasks, demonstrating that circuit-tuning could strike a balancebetween the performance on the target task and the general capabilities. Ourwork offers a new analytical method for the dynamics of fine-tuning, providesnew findings on the mechanisms behind the training process, and inspires thedesign of superior algorithms for the training of neural networks.</description><author>Yueyan Li, Wenhao Gao, Caixia Yuan, Xiaojie Wang</author><pubDate>Tue, 23 Sep 2025 16:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06106v3</guid></item><item><title>Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models</title><link>http://arxiv.org/abs/2509.19191v1</link><description>Vision-Language Models (VLMs) have demonstrated remarkable performance acrossa variety of real-world tasks. However, existing VLMs typically process visualinformation by serializing images, a method that diverges significantly fromthe parallel nature of human vision. Moreover, their opaque internal mechanismshinder both deeper understanding and architectural innovation. Inspired by thedual-stream hypothesis of human vision, which distinguishes the "what" and"where" pathways, we deconstruct the visual processing in VLMs into objectrecognition and spatial perception for separate study. For object recognition,we convert images into text token maps and find that the model's perception ofimage content unfolds as a two-stage process from shallow to deep layers,beginning with attribute recognition and culminating in semanticdisambiguation. For spatial perception, we theoretically derive and empiricallyverify the geometric structure underlying the positional representation inVLMs. Based on these findings, we introduce an instruction-agnostic tokencompression algorithm based on a plug-and-play visual decoder to improvedecoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.Through rigorous experiments, our work validates these analyses, offering adeeper understanding of VLM internals and providing clear principles fordesigning more capable future architectures.</description><author>Yueyan Li, Chenggong Zhao, Zeyuan Zang, Caixia Yuan, Xiaojie Wang</author><pubDate>Tue, 23 Sep 2025 16:07:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19191v1</guid></item><item><title>Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws</title><link>http://arxiv.org/abs/2509.19189v1</link><description>Scaling laws have played a cornerstone role in guiding the training of largelanguage models (LLMs). However, most existing works on scaling laws primarilyfocus on the final-step loss, overlooking the loss dynamics during the trainingprocess and, crucially, the impact of learning rate schedule (LRS). In thispaper, we aim to bridge this gap by studying a teacher-student kernelregression setup trained via online stochastic gradient descent (SGD).Leveraging a novel intrinsic time viewpoint and stochastic differentialequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),which characterizes the evolution of population risk during the trainingprocess for general LRSs. Remarkably, the impact of the LRSs is capturedthrough an explicit convolution-type functional term, making their effectsfully tractable. To illustrate the utility of FSL, we analyze three widely usedLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- underboth data-limited and compute-limited regimes. We provide theoreticaljustification for widely adopted empirical practices in LLMs pre-training suchas (i) higher-capacity models are more data- and compute-efficient; (ii)learning rate decay can improve training efficiency; (iii) WSD-like schedulescan outperform direct-decay schedules. Lastly, we explore the practicalrelevance of FSL as a surrogate model for fitting, predicting and optimizingthe loss curves in LLM pre-training, with experiments conducted across modelsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepenthe understanding of LLM pre-training dynamics and provide insights forimproving large-scale model training.</description><author>Binghui Li, Fengling Chen, Zixun Huang, Lean Wang, Lei Wu</author><pubDate>Tue, 23 Sep 2025 16:05:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19189v1</guid></item><item><title>Topological Feature Compression for Molecular Graph Neural Networks</title><link>http://arxiv.org/abs/2508.07807v2</link><description>Recent advances in molecular representation learning have produced highlyeffective encodings of molecules for numerous cheminformatics andbioinformatics tasks. However, extracting general chemical insight whilebalancing predictive accuracy, interpretability, and computational efficiencyremains a major challenge. In this work, we introduce a novel Graph NeuralNetwork (GNN) architecture that combines compressed higher-order topologicalsignals with standard molecular features. Our approach captures globalgeometric information while preserving computational tractability andhuman-interpretable structure. We evaluate our model across a range ofbenchmarks, from small-molecule datasets to complex material datasets, anddemonstrate superior performance using a parameter-efficient architecture. Weachieve the best performing results in both accuracy and robustness acrossalmost all benchmarks. We open source all code \footnote{All code and resultscan be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.</description><author>Rahul Khorana</author><pubDate>Tue, 23 Sep 2025 15:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07807v2</guid></item><item><title>The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC</title><link>http://arxiv.org/abs/2509.19183v1</link><description>This technical report explores the MOSEv2 track of the LSVOS Challenge, whichtargets complex semi-supervised video object segmentation. By analysing andadapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of itslong-term memory and concept-aware memory, showing that long-term memorypreserves temporal continuity under occlusion and reappearance, whileconcept-aware memory supplies semantic priors that suppress distractors;together, these traits directly benefit several MOSEv2's core challenges. Oursolution achieves a JF score of 39.89% on the test set, ranking 1st in theMOSEv2 track of the LSVOS Challenge.</description><author>Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han</author><pubDate>Tue, 23 Sep 2025 15:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19183v1</guid></item><item><title>YAC: Bridging Natural Language and Interactive Visual Exploration with Generative AI for Biomedical Data Discovery</title><link>http://arxiv.org/abs/2509.19182v1</link><description>Incorporating natural language input has the potential to improve thecapabilities of biomedical data discovery interfaces. However, user interfaceelements and visualizations are still powerful tools for interacting with data,even in the new world of generative AI. In our prototype system, YAC, YetAnother Chatbot, we bridge the gap between natural language and interactivevisualizations by generating structured declarative output with a multi-agentsystem and interpreting that output to render linked interactive visualizationsand apply data filters. Furthermore, we include widgets, which allow users toadjust the values of that structured output through user interface elements. Wereflect on the capabilities and design of this system with an analysis of itstechnical dimensions and illustrate the capabilities through four usagescenarios.</description><author>Devin Lange, Shanghua Gao, Pengwei Sui, Austen Money, Priya Misner, Marinka Zitnik, Nils Gehlenborg</author><pubDate>Tue, 23 Sep 2025 15:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19182v1</guid></item><item><title>Individualized Mapping of Aberrant Cortical Thickness via Stochastic Cortical Self-Reconstruction</title><link>http://arxiv.org/abs/2403.06837v2</link><description>Understanding individual differences in cortical structure is key toadvancing diagnostics in neurology and psychiatry. Reference models aid indetecting aberrant cortical thickness, yet site-specific biases limit theirdirect application to unseen data, and region-wise averages prevent thedetection of localized cortical changes. To address these limitations, wedeveloped the Stochastic Cortical Self-Reconstruction (SCSR), a novel methodthat leverages deep learning to reconstruct cortical thickness maps at thevertex level without needing additional subject information. Trained on over25,000 healthy individuals, SCSR generates highly individualized corticalreconstructions that can detect subtle thickness deviations. Our evaluations onindependent test sets demonstrated that SCSR achieved significantly lowerreconstruction errors and identified atrophy patterns that enabled betterdisease discrimination than established methods. It also hints at corticalthinning in preterm infants that went undetected by existing models, showcasingits versatility. Finally, SCSR excelled in mapping highly resolved corticaldeviations of dementia patients from clinical data, highlighting its potentialfor supporting diagnosis in clinical practice.</description><author>Christian Wachinger, Dennis Hedderich, Melissa Thalhammer, Fabian Bongratz</author><pubDate>Tue, 23 Sep 2025 15:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06837v2</guid></item><item><title>SupertonicTTS: Towards Highly Efficient and Streamlined Text-to-Speech System</title><link>http://arxiv.org/abs/2503.23108v3</link><description>We introduce SupertonicTTS, a novel text-to-speech (TTS) system designed forefficient and streamlined speech synthesis. SupertonicTTS comprises threecomponents: a speech autoencoder for continuous latent representation, atext-to-latent module leveraging flow-matching for text-to-latent mapping, andan utterance-level duration predictor. To enable a lightweight architecture, weemploy a low-dimensional latent space, temporal compression of latents, andConvNeXt blocks. The TTS pipeline is further simplified by operating directlyon raw character-level text and employing cross-attention for text-speechalignment, thus eliminating the need for grapheme-to-phoneme (G2P) modules andexternal aligners. In addition, we propose context-sharing batch expansion thataccelerates loss convergence and stabilizes text-speech alignment with minimalmemory and I/O overhead. Experimental results demonstrate that SupertonicTTSdelivers performance comparable to contemporary zero-shot TTS models with only44M parameters, while significantly reducing architectural complexity andcomputational cost. Audio samples are available at:https://supertonictts.github.io/.</description><author>Hyeongju Kim, Jinhyeok Yang, Yechan Yu, Seunghun Ji, Jacob Morton, Frederik Bous, Joon Byun, Juheon Lee</author><pubDate>Tue, 23 Sep 2025 15:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.23108v3</guid></item><item><title>Hierarchical Evaluation Function: A Multi-Metric Approach for Optimizing Demand Forecasting Models</title><link>http://arxiv.org/abs/2508.13057v4</link><description>Accurate demand forecasting is crucial for effective inventory management indynamic and competitive environments, where decisions are influenced byuncertainty, financial constraints, and logistical limitations. Traditionalevaluation metrics such as Mean Absolute Error (MAE) and Root Mean SquaredError (RMSE) provide complementary perspectives but may lead to biasedassessments when applied individually. To address this limitation, we proposethe Hierarchical Evaluation Function (HEF), a composite function thatintegrates R2, MAE, and RMSE within a hierarchical and adaptive framework. Thefunction incorporates dynamic weights, tolerance thresholds derived from thestatistical properties of the series, and progressive penalty mechanisms toensure robustness against extreme errors and invalid predictions. HEF wasimplemented to optimize multiple forecasting models using Grid Search, ParticleSwarm Optimization (PSO), and Optuna, and tested on benchmark datasetsincluding Walmart, M3, M4, and M5. Experimental results, validated throughstatistical tests, demonstrate that HEF consistently outperforms MAE as anevaluation function in global metrics such as R2, Global Relative Accuracy(GRA), RMSE, and RMSSE, thereby providing greater explanatory power,adaptability, and stability. While MAE retains advantages in simplicity andefficiency, HEF proves more effective for long-term planning and complexcontexts. Overall, HEF constitutes a robust and adaptive alternative for modelselection and hyperparameter optimization in highly variable demand forecastingenvironments.</description><author>Adolfo González, Víctor Parada</author><pubDate>Tue, 23 Sep 2025 15:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13057v4</guid></item><item><title>Soft Tokens, Hard Truths</title><link>http://arxiv.org/abs/2509.19170v1</link><description>The use of continuous instead of discrete tokens during the Chain-of-Thought(CoT) phase of reasoning LLMs has garnered attention recently, based on theintuition that a continuous mixture of discrete tokens could simulate asuperposition of several reasoning paths simultaneously. Theoretical resultshave formally proven that continuous tokens have much greater expressivity andcan solve specific problems more efficiently. However, practical use ofcontinuous tokens has been limited by strong training difficulties: previousworks either just use continuous tokens at inference time on a pre-traineddiscrete-token model, or must distill the continuous CoT from ground-truthdiscrete CoTs and face computational costs that limit the CoT to very fewtokens. This is the first work introducing a scalable method to learn continuous CoTsvia reinforcement learning (RL), without distilling from reference discreteCoTs. We use "soft" tokens: mixtures of tokens together with noise on the inputembedding to provide RL exploration. Computational overhead is minimal,enabling us to learn continuous CoTs with hundreds of tokens. On math reasoningbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTsmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showinggreater CoT diversity. In systematic comparisons, the best-performing scenariois to train with continuous CoT tokens then use discrete tokens for inference,meaning the "soft" models can be deployed in a standard way. Finally, we showcontinuous CoT RL training better preserves the predictions of the base modelon out-of-domain tasks, thus providing a softer touch to the base model.</description><author>Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, Yann Ollivier</author><pubDate>Tue, 23 Sep 2025 15:43:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19170v1</guid></item><item><title>YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives</title><link>http://arxiv.org/abs/2509.19166v1</link><description>Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormalmucosal cell proliferation called polyps in the inner wall of the colon. Whenleft undetected, polyps can become malignant tumors. Colonoscopy is thestandard procedure for detecting polyps, as it enables direct visualization andremoval of suspicious lesions. Manual detection by colonoscopy can beinconsistent and is subject to oversight. Therefore, object detection based ondeep learning offers a better solution for a more accurate and real-timediagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-basedpolyp detection pipeline, trained using M2IoU loss, versatile dataaugmentations and negative data to replicate real clinical situations. Ourpipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolypdatasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-segdataset. The significant increase is achieved in mAP$_{50:95}$ score, showingthe precision of polyp detection. We show robustness based on polyp size andprecise location detection, making it clinically relevant in AI-assistedcolorectal screening.</description><author>Siddharth Gupta, Jitin Singla</author><pubDate>Tue, 23 Sep 2025 15:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19166v1</guid></item><item><title>RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions</title><link>http://arxiv.org/abs/2509.19165v1</link><description>Recent self-supervised stereo matching methods have made significantprogress, but their performance significantly degrades under adverse weatherconditions such as night, rain, and fog. We identify two primary weaknessescontributing to this performance degradation. First, adverse weather introducesnoise and reduces visibility, making CNN-based feature extractors struggle withdegraded regions like reflective and textureless areas. Second, these degradedregions can disrupt accurate pixel correspondences, leading to ineffectivesupervision based on the photometric consistency assumption. To address thesechallenges, we propose injecting robust priors derived from the visualfoundation model into the CNN-based feature extractor to improve featurerepresentation under adverse weather conditions. We then introduce scenecorrespondence priors to construct robust supervisory signals rather thanrelying solely on the photometric consistency assumption. Specifically, wecreate synthetic stereo datasets with realistic weather degradations. Thesedatasets feature clear and adverse image pairs that maintain the same semanticcontext and disparity, preserving the scene correspondence property. With thisknowledge, we propose a robust self-supervised training paradigm, consisting oftwo key steps: robust self-supervised scene correspondence learning and adverseweather distillation. Both steps aim to align underlying scene results fromclean and adverse image pairs, thus improving model disparity estimation underadverse weather effects. Extensive experiments demonstrate the effectivenessand versatility of our proposed solution, which outperforms existingstate-of-the-art self-supervised methods. Codes are available at\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.</description><author>Yun Wang, Junjie Hu, Junhui Hou, Chenghao Zhang, Renwei Yang, Dapeng Oliver Wu</author><pubDate>Tue, 23 Sep 2025 15:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19165v1</guid></item><item><title>Your Turn: At Home Turning Angle Estimation for Parkinson's Disease Severity Assessment</title><link>http://arxiv.org/abs/2408.08182v4</link><description>People with Parkinson's Disease (PD) often experience progressively worseninggait, including changes in how they turn around, as the disease progresses.Existing clinical rating tools are not capable of capturing hour-by-hourvariations of PD symptoms, as they are confined to brief assessments withinclinic settings. Measuring gait turning angles continuously and passively is acomponent step towards using gait characteristics as sensitive indicators ofdisease progression in PD. This paper presents a deep learning-based approachto automatically quantify turning angles by extracting 3D skeletons from videosand calculating the rotation of hip and knee joints. We utilisestate-of-the-art human pose estimation models, Fastpose and StridedTransformer, on a total of 1386 turning video clips from 24 subjects (12 peoplewith PD and 12 healthy control volunteers), trimmed from a PD dataset ofunscripted free-living videos in a home-like setting (Turn-REMAP). We alsocurate a turning video dataset, Turn-H3.6M, from the public Human3.6M humanpose benchmark with 3D ground truth, to further validate our method. Previousgait research has primarily taken place in clinics or laboratories evaluatingscripted gait outcomes, but this work focuses on free-living home settingswhere complexities exist, such as baggy clothing and poor lighting. Due todifficulties in obtaining accurate ground truth data in a free-living setting,we quantise the angle into the nearest bin $45^\circ$ based on the manuallabelling of expert clinicians. Our method achieves a turning calculationaccuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7{\deg}, and a weightedprecision WPrec of 68.3% for Turn-REMAP. This is the first work to explore theuse of single monocular camera data to quantify turns by PD patients in a homesetting.</description><author>Qiushuo Cheng, Catherine Morgan, Arindam Sikdar, Alessandro Masullo, Alan Whone, Majid Mirmehdi</author><pubDate>Tue, 23 Sep 2025 15:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08182v4</guid></item><item><title>Measuring AI "Slop" in Text</title><link>http://arxiv.org/abs/2509.19163v1</link><description>AI "slop" is an increasingly popular term used to describe low-qualityAI-generated text, but there is currently no agreed upon definition of thisterm nor a means to measure its occurrence. In this work, we develop a taxonomyof "slop" through interviews with experts in NLP, writing, and philosophy, andpropose a set of interpretable dimensions for its assessment in text. Throughspan-level annotation, we find that binary "slop" judgments are (somewhat)subjective, but such determinations nonetheless correlate with latentdimensions such as coherence and relevance. Our framework can be used toevaluate AI-generated text in both detection and binary preference tasks,potentially offering new insights into the linguistic and stylistic factorsthat contribute to quality judgments.</description><author>Chantal Shaib, Tuhin Chakrabarty, Diego Garcia-Olano, Byron C. Wallace</author><pubDate>Tue, 23 Sep 2025 15:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19163v1</guid></item><item><title>CayleyPy Growth: Efficient growth computations and hundreds of new conjectures on Cayley graphs (Brief version)</title><link>http://arxiv.org/abs/2509.19162v1</link><description>This is the third paper of the CayleyPy project applying artificialintelligence to problems in group theory. We announce the first public releaseof CayleyPy, an open source Python library for computations with Cayley andSchreier graphs. Compared with systems such as GAP and Sage, CayleyPy handlesmuch larger graphs and performs several orders of magnitude faster. Using CayleyPy we obtained about 200 new conjectures on Cayley and Schreiergraphs, focused on diameters and growth. For many Cayley graphs of symmetricgroups Sn we observe quasi polynomial diameter formulas: a small set ofquadratic or linear polynomials indexed by n mod s. We conjecture that this isa general phenomenon, giving efficient diameter computation despite the problembeing NP hard. We propose a refinement of the Babai type conjecture ondiameters of Sn: n^2/2 + 4n upper bounds in the undirected case, compared toprevious O(n^2) bounds. We also provide explicit generator families, related toinvolutions in a square with whiskers pattern, conjectured to maximize thediameter; search confirms this for all n up to 15. We further conjecture ananswer to a question posed by V M Glushkov in 1968 on directed Cayley graphsgenerated by a cyclic shift and a transposition. For nilpotent groups we conjecture an improvement of J S Ellenberg's resultson upper unitriangular matrices over Z/pZ, showing linear dependence ofdiameter on p. Moreover. Some conjectures are LLM friendly, naturally stated as sorting problemsverifiable by algorithms or Python code. To benchmark path finding we createdmore than 10 Kaggle datasets. CayleyPy works with arbitrary permutation ormatrix groups and includes over 100 predefined generators. Our growthcomputation code outperforms GAP and Sage up to 1000 times in speed and size.</description><author>A. Chervov, D. Fedoriaka, E. Konstantinova, A. Naumov, I. Kiselev, A. Sheveleva, I. Koltsov, S. Lytkin, A. Smolensky, A. Soibelman, F. Levkovich-Maslyuk, R. Grimov, D. Volovich, A. Isakov, A. Kostin, M. Litvinov, N. Vilkin-Krom, A. Bidzhiev, A. Krasnyi, M. Evseev, E. Geraseva, L. Grunwald, S. Galkin, E. Koldunov, S. Diner, A. Chevychelov, E. Kudasheva, A. Sychev, A. Kravchenko, Z. Kogan, A. Natyrova, L. Shishina, L. Cheldieva, V. Zamkovoy, D. Kovalenko, O. Papulov, S. Kudashev, D. Shiltsov, R. Turtayev, O. Nikitina, D. Mamayeva, S. Nikolenko, M. Obozov, A. Titarenko, A. Dolgorukova, A. Aparnev, O. Debeaupuis, S. Alami C., H. Isambert</author><pubDate>Tue, 23 Sep 2025 15:40:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19162v1</guid></item><item><title>Circuit Complexity From Physical Constraints: Scaling Limitations of Attention</title><link>http://arxiv.org/abs/2509.19161v1</link><description>We argue that the standard circuit complexity measures derived from $NC, AC,TC$ provide limited practical information and are now insufficient to furtherdifferentiate model expressivity. To address these new limitations, we define anovel notion of local uniformity and a family of circuit complexity classes$RC(\cdot)$ that capture the fundamental constraints of scaling physicalcircuits. Through the lens of $RC(\cdot)$, we show that attention mechanismswith $\omega(n^{3/2})$ runtime cannot scale to accommodate the entropy ofincreasingly complex datasets. Our results simultaneously provide a methodologyfor defining meaningful bounds on transformer expressivity and naturally exposethe restricted viability of attention.</description><author>Benjamin Prada, Ankur Mali</author><pubDate>Tue, 23 Sep 2025 15:40:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19161v1</guid></item><item><title>Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery</title><link>http://arxiv.org/abs/2508.17681v3</link><description>Bold claims about AI's role in science-from "AGI will cure all diseases" topromises of radically accelerated discovery-raise a central epistemic question:do large language models (LLMs) truly generate new knowledge, or do they merelyremix memorized fragments? We propose unlearning-as-ablation as a falsifiableprobe of constructive scientific discovery. The idea is to systematicallyremove a target result together with its forget-closure (supporting lemmas,paraphrases, and multi-hop entailments) and then evaluate whether the model canre-derive the result from only permitted axioms and tools. Success wouldindicate generative capability beyond recall; failure would expose currentlimits. Unlike prevailing motivations for unlearning-privacy, copyright, orsafety-our framing repositions it as an epistemic probe for AI-for-Science. Weoutline a minimal pilot in mathematics and algorithms to illustratefeasibility, and sketch how the same approach could later be extended todomains such as physics or chemistry. This is a position paper: ourcontribution is conceptual and methodological, not empirical. We aim tostimulate discussion on how principled ablation tests could help distinguishmodels that reconstruct knowledge from those that merely retrieve it, and howsuch probes might guide the next generation of AI-for-Science benchmarks.</description><author>Robert Yang</author><pubDate>Tue, 23 Sep 2025 15:40:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17681v3</guid></item><item><title>Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL</title><link>http://arxiv.org/abs/2509.09177v2</link><description>We propose FSPO (Fair Sequence Policy Optimization), a sequence-levelreinforcement learning method for LLMs that enforces length-fair clipping onthe importance-sampling (IS) weight. We study RL methods with sequence-level ISand identify a mismatch when PPO/GRPO-style clipping is transplanted tosequences: a fixed clip range systematically reweights short vs.\ longresponses, distorting the optimization direction. FSPO introduces a simpleremedy: we clip the sequence log-IS ratio with a band that scales as$\sqrt{L}$. Theoretically, we formalize length fairness via a LengthReweighting Error (LRE) and prove that small LRE yields a cosine directionalguarantee between the clipped and true updates. Empirically, FSPO flattens cliprates across length bins, stabilizes training, and outperforms all baselinesacross multiple evaluation datasets on Qwen3-8B-Base model.</description><author>Hanyi Mao, Quanjia Xiao, Lei Pang, Haixiao Liu</author><pubDate>Tue, 23 Sep 2025 15:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.09177v2</guid></item><item><title>Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks</title><link>http://arxiv.org/abs/2505.08022v3</link><description>Deployment of neural networks on resource-constrained devices demands modelsthat are both compact and robust to adversarial inputs. However, compressionand adversarial robustness often conflict. In this work, we introduce adynamical low-rank training scheme enhanced with a novel spectral regularizerthat controls the condition number of the low-rank core in each layer. Thisapproach mitigates the sensitivity of compressed models to adversarialperturbations without sacrificing accuracy on clean data. The method is model-and data-agnostic, computationally efficient, and supports rank adaptivity toautomatically compress the network at hand. Extensive experiments acrossstandard architectures, datasets, and adversarial attacks show the regularizednetworks can achieve over 94% compression while recovering or improvingadversarial accuracy relative to uncompressed baselines.</description><author>Steffen Schotthöfer, H. Lexie Yang, Stefan Schnake</author><pubDate>Tue, 23 Sep 2025 15:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.08022v3</guid></item><item><title>One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning</title><link>http://arxiv.org/abs/2508.01561v5</link><description>Generalizing to complex and temporally extended task objectives and safetyconstraints remains a critical challenge in reinforcement learning (RL). Lineartemporal logic (LTL) offers a unified formalism to specify such requirements,yet existing methods are limited in their abilities to handle nestedlong-horizon tasks and safety constraints, and cannot identify situations whena subgoal is not satisfiable and an alternative should be sought. In thispaper, we introduce GenZ-LTL, a method that enables zero-shot generalization toarbitrary LTL specifications. GenZ-LTL leverages the structure of B\"uchiautomata to decompose an LTL task specification into sequences of reach-avoidsubgoals. Contrary to the current state-of-the-art method that conditions onsubgoal sequences, we show that it is more effective to achieve zero-shotgeneralization by solving these reach-avoid problems \textit{one subgoal at atime} through proper safe RL formulations. In addition, we introduce a novelsubgoal-induced observation reduction technique that can mitigate theexponential complexity of subgoal-state combinations under realisticassumptions. Empirical results show that GenZ-LTL substantially outperformsexisting methods in zero-shot generalization to unseen LTL specifications.</description><author>Zijian Guo, İlker Işık, H. M. Sabbir Ahmad, Wenchao Li</author><pubDate>Tue, 23 Sep 2025 15:38:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.01561v5</guid></item></channel></rss>