<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 19 Sep 2024 01:00:35 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework</title><link>http://arxiv.org/abs/2409.10289v2</link><description>Empathetic response generation necessitates the integration of emotional andintentional dynamics to foster meaningful interactions. Existing researcheither neglects the intricate interplay between emotion and intent, leading tosuboptimal controllability of empathy, or resorts to large language models(LLMs), which incur significant computational overhead. In this paper, weintroduce ReflectDiffu, a lightweight and comprehensive framework forempathetic response generation. This framework incorporates emotion contagionto augment emotional expressiveness and employs an emotion-reasoning mask topinpoint critical emotional elements. Additionally, it integrates intentmimicry within reinforcement learning for refinement during diffusion. Byharnessing an intent twice reflect the mechanism ofExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotionaldecision-making into precise intent actions, thereby addressing empatheticresponse misalignments stemming from emotional misrecognition. Throughreflection, the framework maps emotional states to intents, markedly enhancingboth response empathy and flexibility. Comprehensive experiments reveal thatReflectDiffu outperforms existing models regarding relevance, controllability,and informativeness, achieving state-of-the-art results in both automatic andhuman evaluations.</description><author>Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem</author><pubDate>Wed, 18 Sep 2024 17:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10289v2</guid></item><item><title>Contextual Breach: Assessing the Robustness of Transformer-based QA Models</title><link>http://arxiv.org/abs/2409.10997v2</link><description>Contextual question-answering models are susceptible to adversarialperturbations to input context, commonly observed in real-world scenarios.These adversarial noises are designed to degrade the performance of the modelby distorting the textual input. We introduce a unique dataset thatincorporates seven distinct types of adversarial noise into the context, eachapplied at five different intensity levels on the SQuAD dataset. To quantifythe robustness, we utilize robustness metrics providing a standardized measurefor assessing model performance across varying noise types and levels.Experiments on transformer-based question-answering models reveal robustnessvulnerabilities and important insights into the model's performance inrealistic textual input.</description><author>Asir Saadat, Nahian Ibn Asad, Md Farhan Ishmam</author><pubDate>Wed, 18 Sep 2024 16:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10997v2</guid></item><item><title>Visualizing Temporal Topic Embeddings with a Compass</title><link>http://arxiv.org/abs/2409.10649v2</link><description>Dynamic topic modeling is useful at discovering the development and change inlatent topics over time. However, present methodology relies on algorithms thatseparate document and word representations. This prevents the creation of ameaningful embedding space where changes in word usage and documents can bedirectly analyzed in a temporal context. This paper proposes an expansion ofthe compass-aligned temporal Word2Vec methodology into dynamic topic modeling.Such a method allows for the direct comparison of word and document embeddingsacross time in dynamic topics. This enables the creation of visualizations thatincorporate temporal word embeddings within the context of documents into topicvisualizations. In experiments against the current state-of-the-art, ourproposed method demonstrates overall competitive performance in topic relevancyand diversity across temporal datasets of varying size. Simultaneously, itprovides insightful visualizations focused on temporal word embeddings whilemaintaining the insights provided by global topic evolution, advancing ourunderstanding of how topics evolve over time.</description><author>Daniel Palamarchuk, Lemara Williams, Brian Mayer, Thomas Danielson, Rebecca Faust, Larry Deschaine, Chris North</author><pubDate>Wed, 18 Sep 2024 15:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10649v2</guid></item><item><title>Operational Wind Speed Forecasts for Chile's Electric Power Sector Using a Hybrid ML Model</title><link>http://arxiv.org/abs/2409.09263v3</link><description>As Chile's electric power sector advances toward a future powered byrenewable energy, accurate forecasting of renewable generation is essential formanaging grid operations. The integration of renewable energy sources isparticularly challenging due to the operational difficulties of managing theirpower generation, which is highly variable compared to fossil fuel sources,delaying the availability of clean energy. To mitigate this, we quantify theimpact of increasing intermittent generation from wind and solar on thermalpower plants in Chile and introduce a hybrid wind speed forecasting methodologywhich combines two custom ML models for Chile. The first model is based onTiDE, an MLP-based ML model for short-term forecasts, and the second is basedon a graph neural network, GraphCast, for medium-term forecasts up to 10 days.Our hybrid approach outperforms the most accurate operational deterministicsystems by 4-21% for short-term forecasts and 5-23% for medium-term forecastsand can directly lower the impact of wind generation on thermal ramping,curtailment, and system-level emissions in Chile.</description><author>Dhruv Suri, Praneet Dutta, Flora Xue, Ines Azevedo, Ravi Jain</author><pubDate>Wed, 18 Sep 2024 15:17:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09263v3</guid></item><item><title>LOLA -- An Open-Source Massively Multilingual Large Language Model</title><link>http://arxiv.org/abs/2409.11272v2</link><description>This paper presents LOLA, a massively multilingual large language modeltrained on more than 160 languages using a sparse Mixture-of-ExpertsTransformer architecture. Our architectural and implementation choices addressthe challenge of harnessing linguistic diversity while maintaining efficiencyand avoiding the common pitfalls of multilinguality. Our analysis of theevaluation results shows competitive performance in natural language generationand understanding tasks. Additionally, we demonstrate how the learnedexpert-routing mechanism exploits implicit phylogenetic linguistic patterns topotentially alleviate the curse of multilinguality. We provide an in-depth lookat the training process, an analysis of the datasets, and a balancedexploration of the model's strengths and limitations. As an open-source model,LOLA promotes reproducibility and serves as a robust foundation for futureresearch. Our findings enable the development of compute-efficient multilingualmodels with strong, scalable performance across languages.</description><author>Nikit Srivastava, Denis Kuchelev, Tatiana Moteu, Kshitij Shetty, Michael Röder, Diego Moussallem, Hamada Zahera, Axel-Cyrille Ngonga Ngomo</author><pubDate>Wed, 18 Sep 2024 13:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11272v2</guid></item><item><title>OneEncoder: A Lightweight Framework for Progressive Alignment of Modalities</title><link>http://arxiv.org/abs/2409.11059v2</link><description>Cross-modal alignment Learning integrates information from differentmodalities like text, image, audio and video to create unified models. Thisapproach develops shared representations and learns correlations betweenmodalities, enabling applications such as visual question answering andaudiovisual content analysis. Current techniques rely on largemodality-specific encoders, necessitating fine-tuning or training from scratchon vast aligned datasets (e.g., text-image, text-audio, image-audio). Thisapproach has limitations: (i) it is very expensive due to the need for traininglarge encoders on extensive datasets, (ii) acquiring aligned large paireddatasets is challenging, and (iii) adding new modalities requires retrainingthe entire framework to incorporate these modalities. To address these issues,we propose OneEncoder, a lightweight framework that progressively representsand aligns four modalities (image, text, audio, video). Initially, we train alightweight Universal Projection module (UP) to align image and textmodalities. Then, we freeze the pretrained UP and progressively align futuremodalities to those already aligned. OneEncoder operates efficiently andcost-effectively, even in scenarios where vast aligned datasets areunavailable, due to its lightweight design. Trained on small paired datasets,it shows strong performance in tasks like classification, querying, and visualquestion answering, surpassing methods that rely on large datasets andspecialized encoders.</description><author>Bilal Faye, Hanane Azzag, Mustapha Lebbah</author><pubDate>Wed, 18 Sep 2024 13:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11059v2</guid></item><item><title>RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval</title><link>http://arxiv.org/abs/2409.10516v2</link><description>Transformer-based Large Language Models (LLMs) have become increasinglyimportant. However, due to the quadratic time complexity of attentioncomputation, scaling LLMs to longer contexts incurs extremely slow inferencelatency and high GPU memory consumption for caching key-value (KV) vectors.This paper proposes RetrievalAttention, a training-free approach to bothaccelerate attention computation and reduce GPU memory consumption. Byleveraging the dynamic sparsity of attention mechanism, RetrievalAttentionproposes to use approximate nearest neighbor search (ANNS) indexes for KVvectors in CPU memory and retrieves the most relevant ones with vector searchduring generation. Unfortunately, we observe that the off-the-shelf ANNSindexes are often ineffective for such retrieval tasks due to theout-of-distribution (OOD) between query vectors and key vectors in attentionmechanism. RetrievalAttention addresses the OOD challenge by designing anattention-aware vector search algorithm that can adapt to the distribution ofquery vectors. Our evaluation shows that RetrievalAttention only needs toaccess 1--3% of data while maintaining high model accuracy. This leads tosignificant reduction in the inference cost of long-context LLMs with muchlower GPU memory footprint. In particular, RetrievalAttention only needs asingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8Bparameters, which is capable of generating one token in 0.188 seconds.</description><author>Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu</author><pubDate>Wed, 18 Sep 2024 13:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10516v2</guid></item><item><title>Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary Detection</title><link>http://arxiv.org/abs/2409.08513v3</link><description>Open-vocabulary detection (OVD) aims to detect objects beyond a predefinedset of categories. As a pioneering model incorporating the YOLO series intoOVD, YOLO-World is well-suited for scenarios prioritizing speed and efficiency.However, its performance is hindered by its neck feature fusion mechanism,which causes the quadratic complexity and the limited guided receptive fields.To address these limitations, we present Mamba-YOLO-World, a novel YOLO-basedOVD model employing the proposed MambaFusion Path Aggregation Network(MambaFusion-PAN) as its neck architecture. Specifically, we introduce aninnovative State Space Model-based feature fusion mechanism consisting of aParallel-Guided Selective Scan algorithm and a Serial-Guided Selective Scanalgorithm with linear complexity and globally guided receptive fields. Itleverages multi-modal input sequences and mamba hidden states to guide theselective scanning process. Experiments demonstrate that our model outperformsthe original YOLO-World on the COCO and LVIS benchmarks in both zero-shot andfine-tuning settings while maintaining comparable parameters and FLOPs.Additionally, it surpasses existing state-of-the-art OVD methods with fewerparameters and FLOPs.</description><author>Haoxuan Wang, Qingdong He, Jinlong Peng, Hao Yang, Mingmin Chi, Yabiao Wang</author><pubDate>Wed, 18 Sep 2024 12:30:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08513v3</guid></item><item><title>GEIC: Universal and Multilingual Named Entity Recognition with Large Language Models</title><link>http://arxiv.org/abs/2409.11022v2</link><description>Large Language Models (LLMs) have supplanted traditional methods in numerousnatural language processing tasks. Nonetheless, in Named Entity Recognition(NER), existing LLM-based methods underperform compared to baselines andrequire significantly more computational resources, limiting their application.In this paper, we introduce the task of generation-based extraction andin-context classification (GEIC), designed to leverage LLMs' prior knowledgeand self-attention mechanisms for NER tasks. We then propose CascadeNER, auniversal and multilingual GEIC framework for few-shot and zero-shot NER.CascadeNER employs model cascading to utilize two small-parameter LLMs toextract and classify independently, reducing resource consumption whileenhancing accuracy. We also introduce AnythingNER, the first NER datasetspecifically designed for LLMs, including 8 languages, 155 entity types and anovel dynamic categorization system. Experiments show that CascadeNER achievesstate-of-the-art performance on low-resource and fine-grained scenarios,including CrossNER and FewNERD. Our work is openly accessible.</description><author>Hanjun Luo, Yingbin Jin, Xuecheng Liu, Tong Shang, Ruizhe Chen, Zuozhu Liu</author><pubDate>Wed, 18 Sep 2024 10:05:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11022v2</guid></item><item><title>High-Order Evolving Graphs for Enhanced Representation of Traffic Dynamics</title><link>http://arxiv.org/abs/2409.11206v2</link><description>We present an innovative framework for traffic dynamics analysis usingHigh-Order Evolving Graphs, designed to improve spatio-temporal representationsin autonomous driving contexts. Our approach constructs temporal bidirectionalbipartite graphs that effectively model the complex interactions within trafficscenes in real-time. By integrating Graph Neural Networks (GNNs) withhigh-order multi-aggregation strategies, we significantly enhance the modelingof traffic scene dynamics, providing a more accurate and detailed analysis ofthese interactions. Additionally, we incorporate inductive learning techniquesinspired by the GraphSAGE framework, enabling our model to adapt to new andunseen traffic scenarios without the need for retraining, thus ensuring robustgeneralization. Through extensive experiments on the ROAD and ROAD Waymodatasets, we establish a comprehensive baseline for further developments,demonstrating the potential of our method in accurately capturing trafficbehavior. Our results emphasize the value of high-order statistical moments andfeature-gated attention mechanisms in improving traffic behavior analysis,laying the groundwork for advancing autonomous driving technologies. Our sourcecode is available at: https://github.com/Addy-1998/High_Order_Graphs</description><author>Aditya Humnabadkar, Arindam Sikdar, Benjamin Cave, Huaizhong Zhang, Paul Bakaki, Ardhendu Behera</author><pubDate>Wed, 18 Sep 2024 09:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11206v2</guid></item><item><title>The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives</title><link>http://arxiv.org/abs/2409.11261v2</link><description>This paper introduces the concept of an education tool that utilizesGenerative Artificial Intelligence (GenAI) to enhance storytelling forchildren. The system combines GenAI-driven narrative co-creation,text-to-speech conversion, and text-to-video generation to produce an engagingexperience for learners. We describe the co-creation process, the adaptation ofnarratives into spoken words using text-to-speech models, and thetransformation of these narratives into contextually relevant visuals throughtext-to-video technology. Our evaluation covers the linguistics of thegenerated stories, the text-to-speech conversion quality, and the accuracy ofthe generated visuals.</description><author>Samee Arif, Taimoor Arif, Aamina Jamal Khan, Muhammad Saad Haroon, Agha Ali Raza, Awais Athar</author><pubDate>Wed, 18 Sep 2024 09:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11261v2</guid></item><item><title>Annealed Winner-Takes-All for Motion Forecasting</title><link>http://arxiv.org/abs/2409.11172v2</link><description>In autonomous driving, motion prediction aims at forecasting the futuretrajectories of nearby agents, helping the ego vehicle to anticipate behaviorsand drive safely. A key challenge is generating a diverse set of futurepredictions, commonly addressed using data-driven models with Multiple ChoiceLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.However, these methods face initialization sensitivity and traininginstabilities. Additionally, to compensate for limited performance, someapproaches rely on training with a large set of hypotheses, requiring apost-selection step during inference to significantly reduce the number ofpredictions. To tackle these issues, we take inspiration from annealed MCL, arecently introduced technique that improves the convergence properties of MCLmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, wedemonstrate how the aWTA loss can be integrated with state-of-the-art motionforecasting models to enhance their performance using only a minimal set ofhypotheses, eliminating the need for the cumbersome post-selection step. Ourapproach can be easily incorporated into any trajectory prediction modelnormally trained using WTA and yields significant improvements. To facilitatethe application of our approach to future motion forecasting models, the codewill be made publicly available upon acceptance:https://github.com/valeoai/MF_aWTA.</description><author>Yihong Xu, Victor Letzelter, Mickaël Chen, Éloi Zablocki, Matthieu Cord</author><pubDate>Wed, 18 Sep 2024 09:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11172v2</guid></item><item><title>Active learning for energy-based antibody optimization and enhanced screening</title><link>http://arxiv.org/abs/2409.10964v2</link><description>Accurate prediction and optimization of protein-protein binding affinity iscrucial for therapeutic antibody development. Although machine learning-basedprediction methods $\Delta\Delta G$ are suitable for large-scale mutantscreening, they struggle to predict the effects of multiple mutations fortargets without existing binders. Energy function-based methods, though moreaccurate, are time consuming and not ideal for large-scale screening. Toaddress this, we propose an active learning workflow that efficiently trains adeep learning model to learn energy functions for specific targets, combiningthe advantages of both approaches. Our method integrates the RDE-Network deeplearning model with Rosetta's energy function-based Flex ddG to efficientlyexplore mutants. In a case study targeting HER2-binding Trastuzumab mutants,our approach significantly improved the screening performance over randomselection and demonstrated the ability to identify mutants with better bindingproperties without experimental $\Delta\Delta G$ data. This workflow advancescomputational antibody design by combining machine learning, physics-basedcomputations, and active learning to achieve more efficient antibodydevelopment.</description><author>Kairi Furui, Masahito Ohue</author><pubDate>Wed, 18 Sep 2024 07:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10964v2</guid></item><item><title>Exploring Fine-tuned Generative Models for Keyphrase Selection: A Case Study for Russian</title><link>http://arxiv.org/abs/2409.10640v2</link><description>Keyphrase selection plays a pivotal role within the domain of scholarlytexts, facilitating efficient information retrieval, summarization, andindexing. In this work, we explored how to apply fine-tuned generativetransformer-based models to the specific task of keyphrase selection withinRussian scientific texts. We experimented with four distinct generative models,such as ruT5, ruGPT, mT5, and mBART, and evaluated their performance in bothin-domain and cross-domain settings. The experiments were conducted on thetexts of Russian scientific abstracts from four domains: mathematics &amp; computerscience, history, medicine, and linguistics. The use of generative models,namely mBART, led to gains in in-domain performance (up to 4.9% in BERTScore,9.0% in ROUGE-1, and 12.2% in F1-score) over three keyphrase extractionbaselines for the Russian language. Although the results for cross-domain usagewere significantly lower, they still demonstrated the capability to surpassbaseline performances in several cases, underscoring the promising potentialfor further exploration and refinement in this research field.</description><author>Anna Glazkova, Dmitry Morozov</author><pubDate>Wed, 18 Sep 2024 07:35:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10640v2</guid></item><item><title>Performance of Cross-Validated Targeted Maximum Likelihood Estimation</title><link>http://arxiv.org/abs/2409.11265v2</link><description>Background: Advanced methods for causal inference, such as targeted maximumlikelihood estimation (TMLE), require certain conditions for statisticalinference. However, in situations where there is not differentiability due todata sparsity or near-positivity violations, the Donsker class condition isviolated. In such situations, TMLE variance can suffer from inflation of thetype I error and poor coverage, leading to conservative confidence intervals.Cross-validation of the TMLE algorithm (CVTMLE) has been suggested to improveon performance compared to TMLE in settings of positivity or Donsker classviolations. We aim to investigate the performance of CVTMLE compared to TMLE invarious settings. Methods: We utilised the data-generating mechanism as described in Leger etal. (2022) to run a Monte Carlo experiment under different Donsker classviolations. Then, we evaluated the respective statistical performances of TMLEand CVTMLE with different super learner libraries, with and without regressiontree methods. Results: We found that CVTMLE vastly improves confidence interval coveragewithout adversely affecting bias, particularly in settings with small samplesizes and near-positivity violations. Furthermore, incorporating regressiontrees using standard TMLE with ensemble super learner-based initial estimatesincreases bias and variance leading to invalid statistical inference. Conclusions: It has been shown that when using CVTMLE the Donsker classcondition is no longer necessary to obtain valid statistical inference whenusing regression trees and under either data sparsity or near-positivityviolations. We show through simulations that CVTMLE is much less sensitive tothe choice of the super learner library and thereby provides better estimationand inference in cases where the super learner library uses more flexiblecandidates and is prone to overfitting.</description><author>Matthew J. Smith, Rachael V. Phillips, Camille Maringe, Miguel Angel Luque-Fernandez</author><pubDate>Wed, 18 Sep 2024 07:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11265v2</guid></item><item><title>Propulsion: Steering LLM with Tiny Fine-Tuning</title><link>http://arxiv.org/abs/2409.10927v2</link><description>The rapid advancements in Large Language Models (LLMs) have revolutionizednatural language processing (NLP) and related fields. However, fine-tuningthese models for specific tasks remains computationally expensive and risksdegrading pre-learned features. To address these challenges, we proposePropulsion, a novel parameter efficient fine-tuning (PEFT) method designed tooptimize task-specific performance while drastically reducing computationaloverhead. Inspired by the concept of controlled adjustments in physical motion,Propulsion selectively re-scales specific dimensions of a pre-trained model,guiding output predictions toward task objectives without modifying the model'sparameters. By introducing lightweight, trainable Propulsion parameters at thepre-trained layer, we minimize the number of parameters updated duringfine-tuning, preventing overfitting or overwriting of existing knowledge. Ourtheoretical analysis, supported by Neural Tangent Kernel (NTK) theory, showsthat Propulsion approximates the performance of full fine-tuning with far fewertrainable parameters. Empirically, Propulsion reduces the parameter count from355.3 million to just 0.086 million, achieving over a 10x reduction compared tostandard approaches like LoRA while maintaining competitive performance acrossbenchmarks.</description><author>Md Kowsher, Nusrat Jahan Prottasha, Prakash Bhat</author><pubDate>Wed, 18 Sep 2024 07:23:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10927v2</guid></item><item><title>Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming</title><link>http://arxiv.org/abs/2409.11041v2</link><description>While there has been a lot of research recently on robots in householdenvironments, at the present time, most robots in existence can be found onshop floors, and most interactions between humans and robots happen there.``Collaborative robots'' (cobots) designed to work alongside humans on assemblylines traditionally require expert programming, limiting ability to makechanges, or manual guidance, limiting expressivity of the resulting programs.To address these limitations, we explore using Large Language Models (LLMs),and in particular, their abilities of doing in-context learning, forconversational code generation. As a first step, we define RATS, the``Repetitive Assembly Task'', a 2D building task designed to lay the foundationfor simulating industry assembly scenarios. In this task, a `programmer'instructs a cobot, using natural language, on how a certain assembly is to bebuilt; that is, the programmer induces a program, through natural language. Wecreate a dataset that pairs target structures with various example instructions(human-authored, template-based, and model-generated) and example code. Withthis, we systematically evaluate the capabilities of state-of-the-art LLMs forsynthesising this kind of code, given in-context examples. Evaluating in asimulated environment, we find that LLMs are capable of generating accurate`first order code' (instruction sequences), but have problems producing`higher-order code' (abstractions such as functions, or use of loops).</description><author>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</author><pubDate>Wed, 18 Sep 2024 07:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11041v2</guid></item><item><title>Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling</title><link>http://arxiv.org/abs/2409.11283v2</link><description>LLMs obtain remarkable performance but suffer from hallucinations. Mostresearch on detecting hallucination focuses on the questions with short andconcrete correct answers that are easy to check the faithfulness. Hallucinationdetections for text generation with open-ended answers are more challenging.Some researchers use external knowledge to detect hallucinations in generatedtexts, but external resources for specific scenarios are hard to access. Recentstudies on detecting hallucinations in long text without external resourcesconduct consistency comparison among multiple sampled outputs. To handle longtexts, researchers split long texts into multiple facts and individuallycompare the consistency of each pairs of facts. However, these methods (1)hardly achieve alignment among multiple facts; (2) overlook dependenciesbetween multiple contextual facts. In this paper, we propose a graph-basedcontext-aware (GCA) hallucination detection for text generations, which alignsknowledge facts and considers the dependencies between contextual knowledgetriples in consistency comparison. Particularly, to align multiple facts, weconduct a triple-oriented response segmentation to extract multiple knowledgetriples. To model dependencies among contextual knowledge triple (facts), weconstruct contextual triple into a graph and enhance triples' interactions viamessage passing and aggregating via RGCN. To avoid the omission of knowledgetriples in long text, we conduct a LLM-based reverse verification viareconstructing the knowledge triples. Experiments show that our model enhanceshallucination detection and excels all baselines.</description><author>Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li</author><pubDate>Wed, 18 Sep 2024 05:42:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11283v2</guid></item><item><title>MURRE: Multi-Hop Table Retrieval with Removal for Open-Domain Text-to-SQL</title><link>http://arxiv.org/abs/2402.10666v5</link><description>The open-domain text-to-SQL task aims to retrieve question-relevant tablesfrom massive databases and generate SQL. However, the performance of currentmethods is constrained by single-hop retrieval, and existing multi-hopretrieval of open-domain question answering is not directly applicable due tothe tendency to retrieve tables similar to the retrieved ones but irrelevant tothe question. Since the questions in text-to-SQL usually contain all requiredinformation, while previous multi-hop retrieval supplements the questions withretrieved documents. Therefore, we propose the multi-hop table retrieval withremoval (MURRE), which removes previously retrieved information from thequestion to guide the retriever towards unretrieved relevant tables. Ourexperiments on two open-domain text-to-SQL datasets demonstrate an averageimprovement of 5.7% over the previous state-of-the-art results.</description><author>Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu, Wanxiang Che</author><pubDate>Wed, 18 Sep 2024 02:48:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10666v5</guid></item><item><title>Beyond principlism: Practical strategies for ethical AI use in research practices</title><link>http://arxiv.org/abs/2401.15284v4</link><description>The rapid adoption of generative artificial intelligence (AI) in scientificresearch, particularly large language models (LLMs), has outpaced thedevelopment of ethical guidelines, leading to a Triple-Too problem: too manyhigh-level ethical initiatives, too abstract principles lacking contextual andpractical relevance, and too much focus on restrictions and risks over benefitsand utilities. Existing approaches, including principlism (reliance on abstractethical principles), formalism (rigid application of rules), and technicalsolutionism (overemphasis on technological fixes), offer little practicalguidance for addressing ethical challenges of AI in scientific researchpractices. To bridge the gap between abstract principles and day-to-dayresearch practices, a user-centered, realism-inspired approach is proposedhere. It outlines five specific goals for ethical AI use: 1) understandingmodel training and output, including bias mitigation strategies; 2) respectingprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policyviolations; 4) applying AI beneficially compared to alternatives; and 5) usingAI transparently and reproducibly. Each goal is accompanied by actionablestrategies and realistic cases of misuse and corrective measures. I argue thatethical AI application requires evaluating its utility against existingalternatives rather than isolated performance metrics. Additionally, I proposedocumentation guidelines to enhance transparency and reproducibility inAI-assisted research. Moving forward, we need targeted professionaldevelopment, training programs, and balanced enforcement mechanisms to promoteresponsible AI use while fostering innovation. By refining these ethicalguidelines and adapting them to emerging AI capabilities, we can acceleratescientific progress without compromising research integrity.</description><author>Zhicheng Lin</author><pubDate>Wed, 18 Sep 2024 02:03:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15284v4</guid></item><item><title>Mobility-GCN: a human mobility-based graph convolutional network for tracking and analyzing the spatial dynamics of the synthetic opioid crisis in the USA, 2013-2020</title><link>http://arxiv.org/abs/2409.09945v3</link><description>Synthetic opioids are the most common drugs involved in drug-involvedoverdose mortalities in the U.S. The Center for Disease Control and Preventionreported that in 2018, about 70% of all drug overdose deaths involved opioidsand 67% of all opioid-involved deaths were accounted for by synthetic opioids.In this study, we investigated the spread of synthetic opioids between 2013 and2020 in the U.S. We analyzed the relationship between the spatiotemporalpattern of synthetic opioid-involved deaths and another key opioid, heroin, andcompared patterns of deaths involving these two types of drugs during thisperiod. Spatial connections and human mobility between counties wereincorporated into a graph convolutional neural network model to represent andanalyze the spread of synthetic opioid-involved deaths in the context ofprevious heroin-involved death patterns.</description><author>Zhiyue Xia, Kathleen Stewart</author><pubDate>Wed, 18 Sep 2024 01:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09945v3</guid></item><item><title>MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</title><link>http://arxiv.org/abs/2305.04743v5</link><description>Evaluating car damages from misfortune is critical to the car insuranceindustry. However, the accuracy is still insufficient for real-worldapplications since the deep learning network is not designed for car damageimages as inputs, and its segmented masks are still very coarse. This paperpresents MARS (Mask Attention Refinement with Sequential quadtree nodes) forcar damage instance segmentation. Our MARS represents self-attention mechanismsto draw global dependencies between the sequential quadtree nodes layer andquadtree transformer to recalibrate channel weights and predict highly accurateinstance masks. Our extensive experiments demonstrate that MARS outperformsstate-of-the-art (SOTA) instance segmentation methods on three popularbenchmarks such as Mask R-CNN [9], PointRend [13], and Mask Transfiner [12], bya large margin of +1.3 maskAP-based R50-FPN backbone and +2.3 maskAP-basedR101-FPN backbone on Thai car-damage dataset. Our demos are available athttps://github.com/kaopanboonyuen/MARS.</description><author>Teerapong Panboonyuen, Naphat Nithisopa, Panin Pienroj, Laphonchai Jirachuphun, Chaiwasut Watthanasirikrit, Naruepon Pornwiriyakul</author><pubDate>Tue, 17 Sep 2024 21:29:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04743v5</guid></item><item><title>Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion</title><link>http://arxiv.org/abs/2409.11406v1</link><description>In 3D modeling, designers often use an existing 3D model as a reference tocreate new ones. This practice has inspired the development of Phidias, a novelgenerative model that uses diffusion for reference-augmented 3D generation.Given an image, our method leverages a retrieved or user-provided 3D referencemodel to guide the generation process, thereby enhancing the generationquality, generalization ability, and controllability. Our model integratesthree key components: 1) meta-ControlNet that dynamically modulates theconditioning strength, 2) dynamic reference routing that mitigates misalignmentbetween the input image and 3D reference, and 3) self-reference augmentationsthat enable self-supervised training with a progressive curriculum.Collectively, these designs result in a clear improvement over existingmethods. Phidias establishes a unified framework for 3D generation using text,image, and 3D conditions with versatile applications.</description><author>Zhenwei Wang, Tengfei Wang, Zexin He, Gerhard Hancke, Ziwei Liu, Rynson W. H. Lau</author><pubDate>Tue, 17 Sep 2024 17:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11406v1</guid></item><item><title>AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs</title><link>http://arxiv.org/abs/2409.11404v1</link><description>Arabic, with its rich diversity of dialects, remains significantlyunderrepresented in Large Language Models, particularly in dialectalvariations. We address this gap by introducing seven synthetic datasets indialects alongside Modern Standard Arabic (MSA), created using MachineTranslation (MT) combined with human post-editing. We present AraDiCE, abenchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs ondialect comprehension and generation, focusing specifically on low-resourceArabic dialects. Additionally, we introduce the first-ever fine-grainedbenchmark designed to evaluate cultural awareness across the Gulf, Egypt, andLevant regions, providing a novel dimension to LLM evaluation. Our findingsdemonstrate that while Arabic-specific models like Jais and AceGPT outperformmultilingual models on dialectal tasks, significant challenges persist indialect identification, generation, and translation. This work contributes ~45Kpost-edited samples, a cultural benchmark, and highlights the importance oftailored training to improve LLM performance in capturing the nuances ofdiverse Arabic dialects and cultural contexts. We will release the dialectaltranslation models and benchmarks curated in this study.</description><author>Basel Mousi, Nadir Durrani, Fatema Ahmad, Md. Arid Hasan, Maram Hasanain, Tameem Kabbani, Fahim Dalvi, Shammur Absar Chowdhury, Firoj Alam</author><pubDate>Tue, 17 Sep 2024 17:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11404v1</guid></item><item><title>NVLM: Open Frontier-Class Multimodal LLMs</title><link>http://arxiv.org/abs/2409.11402v1</link><description>We introduce NVLM 1.0, a family of frontier-class multimodal large languagemodels (LLMs) that achieve state-of-the-art results on vision-language tasks,rivaling the leading proprietary models (e.g., GPT-4o) and open-access models(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improvedtext-only performance over its LLM backbone after multimodal training. In termsof model design, we perform a comprehensive comparison between decoder-onlymultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,Flamingo). Based on the strengths and weaknesses of both approaches, we proposea novel architecture that enhances both training efficiency and multimodalreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design fortile-based dynamic high-resolution images, which significantly boostsperformance on multimodal reasoning and OCR-related tasks. Regarding trainingdata, we meticulously curate and provide detailed information on our multimodalpretraining and supervised fine-tuning datasets. Our findings indicate thatdataset quality and task diversity are more important than scale, even duringthe pretraining phase, across all architectures. Notably, we developproduction-grade multimodality for the NVLM-1.0 models, enabling them to excelin vision-language tasks while maintaining and even improving text-onlyperformance compared to their LLM backbones. To achieve this, we craft andintegrate a high-quality text-only dataset into multimodal training, alongsidea substantial amount of multimodal math and reasoning data, leading to enhancedmath and coding capabilities across modalities. To advance research in thefield, we are releasing the model weights and will open-source the code for thecommunity: https://nvlm-project.github.io/.</description><author>Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</author><pubDate>Tue, 17 Sep 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11402v1</guid></item><item><title>LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents</title><link>http://arxiv.org/abs/2409.11393v1</link><description>The integration of tools in LLM-based agents overcame the difficulties ofstandalone LLMs and traditional agents' limited capabilities. However, theconjunction of these technologies and the proposed enhancements in severalstate-of-the-art works followed a non-unified software architecture resultingin a lack of modularity. Indeed, they focused mainly on functionalities andoverlooked the definition of the component's boundaries within the agent. Thiscaused terminological and architectural ambiguities between researchers whichwe addressed in this paper by proposing a unified framework that establishes aclear foundation for LLM-based agents' development from both functional andsoftware architectural perspectives. Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),clearly distinguishes between the different components of an agent, settingLLMs, and tools apart from a newly introduced element: the core-agent, playingthe role of the central coordinator of the agent which comprises five modules:planning, memory, profile, action, and security, the latter often neglected inprevious works. Differences in the internal structure of core-agents led us toclassify them into a taxonomy of passive and active types. Based on this, weproposed different multi-core agent architectures combining uniquecharacteristics of various individual agents. For evaluation purposes, we applied this framework to a selection ofstate-of-the-art agents, thereby demonstrating its alignment with theirfunctionalities and clarifying the overlooked architectural aspects. Moreover,we thoroughly assessed four of our proposed architectures by integratingdistinctive agents into hybrid active/passive core-agents' systems. Thisanalysis provided clear insights into potential improvements and highlightedthe challenges involved in the combination of specific agents.</description><author>Amine B. Hassouna, Hana Chaari, Ines Belhaj</author><pubDate>Tue, 17 Sep 2024 17:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11393v1</guid></item><item><title>Says Who? Effective Zero-Shot Annotation of Focalization</title><link>http://arxiv.org/abs/2409.11390v1</link><description>Focalization, the perspective through which narrative is presented, isencoded via a wide range of lexico-grammatical features and is subject toreader interpretation. Moreover, trained readers regularly disagree oninterpretations, suggesting that this problem may be computationallyintractable. In this paper, we provide experiments to test how wellcontemporary Large Language Models (LLMs) perform when annotating literarytexts for focalization mode. Despite the challenging nature of the task, LLMsshow comparable performance to trained human annotators in our experiments. Weprovide a case study working with the novels of Stephen King to demonstrate theusefulness of this approach for computational literary studies, illustratinghow focalization can be studied at scale.</description><author>Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan</author><pubDate>Tue, 17 Sep 2024 17:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11390v1</guid></item><item><title>Normalization in Proportional Feature Spaces</title><link>http://arxiv.org/abs/2409.11389v1</link><description>The subject of features normalization plays an important central role in datarepresentation, characterization, visualization, analysis, comparison,classification, and modeling, as it can substantially influence and beinfluenced by all of these activities and respective aspects. The selection ofan appropriate normalization method needs to take into account the type andcharacteristics of the involved features, the methods to be used subsequentlyfor the just mentioned data processing, as well as the specific questions beingconsidered. After briefly considering how normalization constitutes one of themany interrelated parts typically involved in data analysis and modeling, thepresent work addressed the important issue of feature normalization from theperspective of uniform and proportional (right skewed) features and comparisonoperations. More general right skewed features are also considered in anapproximated manner. Several concepts, properties, and results are describedand discussed, including the description of a duality relationship betweenuniform and proportional feature spaces and respective comparisons, specifyingconditions for consistency between comparisons in each of the two domains. Twonormalization possibilities based on non-centralized dispersion of features arealso presented, and also described is a modified version of the Jaccardsimilarity index which incorporates intrinsically normalization. Preliminaryexperiments are presented in order to illustrate the developed concepts andmethods.</description><author>Alexandre Benatti, Luciano da F. Costa</author><pubDate>Tue, 17 Sep 2024 17:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11389v1</guid></item><item><title>Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks</title><link>http://arxiv.org/abs/2409.06173v2</link><description>In-Context Learning (ICL) in Large Language Models (LLM) has emerged as thedominant technique for performing natural language tasks, as it does notrequire updating the model parameters with gradient-based methods. ICL promisesto "adapt" the LLM to perform the present task at a competitive orstate-of-the-art level at a fraction of the computational cost. ICL can beaugmented by incorporating the reasoning process to arrive at the final labelexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.However, recent work has found that ICL relies mostly on the retrieval of taskpriors and less so on "learning" to perform tasks, especially for complexsubjective domains like emotion and morality, where priors ossify posteriorpredictions. In this work, we examine whether "enabling" reasoning also createsthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priorsthat remain relatively unchanged despite the evidence in the prompt. We findthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICLfor larger language models. Code is avalaible athttps://github.com/gchochla/cot-priors.</description><author>Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan</author><pubDate>Tue, 17 Sep 2024 17:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06173v2</guid></item><item><title>Training Datasets Generation for Machine Learning: Application to Vision Based Navigation</title><link>http://arxiv.org/abs/2409.11383v1</link><description>Vision Based Navigation consists in utilizing cameras as precision sensorsfor GNC after extracting information from images. To enable the adoption ofmachine learning for space applications, one of obstacles is the demonstrationthat available training datasets are adequate to validate the algorithms. Theobjective of the study is to generate datasets of images and metadata suitablefor training machine learning algorithms. Two use cases were selected and arobust methodology was developed to validate the datasets including the groundtruth. The first use case is in-orbit rendezvous with a man-made object: amockup of satellite ENVISAT. The second use case is a Lunar landing scenario.Datasets were produced from archival datasets (Chang'e 3), from the laboratoryat DLR TRON facility and at Airbus Robotic laboratory, from SurRender softwarehigh fidelity image simulator using Model Capture and from GenerativeAdversarial Networks. The use case definition included the selection ofalgorithms as benchmark: an AI-based pose estimation algorithm and a denseoptical flow algorithm were selected. Eventually it is demonstrated thatdatasets produced with SurRender and selected laboratory facilities areadequate to train machine learning algorithms.</description><author>Jérémy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco</author><pubDate>Tue, 17 Sep 2024 17:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11383v1</guid></item><item><title>Wave-U-Mamba: An End-To-End Framework For High-Quality And Efficient Speech Super Resolution</title><link>http://arxiv.org/abs/2409.09337v2</link><description>Speech Super-Resolution (SSR) is a task of enhancing low-resolution speechsignals by restoring missing high-frequency components. Conventional approachestypically reconstruct log-mel features, followed by a vocoder that generateshigh-resolution speech in the waveform domain. However, as log-mel featureslack phase information, this can result in performance degradation during thereconstruction phase. Motivated by recent advances with Selective State SpacesModels (SSMs), we propose a method, referred to as Wave-U-Mamba that directlyperforms SSR in time domain. In our comparative study, including models such asWSRGlow, NU-Wave 2, and AudioSR, Wave-U-Mamba demonstrates superiorperformance, achieving the lowest Log-Spectral Distance (LSD) across variouslow-resolution sampling rates, ranging from 8 kHz to 24 kHz. Additionally,subjective human evaluations, scored using Mean Opinion Score (MOS) reveal thatour method produces SSR with natural and human-like quality. Furthermore,Wave-U-Mamba achieves these results while generating high-resolution speechover nine times faster than baseline models on a single A100 GPU, withparameter sizes less than 2% of those in the baseline models.</description><author>Yongjoon Lee, Chanwoo Kim</author><pubDate>Tue, 17 Sep 2024 17:33:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09337v2</guid></item><item><title>S$^3$Attention: Improving Long Sequence Attention with Smoothed Skeleton Sketching</title><link>http://arxiv.org/abs/2408.08567v3</link><description>Attention based models have achieved many remarkable breakthroughs innumerous applications. However, the quadratic complexity of Attention makes thevanilla Attention based models hard to apply to long sequence tasks. Variousimproved Attention structures are proposed to reduce the computation cost byinducing low rankness and approximating the whole sequence by sub-sequences.The most challenging part of those approaches is maintaining the proper balancebetween information preservation and computation reduction: the longersub-sequences used, the better information is preserved, but at the price ofintroducing more noise and computational costs. In this paper, we propose asmoothed skeleton sketching based Attention structure, coined S$^3$Attention,which significantly improves upon the previous attempts to negotiate thistrade-off. S$^3$Attention has two mechanisms to effectively minimize the impactof noise while keeping the linear complexity to the sequence length: asmoothing block to mix information over long sequences and a matrix sketchingmethod that simultaneously selects columns and rows from the input matrix. Weverify the effectiveness of S$^3$Attention both theoretically and empirically.Extensive studies over Long Range Arena (LRA) datasets and six time-seriesforecasting show that S$^3$Attention significantly outperforms both vanillaAttention and other state-of-the-art variants of Attention structures.</description><author>Xue Wang, Tian Zhou, Jianqing Zhu, Jialin Liu, Kun Yuan, Tao Yao, Wotao Yin, Rong Jin, HanQin Cai</author><pubDate>Tue, 17 Sep 2024 17:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08567v3</guid></item><item><title>Ultrasound Image Enhancement with the Variance of Diffusion Models</title><link>http://arxiv.org/abs/2409.11380v1</link><description>Ultrasound imaging, despite its widespread use in medicine, often suffersfrom various sources of noise and artifacts that impact the signal-to-noiseratio and overall image quality. Enhancing ultrasound images requires adelicate balance between contrast, resolution, and speckle preservation. Thispaper introduces a novel approach that integrates adaptive beamforming withdenoising diffusion-based variance imaging to address this challenge. Byapplying Eigenspace-Based Minimum Variance (EBMV) beamforming and employing adenoising diffusion model fine-tuned on ultrasound data, our method computesthe variance across multiple diffusion-denoised samples to produce high-qualitydespeckled images. This approach leverages both the inherent multiplicativenoise of ultrasound and the stochastic nature of diffusion models. Experimentalresults on a publicly available dataset demonstrate the effectiveness of ourmethod in achieving superior image reconstructions from single plane-waveacquisitions. The code is available at:https://github.com/Yuxin-Zhang-Jasmine/IUS2024_Diffusion.</description><author>Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus</author><pubDate>Tue, 17 Sep 2024 17:29:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11380v1</guid></item><item><title>Dated Data: Tracing Knowledge Cutoffs in Large Language Models</title><link>http://arxiv.org/abs/2403.12958v2</link><description>Released Large Language Models (LLMs) are often paired with a claimedknowledge cutoff date, or the dates at which training data was gathered. Suchinformation is crucial for applications where the LLM must provide up to dateinformation. However, this statement only scratches the surface: do allresources in the training data share the same knowledge cutoff date? Does themodel's demonstrated knowledge for these subsets closely align to their cutoffdates? In this work, we define the notion of an effective cutoff. This isdistinct from the LLM designer reported cutoff and applies separately tosub-resources and topics. We propose a simple approach to estimate effectivecutoffs on the resource-level temporal alignment of an LLM by probing acrossversions of the data. Using this analysis, we find that effective cutoffs oftendiffer from reported cutoffs. To understand the root cause of this observation,we conduct a direct large-scale analysis on open pre-training datasets. Ouranalysis reveals two reasons for these inconsistencies: (1) temporal biases ofCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)complications in LLM deduplication schemes involving semantic duplicates andlexical near-duplicates. Overall, our results show that knowledge cutoffs arenot as simple as they have seemed and that care must be taken both by LLMdataset curators as well as practitioners who seek to use information fromthese models.</description><author>Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme</author><pubDate>Tue, 17 Sep 2024 17:25:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12958v2</guid></item><item><title>Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement</title><link>http://arxiv.org/abs/2409.11378v1</link><description>Finetuning large language models on instruction data is crucial for enhancingpre-trained knowledge and improving instruction-following capabilities. Asinstruction datasets proliferate, selecting optimal data for effective trainingbecomes increasingly important. This work addresses the question: How can wedetermine the optimal subset of data for effective training? While existingresearch often emphasizes local criteria like instance quality for subsetselection, we argue that a global approach focused on data diversity is morecritical. Our method employs k-means clustering to ensure the selected subseteffectively represents the full dataset. We propose an iterative refinementmethod inspired by active learning techniques to resample instances fromclusters, reassessing each cluster's importance and sampling weight in everytraining iteration. This approach reduces the effect of outliers andautomatically filters out clusters containing low-quality data. Throughextensive evaluation across natural language reasoning, general worldknowledge, code and math reasoning tasks, and by fine-tuning models fromvarious families, we observe consistent improvements, achieving a 7% increaseover random selection and a 3.8% improvement over state-of-the-art samplingmethods. Our work highlights the significance of diversity-first sampling whenfinetuning LLMs to enhance performance across a broad array of evaluationtasks. Our code is available athttps://github.com/for-ai/iterative-data-selection.</description><author>Simon Yu, Liangyu Chen, Sara Ahmadian, Marzieh Fadaee</author><pubDate>Tue, 17 Sep 2024 17:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11378v1</guid></item><item><title>Machine Learning on Dynamic Functional Connectivity: Promise, Pitfalls, and Interpretations</title><link>http://arxiv.org/abs/2409.11377v1</link><description>An unprecedented amount of existing functional Magnetic Resonance Imaging(fMRI) data provides a new opportunity to understand the relationship betweenfunctional fluctuation and human cognition/behavior using a data-drivenapproach. To that end, tremendous efforts have been made in machine learning topredict cognitive states from evolving volumetric images ofblood-oxygen-level-dependent (BOLD) signals. Due to the complex nature of brainfunction, however, the evaluation on learning performance and discoveries arenot often consistent across current state-of-the-arts (SOTA). By capitalizingon large-scale existing neuroimaging data (34,887 data samples from six publicdatabases), we seek to establish a well-founded empirical guideline fordesigning deep models for functional neuroimages by linking the methodologyunderpinning with knowledge from the neuroscience domain. Specifically, we putthe spotlight on (1) What is the current SOTA performance in cognitive taskrecognition and disease diagnosis using fMRI? (2) What are the limitations ofcurrent deep models? and (3) What is the general guideline for selecting thesuitable machine learning backbone for new neuroimaging applications? We haveconducted a comprehensive evaluation and statistical analysis, in varioussettings, to answer the above outstanding questions.</description><author>Jiaqi Ding, Tingting Dan, Ziquan Wei, Hyuna Cho, Paul J. Laurienti, Won Hwa Kim, Guorong Wu</author><pubDate>Tue, 17 Sep 2024 17:24:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11377v1</guid></item><item><title>Towards Time Series Reasoning with LLMs</title><link>http://arxiv.org/abs/2409.11376v1</link><description>Multi-modal large language models (MLLMs) have enabled numerous advances inunderstanding and reasoning in domains like vision, but we have not yet seenthis broad success for time-series. Although prior works on time-series MLLMshave shown promising performance in time-series forecasting, very few worksshow how an LLM could be used for time-series reasoning in natural language. Wepropose a novel multi-modal time-series LLM approach that learns generalizableinformation across various domains with powerful zero-shot performance. First,we train a lightweight time-series encoder on top of an LLM to directly extracttime-series information. Then, we fine-tune our model with chain-of-thoughtaugmented time-series tasks to encourage the model to generate reasoning paths.We show that our model learns a latent representation that reflects specifictime-series features (e.g. slope, frequency), as well as outperforming GPT-4oon a set of zero-shot reasoning tasks on a variety of domains.</description><author>Winnie Chow, Lauren Gardiner, Haraldur T. Hallgrímsson, Maxwell A. Xu, Shirley You Ren</author><pubDate>Tue, 17 Sep 2024 17:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11376v1</guid></item><item><title>Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification</title><link>http://arxiv.org/abs/2409.11375v1</link><description>In the medical domain, acquiring large datasets poses significant challengesdue to privacy concerns. Nonetheless, the development of a robust deep-learningmodel for retinal disease diagnosis necessitates a substantial dataset fortraining. The capacity to generalize effectively on smaller datasets remains apersistent challenge. The scarcity of data presents a significant barrier tothe practical implementation of scalable medical AI solutions. To address thisissue, we've combined a wide range of data sources to improve performance andgeneralization to new data by giving it a deeper understanding of the datarepresentation from multi-modal datasets and developed a self-supervisedframework based on large language models (LLMs), SwinV2 to gain a deeperunderstanding of multi-modal dataset representations, enhancing the model'sability to extrapolate to new data for the detection of eye diseases usingoptical coherence tomography (OCT) images. We adopt a two-phase trainingmethodology, self-supervised pre-training, and fine-tuning on a downstreamsupervised classifier. An ablation study conducted across three datasetsemploying various encoder backbones, without data fusion, with low dataavailability setting, and without self-supervised pre-training scenarios,highlights the robustness of our method. Our findings demonstrate consistentperformance across these diverse conditions, showcasing superior generalizationcapabilities compared to the baseline model, ResNet-50.</description><author>Fatema-E- Jannat, Sina Gholami, Jennifer I. Lim, Theodore Leng, Minhaj Nur Alam, Hamed Tabkhi</author><pubDate>Tue, 17 Sep 2024 17:22:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11375v1</guid></item><item><title>Uncertainty and Prediction Quality Estimation for Semantic Segmentation via Graph Neural Networks</title><link>http://arxiv.org/abs/2409.11373v1</link><description>When employing deep neural networks (DNNs) for semantic segmentation insafety-critical applications like automotive perception or medical imaging, itis important to estimate their performance at runtime, e.g. via uncertaintyestimates or prediction quality estimates. Previous works mostly performeduncertainty estimation on pixel-level. In a line of research, aconnected-component-wise (segment-wise) perspective was taken, approachinguncertainty estimation on an object-level by performing so-called metaclassification and regression to estimate uncertainty and prediction quality,respectively. In those works, each predicted segment is considered individuallyto estimate its uncertainty or prediction quality. However, the neighboringsegments may provide additional hints on whether a given predicted segment isof high quality, which we study in the present work. On the basis ofuncertainty indicating metrics on segment-level, we use graph neural networks(GNNs) to model the relationship of a given segment's quality as a function ofthe given segment's metrics as well as those of its neighboring segments. Wecompare different GNN architectures and achieve a notable performanceimprovement.</description><author>Edgar Heinert, Stephan Tilgner, Timo Palm, Matthias Rottmann</author><pubDate>Tue, 17 Sep 2024 17:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11373v1</guid></item><item><title>Compact Implicit Neural Representations for Plane Wave Images</title><link>http://arxiv.org/abs/2409.11370v1</link><description>Ultrafast Plane-Wave (PW) imaging often produces artifacts and shadows thatvary with insonification angles. We propose a novel approach using ImplicitNeural Representations (INRs) to compactly encode multi-planar sequences whilepreserving crucial orientation-dependent information. To our knowledge, this isthe first application of INRs for PW angular interpolation. Our method employsa Multi-Layer Perceptron (MLP)-based model with a concise physics-enhancedrendering technique. Quantitative evaluations using SSIM, PSNR, and standardultrasound metrics, along with qualitative visual assessments, confirm theeffectiveness of our approach. Additionally, our method demonstratessignificant storage efficiency, with model weights requiring 530 KB compared to8 MB for directly storing the 75 PW images, achieving a notable compressionratio of approximately 15:1.</description><author>Mathilde Monvoisin, Yuxin Zhang, Diana Mateus</author><pubDate>Tue, 17 Sep 2024 17:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11370v1</guid></item><item><title>Learning Spatially-Aware Language and Audio Embedding</title><link>http://arxiv.org/abs/2409.11369v1</link><description>Humans can picture a sound scene given an imprecise natural languagedescription. For example, it is easy to imagine an acoustic environment given aphrase like "the lion roar came from right behind me!". For a machine to havethe same degree of comprehension, the machine must know what a lion is(semantic attribute), what the concept of "behind" is (spatial attribute) andhow these pieces of linguistic information align with the semantic and spatialattributes of the sound (what a roar sounds like when its coming from behind).State-of-the-art audio foundation models which learn to map between audioscenes and natural textual descriptions, are trained on non-spatial audio andtext pairs, and hence lack spatial awareness. In contrast, sound eventlocalization and detection models are limited to recognizing sounds from afixed number of classes, and they localize the source to absolute position(e.g., 0.2m) rather than a position described using natural language (e.g.,"next to me"). To address these gaps, we present ELSA a spatially aware-audioand text embedding model trained using multimodal contrastive learning. ELSAsupports non-spatial audio, spatial audio, and open vocabulary text captionsdescribing both the spatial and semantic components of sound. To train ELSA:(a) we spatially augment the audio and captions of three open-source audiodatasets totaling 4,738 hours of audio, and (b) we design an encoder to capturethe semantics of non-spatial audio, and the semantics and spatial attributes ofspatial audio using contrastive learning. ELSA is competitive withstate-of-the-art for both semantic retrieval and 3D source localization. Inparticular, ELSA achieves +2.8% mean audio-to-text and text-to-audio R@1 abovethe baseline, and outperforms by -11.6{\deg} mean-absolute-error in 3D sourcelocalization over the baseline.</description><author>Bhavika Devnani, Skyler Seto, Zakaria Aldeneh, Alessandro Toso, Elena Menyaylenko, Barry-John Theobald, Jonathan Sheaffer, Miguel Sarabia</author><pubDate>Tue, 17 Sep 2024 17:17:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11369v1</guid></item><item><title>OSV: One Step is Enough for High-Quality Image to Video Generation</title><link>http://arxiv.org/abs/2409.11367v1</link><description>Video diffusion models have shown great potential in generating high-qualityvideos, making them an increasingly popular focus. However, their inherentiterative nature leads to substantial computational and time costs. Whileefforts have been made to accelerate video diffusion by reducing inferencesteps (through techniques like consistency distillation) and GAN training(these approaches often fall short in either performance or trainingstability). In this work, we introduce a two-stage training framework thateffectively combines consistency distillation with GAN training to addressthese challenges. Additionally, we propose a novel video discriminator design,which eliminates the need for decoding the video latents and improves the finalperformance. Our model is capable of producing high-quality videos in merelyone-step, with the flexibility to perform multi-step refinement for furtherperformance enhancement. Our quantitative evaluation on the OpenWebVid-1Mbenchmark shows that our model significantly outperforms existing methods.Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance ofthe consistency distillation based method, AnimateLCM (FVD 184.79), andapproaches the 25-step performance of advanced Stable Video Diffusion (FVD156.94).</description><author>Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Wenbing Zhu, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang</author><pubDate>Tue, 17 Sep 2024 17:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11367v1</guid></item><item><title>Towards Optimal Branching of Linear and Semidefinite Relaxations for Neural Network Robustness Certification</title><link>http://arxiv.org/abs/2101.09306v3</link><description>In this paper, we study certifying the robustness of ReLU neural networksagainst adversarial input perturbations. To diminish the relaxation errorsuffered by the popular linear programming (LP) and semidefinite programming(SDP) certification methods, we take a branch-and-bound approach to proposepartitioning the input uncertainty set and solving the relaxations on each partseparately. We show that this approach reduces relaxation error, and that theerror is eliminated entirely upon performing an LP relaxation with a partitionintelligently designed to exploit the nature of the ReLU activations. To scalethis approach to large networks, we consider using a coarser partition wherebythe number of parts in the partition is reduced. We prove that computing such acoarse partition that directly minimizes the LP relaxation error is NP-hard. Byinstead minimizing the worst-case LP relaxation error, we develop a closed-formbranching scheme in the single-hidden layer case. We extend the analysis to theSDP, where the feasible set geometry is exploited to design a branching schemethat minimizes the worst-case SDP relaxation error. Experiments on MNIST,CIFAR-10, and Wisconsin breast cancer diagnosis classifiers demonstratesignificant increases in the percentages of test samples certified. Byindependently increasing the input size and the number of layers, weempirically illustrate under which regimes the branched LP and branched SDP arebest applied. Finally, we extend our LP branching method into a multi-layerbranching heuristic, which attains comparable performance to priorstate-of-the-art heuristics on large-scale, deep neural network certificationbenchmarks.</description><author>Brendon G. Anderson, Ziye Ma, Jingqi Li, Somayeh Sojoudi</author><pubDate>Tue, 17 Sep 2024 17:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2101.09306v3</guid></item><item><title>CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration</title><link>http://arxiv.org/abs/2409.11365v1</link><description>The deployment of multimodal large language models (MLLMs) has demonstratedremarkable success in engaging in conversations involving visual inputs, thanksto the superior power of large language models (LLMs). Those MLLMs aretypically built based on the LLMs, with an image encoder to process images intothe token embedding space of the LLMs. However, the integration of visualmodality has introduced a unique vulnerability: the MLLM becomes susceptible tomalicious visual inputs and prone to generating sensitive or harmful responses,even though the LLM has been trained on textual dataset to align with humanvalue. In this paper, we first raise the question: ``Do the MLLMs possesssafety-awareness against malicious image inputs?". We find that after adding aprinciple that specifies the safety requirement into the input of the MLLM, themodel's safety awareness becomes boosted. This phenomenon verifies theexistence of MLLM's safety-awareness against image inputs, it is only weakenedby the modality gap. We then introduce a simple yet effective technique termedCoCA, which amplifies the safety-awareness of the MLLM by calibrating itsoutput distribution. Our proposed strategy helps the model reclaim its originalsafety awareness without losing its original capabilities. We verify theeffectiveness of our approach on both multimodal safety and understandingbenchmarks.</description><author>Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, Zhenguo Li</author><pubDate>Tue, 17 Sep 2024 17:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11365v1</guid></item><item><title>SpatialBot: Precise Spatial Understanding with Vision Language Models</title><link>http://arxiv.org/abs/2406.13642v6</link><description>Vision Language Models (VLMs) have achieved impressive performance in 2Dimage understanding, however they are still struggling with spatialunderstanding which is the foundation of Embodied AI. In this paper, we proposeSpatialBot for better spatial understanding by feeding both RGB and depthimages. Additionally, we have constructed the SpatialQA dataset, which involvesmulti-level depth-related questions to train VLMs for depth understanding.Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilitiesin spatial understanding at different levels. Extensive experiments on ourspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. Themodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.</description><author>Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao</author><pubDate>Tue, 17 Sep 2024 17:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13642v6</guid></item><item><title>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</title><link>http://arxiv.org/abs/2409.11363v1</link><description>AI agents have the potential to aid users on a variety of consequentialtasks, including conducting scientific research. To spur the development ofuseful agents, we need benchmarks that are challenging, but more crucially,directly correspond to real-world tasks of interest. This paper introduces sucha benchmark, designed to measure the accuracy of AI agents in tackling acrucial yet surprisingly challenging aspect of scientific research:computational reproducibility. This task, fundamental to the scientificprocess, involves reproducing the results of a study using the provided codeand data. We introduce CORE-Bench (Computational Reproducibility AgentBenchmark), a benchmark consisting of 270 tasks based on 90 scientific papersacross three disciplines (computer science, social science, and medicine).Tasks in CORE-Bench consist of three difficulty levels and include bothlanguage-only and vision-language tasks. We provide an evaluation system tomeasure the accuracy of agents in a fast and parallelizable way, saving days ofevaluation time for each run compared to a sequential implementation. Weevaluated two baseline agents: the general-purpose AutoGPT and a task-specificagent called CORE-Agent. We tested both variants using two underlying languagemodels: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% onthe hardest task, showing the vast scope for improvement in automating routinescientific tasks. Having agents that can reproduce existing work is a necessarystep towards building agents that can conduct novel research and could verifyand improve the performance of other research agents. We hope that CORE-Benchcan improve the state of reproducibility and spur the development of futureresearch agents.</description><author>Zachary S. Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan</author><pubDate>Tue, 17 Sep 2024 17:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11363v1</guid></item><item><title>AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances</title><link>http://arxiv.org/abs/2409.11360v1</link><description>Large language models (LLMs) are being increasingly integrated into everydayproducts and services, such as coding tools and writing assistants. As theseembedded AI applications are deployed globally, there is a growing concern thatthe AI models underlying these applications prioritize Western values. Thispaper investigates what happens when a Western-centric AI model provideswriting suggestions to users from a different cultural background. We conducteda cross-cultural controlled experiment with 118 participants from India and theUnited States who completed culturally grounded writing tasks with and withoutAI suggestions. Our analysis reveals that AI provided greater efficiency gainsfor Americans compared to Indians. Moreover, AI suggestions led Indianparticipants to adopt Western writing styles, altering not just what is writtenbut also how it is written. These findings show that Western-centric AI modelshomogenize writing toward Western norms, diminishing nuances that differentiatecultural expression.</description><author>Dhruv Agarwal, Mor Naaman, Aditya Vashistha</author><pubDate>Tue, 17 Sep 2024 17:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11360v1</guid></item><item><title>Evaluating language models as risk scores</title><link>http://arxiv.org/abs/2407.14614v2</link><description>Current question-answering benchmarks predominantly focus on accuracy inrealizable prediction tasks. Conditioned on a question and answer-key, does themost likely token match the ground truth? Such benchmarks necessarily fail toevaluate language models' ability to quantify outcome uncertainty. In thiswork, we focus on the use of language models as risk scores for unrealizableprediction tasks. We introduce folktexts, a software package to systematicallygenerate risk scores using language models, and evaluate them against US Censusdata products. A flexible API enables the use of different prompting schemes,local or web-hosted models, and diverse census columns that can be used tocompose custom prediction tasks. We demonstrate the utility of folktextsthrough a sweep of empirical insights into the statistical properties of 17recent large language models across five natural text benchmark tasks. We findthat zero-shot risk scores produced by multiple-choice question-answering havehigh predictive signal but are widely miscalibrated. Base models consistentlyoverestimate outcome uncertainty, while instruction-tuned models underestimateuncertainty and produce over-confident risk scores. In fact, instruction-tuningpolarizes answer distribution regardless of true underlying data uncertainty.Conversely, verbally querying models for probability estimates results insubstantially improved calibration across all instruction-tuned models. Thesedifferences in ability to quantify data uncertainty cannot be revealed inrealizable settings, and highlight a blind-spot in the current evaluationecosystem that \folktexts covers.</description><author>André F. Cruz, Moritz Hardt, Celestine Mendler-Dünner</author><pubDate>Tue, 17 Sep 2024 17:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14614v2</guid></item><item><title>RenderWorld: World Model with Self-Supervised 3D Label</title><link>http://arxiv.org/abs/2409.11356v1</link><description>End-to-end autonomous driving with vision-only is not only morecost-effective compared to LiDAR-vision fusion but also more reliable thantraditional methods. To achieve a economical and robust purely visualautonomous driving system, we propose RenderWorld, a vision-only end-to-endautonomous driving framework, which generates 3D occupancy labels using aself-supervised gaussian-based Img2Occ Module, then encodes the labels byAM-VAE, and uses world model for forecasting and planning. RenderWorld employsGaussian Splatting to represent 3D scenes and render 2D images greatly improvessegmentation accuracy and reduces GPU memory consumption compared withNeRF-based methods. By applying AM-VAE to encode air and non-air separately,RenderWorld achieves more fine-grained scene element representation, leading tostate-of-the-art performance in both 4D occupancy forecasting and motionplanning from autoregressive world model.</description><author>Ziyang Yan, Wenzhen Dong, Yihua Shao, Yuhang Lu, Liu Haiyang, Jingwen Liu, Haozhe Wang, Zhe Wang, Yan Wang, Fabio Remondino, Yuexin Ma</author><pubDate>Tue, 17 Sep 2024 17:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11356v1</guid></item><item><title>Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think</title><link>http://arxiv.org/abs/2409.11355v1</link><description>Recent work showed that large diffusion models can be reused as highlyprecise monocular depth estimators by casting depth estimation as animage-conditional image generation task. While the proposed model achievedstate-of-the-art results, high computational demands due to multi-stepinference limited its use in many scenarios. In this paper, we show that theperceived inefficiency was caused by a flaw in the inference pipeline that hasso far gone unnoticed. The fixed model performs comparably to the bestpreviously reported configuration while being more than 200$\times$ faster. Tooptimize for downstream task performance, we perform end-to-end fine-tuning ontop of the single-step model with task-specific losses and get a deterministicmodel that outperforms all other diffusion-based depth and normal estimationmodels on common zero-shot benchmarks. We surprisingly find that thisfine-tuning protocol also works directly on Stable Diffusion and achievescomparable performance to current state-of-the-art diffusion-based depth andnormal estimation models, calling into question some of the conclusions drawnfrom prior works.</description><author>Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, Bastian Leibe</author><pubDate>Tue, 17 Sep 2024 16:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11355v1</guid></item><item><title>THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models</title><link>http://arxiv.org/abs/2409.11353v1</link><description>Hallucination, the generation of factually incorrect content, is a growingchallenge in Large Language Models (LLMs). Existing detection and mitigationmethods are often isolated and insufficient for domain-specific needs, lackinga standardized pipeline. This paper introduces THaMES (Tool for HallucinationMitigations and EvaluationS), an integrated framework and library addressingthis gap. THaMES offers an end-to-end solution for evaluating and mitigatinghallucinations in LLMs, featuring automated test set generation, multifacetedbenchmarking, and adaptable mitigation strategies. It automates test setcreation from any corpus, ensuring high data quality, diversity, andcost-efficiency through techniques like batch processing, weighted sampling,and counterfactual validation. THaMES assesses a model's ability to detect andreduce hallucinations across various tasks, including text generation andbinary classification, applying optimal mitigation strategies like In-ContextLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-EfficientFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge baseof academic papers, political news, and Wikipedia reveal that commercial modelslike GPT-4o benefit more from RAG than ICL, while open-weight models likeLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFTsignificantly enhances the performance of Llama-3.1-8B-Instruct in bothevaluation tasks.</description><author>Mengfei Liang, Archish Arun, Zekun Wu, Cristian Munoz, Jonathan Lutch, Emre Kazim, Adriano Koshiyama, Philip Treleaven</author><pubDate>Tue, 17 Sep 2024 16:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11353v1</guid></item><item><title>Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry</title><link>http://arxiv.org/abs/2409.11350v1</link><description>Machine-learning (ML) models in flow cytometry have the potential to reduceerror rates, increase reproducibility, and boost the efficiency of clinicallabs. While numerous ML models for flow cytometry data have been proposed, fewstudies have described the clinical deployment of such models. Realizing thepotential gains of ML models in clinical labs requires not only an accuratemodel, but infrastructure for automated inference, error detection, analyticsand monitoring, and structured data extraction. Here, we describe an ML modelfor detection of Acute Myeloid Leukemia (AML), along with the infrastructuresupporting clinical implementation. Our infrastructure leverages the resilienceand scalability of the cloud for model inference, a Kubernetes-based workflowsystem that provides model reproducibility and resource management, and asystem for extracting structured diagnoses from full-text reports. We alsodescribe our model monitoring and visualization platform, an essential elementfor ensuring continued model accuracy. Finally, we present a post-deploymentanalysis of impacts on turn-around time and compare production accuracy to theoriginal validation statistics.</description><author>Lauren M. Zuromski, Jacob Durtschi, Aimal Aziz, Jeffrey Chumley, Mark Dewey, Paul English, Muir Morrison, Keith Simmon, Blaine Whipple, Brendan O'Fallon, David P. Ng</author><pubDate>Tue, 17 Sep 2024 16:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11350v1</guid></item><item><title>OmniGen: Unified Image Generation</title><link>http://arxiv.org/abs/2409.11340v1</link><description>In this work, we introduce OmniGen, a new diffusion model for unified imagegeneration. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGenno longer requires additional modules such as ControlNet or IP-Adapter toprocess diverse control conditions. OmniGenis characterized by the followingfeatures: 1) Unification: OmniGen not only demonstrates text-to-imagegeneration capabilities but also inherently supports other downstream tasks,such as image editing, subject-driven generation, and visual-conditionalgeneration. Additionally, OmniGen can handle classical computer vision tasks bytransforming them into image generation tasks, such as edge detection and humanpose recognition. 2) Simplicity: The architecture of OmniGen is highlysimplified, eliminating the need for additional text encoders. Moreover, it ismore user-friendly compared to existing diffusion models, enabling complextasks to be accomplished through instructions without the need for extrapreprocessing steps (e.g., human pose estimation), thereby significantlysimplifying the workflow of image generation. 3) Knowledge Transfer: Throughlearning in a unified format, OmniGen effectively transfers knowledge acrossdifferent tasks, manages unseen tasks and domains, and exhibits novelcapabilities. We also explore the model's reasoning capabilities and potentialapplications of chain-of-thought mechanism. This work represents the firstattempt at a general-purpose image generation model, and there remain severalunresolved issues. We will open-source the related resources athttps://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.</description><author>Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, Zheng Liu</author><pubDate>Tue, 17 Sep 2024 16:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11340v1</guid></item><item><title>CLIP Adaptation by Intra-modal Overlap Reduction</title><link>http://arxiv.org/abs/2409.11338v1</link><description>Numerous methods have been proposed to adapt a pre-trained foundational CLIPmodel for few-shot classification. As CLIP is trained on a large corpus, itgeneralises well through adaptation to few-shot classification. In this work,we analyse the intra-modal overlap in image space in terms of embeddingrepresentation. Our analysis shows that, due to contrastive learning,embeddings from CLIP model exhibit high cosine similarity distribution overlapin the image space between paired and unpaired examples affecting theperformance of few-shot training-free classification methods which rely onsimilarity in the image space for their predictions. To tackle intra-modaloverlap we propose to train a lightweight adapter on a generic set of samplesfrom the Google Open Images dataset demonstrating that this improves accuracyfor few-shot training-free classification. We validate our contribution throughextensive empirical analysis and demonstrate that reducing the intra-modaloverlap leads to a) improved performance on a number of standard datasets, b)increased robustness to distribution shift and c) higher feature variancerendering the features more discriminative for downstream tasks.</description><author>Alexey Kravets, Vinay Namboodiri</author><pubDate>Tue, 17 Sep 2024 16:40:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11338v1</guid></item><item><title>MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks</title><link>http://arxiv.org/abs/2312.13322v2</link><description>With easier access to powerful compute resources, there is a growing trend inAI for software development to develop large language models (LLMs) to addressa variety of programming tasks. Even LLMs applied to tasks from thehigh-performance computing (HPC) domain are huge in size and demand expensivecompute resources for training. This is partly because LLMs for HPC tasks areobtained by finetuning existing LLMs that support several natural and/orprogramming languages. We found this design choice confusing - why do we needLLMs trained on natural languages and programming languages unrelated to HPCfor HPC-specific tasks? In this line of work, we aim to question choices madeby existing LLMs by developing smaller language models (LMs) for specificdomains - we call them domain-specific LMs. Specifically, we start with HPC asa domain and build an HPC-specific LM, named MonoCoder, which is orders ofmagnitude smaller than existing LMs but delivers better performance on non-HPCand HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specificdataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluatedthe performance of MonoCoder against state-of-the-art multi-lingual LLMs.Results demonstrate that MonoCoder, although much smaller than existing LMs,outperforms other LLMs on normalized-perplexity tests (in relation to modelsize) while also delivering competing CodeBLEU scores for high-performance andparallel code generations. In other words, results suggest that MonoCoderunderstands HPC code better than state-of-the-art LLMs.</description><author>Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Mihai Capota, Abdul Wasay, Nesreen Ahmed, Ted Willke, Guy Tamir, Yuval Pinter, Timothy Mattson, Gal Oren</author><pubDate>Tue, 17 Sep 2024 16:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13322v2</guid></item><item><title>BoostDream: Efficient Refining for High-Quality Text-to-3D Generation from Multi-View Diffusion</title><link>http://arxiv.org/abs/2401.16764v3</link><description>Witnessing the evolution of text-to-image diffusion models, significantstrides have been made in text-to-3D generation. Currently, two primaryparadigms dominate the field of text-to-3D: the feed-forward generationsolutions, capable of swiftly producing 3D assets but often yielding coarseresults, and the Score Distillation Sampling (SDS) based solutions, known forgenerating high-fidelity 3D assets albeit at a slower pace. The synergisticintegration of these methods holds substantial promise for advancing 3Dgeneration techniques. In this paper, we present BoostDream, a highly efficientplug-and-play 3D refining method designed to transform coarse 3D assets intohigh-quality. The BoostDream framework comprises three distinct processes: (1)We introduce 3D model distillation that fits differentiable representationsfrom the 3D assets obtained through feed-forward generation. (2) A novelmulti-view SDS loss is designed, which utilizes a multi-view aware 2D diffusionmodel to refine the 3D assets. (3) We propose to use prompt and multi-viewconsistent normal maps as guidance in refinement.Our extensive experiment isconducted on different differentiable 3D representations, revealing thatBoostDream excels in generating high-quality 3D assets rapidly, overcoming theJanus problem compared to conventional SDS-based methods. This breakthroughsignifies a substantial advancement in both the efficiency and quality of 3Dgeneration processes.</description><author>Yonghao Yu, Shunan Zhu, Huai Qin, Haorui Li</author><pubDate>Tue, 17 Sep 2024 16:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16764v3</guid></item><item><title>Reducing Catastrophic Forgetting in Online Class Incremental Learning Using Self-Distillation</title><link>http://arxiv.org/abs/2409.11329v1</link><description>In continual learning, there is a serious problem of catastrophic forgetting,in which previous knowledge is forgotten when a model learns new tasks. Variousmethods have been proposed to solve this problem. Replay methods which replaydata from previous tasks in later training, have shown good accuracy. However,replay methods have a generalizability problem from a limited memory buffer. Inthis paper, we tried to solve this problem by acquiring transferable knowledgethrough self-distillation using highly generalizable output in shallow layer asa teacher. Furthermore, when we deal with a large number of classes orchallenging data, there is a risk of learning not converging and notexperiencing overfitting. Therefore, we attempted to achieve more efficient andthorough learning by prioritizing the storage of easily misclassified samplesthrough a new method of memory update. We confirmed that our proposed methodoutperformed conventional methods by experiments on CIFAR10, CIFAR100, andMiniimageNet datasets.</description><author>Kotaro Nagata, Hiromu Ono, Kazuhiro Hotta</author><pubDate>Tue, 17 Sep 2024 16:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11329v1</guid></item><item><title>Learning Unstable Continuous-Time Stochastic Linear Control Systems</title><link>http://arxiv.org/abs/2409.11327v1</link><description>We study the problem of system identification for stochastic continuous-timedynamics, based on a single finite-length state trajectory. We present a methodfor estimating the possibly unstable open-loop matrix by employing properlyrandomized control inputs. Then, we establish theoretical performanceguarantees showing that the estimation error decays with trajectory length, ameasure of excitability, and the signal-to-noise ratio, while it grows withdimension. Numerical illustrations that showcase the rates of learning thedynamics, will be provided as well. To perform the theoretical analysis, wedevelop new technical tools that are of independent interest. That includesnon-asymptotic stochastic bounds for highly non-stationary martingales andgeneralized laws of iterated logarithms, among others.</description><author>Reza Sadeghi Hafshejani, Mohamad Kazem Shirani Fradonbeh</author><pubDate>Tue, 17 Sep 2024 16:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11327v1</guid></item><item><title>Learning by Self-Explaining</title><link>http://arxiv.org/abs/2309.08395v3</link><description>Much of explainable AI research treats explanations as a means for modelinspection. Yet, this neglects findings from human psychology that describe thebenefit of self-explanations in an agent's learning process. Motivated by this,we introduce a novel workflow in the context of image classification, termedLearning by Self-Explaining (LSX). LSX utilizes aspects of self-refining AI andhuman-guided explanatory machine learning. The underlying idea is that alearner model, in addition to optimizing for the original predictive task, isfurther optimized based on explanatory feedback from an internal critic model.Intuitively, a learner's explanations are considered "useful" if the internalcritic can perform the same task given these explanations. We provide anoverview of important components of LSX and, based on this, perform extensiveexperimental evaluations via three different example instantiations. Ourresults indicate improvements via Learning by Self-Explaining on severallevels: in terms of model generalization, reducing the influence of confoundingfactors, and providing more task-relevant and faithful model explanations.Overall, our work provides evidence for the potential of self-explaining withinthe learning phase of an AI model.</description><author>Wolfgang Stammer, Felix Friedrich, David Steinmann, Manuel Brack, Hikaru Shindo, Kristian Kersting</author><pubDate>Tue, 17 Sep 2024 16:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08395v3</guid></item><item><title>LYT-NET: Lightweight YUV Transformer-based Network for Low-light Image Enhancement</title><link>http://arxiv.org/abs/2401.15204v6</link><description>This letter introduces LYT-Net, a novel lightweight transformer-based modelfor low-light image enhancement (LLIE). LYT-Net consists of several layers anddetachable blocks, including our novel blocks--Channel-Wise Denoiser (CWD) andMulti-Stage Squeeze &amp; Excite Fusion (MSEF)--along with the traditionalTransformer block, Multi-Headed Self-Attention (MHSA). In our method we adopt adual-path approach, treating chrominance channels U and V and luminance channelY as separate entities to help the model better handle illumination adjustmentand corruption restoration. Our comprehensive evaluation on established LLIEdatasets demonstrates that, despite its low complexity, our model outperformsrecent LLIE methods. The source code and pre-trained models are available athttps://github.com/albrateanu/LYT-Net</description><author>A. Brateanu, R. Balmez, A. Avram, C. Orhei, C. Ancuti</author><pubDate>Tue, 17 Sep 2024 16:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15204v6</guid></item><item><title>TopoMaskV2: Enhanced Instance-Mask-Based Formulation for the Road Topology Problem</title><link>http://arxiv.org/abs/2409.11325v1</link><description>Recently, the centerline has become a popular representation of lanes due toits advantages in solving the road topology problem. To enhance centerlineprediction, we have developed a new approach called TopoMask. Unlike previousmethods that rely on keypoints or parametric methods, TopoMask utilizes aninstance-mask-based formulation coupled with a masked-attention-basedtransformer architecture. We introduce a quad-direction label representation toenrich the mask instances with flow information and design a correspondingpost-processing technique for mask-to-centerline conversion. Additionally, wedemonstrate that the instance-mask formulation provides complementaryinformation to parametric Bezier regressions, and fusing both outputs leads toimproved detection and topology performance. Moreover, we analyze theshortcomings of the pillar assumption in the Lift Splat technique and adapt amulti-height bin configuration. Experimental results show that TopoMaskachieves state-of-the-art performance in the OpenLane-V2 dataset, increasingfrom 44.1 to 49.4 for Subset-A and 44.7 to 51.8 for Subset-B in the V1.1 OLSbaseline.</description><author>M. Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel</author><pubDate>Tue, 17 Sep 2024 16:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11325v1</guid></item><item><title>Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on Supervised Regression (Preprint)</title><link>http://arxiv.org/abs/2408.12308v2</link><description>In this tutorial, we present a compact and holistic discussion of DeepLearning with a focus on Convolutional Neural Networks (CNNs) and supervisedregression. While there are numerous books and articles on the individualtopics we cover, comprehensive and detailed tutorials that address DeepLearning from a foundational yet rigorous and accessible perspective are rare.Most resources on CNNs are either too advanced, focusing on cutting-edgearchitectures, or too narrow, addressing only specific applications like imageclassification.This tutorial not only summarizes the most relevant concepts butalso provides an in-depth exploration of each, offering a complete yet agileset of ideas. Moreover, we highlight the powerful synergy between learningtheory, statistic, and machine learning, which together underpin the DeepLearning and CNN frameworks. We aim for this tutorial to serve as an optimalresource for students, professors, and anyone interested in understanding thefoundations of Deep Learning. Upon acceptance we will provide an accompanyingrepository under\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial} Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, MachineLearning.</description><author>Yansel Gonzalez Tejeda, Helmut A. Mayer</author><pubDate>Tue, 17 Sep 2024 16:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12308v2</guid></item><item><title>Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains</title><link>http://arxiv.org/abs/2406.11423v2</link><description>Proactive content moderation requires platforms to rapidly and continuouslyevaluate the credibility of websites. Leveraging the direct and indirect pathsusers follow to unreliable websites, we develop a website credibilityclassification and discovery system that integrates both webgraph andlarge-scale social media contexts. We additionally introduce the concept ofdredge words, terms or phrases for which unreliable domains rank highly onsearch engines, and provide the first exploration of their usage on socialmedia. Our graph neural networks that combine webgraph and social mediacontexts generate to state-of-the-art results in website credibilityclassification and significantly improves the top-k identification ofunreliable domains. Additionally, we release a novel dataset of dredge words,highlighting their strong connections to both social media and online commerceplatforms.</description><author>Evan M. Williams, Peter Carragher, Kathleen M. Carley</author><pubDate>Tue, 17 Sep 2024 16:20:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11423v2</guid></item><item><title>LPT++: Efficient Training on Mixture of Long-tailed Experts</title><link>http://arxiv.org/abs/2409.11323v1</link><description>We introduce LPT++, a comprehensive framework for long-tailed classificationthat combines parameter-efficient fine-tuning (PEFT) with a learnable modelensemble. LPT++ enhances frozen Vision Transformers (ViTs) through theintegration of three core components. The first is a universal long-tailedadaptation module, which aggregates long-tailed prompts and visual adapters toadapt the pretrained model to the target domain, meanwhile improving itsdiscriminative ability. The second is the mixture of long-tailed expertsframework with a mixture-of-experts (MoE) scorer, which adaptively calculatesreweighting coefficients for confidence scores from both visual-only andvisual-language (VL) model experts to generate more accurate predictions.Finally, LPT++ employs a three-phase training framework, wherein each criticalmodule is learned separately, resulting in a stable and effective long-tailedclassification training paradigm. Besides, we also propose the simple versionof LPT++ namely LPT, which only integrates visual-only pretrained ViT andlong-tailed prompts to formulate a single model method. LPT can clearlyillustrate how long-tailed prompts works meanwhile achieving comparableperformance without VL pretrained models. Experiments show that, with only ~1%extra trainable parameters, LPT++ achieves comparable accuracy against all thecounterparts.</description><author>Bowen Dong, Pan Zhou, Wangmeng Zuo</author><pubDate>Tue, 17 Sep 2024 16:19:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11323v1</guid></item><item><title>When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather</title><link>http://arxiv.org/abs/2403.13762v2</link><description>In Federated Learning (FL), multiple clients collaboratively train a globalmodel without sharing private data. In semantic segmentation, the Federatedsource Free Domain Adaptation (FFreeDA) setting is of particular interest,where clients undergo unsupervised training after supervised pretraining at theserver side. While few recent works address FL for autonomous vehicles,intrinsic real-world challenges such as the presence of adverse weatherconditions and the existence of different autonomous agents are stillunexplored. To bridge this gap, we address both problems and introduce a newfederated semantic segmentation setting where both car and drone clientsco-exist and collaborate. Specifically, we propose a novel approach for thissetting which exploits a batch-norm weather-aware strategy to dynamically adaptthe model to the different weather conditions, while hyperbolic spaceprototypes are used to align the heterogeneous client representations. Finally,we introduce FLYAWARE, the first semantic segmentation dataset with adverseweather data for aerial vehicles.</description><author>Giulia Rizzoli, Matteo Caligiuri, Donald Shenaj, Francesco Barbato, Pietro Zanuttigh</author><pubDate>Tue, 17 Sep 2024 16:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13762v2</guid></item><item><title>SOAP: Improving and Stabilizing Shampoo using Adam</title><link>http://arxiv.org/abs/2409.11321v1</link><description>There is growing evidence of the effectiveness of Shampoo, a higher-orderpreconditioning method, over Adam in deep learning optimization tasks. However,Shampoo's drawbacks include additional hyperparameters and computationaloverhead when compared to Adam, which only updates running averages of first-and second-moment quantities. This work establishes a formal connection betweenShampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficientapproximation of Adam -- showing that Shampoo is equivalent to runningAdafactor in the eigenbasis of Shampoo's preconditioner. This insight leads tothe design of a simpler and computationally efficient algorithm:$\textbf{S}$hampo$\textbf{O}$ with $\textbf{A}$dam in the$\textbf{P}$reconditioner's eigenbasis (SOAP). With regards to improving Shampoo's computational efficiency, the moststraightforward approach would be to simply compute Shampoo'seigendecomposition less frequently. Unfortunately, as our empirical resultsshow, this leads to performance degradation that worsens with this frequency.SOAP mitigates this degradation by continually updating the running average ofthe second moment, just as Adam does, but in the current (slowly changing)coordinate basis. Furthermore, since SOAP is equivalent to running Adam in arotated space, it introduces only one additional hyperparameter (thepreconditioning frequency) compared to Adam. We empirically evaluate SOAP onlanguage model pre-training with 360m and 660m sized models. In the large batchregime, SOAP reduces the number of iterations by over 40% and wall clock timeby over 35% compared to AdamW, with approximately 20% improvements in bothmetrics compared to Shampoo. An implementation of SOAP is available athttps://github.com/nikhilvyas/SOAP.</description><author>Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham Kakade</author><pubDate>Tue, 17 Sep 2024 16:18:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11321v1</guid></item><item><title>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping</title><link>http://arxiv.org/abs/2409.11316v1</link><description>Few-shot Semantic Segmentation addresses the challenge of segmenting objectsin query images with only a handful of annotated examples. However, manyprevious state-of-the-art methods either have to discard intricate localsemantic features or suffer from high computational complexity. To addressthese challenges, we propose a new Few-shot Semantic Segmentation frameworkbased on the transformer architecture. Our approach introduces the spatialtransformer decoder and the contextual mask generation module to improve therelational understanding between support and query images. Moreover, weintroduce a multi-scale decoder to refine the segmentation mask byincorporating features from different resolutions in a hierarchical manner.Additionally, our approach integrates global features from intermediate encoderstages to improve contextual understanding, while maintaining a lightweightstructure to reduce complexity. This balance between performance and efficiencyenables our method to achieve state-of-the-art results on benchmark datasetssuch as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings.Notably, our model with only 1.5 million parameters demonstrates competitiveperformance while overcoming limitations of existing methodologies.https://github.com/amirrezafateh/MSDNet</description><author>Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</author><pubDate>Tue, 17 Sep 2024 16:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11316v1</guid></item><item><title>fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction</title><link>http://arxiv.org/abs/2409.11315v1</link><description>Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI)data, introduced as Recon3DMind in our conference work, is of significantinterest to both cognitive neuroscience and computer vision. To advance thistask, we present the fMRI-3D dataset, which includes data from 15 participantsand showcases a total of 4768 3D objects. The dataset comprises two components:fMRI-Shape, previously introduced and accessible athttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse,proposed in this paper and available athttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverseincludes data from 5 subjects, 4 of whom are also part of the Core set infMRI-Shape, with each subject viewing 3142 3D objects across 117 categories,all accompanied by text captions. This significantly enhances the diversity andpotential applications of the dataset. Additionally, we propose MinD-3D, anovel framework designed to decode 3D visual information from fMRI signals. Theframework first extracts and aggregates features from fMRI data using aneuro-fusion encoder, then employs a feature-bridge diffusion model to generatevisual features, and finally reconstructs the 3D object using a generativetransformer decoder. We establish new benchmarks by designing metrics at bothsemantic and structural levels to evaluate model performance. Furthermore, weassess our model's effectiveness in an Out-of-Distribution setting and analyzethe attribution of the extracted features and the visual ROIs in fMRI signals.Our experiments demonstrate that MinD-3D not only reconstructs 3D objects withhigh semantic and spatial accuracy but also deepens our understanding of howhuman brain processes 3D visual information. Project page at:https://jianxgao.github.io/MinD-3D.</description><author>Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng, Yanwei Fu</author><pubDate>Tue, 17 Sep 2024 16:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11315v1</guid></item><item><title>SpMis: An Investigation of Synthetic Spoken Misinformation Detection</title><link>http://arxiv.org/abs/2409.11308v1</link><description>In recent years, speech generation technology has advanced rapidly, fueled bygenerative models and large-scale training techniques. While these developmentshave enabled the production of high-quality synthetic speech, they have alsoraised concerns about the misuse of this technology, particularly forgenerating synthetic misinformation. Current research primarily focuses ondistinguishing machine-generated speech from human-produced speech, but themore urgent challenge is detecting misinformation within spoken content. Thistask requires a thorough analysis of factors such as speaker identity, topic,and synthesis. To address this need, we conduct an initial investigation intosynthetic spoken misinformation detection by introducing an open-sourcedataset, SpMis. SpMis includes speech synthesized from over 1,000 speakersacross five common topics, utilizing state-of-the-art text-to-speech systems.Although our results show promising detection capabilities, they also revealsubstantial challenges for practical implementation, underscoring theimportance of ongoing research in this critical area.</description><author>Peizhuo Liu, Li Wang, Renqiang He, Haorui He, Lei Wang, Huadi Zheng, Jie Shi, Tong Xiao, Zhizheng Wu</author><pubDate>Tue, 17 Sep 2024 16:05:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11308v1</guid></item><item><title>GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module</title><link>http://arxiv.org/abs/2409.11307v1</link><description>3D Gaussian Splatting (3DGS) integrates the strengths of primitive-basedrepresentations and volumetric rendering techniques, enabling real-time,high-quality rendering. However, 3DGS models typically overfit to single-scenetraining and are highly sensitive to the initialization of Gaussian ellipsoids,heuristically derived from Structure from Motion (SfM) point clouds, whichlimits both generalization and practicality. To address these limitations, wepropose GS-Net, a generalizable, plug-and-play 3DGS module that densifiesGaussian ellipsoids from sparse SfM point clouds, enhancing geometric structurerepresentation. To the best of our knowledge, GS-Net is the first plug-and-play3DGS module with cross-scene generalization capabilities. Additionally, weintroduce the CARLA-NVS dataset, which incorporates additional cameraviewpoints to thoroughly evaluate reconstruction and rendering quality.Extensive experiments demonstrate that applying GS-Net to 3DGS yields a PSNRimprovement of 2.08 dB for conventional viewpoints and 1.86 dB for novelviewpoints, confirming the method's effectiveness and robustness.</description><author>Yichen Zhang, Zihan Wang, Jiali Han, Peilin Li, Jiaxun Zhang, Jianqiang Wang, Lei He, Keqiang Li</author><pubDate>Tue, 17 Sep 2024 16:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11307v1</guid></item><item><title>UniMODE: Unified Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2402.18573v4</link><description>Realizing unified monocular 3D object detection, including both indoor andoutdoor scenes, holds great importance in applications like robot navigation.However, involving various scenarios of data to train models poses challengesdue to their significantly different characteristics, e.g., diverse geometryproperties and heterogeneous domain distributions. To address these challenges,we build a detector based on the bird's-eye-view (BEV) detection paradigm,where the explicit feature projection is beneficial to addressing the geometrylearning ambiguity when employing multiple scenarios of data to traindetectors. Then, we split the classical BEV detection architecture into twostages and propose an uneven BEV grid design to handle the convergenceinstability caused by the aforementioned challenges. Moreover, we develop asparse BEV feature projection strategy to reduce computational cost and aunified domain alignment method to handle heterogeneous domains. Combiningthese techniques, a unified detector UniMODE is derived, which surpasses theprevious state-of-the-art on the challenging Omni3D dataset (a large-scaledataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing thefirst successful generalization of a BEV detector to unified 3D objectdetection.</description><author>Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao</author><pubDate>Tue, 17 Sep 2024 16:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18573v4</guid></item><item><title>Beyond LoRA: Exploring Efficient Fine-Tuning Techniques for Time Series Foundational Models</title><link>http://arxiv.org/abs/2409.11302v1</link><description>Time Series Foundation Models (TSFMs) have recently garnered attention fortheir ability to model complex, large-scale time series data across domainssuch as retail, finance, and transportation. However, their application tosensitive, domain-specific fields like healthcare remains challenging,primarily due to the difficulty of fine-tuning these models for specialized,out-of-domain tasks with scarce publicly available datasets. In this work, weexplore the use of Parameter-Efficient Fine-Tuning (PEFT) techniques to addressthese limitations, focusing on healthcare applications, particularly ICU vitalsforecasting for sepsis patients. We introduce and evaluate two selective(BitFit and LayerNorm Tuning) and two additive (VeRA and FourierFT) PEFTtechniques on multiple configurations of the Chronos TSFM for forecasting vitalsigns of sepsis patients. Our comparative analysis demonstrates that some ofthese PEFT methods outperform LoRA in terms of parameter efficiency and domainadaptation, establishing state-of-the-art (SOTA) results in ICU vitalforecasting tasks. Interestingly, FourierFT applied to the Chronos (Tiny)variant surpasses the SOTA model while fine-tuning only 2,400 parameterscompared to the 700K parameters of the benchmark.</description><author>Divij Gupta, Anubhav Bhatti, Surajsinh Parmar</author><pubDate>Tue, 17 Sep 2024 15:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11302v1</guid></item><item><title>TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation</title><link>http://arxiv.org/abs/2409.11299v1</link><description>Biomedical image segmentation is crucial for accurately diagnosing andanalyzing various diseases. However, Convolutional Neural Networks (CNNs) andTransformers, the most commonly used architectures for this task, struggle toeffectively capture long-range dependencies due to the inherent locality ofCNNs and the computational complexity of Transformers. To address thislimitation, we introduce TTT-Unet, a novel framework that integrates Test-TimeTraining (TTT) layers into the traditional U-Net architecture for biomedicalimage segmentation. TTT-Unet dynamically adjusts model parameters during thetesting time, enhancing the model's ability to capture both local andlong-range features. We evaluate TTT-Unet on multiple medical imaging datasets,including 3D abdominal organ segmentation in CT and MR images, instrumentsegmentation in endoscopy images, and cell segmentation in microscopy images.The results demonstrate that TTT-Unet consistently outperforms state-of-the-artCNN-based and Transformer-based segmentation models across all tasks. The codeis available at https://github.com/rongzhou7/TTT-Unet.</description><author>Rong Zhou, Zhengqing Yuan, Zhiling Yan, Weixiang Sun, Kai Zhang, Yiwei Li, Yanfang Ye, Xiang Li, Lifang He, Lichao Sun</author><pubDate>Tue, 17 Sep 2024 15:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11299v1</guid></item><item><title>SMILe: Leveraging Submodular Mutual Information For Robust Few-Shot Object Detection</title><link>http://arxiv.org/abs/2407.02665v2</link><description>Confusion and forgetting of object classes have been challenges of primeinterest in Few-Shot Object Detection (FSOD). To overcome these pitfalls inmetric learning based FSOD techniques, we introduce a novel Submodular MutualInformation Learning (SMILe) framework which adopts combinatorial mutualinformation functions to enforce the creation of tighter and discriminativefeature clusters in FSOD. Our proposed approach generalizes to several existingapproaches in FSOD, agnostic of the backbone architecture demonstratingelevated performance gains. A paradigm shift from instance based objectivefunctions to combinatorial objectives in SMILe naturally preserves thediversity within an object class resulting in reduced forgetting when subjectedto few training examples. Furthermore, the application of mutual informationbetween the already learnt (base) and newly added (novel) objects ensuressufficient separation between base and novel classes, minimizing the effect ofclass confusion. Experiments on popular FSOD benchmarks, PASCAL-VOC and MS-COCOshow that our approach generalizes to State-of-the-Art (SoTA) approachesimproving their novel class performance by up to 5.7% (3.3 mAP points) and 5.4%(2.6 mAP points) on the 10-shot setting of VOC (split 3) and 30-shot setting ofCOCO datasets respectively. Our experiments also demonstrate better retentionof base class performance and up to 2x faster convergence over existingapproaches agnostic of the underlying architecture.</description><author>Anay Majee, Ryan Sharp, Rishabh Iyer</author><pubDate>Tue, 17 Sep 2024 15:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02665v2</guid></item><item><title>EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage</title><link>http://arxiv.org/abs/2409.11295v1</link><description>Generalist web agents have evolved rapidly and demonstrated remarkablepotential. However, there are unprecedented safety risks associated with thesethem, which are nearly unexplored so far. In this work, we aim to narrow thisgap by conducting the first study on the privacy risks of generalist web agentsin adversarial environments. First, we present a threat model that discussesthe adversarial targets, constraints, and attack scenarios. Particularly, weconsider two types of adversarial targets: stealing users' specific personallyidentifiable information (PII) or stealing the entire user request. To achievethese objectives, we propose a novel attack method, termed EnvironmentalInjection Attack (EIA). This attack injects malicious content designed to adaptwell to different environments where the agents operate, causing them toperform unintended actions. This work instantiates EIA specifically for theprivacy scenario. It inserts malicious web elements alongside persuasiveinstructions that mislead web agents into leaking private information, and canfurther leverage CSS and JavaScript features to remain stealthy. We collect 177actions steps that involve diverse PII categories on realistic websites fromthe Mind2Web dataset, and conduct extensive experiments using one of the mostcapable generalist web agent frameworks to date, SeeAct. The resultsdemonstrate that EIA achieves up to 70% ASR in stealing users' specific PII.Stealing full user requests is more challenging, but a relaxed version of EIAcan still achieve 16% ASR. Despite these concerning results, it is important tonote that the attack can still be detectable through careful human inspection,highlighting a trade-off between high autonomy and security. This leads to ourdetailed discussion on the efficacy of EIA under different levels of humansupervision as well as implications on defenses for generalist web agents.</description><author>Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun</author><pubDate>Tue, 17 Sep 2024 15:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11295v1</guid></item><item><title>Navigating Process Mining: A Case study using pm4py</title><link>http://arxiv.org/abs/2409.11294v1</link><description>Process-mining techniques have emerged as powerful tools for analyzing eventdata to gain insights into business processes. In this paper, we present acomprehensive analysis of road traffic fine management processes using thepm4py library in Python. We start by importing an event log dataset and exploreits characteristics, including the distribution of activities and processvariants. Through filtering and statistical analysis, we uncover key patternsand variations in the process executions. Subsequently, we apply variousprocess-mining algorithms, including the Alpha Miner, Inductive Miner, andHeuristic Miner, to discover process models from the event log data. Wevisualize the discovered models to understand the workflow structures anddependencies within the process. Additionally, we discuss the strengths andlimitations of each mining approach in capturing the underlying processdynamics. Our findings shed light on the efficiency and effectiveness of roadtraffic fine management processes, providing valuable insights for processoptimization and decision-making. This study demonstrates the utility of pm4pyin facilitating process mining tasks and its potential for analyzing real-worldbusiness processes.</description><author>Ali Jlidi, László Kovács</author><pubDate>Tue, 17 Sep 2024 15:48:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11294v1</guid></item><item><title>SeFlow: A Self-Supervised Scene Flow Method in Autonomous Driving</title><link>http://arxiv.org/abs/2407.01702v2</link><description>Scene flow estimation predicts the 3D motion at each point in successiveLiDAR scans. This detailed, point-level, information can help autonomousvehicles to accurately predict and understand dynamic changes in theirsurroundings. Current state-of-the-art methods require annotated data to trainscene flow networks and the expense of labeling inherently limits theirscalability. Self-supervised approaches can overcome the above limitations, yetface two principal challenges that hinder optimal performance: pointdistribution imbalance and disregard for object-level motion constraints. Inthis paper, we propose SeFlow, a self-supervised method that integratesefficient dynamic classification into a learning-based scene flow pipeline. Wedemonstrate that classifying static and dynamic points helps design targetedobjective functions for different motion patterns. We also emphasize theimportance of internal cluster consistency and correct object point associationto refine the scene flow estimation, in particular on object details. Ourreal-time capable method achieves state-of-the-art performance on theself-supervised scene flow task on Argoverse 2 and Waymo datasets. The code isopen-sourced at https://github.com/KTH-RPL/SeFlow along with trained modelweights.</description><author>Qingwen Zhang, Yi Yang, Peizheng Li, Olov Andersson, Patric Jensfelt</author><pubDate>Tue, 17 Sep 2024 15:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01702v2</guid></item><item><title>Neural Networks for Vehicle Routing Problem</title><link>http://arxiv.org/abs/2409.11290v1</link><description>The Vehicle Routing Problem is about optimizing the routes of vehicles tomeet the needs of customers at specific locations. The route graph consists ofdepots on several levels and customer positions. Several optimization methodshave been developed over the years, most of which are based on some type ofclassic heuristic: genetic algorithm, simulated annealing, tabu search, antcolony optimization, firefly algorithm. Recent developments in machine learningprovide a new toolset, the rich family of neural networks, for tackling complexproblems. The main area of application of neural networks is the area ofclassification and regression. Route optimization can be viewed as a newchallenge for neural networks. The article first presents an analysis of theapplicability of neural network tools, then a novel graphical neural networkmodel is presented in detail. The efficiency analysis based on test experimentsshows the applicability of the proposed NN architecture.</description><author>László Kovács, Ali Jlidi</author><pubDate>Tue, 17 Sep 2024 15:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11290v1</guid></item><item><title>Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling</title><link>http://arxiv.org/abs/2409.11283v1</link><description>LLMs obtain remarkable performance but suffer from hallucinations. Mostresearch on detecting hallucination focuses on the questions with short andconcrete correct answers that are easy to check the faithfulness. Hallucinationdetections for text generation with open-ended answers are more challenging.Some researchers use external knowledge to detect hallucinations in generatedtexts, but external resources for specific scenarios are hard to access. Recentstudies on detecting hallucinations in long text without external resourcesconduct consistency comparison among multiple sampled outputs. To handle longtexts, researchers split long texts into multiple facts and individuallycompare the consistency of each pairs of facts. However, these methods (1)hardly achieve alignment among multiple facts; (2) overlook dependenciesbetween multiple contextual facts. In this paper, we propose a graph-basedcontext-aware (GCA) hallucination detection for text generations, which alignsknowledge facts and considers the dependencies between contextual knowledgetriples in consistency comparison. Particularly, to align multiple facts, weconduct a triple-oriented response segmentation to extract multiple knowledgetriples. To model dependencies among contextual knowledge triple (facts), weconstruct contextual triple into a graph and enhance triples' interactions viamessage passing and aggregating via RGCN. To avoid the omission of knowledgetriples in long text, we conduct a LLM-based reverse verification viareconstructing the knowledge triples. Experiments show that our model enhanceshallucination detection and excels all baselines.</description><author>Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li</author><pubDate>Tue, 17 Sep 2024 15:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11283v1</guid></item><item><title>Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5</title><link>http://arxiv.org/abs/2409.11282v1</link><description>The surge of digital documents in various formats, including lessstandardized documents such as business reports and environmental assessments,underscores the growing importance of Document Understanding. While LargeLanguage Models (LLMs) have showcased prowess across diverse natural languageprocessing tasks, their direct application to Document Understanding remains achallenge. Previous research has demonstrated the utility of LLMs in thisdomain, yet their significant computational demands make them challenging todeploy effectively. Additionally, proprietary Blackbox LLMs often outperformtheir open-source counterparts, posing a barrier to widespread accessibility.In this paper, we delve into the realm of document understanding, leveragingdistillation methods to harness the power of large LLMs while accommodatingcomputational limitations. Specifically, we present a novel approach wherein wedistill document understanding knowledge from the proprietary LLM ChatGPT intoFLAN-T5. Our methodology integrates labeling and curriculum-learning mechanismsto facilitate efficient knowledge transfer. This work contributes to theadvancement of document understanding methodologies by offering a scalablesolution that bridges the gap between resource-intensive LLMs and practicalapplications. Our findings underscore the potential of distillation techniquesin facilitating the deployment of sophisticated language models in real-worldscenarios, thereby fostering advancements in natural language processing anddocument comprehension domains.</description><author>Marcel Lamott, Muhammad Armaghan Shakir</author><pubDate>Tue, 17 Sep 2024 15:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11282v1</guid></item><item><title>P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task</title><link>http://arxiv.org/abs/2409.11279v1</link><description>Embodied Everyday Task is a popular task in the embodied AI community,requiring agents to make a sequence of actions based on natural languageinstructions and visual observations. Traditional learning-based approachesface two challenges. Firstly, natural language instructions often lack explicittask planning. Secondly, extensive training is required to equip models withknowledge of the task environment. Previous works based on Large Language Model(LLM) either suffer from poor performance due to the lack of task-specificknowledge or rely on ground truth as few-shot samples. To address the abovelimitations, we propose a novel approach called Progressive Retrieval AugmentedGeneration (P-RAG), which not only effectively leverages the powerful languageprocessing capabilities of LLMs but also progressively accumulatestask-specific knowledge without ground-truth. Compared to the conventional RAGmethods, which retrieve relevant information from the database in a one-shotmanner to assist generation, P-RAG introduces an iterative approach toprogressively update the database. In each iteration, P-RAG retrieves thelatest database and obtains historical information from the previousinteraction as experiential references for the current interaction. Moreover,we also introduce a more granular retrieval scheme that not only retrievessimilar tasks but also incorporates retrieval of similar situations to providemore valuable reference experiences. Extensive experiments reveal that P-RAGachieves competitive results without utilizing ground truth and can evenfurther improve performance through self-iterations.</description><author>Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li</author><pubDate>Tue, 17 Sep 2024 15:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11279v1</guid></item><item><title>Machine Learning and Theory Ladenness -- A Phenomenological Account</title><link>http://arxiv.org/abs/2409.11277v1</link><description>In recent years, the dissemination of machine learning (ML) methodologies inscientific research has prompted discussions on theory ladenness. Morespecifically, the issue of theory ladenness has remerged as questions aboutwhether and how ML models (MLMs) and ML modelling strategies are impacted bythe domain theory of the scientific field in which ML is used and implemented(e.g., physics, chemistry, biology, etc). On the one hand, some have arguedthat there is no difference between traditional (pre ML) and ML assistedscience. In both cases, theory plays an essential and unavoidable role in theanalysis of phenomena and the construction and use of models. Others haveargued instead that ML methodologies and models are theory independent and, insome cases, even theory free. In this article, we argue that both positions areoverly simplistic and do not advance our understanding of the interplay betweenML methods and domain theories. Specifically, we provide an analysis of theoryladenness in ML assisted science. Our analysis reveals that, while theconstruction of MLMs can be relatively independent of domain theory, thepractical implementation and interpretation of these models within a givenspecific domain still relies on fundamental theoretical assumptions andbackground knowledge.</description><author>Alberto Termine, Emanuele Ratti, Alessandro Facchini</author><pubDate>Tue, 17 Sep 2024 15:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11277v1</guid></item><item><title>A Systematic Review of Generalization Research in Medical Image Classification</title><link>http://arxiv.org/abs/2403.12167v3</link><description>Numerous Deep Learning (DL) classification models have been developed for alarge spectrum of medical image analysis applications, which promises toreshape various facets of medical practice. Despite early advances in DL modelvalidation and implementation, which encourage healthcare institutions to adoptthem, a fundamental questions remain: how can these models effectively handledomain shift? This question is crucial to limit DL models performancedegradation. Medical data are dynamic and prone to domain shift, due tomultiple factors. Two main shift types can occur over time: 1) covariate shiftmainly arising due to updates to medical equipment and 2) concept shift causedby inter-grader variability. To mitigate the problem of domain shift, existingsurveys mainly focus on domain adaptation techniques, with an emphasis oncovariate shift. More generally, no work has reviewed the state-of-the-artsolutions while focusing on the shift types. This paper aims to exploreexisting domain generalization methods for DL-based classification modelsthrough a systematic review of literature. It proposes a taxonomy based on theshift type they aim to solve. Papers were searched and gathered on Scopus till10 April 2023, and after the eligibility screening and quality evaluation, 77articles were identified. Exclusion criteria included: lack of methodologicalnovelty (e.g., reviews, benchmarks), experiments conducted on a singlemono-center dataset, or articles not written in English. The results of thispaper show that learning based methods are emerging, for both shift types.Finally, we discuss future challenges, including the need for improvedevaluation protocols and benchmarks, and envisioned future developments toachieve robust, generalized models for medical image classification.</description><author>Sarah Matta, Mathieu Lamard, Philippe Zhang, Alexandre Le Guilcher, Laurent Borderie, Béatrice Cochener, Gwenolé Quellec</author><pubDate>Tue, 17 Sep 2024 15:27:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12167v3</guid></item><item><title>MURRE: Multi-Hop Table Retrieval with Removal for Open-Domain Text-to-SQL</title><link>http://arxiv.org/abs/2402.10666v4</link><description>The open-domain text-to-SQL task aims to retrieve question-relevant tablesfrom massive databases and generate SQL. However, the performance of currentmethods is constrained by single-hop retrieval, and existing multi-hopretrieval of open-domain question answering is not directly applicable due tothe tendency to retrieve tables similar to the retrieved ones but irrelevant tothe question. Since the questions in text-to-SQL usually contain all requiredinformation, while previous multi-hop retrieval supplements the questions withretrieved documents. Therefore, we propose the multi-hop table retrieval withremoval (MURRE), which removes previously retrieved information from thequestion to guide the retriever towards unretrieved relevant tables. Ourexperiments on two open-domain text-to-SQL datasets demonstrate an averageimprovement of 5.7% over the previous state-of-the-art results.</description><author>Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu, Wanxiang Che</author><pubDate>Tue, 17 Sep 2024 15:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10666v4</guid></item><item><title>Task Arithmetic for Language Expansion in Speech Translation</title><link>http://arxiv.org/abs/2409.11274v1</link><description>Recent advances in large language models (LLMs) have gained interest inspeech-text multimodal foundation models, achieving strong performance oninstruction-based speech translation (ST). However, expanding language pairsfrom an existing instruction-tuned ST system is costly due to the necessity ofre-training on a combination of new and previous datasets. We propose to expandnew language pairs by merging the model trained on new language pairs and theexisting model, using task arithmetic. We find that the direct application oftask arithmetic for ST causes the merged model to fail to follow instructions;thus, generating translation in incorrect languages. To eliminate languageconfusion, we propose an augmented task arithmetic method that merges anadditional language control model. It is trained to generate the correct targetlanguage token following the instructions. Our experiments demonstrate that ourproposed language control model can achieve language expansion by eliminatinglanguage confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66and 4.92 BLEU scores improvement, respectively. In addition, we demonstrate theuse of our task arithmetic framework can expand to a language pair whereneither paired ST training data nor a pre-trained ST model is available. Wefirst synthesize the ST system from machine translation (MT) systems via taskanalogy, then merge the synthesized ST system to the existing ST model.</description><author>Yao-Fei Cheng, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Wen Shen Teo, Siddhant Arora, Shinji Watanabe</author><pubDate>Tue, 17 Sep 2024 15:25:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11274v1</guid></item><item><title>LOLA -- An Open-Source Massively Multilingual Large Language Model</title><link>http://arxiv.org/abs/2409.11272v1</link><description>This paper presents LOLA, a massively multilingual large language modeltrained on more than 160 languages using a sparse Mixture-of-ExpertsTransformer architecture. Our architectural and implementation choices addressthe challenge of harnessing linguistic diversity while maintaining efficiencyand avoiding the common pitfalls of multilinguality. Our analysis of theevaluation results shows competitive performance in natural language generationand understanding tasks. Additionally, we demonstrate how the learnedexpert-routing mechanism exploits implicit phylogenetic linguistic patterns topotentially alleviate the curse of multilinguality. We provide an in-depth lookat the training process, an analysis of the datasets, and a balancedexploration of the model's strengths and limitations. As an open-source model,LOLA promotes reproducibility and serves as a robust foundation for futureresearch. Our findings enable the development of compute-efficient multilingualmodels with strong, scalable performance across languages.</description><author>Nikit Srivastava, Denis Kuchelev, Tatiana Moteu, Kshitij Shetty, Michael Roeder, Diego Moussallem, Hamada Zahera, Axel-Cyrille Ngonga Ngomo</author><pubDate>Tue, 17 Sep 2024 15:23:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11272v1</guid></item><item><title>Geometry Aware Meta-Learning Neural Network for Joint Phase and Precoder Optimization in RIS</title><link>http://arxiv.org/abs/2409.11270v1</link><description>In reconfigurable intelligent surface (RIS) aided systems, the jointoptimization of the precoder matrix at the base station and the phase shifts ofthe RIS elements involves significant complexity. In this paper, we propose acomplex-valued, geometry aware meta-learning neural network that maximizes theweighted sum rate in a multi-user multiple input single output system. Byleveraging the complex circle geometry for phase shifts and spherical geometryfor the precoder, the optimization occurs on Riemannian manifolds, leading tofaster convergence. We use a complex-valued neural network for phase shifts andan Euler inspired update for the precoder network. Our approach outperformsexisting neural network-based algorithms, offering higher weighted sum rates,lower power consumption, and significantly faster convergence. Specifically, itconverges faster by nearly 100 epochs, with a 0.7 bps improvement in weightedsum rate and a 1.8 dBm power gain when compared with existing work.</description><author>Dahlia Devapriya, Sheetal Kalyani</author><pubDate>Tue, 17 Sep 2024 15:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11270v1</guid></item><item><title>Do Language Models Exhibit Human-like Structural Priming Effects?</title><link>http://arxiv.org/abs/2406.04847v2</link><description>We explore which linguistic factors -- at the sentence and token level --play an important role in influencing language model predictions, andinvestigate whether these are reflective of results found in humans and humancorpora (Gries and Kootstra, 2017). We make use of the structural primingparadigm, where recent exposure to a structure facilitates processing of thesame structure. We don't only investigate whether, but also where primingeffects occur, and what factors predict them. We show that these effects can beexplained via the inverse frequency effect, known in human priming, where rarerelements within a prime increase priming effects, as well as lexical dependencebetween prime and target. Our results provide an important piece in the puzzleof understanding how properties within their context affect structuralprediction in language models.</description><author>Jaap Jumelet, Willem Zuidema, Arabella Sinclair</author><pubDate>Tue, 17 Sep 2024 15:17:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04847v2</guid></item><item><title>Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids</title><link>http://arxiv.org/abs/2409.11267v1</link><description>This work proposes an approach that integrates reinforcement learning andmodel predictive control (MPC) to efficiently solve finite-horizon optimalcontrol problems in mixed-logical dynamical systems. Optimization-based controlof such systems with discrete and continuous decision variables entails theonline solution of mixed-integer quadratic or linear programs, which sufferfrom the curse of dimensionality. Our approach aims at mitigating this issue byeffectively decoupling the decision on the discrete variables and the decisionon the continuous variables. Moreover, to mitigate the combinatorial growth inthe number of possible actions due to the prediction horizon, we conceive thedefinition of decoupled Q-functions to make the learning problem moretractable. The use of reinforcement learning reduces the online optimizationproblem of the MPC controller from a mixed-integer linear (quadratic) programto a linear (quadratic) program, greatly reducing the computational time.Simulation experiments for a microgrid, based on real-world data, demonstratethat the proposed method significantly reduces the online computation time ofthe MPC approach and that it generates policies with small optimality gaps andhigh feasibility rates.</description><author>Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter</author><pubDate>Tue, 17 Sep 2024 15:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11267v1</guid></item><item><title>Performance of Cross-Validated Targeted Maximum Likelihood Estimation</title><link>http://arxiv.org/abs/2409.11265v1</link><description>Background: Advanced methods for causal inference, such as targeted maximumlikelihood estimation (TMLE), require certain conditions for statisticalinference. However, in situations where there is not differentiability due todata sparsity or near-positivity violations, the Donsker class condition isviolated. In such situations, TMLE variance can suffer from inflation of thetype I error and poor coverage, leading to conservative confidence intervals.Cross-validation of the TMLE algorithm (CVTMLE) has been suggested to improveon performance compared to TMLE in settings of positivity or Donsker classviolations. We aim to investigate the performance of CVTMLE compared to TMLE invarious settings. Methods: We utilised the data-generating mechanism as described in Leger etal. (2022) to run a Monte Carlo experiment under different Donsker classviolations. Then, we evaluated the respective statistical performances of TMLEand CVTMLE with different super learner libraries, with and without regressiontree methods. Results: We found that CVTMLE vastly improves confidence interval coveragewithout adversely affecting bias, particularly in settings with small samplesizes and near-positivity violations. Furthermore, incorporating regressiontrees using standard TMLE with ensemble super learner-based initial estimatesincreases bias and variance leading to invalid statistical inference. Conclusions: It has been shown that when using CVTMLE the Donsker classcondition is no longer necessary to obtain valid statistical inference whenusing regression trees and under either data sparsity or near-positivityviolations. We show through simulations that CVTMLE is much less sensitive tothe choice of the super learner library and thereby provides better estimationand inference in cases where the super learner library uses more flexiblecandidates and is prone to overfitting.</description><author>Matthew J. Smith, Rachael V. Phillips, Camille Maringe, Miguel Angel Luque Fernandez</author><pubDate>Tue, 17 Sep 2024 15:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11265v1</guid></item><item><title>LC-Protonets: Multi-label Few-shot learning for world music audio tagging</title><link>http://arxiv.org/abs/2409.11264v1</link><description>We introduce Label-Combination Prototypical Networks (LC-Protonets) toaddress the problem of multi-label few-shot classification, where a model mustgeneralize to new classes based on only a few available examples. ExtendingPrototypical Networks, LC-Protonets generate one prototype per labelcombination, derived from the power set of labels present in the limitedtraining items, rather than one prototype per label. Our method is applied toautomatic audio tagging across diverse music datasets, covering variouscultures and including both modern and traditional music, and is evaluatedagainst existing approaches in the literature. The results demonstrate asignificant performance improvement in almost all domains and training setupswhen using LC-Protonets for multi-label classification. In addition to traininga few-shot learning model from scratch, we explore the use of a pre-trainedmodel, obtained via supervised learning, to embed items in the feature space.Fine-tuning improves the generalization ability of all methods, yetLC-Protonets achieve high-level performance even without fine-tuning, incontrast to the comparative approaches. We finally analyze the scalability ofthe proposed method, providing detailed quantitative metrics from ourexperiments. The implementation and experimental setup are made publiclyavailable, offering a benchmark for future research.</description><author>Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</author><pubDate>Tue, 17 Sep 2024 15:13:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11264v1</guid></item><item><title>Bio-Inspired Mamba: Temporal Locality and Bioplausible Learning in Selective State Space Models</title><link>http://arxiv.org/abs/2409.11263v1</link><description>This paper introduces Bio-Inspired Mamba (BIM), a novel online learningframework for selective state space models that integrates biological learningprinciples with the Mamba architecture. BIM combines Real-Time RecurrentLearning (RTRL) with Spike-Timing-Dependent Plasticity (STDP)-like locallearning rules, addressing the challenges of temporal locality and biologicalplausibility in training spiking neural networks. Our approach leverages theinherent connection between backpropagation through time and STDP, offering acomputationally efficient alternative that maintains the ability to capturelong-range dependencies. We evaluate BIM on language modeling, speechrecognition, and biomedical signal analysis tasks, demonstrating competitiveperformance against traditional methods while adhering to biological learningprinciples. Results show improved energy efficiency and potential forneuromorphic hardware implementation. BIM not only advances the field ofbiologically plausible machine learning but also provides insights into themechanisms of temporal information processing in biological neural networks.</description><author>Jiahao Qin</author><pubDate>Tue, 17 Sep 2024 15:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11263v1</guid></item><item><title>The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound Event Detection</title><link>http://arxiv.org/abs/2409.11262v1</link><description>This paper presents a residential audio dataset to support sound eventdetection research for smart home applications aimed at promoting wellbeing forolder adults. The dataset is constructed by deploying audio recording systemsin the homes of 8 participants aged 55-80 years for a 7-day period. Acousticcharacteristics are documented through detailed floor plans and constructionmaterial information to enable replication of the recording environments for AImodel deployment. A novel automated speech removal pipeline is developed, usingpre-trained audio neural networks to detect and remove segments containingspoken voice, while preserving segments containing other sound events. Theresulting dataset consists of privacy-compliant audio recordings thataccurately capture the soundscapes and activities of daily living withinresidential spaces. The paper details the dataset creation methodology, thespeech removal pipeline utilizing cascaded model architectures, and an analysisof the vocal label distribution to validate the speech removal process. Thisdataset enables the development and benchmarking of sound event detectionmodels tailored specifically for in-home applications.</description><author>Gabriel Bibbó, Thomas Deacon, Arshdeep Singh, Mark D. Plumbley</author><pubDate>Tue, 17 Sep 2024 15:10:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11262v1</guid></item><item><title>The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives</title><link>http://arxiv.org/abs/2409.11261v1</link><description>This paper introduces the concept of an education tool that utilizesGenerative Artificial Intelligence (GenAI) to enhance storytelling forchildren. The system combines GenAI-driven narrative co-creation,text-to-speech conversion, and text-to-video generation to produce an engagingexperience for learners. We describe the co-creation process, the adaptation ofnarratives into spoken words using text-to-speech models, and thetransformation of these narratives into contextually relevant visuals throughtext-to-video technology. Our evaluation covers the linguistics of thegenerated stories, the text-to-speech conversion quality, and the accuracy ofthe generated visuals.</description><author>Samee Arif, Taimoor Arif, Aamina Jamal Khan, Muhammad Saad Haroon, Agha Ali Raza, Awais Athar</author><pubDate>Tue, 17 Sep 2024 15:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11261v1</guid></item><item><title>Attacking Slicing Network via Side-channel Reinforcement Learning Attack</title><link>http://arxiv.org/abs/2409.11258v1</link><description>Network slicing in 5G and the future 6G networks will enable the creation ofmultiple virtualized networks on a shared physical infrastructure. Thisinnovative approach enables the provision of tailored networks to accommodatespecific business types or industry users, thus delivering more customized andefficient services. However, the shared memory and cache in network slicingintroduce security vulnerabilities that have yet to be fully addressed. In thispaper, we introduce a reinforcement learning-based side-channel cache attackframework specifically designed for network slicing environments. Unliketraditional cache attack methods, our framework leverages reinforcementlearning to dynamically identify and exploit cache locations storing sensitiveinformation, such as authentication keys and user registration data. We assumethat one slice network is compromised and demonstrate how the attacker caninduce another shared slice to send registration requests, thereby estimatingthe cache locations of critical data. By formulating the cache timing channelattack as a reinforcement learning-driven guessing game between the attackslice and the victim slice, our model efficiently explores possible actions topinpoint memory blocks containing sensitive information. Experimental resultsshowcase the superiority of our approach, achieving a success rate ofapproximately 95\% to 98\% in accurately identifying the storage locations ofsensitive data. This high level of accuracy underscores the potential risks inshared network slicing environments and highlights the need for robust securitymeasures to safeguard against such advanced side-channel attacks.</description><author>Wei Shao, Chandra Thapa, Rayne Holland, Sarah Ali Siddiqui, Seyit Camtepe</author><pubDate>Tue, 17 Sep 2024 15:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11258v1</guid></item><item><title>Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained Image Denoisers</title><link>http://arxiv.org/abs/2409.11256v1</link><description>Recent advancements in deep learning have shown impressive results in imageand video denoising, leveraging extensive pairs of noisy and noise-free datafor supervision. However, the challenge of acquiring paired videos for dynamicscenes hampers the practical deployment of deep video denoising techniques. Incontrast, this obstacle is less pronounced in image denoising, where paireddata is more readily available. Thus, a well-trained image denoiser could serveas a reliable spatial prior for video denoising. In this paper, we propose anovel unsupervised video denoising framework, named ``Temporal As a Plugin''(TAP), which integrates tunable temporal modules into a pre-trained imagedenoiser. By incorporating temporal modules, our method can harness temporalinformation across noisy frames, complementing its power of spatial denoising.Furthermore, we introduce a progressive fine-tuning strategy that refines eachtemporal module using the generated pseudo clean video frames, progressivelyenhancing the network's denoising performance. Compared to other unsupervisedvideo denoising methods, our framework demonstrates superior performance onboth sRGB and raw video denoising datasets.</description><author>Zixuan Fu, Lanqing Guo, Chong Wang, Yufei Wang, Zhihao Li, Bihan Wen</author><pubDate>Tue, 17 Sep 2024 15:05:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11256v1</guid></item><item><title>A Dynamical System View of Langevin-Based Non-Convex Sampling</title><link>http://arxiv.org/abs/2210.13867v3</link><description>Non-convex sampling is a key challenge in machine learning, central tonon-convex optimization in deep learning as well as to approximateprobabilistic inference. Despite its significance, theoretically there remainmany important challenges: Existing guarantees (1) typically only hold for theaveraged iterates rather than the more desirable last iterates, (2) lackconvergence metrics that capture the scales of the variables such asWasserstein distances, and (3) mainly apply to elementary schemes such asstochastic gradient Langevin dynamics. In this paper, we develop a newframework that lifts the above issues by harnessing several tools from thetheory of dynamical systems. Our key result is that, for a large class ofstate-of-the-art sampling schemes, their last-iterate convergence inWasserstein distances can be reduced to the study of their continuous-timecounterparts, which is much better understood. Coupled with standardassumptions of MCMC sampling, our theory immediately yields the last-iterateWasserstein convergence of many advanced sampling schemes such as proximal,randomized mid-point, and Runge-Kutta integrators. Beyond existing methods, ourframework also motivates more efficient schemes that enjoy the same rigorousguarantees.</description><author>Mohammad Reza Karimi, Ya-Ping Hsieh, Andreas Krause</author><pubDate>Tue, 17 Sep 2024 15:03:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.13867v3</guid></item><item><title>Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach</title><link>http://arxiv.org/abs/2409.11254v1</link><description>As the complexity and connectivity of networks increase, the need for novelmalware detection approaches becomes imperative. Traditional security defensesare becoming less effective against the advanced tactics of today'scyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology instrengthening network security, offering detailed analysis of network trafficthat goes beyond simple metadata analysis. DPI examines not only the packetheaders but also the payload content within, offering a thorough insight intothe data traversing the network. This study proposes a novel approach thatleverages a large language model (LLM) and few-shot learning to accuratelyrecognizes novel, unseen malware types with few labels samples. Our proposedapproach uses a pretrained LLM on known malware types to extract the embeddingsfrom packets. The embeddings are then used alongside few labeled samples of anunseen malware type. This technique is designed to acclimate the model todifferent malware representations, further enabling it to generate robustembeddings for each trained and unseen classes. Following the extraction ofembeddings from the LLM, few-shot learning is utilized to enhance performancewith minimal labeled data. Our evaluation, which utilized two renowneddatasets, focused on identifying malware types within network traffic andInternet of Things (IoT) environments. Our approach shows promising resultswith an average accuracy of 86.35% and F1-Score of 86.40% on different malwaretypes across the two datasets.</description><author>Kyle Stein, Andrew A. Mahyari, Guillermo Francia III, Eman El-Sheikh</author><pubDate>Tue, 17 Sep 2024 15:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11254v1</guid></item><item><title>Norm of Mean Contextualized Embeddings Determines their Variance</title><link>http://arxiv.org/abs/2409.11253v1</link><description>Contextualized embeddings vary by context, even for the same token, and forma distribution in the embedding space. To analyze this distribution, we focuson the norm of the mean embedding and the variance of the embeddings. In thisstudy, we first demonstrate that these values follow the well-known formula forvariance in statistics and provide an efficient sequential computation method.Then, by observing embeddings from intermediate layers of several Transformermodels, we found a strong trade-off relationship between the norm and thevariance: as the mean embedding becomes closer to the origin, the varianceincreases. This trade-off is likely influenced by the layer normalizationmechanism used in Transformer models. Furthermore, when the sets of tokenembeddings are treated as clusters, we show that the variance of the entireembedding set can theoretically be decomposed into the within-cluster varianceand the between-cluster variance. We found experimentally that as the layers ofTransformer models deepen, the embeddings move farther from the origin, thebetween-cluster variance relatively decreases, and the within-cluster variancerelatively increases. These results are consistent with existing studies on theanisotropy of the embedding spaces across layers.</description><author>Hiroaki Yamagiwa, Hidetoshi Shimodaira</author><pubDate>Tue, 17 Sep 2024 15:02:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11253v1</guid></item><item><title>Design Optimization of NOMA Aided Multi-STAR-RIS for Indoor Environments: A Convex Approximation Imitated Reinforcement Learning Approach</title><link>http://arxiv.org/abs/2406.13280v2</link><description>Non-orthogonal multiple access (NOMA) enables multiple users to share thesame frequency band, and simultaneously transmitting and reflectingreconfigurable intelligent surface (STAR-RIS) provides 360-degree full-spacecoverage, optimizing both transmission and reflection for improved networkperformance and dynamic control of the indoor environment. However, deployingSTAR-RIS indoors presents challenges in interference mitigation, powerconsumption, and real-time configuration. In this work, a novel networkarchitecture utilizing multiple access points (APs), STAR-RISs, and NOMA isproposed for indoor communication. To address these, we formulate anoptimization problem involving user assignment, access point (AP) beamforming,and STAR-RIS phase control. A decomposition approach is used to solve thecomplex problem efficiently, employing a many-to-one matching algorithm foruser-AP assignment and K-means clustering for resource management.Additionally, multi-agent deep reinforcement learning (MADRL) is leveraged tooptimize the control of the STAR-RIS. Within the proposed MADRL framework, anovel approach is introduced in which each decision variable acts as anindependent agent, enabling collaborative learning and decision making. TheMADRL framework is enhanced by incorporating convex approximation (CA), whichaccelerates policy learning through suboptimal solutions from successive convexapproximation (SCA), leading to faster adaptation and convergence. Simulationsdemonstrate significant improvements in network utility compared to baselineapproaches.</description><author>Yu Min Park, Sheikh Salman Hassan, Yan Kyaw Tun, Eui-Nam Huh, Walid Saad, Choong Seon Hong</author><pubDate>Tue, 17 Sep 2024 15:02:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13280v2</guid></item></channel></rss>