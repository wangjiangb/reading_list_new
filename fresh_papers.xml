<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 05 Oct 2024 01:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats</title><link>http://arxiv.org/abs/2410.02764v1</link><description>We introduce a simple yet effective approach for separating transmitted andreflected light. Our key insight is that the powerful novel view synthesiscapabilities provided by modern inverse rendering methods (e.g.,~3D Gaussiansplatting) allow one to perform flash/no-flash reflection separation usingunpaired measurements -- this relaxation dramatically simplifies imageacquisition over conventional paired flash/no-flash reflection separationmethods. Through extensive real-world experiments, we demonstrate our method,Flash-Splat, accurately reconstructs both transmitted and reflected scenes in3D. Our method outperforms existing 3D reflection separation methods, which donot leverage illumination control, by a large margin. Our project webpage is athttps://flash-splat.github.io/.</description><author>Mingyang Xie, Haoming Cai, Sachin Shah, Yiran Xu, Brandon Y. Feng, Jia-Bin Huang, Christopher A. Metzler</author><pubDate>Thu, 03 Oct 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02764v1</guid></item><item><title>Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos</title><link>http://arxiv.org/abs/2410.02763v1</link><description>There has been growing sentiment recently that modern large multimodal models(LMMs) have addressed most of the key challenges related to short videocomprehension. As a result, both academia and industry are gradually shiftingtheir attention towards the more complex challenges posed by understandinglong-form videos. However, is this really the case? Our studies indicate thatLMMs still lack many fundamental reasoning capabilities even when dealing withshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluationbenchmark encompassing 1000 short and natural video-caption pairs. Wedemonstrate that existing LMMs severely struggle to distinguish temporaldifferences between different actions and object transformations. For example,the best model GPT-4o only obtains ~50% on our text and video scores, showing alarge gap compared to the human baseline of ~90%. All open-source multimodalmodels and CLIP-based models perform much worse, producing mostly random chanceperformance. Through this work, we shed light onto the fact that temporalreasoning in short videos is a problem yet to be fully solved. The dataset andevaluation code are available at https://vinoground.github.io.</description><author>Jianrui Zhang, Mu Cai, Yong Jae Lee</author><pubDate>Thu, 03 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02763v1</guid></item><item><title>Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations</title><link>http://arxiv.org/abs/2410.02762v1</link><description>We investigate the internal representations of vision-language models (VLMs)to address hallucinations, a persistent challenge despite advances in modelsize and training. We project VLMs' internal image representations to theirlanguage vocabulary and observe more confident output probabilities on realobjects than hallucinated objects. We additionally use these outputprobabilities to spatially localize real objects. Building on this approach, weintroduce a knowledge erasure algorithm that removes hallucinations by linearlyorthogonalizing image features with respect to hallucinated object features. Weshow that targeted edits to a model's latent representations can reducehallucinations by up to 25.7% on the COCO2014 dataset while preservingperformance. Our findings demonstrate how a deeper understanding of VLMs'latent representations can enhance reliability and enable novel capabilities,such as zero-shot segmentation.</description><author>Nick Jiang, Anish Kachinthaya, Suzie Petryk, Yossi Gandelsman</author><pubDate>Thu, 03 Oct 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02762v1</guid></item><item><title>Which questions should I answer? Salience Prediction of Inquisitive Questions</title><link>http://arxiv.org/abs/2404.10917v2</link><description>Inquisitive questions -- open-ended, curiosity-driven questions people ask asthey read -- are an integral part of discourse processing (Kehler and Rohde,2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP hastaken advantage of question generation capabilities of LLMs to enhance a widerange of applications. But the space of inquisitive questions is vast: manyquestions can be evoked from a given context. So which of those should beprioritized to find answers? Linguistic theories, unfortunately, have not yetprovided an answer to this question. This paper presents QSALIENCE, a saliencepredictor of inquisitive questions. QSALIENCE is instruction-tuned over ourdataset of linguist-annotated salience scores of 1,766 (context, question)pairs. A question scores high on salience if answering it would greatly enhancethe understanding of the text (Van Rooy, 2003). We show that highly salientquestions are empirically more likely to be answered in the same article,bridging potential questions (Onea, 2016) with Questions Under Discussion(Roberts, 2012). We further validate our findings by showing that answeringsalient questions is an indicator of summarization quality in news.</description><author>Yating Wu, Ritika Mangla, Alexandros G. Dimakis, Greg Durrett, Junyi Jessy Li</author><pubDate>Thu, 03 Oct 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10917v2</guid></item><item><title>FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2410.02761v1</link><description>The rapid development of generative AI is a double-edged sword, which notonly facilitates content creation but also makes image manipulation easier andmore difficult to detect. Although current image forgery detection andlocalization (IFDL) methods are generally effective, they tend to face twochallenges: \textbf{1)} black-box nature with unknown detection principle,\textbf{2)} limited generalization across diverse tampering methods (e.g.,Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose theexplainable IFDL task and design FakeShield, a multi-modal framework capable ofevaluating image authenticity, generating tampered region masks, and providinga judgment basis based on pixel-level and image-level tampering clues.Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creatingthe Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield'stampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guidedExplainable Forgery Detection Module (DTE-FDM) and a Multi-modal ForgeryLocalization Module (MFLM) to address various types of tamper detectioninterpretation and achieve forgery localization guided by detailed textualdescriptions. Extensive experiments demonstrate that FakeShield effectivelydetects and localizes various tampering techniques, offering an explainable andsuperior solution compared to previous IFDL methods.</description><author>Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, Jian Zhang</author><pubDate>Thu, 03 Oct 2024 17:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02761v1</guid></item><item><title>Erasing Conceptual Knowledge from Language Models</title><link>http://arxiv.org/abs/2410.02760v1</link><description>Concept erasure in language models has traditionally lacked a comprehensiveevaluation framework, leading to incomplete assessments of effectiveness oferasure methods. We propose an evaluation paradigm centered on three criticalcriteria: innocence (complete knowledge removal), seamlessness (maintainingconditional fluent generation), and specificity (preserving unrelated taskperformance). Our evaluation metrics naturally motivate the development ofErasure of Language Memory (ELM), a new method designed to address all threedimensions. ELM employs targeted low-rank updates to alter output distributionsfor erased concepts while preserving overall model capabilities includingfluency when prompted for an erased concept. We demonstrate ELM's efficacy onbiosecurity, cybersecurity, and literary domain erasure tasks. Comparativeanalysis shows that ELM achieves superior performance across our proposedmetrics, including near-random scores on erased topic assessments, generationfluency, maintained accuracy on unrelated benchmarks, and robustness underadversarial attacks. Our code, data, and trained models are available athttps://elm.baulab.info</description><author>Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau</author><pubDate>Thu, 03 Oct 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02760v1</guid></item><item><title>CMP: Cooperative Motion Prediction with Multi-Agent Communication</title><link>http://arxiv.org/abs/2403.17916v2</link><description>The confluence of the advancement of Autonomous Vehicles (AVs) and thematurity of Vehicle-to-Everything (V2X) communication has enabled thecapability of cooperative connected and automated vehicles (CAVs). Building ontop of cooperative perception, this paper explores the feasibility andeffectiveness of cooperative motion prediction. Our method, CMP, takes LiDARsignals as model input to enhance tracking and prediction capabilities. Unlikeprevious work that focuses separately on either cooperative perception ormotion prediction, our framework, to the best of our knowledge, is the first toaddress the unified problem where CAVs share information in both perception andprediction modules. Incorporated into our design is the unique capability totolerate realistic V2X bandwidth limitations and transmission delays, whiledealing with bulky perception representations. We also propose a predictionaggregation module, which unifies the predictions obtained by different CAVsand generates the final prediction. Through extensive experiments and ablationstudies on the OPV2V and V2V4Real datasets, we demonstrate the effectiveness ofour method in cooperative perception, tracking, and motion prediction. Inparticular, CMP reduces the average prediction error by 16.4\% with fewermissing detections compared with the no cooperation setting and by 12.3\%compared with the strongest baseline. Our work marks a significant step forwardin the cooperative capabilities of CAVs, showcasing enhanced performance incomplex scenarios. The code can be found on the project website:https://cmp-cooperative-prediction.github.io/.</description><author>Zehao Wang, Yuping Wang, Zhuoyuan Wu, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li</author><pubDate>Thu, 03 Oct 2024 17:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17916v2</guid></item><item><title>Forecasting Smog Clouds With Deep Learning</title><link>http://arxiv.org/abs/2410.02759v1</link><description>In this proof-of-concept study, we conduct multivariate timeseriesforecasting for the concentrations of nitrogen dioxide (NO2), ozone (O3), and(fine) particulate matter (PM10 &amp; PM2.5) with meteorological covariates betweentwo locations using various deep learning models, with a focus on longshort-term memory (LSTM) and gated recurrent unit (GRU) architectures. Inparticular, we propose an integrated, hierarchical model architecture inspiredby air pollution dynamics and atmospheric science that employs multi-tasklearning and is benchmarked by unidirectional and fully-connected models.Results demonstrate that, above all, the hierarchical GRU proves itself as acompetitive and efficient method for forecasting the concentration ofsmog-related pollutants.</description><author>Valentijn Oldenburg, Juan Cardenas-Cartagena, Matias Valdenegro-Toro</author><pubDate>Thu, 03 Oct 2024 17:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02759v1</guid></item><item><title>Loong: Generating Minute-level Long Videos with Autoregressive Language Models</title><link>http://arxiv.org/abs/2410.02757v1</link><description>It is desirable but challenging to generate content-rich long videos in thescale of minutes. Autoregressive large language models (LLMs) have achievedgreat success in generating coherent and long sequences of tokens in the domainof natural language processing, while the exploration of autoregressive LLMsfor video generation is limited to generating short videos of several seconds.In this work, we conduct a deep analysis of the challenges that preventautoregressive LLM-based video generators from generating long videos. Based onthe observations and analysis, we propose Loong, a new autoregressive LLM-basedvideo generator that can generate minute-long videos. Specifically, we modelthe text tokens and video tokens as a unified sequence for autoregressive LLMsand train the model from scratch. We propose progressive short-to-long trainingwith a loss re-weighting scheme to mitigate the loss imbalance problem for longvideo training. We further investigate inference strategies, including videotoken re-encoding and sampling strategies, to diminish error accumulationduring inference. Our proposed Loong can be trained on 10-second videos and beextended to generate minute-level long videos conditioned on text prompts, asdemonstrated by the results. More samples are available at:https://epiphqny.github.io/Loong-video.</description><author>Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, Xihui Liu</author><pubDate>Thu, 03 Oct 2024 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02757v1</guid></item><item><title>CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text</title><link>http://arxiv.org/abs/2410.02756v1</link><description>We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task onMultilingual Coreference Resolution. In this third iteration of the sharedtask, a novel objective is to also predict empty nodes needed for zerocoreference mentions (while the empty nodes were given on input in previousyears). This way, coreference resolution can be performed on raw text. Weevaluate two model variants: a~two-stage approach (where the empty nodes arepredicted first using a pretrained encoder model and then processed togetherwith sentence words by another pretrained model) and a single-stage approach(where a single pretrained encoder model generates empty nodes, coreferencementions, and coreference links jointly). In both settings, CorPipe surpassesother participants by a large margin of 3.9 and 2.8 percent points,respectively. The source code and the trained model are available athttps://github.com/ufal/crac2024-corpipe .</description><author>Milan Straka</author><pubDate>Thu, 03 Oct 2024 17:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02756v1</guid></item><item><title>SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at 1% the Cost</title><link>http://arxiv.org/abs/2410.02755v1</link><description>Creating specialized large language models requires vast amounts of clean,special purpose data for training and fine-tuning. With only a handful ofexisting large-scale, domain-specific datasets, creation of new datasets isrequired in most applications. This requires the development of newapplication-specific filtering of web-scale data. Filtering with ahigh-performance, general-purpose LLM such as GPT-4o can be highly effective,but this is extremely expensive at web-scale. This paper proposes SIEVE, alightweight alternative that matches GPT-4o accuracy at a fraction of the cost.SIEVE can perform up to 500 filtering operations for the cost of one GPT-4ofiltering call. The key to SIEVE is a seamless integration of GPT-4o andlightweight T5 models, using active learning to fine-tune T5 in the backgroundwith a small number of calls to GPT-4o. Once trained, it performs as well asGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on theOpenWebText dataset, using five highly customized filter tasks targeting highquality and domain-specific content. Our results demonstrate the effectivenessand efficiency of our method in curating large, high-quality datasets forlanguage model training at a substantially lower cost (1%) than existingtechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4oachieve similar accuracy, with human evaluators preferring SIEVE's filteringresults to those of GPT-4o.</description><author>Jifan Zhang, Robert Nowak</author><pubDate>Thu, 03 Oct 2024 17:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02755v1</guid></item><item><title>ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for Embodied AI</title><link>http://arxiv.org/abs/2410.02751v1</link><description>Intelligent embodied agents need to quickly adapt to new scenarios byintegrating long histories of experience into decision-making. For instance, arobot in an unfamiliar house initially wouldn't know the locations of objectsneeded for tasks and might perform inefficiently. However, as it gathers moreexperience, it should learn the layout of its environment and remember whereobjects are, allowing it to complete new tasks more efficiently. To enable suchrapid adaptation to new tasks, we present ReLIC, a new approach for in-contextreinforcement learning (RL) for embodied agents. With ReLIC, agents are capableof adapting to new environments using 64,000 steps of in-context experiencewith full attention while being trained through self-generated experience viaRL. We achieve this by proposing a novel policy update scheme for on-policy RLcalled "partial updates'' as well as a Sink-KV mechanism that enables effectiveutilization of a long observation history for embodied agents. Our methodoutperforms a variety of meta-RL baselines in adapting to unseen houses in anembodied multi-object navigation task. In addition, we find that ReLIC iscapable of few-shot imitation learning despite never being trained with expertdemonstrations. We also provide a comprehensive analysis of ReLIC, highlightingthat the combination of large-scale RL training, the proposed partial updatesscheme, and the Sink-KV are essential for effective in-context learning. Thecode for ReLIC and all our experiments is at https://github.com/aielawady/relic</description><author>Ahmad Elawady, Gunjan Chhablani, Ram Ramrakhya, Karmesh Yadav, Dhruv Batra, Zsolt Kira, Andrew Szot</author><pubDate>Thu, 03 Oct 2024 17:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02751v1</guid></item><item><title>NVDS+: Towards Efficient and Versatile Neural Stabilizer for Video Depth Estimation</title><link>http://arxiv.org/abs/2307.08695v3</link><description>Video depth estimation aims to infer temporally consistent depth. Oneapproach is to finetune a single-image model on each video with geometryconstraints, which proves inefficient and lacks robustness. An alternative islearning to enforce consistency from data, which requires well-designed modelsand sufficient video depth data. To address both challenges, we introduce NVDS+that stabilizes inconsistent depth estimated by various single-image models ina plug-and-play manner. We also elaborate a large-scale Video Depth in the Wild(VDW) dataset, which contains 14,203 videos with over two million frames,making it the largest natural-scene video depth dataset. Additionally, abidirectional inference strategy is designed to improve consistency byadaptively fusing forward and backward predictions. We instantiate a modelfamily ranging from small to large scales for different applications. Themethod is evaluated on VDW dataset and three public benchmarks. To furtherprove the versatility, we extend NVDS+ to video semantic segmentation andseveral downstream applications like bokeh rendering, novel view synthesis, and3D reconstruction. Experimental results show that our method achievessignificant improvements in consistency, accuracy, and efficiency. Our workserves as a solid baseline and data foundation for learning-based video depthestimation. Code and dataset are available at:https://github.com/RaymondWang987/NVDS</description><author>Yiran Wang, Min Shi, Jiaqi Li, Chaoyi Hong, Zihao Huang, Juewen Peng, Zhiguo Cao, Jianming Zhang, Ke Xian, Guosheng Lin</author><pubDate>Thu, 03 Oct 2024 17:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08695v3</guid></item><item><title>Accelerating Training with Neuron Interaction and Nowcasting Networks</title><link>http://arxiv.org/abs/2409.04434v2</link><description>Neural network training can be accelerated when a learnable update rule isused in lieu of classic adaptive optimizers (e.g. Adam). However, learnableupdate rules can be costly and unstable to train and use. Recently, Jang et al.(2023) proposed a simpler approach to accelerate training based on weightnowcaster networks (WNNs). In their approach, Adam is used for most of theoptimization steps and periodically, only every few steps, a WNN nowcasts(predicts near future) parameters. We improve WNNs by proposing neuroninteraction and nowcasting (NiNo) networks. In contrast to WNNs, NiNo leveragesneuron connectivity and graph neural networks to more accurately nowcastparameters. We further show that in some networks, such as Transformers,modeling neuron connectivity accurately is challenging. We address this andother limitations, which allows NiNo to accelerate Adam training by up to 50%in vision and language tasks.</description><author>Boris Knyazev, Abhinav Moudgil, Guillaume Lajoie, Eugene Belilovsky, Simon Lacoste-Julien</author><pubDate>Thu, 03 Oct 2024 17:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04434v2</guid></item><item><title>An Online Automatic Modulation Classification Scheme Based on Isolation Distributional Kernel</title><link>http://arxiv.org/abs/2410.02750v1</link><description>Automatic Modulation Classification (AMC), as a crucial technique in modernnon-cooperative communication networks, plays a key role in various civil andmilitary applications. However, existing AMC methods usually are complicatedand can work in batch mode only due to their high computational complexity.This paper introduces a new online AMC scheme based on Isolation DistributionalKernel. Our method stands out in two aspects. Firstly, it is the first proposalto represent baseband signals using a distributional kernel. Secondly, itintroduces a pioneering AMC technique that works well in online settings underrealistic time-varying channel conditions. Through extensive experiments inonline settings, we demonstrate the effectiveness of the proposed classifier.Our results indicate that the proposed approach outperforms existing baselinemodels, including two state-of-the-art deep learning classifiers. Moreover, itdistinguishes itself as the first online classifier for AMC with linear timecomplexity, which marks a significant efficiency boost for real-timeapplications.</description><author>Xinpeng Li, Zile Jiang, Kai Ming Ting, Ye Zhu</author><pubDate>Thu, 03 Oct 2024 17:57:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02750v1</guid></item><item><title>Training Language Models on Synthetic Edit Sequences Improves Code Synthesis</title><link>http://arxiv.org/abs/2410.02749v1</link><description>Software engineers mainly write code by editing existing programs. Incontrast, large language models (LLMs) autoregressively synthesize programs ina single pass. One explanation for this is the scarcity of open-sourced editdata. While high-quality instruction data for code synthesis is already scarce,high-quality edit data is even scarcer. To fill this gap, we develop asynthetic data generation algorithm called LintSeq. This algorithm refactorsexisting code into a sequence of code edits by using a linter to procedurallysample across the error-free insertions that can be used to sequentially writeprograms. It outputs edit sequences as text strings consisting of consecutiveprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction+ program pairs into instruction + program-diff-sequence tuples. Then, weinstruction finetune a series of smaller LLMs ranging from 2.6B to 14Bparameters on both the re-factored and original versions of this dataset,comparing zero-shot performance on code synthesis benchmarks. We show thatduring repeated sampling, edit sequence finetuned models produce more diverseprograms than baselines. This results in better inference-time scaling forbenchmark coverage as a function of samples, i.e. the fraction of problems"pass@k" solved by any attempt given "k" tries. For example, on HumanEvalpass@50, small LLMs finetuned on synthetic edit sequences are competitive withGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)in absolute score. Finally, we also pretrain our own tiny LMs for codeunderstanding. We show that finetuning tiny models on synthetic code editsresults in state-of-the-art code synthesis for the on-device model class. Our150M parameter edit sequence LM matches or outperforms code models with twiceas many parameters, both with and without repeated sampling, including Codexand AlphaCode.</description><author>Ulyana Piterbarg, Lerrel Pinto, Rob Fergus</author><pubDate>Thu, 03 Oct 2024 17:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02749v1</guid></item><item><title>LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction</title><link>http://arxiv.org/abs/2409.18957v2</link><description>Classification tasks are typically handled using Machine Learning (ML)models, which lack a balance between accuracy and interpretability. This paperintroduces a new approach to using Large Language Models (LLMs) forclassification tasks in an explainable way. Unlike ML models that rely heavilyon data cleaning and feature engineering, this method streamlines the processusing LLMs. This paper proposes a new concept called "Language Model Learning(LML)" powered by a new method called "Data-Augmented Prediction (DAP)". Theclassification is performed by LLMs using a method similar to humans manuallyexploring and understanding the data and deciding classifications using data asa reference. In the LML process, a dataset is summarized and evaluated todetermine the features that lead to the classification of each label the most.In the process of DAP, the system uses the data summary and a row of thetesting dataset to automatically generate a query, which is used to retrieverelevant rows from the dataset. A classification is generated by the LLM usingdata summary and relevant rows, ensuring satisfactory accuracy even withcomplex data using context-aware decision-making. LML and DAP unlock thepossibilities of new applications. The proposed method uses the words "Act asan Explainable Machine Learning Model" in the prompt to enhance theinterpretability of the predictions by allowing users to review the logicbehind each prediction. In some test cases, the system scored an accuracy above90%, proving the effectiveness of the system and its potential to outperformconventional ML models in various scenarios. The code is available athttps://github.com/Pro-GenAI/LML-DAP</description><author>Praneeth Vadlapati</author><pubDate>Thu, 03 Oct 2024 17:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18957v2</guid></item><item><title>CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation</title><link>http://arxiv.org/abs/2410.02748v1</link><description>Large language models (LLMs) can generate fluent summaries across domainsusing prompting techniques, reducing the need to train models for summarizationapplications. However, crafting effective prompts that guide LLMs to generatesummaries with the appropriate level of detail and writing style remains achallenge. In this paper, we explore the use of salient information extractedfrom the source document to enhance summarization prompts. We show that addingkeyphrases in prompts can improve ROUGE F1 and recall, making the generatedsummaries more similar to the reference and more complete. The number ofkeyphrases can control the precision-recall trade-off. Furthermore, ouranalysis reveals that incorporating phrase-level salient information issuperior to word- or sentence-level. However, the impact on hallucination isnot universally positive across LLMs. To conduct this analysis, we introduceKeyphrase Signal Extractor (CriSPO), a lightweight model that can be finetunedto extract salient keyphrases. By using CriSPO, we achieve consistent ROUGEimprovements across datasets and open-weight and proprietary LLMs without anyLLM customization. Our findings provide insights into leveraging salientinformation in building prompt-based summarization systems.</description><author>Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff</author><pubDate>Thu, 03 Oct 2024 17:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02748v1</guid></item><item><title>SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing</title><link>http://arxiv.org/abs/2404.05717v3</link><description>Effective editing of personal content holds a pivotal role in enablingindividuals to express their creativity, weaving captivating narratives withintheir visual stories, and elevate the overall quality and impact of theirvisual content. Therefore, in this work, we introduce SwapAnything, a novelframework that can swap any objects in an image with personalized conceptsgiven by the reference, while keeping the context unchanged. Compared withexisting methods for personalized subject swapping, SwapAnything has threeunique advantages: (1) precise control of arbitrary objects and parts ratherthan the main subject, (2) more faithful preservation of context pixels, (3)better adaptation of the personalized concept to the image. First, we proposetargeted variable swapping to apply region control over latent feature maps andswap masked variables for faithful context preservation and initial semanticconcept swapping. Then, we introduce appearance adaptation, to seamlessly adaptthe semantic concept into the original image in terms of target location,shape, style, and content during the image generation process. Extensiveresults on both human and automatic evaluation demonstrate significantimprovements of our approach over baseline methods on personalized swapping.Furthermore, SwapAnything shows its precise and faithful swapping abilitiesacross single object, multiple objects, partial object, and cross-domainswapping tasks. SwapAnything also achieves great performance on text-basedswapping and tasks beyond swapping such as object insertion.</description><author>Jing Gu, Nanxuan Zhao, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, Yilin Wang, Xin Eric Wang</author><pubDate>Thu, 03 Oct 2024 17:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05717v3</guid></item><item><title>Tokenization Falling Short: The Curse of Tokenization</title><link>http://arxiv.org/abs/2406.11687v2</link><description>Language models typically tokenize raw text into sequences of subwordidentifiers from a predefined vocabulary, a process inherently sensitive totypographical errors, length variations, and largely oblivious to the internalstructure of tokens--issues we term the curse of tokenization. In this study,we delve into these drawbacks and demonstrate that large language models (LLMs)remain susceptible to these problems. This study systematically investigatesthese challenges and their impact on LLMs through three critical researchquestions: (1) complex problem solving, (2) token structure probing, and (3)resilience to typographical variation. Our findings reveal that scaling modelparameters can mitigate the issue of tokenization; however, LLMs still sufferfrom biases induced by typos and other text format variations. Our experimentsshow that subword regularization such as BPE-dropout can mitigate this issue.We release our evaluation code and data at https://github.com/FloatAI/TKEval.</description><author>Yekun Chai, Yewei Fang, Qiwei Peng, Xuhong Li</author><pubDate>Thu, 03 Oct 2024 17:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11687v2</guid></item><item><title>On Training Data Influence of GPT Models</title><link>http://arxiv.org/abs/2404.07840v3</link><description>Amidst the rapid advancements in generative language models, theinvestigation of how training data shapes the performance of GPT models isstill emerging. This paper presents GPTfluence, a novel approach that leveragesa featurized simulation to assess the impact of training examples on thetraining dynamics of GPT models. Our approach not only traces the influence ofindividual training instances on performance trajectories, such as loss andother key metrics, on targeted test points but also enables a comprehensivecomparison with existing methods across various training scenarios in GPTmodels, ranging from 14 million to 2.8 billion parameters, across a range ofdownstream tasks. Contrary to earlier methods that struggle with generalizationto new data, GPTfluence introduces a parameterized simulation of trainingdynamics, demonstrating robust generalization capabilities to unseen trainingdata. This adaptability is evident across both fine-tuning andinstruction-tuning scenarios, spanning tasks in natural language understandingand generation. We make our code and data publicly available athttps://github.com/ernie-research/gptfluence.</description><author>Yekun Chai, Qingyi Liu, Shuohuan Wang, Yu Sun, Qiwei Peng, Hua Wu</author><pubDate>Thu, 03 Oct 2024 17:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07840v3</guid></item><item><title>Contrastive Localized Language-Image Pre-Training</title><link>http://arxiv.org/abs/2410.02746v1</link><description>Contrastive Language-Image Pre-training (CLIP) has been a celebrated methodfor training vision encoders to generate image/text representationsfacilitating various applications. Recently, CLIP has been widely adopted asthe vision backbone of multimodal large language models (MLLMs) to connectimage inputs for language interactions. The success of CLIP as avision-language foundation model relies on aligning web-crawled noisy textannotations at image levels. Nevertheless, such criteria may becomeinsufficient for downstream tasks in need of fine-grained visionrepresentations, especially when region-level understanding is demanding forMLLMs. In this paper, we improve the localization capability of CLIP withseveral advances. We propose a pre-training method called Contrastive LocalizedLanguage-Image Pre-training (CLOC) by complementing CLIP with region-textcontrastive loss and modules. We formulate a new concept, promptableembeddings, of which the encoder produces image embeddings easy to transforminto region representations given spatial hints. To support large-scalepre-training, we design a visually-enriched and spatially-localized captioningframework to effectively generate region-text pseudo-labels at scale. Byscaling up to billions of annotated images, CLOC enables high-quality regionalembeddings for image region recognition and retrieval tasks, and can be adrop-in replacement of CLIP to enhance MLLMs, especially on referring andgrounding tasks.</description><author>Hong-You Chen, Zhengfeng Lai, Haotian Zhang, Xinze Wang, Marcin Eichner, Keen You, Meng Cao, Bowen Zhang, Yinfei Yang, Zhe Gan</author><pubDate>Thu, 03 Oct 2024 17:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02746v1</guid></item><item><title>PharmacyGPT: The AI Pharmacist</title><link>http://arxiv.org/abs/2307.10432v3</link><description>In this study, we introduce PharmacyGPT, a novel framework to assess thecapabilities of large language models (LLMs) such as ChatGPT and GPT-4 inemulating the role of clinical pharmacists. Our methodology encompasses theutilization of LLMs to generate comprehensible patient clusters, formulatemedication plans, and forecast patient outcomes. We conduct our investigationusing real data acquired from the intensive care unit (ICU) at the Universityof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuableinsights into the potential applications and limitations of LLMs in the fieldof clinical pharmacy, with implications for both patient care and thedevelopment of future AI-driven healthcare solutions. By evaluating theperformance of PharmacyGPT, we aim to contribute to the ongoing discoursesurrounding the integration of artificial intelligence in healthcare settings,ultimately promoting the responsible and efficacious use of such technologies.</description><author>Zhengliang Liu, Zihao Wu, Mengxuan Hu, Bokai Zhao, Lin Zhao, Tianyi Zhang, Haixing Dai, Xianyan Chen, Ye Shen, Sheng Li, Quanzheng Li, Xiang Li, Brian Murray, Tianming Liu, Andrea Sikora</author><pubDate>Thu, 03 Oct 2024 17:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10432v3</guid></item><item><title>Neutral residues: revisiting adapters for model extension</title><link>http://arxiv.org/abs/2410.02744v1</link><description>We address the problem of extending a pretrained large language model to anew domain that was not seen at training time, like adding a language for whichthe original model has seen no or little training data. Popular solutions likefine-tuning or low-rank adaptation are successful at domain adaptation, butformally they do not add any extra capacity and degrade the performance in theoriginal domain. Our paper analyzes this extension problem under three angles: data,architecture and training procedure, which are advantageously consideredjointly. In particular, we improve adapters and make it possible to learn anentire new language while ensuring that the output of the neural network isalmost unchanged in the original domain. For this purpose, we modify the newresidual blocks in a way that leads each new residual block to outputnear-zeros in the original domain. This solution of neutral residues, which borrows architectural componentsfrom mixture of experts, is effective: with only 20% extra learnable weightscompared to an original model trained on English, we get results that aresignificantly better than concurrent approaches (fine-tuning, low-rank orvanilla adapters) in terms of the trade-off between learning a new language andnot forgetting English.</description><author>Franck Signe Talla, Herve Jegou, Edouard Grave</author><pubDate>Thu, 03 Oct 2024 17:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02744v1</guid></item><item><title>MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions</title><link>http://arxiv.org/abs/2410.02743v1</link><description>Reinforcement learning from human feedback (RLHF) has demonstratedeffectiveness in aligning large language models (LLMs) with human preferences.However, token-level RLHF suffers from the credit assignment problem over longsequences, where delayed rewards make it challenging for the model to discernwhich actions contributed to successful outcomes. This hinders learningefficiency and slows convergence. In this paper, we propose MA-RLHF, a simpleyet effective RLHF framework that incorporates macro actions -- sequences oftokens or higher-level language constructs -- into the learning process. Byoperating at this higher level of abstraction, our approach reduces thetemporal distance between actions and rewards, facilitating faster and moreaccurate credit assignment. This results in more stable policy gradientestimates and enhances learning efficiency within each episode, all withoutincreasing computational complexity during training or inference. We validateour approach through extensive experiments across various model sizes andtasks, including text summarization, dialogue generation, question answering,and program synthesis. Our method achieves substantial performance improvementsover standard RLHF, with performance gains of up to 30% in text summarizationand code generation, 18% in dialogue, and 8% in question answering tasks.Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster interms of training time and continues to outperform it with further training. Wewill make our code and data publicly available athttps://github.com/ernie-research/MA-RLHF .</description><author>Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, Hua Wu</author><pubDate>Thu, 03 Oct 2024 17:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02743v1</guid></item><item><title>Grounding Large Language Models In Embodied Environment With Imperfect World Models</title><link>http://arxiv.org/abs/2410.02742v1</link><description>Despite a widespread success in various applications, large language models(LLMs) often stumble when tackling basic physical reasoning or executingrobotics tasks, due to a lack of direct experience with the physical nuances ofthe real world. To address these issues, we propose a Grounding Large languagemodel with Imperfect world MOdel (GLIMO), which utilizes proxy world modelssuch as simulators to collect and synthesize trining data. GLIMO incorporatesan LLM agent-based data generator to automatically create high-quality anddiverse instruction datasets. The generator includes an iterative self-refiningmodule for temporally consistent experience sampling, a diverse set ofquestion-answering instruction seeds, and a retrieval-augmented generationmodule for reflecting on prior experiences. Comprehensive experiments show thatour approach improve the performance of strong open-source LLMs like LLaMA-3with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$across three different benchmarks, respectively. The performance is able tocompete with or surpass their larger counterparts such as GPT-4.</description><author>Haolan Liu, Jishen Zhao</author><pubDate>Thu, 03 Oct 2024 17:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02742v1</guid></item><item><title>Salient Information Prompting to Steer Content in Prompt-based Abstractive Summarization</title><link>http://arxiv.org/abs/2410.02741v1</link><description>Large language models (LLMs) can generate fluent summaries across domainsusing prompting techniques, reducing the need to train models for summarizationapplications. However, crafting effective prompts that guide LLMs to generatesummaries with the appropriate level of detail and writing style remains achallenge. In this paper, we explore the use of salient information extractedfrom the source document to enhance summarization prompts. We show that addingkeyphrases in prompts can improve ROUGE F1 and recall, making the generatedsummaries more similar to the reference and more complete. The number ofkeyphrases can control the precision-recall trade-off. Furthermore, ouranalysis reveals that incorporating phrase-level salient information issuperior to word- or sentence-level. However, the impact on hallucination isnot universally positive across LLMs. To conduct this analysis, we introduceKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetunedto extract salient keyphrases. By using SigExt, we achieve consistent ROUGEimprovements across datasets and open-weight and proprietary LLMs without anyLLM customization. Our findings provide insights into leveraging salientinformation in building prompt-based summarization systems.</description><author>Lei Xu, Mohammed Asad Karim, Saket Dingliwal, Aparna Elangovan</author><pubDate>Thu, 03 Oct 2024 17:54:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02741v1</guid></item><item><title>Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models</title><link>http://arxiv.org/abs/2410.02740v1</link><description>Recent advancements in multimodal models highlight the value of rewrittencaptions for improving performance, yet key challenges remain. For example,while synthetic captions often provide superior quality and image-textalignment, it is not clear whether they can fully replace AltTexts: the role ofsynthetic captions and their interaction with original web-crawled AltTexts inpre-training is still not well understood. Moreover, different multimodalfoundation models may have unique preferences for specific caption formats, butefforts to identify the optimal captions for each model remain limited. In thiswork, we propose a novel, controllable, and scalable captioning pipelinedesigned to generate diverse caption formats tailored to various multimodalmodels. By examining Short Synthetic Captions (SSC) towards Dense SyntheticCaptions (DSC+) as case studies, we systematically explore their effects andinteractions with AltTexts across models such as CLIP, multimodal LLMs, anddiffusion models. Our findings reveal that a hybrid approach that keeps bothsynthetic captions and AltTexts can outperform the use of synthetic captionsalone, improving both alignment and performance, with each model demonstratingpreferences for particular caption formats. This comprehensive analysisprovides valuable insights into optimizing captioning strategies, therebyadvancing the pre-training of multimodal foundation models.</description><author>Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang, Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch, Meng Cao, Yinfei Yang</author><pubDate>Thu, 03 Oct 2024 17:54:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02740v1</guid></item><item><title>Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge</title><link>http://arxiv.org/abs/2410.02736v1</link><description>LLM-as-a-Judge has been widely utilized as an evaluation method in variousbenchmarks and served as supervised rewards in model training. However, despitetheir excellence in many domains, potential issues are under-explored,undermining their reliability and the scope of their utility. Therefore, weidentify 12 key potential biases and propose a new automated biasquantification framework-CALM-which systematically quantifies and analyzes eachtype of bias in LLM-as-a-Judge by using automated and principle-guidedmodification. Our experiments cover multiple popular language models, and theresults indicate that while advanced models have achieved commendable overallperformance, significant biases persist in certain specific tasks. Empiricalresults suggest that there remains room for improvement in the reliability ofLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influenceof these biases and give some suggestions for the reliable application ofLLM-as-a-Judge. Our work highlights the need for stakeholders to address theseissues and remind users to exercise caution in LLM-as-a-Judge applications.</description><author>Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh V Chawla, Xiangliang Zhang</author><pubDate>Thu, 03 Oct 2024 17:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02736v1</guid></item><item><title>Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation</title><link>http://arxiv.org/abs/2303.17051v3</link><description>The recent popularity of foundation models and the pre-train-and-adaptparadigm, where a large-scale model is transferred to downstream tasks, isgaining attention for volumetric medical image segmentation. However, currenttransfer learning strategies devoted to full fine-tuning for transfer learningmay require significant resources and yield sub-optimal results when thelabeled data of the target task is scarce. This makes its applicability in realclinical settings challenging since these institutions are usually constrainedon data and computational resources to develop proprietary solutions. Toaddress this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), anovel and realistic scenario for adapting medical image segmentation foundationmodels. This setting considers the key role of both data- and parameter-efficiency during adaptation. Building on a foundation model pre-trained onopen-access CT organ segmentation sources, we propose leveragingParameter-Efficient Fine-Tuning and black-box Adapters to address suchchallenges. Furthermore, novel efficient adaptation methodologies areintroduced in this work, which include Spatial black-box Adapters that are moreappropriate for dense prediction tasks and constrained transductive inference,leveraging task-specific prior knowledge. Our comprehensive transfer learningexperiments confirm the suitability of foundation models in medical imagesegmentation and unveil the limitations of popular fine-tuning strategies infew-shot scenarios.</description><author>Julio Silva-Rodr√≠guez, Jose Dolz, Ismail Ben Ayed</author><pubDate>Thu, 03 Oct 2024 17:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17051v3</guid></item><item><title>Anchor-Controlled Generative Adversarial Network for High-Fidelity Electromagnetic and Structurally Diverse Metasurface Design</title><link>http://arxiv.org/abs/2408.16231v2</link><description>Metasurfaces, capable of manipulating light at subwavelength scales, holdgreat potential for advancing optoelectronic applications. Generative models,particularly Generative Adversarial Networks (GANs), offer a promising approachfor metasurface inverse design by efficiently navigating complex design spacesand capturing underlying data patterns. However, existing generative modelsstruggle to achieve high electromagnetic fidelity and structural diversity.These challenges arise from the lack of explicit electromagnetic constraintsduring training, which hinders accurate structure-to-electromagnetic responsemapping, and the absence of mechanisms to handle one-to-many mappings dilemma,resulting in insufficient structural diversity. To address these issues, wepropose the Anchor-controlled Generative Adversarial Network (AcGAN), a novelframework that improves both electromagnetic fidelity and structural diversity.To achieve high electromagnetic fidelity, AcGAN proposes the Spectral OverlapCoefficient (SOC) for precise spectral fidelity assessment and developsAnchorNet, which provides real-time feedback on electromagnetic performance torefine the structure-to-electromagnetic mapping. To enhance structuraldiversity, AcGAN incorporates a cluster-guided controller that refines inputprocessing and ensures multi-level spectral integration, guiding the generationprocess to explore multiple configurations for the same spectral target.Additionally, a dynamic loss function progressively shifts the focus fromdata-driven learning to optimizing both spectral fidelity and structuraldiversity. Empirical analysis shows that AcGAN reduces the Mean Squared Error(MSE) by 73% compared to current state-of-the-art GANs methods andsignificantly expands the design space to generate diverse metasurfacearchitectures that meet precise spectral demands.</description><author>Yunhui Zeng, Hongkun Cao, Xin Jin</author><pubDate>Thu, 03 Oct 2024 17:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16231v2</guid></item><item><title>OOD-Chameleon: Is Algorithm Selection for OOD Generalization Learnable?</title><link>http://arxiv.org/abs/2410.02735v1</link><description>Out-of-distribution (OOD) generalization is challenging because distributionshifts come in many forms. A multitude of learning algorithms exist and eachcan improve performance in specific OOD situations. We posit that much of thechallenge of OOD generalization lies in choosing the right algorithm for theright dataset. However, such algorithm selection is often elusive under complexreal-world shifts. In this work, we formalize the task of algorithm selectionfor OOD generalization and investigate whether it could be approached bylearning. We propose a solution, dubbed OOD-Chameleon that treats the task as asupervised classification over candidate algorithms. We construct a dataset ofdatasets to learn from, which represents diverse types, magnitudes andcombinations of shifts (covariate shift, label shift, spurious correlations).We train the model to predict the relative performance of algorithms given adataset's characteristics. This enables a priori selection of the best learningstrategy, i.e. without training various models as needed with traditional modelselection. Our experiments show that the adaptive selection outperforms anyindividual algorithm and simple selection heuristics, on unseen datasets ofcontrollable and realistic image data. Inspecting the model shows that itlearns non-trivial data/algorithms interactions, and reveals the conditions forany one algorithm to surpass another. This opens new avenues for (1) enhancingOOD generalization with existing algorithms instead of designing new ones, and(2) gaining insights into the applicability of existing algorithms with respectto datasets' properties.</description><author>Liangze Jiang, Damien Teney</author><pubDate>Thu, 03 Oct 2024 17:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02735v1</guid></item><item><title>Data Similarity-Based One-Shot Clustering for Multi-Task Hierarchical Federated Learning</title><link>http://arxiv.org/abs/2410.02733v1</link><description>We address the problem of cluster identity estimation in a hierarchicalfederated learning setting in which users work toward learning different tasks.To overcome the challenge of task heterogeneity, users need to be grouped in away such that users with the same task are in the same group, conductingtraining together, while sharing the weights of feature extraction layers withthe other groups. Toward that end, we propose a one-shot clustering algorithmthat can effectively identify and group users based on their data similarity.This enables more efficient collaboration and sharing of a common layerrepresentation within the federated learning system. Our proposed algorithm notonly enhances the clustering process, but also overcomes challenges related toprivacy concerns, communication overhead, and the need for prior knowledgeabout learning models or loss function behaviors. We validate our proposedalgorithm using various datasets such as CIFAR-10 and Fashion MNIST, and showthat it outperforms the baseline in terms of accuracy and variance reduction.</description><author>Abdulmoneam Ali, Ahmed Arafa</author><pubDate>Thu, 03 Oct 2024 17:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02733v1</guid></item><item><title>Preble: Efficient Distributed Prompt Scheduling for LLM Serving</title><link>http://arxiv.org/abs/2407.00023v2</link><description>Prompts to large language models (LLMs) have evolved beyond simple userquestions. For LLMs to solve complex problems, today's practices are to includedomain-specific instructions, illustration of tool usages, and/or long contextsuch as textbook chapters in prompts. As such, many parts of prompts arerepetitive across requests. Recent works propose to cache and reuse KV state ofprompts. However, they are all confined to a single-GPU optimization, whileproduction LLM serving systems are distributed by nature. This paper proposes Preble, the first distributed LLM serving platform thattargets and optimizes for prompt sharing. We designed a distributed schedulingsystem that co-optimizes KV state reuse and computation load-balancing with anew scheduling algorithm and a hierarchical scheduling mechanism. Ourevaluation of Preble with real workloads and request arrival patterns on twoopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5Xto 14.5X on average latency and 2X to 10X on p99 latency.</description><author>Vikranth Srivatsa, Zijian He, Reyna Abhyankar, Dongming Li, Yiying Zhang</author><pubDate>Thu, 03 Oct 2024 17:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00023v2</guid></item><item><title>Custom Non-Linear Model Predictive Control for Obstacle Avoidance in Indoor and Outdoor Environments</title><link>http://arxiv.org/abs/2410.02732v1</link><description>Navigating complex environments requires Unmanned Aerial Vehicles (UAVs) andautonomous systems to perform trajectory tracking and obstacle avoidance inreal-time. While many control strategies have effectively utilized linearapproximations, addressing the non-linear dynamics of UAV, especially inobstacle-dense environments, remains a key challenge that requires furtherresearch. This paper introduces a Non-linear Model Predictive Control (NMPC)framework for the DJI Matrice 100, addressing these challenges by using adynamic model and B-spline interpolation for smooth reference trajectories,ensuring minimal deviation while respecting safety constraints. The frameworksupports various trajectory types and employs a penalty-based cost function forcontrol accuracy in tight maneuvers. The framework utilizes CasADi forefficient real-time optimization, enabling the UAV to maintain robust operationeven under tight computational constraints. Simulation and real-world indoorand outdoor experiments demonstrated the NMPC ability to adapt to disturbances,resulting in smooth, collision-free navigation.</description><author>Lara Laban, Mariusz Wzorek, Piotr Rudol, Tommy Persson</author><pubDate>Thu, 03 Oct 2024 17:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02732v1</guid></item><item><title>DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects</title><link>http://arxiv.org/abs/2410.02730v1</link><description>Object navigation in unknown environments is crucial for deploying embodiedagents in real-world applications. While we have witnessed huge progress due tolarge-scale scene datasets, faster simulators, and stronger models, previousstudies mainly focus on limited scene types and target objects. In this paper,we study a new task of navigating to diverse target objects in a large numberof scene types. To benchmark the problem, we present a large-scale scenedataset, DivScene, which contains 4,614 scenes across 81 different types. Withthe dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning aLarge Vision Language Model (LVLM) through imitation learning. The LVLM istrained to take previous observations from the environment and generate thenext actions. We also introduce CoT explanation traces of the action predictionfor better performance when tuning LVLMs. Our extensive experiments find thatwe can build a performant LVLM-based agent through imitation learning on theshortest paths constructed by a BFS planner without any human supervision. Ouragent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, wecarry out various analyses showing the generalization ability of our agent.</description><author>Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu</author><pubDate>Thu, 03 Oct 2024 17:49:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02730v1</guid></item><item><title>Unified Multi-Modal Interleaved Document Representation for Information Retrieval</title><link>http://arxiv.org/abs/2410.02729v1</link><description>Information Retrieval (IR) methods aim to identify relevant documents inresponse to a given query, which have gained remarkable attention due to theirsuccessful application in various natural language tasks. However, existingapproaches typically consider only the textual information within thedocuments, which overlooks the fact that documents can contain multiplemodalities, including texts, images, and tables. Further, they often segmenteach long document into multiple discrete passages for embedding, preventingthem from capturing the overall document context and interactions betweenparagraphs. We argue that these two limitations lead to suboptimal documentrepresentations for retrieval. In this work, to address them, we aim to producemore comprehensive and nuanced document representations by holisticallyembedding documents interleaved with different modalities. Specifically, weachieve this by leveraging the capability of recent vision-language models thatenable the processing and integration of text, images, and tables into aunified format and representation. Moreover, to mitigate the information lossfrom segmenting documents into passages, instead of representing and retrievingpassages individually, we further merge the representations of segmentedpassages into one single document representation, while we additionallyintroduce a reranking strategy to decouple and identify the relevant passagewithin the document if necessary. Then, through extensive experiments ondiverse information retrieval scenarios considering both the textual andmultimodal queries, we show that our approach substantially outperformsrelevant baselines, thanks to the consideration of the multimodal informationinterleaved within the documents in a unified way.</description><author>Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, Sung Ju Hwang</author><pubDate>Thu, 03 Oct 2024 17:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02729v1</guid></item><item><title>Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation</title><link>http://arxiv.org/abs/2410.02725v1</link><description>Inference-time computation is a powerful paradigm to enhance the performanceof large language models (LLMs), with Best-of-N sampling being a widely usedtechnique. However, this method is computationally expensive, requiring both(1) an external reward model and (2) the generation of multiple samples. Inthis work, we introduce a new generative self-evaluation scheme designed toadaptively reduce the number of generated samples while maintaining or evenimproving performance. We use a generative reward model formulation, allowingthe LLM to predict mid-generation the probability that restarting thegeneration will yield a better response. These predictions are obtained withoutan external reward model and can be used to decide whether or not to generatemore samples, prune unpromising samples early on, or to pick the best sample.This capability is very inexpensive as it involves generating a singlepredefined token. Trained using a dataset constructed with real unfilteredLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEvalincreases from 21% to 34% with 16 samples and math performance on GSM8Kimproves from 84% to 91%. By sampling only when the LLM determines that it isbeneficial to do so and adaptively adjusting temperature annealing, wedemonstrate that 74% of the improvement from using 16 samples can be achievedwith only 1.2 samples on average. We further demonstrate that 50-75% of samplescan be pruned early in generation with minimal degradation in performance.Overall, our methods enable more efficient and scalable compute utilizationduring inference for LLMs.</description><author>Rohin Manvi, Anikait Singh, Stefano Ermon</author><pubDate>Thu, 03 Oct 2024 17:47:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02725v1</guid></item><item><title>Autoregressive Pre-Training on Pixels and Texts</title><link>http://arxiv.org/abs/2404.10710v3</link><description>The integration of visual and textual information represents a promisingdirection in the advancement of language models. In this paper, we explore thedual modality of language--both visual and textual--within an autoregressiveframework, pre-trained on both document images and texts. Our method employs amultimodal training strategy, utilizing visual data through next patchprediction with a regression head and/or textual data through next tokenprediction with a classification head. We focus on understanding theinteraction between these two modalities and their combined impact on modelperformance. Our extensive evaluation across a wide range of benchmarks showsthat incorporating both visual and textual data significantly improves theperformance of pixel-based language models. Remarkably, we find that aunidirectional pixel-based model trained solely on visual data can achievecomparable results to state-of-the-art bidirectional models on several languageunderstanding tasks. This work uncovers the untapped potential of integratingvisual and textual modalities for more effective language modeling. We releaseour code, data, and model checkpoints at\url{https://github.com/ernie-research/pixelgpt}.</description><author>Yekun Chai, Qingyi Liu, Jingwu Xiao, Shuohuan Wang, Yu Sun, Hua Wu</author><pubDate>Thu, 03 Oct 2024 17:46:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10710v3</guid></item><item><title>Large Language Models as Markov Chains</title><link>http://arxiv.org/abs/2410.02724v1</link><description>Large language models (LLMs) have proven to be remarkably efficient, bothacross a wide range of natural language processing tasks and well beyond them.However, a comprehensive theoretical analysis of the origins of theirimpressive performance remains elusive. In this paper, we approach thischallenging task by drawing an equivalence between generic autoregressivelanguage models with vocabulary of size $T$ and context window of size $K$ andMarkov chains defined on a finite state space of size $\mathcal{O}(T^K)$. Wederive several surprising findings related to the existence of a stationarydistribution of Markov chains that capture the inference power of LLMs, theirspeed of convergence to it, and the influence of the temperature on the latter.We then prove pre-training and in-context generalization bounds and show howthe drawn equivalence allows us to enrich their interpretation. Finally, weillustrate our theoretical guarantees with experiments on several recent LLMsto highlight how they capture the behavior observed in practice.</description><author>Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boull√©, Ievgen Redko</author><pubDate>Thu, 03 Oct 2024 17:45:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02724v1</guid></item><item><title>E(n) Equivariant Topological Neural Networks</title><link>http://arxiv.org/abs/2405.15429v4</link><description>Graph neural networks excel at modeling pairwise interactions, but theycannot flexibly accommodate higher-order interactions and features. Topologicaldeep learning (TDL) has emerged recently as a promising tool for addressingthis issue. TDL enables the principled modeling of arbitrary multi-way,hierarchical higher-order interactions by operating on combinatorialtopological spaces, such as simplicial or cell complexes, instead of graphs.However, little is known about how to leverage geometric features such aspositions and velocities for TDL. This paper introduces E(n)-EquivariantTopological Neural Networks (ETNNs), which are E(n)-equivariant message-passingnetworks operating on combinatorial complexes, formal objects unifying graphs,hypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometricnode features while respecting rotation, reflection, and translationequivariance. Moreover, ETNNs are natively ready for settings withheterogeneous interactions. We provide a theoretical analysis to show theimproved expressiveness of ETNNs over architectures for geometric graphs. Wealso show how E(n)-equivariant variants of TDL models can be directly derivedfrom our framework. The broad applicability of ETNNs is demonstrated throughtwo tasks of vastly different scales: i) molecular property prediction on theQM9 benchmark and ii) land-use regression for hyper-local estimation of airpollution with multi-resolution irregular geospatial data. The results indicatethat ETNNs are an effective tool for learning from diverse types of richlystructured data, as they match or surpass SotA equivariant TDL models with asignificantly smaller computational burden, thus highlighting the benefits of aprincipled geometric inductive bias.</description><author>Claudio Battiloro, Ege Karaismailoƒülu, Mauricio Tec, George Dasoulas, Michelle Audirac, Francesca Dominici</author><pubDate>Thu, 03 Oct 2024 17:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15429v4</guid></item><item><title>Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization</title><link>http://arxiv.org/abs/2410.02721v1</link><description>Large Language Models (LLMs) are pre-trained on large-scale corpora and excelin numerous general natural language processing (NLP) tasks, such as questionanswering (QA). Despite their advanced language capabilities, when it comes todomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,knowledge cut-offs, and lack of knowledge attributions. Additionally, finetuning LLMs' intrinsic knowledge to highly specific domains is an expensive andtime consuming process. The retrieval-augmented generation (RAG) process hasrecently emerged as a method capable of optimization of LLM responses, byreferencing them to a predetermined ontology. It was shown that using aKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking intoaccount relevant sub-graphs that preserve the information in a structuredmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLMframework, that integrates RAG with KG and a vector store (VS) that storefactual domain specific information. Importantly, to avoid hallucinations inthe KG, we build these highly domain-specific KGs and VSs without the use ofLLMs, but via NLP, data mining, and nonnegative tensor factorization withautomatic model selection. Pairing our RAG with a domain-specific: (i) KG(containing structured information), and (ii) VS (containing unstructuredinformation) enables the development of domain-specific chat-bots thatattribute the source of information, mitigate hallucinations, lessen the needfor fine-tuning, and excel in highly domain-specific question answering tasks.We pair SMART-SLIC with chain-of-thought prompting agents. The framework isdesigned to be generalizable to adapt to any specific or specialized domain. Inthis paper, we demonstrate the question answering capabilities of our frameworkon a corpus of scientific publications on malware analysis and anomalydetection.</description><author>Ryan C. Barron, Ves Grantcharov, Selma Wanna, Maksim E. Eren, Manish Bhattarai, Nicholas Solovyev, George Tompkins, Charles Nicholas, Kim √ò. Rasmussen, Cynthia Matuszek, Boian S. Alexandrov</author><pubDate>Thu, 03 Oct 2024 17:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02721v1</guid></item><item><title>Curvature Diversity-Driven Deformation and Domain Alignment for Point Cloud</title><link>http://arxiv.org/abs/2410.02720v1</link><description>Unsupervised Domain Adaptation (UDA) is crucial for reducing the need forextensive manual data annotation when training deep networks on point clouddata. A significant challenge of UDA lies in effectively bridging the domaingap. To tackle this challenge, we propose \textbf{C}urvature\textbf{D}iversity-Driven \textbf{N}uclear-Norm Wasserstein \textbf{D}omainAlignment (CDND). Our approach first introduces a \textit{\textbf{Curv}atureDiversity-driven Deformation \textbf{Rec}onstruction (CurvRec)} task, whicheffectively mitigates the gap between the source and target domains by enablingthe model to extract salient features from semantically rich regions of a givenpoint cloud. We then propose \textit{\textbf{D}eformation-based\textbf{N}uclear-norm \textbf{W}asserstein \textbf{D}iscrepancy (D-NWD)}, whichapplies the Nuclear-norm Wasserstein Discrepancy to both \textit{deformed andoriginal} data samples to align the source and target domains. Furthermore, wecontribute a theoretical justification for the effectiveness of D-NWD indistribution alignment and demonstrate that it is \textit{generic} enough to beapplied to \textbf{any} deformations. To validate our method, we conductextensive experiments on two public domain adaptation datasets for point cloudclassification and segmentation tasks. Empirical experiment results show thatour CDND achieves state-of-the-art performance by a noticeable margin overexisting approaches.</description><author>Mengxi Wu, Hao Huang, Yi Fang, Mohammad Rostami</author><pubDate>Thu, 03 Oct 2024 17:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02720v1</guid></item><item><title>UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling for Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2410.02719v1</link><description>We present UncertaintyRAG, a novel approach for long-contextRetrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio(SNR)-based span uncertainty to estimate similarity between text chunks. Thisspan uncertainty enhances model calibration, improving robustness andmitigating semantic inconsistencies introduced by random chunking. Leveragingthis insight, we propose an efficient unsupervised learning technique to trainthe retrieval model, alongside an effective data sampling and scaling strategy.UncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achievingstate-of-the-art results while using only 4% of the training data compared toother advanced open-source retrieval models under distribution shift settings.Our method demonstrates strong calibration through span uncertainty, leading toimproved generalization and robustness in long-context RAG tasks. Additionally,UncertaintyRAG provides a lightweight retrieval model that can be integratedinto any large language model with varying context window lengths, without theneed for fine-tuning, showcasing the flexibility of our approach.</description><author>Zixuan Li, Jing Xiong, Fanghua Ye, Chuanyang Zheng, Xun Wu, Jianqiao Lu, Zhongwei Wan, Xiaodan Liang, Chengming Li, Zhenan Sun, Lingpeng Kong, Ngai Wong</author><pubDate>Thu, 03 Oct 2024 17:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02719v1</guid></item><item><title>SynthFormer: Equivariant Pharmacophore-based Generation of Molecules for Ligand-Based Drug Design</title><link>http://arxiv.org/abs/2410.02718v1</link><description>Drug discovery is a complex and resource-intensive process, with significanttime and cost investments required to bring new medicines to patients. Recentadvancements in generative machine learning (ML) methods offer promisingavenues to accelerate early-stage drug discovery by efficiently exploringchemical space. This paper addresses the gap between in silico generativeapproaches and practical in vitro methodologies, highlighting the need fortheir integration to optimize molecule discovery. We introduce SynthFormer, anovel ML model that utilizes a 3D equivariant encoder for pharmacophores togenerate fully synthesizable molecules, constructed as synthetic trees. Unlikeprevious methods, SynthFormer incorporates 3D information and providessynthetic paths, enhancing its ability to produce molecules with good dockingscores across various proteins. Our contributions include a new methodology forefficient chemical space exploration using 3D information, a novel architecturecalled Synthformer for translating 3D pharmacophore representations intomolecules, and a meaningful embedding space that organizes reagents for drugdiscovery optimization. Synthformer generates molecules that dock well andenables effective late-stage optimization restricted by synthesis paths.</description><author>Zygimantas Jocys, Henriette M. G. Willems, Katayoun Farrahi</author><pubDate>Thu, 03 Oct 2024 17:38:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02718v1</guid></item><item><title>Measurements with Noise: Bayesian Optimization for Co-optimizing Noise and Property Discovery in Automated Experiments</title><link>http://arxiv.org/abs/2410.02717v1</link><description>We have developed a Bayesian optimization (BO) workflow that integratesintra-step noise optimization into automated experimental cycles. TraditionalBO approaches in automated experiments focus on optimizing experimentaltrajectories but often overlook the impact of measurement noise on data qualityand cost. Our proposed framework simultaneously optimizes both the targetproperty and the associated measurement noise by introducing time as anadditional input parameter, thereby balancing the signal-to-noise ratio andexperimental duration. Two approaches are explored: a reward-driven noiseoptimization and a double-optimization acquisition function, both enhancing theefficiency of automated workflows by considering noise and cost within theoptimization process. We validate our method through simulations and real-worldexperiments using Piezoresponse Force Microscopy (PFM), demonstrating thesuccessful optimization of measurement duration and property exploration. Ourapproach offers a scalable solution for optimizing multiple variables inautomated experimental workflows, improving data quality, and reducing resourceexpenditure in materials science and beyond.</description><author>Boris N. Slautin, Yu Liu, Jan Dec, Vladimir V. Shvartsman, Doru C. Lupascu, Maxim Ziatdinov, Sergei V. Kalinin</author><pubDate>Thu, 03 Oct 2024 17:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02717v1</guid></item><item><title>Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits</title><link>http://arxiv.org/abs/2402.05689v3</link><description>We consider the infinite-horizon, average-reward restless bandit problem indiscrete time. We propose a new class of policies that are designed to drive aprogressively larger subset of arms toward the optimal distribution. We showthat our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimalitygap for an $N$-armed problem, assuming only a unichain and aperiodicityassumption. Our approach departs from most existing work that focuses on indexor priority policies, which rely on the Global Attractor Property (GAP) toguarantee convergence to the optimum, or a recently developed simulation-basedpolicy, which requires a Synchronization Assumption (SA).</description><author>Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang</author><pubDate>Thu, 03 Oct 2024 17:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05689v3</guid></item><item><title>AlzhiNet: Traversing from 2DCNN to 3DCNN, Towards Early Detection and Diagnosis of Alzheimer's Disease</title><link>http://arxiv.org/abs/2410.02714v1</link><description>Alzheimer's disease (AD) is a progressive neurodegenerative disorder withincreasing prevalence among the aging population, necessitating early andaccurate diagnosis for effective disease management. In this study, we presenta novel hybrid deep learning framework that integrates both 2D ConvolutionalNeural Networks (2D-CNN) and 3D Convolutional Neural Networks (3D-CNN), alongwith a custom loss function and volumetric data augmentation, to enhancefeature extraction and improve classification performance in AD diagnosis.According to extensive experiments, AlzhiNet outperforms standalone 2D and 3Dmodels, highlighting the importance of combining these complementaryrepresentations of data. The depth and quality of 3D volumes derived from theaugmented 2D slices also significantly influence the model's performance. Theresults indicate that carefully selecting weighting factors in hybridpredictions is imperative for achieving optimal results. Our framework has beenvalidated on the Magnetic Resonance Imaging (MRI) from Kaggle and MIRIADdatasets, obtaining accuracies of 98.9% and 99.99%, respectively, with an AUCof 100%. Furthermore, AlzhiNet was studied under a variety of perturbationscenarios on the Alzheimer's Kaggle dataset, including Gaussian noise,brightness, contrast, salt and pepper noise, color jitter, and occlusion. Theresults obtained show that AlzhiNet is more robust to perturbations thanResNet-18, making it an excellent choice for real-world applications. Thisapproach represents a promising advancement in the early diagnosis andtreatment planning for Alzheimer's disease.</description><author>Romoke Grace Akindele, Samuel Adebayo, Paul Shekonya Kanda, Ming Yu</author><pubDate>Thu, 03 Oct 2024 17:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02714v1</guid></item><item><title>Video Instruction Tuning With Synthetic Data</title><link>http://arxiv.org/abs/2410.02713v1</link><description>The development of video large multimodal models (LMMs) has been hindered bythe difficulty of curating large amounts of high-quality raw data from the web.To address this, we propose an alternative approach by creating a high-qualitysynthetic dataset specifically for video instruction-following, namelyLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,open-ended question-answering (QA), and multiple-choice QA. By training on thisdataset, in combination with existing visual instruction tuning data, weintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate thatLLaVA-Video achieves strong performance across various video benchmarks,highlighting the effectiveness of our dataset. We plan to release the dataset,its generation pipeline, and the model checkpoints.</description><author>Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, Chunyuan Li</author><pubDate>Thu, 03 Oct 2024 17:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02713v1</guid></item><item><title>LLaVA-Critic: Learning to Evaluate Multimodal Models</title><link>http://arxiv.org/abs/2410.02712v1</link><description>We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)designed as a generalist evaluator to assess performance across a wide range ofmultimodal tasks. LLaVA-Critic is trained using a high-quality criticinstruction-following dataset that incorporates diverse evaluation criteria andscenarios. Our experiments demonstrate the model's effectiveness in two keyareas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluationscores, performing on par with or surpassing GPT models on multiple evaluationbenchmarks; and (2) Preference Learning, where it generates reward signals forpreference learning, enhancing model alignment capabilities. This workunderscores the potential of open-source LMMs in self-critique and evaluation,setting the stage for future research into scalable, superhuman alignmentfeedback mechanisms for LMMs.</description><author>Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, Chunyuan Li</author><pubDate>Thu, 03 Oct 2024 17:36:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02712v1</guid></item><item><title>NETS: A Non-Equilibrium Transport Sampler</title><link>http://arxiv.org/abs/2410.02711v1</link><description>We propose an algorithm, termed the Non-Equilibrium Transport Sampler (NETS),to sample from unnormalized probability distributions. NETS can be viewed as avariant of annealed importance sampling (AIS) based on Jarzynski's equality, inwhich the stochastic differential equation used to perform the non-equilibriumsampling is augmented with an additional learned drift term that lowers theimpact of the unbiasing weights used in AIS. We show that this drift is theminimizer of a variety of objective functions, which can all be estimated in anunbiased fashion without backpropagating through solutions of the stochasticdifferential equations governing the sampling. We also prove that some theseobjectives control the Kullback-Leibler divergence of the estimateddistribution from its target. NETS is shown to be unbiased and, in addition,has a tunable diffusion coefficient which can be adjusted post-training tomaximize the effective sample size. We demonstrate the efficacy of the methodon standard benchmarks, high-dimensional Gaussian mixture distributions, and amodel from statistical lattice field theory, for which it surpasses theperformances of related work and existing baselines.</description><author>Michael S. Albergo, Eric Vanden-Eijnden</author><pubDate>Thu, 03 Oct 2024 17:35:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02711v1</guid></item><item><title>SteerDiff: Steering towards Safe Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2410.02710v1</link><description>Text-to-image (T2I) diffusion models have drawn attention for their abilityto generate high-quality images with precise text alignment. However, thesemodels can also be misused to produce inappropriate content. Existing safetymeasures, which typically rely on text classifiers or ControlNet-likeapproaches, are often insufficient. Traditional text classifiers rely onlarge-scale labeled datasets and can be easily bypassed by rephrasing. Asdiffusion models continue to scale, fine-tuning these safeguards becomesincreasingly challenging and lacks flexibility. Recent red-teaming attackresearches further underscore the need for a new paradigm to prevent thegeneration of inappropriate content. In this paper, we introduce SteerDiff, alightweight adaptor module designed to act as an intermediary between userinput and the diffusion model, ensuring that generated images adhere to ethicaland safety standards with little to no impact on usability. SteerDiffidentifies and manipulates inappropriate concepts within the text embeddingspace to guide the model away from harmful outputs. We conduct extensiveexperiments across various concept unlearning tasks to evaluate theeffectiveness of our approach. Furthermore, we benchmark SteerDiff againstmultiple red-teaming strategies to assess its robustness. Finally, we explorethe potential of SteerDiff for concept forgetting tasks, demonstrating itsversatility in text-conditioned image generation.</description><author>Hongxiang Zhang, Yifeng He, Hao Chen</author><pubDate>Thu, 03 Oct 2024 17:34:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02710v1</guid></item><item><title>LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations</title><link>http://arxiv.org/abs/2410.02707v1</link><description>Large language models (LLMs) often produce errors, including factualinaccuracies, biases, and reasoning failures, collectively referred to as"hallucinations". Recent studies have demonstrated that LLMs' internal statesencode information regarding the truthfulness of their outputs, and that thisinformation can be utilized to detect errors. In this work, we show that theinternal representations of LLMs encode much more information abouttruthfulness than previously recognized. We first discover that thetruthfulness information is concentrated in specific tokens, and leveragingthis property significantly enhances error detection performance. Yet, we showthat such error detectors fail to generalize across datasets, implying that --contrary to prior claims -- truthfulness encoding is not universal but rathermultifaceted. Next, we show that internal representations can also be used forpredicting the types of errors the model is likely to make, facilitating thedevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancybetween LLMs' internal encoding and external behavior: they may encode thecorrect answer, yet consistently generate an incorrect one. Taken together,these insights deepen our understanding of LLM errors from the model's internalperspective, which can guide future research on enhancing error analysis andmitigation.</description><author>Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov</author><pubDate>Thu, 03 Oct 2024 17:31:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02707v1</guid></item><item><title>ControlAR: Controllable Image Generation with Autoregressive Models</title><link>http://arxiv.org/abs/2410.02705v1</link><description>Autoregressive (AR) models have reformulated image generation as next-tokenprediction, demonstrating remarkable potential and emerging as strongcompetitors to diffusion models. However, control-to-image generation, akin toControlNet, remains largely unexplored within AR models. Although a naturalapproach, inspired by advancements in Large Language Models, is to tokenizecontrol images into tokens and prefill them into the autoregressive modelbefore decoding image tokens, it still falls short in generation qualitycompared to ControlNet and suffers from inefficiency. To this end, we introduceControlAR, an efficient and effective framework for integrating spatialcontrols into autoregressive image generation models. Firstly, we explorecontrol encoding for AR models and propose a lightweight control encoder totransform spatial inputs (e.g., canny edges or depth maps) into control tokens.Then ControlAR exploits the conditional decoding method to generate the nextimage token conditioned on the per-token fusion between control and imagetokens, similar to positional encodings. Compared to prefilling tokens, usingconditional decoding significantly strengthens the control capability of ARmodels but also maintains the model's efficiency. Furthermore, the proposedControlAR surprisingly empowers AR models with arbitrary-resolution imagegeneration via conditional decoding and specific controls. Extensiveexperiments can demonstrate the controllability of the proposed ControlAR forthe autoregressive control-to-image generation across diverse inputs, includingedges, depths, and segmentation masks. Furthermore, both quantitative andqualitative results indicate that ControlAR surpasses previous state-of-the-artcontrollable diffusion models, e.g., ControlNet++. Code, models, and demo willsoon be available at https://github.com/hustvl/ControlAR.</description><author>Zongming Li, Tianheng Cheng, Shoufa Chen, Peize Sun, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang</author><pubDate>Thu, 03 Oct 2024 17:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02705v1</guid></item><item><title>Selective Attention Improves Transformer</title><link>http://arxiv.org/abs/2410.02703v1</link><description>Unneeded elements in the attention's context degrade performance. Weintroduce Selective Attention, a simple parameter-free change to the standardattention mechanism which reduces attention to unneeded elements. Selectiveattention improves language modeling performance in a variety of model sizesand context lengths. For example, a range of transformers trained with thelanguage modeling objective on C4 with selective attention perform equivalentlyto standard transformers with ~2X more heads and parameters in their attentionmodules. Selective attention also allows decreasing the size of the attention'scontext buffer, leading to meaningful reductions in the memory and computerequirements during inference. For example, transformers with 100M parameterstrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and47X less memory for their attention module, respectively, when equipped withselective attention, as those without selective attention, with the samevalidation perplexity.</description><author>Yaniv Leviathan, Matan Kalman, Yossi Matias</author><pubDate>Thu, 03 Oct 2024 17:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02703v1</guid></item><item><title>Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?</title><link>http://arxiv.org/abs/2312.12683v2</link><description>The vast majority of today's large language models (LLMs) areEnglish-centric, having been pretrained predominantly on English text. Yet, inorder to meet user expectations, models need to be able to respondappropriately in multiple languages once deployed in downstream applications.This requires strong cross-lingual transfer abilities. In this work, weinvestigate the minimal amount of multilinguality required during finetuning toelicit cross-lingual generalisation in English-centric LLMs. In experimentsacross four LLMs, we find that multilingual instruction tuning with as few astwo to three languages is both necessary and sufficient to elicit effectivecross-lingual generalisation, with the limiting factor being the degree towhich a target language is seen during pretraining. Evaluations on fivedifferent tasks further reveal that multilingual instruction tuning is mostbeneficial for generative tasks that assume input/output language agreement,such as in chat settings, while being of less importance for highly structuredclassification-style tasks. Our code and data is available athttps://github.com/ZurichNLP/multilingual-instruction-tuning.</description><author>Tannon Kew, Florian Schottmann, Rico Sennrich</author><pubDate>Thu, 03 Oct 2024 17:27:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12683v2</guid></item><item><title>Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps</title><link>http://arxiv.org/abs/2407.07071v2</link><description>When asked to summarize articles or answer questions given a passage, largelanguage models (LLMs) can hallucinate details and respond with unsubstantiatedanswers that are inaccurate with respect to the input context. This paperdescribes a simple approach for detecting such contextual hallucinations. Wehypothesize that contextual hallucinations are related to the extent to whichan LLM attends to information in the provided context versus its owngenerations. Based on this intuition, we propose a simple hallucinationdetection model whose input features are given by the ratio of attentionweights on the context versus newly generated tokens (for each attention head).We find that a linear classifier based on these lookback ratio features is aseffective as a richer detector that utilizes the entire hidden states of an LLMor a text-based entailment model. The lookback ratio-based detector -- LookbackLens -- is found to transfer across tasks and even models, allowing a detectorthat is trained on a 7B model to be applied (without retraining) to a larger13B model. We further apply this detector to mitigate contextualhallucinations, and find that a simple classifier-guided decoding approach isable to reduce the amount of hallucination, for example by 9.6% in the XSumsummarization task.</description><author>Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass</author><pubDate>Thu, 03 Oct 2024 17:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07071v2</guid></item><item><title>The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis</title><link>http://arxiv.org/abs/2311.00237v3</link><description>Understanding in-context learning (ICL) capability that enables largelanguage models (LLMs) to excel in proficiency through demonstration examplesis of utmost importance. This importance stems not only from the betterutilization of this capability across various tasks, but also from theproactive identification and mitigation of potential risks, including concernsregarding truthfulness, bias, and toxicity, that may arise alongside thecapability. In this paper, we present a thorough survey on the interpretationand analysis of in-context learning. First, we provide a concise introductionto the background and definition of in-context learning. Then, we give anoverview of advancements from two perspectives: 1) a theoretical perspective,emphasizing studies on mechanistic interpretability and delving into themathematical foundations behind ICL; and 2) an empirical perspective,concerning studies that empirically analyze factors associated with ICL. Weconclude by highlighting the challenges encountered and suggesting potentialavenues for future research. We believe that our work establishes the basis forfurther exploration into the interpretation of in-context learning.Additionally, we have created a repository containing the resources referencedin our survey.</description><author>Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He</author><pubDate>Thu, 03 Oct 2024 17:25:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00237v3</guid></item><item><title>VideoPhy: Evaluating Physical Commonsense for Video Generation</title><link>http://arxiv.org/abs/2406.03520v2</link><description>Recent advances in internet-scale video data pretraining have led to thedevelopment of text-to-video generative models that can create high-qualityvideos across a broad range of visual concepts, synthesize realistic motionsand render complex objects. Hence, these generative models have the potentialto become general-purpose simulators of the physical world. However, it isunclear how far we are from this goal with the existing text-to-videogenerative models. To this end, we present VideoPhy, a benchmark designed toassess whether the generated videos follow physical commonsense for real-worldactivities (e.g. marbles will roll down when placed on a slanted surface).Specifically, we curate diverse prompts that involve interactions betweenvarious material types in the physical world (e.g., solid-solid, solid-fluid,fluid-fluid). We then generate videos conditioned on these captions fromdiverse state-of-the-art text-to-video generative models, including open models(e.g., CogVideoX) and closed models (e.g., Lumiere, Dream Machine). Our humanevaluation reveals that the existing models severely lack the ability togenerate videos adhering to the given text prompts, while also lack physicalcommonsense. Specifically, the best performing model, CogVideoX-5B, generatesvideos that adhere to the caption and physical laws for 39.6% of the instances.VideoPhy thus highlights that the video generative models are far fromaccurately simulating the physical world. Finally, we propose anauto-evaluator, VideoCon-Physics, to assess the performance reliably for thenewly released models.</description><author>Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, Aditya Grover</author><pubDate>Thu, 03 Oct 2024 17:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03520v2</guid></item><item><title>Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups</title><link>http://arxiv.org/abs/2410.02698v1</link><description>The quest for robust and generalizable machine learning models has drivenrecent interest in exploiting symmetries through equivariant neural networks.In the context of PDE solvers, recent works have shown that Lie pointsymmetries can be a useful inductive bias for Physics-Informed Neural Networks(PINNs) through data and loss augmentation. Despite this, directly enforcingequivariance within the model architecture for these problems remains elusive.This is because many PDEs admit non-compact symmetry groups, oftentimes notstudied beyond their infinitesimal generators, making them incompatible withmost existing equivariant architectures. In this work, we propose Lie aLgebrACanonicalization (LieLAC), a novel approach that exploits only the action ofinfinitesimal generators of the symmetry group, circumventing the need forknowledge of the full group structure. To achieve this, we address existingtheoretical issues in the canonicalization literature, establishing connectionswith frame averaging in the case of continuous non-compact groups. Operatingwithin the framework of canonicalization, LieLAC can easily be integrated withunconstrained pre-trained models, transforming inputs to a canonical formbefore feeding them into the existing model, effectively aligning the input formodel inference according to allowed symmetries. LieLAC utilizes standard Liegroup descent schemes, achieving equivariance in pre-trained models. Finally,we showcase LieLAC's efficacy on tasks of invariant image classification andLie point symmetry equivariant neural PDE solvers using pre-trained models.</description><author>Zakhar Shumaylov, Peter Zaika, James Rowbottom, Ferdia Sherry, Melanie Weber, Carola-Bibiane Sch√∂nlieb</author><pubDate>Thu, 03 Oct 2024 17:21:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02698v1</guid></item><item><title>HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly</title><link>http://arxiv.org/abs/2410.02694v1</link><description>There have been many benchmarks for evaluating long-context language models(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translateto the diverse downstream applications of LCLMs, and the inconsistency furthercomplicates model comparison. We investigate the underlying reasons behindcurrent practices and find that existing benchmarks often provide noisy signalsdue to low coverage of applications, insufficient lengths, unreliable metrics,and incompatibility with base models. In this work, we present HELMET (How toEvaluate Long-context Models Effectively and Thoroughly), a comprehensivebenchmark encompassing seven diverse, application-centric categories. We alsoaddress many issues in previous benchmarks by adding controllable lengths up to128k tokens, model-based evaluation for reliable metrics, and few-shotprompting for robustly evaluating base models. Consequently, we demonstratethat HELMET offers more reliable and consistent rankings of frontier LCLMs.Through a comprehensive study of 51 LCLMs, we find that (1) synthetic taskslike NIAH are not good predictors of downstream performance; (2) the diversecategories in HELMET exhibit distinct trends and low correlation with eachother; and (3) while most LCLMs achieve perfect NIAH scores, open-source modelssignificantly lag behind closed ones when the task requires full-contextreasoning or following complex instructions -- the gap widens with increasedlengths. Finally, we recommend using our RAG tasks for fast model development,as they are easy to run and more predictive of other downstream performance;ultimately, we advocate for a holistic evaluation across diverse tasks.</description><author>Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izasak, Moshe Wasserblat, Danqi Chen</author><pubDate>Thu, 03 Oct 2024 17:20:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02694v1</guid></item><item><title>Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals</title><link>http://arxiv.org/abs/2402.17917v2</link><description>In Intensive Care Units (ICU), the abundance of multivariate time seriespresents an opportunity for machine learning (ML) to enhance patientphenotyping. In contrast to previous research focused on electronic healthrecords (EHR), here we propose an ML approach for phenotyping using routinelycollected physiological time series data. Our new algorithm integrates LongShort-Term Memory (LSTM) networks with collaborative filtering concepts toidentify common physiological states across patients. Tested on real-world ICUclinical data for intracranial hypertension (IH) detection in patients withbrain injury, our method achieved an area under the curve (AUC) of 0.889 andaverage precision (AP) of 0.725. Moreover, our algorithm outperformsautoencoders in learning more structured latent representations of thephysiological signals. These findings highlight the promise of our methodologyfor patient phenotyping, leveraging routinely collected multivariate timeseries to improve clinical care practices.</description><author>Hollan Haule, Ian Piper, Patricia Jones, Tsz-Yan Milly Lo, Javier Escudero</author><pubDate>Thu, 03 Oct 2024 17:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17917v2</guid></item><item><title>Discovering Clues of Spoofed LM Watermarks</title><link>http://arxiv.org/abs/2410.02693v1</link><description>LLM watermarks stand out as a promising way to attribute ownership ofLLM-generated text. One threat to watermark credibility comes from spoofingattacks, where an unauthorized third party forges the watermark, enabling it tofalsely attribute arbitrary texts to a particular LLM. While recent works havedemonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,they lack deeper qualitative analysis of the texts produced by spoofingmethods. In this work, we for the first time reveal that there are observabledifferences between genuine and spoofed watermark texts. Namely, we show thatregardless of their underlying approach, all current spoofing methodsconsistently leave observable artifacts in spoofed texts, indicative ofwatermark forgery. We build upon these findings to propose rigorous statisticaltests that reliably reveal the presence of such artifacts, effectivelydiscovering that a watermark was spoofed. Our experimental evaluation showshigh test power across all current spoofing methods, providing insights intotheir fundamental limitations, and suggesting a way to mitigate this threat.</description><author>Thibaud Gloaguen, Nikola Jovanoviƒá, Robin Staab, Martin Vechev</author><pubDate>Thu, 03 Oct 2024 17:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02693v1</guid></item><item><title>On the Proper Treatment of Tokenization in Psycholinguistics</title><link>http://arxiv.org/abs/2410.02691v1</link><description>Language models are widely used in computational psycholinguistics to testtheories that relate the negative log probability (the surprisal) of a regionof interest (a substring of characters) under a language model to its cognitivecost experienced by readers, as operationalized, for example, by gaze durationon the region. However, the application of modern language models topsycholinguistic studies is complicated by the practice of using tokenizationas an intermediate step in training a model. Doing so results in a languagemodel over token strings rather than one over character strings. Vexingly,regions of interest are generally misaligned with these token strings. Thepaper argues that token-level language models should be (approximately)marginalized into character-level language models before they are used inpsycholinguistic studies to compute the surprisal of a region of interest;then, the marginalized character-level language model can be used to computethe surprisal of an arbitrary character substring, which we term a focal area,that the experimenter may wish to use as a predictor. Our proposal ofmarginalizing a token-level model into a character-level one solves thismisalignment issue independently of the tokenization scheme. Empirically, wediscover various focal areas whose surprisal is a better psychometric predictorthan the surprisal of the region of interest itself.</description><author>Mario Giulianelli, Luca Malagutti, Juan Luis Gastaldi, Brian DuSell, Tim Vieira, Ryan Cotterell</author><pubDate>Thu, 03 Oct 2024 17:18:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02691v1</guid></item><item><title>User-centric Immersive Communications in 6G: A Data-oriented Approach via Digital Twin</title><link>http://arxiv.org/abs/2410.02688v1</link><description>In this article, we present a novel user-centric service provision forimmersive communications (IC) in 6G to deal with the uncertainty of individualuser behaviors while satisfying unique requirements on the quality ofmulti-sensory experience. To this end, we propose a data-oriented approach fornetwork resource management, featuring personalized data management that cansupport network modeling tailored to different user demands. Our approachleverages the digital twin (DT) technique as a key enabler. Particularly, a DTis established for each user, and the data attributes in the DT are customizedbased on the characteristics of the user. The DT functions, corresponding tovarious data operations, are customized in the development, evaluation, andupdate of network models to meet unique user demands. A trace-driven case studydemonstrates the effectiveness of our approach in achieving user-centric IC andthe significance of personalized data management in 6G.</description><author>Conghao Zhou, Shisheng Hu, Jie Gao, Xinyu Huang, Weihua Zhuang, Xuemin Shen</author><pubDate>Thu, 03 Oct 2024 17:15:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02688v1</guid></item><item><title>Enhanced Automated Code Vulnerability Repair using Large Language Models</title><link>http://arxiv.org/abs/2401.03741v2</link><description>This research addresses the complex challenge of automated repair of codevulnerabilities, vital for enhancing digital security in an increasinglytechnology-driven world. The study introduces a novel and efficient format forthe representation of code modification, using advanced Large Language Models(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasetsfeaturing C code vulnerabilities, significantly improve the accuracy andadaptability of automated code repair techniques. A key finding is the enhancedrepair accuracy of these models when compared to previous methods such asVulRepair, which underscores their practical utility and efficiency. Theresearch also offers a critical assessment of current evaluation metrics, suchas perfect predictions, and their limitations in reflecting the truecapabilities of automated repair models in real-world scenarios. Followingthis, it underscores the importance of using test datasets devoid of trainsamples, emphasizing the need for dataset integrity to enhance theeffectiveness of LLMs in code repair tasks. The significance of this work isits contribution to digital security, setting new standards for automated codevulnerability repair and paving the way for future advancements in the fieldsof cybersecurity and artificial intelligence. The study does not only highlightthe potential of LLMs in enhancing code security but also fosters furtherexploration and research in these crucial areas.</description><author>David de-Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, Jose-Javier Martinez-Herraiz</author><pubDate>Thu, 03 Oct 2024 17:15:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03741v2</guid></item><item><title>Generalizing Medical Image Representations via Quaternion Wavelet Networks</title><link>http://arxiv.org/abs/2310.10224v4</link><description>Neural network generalizability is becoming a broad research field due to theincreasing availability of datasets from different sources and for varioustasks. This issue is even wider when processing medical data, where a lack ofmethodological standards causes large variations being provided by differentimaging centers or acquired with various devices and cofactors. To overcomethese limitations, we introduce a novel, generalizable, data- and task-agnosticframework able to extract salient features from medical images. The proposedquaternion wavelet network (QUAVE) can be easily integrated with anypre-existing medical image analysis or synthesis task, and it can be involvedwith real, quaternion, or hypercomplex-valued models, generalizing theiradoption to single-channel data. QUAVE first extracts different sub-bandsthrough the quaternion wavelet transform, resulting in bothlow-frequency/approximation bands and high-frequency/fine-grained features.Then, it weighs the most representative set of sub-bands to be involved asinput to any other neural model for image processing, replacing standard datasamples. We conduct an extensive experimental evaluation comprising differentdatasets, diverse image analysis, and synthesis tasks including reconstruction,segmentation, and modality translation. We also evaluate QUAVE in combinationwith both real and quaternion-valued models. Results demonstrate theeffectiveness and the generalizability of the proposed framework that improvesnetwork performance while being flexible to be adopted in manifold scenariosand robust to domain shifts. The full code is available at:https://github.com/ispamm/QWT.</description><author>Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello</author><pubDate>Thu, 03 Oct 2024 17:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10224v4</guid></item><item><title>On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization</title><link>http://arxiv.org/abs/2409.03650v2</link><description>Reinforcement Learning from Human Feedback (RLHF) is an effective approachfor aligning language models to human preferences. Central to RLHF is learninga reward function for scoring human preferences. Two main approaches forlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as inRLHF, and 2) using an implicit reward learned from preference data throughmethods such as Direct Preference Optimization (DPO). Prior work has shown thatthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM inthe limit. DPORM's effectiveness directly implies the optimality of the learnedpolicy, and also has practical implication for LLM alignment methods includingiterative DPO. However, it is unclear how well DPORM empirically matches theperformance of EXRM. This work studies the accuracy at distinguishing preferredand rejected answers for both DPORM and EXRM. Our findings indicate that eventhough DPORM fits the training dataset comparably, it generalizes lesseffectively than EXRM, especially when the validation datasets containdistribution shifts. Across five out-of-distribution settings, DPORM has a meandrop in accuracy of 3% and a maximum drop of 7%. These findings highlight thatDPORM has limited generalization ability and substantiates the integration ofan explicit reward model in iterative DPO approaches.</description><author>Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang</author><pubDate>Thu, 03 Oct 2024 17:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03650v2</guid></item><item><title>HiddenGuard: Fine-Grained Safe Generation with Specialized Representation Router</title><link>http://arxiv.org/abs/2410.02684v1</link><description>As Large Language Models (LLMs) grow increasingly powerful, ensuring theirsafety and alignment with human values remains a critical challenge. Ideally,LLMs should provide informative responses while avoiding the disclosure ofharmful or sensitive information. However, current alignment approaches, whichrely heavily on refusal strategies, such as training models to completelyreject harmful prompts or applying coarse filters are limited by their binarynature. These methods either fully deny access to information or grant itwithout sufficient nuance, leading to overly cautious responses or failures todetect subtle harmful content. For example, LLMs may refuse to provide basic,public information about medication due to misuse concerns. Moreover, theserefusal-based methods struggle to handle mixed-content scenarios and lack theability to adapt to context-dependent sensitivities, which can result inover-censorship of benign content. To overcome these challenges, we introduceHiddenGuard, a novel framework for fine-grained, safe generation in LLMs.HiddenGuard incorporates Prism (rePresentation Router for In-StreamModeration), which operates alongside the LLM to enable real-time, token-leveldetection and redaction of harmful content by leveraging intermediate hiddenstates. This fine-grained approach allows for more nuanced, context-awaremoderation, enabling the model to generate informative responses whileselectively redacting or replacing sensitive information, rather than outrightrefusal. We also contribute a comprehensive dataset with token-levelfine-grained annotations of potentially harmful information across diversecontexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1score for detecting and redacting harmful content while preserving the overallutility and informativeness of the model's responses.</description><author>Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, Xueqi Cheng</author><pubDate>Thu, 03 Oct 2024 17:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02684v1</guid></item><item><title>Evaluating Perceptual Distance Models by Fitting Binomial Distributions to Two-Alternative Forced Choice Data</title><link>http://arxiv.org/abs/2403.10390v2</link><description>The two-alternative forced choice (2AFC) experimental method is popular inthe visual perception literature, where practitioners aim to understand howhuman observers perceive distances within triplets made of a reference imageand two distorted versions. In the past, this had been conducted in controlledenvironments, with triplets sharing images, so it was possible to rank theperceived quality. This ranking would then be used to evaluate perceptualdistance models against the experimental data. Recently, crowd-sourcedperceptual datasets have emerged, with no images shared between triplets,making ranking infeasible. Evaluating perceptual distance models using thisdata reduces the judgements on a triplet to a binary decision, namely, whetherthe distance model agrees with the human decision - which is suboptimal andprone to misleading conclusions. Instead, we statistically model the underlyingdecision-making process during 2AFC experiments using a binomial distribution.Having enough empirical data, we estimate a smooth and consistent distributionof the judgements on the reference-distorted distance plane, according to eachdistance model. By applying maximum likelihood, we estimate the parameter ofthe local binomial distribution, and a global measurement of the expectedlog-likelihood of the measured responses. We calculate meaningful andwell-founded metrics for the distance model, beyond the mere predictionaccuracy as percentage agreement, even with variable numbers of judgements pertriplet -- key advantages over both classical and neural network methods.</description><author>Alexander Hepburn, Raul Santos-Rodriguez, Javier Portilla</author><pubDate>Thu, 03 Oct 2024 17:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10390v2</guid></item><item><title>Jailbreaking LLMs with Arabic Transliteration and Arabizi</title><link>http://arxiv.org/abs/2406.18725v2</link><description>This study identifies the potential vulnerabilities of Large Language Models(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language andits various forms. While most research has concentrated on English-based promptmanipulation, our investigation broadens the scope to investigate the Arabiclanguage. We initially tested the AdvBench benchmark in Standardized Arabic,finding that even with prompt manipulation techniques like prefix injection, itwas insufficient to provoke LLMs into generating unsafe content. However, whenusing Arabic transliteration and chatspeak (or arabizi), we found that unsafecontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3Sonnet. Our findings suggest that using Arabic and its various forms couldexpose information that might remain hidden, potentially increasing the risk ofjailbreak attacks. We hypothesize that this exposure could be due to themodel's learned connection to specific words, highlighting the need for morecomprehensive safety training across all language forms.</description><author>Mansour Al Ghanim, Saleh Almohaimeed, Mengxin Zheng, Yan Solihin, Qian Lou</author><pubDate>Thu, 03 Oct 2024 17:10:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18725v2</guid></item><item><title>DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life</title><link>http://arxiv.org/abs/2410.02683v1</link><description>As we increasingly seek guidance from LLMs for decision-making in daily life,many of these decisions are not clear-cut and depend significantly on thepersonal values and ethical standards of the users. We present DailyDilemmas, adataset of 1,360 moral dilemmas encountered in everyday life. Each dilemmaincludes two possible actions and with each action, the affected parties andhuman values invoked. Based on these dilemmas, we consolidated a set of humanvalues across everyday topics e.g., interpersonal relationships, workplace, andenvironmental issues. We evaluated LLMs on these dilemmas to determine whataction they will take and the values represented by these actions. Then, weanalyzed these values through the lens of five popular theories inspired bysociology, psychology and philosophy. These theories are: World Value Survey,Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, andPlutchik Wheel of Emotion. We find that LLMs are most aligned with theself-expression over survival values in terms of World Value Survey, care overloyalty in Moral Foundation Theory. Interestingly, we find large preferencesdifferences in models for some core values such as truthfulness e.g.,Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends toselect it by 9.4%. We also study the recent guidance released by OpenAI(ModelSpec), and Anthropic (Constitutional AI) to understand how their releasedprinciples reflect their actual value prioritization when facing nuanced moralreasoning in daily-life settings. We find that end users cannot effectivelysteer such prioritization using system prompts.</description><author>Yu Ying Chiu, Liwei Jiang, Yejin Choi</author><pubDate>Thu, 03 Oct 2024 17:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02683v1</guid></item><item><title>Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models</title><link>http://arxiv.org/abs/2410.02681v1</link><description>Confidence calibration is critical for the safe deployment of machinelearning models in the real world. However, such issue in vision-languagemodels like CLIP, particularly after fine-tuning, has not been fully addressed.In this work, we demonstrate that existing prompt tuning methods usually leadto a trade-off of calibration between base and new classes: the cross-entropyloss in CoOp causes overconfidence in new classes by increasing textual labeldivergence, whereas the regularization of KgCoOp maintains the confidence levelbut results in underconfidence in base classes due to the improved accuracy.Inspired by the observations, we introduce Dynamic Outlier Regularization (DOR)to ensure the confidence calibration on both base and new classes afterfine-tuning. In particular, we propose to minimize the feature deviation ofnovel textual labels (instead of base classes) sampled from a large vocabulary.In effect, DOR prevents the increase in textual divergence for new labels whileeasing restrictions on base classes. Extensive experiments demonstrate that DORcan enhance the calibration performance of current fine-tuning methods on baseand new classes.</description><author>Shuoyuan Wang, Yixuan Li, Hongxin Wei</author><pubDate>Thu, 03 Oct 2024 17:06:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02681v1</guid></item><item><title>Highly Adaptive Ridge</title><link>http://arxiv.org/abs/2410.02680v1</link><description>In this paper we propose the Highly Adaptive Ridge (HAR): a regression methodthat achieves a $n^{-1/3}$ dimension-free L2 convergence rate in the class ofright-continuous functions with square-integrable sectional derivatives. Thisis a large nonparametric function class that is particularly appropriate fortabular data. HAR is exactly kernel ridge regression with a specificdata-adaptive kernel based on a saturated zero-order tensor-product splinebasis expansion. We use simulation and real data to confirm our theory. Wedemonstrate empirical performance better than state-of-the-art algorithms forsmall datasets in particular.</description><author>Alejandro Schuler, Alexander Hagemeister, Mark van der Laan</author><pubDate>Thu, 03 Oct 2024 17:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02680v1</guid></item><item><title>Fair Allocation in Dynamic Mechanism Design</title><link>http://arxiv.org/abs/2406.00147v3</link><description>We consider a dynamic mechanism design problem where an auctioneer sells anindivisible good to groups of buyers in every round, for a total of $T$ rounds.The auctioneer aims to maximize their discounted overall revenue while adheringto a fairness constraint that guarantees a minimum average allocation for eachgroup. We begin by studying the static case ($T=1$) and establish that theoptimal mechanism involves two types of subsidization: one that increases theoverall probability of allocation to all buyers, and another that favors thegroups which otherwise have a lower probability of winning the item. We thenextend our results to the dynamic case by characterizing a set of recursivefunctions that determine the optimal allocation and payments in each round.Notably, our results establish that in the dynamic case, the seller, on the onehand, commits to a participation bonus to incentivize truth-telling, and on theother hand, charges an entry fee for every round. Moreover, the optimalallocation once more involves subsidization, which its extent depends on thedifference in future utilities for both the seller and buyers when allocatingthe item to one group versus the others. Finally, we present an approximationscheme to solve the recursive equations and determine an approximately optimaland fair allocation efficiently.</description><author>Alireza Fallah, Michael I. Jordan, Annie Ulichney</author><pubDate>Thu, 03 Oct 2024 17:05:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00147v3</guid></item><item><title>Signature Isolation Forest</title><link>http://arxiv.org/abs/2403.04405v2</link><description>Functional Isolation Forest (FIF) is a recent state-of-the-art AnomalyDetection (AD) algorithm designed for functional data. It relies on a treepartition procedure where an abnormality score is computed by projecting eachcurve observation on a drawn dictionary through a linear inner product. Suchlinear inner product and the dictionary are a priori choices that highlyinfluence the algorithm's performances and might lead to unreliable results,particularly with complex datasets. This work addresses these challenges byintroducing \textit{Signature Isolation Forest}, a novel AD algorithm classleveraging the rough path theory's signature transform. Our objective is toremove the constraints imposed by FIF through the proposition of two algorithmswhich specifically target the linearity of the FIF inner product and the choiceof the dictionary. We provide several numerical experiments, including areal-world applications benchmark showing the relevance of our methods.</description><author>Marta Campi, Guillaume Staerman, Gareth W. Peters, Tomoko Matsui</author><pubDate>Thu, 03 Oct 2024 17:05:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04405v2</guid></item><item><title>StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for Children's Story-Based Learning</title><link>http://arxiv.org/abs/2311.09756v2</link><description>Interactive story reading is a common parent-child activity, where parentsexpect to teach both language skills and real-world knowledge beyond the story.While increasing storytelling and reading systems have been developed for thisactivity, they often fail to infuse real-world knowledge into the conversation.This limitation can be attributed to the existing question-answering (QA)datasets used for children's education, upon which the systems are built,failing to capture the nuances of how education experts think when conductinginteractive story reading activities. To bridge this gap, we design anannotation framework, empowered by existing knowledge graph to capture experts'annotations and thinking process, and leverage this framework to constructStorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs withreal-world knowledge. We conduct automated and human expert evaluations acrossvarious QA pair generation settings to demonstrate that our StorySparkQA caneffectively support models in generating QA pairs that target real-worldknowledge beyond story content. StorySparkQA is available athttps://huggingface.co/datasets/NEU-HAI/StorySparkQA.</description><author>Jiaju Chen, Yuxuan Lu, Shao Zhang, Bingsheng Yao, Yuanzhe Dong, Ying Xu, Yunyao Li, Qianwen Wang, Dakuo Wang, Yuling Sun</author><pubDate>Thu, 03 Oct 2024 17:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09756v2</guid></item><item><title>Distilling an End-to-End Voice Assistant Without Instruction Training Data</title><link>http://arxiv.org/abs/2410.02678v1</link><description>Voice assistants, such as Siri and Google Assistant, typically model audioand text separately, resulting in lost speech information and increasedcomplexity. Recent efforts to address this with end-to-end Speech LargeLanguage Models (LLMs) trained with supervised finetuning (SFT) have led to models ``forgetting" capabilities from text-only LLMs. Our workproposes an alternative paradigm for training Speech LLMs without instructiondata, using the response of a text-only LLM to transcripts as self-supervision.Importantly, this process can be performed without annotated responses. We showthat our Distilled Voice Assistant (DiVA) generalizes to Spoken QuestionAnswering, Classification, and Translation. Furthermore, we show that DiVAbetter meets user preferences, achieving a 72\% win rate compared withstate-of-the-art models like Qwen 2 Audio, despite using $&gt;$100x less trainingcompute.</description><author>William Held, Ella Li, Michael Ryan, Weiyan Shi, Yanzhe Zhang, Diyi Yang</author><pubDate>Thu, 03 Oct 2024 17:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02678v1</guid></item><item><title>CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring the (Lack of) Cultural Knowledge of LLMs</title><link>http://arxiv.org/abs/2410.02677v1</link><description>To make large language models (LLMs) more helpful across diverse cultures, itis essential to have effective cultural knowledge benchmarks to measure andtrack our progress. Effective benchmarks need to be robust, diverse, andchallenging. We introduce CulturalBench: a set of 1,227 human-written andhuman-verified questions for effectively assessing LLMs' cultural knowledge,covering 45 global regions including the underrepresented ones like Bangladesh,Zimbabwe, and Peru. Questions - each verified by five independent annotators -span 17 diverse topics ranging from food preferences to greeting etiquettes. Weevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard whichshare the same questions but asked differently. We find that LLMs are sensitiveto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared tohuman performance (92.6% accuracy), CulturalBench-Hard is more challenging forfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and theworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle withtricky questions that have multiple correct answers (e.g., What utensils do theChinese usually use?), revealing a tendency to converge to a single answer. Ourresults also indicate that OpenAI GPT-4o substantially outperform otherproprietary and open source models in questions related to all but one region(Oceania). Nonetheless, all models consistently underperform on questionsrelated to South America and the Middle East.</description><author>Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi</author><pubDate>Thu, 03 Oct 2024 17:04:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02677v1</guid></item><item><title>FAN: Fourier Analysis Networks</title><link>http://arxiv.org/abs/2410.02675v1</link><description>Despite the remarkable success achieved by neural networks, particularlythose represented by MLP and Transformer, we reveal that they exhibit potentialflaws in the modeling and reasoning of periodicity, i.e., they tend to memorizethe periodic data rather than genuinely understanding the underlying principlesof periodicity. However, periodicity is a crucial trait in various forms ofreasoning and generalization, underpinning predictability across natural andengineered systems through recurring patterns in observations. In this paper,we propose FAN, a novel network architecture based on Fourier Analysis, whichempowers the ability to efficiently model and reason about periodic phenomena.By introducing Fourier Series, the periodicity is naturally integrated into thestructure and computational processes of the neural network, thus achieving amore accurate expression and prediction of periodic patterns. As a promisingsubstitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP invarious models with fewer parameters and FLOPs. Through extensive experiments,we demonstrate the effectiveness of FAN in modeling and reasoning aboutperiodic functions, and the superiority and generalizability of FAN across arange of real-world tasks, including symbolic formula representation, timeseries forecasting, and language modeling.</description><author>Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jing Su, Jun Zhang, Jingjing Xu</author><pubDate>Thu, 03 Oct 2024 17:02:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02675v1</guid></item><item><title>DyGPrompt: Learning Feature and Time Prompts on Dynamic Graphs</title><link>http://arxiv.org/abs/2405.13937v6</link><description>Dynamic graphs capture evolving interactions between entities, such as insocial networks, online learning platforms, and crowdsourcing projects. Fordynamic graph modeling, dynamic graph neural networks (DGNNs) have emerged as amainstream technique. However, they are generally pre-trained on the linkprediction task, leaving a significant gap from the objectives of downstreamtasks such as node classification. To bridge the gap, prompt-based learning hasgained traction on graphs, but most existing efforts focus on static graphs,neglecting the evolution of dynamic graphs. In this paper, we proposeDYGPROMPT, a novel pre-training and prompt learning framework for dynamic graphmodeling. First, we design dual prompts to address the gap in both taskobjectives and temporal variations across pre-training and downstream tasks.Second, we recognize that node and time features mutually characterize eachother, and propose dual condition-nets to model the evolving node-time patternsin downstream tasks. Finally, we thoroughly evaluate and analyze DYGPROMPTthrough extensive experiments on four public datasets.</description><author>Xingtong Yu, Zhenghao Liu, Yuan Fang, Xinming Zhang</author><pubDate>Thu, 03 Oct 2024 16:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13937v6</guid></item><item><title>Examining Language Modeling Assumptions Using an Annotated Literary Dialect Corpus</title><link>http://arxiv.org/abs/2410.02674v1</link><description>We present a dataset of 19th century American literary orthovariant tokenswith a novel layer of human-annotated dialect group tags designed to serve asthe basis for computational experiments exploring literarily meaningfulorthographic variation. We perform an initial broad set of experiments overthis dataset using both token (BERT) and character (CANINE)-level contextuallanguage models. We find indications that the "dialect effect" produced byintentional orthographic variation employs multiple linguistic channels, andthat these channels are able to be surfaced to varied degrees given particularlanguage modelling assumptions. Specifically, we find evidence showing thatchoice of tokenization scheme meaningfully impact the type of orthographicinformation a model is able to surface.</description><author>Craig Messner, Tom Lippincott</author><pubDate>Thu, 03 Oct 2024 16:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02674v1</guid></item><item><title>Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM Reliance</title><link>http://arxiv.org/abs/2407.07950v2</link><description>The ability to communicate uncertainty, risk, and limitation is crucial forthe safety of large language models. However, current evaluations of theseabilities rely on simple calibration, asking whether the language generated bythe model matches appropriate probabilities. Instead, evaluation of this aspectof LLM communication should focus on the behaviors of their humaninterlocutors: how much do they rely on what the LLM says? Here we introduce aninteraction-centered evaluation framework called Rel-A.I. (pronounced "rely"})that measures whether humans rely on LLM generations. We use this framework tostudy how reliance is affected by contextual features of the interaction (e.g,the knowledge domain that is being discussed), or the use of greetingscommunicating warmth or competence (e.g., "I'm happy to help!"). We find thatcontextual characteristics significantly affect human reliance behavior. Forexample, people rely 10% more on LMs when responding to questions involvingcalculations and rely 30% more on LMs that are perceived as more competent. Ourresults show that calibration and language quality alone are insufficient inevaluating the risks of human-LM interactions, and illustrate the need toconsider features of the interactional context.</description><author>Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Nouha Dziri, Dan Jurafsky, Maarten Sap</author><pubDate>Thu, 03 Oct 2024 16:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07950v2</guid></item><item><title>Unsupervised Point Cloud Completion through Unbalanced Optimal Transport</title><link>http://arxiv.org/abs/2410.02671v1</link><description>Unpaired point cloud completion explores methods for learning a completionmap from unpaired incomplete and complete point cloud data. In this paper, wepropose a novel approach for unpaired point cloud completion using theunbalanced optimal transport map, called Unbalanced Optimal Transport Map forUnpaired Point Cloud Completion (UOT-UPC). We demonstrate that the unpairedpoint cloud completion can be naturally interpreted as the Optimal Transport(OT) problem and introduce the Unbalanced Optimal Transport (UOT) approach toaddress the class imbalance problem, which is prevalent in unpaired point cloudcompletion datasets. Moreover, we analyze the appropriate cost function forunpaired completion tasks. This analysis shows that the InfoCD cost function isparticularly well-suited for this task. Our model is the first attempt toleverage UOT for unpaired point cloud completion, achieving competitive orsuperior results on both single-category and multi-category datasets. Inparticular, our model is especially effective in scenarios with classimbalance, where the proportions of categories are different between theincomplete and complete point cloud datasets.</description><author>Taekyung Lee, Jaemoo Choi, Jaewoong Choi</author><pubDate>Thu, 03 Oct 2024 16:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02671v1</guid></item><item><title>GUD: Generation with Unified Diffusion</title><link>http://arxiv.org/abs/2410.02667v1</link><description>Diffusion generative models transform noise into data by inverting a processthat progressively adds noise to data samples. Inspired by concepts from therenormalization group in physics, which analyzes systems across differentscales, we revisit diffusion models by exploring three key design aspects: 1)the choice of representation in which the diffusion process operates (e.g.pixel-, PCA-, Fourier-, or wavelet-basis), 2) the prior distribution that datais transformed into during diffusion (e.g. Gaussian with covariance $\Sigma$),and 3) the scheduling of noise levels applied separately to different parts ofthe data, captured by a component-wise noise schedule. Incorporating theflexibility in these choices, we develop a unified framework for diffusiongenerative models with greatly enhanced design freedom. In particular, weintroduce soft-conditioning models that smoothly interpolate between standarddiffusion models and autoregressive models (in any basis), conceptuallybridging these two approaches. Our framework opens up a wide design space whichmay lead to more efficient training and data generation, and paves the way tonovel architectures integrating different generative approaches and generationtasks.</description><author>Mathis Gerdes, Max Welling, Miranda C. N. Cheng</author><pubDate>Thu, 03 Oct 2024 16:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02667v1</guid></item><item><title>AlphaIntegrator: Transformer Action Search for Symbolic Integration Proofs</title><link>http://arxiv.org/abs/2410.02666v1</link><description>We present the first correct-by-construction learning-based system forstep-by-step mathematical integration. The key idea is to learn a policy,represented by a GPT transformer model, which guides the search for the rightmathematical integration rule, to be carried out by a symbolic solver.Concretely, we introduce a symbolic engine with axiomatically correct actionson mathematical expressions, as well as the first dataset for step-by-stepintegration. Our GPT-style transformer model, trained on this synthetic data,demonstrates strong generalization by surpassing its own data generator inaccuracy and efficiency, using 50% fewer search steps. Our experimental resultswith SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMson a set of question-answer pairs is insufficient for solving this mathematicaltask. This motivates the importance of discovering creative methods forcombining LLMs with symbolic reasoning engines, of which our work is aninstance.</description><author>Mert √únsal, Timon Gehr, Martin Vechev</author><pubDate>Thu, 03 Oct 2024 16:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02666v1</guid></item><item><title>Grounded Answers for Multi-agent Decision-making Problem through Generative World Model</title><link>http://arxiv.org/abs/2410.02664v1</link><description>Recent progress in generative models has stimulated significant innovationsin many fields, such as image generation and chatbots. Despite their success,these models often produce sketchy and misleading solutions for complexmulti-agent decision-making problems because they miss the trial-and-errorexperience and reasoning as humans. To address this limitation, we explore aparadigm that integrates a language-guided simulator into the multi-agentreinforcement learning pipeline to enhance the generated answer. The simulatoris a world model that separately learns dynamics and reward, where the dynamicsmodel comprises an image tokenizer as well as a causal transformer to generateinteraction transitions autoregressively, and the reward model is abidirectional transformer learned by maximizing the likelihood of trajectoriesin the expert demonstrations under language guidance. Given an image of thecurrent state and the task description, we use the world model to train thejoint policy and produce the image sequence as the answer by running theconverged policy on the dynamics model. The empirical results demonstrate thatthis framework can improve the answers for multi-agent decision-making problemsby showing superior performance on the training and unseen tasks of theStarCraft Multi-Agent Challenge benchmark. In particular, it can generateconsistent interaction sequences and explainable reward functions atinteraction states, opening the path for training generative models of thefuture.</description><author>Zeyang Liu, Xinrui Yang, Shiguang Sun, Long Qian, Lipeng Wan, Xingyu Chen, Xuguang Lan</author><pubDate>Thu, 03 Oct 2024 16:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02664v1</guid></item><item><title>On Leakage of Code Generation Evaluation Datasets</title><link>http://arxiv.org/abs/2407.07565v3</link><description>In this paper, we consider contamination by code generation test sets, inparticular in their use in modern large language models. We discuss threepossible sources of such contamination and show findings supporting each ofthem: (i) direct data leakage, (ii) indirect data leakage through the use ofsynthetic data and (iii) overfitting to evaluation sets during model selection.To address this, we release Less Basic Python Problems (LBPP): anuncontaminated new benchmark of 161 prompts with their associated Pythonsolutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp .</description><author>Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, Matthias Gall√©</author><pubDate>Thu, 03 Oct 2024 16:48:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07565v3</guid></item><item><title>How to Train Long-Context Language Models (Effectively)</title><link>http://arxiv.org/abs/2410.02660v1</link><description>We study continued training and supervised fine-tuning (SFT) of a languagemodel (LM) to make effective use of long-context information. We firstestablish a reliable evaluation protocol to guide model development -- Insteadof perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad setof long-context tasks, and we evaluate models after SFT with instruction dataas this better reveals long-context abilities. Supported by our robustevaluations, we run thorough experiments to decide the data mix for continuedpre-training, the instruction tuning dataset, and many other design choices. Wefind that (1) code repositories and books are excellent sources of long data,but it is crucial to combine them with high-quality short data; (2) trainingwith a sequence length beyond the evaluation length boosts long-contextperformance; (3) for SFT, using only short instruction datasets yields strongperformance on long-context tasks. Our final model, ProLong-8B, which isinitialized from Llama-3 and trained on 40B tokens, demonstratesstate-of-the-art long-context performance among similarly sized models at alength of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority oflong-context tasks despite having seen only 5% as many tokens duringlong-context training. Additionally, ProLong can effectively process up to 512Ktokens, one of the longest context windows of publicly available LMs.</description><author>Tianyu Gao, Alexander Wettig, Howard Yen, Danqi Chen</author><pubDate>Thu, 03 Oct 2024 16:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02660v1</guid></item><item><title>Does Refusal Training in LLMs Generalize to the Past Tense?</title><link>http://arxiv.org/abs/2407.11969v3</link><description>Refusal training is widely used to prevent LLMs from generating harmful,undesirable, or illegal outputs. We reveal a curious generalization gap in thecurrent refusal training approaches: simply reformulating a harmful request inthe past tense (e.g., "How to make a Molotov cocktail?" to "How did people makea Molotov cocktail?") is often sufficient to jailbreak many state-of-the-artLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,o1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. Forexample, the success rate of this simple attack on GPT-4o increases from 1%using direct requests to 88% using 20 past tense reformulation attempts onharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.Interestingly, we also find that reformulations in the future tense are lesseffective, suggesting that refusal guardrails tend to consider past historicalquestions more benign than hypothetical future questions. Moreover, ourexperiments on fine-tuning GPT-3.5 Turbo show that defending against pastreformulations is feasible when past tense examples are explicitly included inthe fine-tuning data. Overall, our findings highlight that the widely usedalignment techniques -- such as SFT, RLHF, and adversarial training -- employedto align the studied models can be brittle and do not always generalize asintended. We provide code and jailbreak artifacts athttps://github.com/tml-epfl/llm-past-tense.</description><author>Maksym Andriushchenko, Nicolas Flammarion</author><pubDate>Thu, 03 Oct 2024 16:46:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11969v3</guid></item><item><title>Hate Personified: Investigating the role of LLMs in content moderation</title><link>http://arxiv.org/abs/2410.02657v1</link><description>For subjective tasks such as hate detection, where people perceive hatedifferently, the Large Language Model's (LLM) ability to represent diversegroups is unclear. By including additional context in prompts, wecomprehensively analyze LLM's sensitivity to geographical priming, personaattributes, and numerical information to assess how well the needs of variousgroups are reflected. Our findings on two LLMs, five languages, and sixdatasets reveal that mimicking persona-based attributes leads to annotationvariability. Meanwhile, incorporating geographical signals leads to betterregional alignment. We also find that the LLMs are sensitive to numericalanchors, indicating the ability to leverage community-based flagging effortsand exposure to adversaries. Our work provides preliminary guidelines andhighlights the nuances of applying LLMs in culturally sensitive cases.</description><author>Sarah Masud, Sahajpreet Singh, Viktor Hangya, Alexander Fraser, Tanmoy Chakraborty</author><pubDate>Thu, 03 Oct 2024 16:43:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02657v1</guid></item><item><title>Scalable Simulation-free Entropic Unbalanced Optimal Transport</title><link>http://arxiv.org/abs/2410.02656v1</link><description>The Optimal Transport (OT) problem investigates a transport map that connectstwo distributions while minimizing a given cost function. Finding such atransport map has diverse applications in machine learning, such as generativemodeling and image-to-image translation. In this paper, we introduce a scalableand simulation-free approach for solving the Entropic Unbalanced OptimalTransport (EUOT) problem. We derive the dynamical form of this EUOT problem,which is a generalization of the Schr\"odinger bridges (SB) problem. Based onthis, we derive dual formulation and optimality conditions of the EUOT problemfrom the stochastic optimal control interpretation. By leveraging theseproperties, we propose a simulation-free algorithm to solve EUOT, calledSimulation-free EUOT (SF-EUOT). While existing SB models require expensivesimulation costs during training and evaluation, our model achievessimulation-free training and one-step generation by utilizing the reciprocalproperty. Our model demonstrates significantly improved scalability ingenerative modeling and image-to-image translation tasks compared to previousSB methods.</description><author>Jaemoo Choi, Jaewoong Choi</author><pubDate>Thu, 03 Oct 2024 16:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02656v1</guid></item><item><title>Scalable Label Distribution Learning for Multi-Label Classification</title><link>http://arxiv.org/abs/2311.16556v2</link><description>Multi-label classification (MLC) refers to the problem of tagging a giveninstance with a set of relevant labels. Most existing MLC methods are based onthe assumption that the correlation of two labels in each label pair issymmetric, which is violated in many real-world scenarios. Moreover, mostexisting methods design learning processes associated with the number oflabels, which makes their computational complexity a bottleneck when scaling upto large-scale output space. To tackle these issues, we propose a novel methodnamed Scalable Label Distribution Learning (SLDL) for multi-labelclassification which can describe different labels as distributions in a latentspace, where the label correlation is asymmetric and the dimension isindependent of the number of labels. Specifically, SLDL first converts labelsinto continuous distributions within a low-dimensional latent space andleverages the asymmetric metric to establish the correlation between differentlabels. Then, it learns the mapping from the feature space to the latent space,resulting in the computational complexity is no longer related to the number oflabels. Finally, SLDL leverages a nearest-neighbor-based strategy to decode thelatent representations and obtain the final predictions. Extensive experimentsillustrate that SLDL achieves very competitive classification performances withlittle computational consumption.</description><author>Xingyu Zhao, Yuexuan An, Lei Qi, Xin Geng</author><pubDate>Thu, 03 Oct 2024 16:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16556v2</guid></item><item><title>Deconstructing Recurrence, Attention, and Gating: Investigating the transferability of Transformers and Gated Recurrent Neural Networks in forecasting of dynamical systems</title><link>http://arxiv.org/abs/2410.02654v1</link><description>Machine learning architectures, including transformers and recurrent neuralnetworks (RNNs) have revolutionized forecasting in applications ranging fromtext processing to extreme weather. Notably, advanced network architectures,tuned for applications such as natural language processing, are transferable toother tasks such as spatiotemporal forecasting tasks. However, there is ascarcity of ablation studies to illustrate the key components that enable thisforecasting accuracy. The absence of such studies, although explainable due tothe associated computational cost, intensifies the belief that these modelsought to be considered as black boxes. In this work, we decompose the keyarchitectural components of the most powerful neural architectures, namelygating and recurrence in RNNs, and attention mechanisms in transformers. Then,we synthesize and build novel hybrid architectures from the standard blocks,performing ablation studies to identify which mechanisms are effective for eachtask. The importance of considering these components as hyper-parameters thatcan augment the standard architectures is exhibited on various forecastingdatasets, from the spatiotemporal chaotic dynamics of the multiscale Lorenz 96system, the Kuramoto-Sivashinsky equation, as well as standard real worldtime-series benchmarks. A key finding is that neural gating and attentionimproves the performance of all standard RNNs in most tasks, while the additionof a notion of recurrence in transformers is detrimental. Furthermore, ourstudy reveals that a novel, sparsely used, architecture which integratesRecurrent Highway Networks with neural gating and attention mechanisms, emergesas the best performing architecture in high-dimensional spatiotemporalforecasting of dynamical systems.</description><author>Hunter Heidenreich, Pantelis R. Vlachas, etros Koumoutsakos</author><pubDate>Thu, 03 Oct 2024 16:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02654v1</guid></item><item><title>Immunization against harmful fine-tuning attacks</title><link>http://arxiv.org/abs/2402.16382v2</link><description>Large Language Models (LLMs) are often trained with safety guards intended toprevent harmful text generation. However, such safety training can be removedby fine-tuning the LLM on harmful datasets. While this emerging threat (harmfulfine-tuning attacks) has been characterized by previous work, there is littleunderstanding of how we should proceed in constructing and validating defensesagainst these attacks especially in the case where defenders would not havecontrol of the fine-tuning process. We introduce a formal framework based onthe training budget of an attacker which we call "Immunization" conditions.Using a formal characterisation of the harmful fine-tuning problem, we providea thorough description of what a successful defense must comprise of andestablish a set of guidelines on how rigorous defense research that gives usconfidence should proceed.</description><author>Domenic Rosati, Jan Wehner, Kai Williams, ≈Åukasz Bartoszcze, Jan Batzner, Hassan Sajjad, Frank Rudzicz</author><pubDate>Thu, 03 Oct 2024 16:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16382v2</guid></item><item><title>Measuring and Improving Persuasiveness of Generative Models</title><link>http://arxiv.org/abs/2410.02653v1</link><description>LLMs are increasingly being used in workflows involving generating content tobe consumed by humans (e.g., marketing) and also in directly interacting withhumans (e.g., through chatbots). The development of such systems that arecapable of generating verifiably persuasive messages presents bothopportunities and challenges for society. On the one hand, such systems couldpositively impact domains like advertising and social good, such as addressingdrug addiction, and on the other, they could be misused for spreadingmisinformation and shaping political opinions. To channel LLMs' impact onsociety, we need to develop systems to measure and benchmark theirpersuasiveness. With this motivation, we introduce PersuasionBench andPersuasionArena, the first large-scale benchmark and arena containing a batteryof tasks to measure the persuasion ability of generative models automatically.We investigate to what extent LLMs know and leverage linguistic patterns thatcan help them generate more persuasive language. Our findings indicate that thepersuasiveness of LLMs correlates positively with model size, but smallermodels can also be made to have a higher persuasiveness than much largermodels. Notably, targeted training using synthetic and natural datasetssignificantly enhances smaller models' persuasive capabilities, challengingscale-dependent assumptions. Our findings carry key implications for both modeldevelopers and policymakers. For instance, while the EU AI Act and California'sSB-1047 aim to regulate AI models based on the number of floating pointoperations, we demonstrate that simple metrics like this alone fail to capturethe full scope of AI's societal impact. We invite the community to explore andcontribute to PersuasionArena and PersuasionBench, available athttps://bit.ly/measure-persuasion, to advance our understanding of AI-drivenpersuasion and its societal implications.</description><author>Somesh Singh, Yaman K Singla, Harini SI, Balaji Krishnamurthy</author><pubDate>Thu, 03 Oct 2024 16:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02653v1</guid></item><item><title>CAX: Cellular Automata Accelerated in JAX</title><link>http://arxiv.org/abs/2410.02651v1</link><description>Cellular automata have become a cornerstone for investigating emergence andself-organization across diverse scientific disciplines, spanning neuroscience,artificial life, and theoretical physics. However, the absence of ahardware-accelerated cellular automata library limits the exploration of newresearch directions, hinders collaboration, and impedes reproducibility. Inthis work, we introduce CAX (Cellular Automata Accelerated in JAX), ahigh-performance and flexible open-source library designed to acceleratecellular automata research. CAX offers cutting-edge performance and a modulardesign through a user-friendly interface, and can support both discrete andcontinuous cellular automata with any number of dimensions. We demonstrateCAX's performance and flexibility through a wide range of benchmarks andapplications. From classic models like elementary cellular automata andConway's Game of Life to advanced applications such as growing neural cellularautomata and self-classifying MNIST digits, CAX speeds up simulations up to2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerateresearch by presenting a collection of three novel cellular automataexperiments, each implemented in just a few lines of code thanks to thelibrary's modular architecture. Notably, we show that a simple one-dimensionalcellular automaton can outperform GPT-4 on the 1D-ARC challenge.</description><author>Maxence Faldor, Antoine Cully</author><pubDate>Thu, 03 Oct 2024 16:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02651v1</guid></item><item><title>Undesirable Memorization in Large Language Models: A Survey</title><link>http://arxiv.org/abs/2410.02650v1</link><description>While recent research increasingly showcases the remarkable capabilities ofLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.Among these challenges, the issue of memorization stands out, posingsignificant ethical and legal risks. In this paper, we presents aSystematization of Knowledge (SoK) on the topic of memorization in LLMs.Memorization is the effect that a model tends to store and reproduce phrases orpassages from the training data and has been shown to be the fundamental issueto various privacy and security attacks against LLMs. We begin by providing an overview of the literature on the memorization,exploring it across five key dimensions: intentionality, degree,retrievability, abstraction, and transparency. Next, we discuss the metrics andmethods used to measure memorization, followed by an analysis of the factorsthat contribute to memorization phenomenon. We then examine how memorizationmanifests itself in specific model architectures and explore strategies formitigating these effects. We conclude our overview by identifying potentialresearch topics for the near future: to develop methods for balancingperformance and privacy in LLMs, and the analysis of memorization in specificcontexts, including conversational agents, retrieval-augmented generation,multilingual language models, and diffusion language models.</description><author>Ali Satvaty, Suzan Verberne, Fatih Turkmen</author><pubDate>Thu, 03 Oct 2024 16:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02650v1</guid></item><item><title>Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection</title><link>http://arxiv.org/abs/2410.02647v1</link><description>Immunogenicity prediction is a central topic in reverse vaccinology forfinding candidate vaccines that can trigger protective immune responses.Existing approaches typically rely on highly compressed features and simplemodel architectures, leading to limited prediction accuracy and poorgeneralizability. To address these challenges, we introduce ProVaccine, a noveldeep learning solution with a dual attention mechanism that integratespre-trained latent vector representations of protein sequences and structures.We also compile the most comprehensive immunogenicity dataset to date,encompassing over 9,500 antigen sequences, structures, and immunogenicitylabels from bacteria, viruses, and tumors. Extensive experiments demonstratethat ProVaccine outperforms existing methods across a wide range of evaluationmetrics. Furthermore, we establish a post-hoc validation protocol to assess thepractical significance of deep learning models in tackling vaccine designchallenges. Our work provides an effective tool for vaccine design and setsvaluable benchmarks for future research.</description><author>Song Li, Yang Tan, Song Ke, Liang Hong, Bingxin Zhou</author><pubDate>Thu, 03 Oct 2024 16:33:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02647v1</guid></item><item><title>Foundations of Large Language Model Compression -- Part 1: Weight Quantization</title><link>http://arxiv.org/abs/2409.02026v2</link><description>In recent years, compression of large language models (LLMs) has emerged asan important problem to enable language model deployment onresource-constrained devices, reduce computational costs, and mitigate theenvironmental footprint of large-scale AI infrastructure. In this paper, we laydown the foundation for LLM quantization from a convex optimization perspectiveand propose a quantization technique that builds on this foundation for optimumquantization outcomes. Our quantization framework, CVXQ, scales to modelscontaining hundreds of billions of weight parameters and provides users withthe flexibility to compress models to any specified model size, post-training.A reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.</description><author>Sean I. Young</author><pubDate>Thu, 03 Oct 2024 16:31:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02026v2</guid></item></channel></rss>