<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 15 Dec 2024 13:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Illusion3D: 3D Multiview Illusion with 2D Diffusion Priors</title><link>http://arxiv.org/abs/2412.09625v1</link><description>Automatically generating multiview illusions is a compelling challenge, wherea single piece of visual content offers distinct interpretations from differentviewing perspectives. Traditional methods, such as shadow art and wire art,create interesting 3D illusions but are limited to simple visual outputs (i.e.,figure-ground or line drawing), restricting their artistic expressiveness andpractical versatility. Recent diffusion-based illusion generation methods cangenerate more intricate designs but are confined to 2D images. In this work, wepresent a simple yet effective approach for creating 3D multiview illusionsbased on user-provided text prompts or images. Our method leverages apre-trained text-to-image diffusion model to optimize the textures and geometryof neural 3D representations through differentiable rendering. When viewed frommultiple angles, this produces different interpretations. We develop severaltechniques to improve the quality of the generated 3D multiview illusions. Wedemonstrate the effectiveness of our approach through extensive experiments andshowcase illusion generation with diverse 3D forms.</description><author>Yue Feng, Vaibhav Sanjay, Spencer Lutz, Badour AlBahar, Songwei Ge, Jia-Bin Huang</author><pubDate>Thu, 12 Dec 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09625v1</guid></item><item><title>Doe-1: Closed-Loop Autonomous Driving with Large World Model</title><link>http://arxiv.org/abs/2412.09627v1</link><description>End-to-end autonomous driving has received increasing attention due to itspotential to learn from large amounts of data. However, most existing methodsare still open-loop and suffer from weak scalability, lack of high-orderinteractions, and inefficient decision-making. In this paper, we explore aclosed-loop framework for autonomous driving and propose a large Driving wOrldmodEl (Doe-1) for unified perception, prediction, and planning. We formulateautonomous driving as a next-token generation problem and use multi-modaltokens to accomplish different tasks. Specifically, we use free-form texts(i.e., scene descriptions) for perception and generate future predictionsdirectly in the RGB space with image tokens. For planning, we employ aposition-aware tokenizer to effectively encode action into discrete tokens. Wetrain a multi-modal transformer to autoregressively generate perception,prediction, and planning tokens in an end-to-end and unified manner.Experiments on the widely used nuScenes dataset demonstrate the effectivenessof Doe-1 in various tasks including visual question-answering,action-conditioned video generation, and motion planning. Code:https://github.com/wzzheng/Doe.</description><author>Wenzhao Zheng, Zetian Xia, Yuanhui Huang, Sicheng Zuo, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 12 Dec 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09627v1</guid></item><item><title>FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion</title><link>http://arxiv.org/abs/2412.09626v1</link><description>Visual diffusion models achieve remarkable progress, yet they are typicallytrained at limited resolutions due to the lack of high-resolution data andconstrained computation resources, hampering their ability to generatehigh-fidelity images or videos at higher resolutions. Recent efforts haveexplored tuning-free strategies to exhibit the untapped potentialhigher-resolution visual generation of pre-trained models. However, thesemethods are still prone to producing low-quality visual content with repetitivepatterns. The key obstacle lies in the inevitable increase in high-frequencyinformation when the model generates visual content exceeding its trainingresolution, leading to undesirable repetitive patterns deriving from theaccumulated errors. To tackle this challenge, we propose FreeScale, atuning-free inference paradigm to enable higher-resolution visual generationvia scale fusion. Specifically, FreeScale processes information from differentreceptive scales and then fuses it by extracting desired frequency components.Extensive experiments validate the superiority of our paradigm in extending thecapabilities of higher-resolution visual generation for both image and videomodels. Notably, compared with the previous best-performing method, FreeScaleunlocks the generation of 8k-resolution images for the first time.</description><author>Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, Ziwei Liu</author><pubDate>Thu, 12 Dec 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09626v1</guid></item><item><title>GenEx: Generating an Explorable World</title><link>http://arxiv.org/abs/2412.09624v1</link><description>Understanding, navigating, and exploring the 3D physical real world has longbeen a central challenge in the development of artificial intelligence. In thiswork, we take a step toward this goal by introducing GenEx, a system capable ofplanning complex embodied world exploration, guided by its generativeimagination that forms priors (expectations) about the surroundingenvironments. GenEx generates an entire 3D-consistent imaginative environmentfrom as little as a single RGB image, bringing it to life through panoramicvideo streams. Leveraging scalable 3D world data curated from Unreal Engine,our generative model is rounded in the physical world. It captures a continuous360-degree environment with little effort, offering a boundless landscape forAI agents to explore and interact with. GenEx achieves high-quality worldgeneration, robust loop consistency over long trajectories, and demonstratesstrong 3D capabilities such as consistency and active 3D mapping. Powered bygenerative imagination of the world, GPT-assisted agents are equipped toperform complex embodied tasks, including both goal-agnostic exploration andgoal-driven navigation. These agents utilize predictive expectation regardingunseen parts of the physical world to refine their beliefs, simulate differentoutcomes based on potential decisions, and make more informed choices. Insummary, we demonstrate that GenEx provides a transformative platform foradvancing embodied AI in imaginative spaces and brings potential for extendingthese capabilities to real-world exploration.</description><author>Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, Jieneng Chen</author><pubDate>Thu, 12 Dec 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09624v1</guid></item><item><title>OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation</title><link>http://arxiv.org/abs/2412.09623v1</link><description>As virtual reality gains popularity, the demand for controllable creation ofimmersive and dynamic omnidirectional videos (ODVs) is increasing. Whileprevious text-to-ODV generation methods achieve impressive results, theystruggle with content inaccuracies and inconsistencies due to reliance solelyon textual inputs. Although recent motion control techniques providefine-grained control for video generation, directly applying these methods toODVs often results in spatial distortion and unsatisfactory performance,especially with complex spherical motions. To tackle these challenges, wepropose OmniDrag, the first approach enabling both scene- and object-levelmotion control for accurate, high-quality omnidirectional image-to-videogeneration. Building on pretrained video diffusion models, we introduce anomnidirectional control module, which is jointly fine-tuned with temporalattention layers to effectively handle complex spherical motion. In addition,we develop a novel spherical motion estimator that accurately extractsmotion-control signals and allows users to perform drag-style ODV generation bysimply drawing handle and target points. We also present a new dataset, namedMove360, addressing the scarcity of ODV data with large scene and objectmotions. Experiments demonstrate the significant superiority of OmniDrag inachieving holistic scene-level and fine-grained object-level control for ODVgeneration. The project page is available athttps://lwq20020127.github.io/OmniDrag.</description><author>Weiqi Li, Shijie Zhao, Chong Mou, Xuhan Sheng, Zhenyu Zhang, Qian Wang, Junlin Li, Li Zhang, Jian Zhang</author><pubDate>Thu, 12 Dec 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09623v1</guid></item><item><title>LoRACLR: Contrastive Adaptation for Customization of Diffusion Models</title><link>http://arxiv.org/abs/2412.09622v1</link><description>Recent advances in text-to-image customization have enabled high-fidelity,context-rich generation of personalized images, allowing specific concepts toappear in a variety of scenarios. However, current methods struggle withcombining multiple personalized models, often leading to attribute entanglementor requiring separate training to preserve concept distinctiveness. We presentLoRACLR, a novel approach for multi-concept image generation that mergesmultiple LoRA models, each fine-tuned for a distinct concept, into a single,unified model without additional individual fine-tuning. LoRACLR uses acontrastive objective to align and merge the weight spaces of these models,ensuring compatibility while minimizing interference. By enforcing distinct yetcohesive representations for each concept, LoRACLR enables efficient, scalablemodel composition for high-quality, multi-concept image synthesis. Our resultshighlight the effectiveness of LoRACLR in accurately merging multiple concepts,advancing the capabilities of personalized image generation.</description><author>Enis Simsar, Thomas Hofmann, Federico Tombari, Pinar Yanardag</author><pubDate>Thu, 12 Dec 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09622v1</guid></item><item><title>Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos</title><link>http://arxiv.org/abs/2412.09621v1</link><description>Learning to understand dynamic 3D scenes from imagery is crucial forapplications ranging from robotics to scene reconstruction. Yet, unlike otherproblems where large-scale supervised training has enabled rapid progress,directly supervising methods for recovering 3D motion remains challenging dueto the fundamental difficulty of obtaining ground truth annotations. We presenta system for mining high-quality 4D reconstructions from internet stereoscopic,wide-angle videos. Our system fuses and filters the outputs of camera poseestimation, stereo depth estimation, and temporal tracking methods intohigh-quality dynamic 3D reconstructions. We use this method to generatelarge-scale data in the form of world-consistent, pseudo-metric 3D point cloudswith long-term motion trajectories. We demonstrate the utility of this data bytraining a variant of DUSt3R to predict structure and 3D motion from real-worldimage pairs, showing that training on our reconstructed data enablesgeneralization to diverse real-world scenes. Project page:https://stereo4d.github.io</description><author>Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander Holynski</author><pubDate>Thu, 12 Dec 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09621v1</guid></item><item><title>Learning Camera Movement Control from Real-World Drone Videos</title><link>http://arxiv.org/abs/2412.09620v1</link><description>This study seeks to automate camera movement control for filming existingsubjects into attractive videos, contrasting with the creation of non-existentcontent by directly generating the pixels. We select drone videos as our testcase due to their rich and challenging motion patterns, distinctive viewingangles, and precise controls. Existing AI videography methods struggle withlimited appearance diversity in simulation training, high costs of recordingexpert operations, and difficulties in designing heuristic-based goals to coverall scenarios. To avoid these issues, we propose a scalable method thatinvolves collecting real-world training data to improve diversity, extractingcamera trajectories automatically to minimize annotation costs, and training aneffective architecture that does not rely on heuristics. Specifically, wecollect 99k high-quality trajectories by running 3D reconstruction on onlinevideos, connecting camera poses from consecutive frames to formulate 3D camerapaths, and using Kalman filter to identify and remove low-quality data.Moreover, we introduce DVGFormer, an auto-regressive transformer that leveragesthe camera path and images from all past frames to predict camera movement inthe next frame. We evaluate our system across 38 synthetic natural scenes and 7real city 3D scans. We show that our system effectively learns to performchallenging camera movements such as navigating through obstacles, maintaininglow altitude to increase perceived speed, and orbiting towers and buildings,which are very useful for recording high-quality videos. Data and code areavailable at dvgformer.github.io.</description><author>Yunzhong Hou, Liang Zheng, Philip Torr</author><pubDate>Thu, 12 Dec 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09620v1</guid></item><item><title>SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training</title><link>http://arxiv.org/abs/2412.09619v1</link><description>Existing text-to-image (T2I) diffusion models face several limitations,including large model sizes, slow runtime, and low-quality generation on mobiledevices. This paper aims to address all of these challenges by developing anextremely small and fast T2I model that generates high-resolution andhigh-quality images on mobile platforms. We propose several techniques toachieve this goal. First, we systematically examine the design choices of thenetwork architecture to reduce model parameters and latency, while ensuringhigh-quality generation. Second, to further improve generation quality, weemploy cross-architecture knowledge distillation from a much larger model,using a multi-level approach to guide the training of our model from scratch.Third, we enable a few-step generation by integrating adversarial guidance withknowledge distillation. For the first time, our model SnapGen, demonstrates thegeneration of 1024x1024 px images on a mobile device around 1.4 seconds. OnImageNet-1K, our model, with only 372M parameters, achieves an FID of 2.06 for256x256 px generation. On T2I benchmarks (i.e., GenEval and DPG-Bench), ourmodel with merely 379M parameters, surpasses large-scale models with billionsof parameters at a significantly smaller size (e.g., 7x smaller than SDXL, 14xsmaller than IF-XL).</description><author>Dongting Hu, Jierun Chen, Xijie Huang, Huseyin Coskun, Arpit Sahni, Aarush Gupta, Anujraaj Goyal, Dishani Lahiri, Rajesh Singh, Yerlan Idelbayev, Junli Cao, Yanyu Li, Kwang-Ting Cheng, S. -H. Gary Chan, Mingming Gong, Sergey Tulyakov, Anil Kag, Yanwu Xu, Jian Ren</author><pubDate>Thu, 12 Dec 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09619v1</guid></item><item><title>EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM</title><link>http://arxiv.org/abs/2412.09618v1</link><description>Significant achievements in personalization of diffusion models have beenwitnessed. Conventional tuning-free methods mostly encode multiple referenceimages by averaging their image embeddings as the injection condition, but suchan image-independent operation cannot perform interaction among images tocapture consistent visual elements within multiple references. Although thetuning-based Low-Rank Adaptation (LoRA) can effectively extract consistentelements within multiple images through the training process, it necessitatesspecific finetuning for each distinct image group. This paper introducesEasyRef, a novel plug-and-play adaptation method that enables diffusion modelsto be conditioned on multiple reference images and the text prompt. Toeffectively exploit consistent visual elements within multiple images, weleverage the multi-image comprehension and instruction-following capabilitiesof the multimodal large language model (MLLM), prompting it to captureconsistent visual elements based on the instruction. Besides, injecting theMLLM's representations into the diffusion process through adapters can easilygeneralize to unseen domains, mining the consistent visual elements withinunseen data. To mitigate computational costs and enhance fine-grained detailpreservation, we introduce an efficient reference aggregation strategy and aprogressive training scheme. Finally, we introduce MRBench, a newmulti-reference image generation benchmark. Experimental results demonstrateEasyRef surpasses both tuning-free methods like IP-Adapter and tuning-basedmethods like LoRA, achieving superior aesthetic quality and robust zero-shotgeneralization across diverse domains.</description><author>Zhuofan Zong, Dongzhi Jiang, Bingqi Ma, Guanglu Song, Hao Shao, Dazhong Shen, Yu Liu, Hongsheng Li</author><pubDate>Thu, 12 Dec 2024 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09618v1</guid></item><item><title>V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding</title><link>http://arxiv.org/abs/2412.09616v1</link><description>Vision-Language Models (VLMs) have shown promising capabilities in handlingvarious multimodal tasks, yet they struggle in long-context scenarios,particularly in tasks involving videos, high-resolution images, or lengthyimage-text documents. In our work, we first conduct an empirical analysis ofthe long-context capabilities of VLMs using our augmented long-contextmultimodal datasets. Our findings reveal that directly applying the positionalencoding mechanism used for textual tokens to visual tokens is suboptimal, andVLM performance degrades sharply when the position encoding exceeds the model'scontext window. To address this, we propose Variable Visual Position Encoding(V2PE), a novel positional encoding approach that employs variable and smallerincrements for visual tokens, enabling more efficient management of longmultimodal sequences. Our experiments demonstrate the effectiveness of V2PE toenhances VLMs' ability to effectively understand and reason over longmultimodal contexts. We further integrate V2PE with our augmented long-contextmultimodal datasets to fine-tune the open-source VLM, InternVL2. The fine-tunedmodel achieves strong performance on both standard and long-context multimodaltasks. Notably, when the sequence length of the training dataset is increasedto 256K tokens, the model is capable of processing multimodal sequences up to1M tokens, highlighting its potential for real-world long-context applications.</description><author>Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, Xizhou Zhu</author><pubDate>Thu, 12 Dec 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09616v1</guid></item><item><title>Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG</title><link>http://arxiv.org/abs/2412.09614v1</link><description>We introduce a novel approach to enhance the capabilities of text-to-imagemodels by incorporating a graph-based RAG. Our system dynamically retrievesdetailed character information and relational data from the knowledge graph,enabling the generation of visually accurate and contextually rich images. Thiscapability significantly improves upon the limitations of existing T2I models,which often struggle with the accurate depiction of complex or culturallyspecific subjects due to dataset constraints. Furthermore, we propose a novelself-correcting mechanism for text-to-image models to ensure consistency andfidelity in visual outputs, leveraging the rich context from the graph to guidecorrections. Our qualitative and quantitative experiments demonstrate thatContext Canvas significantly enhances the capabilities of popular models suchas Flux, Stable Diffusion, and DALL-E, and improves the functionality ofControlNet for fine-grained image editing tasks. To our knowledge, ContextCanvas represents the first application of graph-based RAG in enhancing T2Imodels, representing a significant advancement for producing high-fidelity,context-aware multi-faceted images.</description><author>Kavana Venkatesh, Yusuf Dalva, Ismini Lourentzou, Pinar Yanardag</author><pubDate>Thu, 12 Dec 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09614v1</guid></item><item><title>PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models</title><link>http://arxiv.org/abs/2412.09613v1</link><description>Large Vision-Language Models (VLMs) have been extended to understand bothimages and videos. Visual token compression is leveraged to reduce theconsiderable token length of visual inputs. To meet the needs of differenttasks, existing high-performance models usually process images and videosseparately with different token compression strategies, limiting thecapabilities of combining images and videos. To this end, we extend each imageinto a "static" video and introduce a unified token compression strategy calledProgressive Visual Token Compression (PVC), where the tokens of each frame areprogressively encoded and adaptively compressed to supplement the informationnot extracted from previous frames. Video tokens are efficiently compressedwith exploiting the inherent temporal redundancy. Images are repeated as staticvideos, and the spatial details can be gradually supplemented in multipleframes. PVC unifies the token compressing of images and videos. With a limitednumber of tokens per frame (64 tokens by default), spatial details and temporalchanges can still be preserved. Experiments show that our model achievesstate-of-the-art performance across various video understanding benchmarks,including long video tasks and fine-grained short video tasks. Meanwhile, ourunified token compression strategy incurs no performance loss on imagebenchmarks, particularly in detail-sensitive tasks.</description><author>Chenyu Yang, Xuan Dong, Xizhou Zhu, Weijie Su, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, Jifeng Dai</author><pubDate>Thu, 12 Dec 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09613v1</guid></item><item><title>Olympus: A Universal Task Router for Computer Vision Tasks</title><link>http://arxiv.org/abs/2412.09612v1</link><description>We introduce Olympus, a new approach that transforms Multimodal LargeLanguage Models (MLLMs) into a unified framework capable of handling a widearray of computer vision tasks. Utilizing a controller MLLM, Olympus delegatesover 20 specialized tasks across images, videos, and 3D objects to dedicatedmodules. This instruction-based routing enables complex workflows throughchained actions without the need for training heavy generative models. Olympuseasily integrates with existing MLLMs, expanding their capabilities withcomparable performance. Experimental results demonstrate that Olympus achievesan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%in chained action scenarios, showcasing its effectiveness as a universal taskrouter that can solve a diverse range of computer vision tasks. Project page:https://github.com/yuanze-lin/Olympus_page</description><author>Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip H. S. Torr</author><pubDate>Thu, 12 Dec 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09612v1</guid></item><item><title>FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers</title><link>http://arxiv.org/abs/2412.09611v1</link><description>Rectified flow models have emerged as a dominant approach in imagegeneration, showcasing impressive capabilities in high-quality image synthesis.However, despite their effectiveness in visual generation, rectified flowmodels often struggle with disentangled editing of images. This limitationprevents the ability to perform precise, attribute-specific modificationswithout affecting unrelated aspects of the image. In this paper, we introduceFluxSpace, a domain-agnostic image editing method leveraging a representationspace with the ability to control the semantics of images generated byrectified flow transformers, such as Flux. By leveraging the representationslearned by the transformer blocks within the rectified flow models, we proposea set of semantically interpretable representations that enable a wide range ofimage editing tasks, from fine-grained image editing to artistic creation. Thiswork offers a scalable and effective image editing approach, along with itsdisentanglement capabilities.</description><author>Yusuf Dalva, Kavana Venkatesh, Pinar Yanardag</author><pubDate>Thu, 12 Dec 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09611v1</guid></item><item><title>Representing Long Volumetric Video with Temporal Gaussian Hierarchy</title><link>http://arxiv.org/abs/2412.09608v1</link><description>This paper aims to address the challenge of reconstructing long volumetricvideos from multi-view RGB videos. Recent dynamic view synthesis methodsleverage powerful 4D representations, like feature grids or point cloudsequences, to achieve high-quality rendering results. However, they aretypically limited to short (1~2s) video clips and often suffer from largememory footprints when dealing with longer videos. To solve this issue, wepropose a novel 4D representation, named Temporal Gaussian Hierarchy, tocompactly model long volumetric videos. Our key observation is that there aregenerally various degrees of temporal redundancy in dynamic scenes, whichconsist of areas changing at different speeds. Motivated by this, our approachbuilds a multi-level hierarchy of 4D Gaussian primitives, where each levelseparately describes scene regions with different degrees of content change,and adaptively shares Gaussian primitives to represent unchanged scene contentover different temporal segments, thus effectively reducing the number ofGaussian primitives. In addition, the tree-like structure of the Gaussianhierarchy allows us to efficiently represent the scene at a particular momentwith a subset of Gaussian primitives, leading to nearly constant GPU memoryusage during the training or rendering regardless of the video length.Extensive experimental results demonstrate the superiority of our method overalternative methods in terms of training cost, rendering speed, and storageusage. To our knowledge, this work is the first approach capable of efficientlyhandling minutes of volumetric video data while maintaining state-of-the-artrendering quality. Our project page is available at:https://zju3dv.github.io/longvolcap.</description><author>Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, Xiaowei Zhou</author><pubDate>Thu, 12 Dec 2024 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09608v1</guid></item><item><title>Spectral Image Tokenizer</title><link>http://arxiv.org/abs/2412.09607v1</link><description>Image tokenizers map images to sequences of discrete tokens, and are acrucial component of autoregressive transformer-based image generation. Thetokens are typically associated with spatial locations in the input image,arranged in raster scan order, which is not ideal for autoregressive modeling.In this paper, we propose to tokenize the image spectrum instead, obtained froma discrete wavelet transform (DWT), such that the sequence of tokens representsthe image in a coarse-to-fine fashion. Our tokenizer brings several advantages:1) it leverages that natural images are more compressible at high frequencies,2) it can take and reconstruct images of different resolutions withoutretraining, 3) it improves the conditioning for next-token prediction --instead of conditioning on a partial line-by-line reconstruction of the image,it takes a coarse reconstruction of the full image, 4) it enables partialdecoding where the first few generated tokens can reconstruct a coarse versionof the image, 5) it enables autoregressive models to be used for imageupsampling. We evaluate the tokenizer reconstruction metrics as well asmultiscale image generation, text-guided image upsampling and editing.</description><author>Carlos Esteves, Mohammed Suhail, Ameesh Makadia</author><pubDate>Thu, 12 Dec 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09607v1</guid></item><item><title>Feat2GS: Probing Visual Foundation Models with Gaussian Splatting</title><link>http://arxiv.org/abs/2412.09606v1</link><description>Given that visual foundation models (VFMs) are trained on extensive datasetsbut often limited to 2D images, a natural question arises: how well do theyunderstand the 3D world? With the differences in architecture and trainingprotocols (i.e., objectives, proxy tasks), a unified framework to fairly andcomprehensively probe their 3D awareness is urgently needed. Existing works on3D probing suggest single-view 2.5D estimation (e.g., depth and normal) ortwo-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately,these tasks ignore texture awareness, and require 3D data as ground-truth,which limits the scale and diversity of their evaluation set. To address theseissues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFMfeatures extracted from unposed images. This allows us to probe 3D awarenessfor geometry and texture via novel view synthesis, without requiring 3D data.Additionally, the disentanglement of 3DGS parameters - geometry($\boldsymbol{x}, \alpha, \Sigma$) and texture ($\boldsymbol{c}$) - enablesseparate analysis of texture and geometry awareness. Under Feat2GS, we conductextensive experiments to probe the 3D awareness of several VFMs, andinvestigate the ingredients that lead to a 3D aware VFM. Building on thesefindings, we develop several variants that achieve state-of-the-art acrossdiverse datasets. This makes Feat2GS useful for probing VFMs, and as asimple-yet-effective baseline for novel-view synthesis. Code and data will bemade available at https://fanegg.github.io/Feat2GS/.</description><author>Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, Yuliang Xiu</author><pubDate>Thu, 12 Dec 2024 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09606v1</guid></item><item><title>AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials</title><link>http://arxiv.org/abs/2412.09605v1</link><description>Graphical User Interface (GUI) agents hold great potential for automatingcomplex tasks across diverse digital environments, from web applications todesktop software. However, the development of such agents is hindered by thelack of high-quality, multi-step trajectory data required for effectivetraining. Existing approaches rely on expensive and labor-intensive humanannotation, making them unsustainable at scale. To address this challenge, wepropose AgentTrek, a scalable data synthesis pipeline that generateshigh-quality GUI agent trajectories by leveraging web tutorials. Our methodautomatically gathers tutorial-like texts from the internet, transforms theminto task goals with step-by-step instructions, and employs a visual-languagemodel agent to simulate their execution in a real digital environment. AVLM-based evaluator ensures the correctness of the generated trajectories. Wedemonstrate that training GUI agents with these synthesized trajectoriessignificantly improves their grounding and planning performance over thecurrent models. Moreover, our approach is more cost-efficient compared totraditional human annotation methods. This work underscores the potential ofguided replay with web tutorials as a viable strategy for large-scale GUI agenttraining, paving the way for more capable and autonomous digital agents.</description><author>Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, Tao Yu</author><pubDate>Thu, 12 Dec 2024 18:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09605v1</guid></item><item><title>SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding</title><link>http://arxiv.org/abs/2412.09604v1</link><description>The remarkable success of Large Language Models (LLMs) has extended to themultimodal domain, achieving outstanding performance in image understanding andgeneration. Recent efforts to develop unified Multimodal Large Language Models(MLLMs) that integrate these capabilities have shown promising results.However, existing approaches often involve complex designs in modelarchitecture or training pipeline, increasing the difficulty of model trainingand scaling. In this paper, we propose SynerGen-VL, a simple yet powerfulencoder-free MLLM capable of both image understanding and generation. Toaddress challenges identified in existing encoder-free unified MLLMs, weintroduce the token folding mechanism and the vision-expert-based progressivealignment pretraining strategy, which effectively support high-resolution imageunderstanding while reducing training complexity. After being trained onlarge-scale mixed image-text data with a unified next-token predictionobjective, SynerGen-VL achieves or surpasses the performance of existingencoder-free unified MLLMs with comparable or smaller parameter sizes, andnarrows the gap with task-specific state-of-the-art models, highlighting apromising path toward future unified MLLMs. Our code and models shall bereleased.</description><author>Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai</author><pubDate>Thu, 12 Dec 2024 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09604v1</guid></item><item><title>Do Multimodal Large Language Models See Like Humans?</title><link>http://arxiv.org/abs/2412.09603v1</link><description>Multimodal Large Language Models (MLLMs) have achieved impressive results onvarious vision tasks, leveraging recent advancements in large language models.However, a critical question remains unaddressed: do MLLMs perceive visualinformation similarly to humans? Current benchmarks lack the ability toevaluate MLLMs from this perspective. To address this challenge, we introduceHVSBench, a large-scale benchmark designed to assess the alignment betweenMLLMs and the human visual system (HVS) on fundamental vision tasks that mirrorhuman vision. HVSBench curated over 85K multimodal samples, spanning 13categories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,Free-Viewing, and Searching. Extensive experiments demonstrate theeffectiveness of our benchmark in providing a comprehensive evaluation ofMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best modelsshow significant room for improvement, with most achieving only moderateresults. Our experiments reveal that HVSBench presents a new and significantchallenge for cutting-edge MLLMs. We believe that HVSBench will facilitateresearch on human-aligned and explainable MLLMs, marking a key step inunderstanding how MLLMs perceive and process visual information.</description><author>Jiaying Lin, Shuquan Ye, Rynson W. H. Lau</author><pubDate>Thu, 12 Dec 2024 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09603v1</guid></item><item><title>Hidden Biases of End-to-End Driving Datasets</title><link>http://arxiv.org/abs/2412.09602v1</link><description>End-to-end driving systems have made rapid progress, but have so far not beenapplied to the challenging new CARLA Leaderboard 2.0. Further, while there is alarge body of literature on end-to-end architectures and training strategies,the impact of the training dataset is often overlooked. In this work, we make afirst attempt at end-to-end driving for Leaderboard 2.0. Instead ofinvestigating architectures, we systematically analyze the training dataset,leading to new insights: (1) Expert style significantly affects downstreampolicy performance. (2) In complex data sets, the frames should not be weightedon the basis of simplistic criteria such as class frequencies. (3) Instead,estimating whether a frame changes the target labels compared to previousframes can reduce the size of the dataset without removing importantinformation. By incorporating these findings, our model ranks first and secondrespectively on the map and sensors tracks of the 2024 CARLA Challenge, andsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncovera design flaw in the current evaluation metrics and propose a modification forfuture challenges. Our dataset, code, and pre-trained models are publiclyavailable at https://github.com/autonomousvision/carla_garage.</description><author>Julian Zimmerlin, Jens Bei√üwenger, Bernhard Jaeger, Andreas Geiger, Kashyap Chitta</author><pubDate>Thu, 12 Dec 2024 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09602v1</guid></item><item><title>TimeRefine: Temporal Grounding with Time Refining Video LLM</title><link>http://arxiv.org/abs/2412.09601v1</link><description>Video temporal grounding aims to localize relevant temporal boundaries in avideo given a textual prompt. Recent work has focused on enabling Video LLMs toperform video temporal grounding via next-token prediction of temporaltimestamps. However, accurately localizing timestamps in videos remainschallenging for Video LLMs when relying solely on temporal token prediction.Our proposed TimeRefine addresses this challenge in two ways. First, instead ofdirectly predicting the start and end timestamps, we reformulate the temporalgrounding task as a temporal refining task: the model first makes roughpredictions and then refines them by predicting offsets to the target segment.This refining process is repeated multiple times, through which the modelprogressively self-improves its temporal localization accuracy. Second, toenhance the model's temporal perception capabilities, we incorporate anauxiliary prediction head that penalizes the model more if a predicted segmentdeviates further from the ground truth, thus encouraging the model to makecloser and more accurate predictions. Our plug-and-play method can beintegrated into most LLM-based temporal grounding approaches. The experimentalresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements onthe ActivityNet and Charades-STA datasets, respectively. Code and pretrainedmodels will be released.</description><author>Xizi Wang, Feng Cheng, Ziyang Wang, Huiyu Wang, Md Mohaiminul Islam, Lorenzo Torresani, Mohit Bansal, Gedas Bertasius, David Crandall</author><pubDate>Thu, 12 Dec 2024 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09601v1</guid></item><item><title>Owl-1: Omni World Model for Consistent Long Video Generation</title><link>http://arxiv.org/abs/2412.09600v1</link><description>Video generation models (VGMs) have received extensive attention recently andserve as promising candidates for general-purpose large vision models. Whilethey can only generate short videos each time, existing methods achieve longvideo generation by iteratively calling the VGMs, using the last-frame outputas the condition for the next-round generation. However, the last frame onlycontains short-term fine-grained information about the scene, resulting ininconsistency in the long horizon. To address this, we propose an Omni WorldmodeL (Owl-1) to produce long-term coherent and comprehensive conditions forconsistent long video generation. As videos are observations of the underlyingevolving world, we propose to model the long-term developments in a latentspace and use VGMs to film them into videos. Specifically, we represent theworld with a latent state variable which can be decoded into explicit videoobservations. These observations serve as a basis for anticipating temporaldynamics which in turn update the state variable. The interaction betweenevolving dynamics and persistent state enhances the diversity and consistencyof the long videos. Extensive experiments show that Owl-1 achieves comparableperformance with SOTA methods on VBench-I2V and VBench-Long, validating itsability to generate high-quality video observations. Code:https://github.com/huang-yh/Owl.</description><author>Yuanhui Huang, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Di Zhang, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 12 Dec 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09600v1</guid></item><item><title>RatBodyFormer: Rodent Body Surface from Keypoints</title><link>http://arxiv.org/abs/2412.09599v1</link><description>Rat behavior modeling goes to the heart of many scientific studies, yet thetextureless body surface evades automatic analysis as it literally has nokeypoints that detectors can find. The movement of the body surface, however,is a rich source of information for deciphering the rat behavior. We introducetwo key contributions to automatically recover densely 3D sampled rat bodysurface points, passively. The first is RatDome, a novel multi-camera systemfor rat behavior capture, and a large-scale dataset captured with it thatconsists of pairs of 3D keypoints and 3D body surface points. The second isRatBodyFormer, a novel network to transform detected keypoints to 3D bodysurface points. RatBodyFormer is agnostic to the exact locations of the 3D bodysurface points in the training data and is trained with masked-learning. Weexperimentally validate our framework with a number of real-world experiments.Our results collectively serve as a novel foundation for automated rat behavioranalysis and will likely have far-reaching implications for biomedical andneuroscientific research.</description><author>Ayaka Higami, Karin Oshima, Tomoyo Isoguchi Shiramatsu, Hirokazu Takahashi, Shohei Nobuhara, Ko Nishino</author><pubDate>Thu, 12 Dec 2024 18:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09599v1</guid></item><item><title>LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video Generation Priors</title><link>http://arxiv.org/abs/2412.09597v1</link><description>Single-image 3D reconstruction remains a fundamental challenge in computervision due to inherent geometric ambiguities and limited viewpoint information.Recent advances in Latent Video Diffusion Models (LVDMs) offer promising 3Dpriors learned from large-scale video data. However, leveraging these priorseffectively faces three key challenges: (1) degradation in quality across largecamera motions, (2) difficulties in achieving precise camera control, and (3)geometric distortions inherent to the diffusion process that damage 3Dconsistency. We address these challenges by proposing LiftImage3D, a frameworkthat effectively releases LVDMs' generative priors while ensuring 3Dconsistency. Specifically, we design an articulated trajectory strategy togenerate video frames, which decomposes video sequences with large cameramotions into ones with controllable small motions. Then we use robust neuralmatching models, i.e. MASt3R, to calibrate the camera poses of generated framesand produce corresponding point clouds. Finally, we propose a distortion-aware3D Gaussian splatting representation, which can learn independent distortionsbetween frames and output undistorted canonical Gaussians. Extensiveexperiments demonstrate that LiftImage3D achieves state-of-the-art performanceon two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, andgeneralizes well to diverse in-the-wild images, from cartoon illustrations tocomplex real-world scenes.</description><author>Yabo Chen, Chen Yang, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, Wei Shen, Wenrui Dai, Hongkai Xiong, Qi Tian</author><pubDate>Thu, 12 Dec 2024 18:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09597v1</guid></item><item><title>LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living</title><link>http://arxiv.org/abs/2406.09390v2</link><description>Current Large Language Vision Models (LLVMs) trained on web videos performwell in general video understanding but struggle with fine-grained details,complex human-object interactions (HOI), and view-invariant representationlearning essential for Activities of Daily Living (ADL). This limitation stemsfrom a lack of specialized ADL video instruction-tuning datasets andinsufficient modality integration to capture discriminative actionrepresentations. To address this, we propose a semi-automated framework forcurating ADL datasets, creating ADL-X, a multiview, multimodal RGBSinstruction-tuning dataset. Additionally, we introduce LLAVIDAL, an LLVMintegrating videos, 3D skeletons, and HOIs to model ADL's complexspatiotemporal relationships. For training LLAVIDAL a simple joint alignment ofall modalities yields suboptimal results; thus, we propose a MultimodalProgressive (MMPro) training strategy, incorporating modalities in stagesfollowing a curriculum. We also establish ADL MCQ and video descriptionbenchmarks to assess LLVM performance in ADL tasks. Trained on ADL-X, LLAVIDALachieves state-of-the-art performance across ADL benchmarks. Code and data willbe made publicly available at: https://adl-x.github.io/.</description><author>Dominick Reilly, Rajatsubhra Chakraborty, Arkaprava Sinha, Manish Kumar Govind, Pu Wang, Francois Bremond, Le Xue, Srijan Das</author><pubDate>Thu, 12 Dec 2024 18:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09390v2</guid></item><item><title>InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions</title><link>http://arxiv.org/abs/2412.09596v1</link><description>Creating AI systems that can interact with environments over long periods,similar to human cognition, has been a longstanding research goal. Recentadvancements in multimodal large language models (MLLMs) have made significantstrides in open-world understanding. However, the challenge of continuous andsimultaneous streaming perception, memory, and reasoning remains largelyunexplored. Current MLLMs are constrained by their sequence-to-sequencearchitecture, which limits their ability to process inputs and generateresponses simultaneously, akin to being unable to think while perceiving.Furthermore, relying on long contexts to store historical data is impracticalfor long-term interactions, as retaining all information becomes costly andinefficient. Therefore, rather than relying on a single foundation model toperform all functions, this project draws inspiration from the concept of theSpecialized Generalist AI and introduces disentangled streaming perception,reasoning, and memory mechanisms, enabling real-time interaction with streamingvideo and audio input. The proposed framework InternLM-XComposer2.5-OmniLive(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:Processes multimodal information in real-time, storing key details in memoryand triggering reasoning in response to user queries. (2) Multi-modal LongMemory Module: Integrates short-term and long-term memory, compressingshort-term memories into long-term ones for efficient retrieval and improvedaccuracy. (3) Reasoning Module: Responds to queries and executes reasoningtasks, coordinating with the perception and memory modules. This projectsimulates human-like cognition, enabling multimodal large language models toprovide continuous and adaptive service over time.</description><author>Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, Jiaqi Wang</author><pubDate>Thu, 12 Dec 2024 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09596v1</guid></item><item><title>Wait-Less Offline Tuning and Re-solving for Online Decision Making</title><link>http://arxiv.org/abs/2412.09594v1</link><description>Online linear programming (OLP) has found broad applications in revenuemanagement and resource allocation. State-of-the-art OLP algorithms achieve lowregret by repeatedly solving linear programming (LP) subproblems thatincorporate updated resource information. However, LP-based methods arecomputationally expensive and often inefficient for large-scale applications.In contrast, recent first-order OLP algorithms are more computationallyefficient but typically suffer from worse regret guarantees. To address theseshortcomings, we propose a new algorithm that combines the strengths ofLP-based and first-order OLP methods. The algorithm re-solves the LPsubproblems periodically at a predefined frequency $f$ and uses the latest dualprices to guide online decision-making. In addition, a first-order method runsin parallel during each interval between LP re-solves, smoothing resourceconsumption. Our algorithm achieves $\mathscr{O}(\log (T/f) + \sqrt{f})$regret, delivering a "wait-less" online decision-making process that balancesthe computational efficiency of first-order methods and the superior regretguarantee of LP-based methods.</description><author>Jingruo Sun, Wenzhi Gao, Ellen Vitercik, Yinyu Ye</author><pubDate>Thu, 12 Dec 2024 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09594v1</guid></item><item><title>Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion</title><link>http://arxiv.org/abs/2412.09593v1</link><description>Recovering the geometry and materials of objects from a single image ischallenging due to its under-constrained nature. In this paper, we presentNeural LightRig, a novel framework that boosts intrinsic estimation byleveraging auxiliary multi-lighting conditions from 2D diffusion priors.Specifically, 1) we first leverage illumination priors from large-scalediffusion models to build our multi-light diffusion model on a syntheticrelighting dataset with dedicated designs. This diffusion model generatesmultiple consistent images, each illuminated by point light sources indifferent directions. 2) By using these varied lighting images to reduceestimation uncertainty, we train a large G-buffer model with a U-Net backboneto accurately predict surface normals and materials. Extensive experimentsvalidate that our approach significantly outperforms state-of-the-art methods,enabling accurate surface normal and PBR material estimation with vividrelighting effects. Code and dataset are available on our project page athttps://projects.zxhezexin.com/neural-lightrig.</description><author>Zexin He, Tengfei Wang, Xin Huang, Xingang Pan, Ziwei Liu</author><pubDate>Thu, 12 Dec 2024 18:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09593v1</guid></item><item><title>OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets in 50+ Languages</title><link>http://arxiv.org/abs/2412.09587v1</link><description>We present OpenNER 1.0, a standardized collection of openly available namedentity recognition (NER) datasets. OpenNER contains 34 datasets spanning 51languages, annotated in varying named entity ontologies. We correct annotationformat issues, standardize the original datasets into a uniform representation,map entity type names to be more consistent across corpora, and provide thecollection in a structure that enables research in multilingual andmulti-ontology NER. We provide baseline models using three pretrainedmultilingual language models to compare the performance of recent models andfacilitate future research in NER.</description><author>Chester Palen-Michel, Maxwell Pickering, Maya Kruse, Jonne S√§lev√§, Constantine Lignos</author><pubDate>Thu, 12 Dec 2024 18:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09587v1</guid></item><item><title>Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders</title><link>http://arxiv.org/abs/2412.09586v1</link><description>We address the problem of gaze target estimation, which aims to predict wherea person is looking in a scene. Predicting a person's gaze target requiresreasoning both about the person's appearance and the contents of the scene.Prior works have developed increasingly complex, hand-crafted pipelines forgaze target estimation that carefully fuse features from separate sceneencoders, head encoders, and auxiliary models for signals like depth and pose.Motivated by the success of general-purpose feature extractors on a variety ofvisual tasks, we propose Gaze-LLE, a novel transformer framework thatstreamlines gaze target estimation by leveraging features from a frozen DINOv2encoder. We extract a single feature representation for the scene, and apply aperson-specific positional prompt to decode gaze with a lightweight module. Wedemonstrate state-of-the-art performance across several gaze benchmarks andprovide extensive analysis to validate our design choices. Our code isavailable at: http://github.com/fkryan/gazelle .</description><author>Fiona Ryan, Ajay Bati, Sangmin Lee, Daniel Bolya, Judy Hoffman, James M. Rehg</author><pubDate>Thu, 12 Dec 2024 18:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09586v1</guid></item><item><title>OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation</title><link>http://arxiv.org/abs/2412.09585v1</link><description>The standard practice for developing contemporary MLLMs is to feed featuresfrom vision encoder(s) into the LLM and train with natural languagesupervision. In this work, we posit an overlooked opportunity to optimize theintermediate LLM representations through a vision perspective (objective),i.e., solely natural language supervision is sub-optimal for the MLLM's visualunderstanding ability. To that end, we propose OLA-VLM, the first approachdistilling knowledge into the LLM's hidden representations from a set of targetvisual representations. Firstly, we formulate the objective during thepretraining stage in MLLMs as a coupled optimization of predictive visualembedding and next text-token prediction. Secondly, we investigate MLLMstrained solely with natural language supervision and identify a positivecorrelation between the quality of visual representations within these modelsand their downstream performance. Moreover, upon probing our OLA-VLM, weobserve improved representation quality owing to the embedding optimization.Thirdly, we demonstrate that our OLA-VLM outperforms the single andmulti-encoder baselines, proving our approach's superiority over explicitlyfeeding the corresponding features to the LLM. Particularly, OLA-VLM boostsperformance by an average margin of up to 2.5% on various benchmarks, with anotable improvement of 8.7% on the Depth task in CV-Bench. Our code isopen-sourced at https://github.com/SHI-Labs/OLA-VLM .</description><author>Jitesh Jain, Zhengyuan Yang, Humphrey Shi, Jianfeng Gao, Jianwei Yang</author><pubDate>Thu, 12 Dec 2024 18:55:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09585v1</guid></item><item><title>Neptune: The Long Orbit to Benchmarking Long Video Understanding</title><link>http://arxiv.org/abs/2412.09582v1</link><description>This paper describes a semi-automatic pipeline to generate challengingquestion-answer-decoy sets for understanding long videos. Many existing videodatasets and models are focused on short clips (10s-30s). While some long videodatasets do exist, they can often be solved by powerful image models appliedper frame (and often to very few frames) in a video, and are usually manuallyannotated at high cost. In order to mitigate both these problems, we propose ascalable dataset creation pipeline which leverages large models (VLMs andLLMs), to automatically generate dense, time-aligned video captions, as well astough question answer decoy sets for video segments (up to 15 minutes inlength). Our dataset Neptune covers a broad range of long video reasoningabilities and consists of a subset that emphasizes multimodal reasoning. Sinceexisting metrics for open-ended question answering are either rule-based or mayrely on proprietary models, we provide a new open source model-based metric GEMto score open-ended responses on Neptune. Benchmark evaluations reveal thatmost current open-source long video models perform poorly on Neptune,particularly on questions testing temporal ordering, counting and statechanges. Through Neptune, we aim to spur the development of more advancedmodels capable of understanding long videos. The dataset is available athttps://github.com/google-deepmind/neptune</description><author>Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, Tobias Weyand</author><pubDate>Thu, 12 Dec 2024 18:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09582v1</guid></item><item><title>A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks</title><link>http://arxiv.org/abs/2412.09579v1</link><description>Knowledge distillation, where a small student model learns from a pre-trainedlarge teacher model, has achieved substantial empirical success since theseminal work of \citep{hinton2015distilling}. Despite prior theoretical studiesexploring the benefits of knowledge distillation, an important question remainsunanswered: why does soft-label training from the teacher require significantlyfewer neurons than directly training a small neural network with hard labels?To address this, we first present motivating experimental results using simpleneural network models on a binary classification problem. These resultsdemonstrate that soft-label training consistently outperforms hard-labeltraining in accuracy, with the performance gap becoming more pronounced as thedataset becomes increasingly difficult to classify. We then substantiate theseobservations with a theoretical contribution based on two-layer neural networkmodels. Specifically, we show that soft-label training using gradient descentrequires only $O\left(\frac{1}{\gamma^2 \epsilon}\right)$ neurons to achieve aclassification loss averaged over epochs smaller than some $\epsilon &gt; 0$,where $\gamma$ is the separation margin of the limiting kernel. In contrast,hard-label training requires $O\left(\frac{1}{\gamma^4} \cdot\ln\left(\frac{1}{\epsilon}\right)\right)$ neurons, as derived from an adaptedversion of the gradient descent analysis in \citep{ji2020polylogarithmic}. Thisimplies that when $\gamma \leq \epsilon$, i.e., when the dataset is challengingto classify, the neuron requirement for soft-label training can besignificantly lower than that for hard-label training. Finally, we presentexperimental results on deep neural networks, further validating thesetheoretical findings.</description><author>Saptarshi Mandal, Xiaojun Lin, R. Srikant</author><pubDate>Thu, 12 Dec 2024 18:54:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09579v1</guid></item><item><title>DISHONEST: Dissecting misInformation Spread using Homogeneous sOcial NEtworks and Semantic Topic classification</title><link>http://arxiv.org/abs/2412.09578v1</link><description>The emergence of the COVID-19 pandemic resulted in a significant rise in thespread of misinformation on online platforms such as Twitter. Oftentimes thisgrowth is blamed on the idea of the "echo chamber." However, the behavior saidto characterize these echo chambers exists in two dimensions. The first is in auser's social interactions, where they are said to stick with the same cliqueof like-minded users. The second is in the content of their posts, where theyare said to repeatedly espouse homogeneous ideas. In this study, we link thetwo by using Twitter's network of retweets to study social interactions andtopic modeling to study tweet content. In order to measure the diversity of auser's interactions over time, we develop a novel metric to track the speed atwhich they travel through the social network. The application of these analysismethods to misinformation-focused data from the pandemic demonstratescorrelation between social behavior and tweet content. We believe thiscorrelation supports the common intuition about how antisocial users behave,and further suggests that it holds even in subcommunities already rife withmisinformation.</description><author>Caleb Stam, Emily Saldanha, Mahantesh Halappanavar, Anurag Acharya</author><pubDate>Thu, 12 Dec 2024 18:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09578v1</guid></item><item><title>FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction</title><link>http://arxiv.org/abs/2412.09573v1</link><description>Existing sparse-view reconstruction models heavily rely on accurate knowncamera poses. However, deriving camera extrinsics and intrinsics fromsparse-view images presents significant challenges. In this work, we presentFreeSplatter, a highly scalable, feed-forward reconstruction framework capableof generating high-quality 3D Gaussians from uncalibrated sparse-view imagesand recovering their camera parameters in mere seconds. FreeSplatter is builtupon a streamlined transformer architecture, comprising sequentialself-attention blocks that facilitate information exchange among multi-viewimage tokens and decode them into pixel-wise 3D Gaussian primitives. Thepredicted Gaussian primitives are situated in a unified reference frame,allowing for high-fidelity 3D modeling and instant camera parameter estimationusing off-the-shelf solvers. To cater to both object-centric and scene-levelreconstruction, we train two model variants of FreeSplatter on extensivedatasets. In both scenarios, FreeSplatter outperforms state-of-the-artbaselines in terms of reconstruction quality and pose estimation accuracy.Furthermore, we showcase FreeSplatter's potential in enhancing the productivityof downstream applications, such as text/image-to-3D content creation.</description><author>Jiale Xu, Shenghua Gao, Ying Shan</author><pubDate>Thu, 12 Dec 2024 18:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09573v1</guid></item><item><title>DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction</title><link>http://arxiv.org/abs/2412.09572v1</link><description>Quantifying the uncertainty in the factual parametric knowledge of LargeLanguage Models (LLMs), especially in a black-box setting, poses a significantchallenge. Existing methods, which gauge a model's uncertainty throughevaluating self-consistency in responses to the original query, do not alwayscapture true uncertainty. Models might respond consistently to the origin querywith a wrong answer, yet respond correctly to varied questions from differentperspectives about the same query, and vice versa. In this paper, we propose anovel method, DiverseAgentEntropy, for evaluating a model's uncertainty usingmulti-agent interaction under the assumption that if a model is certain, itshould consistently recall the answer to the original query across a diversecollection of questions about the same original query. We further implement anabstention policy to withhold responses when uncertainty is high. Our methodoffers a more accurate prediction of the model's reliability and furtherdetects hallucinations, outperforming other self-consistency-based methods.Additionally, it demonstrates that existing models often fail to consistentlyretrieve the correct answer to the same query under diverse varied questionseven when knowing the correct answer.</description><author>Yu Feng, Phu Mon Htut, Zheng Qi, Wei Xiao, Manuel Mager, Nikolaos Pappas, Kishaloy Halder, Yang Li, Yassine Benajiba, Dan Roth</author><pubDate>Thu, 12 Dec 2024 18:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09572v1</guid></item><item><title>JuStRank: Benchmarking LLM Judges for System Ranking</title><link>http://arxiv.org/abs/2412.09569v1</link><description>Given the rapid progress of generative AI, there is a pressing need tosystematically compare and choose between the numerous models andconfigurations available. The scale and versatility of such evaluations makethe use of LLM-based judges a compelling solution for this challenge.Crucially, this approach requires first to validate the quality of the LLMjudge itself. Previous work has focused on instance-based assessment of LLMjudges, where a judge is evaluated over a set of responses, or response pairs,while being agnostic to their source systems. We argue that this settingoverlooks critical factors affecting system-level ranking, such as a judge'spositive or negative bias towards certain systems. To address this gap, weconduct the first large-scale study of LLM judges as system rankers. Systemscores are generated by aggregating judgment scores over multiple systemoutputs, and the judge's quality is assessed by comparing the resulting systemranking to a human-based ranking. Beyond overall judge assessment, our analysisprovides a fine-grained characterization of judge behavior, including theirdecisiveness and bias.</description><author>Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden, Asaf Yehudai</author><pubDate>Thu, 12 Dec 2024 18:51:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09569v1</guid></item><item><title>Obfuscated Activations Bypass LLM Latent-Space Defenses</title><link>http://arxiv.org/abs/2412.09565v1</link><description>Recent latent-space monitoring techniques have shown promise as defensesagainst LLM attacks. These defenses act as scanners that seek to detect harmfulactivations before they lead to undesirable actions. This prompts the question:Can models execute harmful behavior via inconspicuous latent states? Here, westudy such obfuscated activations. We show that state-of-the-art latent-spacedefenses -- including sparse autoencoders, representation probing, and latentOOD detection -- are all vulnerable to obfuscated activations. For example,against probes trained to classify harmfulness, our attacks can often reducerecall from 100% to 0% while retaining a 90% jailbreaking rate. However,obfuscation has limits: we find that on a complex task (writing SQL code),obfuscation reduces model performance. Together, our results demonstrate thatneural activations are highly malleable: we can reshape activation patterns ina variety of ways, often while preserving a network's behavior. This poses afundamental challenge to latent-space defenses.</description><author>Luke Bailey, Alex Serrano, Abhay Sheshadri, Mikhail Seleznyov, Jordan Taylor, Erik Jenner, Jacob Hilton, Stephen Casper, Carlos Guestrin, Scott Emmons</author><pubDate>Thu, 12 Dec 2024 18:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09565v1</guid></item><item><title>Improving the Reliability of Cable Broadband Networks via Proactive Network Maintenance</title><link>http://arxiv.org/abs/2412.09564v1</link><description>Cable broadband networks are one of the few "last-mile" broadbandtechnologies widely available in the U.S. Unfortunately, they have poorreliability after decades of deployment. The cable industry proposed aframework called Proactive Network Maintenance (PNM) to diagnose the cablenetworks. However, there is little public knowledge or systematic study on howto use these data to detect and localize cable network problems. Existing toolsin the public domain have prohibitive high false-positive rates. In this paper,we propose CableMon, the first public-domain system that applies machinelearning techniques to PNM data to improve the reliability of cable broadbandnetworks. CableMon tackles two key challenges faced by cable ISPs: accuratelydetecting failures, and distinguishing whether a failure occurs within anetwork or at a subscriber's premise. CableMon uses statistical models togenerate features from time series data and uses customer trouble tickets ashints to infer abnormal/failure thresholds for these generated features.Further, CableMon employs an unsupervised learning model to group cable devicessharing similar anomalous patterns and effectively identify impairments thatoccur inside a cable network and impairments occur at a subscriber's premise,as these two different faults require different types of technical personnel torepair them. We use eight months of PNM data and customer trouble tickets froman ISP and experimental deployment to evaluate CableMon's performance. Ourevaluation results show that CableMon can effectively detect and distinguishfailures from PNM data and outperforms existing public-domain tools.</description><author>Jiyao Hu, Zhenyu Zhou, Xiaowei Yang</author><pubDate>Thu, 12 Dec 2024 18:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09564v1</guid></item><item><title>Does Representation Matter? Exploring Intermediate Layers in Large Language Models</title><link>http://arxiv.org/abs/2412.09563v1</link><description>Understanding what defines a good representation in large language models(LLMs) is fundamental to both theoretical understanding and practicalapplications. In this paper, we investigate the quality of intermediaterepresentations in various LLM architectures, including Transformers and StateSpace Models (SSMs). We find that intermediate layers often yield moreinformative representations for downstream tasks than the final layers. Tomeasure the representation quality, we adapt and apply a suite of metrics -such as prompt entropy, curvature, and augmentation-invariance - originallyproposed in other contexts. Our empirical study reveals significantarchitectural differences, how representations evolve throughout training, andhow factors like input randomness and prompt length affect each layer. Notably,we observe a bimodal pattern in the entropy of some intermediate layers andconsider potential explanations tied to training data. Overall, our resultsilluminate the internal mechanics of LLMs and guide strategies forarchitectural optimization and training.</description><author>Oscar Skean, Md Rifat Arefin, Yann LeCun, Ravid Shwartz-Ziv</author><pubDate>Thu, 12 Dec 2024 18:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09563v1</guid></item><item><title>Localizing Memorization in SSL Vision Encoders</title><link>http://arxiv.org/abs/2409.19069v3</link><description>Recent work on studying memorization in self-supervised learning (SSL)suggests that even though SSL encoders are trained on millions of images, theystill memorize individual data points. While effort has been put intocharacterizing the memorized data and linking encoder memorization todownstream utility, little is known about where the memorization happens insideSSL encoders. To close this gap, we propose two metrics for localizingmemorization in SSL encoders on a per-layer (layermem) and per-unit basis(unitmem). Our localization methods are independent of the downstream task, donot require any label information, and can be performed in a forward pass. Bylocalizing memorization in various encoder architectures (convolutional andtransformer-based) trained on diverse datasets with contrastive andnon-contrastive SSL frameworks, we find that (1) while SSL memorizationincreases with layer depth, highly memorizing units are distributed across theentire encoder, (2) a significant fraction of units in SSL encoders experiencessurprisingly high memorization of individual data points, which is in contrastto models trained under supervision, (3) atypical (or outlier) data pointscause much higher layer and unit memorization than standard data points, and(4) in vision transformers, most memorization happens in the fully-connectedlayers. Finally, we show that localizing memorization in SSL has the potentialto improve fine-tuning and to inform pruning strategies.</description><author>Wenhao Wang, Adam Dziedzic, Michael Backes, Franziska Boenisch</author><pubDate>Thu, 12 Dec 2024 18:48:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19069v3</guid></item><item><title>Merging versus Ensembling in Multi-Study Prediction: Theoretical Insight from Random Effects</title><link>http://arxiv.org/abs/1905.07382v4</link><description>A critical decision point when training predictors using multiple studies iswhether studies should be combined or treated separately. We compare twomulti-study prediction approaches in the presence of potential heterogeneity inpredictor-outcome relationships across datasets: 1) merging all of the datasetsand training a single learner, and 2) multi-study ensembling, which involvestraining a separate learner on each dataset and combining the predictionsresulting from each learner. For ridge regression, we show analytically andconfirm via simulation that merging yields lower prediction error thanensembling when the predictor-outcome relationships are relatively homogeneousacross studies. However, as cross-study heterogeneity increases, there exists atransition point beyond which ensembling outperforms merging. We provideanalytic expressions for the transition point in various scenarios, studyasymptotic properties, and illustrate how transition point theory can be usedfor deciding when studies should be combined with an application frommetagenomics.</description><author>Zoe Guan, Giovanni Parmigiani, Prasad Patil</author><pubDate>Thu, 12 Dec 2024 18:47:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1905.07382v4</guid></item><item><title>Foundational Large Language Models for Materials Research</title><link>http://arxiv.org/abs/2412.09560v1</link><description>Materials discovery and development are critical for addressing globalchallenges. Yet, the exponential growth in materials science literaturecomprising vast amounts of textual data has created significant bottlenecks inknowledge extraction, synthesis, and scientific reasoning. Large LanguageModels (LLMs) offer unprecedented opportunities to accelerate materialsresearch through automated analysis and prediction. Still, their effectivedeployment requires domain-specific adaptation for understanding and solvingdomain-relevant tasks. Here, we present LLaMat, a family of foundational modelsfor materials science developed through continued pretraining of LLaMA modelson an extensive corpus of materials literature and crystallographic data.Through systematic evaluation, we demonstrate that LLaMat excels inmaterials-specific NLP and structured information extraction while maintaininggeneral linguistic capabilities. The specialized LLaMat-CIF variantdemonstrates unprecedented capabilities in crystal structure generation,predicting stable crystals with high coverage across the periodic table.Intriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,we observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specificperformance across diverse materials science tasks, including structuredinformation extraction from text and tables, more particularly in crystalstructure generation, a potential adaptation rigidity in overtrained LLMs.Altogether, the present work demonstrates the effectiveness of domainadaptation towards developing practically deployable LLM copilots for materialsresearch. Beyond materials science, our findings reveal importantconsiderations for domain adaptation of LLMs, such as model selection, trainingmethodology, and domain-specific performance, which may influence thedevelopment of specialized scientific AI systems.</description><author>Vaibhav Mishra, Somaditya Singh, Dhruv Ahlawat, Mohd Zaki, Vaibhav Bihani, Hargun Singh Grover, Biswajit Mishra, Santiago Miret, Mausam, N. M. Anoop Krishnan</author><pubDate>Thu, 12 Dec 2024 18:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09560v1</guid></item><item><title>Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning</title><link>http://arxiv.org/abs/2405.20535v2</link><description>Instruction Fine-Tuning (IFT) significantly enhances the zero-shotcapabilities of pretrained Large Language Models (LLMs). While coding data isknown to boost LLM reasoning abilities during pretraining, its role inactivating internal reasoning capacities during IFT remains understudied. Thispaper investigates a key question: How does coding data impact LLMs' reasoningcapacities during IFT stage? To explore this, we thoroughly examine the impactof coding data across different coding data proportions, model families, sizes,and reasoning domains, from various perspectives. Specifically, we create threeIFT datasets with increasing coding data proportions, fine-tune six LLMbackbones across different families and scales on these datasets, evaluate thetuned models' performance across twelve tasks in three reasoning domains, andanalyze the outcomes from three broad-to-granular perspectives: overall,domain-level, and task-specific. Our holistic analysis provides valuableinsights into each perspective. First, coding data tuning enhances the overallreasoning capabilities of LLMs across different model families and scales.Moreover, while the impact of coding data varies by domain, it shows consistenttrends within each domain across different model families and scales.Additionally, coding data generally provides comparable task-specific benefitsacross model families, with optimal proportions in IFT datasets beingtask-dependent.</description><author>Xinlu Zhang, Zhiyu Zoey Chen, Xi Ye, Xianjun Yang, Lichang Chen, William Yang Wang, Linda Ruth Petzold</author><pubDate>Thu, 12 Dec 2024 18:45:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20535v2</guid></item><item><title>Experimental Machine Learning with Classical and Quantum Data via NMR Quantum Kernels</title><link>http://arxiv.org/abs/2412.09557v1</link><description>Kernel methods map data into high-dimensional spaces, enabling linearalgorithms to learn nonlinear functions without explicitly storing the featurevectors. Quantum kernel methods promise efficient learning by encoding featuremaps into exponentially large Hilbert spaces inherent in quantum systems. Inthis work we implement quantum kernels on a 10-qubit star-topology register ina nuclear magnetic resonance (NMR) platform. We experimentally encode classicaldata in the evolution of multiple quantum coherence orders using data-dependentunitary transformations and then demonstrate one-dimensional regression andtwo-dimensional classification tasks. By extending the register to adouble-layered star configuration, we propose an extended quantum kernel tohandle non-parametrized operator inputs. By numerically simulating the extendedquantum kernel, we show classification of entangling and nonentanglingunitaries. These results confirm that quantum kernels exhibit strongcapabilities in classical as well as quantum machine learning tasks.</description><author>Vivek Sabarad, T. S. Mahesh</author><pubDate>Thu, 12 Dec 2024 18:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09557v1</guid></item><item><title>Enhancing Convergence of Decentralized Gradient Tracking under the KL Property</title><link>http://arxiv.org/abs/2412.09556v1</link><description>We study decentralized multiagent optimization over networks, modeled asundirected graphs. The optimization problem consists of minimizing a nonconvexsmooth function plus a convex extended-value function, which enforcesconstraints or extra structure on the solution (e.g., sparsity, low-rank). Wefurther assume that the objective function satisfies the Kurdyka-{\L}ojasiewicz(KL) property, with given exponent $\theta\in [0,1)$. The KL property issatisfied by several (nonconvex) functions of practical interest, e.g., arisingfrom machine learning applications; in the centralized setting, it permits toachieve strong convergence guarantees. Here we establish convergence of thesame type for the notorious decentralized gradient-tracking-based algorithmSONATA. Specifically, $\textbf{(i)}$ when $\theta\in (0,1/2]$, the sequencegenerated by SONATA converges to a stationary solution of the problem atR-linear rate;$ \textbf{(ii)} $when $\theta\in (1/2,1)$, sublinear rate iscertified; and finally $\textbf{(iii)}$ when $\theta=0$, the iterates willeither converge in a finite number of steps or converges at R-linear rate. Thismatches the convergence behavior of centralized proximal-gradient algorithmsexcept when $\theta=0$. Numerical results validate our theoretical findings.</description><author>Xiaokai Chen, Tianyu Cao, Gesualdo Scutari</author><pubDate>Thu, 12 Dec 2024 18:44:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09556v1</guid></item><item><title>Learning Flow Fields in Attention for Controllable Person Image Generation</title><link>http://arxiv.org/abs/2412.08486v2</link><description>Controllable person image generation aims to generate a person imageconditioned on reference images, allowing precise control over the person'sappearance or pose. However, prior methods often distort fine-grained texturaldetails from the reference image, despite achieving high overall image quality.We attribute these distortions to inadequate attention to corresponding regionsin the reference image. To address this, we thereby propose learning flowfields in attention (Leffa), which explicitly guides the target query to attendto the correct reference key in the attention layer during training.Specifically, it is realized via a regularization loss on top of the attentionmap within a diffusion-based baseline. Our extensive experiments show thatLeffa achieves state-of-the-art performance in controlling appearance (virtualtry-on) and pose (pose transfer), significantly reducing fine-grained detaildistortion while maintaining high image quality. Additionally, we show that ourloss is model-agnostic and can be used to improve the performance of otherdiffusion models.</description><author>Zijian Zhou, Shikun Liu, Xiao Han, Haozhe Liu, Kam Woh Ng, Tian Xie, Yuren Cong, Hang Li, Mengmeng Xu, Juan-Manuel P√©rez-R√∫a, Aditya Patel, Tao Xiang, Miaojing Shi, Sen He</author><pubDate>Thu, 12 Dec 2024 18:43:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08486v2</guid></item><item><title>Leveraging Medical Foundation Model Features in Graph Neural Network-Based Retrieval of Breast Histopathology Images</title><link>http://arxiv.org/abs/2405.04211v3</link><description>Breast cancer is the most common cancer type in women worldwide. Earlydetection and appropriate treatment can significantly reduce its impact. Whilehistopathology examinations play a vital role in rapid and accurate diagnosis,they often require experienced medical experts for proper recognition andcancer grading. Automated image retrieval systems have the potential to assistpathologists in identifying cancerous tissues, thereby accelerating thediagnostic process. Nevertheless, proposing an accurate image retrieval modelis challenging due to considerable variability among the tissue and cellpatterns in histological images. In this work, we leverage the features fromfoundation models in a novel attention-based adversarially regularizedvariational graph autoencoder model for breast histological image retrieval.Our results confirm the superior performance of models trained with foundationmodel features compared to those using pre-trained convolutional neuralnetworks (up to 7.7% and 15.5% for mAP and mMV, respectively), with thepre-trained general-purpose self-supervised model for computational pathology(UNI) delivering the best overall performance. By evaluating two publiclyavailable histology image datasets of breast cancer, our top-performing model,trained with UNI features, achieved average mAP/mMV scores of 96.7%/91.5% and97.6%/94.2% for the BreakHis and BACH datasets, respectively. Our proposedretrieval model has the potential to be used in clinical settings to enhancediagnostic performance and ultimately benefit patients.</description><author>Nematollah Saeidi, Hossein Karshenas, Bijan Shoushtarian, Sepideh Hatamikia, Ramona Woitek, Amirreza Mahbod</author><pubDate>Thu, 12 Dec 2024 18:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04211v3</guid></item><item><title>Video Creation by Demonstration</title><link>http://arxiv.org/abs/2412.09551v1</link><description>We explore a novel video creation experience, namely Video Creation byDemonstration. Given a demonstration video and a context image from a differentscene, we generate a physically plausible video that continues naturally fromthe context image and carries out the action concepts from the demonstration.To enable this capability, we present $\delta$-Diffusion, a self-supervisedtraining approach that learns from unlabeled videos by conditional future frameprediction. Unlike most existing video generation controls that are based onexplicit signals, we adopts the form of implicit latent control for maximalflexibility and expressiveness required by general videos. By leveraging avideo foundation model with an appearance bottleneck design on top, we extractaction latents from demonstration videos for conditioning the generationprocess with minimal appearance leakage. Empirically, $\delta$-Diffusionoutperforms related baselines in terms of both human preference and large-scalemachine evaluations, and demonstrates potentials towards interactive worldsimulation. Sampled video generation results are available athttps://delta-diffusion.github.io/.</description><author>Yihong Sun, Hao Zhou, Liangzhe Yuan, Jennifer J. Sun, Yandong Li, Xuhui Jia, Hartwig Adam, Bharath Hariharan, Long Zhao, Ting Liu</author><pubDate>Thu, 12 Dec 2024 18:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09551v1</guid></item><item><title>Exemplar Masking for Multimodal Incremental Learning</title><link>http://arxiv.org/abs/2412.09549v1</link><description>Multimodal incremental learning needs to digest the information from multiplemodalities while concurrently learning new knowledge without forgetting thepreviously learned information. There are numerous challenges for this task,mainly including the larger storage size of multimodal data in exemplar-basedmethods and the computational requirement of finetuning on huge multimodalmodels. In this paper, we leverage the parameter-efficient tuning scheme toreduce the burden of fine-tuning and propose the exemplar masking framework toefficiently replay old knowledge. Specifically, the non-important tokens aremasked based on the attention weights and the correlation across differentmodalities, significantly reducing the storage size of an exemplar andconsequently saving more exemplars under the same memory buffer. Moreover, wedesign a multimodal data augmentation technique to diversify exemplars forreplaying prior knowledge. In experiments, we not only evaluate our method inexisting multimodal datasets but also extend the ImageNet-R dataset to amultimodal dataset as a real-world application, where captions are generated byquerying multimodal large language models (e.g., InstructBLIP). Extensiveexperiments show that our exemplar masking framework is more efficient androbust to catastrophic forgetting under the same limited memory buffer. Code isavailable at https://github.com/YiLunLee/Exemplar_Masking_MCIL.</description><author>Yi-Lun Lee, Chen-Yu Lee, Wei-Chen Chiu, Yi-Hsuan Tsai</author><pubDate>Thu, 12 Dec 2024 18:40:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09549v1</guid></item><item><title>From Imitation to Refinement -- Residual RL for Precise Assembly</title><link>http://arxiv.org/abs/2407.16677v4</link><description>Recent advances in Behavior Cloning (BC) have made it easy to teach robotsnew tasks. However, we find that the ease of teaching comes at the cost ofunreliable performance that saturates with increasing data for tasks requiringprecision. The performance saturation can be attributed to two criticalfactors: (a) distribution shift resulting from the use of offline data and (b)the lack of closed-loop corrective control caused by action chucking(predicting a set of future actions executed open-loop) critical for BCperformance. Our key insight is that by predicting action chunks, BC policiesfunction more like trajectory "planners" than closed-loop controllers necessaryfor reliable execution. To address these challenges, we devise a simple yeteffective method, ResiP (Residual for Precise Manipulation), that overcomes thereliability problem while retaining BC's ease of teaching and long-horizoncapabilities. ResiP augments a frozen, chunked BC model with a fullyclosed-loop residual policy trained with reinforcement learning (RL) thataddresses distribution shifts and introduces closed-loop corrections overopen-loop execution of action chunks predicted by the BC trajectory planner.Videos, code, and data: https://residual-assembly.github.io.</description><author>Lars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, Pulkit Agrawal</author><pubDate>Thu, 12 Dec 2024 18:40:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16677v4</guid></item><item><title>Meshtron: High-Fidelity, Artist-Like 3D Mesh Generation at Scale</title><link>http://arxiv.org/abs/2412.09548v1</link><description>Meshes are fundamental representations of 3D surfaces. However, creatinghigh-quality meshes is a labor-intensive task that requires significant timeand expertise in 3D modeling. While a delicate object often requires over$10^4$ faces to be accurately modeled, recent attempts at generatingartist-like meshes are limited to $1.6$K faces and heavy discretization ofvertex coordinates. Hence, scaling both the maximum face count and vertexcoordinate resolution is crucial to producing high-quality meshes of realistic,complex 3D objects. We present Meshtron, a novel autoregressive mesh generationmodel able to generate meshes with up to 64K faces at 1024-level coordinateresolution --over an order of magnitude higher face count and $8{\times}$higher coordinate resolution than current state-of-the-art methods. Meshtron'sscalability is driven by four key components: (1) an hourglass neuralarchitecture, (2) truncated sequence training, (3) sliding window inference,(4) a robust sampling strategy that enforces the order of mesh sequences. Thisresults in over $50{\%}$ less training memory, $2.5{\times}$ faster throughput,and better consistency than existing works. Meshtron generates meshes ofdetailed, complex 3D objects at unprecedented levels of resolution andfidelity, closely resembling those created by professional artists, and openingthe door to more realistic generation of detailed 3D assets for animation,gaming, and virtual environments.</description><author>Zekun Hao, David W. Romero, Tsung-Yi Lin, Ming-Yu Liu</author><pubDate>Thu, 12 Dec 2024 18:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09548v1</guid></item><item><title>SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing</title><link>http://arxiv.org/abs/2412.09545v1</link><description>We introduce SimAvatar, a framework designed to generate simulation-readyclothed 3D human avatars from a text prompt. Current text-driven human avatargeneration methods either model hair, clothing, and the human body using aunified geometry or produce hair and garments that are not easily adaptable forsimulation within existing simulation pipelines. The primary challenge lies inrepresenting the hair and garment geometry in a way that allows leveragingestablished prior knowledge from foundational image diffusion models (e.g.,Stable Diffusion) while being simulation-ready using either physics or neuralsimulators. To address this task, we propose a two-stage framework thatcombines the flexibility of 3D Gaussians with simulation-ready hair strands andgarment meshes. Specifically, we first employ three text-conditioned 3Dgenerative models to generate garment mesh, body shape and hair strands fromthe given text prompt. To leverage prior knowledge from foundational diffusionmodels, we attach 3D Gaussians to the body mesh, garment mesh, as well as hairstrands and learn the avatar appearance through optimization. To drive theavatar given a pose sequence, we first apply physics simulators onto thegarment meshes and hair strands. We then transfer the motion onto 3D Gaussiansthrough carefully designed mechanisms for each body part. As a result, oursynthesized avatars have vivid texture and realistic dynamic motion. To thebest of our knowledge, our method is the first to produce highly realistic,fully simulation-ready 3D avatars, surpassing the capabilities of currentapproaches.</description><author>Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal</author><pubDate>Thu, 12 Dec 2024 18:35:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09545v1</guid></item><item><title>Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking</title><link>http://arxiv.org/abs/2412.09544v1</link><description>Aligning AI systems with human preferences typically suffers from theinfamous reward hacking problem, where optimization of an imperfect rewardmodel leads to undesired behaviors. In this paper, we investigate rewardhacking in offline preference optimization, which aims to improve an initialmodel using a preference dataset. We identify two types of reward hackingstemming from statistical fluctuations in the dataset: Type I Reward Hackingdue to subpar choices appearing more favorable, and Type II Reward Hacking dueto decent choices appearing less favorable. We prove that many (mainstream ortheoretical) preference optimization methods suffer from both types of rewardhacking. To mitigate Type I Reward Hacking, we propose POWER, a new preferenceoptimization method that combines Guiasu's weighted entropy with a robustreward maximization objective. POWER enjoys finite-sample guarantees undergeneral function approximation, competing with the best covered policy in thedata. To mitigate Type II Reward Hacking, we analyze the learning dynamics ofpreference optimization and develop a novel technique that dynamically updatespreference labels toward certain "stationary labels", resulting in diminishinggradients for untrustworthy samples. Empirically, POWER with dynamic labels(POWER-DL) consistently outperforms state-of-the-art methods on alignmentbenchmarks, achieving improvements of up to 13.0 points on AlpacaEval 2.0 and11.5 points on Arena-Hard over DPO, while also improving or maintainingperformance on downstream tasks such as mathematical reasoning. Strongtheoretical guarantees and empirical results demonstrate the promise ofPOWER-DL in mitigating reward hacking.</description><author>Paria Rashidinejad, Yuandong Tian</author><pubDate>Thu, 12 Dec 2024 18:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09544v1</guid></item><item><title>Capturing the Temporal Dependence of Training Data Influence</title><link>http://arxiv.org/abs/2412.09538v1</link><description>Traditional data influence estimation methods, like influence function,assume that learning algorithms are permutation-invariant with respect totraining data. However, modern training paradigms, especially for foundationmodels using stochastic algorithms and multi-stage curricula, are sensitive todata ordering, thus violating this assumption. This mismatch renders influencefunctions inadequate for answering a critical question in machine learning: Howcan we capture the dependence of data influence on the optimization trajectoryduring training? To address this gap, we formalize the concept oftrajectory-specific leave-one-out (LOO) influence, which quantifies the impactof removing a data point from a specific iteration during training, accountingfor the exact sequence of data encountered and the model's optimizationtrajectory. However, exactly evaluating the trajectory-specific LOO presents asignificant computational challenge. To address this, we propose data valueembedding, a novel technique enabling efficient approximation oftrajectory-specific LOO. Specifically, we compute a training data embeddingthat encapsulates the cumulative interactions between data and the evolvingmodel parameters. The LOO can then be efficiently approximated through a simpledot-product between the data value embedding and the gradient of the given testdata. As data value embedding captures training data ordering, it offersvaluable insights into model training dynamics. In particular, we uncoverdistinct phases of data influence, revealing that data points in the early andlate stages of training exert a greater impact on the final model. Theseinsights translate into actionable strategies for managing the computationaloverhead of data selection by strategically timing the selection process,potentially opening new avenues in data curation research.</description><author>Jiachen T. Wang, Dawn Song, James Zou, Prateek Mittal, Ruoxi Jia</author><pubDate>Thu, 12 Dec 2024 18:28:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09538v1</guid></item><item><title>Altogether: Image Captioning via Re-aligning Alt-text</title><link>http://arxiv.org/abs/2410.17251v2</link><description>This paper focuses on creating synthetic data to improve the quality of imagecaptions. Existing works typically have two shortcomings. First, they captionimages from scratch, ignoring existing alt-text metadata, and second, lacktransparency if the captioners' training data (e.g. GPT) is unknown. In thispaper, we study a principled approach Altogether based on the key idea to editand re-align existing alt-texts associated with the images. To generatetraining data, we perform human annotation where annotators start with theexisting alt-text and re-align it to the image content in multiple rounds,consequently constructing captions with rich visual concepts. This differs fromprior work that carries out human annotation as a one-time description tasksolely based on images and annotator knowledge. We train a captioner on thisdata that generalizes the process of re-aligning alt-texts at scale. Ourresults show our Altogether approach leads to richer image captions that alsoimprove text-to-image generation and zero-shot image classification tasks.</description><author>Hu Xu, Po-Yao Huang, Xiaoqing Ellen Tan, Ching-Feng Yeh, Jacob Kahn, Christine Jou, Gargi Ghosh, Omer Levy, Luke Zettlemoyer, Wen-tau Yih, Shang-Wen Li, Saining Xie, Christoph Feichtenhofer</author><pubDate>Thu, 12 Dec 2024 18:26:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17251v2</guid></item><item><title>Disentangling Mean Embeddings for Better Diagnostics of Image Generators</title><link>http://arxiv.org/abs/2409.01314v2</link><description>The evaluation of image generators remains a challenge due to the limitationsof traditional metrics in providing nuanced insights into specific imageregions. This is a critical problem as not all regions of an image may belearned with similar ease. In this work, we propose a novel approach todisentangle the cosine similarity of mean embeddings into the product of cosinesimilarities for individual pixel clusters via central kernel alignment.Consequently, we can quantify the contribution of the cluster-wise performanceto the overall image generation performance. We demonstrate how this enhancesthe explainability and the likelihood of identifying pixel regions of modelmisbehavior across various real-world use cases.</description><author>Sebastian G. Gruber, Pascal Tobias Ziegler, Florian Buettner</author><pubDate>Thu, 12 Dec 2024 18:21:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01314v2</guid></item><item><title>Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM</title><link>http://arxiv.org/abs/2412.09530v1</link><description>The application of Large Vision-Language Models (LVLMs) for analyzing imagesand videos is an exciting and rapidly evolving field. In recent years, we'veseen significant growth in high-quality image-text datasets for fine-tuningimage understanding, but there is still a lack of comparable datasets forvideos. Additionally, many VideoLLMs are extensions of single-image VLMs, whichmay not efficiently handle the complexities of longer videos. In this study, weintroduce a large-scale synthetic dataset created from proprietary models,using carefully designed prompts to tackle a wide range of questions. We alsoexplore a dynamic visual token compression architecture that strikes a balancebetween computational efficiency and performance. Our proposed \model{}achieves state-of-the-art results across various video tasks and showsimpressive generalization, setting new baselines in multi-image understanding.Notably, \model{} delivers an absolute improvement of 2.7\% overLLaVA-OneVision on VideoMME and 10.7\% on MuirBench. Codes are available athttps://github.com/Hon-Wong/ByteVideoLLM</description><author>Han Wang, Yuxiang Nie, Yongjie Ye, Deng GuanYu, Yanjie Wang, Shuai Li, Haiyang Yu, Jinghui Lu, Can Huang</author><pubDate>Thu, 12 Dec 2024 18:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09530v1</guid></item><item><title>Can Modern LLMs Act as Agent Cores in Radiology~Environments?</title><link>http://arxiv.org/abs/2412.09529v1</link><description>Advancements in large language models (LLMs) have paved the way for LLM-basedagent systems that offer enhanced accuracy and interpretability across variousdomains. Radiology, with its complex analytical requirements, is an ideal fieldfor the application of these agents. This paper aims to investigate thepre-requisite question for building concrete radiology agents which is, `Canmodern LLMs act as agent cores in radiology environments?' To investigate it,we introduce RadABench with three-fold contributions: First, we presentRadABench-Data, a comprehensive synthetic evaluation dataset for LLM-basedagents, generated from an extensive taxonomy encompassing 6 anatomies, 5imaging modalities, 10 tool categories, and 11 radiology tasks. Second, wepropose RadABench-EvalPlat, a novel evaluation platform for agents featuring aprompt-driven workflow and the capability to simulate a wide range of radiologytoolsets. Third, we assess the performance of 7 leading LLMs on our benchmarkfrom 5 perspectives with multiple metrics. Our findings indicate that whilecurrent LLMs demonstrate strong capabilities in many areas, they are still notsufficiently advanced to serve as the central agent core in a fully operationalradiology agent system. Additionally, we identify key factors influencing theperformance of LLM-based agent cores, offering insights for clinicians on howto apply agent systems in real-world radiology practices effectively. All ofour code and data are open-sourced inhttps://github.com/MAGIC-AI4Med/RadABench.</description><author>Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Thu, 12 Dec 2024 18:20:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09529v1</guid></item><item><title>Addressing common misinterpretations of KART and UAT in neural network literature</title><link>http://arxiv.org/abs/2408.16389v3</link><description>This note addresses the Kolmogorov-Arnold Representation Theorem (KART) andthe Universal Approximation Theorem (UAT), focusing on their commonmisinterpretations in some papers related to neural network approximation. Ourremarks aim to support a more accurate understanding of KART and UAT amongneural network specialists.</description><author>Vugar Ismailov</author><pubDate>Thu, 12 Dec 2024 18:19:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16389v3</guid></item><item><title>Non-IID data in Federated Learning: A Survey with Taxonomy, Metrics, Methods, Frameworks and Future Directions</title><link>http://arxiv.org/abs/2411.12377v2</link><description>Recent advances in machine learning have highlighted Federated Learning (FL)as a promising approach that enables multiple distributed users (so-calledclients) to collectively train ML models without sharing their private data.While this privacy-preserving method shows potential, it struggles when dataacross clients is not independent and identically distributed (non-IID) data.The latter remains an unsolved challenge that can result in poorer modelperformance and slower training times. Despite the significance of non-IID datain FL, there is a lack of consensus among researchers about its classificationand quantification. This technical survey aims to fill that gap by providing adetailed taxonomy for non-IID data, partition protocols, and metrics toquantify data heterogeneity. Additionally, we describe popular solutions toaddress non-IID data and standardized frameworks employed in FL withheterogeneous data. Based on our state-of-the-art survey, we present keylessons learned and suggest promising future research directions.</description><author>Daniel M. Jimenez G., David Solans, Mikko Heikkila, Andrea Vitaletti, Nicolas Kourtellis, Aris Anagnostopoulos, Ioannis Chatzigiannakis</author><pubDate>Thu, 12 Dec 2024 18:16:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12377v2</guid></item><item><title>Towards Secure and Private AI: A Framework for Decentralized Inference</title><link>http://arxiv.org/abs/2407.19401v2</link><description>The rapid advancement of ML models in critical sectors such as healthcare,finance, and security has intensified the need for robust data security, modelintegrity, and reliable outputs. Large multimodal foundational models, whilecrucial for complex tasks, present challenges in scalability, reliability, andpotential misuse. Decentralized systems offer a solution by distributingworkload and mitigating central points of failure, but they introduce risks ofunauthorized access to sensitive data across nodes. We address these challengeswith a comprehensive framework designed for responsible AI development. Ourapproach incorporates: 1) Zero-knowledge proofs for secure model verification,enhancing trust without compromising privacy. 2) Consensus-based verificationchecks to ensure consistent outputs across nodes, mitigating hallucinations andmaintaining model integrity. 3) Split Learning techniques that segment modelsacross different nodes, preserving data privacy by preventing full data accessat any point. 4) Hardware-based security through trusted execution environments(TEEs) to protect data and computations. This framework aims to enhancesecurity and privacy and improve the reliability and fairness of multimodal AIsystems. Promoting efficient resource utilization contributes to moresustainable AI development. Our state-of-the-art proofs and principlesdemonstrate the framework's effectiveness in responsibly democratizingartificial intelligence, offering a promising approach for building secure andprivate foundational models.</description><author>Hongyang Zhang, Yue Zhao, Claudio Angione, Harry Yang, James Buban, Ahmad Farhan, Fielding Johnston, Patrick Colangelo</author><pubDate>Thu, 12 Dec 2024 18:10:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19401v2</guid></item><item><title>Liquid: Language Models are Scalable Multi-modal Generators</title><link>http://arxiv.org/abs/2412.04332v2</link><description>We present Liquid, an auto-regressive generation paradigm that seamlesslyintegrates visual comprehension and generation by tokenizing images intodiscrete codes and learning these code embeddings alongside text tokens withina shared feature space for both vision and language. Unlike previous multimodallarge language model (MLLM), Liquid achieves this integration using a singlelarge language model (LLM), eliminating the need for external pretrained visualembeddings such as CLIP. For the first time, Liquid uncovers a scaling law thatperformance drop unavoidably brought by the unified training of visual andlanguage tasks diminishes as the model size increases. Furthermore, the unifiedtoken space enables visual generation and comprehension tasks to mutuallyenhance each other, effectively removing the typical interference seen inearlier models. We show that existing LLMs can serve as strong foundations forLiquid, saving 100x in training costs while outperforming Chameleon inmultimodal capabilities and maintaining language performance comparable tomainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 andSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language andtext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2are powerful multimodal generators, offering a scalable solution for enhancingboth vision-language understanding and generation. The code and models will bereleased at https://github.com/FoundationVision/Liquid.</description><author>Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, Xiang Bai</author><pubDate>Thu, 12 Dec 2024 18:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04332v2</guid></item><item><title>Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis</title><link>http://arxiv.org/abs/2412.09521v1</link><description>Pathological diagnosis is vital for determining disease characteristics,guiding treatment, and assessing prognosis, relying heavily on detailed,multi-scale analysis of high-resolution whole slide images (WSI). However,traditional pure vision models face challenges of redundant feature extraction,whereas existing large vision-language models (LVLMs) are limited by inputresolution constraints, hindering their efficiency and accuracy. To overcomethese issues, we propose two innovative strategies: the mixed task-guidedfeature enhancement, which directs feature extraction toward lesion-relateddetails across scales, and the prompt-guided detail feature completion, whichintegrates coarse- and fine-grained features from WSI based on specific promptswithout compromising inference speed. Leveraging a comprehensive dataset of490,000 samples from diverse pathology tasks-including cancer detection,grading, vascular and neural invasion identification, and so on-we trained thepathology-specialized LVLM, OmniPath. Extensive experiments demonstrate thatthis model significantly outperforms existing methods in diagnostic accuracyand efficiency, offering an interactive, clinically aligned approach forauxiliary diagnosis in a wide range of pathology applications.</description><author>Shengxuming Zhang, Weihan Li, Tianhong Gao, Jiacong Hu, Haoming Luo, Mingli Song, Xiuming Zhang, Zunlei Feng</author><pubDate>Thu, 12 Dec 2024 18:07:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09521v1</guid></item><item><title>GainAdaptor: Learning Quadrupedal Locomotion with Dual Actors for Adaptable and Energy-Efficient Walking on Various Terrains</title><link>http://arxiv.org/abs/2412.09520v1</link><description>Deep reinforcement learning (DRL) has emerged as an innovative solution forcontrolling legged robots in challenging environments using minimalistarchitectures. Traditional control methods for legged robots, such as inversedynamics, either directly manage joint torques or use proportional-derivative(PD) controllers to regulate joint positions at a higher level. In case of DRL,direct torque control presents significant challenges, leading to a preferencefor joint position control. However, this approach necessitates carefuladjustment of joint PD gains, which can limit both adaptability and efficiency.In this paper, we propose GainAdaptor, an adaptive gain control framework thatautonomously tunes joint PD gains to enhance terrain adaptability and energyefficiency. The framework employs a dual-actor algorithm to dynamically adjustthe PD gains based on varying ground conditions. By utilizing a divided actionspace, GainAdaptor efficiently learns stable and energy-efficient locomotion.We validate the effectiveness of the proposed method through experimentsconducted on a Unitree Go1 robot, demonstrating improved locomotion performanceacross diverse terrains.</description><author>Mincheol Kim, Nahyun Kwon, Jung-Yup Kim</author><pubDate>Thu, 12 Dec 2024 18:06:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09520v1</guid></item><item><title>BEACON: Benchmark for Comprehensive RNA Tasks and Language Models</title><link>http://arxiv.org/abs/2406.10391v2</link><description>RNA plays a pivotal role in translating genetic instructions into functionaloutcomes, underscoring its importance in biological processes and diseasemechanisms. Despite the emergence of numerous deep learning approaches for RNA,particularly universal RNA language models, there remains a significant lack ofstandardized benchmarks to assess the effectiveness of these methods. In thisstudy, we introduce the first comprehensive RNA benchmark BEACON(\textbf{BE}nchm\textbf{A}rk for \textbf{CO}mprehensive R\textbf{N}A Task andLanguage Models). First, BEACON comprises 13 distinct tasks derived fromextensive previous work covering structural analysis, functional studies, andengineering applications, enabling a comprehensive assessment of theperformance of methods on various RNA understanding tasks. Second, we examine arange of models, including traditional approaches like CNNs, as well asadvanced RNA foundation models based on language models, offering valuableinsights into the task-specific performances of these models. Third, weinvestigate the vital RNA language model components from the tokenizer andpositional encoding aspects. Notably, our findings emphasize the superiority ofsingle nucleotide tokenization and the effectiveness of Attention with LinearBiases (ALiBi) over traditional positional encoding methods. Based on theseinsights, a simple yet strong baseline called BEACON-B is proposed, which canachieve outstanding performance with limited data and computational resources.The datasets and source code of our benchmark are available athttps://github.com/terry-r123/RNABenchmark.</description><author>Yuchen Ren, Zhiyuan Chen, Lifeng Qiao, Hongtai Jing, Yuchen Cai, Sheng Xu, Peng Ye, Xinzhu Ma, Siqi Sun, Hongliang Yan, Dong Yuan, Wanli Ouyang, Xihui Liu</author><pubDate>Thu, 12 Dec 2024 18:00:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10391v2</guid></item><item><title>Agent-based Video Trimming</title><link>http://arxiv.org/abs/2412.09513v1</link><description>As information becomes more accessible, user-generated videos are increasingin length, placing a burden on viewers to sift through vast content forvaluable insights. This trend underscores the need for an algorithm to extractkey video information efficiently. Despite significant advancements inhighlight detection, moment retrieval, and video summarization, currentapproaches primarily focus on selecting specific time intervals, oftenoverlooking the relevance between segments and the potential for segmentarranging. In this paper, we introduce a novel task called Video Trimming (VT),which focuses on detecting wasted footage, selecting valuable segments, andcomposing them into a final video with a coherent story. To address this task,we propose Agent-based Video Trimming (AVT), structured into three phases:Video Structuring, Clip Filtering, and Story Composition. Specifically, weemploy a Video Captioning Agent to convert video slices into structured textualdescriptions, a Filtering Module to dynamically discard low-quality footagebased on the structured information of each clip, and a Video Arrangement Agentto select and compile valid clips into a coherent final narrative. Forevaluation, we develop a Video Evaluation Agent to assess trimmed videos,conducting assessments in parallel with human evaluations. Additionally, wecurate a new benchmark dataset for video trimming using raw user videos fromthe internet. As a result, AVT received more favorable evaluations in userstudies and demonstrated superior mAP and precision on the YouTube Highlights,TVSum, and our own dataset for the highlight detection task. The code andmodels are available at https://ylingfeng.github.io/AVT.</description><author>Lingfeng Yang, Zhenyuan Chen, Xiang Li, Peiyang Jia, Liangqu Long, Jian Yang</author><pubDate>Thu, 12 Dec 2024 17:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09513v1</guid></item><item><title>GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency</title><link>http://arxiv.org/abs/2412.09511v1</link><description>Identifying affordance regions on 3D objects from semantic cues is essentialfor robotics and human-machine interaction. However, existing 3D affordancelearning methods struggle with generalization and robustness due to limitedannotated data and a reliance on 3D backbones focused on geometric encoding,which often lack resilience to real-world noise and data corruption. We proposeGEAL, a novel framework designed to enhance the generalization and robustnessof 3D affordance learning by leveraging large-scale pre-trained 2D models. Weemploy a dual-branch architecture with Gaussian splatting to establishconsistent mappings between 3D point clouds and 2D representations, enablingrealistic 2D renderings from sparse point clouds. A granularity-adaptive fusionmodule and a 2D-3D consistency alignment module further strengthen cross-modalalignment and knowledge transfer, allowing the 3D branch to benefit from therich semantics and generalization capacity of 2D models. To holistically assessthe robustness, we introduce two new corruption-based benchmarks: PIAD-C andLASO-C. Extensive experiments on public datasets and our benchmarks show thatGEAL consistently outperforms existing methods across seen and novel objectcategories, as well as corrupted data, demonstrating robust and adaptableaffordance prediction under diverse conditions. Code and corruption datasetshave been made publicly available.</description><author>Dongyue Lu, Lingdong Kong, Tianxin Huang, Gim Hee Lee</author><pubDate>Thu, 12 Dec 2024 17:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09511v1</guid></item><item><title>Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction</title><link>http://arxiv.org/abs/2412.09507v1</link><description>Vision Transformers (ViTs) have demonstrated remarkable success in achievingstate-of-the-art performance across various image-based tasks and beyond. Inthis study, we employ a ViT-based neural network to address the problem ofindoor pathloss radio map prediction. The network's generalization ability isevaluated across diverse settings, including unseen buildings, frequencies, andantennas with varying radiation patterns. By leveraging extensive dataaugmentation techniques and pretrained DINOv2 weights, we achieve promisingresults, even under the most challenging scenarios.</description><author>Edvard Ghukasyan, Hrant Khachatrian, Rafayel Mkrtchyan, Theofanis P. Raptis</author><pubDate>Thu, 12 Dec 2024 17:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09507v1</guid></item><item><title>Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition</title><link>http://arxiv.org/abs/2412.09501v1</link><description>As Multi-modal Large Language Models (MLLMs) evolve, expanding beyondsingle-domain capabilities is essential to meet the demands for more versatileand efficient AI. However, previous omni-models have insufficiently exploredspeech, neglecting its integration with multi-modality. We introduce Lyra, anefficient MLLM that enhances multimodal abilities, including advancedlong-speech comprehension, sound understanding, cross-modality efficiency, andseamless speech interaction. To achieve efficiency and speech-centriccapabilities, Lyra employs three strategies: (1) leveraging existingopen-source large models and a proposed multi-modality LoRA to reduce trainingcosts and data requirements; (2) using a latent multi-modality regularizer andextractor to strengthen the relationship between speech and other modalities,thereby enhancing model performance; and (3) constructing a high-quality,extensive dataset that includes 1.5M multi-modal (language, vision, audio) datasamples and 12K long speech samples, enabling Lyra to handle complex longspeech inputs and achieve more robust omni-cognition. Compared to otheromni-methods, Lyra achieves state-of-the-art performance on variousvision-language, vision-speech, and speech-language benchmarks, while alsousing fewer computational resources and less training data.</description><author>Zhisheng Zhong, Chengyao Wang, Yuqi Liu, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, Shaozuo Yu, Sitong Wu, Eric Lo, Shu Liu, Jiaya Jia</author><pubDate>Thu, 12 Dec 2024 17:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09501v1</guid></item><item><title>Loss function to optimise signal significance in particle physics</title><link>http://arxiv.org/abs/2412.09500v1</link><description>We construct a surrogate loss to directly optimise the significance metricused in particle physics. We evaluate our loss function for a simple eventclassification task using a linear model and show that it produces decisionboundaries that change according to the cross sections of the processesinvolved. We find that the models trained with the new loss have higher signalefficiency for similar values of estimated signal significance compared to onestrained with a cross-entropy loss, showing promise to improve sensitivity ofparticle physics searches at colliders.</description><author>Jai Bardhan, Cyrin Neeraj, Subhadip Mitra, Tanumoy Mandal</author><pubDate>Thu, 12 Dec 2024 17:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09500v1</guid></item><item><title>A novel ML-fuzzy control system for optimizing PHEV fuel efficiency and extending electric range under diverse driving conditions</title><link>http://arxiv.org/abs/2412.09499v1</link><description>Aiming for a greener transportation future, this study introduces aninnovative control system for plug-in hybrid electric vehicles (PHEVs) thatutilizes machine learning (ML) techniques to forecast energy usage in the pureelectric mode of the vehicle and optimize power allocation across differentoperational modes, including pure electric, series hybrid, parallel hybrid, andinternal combustion operation. The fuzzy logic decision-making process governsthe vehicle control system. The performance was assessed under various drivingconditions. Key findings include a significant enhancement in pure electricmode efficiency, achieving an extended full-electric range of approximately 84kilometers on an 80% utilization of a 20-kWh battery pack. During the WLTCdriving cycle, the control system reduced fuel consumption to 2.86 L/100km,representing a 20% reduction in gasoline-equivalent fuel consumption.Evaluations of vehicle performance at discrete driving speeds, highlightedeffective energy management, with the vehicle battery charging at lower speedsand discharging at higher speeds, showing optimized energy recovery andconsumption strategies. Initial battery charge levels notably influencedvehicle performance. A 90% initial charge enabled prolonged all-electricoperation, minimizing fuel consumption to 2 L/100km less than that of the basecontrol system. Real-world driving pattern analysis revealed significantvariations, with shorter, slower cycles requiring lower fuel consumption due toprioritized electric propulsion, while longer, faster cycles increased internalcombustion engine usage. The control system also adapted to different batterystate of health (SOH) conditions, with higher SOH facilitating extendedelectric mode usage, reducing total fuel consumption by up to 2.87 L/100km.</description><author>Mehrdad Raeesi, Saba Mansour, Sina Changizian</author><pubDate>Thu, 12 Dec 2024 17:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09499v1</guid></item><item><title>Gradient descent inference in empirical risk minimization</title><link>http://arxiv.org/abs/2412.09498v1</link><description>Gradient descent is one of the most widely used iterative algorithms inmodern statistical learning. However, its precise algorithmic dynamics inhigh-dimensional settings remain only partially understood, which has thereforelimited its broader potential for statistical inference applications. This paper provides a precise, non-asymptotic distributional characterizationof gradient descent iterates in a broad class of empirical risk minimizationproblems, in the so-called mean-field regime where the sample size isproportional to the signal dimension. Our non-asymptotic state evolution theoryholds for both general non-convex loss functions and non-Gaussian data, andreveals the central role of two Onsager correction matrices that preciselycharacterize the non-trivial dependence among all gradient descent iterates inthe mean-field regime. Although the Onsager correction matrices are typically analyticallyintractable, our state evolution theory facilitates a generic gradient descentinference algorithm that consistently estimates these matrices across a broadclass of models. Leveraging this algorithm, we show that the state evolutioncan be inverted to construct (i) data-driven estimators for the generalizationerror of gradient descent iterates and (ii) debiased gradient descent iteratesfor inference of the unknown signal. Detailed applications to two canonicalmodels--linear regression and (generalized) logistic regression--are worked outto illustrate model-specific features of our general theory and inferencemethods.</description><author>Qiyang Han, Xiaocong Xu</author><pubDate>Thu, 12 Dec 2024 17:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09498v1</guid></item><item><title>Achieving Constant Regret in Linear Markov Decision Processes</title><link>http://arxiv.org/abs/2404.10745v2</link><description>We study the constant regret guarantees in reinforcement learning (RL). Ourobjective is to design an algorithm that incurs only finite regret overinfinite episodes with high probability. We introduce an algorithm,Cert-LSVI-UCB, for misspecified linear Markov decision processes (MDPs) whereboth the transition kernel and the reward function can be approximated by somelinear function up to misspecification level $\zeta$. At the core ofCert-LSVI-UCB is an innovative \method, which facilitates a fine-grainedconcentration analysis for multi-phase value-targeted regression, enabling usto establish an instance-dependent regret bound that is constant w.r.t. thenumber of episodes. Specifically, we demonstrate that for a linear MDPcharacterized by a minimal suboptimality gap $\Delta$, Cert-LSVI-UCB has acumulative regret of $\tilde{\mathcal{O}}(d^3H^5/\Delta)$ with highprobability, provided that the misspecification level $\zeta$ is below$\tilde{\mathcal{O}}(\Delta / (\sqrt{d}H^2))$. Here $d$ is the dimension of thefeature space and $H$ is the horizon. Remarkably, this regret bound isindependent of the number of episodes $K$. To the best of our knowledge,Cert-LSVI-UCB is the first algorithm to achieve a constant, instance-dependent,high-probability regret bound in RL with linear function approximation withoutrelying on prior distribution assumptions.</description><author>Weitong Zhang, Zhiyuan Fan, Jiafan He, Quanquan Gu</author><pubDate>Thu, 12 Dec 2024 17:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10745v2</guid></item><item><title>Video Seal: Open and Efficient Video Watermarking</title><link>http://arxiv.org/abs/2412.09492v1</link><description>The proliferation of AI-generated content and sophisticated video editingtools has made it both important and challenging to moderate digital platforms.Video watermarking addresses these challenges by embedding imperceptiblesignals into videos, allowing for identification. However, the rare open toolsand methods often fall short on efficiency, robustness, and flexibility. Toreduce these gaps, this paper introduces Video Seal, a comprehensive frameworkfor neural video watermarking and a competitive open-sourced model. Ourapproach jointly trains an embedder and an extractor, while ensuring thewatermark robustness by applying transformations in-between, e.g., videocodecs. This training is multistage and includes image pre-training, hybridpost-training and extractor fine-tuning. We also introduce temporal watermarkpropagation, a technique to convert any image watermarking model to anefficient video watermarking model without the need to watermark everyhigh-resolution frame. We present experimental results demonstrating theeffectiveness of the approach in terms of speed, imperceptibility, androbustness. Video Seal achieves higher robustness compared to strong baselinesespecially under challenging distortions combining geometric transformationsand video compression. Additionally, we provide new insights such as the impactof video compression during training, and how to compare methods operating ondifferent payloads. Contributions in this work - including the codebase,models, and a public demo - are open-sourced under permissive licenses tofoster further research and development in the field.</description><author>Pierre Fernandez, Hady Elsahar, I. Zeki Yalniz, Alexandre Mourachko</author><pubDate>Thu, 12 Dec 2024 17:41:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09492v1</guid></item><item><title>The rate of convergence of Bregman proximal methods: Local geometry vs. regularity vs. sharpness</title><link>http://arxiv.org/abs/2211.08043v3</link><description>We examine the last-iterate convergence rate of Bregman proximal methods -from mirror descent to mirror-prox and its optimistic variants - as a functionof the local geometry induced by the prox-mapping defining the method. Forgenerality, we focus on local solutions of constrained, non-monotonevariational inequalities, and we show that the convergence rate of a givenmethod depends sharply on its associated Legendre exponent, a notion thatmeasures the growth rate of the underlying Bregman function (Euclidean,entropic, or other) near a solution. In particular, we show that boundarysolutions exhibit a stark separation of regimes between methods with a zero andnon-zero Legendre exponent: the former converge at a linear rate, while thelatter converge, in general, sublinearly. This dichotomy becomes even morepronounced in linearly constrained problems where methods with entropicregularization achieve a linear convergence rate along sharp directions,compared to convergence in a finite number of steps under Euclideanregularization.</description><author>Wa√Øss Azizian, Franck Iutzeler, J√©r√¥me Malick, Panayotis Mertikopoulos</author><pubDate>Thu, 12 Dec 2024 17:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08043v3</guid></item><item><title>Regression and Classification with Single-Qubit Quantum Neural Networks</title><link>http://arxiv.org/abs/2412.09486v1</link><description>Since classical machine learning has become a powerful tool for developingdata-driven algorithms, quantum machine learning is expected to similarlyimpact the development of quantum algorithms. The literature reflects amutually beneficial relationship between machine learning and quantumcomputing, where progress in one field frequently drives improvements in theother. Motivated by the fertile connection between machine learning and quantumcomputing enabled by parameterized quantum circuits, we use aresource-efficient and scalable Single-Qubit Quantum Neural Network (SQQNN) forboth regression and classification tasks. The SQQNN leverages parameterizedsingle-qubit unitary operators and quantum measurements to achieve efficientlearning. To train the model, we use gradient descent for regression tasks. Forclassification, we introduce a novel training method inspired by the Taylorseries, which can efficiently find a global minimum in a single step. Thisapproach significantly accelerates training compared to iterative methods.Evaluated across various applications, the SQQNN exhibits virtually error-freeand strong performance in regression and classification tasks, including theMNIST dataset. These results demonstrate the versatility, scalability, andsuitability of the SQQNN for deployment on near-term quantum devices.</description><author>Leandro C. Souza, Bruno C. Guingo, Gilson Giraldi, Renato Portugal</author><pubDate>Thu, 12 Dec 2024 17:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09486v1</guid></item><item><title>Early Detection of At-Risk Students Using Machine Learning</title><link>http://arxiv.org/abs/2412.09483v1</link><description>This research presents preliminary work to address the challenge ofidentifying at-risk students using supervised machine learning and three uniquedata categories: engagement, demographics, and performance data collected fromFall 2023 using Canvas and the California State University, Fullertondashboard. We aim to tackle the persistent challenges of higher educationretention and student dropout rates by screening for at-risk students andbuilding a high-risk identification system. By focusing on previouslyoverlooked behavioral factors alongside traditional metrics, this work aims toaddress educational gaps, enhance student outcomes, and significantly booststudent success across disciplines at the University. Pre-processing steps takeplace to establish a target variable, anonymize student information, managemissing data, and identify the most significant features. Given the mixed datatypes in the datasets and the binary classification nature of this study, thiswork considers several machine learning models, including Support VectorMachines (SVM), Naive Bayes, K-nearest neighbors (KNN), Decision Trees,Logistic Regression, and Random Forest. These models predict at-risk studentsand identify critical periods of the semester when student performance is mostvulnerable. We will use validation techniques such as train test split andk-fold cross-validation to ensure the reliability of the models. Our analysisindicates that all algorithms generate an acceptable outcome for at-riskstudent predictions, while Naive Bayes performs best overall.</description><author>Azucena L. Jimenez Martinez, Kanika Sood, Rakeshkumar Mahto</author><pubDate>Thu, 12 Dec 2024 17:33:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09483v1</guid></item><item><title>LCFO: Long Context and Long Form Output Dataset and Benchmarking</title><link>http://arxiv.org/abs/2412.08268v2</link><description>This paper presents the Long Context and Form Output (LCFO) benchmark, anovel evaluation framework for assessing gradual summarization and summaryexpansion capabilities across diverse domains. LCFO consists of long inputdocuments (5k words average length), each of which comes with three summariesof different lengths (20%, 10%, and 5% of the input text), as well asapproximately 15 questions and answers (QA) related to the input content.Notably, LCFO also provides alignments between specific QA pairs andcorresponding summaries in 7 domains. The primary motivation behind providingsummaries of different lengths is to establish a controllable framework forgenerating long texts from shorter inputs, i.e. summary expansion. To establishan evaluation metric framework for summarization and summary expansion, weprovide human evaluation scores for human-generated outputs, as well as resultsfrom various state-of-the-art large language models (LLMs). GPT-4o-miniachieves best human scores among automatic systems in both summarization andsummary expansion tasks (~ +10% and +20%, respectively). It even surpasseshuman output quality in the case of short summaries (~ +7%). Overall automaticmetrics achieve low correlations with human evaluation scores (~ 0.4) butmoderate correlation on specific evaluation aspects such as fluency andattribution (~ 0.6). The LCFO benchmark offers a standardized platform forevaluating summarization and summary expansion performance, as well ascorresponding automatic metrics, thereby providing an important evaluationframework to advance generative AI.</description><author>Marta R. Costa-juss√†, Pierre Andrews, Mariano Coria Meglioli, Joy Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo S√°nchez, Holger Schwenk, Tuan Tran, Arina Turkatenko, Carleigh Wood</author><pubDate>Thu, 12 Dec 2024 17:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08268v2</guid></item><item><title>The Parameters of Educability</title><link>http://arxiv.org/abs/2412.09480v1</link><description>The educability model is a computational model that has been recentlyproposed to describe the cognitive capability that makes humans unique amongexisting biological species on Earth in being able to create advancedcivilizations. Educability is defined as a capability for acquiring andapplying knowledge. It is intended both to describe human capabilities and,equally, as an aspirational description of what can be usefully realized bymachines. While the intention is to have a mathematically well-definedcomputational model, in constructing an instance of the model there are anumber of decisions to make. We call these decisions {\it parameters}. In astandard computer, two parameters are the memory capacity and clock rate. Thereis no universally optimal choice for either one, or even for their ratio.Similarly, in a standard machine learning system, two parameters are thelearning algorithm and the dataset used for training. Again, there are nouniversally optimal choices known for either. An educable system has many moreparameters than either of these two kinds of system. This short paper discussessome of the main parameters of educable systems, and the broader implicationsof their existence.</description><author>Leslie G. Valiant</author><pubDate>Thu, 12 Dec 2024 17:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09480v1</guid></item><item><title>Improving the Validity of Automatically Generated Feedback via Reinforcement Learning</title><link>http://arxiv.org/abs/2403.01304v2</link><description>Automatically generating feedback via large language models (LLMs) inintelligent tutoring systems and online learning platforms has the potential toimprove the learning outcomes of many students. However, both feedbackgeneration and evaluation are challenging: feedback content has to be validespecially in subjects like math, which requires models to understand theproblem, the solution, and where the student's error lies. Feedback also has tobe pedagogically valid to reflect effective tutoring strategies, such asexplaining possible misconceptions and encouraging the student, among otherdesirable features. In this work, we address both problems of automaticallygenerating and evaluating feedback while considering both correctness andalignment. First, we propose a rubric for evaluating math feedback and showthat GPT-4 is able to effectively use it to annotate human-written andLLM-generated feedback. Second, we propose a framework for feedback generationthat optimizes both correctness and alignment using reinforcement learning(RL). Specifically, we use GPT-4's annotations to create preferences overfeedback pairs in an augmented dataset for training via direct preferenceoptimization (DPO). We show that our methods significantly increase thecorrectness and alignment of generated feedback with Llama 2, an open-sourceLLM, qualitatively analyze our generation and evaluation systems using casestudies, and outline several areas for future work.</description><author>Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan</author><pubDate>Thu, 12 Dec 2024 17:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01304v2</guid></item><item><title>Nearly Minimax Optimal Submodular Maximization with Bandit Feedback</title><link>http://arxiv.org/abs/2310.18465v2</link><description>We consider maximizing an unknown monotonic, submodular set function $f:2^{[n]} \rightarrow [0,1]$ with cardinality constraint under stochastic banditfeedback. At each time $t=1,\dots,T$ the learner chooses a set $S_t \subset[n]$ with $|S_t| \leq k$ and receives reward $f(S_t) + \eta_t$ where $\eta_t$is mean-zero sub-Gaussian noise. The objective is to minimize the learner'sregret with respect to an approximation of the maximum $f(S_*)$ with $|S_*| =k$, obtained through robust greedy maximization of $f$. To date, the bestregret bound in the literature scales as $k n^{1/3} T^{2/3}$. And by triviallytreating every set as a unique arm one deduces that $\sqrt{ {n \choose k} T }$is also achievable using standard multi-armed bandit algorithms. In this work,we establish the first minimax lower bound for this setting that scales like$\tilde{\Omega}(\min_{L \le k}(L^{1/3}n^{1/3}T^{2/3} + \sqrt{{n \choose k -L}T}))$. For a slightly restricted algorithm class, we prove a stronger regretlower bound of $\tilde{\Omega}(\min_{L \le k}(Ln^{1/3}T^{2/3} + \sqrt{{n\choose k - L}T}))$. Moreover, we propose an algorithm Sub-UCB that achievesregret $\tilde{\mathcal{O}}(\min_{L \le k}(Ln^{1/3}T^{2/3} + \sqrt{{n \choose k- L}T}))$ capable of matching the lower bound on regret for the restrictedclass up to logarithmic factors.</description><author>Artin Tajdini, Lalit Jain, Kevin Jamieson</author><pubDate>Thu, 12 Dec 2024 17:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18465v2</guid></item><item><title>Training Free Guided Flow Matching with Optimal Control</title><link>http://arxiv.org/abs/2410.18070v2</link><description>Controlled generation with pre-trained Diffusion and Flow Matching models hasvast applications. One strategy for guiding ODE-based generative models isthrough optimizing a target loss $R(x_1)$ while staying close to the priordistribution. Along this line, some recent work showed the effectiveness ofguiding flow model by differentiating through its ODE sampling process. Despitethe superior performance, the theoretical understanding of this line of methodsis still preliminary, leaving space for algorithm improvement. Moreover,existing methods predominately focus on Euclidean data manifold, and there is acompelling need for guided flow methods on complex geometries such as SO(3),which prevails in high-stake scientific applications like protein design. Wepresent OC-Flow, a general and theoretically grounded training-free frameworkfor guided flow matching using optimal control. Building upon advances inoptimal control theory, we develop effective and practical algorithms forsolving optimal control in guided ODE-based generation and provide a systematictheoretical analysis of the convergence guarantee in both Euclidean and SO(3).We show that existing backprop-through-ODE methods can be interpreted asspecial cases of Euclidean OC-Flow. OC-Flow achieved superior performance inextensive experiments on text-guided image manipulation, conditional moleculegeneration, and all-atom peptide design.</description><author>Luran Wang, Chaoran Cheng, Yizhen Liao, Yanru Qu, Ge Liu</author><pubDate>Thu, 12 Dec 2024 17:23:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18070v2</guid></item><item><title>Bayesian Optimization via Continual Variational Last Layer Training</title><link>http://arxiv.org/abs/2412.09477v1</link><description>Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogatemodels for Bayesian optimization (BO) due to their ability to model uncertaintyand their performance on tasks where correlations are easily captured (such asthose defined by Euclidean metrics) and their ability to be efficiently updatedonline. However, the performance of GPs depends on the choice of kernel, andkernel selection for complex correlation structures is often difficult or mustbe made bespoke. While Bayesian neural networks (BNNs) are a promisingdirection for higher capacity surrogate models, they have so far seen limiteduse due to poor performance on some problem types. In this paper, we propose anapproach which shows competitive performance on many problem types, includingsome that BNNs typically struggle with. We build on variational Bayesian lastlayers (VBLLs), and connect training of these models to exact conditioning inGPs. We exploit this connection to develop an efficient online trainingalgorithm that interleaves conditioning and optimization. Our findings suggestthat VBLL networks significantly outperform GPs and other BNN architectures ontasks with complex input correlations, and match the performance of well-tunedGPs on established benchmark tasks.</description><author>Paul Brunzema, Mikkel Jordahn, John Willes, Sebastian Trimpe, Jasper Snoek, James Harrison</author><pubDate>Thu, 12 Dec 2024 17:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09477v1</guid></item><item><title>New keypoint-based approach for recognising British Sign Language (BSL) from sequences</title><link>http://arxiv.org/abs/2412.09475v1</link><description>In this paper, we present a novel keypoint-based classification modeldesigned to recognise British Sign Language (BSL) words within continuoussigning sequences. Our model's performance is assessed using the BOBSL dataset,revealing that the keypoint-based approach surpasses its RGB-based counterpartin computational efficiency and memory usage. Furthermore, it offers expeditedtraining times and demands fewer computational resources. To the best of ourknowledge, this is the inaugural application of a keypoint-based model for BSLword classification, rendering direct comparisons with existing worksunavailable.</description><author>Oishi Deb, KR Prajwal, Andrew Zisserman</author><pubDate>Thu, 12 Dec 2024 17:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09475v1</guid></item><item><title>A Novel Ensemble-Based Deep Learning Model with Explainable AI for Accurate Kidney Disease Diagnosis</title><link>http://arxiv.org/abs/2412.09472v1</link><description>Chronic Kidney Disease (CKD) represents a significant global healthchallenge, characterized by the progressive decline in renal function, leadingto the accumulation of waste products and disruptions in fluid balance withinthe body. Given its pervasive impact on public health, there is a pressing needfor effective diagnostic tools to enable timely intervention. Our study delvesinto the application of cutting-edge transfer learning models for the earlydetection of CKD. Leveraging a comprehensive and publicly available dataset, wemeticulously evaluate the performance of several state-of-the-art models,including EfficientNetV2, InceptionNetV2, MobileNetV2, and the VisionTransformer (ViT) technique. Remarkably, our analysis demonstrates superioraccuracy rates, surpassing the 90% threshold with MobileNetV2 and achieving91.5% accuracy with ViT. Moreover, to enhance predictive capabilities further,we integrate these individual methodologies through ensemble modeling,resulting in our ensemble model exhibiting a remarkable 96% accuracy in theearly detection of CKD. This significant advancement holds immense promise forimproving clinical outcomes and underscores the critical role of machinelearning in addressing complex medical challenges.</description><author>Md. Arifuzzaman, Iftekhar Ahmed, Md. Jalal Uddin Chowdhury, Shadman Sakib, Mohammad Shoaib Rahman, Md. Ebrahim Hossain, Shakib Absar</author><pubDate>Thu, 12 Dec 2024 17:18:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09472v1</guid></item><item><title>Neural Network Symmetrisation in Concrete Settings</title><link>http://arxiv.org/abs/2412.09469v1</link><description>Cornish (2024) recently gave a general theory of neural networksymmetrisation in the abstract context of Markov categories. We give ahigh-level overview of these results, and their concrete implications for thesymmetrisation of deterministic functions and of Markov kernels.</description><author>Rob Cornish</author><pubDate>Thu, 12 Dec 2024 17:16:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09469v1</guid></item><item><title>Audios Don't Lie: Multi-Frequency Channel Attention Mechanism for Audio Deepfake Detection</title><link>http://arxiv.org/abs/2412.09467v1</link><description>With the rapid development of artificial intelligence technology, theapplication of deepfake technology in the audio field has gradually increased,resulting in a wide range of security risks. Especially in the financial andsocial security fields, the misuse of deepfake audios has raised seriousconcerns. To address this challenge, this study proposes an audio deepfakedetection method based on multi-frequency channel attention mechanism (MFCA)and 2D discrete cosine transform (DCT). By processing the audio signal into amelspectrogram, using MobileNet V2 to extract deep features, and combining itwith the MFCA module to weight different frequency channels in the audiosignal, this method can effectively capture the fine-grained frequency domainfeatures in the audio signal and enhance the Classification capability of fakeaudios. Experimental results show that compared with traditional methods, themodel proposed in this study shows significant advantages in accuracy,precision,recall, F1 score and other indicators. Especially in complex audioscenarios, this method shows stronger robustness and generalizationcapabilities and provides a new idea for audio deepfake detection and hasimportant practical application value. In the future, more advanced audiodetection technologies and optimization strategies will be explored to furtherimprove the accuracy and generalization capabilities of audio deepfakedetection.</description><author>Yangguang Feng</author><pubDate>Thu, 12 Dec 2024 17:15:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09467v1</guid></item><item><title>STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading</title><link>http://arxiv.org/abs/2412.09468v1</link><description>In financial trading, factor models are widely used to price assets andcapture excess returns from mispricing. Recently, we have witnessed the rise ofvariational autoencoder-based latent factor models, which learn latent factorsself-adaptively. While these models focus on modeling overall marketconditions, they often fail to effectively capture the temporal patterns ofindividual stocks. Additionally, representing multiple factors as single valuessimplifies the model but limits its ability to capture complex relationshipsand dependencies. As a result, the learned factors are of low quality and lackdiversity, reducing their effectiveness and robustness across different tradingperiods. To address these issues, we propose a Spatio-Temporal factOR Modelbased on dual vector quantized variational autoencoders, named STORM, whichextracts features of stocks from temporal and spatial perspectives, then fusesand aligns these features at the fine-grained and semantic level, andrepresents the factors as multi-dimensional embeddings. The discrete codebookscluster similar factor embeddings, ensuring orthogonality and diversity, whichhelps distinguish between different factors and enables factor selection infinancial trading. To show the performance of the proposed factor model, weapply it to two downstream experiments: portfolio management on two stockdatasets and individual trading tasks on six specific stocks. The extensiveexperiments demonstrate STORM's flexibility in adapting to downstream tasks andsuperior performance over baseline models.</description><author>Yilei Zhao, Wentao Zhang, Tingran Yang, Yong Jiang, Fei Huang, Wei Yang Bryan Lim</author><pubDate>Thu, 12 Dec 2024 17:15:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09468v1</guid></item><item><title>OFTSR: One-Step Flow for Image Super-Resolution with Tunable Fidelity-Realism Trade-offs</title><link>http://arxiv.org/abs/2412.09465v1</link><description>Recent advances in diffusion and flow-based generative models havedemonstrated remarkable success in image restoration tasks, achieving superiorperceptual quality compared to traditional deep learning approaches. However,these methods either require numerous sampling steps to generate high-qualityimages, resulting in significant computational overhead, or rely on modeldistillation, which usually imposes a fixed fidelity-realism trade-off and thuslacks flexibility. In this paper, we introduce OFTSR, a novel flow-basedframework for one-step image super-resolution that can produce outputs withtunable levels of fidelity and realism. Our approach first trains a conditionalflow-based super-resolution model to serve as a teacher model. We then distillthis teacher model by applying a specialized constraint. Specifically, we forcethe predictions from our one-step student model for same input to lie on thesame sampling ODE trajectory of the teacher model. This alignment ensures thatthe student model's single-step predictions from initial states match theteacher's predictions from a closer intermediate state. Through extensiveexperiments on challenging datasets including FFHQ (256$\times$256), DIV2K, andImageNet (256$\times$256), we demonstrate that OFTSR achieves state-of-the-artperformance for one-step image super-resolution, while having the ability toflexibly tune the fidelity-realism trade-off. Code and pre-trained models areavailable at https://github.com/yuanzhi-zhu/OFTSR andhttps://huggingface.co/Yuanzhi/OFTSR, respectively.</description><author>Yuanzhi Zhu, Ruiqing Wang, Shilin Lu, Junnan Li, Hanshu Yan, Kai Zhang</author><pubDate>Thu, 12 Dec 2024 17:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09465v1</guid></item><item><title>Autonomous Goal Detection and Cessation in Reinforcement Learning: A Case Study on Source Term Estimation</title><link>http://arxiv.org/abs/2409.09541v3</link><description>Reinforcement Learning has revolutionized decision-making processes indynamic environments, yet it often struggles with autonomously detecting andachieving goals without clear feedback signals. For example, in a Source TermEstimation problem, the lack of precise environmental information makes itchallenging to provide clear feedback signals and to define and evaluate howthe source's location is determined. To address this challenge, the AutonomousGoal Detection and Cessation (AGDC) module was developed, enhancing various RLalgorithms by incorporating a self-feedback mechanism for autonomous goaldetection and cessation upon task completion. Our method effectively identifiesand ceases undefined goals by approximating the agent's belief, significantlyenhancing the capabilities of RL algorithms in environments with limitedfeedback. To validate effectiveness of our approach, we integrated AGDC withdeep Q-Network, proximal policy optimization, and deep deterministic policygradient algorithms, and evaluated its performance on the Source TermEstimation problem. The experimental results showed that AGDC-enhanced RLalgorithms significantly outperformed traditional statistical methods such asinfotaxis, entrotaxis, and dual control for exploitation and exploration, aswell as a non-statistical random action selection method. These improvementswere evident in terms of success rate, mean traveled distance, and search time,highlighting AGDC's effectiveness and efficiency in complex, real-worldscenarios.</description><author>Yiwei Shi, Muning Wen, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu</author><pubDate>Thu, 12 Dec 2024 17:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09541v3</guid></item><item><title>The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective</title><link>http://arxiv.org/abs/2412.09460v1</link><description>The use of copyrighted materials in training generative language modelsraises critical legal and ethical questions. This paper presents a frameworkfor and the results of empirically assessing the impact of copyrightedmaterials on the performance of large language models (LLMs) for Norwegian. Wefound that both books and newspapers contribute positively when the models areevaluated on a diverse set of Norwegian benchmarks, while fiction workspossibly lead to decreased performance. Our experiments could inform thecreation of a compensation scheme for authors whose works contribute to AIdevelopment.</description><author>Javier de la Rosa, Vladislav Mikhailov, Lemei Zhang, Freddy Wetjen, David Samuel, Peng Liu, Rolv-Arild Braaten, Petter M√¶hlum, Magnus Breder Birkenes, Andrey Kutuzov, Tita Enstad, Svein Arne Brygfjeld, Jon Atle Gulla, Stephan Oepen, Erik Velldal, Wilfred √òstgulen, Liljia √òvrelid, Aslak Sira Myhre</author><pubDate>Thu, 12 Dec 2024 17:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09460v1</guid></item><item><title>Finite-PINN: A Physics-Informed Neural Network Architecture for Solving Solid Mechanics Problems with General Geometries</title><link>http://arxiv.org/abs/2412.09453v1</link><description>PINN models have demonstrated impressive capabilities in addressing fluid PDEproblems, and their potential in solid mechanics is beginning to emerge. Thisstudy identifies two key challenges when using PINN to solve general solidmechanics problems. These challenges become evident when comparing thelimitations of PINN with the well-established numerical methods commonly usedin solid mechanics, such as the finite element method (FEM). Specifically: a)PINN models generate solutions over an infinite domain, which conflicts withthe finite boundaries typical of most solid structures; and b) the solutionspace utilised by PINN is Euclidean, which is inadequate for addressing thecomplex geometries often present in solid structures. This work proposes a PINN architecture used for general solid mechanicsproblems, termed the Finite-PINN model. The proposed model aims to effectivelyaddress these two challenges while preserving as much of the originalimplementation of PINN as possible. The unique architecture of the Finite-PINNmodel addresses these challenges by separating the approximation of stress anddisplacement fields, and by transforming the solution space from thetraditional Euclidean space to a Euclidean-topological joint space. Severalcase studies presented in this paper demonstrate that the Finite-PINN modelprovides satisfactory results for a variety of problem types, including bothforward and inverse problems, in both 2D and 3D contexts. The developedFinite-PINN model offers a promising tool for addressing general solidmechanics problems, particularly those not yet well-explored in currentresearch.</description><author>Haolin Li, Yuyang Miao, Zahra Sharif Khodaei, M. H. Aliabadi</author><pubDate>Thu, 12 Dec 2024 17:06:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09453v1</guid></item><item><title>FullStack Bench: Evaluating LLMs as Full Stack Coders</title><link>http://arxiv.org/abs/2412.00535v4</link><description>As the capabilities of code large language models (LLMs) continue to expand,their applications across diverse code intelligence domains are rapidlyincreasing. However, most existing datasets only evaluate limited applicationdomains. To address this gap, we have developed a comprehensive code evaluationdataset FullStack Bench focusing on full-stack programming, which encompasses awide range of application domains (e.g., basic programming, data analysis,software engineering, mathematics, and machine learning). Besides, to assessmultilingual programming capabilities, in FullStack Bench, we design real-worldinstructions and corresponding unit test cases from 16 widely-used programminglanguages to reflect real-world usage scenarios rather than simpletranslations. Moreover, we also release an effective code sandbox executiontool (i.e., SandboxFusion) supporting various programming languages andpackages to evaluate the performance of our FullStack Bench efficiently.Comprehensive experimental results on our FullStack Bench demonstrate thenecessity and effectiveness of our FullStack Bench and SandboxFusion.</description><author>Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, Z. Y. Peng, Shukai Liu, Zhaoxiang Zhang, Jing Mai, Ge Zhang, Wenhao Huang, Kai Shen, Liang Xiang</author><pubDate>Thu, 12 Dec 2024 16:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00535v4</guid></item><item><title>Embeddings are all you need! Achieving High Performance Medical Image Classification through Training-Free Embedding Analysis</title><link>http://arxiv.org/abs/2412.09445v1</link><description>Developing artificial intelligence (AI) and machine learning (ML) models formedical imaging typically involves extensive training and testing on largedatasets, consuming significant computational time, energy, and resources.There is a need for more efficient methods that can achieve comparable orsuperior diagnostic performance without the associated resource burden. Weinvestigated the feasibility of replacing conventional training procedures withan embedding-based approach that leverages concise and semantically meaningfulrepresentations of medical images. Using pre-trained foundationalmodels-specifically, convolutional neural networks (CNN) like ResNet andmultimodal models like Contrastive Language-Image Pre-training (CLIP)-wegenerated image embeddings for multi-class classification tasks. Simple linearclassifiers were then applied to these embeddings. The approach was evaluatedacross diverse medical imaging modalities, including retinal images,mammography, dermatoscopic images, and chest radiographs. Performance wascompared to benchmark models trained and tested using traditional methods. Theembedding-based models surpassed the benchmark area under the receiveroperating characteristic curve (AUC-ROC) scores by up to 87 percentage inmulti-class classification tasks across the various medical imaging modalities.Notably, CLIP embedding models achieved the highest AUC-ROC scores,demonstrating superior classification performance while significantly reducingcomputational demands. Our study indicates that leveraging embeddings frompre-trained foundational models can effectively replace conventional,resource-intensive training and testing procedures in medical image analysis.This embedding-based approach offers a more efficient alternative for imagesegmentation, classification, and prediction, potentially accelerating AItechnology integration into clinical practice.</description><author>Raj Hansini Khoiwal, Alan B. McMillan</author><pubDate>Thu, 12 Dec 2024 16:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09445v1</guid></item><item><title>Search Strategy Generation for Branch and Bound Using Genetic Programming</title><link>http://arxiv.org/abs/2412.09444v1</link><description>Branch-and-Bound (B\&amp;B) is an exact method in integer programming thatrecursively divides the search space into a tree. During the resolutionprocess, determining the next subproblem to explore within the tree-known asthe search strategy-is crucial. Hand-crafted heuristics are commonly used, butnone are effective over all problem classes. Recent approaches utilizing neuralnetworks claim to make more intelligent decisions but are computationallyexpensive. In this paper, we introduce GP2S (Genetic Programming for SearchStrategy), a novel machine learning approach that automatically generates aB\&amp;B search strategy heuristic, aiming to make intelligent decisions whilebeing computationally lightweight. We define a policy as a function thatevaluates the quality of a B\&amp;B node by combining features from the node andthe problem; the search strategy policy is then defined by a best-first searchbased on this node ranking. The policy space is explored using a geneticprogramming algorithm, and the policy that achieves the best performance on atraining set is selected. We compare our approach with the standard method ofthe SCIP solver, a recent graph neural network-based method, and handcraftedheuristics. Our first evaluation includes three types of primal hard problems,tested on instances similar to the training set and on larger instances. Ourmethod is at most 2\% slower than the best baseline and consistentlyoutperforms SCIP, achieving an average speedup of 11.3\%. Additionally, GP2S istested on the MIPLIB 2017 dataset, generating multiple heuristics fromdifferent subsets of instances. It exceeds SCIP's average performance in 7 outof 10 cases across 15 times more instances and under a time limit 15 timeslonger, with some GP2S methods leading on most experiments in terms of thenumber of feasible solutions or optimality gap.</description><author>Gwen Maudet, Gr√©goire Danoy</author><pubDate>Thu, 12 Dec 2024 16:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09444v1</guid></item><item><title>MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental Learning</title><link>http://arxiv.org/abs/2412.09441v1</link><description>Class-Incremental Learning (CIL) requires models to continually acquireknowledge of new classes without forgetting old ones. Despite Pre-trainedModels (PTMs) have shown excellent performance in CIL, catastrophic forgettingstill occurs as the model learns new concepts. Existing work seeks to utilizelightweight components to adjust the PTM, while the forgetting phenomenon stillcomes from {\em parameter and retrieval} levels. Specifically, iterativeupdates of the model result in parameter drift, while mistakenly retrievingirrelevant modules leads to the mismatch during inference. To this end, wepropose MOdel Surgery (MOS) to rescue the model from forgetting previousknowledge. By training task-specific adapters, we continually adjust the PTM todownstream tasks. To mitigate parameter-level forgetting, we present an adaptermerging approach to learn task-specific adapters, which aims to bridge the gapbetween different components while reserve task-specific information. Besides,to address retrieval-level forgetting, we introduce a training-freeself-refined adapter retrieval mechanism during inference, which leverages themodel's inherent ability for better adapter retrieval. By jointly rectifyingthe model with those steps, MOS can robustly resist catastrophic forgetting inthe learning process. Extensive experiments on seven benchmark datasetsvalidate MOS's state-of-the-art performance. Code is available at:https://github.com/sun-hailong/AAAI25-MOS</description><author>Hai-Long Sun, Da-Wei Zhou, Hanbin Zhao, Le Gan, De-Chuan Zhan, Han-Jia Ye</author><pubDate>Thu, 12 Dec 2024 16:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09441v1</guid></item><item><title>ATPrompt: Textual Prompt Learning with Embedded Attributes</title><link>http://arxiv.org/abs/2412.09442v1</link><description>Textual-based prompt learning methods primarily employ multiple learnablesoft prompts and hard class tokens in a cascading manner as text prompt inputs,aiming to align image and text (category) spaces for downstream tasks. However,current training is restricted to aligning images with predefined knowncategories and cannot be associated with unknown categories. In this work, wepropose utilizing universal attributes as a bridge to enhance the alignmentbetween images and unknown categories. Specifically, we introduce anAttribute-embedded Textual Prompt learning method for vision-language models,named ATPrompt. This approach expands the learning space of soft prompts fromthe original one-dimensional category level into the multi-dimensionalattribute level by incorporating multiple universal attribute tokens into thelearnable soft prompts. Through this modification, we transform the text promptfrom a category-centric form to an attribute-category hybrid form. To finalizethe attributes for downstream tasks, we propose a differentiable attributesearch method that learns to identify representative and suitable attributesfrom a candidate pool summarized by a large language model. As an easy-to-useplug-in technique, ATPrompt can seamlessly replace the existing prompt formatof textual-based methods, offering general improvements at a negligiblecomputational cost. Extensive experiments on 11 datasets demonstrate theeffectiveness of our method.</description><author>Zheng Li, Yibing Song, Penghai Zhao, Ming-Ming Cheng, Xiang Li, Jian Yang</author><pubDate>Thu, 12 Dec 2024 16:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09442v1</guid></item></channel></rss>