<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 02 Aug 2023 06:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Variational Diffusion Models 2.0: Understanding Diffusion Model Objectives as the ELBO with Simple Data Augmentation</title><link>http://arxiv.org/abs/2303.00848v5</link><description>To achieve the highest perceptual quality, state-of-the-art diffusion modelsare optimized with objectives that look very different from the maximumlikelihood and the Evidence Lower Bound (ELBO) objectives. In this work, wereveal that diffusion model objectives are actually closely related to theELBO. Specifically, we show that all commonly used diffusion model objectivesequate to a weighted integral of ELBOs over different noise levels, where theweighting depends on the specific objective used. Under the condition ofmonotonic weighting, the connection is even closer: the diffusion objectivethen equals the ELBO, combined with simple data augmentation, namely Gaussiannoise perturbation. We show that this condition holds for a number ofstate-of-the-art diffusion models. In experiments, we explore new monotonic weightings and demonstrate theireffectiveness, achieving state-of-the-art FID scores on the high-resolutionImageNet benchmark.</description><author>Diederik P. Kingma, Ruiqi Gao</author><pubDate>Tue, 01 Aug 2023 18:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00848v5</guid></item><item><title>LISA: Reasoning Segmentation via Large Language Model</title><link>http://arxiv.org/abs/2308.00692v1</link><description>Although perception systems have made remarkable advancements in recentyears, they still rely on explicit human instruction to identify the targetobjects or categories before executing visual recognition tasks. Such systemslack the ability to actively reason and comprehend implicit user intentions. Inthis work, we propose a new segmentation task -- reasoning segmentation. Thetask is designed to output a segmentation mask given a complex and implicitquery text. Furthermore, we establish a benchmark comprising over one thousandimage-instruction pairs, incorporating intricate reasoning and world knowledgefor evaluation purposes. Finally, we present LISA: large Language InstructedSegmentation Assistant, which inherits the language generation capabilities ofthe multi-modal Large Language Model (LLM) while also possessing the ability toproduce segmentation masks. We expand the original vocabulary with a &lt;SEG&gt;token and propose the embedding-as-mask paradigm to unlock the segmentationcapability. Remarkably, LISA can handle cases involving: 1) complex reasoning;2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also,it demonstrates robust zero-shot capability when trained exclusively onreasoning-free datasets. In addition, fine-tuning the model with merely 239reasoning segmentation image-instruction pairs results in further performanceenhancement. Experiments show our method not only unlocks new reasoningsegmentation capabilities but also proves effective in both complex reasoningsegmentation and standard referring segmentation tasks. Code, models, and demoare at https://github.com/dvlab-research/LISA.</description><author>Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia</author><pubDate>Tue, 01 Aug 2023 18:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00692v1</guid></item><item><title>AnyLoc: Towards Universal Visual Place Recognition</title><link>http://arxiv.org/abs/2308.00688v1</link><description>Visual Place Recognition (VPR) is vital for robot localization. To date, themost performant VPR approaches are environment- and task-specific: while theyexhibit strong performance in structured environments (predominantly urbandriving), their performance degrades severely in unstructured environments,rendering most approaches brittle to robust real-world deployment. In thiswork, we develop a universal solution to VPR -- a technique that works across abroad range of structured and unstructured environments (urban, outdoors,indoors, aerial, underwater, and subterranean environments) without anyre-training or fine-tuning. We demonstrate that general-purpose featurerepresentations derived from off-the-shelf self-supervised models with noVPR-specific training are the right substrate upon which to build such auniversal VPR solution. Combining these derived features with unsupervisedfeature aggregation enables our suite of methods, AnyLoc, to achieve up to 4Xsignificantly higher performance than existing approaches. We further obtain a6% improvement in performance by characterizing the semantic properties ofthese features, uncovering unique domains which encapsulate datasets fromsimilar environments. Our detailed experiments and analysis lay a foundationfor building VPR solutions that may be deployed anywhere, anytime, and acrossanyview. We encourage the readers to explore our project page and interactivedemos: https://anyloc.github.io/.</description><author>Nikhil Keetha, Avneesh Mishra, Jay Karhade, Krishna Murthy Jatavallabhula, Sebastian Scherer, Madhava Krishna, Sourav Garg</author><pubDate>Tue, 01 Aug 2023 18:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00688v1</guid></item><item><title>Image Denoising and the Generative Accumulation of Photons</title><link>http://arxiv.org/abs/2307.06607v2</link><description>We present a fresh perspective on shot noise corrupted images and noiseremoval. By viewing image formation as the sequential accumulation of photonson a detector grid, we show that a network trained to predict where the nextphoton could arrive is in fact solving the minimum mean square error (MMSE)denoising task. This new perspective allows us to make three contributions: Wepresent a new strategy for self-supervised denoising, We present a new methodfor sampling from the posterior of possible solutions by iteratively samplingand adding small numbers of photons to the image. We derive a full generativemodel by starting this process from an empty canvas. We call this approachgenerative accumulation of photons (GAP). We evaluate our method quantitativelyand qualitatively on 4 new fluorescence microscopy datasets, which will be madeavailable to the community. We find that it outperforms supervised,self-supervised and unsupervised baselines or performs on-par.</description><author>Alexander Krull, Hector Basevi, Benjamin Salmon, Andre Zeug, Franziska Müller, Samuel Tonks, Leela Muppala, Ales Leonardis</author><pubDate>Tue, 01 Aug 2023 18:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06607v2</guid></item><item><title>The Current State of Summarization</title><link>http://arxiv.org/abs/2305.04853v2</link><description>With the explosive growth of textual information, summarization systems havebecome increasingly important. This work aims to concisely indicate the currentstate of the art in abstractive text summarization. As part of this, we outlinethe current paradigm shifts towards pre-trained encoder-decoder models andlarge autoregressive language models. Additionally, we delve further into thechallenges of evaluating summarization systems and the potential ofinstruction-tuned models for zero-shot summarization. Finally, we provide abrief overview of how summarization systems are currently being integrated intocommercial applications.</description><author>Fabian Retkowski</author><pubDate>Tue, 01 Aug 2023 18:42:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04853v2</guid></item><item><title>Learning from Hypervectors: A Survey on Hypervector Encoding</title><link>http://arxiv.org/abs/2308.00685v1</link><description>Hyperdimensional computing (HDC) is an emerging computing paradigm thatimitates the brain's structure to offer a powerful and efficient processing andlearning model. In HDC, the data are encoded with long vectors, calledhypervectors, typically with a length of 1K to 10K. The literature providesseveral encoding techniques to generate orthogonal or correlated hypervectors,depending on the intended application. The existing surveys in the literatureoften focus on the overall aspects of HDC systems, including system inputs,primary computations, and final outputs. However, this study takes a morespecific approach. It zeroes in on the HDC system input and the generation ofhypervectors, directly influencing the hypervector encoding process. Thissurvey brings together various methods for hypervector generation fromdifferent studies and explores the limitations, challenges, and potentialbenefits they entail. Through a comprehensive exploration of this survey,readers will acquire a profound understanding of various encoding types in HDCand gain insights into the intricate process of hypervector generation fordiverse applications.</description><author>Sercan Aygun, Mehran Shoushtari Moghadam, M. Hassan Najafi, Mohsen Imani</author><pubDate>Tue, 01 Aug 2023 18:42:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00685v1</guid></item><item><title>CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code</title><link>http://arxiv.org/abs/2308.00683v1</link><description>Recent works have widely adopted large language model pretraining for sourcecode, suggested source code-specific pretraining objectives and investigatedthe applicability of various Transformer-based language model architectures forsource code. This work investigates another important aspect of such models,namely the effect of different subtokenization options, and aims at identifyingmost effective and length-efficient subtokenizations, taking into account codespecifics. We propose subtokenziation that reduces average length by 17%without downstream performance drop, and show that a carefully chosensubtokenization may improve quality by 0.5-2%, possibly with some lengthincrease.</description><author>Nadezhda Chirkova, Sergey Troshin</author><pubDate>Tue, 01 Aug 2023 18:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00683v1</guid></item><item><title>Reinforcement Learning With Reward Machines in Stochastic Games</title><link>http://arxiv.org/abs/2305.17372v2</link><description>We investigate multi-agent reinforcement learning for stochastic games withcomplex tasks, where the reward functions are non-Markovian. We utilize rewardmachines to incorporate high-level knowledge of complex tasks. We develop analgorithm called Q-learning with reward machines for stochastic games (QRM-SG),to learn the best-response strategy at Nash equilibrium for each agent. InQRM-SG, we define the Q-function at a Nash equilibrium in augmented statespace. The augmented state space integrates the state of the stochastic gameand the state of reward machines. Each agent learns the Q-functions of allagents in the system. We prove that Q-functions learned in QRM-SG converge tothe Q-functions at a Nash equilibrium if the stage game at each time stepduring learning has a global optimum point or a saddle point, and the agentsupdate Q-functions based on the best-response strategy at this point. We usethe Lemke-Howson method to derive the best-response strategy given currentQ-functions. The three case studies show that QRM-SG can learn thebest-response strategies effectively. QRM-SG learns the best-responsestrategies after around 7500 episodes in Case Study I, 1000 episodes in CaseStudy II, and 1500 episodes in Case Study III, while baseline methods such asNash Q-learning and MADDPG fail to converge to the Nash equilibrium in allthree case studies.</description><author>Jueming Hu, Jean-Raphael Gaglione, Yanze Wang, Zhe Xu, Ufuk Topcu, Yongming Liu</author><pubDate>Tue, 01 Aug 2023 18:33:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17372v2</guid></item><item><title>Applicability of scaling laws to vision encoding models</title><link>http://arxiv.org/abs/2308.00678v1</link><description>In this paper, we investigated how to build a high-performance visionencoding model to predict brain activity as part of our participation in theAlgonauts Project 2023 Challenge. The challenge provided brain activityrecorded by functional MRI (fMRI) while participants viewed images. Severalvision models with parameter sizes ranging from 86M to 4.3B were used to buildpredictive models. To build highly accurate models, we focused our analysis ontwo main aspects: (1) How does the sample size of the fMRI training set changethe prediction accuracy? (2) How does the prediction accuracy across the visualcortex vary with the parameter size of the vision models? The results show thatas the sample size used during training increases, the prediction accuracyimproves according to the scaling law. Similarly, we found that as theparameter size of the vision models increases, the prediction accuracy improvesaccording to the scaling law. These results suggest that increasing the samplesize of the fMRI training set and the parameter size of visual models maycontribute to more accurate visual models of the brain and lead to a betterunderstanding of visual neuroscience.</description><author>Takuya Matsuyama, Kota S Sasaki, Shinji Nishimoto</author><pubDate>Tue, 01 Aug 2023 18:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00678v1</guid></item><item><title>The Intel Neuromorphic DNS Challenge</title><link>http://arxiv.org/abs/2303.09503v3</link><description>A critical enabler for progress in neuromorphic computing research is theability to transparently evaluate different neuromorphic solutions on importanttasks and to compare them to state-of-the-art conventional solutions. The IntelNeuromorphic Deep Noise Suppression Challenge (Intel N-DNS Challenge), inspiredby the Microsoft DNS Challenge, tackles a ubiquitous and commercially relevanttask: real-time audio denoising. Audio denoising is likely to reap the benefitsof neuromorphic computing due to its low-bandwidth, temporal nature and itsrelevance for low-power devices. The Intel N-DNS Challenge consists of twotracks: a simulation-based algorithmic track to encourage algorithmicinnovation, and a neuromorphic hardware (Loihi 2) track to rigorously evaluatesolutions. For both tracks, we specify an evaluation methodology based onenergy, latency, and resource consumption in addition to output audio quality.We make the Intel N-DNS Challenge dataset scripts and evaluation code freelyaccessible, encourage community participation with monetary prizes, and releasea neuromorphic baseline solution which shows promising audio quality, highpower efficiency, and low resource consumption when compared to MicrosoftNsNet2 and a proprietary Intel denoising model used in production. We hope theIntel N-DNS Challenge will hasten innovation in neuromorphic algorithmsresearch, especially in the area of training tools and methods for real-timesignal processing. We expect the winners of the challenge will demonstrate thatfor problems like audio denoising, significant gains in power and resources canbe realized on neuromorphic devices available today compared to conventionalstate-of-the-art solutions.</description><author>Jonathan Timcheck, Sumit Bam Shrestha, Daniel Ben Dayan Rubin, Adam Kupryjanow, Garrick Orchard, Lukasz Pindor, Timothy Shea, Mike Davies</author><pubDate>Tue, 01 Aug 2023 18:27:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09503v3</guid></item><item><title>Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models</title><link>http://arxiv.org/abs/2308.00675v1</link><description>Today, large language models (LLMs) are taught to use new tools by providinga few demonstrations of the tool's usage. Unfortunately, demonstrations arehard to acquire, and can result in undesirable biased usage if the wrongdemonstration is chosen. Even in the rare scenario that demonstrations arereadily available, there is no principled selection protocol to determine howmany and which ones to provide. As tasks grow more complex, the selectionsearch grows combinatorially and invariably becomes intractable. Our workprovides an alternative to demonstrations: tool documentation. We advocate theuse of tool documentation, descriptions for the individual tool usage, overdemonstrations. We substantiate our claim through three main empirical findingson 6 tasks across both vision and language modalities. First, on existingbenchmarks, zero-shot prompts with only tool documentation are sufficient foreliciting proper tool usage, achieving performance on par with few-shotprompts. Second, on a newly collected realistic tool-use dataset with hundredsof available tool APIs, we show that tool documentation is significantly morevaluable than demonstrations, with zero-shot documentation significantlyoutperforming few-shot without documentation. Third, we highlight the benefitsof tool documentations by tackling image generation and video tracking usingjust-released unseen state-of-the-art models as tools. Finally, we highlightthe possibility of using tool documentation to automatically enable newapplications: by using nothing more than the documentation of GroundingDino,Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of thejust-released Grounded-SAM and Track Anything models.</description><author>Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister</author><pubDate>Tue, 01 Aug 2023 18:21:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00675v1</guid></item><item><title>RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction</title><link>http://arxiv.org/abs/2210.09309v4</link><description>Automatic rib labeling and anatomical centerline extraction are commonprerequisites for various clinical applications. Prior studies either usein-house datasets that are inaccessible to communities, or focus on ribsegmentation that neglects the clinical significance of rib labeling. Toaddress these issues, we extend our prior dataset (RibSeg) on the binary ribsegmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CTscans (15,466 individual ribs in total) and annotations manually inspected byexperts for rib labeling and anatomical centerline extraction. Based on theRibSeg v2, we develop a pipeline including deep learning-based methods for riblabeling, and a skeletonization-based method for centerline extraction. Toimprove computational efficiency, we propose a sparse point cloudrepresentation of CT scans and compare it with standard dense voxel grids.Moreover, we design and analyze evaluation metrics to address the keychallenges of each task. Our dataset, code, and model are available online tofacilitate open research at https://github.com/M3DV/RibSeg</description><author>Liang Jin, Shixuan Gu, Donglai Wei, Jason Ken Adhinarta, Kaiming Kuang, Yongjie Jessica Zhang, Hanspeter Pfister, Bingbing Ni, Jiancheng Yang, Ming Li</author><pubDate>Tue, 01 Aug 2023 18:20:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.09309v4</guid></item><item><title>Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff</title><link>http://arxiv.org/abs/2301.09522v2</link><description>Spiking neural networks (SNNs), a variant of artificial neural networks(ANNs) with the benefit of energy efficiency, have achieved the accuracy closeto its ANN counterparts, on benchmark datasets such as CIFAR10/100 andImageNet. However, comparing with frame-based input (e.g., images), event-basedinputs from e.g., Dynamic Vision Sensor (DVS) can make a better use of SNNsthanks to the SNNs' asynchronous working mechanism. In this paper, westrengthen the marriage between SNNs and event-based inputs with a proposal toconsider anytime optimal inference SNNs, or AOI-SNNs, which can terminateanytime during the inference to achieve optimal inference result. Two noveloptimisation techniques are presented to achieve AOI-SNNs: a regularisation anda cutoff. The regularisation enables the training and construction of SNNs withoptimised performance, and the cutoff technique optimises the inference of SNNson event-driven inputs. We conduct an extensive set of experiments on multiplebenchmark event-based datasets, including CIFAR10-DVS, N-Caltech101 and DVS128Gesture. The experimental results demonstrate that our techniques are superiorto the state-of-the-art with respect to the accuracy and latency.</description><author>Dengyu Wu, Gaojie Jin, Han Yu, Xinping Yi, Xiaowei Huang</author><pubDate>Tue, 01 Aug 2023 18:15:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09522v2</guid></item><item><title>Decoupled Diffusion Models with Explicit Transition Probability</title><link>http://arxiv.org/abs/2306.13720v2</link><description>Recent diffusion probabilistic models (DPMs) have shown remarkable abilitiesof generated content, however, they often suffer from complex forwardprocesses, resulting in inefficient solutions for the reversed process andprolonged sampling times. In this paper, we aim to address the aforementionedchallenges by focusing on the diffusion process itself that we propose todecouple the intricate diffusion process into two comparatively simpler processto improve the generative efficacy and speed. In particular, we present a noveldiffusion paradigm named DDM (Decoupled Diffusion Models) based on the Itodiffusion process, in which the image distribution is approximated by anexplicit transition probability while the noise path is controlled by thestandard Wiener process. We find that decoupling the diffusion process reducesthe learning difficulty and the explicit transition probability improves thegenerative speed significantly. We prove a new training objective for DPM,which enables the model to learn to predict the noise and image componentsseparately. Moreover, given the novel forward diffusion equation, we derive thereverse denoising formula of DDM that naturally supports fewer steps ofgeneration without ordinary differential equation (ODE) based accelerators. Ourexperiments demonstrate that DDM outperforms previous DPMs by a large margin infewer function evaluations setting and gets comparable performances in longfunction evaluations setting. We also show that our framework can be applied toimage-conditioned generation and high-resolution image synthesis, and that itcan generate high-quality images with only 10 function evaluations.</description><author>Yuhang Huang, Zheng Qin, Xinwang Liu, Kai Xu</author><pubDate>Tue, 01 Aug 2023 18:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13720v2</guid></item><item><title>Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration</title><link>http://arxiv.org/abs/2307.16499v2</link><description>Tool use, a hallmark feature of human intelligence, remains a challengingproblem in robotics due the complex contacts and high-dimensional action space.In this work, we present a novel method to enable reinforcement learning oftool use behaviors. Our approach provides a scalable way to learn the operationof tools in a new category using only a single demonstration. To this end, wepropose a new method for generalizing grasping configurations of multi-fingeredrobotic hands to novel objects. This is used to guide the policy search viafavorable initializations and a shaped reward signal. The learned policiessolve complex tool use tasks and generalize to unseen tools at test time.Visualizations and videos of the trained policies are available athttps://maltemosbach.github.io/generalizable_tool_use.</description><author>Malte Mosbach, Sven Behnke</author><pubDate>Tue, 01 Aug 2023 17:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16499v2</guid></item><item><title>An Empirical Study on Bugs Inside PyTorch: A Replication Study</title><link>http://arxiv.org/abs/2307.13777v2</link><description>Software systems are increasingly relying on deep learning components, due totheir remarkable capability of identifying complex data patterns and poweringintelligent behaviour. A core enabler of this change in software development isthe availability of easy-to-use deep learning libraries. Libraries like PyTorchand TensorFlow empower a large variety of intelligent systems, offering amultitude of algorithms and configuration options, applicable to numerousdomains of systems. However, bugs in those popular deep learning libraries alsomay have dire consequences for the quality of systems they enable; thus, it isimportant to understand how bugs are identified and fixed in those libraries. Inspired by a study of Jia et al., which investigates the bug identificationand fixing process at TensorFlow, we characterize bugs in the PyTorch library,a very popular deep learning framework. We investigate the causes and symptomsof bugs identified during PyTorch's development, and assess their localitywithin the project, and extract patterns of bug fixes. Our results highlightthat PyTorch bugs are more like traditional software projects bugs, thanrelated to deep learning characteristics. Finally, we also compare our resultswith the study on TensorFlow, highlighting similarities and differences acrossthe bug identification and fixing process.</description><author>Sharon Chee Yin Ho, Vahid Majdinasab, Mohayeminul Islam, Diego Elias Costa, Emad Shihab, Foutse Khomh, Sarah Nadi, Muhammad Raza</author><pubDate>Tue, 01 Aug 2023 17:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13777v2</guid></item><item><title>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks</title><link>http://arxiv.org/abs/2307.02477v2</link><description>The impressive performance of recent language models across a wide range oftasks suggests that they possess a degree of abstract reasoning skills. Arethese skills general and transferable, or specialized to specific tasks seenduring pretraining? To disentangle these effects, we propose an evaluationframework based on "counterfactual" task variants that deviate from the defaultassumptions underlying standard tasks. Across a suite of 11 tasks, we observenontrivial performance on the counterfactual variants, but nevertheless findthat performance substantially and consistently degrades compared to thedefault conditions. This suggests that while current LMs may possess abstracttask-solving skills to a degree, they often also rely on narrow,non-transferable procedures for task-solving. These results motivate a morecareful interpretation of language model performance that teases apart theseaspects of behavior.</description><author>Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim</author><pubDate>Tue, 01 Aug 2023 17:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02477v2</guid></item><item><title>Parallel Context Windows for Large Language Models</title><link>http://arxiv.org/abs/2212.10947v3</link><description>When applied to processing long text, Large Language Models (LLMs) arelimited by their context window. Existing efforts to address this limitationinvolve training specialized architectures, and cannot be easily applied tooff-the-shelf LLMs. We present Parallel Context Windows (PCW), a method thatalleviates the context window restriction for any off-the-shelf LLM withoutfurther training. The key to the approach is to carve a long context intochunks (``windows''), restrict the attention mechanism to apply only withineach window, and re-use the positional embeddings across the windows. Our mainresults test the PCW approach on in-context learning with models that range insize between 750 million and 178 billion parameters, and show substantialimprovements for tasks with diverse input and output spaces. We show additionalbenefits in other settings where long context windows may be beneficial:multi-hop questions and retrieval-augmented question answering with multipleretrieved documents. Our results highlight Parallel Context Windows as apromising method for applying off-the-shelf LLMs in a range of settings thatrequire long text sequences. We make our code publicly available athttps://github.com/ai21labs/parallel-context-windows.</description><author>Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham</author><pubDate>Tue, 01 Aug 2023 17:48:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10947v3</guid></item><item><title>Toward Zero-shot Character Recognition: A Gold Standard Dataset with Radical-level Annotations</title><link>http://arxiv.org/abs/2308.00655v1</link><description>Optical character recognition (OCR) methods have been applied to diversetasks, e.g., street view text recognition and document analysis. Recently,zero-shot OCR has piqued the interest of the research community because itconsiders a practical OCR scenario with unbalanced data distribution. However,there is a lack of benchmarks for evaluating such zero-shot methods that applya divide-and-conquer recognition strategy by decomposing characters intoradicals. Meanwhile, radical recognition, as another important OCR task, alsolacks radical-level annotation for model training. In this paper, we constructan ancient Chinese character image dataset that contains both radical-level andcharacter-level annotations to satisfy the requirements of the above-mentionedmethods, namely, ACCID, where radical-level annotations include radicalcategories, radical locations, and structural relations. To increase theadaptability of ACCID, we propose a splicing-based synthetic characteralgorithm to augment the training samples and apply an image denoising methodto improve the image quality. By introducing character decomposition andrecombination, we propose a baseline method for zero-shot OCR. The experimentalresults demonstrate the validity of ACCID and the baseline model quantitativelyand qualitatively.</description><author>Xiaolei Diao, Daqian Shi, Jian Li, Lida Shi, Mingzhe Yue, Ruihua Qi, Chuntao Li, Hao Xu</author><pubDate>Tue, 01 Aug 2023 17:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00655v1</guid></item><item><title>Data-driven low-dimensional dynamic model of Kolmogorov flow</title><link>http://arxiv.org/abs/2210.16708v2</link><description>Reduced order models (ROMs) that capture flow dynamics are of interest fordecreasing computational costs for simulation as well as for model-basedcontrol approaches. This work presents a data-driven framework forminimal-dimensional models that effectively capture the dynamics and propertiesof the flow. We apply this to Kolmogorov flow in a regime consisting of chaoticand intermittent behavior, which is common in many flows processes and ischallenging to model. The trajectory of the flow travels near relative periodicorbits (RPOs), interspersed with sporadic bursting events corresponding toexcursions between the regions containing the RPOs. The first step indevelopment of the models is use of an undercomplete autoencoder to map fromthe full state data down to a latent space of dramatically lower dimension.Then models of the discrete-time evolution of the dynamics in the latent spaceare developed. By analyzing the model performance as a function of latent spacedimension we can estimate the minimum number of dimensions required to capturethe system dynamics. To further reduce the dimension of the dynamical model, wefactor out a phase variable in the direction of translational invariance forthe flow, leading to separate evolution equations for the pattern and phase. Ata model dimension of five for the pattern dynamics, as opposed to the fullstate dimension of 1024 (i.e. a 32x32 grid), accurate predictions are found forindividual trajectories out to about two Lyapunov times, as well as forlong-time statistics. Further small improvements in the results occur at adimension of nine. The nearly heteroclinic connections between the differentRPOs, including the quiescent and bursting time scales, are well captured. Wealso capture key features of the phase dynamics. Finally, we use thelow-dimensional representation to predict future bursting events, finding goodsuccess.</description><author>Carlos E. Pérez De Jesús, Michael D. Graham</author><pubDate>Tue, 01 Aug 2023 17:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16708v2</guid></item><item><title>FedGH: Heterogeneous Federated Learning with Generalized Global Header</title><link>http://arxiv.org/abs/2303.13137v2</link><description>Federated learning (FL) is an emerging machine learning paradigm that allowsmultiple parties to train a shared model collaboratively in aprivacy-preserving manner. Existing horizontal FL methods generally assume thatthe FL server and clients hold the same model structure. However, due to systemheterogeneity and the need for personalization, enabling clients to hold modelswith diverse structures has become an important direction. Existingmodel-heterogeneous FL approaches often require publicly available datasets andincur high communication and/or computational costs, which limit theirperformances. To address these limitations, we propose a simple but effectiveFederated Global prediction Header (FedGH) approach. It is a communication andcomputation-efficient model-heterogeneous FL framework which trains a sharedgeneralized global prediction header with representations extracted byheterogeneous extractors for clients' models at the FL server. The trainedgeneralized global prediction header learns from different clients. Theacquired global knowledge is then transferred to clients to substitute eachclient's local prediction header. We derive the non-convex convergence rate ofFedGH. Extensive experiments on two real-world datasets demonstrate that FedGHachieves significantly more advantageous performance in both model-homogeneousand -heterogeneous FL scenarios compared to seven state-of-the-art personalizedFL models, beating the best-performing baseline by up to 8.87% (formodel-homogeneous FL) and 1.83% (for model-heterogeneous FL) in terms ofaverage test accuracy, while saving up to 85.53% of communication overhead.</description><author>Liping Yi, Gang Wang, Xiaoguang Liu, Zhuan Shi, Han Yu</author><pubDate>Tue, 01 Aug 2023 17:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13137v2</guid></item><item><title>Seeing Behind Dynamic Occlusions with Event Cameras</title><link>http://arxiv.org/abs/2307.15829v2</link><description>Unwanted camera occlusions, such as debris, dust, rain-drops, and snow, canseverely degrade the performance of computer-vision systems. Dynamic occlusionsare particularly challenging because of the continuously changing pattern.Existing occlusion-removal methods currently use synthetic aperture imaging orimage inpainting. However, they face issues with dynamic occlusions as theserequire multiple viewpoints or user-generated masks to hallucinate thebackground intensity. We propose a novel approach to reconstruct the backgroundfrom a single viewpoint in the presence of dynamic occlusions. Our solutionrelies for the first time on the combination of a traditional camera with anevent camera. When an occlusion moves across a background image, it causesintensity changes that trigger events. These events provide additionalinformation on the relative intensity changes between foreground and backgroundat a high temporal resolution, enabling a truer reconstruction of thebackground content. We present the first large-scale dataset consisting ofsynchronized images and event sequences to evaluate our approach. We show thatour method outperforms image inpainting methods by 3dB in terms of PSNR on ourdataset.</description><author>Rong Zou, Manasi Muglikar, Nico Messikommer, Davide Scaramuzza</author><pubDate>Tue, 01 Aug 2023 17:18:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15829v2</guid></item><item><title>Spectral learning of multivariate extremes</title><link>http://arxiv.org/abs/2111.07799v3</link><description>We propose a spectral clustering algorithm for analyzing the dependencestructure of multivariate extremes. More specifically, we focus on theasymptotic dependence of multivariate extremes characterized by the angular orspectral measure in extreme value theory. Our work studies the theoreticalperformance of spectral clustering based on a random $k$-nearest neighbor graphconstructed from an extremal sample, i.e., the angular part of random vectorsfor which the radius exceeds a large threshold. In particular, we derive theasymptotic distribution of extremes arising from a linear factor model andprove that, under certain conditions, spectral clustering can consistentlyidentify the clusters of extremes arising in this model. Leveraging this resultwe propose a simple consistent estimation strategy for learning the angularmeasure. Our theoretical findings are complemented with numerical experimentsillustrating the finite sample performance of our methods.</description><author>Marco Avella Medina, Richard A. Davis, Gennady Samorodnitsky</author><pubDate>Tue, 01 Aug 2023 17:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.07799v3</guid></item><item><title>Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning</title><link>http://arxiv.org/abs/2307.15411v2</link><description>Large language models (LLMs) have shown remarkable capacity for in-contextlearning (ICL), where learning a new task from just a few training examples isdone without being explicitly pre-trained. However, despite the success ofLLMs, there has been little understanding of how ICL learns the knowledge fromthe given prompts. In this paper, to make progress toward understanding thelearning behaviour of ICL, we train the same LLMs with the same demonstrationexamples via ICL and supervised learning (SL), respectively, and investigatetheir performance under label perturbations (i.e., noisy labels and labelimbalance) on a range of classification tasks. First, via extensiveexperiments, we find that gold labels have significant impacts on thedownstream in-context performance, especially for large language models;however, imbalanced labels matter little to ICL across all model sizes. Second,when comparing with SL, we show empirically that ICL is less sensitive to labelperturbations than SL, and ICL gradually attains comparable performance to SLas the model size increases.</description><author>Xindi Wang, Yufei Wang, Can Xu, Xiubo Geng, Bowen Zhang, Chongyang Tao, Frank Rudzicz, Robert E. Mercer, Daxin Jiang</author><pubDate>Tue, 01 Aug 2023 17:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15411v2</guid></item><item><title>Hessian-Aware Bayesian Optimization for Decision Making Systems</title><link>http://arxiv.org/abs/2308.00629v1</link><description>Many approaches for optimizing decision making systems rely on gradient basedmethods requiring informative feedback from the environment. However, in thecase where such feedback is sparse or uninformative, such approaches may resultin poor performance. Derivative-free approaches such as Bayesian Optimizationmitigate the dependency on the quality of gradient feedback, but are known toscale poorly in the high-dimension setting of complex decision making systems.This problem is exacerbated if the system requires interactions between severalactors cooperating to accomplish a shared goal. To address the dimensionalitychallenge, we propose a compact multi-layered architecture modeling thedynamics of actor interactions through the concept of role. Additionally, weintroduce Hessian-aware Bayesian Optimization to efficiently optimize themulti-layered architecture parameterized by a large number of parameters.Experimental results demonstrate that our method (HA-GP-UCB) works effectivelyon several benchmarks under resource constraints and malformed feedbacksettings.</description><author>Mohit Rajpal, Lac Gia Tran, Yehong Zhang, Bryan Kian Hsiang Low</author><pubDate>Tue, 01 Aug 2023 16:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00629v1</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v1</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Tue, 01 Aug 2023 16:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v1</guid></item><item><title>JIANG: Chinese Open Foundation Language Model</title><link>http://arxiv.org/abs/2308.00624v1</link><description>With the advancements in large language model technology, it has showcasedcapabilities that come close to those of human beings across various tasks.This achievement has garnered significant interest from companies andscientific research institutions, leading to substantial investments in theresearch and development of these models. While numerous large models haveemerged during this period, the majority of them have been trained primarily onEnglish data. Although they exhibit decent performance in other languages, suchas Chinese, their potential remains limited due to factors like vocabularydesign and training corpus. Consequently, their ability to fully express theircapabilities in Chinese falls short. To address this issue, we introduce themodel named JIANG (Chinese pinyin of ginger) specifically designed for theChinese language. We have gathered a substantial amount of Chinese corpus totrain the model and have also optimized its structure. The extensiveexperimental results demonstrate the excellent performance of our model.</description><author>Qinhua Duan, Wenchao Gu, Yujia Chen, Wenxin Mao, Zewen Tian, Hui Cao</author><pubDate>Tue, 01 Aug 2023 16:51:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00624v1</guid></item><item><title>NeRT: Implicit Neural Representations for General Unsupervised Turbulence Mitigation</title><link>http://arxiv.org/abs/2308.00622v1</link><description>The atmospheric and water turbulence mitigation problems have emerged aschallenging inverse problems in computer vision and optics communities over theyears. However, current methods either rely heavily on the quality of thetraining dataset or fail to generalize over various scenarios, such as staticscenes, dynamic scenes, and text reconstructions. We propose a general implicitneural representation for unsupervised atmospheric and water turbulencemitigation (NeRT). NeRT leverages the implicit neural representations and thephysically correct tilt-then-blur turbulence model to reconstruct the clean,undistorted image, given only dozens of distorted input images. Moreover, weshow that NeRT outperforms the state-of-the-art through various qualitative andquantitative evaluations of atmospheric and water turbulence datasets.Furthermore, we demonstrate the ability of NeRT to eliminate uncontrolledturbulence from real-world environments. Lastly, we incorporate NeRT intocontinuously captured video sequences and demonstrate $48 \times$ speedup.</description><author>Weiyun Jiang, Vivek Boominathan, Ashok Veeraraghavan</author><pubDate>Tue, 01 Aug 2023 16:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00622v1</guid></item><item><title>Learning Graphical Factor Models with Riemannian Optimization</title><link>http://arxiv.org/abs/2210.11950v2</link><description>Graphical models and factor analysis are well-established tools inmultivariate statistics. While these models can be both linked to structuresexhibited by covariance and precision matrices, they are generally not jointlyleveraged within graph learning processes. This paper therefore addresses thisissue by proposing a flexible algorithmic framework for graph learning underlow-rank structural constraints on the covariance matrix. The problem isexpressed as penalized maximum likelihood estimation of an ellipticaldistribution (a generalization of Gaussian graphical models to possiblyheavy-tailed distributions), where the covariance matrix is optionallyconstrained to be structured as low-rank plus diagonal (low-rank factor model).The resolution of this class of problems is then tackled with Riemannianoptimization, where we leverage geometries of positive definite matrices andpositive semi-definite matrices of fixed rank that are well suited toelliptical models. Numerical experiments on real-world data sets illustrate theeffectiveness of the proposed approach.</description><author>Alexandre Hippert-Ferrer, Florent Bouchard, Ammar Mian, Titouan Vayer, Arnaud Breloy</author><pubDate>Tue, 01 Aug 2023 16:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.11950v2</guid></item><item><title>FAIR for AI: An interdisciplinary and international community building perspective</title><link>http://arxiv.org/abs/2210.08973v2</link><description>A foundational set of findable, accessible, interoperable, and reusable(FAIR) principles were proposed in 2016 as prerequisites for proper datamanagement and stewardship, with the goal of enabling the reusability ofscholarly data. The principles were also meant to apply to other digitalassets, at a high level, and over time, the FAIR guiding principles have beenre-interpreted or extended to include the software, tools, algorithms, andworkflows that produce data. FAIR principles are now being adapted in thecontext of AI models and datasets. Here, we present the perspectives, vision,and experiences of researchers from different countries, disciplines, andbackgrounds who are leading the definition and adoption of FAIR principles intheir communities of practice, and discuss outcomes that may result frompursuing and incentivizing FAIR AI research. The material for this reportbuilds on the FAIR for AI Workshop held at Argonne National Laboratory on June7, 2022.</description><author>E. A. Huerta, Ben Blaiszik, L. Catherine Brinson, Kristofer E. Bouchard, Daniel Diaz, Caterina Doglioni, Javier M. Duarte, Murali Emani, Ian Foster, Geoffrey Fox, Philip Harris, Lukas Heinrich, Shantenu Jha, Daniel S. Katz, Volodymyr Kindratenko, Christine R. Kirkpatrick, Kati Lassila-Perini, Ravi K. Madduri, Mark S. Neubauer, Fotis E. Psomopoulos, Avik Roy, Oliver Rübel, Zhizhen Zhao, Ruike Zhu</author><pubDate>Tue, 01 Aug 2023 16:40:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.08973v2</guid></item><item><title>Explainable Cost-Sensitive Deep Neural Networks for Brain Tumor Detection from Brain MRI Images considering Data Imbalance</title><link>http://arxiv.org/abs/2308.00608v1</link><description>This paper presents a research study on the use of Convolutional NeuralNetwork (CNN), ResNet50, InceptionV3, EfficientNetB0 and NASNetMobile models toefficiently detect brain tumors in order to reduce the time required for manualreview of the report and create an automated system for classifying braintumors. An automated pipeline is proposed, which encompasses five models: CNN,ResNet50, InceptionV3, EfficientNetB0 and NASNetMobile. The performance of theproposed architecture is evaluated on a balanced dataset and found to yield anaccuracy of 99.33% for fine-tuned InceptionV3 model. Furthermore, ExplainableAI approaches are incorporated to visualize the model's latent behavior inorder to understand its black box behavior. To further optimize the trainingprocess, a cost-sensitive neural network approach has been proposed in order towork with imbalanced datasets which has achieved almost 4% more accuracy thanthe conventional models used in our experiments. The cost-sensitive InceptionV3(CS-InceptionV3) and CNN (CS-CNN) show a promising accuracy of 92.31% and arecall value of 1.00 respectively on an imbalanced dataset. The proposed modelshave shown great potential in improving tumor detection accuracy and must befurther developed for application in practical solutions. We have provided thedatasets and made our implementations publicly available at -https://github.com/shahariar-shibli/Explainable-Cost-Sensitive-Deep-Neural-Networks-for-Brain-Tumor-Detection-from-Brain-MRI-Images</description><author>Md Tanvir Rouf Shawon, G. M. Shahariar Shibli, Farzad Ahmed, Sajib Kumar Saha Joy</author><pubDate>Tue, 01 Aug 2023 16:35:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00608v1</guid></item><item><title>Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers</title><link>http://arxiv.org/abs/2308.00607v1</link><description>Images are loaded with semantic information that pertains to real-worldontologies: dog breeds share mammalian similarities, food pictures are oftendepicted in domestic environments, and so on. However, when training machinelearning models for image classification, the relative similarities amongstobject classes are commonly paired with one-hot-encoded labels. According tothis logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark'are equally wrong in terms of training loss. To overcome this limitation, weexplore the integration of additional goals that reflect ontological andsemantic knowledge, improving model interpretability and trustworthiness. Wesuggest a generic approach that allows to derive an additional loss termstarting from any kind of semantic information about the classification label.First, we show how to apply our approach to ontologies and word embeddings, anddiscuss how the resulting information can drive a supervised learning process.Second, we use our semantically enriched loss to train image classifiers, andanalyse the trade-offs between accuracy, mistake severity, and learned internalrepresentations. Finally, we discuss how this approach can be further exploitedin terms of explainability and adversarial robustness. Code repository:https://github.com/S1M0N38/semantic-encodings</description><author>Alan Perotti, Simone Bertolotto, Eliana Pastor, André Panisson</author><pubDate>Tue, 01 Aug 2023 16:34:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00607v1</guid></item><item><title>Phase Matching for Out-of-Distribution Generalization</title><link>http://arxiv.org/abs/2307.12622v3</link><description>The Fourier transform, serving as an explicit decomposition method for visualsignals, has been employed to explain the out-of-distribution generalizationbehaviors of Convolutional Neural Networks (CNNs). Previous studies haveindicated that the amplitude spectrum is susceptible to the disturbance causedby distribution shifts. On the other hand, the phase spectrum preserveshighly-structured spatial information, which is crucial for robust visualrepresentation learning. However, the spatial relationships of phase spectrumremain unexplored in previous researches. In this paper, we aim to clarify therelationships between Domain Generalization (DG) and the frequency components,and explore the spatial relationships of the phase spectrum. Specifically, wefirst introduce a Fourier-based structural causal model which interprets thephase spectrum as semi-causal factors and the amplitude spectrum as non-causalfactors. Then, we propose Phase Matching (PhaMa) to address DG problems. Ourmethod introduces perturbations on the amplitude spectrum and establishesspatial relationships to match the phase components. Through experiments onmultiple benchmarks, we demonstrate that our proposed method achievesstate-of-the-art performance in domain generalization and out-of-distributionrobustness tasks.</description><author>Chengming Hu, Yeqian Du, Rui Wang, Hao Chen</author><pubDate>Tue, 01 Aug 2023 16:23:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12622v3</guid></item><item><title>Towards Multi-Agent Reinforcement Learning driven Over-The-Counter Market Simulations</title><link>http://arxiv.org/abs/2210.07184v2</link><description>We study a game between liquidity provider and liquidity taker agentsinteracting in an over-the-counter market, for which the typical example isforeign exchange. We show how a suitable design of parameterized families ofreward functions coupled with shared policy learning constitutes an efficientsolution to this problem. By playing against each other, ourdeep-reinforcement-learning-driven agents learn emergent behaviors relative toa wide spectrum of objectives encompassing profit-and-loss, optimal executionand market share. In particular, we find that liquidity providers naturallylearn to balance hedging and skewing, where skewing refers to setting their buyand sell prices asymmetrically as a function of their inventory. We furtherintroduce a novel RL-based calibration algorithm which we found performed wellat imposing constraints on the game equilibrium. On the theoretical side, weare able to show convergence rates for our multi-agent policy gradientalgorithm under a transitivity assumption, closely related to generalizedordinal potential games.</description><author>Nelson Vadori, Leo Ardon, Sumitra Ganesh, Thomas Spooner, Selim Amrouni, Jared Vann, Mengda Xu, Zeyu Zheng, Tucker Balch, Manuela Veloso</author><pubDate>Tue, 01 Aug 2023 16:22:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07184v2</guid></item><item><title>Benchmarking Compositionality with Formal Languages</title><link>http://arxiv.org/abs/2208.08195v3</link><description>Recombining known primitive concepts into larger novel combinations is aquintessentially human cognitive capability. Whether large neural models in NLPcan acquire this ability while learning from data is an open question. In thispaper, we investigate this problem from the perspective of formal languages. Weuse deterministic finite-state transducers to make an unbounded number ofdatasets with controllable properties governing compositionality. By randomlysampling over many transducers, we explore which of their properties contributeto learnability of a compositional relation by a neural network. We find thatthe models either learn the relations completely or not at all. The key istransition coverage, setting a soft learnability limit at 400 examples pertransition.</description><author>Josef Valvoda, Naomi Saphra, Jonathan Rawski, Adina Williams, Ryan Cotterell</author><pubDate>Tue, 01 Aug 2023 16:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.08195v3</guid></item><item><title>mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection</title><link>http://arxiv.org/abs/2303.09901v3</link><description>This paper presents the winning system for the zero-shot Spanish framingdetection task, which also achieves competitive places in eight additionallanguages. The challenge of the framing detection task lies in identifying aset of 14 frames when only a few or zero samples are available, i.e., amultilingual multi-label few- or zero-shot setting. Our developed solutionemploys a pre-training procedure based on multilingual Transformers using alabel-aware contrastive loss function. In addition to describing the system, weperform an embedding space analysis and ablation study to demonstrate how ourpre-training procedure supports framing detection to advance computationalframing analysis.</description><author>Markus Reiter-Haas, Alexander Ertl, Kevin Innerebner, Elisabeth Lex</author><pubDate>Tue, 01 Aug 2023 16:16:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09901v3</guid></item><item><title>MonoNext: A 3D Monocular Object Detection with ConvNext</title><link>http://arxiv.org/abs/2308.00596v1</link><description>Autonomous driving perception tasks rely heavily on cameras as the primarysensor for Object Detection, Semantic Segmentation, Instance Segmentation, andObject Tracking. However, RGB images captured by cameras lack depthinformation, which poses a significant challenge in 3D detection tasks. Tosupplement this missing data, mapping sensors such as LIDAR and RADAR are usedfor accurate 3D Object Detection. Despite their significant accuracy, themulti-sensor models are expensive and require a high computational demand. Incontrast, Monocular 3D Object Detection models are becoming increasinglypopular, offering a faster, cheaper, and easier-to-implement solution for 3Ddetections. This paper introduces a different Multi-Tasking Learning approachcalled MonoNext that utilizes a spatial grid to map objects in the scene.MonoNext employs a straightforward approach based on the ConvNext network andrequires only 3D bounding box annotated data. In our experiments with the KITTIdataset, MonoNext achieved high precision and competitive performancecomparable with state-of-the-art approaches. Furthermore, by adding moretraining data, MonoNext surpassed itself and achieved higher accuracies.</description><author>Marcelo Eduardo Pederiva, José Mario De Martino, Alessandro Zimmer</author><pubDate>Tue, 01 Aug 2023 16:15:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00596v1</guid></item><item><title>Visibility Enhancement for Low-light Hazy Scenarios</title><link>http://arxiv.org/abs/2308.00591v1</link><description>Low-light hazy scenes commonly appear at dusk and early morning. The visualenhancement for low-light hazy images is an ill-posed problem. Even thoughnumerous methods have been proposed for image dehazing and low-lightenhancement respectively, simply integrating them cannot deliver pleasingresults for this particular task. In this paper, we present a novel method toenhance visibility for low-light hazy scenarios. To handle this challengingtask, we propose two key techniques, namely cross-consistencydehazing-enhancement framework and physically based simulation for low-lighthazy dataset. Specifically, the framework is designed for enhancing visibilityof the input image via fully utilizing the clues from different sub-tasks. Thesimulation is designed for generating the dataset with ground-truths by theproposed low-light hazy imaging model. The extensive experimental results showthat the proposed method outperforms the SOTA solutions on different metricsincluding SSIM (9.19%) and PSNR(5.03%). In addition, we conduct a user study onreal images to demonstrate the effectiveness and necessity of the proposedmethod by human visual perception.</description><author>Chaoqun Zhuang, Yunfei Liu, Sijia Wen, Feng Lu</author><pubDate>Tue, 01 Aug 2023 16:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00591v1</guid></item><item><title>SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering</title><link>http://arxiv.org/abs/2307.04192v3</link><description>Video question--answering is a fundamental task in the field of videounderstanding. Although current vision--language models (VLMs) equipped withVideo Transformers have enabled temporal modeling and yielded superior results,they are at the cost of huge computational power and thus too expensive todeploy in real-time application scenarios. An economical workaround onlysamples a small portion of frames to represent the main content of that videoand tune an image--text model on these sampled frames. Recent videounderstanding models usually randomly sample a set of frames or clips,regardless of internal correlations between their visual contents, nor theirrelevance to the problem. We argue that such kinds of aimless sampling may omitthe key frames from which the correct answer can be deduced, and the situationgets worse when the sampling sparsity increases, which always happens as thevideo lengths increase. To mitigate this issue, we propose two frame samplingstrategies, namely the most domain frames (MDF) and most implied frames (MIF),to maximally preserve those frames that are most likely vital to the givenquestions. MDF passively minimizes the risk of key frame omission in abootstrap manner, while MIS actively searches key frames customized for eachvideo--question pair with the assistance of auxiliary models. The experimentalresults on three public datasets from three advanced VLMs (CLIP, GIT andAll-in-one) demonstrate that our proposed strategies can boost the performancefor image--text pretrained models. The source codes pertaining to the methodproposed in this paper are publicly available athttps://github.com/declare-lab/sas-vqa.</description><author>Wei Han, Hui Chen, Min-Yen Kan, Soujanya Poria</author><pubDate>Tue, 01 Aug 2023 16:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04192v3</guid></item><item><title>Relation-Aware Distribution Representation Network for Person Clustering with Multiple Modalities</title><link>http://arxiv.org/abs/2308.00588v1</link><description>Person clustering with multi-modal clues, including faces, bodies, andvoices, is critical for various tasks, such as movie parsing and identity-basedmovie editing. Related methods such as multi-view clustering mainly projectmulti-modal features into a joint feature space. However, multi-modal cluefeatures are usually rather weakly correlated due to the semantic gap from themodality-specific uniqueness. As a result, these methods are not suitable forperson clustering. In this paper, we propose a Relation-Aware Distributionrepresentation Network (RAD-Net) to generate a distribution representation formulti-modal clues. The distribution representation of a clue is a vectorconsisting of the relation between this clue and all other clues from allmodalities, thus being modality agnostic and good for person clustering.Accordingly, we introduce a graph-based method to construct distributionrepresentation and employ a cyclic update policy to refine distributionrepresentation progressively. Our method achieves substantial improvements of+6% and +8.2% in F-score on the Video Person-Clustering Dataset (VPCD) andVoxCeleb2 multi-view clustering dataset, respectively. Codes will be releasedpublicly upon acceptance.</description><author>Kaijian Liu, Shixiang Tang, Ziyue Li, Zhishuai Li, Lei Bai, Feng Zhu, Rui Zhao</author><pubDate>Tue, 01 Aug 2023 16:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00588v1</guid></item><item><title>Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel</title><link>http://arxiv.org/abs/2308.00583v1</link><description>Anomaly detection (AD) involves identifying observations or events thatdeviate in some way from the rest of the data. Machine learning techniques haveshown success in automating this process by detecting hidden patterns anddeviations in large-scale data. The potential of quantum computing for machinelearning has been widely recognized, leading to extensive research efforts todevelop suitable quantum machine learning (QML) algorithms. In particular, thesearch for QML algorithms for near-term NISQ devices is in full swing. However,NISQ devices pose additional challenges due to their limited qubit coherencetimes, low number of qubits, and high error rates. Kernel methods based onquantum kernel estimation have emerged as a promising approach to QML on NISQdevices, offering theoretical guarantees, versatility, and compatibility withNISQ constraints. Especially support vector machines (SVM) utilizing quantumkernel estimation have shown success in various supervised learning tasks.However, in the context of AD, semisupervised learning is of great relevance,and yet there is limited research published in this area. This paper introducesan approach to semisupervised AD based on the reconstruction loss of a supportvector regression (SVR) with quantum kernel. This novel model is an alternativeto the variational quantum and quantum kernel one-class classifiers, and iscompared to a quantum autoencoder as quantum baseline and a SVR withradial-basis-function (RBF) kernel as well as a classical autoencoder asclassical baselines. The models are benchmarked extensively on 10 real-world ADdata sets and one toy data set, and it is shown that our SVR model with quantumkernel performs better than the SVR with RBF kernel as well as all othermodels, achieving highest mean AUC over all data sets. In addition, our QSVRoutperforms the quantum autoencoder on 9 out of 11 data sets.</description><author>Kilian Tscharke, Sebastian Issel, Pascal Debus</author><pubDate>Tue, 01 Aug 2023 16:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00583v1</guid></item><item><title>Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach</title><link>http://arxiv.org/abs/2303.05389v2</link><description>Depression is a common disease worldwide. It is difficult to diagnose andcontinues to be underdiagnosed. Because depressed patients constantly sharetheir symptoms, major life events, and treatments on social media, researchersare turning to user-generated digital traces on social media for depressiondetection. Such methods have distinct advantages in combating depressionbecause they can facilitate innovative approaches to fight depression andalleviate its social and economic burden. However, most existing studies lackeffective means to incorporate established medical domain knowledge indepression detection or suffer from feature extraction difficulties that impedegreater performance. Following the design science research paradigm, we proposea Deep Knowledge-aware Depression Detection (DKDD) framework to accuratelydetect social media users at risk of depression and explain the criticalfactors that contribute to such detection. Extensive empirical studies withreal-world data demonstrate that, by incorporating domain knowledge, our methodoutperforms existing state-of-the-art methods. Our work has significantimplications for IS research in knowledge-aware machine learning, digitaltraces utilization, and NLP research in IS. Practically, by providing earlydetection and explaining the critical factors, DKDD can supplement clinicaldepression screening and enable large-scale evaluations of a population'smental health status.</description><author>Wenli Zhang, Jiaheng Xie, Zhu Zhang, Xiang Liu</author><pubDate>Tue, 01 Aug 2023 15:48:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05389v2</guid></item><item><title>A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges</title><link>http://arxiv.org/abs/2112.15424v3</link><description>This is Part II of the two-part comprehensive survey devoted to a computingframework most commonly known under the names Hyperdimensional Computing andVector Symbolic Architectures (HDC/VSA). Both names refer to a family ofcomputational models that use high-dimensional distributed representations andrely on the algebraic properties of their key operations to incorporate theadvantages of structured symbolic representations and vector distributedrepresentations. Holographic Reduced Representations is an influential HDC/VSAmodel that is well-known in the machine learning domain and often used to referto the whole family. However, for the sake of consistency, we use HDC/VSA torefer to the field. Part I of this survey covered foundational aspects of thefield, such as the historical context leading to the development of HDC/VSA,key elements of any HDC/VSA model, known HDC/VSA models, and the transformationof input data of various types into high-dimensional vectors suitable forHDC/VSA. This second part surveys existing applications, the role of HDC/VSA incognitive computing and architectures, as well as directions for future work.Most of the applications lie within the Machine Learning/ArtificialIntelligence domain, however, we also cover other applications to provide acomplete picture. The survey is written to be useful for both newcomers andpractitioners.</description><author>Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, Abbas Rahimi</author><pubDate>Tue, 01 Aug 2023 15:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.15424v3</guid></item><item><title>PVG: Progressive Vision Graph for Vision Recognition</title><link>http://arxiv.org/abs/2308.00574v1</link><description>Convolution-based and Transformer-based vision backbone networks processimages into the grid or sequence structures, respectively, which are inflexiblefor capturing irregular objects. Though Vision GNN (ViG) adopts graph-levelfeatures for complex images, it has some issues, such as inaccurate neighbornode selection, expensive node information aggregation calculation, andover-smoothing in the deep layers. To address the above problems, we propose aProgressive Vision Graph (PVG) architecture for vision recognition task.Compared with previous works, PVG contains three main components: 1)Progressively Separated Graph Construction (PSGC) to introduce second-ordersimilarity by gradually increasing the channel of the global graph branch anddecreasing the channel of local branch as the layer deepens; 2) Neighbor nodesinformation aggregation and update module by using Max pooling and mathematicalExpectation (MaxE) to aggregate rich neighbor information; 3) Graph errorLinear Unit (GraphLU) to enhance low-value information in a relaxed form toreduce the compression of image detail information for alleviating theover-smoothing. Extensive experiments on mainstream benchmarks demonstrate thesuperiority of PVG over state-of-the-art methods, e.g., our PVG-S obtains 83.0%Top-1 accuracy on ImageNet-1K that surpasses GNN-based ViG-S by +0.9 with theparameters reduced by 18.5%, while the largest PVG-B obtains 84.2% that has+0.5 improvement than ViG-B. Furthermore, our PVG-S obtains +1.3 box AP and+0.4 mask AP gains than ViG-S on COCO dataset.</description><author>Jiafu Wu, Jian Li, Jiangning Zhang, Boshen Zhang, Mingmin Chi, Yabiao Wang, Chengjie Wang</author><pubDate>Tue, 01 Aug 2023 15:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00574v1</guid></item><item><title>Phenotype-preserving metric design for high-content image reconstruction by generative inpainting</title><link>http://arxiv.org/abs/2307.14436v2</link><description>In the past decades, automated high-content microscopy demonstrated itsability to deliver large quantities of image-based data powering theversatility of phenotypic drug screening and systems biology applications.However, as the sizes of image-based datasets grew, it became infeasible forhumans to control, avoid and overcome the presence of imaging and samplepreparation artefacts in the images. While novel techniques like machinelearning and deep learning may address these shortcomings through generativeimage inpainting, when applied to sensitive research data this may come at thecost of undesired image manipulation. Undesired manipulation may be caused byphenomena such as neural hallucinations, to which some artificial neuralnetworks are prone. To address this, here we evaluate the state-of-the-artinpainting methods for image restoration in a high-content fluorescencemicroscopy dataset of cultured cells with labelled nuclei. We show thatarchitectures like DeepFill V2 and Edge Connect can faithfully restoremicroscopy images upon fine-tuning with relatively little data. Our resultsdemonstrate that the area of the region to be restored is of higher importancethan shape. Furthermore, to control for the quality of restoration, we proposea novel phenotype-preserving metric design strategy. In this strategy, the sizeand count of the restored biological phenotypes like cell nuclei are quantifiedto penalise undesirable manipulation. We argue that the design principles ofour approach may also generalise to other applications.</description><author>Vaibhav Sharma, Artur Yakimovich</author><pubDate>Tue, 01 Aug 2023 15:31:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14436v2</guid></item><item><title>Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth</title><link>http://arxiv.org/abs/2103.03404v2</link><description>Attention-based architectures have become ubiquitous in machine learning, yetour understanding of the reasons for their effectiveness remains limited. Thiswork proposes a new way to understand self-attention networks: we show thattheir output can be decomposed into a sum of smaller terms, each involving theoperation of a sequence of attention heads across layers. Using thisdecomposition, we prove that self-attention possesses a strong inductive biastowards "token uniformity". Specifically, without skip connections ormulti-layer perceptrons (MLPs), the output converges doubly exponentially to arank-1 matrix. On the other hand, skip connections and MLPs stop the outputfrom degeneration. Our experiments verify the identified convergence phenomenaon different variants of standard transformer architectures.</description><author>Yihe Dong, Jean-Baptiste Cordonnier, Andreas Loukas</author><pubDate>Tue, 01 Aug 2023 15:27:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.03404v2</guid></item><item><title>How is ChatGPT's behavior changing over time?</title><link>http://arxiv.org/abs/2307.09009v2</link><description>GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)services. However, when and how these models are updated over time is opaque.Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 onseveral diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3)opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generatingcode, 6) US Medical License tests, and 7) visual reasoning. We find that theperformance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.For example, GPT-4 (March 2023) was reasonable at identifying prime vs.composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these samequestions (51% accuracy). This is partly explained by a drop in GPT-4's amenityto follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better inJune than in March in this task. GPT-4 became less willing to answer sensitivequestions and opinion survey questions in June than in March. GPT-4 performedbetter at multi-hop questions in June than in March, while GPT-3.5'sperformance dropped on this task. Both GPT-4 and GPT-3.5 had more formattingmistakes in code generation in June than in March. Overall, our findings showthat the behavior of the "same" LLM service can change substantially in arelatively short amount of time, highlighting the need for continuousmonitoring of LLMs.</description><author>Lingjiao Chen, Matei Zaharia, James Zou</author><pubDate>Tue, 01 Aug 2023 15:23:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09009v2</guid></item><item><title>Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning</title><link>http://arxiv.org/abs/2211.09273v3</link><description>Smart speaker voice assistants (VAs) such as Amazon Echo and Google Home havebeen widely adopted due to their seamless integration with smart home devicesand the Internet of Things (IoT) technologies. These VA services raise privacyconcerns, especially due to their access to our speech. This work considers onesuch use case: the unaccountable and unauthorized surveillance of a user'semotion via speech emotion recognition (SER). This paper presents DARE-GP, asolution that creates additive noise to mask users' emotional information whilepreserving the transcription-relevant portions of their speech. DARE-GP doesthis by using a constrained genetic programming approach to learn the spectralfrequency traits that depict target users' emotional content, and thengenerating a universal adversarial audio perturbation that provides thisprivacy protection. Unlike existing works, DARE-GP provides: a) real-timeprotection of previously unheard utterances, b) against previously unseenblack-box SER classifiers, c) while protecting speech transcription, and d)does so in a realistic, acoustic environment. Further, this evasion is robustagainst defenses employed by a knowledgeable adversary. The evaluations in thiswork culminate with acoustic evaluations against two off-the-shelf commercialsmart speakers using a small-form-factor (raspberry pi) integrated with awake-word system to evaluate the efficacy of its real-world, real-timedeployment.</description><author>Brian Testa, Yi Xiao, Harshit Sharma, Avery Gump, Asif Salekin</author><pubDate>Tue, 01 Aug 2023 15:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09273v3</guid></item><item><title>Reinforcement Learning-based Non-Autoregressive Solver for Traveling Salesman Problems</title><link>http://arxiv.org/abs/2308.00560v1</link><description>The Traveling Salesman Problem (TSP) is a well-known problem in combinatorialoptimization with applications in various domains. However, existing TSPsolvers face challenges in producing high-quality solutions with low latency.To address this issue, we propose NAR4TSP, which produces TSP solutions in aNon-Autoregressive (NAR) manner using a specially designed Graph Neural Network(GNN), achieving faster inference speed. Moreover, NAR4TSP is trained using anenhanced Reinforcement Learning (RL) strategy, eliminating the dependency oncostly labels used to train conventional supervised learning-based NAR models.To the best of our knowledge, NAR4TSP is the first TSP solver that successfullycombines RL and NAR decoding. The experimental results on both synthetic andreal-world TSP instances demonstrate that NAR4TSP outperforms fourstate-of-the-art models in terms of solution quality, inference latency, andgeneralization ability. Lastly, we present visualizations of NAR4TSP's decodingprocess and its overall path planning to showcase the feasibility ofimplementing NAR4TSP in an end-to-end manner and its effectiveness,respectively.</description><author>Yubin Xiao, Di Wang, Huanhuan Chen, Boyang Li, Wei Pang, Xuan Wu, Hao Li, Dong Xu, Yanchun Liang, You Zhou</author><pubDate>Tue, 01 Aug 2023 15:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00560v1</guid></item><item><title>Gradient Scaling on Deep Spiking Neural Networks with Spike-Dependent Local Information</title><link>http://arxiv.org/abs/2308.00558v1</link><description>Deep spiking neural networks (SNNs) are promising neural networks for theirmodel capacity from deep neural network architecture and energy efficiency fromSNNs' operations. To train deep SNNs, recently, spatio-temporal backpropagation(STBP) with surrogate gradient was proposed. Although deep SNNs have beensuccessfully trained with STBP, they cannot fully utilize spike information. Inthis work, we proposed gradient scaling with local spike information, which isthe relation between pre- and post-synaptic spikes. Considering the causalitybetween spikes, we could enhance the training performance of deep SNNs.According to our experiments, we could achieve higher accuracy with lowerspikes by adopting the gradient scaling on image classification tasks, such asCIFAR10 and CIFAR100.</description><author>Seongsik Park, Jeonghee Jo, Jongkil Park, Yeonjoo Jeong, Jaewook Kim, Suyoun Lee, Joon Young Kwak, Inho Kim, Jong-Keuk Park, Kyeong Seok Lee, Gye Weon Hwang, Hyun Jae Jang</author><pubDate>Tue, 01 Aug 2023 14:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00558v1</guid></item><item><title>Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms</title><link>http://arxiv.org/abs/2308.00556v1</link><description>In this paper, we investigate the impact of test-time adversarial attacks onlinear regression models and determine the optimal level of robustness that anymodel can reach while maintaining a given level of standard predictiveperformance (accuracy). Through quantitative estimates, we uncover fundamentaltradeoffs between adversarial robustness and accuracy in different regimes. Weobtain a precise characterization which distinguishes between regimes whererobustness is achievable without hurting standard accuracy and regimes where atradeoff might be unavoidable. Our findings are empirically confirmed withsimple experiments that represent a variety of settings. This work applies tofeature covariance matrices and attack norms of any nature, and extends beyondprevious works in this area.</description><author>Elvis Dohmatob, Meyer Scetbon</author><pubDate>Tue, 01 Aug 2023 14:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00556v1</guid></item><item><title>Copula for Instance-wise Feature Selection and Ranking</title><link>http://arxiv.org/abs/2308.00549v1</link><description>Instance-wise feature selection and ranking methods can achieve a goodselection of task-friendly features for each sample in the context of neuralnetworks. However, existing approaches that assume feature subsets to beindependent are imperfect when considering the dependency between features. Toaddress this limitation, we propose to incorporate the Gaussian copula, apowerful mathematical technique for capturing correlations between variables,into the current feature selection framework with no additional changes needed.Experimental results on both synthetic and real datasets, in terms ofperformance comparison and interpretability, demonstrate that our method iscapable of capturing meaningful correlations.</description><author>Hanyu Peng, Guanhua Fang, Ping Li</author><pubDate>Tue, 01 Aug 2023 14:45:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00549v1</guid></item><item><title>End-to-End Neural Network Training for Hyperbox-Based Classification</title><link>http://arxiv.org/abs/2307.09269v2</link><description>Hyperbox-based classification has been seen as a promising technique in whichdecisions on the data are represented as a series of orthogonal,multidimensional boxes (i.e., hyperboxes) that are often interpretable andhuman-readable. However, existing methods are no longer capable of efficientlyhandling the increasing volume of data many application domains face nowadays.We address this gap by proposing a novel, fully differentiable framework forhyperbox-based classification via neural networks. In contrast to previouswork, our hyperbox models can be efficiently trained in an end-to-end fashion,which leads to significantly reduced training times and superior classificationresults.</description><author>Denis Mayr Lima Martins, Christian Lülf, Fabian Gieseke</author><pubDate>Tue, 01 Aug 2023 14:43:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09269v2</guid></item><item><title>A Generalization of the Shortest Path Problem to Graphs with Multiple Edge-Cost Estimates</title><link>http://arxiv.org/abs/2208.11489v4</link><description>The shortest path problem in graphs is a cornerstone of AI theory andapplications. Existing algorithms generally ignore edge weight computationtime. We present a generalized framework for weighted directed graphs, whereedge weight can be computed (estimated) multiple times, at increasing accuracyand run-time expense. This raises several generalized variants of the shortestpath problem. We introduce the problem of finding a path with the tightestlower-bound on the optimal cost. We then present two complete algorithms forthe generalized problem, and empirically demonstrate their efficacy.</description><author>Eyal Weiss, Ariel Felner, Gal A. Kaminka</author><pubDate>Tue, 01 Aug 2023 14:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.11489v4</guid></item><item><title>Pair then Relation: Pair-Net for Panoptic Scene Graph Generation</title><link>http://arxiv.org/abs/2307.08699v2</link><description>Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation(SGG) that aims to create a more comprehensive scene graph representation usingpanoptic segmentation instead of boxes. Compared to SGG, PSG has severalchallenging problems: pixel-level segment outputs and full relationshipexploration (It also considers thing and stuff relation). Thus, current PSGmethods have limited performance, which hinders downstream tasks orapplications. The goal of this work aims to design a novel and strong baselinefor PSG. To achieve that, we first conduct an in-depth analysis to identify thebottleneck of the current PSG models, finding that inter-object pair-wiserecall is a crucial factor that was ignored by previous PSG methods. Based onthis and the recent query-based frameworks, we present a novel framework: Pairthen Relation (Pair-Net), which uses a Pair Proposal Network (PPN) to learn andfilter sparse pair-wise relationships between subjects and objects. Moreover,we also observed the sparse nature of object pairs for both Motivated by this,we design a lightweight Matrix Learner within the PPN, which directly learnpair-wised relationships for pair proposal generation. Through extensiveablation and analysis, our approach significantly improves upon leveraging thesegmenter solid baseline. Notably, our method achieves new state-of-the-artresults on the PSG benchmark, with over 10\% absolute gains compared toPSGFormer. The code of this paper is publicly available athttps://github.com/king159/Pair-Net.</description><author>Jinghao Wang, Zhengyu Wen, Xiangtai Li, Zujin Guo, Jingkang Yang, Ziwei Liu</author><pubDate>Tue, 01 Aug 2023 14:41:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08699v2</guid></item><item><title>Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</title><link>http://arxiv.org/abs/2307.11224v2</link><description>Jina Embeddings constitutes a set of high-performance sentence embeddingmodels adept at translating various textual inputs into numericalrepresentations, thereby capturing the semantic essence of the text. The modelsexcel in applications such as dense retrieval and semantic textual similarity.This paper details the development of Jina Embeddings, starting with thecreation of high-quality pairwise and triplet datasets. It underlines thecrucial role of data cleaning in dataset preparation, gives in-depth insightsinto the model training process, and concludes with a comprehensive performanceevaluation using the Massive Textual Embedding Benchmark (MTEB). To increasethe model's awareness of negations, we constructed a novel training andevaluation dataset of negated and non-negated statements, which we makepublicly available to the community.</description><author>Michael Günther, Louis Milliken, Jonathan Geuter, Georgios Mastrapas, Bo Wang, Han Xiao</author><pubDate>Tue, 01 Aug 2023 14:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11224v2</guid></item><item><title>Detecting Cloud Presence in Satellite Images Using the RGB-based CLIP Vision-Language Model</title><link>http://arxiv.org/abs/2308.00541v1</link><description>This work explores capabilities of the pre-trained CLIP vision-language modelto identify satellite images affected by clouds. Several approaches to usingthe model to perform cloud presence detection are proposed and evaluated,including a purely zero-shot operation with text prompts and severalfine-tuning approaches. Furthermore, the transferability of the methods acrossdifferent datasets and sensor types (Sentinel-2 and Landsat-8) is tested. Theresults that CLIP can achieve non-trivial performance on the cloud presencedetection task with apparent capability to generalise across sensing modalitiesand sensing bands. It is also found that a low-cost fine-tuning stage leads toa strong increase in true negative rate. The results demonstrate that therepresentations learned by the CLIP model can be useful for satellite imageprocessing tasks involving clouds.</description><author>Mikolaj Czerkawski, Robert Atkinson, Christos Tachtatzis</author><pubDate>Tue, 01 Aug 2023 14:36:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00541v1</guid></item><item><title>Predicting Early Dropouts of an Active and Healthy Ageing App</title><link>http://arxiv.org/abs/2308.00539v1</link><description>In this work, we present a machine learning approach for predicting earlydropouts of an active and healthy ageing app. The presented algorithms havebeen submitted to the IFMBE Scientific Challenge 2022, part of IUPESM WC 2022.We have processed the given database and generated seven datasets. We usedpre-processing techniques to construct classification models that predict theadherence of users using dynamic and static features. We submitted 11 officialruns and our results show that machine learning algorithms can providehigh-quality adherence predictions. Based on the results, the dynamic featurespositively influence a model's classification performance. Due to theimbalanced nature of the dataset, we employed oversampling methods such asSMOTE and ADASYN to improve the classification performance. The oversamplingapproaches led to a remarkable improvement of 10\%. Our methods won first placein the IFMBE Scientific Challenge 2022.</description><author>Vasileios Perifanis, Ioanna Michailidi, Giorgos Stamatelatos, George Drosatos, Pavlos S. Efraimidis</author><pubDate>Tue, 01 Aug 2023 14:32:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00539v1</guid></item><item><title>An Integrated Multi-Time-Scale Modeling for Solar Irradiance Forecasting Using Deep Learning</title><link>http://arxiv.org/abs/1905.02616v3</link><description>For short-term solar irradiance forecasting, the traditional pointforecasting methods are rendered less useful due to the non-stationarycharacteristic of solar power. The amount of operating reserves required tomaintain reliable operation of the electric grid rises due to the variabilityof solar energy. The higher the uncertainty in the generation, the greater theoperating-reserve requirements, which translates to an increased cost ofoperation. In this research work, we propose a unified architecture formulti-time-scale predictions for intra-day solar irradiance forecasting usingrecurrent neural networks (RNN) and long-short-term memory networks (LSTMs).This paper also lays out a framework for extending this modeling approach tointra-hour forecasting horizons thus, making it a multi-time-horizonforecasting approach, capable of predicting intra-hour as well as intra-daysolar irradiance. We develop an end-to-end pipeline to effectuate the proposedarchitecture. The performance of the prediction model is tested and validatedby the methodical implementation. The robustness of the approach isdemonstrated with case studies conducted for geographically scattered sitesacross the United States. The predictions demonstrate that our proposed unifiedarchitecture-based approach is effective for multi-time-scale solar forecastsand achieves a lower root-mean-square prediction error when benchmarked againstthe best-performing methods documented in the literature that use separatemodels for each time-scale during the day. Our proposed method results in a71.5% reduction in the mean RMSE averaged across all the test sites compared tothe ML-based best-performing method reported in the literature. Additionally,the proposed method enables multi-time-horizon forecasts with real-time inputs,which have a significant potential for practical industry applications in theevolving grid.</description><author>Sakshi Mishra, Praveen Palanisamy</author><pubDate>Tue, 01 Aug 2023 14:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1905.02616v3</guid></item><item><title>PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps</title><link>http://arxiv.org/abs/2308.00538v1</link><description>We propose PressureTransferNet, a novel method for Human Activity Recognition(HAR) using ground pressure information. Our approach generates body-specificdynamic ground pressure profiles for specific activities by leveraging existingpressure data from different individuals. PressureTransferNet is anencoder-decoder model taking a source pressure map and a target human attributevector as inputs, producing a new pressure map reflecting the target attribute.To train the model, we use a sensor simulation to create a diverse dataset withvarious human attributes and pressure profiles. Evaluation on a real-worlddataset shows its effectiveness in accurately transferring human attributes toground pressure profiles across different scenarios. We visually confirm thefidelity of the synthesized pressure shapes using a physics-based deep learningmodel and achieve a binary R-square value of 0.79 on areas with ground contact.Validation through classification with F1 score (0.911$\pm$0.015) on physicalpressure mat data demonstrates the correctness of the synthesized pressuremaps, making our method valuable for data augmentation, denoising, sensorsimulation, and anomaly detection. Applications span sports science,rehabilitation, and bio-mechanics, contributing to the development of HARsystems.</description><author>Lala Shakti Swarup Ray, Vitor Fortes Rey, Bo Zhou, Sungho Suh, Paul Lukowicz</author><pubDate>Tue, 01 Aug 2023 14:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00538v1</guid></item><item><title>Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies</title><link>http://arxiv.org/abs/2308.00537v1</link><description>Accurate online transient stability prediction is critical for ensuring powersystem stability when facing disturbances. While traditional transient stablityanalysis replies on the time domain simulations can not be quickly adapted tothe power grid toplogy change. In order to vectorize high-dimensional powergrid topological structure information into low-dimensional node-based graphembedding streaming data, graph embedding dynamic feature (GEDF) has beenproposed. The transient stability GEDF-based supervised contrastive learning(GEDF-SCL) model uses supervised contrastive learning to predict transientstability with GEDFs, considering power grid topology information. To evaluatethe performance of the proposed GEDF-SCL model, power grids of varyingtopologies were generated based on the IEEE 39-bus system model. Transientoperational data was obtained by simulating N-1 and N-$\bm{m}$-1 contingencieson these generated power system topologies. Test result demonstrated that theGEDF-SCL model can achieve high accuracy in transient stability prediction andadapt well to changing power grid topologies.</description><author>Zijian Lv, Xin Chen, Zijian Feng</author><pubDate>Tue, 01 Aug 2023 14:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00537v1</guid></item><item><title>Graph Contrastive Learning with Generative Adversarial Network</title><link>http://arxiv.org/abs/2308.00535v1</link><description>Graph Neural Networks (GNNs) have demonstrated promising results onexploiting node representations for many downstream tasks through supervisedend-to-end training. To deal with the widespread label scarcity issue inreal-world applications, Graph Contrastive Learning (GCL) is leveraged to trainGNNs with limited or even no labels by maximizing the mutual informationbetween nodes in its augmented views generated from the original graph.However, the distribution of graphs remains unconsidered in view generation,resulting in the ignorance of unseen edges in most existing literature, whichis empirically shown to be able to improve GCL's performance in ourexperiments. To this end, we propose to incorporate graph generativeadversarial networks (GANs) to learn the distribution of views for GCL, inorder to i) automatically capture the characteristic of graphs foraugmentations, and ii) jointly train the graph GAN model and the GCL model.Specifically, we present GACN, a novel Generative Adversarial Contrastivelearning Network for graph representation learning. GACN develops a viewgenerator and a view discriminator to generate augmented views automatically inan adversarial style. Then, GACN leverages these views to train a GNN encoderwith two carefully designed self-supervised learning losses, including thegraph contrastive loss and the Bayesian personalized ranking Loss. Furthermore,we design an optimization framework to train all GACN modules jointly.Extensive experiments on seven real-world datasets show that GACN is able togenerate high-quality augmented views for GCL and is superior to twelvestate-of-the-art baseline methods. Noticeably, our proposed GACN surprisinglydiscovers that the generated views in data augmentation finally conform to thewell-known preferential attachment rule in online networks.</description><author>Cheng Wu, Chaokun Wang, Jingcao Xu, Ziyang Liu, Kai Zheng, Xiaowei Wang, Yang Song, Kun Gai</author><pubDate>Tue, 01 Aug 2023 14:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00535v1</guid></item><item><title>A Novel Temporal Multi-Gate Mixture-of-Experts Approach for Vehicle Trajectory and Driving Intention Prediction</title><link>http://arxiv.org/abs/2308.00533v1</link><description>Accurate Vehicle Trajectory Prediction is critical for automated vehicles andadvanced driver assistance systems. Vehicle trajectory prediction consists oftwo essential tasks, i.e., longitudinal position prediction and lateralposition prediction. There is a significant correlation between drivingintentions and vehicle motion. In existing work, the three tasks are oftenconducted separately without considering the relationships between thelongitudinal position, lateral position, and driving intention. In this paper,we propose a novel Temporal Multi-Gate Mixture-of-Experts (TMMOE) model forsimultaneously predicting the vehicle trajectory and driving intention. Theproposed model consists of three layers: a shared layer, an expert layer, and afully connected layer. In the model, the shared layer utilizes TemporalConvolutional Networks (TCN) to extract temporal features. Then the expertlayer is built to identify different information according to the three tasks.Moreover, the fully connected layer is used to integrate and export predictionresults. To achieve better performance, uncertainty algorithm is used toconstruct the multi-task loss function. Finally, the publicly available CitySimdataset validates the TMMOE model, demonstrating superior performance comparedto the LSTM model, achieving the highest classification and regression results.Keywords: Vehicle trajectory prediction, driving intentions Classification,Multi-task</description><author>Renteng Yuan, Mohamed Abdel-Aty, Qiaojun Xiang, Zijin Wang, Ou Zheng</author><pubDate>Tue, 01 Aug 2023 14:26:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00533v1</guid></item><item><title>Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case</title><link>http://arxiv.org/abs/2202.05069v2</link><description>With the development of new sensors and monitoring devices, more sources ofdata become available to be used as inputs for machine learning models. Thesecan on the one hand help to improve the accuracy of a model. On the other handhowever, combining these new inputs with historical data remains a challengethat has not yet been studied in enough detail. In this work, we propose atransfer-learning algorithm that combines the new and the historical data, thatis especially beneficial when the new data is scarce. We focus the approach onthe linear regression case, which allows us to conduct a rigorous theoreticalstudy on the benefits of the approach. We show that our approach is robustagainst negative transfer-learning, and we confirm this result empirically withreal and simulated data.</description><author>Luis Pedro Silvestrin, Harry van Zanten, Mark Hoogendoorn, Ger Koole</author><pubDate>Tue, 01 Aug 2023 14:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05069v2</guid></item><item><title>Beating Backdoor Attack at Its Own Game</title><link>http://arxiv.org/abs/2307.15539v2</link><description>Deep neural networks (DNNs) are vulnerable to backdoor attack, which does notaffect the network's performance on clean data but would manipulate the networkbehavior once a trigger pattern is added. Existing defense methods have greatlyreduced attack success rate, but their prediction accuracy on clean data stilllags behind a clean model by a large margin. Inspired by the stealthiness andeffectiveness of backdoor attack, we propose a simple but highly effectivedefense framework which injects non-adversarial backdoors targeting poisonedsamples. Following the general steps in backdoor attack, we detect a small setof suspected samples and then apply a poisoning strategy to them. Thenon-adversarial backdoor, once triggered, suppresses the attacker's backdoor onpoisoned data, but has limited influence on clean data. The defense can becarried out during data preprocessing, without any modification to the standardend-to-end training pipeline. We conduct extensive experiments on multiplebenchmarks with different architectures and representative attacks. Resultsdemonstrate that our method achieves state-of-the-art defense effectivenesswith by far the lowest performance drop on clean data. Considering thesurprising defense ability displayed by our framework, we call for moreattention to utilizing backdoor for backdoor defense. Code is available athttps://github.com/damianliumin/non-adversarial_backdoor.</description><author>Min Liu, Alberto Sangiovanni-Vincentelli, Xiangyu Yue</author><pubDate>Tue, 01 Aug 2023 14:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15539v2</guid></item><item><title>Variational Label-Correlation Enhancement for Congestion Prediction</title><link>http://arxiv.org/abs/2308.00529v1</link><description>The physical design process of large-scale designs is a time-consuming task,often requiring hours to days to complete, with routing being the most criticaland complex step. As the the complexity of Integrated Circuits (ICs) increases,there is an increased demand for accurate routing quality prediction. Accuratecongestion prediction aids in identifying design flaws early on, therebyaccelerating circuit design and conserving resources. Despite the advancementsin current congestion prediction methodologies, an essential aspect that hasbeen largely overlooked is the spatial label-correlation between differentgrids in congestion prediction. The spatial label-correlation is a fundamentalcharacteristic of circuit design, where the congestion status of a grid is notisolated but inherently influenced by the conditions of its neighboring grids.In order to fully exploit the inherent spatial label-correlation betweenneighboring grids, we propose a novel approach, {\ours}, i.e., VAriationalLabel-Correlation Enhancement for Congestion Prediction, which considers thelocal label-correlation in the congestion map, associating the estimatedcongestion value of each grid with a local label-correlation weight influencedby its surrounding grids. {\ours} leverages variational inference techniques toestimate this weight, thereby enhancing the regression model's performance byincorporating spatial dependencies. Experiment results validate the superioreffectiveness of {\ours} on the public available \texttt{ISPD2011} and\texttt{DAC2012} benchmarks using the superblue circuit line.</description><author>Biao Liu, Congyu Qiao, Ning Xu, Xin Geng, Ziran Zhu, Jun Yang</author><pubDate>Tue, 01 Aug 2023 14:15:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00529v1</guid></item><item><title>Provable convergence guarantees for black-box variational inference</title><link>http://arxiv.org/abs/2306.03638v2</link><description>While black-box variational inference is widely used, there is no proof thatits stochastic optimization succeeds. We suggest this is due to a theoreticalgap in existing stochastic optimization proofs-namely the challenge of gradientestimators with unusual noise bounds, and a composite non-smooth objective. Fordense Gaussian variational families, we observe that existing gradientestimators based on reparameterization satisfy a quadratic noise bound and givenovel convergence guarantees for proximal and projected stochastic gradientdescent using this bound. This provides the first rigorous guarantee thatblack-box variational inference converges for realistic inference problems.</description><author>Justin Domke, Guillaume Garrigos, Robert Gower</author><pubDate>Tue, 01 Aug 2023 14:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03638v2</guid></item><item><title>Unimodal Intermediate Training for Multimodal Meme Sentiment Classification</title><link>http://arxiv.org/abs/2308.00528v1</link><description>Internet Memes remain a challenging form of user-generated content forautomated sentiment classification. The availability of labelled memes is abarrier to developing sentiment classifiers of multimodal memes. To address theshortage of labelled memes, we propose to supplement the training of amultimodal meme classifier with unimodal (image-only and text-only) data. Inthis work, we present a novel variant of supervised intermediate training thatuses relatively abundant sentiment-labelled unimodal data. Our results show astatistically significant performance improvement from the incorporation ofunimodal text data. Furthermore, we show that the training set of labelledmemes can be reduced by 40% without reducing the performance of the downstreammodel.</description><author>Muzhaffar Hazman, Susan McKeever, Josephine Griffith</author><pubDate>Tue, 01 Aug 2023 14:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00528v1</guid></item><item><title>Convergence Rates for Non-Log-Concave Sampling and Log-Partition Estimation</title><link>http://arxiv.org/abs/2303.03237v3</link><description>Sampling from Gibbs distributions $p(x) \propto \exp(-V(x)/\varepsilon)$ andcomputing their log-partition function are fundamental tasks in statistics,machine learning, and statistical physics. However, while efficient algorithmsare known for convex potentials $V$, the situation is much more difficult inthe non-convex case, where algorithms necessarily suffer from the curse ofdimensionality in the worst case. For optimization, which can be seen as alow-temperature limit of sampling, it is known that smooth functions $V$ allowfaster convergence rates. Specifically, for $m$-times differentiable functionsin $d$ dimensions, the optimal rate for algorithms with $n$ functionevaluations is known to be $O(n^{-m/d})$, where the constant can potentiallydepend on $m, d$ and the function to be optimized. Hence, the curse ofdimensionality can be alleviated for smooth functions at least in terms of theconvergence rate. Recently, it has been shown that similarly fast rates canalso be achieved with polynomial runtime $O(n^{3.5})$, where the exponent $3.5$is independent of $m$ or $d$. Hence, it is natural to ask whether similar ratesfor sampling and log-partition computation are possible, and whether they canbe realized in polynomial time with an exponent independent of $m$ and $d$. Weshow that the optimal rates for sampling and log-partition computation aresometimes equal and sometimes faster than for optimization. We then analyzevarious polynomial-time sampling algorithms, including an extension of a recentpromising optimization approach, and find that they sometimes exhibitinteresting behavior but no near-optimal rates. Our results also give furtherinsights on the relation between sampling, log-partition, and optimizationproblems.</description><author>David Holzmüller, Francis Bach</author><pubDate>Tue, 01 Aug 2023 14:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.03237v3</guid></item><item><title>Visual attention information can be traced on cortical response but not on the retina: evidence from electrophysiological mouse data using natural images as stimuli</title><link>http://arxiv.org/abs/2308.00526v1</link><description>Visual attention forms the basis of understanding the visual world. In thiswork we follow a computational approach to investigate the biological basis ofvisual attention. We analyze retinal and cortical electrophysiological datafrom mouse. Visual Stimuli are Natural Images depicting real world scenes. Ourresults show that in primary visual cortex (V1), a subset of around $10\%$ ofthe neurons responds differently to salient versus non-salient visual regions.Visual attention information was not traced in retinal response. It appearsthat the retina remains naive concerning visual attention; cortical responsegets modulated to interpret visual attention information. Experimental animalstudies may be designed to further explore the biological basis of visualattention we traced in this study. In applied and translational science, ourstudy contributes to the design of improved visual prostheses systems --systems that create artificial visual percepts to visually impaired individualsby electronic implants placed on either the retina or the cortex.</description><author>Nikos Melanitis, Konstantina Nikita</author><pubDate>Tue, 01 Aug 2023 14:09:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00526v1</guid></item><item><title>Transfer-Ensemble Learning based Deep Convolutional Neural Networks for Diabetic Retinopathy Classification</title><link>http://arxiv.org/abs/2308.00525v1</link><description>This article aims to classify diabetic retinopathy (DR) disease into fivedifferent classes using an ensemble approach based on two popular pre-trainedconvolutional neural networks: VGG16 and Inception V3. The proposed model aimsto leverage the strengths of the two individual nets to enhance theclassification performance for diabetic retinopathy. The ensemble modelarchitecture involves freezing a portion of the layers in each pre-trainedmodel to utilize their learned representations effectively. Global averagepooling layers are added to transform the output feature maps into fixed-lengthvectors. These vectors are then concatenated to form a consolidatedrepresentation of the input image. The ensemble model is trained using adataset of diabetic retinopathy images (APTOS), divided into training andvalidation sets. During the training process, the model learns to classify theretinal images into the corresponding diabetic retinopathy classes.Experimental results on the test set demonstrate the efficacy of the proposedensemble model for DR classification achieving an accuracy of 96.4%.</description><author>Susmita Ghosh, Abhiroop Chatterjee</author><pubDate>Tue, 01 Aug 2023 14:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00525v1</guid></item><item><title>A Framework and Benchmark for Deep Batch Active Learning for Regression</title><link>http://arxiv.org/abs/2203.09410v4</link><description>The acquisition of labels for supervised learning can be expensive. Toimprove the sample efficiency of neural network regression, we study activelearning methods that adaptively select batches of unlabeled data for labeling.We present a framework for constructing such methods out of (network-dependent)base kernels, kernel transformations, and selection methods. Our frameworkencompasses many existing Bayesian methods based on Gaussian processapproximations of neural networks as well as non-Bayesian methods.Additionally, we propose to replace the commonly used last-layer features withsketched finite-width neural tangent kernels and to combine them with a novelclustering method. To evaluate different methods, we introduce an open-sourcebenchmark consisting of 15 large tabular regression data sets. Our proposedmethod outperforms the state-of-the-art on our benchmark, scales to large datasets, and works out-of-the-box without adjusting the network architecture ortraining code. We provide open-source code that includes efficientimplementations of all kernels, kernel transformations, and selection methods,and can be used for reproducing our results.</description><author>David Holzmüller, Viktor Zaverkin, Johannes Kästner, Ingo Steinwart</author><pubDate>Tue, 01 Aug 2023 14:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.09410v4</guid></item><item><title>SurveyLM: A platform to explore emerging value perspectives in augmented language models' behaviors</title><link>http://arxiv.org/abs/2308.00521v1</link><description>This white paper presents our work on SurveyLM, a platform for analyzingaugmented language models' (ALMs) emergent alignment behaviors through theirdynamically evolving attitude and value perspectives in complex socialcontexts. Social Artificial Intelligence (AI) systems, like ALMs, oftenfunction within nuanced social scenarios where there is no singular correctresponse, or where an answer is heavily dependent on contextual factors, thusnecessitating an in-depth understanding of their alignment dynamics. To addressthis, we apply survey and experimental methodologies, traditionally used instudying social behaviors, to evaluate ALMs systematically, thus providingunprecedented insights into their alignment and emergent behaviors. Moreover,the SurveyLM platform leverages the ALMs' own feedback to enhance survey andexperiment designs, exploiting an underutilized aspect of ALMs, whichaccelerates the development and testing of high-quality survey frameworks whileconserving resources. Through SurveyLM, we aim to shed light on factorsinfluencing ALMs' emergent behaviors, facilitate their alignment with humanintentions and expectations, and thereby contributed to the responsibledevelopment and deployment of advanced social AI systems. This white paperunderscores the platform's potential to deliver robust results, highlightingits significance to alignment research and its implications for future socialAI systems.</description><author>Steve J. Bickley, Ho Fai Chan, Bang Dao, Benno Torgler, Son Tran</author><pubDate>Tue, 01 Aug 2023 13:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00521v1</guid></item><item><title>NormKD: Normalized Logits for Knowledge Distillation</title><link>http://arxiv.org/abs/2308.00520v1</link><description>Logit based knowledge distillation gets less attention in recent years sincefeature based methods perform better in most cases. Nevertheless, we find itstill has untapped potential when we re-investigate the temperature, which is acrucial hyper-parameter to soften the logit outputs. For most of the previousworks, it was set as a fixed value for the entire distillation procedure.However, as the logits from different samples are distributed quite variously,it is not feasible to soften all of them to an equal degree by just a singletemperature, which may make the previous work transfer the knowledge of eachsample inadequately. In this paper, we restudy the hyper-parameter temperatureand figure out its incapability to distill the knowledge from each samplesufficiently when it is a single value. To address this issue, we proposeNormalized Knowledge Distillation (NormKD), with the purpose of customizing thetemperature for each sample according to the characteristic of the sample'slogit distribution. Compared to the vanilla KD, NormKD barely has extracomputation or storage cost but performs significantly better on CIRAR-100 andImageNet for image classification. Furthermore, NormKD can be easily applied tothe other logit based methods and achieve better performance which can becloser to or even better than the feature based method.</description><author>Zhihao Chi, Tu Zheng, Hengjia Li, Zheng Yang, Boxi Wu, Binbin Lin, Deng Cai</author><pubDate>Tue, 01 Aug 2023 13:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00520v1</guid></item><item><title>Markerless human pose estimation for biomedical applications: a survey</title><link>http://arxiv.org/abs/2308.00519v1</link><description>Markerless Human Pose Estimation (HPE) proved its potential to supportdecision making and assessment in many fields of application. HPE is oftenpreferred to traditional marker-based Motion Capture systems due to the ease ofsetup, portability, and affordable cost of the technology. However, theexploitation of HPE in biomedical applications is still under investigation.This review aims to provide an overview of current biomedical applications ofHPE. In this paper, we examine the main features of HPE approaches and discusswhether or not those features are of interest to biomedical applications. Wealso identify those areas where HPE is already in use and present peculiaritiesand trends followed by researchers and practitioners. We include here 25approaches to HPE and more than 40 studies of HPE applied to motor developmentassessment, neuromuscolar rehabilitation, and gait &amp; posture analysis. Weconclude that markerless HPE offers great potential for extending diagnosis andrehabilitation outside hospitals and clinics, toward the paradigm of remotemedical care.</description><author>Andrea Avogaro, Federico Cunico, Bodo Rosenhahn, Francesco Setti</author><pubDate>Tue, 01 Aug 2023 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00519v1</guid></item><item><title>SepMark: Deep Separable Watermarking for Unified Source Tracing and Deepfake Detection</title><link>http://arxiv.org/abs/2305.06321v2</link><description>Malicious Deepfakes have led to a sharp conflict over distinguishing betweengenuine and forged faces. Although many countermeasures have been developed todetect Deepfakes ex-post, undoubtedly, passive forensics has not considered anypreventive measures for the pristine face before foreseeable manipulations. Tocomplete this forensics ecosystem, we thus put forward the proactive solutiondubbed SepMark, which provides a unified framework for source tracing andDeepfake detection. SepMark originates from encoder-decoder-based deepwatermarking but with two separable decoders. For the first time the deepseparable watermarking, SepMark brings a new paradigm to the established studyof deep watermarking, where a single encoder embeds one watermark elegantly,while two decoders can extract the watermark separately at different levels ofrobustness. The robust decoder termed Tracer that resists various distortionsmay have an overly high level of robustness, allowing the watermark to surviveboth before and after Deepfake. The semi-robust one termed Detector isselectively sensitive to malicious distortions, making the watermark disappearafter Deepfake. Only SepMark comprising of Tracer and Detector can reliablytrace the trusted source of the marked face and detect whether it has beenaltered since being marked; neither of the two alone can achieve this.Extensive experiments demonstrate the effectiveness of the proposed SepMark ontypical Deepfakes, including face swapping, expression reenactment, andattribute editing.</description><author>Xiaoshuai Wu, Xin Liao, Bo Ou</author><pubDate>Tue, 01 Aug 2023 13:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06321v2</guid></item><item><title>Noisy Interpolation Learning with Shallow Univariate ReLU Networks</title><link>http://arxiv.org/abs/2307.15396v2</link><description>We study the asymptotic overfitting behavior of interpolation with minimumnorm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariateregression. We show that overfitting is tempered for the $L_1$ loss, and any$L_p$ loss for $p&lt;2$, but catastrophic for $p\geq 2$.</description><author>Nirmit Joshi, Gal Vardi, Nathan Srebro</author><pubDate>Tue, 01 Aug 2023 13:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15396v2</guid></item><item><title>Relational Contrastive Learning for Scene Text Recognition</title><link>http://arxiv.org/abs/2308.00508v1</link><description>Context-aware methods achieved great success in supervised scene textrecognition via incorporating semantic priors from words. We argue that suchprior contextual information can be interpreted as the relations of textualprimitives due to the heterogeneous text and background, which can provideeffective self-supervised labels for representation learning. However, textualrelations are restricted to the finite size of dataset due to lexicaldependencies, which causes the problem of over-fitting and compromisesrepresentation robustness. To this end, we propose to enrich the textualrelations via rearrangement, hierarchy and interaction, and design a unifiedframework called RCLSTR: Relational Contrastive Learning for Scene TextRecognition. Based on causality, we theoretically explain that three modulessuppress the bias caused by the contextual prior and thus guaranteerepresentation robustness. Experiments on representation quality show that ourmethod outperforms state-of-the-art self-supervised STR methods. Code isavailable at https://github.com/ThunderVVV/RCLSTR.</description><author>Jinglei Zhang, Tiancheng Lin, Yi Xu, Kai Chen, Rui Zhang</author><pubDate>Tue, 01 Aug 2023 13:46:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00508v1</guid></item><item><title>Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer</title><link>http://arxiv.org/abs/2308.00507v1</link><description>Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal cancer in whichthe tumor-vascular involvement greatly affects the resectability and, thus,overall survival of patients. However, current prognostic prediction methodsfail to explicitly and accurately investigate relationships between the tumorand nearby important vessels. This paper proposes a novel learnable neuraldistance that describes the precise relationship between the tumor and vesselsin CT images of different patients, adopting it as a major feature forprognosis prediction. Besides, different from existing models that used CNNs orLSTMs to exploit tumor enhancement patterns on dynamic contrast-enhanced CTimaging, we improved the extraction of dynamic tumor-related texture featuresin multi-phase contrast-enhanced CT by fusing local and global features usingCNN and transformer modules, further enhancing the features extracted acrossmulti-phase CT images. We extensively evaluated and compared the proposedmethod with existing methods in the multi-center (n=4) dataset with 1,070patients with PDAC, and statistical analysis confirmed its clinicaleffectiveness in the external test set consisting of three centers. Thedeveloped risk marker was the strongest predictor of overall survival amongpreoperative factors and it has the potential to be combined with establishedclinical factors to select patients at higher risk who might benefit fromneoadjuvant therapy.</description><author>Hexin Dong, Jiawen Yao, Yuxing Tang, Mingze Yuan, Yingda Xia, Jian Zhou, Hong Lu, Jingren Zhou, Bin Dong, Le Lu, Li Zhang, Zaiyi Liu, Yu Shi, Ling Zhang</author><pubDate>Tue, 01 Aug 2023 13:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00507v1</guid></item><item><title>Hyper-Laplacian Regularized Concept Factorization in Low-rank Tensor Space for Multi-view Clustering</title><link>http://arxiv.org/abs/2304.11435v2</link><description>Tensor-oriented multi-view subspace clustering has achieved significantstrides in assessing high-order correlations and improving clustering analysisof multi-view data. Nevertheless, most of existing investigations are typicallyhampered by the two flaws. First, self-representation based tensor subspacelearning usually induces high time and space complexity, and is limited inperceiving nonlinear local structure in the embedding space. Second, the tensorsingular value decomposition (t-SVD) model redistributes each singular valueequally without considering the diverse importance among them. To well copewith the issues, we propose a hyper-Laplacian regularized concept factorization(HLRCF) in low-rank tensor space for multi-view clustering. Specifically, weadopt the concept factorization to explore the latent cluster-wiserepresentation of each view. Further, the hypergraph Laplacian regularizationendows the model with the capability of extracting the nonlinear localstructures in the latent space. Considering that different tensor singularvalues associate structural information with unequal importance, we develop aself-weighted tensor Schatten p-norm to constrain the tensor comprised of allcluster-wise representations. Notably, the tensor with smaller size greatlydecreases the time and space complexity in the low-rank optimization. Finally,experimental results on eight benchmark datasets exhibit that HLRCF outperformsother multi-view methods, showingcasing its superior performance.</description><author>Zixiao Yu, Lele Fu, Zhiling Cai, Zhoumin Lu</author><pubDate>Tue, 01 Aug 2023 13:43:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11435v2</guid></item><item><title>Explainable Graph Spectral Clustering of Text Documents</title><link>http://arxiv.org/abs/2308.00504v1</link><description>Spectral clustering methods are known for their ability to represent clustersof diverse shapes, densities etc. However, results of such algorithms, whenapplied e.g. to text documents, are hard to explain to the user, especially dueto embedding in the spectral space which has no obvious relation to documentcontents. Therefore there is an urgent need to elaborate methods for explainingthe outcome of the clustering. This paper presents a contribution towards thisgoal. We present a proposal of explanation of results of combinatorialLaplacian based graph spectral clustering. It is based on showing (approximate)equivalence of combinatorial Laplacian embedding, $K$-embedding (proposed inthis paper) and term vector space embedding. Hence a bridge is constructedbetween the textual contents and the clustering results. We provide theoreticalbackground for this approach. We performed experimental study showing that$K$-embedding approximates well Laplacian embedding under favourable blockmatrix conditions and show that approximation is good enough under otherconditions.</description><author>Bartłomiej Starosta, Mieczysław A. Kłopotek, Sławomir T. Wierzchoń</author><pubDate>Tue, 01 Aug 2023 13:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00504v1</guid></item><item><title>On the Universality of the Double Descent Peak in Ridgeless Regression</title><link>http://arxiv.org/abs/2010.01851v8</link><description>We prove a non-asymptotic distribution-independent lower bound for theexpected mean squared generalization error caused by label noise in ridgelesslinear regression. Our lower bound generalizes a similar known result to theoverparameterized (interpolating) regime. In contrast to most previous works,our analysis applies to a broad class of input distributions with almost surelyfull-rank feature matrices, which allows us to cover various types ofdeterministic or random feature maps. Our lower bound is asymptotically sharpand implies that in the presence of label noise, ridgeless linear regressiondoes not perform well around the interpolation threshold for any of thesefeature maps. We analyze the imposed assumptions in detail and provide a theoryfor analytic (random) feature maps. Using this theory, we can show that ourassumptions are satisfied for input distributions with a (Lebesgue) density andfeature maps given by random deep neural networks with analytic activationfunctions like sigmoid, tanh, softplus or GELU. As further examples, we showthat feature maps from random Fourier features and polynomial kernels alsosatisfy our assumptions. We complement our theory with further experimental andanalytic results.</description><author>David Holzmüller</author><pubDate>Tue, 01 Aug 2023 13:36:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2010.01851v8</guid></item><item><title>Accurate generation of stochastic dynamics based on multi-model Generative Adversarial Networks</title><link>http://arxiv.org/abs/2305.15920v2</link><description>Generative Adversarial Networks (GANs) have shown immense potential in fieldssuch as text and image generation. Only very recently attempts to exploit GANsto statistical-mechanics models have been reported. Here we quantitatively testthis approach by applying it to a prototypical stochastic process on a lattice.By suitably adding noise to the original data we succeed in bringing both theGenerator and the Discriminator loss functions close to their ideal value.Importantly, the discreteness of the model is retained despite the noise. Astypical for adversarial approaches, oscillations around the convergence limitpersist also at large epochs. This undermines model selection and the qualityof the generated trajectories. We demonstrate that a simple multi-modelprocedure where stochastic trajectories are advanced at each step upon randomlyselecting a Generator leads to a remarkable increase in accuracy. This isillustrated by quantitative analysis of both the predicted equilibriumprobability distribution and of the escape-time distribution. Based on thereported findings, we believe that GANs are a promising tool to tackle complexstatistical dynamics by machine learning techniques</description><author>Daniele Lanzoni, Olivier Pierre-Louis, Francesco Montalenti</author><pubDate>Tue, 01 Aug 2023 13:23:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15920v2</guid></item><item><title>An L2-Normalized Spatial Attention Network For Accurate And Fast Classification Of Brain Tumors In 2D T1-Weighted CE-MRI Images</title><link>http://arxiv.org/abs/2308.00491v1</link><description>We propose an accurate and fast classification network for classification ofbrain tumors in MRI images that outperforms all lightweight methodsinvestigated in terms of accuracy. We test our model on a challenging 2DT1-weighted CE-MRI dataset containing three types of brain tumors: Meningioma,Glioma and Pituitary. We introduce an l2-normalized spatial attention mechanismthat acts as a regularizer against overfitting during training. We compare ourresults against the state-of-the-art on this dataset and show that byintegrating l2-normalized spatial attention into a baseline network we achievea performance gain of 1.79 percentage points. Even better accuracy can beattained by combining our model in an ensemble with the pretrained VGG16 at theexpense of execution speed. Our code is publicly available athttps://github.com/juliadietlmeier/MRI_image_classification</description><author>Grace Billingsley, Julia Dietlmeier, Vivek Narayanaswamy, Andreas Spanias, Noel E. OConnor</author><pubDate>Tue, 01 Aug 2023 13:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00491v1</guid></item><item><title>Seeded graph matching for the correlated Wigner model via the projected power method</title><link>http://arxiv.org/abs/2204.04099v2</link><description>In the \emph{graph matching} problem we observe two graphs $G,H$ and the goalis to find an assignment (or matching) between their vertices such that somemeasure of edge agreement is maximized. We assume in this work that theobserved pair $G,H$ has been drawn from the correlated Wigner model -- apopular model for correlated weighted graphs -- where the entries of theadjacency matrices of $G$ and $H$ are independent Gaussians and each edge of$G$ is correlated with one edge of $H$ (determined by the unknown matching)with the edge correlation described by a parameter $\sigma\in [0,1)$. In thispaper, we analyse the performance of the \emph{projected power method} (PPM) asa \emph{seeded} graph matching algorithm where we are given an initialpartially correct matching (called the seed) as side information. We prove thatif the seed is close enough to the ground-truth matching, then with highprobability, PPM iteratively improves the seed and recovers the ground-truthmatching (either partially or exactly) in $\mathcal{O}(\log n)$ iterations. Ourresults prove that PPM works even in regimes of constant $\sigma$, thusextending the analysis in \citep{MaoRud} for the sparse Erd\H{o}s-R\'enyi modelto the (dense) Wigner model. As a byproduct of our analysis, we see that thePPM framework generalizes some of the state-of-art algorithms for seeded graphmatching. We support and complement our theoretical findings with numericalexperiments on synthetic data.</description><author>Ernesto Araya, Guillaume Braun, Hemant Tyagi</author><pubDate>Tue, 01 Aug 2023 13:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.04099v2</guid></item><item><title>Verifiable Goal Recognition for Autonomous Driving with Occlusions</title><link>http://arxiv.org/abs/2206.14163v2</link><description>Goal recognition (GR) involves inferring the goals of other vehicles, such asa certain junction exit, which can enable more accurate prediction of theirfuture behaviour. In autonomous driving, vehicles can encounter many differentscenarios and the environment may be partially observable due to occlusions. Wepresent a novel GR method named Goal Recognition with Interpretable Trees underOcclusion (OGRIT). OGRIT uses decision trees learned from vehicle trajectorydata to infer the probabilities of a set of generated goals. We demonstratethat OGRIT can handle missing data due to occlusions and make inferences acrossmultiple scenarios using the same learned decision trees, while beingcomputationally fast, accurate, interpretable and verifiable. We also releasethe inDO, rounDO and OpenDDO datasets of occluded regions used to evaluateOGRIT.</description><author>Cillian Brewitt, Massimiliano Tamborski, Cheng Wang, Stefano V. Albrecht</author><pubDate>Tue, 01 Aug 2023 13:18:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.14163v2</guid></item><item><title>In-Context Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2302.00083v3</link><description>Retrieval-Augmented Language Modeling (RALM) methods, which condition alanguage model (LM) on relevant documents from a grounding corpus duringgeneration, were shown to significantly improve language modeling performance.In addition, they can mitigate the problem of factually inaccurate textgeneration and provide natural source attribution mechanism. Existing RALMapproaches focus on modifying the LM architecture in order to facilitate theincorporation of external information, significantly complicating deployment.This paper considers a simple alternative, which we dub In-Context RALM:leaving the LM architecture unchanged and prepending grounding documents to theinput, without any further training of the LM. We show that In-Context RALMthat builds on off-the-shelf general purpose retrievers provides surprisinglylarge LM gains across model sizes and diverse corpora. We also demonstrate thatthe document retrieval and ranking mechanism can be specialized to the RALMsetting to further boost performance. We conclude that In-Context RALM hasconsiderable potential to increase the prevalence of LM grounding, particularlyin settings where a pretrained LM must be used without modification or even viaAPI access.</description><author>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham</author><pubDate>Tue, 01 Aug 2023 13:10:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00083v3</guid></item><item><title>Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education</title><link>http://arxiv.org/abs/2308.00479v1</link><description>Large Language Models are increasingly being used for various tasks includingcontent generation and as chatbots. Despite their impressive performances ingeneral tasks, LLMs need to be aligned when applying for domain specific tasksto mitigate the problems of hallucination and producing harmful answers.Retrieval Augmented Generation (RAG) allows to easily attach and manipulate anon-parametric knowledgebases to LLMs. Applications of RAG in the field ofmedical education are discussed in this paper. A combined extractive andabstractive summarization method for large unstructured textual data usingrepresentative vectors is proposed.</description><author>S. S. Manathunga, Y. A. Illangasekara</author><pubDate>Tue, 01 Aug 2023 13:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00479v1</guid></item><item><title>On Multi-objective Policy Optimization as a Tool for Reinforcement Learning: Case Studies in Offline RL and Finetuning</title><link>http://arxiv.org/abs/2106.08199v2</link><description>Many advances that have improved the robustness and efficiency of deepreinforcement learning (RL) algorithms can, in one way or another, beunderstood as introducing additional objectives or constraints in the policyoptimization step. This includes ideas as far ranging as exploration bonuses,entropy regularization, and regularization toward teachers or data priors.Often, the task reward and auxiliary objectives are in conflict, and in thispaper we argue that this makes it natural to treat these cases as instances ofmulti-objective (MO) optimization problems. We demonstrate how this perspectiveallows us to develop novel and more effective RL algorithms. In particular, wefocus on offline RL and finetuning as case studies, and show that existingapproaches can be understood as MO algorithms relying on linear scalarization.We hypothesize that replacing linear scalarization with a better algorithm canimprove performance. We introduce Distillation of a Mixture of Experts (DiME),a new MORL algorithm that outperforms linear scalarization and can be appliedto these non-standard MO problems. We demonstrate that for offline RL, DiMEleads to a simple new algorithm that outperforms state-of-the-art. Forfinetuning, we derive new algorithms that learn to outperform the teacherpolicy.</description><author>Abbas Abdolmaleki, Sandy H. Huang, Giulia Vezzani, Bobak Shahriari, Jost Tobias Springenberg, Shruti Mishra, Dhruva TB, Arunkumar Byravan, Konstantinos Bousmalis, Andras Gyorgy, Csaba Szepesvari, Raia Hadsell, Nicolas Heess, Martin Riedmiller</author><pubDate>Tue, 01 Aug 2023 13:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.08199v2</guid></item><item><title>DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification</title><link>http://arxiv.org/abs/2308.00475v1</link><description>The limited availability of labeled chest X-ray datasets is a significantbottleneck in the development of medical imaging methods. Self-supervisedlearning (SSL) can mitigate this problem by training models on unlabeled data.Furthermore, self-supervised pretraining has yielded promising results invisual recognition of natural images but has not been given much considerationin medical image analysis. In this work, we propose a self-supervised method,DINO-CXR, which is a novel adaptation of a self-supervised method, DINO, basedon a vision transformer for chest X-ray classification. A comparative analysisis performed to show the effectiveness of the proposed method for bothpneumonia and COVID-19 detection. Through a quantitative analysis, it is alsoshown that the proposed method outperforms state-of-the-art methods in terms ofaccuracy and achieves comparable results in terms of AUC and F-1 score whilerequiring significantly less labeled data.</description><author>Mohammadreza Shakouri, Fatemeh Iranmanesh, Mahdi Eftekhari</author><pubDate>Tue, 01 Aug 2023 12:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00475v1</guid></item><item><title>Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?</title><link>http://arxiv.org/abs/2308.00473v1</link><description>Models trained with empirical risk minimization (ERM) are known to learn torely on spurious features, i.e., their prediction is based on undesiredauxiliary features which are strongly correlated with class labels but lackcausal reasoning. This behavior particularly degrades accuracy in groups ofsamples of the correlated class that are missing the spurious feature orsamples of the opposite class but with the spurious feature present. Therecently proposed Deep Feature Reweighting (DFR) method improves accuracy ofthese worst groups. Based on the main argument that ERM mods can learn corefeatures sufficiently well, DFR only needs to retrain the last layer of theclassification model with a small group-balanced data set. In this work, weexamine the applicability of DFR to realistic data in the medical domain.Furthermore, we investigate the reasoning behind the effectiveness oflast-layer retraining and show that even though DFR has the potential toimprove the accuracy of the worst group, it remains susceptible to spuriouscorrelations.</description><author>Phuong Quynh Le, Jörg Schlötterer, Christin Seifert</author><pubDate>Tue, 01 Aug 2023 12:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00473v1</guid></item><item><title>Kernel interpolation generalizes poorly</title><link>http://arxiv.org/abs/2303.15809v2</link><description>One of the most interesting problems in the recent renaissance of the studiesin kernel regression might be whether the kernel interpolation can generalizewell, since it may help us understand the `benign overfitting henomenon'reported in the literature on deep networks. In this paper, under mildconditions, we show that for any $\varepsilon&gt;0$, the generalization error ofkernel interpolation is lower bounded by $\Omega(n^{-\varepsilon})$. In otherwords, the kernel interpolation generalizes poorly for a large class ofkernels. As a direct corollary, we can show that overfitted wide neuralnetworks defined on the sphere generalize poorly.</description><author>Yicheng Li, Haobo Zhang, Qian Lin</author><pubDate>Tue, 01 Aug 2023 12:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15809v2</guid></item><item><title>(Local) Differential Privacy has NO Disparate Impact on Fairness</title><link>http://arxiv.org/abs/2304.12845v2</link><description>In recent years, Local Differential Privacy (LDP), a robustprivacy-preserving methodology, has gained widespread adoption in real-worldapplications. With LDP, users can perturb their data on their devices beforesending it out for analysis. However, as the collection of multiple sensitiveinformation becomes more prevalent across various industries, collecting asingle sensitive attribute under LDP may not be sufficient. Correlatedattributes in the data may still lead to inferences about the sensitiveattribute. This paper empirically studies the impact of collecting multiplesensitive attributes under LDP on fairness. We propose a novel privacy budgetallocation scheme that considers the varying domain size of sensitiveattributes. This generally led to a better privacy-utility-fairness trade-offin our experiments than the state-of-art solution. Our results show that LDPleads to slightly improved fairness in learning problems without significantlyaffecting the performance of the models. We conduct extensive experimentsevaluating three benchmark datasets using several group fairness metrics andseven state-of-the-art LDP protocols. Overall, this study challenges the commonbelief that differential privacy necessarily leads to worsened fairness inmachine learning.</description><author>Héber H. Arcolezi, Karima Makhlouf, Catuscia Palamidessi</author><pubDate>Tue, 01 Aug 2023 12:50:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12845v2</guid></item><item><title>A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography</title><link>http://arxiv.org/abs/2308.00471v1</link><description>Contrast Enhanced Spectral Mammography (CESM) is a dual-energy mammographicimaging technique that first needs intravenously administration of an iodinatedcontrast medium; then, it collects bot a low-energy image, comparable tostandard mammography, and a high-energy image. The two scans are then combinedto get a recombined image showing contrast enhancement. Despite CESM diagnosticadvantages for breast cancer diagnosis, the use of contrast medium can causeside effects, and CESM also beams patients with a higher radiation dosecompared to standard mammography. To address these limitations this workproposes to use deep generative models for virtual contrast enhancement onCESM, aiming to make the CESM contrast-free as well as to reduce the radiationdose. Our deep networks, consisting of an autoencoder and two GenerativeAdversarial Networks, the Pix2Pix, and the CycleGAN, generate syntheticrecombined images solely from low-energy images. We perform an extensivequantitative and qualitative analysis of the model's performance, alsoexploiting radiologists' assessments, on a novel CESM dataset that includes1138 images that, as a further contribution of this work, we make publiclyavailable. The results show that CycleGAN is the most promising deep network togenerate synthetic recombined images, highlighting the potential of artificialintelligence techniques for virtual contrast enhancement in this field.</description><author>Aurora Rofena, Valerio Guarrasi, Marina Sarli, Claudia Lucia Piccolo, Matteo Sammarra, Bruno Beomonte Zobel, Paolo Soda</author><pubDate>Tue, 01 Aug 2023 12:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00471v1</guid></item><item><title>Mirror Natural Evolution Strategies</title><link>http://arxiv.org/abs/2308.00469v1</link><description>The zeroth-order optimization has been widely used in machine learningapplications. However, the theoretical study of the zeroth-order optimizationfocus on the algorithms which approximate (first-order) gradients using(zeroth-order) function value difference at a random direction. The theory ofalgorithms which approximate the gradient and Hessian information byzeroth-order queries is much less studied. In this paper, we focus on thetheory of zeroth-order optimization which utilizes both the first-order andsecond-order information approximated by the zeroth-order queries. We firstpropose a novel reparameterized objective function with parameters $(\mu,\Sigma)$. This reparameterized objective function achieves its optimum at theminimizer and the Hessian inverse of the original objective functionrespectively, but with small perturbations. Accordingly, we propose a newalgorithm to minimize our proposed reparameterized objective, which we call\texttt{MiNES} (mirror descent natural evolution strategy). We show that theestimated covariance matrix of \texttt{MiNES} converges to the inverse ofHessian matrix of the objective function with a convergence rate$\widetilde{\mathcal{O}}(1/k)$, where $k$ is the iteration number and$\widetilde{\mathcal{O}}(\cdot)$ hides the constant and $\log$ terms. We alsoprovide the explicit convergence rate of \texttt{MiNES} and how the covariancematrix promotes the convergence rate.</description><author>Haishan Ye</author><pubDate>Tue, 01 Aug 2023 12:45:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00469v1</guid></item><item><title>ChiPFormer: Transferable Chip Placement via Offline Decision Transformer</title><link>http://arxiv.org/abs/2306.14744v2</link><description>Placement is a critical step in modern chip design, aiming to determine thepositions of circuit modules on the chip canvas. Recent works have shown thatreinforcement learning (RL) can improve human performance in chip placement.However, such an RL-based approach suffers from long training time and lowtransfer ability in unseen chip circuits. To resolve these challenges, we castthe chip placement as an offline RL formulation and present ChiPFormer thatenables learning a transferable placement policy from fixed offline data.ChiPFormer has several advantages that prior arts do not have. First,ChiPFormer can exploit offline placement designs to learn transferable policiesmore efficiently in a multi-task setting. Second, ChiPFormer can promoteeffective finetuning for unseen chip circuits, reducing the placement runtimefrom hours to minutes. Third, extensive experiments on 32 chip circuitsdemonstrate that ChiPFormer achieves significantly better placement qualitywhile reducing the runtime by 10x compared to recent state-of-the-artapproaches in both public benchmarks and realistic industrial tasks. Thedeliverables are released at https://sites.google.com/view/chipformer/home.</description><author>Yao Lai, Jinxin Liu, Zhentao Tang, Bin Wang, Jianye Hao, Ping Luo</author><pubDate>Tue, 01 Aug 2023 12:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14744v2</guid></item><item><title>A Satellite Imagery Dataset for Long-Term Sustainable Development in United States Cities</title><link>http://arxiv.org/abs/2308.00465v1</link><description>Cities play an important role in achieving sustainable development goals(SDGs) to promote economic growth and meet social needs. Especially satelliteimagery is a potential data source for studying sustainable urban development.However, a comprehensive dataset in the United States (U.S.) covering multiplecities, multiple years, multiple scales, and multiple indicators for SDGmonitoring is lacking. To support the research on SDGs in U.S. cities, wedevelop a satellite imagery dataset using deep learning models for five SDGscontaining 25 sustainable development indicators. The proposed dataset coversthe 100 most populated U.S. cities and corresponding Census Block Groups from2014 to 2023. Specifically, we collect satellite imagery and identify objectswith state-of-the-art object detection and semantic segmentation models toobserve cities' bird's-eye view. We further gather population, nighttime light,survey, and built environment data to depict SDGs regarding poverty, health,education, inequality, and living environment. We anticipate the dataset tohelp urban policymakers and researchers to advance SDGs-related studies,especially applying satellite imagery to monitor long-term and multi-scale SDGsin cities.</description><author>Yanxin Xi, Yu Liu, Tong Li, Jintao Ding, Yunke Zhang, Sasu Tarkoma, Yong Li, Pan Hui</author><pubDate>Tue, 01 Aug 2023 12:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00465v1</guid></item><item><title>Center Contrastive Loss for Metric Learning</title><link>http://arxiv.org/abs/2308.00458v1</link><description>Contrastive learning is a major studied topic in metric learning. However,sampling effective contrastive pairs remains a challenge due to factors such aslimited batch size, imbalanced data distribution, and the risk of overfitting.In this paper, we propose a novel metric learning function called CenterContrastive Loss, which maintains a class-wise center bank and compares thecategory centers with the query data points using a contrastive loss. Thecenter bank is updated in real-time to boost model convergence without the needfor well-designed sample mining. The category centers are well-optimizedclassification proxies to re-balance the supervisory signal of each class.Furthermore, the proposed loss combines the advantages of both contrastive andclassification methods by reducing intra-class variations and enhancinginter-class differences to improve the discriminative power of embeddings. Ourexperimental results, as shown in Figure 1, demonstrate that a standard network(ResNet50) trained with our loss achieves state-of-the-art performance andfaster convergence.</description><author>Bolun Cai, Pengfei Xiong, Shangxuan Tian</author><pubDate>Tue, 01 Aug 2023 12:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00458v1</guid></item><item><title>Causal Discovery and Knowledge Injection for Contestable Neural Networks (with Appendices)</title><link>http://arxiv.org/abs/2205.09787v4</link><description>Neural networks have proven to be effective at solving machine learning tasksbut it is unclear whether they learn any relevant causal relationships, whiletheir black-box nature makes it difficult for modellers to understand and debugthem. We propose a novel method overcoming these issues by allowing a two-wayinteraction whereby neural-network-empowered machines can expose theunderpinning learnt causal graphs and humans can contest the machines bymodifying the causal graphs before re-injecting them into the machines. Thelearnt models are guaranteed to conform to the graphs and adhere to expertknowledge, some of which can also be given up-front. By building a window intothe model behaviour and enabling knowledge injection, our method allowspractitioners to debug networks based on the causal structure discovered fromthe data and underpinning the predictions. Experiments with real and synthetictabular data show that our method improves predictive performance up to 2.4xwhile producing parsimonious networks, up to 7x smaller in the input layer,compared to SOTA regularised networks.</description><author>Fabrizio Russo, Francesca Toni</author><pubDate>Tue, 01 Aug 2023 12:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.09787v4</guid></item><item><title>DMFC-GraspNet: Differentiable Multi-Fingered Robotic Grasp Generation in Cluttered Scenes</title><link>http://arxiv.org/abs/2308.00456v1</link><description>Robotic grasping is a fundamental skill required for object manipulation inrobotics. Multi-fingered robotic hands, which mimic the structure of the humanhand, can potentially perform complex object manipulations. Nevertheless,current techniques for multi-fingered robotic grasping frequently predict onlya single grasp for each inference time, limiting their versatility andefficiency. This paper proposes a differentiable multi-fingered graspgeneration network (DMFC-GraspNet) with two main contributions to address thischallenge. Firstly, a novel neural grasp planner is proposed, which predicts anew grasp representation to enable versatile and dense grasp predictions.Secondly, a scene creation and label mapping method is developed for denselabeling of multi-fingered robotic hands, which allows a dense association ofground truth grasps. The proposed approach is evaluated through simulationstudies and compared to existing approaches. The results demonstrate theeffectiveness of the proposed approach in predicting versatile and densegrasps, and in advancing the field of robotic grasping.</description><author>Philipp Blättner, Johannes Brand, Gerhard Neumann, Ngo Anh Vien</author><pubDate>Tue, 01 Aug 2023 12:21:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00456v1</guid></item></channel></rss>