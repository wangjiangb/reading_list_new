<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 24 Mar 2024 14:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Zero-Shot Multi-Object Shape Completion</title><link>http://arxiv.org/abs/2403.14628v1</link><description>We present a 3D shape completion method that recovers the complete geometryof multiple objects in complex scenes from a single RGB-D image. Despitenotable advancements in single object 3D shape completion, high-qualityreconstructions in highly cluttered real-world multi-object scenes remains achallenge. To address this issue, we propose OctMAE, an architecture thatleverages an Octree U-Net and a latent 3D MAE to achieve high-quality and nearreal-time multi-object shape completion through both local and global geometricreasoning. Because a na\"ive 3D MAE can be computationally intractable andmemory intensive even in the latent space, we introduce a novel occlusionmasking strategy and adopt 3D rotary embeddings, which significantly improvesthe runtime and shape completion quality. To generalize to a wide range ofobjects in diverse scenes, we create a large-scale photorealistic dataset,featuring a diverse set of 12K 3D object models from the Objaverse datasetwhich are rendered in multi-object scenes with physics-based positioning. Ourmethod outperforms the current state-of-the-art on both synthetic andreal-world datasets and demonstrates a strong zero-shot capability.</description><author>Shun Iwase, Katherine Liu, Vitor Guizilini, Adrien Gaidon, Kris Kitani, Rares Ambrus, Sergey Zakharov</author><pubDate>Thu, 21 Mar 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14628v1</guid></item><item><title>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</title><link>http://arxiv.org/abs/2403.14627v1</link><description>We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting modellearned from sparse multi-view images. To accurately localize the Gaussiancenters, we propose to build a cost volume representation via plane sweeping inthe 3D space, where the cross-view feature similarities stored in the costvolume can provide valuable geometry cues to the estimation of depth. We learnthe Gaussian primitives' opacities, covariances, and spherical harmonicscoefficients jointly with the Gaussian centers while only relying onphotometric supervision. We demonstrate the importance of the cost volumerepresentation in learning feed-forward Gaussian Splatting models via extensiveexperimental evaluations. On the large-scale RealEstate10K and ACID benchmarks,our model achieves state-of-the-art performance with the fastest feed-forwardinference speed (22 fps). Compared to the latest state-of-the-art methodpixelSplat, our model uses $10\times $ fewer parameters and infers more than$2\times$ faster while providing higher appearance and geometry quality as wellas better cross-dataset generalization.</description><author>Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</author><pubDate>Thu, 21 Mar 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14627v1</guid></item><item><title>ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer</title><link>http://arxiv.org/abs/2403.14626v1</link><description>Obstacle detection and tracking represent a critical component in robotautonomous navigation. In this paper, we propose ODTFormer, a Transformer-basedmodel to address both obstacle detection and tracking problems. For thedetection task, our approach leverages deformable attention to construct a 3Dcost volume, which is decoded progressively in the form of voxel occupancygrids. We further track the obstacles by matching the voxels betweenconsecutive frames. The entire model can be optimized in an end-to-end manner.Through extensive experiments on DrivingStereo and KITTI benchmarks, our modelachieves state-of-the-art performance in the obstacle detection task. We alsoreport comparable accuracy to state-of-the-art obstacle tracking models whilerequiring only a fraction of their computation cost, typically ten-fold totwenty-fold less. The code and model weights will be publicly released.</description><author>Tianye Ding, Hongyu Li, Huaizu Jiang</author><pubDate>Thu, 21 Mar 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14626v1</guid></item><item><title>LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors</title><link>http://arxiv.org/abs/2403.14625v1</link><description>We present a simple self-supervised method to enhance the performance of ViTfeatures for dense downstream tasks. Our Lightweight Feature Transform (LiFT)is a straightforward and compact postprocessing network that can be applied toenhance the features of any pre-trained ViT backbone. LiFT is fast and easy totrain with a self-supervised objective, and it boosts the density of ViTfeatures for minimal extra inference cost. Furthermore, we demonstrate thatLiFT can be applied with approaches that use additional task-specificdownstream modules, as we integrate LiFT with ViTDet for COCO detection andsegmentation. Despite the simplicity of LiFT, we find that it is not simplylearning a more complex version of bilinear interpolation. Instead, our LiFTtraining protocol leads to several desirable emergent properties that benefitViT features in dense downstream tasks. This includes greater scale invariancefor features, and better object boundary maps. By simply training LiFT for afew epochs, we show improved performance on keypoint correspondence, detection,segmentation, and object discovery tasks. Overall, LiFT provides an easy way tounlock the benefits of denser feature arrays for a fraction of thecomputational cost. For more details, refer to our project page athttps://www.cs.umd.edu/~sakshams/LiFT/.</description><author>Saksham Suri, Matthew Walmer, Kamal Gupta, Abhinav Shrivastava</author><pubDate>Thu, 21 Mar 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14625v1</guid></item><item><title>MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</title><link>http://arxiv.org/abs/2403.14624v1</link><description>The remarkable progress of Multi-modal Large Language Models (MLLMs) hasgarnered unparalleled attention, due to their superior performance in visualcontexts. However, their capabilities in visual math problem-solving remaininsufficiently evaluated and understood. We investigate current benchmarks toincorporate excessive visual content within textual questions, whichpotentially assist MLLMs in deducing answers without truly interpreting theinput diagrams. To this end, we introduce MathVerse, an all-around visual mathbenchmark designed for an equitable and in-depth evaluation of MLLMs. Wemeticulously collect 2,612 high-quality, multi-subject math problems withdiagrams from publicly available sources. Each problem is then transformed byhuman annotators into six distinct versions, each offering varying degrees ofinformation content in multi-modality, contributing to 15K test samples intotal. This approach allows MathVerse to comprehensively assess whether and howmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for afine-grained assessment of the output answers. Rather than naively judging Trueor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, andthen score each step with detailed error analysis, which can reveal theintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmarkmay provide unique insights to guide the future development of MLLMs. Projectpage: https://mathverse-cuhk.github.io</description><author>Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li</author><pubDate>Thu, 21 Mar 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14624v1</guid></item><item><title>Simplified Diffusion Schrödinger Bridge</title><link>http://arxiv.org/abs/2403.14623v1</link><description>This paper introduces a novel theoretical simplification of the DiffusionSchr\"odinger Bridge (DSB) that facilitates its unification with Score-basedGenerative Models (SGMs), addressing the limitations of DSB in complex datageneration and enabling faster convergence and enhanced performance. Byemploying SGMs as an initial solution for DSB, our approach capitalizes on thestrengths of both frameworks, ensuring a more efficient training process andimproving the performance of SGM. We also propose a reparameterizationtechnique that, despite theoretical approximations, practically improves thenetwork's fitting capabilities. Our extensive experimental evaluations confirmthe effectiveness of the simplified DSB, demonstrating its significantimprovements. We believe the contributions of this work pave the way foradvanced generative modeling. The code is available athttps://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.</description><author>Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo</author><pubDate>Thu, 21 Mar 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14623v1</guid></item><item><title>Language Repository for Long Video Understanding</title><link>http://arxiv.org/abs/2403.14622v1</link><description>Language has become a prominent modality in computer vision with the rise ofmulti-modal LLMs. Despite supporting long context-lengths, their effectivenessin handling long-term information gradually declines with input length. Thisbecomes critical, especially in applications such as long-form videounderstanding. In this paper, we introduce a Language Repository (LangRepo) forLLMs, that maintains concise and structured information as an interpretable(i.e., all-textual) representation. Our repository is updated iteratively basedon multi-scale video chunks. We introduce write and read operations that focuson pruning redundancies in text, and extracting information at various temporalscales. The proposed framework is evaluated on zero-shot visualquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA andNExT-GQA, showing state-of-the-art performance at its scale. Our code isavailable at https://github.com/kkahatapitiya/LangRepo.</description><author>Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, Michael S. Ryoo</author><pubDate>Thu, 21 Mar 2024 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14622v1</guid></item><item><title>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation</title><link>http://arxiv.org/abs/2403.14621v1</link><description>We introduce GRM, a large-scale reconstructor capable of recovering a 3Dasset from sparse-view images in around 0.1s. GRM is a feed-forwardtransformer-based model that efficiently incorporates multi-view information totranslate the input pixels into pixel-aligned Gaussians, which are unprojectedto create a set of densely distributed 3D Gaussians representing a scene.Together, our transformer architecture and the use of 3D Gaussians unlock ascalable and efficient reconstruction framework. Extensive experimental resultsdemonstrate the superiority of our method over alternatives regarding bothreconstruction quality and efficiency. We also showcase the potential of GRM ingenerative tasks, i.e., text-to-3D and image-to-3D, by integrating it withexisting multi-view diffusion models. Our project website is at:https://justimyhxu.github.io/projects/grm/.</description><author>Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</author><pubDate>Thu, 21 Mar 2024 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14621v1</guid></item><item><title>ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition</title><link>http://arxiv.org/abs/2403.14619v1</link><description>3D decomposition/segmentation still remains a challenge as large-scale 3Dannotated data is not readily available. Contemporary approaches typicallyleverage 2D machine-generated segments, integrating them for 3D consistency.While the majority of these methods are based on NeRFs, they face a potentialweakness that the instance/semantic embedding features derive from independentMLPs, thus preventing the segmentation network from learning the geometricdetails of the objects directly through radiance and density. In this paper, wepropose ClusteringSDF, a novel approach to achieve both segmentation andreconstruction in 3D via the neural implicit surface representation,specifically Signal Distance Function (SDF), where the segmentation renderingis directly integrated with the volume rendering of neural implicit surfaces.Although based on ObjectSDF++, ClusteringSDF no longer requires theground-truth segments for supervision while maintaining the capability ofreconstructing individual object surfaces, but purely with the noisy andinconsistent labels from pre-trained models.As the core of ClusteringSDF, weintroduce a high-efficient clustering mechanism for lifting the 2D labels to 3Dand the experimental results on the challenging scenes from ScanNet and Replicadatasets show that ClusteringSDF can achieve competitive performance comparedagainst the state-of-the-art with significantly reduced training time.</description><author>Tianhao Wu, Chuanxia Zheng, Tat-Jen Cham, Qianyi Wu</author><pubDate>Thu, 21 Mar 2024 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14619v1</guid></item><item><title>Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion</title><link>http://arxiv.org/abs/2403.14617v1</link><description>We introduce Videoshop, a training-free video editing algorithm for localizedsemantic edits. Videoshop allows users to use any editing software, includingPhotoshop and generative inpainting, to modify the first frame; itautomatically propagates those changes, with semantic, spatial, and temporallyconsistent motion, to the remaining frames. Unlike existing methods that enableedits only through imprecise textual instructions, Videoshop allows users toadd or remove objects, semantically change objects, insert stock photos intovideos, etc. with fine-grained control over locations and appearance. Weachieve this through image-based video editing by inverting latents with noiseextrapolation, from which we generate videos conditioned on the edited image.Videoshop produces higher quality edits against 6 baselines on 2 editingbenchmarks using 10 evaluation metrics.</description><author>Xiang Fan, Anand Bhattad, Ranjay Krishna</author><pubDate>Thu, 21 Mar 2024 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14617v1</guid></item><item><title>Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning</title><link>http://arxiv.org/abs/2403.14616v1</link><description>Self-supervised representation learning has been highly promising forhistopathology image analysis with numerous approaches leveraging theirpatient-slide-patch hierarchy to learn better representations. In this paper,we explore how the combination of domain specific natural language informationwith such hierarchical visual representations can benefit rich representationlearning for medical image tasks. Building on automated language descriptiongeneration for features visible in histopathology images, we present a novellanguage-tied self-supervised learning framework, Hierarchical Language-tiedSelf-Supervision (HLSS) for histopathology images. We explore contrastiveobjectives and granular language description based text alignment at multiplehierarchies to inject language modality information into the visualrepresentations. Our resulting model achieves state-of-the-art performance ontwo medical imaging benchmarks, OpenSRH and TCGA datasets. Our framework alsoprovides better interpretability with our language aligned representationspace. Code is available at https://github.com/Hasindri/HLSS.</description><author>Hasindri Watawana, Kanchana Ranasinghe, Tariq Mahmood, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan</author><pubDate>Thu, 21 Mar 2024 18:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14616v1</guid></item><item><title>AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation</title><link>http://arxiv.org/abs/2403.14614v1</link><description>In the image acquisition process, various forms of degradation, includingnoise, haze, and rain, are frequently introduced. These degradations typicallyarise from the inherent limitations of cameras or unfavorable ambientconditions. To recover clean images from degraded versions, numerousspecialized restoration methods have been developed, each targeting a specifictype of degradation. Recently, all-in-one algorithms have garnered significantattention by addressing different types of degradations within a single modelwithout requiring prior information of the input degradation type. However,these methods purely operate in the spatial domain and do not delve into thedistinct frequency variations inherent to different degradation types. Toaddress this gap, we propose an adaptive all-in-one image restoration networkbased on frequency mining and modulation. Our approach is motivated by theobservation that different degradation types impact the image content ondifferent frequency subbands, thereby requiring different treatments for eachrestoration task. Specifically, we first mine low- and high-frequencyinformation from the input features, guided by the adaptively decoupled spectraof the degraded image. The extracted features are then modulated by abidirectional operator to facilitate interactions between different frequencycomponents. Finally, the modulated features are merged into the original inputfor a progressively guided restoration. With this approach, the model achievesadaptive reconstruction by accentuating the informative frequency subbandsaccording to different input degradations. Extensive experiments demonstratethat the proposed method achieves state-of-the-art performance on differentimage restoration tasks, including denoising, dehazing, deraining, motiondeblurring, and low-light image enhancement. Our code is available athttps://github.com/c-yn/AdaIR.</description><author>Yuning Cui, Syed Waqas Zamir, Salman Khan, Alois Knoll, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Thu, 21 Mar 2024 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14614v1</guid></item><item><title>DreamReward: Text-to-3D Generation with Human Preference</title><link>http://arxiv.org/abs/2403.14613v1</link><description>3D content creation from text prompts has shown remarkable success recently.However, current text-to-3D methods often generate 3D results that do not alignwell with human preferences. In this paper, we present a comprehensiveframework, coined DreamReward, to learn and improve text-to-3D models fromhuman preference feedback. To begin with, we collect 25k expert comparisonsbased on a systematic annotation pipeline including rating and ranking. Then,we build Reward3D -- the first general-purpose text-to-3D human preferencereward model to effectively encode human preferences. Building upon the 3Dreward model, we finally perform theoretical analysis and present the Reward3DFeedback Learning (DreamFL), a direct tuning algorithm to optimize themulti-view diffusion models with a redefined scorer. Grounded by theoreticalproof and extensive experiment comparisons, our DreamReward successfullygenerates high-fidelity and 3D consistent results with significant boosts inprompt alignment with human intention. Our results demonstrate the greatpotential for learning from human feedback to improve text-to-3D models.</description><author>Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, Jun Zhu</author><pubDate>Thu, 21 Mar 2024 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14613v1</guid></item><item><title>Explorative Inbetweening of Time and Space</title><link>http://arxiv.org/abs/2403.14611v1</link><description>We introduce bounded generation as a generalized task to control videogeneration to synthesize arbitrary camera and subject motion based only on agiven start and end frame. Our objective is to fully leverage the inherentgeneralization capability of an image-to-video model without additionaltraining or fine-tuning of the original model. This is achieved through theproposed new sampling strategy, which we call Time Reversal Fusion, that fusesthe temporally forward and backward denoising paths conditioned on the startand end frame, respectively. The fused path results in a video that smoothlyconnects the two frames, generating inbetweening of faithful subject motion,novel views of static scenes, and seamless video looping when the two boundingframes are identical. We curate a diverse evaluation dataset of image pairs andcompare against the closest existing methods. We find that Time Reversal Fusionoutperforms related work on all subtasks, exhibiting the ability to generatecomplex motions and 3D-consistent views guided by bounded frames. See projectpage at https://time-reversal.github.io.</description><author>Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael J. Black, Xuaner Zhang</author><pubDate>Thu, 21 Mar 2024 18:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14611v1</guid></item><item><title>T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy</title><link>http://arxiv.org/abs/2403.14610v1</link><description>We present T-Rex2, a highly practical model for open-set object detection.Previous open-set object detection methods relying on text prompts effectivelyencapsulate the abstract concept of common objects, but struggle with rare orcomplex object representation due to data scarcity and descriptive limitations.Conversely, visual prompts excel in depicting novel objects through concretevisual examples, but fall short in conveying the abstract concept of objects aseffectively as text prompts. Recognizing the complementary strengths andweaknesses of both text and visual prompts, we introduce T-Rex2 that synergizesboth prompts within a single model through contrastive learning. T-Rex2 acceptsinputs in diverse formats, including text prompts, visual prompts, and thecombination of both, so that it can handle different scenarios by switchingbetween the two prompt modalities. Comprehensive experiments demonstrate thatT-Rex2 exhibits remarkable zero-shot object detection capabilities across awide spectrum of scenarios. We show that text prompts and visual prompts canbenefit from each other within the synergy, which is essential to cover massiveand complicated real-world scenarios and pave the way towards generic objectdetection. Model API is now available at\url{https://github.com/IDEA-Research/T-Rex}.</description><author>Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, Lei Zhang</author><pubDate>Thu, 21 Mar 2024 18:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14610v1</guid></item><item><title>TableLlama: Towards Open Large Generalist Models for Tables</title><link>http://arxiv.org/abs/2311.09206v2</link><description>Semi-structured tables are ubiquitous. There has been a variety of tasks thataim to automatically interpret, augment, and query tables. Current methodsoften require pretraining on tables or special model architecture design, arerestricted to specific table types, or have simplifying assumptions abouttables and tasks. This paper makes the first step towards developingopen-source large language models (LLMs) as generalists for a diversity oftable-based tasks. Towards that end, we construct TableInstruct, a new datasetwith a variety of realistic tables and tasks, for instruction tuning andevaluating LLMs. We further develop the first open-source generalist model fortables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address thelong context challenge. We experiment under both in-domain setting andout-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achievescomparable or better performance than the SOTA for each task, despite thelatter often has task-specific design. On 6 out-of-domain datasets, it achieves5-44 absolute point gains compared with the base model, showing that trainingon TableInstruct enhances the model's generalizability. We open-source ourdataset and trained model to boost future work on developing open generalistmodels for tables.</description><author>Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun</author><pubDate>Thu, 21 Mar 2024 18:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09206v2</guid></item><item><title>TD-MPC2: Scalable, Robust World Models for Continuous Control</title><link>http://arxiv.org/abs/2310.16828v2</link><description>TD-MPC is a model-based reinforcement learning (RL) algorithm that performslocal trajectory optimization in the latent space of a learned implicit(decoder-free) world model. In this work, we present TD-MPC2: a series ofimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improvessignificantly over baselines across 104 online RL tasks spanning 4 diverse taskdomains, achieving consistently strong results with a single set ofhyperparameters. We further show that agent capabilities increase with modeland data size, and successfully train a single 317M parameter agent to perform80 tasks across multiple task domains, embodiments, and action spaces. Weconclude with an account of lessons, opportunities, and risks associated withlarge TD-MPC2 agents. Explore videos, models, data, code, and more athttps://tdmpc2.com</description><author>Nicklas Hansen, Hao Su, Xiaolong Wang</author><pubDate>Thu, 21 Mar 2024 18:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16828v2</guid></item><item><title>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</title><link>http://arxiv.org/abs/2403.14608v1</link><description>Large models represent a groundbreaking advancement in multiple applicationfields, enabling remarkable achievements across various tasks. However, theirunprecedented scale comes with significant computational costs. These models,often consisting of billions of parameters, require vast amounts ofcomputational resources for execution. Especially, the expansive scale andcomputational demands pose considerable challenges when customizing them forparticular downstream tasks, particularly over the hardware platformsconstrained by computational capabilities. Parameter Efficient Fine-Tuning(PEFT) provides a practical solution by efficiently adapt the large models overthe various downstream tasks. In particular, PEFT refers to the process ofadjusting the parameters of a pre-trained large models to adapt it to aspecific task while minimizing the number of additional parameters introducedor computational resources required. This approach is particularly importantwhen dealing with large language models with high parameter counts, asfine-tuning these models from scratch can be computationally expensive andresource-intensive, posing considerable challenges in the supporting systemplatform design. In this survey, we present comprehensive studies of variousPEFT algorithms, examining their performance and computational overhead.Moreover, we provide an overview of applications developed using different PEFTalgorithms and discuss common techniques employed to mitigate computation costsfor PEFT. In addition to the algorithmic perspective, we overview variousreal-world system designs to investigate the implementation costs associatedwith different PEFT algorithms. This survey serves as an indispensable resourcefor researchers aiming to understand both the PEFT algorithm and its systemimplementation, offering detailed insights into recent advancements andpractical applications.</description><author>Zeyu Han, Chao Gao, Jinyang Liu, Jeff, Zhang, Sai Qian Zhang</author><pubDate>Thu, 21 Mar 2024 18:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14608v1</guid></item><item><title>The Elements of Differentiable Programming</title><link>http://arxiv.org/abs/2403.14606v1</link><description>Artificial intelligence has recently experienced remarkable advances, fueledby large models, vast datasets, accelerated hardware, and, last but not least,the transformative power of differentiable programming. This new programmingparadigm enables end-to-end differentiation of complex computer programs(including those with control flows and data structures), making gradient-basedoptimization of program parameters possible. As an emerging paradigm,differentiable programming builds upon several areas of computer science andapplied mathematics, including automatic differentiation, graphical models,optimization and statistics. This book presents a comprehensive review of thefundamental concepts useful for differentiable programming. We adopt two mainperspectives, that of optimization and that of probability, with clearanalogies between the two. Differentiable programming is not merely thedifferentiation of programs, but also the thoughtful design of programsintended for differentiation. By making programs differentiable, we inherentlyintroduce probability distributions over their execution, providing a means toquantify the uncertainty associated with program outputs.</description><author>Mathieu Blondel, Vincent Roulet</author><pubDate>Thu, 21 Mar 2024 18:55:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14606v1</guid></item><item><title>CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds</title><link>http://arxiv.org/abs/2305.00969v7</link><description>This paper describes the Ubenwa CryCeleb dataset - a labeled collection ofinfant cries - and the accompanying CryCeleb 2023 task, which is a publicspeaker verification challenge based on cry sounds. We released more than 6hours of manually segmented cry sounds from 786 newborns for academic use,aiming to encourage research in infant cry analysis. The inaugural publiccompetition attracted 59 participants, 11 of whom improved the baselineperformance. The top-performing system achieved a significant improvementscoring 25.8% equal error rate, which is still far from the performance ofstate-of-the-art adult speaker verification systems. Therefore, we believethere is room for further research on this dataset, potentially extendingbeyond the verification task.</description><author>David Budaghyan, Charles C. Onu, Arsenii Gorin, Cem Subakan, Doina Precup</author><pubDate>Thu, 21 Mar 2024 18:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00969v7</guid></item><item><title>ReNoise: Real Image Inversion Through Iterative Noising</title><link>http://arxiv.org/abs/2403.14602v1</link><description>Recent advancements in text-guided diffusion models have unlocked powerfulimage manipulation capabilities. However, applying these methods to real imagesnecessitates the inversion of the images into the domain of the pretraineddiffusion model. Achieving faithful inversion remains a challenge, particularlyfor more recent models trained to generate images with a small number ofdenoising steps. In this work, we introduce an inversion method with a highquality-to-operation ratio, enhancing reconstruction accuracy withoutincreasing the number of operations. Building on reversing the diffusionsampling process, our method employs an iterative renoising mechanism at eachinversion sampling step. This mechanism refines the approximation of apredicted point along the forward diffusion trajectory, by iteratively applyingthe pretrained diffusion model, and averaging these predictions. We evaluatethe performance of our ReNoise technique using various sampling algorithms andmodels, including recent accelerated diffusion models. Through comprehensiveevaluations and comparisons, we show its effectiveness in terms of bothaccuracy and speed. Furthermore, we confirm that our method preserveseditability by demonstrating text-driven image editing on real images.</description><author>Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, Daniel Cohen-Or</author><pubDate>Thu, 21 Mar 2024 18:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14602v1</guid></item><item><title>MyVLM: Personalizing VLMs for User-Specific Queries</title><link>http://arxiv.org/abs/2403.14599v1</link><description>Recent large-scale vision-language models (VLMs) have demonstrated remarkablecapabilities in understanding and generating textual descriptions for visualcontent. However, these models lack an understanding of user-specific concepts.In this work, we take a first step toward the personalization of VLMs, enablingthem to learn and reason over user-provided concepts. For example, we explorewhether these models can learn to recognize you in an image and communicatewhat you are doing, tailoring the model to reflect your personal experiencesand relationships. To effectively recognize a variety of user-specificconcepts, we augment the VLM with external concept heads that function astoggles for the model, enabling the VLM to identify the presence of specifictarget concepts in a given image. Having recognized the concept, we learn a newconcept embedding in the intermediate feature space of the VLM. This embeddingis tasked with guiding the language model to naturally integrate the targetconcept in its generated response. We apply our technique to BLIP-2 and LLaVAfor personalized image captioning and further show its applicability forpersonalized visual question-answering. Our experiments demonstrate our abilityto generalize to unseen images of learned concepts while preserving the modelbehavior on unrelated inputs.</description><author>Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, Daniel Cohen-Or</author><pubDate>Thu, 21 Mar 2024 18:51:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14599v1</guid></item><item><title>PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model</title><link>http://arxiv.org/abs/2403.14598v1</link><description>PSALM is a powerful extension of the Large Multi-modal Model (LMM) to addressthe segmentation task challenges. To overcome the limitation of the LMM beinglimited to textual output, PSALM incorporates a mask decoder and awell-designed input schema to handle a variety of segmentation tasks. Thisschema includes images, task instructions, conditional prompts, and masktokens, which enable the model to generate and classify segmentation maskseffectively. The flexible design of PSALM supports joint training acrossmultiple datasets and tasks, leading to improved performance and taskgeneralization. PSALM achieves superior results on several benchmarks, such asRefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive,and further exhibits zero-shot capabilities on unseen tasks, such asopen-vocabulary segmentation, generalized referring expression segmentation andvideo object segmentation, making a significant step towards a GPT moment incomputer vision. Through extensive experiments, PSALM demonstrates itspotential to transform the domain of image segmentation, leveraging the robustvisual understanding capabilities of LMMs as seen in natural languageprocessing. Code and models are available at https://github.com/zamling/PSALM.</description><author>Zheng Zhang, Yeyao Ma, Enming Zhang, Xiang Bai</author><pubDate>Thu, 21 Mar 2024 18:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14598v1</guid></item><item><title>Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach</title><link>http://arxiv.org/abs/2403.14597v1</link><description>The rise of automation has provided an opportunity to achieve higherefficiency in manufacturing processes, yet it often compromises the flexibilityrequired to promptly respond to evolving market needs and meet the demand forcustomization. Human-robot collaboration attempts to tackle these challenges bycombining the strength and precision of machines with human ingenuity andperceptual understanding. In this paper, we conceptualize and propose animplementation framework for an autonomous, machine learning-based manipulatorthat incorporates human-in-the-loop principles and leverages Extended Reality(XR) to facilitate intuitive communication and programming between humans androbots. Furthermore, the conceptual framework foresees human involvementdirectly in the robot learning process, resulting in higher adaptability andtask generalization. The paper highlights key technologies enabling theproposed framework, emphasizing the importance of developing the digitalecosystem as a whole. Additionally, we review the existent implementationapproaches of XR in human-robot collaboration, showcasing diverse perspectivesand methodologies. The challenges and future outlooks are discussed, delvinginto the major obstacles and potential research avenues of XR for more naturalhuman-robot interaction and integration in the industrial landscape.</description><author>Yehor Karpichev, Todd Charter, Homayoun Najjaran</author><pubDate>Thu, 21 Mar 2024 18:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14597v1</guid></item><item><title>VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition</title><link>http://arxiv.org/abs/2403.14594v1</link><description>Recent works on the global place recognition treat the task as a retrievalproblem, where an off-the-shelf global descriptor is commonly designed inimage-based and LiDAR-based modalities. However, it is non-trivial to performaccurate image-LiDAR global place recognition since extracting consistent androbust global descriptors from different domains (2D images and 3D pointclouds) is challenging. To address this issue, we propose a novelVoxel-Cross-Pixel (VXP) approach, which establishes voxel and pixelcorrespondences in a self-supervised manner and brings them into a sharedfeature space. Specifically, VXP is trained in a two-stage manner that firstexplicitly exploits local feature correspondences and enforces similarity ofglobal descriptors. Extensive experiments on the three benchmarks (OxfordRobotCar, ViViD++ and KITTI) demonstrate our method surpasses thestate-of-the-art cross-modal retrieval by a large margin.</description><author>Yun-Jin Li, Mariia Gladkova, Yan Xia, Rui Wang, Daniel Cremers</author><pubDate>Thu, 21 Mar 2024 18:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14594v1</guid></item><item><title>Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery</title><link>http://arxiv.org/abs/2403.14593v1</link><description>Adversarial inverse reinforcement learning (AIRL) stands as a cornerstoneapproach in imitation learning. This paper rethinks the two different angles ofAIRL: policy imitation and transferable reward recovery. We begin withsubstituting the built-in algorithm in AIRL with soft actor-critic (SAC) duringthe policy optimization process to enhance sample efficiency, thanks to theoff-policy formulation of SAC and identifiable Markov decision process (MDP)models with respect to AIRL. It indeed exhibits a significant improvement inpolicy imitation but accidentally brings drawbacks to transferable rewardrecovery. To learn this issue, we illustrate that the SAC algorithm itself isnot feasible to disentangle the reward function comprehensively during the AIRLtraining process, and propose a hybrid framework, PPO-AIRL + SAC, forsatisfactory transfer effect. Additionally, we analyze the capability ofenvironments to extract disentangled rewards from an algebraic theoryperspective.</description><author>Yangchun Zhang, Yirui Zhou</author><pubDate>Thu, 21 Mar 2024 18:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14593v1</guid></item><item><title>Envisioning the Next-Generation AI Coding Assistants: Insights &amp; Proposals</title><link>http://arxiv.org/abs/2403.14592v1</link><description>As a research-product hybrid group in AI for Software Engineering (AI4SE), wepresent four key takeaways from our experience developing in-IDE AI codingassistants. AI coding assistants should set clear expectations for usage,integrate with advanced IDE capabilities and existing extensions, useextendable backend designs, and collect app data responsibly for downstreamanalyses. We propose open questions and challenges that academia and industryshould address to realize the vision of next-generation AI coding assistants.</description><author>Khanh Nghiem, Anh Minh Nguyen, Nghi D. Q. Bui</author><pubDate>Thu, 21 Mar 2024 18:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14592v1</guid></item><item><title>ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training</title><link>http://arxiv.org/abs/2403.14589v1</link><description>Language agents have demonstrated autonomous decision-making abilities byreasoning with foundation models. Recently, efforts have been made to trainlanguage agents for performance improvement, with multi-step reasoning andaction trajectories as the training data. However, collecting such trajectoriesstill requires considerable human effort, by either artificial annotations orimplementations of diverse prompting frameworks. In this work, we proposeA$^3$T, a framework that enables the Autonomous Annotation of AgentTrajectories in the style of ReAct. The central role is an ActRe promptingagent, which explains the reason for an arbitrary action. When randomlysampling an external action, the ReAct-style agent could query the ActRe agentwith the action to obtain its textual rationales. Novel trajectories are thensynthesized by prepending the posterior reasoning from ActRe to the sampledaction. In this way, the ReAct-style agent executes multiple trajectories forthe failed tasks, and selects the successful ones to supplement its failedtrajectory for contrastive self-training. Realized by policy gradient methodswith binarized rewards, the contrastive self-training with accumulatedtrajectories facilitates a closed loop for multiple rounds of language agentself-improvement. We conduct experiments using QLoRA fine-tuning with theopen-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained withA$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterativerounds. In WebShop, the 1-shot performance of the A$^3$T agent matches humanaverage, and 4 rounds of iterative refinement lead to the performanceapproaching human experts. A$^3$T agents significantly outperform existingtechniques, including prompting with GPT-4, advanced agent frameworks, andfully fine-tuned LLMs.</description><author>Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</author><pubDate>Thu, 21 Mar 2024 18:43:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14589v1</guid></item><item><title>An Analysis of Linear Time Series Forecasting Models</title><link>http://arxiv.org/abs/2403.14587v1</link><description>Despite their simplicity, linear models perform well at time seriesforecasting, even when pitted against deeper and more expensive models. Anumber of variations to the linear model have been proposed, often includingsome form of feature normalisation that improves model generalisation. In thispaper we analyse the sets of functions expressible using these linear modelarchitectures. In so doing we show that several popular variants of linearmodels for time series forecasting are equivalent and functionallyindistinguishable from standard, unconstrained linear regression. Wecharacterise the model classes for each linear variant. We demonstrate thateach model can be reinterpreted as unconstrained linear regression over asuitably augmented feature set, and therefore admit closed-form solutions whenusing a mean-squared loss function. We provide experimental evidence that themodels under inspection learn nearly identical solutions, and finallydemonstrate that the simpler closed form solutions are superior forecastersacross 72% of test settings.</description><author>William Toner, Luke Darlow</author><pubDate>Thu, 21 Mar 2024 18:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14587v1</guid></item><item><title>Co-Optimization of Environment and Policies for Decentralized Multi-Agent Navigation</title><link>http://arxiv.org/abs/2403.14583v1</link><description>This work views the multi-agent system and its surrounding environment as aco-evolving system, where the behavior of one affects the other. The goal is totake both agent actions and environment configurations as decision variables,and optimize these two components in a coordinated manner to improve somemeasure of interest. Towards this end, we consider the problem of decentralizedmulti-agent navigation in cluttered environments. By introducing twosub-objectives of multi-agent navigation and environment optimization, wepropose an $\textit{agent-environment co-optimization}$ problem and develop a$\textit{coordinated algorithm}$ that alternates between these sub-objectivesto search for an optimal synthesis of agent actions and obstacle configurationsin the environment; ultimately, improving the navigation performance. Due tothe challenge of explicitly modeling the relation between agents, environmentand performance, we leverage policy gradient to formulate a model-free learningmechanism within the coordinated framework. A formal convergence analysis showsthat our coordinated algorithm tracks the local minimum trajectory of anassociated time-varying non-convex optimization problem. Extensive numericalresults corroborate theoretical findings and show the benefits ofco-optimization over baselines. Interestingly, the results also indicate thatoptimized environment configurations are able to offer structural guidance thatis key to de-conflicting agents in motion.</description><author>Zhan Gao, Guang Yang, Amanda Prorok</author><pubDate>Thu, 21 Mar 2024 18:37:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14583v1</guid></item><item><title>Large Language Models for Multi-Choice Question Classification of Medical Subjects</title><link>http://arxiv.org/abs/2403.14582v1</link><description>The aim of this paper is to evaluate whether large language models trained onmulti-choice question data can be used to discriminate between medicalsubjects. This is an important and challenging task for automatic questionanswering. To achieve this goal, we train deep neural networks for multi-classclassification of questions into the inferred medical subjects. Using ourMulti-Question (MQ) Sequence-BERT method, we outperform the state-of-the-artresults on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on theirdevelopment and test sets, respectively. In this sense, we show the capabilityof AI and LLMs in particular for multi-classification tasks in the Healthcaredomain.</description><author>Víctor Ponce-López</author><pubDate>Thu, 21 Mar 2024 18:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14582v1</guid></item><item><title>RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain</title><link>http://arxiv.org/abs/2403.14578v1</link><description>Large Language Models (LLMs) increasingly support applications in a widerange of domains, some with potential high societal impact such as biomedicine,yet their reliability in realistic use cases is under-researched. In this workwe introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)framework and evaluate whether four state-of-the-art foundation LLMs can serveas reliable assistants in the biomedical domain. We identify prompt robustness,high recall, and a lack of hallucinations as necessary criteria for this usecase. We design shortform tasks and tasks requiring LLM freeform responsesmimicking real-world user interactions. We evaluate LLM performance usingsemantic similarity with a ground truth response, through an evaluator LLM.</description><author>William James Bolton, Rafael Poyiadzi, Edward R. Morrell, Gabriela van Bergen Gonzalez Bueno, Lea Goetz</author><pubDate>Thu, 21 Mar 2024 18:30:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14578v1</guid></item><item><title>Emergent Dominance Hierarchies in Reinforcement Learning Agents</title><link>http://arxiv.org/abs/2401.12258v5</link><description>Modern Reinforcement Learning (RL) algorithms are able to outperform humansin a wide variety of tasks. Multi-agent reinforcement learning (MARL) settingspresent additional challenges, and successful cooperation in mixed-motivegroups of agents depends on a delicate balancing act between individual andgroup objectives. Social conventions and norms, often inspired by humaninstitutions, are used as tools for striking this balance. In this paper, we examine a fundamental, well-studied social convention thatunderlies cooperation in both animal and human societies: dominancehierarchies. We adapt the ethological theory of dominance hierarchies to artificialagents, borrowing the established terminology and definitions with as fewamendments as possible. We demonstrate that populations of RL agents, operatingwithout explicit programming or intrinsic rewards, can invent, learn, enforce,and transmit a dominance hierarchy to new populations. The dominancehierarchies that emerge have a similar structure to those studied in chickens,mice, fish, and other species.</description><author>Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky</author><pubDate>Thu, 21 Mar 2024 18:29:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12258v5</guid></item><item><title>Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model</title><link>http://arxiv.org/abs/2402.19150v2</link><description>Large Vision-Language Models (LVLMs) rely on vision encoders and LargeLanguage Models (LLMs) to exhibit remarkable capabilities on variousmulti-modal tasks in the joint space of vision and language. However, theTypographic Attack, which disrupts vision-language models (VLMs) such asContrastive Language-Image Pretraining (CLIP), has also been expected to be asecurity threat to LVLMs. Firstly, we verify typographic attacks on currentwell-known commercial and open-source LVLMs and uncover the widespreadexistence of this threat. Secondly, to better assess this vulnerability, wepropose the most comprehensive and largest-scale Typographic Dataset to date.The Typographic Dataset not only considers the evaluation of typographicattacks under various multi-modal tasks but also evaluates the effects oftypographic attacks, influenced by texts generated with diverse factors. Basedon the evaluation results, we investigate the causes why typographic attacksmay impact VLMs and LVLMs, leading to three highly insightful discoveries. Bythe examination of our discoveries and experimental validation in theTypographic Dataset, we reduce the performance degradation from $42.07\%$ to$13.90\%$ when LVLMs confront typographic attacks.</description><author>Hao Cheng, Erjia Xiao, Jindong Gu, Le Yang, Jinhao Duan, Jize Zhang, Jiahang Cao, Kaidi Xu, Renjing Xu</author><pubDate>Thu, 21 Mar 2024 18:26:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19150v2</guid></item><item><title>The All-Seeing Project V2: Towards General Relation Comprehension of the Open World</title><link>http://arxiv.org/abs/2402.19474v2</link><description>We present the All-Seeing Project V2: a new model and dataset designed forunderstanding object relations in images. Specifically, we propose theAll-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,object localization, and relation comprehension into a relation conversation(ReC) task. Leveraging this unified task, our model excels not only inperceiving and recognizing all objects within the image but also in graspingthe intricate relation graph between them, diminishing the relationhallucination often encountered by Multi-modal Large Language Models (MLLMs).To facilitate training and evaluation of MLLMs in relation understanding, wecreated the first high-quality ReC dataset ({AS-V2) which is aligned with theformat of standard instruction tuning data. In addition, we design a newbenchmark, termed Circular-based Relation Probing Evaluation (CRPE) forcomprehensively evaluating the relation comprehension capabilities of MLLMs.Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-awarebenchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope thatour work can inspire more future research and contribute to the evolutiontowards artificial general intelligence. Our project is released athttps://github.com/OpenGVLab/all-seeing.</description><author>Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, Yu Qiao, Jifeng Dai</author><pubDate>Thu, 21 Mar 2024 18:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19474v2</guid></item><item><title>m&amp;m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks</title><link>http://arxiv.org/abs/2403.11085v3</link><description>Real-world multi-modal problems are rarely solved by a single machinelearning model, and often require multi-step computational plans that involvestitching several models. Tool-augmented LLMs hold tremendous promise forautomating the generation of such computational plans. However, the lack ofstandardized benchmarks for evaluating LLMs as planners for multi-stepmulti-modal tasks has prevented a systematic study of planner design decisions.Should LLMs generate a full plan in a single shot or step-by-step? Should theyinvoke tools directly with Python code or through structured data formats likeJSON? Does feedback improve planning? To answer these questions and more, weintroduce m&amp;m's: a benchmark containing 4K+ multi-step multi-modal tasksinvolving 33 tools that include multi-modal models, (free) public APIs, andimage processing modules. For each of these task queries, we provideautomatically generated plans using this realistic toolset. We further providea high-quality subset of 1,565 task plans that are human-verified and correctlyexecutable. With m&amp;m's, we evaluate 6 popular LLMs with 2 planning strategies(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3types of feedback (parsing/verification/execution). Finally, we summarizetakeaways from our extensive experiments. Our dataset and code are available onHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github(https://github.com/RAIVNLab/mnms).</description><author>Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna</author><pubDate>Thu, 21 Mar 2024 18:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11085v3</guid></item><item><title>A Transfer Learning Causal Approach to Evaluate Racial/Ethnic and Geographic Variation in Outcomes Following Congenital Heart Surgery</title><link>http://arxiv.org/abs/2403.14573v1</link><description>Congenital heart defects (CHD) are the most prevalent birth defects in theUnited States and surgical outcomes vary considerably across the country. Theoutcomes of treatment for CHD differ for specific patient subgroups, withnon-Hispanic Black and Hispanic populations experiencing higher rates ofmortality and morbidity. A valid comparison of outcomes within racial/ethnicsubgroups is difficult given large differences in case-mix and small subgroupsizes. We propose a causal inference framework for outcome assessment andleverage advances in transfer learning to incorporate data from both target andsource populations to help estimate causal effects while accounting fordifferent sources of risk factor and outcome differences across populations.Using the Society of Thoracic Surgeons' Congenital Heart Surgery Database(STS-CHSD), we focus on a national cohort of patients undergoing the Norwoodoperation from 2016-2022 to assess operative mortality and morbidity outcomesacross U.S. geographic regions by race/ethnicity. We find racial and ethnicoutcome differences after controlling for potential confounding factors. Whilegeography does not have a causal effect on outcomes for non-Hispanic Caucasianpatients, non-Hispanic Black patients experience wide variability in outcomeswith estimated 30-day mortality ranging from 5.9% (standard error 2.2%) to21.6% (4.4%) across U.S. regions.</description><author>Larry Han, Yi Zhang, Meena Nathan, John E. Mayer, Jr., Sara K. Pasquali, Katya Zelevinsky, Rui Duan, Sharon-Lise T. Normand</author><pubDate>Thu, 21 Mar 2024 18:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14573v1</guid></item><item><title>Implicit Style-Content Separation using B-LoRA</title><link>http://arxiv.org/abs/2403.14572v1</link><description>Image stylization involves manipulating the visual appearance and texture(style) of an image while preserving its underlying objects, structures, andconcepts (content). The separation of style and content is essential formanipulating the image's style independently from its content, ensuring aharmonious and visually pleasing result. Achieving this separation requires adeep understanding of both the visual and semantic characteristics of images,often necessitating the training of specialized models or employing heavyoptimization. In this paper, we introduce B-LoRA, a method that leverages LoRA(Low-Rank Adaptation) to implicitly separate the style and content componentsof a single image, facilitating various image stylization tasks. By analyzingthe architecture of SDXL combined with LoRA, we find that jointly learning theLoRA weights of two specific blocks (referred to as B-LoRAs) achievesstyle-content separation that cannot be achieved by training each B-LoRAindependently. Consolidating the training into only two blocks and separatingstyle and content allows for significantly improving style manipulation andovercoming overfitting issues often associated with model fine-tuning. Oncetrained, the two B-LoRAs can be used as independent components to allow variousimage stylization tasks, including image style transfer, text-based imagestylization, consistent style generation, and style-content mixing.</description><author>Yarden Frenkel, Yael Vinker, Ariel Shamir, Daniel Cohen-Or</author><pubDate>Thu, 21 Mar 2024 18:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14572v1</guid></item><item><title>MedCycle: Unpaired Medical Report Generation via Cycle-Consistency</title><link>http://arxiv.org/abs/2403.13444v2</link><description>Generating medical reports for X-ray images presents a significant challenge,particularly in unpaired scenarios where access to paired image-report data fortraining is unavailable. Previous works have typically learned a jointembedding space for images and reports, necessitating a specific labelingschema for both. We introduce an innovative approach that eliminates the needfor consistent labeling schemas, thereby enhancing data accessibility andenabling the use of incompatible datasets. This approach is based oncycle-consistent mapping functions that transform image embeddings into reportembeddings, coupled with report auto-encoding for medical report generation.Our model and objectives consider intricate local details and the overarchingsemantic context within images and reports. This approach facilitates thelearning of effective mapping functions, resulting in the generation ofcoherent reports. It outperforms state-of-the-art results in unpaired chestX-ray report generation, demonstrating improvements in both language andclinical metrics.</description><author>Elad Hirsch, Gefen Dawidowicz, Ayellet Tal</author><pubDate>Thu, 21 Mar 2024 18:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13444v2</guid></item><item><title>A survey on Concept-based Approaches For Model Improvement</title><link>http://arxiv.org/abs/2403.14566v1</link><description>The focus of recent research has shifted from merely increasing the DeepNeural Networks (DNNs) performance in various tasks to DNNs, which are moreinterpretable to humans. The field of eXplainable Artificial Intelligence (XAI)has observed various techniques, including saliency-based and concept-basedapproaches. Concept-based approaches explain the model's decisions in simplehuman understandable terms called Concepts. Concepts are human interpretableunits of data and are the thinking ground of humans. Explanations in terms ofconcepts enable detecting spurious correlations, inherent biases, orclever-hans. With the advent of concept-based explanations, there have beenvarious concept representation methods and automatic concept discoveryalgorithms. Some recent methods use concepts for post-hoc model disentanglementevaluation, while others use them for ante-hoc training. The concept-basedapproaches are new, with many representations coming up, and there is verylimited work on Concept-based Model improvement. We provide a systematic reviewand taxonomy of various concept representations and their discovery algorithmsin DNNs, specifically in vision. We also provide details on concept-based modelimprovement literature, which is the first to survey concept-based modelimprovement methods.</description><author>Avani Gupta, P J Narayanan</author><pubDate>Thu, 21 Mar 2024 18:09:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14566v1</guid></item><item><title>A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science</title><link>http://arxiv.org/abs/2403.14565v1</link><description>This paper explores the use of large language models (LLMs) to score andexplain short-answer assessments in K-12 science. While existing methods canscore more structured math and computer science assessments, they often do notprovide explanations for the scores. Our study focuses on employing GPT-4 forautomated assessment in middle school Earth Science, combining few-shot andactive learning with chain-of-thought reasoning. Using a human-in-the-loopapproach, we successfully score and provide meaningful explanations forformative assessment responses. A systematic analysis of our method's pros andcons sheds light on the potential for human-in-the-loop techniques to enhanceautomated grading for open-ended science assessments.</description><author>Clayton Cohn, Nicole Hutchins, Tuan Le, Gautam Biswas</author><pubDate>Thu, 21 Mar 2024 18:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14565v1</guid></item><item><title>Unraveling the Mystery of Scaling Laws: Part I</title><link>http://arxiv.org/abs/2403.06563v2</link><description>Scaling law principles indicate a power-law correlation between loss andvariables such as model size, dataset size, and computational resourcesutilized during training. These principles play a vital role in optimizingvarious aspects of model pre-training, ultimately contributing to the successof large language models such as GPT-4, Llama and Gemini. However, the originalscaling law paper by OpenAI did not disclose the complete details necessary toderive the precise scaling law formulas, and their conclusions are only basedon models containing up to 1.5 billion parameters. Though some subsequent worksattempt to unveil these details and scale to larger models, they often neglectthe training dependency of important factors such as the learning rate, contextlength and batch size, leading to their failure to establish a reliable formulafor predicting the test loss trajectory. In this technical report, we confirmthat the scaling law formulations proposed in the original OpenAI paper remainvalid when scaling the model size up to 33 billion, but the constantcoefficients in these formulas vary significantly with the experiment setup. Wemeticulously identify influential factors and provide transparent, step-by-stepinstructions to estimate all constant terms in scaling-law formulas by trainingon models with only 1M~60M parameters. Using these estimated formulas, weshowcase the capability to accurately predict various attributes for modelswith up to 33B parameters before their training, including (1) the minimumpossible test loss; (2) the minimum required training steps and processedtokens to achieve a specific loss; (3) the critical batch size with an optimaltime/computation trade-off at any loss value; and (4) the complete test losstrajectory with arbitrary batch size.</description><author>Hui Su, Zhi Tian, Xiaoyu Shen, Xunliang Cai</author><pubDate>Thu, 21 Mar 2024 18:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06563v2</guid></item><item><title>A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa</title><link>http://arxiv.org/abs/2403.06860v2</link><description>Desert locust swarms present a major threat to agriculture and food security.Addressing this challenge, our study develops an operationally-ready model forpredicting locust breeding grounds, which has the potential to enhance earlywarning systems and targeted control measures. We curated a dataset from theUnited Nations Food and Agriculture Organization's (UN-FAO) locust observationrecords and analyzed it using two types of spatio-temporal input features:remotely-sensed environmental and climate data as well as multi-spectral earthobservation images. Our approach employed custom deep learning models(three-dimensional and LSTM-based recurrent convolutional networks), along withthe geospatial foundational model Prithvi recently released by Jakubik et al.,2023. These models notably outperformed existing baselines, with thePrithvi-based model, fine-tuned on multi-spectral images from NASA's HarmonizedLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 andROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant findingfrom our research is that multi-spectral earth observation images alone aresufficient for effective locust breeding ground prediction without the need toexplicitly incorporate climatic or environmental features.</description><author>Ibrahim Salihu Yusuf, Mukhtar Opeyemi Yusuf, Kobby Panford-Quainoo, Arnu Pretorius</author><pubDate>Thu, 21 Mar 2024 18:06:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06860v2</guid></item><item><title>The Era of Semantic Decoding</title><link>http://arxiv.org/abs/2403.14562v1</link><description>Recent work demonstrated great promise in the idea of orchestratingcollaborations between LLMs, human input, and various tools to address theinherent limitations of LLMs. We propose a novel perspective called semanticdecoding, which frames these collaborative processes as optimization proceduresin semantic space. Specifically, we conceptualize LLMs as semantic processorsthat manipulate meaningful pieces of information that we call semantic tokens(known thoughts). LLMs are among a large pool of other semantic processors,including humans and tools, such as search engines or code executors.Collectively, semantic processors engage in dynamic exchanges of semantictokens to progressively construct high-utility outputs. We refer to theseorchestrated interactions among semantic processors, optimizing and searchingin semantic space, as semantic decoding algorithms. This concept draws a directparallel to the well-studied problem of syntactic decoding, which involvescrafting algorithms to best exploit auto-regressive language models forextracting high-utility sequences of syntactic tokens. By focusing on thesemantic level and disregarding syntactic details, we gain a fresh perspectiveon the engineering of AI systems, enabling us to imagine systems with muchgreater complexity and capabilities. In this position paper, we formalize thetransition from syntactic to semantic tokens as well as the analogy betweensyntactic and semantic decoding. Subsequently, we explore the possibilities ofoptimizing within the space of semantic tokens via semantic decodingalgorithms. We conclude with a list of research opportunities and questionsarising from this fresh perspective. The semantic decoding perspective offers apowerful abstraction for search and optimization directly in the space ofmeaningful concepts, with semantic tokens as the fundamental units of a newtype of computation.</description><author>Maxime Peyrard, Martin Josifoski, Robert West</author><pubDate>Thu, 21 Mar 2024 18:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14562v1</guid></item><item><title>Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation</title><link>http://arxiv.org/abs/2403.14559v1</link><description>Localizing predefined 3D keypoints in a 2D image is an effective way toestablish 3D-2D correspondences for 6DoF object pose estimation. However,unreliable localization results of invisible keypoints degrade the quality ofcorrespondences. In this paper, we address this issue by localizing theimportant keypoints in terms of visibility. Since keypoint visibilityinformation is currently missing in dataset collection process, we propose anefficient way to generate binary visibility labels from available object-levelannotations, for keypoints of both asymmetric objects and symmetric objects. Wefurther derive real-valued visibility-aware importance from binary labels basedon PageRank algorithm. Taking advantage of the flexibility of ourvisibility-aware importance, we construct VAPO (Visibility-Aware POseestimator) by integrating the visibility-aware importance with astate-of-the-art pose estimation algorithm, along with additional positionalencoding. Extensive experiments are conducted on popular pose estimationbenchmarks including Linemod, Linemod-Occlusion, and YCB-V. The results showthat, VAPO improves both the keypoint correspondences and final estimatedposes, and clearly achieves state-of-the-art performances.</description><author>Ruyi Lian, Haibin Ling</author><pubDate>Thu, 21 Mar 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14559v1</guid></item><item><title>Towards Flexible, Scalable, and Adaptive Multi-Modal Conditioned Face Synthesis</title><link>http://arxiv.org/abs/2312.16274v2</link><description>Recent progress in multi-modal conditioned face synthesis has enabled thecreation of visually striking and accurately aligned facial images. Yet,current methods still face issues with scalability, limited flexibility, and aone-size-fits-all approach to control strength, not accounting for thediffering levels of conditional entropy, a measure of unpredictability in datagiven some condition, across modalities. To address these challenges, weintroduce a novel uni-modal training approach with modal surrogates, coupledwith an entropy-aware modal-adaptive modulation, to support flexible, scalable,and scalable multi-modal conditioned face synthesis network. Our uni-modaltraining with modal surrogate that only leverage uni-modal data, use modalsurrogate to decorate condition with modal-specific characteristic and serve aslinker for inter-modal collaboration , fully learns each modality control inface synthesis process as well as inter-modal collaboration. The entropy-awaremodal-adaptive modulation finely adjust diffusion noise according tomodal-specific characteristics and given conditions, enabling well-informedstep along denoising trajectory and ultimately leading to synthesis results ofhigh fidelity and quality. Our framework improves multi-modal face synthesisunder various conditions, surpassing current methods in image quality andfidelity, as demonstrated by our thorough experimental results.</description><author>Jingjing Ren, Cheng Xu, Haoyu Chen, Xinran Qin, Lei Zhu</author><pubDate>Thu, 21 Mar 2024 17:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16274v2</guid></item><item><title>Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering</title><link>http://arxiv.org/abs/2403.14554v1</link><description>We propose Gaussian Frosting, a novel mesh-based representation forhigh-quality rendering and editing of complex 3D effects in real-time. Ourapproach builds on the recent 3D Gaussian Splatting framework, which optimizesa set of 3D Gaussians to approximate a radiance field from images. We proposefirst extracting a base mesh from Gaussians during optimization, then buildingand refining an adaptive layer of Gaussians with a variable thickness aroundthe mesh to better capture the fine details and volumetric effects near thesurface, such as hair or grass. We call this layer Gaussian Frosting, as itresembles a coating of frosting on a cake. The fuzzier the material, thethicker the frosting. We also introduce a parameterization of the Gaussians toenforce them to stay inside the frosting layer and automatically adjust theirparameters when deforming, rescaling, editing or animating the mesh. Ourrepresentation allows for efficient rendering using Gaussian splatting, as wellas editing and animation by modifying the base mesh. We demonstrate theeffectiveness of our method on various synthetic and real scenes, and show thatit outperforms existing surface-based approaches. We will release our code anda web-based viewer as additional contributions. Our project page is thefollowing: https://anttwo.github.io/frosting/</description><author>Antoine Guédon, Vincent Lepetit</author><pubDate>Thu, 21 Mar 2024 17:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14554v1</guid></item><item><title>Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer</title><link>http://arxiv.org/abs/2403.14552v1</link><description>While Transformers have rapidly gained popularity in various computer visionapplications, post-hoc explanations of their internal mechanisms remain largelyunexplored. Vision Transformers extract visual information by representingimage regions as transformed tokens and integrating them via attention weights.However, existing post-hoc explanation methods merely consider these attentionweights, neglecting crucial information from the transformed tokens, whichfails to accurately illustrate the rationales behind the models' predictions.To incorporate the influence of token transformation into interpretation, wepropose TokenTM, a novel post-hoc explanation method that utilizes ourintroduced measurement of token transformation effects. Specifically, wequantify token transformation effects by measuring changes in token lengths andcorrelations in their directions pre- and post-transformation. Moreover, wedevelop initialization and aggregation rules to integrate both attentionweights and token transformation effects across all layers, capturing holistictoken contributions throughout the model. Experimental results on segmentationand perturbation tests demonstrate the superiority of our proposed TokenTMcompared to state-of-the-art Vision Transformer explanation methods.</description><author>Junyi Wu, Bin Duan, Weitai Kang, Hao Tang, Yan Yan</author><pubDate>Thu, 21 Mar 2024 17:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14552v1</guid></item><item><title>Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling</title><link>http://arxiv.org/abs/2403.14551v1</link><description>Today's most accurate language models are trained on orders of magnitude morelanguage data than human language learners receive - but with no supervisionfrom other sensory modalities that play a crucial role in human learning. Canwe make LMs' representations and predictions more accurate (and morehuman-like) with more ecologically plausible supervision? This paper describesLexiContrastive Grounding (LCG), a grounded language learning procedure thatleverages visual supervision to improve textual representations.LexiContrastive Grounding combines a next token prediction strategy with acontrastive visual grounding objective, focusing on early-layer representationsthat encode lexical information. Across multiple word-learning andsentence-understanding benchmarks, LexiContrastive Grounding not onlyoutperforms standard language-only models in learning efficiency, but alsoimproves upon vision-and-language learning procedures including CLIP, GIT,Flamingo, and Vokenization. Moreover, LexiContrastive Grounding improvesperplexity by around 5% on multiple language modeling tasks. This workunderscores the potential of incorporating visual grounding into languagemodels, aligning more closely with the multimodal nature of human languageacquisition.</description><author>Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas</author><pubDate>Thu, 21 Mar 2024 17:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14551v1</guid></item><item><title>Dynamic Explanation Emphasis in Human-XAI Interaction with Communication Robot</title><link>http://arxiv.org/abs/2403.14550v1</link><description>Communication robots have the potential to contribute to effective human-XAIinteraction as an interface that goes beyond textual or graphical explanations.One of their strengths is that they can use physical and vocal expressions toadd detailed nuances to explanations. However, it is not clear how a robot canapply such expressions, or in particular, how we can develop a strategy toadaptively use such expressions depending on the task and user in dynamicinteractions. To address this question, this paper proposes DynEmph, a methodfor a communication robot to decide where to emphasize XAI-generatedexplanations with physical expressions. It predicts the effect of emphasizingcertain points on a user and aims to minimize the expected difference betweenpredicted user decisions and AI-suggested ones. DynEmph features a strategy fordeciding where to emphasize in a data-driven manner, relieving engineers fromthe need to manually design a strategy. We further conducted experiments toinvestigate how emphasis selection strategies affect the performance of userdecisions. The results suggest that, while a naive strategy (emphasizingexplanations for an AI's most probable class) does not necessarily work better,DynEmph effectively guides users to better decisions under the condition thatthe performance of the AI suggestion is high.</description><author>Yosuke Fukuchi, Seiji Yamada</author><pubDate>Thu, 21 Mar 2024 17:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14550v1</guid></item><item><title>DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video</title><link>http://arxiv.org/abs/2403.14548v1</link><description>We present DINO-Tracker -- a new framework for long-term dense tracking invideo. The pillar of our approach is combining test-time training on a singlevideo, with the powerful localized semantic features learned by a pre-trainedDINO-ViT model. Specifically, our framework simultaneously adopts DINO'sfeatures to fit to the motion observations of the test video, while training atracker that directly leverages the refined features. The entire framework istrained end-to-end using a combination of self-supervised losses, andregularization that allows us to retain and benefit from DINO's semantic prior.Extensive evaluation demonstrates that our method achieves state-of-the-artresults on known benchmarks. DINO-tracker significantly outperformsself-supervised methods and is competitive with state-of-the-art supervisedtrackers, while outperforming them in challenging cases of tracking underlong-term occlusions.</description><author>Narek Tumanyan, Assaf Singer, Shai Bagon, Tali Dekel</author><pubDate>Thu, 21 Mar 2024 17:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14548v1</guid></item><item><title>MedMamba: Vision Mamba for Medical Image Classification</title><link>http://arxiv.org/abs/2403.03849v2</link><description>Medical image classification is a very fundamental and crucial task in thefield of computer vision. These years, CNN-based and Transformer-based modelshave been widely used to classify various medical images. Unfortunately, Thelimitation of CNNs in long-range modeling capabilities prevents them fromeffectively extracting features in medical images, while Transformers arehampered by their quadratic computational complexity. Recent research has shownthat the state space model (SSM) represented by Mamba can efficiently modellong-range interactions while maintaining linear computational complexity.Inspired by this, we propose Vision Mamba for medical image classification(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSMcombines the local feature extraction ability of convolutional layers with theability of SSM to capture long-range dependency, thereby modeling medicalimages with different modalities. To demonstrate the potential of MedMamba, weconducted extensive experiments using 14 publicly available medical datasetswith different imaging techniques and two private datasets built by ourselves.Extensive experimental results demonstrate that the proposed MedMamba performswell in detecting lesions in various medical images. To the best of ourknowledge, this is the first Vision Mamba tailored for medical imageclassification. The purpose of this work is to establish a new baseline formedical image classification tasks and provide valuable insights for the futuredevelopment of more efficient and effective SSM-based artificial intelligencealgorithms and application systems in the medical. Source code has beenavailable at https://github.com/YubiaoYue/MedMamba.</description><author>Yubiao Yue, Zhenzhang Li</author><pubDate>Thu, 21 Mar 2024 17:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03849v2</guid></item><item><title>Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images</title><link>http://arxiv.org/abs/2403.14547v1</link><description>The application of data augmentation for deep learning (DL) methods plays animportant role in achieving state-of-the-art results in supervised,semi-supervised, and self-supervised image classification. In particular,channel transformations (e.g., solarize, grayscale, brightness adjustments) areintegrated into data augmentation pipelines for remote sensing (RS) imageclassification tasks. However, contradicting beliefs exist about their properapplications to RS images. A common point of critique is that the applicationof channel augmentation techniques may lead to physically inconsistent spectraldata (i.e., pixel signatures). To shed light on the open debate, we propose anapproach to estimate whether a channel augmentation technique affects thephysical information of RS images. To this end, the proposed approach estimatesa score that measures the alignment of a pixel signature within a time seriesthat can be naturally subject to deviations caused by factors such asacquisition conditions or phenological states of vegetation. We compare thescores associated with original and augmented pixel signatures to evaluate thephysical consistency. Experimental results on a multi-label imageclassification task show that channel augmentations yielding a score thatexceeds the expected deviation of original pixel signatures can not improve theperformance of a baseline model trained without augmentation.</description><author>Tom Burgert, Begüm Demir</author><pubDate>Thu, 21 Mar 2024 17:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14547v1</guid></item><item><title>Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis with Generative AI through Rapid Prototyping, Iteration and Curation</title><link>http://arxiv.org/abs/2402.08812v3</link><description>Complex data analysis inherently seeks unexpected insights throughexploratory visual analysis methods, transcending logical, step-by-stepprocessing. However, existing interfaces such as notebooks and dashboards havelimitations in exploration and comparison for visual data analysis. Addressingthese limitations, we introduce a "design-like" intelligent canvas environmentintegrating generative AI into data analysis, offering rapid prototyping,iteration, and comparative visualization management. Our dual contributionsinclude the integration of generative AI components into a canvas interface,and empirical findings from a user study (N=10) evaluating the effectiveness ofthe canvas interface.</description><author>Zijian Ding, Joel Chan</author><pubDate>Thu, 21 Mar 2024 17:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08812v3</guid></item><item><title>EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling</title><link>http://arxiv.org/abs/2403.14541v1</link><description>Recently, Large Language Models (LLMs) have demonstrated outstandingperformance across a wide range of downstream language tasks. Temperaturesampling is a commonly used decoding strategy for LLMs' generation process.However, a fixed temperature parameter is used in most cases, which may notalways be an optimal choice for balancing generation quality and diversity. Inthis paper, we propose an effective Entropy-based Dynamic Temperature (EDT)Sampling method, to achieve a more balanced performance in terms of bothgeneration quality and diversity by dynamically selecting the temperatureparameter. Additionally, we also show model performance and comprehensiveanalyses for 4 different generation benchmarks. Our experiments show that EDTsignificantly outperforms the existing strategies across different tasks.</description><author>Shimao Zhang, Yu Bao, Shujian Huang</author><pubDate>Thu, 21 Mar 2024 17:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14541v1</guid></item><item><title>Instance-aware Exploration-Verification-Exploitation for Instance ImageGoal Navigation</title><link>http://arxiv.org/abs/2402.17587v2</link><description>As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims tonavigate to a specified object depicted by a goal image in an unexploredenvironment. The main challenge of this task lies in identifying the target object fromdifferent viewpoints while rejecting similar distractors. Existing ImageGoal Navigation methods usually adopt the simpleExploration-Exploitation framework and ignore the identification of specificinstance during navigation. In this work, we propose to imitate the human behaviour of ``getting closerto confirm" when distinguishing objects from a distance. Specifically, we design a new modular navigation framework namedInstance-aware Exploration-Verification-Exploitation (IEVE) for instance-levelimage goal navigation. Our method allows for active switching among the exploration, verification,and exploitation actions, thereby facilitating the agent in making reasonabledecisions under different situations. On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, ourmethod surpasses previous state-of-the-art work, with a classical segmentationmodel (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).Our code will be made publicly available at https://github.com/XiaohanLei/IEVE.</description><author>Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li</author><pubDate>Thu, 21 Mar 2024 17:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17587v2</guid></item><item><title>Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild</title><link>http://arxiv.org/abs/2403.14539v1</link><description>One of the biggest challenges in single-view 3D shape reconstruction in thewild is the scarcity of &lt;3D shape, 2D image&gt;-paired data from real-worldenvironments. Inspired by remarkable achievements via domain randomization, wepropose ObjectDR which synthesizes such paired data via a random simulation ofvisual variations in object appearances and backgrounds. Our data synthesisframework exploits a conditional generative model (e.g., ControlNet) togenerate images conforming to spatial conditions such as 2.5D sketches, whichare obtainable through a rendering process of 3D shapes from object collections(e.g., Objaverse-XL). To simulate diverse variations while preserving objectsilhouettes embedded in spatial conditions, we also introduce a disentangledframework which leverages an initial object guidance. After synthesizing a widerange of data, we pre-train a model on them so that it learns to capture adomain-invariant geometry prior which is consistent across various domains. Wevalidate its effectiveness by substantially improving 3D shape reconstructionmodels on a real-world benchmark. In a scale-up evaluation, our pre-trainingachieves 23.6% superior results compared with the pre-training on high-qualitycomputer graphics renderings.</description><author>Junhyeong Cho, Kim Youwang, Hunmin Yang, Tae-Hyun Oh</author><pubDate>Thu, 21 Mar 2024 17:40:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14539v1</guid></item><item><title>Generalizing deep learning models for medical image classification</title><link>http://arxiv.org/abs/2403.12167v2</link><description>Numerous Deep Learning (DL) models have been developed for a large spectrumof medical image analysis applications, which promises to reshape variousfacets of medical practice. Despite early advances in DL model validation andimplementation, which encourage healthcare institutions to adopt them, somefundamental questions remain: are the DL models capable of generalizing? Whatcauses a drop in DL model performances? How to overcome the DL modelperformance drop? Medical data are dynamic and prone to domain shift, due tomultiple factors such as updates to medical equipment, new imaging workflow,and shifts in patient demographics or populations can induce this drift overtime. In this paper, we review recent developments in generalization methodsfor DL-based classification models. We also discuss future challenges,including the need for improved evaluation protocols and benchmarks, andenvisioned future developments to achieve robust, generalized models formedical image classification.</description><author>Matta Sarah, Lamard Mathieu, Zhang Philippe, Alexandre Le Guilcher, Laurent Borderie, Béatrice Cochener, Gwenolé Quellec</author><pubDate>Thu, 21 Mar 2024 17:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12167v2</guid></item><item><title>Transfer Learning for Cross-dataset Isolated Sign Language Recognition in Under-Resourced Datasets</title><link>http://arxiv.org/abs/2403.14534v1</link><description>Sign language recognition (SLR) has recently achieved a breakthrough inperformance thanks to deep neural networks trained on large annotated signdatasets. Of the many different sign languages, these annotated datasets areonly available for a select few. Since acquiring gloss-level labels on signlanguage videos is difficult, learning by transferring knowledge from existingannotated sources is useful for recognition in under-resourced sign languages.This study provides a publicly available cross-dataset transfer learningbenchmark from two existing public Turkish SLR datasets. We use a temporalgraph convolution-based sign language recognition approach to evaluate fivesupervised transfer learning approaches and experiment with closed-set andpartial-set cross-dataset transfer learning. Experiments demonstrate thatimprovement over finetuning based transfer learning is possible withspecialized supervised transfer learning methods.</description><author>Ahmet Alp Kindiroglu, Ozgur Kara, Ogulcan Ozdemir, Lale Akarun</author><pubDate>Thu, 21 Mar 2024 17:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14534v1</guid></item><item><title>HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression</title><link>http://arxiv.org/abs/2403.14530v1</link><description>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novelview synthesis, boasting rapid rendering speed with high fidelity. However, thesubstantial Gaussians and their associated attributes necessitate effectivecompression techniques. Nevertheless, the sparse and unorganized nature of thepoint cloud of Gaussians (or anchors in our paper) presents challenges forcompression. To address this, we make use of the relations between theunorganized anchors and the structured hash grid, leveraging their mutualinformation for context modeling, and propose a Hash-grid Assisted Context(HAC) framework for highly compact 3DGS representation. Our approach introducesa binary hash grid to establish continuous spatial consistencies, allowing usto unveil the inherent spatial relations of anchors through a carefullydesigned context model. To facilitate entropy coding, we utilize Gaussiandistributions to accurately estimate the probability of each quantizedattribute, where an adaptive quantization module is proposed to enablehigh-precision quantization of these attributes for improved fidelityrestoration. Additionally, we incorporate an adaptive masking strategy toeliminate invalid Gaussians and anchors. Importantly, our work is the pioneerto explore context-based compression for 3DGS representation, resulting in aremarkable size reduction of over $75\times$ compared to vanilla 3DGS, whilesimultaneously improving fidelity, and achieving over $11\times$ size reductionover SOTA 3DGS compression approach Scaffold-GS. Our code is available here:https://github.com/YihangChen-ee/HAC</description><author>Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin</author><pubDate>Thu, 21 Mar 2024 17:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14530v1</guid></item><item><title>Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models</title><link>http://arxiv.org/abs/2403.12966v2</link><description>In the realm of vision-language understanding, the proficiency of models ininterpreting and reasoning over visual content has become a cornerstone fornumerous applications. However, it is challenging for the visual encoder inLarge Vision-Language Models (LVLMs) to extract useful features tailored toquestions that aid the language model's response. Furthermore, a commonpractice among existing LVLMs is to utilize lower-resolution images, whichrestricts the ability for visual recognition. Our work introduces theChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novelapproach that enhances feature extraction by focusing on key regions ofinterest (ROI) within the image, corresponding to the posed questions orinstructions. This technique allows LVLMs to access more detailed visualinformation without altering the original image resolution, thereby offeringmulti-granularity image features. By integrating Chain-of-Spot withinstruct-following LLaVA-1.5 models, the process of image reasoningconsistently improves performance across a wide range of multimodal datasetsand benchmarks without bells and whistles and achieves new state-of-the-artresults. Our empirical findings demonstrate a significant improvement in LVLMs'ability to understand and reason about visual content, paving the way for moresophisticated visual instruction-following applications. Code and models areavailable at https://github.com/dongyh20/Chain-of-Spot</description><author>Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 21 Mar 2024 17:26:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12966v2</guid></item><item><title>Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors</title><link>http://arxiv.org/abs/2403.14526v1</link><description>Precise manipulation that is generalizable across scenes and objects remainsa persistent challenge in robotics. Current approaches for this task heavilydepend on having a significant number of training instances to handle objectswith pronounced visual and/or geometric part ambiguities. Our work explores thegrounding of fine-grained part descriptors for precise manipulation in azero-shot setting by utilizing web-trained text-to-image diffusion-basedgenerative models. We tackle the problem by framing it as a dense semantic partcorrespondence task. Our model returns a gripper pose for manipulating aspecific part, using as reference a user-defined click from a source image of avisually different instance of the same object. We require no manual graspingdemonstrations as we leverage the intrinsic object geometry and features.Practical experiments in a real-world tabletop scenario validate the efficacyof our approach, demonstrating its potential for advancing semantic-awarerobotics manipulation. Web page: https://tsagkas.github.io/click2grasp</description><author>Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, Chris Xiaoxuan Lu</author><pubDate>Thu, 21 Mar 2024 17:26:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14526v1</guid></item><item><title>Let's do the time-warp-attend: Learning topological invariants of dynamical systems</title><link>http://arxiv.org/abs/2312.09234v3</link><description>Dynamical systems across the sciences, from electrical circuits to ecologicalnetworks, undergo qualitative and often catastrophic changes in behavior,called bifurcations, when their underlying parameters cross a threshold.Existing methods predict oncoming catastrophes in individual systems but areprimarily time-series-based and struggle both to categorize qualitativedynamical regimes across diverse systems and to generalize to real data. Toaddress this challenge, we propose a data-driven, physically-informeddeep-learning framework for classifying dynamical regimes and characterizingbifurcation boundaries based on the extraction of topologically invariantfeatures. We focus on the paradigmatic case of the supercritical Hopfbifurcation, which is used to model periodic dynamics across a wide range ofapplications. Our convolutional attention method is trained with dataaugmentations that encourage the learning of topological invariants which canbe used to detect bifurcation boundaries in unseen systems and to design modelsof biological systems like oscillatory gene regulatory networks. We furtherdemonstrate our method's use in analyzing real data by recovering distinctproliferation and differentiation dynamics along pancreatic endocrinogenesistrajectory in gene expression space based on single-cell data. Our methodprovides valuable insights into the qualitative, long-term behavior of a widerange of dynamical systems, and can detect bifurcations or catastrophictransitions in large-scale physical and biological systems.</description><author>Noa Moriel, Matthew Ricci, Mor Nitzan</author><pubDate>Thu, 21 Mar 2024 17:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09234v3</guid></item><item><title>Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering</title><link>http://arxiv.org/abs/2009.09213v6</link><description>The current high-fidelity generation and high-precision detection of DeepFakeimages are at an arms race. We believe that producing DeepFakes that are highlyrealistic and 'detection evasive' can serve the ultimate goal of improvingfuture generation DeepFake detection capabilities. In this paper, we propose asimple yet powerful pipeline to reduce the artifact patterns of fake imageswithout hurting image quality by performing implicit spatial-domain notchfiltering. We first demonstrate that frequency-domain notch filtering, althoughfamously shown to be effective in removing periodic noise in the spatialdomain, is infeasible for our task at hand due to the manual designs requiredfor the notch filters. We, therefore, resort to a learning-based approach toreproduce the notch filtering effects, but solely in the spatial domain. Weadopt a combination of adding overwhelming spatial noise for breaking theperiodic noise pattern and deep image filtering to reconstruct the noise-freefake images, and we name our method DeepNotch. Deep image filtering provides aspecialized filter for each pixel in the noisy image, producing filtered imageswith high fidelity compared to their DeepFake counterparts. Moreover, we alsouse the semantic information of the image to generate an adversarial guidancemap to add noise intelligently. Our large-scale evaluation on 3 representativestate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)has demonstrated that our technique significantly reduces the accuracy of these3 fake image detection methods, 36.79% on average and up to 97.02% in the bestcase.</description><author>Yihao Huang, Felix Juefei-Xu, Qing Guo, Yang Liu, Geguang Pu</author><pubDate>Thu, 21 Mar 2024 17:24:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.09213v6</guid></item><item><title>Invisible Needle Detection in Ultrasound: Leveraging Mechanism-Induced Vibration</title><link>http://arxiv.org/abs/2403.14523v1</link><description>In clinical applications that involve ultrasound-guided intervention, thevisibility of the needle can be severely impeded due to steep insertion andstrong distractors such as speckle noise and anatomical occlusion. To addressthis challenge, we propose VibNet, a learning-based framework tailored toenhance the robustness and accuracy of needle detection in ultrasound images,even when the target becomes invisible to the naked eye. Inspired by EulerianVideo Magnification techniques, we utilize an external step motor to inducelow-amplitude periodic motion on the needle. These subtle vibrations offer thepotential to generate robust frequency features for detecting the motionpatterns around the needle. To robustly and precisely detect the needleleveraging these vibrations, VibNet integrates learning-basedShort-Time-Fourier-Transform and Hough-Transform modules to achieve successivesub-goals, including motion feature extraction in the spatiotemporal space,frequency feature aggregation, and needle detection in the Hough space. Basedon the results obtained on distinct ex vivo porcine and bovine tissue samples,the proposed algorithm exhibits superior detection performance with efficientcomputation and generalization capability.</description><author>Chenyang Li, Dianye Huang, Angelos Karlas, Nassir Navab, Zhongliang Jiang</author><pubDate>Thu, 21 Mar 2024 17:23:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14523v1</guid></item><item><title>QuATON: Quantization Aware Training of Optical Neurons</title><link>http://arxiv.org/abs/2310.03049v2</link><description>Optical processors, built with "optical neurons", can efficiently performhigh-dimensional linear operations at the speed of light. Thus they are apromising avenue to accelerate large-scale linear computations. With thecurrent advances in micro-fabrication, such optical processors can now be 3Dfabricated, but with a limited precision. This limitation translates toquantization of learnable parameters in optical neurons, and should be handledduring the design of the optical processor in order to avoid a model mismatch.Specifically, optical neurons should be trained or designed within thephysical-constraints at a predefined quantized precision level. To address thiscritical issues we propose a physics-informed quantization-aware trainingframework. Our approach accounts for physical constraints during the trainingprocess, leading to robust designs. We demonstrate that our approach can designstate of the art optical processors using diffractive networks for multiplephysics based tasks despite quantized learnable parameters. We thus lay thefoundation upon which improved optical processors may be 3D fabricated in thefuture.</description><author>Hasindu Kariyawasam, Ramith Hettiarachchi, Quansan Yang, Alex Matlock, Takahiro Nambara, Hiroyuki Kusaka, Yuichiro Kunai, Peter T C So, Edward S Boyden, Dushan Wadduwage</author><pubDate>Thu, 21 Mar 2024 17:21:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03049v2</guid></item><item><title>Collaborative Distributed Machine Learning</title><link>http://arxiv.org/abs/2309.16584v3</link><description>Various collaborative distributed machine learning (CDML) systems, includingfederated learning systems and swarm learning systems, with different keytraits were developed to leverage resources for development and use of machinelearning (ML) models in a confidentiality-preserving way. To meet use caserequirements, suitable CDML systems need to be selected. However, comparisonbetween CDML systems regarding their suitability for use cases is oftendifficult. This work presents a CDML system conceptualization and CDMLarchetypes to support comparison of CDML systems and introduce scientific andpractical audiences to the principal functioning and key traits of CDMLsystems.</description><author>David Jin, Niclas Kannengießer, Sascha Rank, Ali Sunyaev</author><pubDate>Thu, 21 Mar 2024 17:21:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16584v3</guid></item><item><title>Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference</title><link>http://arxiv.org/abs/2403.14520v1</link><description>In recent years, the application of multimodal large language models (MLLM)in various fields has achieved remarkable success. However, as the foundationmodel for many downstream tasks, current MLLMs are composed of the well-knownTransformer network, which has a less efficient quadratic computationcomplexity. To improve the efficiency of such basic models, we propose Cobra, alinear computational complexity MLLM. Specifically, Cobra integrates theefficient Mamba language model into the visual modality. Moreover, we exploreand study various modal fusion schemes to create an effective multi-modalMamba. Extensive experiments demonstrate that (1) Cobra achieves extremelycompetitive performance with current computationally efficient state-of-the-artmethods, \textit{e.g.}, LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has fasterspeed due to Cobra's linear sequential modeling. (2) Interestingly, the resultsof closed-set challenging prediction benchmarks show that Cobra performs wellin overcoming visual illusions and spatial relationship judgments. (3) Notably,Cobra even achieves comparable performance to LLaVA with about 43% of thenumber of parameters. We will make all codes of Cobra open-source and hope thatthe proposed method can facilitate future research on complexity problems inMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.</description><author>Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, Donglin Wang</author><pubDate>Thu, 21 Mar 2024 17:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14520v1</guid></item><item><title>Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton</title><link>http://arxiv.org/abs/2403.11879v2</link><description>In this study, we propose a methodology for the Emotional Mimicry Intensity(EMI) Estimation task within the context of the 6th Workshop and Competition onAffective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0framework, pre-trained on a comprehensive podcast dataset, to extract a broadrange of audio features encompassing both linguistic and paralinguisticelements. We enhance feature representation through a fusion technique thatintegrates individual features with a global mean vector, introducing globalcontextual insights into our analysis. Additionally, we incorporate apre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficienttemporal analysis of audio data. Utilizing only the provided audio data, ourapproach demonstrates significant improvements over the established baseline.</description><author>Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André</author><pubDate>Thu, 21 Mar 2024 17:15:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11879v2</guid></item><item><title>Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles</title><link>http://arxiv.org/abs/2205.14116v3</link><description>Counterfactual explanations describe how to modify a feature vector in orderto flip the outcome of a trained classifier. Obtaining robust counterfactualexplanations is essential to provide valid algorithmic recourse and meaningfulexplanations. We study the robustness of explanations of randomized ensembles,which are always subject to algorithmic uncertainty even when the training datais fixed. We formalize the generation of robust counterfactual explanations asa probabilistic problem and show the link between the robustness of ensemblemodels and the robustness of base learners. We develop a practical method withgood empirical performance and support it with theoretical guarantees forensembles of convex base learners. Our results show that existing methods givesurprisingly low robustness: the validity of naive counterfactuals is below$50\%$ on most data sets and can fall to $20\%$ on problems with many features.In contrast, our method achieves high robustness with only a small increase inthe distance from counterfactual explanations to their initial observations.</description><author>Alexandre Forel, Axel Parmentier, Thibaut Vidal</author><pubDate>Thu, 21 Mar 2024 17:14:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.14116v3</guid></item><item><title>Building a Language-Learning Game for Brazilian Indigenous Languages: A Case of Study</title><link>http://arxiv.org/abs/2403.14515v1</link><description>In this paper we discuss a first attempt to build a language learning gamefor brazilian indigenous languages and the challenges around it. We present adesign for the tool with gamification aspects. Then we describe a process toautomatically generate language exercises and questions from a dependencytreebank and a lexical database for Tupian languages. We discuss thelimitations of our prototype highlighting ethical and practical implementationconcerns. Finally, we conclude that new data gathering processes should beestablished in partnership with indigenous communities and oriented foreducational purposes.</description><author>Gustavo Polleti</author><pubDate>Thu, 21 Mar 2024 17:11:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14515v1</guid></item><item><title>Neural Radiance Fields in Medical Imaging: Challenges and Next Steps</title><link>http://arxiv.org/abs/2402.17797v3</link><description>Neural Radiance Fields (NeRF), as a pioneering technique in computer vision,offer great potential to revolutionize medical imaging by synthesizingthree-dimensional representations from the projected two-dimensional imagedata. However, they face unique challenges when applied to medicalapplications. This paper presents a comprehensive examination of applicationsof NeRFs in medical imaging, highlighting four imminent challenges, includingfundamental imaging principles, inner structure requirement, object boundarydefinition, and color density significance. We discuss current methods ondifferent organs and discuss related limitations. We also review severaldatasets and evaluation metrics and propose several promising directions forfuture research.</description><author>Xin Wang, Shu Hu, Heng Fan, Hongtu Zhu, Xin Li</author><pubDate>Thu, 21 Mar 2024 17:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17797v3</guid></item><item><title>Assessing the Causal Impact of Humanitarian Aid on Food Security</title><link>http://arxiv.org/abs/2310.11287v2</link><description>In the face of climate change-induced droughts, vulnerable regions encountersevere threats to food security, demanding urgent humanitarian assistance. Thispaper introduces a causal inference framework for the Horn of Africa, aiming toassess the impact of cash-based interventions on food crises. Our contributionsinclude identifying causal relationships within the food security system,harmonizing a comprehensive database including socio-economic, weather andremote sensing data, and estimating the causal effect of humanitarianinterventions on malnutrition. On a country level, our results revealed nosignificant effects, likely due to limited sample size, suboptimal dataquality, and an imperfect causal graph resulting from our limited understandingof multidisciplinary systems like food security. Instead, on a district level,results revealed significant effects, further implying the context-specificnature of the system. This underscores the need to enhance data collection andrefine causal models with domain experts for more effective futureinterventions and policies, improving transparency and accountability inhumanitarian aid.</description><author>Jordi Cerdà-Bautista, José María Tárraga, Vasileios Sitokonstantinou, Gustau Camps-Valls</author><pubDate>Thu, 21 Mar 2024 17:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11287v2</guid></item><item><title>Machine-learning invariant foliations in forced systems for reduced order modelling</title><link>http://arxiv.org/abs/2403.14514v1</link><description>We identify reduced order models (ROM) of forced systems from data usinginvariant foliations. The forcing can be external, parametric, periodic orquasi-periodic. The process has four steps: 1. identify an approximateinvariant torus and the linear dynamics about the torus; 2. identify a globallydefined invariant foliation about the torus; 3. identify a local foliationabout an invariant manifold that complements the global foliation 4. extractthe invariant manifold as the leaf going through the torus and interpret theresult. We combine steps 2 and 3, so that we can track the location of theinvariant torus and scale the invariance equations appropriately. We highlightsome fundamental limitations of invariant manifolds and foliations when fittingthem to data, that require further mathematics to resolve.</description><author>Robert Szalai</author><pubDate>Thu, 21 Mar 2024 17:10:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14514v1</guid></item><item><title>Learning a Depth Covariance Function</title><link>http://arxiv.org/abs/2303.12157v2</link><description>We propose learning a depth covariance function with applications togeometric vision tasks. Given RGB images as input, the covariance function canbe flexibly used to define priors over depth functions, predictivedistributions given observations, and methods for active point selection. Weleverage these techniques for a selection of downstream tasks: depthcompletion, bundle adjustment, and monocular dense visual odometry.</description><author>Eric Dexheimer, Andrew J. Davison</author><pubDate>Thu, 21 Mar 2024 17:09:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12157v2</guid></item><item><title>View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network</title><link>http://arxiv.org/abs/2403.14513v1</link><description>Existing person re-identification methods have achieved remarkable advancesin appearance-based identity association across homogeneous cameras, such asground-ground matching. However, as a more practical scenario, aerial-groundperson re-identification (AGPReID) among heterogeneous cameras has receivedminimal attention. To alleviate the disruption of discriminative identityrepresentation by dramatic view discrepancy as the most significant challengein AGPReID, the view-decoupled transformer (VDT) is proposed as a simple yeteffective framework. Two major components are designed in VDT to decoupleview-related and view-unrelated features, namely hierarchical subtractiveseparation and orthogonal loss, where the former separates these two featuresinside the VDT, and the latter constrains these two to be independent. Inaddition, we contribute a large-scale AGPReID dataset called CARGO, consistingof five/eight aerial/ground cameras, 5,000 identities, and 108,563 images.Experiments on two datasets show that VDT is a feasible and effective solutionfor AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% onCARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computationalcomplexity. Our project is available at https://github.com/LinlyAC/VDT-AGPReID</description><author>Quan Zhang, Lei Wang, Vishal M. Patel, Xiaohua Xie, Jianhuang Lai</author><pubDate>Thu, 21 Mar 2024 17:08:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14513v1</guid></item><item><title>T-MAE: Temporal Masked Autoencoders for Point Cloud Representation Learning</title><link>http://arxiv.org/abs/2312.10217v2</link><description>The scarcity of annotated data in LiDAR point cloud understanding hinderseffective representation learning. Consequently, scholars have been activelyinvestigating efficacious self-supervised pre-training paradigms. Nevertheless,temporal information, which is inherent in the LiDAR point cloud sequence, isconsistently disregarded. To better utilize this property, we propose aneffective pre-training strategy, namely Temporal Masked Auto-Encoders (T-MAE),which takes as input temporally adjacent frames and learns temporal dependency.A SiamWCA backbone, containing a Siamese encoder and a windowed cross-attention(WCA) module, is established for the two-frame input. Considering that themovement of an ego-vehicle alters the view of the same instance, temporalmodeling also serves as a robust and natural data augmentation, enhancing thecomprehension of target objects. SiamWCA is a powerful architecture but heavilyrelies on annotated data. Our T-MAE pre-training strategy alleviates its demandfor annotated data. Comprehensive experiments demonstrate that T-MAE achievesthe best performance on both Waymo and ONCE datasets among competitiveself-supervised approaches.</description><author>Weijie Wei, Fatemeh Karimi Nejadasl, Theo Gevers, Martin R. Oswald</author><pubDate>Thu, 21 Mar 2024 17:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10217v2</guid></item><item><title>Constrained Reinforcement Learning with Smoothed Log Barrier Function</title><link>http://arxiv.org/abs/2403.14508v1</link><description>Reinforcement Learning (RL) has been widely applied to many control tasks andsubstantially improved the performances compared to conventional controlmethods in many domains where the reward function is well defined. However, formany real-world problems, it is often more convenient to formulate optimizationproblems in terms of rewards and constraints simultaneously. Optimizing suchconstrained problems via reward shaping can be difficult as it requires tediousmanual tuning of reward functions with several interacting terms. Recentformulations which include constraints mostly require a pre-training phase,which often needs human expertise to collect data or assumes having asub-optimal policy readily available. We propose a new constrained RL methodcalled CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), whichachieves competitive performance without any pre-training by applying a linearsmoothed log barrier function to an additional safety critic. It implements anadaptive penalty for policy learning and alleviates the numerical issues thatare known to complicate the application of the log barrier function method. Asa result, we show that with CSAC-LB, we achieve state-of-the-art performance onseveral constrained control tasks with different levels of difficulty andevaluate our methods in a locomotion task on a real quadruped robot platform.</description><author>Baohe Zhang, Yuan Zhang, Lilli Frison, Thomas Brox, Joschka Bödecker</author><pubDate>Thu, 21 Mar 2024 17:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14508v1</guid></item><item><title>TMI! Finetuned Models Leak Private Information from their Pretraining Data</title><link>http://arxiv.org/abs/2306.01181v2</link><description>Transfer learning has become an increasingly popular technique in machinelearning as a way to leverage a pretrained model trained for one task to assistwith building a finetuned model for a related task. This paradigm has beenespecially popular for $\textit{privacy}$ in machine learning, where thepretrained model is considered public, and only the data for finetuning isconsidered sensitive. However, there are reasons to believe that the data usedfor pretraining is still sensitive, making it essential to understand how muchinformation the finetuned model leaks about the pretraining data. In this workwe propose a new membership-inference threat model where the adversary only hasaccess to the finetuned model and would like to infer the membership of thepretraining data. To realize this threat model, we implement a novelmetaclassifier-based attack, $\textbf{TMI}$, that leverages the influence ofmemorized pretraining samples on predictions in the downstream task. Weevaluate $\textbf{TMI}$ on both vision and natural language tasks acrossmultiple transfer learning settings, including finetuning with differentialprivacy. Through our evaluation, we find that $\textbf{TMI}$ can successfullyinfer membership of pretraining examples using query access to the finetunedmodel. An open-source implementation of $\textbf{TMI}$ can be found$\href{https://github.com/johnmath/tmi-pets24}{\text{on GitHub}}$.</description><author>John Abascal, Stanley Wu, Alina Oprea, Jonathan Ullman</author><pubDate>Thu, 21 Mar 2024 16:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01181v2</guid></item><item><title>Ins-HOI: Instance Aware Human-Object Interactions Recovery</title><link>http://arxiv.org/abs/2312.09641v2</link><description>Accurately modeling detailed interactions between human/hand and object is anappealing yet challenging task. Current multi-view capture systems are onlycapable of reconstructing multiple subjects into a single, unified mesh, whichfails to model the states of each instance individually during interactions. Toaddress this, previous methods use template-based representations to trackhuman/hand and object. However, the quality of the reconstructions is limitedby the descriptive capabilities of the templates so that these methods areinherently struggle with geometry details, pressing deformations and invisiblecontact surfaces. In this work, we propose an end-to-end Instance-awareHuman-Object Interactions recovery (Ins-HOI) framework by introducing aninstance-level occupancy field representation. However, the real-captured datais presented as a holistic mesh, unable to provide instance-level supervision.To address this, we further propose a complementary training strategy thatleverages synthetic data to introduce instance-level shape priors, enabling thedisentanglement of occupancy fields for different instances. Specifically,synthetic data, created by randomly combining individual scans of humans/handsand objects, guides the network to learn a coarse prior of instances.Meanwhile, real-captured data helps in learning the overall geometry andrestricting interpenetration in contact areas. As demonstrated in experiments,our method Ins-HOI supports instance-level reconstruction and providesreasonable and realistic invisible contact surfaces even in cases of extremelyclose interaction. To facilitate the research of this task, we collect alarge-scale, high-fidelity 3D scan dataset, including 5.2k high-quality scanswith real-world human-chair and hand-object interactions. The code and datawill be public for research purposes.</description><author>Jiajun Zhang, Yuxiang Zhang, Hongwen Zhang, Xiao Zhou, Boyao Zhou, Ruizhi Shao, Zonghai Hu, Yebin Liu</author><pubDate>Thu, 21 Mar 2024 16:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09641v2</guid></item><item><title>Soft Learning Probabilistic Circuits</title><link>http://arxiv.org/abs/2403.14504v1</link><description>Probabilistic Circuits (PCs) are prominent tractable probabilistic models,allowing for a range of exact inferences. This paper focuses on the mainalgorithm for training PCs, LearnSPN, a gold standard due to its efficiency,performance, and ease of use, in particular for tabular data. We show thatLearnSPN is a greedy likelihood maximizer under mild assumptions. Whileinferences in PCs may use the entire circuit structure for processing queries,LearnSPN applies a hard method for learning them, propagating at each sum nodea data point through one and only one of the children/edges as in a hardclustering process. We propose a new learning procedure named SoftLearn, thatinduces a PC using a soft clustering process. We investigate the effect of thislearning-inference compatibility in PCs. Our experiments show that SoftLearnoutperforms LearnSPN in many situations, yielding better likelihoods andarguably better samples. We also analyze comparable tractable models tohighlight the differences between soft/hard learning and model querying.</description><author>Soroush Ghandi, Benjamin Quost, Cassio de Campos</author><pubDate>Thu, 21 Mar 2024 16:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14504v1</guid></item><item><title>Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting</title><link>http://arxiv.org/abs/2403.14499v1</link><description>Monitoring diseases that affect the brain's structural integrity requiresautomated analysis of magnetic resonance (MR) images, e.g., for the evaluationof volumetric changes. However, many of the evaluation tools are optimized foranalyzing healthy tissue. To enable the evaluation of scans containingpathological tissue, it is therefore required to restore healthy tissue in thepathological areas. In this work, we explore and extend denoising diffusionmodels for consistent inpainting of healthy 3D brain tissue. We modifystate-of-the-art 2D, pseudo-3D, and 3D methods working in the image space, aswell as 3D latent and 3D wavelet diffusion models, and train them to synthesizehealthy brain tissue. Our evaluation shows that the pseudo-3D model performsbest regarding the structural-similarity index, peak signal-to-noise ratio, andmean squared error. To emphasize the clinical relevance, we fine-tune thismodel on data containing synthetic MS lesions and evaluate it on a downstreambrain tissue segmentation task, whereby it outperforms the established FMRIBSoftware Library (FSL) lesion-filling method.</description><author>Alicia Durrer, Julia Wolleb, Florentin Bieder, Paul Friedrich, Lester Melie-Garcia, Mario Ocampo-Pineda, Cosmin I. Bercea, Ibrahim E. Hamamci, Benedikt Wiestler, Marie Piraud, Özgür Yaldizli, Cristina Granziera, Bjoern H. Menze, Philippe C. Cattin, Florian Kofler</author><pubDate>Thu, 21 Mar 2024 16:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14499v1</guid></item><item><title>MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection</title><link>http://arxiv.org/abs/2403.14497v1</link><description>We propose a novel approach to video anomaly detection: we treat featurevectors extracted from videos as realizations of a random variable with a fixeddistribution and model this distribution with a neural network. This lets usestimate the likelihood of test videos and detect video anomalies bythresholding the likelihood estimates. We train our video anomaly detectorusing a modification of denoising score matching, a method that injectstraining data with noise to facilitate modeling its distribution. To eliminatehyperparameter selection, we model the distribution of noisy video featuresacross a range of noise levels and introduce a regularizer that tends to alignthe models for different levels of noise. At test time, we combine anomalyindications at multiple noise scales with a Gaussian mixture model. Running ourvideo anomaly detector induces minimal delays as inference requires merelyextracting the features and forward-propagating them through a shallow neuralnetwork and a Gaussian mixture model. Our experiments on five popular videoanomaly detection benchmarks demonstrate state-of-the-art performance, both inthe object-centric and in the frame-centric setup.</description><author>Jakub Micorek, Horst Possegger, Dominik Narnhofer, Horst Bischof, Mateusz Kozinski</author><pubDate>Thu, 21 Mar 2024 16:46:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14497v1</guid></item><item><title>GIVT: Generative Infinite-Vocabulary Transformers</title><link>http://arxiv.org/abs/2312.02116v3</link><description>We introduce generative infinite-vocabulary transformers (GIVT) whichgenerate vector sequences with real-valued entries, instead of discrete tokensfrom a finite vocabulary. To this end, we propose two surprisingly simplemodifications to decoder-only transformers: 1) at the input, we replace thefinite-vocabulary lookup table with a linear projection of the input vectors;and 2) at the output, we replace the logits prediction (usually mapped to acategorical distribution) with the parameters of a multivariate Gaussianmixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,where transformers are used to model the discrete latent sequences of a VQ-VAE,we use GIVT to model the unquantized real-valued latent sequences of a$\beta$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (andimproved variants thereof) as well as MaskGIT, and achieves performancecompetitive with recent latent diffusion models. Finally, we obtain strongresults outside of image generation when applying GIVT to panoptic segmentationand depth estimation with a VAE variant of the UViM framework</description><author>Michael Tschannen, Cian Eastwood, Fabian Mentzer</author><pubDate>Thu, 21 Mar 2024 16:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02116v3</guid></item><item><title>How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey</title><link>http://arxiv.org/abs/2403.14496v1</link><description>Despite its technological breakthroughs, eXplainable Artificial Intelligence(XAI) research has limited success in producing the {\em effectiveexplanations} needed by users. In order to improve XAI systems' usability,practical interpretability, and efficacy for real users, the emerging area of{\em Explainable Interfaces} (EIs) focuses on the user interface and userexperience design aspects of XAI. This paper presents a systematic survey of 53publications to identify current trends in human-XAI interaction and promisingdirections for EI design and development. This is among the first systematicsurvey of EI research.</description><author>Thu Nguyen, Alessandro Canossa, Jichen Zhu</author><pubDate>Thu, 21 Mar 2024 16:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14496v1</guid></item><item><title>Learning to Project for Cross-Task Knowledge Distillation</title><link>http://arxiv.org/abs/2403.14494v1</link><description>Traditional knowledge distillation (KD) relies on a proficient teachertrained on the target task, which is not always available. In this setting,cross-task distillation can be used, enabling the use of any teacher modeltrained on a different task. However, many KD methods prove ineffective whenapplied to this cross-task setting. To address this limitation, we propose asimple modification: the use of an inverted projection. We show that thisdrop-in replacement for a standard projector is effective by learning todisregard any task-specific features which might degrade the student'sperformance. We find that this simple modification is sufficient for extendingmany KD methods to the cross-task setting, where the teacher and student taskscan be very different. In doing so, we obtain up to a 1.9% improvement in thecross-task setting compared to the traditional projection, at no additionalcost. Our method can obtain significant performance improvements (up to 7%)when using even a randomly-initialised teacher on various tasks such as depthestimation, image translation, and semantic segmentation, despite the lack ofany learned knowledge to transfer. To provide conceptual and analyticalinsights into this result, we show that using an inverted projection allows thedistillation loss to be decomposed into a knowledge transfer and a spectralregularisation component. Through this analysis we are additionally able topropose a novel regularisation loss that allows teacher-free distillation,enabling performance improvements of up to 8.57% on ImageNet with no additionaltraining costs.</description><author>Dylan Auty, Roy Miles, Benedikt Kolbeinsson, Krystian Mikolajczyk</author><pubDate>Thu, 21 Mar 2024 16:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14494v1</guid></item><item><title>Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against Query-Based Attacks</title><link>http://arxiv.org/abs/2312.10132v2</link><description>Although promising, existing defenses against query-based attacks share acommon limitation: they offer increased robustness against attacks at the priceof a considerable accuracy drop on clean samples. In this work, we show how toefficiently establish, at test-time, a solid tradeoff between robustness andaccuracy when mitigating query-based attacks. Given that these attacksnecessarily explore low-confidence regions, our insight is that activatingdedicated defenses, such as random noise defense and random imagetransformations, only for low-confidence inputs is sufficient to prevent them.Our approach is independent of training and supported by theory. We verify theeffectiveness of our approach for various existing defenses by conductingextensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirmthat our proposal can indeed enhance these defenses by providing bettertradeoffs between robustness and accuracy when compared to state-of-the-artapproaches while being completely training-free.</description><author>Pascal Zimmer, Sébastien Andreina, Giorgia Azzurra Marson, Ghassan Karame</author><pubDate>Thu, 21 Mar 2024 16:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10132v2</guid></item><item><title>Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations</title><link>http://arxiv.org/abs/2403.13261v2</link><description>The perception of motion behavior in a dynamic environment holds significantimportance for autonomous driving systems, wherein class-agnostic motionprediction methods directly predict the motion of the entire point cloud. Whilemost existing methods rely on fully-supervised learning, the manual labeling ofpoint cloud data is laborious and time-consuming. Therefore, severalannotation-efficient methods have been proposed to address this challenge.Although effective, these methods rely on weak annotations or additionalmulti-modal data like images, and the potential benefits inherent in the pointcloud sequence are still underexplored. To this end, we explore the feasibilityof self-supervised motion prediction with only unlabeled LiDAR point clouds.Initially, we employ an optimal transport solver to establish coarsecorrespondences between current and future point clouds as the coarse pseudomotion labels. Training models directly using such coarse labels leads tonoticeable spatial and temporal prediction inconsistencies. To mitigate theseissues, we introduce three simple spatial and temporal regularization losses,which facilitate the self-supervised training process effectively. Experimentalresults demonstrate the significant superiority of our approach over thestate-of-the-art self-supervised methods.</description><author>Kewei Wang, Yizheng Wu, Jun Cen, Zhiyu Pan, Xingyi Li, Zhe Wang, Zhiguo Cao, Guosheng Lin</author><pubDate>Thu, 21 Mar 2024 16:40:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13261v2</guid></item><item><title>Adversary-Robust Graph-Based Learning of WSIs</title><link>http://arxiv.org/abs/2403.14489v1</link><description>Enhancing the robustness of deep learning models against adversarial attacksis crucial, especially in critical domains like healthcare where significantfinancial interests heighten the risk of such attacks. Whole slide images(WSIs) are high-resolution, digitized versions of tissue samples mounted onglass slides, scanned using sophisticated imaging equipment. The digitalanalysis of WSIs presents unique challenges due to their gigapixel size andmulti-resolution storage format. In this work, we aim at improving therobustness of cancer Gleason grading classification systems against adversarialattacks, addressing challenges at both the image and graph levels. As regardsthe proposed algorithm, we develop a novel and innovative graph-based modelwhich utilizes GNN to extract features from the graph representation of WSIs. Adenoising module, along with a pooling layer is incorporated to manage theimpact of adversarial attacks on the WSIs. The process concludes with atransformer module that classifies various grades of prostate cancer based onthe processed data. To assess the effectiveness of the proposed method, weconducted a comparative analysis using two scenarios. Initially, we trained andtested the model without the denoiser using WSIs that had not been exposed toany attack. We then introduced a range of attacks at either the image or graphlevel and processed them through the proposed network. The performance of themodel was evaluated in terms of accuracy and kappa scores. The results fromthis comparison showed a significant improvement in cancer diagnosis accuracy,highlighting the robustness and efficiency of the proposed method in handlingadversarial challenges in the context of medical imaging.</description><author>Saba Heidari Gheshlaghi, Milan Aryal, Nasim Yahyasoltani, Masoud Ganji</author><pubDate>Thu, 21 Mar 2024 16:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14489v1</guid></item><item><title>Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action Selection in Robot Manipulation Tasks</title><link>http://arxiv.org/abs/2403.14488v1</link><description>Safe and efficient object manipulation is a key enabler of many real-worldrobot applications. However, this is challenging because robot operation mustbe robust to a range of sensor and actuator uncertainties. In this paper, wepresent a physics-informed causal-inference-based framework for a robot toprobabilistically reason about candidate actions in a block stacking task in apartially observable setting. We integrate a physics-based simulation of therigid-body system dynamics with a causal Bayesian network (CBN) formulation todefine a causal generative probabilistic model of the robot decision-makingprocess. Using simulation-based Monte Carlo experiments, we demonstrate ourframework's ability to successfully: (1) predict block tower stability withhigh accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-bestaction for the block stacking task, for execution by an integrated robotsystem, achieving 94.2% task success rate. We also demonstrate our framework'ssuitability for real-world robot systems by demonstrating successful taskexecutions with a domestic support robot, with perception and manipulationsub-system integration. Hence, we show that by embedding physics-based causalreasoning into robots' decision-making processes, we can make robot taskexecution safer, more reliable, and more robust to various types ofuncertainty.</description><author>Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze</author><pubDate>Thu, 21 Mar 2024 16:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14488v1</guid></item><item><title>DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified &amp; Accurate Image Editing</title><link>http://arxiv.org/abs/2403.14487v1</link><description>Recently, how to achieve precise image editing has attracted increasingattention, especially given the remarkable success of text-to-image generationmodels. To unify various spatial-aware image editing abilities into oneframework, we adopt the concept of layers from the design domain to manipulateobjects flexibly with various operations. The key insight is to transform thespatial-aware image editing task into a combination of two sub-tasks:multi-layered latent decomposition and multi-layered latent fusion. First, wesegment the latent representations of the source images into multiple layers,which include several object layers and one incomplete background layer thatnecessitates reliable inpainting. To avoid extra tuning, we further explore theinner inpainting ability within the self-attention mechanism. We introduce akey-masking self-attention scheme that can propagate the surrounding contextinformation into the masked region while mitigating its impact on the regionsoutside the mask. Second, we propose an instruction-guided latent fusion thatpastes the multi-layered latent representations onto a canvas latent. We alsointroduce an artifact suppression scheme in the latent space to enhance theinpainting quality. Due to the inherent modular advantages of suchmulti-layered representations, we can achieve accurate image editing, and wedemonstrate that our approach consistently surpasses the latest spatial editingmethods, including Self-Guidance and DiffEditor. Last, we show that ourapproach is a unified framework that supports various accurate image editingtasks on more than six different editing tasks.</description><author>Yueru Jia, Yuhui Yuan, Aosong Cheng, Chuke Wang, Ji Li, Huizhu Jia, Shanghang Zhang</author><pubDate>Thu, 21 Mar 2024 16:35:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14487v1</guid></item><item><title>EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models</title><link>http://arxiv.org/abs/2402.03049v3</link><description>In recent years, instruction tuning has gained increasing attention andemerged as a crucial technique to enhance the capabilities of Large LanguageModels (LLMs). To construct high-quality instruction datasets, many instructionprocessing approaches have been proposed, aiming to achieve a delicate balancebetween data quantity and data quality. Nevertheless, due to inconsistenciesthat persist among various instruction processing methods, there is no standardopen-source instruction processing implementation framework available for thecommunity, which hinders practitioners from further developing and advancing.To facilitate instruction processing research and development, we presentEasyInstruct, an easy-to-use instruction processing framework for LLMs, whichmodularizes instruction generation, selection, and prompting, while alsoconsidering their combination and interaction. EasyInstruct is publiclyreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,along with an online demo app and a demo video for quick-start, calling forbroader research centered on instruction data and synthetic data.</description><author>Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen</author><pubDate>Thu, 21 Mar 2024 16:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03049v3</guid></item><item><title>ColonNeRF: High-Fidelity Neural Reconstruction of Long Colonoscopy</title><link>http://arxiv.org/abs/2312.02015v2</link><description>Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.However, accurate long-sequence colonoscopy reconstruction faces three majorchallenges: (1) dissimilarity among segments of the colon due to its meanderingand convoluted shape; (2) co-existence of simple and intricately foldedgeometry structures; (3) sparse viewpoints due to constrained cameratrajectories. To tackle these challenges, we introduce a new reconstructionframework based on neural radiance field (NeRF), named ColonNeRF, whichleverages neural rendering for novel view synthesis of long-sequencecolonoscopy. Specifically, to reconstruct the entire colon in a piecewisemanner, our ColonNeRF introduces a region division and integration module,effectively reducing shape dissimilarity and ensuring geometric consistency ineach segment. To learn both the simple and complex geometry in a unifiedframework, our ColonNeRF incorporates a multi-level fusion module thatprogressively models the colon regions from easy to hard. Additionally, toovercome the challenges from sparse views, we devise a DensiNet module fordensifying camera poses under the guidance of semantic consistency. We conductextensive experiments on both synthetic and real-world datasets to evaluate ourColonNeRF. Quantitatively, ColonNeRF exhibits a 67%-85% increase in LPIPS-ALEXscores. Qualitatively, our reconstruction visualizations show much clearertextures and more accurate geometric details. These sufficiently demonstrateour superior performance over the state-of-the-art methods.</description><author>Yufei Shi, Beijia Lu, Jia-Wei Liu, Ming Li, Mike Zheng Shou</author><pubDate>Thu, 21 Mar 2024 16:32:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02015v2</guid></item><item><title>SignBank+: Preparing a Multilingual Sign Language Dataset for Machine Translation Using Large Language Models</title><link>http://arxiv.org/abs/2309.11566v2</link><description>We introduce SignBank+, a clean version of the SignBank dataset, optimizedfor machine translation between spoken language text and SignWriting, aphonetic sign language writing system. In addition to previous work thatemploys complex factorization techniques to enable translation between text andSignWriting, we show that a traditional text-to-text translation approachperforms equally effectively on the cleaned SignBank+ dataset. Our evaluationresults indicate that models trained on SignBank+ surpass those on the originaldataset, establishing a new benchmark for SignWriting-based sign languagetranslation and providing an open resource for future research.</description><author>Amit Moryossef, Zifan Jiang</author><pubDate>Thu, 21 Mar 2024 16:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11566v2</guid></item><item><title>HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges</title><link>http://arxiv.org/abs/2403.14484v1</link><description>Autism Spectrum Disorder (ASD) is a neurodevelopmental conditioncharacterized by varied social cognitive challenges and repetitive behavioralpatterns. Identifying reliable brain imaging-based biomarkers for ASD has beena persistent challenge due to the spectrum's diverse symptomatology. Existingbaselines in the field have made significant strides in this direction, yetthere remains room for improvement in both performance and interpretability. Wepropose \emph{HyperGALE}, which builds upon the hypergraph by incorporatinglearned hyperedges and gated attention mechanisms. This approach has led tosubstantial improvements in the model's ability to interpret complex braingraph data, offering deeper insights into ASD biomarker characterization.Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improvesinterpretability but also demonstrates statistically significant enhancementsin key performance metrics compared to both previous baselines and thefoundational hypergraph model. The advancement \emph{HyperGALE} brings to ASDresearch highlights the potential of sophisticated graph-based techniques inneurodevelopmental studies. The source code and implementation instructions areavailable at GitHub:https://github.com/mehular0ra/HyperGALE.</description><author>Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju Surampudi</author><pubDate>Thu, 21 Mar 2024 16:31:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14484v1</guid></item><item><title>Utilizing the LightGBM Algorithm for Operator User Credit Assessment Research</title><link>http://arxiv.org/abs/2403.14483v1</link><description>Mobile Internet user credit assessment is an important way for communicationoperators to establish decisions and formulate measures, and it is also aguarantee for operators to obtain expected benefits. However, credit evaluationmethods have long been monopolized by financial industries such as banks andcredit. As supporters and providers of platform network technology and networkresources, communication operators are also builders and maintainers ofcommunication networks. Internet data improves the user's credit evaluationstrategy. This paper uses the massive data provided by communication operatorsto carry out research on the operator's user credit evaluation model based onthe fusion LightGBM algorithm. First, for the massive data related to userevaluation provided by operators, key features are extracted by datapreprocessing and feature engineering methods, and a multi-dimensional featureset with statistical significance is constructed; then, linear regression,decision tree, LightGBM, and other machine learning algorithms build multiplebasic models to find the best basic model; finally, integrates Averaging,Voting, Blending, Stacking and other integrated algorithms to refine multiplefusion models, and finally establish the most suitable fusion model foroperator user evaluation.</description><author>Shaojie Li, Xinqi Dong, Danqing Ma, Bo Dang, Hengyi Zang, Yulu Gong</author><pubDate>Thu, 21 Mar 2024 16:29:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14483v1</guid></item><item><title>Detoxifying Large Language Models via Knowledge Editing</title><link>http://arxiv.org/abs/2403.14472v1</link><description>This paper investigates using knowledge editing techniques to detoxify LargeLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nineunsafe categories with various powerful attack prompts and equips comprehensivemetrics for systematic evaluation. We conduct experiments to compare knowledgeediting approaches with previous baselines, indicating that knowledge editinghas the potential to efficiently detoxify LLMs with limited impact on generalperformance. Then, we propose a simple yet effective baseline, dubbedDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish thetoxicity of LLMs within a few tuning steps via only one instance. We furtherprovide an in-depth analysis of the internal mechanism for various detoxifyapproaches, demonstrating that previous methods like SFT and DPO may merelysuppress the activations of toxic parameters, while DINM mitigates the toxicityof the toxic parameters to a certain extent, making permanent adjustments. Wehope that these insights could shed light on future work of developingdetoxifying approaches and the underlying knowledge mechanisms of LLMs. Codeand benchmark are available at https://github.com/zjunlp/EasyEdit.</description><author>Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen</author><pubDate>Thu, 21 Mar 2024 16:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14472v1</guid></item><item><title>Neuromorphic Imaging and Classification with Graph Learning</title><link>http://arxiv.org/abs/2309.15627v2</link><description>Bio-inspired neuromorphic cameras asynchronously record pixel brightnesschanges and generate sparse event streams. They can capture dynamic scenes withlittle motion blur and more details in extreme illumination conditions. Due tothe multidimensional address-event structure, most existing vision algorithmscannot properly handle asynchronous event streams. While several eventrepresentations and processing methods have been developed to address such anissue, they are typically driven by a large number of events, leading tosubstantial overheads in runtime and memory. In this paper, we propose a newgraph representation of the event data and couple it with a Graph Transformerto perform accurate neuromorphic classification. Extensive experiments showthat our approach leads to better results and excels at the challengingrealistic situations where only a small number of events and limitedcomputational resources are available, paving the way for neuromorphicapplications embedded into mobile facilities.</description><author>Pei Zhang, Chutian Wang, Edmund Y. Lam</author><pubDate>Thu, 21 Mar 2024 16:17:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15627v2</guid></item><item><title>ChatGPT Alternative Solutions: Large Language Models Survey</title><link>http://arxiv.org/abs/2403.14469v1</link><description>In recent times, the grandeur of Large Language Models (LLMs) has not onlyshone in the realm of natural language processing but has also cast itsbrilliance across a vast array of applications. This remarkable display of LLMcapabilities has ignited a surge in research contributions within this domain,spanning a diverse spectrum of topics. These contributions encompassadvancements in neural network architecture, context length enhancements, modelalignment, training datasets, benchmarking, efficiency improvements, and more.Recent years have witnessed a dynamic synergy between academia and industry,propelling the field of LLM research to new heights. A notable milestone inthis journey is the introduction of ChatGPT, a powerful AI chatbot grounded inLLMs, which has garnered widespread societal attention. The evolving technologyof LLMs has begun to reshape the landscape of the entire AI community,promising a revolutionary shift in the way we create and employ AI algorithms.Given this swift-paced technical evolution, our survey embarks on a journey toencapsulate the recent strides made in the world of LLMs. Through anexploration of the background, key discoveries, and prevailing methodologies,we offer an up-to-the-minute review of the literature. By examining multipleLLM models, our paper not only presents a comprehensive overview but alsocharts a course that identifies existing challenges and points toward potentialfuture research trajectories. This survey furnishes a well-rounded perspectiveon the current state of generative AI, shedding light on opportunities forfurther exploration, enhancement, and innovation.</description><author>Hanieh Alipour, Nick Pendar, Kohinoor Roy</author><pubDate>Thu, 21 Mar 2024 16:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14469v1</guid></item><item><title>BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image</title><link>http://arxiv.org/abs/2403.08262v2</link><description>Creating personalized hand avatars is important to offer a realisticexperience to users on AR / VR platforms. While most prior studies focused onreconstructing 3D hand shapes, some recent work has tackled the reconstructionof hand textures on top of shapes. However, these methods are often limited tocapturing pixels on the visible side of a hand, requiring diverse views of thehand in a video or multiple images as input. In this paper, we propose a novelmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is thefirst end-to-end trainable method for relightable, pose-free texturereconstruction of two interacting hands taking only a single RGB image, bythree novel components: 1) bi-directional (left $\leftrightarrow$ right)texture reconstruction using the texture symmetry of left / right hands, 2)utilizing a texture parametric model for hand texture recovery, and 3) theoverall coarse-to-fine stage pipeline for reconstructing personalized textureof two interacting hands. BiTT first estimates the scene light condition andalbedo image from an input image, then reconstructs the texture of both handsthrough the texture parametric model and bi-directional texture reconstructor.In experiments using InterHand2.6M and RGB2Hands datasets, our methodsignificantly outperforms state-of-the-art hand texture reconstruction methodsquantitatively and qualitatively. The code is available athttps://github.com/yunminjin2/BiTT</description><author>Minje Kim, Tae-Kyun Kim</author><pubDate>Thu, 21 Mar 2024 16:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08262v2</guid></item></channel></rss>