<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 11 Nov 2025 12:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs</title><link>http://arxiv.org/abs/2511.07419v1</link><description>Sparse Mixture-of-Experts (MoE) have been widely adopted in recent largelanguage models since it can efficiently scale up the model capability withoutincreasing the inference cost. However, evaluations on broad downstream tasksreveal a consistent suboptimality of the routers in existing MoE LLMs, whichresults in a severe performance gap (e.g., 10-20% in accuracy) to the optimalrouting. In this paper, we show that aligning the manifold of routing weightswith that of task embedding can effectively reduce the gap and improve MoELLMs' generalization performance. Our method, "Routing Manifold Alignment(RoMA)", introduces an additional manifold regularization term in thepost-training objective and only requires lightweight finetuning of routers(with other parameters frozen). Specifically, the regularization encourages therouting weights of each sample to be close to those of its successful neighbors(whose routing weights lead to correct answers) in a task embedding space.Consequently, samples targeting similar tasks will share similar expert choicesacross layers. Building such bindings between tasks and experts over differentsamples is essential to achieve better generalization. Moreover, RoMAdemonstrates the advantage of unifying the task understanding (by embeddingmodels) with solution generation (by MoE LLMs). In experiments, we finetunerouters in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diversebenchmarks and extensive comparisons with baselines show the substantialimprovement brought by RoMA.</description><author>Zhongyang Li, Ziyue Li, Tianyi Zhou</author><pubDate>Mon, 10 Nov 2025 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07419v1</guid></item><item><title>Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields</title><link>http://arxiv.org/abs/2511.07418v1</link><description>Despite years of research, real-time diverse grasp synthesis for dexteroushands remains an unsolved core challenge in robotics and computer graphics. Wepresent Lightning Grasp, a novel high-performance procedural grasp synthesisalgorithm that achieves orders-of-magnitude speedups over state-of-the-artapproaches, while enabling unsupervised grasp generation for irregular,tool-like objects. The method avoids many limitations of prior approaches, suchas the need for carefully tuned energy functions and sensitive initialization.This breakthrough is driven by a key insight: decoupling complex geometriccomputation from the search process via a simple, efficient data structure -the Contact Field. This abstraction collapses the problem complexity, enablinga procedural search at unprecedented speeds. We open-source our system topropel further innovation in robotic manipulation.</description><author>Zhao-Heng Yin, Pieter Abbeel</author><pubDate>Mon, 10 Nov 2025 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07418v1</guid></item><item><title>Language Generation with Infinite Contamination</title><link>http://arxiv.org/abs/2511.07417v1</link><description>We study language generation in the limit, where an algorithm observes anadversarial enumeration of strings from an unknown target language $K$ and musteventually generate new, unseen strings from $K$. Kleinberg and Mullainathan[KM24] proved that generation is achievable in surprisingly general settings.But their generator suffers from ``mode collapse,'' producing from anever-smaller subset of the target. To address this, Kleinberg and Wei [KW25]require the generator's output to be ``dense'' in the target language. Theyshowed that generation with density, surprisingly, remains achievable at thesame generality. Both results assume perfect data: no noisy insertions and no omissions. Thisraises a central question: how much contamination can generation tolerate?Recent works made partial progress on this question by studying (non-dense)generation with either finite amounts of noise (but no omissions) or omissions(but no noise). We characterize robustness under contaminated enumerations: 1. Generationunder Contamination: Language generation in the limit is achievable for allcountable collections iff the fraction of contaminated examples converges tozero. When this fails, we characterize which collections are generable. 2.Dense Generation under Contamination: Dense generation is strictly less robustto contamination than generation. As a byproduct, we resolve an open questionof Raman and Raman [ICML25] by showing that generation is possible with onlymembership oracle access under finitely many contaminated examples. Finally, we introduce a beyond-worst-case model inspired by curriculumlearning and prove that dense generation is achievable even with infinitecontamination provided the fraction of contaminated examples converges to zero.This suggests curriculum learning may be crucial for learning from noisy webdata.</description><author>Anay Mehrotra, Grigoris Velegkas, Xifan Yu, Felix Zhou</author><pubDate>Mon, 10 Nov 2025 18:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07417v1</guid></item><item><title>Robot Learning from a Physical World Model</title><link>http://arxiv.org/abs/2511.07416v1</link><description>We introduce PhysWorld, a framework that enables robot learning from videogeneration through physical world modeling. Recent video generation models cansynthesize photorealistic visual demonstrations from language commands andimages, offering a powerful yet underexplored source of training signals forrobotics. However, directly retargeting pixel motions from generated videos torobots neglects physics, often resulting in inaccurate manipulations. PhysWorldaddresses this limitation by coupling video generation with physical worldreconstruction. Given a single image and a task command, our method generatestask-conditioned videos and reconstructs the underlying physical world from thevideos, and the generated video motions are grounded into physically accurateactions through object-centric residual reinforcement learning with thephysical world model. This synergy transforms implicit visual guidance intophysically executable robotic trajectories, eliminating the need for real robotdata collection and enabling zero-shot generalizable robotic manipulation.Experiments on diverse real-world tasks demonstrate that PhysWorldsubstantially improves manipulation accuracy compared to previous approaches.Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage}for details.</description><author>Jiageng Mao, Sicheng He, Hao-Ning Wu, Yang You, Shuyang Sun, Zhicheng Wang, Yanan Bao, Huizhong Chen, Leonidas Guibas, Vitor Guizilini, Howard Zhou, Yue Wang</author><pubDate>Mon, 10 Nov 2025 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07416v1</guid></item><item><title>Wasserstein-Cramér-Rao Theory of Unbiased Estimation</title><link>http://arxiv.org/abs/2511.07414v1</link><description>The quantity of interest in the classical Cram\'er-Rao theory of unbiasedestimation (e.g., the Cram\'er-Rao lower bound, its exact attainment forexponential families, and asymptotic efficiency of maximum likelihoodestimation) is the variance, which represents the instability of an estimatorwhen its value is compared to the value for an independently-sampled data setfrom the same distribution. In this paper we are interested in a quantity whichrepresents the instability of an estimator when its value is compared to thevalue for an infinitesimal additive perturbation of the original data set; werefer to this as the "sensitivity" of an estimator. The resulting theory ofsensitivity is based on the Wasserstein geometry in the same way that theclassical theory of variance is based on the Fisher-Rao (equivalently,Hellinger) geometry, and this insight allows us to determine a collection ofresults which are analogous to the classical case: a Wasserstein-Cram\'er-Raolower bound for the sensitivity of any unbiased estimator, a characterizationof models in which there exist unbiased estimators achieving the lower boundexactly, and some concrete results that show that the Wasserstein projectionestimator achieves the lower bound asymptotically. We use these results totreat many statistical examples, sometimes revealing new optimality propertiesfor existing estimators and other times revealing entirely new estimators.</description><author>Nicolás García Trillos, Adam Quinn Jaffe, Bodhisattva Sen</author><pubDate>Mon, 10 Nov 2025 18:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07414v1</guid></item><item><title>DigiData: Training and Evaluating General-Purpose Mobile Control Agents</title><link>http://arxiv.org/abs/2511.07413v1</link><description>AI agents capable of controlling user interfaces have the potential totransform human interaction with digital devices. To accelerate thistransformation, two fundamental building blocks are essential: high-qualitydatasets that enable agents to achieve complex and human-relevant goals, androbust evaluation methods that allow researchers and practitioners to rapidlyenhance agent performance. In this paper, we introduce DigiData, a large-scale,high-quality, diverse, multi-modal dataset designed for training mobile controlagents. Unlike existing datasets, which derive goals from unstructuredinteractions, DigiData is meticulously constructed through comprehensiveexploration of app features, resulting in greater diversity and higher goalcomplexity. Additionally, we present DigiData-Bench, a benchmark for evaluatingmobile control agents on real-world complex tasks. We demonstrate that thecommonly used step-accuracy metric falls short in reliably assessing mobilecontrol agents and, to address this, we propose dynamic evaluation protocolsand AI-powered evaluations as rigorous alternatives for agent assessment. Ourcontributions aim to significantly advance the development of mobile controlagents, paving the way for more intuitive and effective human-deviceinteractions.</description><author>Yuxuan Sun, Manchen Wang, Shengyi Qian, William R. Wong, Eric Gan, Pierluca D'Oro, Alejandro Castillejo Munoz, Sneha Silwal, Pedro Matias, Nitin Kamra, Satwik Kottur, Nick Raines, Xuanyi Zhao, Joy Chen, Joseph Greer, Andrea Madotto, Allen Bolourchi, James Valori, Kevin Carlberg, Karl Ridgeway, Joseph Tighe</author><pubDate>Mon, 10 Nov 2025 18:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07413v1</guid></item><item><title>TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research</title><link>http://arxiv.org/abs/2511.07412v1</link><description>Developing embodied AI for intelligent surgical systems requires safe,controllable environments for continual learning and evaluation. However,safety regulations and operational constraints in operating rooms (ORs) limitembodied agents from freely perceiving and interacting in realistic settings.Digital twins provide high-fidelity, risk-free environments for exploration andtraining. How we may create photorealistic and dynamic digital representationsof ORs that capture relevant spatial, visual, and behavioral complexity remainsunclear. We introduce TwinOR, a framework for constructing photorealistic,dynamic digital twins of ORs for embodied AI research. The system reconstructsstatic geometry from pre-scan videos and continuously models human andequipment motion through multi-view perception of OR activities. The static anddynamic components are fused into an immersive 3D environment that supportscontrollable simulation and embodied exploration. The proposed frameworkreconstructs complete OR geometry with centimeter level accuracy whilepreserving dynamic interaction across surgical workflows, enabling realisticrenderings and a virtual playground for embodied AI systems. In ourexperiments, TwinOR simulates stereo and monocular sensor streams for geometryunderstanding and visual localization tasks. Models such as FoundationStereoand ORB-SLAM3 on TwinOR-synthesized data achieve performance within theirreported accuracy on real indoor datasets, demonstrating that TwinOR providessensor-level realism sufficient for perception and localization challenges. Byestablishing a real-to-sim pipeline for constructing dynamic, photorealisticdigital twins of OR environments, TwinOR enables the safe, scalable, anddata-efficient development and benchmarking of embodied AI, ultimatelyaccelerating the deployment of embodied AI from sim-to-real.</description><author>Han Zhang, Yiqing Shen, Roger D. Soberanis-Mukul, Ankita Ghosh, Hao Ding, Lalithkumar Seenivasan, Jose L. Porras, Zhekai Mao, Chenjia Li, Wenjie Xiao, Lonny Yarmus, Angela Christine Argento, Masaru Ishii, Mathias Unberath</author><pubDate>Mon, 10 Nov 2025 18:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07412v1</guid></item><item><title>Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective</title><link>http://arxiv.org/abs/2511.07410v1</link><description>Large Language Models (LLMs) and Vision Language Models (VLMs) have beenwidely used for embodied symbolic planning. Yet, how to effectively use thesemodels for closed-loop symbolic planning remains largely unexplored. Becausethey operate as black boxes, LLMs and VLMs can produce unpredictable or costlyerrors, making their use in high-level robotic planning especially challenging.In this work, we investigate how to use VLMs as closed-loop symbolic plannersfor robotic applications from a control-theoretic perspective. Concretely, westudy how the control horizon and warm-starting impact the performance of VLMsymbolic planners. We design and conduct controlled experiments to gaininsights that are broadly applicable to utilizing VLMs as closed-loop symbolicplanners, and we discuss recommendations that can help improve the performanceof VLM symbolic planners.</description><author>Hao Wang, Sathwik Karnik, Bea Lim, Somil Bansal</author><pubDate>Mon, 10 Nov 2025 18:56:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07410v1</guid></item><item><title>DIMO: Diverse 3D Motion Generation for Arbitrary Objects</title><link>http://arxiv.org/abs/2511.07409v1</link><description>We present DIMO, a generative approach capable of generating diverse 3Dmotions for arbitrary objects from a single image. The core idea of our work isto leverage the rich priors in well-trained video models to extract the commonmotion patterns and then embed them into a shared low-dimensional latent space.Specifically, we first generate multiple videos of the same object with diversemotions. We then embed each motion into a latent vector and train a sharedmotion decoder to learn the distribution of motions represented by a structuredand compact motion representation, i.e., neural key point trajectories. Thecanonical 3D Gaussians are then driven by these key points and fused to modelthe geometry and appearance. During inference time with learned latent space,we can instantly sample diverse 3D motions in a single-forward pass and supportseveral interesting applications including 3D motion interpolation andlanguage-guided motion generation. Our project page is available athttps://linzhanm.github.io/dimo.</description><author>Linzhan Mou, Jiahui Lei, Chen Wang, Lingjie Liu, Kostas Daniilidis</author><pubDate>Mon, 10 Nov 2025 18:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07409v1</guid></item><item><title>Entangled Schrödinger Bridge Matching</title><link>http://arxiv.org/abs/2511.07406v1</link><description>Simulating trajectories of multi-particle systems on complex energylandscapes is a central task in molecular dynamics (MD) and drug discovery, butremains challenging at scale due to computationally expensive and longsimulations. Previous approaches leverage techniques such as flow orSchr\"odinger bridge matching to implicitly learn joint trajectories throughdata snapshots. However, many systems, including biomolecular systems andheterogeneous cell populations, undergo dynamic interactions that evolve overtheir trajectory and cannot be captured through static snapshots. To close thisgap, we introduce Entangled Schr\"odinger Bridge Matching (EntangledSBM), aframework that learns the first- and second-order stochastic dynamics ofinteracting, multi-particle systems where the direction and magnitude of eachparticle's path depend dynamically on the paths of the other particles. Wedefine the Entangled Schr\"odinger Bridge (EntangledSB) problem as solving acoupled system of bias forces that entangle particle velocities. We show thatour framework accurately simulates heterogeneous cell populations underperturbations and rare transitions in high-dimensional biomolecular systems.</description><author>Sophia Tang, Yinuo Zhang, Pranam Chatterjee</author><pubDate>Mon, 10 Nov 2025 18:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07406v1</guid></item><item><title>SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations</title><link>http://arxiv.org/abs/2511.07405v1</link><description>We introduce SPOT (Stopping Points in Online Threads), the first annotatedcorpus translating the sociological concept of stopping point into areproducible NLP task. Stopping points are ordinary critical interventions thatpause or redirect online discussions through a range of forms (irony, subtledoubt or fragmentary arguments) that frameworks like counterspeech or socialcorrection often overlook. We operationalize this concept as a binaryclassification task and provide reliable annotation guidelines. The corpuscontains 43,305 manually annotated French Facebook comments linked to URLsflagged as false information by social media users, enriched with contextualmetadata (article, post, parent comment, page or group, and source). Webenchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMsunder various prompting strategies. Results show that fine-tuned encodersoutperform prompted LLMs in F1 score by more than 10 percentage points,confirming the importance of supervised learning for emerging non-Englishsocial media tasks. Incorporating contextual metadata further improves encodermodels F1 scores from 0.75 to 0.78. We release the anonymized dataset, alongwith the annotation guidelines and code in our code repository, to fostertransparency and reproducible research.</description><author>Manon Berriche, Célia Nouri, Chloé Clavel, Jean-Philippe Cointet</author><pubDate>Mon, 10 Nov 2025 18:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07405v1</guid></item><item><title>SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards</title><link>http://arxiv.org/abs/2511.07403v1</link><description>Multimodal large language models (MLLMs) have achieved remarkable progress invision-language tasks, but they continue to struggle with spatialunderstanding. Existing spatial MLLMs often rely on explicit 3D inputs orarchitecture-specific modifications, and remain constrained by large-scaledatasets or sparse supervision. To address these limitations, we introduceSpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatialgrounding with multi-step reasoning. The model simulates human-like spatialperception by constructing a scene graph of task-relevant objects and spatialrelations, and reasoning towards an answer via dense spatial rewards.SpatialThinker consists of two key contributions: (1) a data synthesis pipelinethat generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RLwith a multi-objective dense spatial reward enforcing spatial grounding.SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baselineon spatial understanding and real-world VQA benchmarks, nearly doubling thebase-model gain compared to sparse RL, and surpassing GPT-4o. These resultsshowcase the effectiveness of combining spatial supervision with reward-alignedreasoning in enabling robust 3D spatial understanding with limited data andadvancing MLLMs towards human-level visual reasoning.</description><author>Hunar Batra, Haoqin Tu, Hardy Chen, Yuanze Lin, Cihang Xie, Ronald Clark</author><pubDate>Mon, 10 Nov 2025 18:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07403v1</guid></item><item><title>StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation</title><link>http://arxiv.org/abs/2511.07399v1</link><description>Generative models are reshaping the live-streaming industry by redefining howcontent is created, styled, and delivered. Previous image-based streamingdiffusion models have powered efficient and creative live streaming productsbut have hit limits on temporal consistency due to the foundation ofimage-based designs. Recent advances in video diffusion have markedly improvedtemporal consistency and sampling efficiency for offline generation. However,offline generation systems primarily optimize throughput by batching largeworkloads. In contrast, live online streaming operates under strictservice-level objectives (SLOs): time-to-first-frame must be minimal, and everyframe must meet a per-frame deadline with low jitter. Besides, scalablemulti-GPU serving for real-time streams remains largely unresolved so far. Toaddress this, we present StreamDiffusionV2, a training-free pipeline forinteractive live streaming with video diffusion models. StreamDiffusionV2integrates an SLO-aware batching scheduler and a block scheduler, together witha sink-token--guided rolling KV cache, a motion-aware noise controller, andother system-level optimizations. Moreover, we introduce a scalable pipelineorchestration that parallelizes the diffusion process across denoising stepsand network layers, achieving near-linear FPS scaling without violating latencyguarantees. The system scales seamlessly across heterogeneous GPU environmentsand supports flexible denoising steps (e.g., 1--4), enabling bothultra-low-latency and higher-quality modes. Without TensorRT or quantization,StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPSwith a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on fourH100 GPUs, making state-of-the-art generative live streaming practical andaccessible--from individual creators to enterprise-scale platforms.</description><author>Tianrui Feng, Zhi Li, Shuo Yang, Haocheng Xi, Muyang Li, Xiuyu Li, Lvmin Zhang, Keting Yang, Kelly Peng, Song Han, Maneesh Agrawala, Kurt Keutzer, Akio Kodaira, Chenfeng Xu</author><pubDate>Mon, 10 Nov 2025 18:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07399v1</guid></item><item><title>Solving bilevel optimization via sequential minimax optimization</title><link>http://arxiv.org/abs/2511.07398v1</link><description>In this paper we propose a sequential minimax optimization (SMO) method forsolving a class of constrained bilevel optimization problems in which thelower-level part is a possibly nonsmooth convex optimization problem, while theupper-level part is a possibly nonconvex optimization problem. Specifically,SMO applies a first-order method to solve a sequence of minimax subproblems,which are obtained by employing a hybrid of modified augmented Lagrangian andpenalty schemes on the bilevel optimization problems. Under suitableassumptions, we establish an operation complexity of$O(\varepsilon^{-7}\log\varepsilon^{-1})$ and$O(\varepsilon^{-6}\log\varepsilon^{-1})$, measured in terms of fundamentaloperations, for SMO in finding an $\varepsilon$-KKT solution of the bileveloptimization problems with merely convex and strongly convex lower-levelobjective functions, respectively. The latter result improves the previousbest-known operation complexity by a factor of $\varepsilon^{-1}$. Preliminarynumerical results demonstrate significantly superior computational performancecompared to the recently developed first-order penalty method.</description><author>Zhaosong Lu, Sanyou Mei</author><pubDate>Mon, 10 Nov 2025 18:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07398v1</guid></item><item><title>ConvFill: Model Collaboration for Responsive Conversational Voice Agents</title><link>http://arxiv.org/abs/2511.07397v1</link><description>Deploying conversational voice agents with large language models faces acritical challenge: cloud-based foundation models provide deep reasoning anddomain knowledge but introduce latency that disrupts natural conversation,while on-device models respond immediately but lack sophistication. We proposeconversational infill, a task where a lightweight on-device model generatescontextually appropriate dialogue while seamlessly incorporating streamingknowledge from a powerful backend model. This approach decouples responselatency from model capability, enabling systems that feel responsive whileaccessing the full power of large-scale models. We present ConvFill, a 360Mparameter model trained on synthetic multi-domain conversations. Evaluationacross multiple backend models shows that conversational infill can besuccessfully learned, with ConvFill achieving accuracy improvements of 36-42%over standalone small models of the same size while consistently retainingsub-200ms response latencies. Our results demonstrate the promise of thisapproach for building on-device conversational agents that are both immediatelyresponsive and knowledgeable.</description><author>Vidya Srinivas, Zachary Englhardt, Maximus Powers, Shwetak Patel, Vikram Iyer</author><pubDate>Mon, 10 Nov 2025 18:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07397v1</guid></item><item><title>C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning</title><link>http://arxiv.org/abs/2511.07396v1</link><description>Large language models (LLMs) have achieved impressive results on complexreasoning tasks, but their high inference cost remains a major barrier toreal-world deployment. A promising solution is to use cascaded inference, wheresmall, cheap models handle easy queries, and only the hardest examples areescalated to more powerful models. However, existing cascade methods typicallyrely on supervised training with labeled data, offer no theoreticalgeneralization guarantees, and provide limited control over test-timecomputational cost. We introduce C3PO (Cost Controlled Cascaded PredictionOptimization), a self-supervised framework for optimizing LLM cascades underprobabilistic cost constraints. By focusing on minimizing regret with respectto the most powerful model (MPM), C3PO avoids the need for labeled data byconstructing a cascade using only unlabeled model outputs. It leveragesconformal prediction to bound the probability that inference cost exceeds auser-specified budget. We provide theoretical guarantees on both cost controland generalization error, and show that our optimization procedure is effectiveeven with small calibration sets. Empirically, C3PO achieves state-of-the-artperformance across a diverse set of reasoning benchmarks including GSM8K,MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselinesin both accuracy and cost-efficiency. Our results demonstrate that principled,label-free cascade optimization can enable scalable LLM deployment.</description><author>Antonios Valkanas, Soumyasundar Pal, Pavel Rumiantsev, Yingxue Zhang, Mark Coates</author><pubDate>Mon, 10 Nov 2025 18:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07396v1</guid></item><item><title>Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction</title><link>http://arxiv.org/abs/2511.07392v1</link><description>In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged inthe procedure, making it difficult to access and manipulate multimodal patientdata without interruption. We propose a voice-directed Surgical AgentOrchestrator Platform (SAOP) built on a hierarchical multi-agent framework,consisting of an orchestration agent and three task-specific agents driven byLarge Language Models (LLMs). These LLM-based agents autonomously plan, refine,validate, and reason to map voice commands into specific tasks such asretrieving clinical information, manipulating CT scans, or navigating 3Danatomical models on the surgical video. We also introduce a Multi-levelOrchestration Evaluation Metric (MOEM) to comprehensively assess theperformance and robustness from command-level and category-level perspectives.The SAOP achieves high accuracy and success rates across 240 voice commands,while LLM-based agents improve robustness against speech recognition errors anddiverse or ambiguous free-form commands, demonstrating strong potential tosupport minimally invasive da Vinci robotic surgery.</description><author>Hyeryun Park, Byung Mo Gu, Jun Hee Lee, Byeong Hyeon Choi, Sekeun Kim, Hyun Koo Kim, Kyungsang Kim</author><pubDate>Mon, 10 Nov 2025 18:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07392v1</guid></item><item><title>A Diffusion Model to Shrink Proteins While Maintaining Their Function</title><link>http://arxiv.org/abs/2511.07390v1</link><description>Many proteins useful in modern medicine or bioengineering are challenging tomake in the lab, fuse with other proteins in cells, or deliver to tissues inthe body, because their sequences are too long. Shortening these sequencestypically involves costly, time-consuming experimental campaigns. Ideally, wecould instead use modern models of massive databases of sequences from natureto learn how to propose shrunken proteins that resemble sequences found innature. Unfortunately, these models struggle to efficiently search thecombinatorial space of all deletions, and are not trained with inductive biasesto learn how to delete. To address this gap, we propose SCISOR, a noveldiscrete diffusion model that deletes letters from sequences to generateprotein samples that resemble those found in nature. To do so, SCISOR trains ade-noiser to reverse a forward noising process that adds random insertions tonatural sequences. As a generative model, SCISOR fits evolutionary sequencedata competitively with previous large models. In evaluation, SCISOR achievesstate-of-the-art predictions of the functional effects of deletions onProteinGym. Finally, we use the SCISOR de-noiser to shrink long proteinsequences, and show that its suggested deletions result in significantly morerealistic proteins and more often preserve functional motifs than previousmodels of evolutionary sequences.</description><author>Ethan Baron, Alan N. Amin, Ruben Weitzman, Debora Marks, Andrew Gordon Wilson</author><pubDate>Mon, 10 Nov 2025 18:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07390v1</guid></item><item><title>Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence</title><link>http://arxiv.org/abs/2511.07384v1</link><description>Recent advances in depth-recurrent language models show that recurrence candecouple train-time compute and parameter count from test-time compute. In thiswork, we study how to convert existing pretrained non-recurrent language modelsinto depth-recurrent models. We find that using a curriculum of recurrences toincrease the effective depth of the model over the course of training preservesperformance while reducing total computational cost. In our experiments, onmathematics, we observe that converting pretrained models to recurrent onesresults in better performance at a given compute budget than simplypost-training the original non-recurrent language model.</description><author>Sean McLeish, Ang Li, John Kirchenbauer, Dayal Singh Kalra, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Jonas Geiping, Tom Goldstein, Micah Goldblum</author><pubDate>Mon, 10 Nov 2025 18:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07384v1</guid></item><item><title>Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation</title><link>http://arxiv.org/abs/2511.07382v1</link><description>Large Language Models (LLMs) have advanced the automated generation of codefrom natural language prompts. However, low-resource languages (LRLs) likeBangla remain underrepresented due to the limited availability ofinstruction-to-code datasets and evaluation benchmarks. To address this, theBLP Workshop at IJCNLP-AACL 2025 introduced a shared task on "Code Generationin Bangla". In this work, we propose a method that combines instructionprompting with a test-driven, feedback-guided iterative refinement processusing a fine-tuned Qwen2.5-14B model. The model generates code from Banglainstructions, tests it against unit tests, and iteratively refines any failingoutputs through three evaluation passes, using test feedback to guide eachstep. This approach helped our team "Retriv" to secure 2nd place in the sharedtask with a Pass@1 score of 0.934. The analysis highlights challenges in Banglainstruction understanding and Python code generation, emphasizing the need fortargeted methods in LRLs. We made experimental scripts publicly available forthe community.</description><author>K M Nafi Asib, Sourav Saha, Mohammed Moshiul Hoque</author><pubDate>Mon, 10 Nov 2025 18:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07382v1</guid></item><item><title>Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains</title><link>http://arxiv.org/abs/2511.07380v1</link><description>Large language models (LLMs) have achieved remarkable success acrosswidespread tasks, yet their application in low-resource domains remains asignificant challenge due to data scarcity and the high risk of overfitting.While in-domain data is limited, there exist vast amounts of similargeneral-domain data, and our initial findings reveal that they couldpotentially serve as auxiliary supervision for domain enhancement. Thisobservation leads us to our central research question: \textbf{\textit{how toeffectively select the most valuable auxiliary data to maximize domain-specificperformance}}, particularly when traditional methods are inapplicable due to alack of large in-domain data pools or validation sets. To address this, wepropose \textbf{NTK-Selector}, a principled and efficient framework forselecting general-domain auxiliary data to enhance domain-specific performancevia neural tangent kernels (NTK). Our method tackles two challenges of directlyapplying NTK to LLMs, theoretical assumptions and prohibitive computationalcost, by empirically demonstrating a stable NTK-like behavior in LLMs duringLoRA fine-tuning and proposing a Jacobian-free approximation method. Extensiveexperiments across four low-resource domains (medical, financial, legal, andpsychological) demonstrate that NTK-Selector consistently improves downstreamperformance. Specifically, fine-tuning on 1,000 in-domain samples alone onlyyielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. Incontrast, enriching with 9,000 auxiliary samples selected by NTK-Selector ledto substantial \textbf{gains of +8.7 and +5.1 points}, which corresponds to a\textbf{10.9x and 5.7x improvement} over the domain-only setting.</description><author>Pingjie Wang, Hongcheng Liu, Yusheng Liao, Ziqing Fan, Yaxin Du, Shuo Tang, Yanfeng Wang, Yu Wang</author><pubDate>Mon, 10 Nov 2025 18:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07380v1</guid></item><item><title>LoReTTA: A Low Resource Framework To Poison Continuous Time Dynamic Graphs</title><link>http://arxiv.org/abs/2511.07379v1</link><description>Temporal Graph Neural Networks (TGNNs) are increasingly used in high-stakesdomains, such as financial forecasting, recommendation systems, and frauddetection. However, their susceptibility to poisoning attacks poses a criticalsecurity risk. We introduce LoReTTA (Low Resource Two-phase Temporal Attack), anovel adversarial framework on Continuous-Time Dynamic Graphs, which degradesTGNN performance by an average of 29.47% across 4 widely benchmark datasets and4 State-of-the-Art (SotA) models. LoReTTA operates through a two-stageapproach: (1) sparsify the graph by removing high-impact edges using any of the16 tested temporal importance metrics, (2) strategically replace removed edgeswith adversarial negatives via LoReTTA's novel degree-preserving negativesampling algorithm. Our plug-and-play design eliminates the need for expensivesurrogate models while adhering to realistic unnoticeability constraints.LoReTTA degrades performance by upto 42.0% on MOOC, 31.5% on Wikipedia, 28.8%on UCI, and 15.6% on Enron. LoReTTA outperforms 11 attack baselines, remainsundetectable to 4 leading anomaly detection systems, and is robust to 4 SotAadversarial defense training methods, establishing its effectiveness,unnoticeability, and robustness.</description><author>Himanshu Pal, Venkata Sai Pranav Bachina, Ankit Gangwal, Charu Sharma</author><pubDate>Mon, 10 Nov 2025 18:41:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07379v1</guid></item><item><title>Transformers Provably Learn Chain-of-Thought Reasoning with Length Generalization</title><link>http://arxiv.org/abs/2511.07378v1</link><description>The ability to reason lies at the core of artificial intelligence (AI), andchallenging problems usually call for deeper and longer reasoning to tackle. Acrucial question about AI reasoning is whether models can extrapolate learnedreasoning patterns to solve harder tasks with longer chain-of-thought (CoT). Inthis work, we present a theoretical analysis of transformers learning onsynthetic state-tracking tasks with gradient descent. We mathematically provehow the algebraic structure of state-tracking problems governs the degree ofextrapolation of the learned CoT. Specifically, our theory characterizes thelength generalization of transformers through the mechanism of attentionconcentration, linking the retrieval robustness of the attention layer to thestate-tracking task structure of long-context reasoning. Moreover, fortransformers with limited reasoning length, we prove that a recursiveself-training scheme can progressively extend the range of solvable problemlengths. To our knowledge, we provide the first optimization guarantee thatconstant-depth transformers provably learn $\mathsf{NC}^1$-complete problemswith CoT, significantly going beyond prior art confined in $\mathsf{TC}^0$,unless the widely held conjecture $\mathsf{TC}^0 \neq \mathsf{NC}^1$ fails.Finally, we present a broad set of experiments supporting our theoreticalresults, confirming the length generalization behaviors and the mechanism ofattention concentration.</description><author>Yu Huang, Zixin Wen, Aarti Singh, Yuejie Chi, Yuxin Chen</author><pubDate>Mon, 10 Nov 2025 18:40:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07378v1</guid></item><item><title>Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion</title><link>http://arxiv.org/abs/2511.07377v1</link><description>LiDAR super-resolution addresses the challenge of achieving high-quality 3Dperception from cost-effective, low-resolution sensors. While recenttransformer-based approaches like TULIP show promise, they remain limited tospatial-domain processing with restricted receptive fields. We introduce FLASH(Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), anovel framework that overcomes these limitations through dual-domainprocessing. FLASH integrates two key innovations: (i) Frequency-Aware WindowAttention that combines local spatial attention with global frequency-domainanalysis via FFT, capturing both fine-grained geometry and periodic scanningpatterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion thatreplaces conventional skip connections with learned position-specific featureaggregation, enhanced by CBAM attention for dynamic feature selection.Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-artperformance across all evaluation metrics, surpassing even uncertainty-enhancedbaselines that require multiple forward passes. Notably, FLASH outperformsTULIP with Monte Carlo Dropout while maintaining single-pass efficiency, whichenables real-time deployment. The consistent superiority across all distanceranges validates that our dual-domain approach effectively handles uncertaintythrough architectural design rather than computationally expensive stochasticinference, making it practical for autonomous systems.</description><author>June Moh Goo, Zichao Zeng, Jan Boehm</author><pubDate>Mon, 10 Nov 2025 18:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07377v1</guid></item><item><title>Explaining Human Choice Probabilities with Simple Vector Representations</title><link>http://arxiv.org/abs/2511.03643v2</link><description>When people pursue rewards in stochastic environments, they often match theirchoice frequencies to the observed target frequencies, even when this policy isdemonstrably sub-optimal. We used a ``hide and seek'' task to evaluate thisbehavior under conditions where pursuit (seeking) could be toggled to avoidance(hiding), while leaving the probability distribution fixed, or varyingcomplexity by changing the number of possible choices. We developed a model forparticipant choice built from choice frequency histograms treated as vectors.We posited the existence of a probability antimatching strategy for avoidance(hiding) rounds, and formalized this as a vector reflection of probabilitymatching. We found that only two basis policies: matching/antimatching andmaximizing/minimizing were sufficient to account for participant choices acrossa range of room numbers and opponent probability distributions. This schemarequires only that people have the ability to remember the relative frequencyof the different outcomes. With this knowledge simple operations can constructthe maximizing and minimizing policies as well as matching and antimatchingstrategies. A mixture of these two policies captures human choice patterns in astochastic environment.</description><author>Peter DiBerardino, Britt Anderson</author><pubDate>Mon, 10 Nov 2025 18:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.03643v2</guid></item><item><title>Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection</title><link>http://arxiv.org/abs/2505.13979v2</link><description>Multimodal models play a key role in empathy detection, but their performancecan suffer when modalities provide conflicting cues. To understand thesefailures, we examine cases where unimodal and multimodal predictions diverge.Using fine-tuned models for text, audio, and video, along with a gated fusionmodel, we find that such disagreements often reflect underlying ambiguity, asevidenced by annotator uncertainty. Our analysis shows that dominant signals inone modality can mislead fusion when unsupported by others. We also observethat humans, like models, do not consistently benefit from multimodal input.These insights position disagreement as a useful diagnostic signal foridentifying challenging examples and improving empathy system robustness.</description><author>Maya Srikanth, Run Chen, Julia Hirschberg</author><pubDate>Mon, 10 Nov 2025 18:36:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.13979v2</guid></item><item><title>Provable Benefit of Curriculum in Transformer Tree-Reasoning Post-Training</title><link>http://arxiv.org/abs/2511.07372v1</link><description>Recent curriculum techniques in the post-training stage of LLMs have beenwidely observed to outperform non-curriculum approaches in enhancing reasoningperformance, yet a principled understanding of why and to what extent they workremains elusive. To address this gap, we develop a theoretical frameworkgrounded in the intuition that progressively learning through manageable stepsis more efficient than directly tackling a hard reasoning task, provided eachstage stays within the model's effective competence. Under mild complexityconditions linking consecutive curriculum stages, we show that curriculumpost-training avoids the exponential complexity bottleneck. To substantiate this result, drawing insights from the Chain-of-Thoughts(CoTs) solving mathematical problems such as Countdown and parity, we model CoTgeneration as a states-conditioned autoregressive reasoning tree, define auniform-branching base model to capture pretrained behavior, and formalizecurriculum stages as either depth-increasing (longer reasoning chains) orhint-decreasing (shorter prefixes) subtasks. Our analysis shows that, underoutcome-only reward signals, reinforcement learning finetuning achieves highaccuracy with polynomial sample complexity, whereas direct learning suffersfrom an exponential bottleneck. We further establish analogous guarantees fortest-time scaling, where curriculum-aware querying reduces both reward oraclecalls and sampling cost from exponential to polynomial order.</description><author>Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Hau-San Wong, Qingfu Zhang, Taiji Suzuki</author><pubDate>Mon, 10 Nov 2025 18:29:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07372v1</guid></item><item><title>Consistency Is Not Always Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning</title><link>http://arxiv.org/abs/2511.07368v1</link><description>Foundation models exhibit broad knowledge but limited task-specificreasoning, motivating post-training strategies such as RLVR and inferencescaling with outcome or process reward models (ORM/PRM). While recent workhighlights the role of exploration and entropy stability in improving pass@K,empirical evidence points to a paradox: RLVR and ORM/PRM typically reinforceexisting tree-like reasoning paths rather than expanding the reasoning scope,raising the question of why exploration helps at all if no new patterns emerge. To reconcile this paradox, we adopt the perspective of Kim et al. (2025),viewing easy (e.g., simplifying a fraction) versus hard (e.g., discovering asymmetry) reasoning steps as low- versus high-probability Markov transitions,and formalize post-training dynamics through Multi-task Tree-structured MarkovChains (TMC). In this tractable model, pretraining corresponds to treeexpansion, while post-training corresponds to chain-of-thought reweighting. Weshow that several phenomena recently observed in empirical studies arisenaturally in this setting: (1) RLVR induces a squeezing effect, reducingreasoning entropy and forgetting some correct paths; (2) population rewards ofORM/PRM encourage consistency rather than accuracy, thereby favoring commonpatterns; and (3) certain rare, high-uncertainty reasoning paths by the basemodel are responsible for solving hard problem instances. Together, these explain why exploration -- even when confined to the basemodel's reasoning scope -- remains essential: it preserves access to rare butcrucial reasoning traces needed for difficult cases, which are squeezed out byRLVR or unfavored by inference scaling. Building on this, we further show thatexploration strategies such as rejecting easy instances and KL regularizationhelp preserve rare reasoning traces. Empirical simulations corroborate ourtheoretical results.</description><author>Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Bo Xue, Qingfu Zhang, Hau-San Wong, Taiji Suzuki</author><pubDate>Mon, 10 Nov 2025 18:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07368v1</guid></item><item><title>Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics</title><link>http://arxiv.org/abs/2510.06367v2</link><description>Neural ODEs are a widely used, powerful machine learning technique inparticular for physics. However, not every solution is physical in that it isan Euler-Lagrange equation. We present Helmholtz metrics to quantify thisresemblance for a given ODE and demonstrate their capabilities on severalfundamental systems with noise. We combine them with a second order neural ODEto form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equationsin a direct fashion and with zero additional inference cost. We demonstratethat, using only positional data, they can distinguish Lagrangian andnon-Lagrangian systems and improve the neural ODE solutions.</description><author>Luca Wolf, Tobias Buck, Bjoern Malte Schaefer</author><pubDate>Mon, 10 Nov 2025 18:25:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.06367v2</guid></item><item><title>Machine-Learning Accelerated Calculations of Reduced Density Matrices</title><link>http://arxiv.org/abs/2511.07367v1</link><description>$n$-particle reduced density matrices ($n$-RDMs) play a central role inunderstanding correlated phases of matter. Yet the calculation of $n$-RDMs isoften computationally inefficient for strongly-correlated states, particularlywhen the system sizes are large. In this work, we propose to use neural network(NN) architectures to accelerate the calculation of, and even predict, the$n$-RDMs for large-size systems. The underlying intuition is that $n$-RDMs areoften smooth functions over the Brillouin zone (BZ) (certainly true for gappedstates) and are thus interpolable, allowing NNs trained on small-size $n$-RDMsto predict large-size ones. Building on this intuition, we devise two NNs: (i)a self-attention NN that maps random RDMs to physical ones, and (ii) aSinusoidal Representation Network (SIREN) that directly maps momentum-spacecoordinates to RDM values. We test the NNs in three 2D models: the pair-paircorrelation functions of the Richardson model of superconductivity, thetranslationally-invariant 1-RDM in a four-band model with short-rangerepulsion, and the translation-breaking 1-RDM in the half-filled Hubbard model.We find that a SIREN trained on a $6\times 6$ momentum mesh can predict the$18\times 18$ pair-pair correlation function with a relative accuracy of$0.839$. The NNs trained on $6\times 6 \sim 8\times 8$ meshes can providehigh-quality initial guesses for $50\times 50$ translation-invariantHartree-Fock (HF) and $30\times 30$ fully translation-breaking-allowed HF,reducing the number of iterations required for convergence by up to $91.63\%$and $92.78\%$, respectively, compared to random initializations. Our resultsillustrate the potential of using NN-based methods for interpolable $n$-RDMs,which might open a new avenue for future research on strongly correlatedphases.</description><author>Awwab A. Azam, Lexu Zhao, Jiabin Yu</author><pubDate>Mon, 10 Nov 2025 18:23:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07367v1</guid></item><item><title>UAV-Assisted Resilience in 6G and Beyond Network Energy Saving: A Multi-Agent DRL Approach</title><link>http://arxiv.org/abs/2511.07366v1</link><description>This paper investigates the unmanned aerial vehicle (UAV)-assisted resilienceperspective in the 6G network energy saving (NES) scenario. More specifically,we consider multiple ground base stations (GBSs) and each GBS has threedifferent sectors/cells in the terrestrial networks, and multiple cells areturned off due to NES or incidents, e.g., disasters, hardware failures, oroutages. To address this, we propose a Multi-Agent Deep Deterministic PolicyGradient (MADDPG) framework to enable UAV-assisted communication by jointlyoptimizing UAV trajectories, transmission power, and user-UAV association undera sleeping ground base station (GBS) strategy. This framework aims to ensurethe resilience of active users in the network and the long-term operability ofUAVs. Specifically, it maximizes service coverage for users during poweroutages or NES zones, while minimizing the energy consumption of UAVs.Simulation results demonstrate that the proposed MADDPG policy consistentlyachieves high coverage ratio across different testing episodes, outperformingother baselines. Moreover, the MADDPG framework attains the lowest total energyconsumption, with a reduction of approximately 24\% compared to theconventional all GBS ON configuration, while maintaining a comparable userservice rate. These results confirm the effectiveness of the proposed approachin achieving a superior trade-off between energy efficiency and serviceperformance, supporting the development of sustainable and resilientUAV-assisted cellular networks.</description><author>Dao Lan Vy Dinh, Anh Nguyen Thi Mai, Hung Tran, Giang Quynh Le Vu, Tu Dac Ho, Zhenni Pan, Vo Nhan Van, Symeon Chatzinotas, Dinh-Hieu Tran</author><pubDate>Mon, 10 Nov 2025 18:23:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07366v1</guid></item><item><title>Private Sketches for Linear Regression</title><link>http://arxiv.org/abs/2511.07365v1</link><description>Linear regression is frequently applied in a variety of domains. In order toimprove the efficiency of these methods, various methods have been developedthat compute summaries or \emph{sketches} of the datasets. Certain domains,however, contain sensitive data which necessitates that the application ofthese statistical methods does not reveal private information. Differentiallyprivate (DP) linear regression methods have been developed for mitigating thisproblem. These techniques typically involve estimating a noisy version of theparameter vector. Instead, we propose releasing private sketches of thedatasets. We present differentially private sketches for the problems of leastsquares regression, as well as least absolute deviations regression. Theavailability of these private sketches facilitates the application of commonlyavailable solvers for regression, without the risk of privacy leakage.</description><author>Shrutimoy Das, Debanuj Nayak, Anirban Dasgupta</author><pubDate>Mon, 10 Nov 2025 18:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07365v1</guid></item><item><title>Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection</title><link>http://arxiv.org/abs/2511.07364v1</link><description>Reliability and failure detection of large language models (LLMs) is criticalfor their deployment in high-stakes, multi-step reasoning tasks. Prior workexplores confidence estimation for self-evaluating LLM-scorer systems, withconfidence scorers estimating the likelihood of errors in LLM responses.However, most methods focus on single-step outputs and overlook the challengesof multi-step reasoning. In this work, we extend self-evaluation techniques tomulti-step tasks, testing two intuitive approaches: holistic scoring andstep-by-step scoring. Using two multi-step benchmark datasets, we show thatstepwise evaluation generally outperforms holistic scoring in detectingpotential errors, with up to 15% relative increase in AUC-ROC. Our findingsdemonstrate that self-evaluating LLM systems provide meaningful confidenceestimates in complex reasoning, improving their trustworthiness and providing apractical framework for failure detection.</description><author>Vaibhav Mavi, Shubh Jaroria, Weiqi Sun</author><pubDate>Mon, 10 Nov 2025 18:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07364v1</guid></item><item><title>Inference-Time Scaling of Diffusion Models for Infrared Data Generation</title><link>http://arxiv.org/abs/2511.07362v1</link><description>Infrared imagery enables temperature-based scene understanding using passivesensors, particularly under conditions of low visibility where traditional RGBimaging fails. Yet, developing downstream vision models for infraredapplications is hindered by the scarcity of high-quality annotated data, due tothe specialized expertise required for infrared annotation. While syntheticinfrared image generation has the potential to accelerate model development byproviding large-scale, diverse training data, training foundation-levelgenerative diffusion models in the infrared domain has remained elusive due tolimited datasets. In light of such data constraints, we explore aninference-time scaling approach using a domain-adapted CLIP-based verifier forenhanced infrared image generation quality. We adapt FLUX.1-dev, astate-of-the-art text-to-image diffusion model, to the infrared domain byfinetuning it on a small sample of infrared images using parameter-efficienttechniques. The trained verifier is then employed during inference to guide thediffusion sampling process toward higher quality infrared generations thatbetter align with input text prompts. Empirically, we find that our approachleads to consistent improvements in generation quality, reducing FID scores onthe KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% comparedto unguided baseline samples. Our results suggest that inference-time guidanceoffers a promising direction for bridging the domain gap in low-data infraredsettings.</description><author>Kai A. Horstmann, Maxim Clouser, Kia Khezeli</author><pubDate>Mon, 10 Nov 2025 18:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07362v1</guid></item><item><title>Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning</title><link>http://arxiv.org/abs/2508.00024v2</link><description>Quantum Support Vector Machines face scalability challenges due tohigh-dimensional quantum states and hardware limitations. We propose anembedding-aware quantum-classical pipeline combining class-balanced k-meansdistillation with pretrained Vision Transformer embeddings. Our key finding:ViT embeddings uniquely enable quantum advantage, achieving up to 8.02%accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,while CNN features show performance degradation. Using 16-qubit tensor networksimulation via cuTensorNet, we provide the first systematic evidence thatquantum kernel advantage depends critically on embedding choice, revealingfundamental synergy between transformer attention and quantum feature spaces.This provides a practical pathway for scalable quantum machine learning thatleverages modern neural architectures.</description><author>Sebastián Andrés Cajas Ordóñez, Luis Fernando Torres Torres, Mario Bifulco, Carlos Andrés Durán, Cristian Bosch, Ricardo Simón Carbajo</author><pubDate>Mon, 10 Nov 2025 18:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.00024v2</guid></item><item><title>Sensitivity Analysis for Climate Science with Generative Flow Models</title><link>http://arxiv.org/abs/2511.00663v2</link><description>Sensitivity analysis is a cornerstone of climate science, essential forunderstanding phenomena ranging from storm intensity to long-term climatefeedbacks. However, computing these sensitivities using traditional physicalmodels is often prohibitively expensive in terms of both computation anddevelopment time. While modern AI-based generative models are orders ofmagnitude faster to evaluate, computing sensitivities with them remains asignificant bottleneck. This work addresses this challenge by applying theadjoint state method for calculating gradients in generative flow models. Weapply this method to the cBottle generative model, trained on ERA5 and ICONdata, to perform sensitivity analysis of any atmospheric variable with respectto sea surface temperatures. We quantitatively validate the computedsensitivities against the model's own outputs. Our results provide initialevidence that this approach can produce reliable gradients, reducing thecomputational cost of sensitivity analysis from weeks on a supercomputer with aphysical model to hours on a GPU, thereby simplifying a critical workflow inclimate science. The code can be found athttps://github.com/Kwartzl8/cbottle_adjoint_sensitivity.</description><author>Alex Dobra, Jakiw Pidstrigach, Tim Reichelt, Christian Schroeder de Witt, Philip Torr, Philip Stier</author><pubDate>Mon, 10 Nov 2025 18:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.00663v2</guid></item><item><title>Graph-Conditional Flow Matching for Relational Data Generation</title><link>http://arxiv.org/abs/2505.15668v2</link><description>Data synthesis is gaining momentum as a privacy-enhancing technology. Whilesingle-table tabular data generation has seen considerable progress, currentmethods for multi-table data often lack the flexibility and expressivenessneeded to capture complex relational structures. In particular, they strugglewith long-range dependencies and complex foreign-key relationships, such astables with multiple parent tables or multiple types of links between the samepair of tables. We propose a generative model for relational data thatgenerates the content of a relational dataset given the graph formed by theforeign-key relationships. We do this by learning a deep generative model ofthe content of the whole relational database by flow matching, where the neuralnetwork trained to denoise records leverages a graph neural network to obtaininformation from connected records. Our method is flexible, as it can supportrelational datasets with complex structures, and expressive, as the generationof each record can be influenced by any other record within the same connectedcomponent. We evaluate our method on several benchmark datasets and show thatit achieves state-of-the-art performance in terms of synthetic data fidelity.</description><author>Davide Scassola, Sebastiano Saccani, Luca Bortolussi</author><pubDate>Mon, 10 Nov 2025 18:05:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.15668v2</guid></item><item><title>NOWS: Neural Operator Warm Starts for Accelerating Iterative Solvers</title><link>http://arxiv.org/abs/2511.02481v3</link><description>Partial differential equations (PDEs) underpin quantitative descriptionsacross the physical sciences and engineering, yet high-fidelity simulationremains a major computational bottleneck for many-query, real-time, and designtasks. Data-driven surrogates can be strikingly fast but are often unreliablewhen applied outside their training distribution. Here we introduce NeuralOperator Warm Starts (NOWS), a hybrid strategy that harnesses learned solutionoperators to accelerate classical iterative solvers by producing high-qualityinitial guesses for Krylov methods such as conjugate gradient and GMRES. NOWSleaves existing discretizations and solver infrastructures intact, integratingseamlessly with finite-difference, finite-element, isogeometric analysis,finite volume method, etc. Across our benchmarks, the learned initializationconsistently reduces iteration counts and end-to-end runtime, resulting in areduction of the computational time of up to 90 %, while preserving thestability and convergence guarantees of the underlying numerical algorithms. Bycombining the rapid inference of neural operators with the rigor of traditionalsolvers, NOWS provides a practical and trustworthy approach to acceleratehigh-fidelity PDE simulations.</description><author>Mohammad Sadegh Eshaghi, Cosmin Anitescu, Navid Valizadeh, Yizheng Wang, Xiaoying Zhuang, Timon Rabczuk</author><pubDate>Mon, 10 Nov 2025 17:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.02481v3</guid></item><item><title>Estimation of aboveground biomass in a tropical dry forest: An intercomparison of airborne, unmanned, and space laser scanning</title><link>http://arxiv.org/abs/2510.27408v4</link><description>According to the Paris Climate Change Agreement, all nations are required tosubmit reports on their greenhouse gas emissions and absorption every two yearsby 2024. Consequently, forests play a crucial role in reducing carbonemissions, which is essential for meeting these obligations. Recognizing thesignificance of forest conservation in the global battle against climatechange, Article 5 of the Paris Agreement emphasizes the need for high-qualityforest data. This study focuses on enhancing methods for mapping abovegroundbiomass in tropical dry forests. Tropical dry forests are considered one of theleast understood tropical forest environments; therefore, there is a need foraccurate approaches to estimate carbon pools. We employ a comparative analysisof AGB estimates, utilizing different discrete and full-waveform laser scanningdatasets in conjunction with Ordinary Least Squares and Bayesian approachesSVM. Airborne Laser Scanning, Unmanned Laser Scanning, and Space Laser Scanningwere used as independent variables for extracting forest metrics. Variableselection, SVM regression tuning, and cross-validation via a machine-learningapproach were applied to account for overfitting and underfitting. The resultsindicate that six key variables primarily related to tree height:Elev\.minimum, Elev\.L3, lev\.MAD\.mode, Elev\.mode, Elev\.MAD\.median, andElev\.skewness, are important for AGB estimation using ALSD and ULSD, whileLeaf Area Index, canopy coverage and height, terrain elevation, andfull-waveform signal energy emerged as the most vital variables. AGB valuesestimated from ten permanent tropical dry forest plots in Costa Rica Guanacasteprovince ranged from 26.02 Mg/ha to 175.43 Mg/ha. The SVM regressionsdemonstrated a 17.89 error across all laser scanning systems, with SLSF Wexhibiting the lowest error 17.07 in estimating total biomass per plot.</description><author>Nelson Mattié, Arturo Sanchez-Azofeifa, Pablo Crespo-Peremarch, Juan-Ygnacio López-Hernández</author><pubDate>Mon, 10 Nov 2025 17:53:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.27408v4</guid></item><item><title>CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training</title><link>http://arxiv.org/abs/2510.18784v2</link><description>Despite significant work on low-bit quantization-aware training (QAT), thereis still an accuracy gap between such techniques and native training. Toaddress this, we introduce CAGE (Curvature-Aware Gradient Estimation), a newQAT method that augments the straight-through estimator (STE) gradient with acurvature-aware correction designed to counteract the loss increase induced byquantization. CAGE is derived from a multi-objective view of QAT that balancesloss minimization with the quantization constraints, yielding a principledcorrection term that depends on local curvature information. On the theoreticalside, we introduce the notion of Pareto-optimal solutions for quantizedoptimization, and establish that CAGE yields strong convergence guarantees inthe smooth non-convex setting. In terms of implementation, our approach isoptimizer-agnostic, but we provide a highly-efficient implementation thatleverages Adam statistics. CAGE significantly improves upon the priorstate-of-the-art methods in terms of accuracy, for similar computational cost:for QAT fine-tuning, it halves the compression accuracy loss relative to theprior best method, while for QAT pre-training of Llama models, its accuracy for3-bit weights-and-activations (W3A3) matches the accuracy achieved at 4-bits(W4A4) with the prior best method. The official implementation can be foundover https://github.com/IST-DASLab/CAGE .</description><author>Soroush Tabesh, Mher Safaryan, Andrei Panferov, Alexandra Volkova, Dan Alistarh</author><pubDate>Mon, 10 Nov 2025 17:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.18784v2</guid></item><item><title>Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients</title><link>http://arxiv.org/abs/2511.07347v1</link><description>Neural operators have emerged as powerful tools for learning solutionoperators of partial differential equations (PDEs). However, standard spectralmethods based on Fourier transforms struggle with problems involvingdiscontinuous coefficients due to the Gibbs phenomenon and poor representationof sharp interfaces. We introduce the Walsh-Hadamard Neural Operator (WHNO),which leverages Walsh-Hadamard transforms-a spectral basis of rectangular wavefunctions naturally suited for piecewise constant fields-combined withlearnable spectral weights that transform low-sequency Walsh coefficients tocapture global dependencies efficiently. We validate WHNO on three problems:steady-state Darcy flow (preliminary validation), heat conduction withdiscontinuous thermal conductivity, and the 2D Burgers equation withdiscontinuous initial conditions. In controlled comparisons with Fourier NeuralOperators (FNO) under identical conditions, WHNO demonstrates superior accuracywith better preservation of sharp solution features at material interfaces.Critically, we discover that weighted ensemble combinations of WHNO and FNOachieve substantial improvements over either model alone: for both heatconduction and Burgers equation, optimal ensembles reduce mean squared error by35-40 percent and maximum error by up to 25 percent compared to individualmodels. This demonstrates that Walsh-Hadamard and Fourier representationscapture complementary aspects of discontinuous PDE solutions, with WHNOexcelling at sharp interfaces while FNO captures smooth features effectively.</description><author>Giorrgio M. Cavallazzi, Miguel Perex Cuadrado, Alfredo Pinelli</author><pubDate>Mon, 10 Nov 2025 17:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07347v1</guid></item><item><title>TNT: Improving Chunkwise Training for Test-Time Memorization</title><link>http://arxiv.org/abs/2511.07343v1</link><description>Recurrent neural networks (RNNs) with deep test-time memorization modules,such as Titans and TTT, represent a promising, linearly-scaling paradigmdistinct from Transformers. While these expressive models do not yet match thepeak performance of state-of-the-art Transformers, their potential has beenlargely untapped due to prohibitively slow training and low hardwareutilization. Existing parallelization methods force a fundamental conflictgoverned by the chunksize hyperparameter: large chunks boost speed but degradeperformance, necessitating a fixed, suboptimal compromise. To solve thischallenge, we introduce TNT, a novel training paradigm that decouples trainingefficiency from inference performance through a two-stage process. Stage one isan efficiency-focused pre-training phase utilizing a hierarchical memory. Aglobal module processes large, hardware-friendly chunks for long-range context,while multiple parallel local modules handle fine-grained details. Crucially,by periodically resetting local memory states, we break sequential dependenciesto enable massive context parallelization. Stage two is a brief fine-tuningphase where only the local memory modules are adapted to a smaller,high-resolution chunksize, maximizing accuracy with minimal overhead. Evaluatedon Titans and TTT models, TNT achieves a substantial acceleration in trainingspeed-up to 17 times faster than the most accurate baseline configuration -while simultaneously improving model accuracy. This improvement removes acritical scalability barrier, establishing a practical foundation fordeveloping expressive RNNs and facilitating future work to close theperformance gap with Transformers.</description><author>Zeman Li, Ali Behrouz, Yuan Deng, Peilin Zhong, Praneeth Kacham, Mahdi Karami, Meisam Razaviyayn, Vahab Mirrokni</author><pubDate>Mon, 10 Nov 2025 17:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07343v1</guid></item><item><title>Adaptive Group Robust Ensemble Knowledge Distillation</title><link>http://arxiv.org/abs/2411.14984v2</link><description>Neural networks can learn spurious correlations in the data, often leading toperformance degradation for underrepresented subgroups. Studies havedemonstrated that the disparity is amplified when knowledge is distilled from acomplex teacher model to a relatively ``simple'' student model. Prior work hasshown that ensemble deep learning methods can improve the performance of theworst-case subgroups; however, it is unclear if this advantage carries overwhen distilling knowledge from an ensemble of teachers, especially when theteacher models are debiased. This study demonstrates that traditional ensembleknowledge distillation can significantly drop the performance of the worst-casesubgroups in the distilled student model even when the teacher models aredebiased. To overcome this, we propose Adaptive Group Robust Ensemble KnowledgeDistillation (AGRE-KD), a simple ensembling strategy to ensure that the studentmodel receives knowledge beneficial for unknown underrepresented subgroups.Leveraging an additional biased model, our method selectively chooses teacherswhose knowledge would better improve the worst-performing subgroups byupweighting the teachers with gradient directions deviating from the biasedmodel. Our experiments on several datasets demonstrate the superiority of theproposed ensemble distillation technique and show that it can even outperformclassic model ensembles based on majority voting. Our source code is availableat https://github.com/patrikken/AGRE-KD</description><author>Patrik Kenfack, Ulrich Aïvodji, Samira Ebrahimi Kahou</author><pubDate>Mon, 10 Nov 2025 17:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14984v2</guid></item><item><title>DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas</title><link>http://arxiv.org/abs/2511.07338v1</link><description>Simulating human profiles by instilling personas into large language models(LLMs) is rapidly transforming research in agentic behavioral simulation, LLMpersonalization, and human-AI alignment. However, most existing syntheticpersonas remain shallow and simplistic, capturing minimal attributes andfailing to reflect the rich complexity and diversity of real human identities.We introduce DEEPPERSONA, a scalable generative engine for synthesizingnarrative-complete synthetic personas through a two-stage, taxonomy-guidedmethod. First, we algorithmically construct the largest-ever human-attributetaxonomy, comprising over hundreds of hierarchically organized attributes, bymining thousands of real user-ChatGPT conversations. Second, we progressivelysample attributes from this taxonomy, conditionally generating coherent andrealistic personas that average hundreds of structured attributes and roughly 1MB of narrative text, two orders of magnitude deeper than prior works.Intrinsic evaluations confirm significant improvements in attribute diversity(32 percent higher coverage) and profile uniqueness (44 percent greater)compared to state-of-the-art baselines. Extrinsically, our personas enhanceGPT-4.1-mini's personalized question answering accuracy by 11.6 percent onaverage across ten metrics and substantially narrow (by 31.7 percent) the gapbetween simulated LLM citizens and authentic human responses in social surveys.Our generated national citizens reduced the performance gap on the Big Fivepersonality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONAthus provides a rigorous, scalable, and privacy-free platform for high-fidelityhuman simulation and personalized AI research.</description><author>Zhen Wang, Yufan Zhou, Zhongyan Luo, Lyumanshan Ye, Adam Wood, Man Yao, Luoshang Pan</author><pubDate>Mon, 10 Nov 2025 17:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07338v1</guid></item><item><title>Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for VLM Hallucination Mitigation</title><link>http://arxiv.org/abs/2510.22067v2</link><description>Vision language models (VLMs) often generate hallucination, i.e., contentthat cannot be substantiated by either textual or visual inputs. Prior workprimarily attributes this to over-reliance on linguistic prior knowledge ratherthan visual inputs. Some methods attempt to mitigate hallucination byamplifying visual token attention proportionally to their attention scores.However, these methods overlook the visual attention sink problem, whereattention is frequently misallocated to task-irrelevant visual regions, andneglect cross-modal fusion balance by enhancing only visual attention withoutadjusting attention to the user query. This can result in amplifying incorrectareas while failing to properly interpret the user query. To address thesechallenges, we propose a simple yet effective method called Gaze Shift-GuidedCross-modal Fusion Enhancement (GIFT). GIFT pre-computes a holistic visualsaliency map by tracking positive changes in visual attention, or "gazeshifts", during user query comprehension, and leverages this map to amplifyattention to both salient visual information and the user query at eachdecoding step. This reduces the impact of visual attention sink, as irrelevanttokens exhibit minimal shifts, while ensuring balanced cross-modal fusion forwell-integrated representation. Extensive experiments show that GIFTeffectively mitigates hallucination in VLMs across both generative andclassification tasks, achieving up to 20.7% improvement over greedy decoding,while maintaining general vision-language performance with low computationaloverhead.</description><author>Zheng Qi, Chao Shang, Evangelia Spiliopoulou, Nikolaos Pappas</author><pubDate>Mon, 10 Nov 2025 17:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.22067v2</guid></item><item><title>Grounding Computer Use Agents on Human Demonstrations</title><link>http://arxiv.org/abs/2511.07332v1</link><description>Building reliable computer-use agents requires grounding: accuratelyconnecting natural language instructions to the correct on-screen elements.While large datasets exist for web and mobile interactions, high-qualityresources for desktop environments are limited. To address this gap, weintroduce GroundCUA, a large-scale desktop grounding dataset built from experthuman demonstrations. It covers 87 applications across 12 categories andincludes 56K screenshots, with every on-screen element carefully annotated fora total of over 3.56M human-verified annotations. From these demonstrations, wegenerate diverse instructions that capture a wide range of real-world tasks,providing high-quality data for model training. Using GroundCUA, we develop theGroundNext family of models that map instructions to their target UI elements.At both 3B and 7B scales, GroundNext achieves state-of-the-art results acrossfive benchmarks using supervised fine-tuning, while requiring less thanone-tenth the training data of prior work. Reinforcement learning post-trainingfurther improves performance, and when evaluated in an agentic setting on theOSWorld benchmark using o3 as planner, GroundNext attains comparable orsuperior results to models trained with substantially more data,. These resultsdemonstrate the critical role of high-quality, expert-driven datasets inadvancing general-purpose computer-use agents.</description><author>Aarash Feizi, Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Kaixin Li, Rabiul Awal, Xing Han Lù, Johan Obando-Ceron, Juan A. Rodriguez, Nicolas Chapados, David Vazquez, Adriana Romero-Soriano, Reihaneh Rabbany, Perouz Taslakian, Christopher Pal, Spandana Gella, Sai Rajeswar</author><pubDate>Mon, 10 Nov 2025 17:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07332v1</guid></item><item><title>Which Similarity-Sensitive Entropy?</title><link>http://arxiv.org/abs/2511.03849v2</link><description>A canonical step in quantifying a system is to measure its entropy. Shannonentropy and other traditional entropy measures capture only the informationencoded in the frequencies of a system's elements. Recently, Leinster, Cobbold,and Reeve (LCR) introduced a method that also captures the rich informationencoded in the similarities and differences among elements, yieldingsimilarity-sensitive entropy. More recently, the Vendi score (VS) wasintroduced as an alternative, raising the question of how LCR and VS compare,and which is preferable. Here we address these questions conceptually,analytically, and experimentally, using 53 machine-learning datasets. We showthat LCR and VS can differ by orders of magnitude and can capture complementaryinformation about a system, except in limiting cases. We demonstrate that bothLCR and VS depend on how similarities are scaled and introduce the concept of``half distance'' to parameterize this dependence. We prove that VS provides anupper bound on LCR for several values of the R\'enyi-Hill order parameter andconjecture that this bound holds for all values. We conclude that VS ispreferable only when interpreting elements as linear combinations of a morefundamental set of ``ur-elements'' or when the system or dataset possesses aquantum-mechanical character. In the broader circumstance where one seekssimply to capture the rich information encoded by similarity, LCR is favored;nevertheless, for certain half-distances the two methods can complement eachother.</description><author>Phuc Nguyen, Josiah Couch, Rahul Bansal, Alexandra Morgan, Chris Tam, Miao Li, Rima Arnaout, Ramy Arnaout</author><pubDate>Mon, 10 Nov 2025 17:32:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.03849v2</guid></item><item><title>Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis</title><link>http://arxiv.org/abs/2511.07329v1</link><description>It introduces FractalNet, a fractal-inspired computational architectures foradvanced large language model analysis that mainly challenges model diversityon a large scale in an efficient manner. The new set-up involves atemplate-driven generator, runner, and evaluation framework that, throughsystematic permutations of convolutional, normalization, activation, anddropout layers, can create more than 1,200 variants of neural networks. Fractaltemplates allow for structural recursion and multi-column pathways, thus,models become deeper and wider in a balanced way. Training utilizes PyTorch,Automatic Mixed Precision (AMP), and gradient checkpointing and is carried outon the CIFAR-10 dataset for five epochs. The outcomes show that fractal-basedarchitectures are capable of strong performance and are computationallyefficient. The paper positions fractal design as a feasible andresource-efficient method of automated architecture exploration.</description><author>Yash Mittal, Dmitry Ignatov, Radu Timofte</author><pubDate>Mon, 10 Nov 2025 17:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07329v1</guid></item><item><title>Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training</title><link>http://arxiv.org/abs/2511.07328v1</link><description>Retrieval-Augmented Generation (RAG) methods enhance LLM performance byefficiently filtering relevant context for LLMs, reducing hallucinations andinference cost. However, most existing RAG methods focus on single-stepretrieval, which is often insufficient for answering complex questions thatrequire multi-step search. Recently, multi-step retrieval approaches haveemerged, typically involving the fine-tuning of small LLMs to performmulti-step retrieval. This type of fine-tuning is highly resource-intensive anddoes not enable the use of larger LLMs. In this work, we propose Q-RAG, a novelapproach that fine-tunes the Embedder model for multi-step retrieval usingreinforcement learning (RL). Q-RAG offers a competitive, resource-efficientalternative to existing multi-step retrieval methods for open-domain questionanswering and achieves state-of-the-art results on the popular long-contextbenchmarks Babilong and RULER for contexts up to 10M tokens.</description><author>Artyom Sorokin, Nazar Buzun, Alexander Anokhin, Oleg Inozemcev, Egor Vedernikov, Petr Anokhin, Mikhail Burtsev, Trushkov Alexey, Yin Wenshuai, Evgeny Burnaev</author><pubDate>Mon, 10 Nov 2025 17:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07328v1</guid></item><item><title>IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction</title><link>http://arxiv.org/abs/2511.07327v1</link><description>Recent advances in deep-research agents have shown promise for autonomousknowledge construction through dynamic reasoning over external sources.However, existing approaches rely on a mono-contextual paradigm thataccumulates all information in a single, expanding context window, leading tocontext suffocation and noise contamination that limit their effectiveness onlong-horizon tasks. We introduce IterResearch, a novel iterative deep-researchparadigm that reformulates long-horizon research as a Markov Decision Processwith strategic workspace reconstruction. By maintaining an evolving report asmemory and periodically synthesizing insights, our approach preservesconsistent reasoning capacity across arbitrary exploration depths. We furtherdevelop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learningframework that incentivizes efficient exploration through geometric rewarddiscounting and enables stable distributed training via adaptive downsampling.Extensive experiments demonstrate that IterResearch achieves substantialimprovements over existing open-source agents with average +14.5pp across sixbenchmarks and narrows the gap with frontier proprietary systems. Remarkably,our paradigm exhibits unprecedented interaction scaling, extending to 2048interactions with dramatic performance gains (from 3.5\% to 42.5\%), and servesas an effective prompting strategy, improving frontier models by up to 19.2ppover ReAct on long-horizon tasks. These findings position IterResearch as aversatile solution for long-horizon reasoning, effective both as a trainedagent and as a prompting paradigm for frontier models.</description><author>Guoxin Chen, Zile Qiao, Xuanzhong Chen, Donglei Yu, Haotian Xu, Wayne Xin Zhao, Ruihua Song, Wenbiao Yin, Huifeng Yin, Liwen Zhang, Kuan Li, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</author><pubDate>Mon, 10 Nov 2025 17:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07327v1</guid></item><item><title>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</title><link>http://arxiv.org/abs/2511.04665v2</link><description>Robotic manipulation policies are advancing rapidly, but their directevaluation in the real world remains costly, time-consuming, and difficult toreproduce, particularly for tasks involving deformable objects. Simulationprovides a scalable and systematic alternative, yet existing simulators oftenfail to capture the coupled visual and physical complexity of soft-bodyinteractions. We present a real-to-sim policy evaluation framework thatconstructs soft-body digital twins from real-world videos and renders robots,objects, and environments with photorealistic fidelity using 3D GaussianSplatting. We validate our approach on representative deformable manipulationtasks, including plush toy packing, rope routing, and T-block pushing,demonstrating that simulated rollouts correlate strongly with real-worldexecution performance and reveal key behavioral patterns of learned policies.Our results suggest that combining physics-informed reconstruction withhigh-quality rendering enables reproducible, scalable, and accurate evaluationof robotic manipulation policies. Website: https://real2sim-eval.github.io/</description><author>Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, Yunzhu Li</author><pubDate>Mon, 10 Nov 2025 17:28:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.04665v2</guid></item><item><title>Garbage Vulnerable Point Monitoring using IoT and Computer Vision</title><link>http://arxiv.org/abs/2511.07325v1</link><description>This paper proposes a smart way to manage municipal solid waste by using theInternet of Things (IoT) and computer vision (CV) to monitor illegal wastedumping at garbage vulnerable points (GVPs) in urban areas. The system canquickly detect and monitor dumped waste using a street-level camera and objectdetection algorithm. Data was collected from the Sangareddy district inTelangana, India. A series of comprehensive experiments was carried out usingthe proposed dataset to assess the accuracy and overall performance of variousobject detection models. Specifically, we performed an in-depth evaluation ofYOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models,YOLO11m achieved the highest accuracy of 92.39\% in waste detection,demonstrating its effectiveness in detecting waste. Additionally, it attains anmAP@50 of 0.91, highlighting its high precision. These findings confirm thatthe object detection model is well-suited for monitoring and tracking wastedumping events at GVP locations. Furthermore, the system effectively captureswaste disposal patterns, including hourly, daily, and weekly dumping trends,ensuring comprehensive daily and nightly monitoring.</description><author>R. Kumar, A. Lall, S. Chaudhari, M. Kale, A. Vattem</author><pubDate>Mon, 10 Nov 2025 17:27:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07325v1</guid></item><item><title>FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation</title><link>http://arxiv.org/abs/2511.07322v1</link><description>While LLMs have shown great success in financial tasks like stock predictionand question answering, their application in fully automating Equity ResearchReport generation remains uncharted territory. In this paper, we formulate theEquity Research Report (ERR) Generation task for the first time. To address thedata scarcity and the evaluation metrics absence, we present an open-sourceevaluation benchmark for ERR generation - FinRpt. We frame a DatasetConstruction Pipeline that integrates 7 financial data types and produces ahigh-quality ERR dataset automatically, which could be used for model trainingand evaluation. We also introduce a comprehensive evaluation system including11 metrics to assess the generated ERRs. Moreover, we propose a multi-agentframework specifically tailored to address this task, named FinRpt-Gen, andtrain several LLM-based agents on the proposed datasets using SupervisedFine-Tuning and Reinforcement Learning. Experimental results indicate the dataquality and metrics effectiveness of the benchmark FinRpt and the strongperformance of FinRpt-Gen, showcasing their potential to drive innovation inthe ERR generation field. All code and datasets are publicly available.</description><author>Song Jin, Shuqi Li, Shukun Zhang, Rui Yan</author><pubDate>Mon, 10 Nov 2025 17:22:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07322v1</guid></item><item><title>YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2511.07321v1</link><description>Fast and flexible 3D scene reconstruction from unstructured image collectionsremains a significant challenge. We present YoNoSplat, a feedforward model thatreconstructs high-quality 3D Gaussian Splatting representations from anarbitrary number of images. Our model is highly versatile, operatingeffectively with both posed and unposed, calibrated and uncalibrated inputs.YoNoSplat predicts local Gaussians and camera poses for each view, which areaggregated into a global representation using either predicted or providedposes. To overcome the inherent difficulty of jointly learning 3D Gaussians andcamera parameters, we introduce a novel mixing training strategy. This approachmitigates the entanglement between the two tasks by initially usingground-truth poses to aggregate local Gaussians and gradually transitioning toa mix of predicted and ground-truth poses, which prevents both traininginstability and exposure bias. We further resolve the scale ambiguity problemby a novel pairwise camera-distance normalization scheme and by embeddingcamera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsicparameters, making it feasible for uncalibrated inputs. YoNoSplat demonstratesexceptional efficiency, reconstructing a scene from 100 views (at 280x518resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achievesstate-of-the-art performance on standard benchmarks in both pose-free andpose-dependent settings. Our project page is athttps://botaoye.github.io/yonosplat/.</description><author>Botao Ye, Boqi Chen, Haofei Xu, Daniel Barath, Marc Pollefeys</author><pubDate>Mon, 10 Nov 2025 17:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07321v1</guid></item><item><title>When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs</title><link>http://arxiv.org/abs/2511.07318v1</link><description>Despite substantial advances, large language models (LLMs) continue toexhibit hallucinations, generating plausible yet incorrect responses. In thispaper, we highlight a critical yet previously underexplored class ofhallucinations driven by spurious correlations -- superficial but statisticallyprominent associations between features (e.g., surnames) and attributes (e.g.,nationality) present in the training data. We demonstrate that these spuriouscorrelations induce hallucinations that are confidently generated, immune tomodel scaling, evade current detection methods, and persist even after refusalfine-tuning. Through systematically controlled synthetic experiments andempirical evaluations on state-of-the-art open-source and proprietary LLMs(including GPT-5), we show that existing hallucination detection methods, suchas confidence-based filtering and inner-state probing, fundamentally fail inthe presence of spurious correlations. Our theoretical analysis furtherelucidates why these statistical biases intrinsically undermineconfidence-based detection techniques. Our findings thus emphasize the urgentneed for new approaches explicitly designed to address hallucinations caused byspurious correlations.</description><author>Shaowen Wang, Yiqi Dong, Ruinian Chang, Tansheng Zhu, Yuebo Sun, Kaifeng Lyu, Jian Li</author><pubDate>Mon, 10 Nov 2025 17:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07318v1</guid></item><item><title>Bridging Weakly-Supervised Learning and VLM Distillation: Noisy Partial Label Learning for Efficient Downstream Adaptation</title><link>http://arxiv.org/abs/2506.03229v2</link><description>In the context of noisy partial label learning (NPLL), each training sampleis associated with a set of candidate labels annotated by multiple noisyannotators. With the emergence of high-performance pre-trained vision-languagemodels (VLMs) such as CLIP, LLaVA and GPT-4V, the direction of using thesemodels to replace time-consuming manual annotation workflows and achieve``manual-annotation-free" training for downstream tasks has become a highlypromising research avenue. This paper focuses on learning from noisy partiallabels annotated by pre-trained VLMs and proposes an innovative collaborativeconsistency regularization (Co-Reg) method. Unlike the symmetric noiseprimarily addressed in traditional noisy label learning, the noise generated bypre-trained models is instance-dependent, embodying the underlying patterns ofthe pre-trained models themselves, which significantly increases the learningdifficulty for the model. To address this, we simultaneously train two neuralnetworks that implement collaborative purification of training labels through a``Co-Pseudo-Labeling" mechanism, while enforcing consistency regularizationconstraints in both the label space and feature representation space.Specifically, we construct multiple anti-overfitting mechanisms thatefficiently mine latent information from noisy partially labeled samplesincluding alternating optimization of contrastive feature representations andpseudo-labels, as well as maintaining prototypical class vectors in the sharedfeature space.</description><author>Qian-Wei Wang, Yuqiu Xie, Letian Zhang, Zimo Liu, Shu-Tao Xia</author><pubDate>Mon, 10 Nov 2025 17:19:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.03229v2</guid></item><item><title>RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments</title><link>http://arxiv.org/abs/2511.07317v1</link><description>We introduce Reinforcement Learning (RL) with Adaptive VerifiableEnvironments (RLVE), an approach using verifiable environments thatprocedurally generate problems and provide algorithmically verifiable rewards,to scale up RL for language models (LMs). RLVE enables each verifiableenvironment to dynamically adapt its problem difficulty distribution to thepolicy model's capabilities as training progresses. In contrast, static datadistributions often lead to vanishing learning signals when problems are eithertoo easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, alarge-scale suite of 400 verifiable environments carefully developed throughmanual environment engineering. Using RLVE-Gym, we show that environmentscaling, i.e., expanding the collection of training environments, consistentlyimproves generalizable reasoning capabilities. RLVE with joint training acrossall 400 environments in RLVE-Gym yields a 3.37% absolute average improvementacross six reasoning benchmarks, starting from one of the strongest 1.5Breasoning LMs. By comparison, continuing this LM's original RL training yieldsonly a 0.49% average absolute gain despite using over 3x more compute. Werelease our code publicly.</description><author>Zhiyuan Zeng, Hamish Ivison, Yiping Wang, Lifan Yuan, Shuyue Stella Li, Zhuorui Ye, Siting Li, Jacqueline He, Runlong Zhou, Tong Chen, Chenyang Zhao, Yulia Tsvetkov, Simon Shaolei Du, Natasha Jaques, Hao Peng, Pang Wei Koh, Hannaneh Hajishirzi</author><pubDate>Mon, 10 Nov 2025 17:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07317v1</guid></item><item><title>De-Individualizing fMRI Signals via Mahalanobis Whitening and Bures Geometry</title><link>http://arxiv.org/abs/2511.07313v1</link><description>Functional connectivity has been widely investigated to understand braindisease in clinical studies and imaging-based neuroscience, and analyzingchanges in functional connectivity has proven to be valuable for understandingand computationally evaluating the effects on brain function caused by diseasesor experimental stimuli. By using Mahalanobis data whitening prior to the useof dimensionality reduction algorithms, we are able to distill meaningfulinformation from fMRI signals about subjects and the experimental stimuli usedto prompt them. Furthermore, we offer an interpretation of Mahalanobiswhitening as a two-stage de-individualization of data which is motivated bysimilarity as captured by the Bures distance, which is connected to quantummechanics. These methods have potential to aid discoveries about the mechanismsthat link brain function with cognition and behavior and may improve theaccuracy and consistency of Alzheimer's diagnosis, especially in thepreclinical stage of disease progression.</description><author>Aaron Jacobson, Tingting Dan, Martin Styner, Guorong Wu, Shahar Kovalsky, Caroline Moosmueller</author><pubDate>Mon, 10 Nov 2025 17:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07313v1</guid></item><item><title>Revisiting Stochastic Approximation and Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2505.11343v3</link><description>In this paper, we introduce a new approach to proving the convergence of theStochastic Approximation (SA) and the Stochastic Gradient Descent (SGD)algorithms. The new approach is based on a concept called GSLLN (GeneralizedStrong Law of Large Numbers), which extends the traditional SLLN. Using thisconcept, we provide sufficient conditions for convergence, which effectivelydecouple the properties of the function whose zero we are trying to find, fromthe properties of the measurement errors (noise sequence). The new approachprovides an alternative to the two widely used approaches, namely the ODEapproach and the martingale approach, and also permits a wider class of noisesignals than either of the two known approaches. In particular, the ``noise''or measurement error \textit{need not} have a finite second moment, and undersuitable conditions, not even a finite mean. By adapting this method of proof,we also derive sufficient conditions for the convergence of zero-order SGD,wherein the stochastic gradient is computed using $2d$ function evaluations,but no gradient computations. The sufficient conditions derived here are theweakest to date, thus leading to a considerable expansion of the applicabilityof SA and SGD theory.</description><author>Rajeeva Laxman Karandikar, Bhamidi Visweswara Rao, Mathukumalli Vidyasagar</author><pubDate>Mon, 10 Nov 2025 17:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11343v3</guid></item><item><title>Superhuman AI for Stratego Using Self-Play Reinforcement Learning and Test-Time Search</title><link>http://arxiv.org/abs/2511.07312v1</link><description>Few classical games have been regarded as such significant benchmarks ofartificial intelligence as to have justified training costs in the millions ofdollars. Among these, Stratego -- a board wargame exemplifying the challenge ofstrategic decision making under massive amounts of hidden information -- standsapart as a case where such efforts failed to produce performance at the levelof top humans. This work establishes a step change in both performance and costfor Stratego, showing that it is now possible not only to reach the level oftop humans, but to achieve vastly superhuman level -- and that doing sorequires not an industrial budget, but merely a few thousand dollars. Weachieved this result by developing general approaches for self-playreinforcement learning and test-time search under imperfect information.</description><author>Samuel Sokota, Eugene Vinitsky, Hengyuan Hu, J. Zico Kolter, Gabriele Farina</author><pubDate>Mon, 10 Nov 2025 17:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07312v1</guid></item><item><title>When Bias Helps Learning: Bridging Initial Prejudice and Trainability</title><link>http://arxiv.org/abs/2505.12096v3</link><description>Understanding the statistical properties of deep neural networks (DNNs) atinitialization is crucial for elucidating both their trainability and theintrinsic architectural biases they encode prior to data exposure. Mean-field(MF) analyses have demonstrated that the parameter distribution in randomlyinitialized networks dictates whether gradients vanish or explode. Recent workhas shown that untrained DNNs exhibit an initial-guessing bias (IGB), in whichlarge regions of the input space are assigned to a single class. In this work,we provide a theoretical proof linking IGB to MF analyses, establishing that anetwork predisposition toward specific classes is intrinsically tied to theconditions for efficient learning. This connection leads to a counterintuitiveconclusion: the initialization that optimizes trainability is systematicallybiased rather than neutral. We validate our theory through experiments acrossmultiple architectures and datasets.</description><author>Alberto Bassi, Marco Baity-Jesi, Aurelien Lucchi, Carlo Albert, Emanuele Francazi</author><pubDate>Mon, 10 Nov 2025 17:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.12096v3</guid></item><item><title>ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding</title><link>http://arxiv.org/abs/2511.07311v1</link><description>Automatic ICD coding, the task of assigning disease and procedure codes toelectronic medical records, is crucial for clinical documentation and billing.While existing methods primarily enhance model understanding of codehierarchies and synonyms, they often overlook the pervasive use of medicalacronyms in clinical notes, a key factor in ICD code inference. To address thisgap, we propose a novel effective data augmentation technique that leverageslarge language models to expand medical acronyms, allowing models to be trainedon their full form representations. Moreover, we incorporate consistencytraining to regularize predictions by enforcing agreement between the originaland augmented documents. Extensive experiments on the MIMIC-III datasetdemonstrate that our approach, ACE-ICD establishes new state-of-the-artperformance across multiple settings, including common codes, rare codes, andfull-code assignments. Our code is publicly available.</description><author>Tuan-Dung Le, Shohreh Haddadan, Thanh Q. Thieu</author><pubDate>Mon, 10 Nov 2025 17:11:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07311v1</guid></item><item><title>Can Training Dynamics of Scale-Invariant Neural Networks Be Explained by the Thermodynamics of an Ideal Gas?</title><link>http://arxiv.org/abs/2511.07308v1</link><description>Understanding the training dynamics of deep neural networks remains a majoropen problem, with physics-inspired approaches offering promising insights.Building on this perspective, we develop a thermodynamic framework to describethe stationary distributions of stochastic gradient descent (SGD) with weightdecay for scale-invariant neural networks, a setting that both reflectspractical architectures with normalization layers and permits theoreticalanalysis. We establish analogies between training hyperparameters (e.g.,learning rate, weight decay) and thermodynamic variables such as temperature,pressure, and volume. Starting with a simplified isotropic noise model, weuncover a close correspondence between SGD dynamics and ideal gas behavior,validated through theory and simulation. Extending to training of neuralnetworks, we show that key predictions of the framework, including the behaviorof stationary entropy, align closely with experimental observations. Thisframework provides a principled foundation for interpreting training dynamicsand may guide future work on hyperparameter tuning and the design of learningrate schedulers.</description><author>Ildus Sadrtdinov, Ekaterina Lobacheva, Ivan Klimov, Mikhail I. Katsnelson, Dmitry Vetrov</author><pubDate>Mon, 10 Nov 2025 17:10:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07308v1</guid></item><item><title>Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task Learning Approach for Bangla Hate Speech Identification</title><link>http://arxiv.org/abs/2511.07304v1</link><description>This paper addresses the problem of Bangla hate speech identification, asocially impactful yet linguistically challenging task. As part of the "BanglaMulti-task Hate Speech Identification" shared task at the BLP Workshop,IJCNLP-AACL 2025, our team "Retriv" participated in all three subtasks: (1A)hate type classification, (1B) target group identification, and (1C) jointdetection of type, severity, and target. For subtasks 1A and 1B, we employed asoft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2).For subtask 1C, we trained three multitask variants and aggregated theirpredictions through a weighted voting ensemble. Our systems achieved micro-f1scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62%(1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7thpositions, respectively. These results highlight the promise of transformerensembles and weighted multitask frameworks for advancing Bangla hate speechdetection in low-resource contexts. We made experimental scripts publiclyavailable for the community.</description><author>Sourav Saha, K M Nafi Asib, Mohammed Moshiul Hoque</author><pubDate>Mon, 10 Nov 2025 17:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07304v1</guid></item><item><title>Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection</title><link>http://arxiv.org/abs/2511.07301v1</link><description>Source-Free Object Detection (SFOD) aims to adapt a source-pretrained objectdetector to a target domain without access to source data. However, existingSFOD methods predominantly rely on internal knowledge from the source model,which limits their capacity to generalize across domains and often results inbiased pseudo-labels, thereby hindering both transferability anddiscriminability. In contrast, Vision Foundation Models (VFMs), pretrained onmassive and diverse data, exhibit strong perception capabilities and broadgeneralization, yet their potential remains largely untapped in the SFODsetting. In this paper, we propose a novel SFOD framework that leverages VFMsas external knowledge sources to jointly enhance feature alignment and labelquality. Specifically, we design three VFM-based modules: (1) Patch-weightedGlobal Feature Alignment (PGFA) distills global features from VFMs usingpatch-similarity-based weighting to enhance global feature transferability; (2)Prototype-based Instance Feature Alignment (PIFA) performs instance-levelcontrastive learning guided by momentum-updated VFM prototypes; and (3)Dual-source Enhanced Pseudo-label Fusion (DEPF) fuses predictions fromdetection VFMs and teacher models via an entropy-aware strategy to yield morereliable supervision. Extensive experiments on six benchmarks demonstrate thatour method achieves state-of-the-art SFOD performance, validating theeffectiveness of integrating VFMs to simultaneously improve transferability anddiscriminability.</description><author>Huizai Yao, Sicheng Zhao, Pengteng Li, Yi Cui, Shuo Lu, Weiyu Guo, Yunfan Lu, Yijie Xu, Hui Xiong</author><pubDate>Mon, 10 Nov 2025 17:06:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07301v1</guid></item><item><title>Onboard Hyperspectral Super-Resolution with Deep Pushbroom Neural Network</title><link>http://arxiv.org/abs/2507.20765v2</link><description>Hyperspectral imagers on satellites obtain the fine spectral signaturesessential for distinguishing one material from another at the expense oflimited spatial resolution. Enhancing the latter is thus a desirablepreprocessing step in order to further improve the detection capabilitiesoffered by hyperspectral images on downstream tasks. At the same time, there isa growing interest towards deploying inference methods directly onboard ofsatellites, which calls for lightweight image super-resolution methods that canbe run on the payload in real time. In this paper, we present a novel neuralnetwork design, called Deep Pushbroom Super-Resolution (DPSR) that matches thepushbroom acquisition of hyperspectral sensors by processing an image line byline in the along-track direction with a causal memory mechanism to exploitpreviously acquired lines. This design greatly limits memory requirements andcomputational complexity, achieving onboard real-time performance, i.e., theability to super-resolve a line in the time it takes to acquire the next one,on low-power hardware. Experiments show that the quality of the super-resolvedimages is competitive or even outperforms state-of-the-art methods that aresignificantly more complex.</description><author>Davide Piccinini, Diego Valsesia, Enrico Magli</author><pubDate>Mon, 10 Nov 2025 17:03:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.20765v2</guid></item><item><title>Multiple Streams of Knowledge Retrieval: Enriching and Recalling in Transformers</title><link>http://arxiv.org/abs/2506.20746v2</link><description>When an LLM learns a new fact during finetuning (e.g., new movie releases,newly elected pope, etc.), where does this information go? Are entitiesenriched with relation information, or do models recall informationjust-in-time before a prediction? Or, are ``all of the above'' true with LLMsimplementing multiple redundant heuristics? Existing localization approaches(e.g., activation patching) are ill-suited for this analysis because theyusually \textit{replace} parts of the residual stream, thus overriding previousinformation. To fill this gap, we propose \emph{dynamic weight grafting}, atechnique that selectively grafts weights from a finetuned model onto apretrained model. Using this technique, we show two separate pathways forretrieving finetuned relation information: 1) ``enriching" the residual streamwith relation information while processing the tokens that correspond to anentity (e.g., ``Zendaya'' in ``Zendaya co-starred with John David Washington'')and 2) ``recalling" this information at the final token position beforegenerating a target fact. In some cases, models need information from both ofthese pathways to correctly generate finetuned facts while, in other cases,either the ``enrichment" or ``recall" pathway alone is sufficient. We localizethe ``recall'' pathway to model components -- finding that ``recall" occurs viaboth task-specific attention mechanisms and an entity-specific extraction stepin the feedforward networks of the final layers before the target prediction.By targeting model components and parameters, as opposed to just activations,we are able to understand the \textit{mechanisms} by which finetuned knowledgeis retrieved during generation.</description><author>Todd Nief, David Reber, Sean Richardson, Ari Holtzman</author><pubDate>Mon, 10 Nov 2025 17:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.20746v2</guid></item><item><title>From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching</title><link>http://arxiv.org/abs/2505.24843v2</link><description>Models that learn spurious correlations from training data often fail whendeployed in new environments. While many methods aim to learn invariantrepresentations to address this, they often underperform standard empiricalrisk minimization (ERM). We propose a data-centric alternative that shifts thefocus from learning invariant representations to leveraging invariant datapairs -- pairs of samples that should have the same prediction. We prove thatcertain counterfactuals naturally satisfy this invariance property. Based onthis, we introduce Noisy Counterfactual Matching (NCM), a simpleconstraint-based method that improves robustness by leveraging even a smallnumber of \emph{noisy} counterfactual pairs -- improving upon prior works thatdo not explicitly consider noise. For linear causal models, we prove that NCM'stest-domain error is bounded by its in-domain error plus a term dependent onthe counterfactuals' quality and diversity. Experiments on synthetic datavalidate our theory, and we demonstrate NCM's effectiveness on real-worlddatasets.</description><author>Ruqi Bai, Yao Ji, Zeyu Zhou, David I. Inouye</author><pubDate>Mon, 10 Nov 2025 16:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24843v2</guid></item><item><title>LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging</title><link>http://arxiv.org/abs/2511.07298v1</link><description>Low-dose computed tomography (CT) represents a significant improvement inpatient safety through lower radiation doses, but increased noise, blur, andcontrast loss can diminish diagnostic quality. Therefore, consistency androbustness in image quality assessment become essential for clinicalapplications. In this study, we propose an LLM-based quality assessment systemthat generates both numerical scores and textual descriptions of degradationssuch as noise, blur, and contrast loss. Furthermore, various inferencestrategies - from the zero-shot approach to metadata integration and errorfeedback - are systematically examined, demonstrating the progressivecontribution of each method to overall performance. The resultant assessmentsyield not only highly correlated scores but also interpretable output, therebyadding value to clinical workflows. The source codes of our study are availableat https://github.com/itu-biai/lmms_ldct_iqa.</description><author>Kagan Celik, Mehmet Ozan Unal, Metin Ertas, Isa Yildirim</author><pubDate>Mon, 10 Nov 2025 16:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07298v1</guid></item><item><title>VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models</title><link>http://arxiv.org/abs/2511.07299v1</link><description>Video anomaly understanding (VAU) aims to provide detailed interpretation andsemantic comprehension of anomalous events within videos, addressinglimitations of traditional methods that focus solely on detecting andlocalizing anomalies. However, existing approaches often neglect the deepercausal relationships and interactions between objects, which are critical forunderstanding anomalous behaviors. In this paper, we propose VADER, anLLM-driven framework for Video Anomaly unDErstanding, which integrates keyframeobject Relation features with visual cues to enhance anomaly comprehension fromvideo. Specifically, VADER first applies an Anomaly Scorer to assign per-frameanomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capturethe causal context of each anomalous event. A Relation Feature Extractor and aCOntrastive Relation Encoder (CORE) jointly model dynamic object interactions,producing compact relational representations for downstream reasoning. Thesevisual and relational cues are integrated with LLMs to generate detailed,causally grounded descriptions and support robust anomaly-related questionanswering. Experiments on multiple real-world VAU benchmarks demonstrate thatVADER achieves strong results across anomaly description, explanation, andcausal reasoning tasks, advancing the frontier of explainable video anomalyanalysis.</description><author>Ying Cheng, Yu-Ho Lin, Min-Hung Chen, Fu-En Yang, Shang-Hong Lai</author><pubDate>Mon, 10 Nov 2025 16:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07299v1</guid></item><item><title>Who Is the Story About? Protagonist Entity Recognition in News</title><link>http://arxiv.org/abs/2511.07296v1</link><description>News articles often reference numerous organizations, but traditional NamedEntity Recognition (NER) treats all mentions equally, obscuring which entitiesgenuinely drive the narrative. This limits downstream tasks that rely onunderstanding event salience, influence, or narrative focus. We introduceProtagonist Entity Recognition (PER), a task that identifies the organizationsthat anchor a news story and shape its main developments. To validate PER, wecompare he predictions of Large Language Models (LLMs) against annotations fromfour expert annotators over a gold corpus, establishing both inter-annotatorconsistency and human-LLM agreement. Leveraging these findings, we usestate-of-the-art LLMs to automatically label large-scale news collectionsthrough NER-guided prompting, generating scalable, high-quality supervision. Wethen evaluate whether other LLMs, given reduced context and without explicitcandidate guidance, can still infer the correct protagonists. Our resultsdemonstrate that PER is a feasible and meaningful extension tonarrative-centered information extraction, and that guided LLMs can approximatehuman judgments of narrative importance at scale.</description><author>Jorge Gabín, M. Eduardo Ares, Javier Parapar</author><pubDate>Mon, 10 Nov 2025 16:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07296v1</guid></item><item><title>VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference</title><link>http://arxiv.org/abs/2509.24257v3</link><description>Decentralized inference provides a scalable and resilient paradigm forserving large language models (LLMs), enabling distributed resource utilizationand reducing reliance on centralized providers. However, in a permissionlessenvironment without trusted nodes, ensuring the correctness of model outputsremains a core challenge. We introduce VeriLLM, a publicly verifiable protocolfor decentralized LLM inference that achieves security under aone-honest-verifier assumption while maintaining practical efficiency. VeriLLMcombines lightweight empirical rerunning with cryptographic commitments,allowing verifiers to validate results at approximately 1% of the underlyinginference cost. To prevent verification bottlenecks, we design an isomorphicinference-verification architecture that multiplexes both inference andverification roles across the same GPU workers. This design (i) improves GPUutilization and overall throughput, (ii) enlarges the effective validator set,enhancing robustness and liveness, and (iii) enforces task indistinguishabilityto prevent node-specific optimizations or selective behavior. Throughtheoretical analysis and system-level evaluation, we show that VeriLLM achievesreliable public verifiability with minimal overhead, offering a practicalfoundation for trustworthy and scalable decentralized LLM inference.</description><author>Ke Wang, Zishuo Zhao, Xinyuan Song, Bill Shi, Libin Xia, Chris Tong, Lynn Ai, Felix Qu, Eric Yang</author><pubDate>Mon, 10 Nov 2025 16:52:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.24257v3</guid></item><item><title>Universal Spectral Tokenization via Self-Supervised Panchromatic Representation Learning</title><link>http://arxiv.org/abs/2510.17959v2</link><description>Sequential scientific data span many resolutions and domains, and unifyingthem into a common representation is a key step toward developing foundationmodels for the sciences. Astronomical spectra exemplify this challenge: massivesurveys have collected millions of spectra across a wide range of wavelengthsand resolutions, yet analyses remain fragmented across spectral domains (e.g.,optical vs. infrared) and object types (e.g., stars vs. galaxies), limiting theability to pool information across datasets. We present a deep learning modelthat jointly learns from heterogeneous spectra in a self-supervised manner. Ouruniversal spectral tokenizer processes spectra from a variety of object typesand resolutions directly on their native wavelength grids, producingintrinsically aligned, homogeneous, and physically meaningful representationsthat can be efficiently adapted to achieve competitive performance across arange of downstream tasks. For the first time, we demonstrate that a singlemodel can unify spectral data across resolutions and domains, suggesting thatour model can serve as a powerful building block for foundation models inastronomy -- and potentially extend to other scientific domains withheterogeneous sequential data, such as climate and healthcare.</description><author>Jeff Shen, Francois Lanusse, Liam Holden Parker, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas Meyer, Micah Bowles, Sebastian Wagner-Carena, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim Bourfoune, Nathan Cassereau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno Régaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, Shirley Ho</author><pubDate>Mon, 10 Nov 2025 16:51:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.17959v2</guid></item><item><title>Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender Systems via Large Language Models</title><link>http://arxiv.org/abs/2511.07295v1</link><description>Implicit feedback, employed in training recommender systems, unavoidablyconfronts noise due to factors such as misclicks and position bias. Previousstudies have attempted to identify noisy samples through their diverged datapatterns, such as higher loss values, and mitigate their influence throughsample dropping or reweighting. However, we observed that noisy samples andhard samples display similar patterns, leading to hard-noisy confusion issue.Such confusion is problematic as hard samples are vital for modeling userpreferences. To solve this problem, we propose LLMHNI framework, leveraging twoauxiliary user-item relevance signals generated by Large Language Models (LLMs)to differentiate hard and noisy samples. LLMHNI obtains user-item semanticrelevance from LLM-encoded embeddings, which is used in negative sampling toselect hard negatives while filtering out noisy false negatives. An objectivealignment strategy is proposed to project LLM-encoded embeddings, originallyfor general language tasks, into a representation space optimized for user-itemrelevance modeling. LLMHNI also exploits LLM-inferred logical relevance withinuser-item interactions to identify hard and noisy samples. These LLM-inferredinteractions are integrated into the interaction graph and guide denoising withcross-graph contrastive alignment. To eliminate the impact of unreliableinteractions induced by LLM hallucination, we propose a graph contrastivelearning strategy that aligns representations from randomly edge-dropped viewsto suppress unreliable edges. Empirical results demonstrate that LLMHNIsignificantly improves denoising and recommendation performance.</description><author>Tianrui Song, Wen-Shuo Chao, Hao Liu</author><pubDate>Mon, 10 Nov 2025 16:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07295v1</guid></item><item><title>HyperSHAP: Shapley Values and Interactions for Explaining Hyperparameter Optimization</title><link>http://arxiv.org/abs/2502.01276v2</link><description>Hyperparameter optimization (HPO) is a crucial step in achieving strongpredictive performance. Yet, the impact of individual hyperparameters on modelgeneralization is highly context-dependent, prohibiting a one-size-fits-allsolution and requiring opaque HPO methods to find optimal configurations.However, the black-box nature of most HPO methods undermines user trust anddiscourages adoption. To address this, we propose a game-theoreticexplainability framework for HPO based on Shapley values and interactions. Ourapproach provides an additive decomposition of a performance measure acrosshyperparameters, enabling local and global explanations of hyperparameters'contributions and their interactions. The framework, named HyperSHAP, offersinsights into ablation studies, the tunability of learning algorithms, andoptimizer behavior across different hyperparameter spaces. We demonstrateHyperSHAP's capabilities on various HPO benchmarks to analyze the interactionstructure of the corresponding HPO problems, demonstrating its broadapplicability and actionable insights for improving HPO.</description><author>Marcel Wever, Maximilian Muschalik, Fabian Fumagalli, Marius Lindauer</author><pubDate>Mon, 10 Nov 2025 16:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.01276v2</guid></item><item><title>Verifying rich robustness properties for neural networks</title><link>http://arxiv.org/abs/2511.07293v1</link><description>Robustness is a important problem in AI alignment and safety, with modelssuch as neural networks being increasingly used in safety-critical systems. Inthe last decade, a large body of work has emerged on local robustness, i.e.,checking if the decision of a neural network remains unchanged when the inputis slightly perturbed. However, many of these approaches require specializedencoding and often ignore the confidence of a neural network on its output. Inthis paper, our goal is to build a generalized framework to specify and verifyvariants of robustness in neural network verification. We propose aspecification framework using a simple grammar, which is flexible enough tocapture most existing variants. This allows us to introduce new variants ofrobustness that take into account the confidence of the neural network in itsoutputs. Next, we develop a novel and powerful unified technique to verify allsuch variants in a homogeneous way, viz., by adding a few additional layers tothe neural network. This enables us to use any state-of-the-art neural networkverification tool, without having to tinker with the encoding within, whileincurring an approximation error that we show is bounded. We perform anextensive experimental evaluation over a large suite of 8870 benchmarks having138M parameters in a largest network, and show that we are able to capture awide set of robustness variants and outperform direct encoding approaches by asignificant margin.</description><author>Mohammad Afzal, S. Akshay, Ashutosh Gupta</author><pubDate>Mon, 10 Nov 2025 16:43:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07293v1</guid></item><item><title>PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving</title><link>http://arxiv.org/abs/2511.07292v1</link><description>Most recent work in autonomous driving has prioritized benchmark performanceand methodological innovation over in-depth analysis of model failures, biases,and shortcut learning. This has led to incremental improvements without a deepunderstanding of the current failures. While it is straightforward to look atsituations where the model fails, it is hard to understand the underlyingreason. This motivates us to conduct a systematic study, where inputs to themodel are perturbed and the predictions observed. We introduce PlanT 2.0, alightweight, object-centric planning transformer designed for autonomousdriving research in CARLA. The object-level representation enables controlledanalysis, as the input can be easily perturbed (e.g., by changing the locationor adding or removing certain objects), in contrast to sensor-based models. Totackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0,we introduce multiple upgrades to PlanT, achieving state-of-the-art performanceon Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysisexposes insightful failures, such as a lack of scene understanding caused bylow obstacle diversity, rigid expert behaviors leading to exploitableshortcuts, and overfitting to a fixed set of expert trajectories. Based onthese findings, we argue for a shift toward data-centric development, with afocus on richer, more robust, and less biased datasets. We open-source our codeand model at https://github.com/autonomousvision/plant2.</description><author>Simon Gerstenecker, Andreas Geiger, Katrin Renz</author><pubDate>Mon, 10 Nov 2025 16:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07292v1</guid></item><item><title>CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference Quality Assessment of Compressed Video</title><link>http://arxiv.org/abs/2511.07290v1</link><description>The prevalence of user-generated content (UGC) on platforms such as YouTubeand TikTok has rendered no-reference (NR) perceptual video quality assessment(VQA) vital for optimizing video delivery. Nonetheless, the characteristics ofnon-professional acquisition and the subsequent transcoding of UGC video onsharing platforms present significant challenges for NR-VQA. Although NR-VQAmodels attempt to infer mean opinion scores (MOS), their modeling of subjectivescores for compressed content remains limited due to the absence offine-grained perceptual annotations of artifact types. To address thesechallenges, we propose CAMP-VQA, a novel NR-VQA framework that exploits thesemantic understanding capabilities of large vision-language models. Ourapproach introduces a quality-aware prompting mechanism that integrates videometadata (e.g., resolution, frame rate, bitrate) with key fragments extractedfrom inter-frame variations to guide the BLIP-2 pretraining approach ingenerating fine-grained quality captions. A unified architecture has beendesigned to model perceptual quality across three dimensions: semanticalignment, temporal characteristics, and spatial characteristics. Thesemultimodal features are extracted and fused, then regressed to video qualityscores. Extensive experiments on a wide variety of UGC datasets demonstratethat our model consistently outperforms existing NR-VQA methods, achievingimproved accuracy without the need for costly manual fine-grained annotations.Our method achieves the best performance in terms of average rank and linearcorrelation (SRCC: 0.928, PLCC: 0.938) compared to state-of-the-art methods.The source code and trained models, along with a user-friendly demo, areavailable at: https://github.com/xinyiW915/CAMP-VQA.</description><author>Xinyi Wang, Angeliki Katsenou, Junxiao Shen, David Bull</author><pubDate>Mon, 10 Nov 2025 16:37:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07290v1</guid></item><item><title>FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models</title><link>http://arxiv.org/abs/2510.27486v2</link><description>AdamW has become one of the most effective optimizers for traininglarge-scale models. We have also observed its effectiveness in the context offederated learning (FL). However, directly applying AdamW in federated learningsettings poses significant challenges: (1) due to data heterogeneity, AdamWoften yields high variance in the second-moment estimate $\boldsymbol{v}$; (2)the local overfitting of AdamW may cause client drift; and (3) Reinitializingmoment estimates ($\boldsymbol{v}$, $\boldsymbol{m}$) at each round slows downconvergence. To address these challenges, we propose the first\underline{Fed}erated \underline{AdamW} algorithm, called \texttt{FedAdamW},for training and fine-tuning various large models. \texttt{FedAdamW} alignslocal updates with the global update using both a \textbf{local correctionmechanism} and decoupled weight decay to mitigate local overfitting.\texttt{FedAdamW} efficiently aggregates the \texttt{mean} of the second-momentestimates to reduce their variance and reinitialize them. Theoretically, weprove that \texttt{FedAdamW} achieves a linear speedup convergence rate of$\mathcal{O}(\sqrt{(L \Delta \sigma_l^2)/(S K R \epsilon^2)}+(L \Delta)/R)$without \textbf{heterogeneity assumption}, where $S$ is the number ofparticipating clients per round, $K$ is the number of local iterations, and $R$is the total number of communication rounds. We also employ PAC-Bayesiangeneralization analysis to explain the effectiveness of decoupled weight decayin local training. Empirically, we validate the effectiveness of\texttt{FedAdamW} on language and vision Transformer models. Compared toseveral baselines, \texttt{FedAdamW} significantly reduces communication roundsand improves test accuracy. The code is available inhttps://github.com/junkangLiu0/FedAdamW.</description><author>Junkang Liu, Fanhua Shang, Kewen Zhu, Hongying Liu, Yuanyuan Liu, Jin Liu</author><pubDate>Mon, 10 Nov 2025 16:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.27486v2</guid></item><item><title>Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch</title><link>http://arxiv.org/abs/2511.01934v2</link><description>Training tool-augmented LLMs has emerged as a promising approach to enhancinglanguage models' capabilities for complex tasks. The current supervisedfine-tuning paradigm relies on constructing extensive domain-specific datasetsto train models. However, this approach often struggles to generalizeeffectively to unfamiliar or intricate tool-use scenarios. Recently,reinforcement learning (RL) paradigm can endow LLMs with superior reasoning andgeneralization abilities. In this work, we address a key question: Can the pureRL be used to effectively elicit a model's intrinsic reasoning capabilities andenhance the tool-agnostic generalization? We propose a dynamicgeneralization-guided reward design for rule-based RL, which progressivelyshifts rewards from exploratory to exploitative tool-use patterns. Based onthis design, we introduce the Tool-Zero series models. These models are trainedto enable LLMs to autonomously utilize general tools by directly scaling up RLfrom Zero models (i.e., base models without post-training). Experimentalresults demonstrate that our models achieve over 7% performance improvementcompared to both SFT and RL-with-SFT models under the same experimentalsettings. These gains are consistently replicated across cross-dataset andintra-dataset evaluations, validating the effectiveness and robustness of ourmethods.</description><author>Yirong Zeng, Xiao Ding, Yutai Hou, Yuxian Wang, Li Du, Juyi Dai, Qiuyang Ding, Duyu Tang, Dandan Tu, Weiwen Liu, Bing Qin, Ting Liu</author><pubDate>Mon, 10 Nov 2025 16:36:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.01934v2</guid></item><item><title>Enabling Off-Policy Imitation Learning with Deep Actor Critic Stabilization</title><link>http://arxiv.org/abs/2511.07288v1</link><description>Learning complex policies with Reinforcement Learning (RL) is often hinderedby instability and slow convergence, a problem exacerbated by the difficulty ofreward engineering. Imitation Learning (IL) from expert demonstrations bypassesthis reliance on rewards. However, state-of-the-art IL methods, exemplified byGenerative Adversarial Imitation Learning (GAIL)Ho et. al, suffer from severesample inefficiency. This is a direct consequence of their foundationalon-policy algorithms, such as TRPO Schulman et.al. In this work, we introducean adversarial imitation learning algorithm that incorporates off-policylearning to improve sample efficiency. By combining an off-policy frameworkwith auxiliary techniques specifically, double Q network based stabilizationand value learning without reward function inference we demonstrate a reductionin the samples required to robustly match expert behavior.</description><author>Sayambhu Sen, Shalabh Bhatnagar</author><pubDate>Mon, 10 Nov 2025 16:35:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07288v1</guid></item><item><title>Glioma C6: A Novel Dataset for Training and Benchmarking Cell Segmentation</title><link>http://arxiv.org/abs/2511.07286v1</link><description>We present Glioma C6, a new open dataset for instance segmentation of gliomaC6 cells, designed as both a benchmark and a training resource for deeplearning models. The dataset comprises 75 high-resolution phase-contrastmicroscopy images with over 12,000 annotated cells, providing a realistictestbed for biomedical image analysis. It includes soma annotations andmorphological cell categorization provided by biologists. Additionalcategorization of cells, based on morphology, aims to enhance the utilizationof image data for cancer cell research. Glioma C6 consists of two parts: thefirst is curated with controlled parameters for benchmarking, while the secondsupports generalization testing under varying conditions. We evaluate theperformance of several generalist segmentation models, highlighting theirlimitations on our dataset. Our experiments demonstrate that training on GliomaC6 significantly enhances segmentation performance, reinforcing its value fordeveloping robust and generalizable models. The dataset is publicly availablefor researchers.</description><author>Roman Malashin, Svetlana Pashkevich, Daniil Ilyukhin, Arseniy Volkov, Valeria Yachnaya, Andrey Denisov, Maria Mikhalkova</author><pubDate>Mon, 10 Nov 2025 16:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07286v1</guid></item><item><title>Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks</title><link>http://arxiv.org/abs/2507.02735v2</link><description>Prompt injection attack has been listed as the top-1 security threat toLLM-integrated applications, which interact with external environment data forcomplex tasks. The untrusted data may contain an injected prompt trying toarbitrarily manipulate the system. Model-level prompt injection defenses haveshown strong effectiveness, but are currently deployed into commercial-grademodels in a closed-source manner. We believe open-source secure models areneeded by the AI security community, where co-development of attacks anddefenses through open research drives scientific progress in mitigating promptinjection attacks. To this end, we develop Meta SecAlign, the first fullyopen-source LLM with built-in model-level defense that achievescommercial-grade performance, powerful enough for complex agentic tasks. Weprovide complete details of our training recipe, an improved version of theSOTA SecAlign defense. We perform the most comprehensive evaluation to date on9 utility benchmarks and 7 security benchmarks on general knowledge,instruction following, and agentic workflows. Results show that Meta SecAlign,despite being trained on generic instruction-tuning samples only, surprisinglyconfers security in unseen downstream tasks, including tool-calling andweb-navigation, in addition to general instruction-following. Our best model --Meta-SecAlign-70B -- establishes a new frontier of utility-security trade-offfor open-source LLMs. Even compared to closed-course commercial models such asGPT-5, our model is much securer than most of them. Below are links for thecode (https://github.com/facebookresearch/Meta_SecAlign),Meta-SecAlign-70B(https://huggingface.co/facebook/Meta-SecAlign-70B), andMeta-SecAlign-8B(https://huggingface.co/facebook/Meta-SecAlign-8B) models.</description><author>Sizhe Chen, Arman Zharmagambetov, David Wagner, Chuan Guo</author><pubDate>Mon, 10 Nov 2025 16:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.02735v2</guid></item><item><title>MG-HGNN: A Heterogeneous GNN Framework for Indoor Wi-Fi Fingerprint-Based Localization</title><link>http://arxiv.org/abs/2511.07282v1</link><description>Received signal strength indicator (RSSI) is the primary representation ofWi-Fi fingerprints and serves as a crucial tool for indoor localization.However, existing RSSI-based positioning methods often suffer from reducedaccuracy due to environmental complexity and challenges in processingmulti-source information. To address these issues, we propose a novelmulti-graph heterogeneous GNN framework (MG-HGNN) to enhance spatial awarenessand improve positioning performance. In this framework, two graph constructionbranches perform node and edge embedding, respectively, to generate informativegraphs. Subsequently, a heterogeneous graph neural network is employed forgraph representation learning, enabling accurate positioning. The MG-HGNNframework introduces the following key innovations: 1) multi-type task-directedgraph construction that combines label estimation and feature encoding forricher graph information; 2) a heterogeneous GNN structure that enhances theperformance of conventional GNN models. Evaluations on the UJIIndoorLoc andUTSIndoorLoc public datasets demonstrate that MG-HGNN not only achievessuperior performance compared to several state-of-the-art methods, but alsoprovides a novel perspective for enhancing GNN-based localization methods.Ablation studies further confirm the rationality and effectiveness of theproposed framework.</description><author>Yibu Wang, Zhaoxin Zhang, Ning Li, Xinlong Zhao, Dong Zhao, Tianzi Zhao</author><pubDate>Mon, 10 Nov 2025 16:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07282v1</guid></item><item><title>Segmentation of Ischemic Stroke Lesions using Transfer Learning on Multi-sequence MRI</title><link>http://arxiv.org/abs/2511.07281v1</link><description>The accurate understanding of ischemic stroke lesions is critical forefficient therapy and prognosis of stroke patients. Magnetic resonance imaging(MRI) is sensitive to acute ischemic stroke and is a common diagnostic methodfor stroke. However, manual lesion segmentation performed by experts istedious, time-consuming, and prone to observer inconsistency. Automatic medicalimage analysis methods have been proposed to overcome this challenge. However,previous approaches have relied on hand-crafted features that may not capturethe irregular and physiologically complex shapes of ischemic stroke lesions. Inthis study, we present a novel framework for quickly and automaticallysegmenting ischemic stroke lesions on various MRI sequences, includingT1-weighted, T2-weighted, DWI, and FLAIR. The proposed methodology is validatedon the ISLES 2015 Brain Stroke sequence dataset, where we trained our modelusing the Res-Unet architecture twice: first, with pre-existing weights, andthen without, to explore the benefits of transfer learning. Evaluation metrics,including the Dice score and sensitivity, were computed across 3D volumes.Finally, a Majority Voting Classifier was integrated to amalgamate the outcomesfrom each axis, resulting in a comprehensive segmentation method. Our effortsculminated in achieving a Dice score of 80.5\% and an accuracy of 74.03\%,showcasing the efficacy of our segmentation approach.</description><author>R. P. Chowdhury, T. Rahman</author><pubDate>Mon, 10 Nov 2025 16:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07281v1</guid></item><item><title>The Value of Personalized Recommendations: Evidence from Netflix</title><link>http://arxiv.org/abs/2511.07280v1</link><description>Personalized recommendation systems shape much of user choice online, yettheir targeted nature makes separating out the value of recommendation and theunderlying goods challenging. We build a discrete choice model that embedsrecommendation-induced utility, low-rank heterogeneity, and flexible statedependence and apply the model to viewership data at Netflix. We exploitidiosyncratic variation introduced by the recommendation algorithm to identifyand separately value these components as well as to recover model-freediversion ratios that we can use to validate our structural model. We use themodel to evaluate counterfactuals that quantify the incremental engagementgenerated by personalized recommendations. First, we show that replacing thecurrent recommender system with a matrix factorization or popularity-basedalgorithm would lead to 4% and 12% reduction in engagement, respectively, anddecreased consumption diversity. Second, most of the consumption increase fromrecommendations comes from effective targeting, not mechanical exposure, withthe largest gains for mid-popularity goods (as opposed to broadly appealing orvery niche goods).</description><author>Kevin Zielnicki, Guy Aridor, Aurélien Bibaut, Allen Tran, Winston Chou, Nathan Kallus</author><pubDate>Mon, 10 Nov 2025 16:27:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07280v1</guid></item><item><title>StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression</title><link>http://arxiv.org/abs/2511.07278v1</link><description>Video Large Language Models (Video-LLMs) have demonstrated significantpotential in the areas of video captioning, search, and summarization. However,current Video-LLMs still face challenges with long real-world videos. Recentmethods have introduced a retrieval mechanism that retrieves query-relevant KVcaches for question answering, enhancing the efficiency and accuracy of longreal-world videos. However, the compression and retrieval of KV caches arestill not fully explored. In this paper, we propose \textbf{StreamKV}, atraining-free framework that seamlessly equips Video-LLMs with advanced KVcache retrieval and compression. Compared to previous methods that used uniformpartitioning, StreamKV dynamically partitions video streams into semanticsegments, which better preserves semantic information. For KV cache retrieval,StreamKV calculates a summary vector for each segment to retain segment-levelinformation essential for retrieval. For KV cache compression, StreamKVintroduces a guidance prompt designed to capture the key semantic elementswithin each segment, ensuring only the most informative KV caches are retainedfor answering questions. Moreover, StreamKV unifies KV cache retrieval andcompression within a single module, performing both in a layer-adaptive manner,thereby further improving the effectiveness of streaming video questionanswering. Extensive experiments on public StreamingVQA benchmarks demonstratethat StreamKV significantly outperforms existing Online Video-LLMs, achievingsuperior accuracy while substantially improving both memory efficiency andcomputational latency. The code has been released athttps://github.com/sou1p0wer/StreamKV.</description><author>Yilong Chen, Xiang Bai, Zhibin Wang, Chengyu Bai, Yuhan Dai, Ming Lu, Shanghang Zhang</author><pubDate>Mon, 10 Nov 2025 16:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07278v1</guid></item><item><title>Designing Beyond Language: Sociotechnical Barriers in AI Health Technologies for Limited English Proficiency</title><link>http://arxiv.org/abs/2511.07277v1</link><description>Limited English proficiency (LEP) patients in the U.S. face systemic barriersto healthcare beyond language and interpreter access, encompassing proceduraland institutional constraints. AI advances may support communication and carethrough on-demand translation and visit preparation, but also risk exacerbatingexisting inequalities. We conducted storyboard-driven interviews with 14patient navigators to explore how AI could shape care experiences forSpanish-speaking LEP individuals. We identified tensions around linguistic andcultural misunderstandings, privacy concerns, and opportunities and risks forAI to augment care workflows. Participants highlighted structural factors thatcan undermine trust in AI systems, including sensitive information disclosure,unstable technology access, and low digital literacy. While AI tools canpotentially alleviate social barriers and institutional constraints, there arerisks of misinformation and uprooting human camaraderie. Our findingscontribute design considerations for AI that support LEP patients and careteams via rapport-building, education, and language support, and minimizingdisruptions to existing practices.</description><author>Michelle Huang, Violeta J. Rodriguez, Koustuv Saha, Tal August</author><pubDate>Mon, 10 Nov 2025 16:23:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07277v1</guid></item><item><title>RobustA: Robust Anomaly Detection in Multimodal Data</title><link>http://arxiv.org/abs/2511.07276v1</link><description>In recent years, multimodal anomaly detection methods have demonstratedremarkable performance improvements over video-only models. However, real-worldmultimodal data is often corrupted due to unforeseen environmental distortions.In this paper, we present the first-of-its-kind work that comprehensivelyinvestigates the adverse effects of corrupted modalities on multimodal anomalydetection task. To streamline this work, we propose RobustA, a carefullycurated evaluation dataset to systematically observe the impacts of audio andvisual corruptions on the overall effectiveness of anomaly detection systems.Furthermore, we propose a multimodal anomaly detection method, which showsnotable resilience against corrupted modalities. The proposed method learns ashared representation space for different modalities and employs a dynamicweighting scheme during inference based on the estimated level of corruption.Our work represents a significant step forward in enabling the real-worldapplication of multimodal anomaly detection, addressing situations where thelikely events of modality corruptions occur. The proposed evaluation datasetwith corrupted modalities and respective extracted features will be madepublicly available.</description><author>Salem AlMarri, Muhammad Irzam Liaqat, Muhammad Zaigham Zaheer, Shah Nawaz, Karthik Nandakumar, Markus Schedl</author><pubDate>Mon, 10 Nov 2025 16:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07276v1</guid></item><item><title>Multi-modal Dynamic Proxy Learning for Personalized Multiple Clustering</title><link>http://arxiv.org/abs/2511.07274v1</link><description>Multiple clustering aims to discover diverse latent structures from differentperspectives, yet existing methods generate exhaustive clusterings withoutdiscerning user interest, necessitating laborious manual screening. Currentmulti-modal solutions suffer from static semantic rigidity: predefinedcandidate words fail to adapt to dataset-specific concepts, and fixed fusionstrategies ignore evolving feature interactions. To overcome these limitations,we propose Multi-DProxy, a novel multi-modal dynamic proxy learning frameworkthat leverages cross-modal alignment through learnable textual proxies.Multi-DProxy introduces 1) gated cross-modal fusion that synthesizesdiscriminative joint representations by adaptively modeling featureinteractions. 2) dual-constraint proxy optimization where user interestconstraints enforce semantic consistency with domain concepts while conceptconstraints employ hard example mining to enhance cluster discrimination. 3)dynamic candidate management that refines textual proxies through iterativeclustering feedback. Therefore, Multi-DProxy not only effectively captures auser's interest through proxies but also enables the identification of relevantclusterings with greater precision. Extensive experiments demonstratestate-of-the-art performance with significant improvements over existingmethods across a broad set of multi-clustering benchmarks.</description><author>Jinfeng Xu, Zheyu Chen, Shuo Yang, Jinze Li, Ziyue Peng, Zewei Liu, Hewei Wang, Jiayi Zhang, Edith C. H. Ngai</author><pubDate>Mon, 10 Nov 2025 16:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07274v1</guid></item><item><title>Understanding the role of depth in the neural tangent kernel for overparameterized neural networks</title><link>http://arxiv.org/abs/2511.07272v1</link><description>Overparameterized fully-connected neural networks have been shown to behavelike kernel models when trained with gradient descent, under mild conditions onthe width, the learning rate, and the parameter initialization. In the limit ofinfinitely large widths and small learning rate, the kernel that is obtainedallows to represent the output of the learned model with a closed-formsolution. This closed-form solution hinges on the invertibility of the limitingkernel, a property that often holds on real-world datasets. In this work, weanalyze the sensitivity of large ReLU networks to increasing depths bycharacterizing the corresponding limiting kernel. Our theoretical resultsdemonstrate that the normalized limiting kernel approaches the matrix of ones.In contrast, they show the corresponding closed-form solution approaches afixed limit on the sphere. We empirically evaluate the order of magnitude innetwork depth required to observe this convergent behavior, and we describe theessential properties that enable the generalization of our results to otherkernels.</description><author>William St-Arnaud, Margarida Carvalho, Golnoosh Farnadi</author><pubDate>Mon, 10 Nov 2025 16:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07272v1</guid></item><item><title>High-Dimensional Asymptotics of Differentially Private PCA</title><link>http://arxiv.org/abs/2511.07270v1</link><description>In differential privacy, statistics of a sensitive dataset are privatized byintroducing random noise. Most privacy analyses provide privacy boundsspecifying a noise level sufficient to achieve a target privacy guarantee.Sometimes, these bounds are pessimistic and suggest adding excessive noise,which overwhelms the meaningful signal. It remains unclear if such high noiselevels are truly necessary or a limitation of the proof techniques. This paperexplores whether we can obtain sharp privacy characterizations that identifythe smallest noise level required to achieve a target privacy level for a givenmechanism. We study this problem in the context of differentially privateprincipal component analysis, where the goal is to privatize the leadingprincipal components (PCs) of a dataset with n samples and p features. Weanalyze the exponential mechanism for this problem in a model-free setting andprovide sharp utility and privacy characterizations in the high-dimensionallimit ($p\rightarrow\infty$). Our privacy result shows that, in highdimensions, detecting the presence of a target individual in the dataset usingthe privatized PCs is exactly as hard as distinguishing two Gaussians withslightly different means, where the mean difference depends on certain spectralproperties of the dataset. Our privacy analysis combines the hypothesis-testingformulation of privacy guarantees proposed by Dong, Roth, and Su (2022) withclassical contiguity arguments due to Le Cam to obtain sharp high-dimensionalprivacy characterizations.</description><author>Youngjoo Yun, Rishabh Dudeja</author><pubDate>Mon, 10 Nov 2025 16:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07270v1</guid></item><item><title>GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering</title><link>http://arxiv.org/abs/2510.14270v3</link><description>Scene reconstruction has emerged as a central challenge in computer vision,with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splattingachieving remarkable progress. While Gaussian Splatting demonstrates strongperformance on large-scale datasets, it often struggles to capture fine detailsor maintain realism in regions with sparse coverage, largely due to theinherent limitations of sparse 3D training data. In this work, we propose GauSSmart, a hybrid method that effectively bridges2D foundational models and 3D Gaussian Splatting reconstruction. Our approachintegrates established 2D computer vision techniques, including convexfiltering and semantic feature supervision from foundational models such asDINO, to enhance Gaussian-based scene reconstruction. By leveraging 2Dsegmentation priors and high-dimensional feature embeddings, our method guidesthe densification and refinement of Gaussian splats, improving coverage inunderrepresented areas and preserving intricate structural details. We validate our approach across three datasets, where GauSSmart consistentlyoutperforms existing Gaussian Splatting in the majority of evaluated scenes.Our results demonstrate the significant potential of hybrid 2D-3D approaches,highlighting how the thoughtful combination of 2D foundational models with 3Dreconstruction pipelines can overcome the limitations inherent in eitherapproach alone.</description><author>Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang</author><pubDate>Mon, 10 Nov 2025 16:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2510.14270v3</guid></item><item><title>Beyond Detection: Exploring Evidence-based Multi-Agent Debate for Misinformation Intervention and Persuasion</title><link>http://arxiv.org/abs/2511.07267v1</link><description>Multi-agent debate (MAD) frameworks have emerged as promising approaches formisinformation detection by simulating adversarial reasoning. While prior workhas focused on detection accuracy, it overlooks the importance of helping usersunderstand the reasoning behind factual judgments and develop futureresilience. The debate transcripts generated during MAD offer a rich butunderutilized resource for transparent reasoning. In this study, we introduceED2D, an evidence-based MAD framework that extends previous approach byincorporating factual evidence retrieval. More importantly, ED2D is designednot only as a detection framework but also as a persuasive multi-agent systemaimed at correcting user beliefs and discouraging misinformation sharing. Wecompare the persuasive effects of ED2D-generated debunking transcripts withthose authored by human experts. Results demonstrate that ED2D outperformsexisting baselines across three misinformation detection benchmarks. When ED2Dgenerates correct predictions, its debunking transcripts exhibit persuasiveeffects comparable to those of human experts; However, when ED2D misclassifies,its accompanying explanations may inadvertently reinforce users'misconceptions,even when presented alongside accurate human explanations. Our findingshighlight both the promise and the potential risks of deploying MAD systems formisinformation intervention. We further develop a public community website tohelp users explore ED2D, fostering transparency, critical thinking, andcollaborative fact-checking.</description><author>Chen Han, Yijia Ma, Jin Tan, Wenzhen Zheng, Xijin Tang</author><pubDate>Mon, 10 Nov 2025 16:15:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07267v1</guid></item><item><title>AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning</title><link>http://arxiv.org/abs/2511.07262v1</link><description>Scientific Machine Learning (SciML) integrates data-driven inference withphysical modeling to solve complex problems in science and engineering.However, the design of SciML architectures, loss formulations, and trainingstrategies remains an expert-driven research process, requiring extensiveexperimentation and problem-specific insights. Here we introduce AgenticSciML,a collaborative multi-agent system in which over 10 specialized AI agentscollaborate to propose, critique, and refine SciML solutions through structuredreasoning and iterative evolution. The framework integrates structured debate,retrieval-augmented method memory, and ensemble-guided evolutionary search,enabling the agents to generate and assess new hypotheses about architecturesand optimization procedures. Across physics-informed learning and operatorlearning tasks, the framework discovers solution methods that outperformsingle-agent and human-designed baselines by up to four orders of magnitude inerror reduction. The agents produce novel strategies -- including adaptivemixture-of-expert architectures, decomposition-based PINNs, andphysics-informed operator learning models -- that do not appear explicitly inthe curated knowledge base. These results show that collaborative reasoningamong AI agents can yield emergent methodological innovation, suggesting a pathtoward scalable, transparent, and autonomous discovery in scientific computing.</description><author>Qile Jiang, George Karniadakis</author><pubDate>Mon, 10 Nov 2025 16:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07262v1</guid></item><item><title>High-dimensional Bayesian filtering through deep density approximation</title><link>http://arxiv.org/abs/2511.07261v1</link><description>In this work, we benchmark two recently developed deep density methods fornonlinear filtering. Starting from the Fokker--Planck equation with Bayesupdates, we model the filtering density of a discretely observed SDE. The twofilters: the deep splitting filter and the deep BSDE filter, are both based onFeynman--Kac formulas, Euler--Maruyama discretizations and neural networks. Thetwo methods are extended to logarithmic formulations providing sound and robustimplementations in increasing state dimension. Comparing to the classicalparticle filters and ensemble Kalman filters, we benchmark the methods onnumerous examples. In the low-dimensional examples the particle filters workwell, but when we scale up to a partially observed 100-dimensional Lorenz-96model the particle-based methods fail and the logarithmic deep density methodprevails. In terms of computational efficiency, the deep density methods reduceinference time by roughly two to five orders of magnitude relative to theparticle-based filters.</description><author>Kasper Bågmark, Filip Rydin</author><pubDate>Mon, 10 Nov 2025 16:06:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07261v1</guid></item><item><title>PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork</title><link>http://arxiv.org/abs/2511.07260v1</link><description>Ad hoc teamwork (AHT) requires agents to collaborate with previously unseenteammates, which is crucial for many real-world applications. The corechallenge of AHT is to develop an ego agent that can predict and adapt tounknown teammates on the fly. Conventional RL-based approaches optimize asingle expected return, which often causes policies to collapse into a singledominant behavior, thus failing to capture the multimodal cooperation patternsinherent in AHT. In this work, we introduce PADiff, a diffusion-based approachthat captures agent's multimodal behaviors, unlocking its diverse cooperationmodes with teammates. However, standard diffusion models lack the ability topredict and adapt in highly non-stationary AHT scenarios. To address thislimitation, we propose a novel diffusion-based policy that integrates criticalpredictive information about teammates into the denoising process. Extensiveexperiments across three cooperation environments demonstrate that PADiffoutperforms existing AHT methods significantly.</description><author>Hohei Chan, Xinzhi Zhang, Antao Xiang, Weinan Zhang, Mengchen Zhao</author><pubDate>Mon, 10 Nov 2025 16:05:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07260v1</guid></item><item><title>Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models</title><link>http://arxiv.org/abs/2511.07253v1</link><description>Large language models (LLMs) have recently achieved impressive results inspeech recognition across multiple modalities, including Auditory SpeechRecognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual SpeechRecognition (AVSR). Despite this progress, current LLM-based approachestypically address each task independently, training separate models that raisecomputational and deployment resource use while missing potential cross-tasksynergies. They also rely on fixed-rate token compression, which restrictsflexibility in balancing accuracy with efficiency. These limitations highlightthe need for a unified framework that can support ASR, VSR, and AVSR whileenabling elastic inference. To this end, we present Omni-AVSR, a unifiedaudio-visual LLM that combines efficient multi-granularity training withparameter-efficient adaptation. Specifically, we adapt the matryoshkarepresentation learning paradigm to efficiently train across multiple audio andvisual granularities, reducing its inherent training resource use. Furthermore,we explore three LoRA-based strategies for adapting the backbone LLM, balancingshared and task-specific specialization. Experiments on LRS2 and LRS3 show thatOmni-AVSR achieves comparable or superior accuracy to state-of-the-artbaselines while training a single model at substantially lower training anddeployment resource use. The model also remains robust under acoustic noise,and we analyze its scaling behavior as LLM size increases, providing insightsinto the trade-off between performance and efficiency.</description><author>Umberto Cappellazzo, Xubo Liu, Pingchuan Ma, Stavros Petridis, Maja Pantic</author><pubDate>Mon, 10 Nov 2025 16:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07253v1</guid></item><item><title>LangBridge: Interpreting Image as a Combination of Language Embeddings</title><link>http://arxiv.org/abs/2503.19404v3</link><description>Recent years have witnessed remarkable advances in Large Vision-LanguageModels (LVLMs), which have achieved human-level performance across variouscomplex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMstypically employ a shallow MLP for visual-language alignment through atwo-stage training process: pretraining for cross-modal alignment followed byinstruction tuning. While this approach has proven effective, the underlyingmechanisms of how MLPs bridge the modality gap remain poorly understood.Although some research has explored how LLMs process transformed visual tokens,few studies have investigated the fundamental alignment mechanism. Furthermore,the MLP adapter requires retraining whenever switching LLM backbones. Toaddress these limitations, we first investigate the working principles of MLPadapters and discover that they learn to project visual embeddings intosubspaces spanned by corresponding text embeddings progressively. Based on thisinsight, we propose LangBridge, a novel adapter that explicitly maps visualtokens to linear combinations of LLM vocabulary embeddings. This innovativedesign enables pretraining-free adapter transfer across different LLMs whilemaintaining performance. Our experimental results demonstrate that a LangBridgeadapter pre-trained on Qwen2-0.5B can be directly applied to larger models suchas LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall,LangBridge enables interpretable vision-language alignment by grounding visualrepresentations in LLM vocab embedding, while its plug-and-play design ensuresefficient reuse across multiple LLMs with nearly no performance degradation.See our project page at https://curryx-001.github.io/LangBridge.github.io/</description><author>Jiaqi Liao, Yuwei Niu, Fanqing Meng, Hao Li, Changyao Tian, Yinuo Du, Yuwen Xiong, Dianqi Li, Xizhou Zhu, Li Yuan, Jifeng Dai, Yu Cheng</author><pubDate>Mon, 10 Nov 2025 16:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.19404v3</guid></item><item><title>MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs</title><link>http://arxiv.org/abs/2511.07250v1</link><description>The advent of Multimodal Large Language Models (MLLMs) has expanded AIcapabilities to visual modalities, yet existing evaluation benchmarks remainlimited to single-video understanding, overlooking the critical need formulti-video understanding in real-world scenarios (e.g., sports analytics andautonomous driving). To address this significant gap, we introduce MVU-Eval,the first comprehensive benchmark for evaluating Multi-Video Understanding forMLLMs. Specifically, our MVU-Eval mainly assesses eight core competenciesthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videosfrom diverse domains, addressing both fundamental perception tasks andhigh-order reasoning tasks. These capabilities are rigorously aligned withreal-world applications such as multi-sensor synthesis in autonomous systemsand cross-angle sports analytics. Through extensive evaluation ofstate-of-the-art open-source and closed-source models, we reveal significantperformance discrepancies and limitations in current MLLMs' ability to performunderstanding across multiple videos. The benchmark will be made publiclyavailable to foster future research.</description><author>Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Ge Zhang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Wenhao Huang, Zhaoxiang Zhang, Jiaheng Liu</author><pubDate>Mon, 10 Nov 2025 16:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2511.07250v1</guid></item></channel></rss>