<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 14 Nov 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2311.07575v1</link><description>We present SPHINX, a versatile multi-modal large language model (MLLM) with ajoint mixing of model weights, tuning tasks, and visual embeddings. First, forstronger vision-language alignment, we unfreeze the large language model (LLM)during pre-training, and introduce a weight mix strategy between LLMs trainedby real-world and synthetic data. By directly integrating the weights from twodomains, the mixed LLM can efficiently incorporate diverse semantics withfavorable robustness. Then, to enable multi-purpose capabilities, we mix avariety of tasks for joint visual instruction tuning, and design task-specificinstructions to avoid inter-task conflict. In addition to the basic visualquestion answering, we include more challenging tasks such as region-levelunderstanding, caption grounding, document layout detection, and human poseestimation, contributing to mutual enhancement over different scenarios.Additionally, we propose to extract comprehensive visual embeddings fromvarious network architectures, pre-training paradigms, and informationgranularity, providing language models with more robust image representations.Based on our proposed joint mixing, SPHINX exhibits superior multi-modalunderstanding capabilities on a wide range of applications. On top of this, wefurther propose an efficient strategy aiming to better capture fine-grainedappearances of high-resolution images. With a mixing of different scales andhigh-resolution sub-images, SPHINX attains exceptional visual parsing andreasoning performance on existing evaluation benchmarks. We hope our work maycast a light on the exploration of joint mixing in future MLLM research. Codeis released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.</description><author>Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Qiao</author><pubDate>Mon, 13 Nov 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07575v1</guid></item><item><title>To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning</title><link>http://arxiv.org/abs/2311.07574v1</link><description>Existing visual instruction tuning methods typically prompt large languagemodels with textual descriptions to generate instruction-following data.Despite the promising performance achieved, these descriptions are derived fromimage annotations, which are oftentimes coarse-grained. Furthermore, theinstructions might even contradict the visual content without observing theentire visual context. To address this challenge, we introduce a fine-grainedvisual instruction dataset, LVIS-Instruct4V, which contains 220K visuallyaligned and context-aware instructions produced by prompting the powerfulGPT-4V with images from LVIS. Through experimental validation and case studies,we demonstrate that high-quality visual instructional data could improve theperformance of LLaVA-1.5, a state-of-the-art large multimodal model, across awide spectrum of benchmarks by clear margins. Notably, by simply replacing theLLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVAon most challenging LMM benchmarks, e.g., LLaVA$^w$ (76.7 vs. 70.7) and MM-Vet(40.2 vs. 35.4). We release our data and model athttps://github.com/X2FD/LVIS-INSTRUCT4V.</description><author>Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Mon, 13 Nov 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07574v1</guid></item><item><title>Feature emergence via margin maximization: case studies in algebraic tasks</title><link>http://arxiv.org/abs/2311.07568v1</link><description>Understanding the internal representations learned by neural networks is acornerstone challenge in the science of machine learning. While there have beensignificant recent strides in some cases towards understanding how neuralnetworks implement specific target functions, this paper explores acomplementary question -- why do networks arrive at particular computationalstrategies? Our inquiry focuses on the algebraic learning tasks of modularaddition, sparse parities, and finite group operations. Our primary theoreticalfindings analytically characterize the features learned by stylized neuralnetworks for these algebraic tasks. Notably, our main technique demonstrateshow the principle of margin maximization alone can be used to fully specify thefeatures learned by the network. Specifically, we prove that the trainednetworks utilize Fourier features to perform modular addition and employfeatures corresponding to irreducible group-theoretic representations toperform compositions in general groups, aligning closely with the empiricalobservations of Nanda et al. and Chughtai et al. More generally, we hope ourtechniques can help to foster a deeper understanding of why neural networksadopt specific computational strategies.</description><author>Depen Morwani, Benjamin L. Edelman, Costin-Andrei Oncescu, Rosie Zhao, Sham Kakade</author><pubDate>Mon, 13 Nov 2023 18:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07568v1</guid></item><item><title>Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets</title><link>http://arxiv.org/abs/2308.13488v2</link><description>Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) isa widely used modality for diagnosing myocardial blood flow (perfusion)abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300time-resolved images of myocardial perfusion are acquired at various contrast"wash in/out" phases. Manual segmentation of myocardial contours in eachtime-frame of a DCE image series can be tedious and time-consuming,particularly when non-rigid motion correction has failed or is unavailable.While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRIdatasets, a "dynamic quality control" (dQC) technique for reliably detectingfailed segmentations is lacking. Here we propose a new space-time uncertaintymetric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRIdatasets by validating the proposed metric on an external dataset andestablishing a human-in-the-loop framework to improve the segmentation results.In the proposed approach, we referred the top 10% most uncertain segmentationsas detected by our dQC tool to the human expert for refinement. This approachresulted in a significant increase in the Dice score (p&lt;0.001) and a notabledecrease in the number of images with failed segmentation (16.2% to 11.3%)whereas the alternative approach of randomly selecting the same number ofsegmentations for human referral did not achieve any significant improvement.Our results suggest that the proposed dQC framework has the potential toaccurately identify poor-quality segmentations and may enable efficientDNN-based analysis of DCE-CMRI in a human-in-the-loop pipeline for clinicalinterpretation and reporting of dynamic CMRI datasets.</description><author>Dilek M. Yalcinkaya, Khalid Youssef, Bobak Heydari, Orlando Simonetti, Rohan Dharmakumar, Subha Raman, Behzad Sharif</author><pubDate>Mon, 13 Nov 2023 18:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13488v2</guid></item><item><title>Exploration via linearly perturbed loss minimisation</title><link>http://arxiv.org/abs/2311.07565v1</link><description>We introduce exploration via linear loss perturbations (EVILL), a randomisedexploration method for structured stochastic bandit problems that works bysolving for the minimiser of a linearly perturbed regularised negativelog-likelihood function. We show that, for the case of generalised linearbandits, EVILL reduces to perturbed history exploration (PHE), a method whereexploration is done by training on randomly perturbed rewards. In doing so, weprovide a simple and clean explanation of when and why random rewardperturbations give rise to good bandit algorithms. With the data-dependentperturbations we propose, not present in previous PHE-type methods, EVILL isshown to match the performance of Thompson-sampling-styleparameter-perturbation methods, both in theory and in practice. Moreover, weshow an example outside of generalised linear bandits where PHE leads toinconsistent estimates, and thus linear regret, while EVILL remains performant.Like PHE, EVILL can be implemented in just a few lines of code.</description><author>David Janz, Shuai Liu, Alex Ayoub, Csaba Szepesvári</author><pubDate>Mon, 13 Nov 2023 18:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07565v1</guid></item><item><title>Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?</title><link>http://arxiv.org/abs/2311.07564v1</link><description>Authorship verification is the problem of determining if two distinct writingsamples share the same author and is typically concerned with the attributionof written text. In this paper, we explore the attribution of transcribedspeech, which poses novel challenges. The main challenge is that many stylisticfeatures, such as punctuation and capitalization, are not available orreliable. Therefore, we expect a priori that transcribed speech is a morechallenging domain for attribution. On the other hand, other stylisticfeatures, such as speech disfluencies, may enable more successful attributionbut, being specific to speech, require special purpose models. To betterunderstand the challenges of this setting, we contribute the first systematicstudy of speaker attribution based solely on transcribed speech. Specifically,we propose a new benchmark for speaker attribution focused on conversationalspeech transcripts. To control for spurious associations of speakers withtopic, we employ both conversation prompts and speakers' participating in thesame conversation to construct challenging verification trials of varyingdifficulties. We establish the state of the art on this new benchmark bycomparing a suite of neural and non-neural baselines, finding that althoughwritten text attribution models achieve surprisingly good performance incertain settings, they struggle in the hardest settings we consider.</description><author>Cristina Aggazzotti, Nicholas Andrews, Elizabeth Allyn Smith</author><pubDate>Mon, 13 Nov 2023 18:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07564v1</guid></item><item><title>Learning Control Policies of Hodgkin-Huxley Neuronal Dynamics</title><link>http://arxiv.org/abs/2311.07563v1</link><description>We present a neural network approach for closed-loop deep brain stimulation(DBS). We cast the problem of finding an optimal neurostimulation strategy as acontrol problem. In this setting, control policies aim to optimize therapeuticoutcomes by tailoring the parameters of a DBS system, typically via electricalstimulation, in real time based on the patient's ongoing neuronal activity. Weapproximate the value function offline using a neural network to enablegenerating controls (stimuli) in real time via the feedback form. The neuronalactivity is characterized by a nonlinear, stiff system of differentialequations as dictated by the Hodgkin-Huxley model. Our training processleverages the relationship between Pontryagin's maximum principle andHamilton-Jacobi-Bellman equations to update the value function estimatessimultaneously. Our numerical experiments illustrate the accuracy of ourapproach for out-of-distribution samples and the robustness to moderate shocksand disturbances in the system.</description><author>Malvern Madondo, Deepanshu Verma, Lars Ruthotto, Nicholas Au Yong</author><pubDate>Mon, 13 Nov 2023 18:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07563v1</guid></item><item><title>GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation</title><link>http://arxiv.org/abs/2311.07562v1</link><description>We present MM-Navigator, a GPT-4V-based agent for the smartphone graphicaluser interface (GUI) navigation task. MM-Navigator can interact with asmartphone screen as human users, and determine subsequent actions to fulfillgiven instructions. Our findings demonstrate that large multimodal models(LMMs), specifically GPT-4V, excel in zero-shot GUI navigation through itsadvanced screen interpretation, action reasoning, and precise actionlocalization capabilities. We first benchmark MM-Navigator on our collected iOSscreen dataset. According to human assessments, the system exhibited a 91\%accuracy rate in generating reasonable action descriptions and a 75\% accuracyrate in executing the correct actions for single-step instructions on iOS.Additionally, we evaluate the model on a subset of an Android screen navigationdataset, where the model outperforms previous GUI navigators in a zero-shotfashion. Our benchmark and detailed analyses aim to lay a robust groundwork forfuture research into the GUI navigation task. The project page is athttps://github.com/zzxslp/MM-Navigator.</description><author>An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, Lijuan Wang</author><pubDate>Mon, 13 Nov 2023 18:53:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07562v1</guid></item><item><title>Fast Normalized Cross-Correlation for Template Matching with Rotations</title><link>http://arxiv.org/abs/2311.07561v1</link><description>Normalized cross-correlation is the reference approach to carry out templatematching on images. When it is computed in Fourier space, it can handleefficiently template translations but it cannot do so with template rotations.Including rotations requires sampling the whole space of rotations, repeatingthe computation of the correlation each time. This article develops an alternative mathematical theory to handleefficiently, at the same time, rotations and translations. Our proposal has areduced computational complexity because it does not require to repeatedlysample the space of rotations. To do so, we integrate the information relativeto all rotated versions of the template into a unique symmetric tensor template-which is computed only once per template-. Afterward, we demonstrate that thecorrelation between the image to be processed with the independent tensorcomponents of the tensorial template contains enough information to recovertemplate instance positions and rotations. Our proposed method has the potential to speed up conventional templatematching computations by a factor of several magnitude orders for the case of3D images.</description><author>José María Almira, Harold Phelippeau, Antonio Martinez-Sanchez</author><pubDate>Mon, 13 Nov 2023 18:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07561v1</guid></item><item><title>Data-Efficient Task Generalization via Probabilistic Model-based Meta Reinforcement Learning</title><link>http://arxiv.org/abs/2311.07558v1</link><description>We introduce PACOH-RL, a novel model-based Meta-Reinforcement Learning(Meta-RL) algorithm designed to efficiently adapt control policies to changingdynamics. PACOH-RL meta-learns priors for the dynamics model, allowing swiftadaptation to new dynamics with minimal interaction data. Existing Meta-RLmethods require abundant meta-learning data, limiting their applicability insettings such as robotics, where data is costly to obtain. To address this,PACOH-RL incorporates regularization and epistemic uncertainty quantificationin both the meta-learning and task adaptation stages. When facing new dynamics,we use these uncertainty estimates to effectively guide exploration and datacollection. Overall, this enables positive transfer, even when access to datafrom prior tasks or dynamic settings is severely limited. Our experimentresults demonstrate that PACOH-RL outperforms model-based RL and model-basedMeta-RL baselines in adapting to new dynamic conditions. Finally, on a realrobotic car, we showcase the potential for efficient RL policy adaptation indiverse, data-scarce conditions.</description><author>Arjun Bhardwaj, Jonas Rothfuss, Bhavya Sukhija, Yarden As, Marco Hutter, Stelian Coros, Andreas Krause</author><pubDate>Mon, 13 Nov 2023 18:51:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07558v1</guid></item><item><title>Learning to Generate Better Than Your LLM</title><link>http://arxiv.org/abs/2306.11816v2</link><description>Reinforcement learning (RL) has emerged as a powerful paradigm forfine-tuning Large Language Models (LLMs) for text generation. In particular,recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations withusers after finetuning with RL. Capitalizing on key properties of textgeneration, we seek to investigate RL algorithms beyond general purposealgorithms like Proximal Policy Optimization (PPO). In particular, we extend RLalgorithms to allow them to interact with a dynamic black-box guide LLM andpropose RL with guided feedback (RLGF), a suite of RL algorithms for LLMfine-tuning. We provide two ways for the guide LLM to interact with the LLM tobe optimized for maximizing rewards. The guide LLM can generate text whichserves as additional starting states for the RL optimization procedure. Theguide LLM can also be used to complete the partial sentences generated by theLLM that is being optimized, treating the guide LLM as an expert to imitate andsurpass eventually. We experiment on the IMDB positive sentiment, CommonGen,and TL;DR summarization tasks. We show that our RL algorithms achieve higherperformance than supervised learning (SL) and the RL baseline PPO,demonstrating the benefit of interaction with the guide LLM. On both CommonGenand TL;DR, we not only outperform our SL baselines but also improve upon PPOacross a variety of metrics beyond the one we optimized for. Our code can befound at https://github.com/Cornell-RL/tril.</description><author>Jonathan D. Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, Wen Sun</author><pubDate>Mon, 13 Nov 2023 18:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11816v2</guid></item><item><title>Using Natural Language Explanations to Improve Robustness of In-context Learning for Natural Language Inference</title><link>http://arxiv.org/abs/2311.07556v1</link><description>Recent studies have demonstrated that large language models (LLMs) excel indiverse tasks through in-context learning (ICL) facilitated by task-specificprompts and examples. However, the existing literature shows that ICLencounters performance deterioration when exposed to adversarial inputs.Enhanced performance has been observed when ICL is augmented with naturallanguage explanations (NLEs) (we refer to it as X-ICL). Thus, this workinvestigates whether X-ICL can improve the robustness of LLMs on a suite ofseven adversarial and challenging natural language inference datasets.Moreover, we introduce a new approach to X-ICL by prompting an LLM (ChatGPT inour case) with few human-generated NLEs to produce further NLEs (we call itChatGPT few-shot), which we show superior to both ChatGPT zero-shot andhuman-generated NLEs alone. We evaluate five popular LLMs (GPT3.5-turbo,LLaMa2, Vicuna, Zephyr, Mistral) and show that X-ICL with ChatGPT few-shotyields over 6% improvement over ICL. Furthermore, while prompt selectionstrategies were previously shown to significantly improve ICL onin-distribution test sets, we show that these strategies do not match theefficacy of the X-ICL paradigm in robustness-oriented evaluations.</description><author>Xuanli He, Yuxiang Wu, Oana-Maria Camburu, Pasquale Minervini, Pontus Stenetorp</author><pubDate>Mon, 13 Nov 2023 18:49:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07556v1</guid></item><item><title>An Extensive Study on Adversarial Attack against Pre-trained Models of Code</title><link>http://arxiv.org/abs/2311.07553v1</link><description>Transformer-based pre-trained models of code (PTMC) have been widely utilizedand have achieved state-of-the-art performance in many mission-criticalapplications. However, they can be vulnerable to adversarial attacks throughidentifier substitution or coding style transformation, which can significantlydegrade accuracy and may further incur security concerns. Although severalapproaches have been proposed to generate adversarial examples for PTMC, theeffectiveness and efficiency of such approaches, especially on different codeintelligence tasks, has not been well understood. To bridge this gap, thisstudy systematically analyzes five state-of-the-art adversarial attackapproaches from three perspectives: effectiveness, efficiency, and the qualityof generated examples. The results show that none of the five approachesbalances all these perspectives. Particularly, approaches with a high attacksuccess rate tend to be time-consuming; the adversarial code they generateoften lack naturalness, and vice versa. To address this limitation, we explorethe impact of perturbing identifiers under different contexts and find thatidentifier substitution within for and if statements is the most effective.Based on these findings, we propose a new approach that prioritizes differenttypes of statements for various tasks and further utilizes beam search togenerate adversarial examples. Evaluation results show that it outperforms thestate-of-the-art ALERT in terms of both effectiveness and efficiency whilepreserving the naturalness of the generated adversarial examples.</description><author>Xiaohu Du, Ming Wen, Zichao Wei, Shangwen Wang, Hai Jin</author><pubDate>Mon, 13 Nov 2023 18:48:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07553v1</guid></item><item><title>Ghostbuster: Detecting Text Ghostwritten by Large Language Models</title><link>http://arxiv.org/abs/2305.15047v2</link><description>We introduce Ghostbuster, a state-of-the-art system for detectingAI-generated text. Our method works by passing documents through a series ofweaker language models, running a structured search over possible combinationsof their features, and then training a classifier on the selected features topredict whether documents are AI-generated. Crucially, Ghostbuster does notrequire access to token probabilities from the target model, making it usefulfor detecting text generated by black-box models or unknown model versions. Inconjunction with our model, we release three new datasets of human- andAI-generated text as detection benchmarks in the domains of student essays,creative writing, and news articles. We compare Ghostbuster to a variety ofexisting detectors, including DetectGPT and GPTZero, as well as a new RoBERTabaseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is5.9 F1 higher than the best preexisting model. It also outperforms all previousapproaches in generalization across writing domains (+7.5 F1), promptingstrategies (+2.1 F1), and language models (+4.4 F1). We also analyze therobustness of our system to a variety of perturbations and paraphrasing attacksand evaluate its performance on documents written by non-native Englishspeakers.</description><author>Vivek Verma, Eve Fleisig, Nicholas Tomlin, Dan Klein</author><pubDate>Mon, 13 Nov 2023 18:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15047v2</guid></item><item><title>RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation</title><link>http://arxiv.org/abs/2311.01455v2</link><description>We present RoboGen, a generative robotic agent that automatically learnsdiverse robotic skills at scale via generative simulation. RoboGen leveragesthe latest advancements in foundation and generative models. Instead ofdirectly using or adapting these models to produce policies or low-levelactions, we advocate for a generative scheme, which uses these models toautomatically generate diversified tasks, scenes, and training supervisions,thereby scaling up robotic skill learning with minimal human supervision. Ourapproach equips a robotic agent with a self-guided propose-generate-learncycle: the agent first proposes interesting tasks and skills to develop, andthen generates corresponding simulation environments by populating pertinentobjects and assets with proper spatial configurations. Afterwards, the agentdecomposes the proposed high-level task into sub-tasks, selects the optimallearning approach (reinforcement learning, motion planning, or trajectoryoptimization), generates required training supervision, and then learnspolicies to acquire the proposed skill. Our work attempts to extract theextensive and versatile knowledge embedded in large-scale models and transferthem to the field of robotics. Our fully generative pipeline can be queriedrepeatedly, producing an endless stream of skill demonstrations associated withdiverse tasks and environments.</description><author>Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Zackory Erickson, David Held, Chuang Gan</author><pubDate>Mon, 13 Nov 2023 18:40:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01455v2</guid></item><item><title>Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data</title><link>http://arxiv.org/abs/2311.07550v1</link><description>Deep neural networks (DNNs) have shown great promise in various domains.Alongside these developments, vulnerabilities associated with DNN training,such as backdoor attacks, are a significant concern. These attacks involve thesubtle insertion of triggers during model training, allowing for manipulatedpredictions. More recently, DNNs for tabular data have gained increasingattention due to the rise of transformer models. Our research presents a comprehensive analysis of backdoor attacks on tabulardata using DNNs, particularly focusing on transformer-based networks. Given theinherent complexities of tabular data, we explore the challenges of embeddingbackdoors. Through systematic experimentation across benchmark datasets, weuncover that transformer-based DNNs for tabular data are highly susceptible tobackdoor attacks, even with minimal feature value alterations. Our resultsindicate nearly perfect attack success rates (approx100%) by introducing novelbackdoor attack strategies to tabular data. Furthermore, we evaluate severaldefenses against these attacks, identifying Spectral Signatures as the mosteffective one. Our findings highlight the urgency to address suchvulnerabilities and provide insights into potential countermeasures forsecuring DNN models against backdoors on tabular data.</description><author>Bart Pleiter, Behrad Tajalli, Stefanos Koffas, Gorka Abad, Jing Xu, Martha Larson, Stjepan Picek</author><pubDate>Mon, 13 Nov 2023 18:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07550v1</guid></item><item><title>Interpretable Fine-Tuning for Graph Neural Network Surrogate Models</title><link>http://arxiv.org/abs/2311.07548v1</link><description>Data-based surrogate modeling has surged in capability in recent years withthe emergence of graph neural networks (GNNs), which can operate directly onmesh-based representations of data. The goal of this work is to introduce aninterpretable fine-tuning strategy for GNNs, with application to unstructuredmesh-based fluid dynamics modeling. The end result is a fine-tuned GNN thatadds interpretability to a pre-trained baseline GNN through an adaptivesub-graph sampling strategy that isolates regions in physical spaceintrinsically linked to the forecasting task, while retaining the predictivecapability of the baseline. The structures identified by the fine-tuned GNNs,which are adaptively produced in the forward pass as explicit functions of theinput, serve as an accessible link between the baseline model architecture, theoptimization goal, and known problem-specific physics. Additionally, through aregularization procedure, the fine-tuned GNNs can also be used to identify,during inference, graph nodes that correspond to a majority of the anticipatedforecasting error, adding a novel interpretable error-tagging capability tobaseline models. Demonstrations are performed using unstructured flow datasourced from flow over a backward-facing step at high Reynolds numbers.</description><author>Shivam Barwey, Romit Maulik</author><pubDate>Mon, 13 Nov 2023 18:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07548v1</guid></item><item><title>GPT-4V(ision) as A Social Media Analysis Engine</title><link>http://arxiv.org/abs/2311.07547v1</link><description>Recent research has offered insights into the extraordinary capabilities ofLarge Multimodal Models (LMMs) in various general vision and language tasks.There is growing interest in how LMMs perform in more specialized domains.Social media content, inherently multimodal, blends text, images, videos, andsometimes audio. Understanding social multimedia content remains a challengingproblem for contemporary machine learning frameworks. In this paper, we exploreGPT-4V(ision)'s capabilities for social multimedia analysis. We select fiverepresentative tasks, including sentiment analysis, hate speech detection, fakenews identification, demographic inference, and political ideology detection,to evaluate GPT-4V. Our investigation begins with a preliminary quantitativeanalysis for each task using existing benchmark datasets, followed by a carefulreview of the results and a selection of qualitative samples that illustrateGPT-4V's potential in understanding multimodal social media content. GPT-4Vdemonstrates remarkable efficacy in these tasks, showcasing strengths such asjoint understanding of image-text pairs, contextual and cultural awareness, andextensive commonsense knowledge. Despite the overall impressive capacity ofGPT-4V in the social media domain, there remain notable challenges. GPT-4Vstruggles with tasks involving multilingual social multimedia comprehension andhas difficulties in generalizing to the latest trends in social media.Additionally, it exhibits a tendency to generate erroneous information in thecontext of evolving celebrity and politician knowledge, reflecting the knownhallucination problem. The insights gleaned from our findings underscore apromising future for LMMs in enhancing our comprehension of social mediacontent and its users through the analysis of multimodal information.</description><author>Hanjia Lyu, Jinfa Huang, Daoan Zhang, Yongsheng Yu, Xinyi Mou, Jinsheng Pan, Zhengyuan Yang, Zhongyu Wei, Jiebo Luo</author><pubDate>Mon, 13 Nov 2023 18:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07547v1</guid></item><item><title>Simplicial Models for the Epistemic Logic of Faulty Agents</title><link>http://arxiv.org/abs/2311.01351v2</link><description>In recent years, several authors have been investigating simplicial models, amodel of epistemic logic based on higher-dimensional structures calledsimplicial complexes. In the original formulation, simplicial models werealways assumed to be pure, meaning that all worlds have the same dimension.This is equivalent to the standard S5n semantics of epistemic logic, based onKripke models. By removing the assumption that models must be pure, we can gobeyond the usual Kripke semantics and study epistemic logics where the numberof agents participating in a world can vary. This approach has been developedin a number of papers, with applications in fault-tolerant distributedcomputing where processes may crash during the execution of a system. Adifficulty that arises is that subtle design choices in the definition ofimpure simplicial models can result in different axioms of the resulting logic.In this paper, we classify those design choices systematically, and axiomatizethe corresponding logics. We illustrate them via distributed computing examplesof synchronous systems where processes may crash.</description><author>Eric Goubault, Roman Kniazev, Jeremy Ledent, Sergio Rajsbaum</author><pubDate>Mon, 13 Nov 2023 18:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01351v2</guid></item><item><title>mlscorecheck: Testing the consistency of reported performance scores and experiments in machine learning</title><link>http://arxiv.org/abs/2311.07541v1</link><description>Addressing the reproducibility crisis in artificial intelligence through thevalidation of reported experimental results is a challenging task. Itnecessitates either the reimplementation of techniques or a meticulousassessment of papers for deviations from the scientific method and beststatistical practices. To facilitate the validation of reported results, wehave developed numerical techniques capable of identifying inconsistenciesbetween reported performance scores and various experimental setups in machinelearning problems, including binary/multiclass classification and regression.These consistency tests are integrated into the open-source packagemlscorecheck, which also provides specific test bundles designed to detectsystematically recurring flaws in various fields, such as retina imageprocessing and synthetic minority oversampling.</description><author>György Kovács, Attila Fazekas</author><pubDate>Mon, 13 Nov 2023 18:31:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07541v1</guid></item><item><title>Leveraging Multiple Teachers for Test-Time Adaptation of Language-Guided Classifiers</title><link>http://arxiv.org/abs/2311.07538v1</link><description>Recent approaches have explored language-guided classifiers capable ofclassifying examples from novel tasks when provided with task-specific naturallanguage explanations, instructions or prompts (Sanh et al., 2022; R. Menon etal., 2022). While these classifiers can generalize in zero-shot settings, theirtask performance often varies substantially between different languageexplanations in unpredictable ways (Lu et al., 2022; Gonen et al., 2022). Also,current approaches fail to leverage unlabeled examples that may be available inmany scenarios. Here, we introduce TALC, a framework that uses data programmingto adapt a language-guided classifier for a new task during inference whenprovided with explanations from multiple teachers and unlabeled test examples.Our results show that TALC consistently outperforms a competitive baseline fromprior work by an impressive 9.3% (relative improvement). Further, wedemonstrate the robustness of TALC to variations in the quality and quantity ofprovided explanations, highlighting its potential in scenarios where learningfrom multiple teachers or a crowd is involved. Our code is available at:https://github.com/WeiKangda/TALC.git.</description><author>Kangda Wei, Sayan Ghosh, Rakesh R. Menon, Shashank Srivastava</author><pubDate>Mon, 13 Nov 2023 18:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07538v1</guid></item><item><title>Goal-Driven Explainable Clustering via Language Descriptions</title><link>http://arxiv.org/abs/2305.13749v2</link><description>Unsupervised clustering is widely used to explore large corpora, but existingformulations neither consider the users' goals nor explain clusters' meanings.We propose a new task formulation, "Goal-Driven Clustering with Explanations"(GoalEx), which represents both the goal and the explanations as free-formlanguage descriptions. For example, to categorize the errors made by asummarization system, the input to GoalEx is a corpus of annotator-writtencomments for system-generated summaries and a goal description "cluster thecomments based on why the annotators think the summary is imperfect.''; theoutputs are text clusters each with an explanation ("this cluster mentions thatthe summary misses important context information."), which relates to the goaland precisely explain which comments should (not) belong to a cluster. Totackle GoalEx, we prompt a language model with "[corpus subset] + [goal] +Brainstorm a list of explanations each representing a cluster."; then weclassify whether each sample belongs to a cluster based on its explanation;finally, we use integer linear programming to select a subset of candidateclusters to cover most samples while minimizing overlaps. Under both automaticand human evaluation on corpora with or without labels, our method producesmore accurate and goal-related explanations than prior methods. We release ourdata and implementation at https://github.com/ZihanWangKi/GoalEx.</description><author>Zihan Wang, Jingbo Shang, Ruiqi Zhong</author><pubDate>Mon, 13 Nov 2023 18:27:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13749v2</guid></item><item><title>Deep Learning in Cardiology</title><link>http://arxiv.org/abs/1902.11122v4</link><description>The medical field is creating large amount of data that physicians are unableto decipher and use efficiently. Moreover, rule-based expert systems areinefficient in solving complicated medical tasks or for creating insights usingbig data. Deep learning has emerged as a more accurate and effective technologyin a wide range of medical problems such as diagnosis, prediction andintervention. Deep learning is a representation learning method that consistsof layers that transform the data non-linearly, thus, revealing hierarchicalrelationships and structures. In this review we survey deep learningapplication papers that use structured data, signal and imaging modalities fromcardiology. We discuss the advantages and limitations of applying deep learningin cardiology that also apply in medicine in general, while proposing certaindirections as the most viable for clinical use.</description><author>Paschalis Bizopoulos, Dimitrios Koutsouris</author><pubDate>Mon, 13 Nov 2023 18:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1902.11122v4</guid></item><item><title>SciRepEval: A Multi-Format Benchmark for Scientific Document Representations</title><link>http://arxiv.org/abs/2211.13308v4</link><description>Learned representations of scientific documents can serve as valuable inputfeatures for downstream tasks without further fine-tuning. However, existingbenchmarks for evaluating these representations fail to capture the diversityof relevant tasks. In response, we introduce SciRepEval, the firstcomprehensive benchmark for training and evaluating scientific documentrepresentations. It includes 24 challenging and realistic tasks, 8 of which arenew, across four formats: classification, regression, ranking and search. Wethen use this benchmark to study and improve the generalization ability ofscientific document representation models. We show how state-of-the-art modelslike SPECTER and SciNCL struggle to generalize across the task formats, andthat simple multi-task training fails to improve them. However, a new approachthat learns multiple embeddings per document, each tailored to a differentformat, can improve performance. We experiment with task-format-specificcontrol codes and adapters and find they outperform the existingsingle-embedding state-of-the-art by over 2 points absolute. We release theresulting family of multi-format models, called SPECTER2, for the community touse and build on.</description><author>Amanpreet Singh, Mike D'Arcy, Arman Cohan, Doug Downey, Sergey Feldman</author><pubDate>Mon, 13 Nov 2023 18:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13308v4</guid></item><item><title>Byte Pair Encoding for Symbolic Music</title><link>http://arxiv.org/abs/2301.11975v3</link><description>When used with deep learning, the symbolic music modality is often coupledwith language model architectures. To do so, the music needs to be tokenized,i.e. converted into a sequence of discrete tokens. This can be achieved bydifferent approaches, as music can be composed of simultaneous tracks, ofsimultaneous notes with several attributes. Until now, the proposedtokenizations rely on small vocabularies of tokens describing the noteattributes and time events, resulting in fairly long token sequences, and asub-optimal use of the embedding space of language models. Recent research hasput efforts on reducing the overall sequence length by merging embeddings orcombining tokens. In this paper, we show that Byte Pair Encoding, a compressiontechnique widely used for natural language, significantly decreases thesequence length while increasing the vocabulary size. By doing so, we leveragethe embedding capabilities of such models with more expressive tokens,resulting in both better results and faster inference in generation andclassification tasks. The source code is shared on Github, along with acompanion website. Finally, BPE is directly implemented in MidiTok, allowingthe reader to easily benefit from this method.</description><author>Nathan Fradet, Nicolas Gutowski, Fabien Chhel, Jean-Pierre Briot</author><pubDate>Mon, 13 Nov 2023 18:24:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11975v3</guid></item><item><title>Towards Last-layer Retraining for Group Robustness with Fewer Annotations</title><link>http://arxiv.org/abs/2309.08534v2</link><description>Empirical risk minimization (ERM) of neural networks is prone toover-reliance on spurious correlations and poor generalization on minoritygroups. The recent deep feature reweighting (DFR) technique achievesstate-of-the-art group robustness via simple last-layer retraining, but itrequires held-out group and class annotations to construct a group-balancedreweighting dataset. In this work, we examine this impractical requirement andfind that last-layer retraining can be surprisingly effective with no groupannotations (other than for model selection) and only a handful of classannotations. We first show that last-layer retraining can greatly improveworst-group accuracy even when the reweighting dataset has only a smallproportion of worst-group data. This implies a "free lunch" where holding out asubset of training data to retrain the last layer can substantially outperformERM on the entire dataset with no additional data or annotations. To furtherimprove group robustness, we introduce a lightweight method called selectivelast-layer finetuning (SELF), which constructs the reweighting dataset usingmisclassifications or disagreements. Our empirical and theoretical resultspresent the first evidence that model disagreement upsamples worst-group data,enabling SELF to nearly match DFR on four well-established benchmarks acrossvision and language tasks with no group annotations and less than 3% of theheld-out class annotations. Our code is available athttps://github.com/tmlabonte/last-layer-retraining.</description><author>Tyler LaBonte, Vidya Muthukumar, Abhishek Kumar</author><pubDate>Mon, 13 Nov 2023 18:24:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08534v2</guid></item><item><title>Estimating optical vegetation indices with Sentinel-1 SAR data and AutoML</title><link>http://arxiv.org/abs/2311.07537v1</link><description>Current optical vegetation indices (VIs) for monitoring forest ecosystems arewidely used in various applications. However, continuous monitoring based onoptical satellite data can be hampered by atmospheric effects such as clouds.On the contrary, synthetic aperture radar (SAR) data can offer insightful andsystematic forest monitoring with complete time series due to signalpenetration through clouds and day and night acquisitions. The goal of thiswork is to overcome the issues affecting optical data with SAR data and serveas a substitute for estimating optical VIs for forests using machine learning.Time series of four VIs (LAI, FAPAR, EVI and NDVI) were estimated usingmultitemporal Sentinel-1 SAR and ancillary data. This was enabled by creating apaired multi-temporal and multi-modal dataset in Google Earth Engine (GEE),including temporally and spatially aligned Sentinel-1, Sentinel-2, digitalelevation model (DEM), weather and land cover datasets (MMT-GEE). The use ofancillary features generated from DEM and weather data improved the results.The open-source Automatic Machine Learning (AutoML) approach, auto-sklearn,outperformed Random Forest Regression for three out of four VIs, while a 1-houroptimization length was enough to achieve sufficient results with an R2 of69-84% low errors (0.05-0.32 of MAE depending on VI). Great agreement was alsofound for selected case studies in the time series analysis and in the spatialcomparison between the original and estimated SAR-based VIs. In general,compared to VIs from currently freely available optical satellite data andavailable global VI products, a better temporal resolution (up to 240measurements/year) and a better spatial resolution (20 m) were achieved usingestimated SAR-based VIs. A great advantage of the SAR-based VI is the abilityto detect abrupt forest changes with a sub-weekly temporal accuracy.</description><author>Daniel Paluba, Bertrand Le Saux, Francesco Sarti, Přemysl Stych</author><pubDate>Mon, 13 Nov 2023 18:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07537v1</guid></item><item><title>A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering</title><link>http://arxiv.org/abs/2311.07536v1</link><description>The emergence of multimodal large models (MLMs) has significantly advancedthe field of visual understanding, offering remarkable capabilities in therealm of visual question answering (VQA). Yet, the true challenge lies in thedomain of knowledge-intensive VQA tasks, which necessitate not just recognitionof visual elements, but also a deep comprehension of the visual information inconjunction with a vast repository of learned knowledge. To uncover suchcapabilities of MLMs, particularly the newly introduced GPT-4V, we provide anin-depth evaluation from three perspectives: 1) Commonsense Knowledge, whichassesses how well models can understand visual cues and connect to generalknowledge; 2) Fine-grained World Knowledge, which tests the model's skill inreasoning out specific knowledge from images, showcasing their proficiencyacross various specialized fields; 3) Comprehensive Knowledge withDecision-making Rationales, which examines model's capability to providelogical explanations for its inference, facilitating a deeper analysis from theinterpretability perspective. Extensive experiments indicate that GPT-4Vachieves SOTA performance on above three tasks. Interestingly, we find that: a)GPT-4V demonstrates enhanced reasoning and explanation when using compositeimages as few-shot; b) GPT-4V produces severe hallucinations when dealing withworld knowledge, highlighting the future need for advancements in this researchdirection.</description><author>Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang Lyu, Min Zhang</author><pubDate>Mon, 13 Nov 2023 18:22:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07536v1</guid></item><item><title>Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans</title><link>http://arxiv.org/abs/2009.06412v7</link><description>Recently there has been an explosion in the use of Deep Learning (DL) methodsfor medical image segmentation. However the field's reliability is hindered bythe lack of a common base of reference for accuracy/performance evaluation andthe fact that previous research uses different datasets for evaluation. In thispaper, an extensive comparison of DL models for lung and COVID-19 lesionsegmentation in Computerized Tomography (CT) scans is presented, which can alsobe used as a benchmark for testing medical image segmentation models. Four DLarchitectures (Unet, Linknet, FPN, PSPNet) are combined with 25 randomlyinitialized and pretrained encoders (variations of VGG, DenseNet, ResNet,ResNext, DPN, MobileNet, Xception, Inception-v4, EfficientNet), to construct200 tested models. Three experimental setups are conducted for lungsegmentation, lesion segmentation and lesion segmentation using the originallung masks. A public COVID-19 dataset with 100 CT scan images (80 for train, 20for validation) is used for training/validation and a different public datasetconsisting of 829 images from 9 CT scan volumes for testing. Multiple findingsare provided including the best architecture-encoder models for each experimentas well as mean Dice results for each experiment, architecture and encoderindependently. Finally, the upper bounds improvements when using lung masks asa preprocessing step or when using pretrained models are quantified. The sourcecode and 600 pretrained models for the three experiments are provided, suitablefor fine-tuning in experimental setups without GPU capabilities.</description><author>Paschalis Bizopoulos, Nicholas Vretos, Petros Daras</author><pubDate>Mon, 13 Nov 2023 18:22:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.06412v7</guid></item><item><title>Unsupervised Musical Object Discovery from Audio</title><link>http://arxiv.org/abs/2311.07534v1</link><description>Current object-centric learning models such as the popular SlotAttentionarchitecture allow for unsupervised visual scene decomposition. Our novelMusicSlots method adapts SlotAttention to the audio domain, to achieveunsupervised music decomposition. Since concepts of opacity and occlusion invision have no auditory analogues, the softmax normalization of alpha masks inthe decoders of visual object-centric models is not well-suited for decomposingaudio objects. MusicSlots overcomes this problem. We introduce aspectrogram-based multi-object music dataset tailored to evaluateobject-centric learning on western tonal music. MusicSlots achieves goodperformance on unsupervised note discovery and outperforms several establishedbaselines on supervised note property prediction tasks.</description><author>Joonsu Gha, Vincent Herrmann, Benjamin Grewe, Jürgen Schmidhuber, Anand Gopalakrishnan</author><pubDate>Mon, 13 Nov 2023 18:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07534v1</guid></item><item><title>COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances</title><link>http://arxiv.org/abs/2311.01012v2</link><description>We present publicly available COPAL-ID, a novel Indonesian language commonsense reasoning dataset. Unlike the previous Indonesian COPA dataset(XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, andtherefore, provides a more natural portrayal of day-to-day causal reasoningwithin the Indonesian cultural sphere. Professionally written by natives fromscratch, COPAL-ID is more fluent and free from awkward phrases, unlike thetranslated XCOPA-ID. In addition, we present COPAL-ID in both standardIndonesian and in Jakartan Indonesian--a dialect commonly used in dailyconversation. COPAL-ID poses a greater challenge for existing open-sourced andclosed state-of-the-art multilingual language models, yet is trivially easy forhumans. Our findings suggest that even the current best open-source,multilingual model struggles to perform well, achieving 65.47% accuracy onCOPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%).Despite GPT-4's impressive score, it suffers the same performance degradationcompared to its XCOPA-ID score, and it still falls short of human performance.This shows that these language models are still way behind in comprehending thelocal nuances of Indonesian.</description><author>Haryo Akbarianto Wibowo, Erland Hilman Fuadi, Made Nindyatama Nityasya, Radityo Eko Prasojo, Alham Fikri Aji</author><pubDate>Mon, 13 Nov 2023 18:19:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01012v2</guid></item><item><title>Phase Transitions of Civil Unrest across Countries and Time</title><link>http://arxiv.org/abs/2306.08698v3</link><description>Phase transitions, characterized by abrupt shifts between macroscopicpatterns of organization, are ubiquitous in complex systems. Despiteconsiderable research in the physical and natural sciences, the empirical studyof this phenomenon in societal systems is relatively underdeveloped. The goalof this study is to explore whether the dynamics of collective civil unrest canbe plausibly characterized as a sequence of recurrent phase shifts, with eachphase having measurable and identifiable latent characteristics. Building onprevious efforts to characterize civil unrest as a self-organized criticalsystem, we introduce a macro-level statistical model of civil unrest andevaluate its plausibility using a comprehensive dataset of civil unrest eventsin 170 countries from 1946 to 2017. Our findings demonstrate that themacro-level phase model effectively captures the characteristics of civilunrest data from diverse countries globally and that universal mechanisms mayunderlie certain aspects of the dynamics of civil unrest. We also introduce ascale to quantify a country's long-term unrest per unit of time and show thatcivil unrest events tend to cluster geographically, with the magnitude of civilunrest concentrated in specific regions. Our approach has the potential toidentify and measure phase transitions in various collective human phenomenabeyond civil unrest, contributing to a better understanding of complex socialsystems.</description><author>Dan Braha</author><pubDate>Mon, 13 Nov 2023 18:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08698v3</guid></item><item><title>It's Not Easy Being Wrong: Evaluating Process of Elimination Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2311.07532v1</link><description>Chain-of-thought (COT) prompting can help large language models (LLMs) reasontoward correct answers, but its efficacy in reasoning toward incorrect answersis unexplored. This strategy of process of elimination (PoE), when used withCOT, has the potential to enhance interpretability in tasks like medicaldiagnoses of exclusion. Thus, we propose PoE with COT, a new task where LLMsmust reason toward incorrect options on multiple-choice questions. We evaluatethe ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on 2-choicecommonsense and scientific reasoning datasets. We show that PoE consistentlyunderperforms directly choosing the correct answer. The agreement of thesestrategies is also lower than the self-consistency of each strategy. To studythese issues further, we conduct an error analysis and give suggestions forfuture work.</description><author>Nishant Balepur, Shramay Palta, Rachel Rudinger</author><pubDate>Mon, 13 Nov 2023 18:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07532v1</guid></item><item><title>Automatic Identification of Driving Maneuver Patterns using a Robust Hidden Semi-Markov Models</title><link>http://arxiv.org/abs/2311.07527v1</link><description>There is an increase in interest to model driving maneuver patterns via theautomatic unsupervised clustering of naturalistic sequential kinematic drivingdata. The patterns learned are often used in transportation research areas suchas eco-driving, road safety, and intelligent vehicles. One such model capableof modeling these patterns is the Hierarchical Dirichlet Process HiddenSemi-Markov Model (HDP-HSMM), as it is often used to estimate datasegmentation, state duration, and transition probabilities. While this model isa powerful tool for automatically clustering observed sequential data, theexisting HDP-HSMM estimation suffers from an inherent tendency to overestimatethe number of states. This can result in poor estimation, which can potentiallyimpact impact transportation research through incorrect inference of drivingpatterns. In this paper, a new robust HDP-HSMM (rHDP-HSMM) method is proposedto reduce the number of redundant states and improve the consistency of themodel's estimation. Both a simulation study and a case study using naturalisticdriving data are presented to demonstrate the effectiveness of the proposedrHDP-HSMM in identifying and inference of driving maneuver patterns.</description><author>Matthew Aguirre, Wenbo Sun, Jionghua, Jin, Yang Chen</author><pubDate>Mon, 13 Nov 2023 18:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07527v1</guid></item><item><title>A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset</title><link>http://arxiv.org/abs/2307.10455v3</link><description>In an effort to catalog insect biodiversity, we propose a new large datasetof hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record istaxonomically classified by an expert, and also has associated geneticinformation including raw nucleotide barcode sequences and assigned barcodeindex numbers, which are genetically-based proxies for species classification.This paper presents a curated million-image dataset, primarily to traincomputer-vision models capable of providing image-based taxonomic assessment,however, the dataset also presents compelling characteristics, the study ofwhich would be of interest to the broader machine learning community. Driven bythe biological nature inherent to the dataset, a characteristic long-tailedclass-imbalance distribution is exhibited. Furthermore, taxonomic labelling isa hierarchical classification scheme, presenting a highly fine-grainedclassification problem at lower levels. Beyond spurring interest inbiodiversity research within the machine learning community, progress oncreating an image-based taxonomic classifier will also further the ultimategoal of all BIOSCAN research: to lay the foundation for a comprehensive surveyof global biodiversity. This paper introduces the dataset and explores theclassification task through the implementation and analysis of a baselineclassifier.</description><author>Zahra Gharaee, ZeMing Gong, Nicholas Pellegrino, Iuliia Zarubiieva, Joakim Bruslund Haurum, Scott C. Lowe, Jaclyn T. A. McKeown, Chris C. Y. Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke, Angel X. Chang, Graham W. Taylor, Paul Fieguth</author><pubDate>Mon, 13 Nov 2023 18:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10455v3</guid></item><item><title>Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models</title><link>http://arxiv.org/abs/2302.13439v2</link><description>The increased deployment of LMs for real-world tasks involving knowledge andfacts makes it important to understand model epistemology: what LMs think theyknow, and how their attitudes toward that knowledge are affected by languageuse in their inputs. Here, we study an aspect of model epistemology: howepistemic markers of certainty, uncertainty, or evidentiality like "I'm sureit's", "I think it's", or "Wikipedia says it's" affect models, and whether theycontribute to model failures. We develop a typology of epistemic markers andinject 50 markers into prompts for question answering. We find that LMs arehighly sensitive to epistemic markers in prompts, with accuracies varying morethan 80%. Surprisingly, we find that expressions of high certainty result in a7% decrease in accuracy as compared to low certainty expressions; similarly,factive verbs hurt performance, while evidentials benefit performance. Ouranalysis of a popular pretraining dataset shows that these markers ofuncertainty are associated with answers on question-answering websites, whilemarkers of certainty are associated with questions. These associations maysuggest that the behavior of LMs is based on mimicking observed language use,rather than truly reflecting epistemic uncertainty.</description><author>Kaitlyn Zhou, Dan Jurafsky, Tatsunori Hashimoto</author><pubDate>Mon, 13 Nov 2023 18:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13439v2</guid></item><item><title>Machine Learning For Beamline Steering</title><link>http://arxiv.org/abs/2311.07519v1</link><description>Beam steering is the process involving the calibration of the angle andposition at which a particle accelerator's electron beam is incident upon thex-ray target with respect to the rotation axis of the collimator. Beam Steeringis an essential task for light sources. In the case under study, the LINAC ToUndulator (LTU) section of the beamline is difficult to aim. Each use of theaccelerator requires re-calibration of the magnets in this section. Thisinvolves a substantial amount of time and effort from human operators, whilereducing scientific throughput of the light source. We investigate the use ofdeep neural networks to assist in this task. The deep learning models aretrained on archival data and then validated on simulation data. The performanceof the deep learning model is contrasted against that of trained humanoperators.</description><author>Isaac Kante</author><pubDate>Mon, 13 Nov 2023 18:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07519v1</guid></item><item><title>FEMDA: a unified framework for discriminant analysis</title><link>http://arxiv.org/abs/2311.07518v1</link><description>Although linear and quadratic discriminant analysis are widely recognizedclassical methods, they can encounter significant challenges when dealing withnon-Gaussian distributions or contaminated datasets. This is primarily due totheir reliance on the Gaussian assumption, which lacks robustness. We firstexplain and review the classical methods to address this limitation and thenpresent a novel approach that overcomes these issues. In this new approach, themodel considered is an arbitrary Elliptically Symmetrical (ES) distribution percluster with its own arbitrary scale parameter. This flexible model allows forpotentially diverse and independent samples that may not follow identicaldistributions. By deriving a new decision rule, we demonstrate thatmaximum-likelihood parameter estimation and classification are simple,efficient, and robust compared to state-of-the-art methods.</description><author>Pierre Houdouin, Matthieu Jonckheere, Frederic Pascal</author><pubDate>Mon, 13 Nov 2023 17:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07518v1</guid></item><item><title>Bigger, Better, Faster: Human-level Atari with human-level efficiency</title><link>http://arxiv.org/abs/2305.19452v3</link><description>We introduce a value-based RL agent, which we call BBF, that achievessuper-human performance in the Atari 100K benchmark. BBF relies on scaling theneural networks used for value estimation, as well as a number of other designchoices that enable this scaling in a sample-efficient manner. We conductextensive analyses of these design choices and provide insights for futurework. We end with a discussion about updating the goalposts forsample-efficient RL research on the ALE. We make our code and data publiclyavailable athttps://github.com/google-research/google-research/tree/master/bigger_better_faster.</description><author>Max Schwarzer, Johan Obando-Ceron, Aaron Courville, Marc Bellemare, Rishabh Agarwal, Pablo Samuel Castro</author><pubDate>Mon, 13 Nov 2023 17:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19452v3</guid></item><item><title>VGSG: Vision-Guided Semantic-Group Network for Text-based Person Search</title><link>http://arxiv.org/abs/2311.07514v1</link><description>Text-based Person Search (TBPS) aims to retrieve images of target pedestrianindicated by textual descriptions. It is essential for TBPS to extractfine-grained local features and align them crossing modality. Existing methodsutilize external tools or heavy cross-modal interaction to achieve explicitalignment of cross-modal fine-grained features, which is inefficient andtime-consuming. In this work, we propose a Vision-Guided Semantic-Group Network(VGSG) for text-based person search to extract well-aligned fine-grained visualand textual features. In the proposed VGSG, we develop a Semantic-Group TextualLearning (SGTL) module and a Vision-guided Knowledge Transfer (VGKT) module toextract textual local features under the guidance of visual local clues. InSGTL, in order to obtain the local textual representation, we group textualfeatures from the channel dimension based on the semantic cues of languageexpression, which encourages similar semantic patterns to be grouped implicitlywithout external tools. In VGKT, a vision-guided attention is employed toextract visual-related textual features, which are inherently aligned withvisual cues and termed vision-guided textual features. Furthermore, we design arelational knowledge transfer, including a vision-language similarity transferand a class probability transfer, to adaptively propagate information of thevision-guided textual features to semantic-group textual features. With thehelp of relational knowledge transfer, VGKT is capable of aligningsemantic-group textual features with corresponding visual features withoutexternal tools and complex pairwise interaction. Experimental results on twochallenging benchmarks demonstrate its superiority over state-of-the-artmethods.</description><author>Shuting He, Hao Luo, Wei Jiang, Xudong Jiang, Henghui Ding</author><pubDate>Mon, 13 Nov 2023 17:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07514v1</guid></item><item><title>A Hypothesis on Good Practices for AI-based Systems for Financial Time Series Forecasting: Towards Domain-Driven XAI Methods</title><link>http://arxiv.org/abs/2311.07513v1</link><description>Machine learning and deep learning have become increasingly prevalent infinancial prediction and forecasting tasks, offering advantages such asenhanced customer experience, democratising financial services, improvingconsumer protection, and enhancing risk management. However, these complexmodels often lack transparency and interpretability, making them challenging touse in sensitive domains like finance. This has led to the rise of eXplainableArtificial Intelligence (XAI) methods aimed at creating models that are easilyunderstood by humans. Classical XAI methods, such as LIME and SHAP, have beendeveloped to provide explanations for complex models. While these methods havemade significant contributions, they also have limitations, includingcomputational complexity, inherent model bias, sensitivity to data sampling,and challenges in dealing with feature dependence. In this context, this paperexplores good practices for deploying explainability in AI-based systems forfinance, emphasising the importance of data quality, audience-specific methods,consideration of data properties, and the stability of explanations. Thesepractices aim to address the unique challenges and requirements of thefinancial industry and guide the development of effective XAI tools.</description><author>Branka Hadji Misheva, Joerg Osterrieder</author><pubDate>Mon, 13 Nov 2023 17:56:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07513v1</guid></item><item><title>InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis</title><link>http://arxiv.org/abs/2302.08624v6</link><description>We introduce InstructABSA, an instruction learning paradigm for Aspect-BasedSentiment Analysis (ABSA) subtasks. Our method introduces positive, negative,and neutral examples to each training sample, and instruction tune the model(Tk-Instruct) for ABSA subtasks, yielding significant performance improvements.Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate thatInstructABSA outperforms the previous state-of-the-art (SOTA) approaches onTerm Extraction (ATE), Sentiment Classification(ATSC) and Sentiment PairExtraction (ASPE) subtasks. In particular, InstructABSA outperforms theprevious state-of-the-art (SOTA) on the Rest14 ATE subtask by 5.69% points, theRest15 ATSC subtask by 9.59% points, and the Lapt14 AOPE subtask by 3.37%points, surpassing 7x larger models. We also get competitive results on AOOE,AOPE, and AOSTE subtasks indicating strong generalization ability to allsubtasks. Exploring sample efficiency reveals that just 50% train data isrequired to get competitive results with other instruction tuning approaches.Lastly, we assess the quality of instructions and observe that InstructABSA'sperformance experiences a decline of ~10% when adding misleading examples.</description><author>Kevin Scaria, Himanshu Gupta, Siddharth Goyal, Saurabh Arjun Sawant, Swaroop Mishra, Chitta Baral</author><pubDate>Mon, 13 Nov 2023 17:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08624v6</guid></item><item><title>Machine learning for uncertainty estimation in fusing precipitation observations from satellites and ground-based gauges</title><link>http://arxiv.org/abs/2311.07511v1</link><description>To form precipitation datasets that are accurate and, at the same time, havehigh spatial densities, data from satellites and gauges are often merged in theliterature. However, uncertainty estimates for the data acquired in this mannerare scarcely provided, although the importance of uncertainty quantification inpredictive modelling is widely recognized. Furthermore, the benefits thatmachine learning can bring to the task of providing such estimates have notbeen broadly realized and properly explored through benchmark experiments. Thepresent study aims at filling in this specific gap by conducting the firstbenchmark tests on the topic. On a large dataset that comprises 15-year-longmonthly data spanning across the contiguous United States, we extensivelycompared six learners that are, by their construction, appropriate forpredictive uncertainty quantification. These are the quantile regression (QR),quantile regression forests (QRF), generalized random forests (GRF), gradientboosting machines (GBM), light gradient boosting machines (LightGBM) andquantile regression neural networks (QRNN). The comparison referred to thecompetence of the learners in issuing predictive quantiles at nine levels thatfacilitate a good approximation of the entire predictive probabilitydistribution, and was primarily based on the quantile and continuous rankedprobability skill scores. Three types of predictor variables (i.e., satelliteprecipitation variables, distances between a point of interest and satellitegrid points, and elevation at a point of interest) were used in the comparisonand were additionally compared with each other. This additional comparison wasbased on the explainable machine learning concept of feature importance. Theresults suggest that the order from the best to the worst of the learners forthe task investigated is the following: LightGBM, QRF, GRF, GBM, QRNN and QR...</description><author>Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis, Anastasios Doulamis</author><pubDate>Mon, 13 Nov 2023 17:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07511v1</guid></item><item><title>Explicit Foundation Model Optimization with Self-Attentive Feed-Forward Neural Units</title><link>http://arxiv.org/abs/2311.07510v1</link><description>Iterative approximation methods using backpropagation enable the optimizationof neural networks, but they remain computationally expensive, especially whenused at scale. This paper presents an efficient alternative for optimizingneural networks that reduces the costs of scaling neural networks and provideshigh-efficiency optimizations for low-resource applications. We will discuss ageneral result about feed-forward neural networks and then extend this solutionto compositional (mult-layer) networks, which are applied to a simplifiedtransformer block containing feed-forward and self-attention layers. Thesemodels are used to train highly-specified and complex multi-layer neuralarchitectures that we refer to as self-attentive feed-forward unit (SAFFU)layers, which we use to develop a transformer that appears to generalize wellover small, cognitively-feasible, volumes of data. Testing demonstratesexplicit solutions outperform models optimized by backpropagation alone.Moreover, further application of backpropagation after explicit solutions leadsto better optima from smaller scales of data, training effective models frommuch less data is enabled by explicit solution warm starts. We then carry outablation experiments training a roadmap of about 250 transformer models over1-million tokens to determine ideal settings. We find that multiple differentarchitectural variants produce highly-performant models, and discover from thisablation that some of the best are not the most parameterized. This appears toindicate well-generalized models could be reached using less data by usingexplicit solutions, and that architectural exploration using explicit solutionspays dividends in guiding the search for efficient variants with fewerparameters, and which could be incorporated into low-resource hardware where AImight be embodied.</description><author>Jake Ryland Williams, Haoran Zhao</author><pubDate>Mon, 13 Nov 2023 17:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07510v1</guid></item><item><title>A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases</title><link>http://arxiv.org/abs/2311.07509v1</link><description>Enterprise applications of Large Language Models (LLMs) hold promise forquestion answering on enterprise SQL databases. However, the extent to whichLLMs can accurately respond to enterprise questions in such databases remainsunclear, given the absence of suitable Text-to-SQL benchmarks tailored toenterprise settings. Additionally, the potential of Knowledge Graphs (KGs) toenhance LLM-based question answering by providing business context is not wellunderstood. This study aims to evaluate the accuracy of LLM-powered questionanswering systems in the context of enterprise questions and SQL databases,while also exploring the role of knowledge graphs in improving accuracy. Toachieve this, we introduce a benchmark comprising an enterprise SQL schema inthe insurance domain, a range of enterprise queries encompassing reporting tometrics, and a contextual layer incorporating an ontology and mappings thatdefine a knowledge graph. Our primary finding reveals that question answeringusing GPT-4, with zero-shot prompts directly on SQL databases, achieves anaccuracy of 16%. Notably, this accuracy increases to 54% when questions areposed over a Knowledge Graph representation of the enterprise SQL database.Therefore, investing in Knowledge Graph provides higher accuracy for LLMpowered question answering systems.</description><author>Juan Sequeda, Dean Allemang, Bryon Jacob</author><pubDate>Mon, 13 Nov 2023 17:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07509v1</guid></item><item><title>Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models</title><link>http://arxiv.org/abs/2304.03738v3</link><description>As the capabilities of generative language models continue to advance, theimplications of biases ingrained within these models have garnered increasingattention from researchers, practitioners, and the broader public. This articleinvestigates the challenges and risks associated with biases in large-scalelanguage models like ChatGPT. We discuss the origins of biases, stemming from,among others, the nature of training data, model specifications, algorithmicconstraints, product design, and policy decisions. We explore the ethicalconcerns arising from the unintended consequences of biased model outputs. Wefurther analyze the potential opportunities to mitigate biases, theinevitability of some biases, and the implications of deploying these models invarious applications, such as virtual assistants, content generation, andchatbots. Finally, we review the current approaches to identify, quantify, andmitigate biases in language models, emphasizing the need for amulti-disciplinary, collaborative effort to develop more equitable,transparent, and responsible AI systems. This article aims to stimulate athoughtful dialogue within the artificial intelligence community, encouragingresearchers and developers to reflect on the role of biases in generativelanguage models and the ongoing pursuit of ethical AI.</description><author>Emilio Ferrara</author><pubDate>Mon, 13 Nov 2023 17:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03738v3</guid></item><item><title>STEM Rebalance: A Novel Approach for Tackling Imbalanced Datasets using SMOTE, Edited Nearest Neighbour, and Mixup</title><link>http://arxiv.org/abs/2311.07504v1</link><description>Imbalanced datasets in medical imaging are characterized by skewed classproportions and scarcity of abnormal cases. When trained using such data,models tend to assign higher probabilities to normal cases, leading to biasedperformance. Common oversampling techniques such as SMOTE rely on localinformation and can introduce marginalization issues. This paper investigatesthe potential of using Mixup augmentation that combines two training examplesalong with their corresponding labels to generate new data points as a genericvicinal distribution. To this end, we propose STEM, which combines SMOTE-ENNand Mixup at the instance level. This integration enables us to effectivelyleverage the entire distribution of minority classes, thereby mitigating bothbetween-class and within-class imbalances. We focus on the breast cancerproblem, where imbalanced datasets are prevalent. The results demonstrate theeffectiveness of STEM, which achieves AUC values of 0.96 and 0.99 in theDigital Database for Screening Mammography and Wisconsin Breast Cancer(Diagnostics) datasets, respectively. Moreover, this method shows promisingpotential when applied with an ensemble of machine learning (ML) classifiers.</description><author>Yumnah Hasan, Fatemeh Amerehi, Patrick Healy, Conor Ryan</author><pubDate>Mon, 13 Nov 2023 17:45:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07504v1</guid></item><item><title>Reducing the Need for Backpropagation and Discovering Better Optima With Explicit Optimizations of Neural Networks</title><link>http://arxiv.org/abs/2311.07498v1</link><description>Iterative differential approximation methods that rely upon backpropagationhave enabled the optimization of neural networks; however, at present, theyremain computationally expensive, especially when training models at scale. Inthis paper, we propose a computationally efficient alternative for optimizingneural networks that can both reduce the costs of scaling neural networks andprovide high-efficiency optimizations for low-resource applications. We derivean explicit solution to a simple feed-forward language model (LM) bymathematically analyzing its gradients. This solution generalizes fromsingle-layer LMs to the class of all single-layer feed-forwardsoftmax-activated neural models trained on positive-valued features, as isdemonstrated by our extension of this solution application to MNIST digitclassification. For both LM and digit classifiers, we find computationally thatexplicit solutions perform near-optimality in experiments showing that 1)iterative optimization only marginally improves the explicit solutionparameters and 2) randomly initialized parameters iteratively optimize towardsthe explicit solution. We also preliminarily apply the explicit solutionlocally by layer in multi-layer networks and discuss how the solution'scomputational savings increase with model complexity -- for both single- andmult-layer applications of the explicit solution, we emphasize that the optimaachieved cannot be reached by backpropagation alone, i.e., better optima appeardiscoverable only after explicit solutions are applied. Finally, we discuss thesolution's computational savings alongside its impact on model interpretabilityand suggest future directions for the derivation of explicit solutions tocomplex- and multi-layer architectures.</description><author>Jake Ryland Williams, Haoran Zhao</author><pubDate>Mon, 13 Nov 2023 17:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07498v1</guid></item><item><title>Multilingual Nonce Dependency Treebanks: Understanding how LLMs represent and process syntactic structure</title><link>http://arxiv.org/abs/2311.07497v1</link><description>We introduce SPUD (Semantically Perturbed Universal Dependencies), aframework for creating nonce treebanks for the multilingual UniversalDependencies (UD) corpora. SPUD data satisfies syntactic argument structure,provides syntactic annotations, and ensures grammaticality vialanguage-specific rules. We create nonce data in Arabic, English, French,German, and Russian, and demonstrate two use cases of SPUD treebanks. First, weinvestigate the effect of nonce data on word co-occurrence statistics, asmeasured by perplexity scores of autoregressive (ALM) and masked languagemodels (MLM). We find that ALM scores are significantly more affected by noncedata than MLM scores. Second, we show how nonce data affects the performance ofsyntactic dependency probes. We replicate the findings of M\"uller-Eberstein etal. (2022) on nonce test data and show that the performance declines on bothMLMs and ALMs wrt. original test data. However, a majority of the performanceis kept, suggesting that the probe indeed learns syntax independently fromsemantics.</description><author>David Arps, Laura Kallmeyer, Younes Samih, Hassan Sajjad</author><pubDate>Mon, 13 Nov 2023 17:36:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07497v1</guid></item><item><title>Source Code Data Augmentation for Deep Learning: A Survey</title><link>http://arxiv.org/abs/2305.19915v4</link><description>The increasingly popular adoption of deep learning models in many criticalsource code tasks motivates the development of data augmentation (DA)techniques to enhance training data and improve various capabilities (e.g.,robustness and generalizability) of these models. Although a series of DAmethods have been proposed and tailored for source code models, there lacks acomprehensive survey and examination to understand their effectiveness andimplications. This paper fills this gap by conducting a comprehensive andintegrative survey of data augmentation for source code, wherein wesystematically compile and encapsulate existing literature to provide acomprehensive overview of the field. We start with an introduction of dataaugmentation in source code and then provide a discussion on majorrepresentative approaches. Next, we highlight the general strategies andtechniques to optimize the DA quality. Subsequently, we underscore techniquesuseful in real-world source code scenarios and downstream tasks. Finally, weoutline the prevailing challenges and potential opportunities for futureresearch. In essence, we aim to demystify the corpus of existing literature onsource code DA for deep learning, and foster further exploration in thissphere. Complementing this, we present a continually updated GitHub repositorythat hosts a list of update-to-date papers on DA for source code modeling,accessible at \url{https://github.com/terryyz/DataAug4Code}.</description><author>Terry Yue Zhuo, Zhou Yang, Zhensu Sun, Yufei Wang, Li Li, Xiaoning Du, Zhenchang Xing, David Lo</author><pubDate>Mon, 13 Nov 2023 17:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19915v4</guid></item><item><title>CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation</title><link>http://arxiv.org/abs/2306.13761v2</link><description>Deep learning has been extensively used in wireless communication problems,including channel estimation. Although several data-driven approaches exist, afair and realistic comparison between them is difficult due to inconsistenciesin the experimental conditions and the lack of a standardized experimentaldesign. In addition, the performance of data-driven approaches is oftencompared based on empirical analysis. The lack of reproducibility andavailability of standardized evaluation tools (e.g., datasets, codebases)hinder the development and progress of data-driven methods for channelestimation and wireless communication in general. In this work, we introduce aninitiative to build benchmarks that unify several data-driven OFDM channelestimation approaches. Specifically, we present CeBed (a testbed for channelestimation) including different datasets covering various systems models andpropagation conditions along with the implementation of ten deep andtraditional baselines. This benchmark considers different practical aspectssuch as the robustness of the data-driven models, the number and thearrangement of pilots, and the number of receive antennas. This work offers acomprehensive and unified framework to help researchers evaluate and designdata-driven channel estimation algorithms.</description><author>Amal Feriani, Di Wu, Steve Liu, Greg Dudek</author><pubDate>Mon, 13 Nov 2023 17:33:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13761v2</guid></item><item><title>A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models</title><link>http://arxiv.org/abs/2311.07491v1</link><description>While large language models exhibit remarkable performance in the QuestionAnswering task, they are susceptible to hallucinations. Challenges arise whenthese models grapple with understanding multi-hop relations in complexquestions or lack the necessary knowledge for a comprehensive response. Toaddress this issue, we introduce the "Decompose-and-Query" framework (D&amp;Q).This framework guides the model to think and utilize external knowledge similarto ReAct, while also restricting its thinking to reliable information,effectively mitigating the risk of hallucinations. Experiments confirm theeffectiveness of D&amp;Q: On our ChitChatQA dataset, D&amp;Q does not lose to ChatGPTin 67% of cases; on the HotPotQA question-only setting, D&amp;Q achieved an F1score of 59.6%. Our code is available athttps://github.com/alkaidpku/DQ-ToolQA.</description><author>Hejing Cao, Zhenwei An, Jiazhan Feng, Kun Xu, Liwei Chen, Dongyan Zhao</author><pubDate>Mon, 13 Nov 2023 17:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07491v1</guid></item><item><title>EvoFed: Leveraging Evolutionary Strategies for Communication-Efficient Federated Learning</title><link>http://arxiv.org/abs/2311.07485v1</link><description>Federated Learning (FL) is a decentralized machine learning paradigm thatenables collaborative model training across dispersed nodes without having toforce individual nodes to share data. However, its broad adoption is hinderedby the high communication costs of transmitting a large number of modelparameters. This paper presents EvoFed, a novel approach that integratesEvolutionary Strategies (ES) with FL to address these challenges. EvoFedemploys a concept of 'fitness-based information sharing', deviatingsignificantly from the conventional model-based FL. Rather than exchanging theactual updated model parameters, each node transmits a distance-basedsimilarity measure between the locally updated model and each member of thenoise-perturbed model population. Each node, as well as the server, generatesan identical population set of perturbed models in a completely synchronizedfashion using the same random seeds. With properly chosen noise variance andpopulation size, perturbed models can be combined to closely reflect the actualmodel updated using the local dataset, allowing the transmitted similaritymeasures (or fitness values) to carry nearly the complete information about themodel parameters. As the population size is typically much smaller than thenumber of model parameters, the savings in communication load is large. Theserver aggregates these fitness values and is able to update the global model.This global fitness vector is then disseminated back to the nodes, each ofwhich applies the same update to be synchronized to the global model. Ouranalysis shows that EvoFed converges, and our experimental results validatethat at the cost of increased local processing loads, EvoFed achievesperformance comparable to FedAvg while reducing overall communicationrequirements drastically in various practical settings.</description><author>Mohammad Mahdi Rahimi, Hasnain Irshad Bhatti, Younghyun Park, Humaira Kousar, Jaekyun Moon</author><pubDate>Mon, 13 Nov 2023 17:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07485v1</guid></item><item><title>Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination</title><link>http://arxiv.org/abs/2305.16963v2</link><description>LiDAR (Light Detection and Ranging) has become an essential part of theremote sensing toolbox used for biosphere monitoring. In particular, LiDARprovides the opportunity to map forest leaf area with unprecedented accuracy,while leaf area has remained an important source of uncertainty affectingmodels of gas exchanges between the vegetation and the atmosphere. UnmannedAerial Vehicles (UAV) are easy to mobilize and therefore allow frequentrevisits to track the response of vegetation to climate change. However,miniature sensors embarked on UAVs usually provide point clouds of limiteddensity, which are further affected by a strong decrease in density from top tobottom of the canopy due to progressively stronger occlusion. In such acontext, discriminating leaf points from wood points presents a significantchallenge due in particular to strong class imbalance and spatially irregularsampling intensity. Here we introduce a neural network model based on thePointnet ++ architecture which makes use of point geometry only (excluding anyspectral information). To cope with local data sparsity, we propose aninnovative sampling scheme which strives to preserve local important geometricinformation. We also propose a loss function adapted to the severe classimbalance. We show that our model outperforms state-of-the-art alternatives onUAV point clouds. We discuss future possible improvements, particularlyregarding much denser point clouds acquired from below the canopy.</description><author>Yuchen Bai, Jean-Baptiste Durand, Florence Forbes, Grégoire Vincent</author><pubDate>Mon, 13 Nov 2023 17:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16963v2</guid></item><item><title>Psychometric Predictive Power of Large Language Models</title><link>http://arxiv.org/abs/2311.07484v1</link><description>Next-word probabilities from language models have been shown to successfullysimulate human reading behavior. Building on this, we show that, interestingly,instruction-tuned large language models (LLMs) yield worse psychometricpredictive power (PPP) for human reading behavior than base LLMs withequivalent perplexities. In other words, instruction tuning, which helps LLMsprovide human-preferred responses, does not always make them human-like fromthe computational psycholinguistics perspective. In addition, we exploreprompting methodologies in simulating human reading behavior with LLMs, showingthat prompts reflecting a particular linguistic hypothesis lead LLMs to exhibitbetter PPP but are still worse than base LLMs. These highlight that recentinstruction tuning and prompting do not offer better estimates than directprobability measurements from base LLMs in cognitive modeling.</description><author>Tatsuki Kuribayashi, Yohei Oseki, Timothy Baldwin</author><pubDate>Mon, 13 Nov 2023 17:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07484v1</guid></item><item><title>Temporal Performance Prediction for Deep Convolutional Long Short-Term Memory Networks</title><link>http://arxiv.org/abs/2311.07477v1</link><description>Quantifying predictive uncertainty of deep semantic segmentation networks isessential in safety-critical tasks. In applications like autonomous driving,where video data is available, convolutional long short-term memory networksare capable of not only providing semantic segmentations but also predictingthe segmentations of the next timesteps. These models use cell states tobroadcast information from previous data by taking a time series of inputs topredict one or even further steps into the future. We present a temporalpostprocessing method which estimates the prediction performance ofconvolutional long short-term memory networks by either predicting theintersection over union of predicted and ground truth segments or classifyingbetween intersection over union being equal to zero or greater than zero. Tothis end, we create temporal cell state-based input metrics per segment andinvestigate different models for the estimation of the predictive quality basedon these metrics. We further study the influence of the number of consideredcell states for the proposed metrics.</description><author>Laura Fieback, Bidya Dash, Jakob Spiegelberg, Hanno Gottschalk</author><pubDate>Mon, 13 Nov 2023 17:11:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07477v1</guid></item><item><title>Masked Face Dataset Generation and Masked Face Recognition</title><link>http://arxiv.org/abs/2311.07475v1</link><description>In the post-pandemic era, wearing face masks has posed great challenge to theordinary face recognition. In the previous study, researchers has appliedpretrained VGG16, and ResNet50 to extract features on the elaborate curatedexisting masked face recognition (MFR) datasets, RMFRD and SMFRD. To make themodel more adaptable to the real world situation where the sample size issmaller and the camera environment has greater changes, we created a morechallenging masked face dataset ourselves, by selecting 50 identities with 1702images from Labelled Faces in the Wild (LFW) Dataset, and simulated face masksthrough key point detection. The another part of our study is to solve themasked face recognition problem, and we chose models by referring to the formerstate of the art results, instead of directly using pretrained models, we finetuned the model on our new dataset and use the last linear layer to do theclassification directly. Furthermore, we proposed using data augmentationstrategy to further increase the test accuracy, and fine tuned a new networksbeyond the former study, one of the most SOTA networks, Inception ResNet v1.The best test accuracy on 50 identity MFR has achieved 95%.</description><author>Rui Cai, Xuying Ning, Peter N. Belhumeur</author><pubDate>Mon, 13 Nov 2023 17:09:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07475v1</guid></item><item><title>A Federated Data Fusion-Based Prognostic Model for Applications with Multi-Stream Incomplete Signals</title><link>http://arxiv.org/abs/2311.07474v1</link><description>Most prognostic methods require a decent amount of data for model training.In reality, however, the amount of historical data owned by a singleorganization might be small or not large enough to train a reliable prognosticmodel. To address this challenge, this article proposes a federated prognosticmodel that allows multiple users to jointly construct a failure time predictionmodel using their multi-stream, high-dimensional, and incomplete data whilekeeping each user's data local and confidential. The prognostic model firstemploys multivariate functional principal component analysis to fuse themulti-stream degradation signals. Then, the fused features coupled with thetimes-to-failure are utilized to build a (log)-location-scale regression modelfor failure prediction. To estimate parameters using distributed datasets andkeep the data privacy of all participants, we propose a new federated algorithmfor feature extraction. Numerical studies indicate that the performance of theproposed model is the same as that of classic non-federated prognostic modelsand is better than that of the models constructed by each user itself.</description><author>Madi Arabi, Xiaolei Fang</author><pubDate>Mon, 13 Nov 2023 17:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07474v1</guid></item><item><title>Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer</title><link>http://arxiv.org/abs/2311.07470v1</link><description>Multi-modal large language models (LLM) have achieved powerful capabilitiesfor visual semantic understanding in recent years. However, little is knownabout how LLMs comprehend visual information and interpret different modalitiesof features. In this paper, we propose a new method for identifying multi-modalneurons in transformer-based multi-modal LLMs. Through a series of experiments,We highlight three critical properties of multi-modal neurons by fourwell-designed quantitative evaluation metrics. Furthermore, we introduce aknowledge editing method based on the identified multi-modal neurons, formodifying a specific token to another designative token. We hope our findingscan inspire further explanatory researches on understanding mechanisms ofmulti-modal LLMs.</description><author>Haowen Pan, Yixin Cao, Xiaozhi Wang, Xun Yang</author><pubDate>Mon, 13 Nov 2023 17:03:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07470v1</guid></item><item><title>InCA: Rethinking In-Car Conversational System Assessment Leveraging Large Language Models</title><link>http://arxiv.org/abs/2311.07469v1</link><description>The assessment of advanced generative large language models (LLMs) poses asignificant challenge, given their heightened complexity in recentdevelopments. Furthermore, evaluating the performance of LLM-based applicationsin various industries, as indicated by Key Performance Indicators (KPIs), is acomplex undertaking. This task necessitates a profound understanding ofindustry use cases and the anticipated system behavior. Within the context ofthe automotive industry, existing evaluation metrics prove inadequate forassessing in-car conversational question answering (ConvQA) systems. The uniquedemands of these systems, where answers may relate to driver or car safety andare confined within the car domain, highlight the limitations of currentmetrics. To address these challenges, this paper introduces a set of KPIstailored for evaluating the performance of in-car ConvQA systems, along withdatasets specifically designed for these KPIs. A preliminary and comprehensiveempirical evaluation substantiates the efficacy of our proposed approach.Furthermore, we investigate the impact of employing varied personas in promptsand found that it enhances the model's capacity to simulate diverse viewpointsin assessments, mirroring how individuals with different backgrounds perceive atopic.</description><author>Ken E. Friedl, Abbas Goher Khan, Soumya Ranjan Sahoo, Md Rashad Al Hasan Rony, Jana Germies, Christian Süß</author><pubDate>Mon, 13 Nov 2023 17:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07469v1</guid></item><item><title>Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation of the Reversal Curse</title><link>http://arxiv.org/abs/2311.07468v1</link><description>Recent studies have highlighted a phenomenon in large language models (LLMs)known as "the reversal curse," in which the order of knowledge entities in thetraining data biases the models' comprehension. For example, if a model istrained on sentences where entity A consistently appears before entity B, itcan respond to queries about A by providing B. However, it may encounterconfusion when presented with questions concerning B. We contend that thereversal curse is partially a result of specific model training objectives,particularly evident in the prevalent use of the next-token prediction withinmost causal language models. For the next-token prediction, models solely focuson a token's preceding context, resulting in a restricted comprehension of theinput. In contrast, we illustrate that the GLM, trained using theautoregressive blank infilling objective where tokens to be predicted haveaccess to the entire context, exhibits better resilience against the reversalcurse. We propose a novel training method, BIdirectional Casual languagemodeling Optimization (BICO), designed to mitigate the reversal curse whenfine-tuning pretrained causal language models on new data. BICO modifies thecausal attention mechanism to function bidirectionally and employs a maskdenoising optimization. In the task designed to assess the reversal curse, ourapproach improves Llama's accuracy from the original 0% to around 70%. We hopethat more attention can be focused on exploring and addressing these inherentweaknesses of the current LLMs, in order to achieve a higher level ofintelligence.</description><author>Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, Rui Yan</author><pubDate>Mon, 13 Nov 2023 17:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07468v1</guid></item><item><title>Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</title><link>http://arxiv.org/abs/2311.06237v2</link><description>Engaging in the deliberate generation of abnormal outputs from large languagemodels (LLMs) by attacking them is a novel human activity. This paper presentsa thorough exposition of how and why people perform such attacks. Using aformal qualitative methodology, we interviewed dozens of practitioners from abroad range of backgrounds, all contributors to this novel work of attemptingto cause LLMs to fail. We relate and connect this activity between itspractitioners' motivations and goals; the strategies and techniques theydeploy; and the crucial role the community plays. As a result, this paperpresents a grounded theory of how and why people attack large language models:LLM red teaming in the wild.</description><author>Nanna Inie, Jonathan Stray, Leon Derczynski</author><pubDate>Mon, 13 Nov 2023 17:00:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06237v2</guid></item><item><title>FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning</title><link>http://arxiv.org/abs/2307.10317v2</link><description>Federated Learning (FL) offers a collaborative training framework, allowingmultiple clients to contribute to a shared model without compromising dataprivacy. Due to the heterogeneous nature of local datasets, updated clientmodels may overfit and diverge from one another, commonly known as the problemof client drift. In this paper, we propose FedBug (Federated Learning withBottom-Up Gradual Unfreezing), a novel FL framework designed to effectivelymitigate client drift. FedBug adaptively leverages the client model parameters,distributed by the server at each global round, as the reference points forcross-client alignment. Specifically, on the client side, FedBug begins byfreezing the entire model, then gradually unfreezes the layers, from the inputlayer to the output layer. This bottom-up approach allows models to train thenewly thawed layers to project data into a latent space, wherein the separatinghyperplanes remain consistent across all clients. We theoretically analyzeFedBug in a novel over-parameterization FL setup, revealing its superiorconvergence rate compared to FedAvg. Through comprehensive experiments,spanning various datasets, training conditions, and network architectures, wevalidate the efficacy of FedBug. Our contributions encompass a novel FLframework, theoretical analysis, and empirical validation, demonstrating thewide potential and applicability of FedBug.</description><author>Chia-Hsiang Kao, Yu-Chiang Frank Wang</author><pubDate>Mon, 13 Nov 2023 16:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10317v2</guid></item><item><title>On Measuring Faithfulness of Natural Language Explanations</title><link>http://arxiv.org/abs/2311.07466v1</link><description>Large language models (LLMs) can explain their own predictions, throughpost-hoc or Chain-of-Thought (CoT) explanations. However the LLM could make upreasonably sounding explanations that are unfaithful to its underlyingreasoning. Recent work has designed tests that aim to judge the faithfulness ofeither post-hoc or CoT explanations. In this paper we argue that existingfaithfulness tests are not actually measuring faithfulness in terms of themodels' inner workings, but only evaluate their self-consistency on the outputlevel. The aims of our work are two-fold. i) We aim to clarify the status ofexisting faithfulness tests in terms of model explainability, characterisingthem as self-consistency tests instead. This assessment we underline byconstructing a Comparative Consistency Bank for self-consistency tests that forthe first time compares existing tests on a common suite of 11 open-source LLMsand 5 datasets -- including ii) our own proposed self-consistency measureCC-SHAP. CC-SHAP is a new fine-grained measure (not test) of LLMself-consistency that compares a model's input contributions to answerprediction and generated explanation. With CC-SHAP, we aim to take a stepfurther towards measuring faithfulness with a more interpretable andfine-grained method. Code available at\url{https://github.com/Heidelberg-NLP/CC-SHAP}</description><author>Letitia Parcalabescu, Anette Frank</author><pubDate>Mon, 13 Nov 2023 16:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07466v1</guid></item><item><title>Computerized Tomography and Reproducing Kernels</title><link>http://arxiv.org/abs/2311.07465v1</link><description>The X-ray transform is one of the most fundamental integral operators inimage processing and reconstruction. In this article, we revisit itsmathematical formalism, and propose an innovative approach making use ofReproducing Kernel Hilbert Spaces (RKHS). Within this framework, the X-raytransform can be considered as a natural analogue of Euclidean projections. TheRKHS framework considerably simplifies projection image interpolation, andleads to an analogue of the celebrated representer theorem for the problem oftomographic reconstruction. It leads to methodology that is dimension-free andstands apart from conventional filtered back-projection techniques, as it doesnot hinge on the Fourier transform. It also allows us to establish sharpstability results at a genuinely functional level, but in the realistic settingwhere the data are discrete and noisy. The RKHS framework is amenable to anyreproducing kernel on a unit ball, affording a high level of generality. Whenthe kernel is chosen to be rotation-invariant, one can obtain explicit spectralrepresentations which elucidate the regularity structure of the associatedHilbert spaces, and one can also solve the reconstruction problem at the samecomputational cost as filtered back-projection.</description><author>Ho Yun, Victor M. Panaretos</author><pubDate>Mon, 13 Nov 2023 16:53:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07465v1</guid></item><item><title>Fast Machine Learning Method with Vector Embedding on Orthonormal Basis and Spectral Transform</title><link>http://arxiv.org/abs/2310.18424v2</link><description>This paper presents a novel fast machine learning method that leverages twotechniques: Vector Embedding on Orthonormal Basis (VEOB) and Spectral Transform(ST). The VEOB converts the original data encoding into a vector embedding withcoordinates projected onto orthonormal bases. The Singular Value Decomposition(SVD) technique is used to calculate the vector basis and projectioncoordinates, leading to an enhanced distance measurement in the embedding spaceand facilitating data compression by preserving the projection vectorsassociated with the largest singular values. On the other hand, ST transformssequence of vector data into spectral space. By applying the Discrete CosineTransform (DCT) and selecting the most significant components, it streamlinesthe handling of lengthy vector sequences. The paper provides examples of wordembedding, text chunk embedding, and image embedding, implemented in Julialanguage with a vector database. It also investigates unsupervised learning andsupervised learning using this method, along with strategies for handling largedata volumes.</description><author>Louis Yu Lu</author><pubDate>Mon, 13 Nov 2023 16:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18424v2</guid></item><item><title>Simulator-Based Inference with Waldo: Confidence Regions by Leveraging Prediction Algorithms and Posterior Estimators for Inverse Problems</title><link>http://arxiv.org/abs/2205.15680v4</link><description>Prediction algorithms, such as deep neural networks (DNNs), are used in manydomain sciences to directly estimate internal parameters of interest insimulator-based models, especially in settings where the observations includeimages or complex high-dimensional data. In parallel, modern neural densityestimators, such as normalizing flows, are becoming increasingly popular foruncertainty quantification, especially when both parameters and observationsare high-dimensional. However, parameter inference is an inverse problem andnot a prediction task; thus, an open challenge is to construct conditionallyvalid and precise confidence regions, with a guaranteed probability of coveringthe true parameters of the data-generating process, no matter what the(unknown) parameter values are, and without relying on large-sample theory.Many simulator-based inference (SBI) methods are indeed known to produce biasedor overly confident parameter regions, yielding misleading uncertaintyestimates. This paper presents WALDO, a novel method to construct confidenceregions with finite-sample conditional validity by leveraging predictionalgorithms or posterior estimators that are currently widely adopted in SBI.WALDO reframes the well-known Wald test statistic, and uses a computationallyefficient regression-based machinery for classical Neyman inversion ofhypothesis tests. We apply our method to a recent high-energy physics problem,where prediction with DNNs has previously led to estimates with predictionbias. We also illustrate how our approach can correct overly confidentposterior regions computed with normalizing flows.</description><author>Luca Masserano, Tommaso Dorigo, Rafael Izbicki, Mikael Kuusela, Ann B. Lee</author><pubDate>Mon, 13 Nov 2023 16:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.15680v4</guid></item><item><title>MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks</title><link>http://arxiv.org/abs/2311.07463v1</link><description>Recently, there has been a rapid advancement in research on Large LanguageModels (LLMs), resulting in significant progress in several Natural LanguageProcessing (NLP) tasks. Consequently, there has been a surge in LLM evaluationresearch to comprehend the models' capabilities and limitations. However, muchof this research has been confined to the English language, leaving LLMbuilding and evaluation for non-English languages relatively unexplored. Therehas been an introduction of several new LLMs, necessitating their evaluation onnon-English languages. This study aims to expand our MEGA benchmarking suite byincluding six new datasets to form the MEGAVERSE benchmark. The benchmarkcomprises 22 datasets covering 81 languages, including low-resource Africanlanguages. We evaluate several state-of-the-art LLMs like GPT-3.5-Turbo, GPT4,PaLM2, and Llama2 on the MEGAVERSE datasets. Additionally, we include twomultimodal datasets in the benchmark and assess the performance of theLLaVa-v1.5 model. Our experiments suggest that GPT4 and PaLM2 outperform theLlama models on various tasks, notably on low-resource languages, with GPT4outperforming PaLM2 on more datasets than vice versa. However, issues such asdata contamination must be addressed to obtain an accurate assessment of LLMperformance on non-English languages.</description><author>Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali, Sunayana Sitaram</author><pubDate>Mon, 13 Nov 2023 16:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07463v1</guid></item><item><title>On Self-Supervised Dynamic Incremental Regularised Adaptation</title><link>http://arxiv.org/abs/2311.07461v1</link><description>In this paper, we overview a recent method for dynamic domain adaptationnamed DIRA, which relies on a few samples in addition to a regularisationapproach named elastic weight consolidation to achieve state-of-the-art (SOTA)domain adaptation results. DIRA has been previously shown to performcompetitively with SOTA unsupervised adaption techniques. However, a limitationof DIRA is that it relies on labels to be provided for the few samples used inadaption. This makes it a supervised technique. In this paper, we discuss aproposed alteration to the DIRA method to make it self-supervised i.e. removethe need for providing labels. Experiments on our proposed alteration will beprovided in future work.</description><author>Abanoub Ghobrial, Kerstin Eder</author><pubDate>Mon, 13 Nov 2023 16:44:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07461v1</guid></item><item><title>KnowSafe: Combined Knowledge and Data Driven Hazard Mitigation in Artificial Pancreas Systems</title><link>http://arxiv.org/abs/2311.07460v1</link><description>Significant progress has been made in anomaly detection and run-timemonitoring to improve the safety and security of cyber-physical systems (CPS).However, less attention has been paid to hazard mitigation. This paper proposesa combined knowledge and data driven approach, KnowSafe, for the design ofsafety engines that can predict and mitigate safety hazards resulting fromsafety-critical malicious attacks or accidental faults targeting a CPScontroller. We integrate domain-specific knowledge of safety constraints andcontext-specific mitigation actions with machine learning (ML) techniques toestimate system trajectories in the far and near future, infer potentialhazards, and generate optimal corrective actions to keep the system safe.Experimental evaluation on two realistic closed-loop testbeds for artificialpancreas systems (APS) and a real-world clinical trial dataset for diabetestreatment demonstrates that KnowSafe outperforms the state-of-the-art byachieving higher accuracy in predicting system state trajectories and potentialhazards, a low false positive rate, and no false negatives. It also maintainsthe safe operation of the simulated APS despite faults or attacks withoutintroducing any new hazards, with a hazard mitigation success rate of 92.8%,which is at least 76% higher than solely rule-based (50.9%) and data-driven(52.7%) methods.</description><author>Xugui Zhou, Maxfield Kouzel, Chloe Smith, Homa Alemzadeh</author><pubDate>Mon, 13 Nov 2023 16:43:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07460v1</guid></item><item><title>HyperMixer: An MLP-based Low Cost Alternative to Transformers</title><link>http://arxiv.org/abs/2203.03691v3</link><description>Transformer-based architectures are the model of choice for natural languageunderstanding, but they come at a significant cost, as they have quadraticcomplexity in the input length, require a lot of training data, and can bedifficult to tune. In the pursuit of lower costs, we investigate simpleMLP-based architectures. We find that existing architectures such as MLPMixer,which achieves token mixing through a static MLP applied to each featureindependently, are too detached from the inductive biases required for naturallanguage understanding. In this paper, we propose a simple variant, HyperMixer,which forms the token mixing MLP dynamically using hypernetworks. Empirically,we demonstrate that our model performs better than alternative MLP-basedmodels, and on par with Transformers. In contrast to Transformers, HyperMixerachieves these results at substantially lower costs in terms of processingtime, training data, and hyperparameter tuning.</description><author>Florian Mai, Arnaud Pannatier, Fabio Fehr, Haolin Chen, Francois Marelli, Francois Fleuret, James Henderson</author><pubDate>Mon, 13 Nov 2023 16:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.03691v3</guid></item><item><title>Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling</title><link>http://arxiv.org/abs/2309.17105v2</link><description>Action Quality Assessment (AQA) is a task that tries to answer how well anaction is carried out. While remarkable progress has been achieved, existingworks on AQA assume that all the training data are visible for training in onetime, but do not enable continual learning on assessing new technical actions.In this work, we address such a Continual Learning problem in AQA(Continual-AQA), which urges a unified model to learn AQA tasks sequentiallywithout forgetting. Our idea for modeling Continual-AQA is to sequentiallylearn a task-consistent score-discriminative feature distribution, in which thelatent features express a strong correlation with the score labels regardlessof the task or action types. From this perspective, we aim to mitigate theforgetting in Continual-AQA from two aspects. Firstly, to fuse the features ofnew and previous data into a score-discriminative distribution, a novelFeature-Score Correlation-Aware Rehearsal is proposed to store and reuse datafrom previous tasks with limited memory size. Secondly, an ActionGeneral-Specific Graph is developed to learn and decouple the action-generaland action-specific knowledge so that the task-consistent score-discriminativefeatures can be better extracted across various tasks. Extensive experimentsare conducted to evaluate the contributions of proposed components. Thecomparisons with the existing continual learning methods additionally verifythe effectiveness and versatility of our approach.</description><author>Yuan-Ming Li, Ling-An Zeng, Jing-Ke Meng, Wei-Shi Zheng</author><pubDate>Mon, 13 Nov 2023 16:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17105v2</guid></item><item><title>Causal Discovery under Latent Class Confounding</title><link>http://arxiv.org/abs/2311.07454v1</link><description>Directed acyclic graphs are used to model the causal structure of a system.``Causal discovery'' describes the problem of learning this structure fromdata. When data is an aggregate from multiple sources (populations orenvironments), global confounding obscures conditional independence propertiesthat drive many causal discovery algorithms. For this reason, existing causaldiscovery algorithms are not suitable for the multiple-source setting. Wedemonstrate that, if the confounding is of bounded cardinality (i.e. the datacomes from a limited number of sources), causal discovery can still beachieved. The feasibility of this problem is governed by a trade-off betweenthe cardinality of the global confounder, the cardinalities of the observedvariables, and the sparsity of the causal structure.</description><author>Bijan Mazaheri, Spencer Gordon, Yuval Rabani, Leonard Schulman</author><pubDate>Mon, 13 Nov 2023 16:35:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07454v1</guid></item><item><title>ChartCheck: An Evidence-Based Fact-Checking Dataset over Real-World Chart Images</title><link>http://arxiv.org/abs/2311.07453v1</link><description>Data visualizations are common in the real-world. We often use them in datasources such as scientific documents, news articles, textbooks, and socialmedia to summarize key information in a visual form. Charts can also misleadits audience by communicating false information or biasing them towards aspecific agenda. Verifying claims against charts is not a straightforwardprocess. It requires analyzing both the text and visual components of thechart, considering characteristics such as colors, positions, and orientations.Moreover, to determine if a claim is supported by the chart content oftenrequires different types of reasoning. To address this challenge, we introduceChartCheck, a novel dataset for fact-checking against chart images. ChartCheckis the first large-scale dataset with 1.7k real-world charts and 10.5khuman-written claims and explanations. We evaluated the dataset onstate-of-the-art models and achieved an accuracy of 73.9 in the finetunedsetting. Additionally, we identified chart characteristics and reasoning typesthat challenge the models.</description><author>Mubashara Akhtar, Nikesh Subedi, Vivek Gupta, Sahar Tahmasebi, Oana Cocarascu, Elena Simperl</author><pubDate>Mon, 13 Nov 2023 16:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07453v1</guid></item><item><title>Explainable Boosting Machines with Sparsity -- Maintaining Explainability in High-Dimensional Settings</title><link>http://arxiv.org/abs/2311.07452v1</link><description>Compared to "black-box" models, like random forests and deep neural networks,explainable boosting machines (EBMs) are considered "glass-box" models that canbe competitively accurate while also maintaining a higher degree oftransparency and explainability. However, EBMs become readily less transparentand harder to interpret in high-dimensional settings with many predictorvariables; they also become more difficult to use in production due toincreases in scoring time. We propose a simple solution based on the leastabsolute shrinkage and selection operator (LASSO) that can help introducesparsity by reweighting the individual model terms and removing the lessrelevant ones, thereby allowing these models to maintain their transparency andrelatively fast scoring times in higher-dimensional settings. In short,post-processing a fitted EBM with many (i.e., possibly hundreds or thousands)of terms using the LASSO can help reduce the model's complexity and drasticallyimprove scoring time. We illustrate the basic idea using two real-worldexamples with code.</description><author>Brandon M. Greenwell, Annika Dahlmann, Saurabh Dhoble</author><pubDate>Mon, 13 Nov 2023 16:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07452v1</guid></item><item><title>Language Grounded QFormer for Efficient Vision Language Understanding</title><link>http://arxiv.org/abs/2311.07449v1</link><description>Large-scale pretraining and instruction tuning have been successful fortraining general-purpose language models with broad competencies. However,extending to general-purpose vision-language models is challenging due to thedistributional diversity in visual inputs. A recent line of work exploresvision-language instruction tuning, taking inspiration from the QueryTransformer (QFormer) approach proposed in BLIP-2 models for bridging frozenmodalities. However, these approaches rely heavily on large-scale multi-modalpretraining for representation learning before eventual finetuning, incurring ahuge computational overhead, poor scaling, and limited accessibility. To thatend, we propose a more efficient method for QFormer-based vision-languagealignment and demonstrate the effectiveness of our strategy compared toexisting baselines in improving the efficiency of vision-language pretraining.</description><author>Moulik Choraria, Nitesh Sekhar, Yue Wu, Xu Zhang, Prateek Singhal, Lav R. Varshney</author><pubDate>Mon, 13 Nov 2023 16:30:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07449v1</guid></item><item><title>Price-Aware Deep Learning for Electricity Markets</title><link>http://arxiv.org/abs/2308.01436v2</link><description>While deep learning gradually penetrates operational planning, its inherentprediction errors may significantly affect electricity prices. This letterexamines how prediction errors propagate into electricity prices, revealingnotable pricing errors and their spatial disparity in congested power systems.To improve fairness, we propose to embed electricity market-clearingoptimization as a deep learning layer. Differentiating through this layerallows for balancing between prediction and pricing errors, as oppose tominimizing prediction errors alone. This layer implicitly optimizes fairnessand controls the spatial distribution of price errors across the system. Weshowcase the price-aware deep learning in the nexus of wind power forecastingand short-term electricity market clearing.</description><author>Vladimir Dvorkin, Ferdinando Fioretto</author><pubDate>Mon, 13 Nov 2023 16:24:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01436v2</guid></item><item><title>Story-to-Motion: Synthesizing Infinite and Controllable Character Animation from Long Text</title><link>http://arxiv.org/abs/2311.07446v1</link><description>Generating natural human motion from a story has the potential to transformthe landscape of animation, gaming, and film industries. A new and challengingtask, Story-to-Motion, arises when characters are required to move to variouslocations and perform specific motions based on a long text description. Thistask demands a fusion of low-level control (trajectories) and high-levelcontrol (motion semantics). Previous works in character control andtext-to-motion have addressed related aspects, yet a comprehensive solutionremains elusive: character control methods do not handle text description,whereas text-to-motion methods lack position constraints and often produceunstable motions. In light of these limitations, we propose a novel system thatgenerates controllable, infinitely long motions and trajectories aligned withthe input text. (1) We leverage contemporary Large Language Models to act as atext-driven motion scheduler to extract a series of (text, position, duration)pairs from long text. (2) We develop a text-driven motion retrieval scheme thatincorporates motion matching with motion semantic and trajectory constraints.(3) We design a progressive mask transformer that addresses common artifacts inthe transition motion such as unnatural pose and foot sliding. Beyond itspioneering role as the first comprehensive solution for Story-to-Motion, oursystem undergoes evaluation across three distinct sub-tasks: trajectoryfollowing, temporal action composition, and motion blending, where itoutperforms previous state-of-the-art motion synthesis methods across theboard. Homepage: https://story2motion.github.io/.</description><author>Zhongfei Qing, Zhongang Cai, Zhitao Yang, Lei Yang</author><pubDate>Mon, 13 Nov 2023 16:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07446v1</guid></item><item><title>Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue</title><link>http://arxiv.org/abs/2311.07445v1</link><description>The emergence of large language models (LLMs) further improves thecapabilities of open-domain dialogue systems and can generate fluent, coherent,and diverse responses. However, LLMs still lack an important ability:communication skills, which makes them more like information seeking tools thananthropomorphic chatbots. To make LLMs more anthropomorphic and proactiveduring the conversation, we add five communication skills to the responsegeneration process: topic transition, proactively asking questions, conceptguidance, empathy, and summarising often. The addition of communication skillsincreases the interest of users in the conversation and attracts them to chatfor longer. To enable LLMs better understand and use communication skills, wedesign and add the inner monologue to LLMs. The complete process is achievedthrough prompt engineering and in-context learning. To evaluate communicationskills, we construct a benchmark named Cskills for evaluating variouscommunication skills, which can also more comprehensively evaluate the dialoguegeneration ability of the model. Experimental results show that the proposedCSIM strategy improves the backbone models and outperforms the baselines inboth automatic and human evaluations.</description><author>Junkai Zhou, Liang Pang, Huawei Shen, Xueqi Cheng</author><pubDate>Mon, 13 Nov 2023 16:19:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07445v1</guid></item><item><title>On the Robustness of Neural Collapse and the Neural Collapse of Robustness</title><link>http://arxiv.org/abs/2311.07444v1</link><description>Neural Collapse refers to the curious phenomenon in the end of training of aneural network, where feature vectors and classification weights converge to avery simple geometrical arrangement (a simplex). While it has been observedempirically in various cases and has been theoretically motivated, itsconnection with crucial properties of neural networks, like theirgeneralization and robustness, remains unclear. In this work, we study thestability properties of these simplices. We find that the simplex structuredisappears under small adversarial attacks, and that perturbed examples "leap"between simplex vertices. We further analyze the geometry of networks that areoptimized to be robust against adversarial perturbations of the input, and findthat Neural Collapse is a pervasive phenomenon in these cases as well, withclean and perturbed representations forming aligned simplices, and giving riseto a robust simple nearest-neighbor classifier. By studying the propagation ofthe amount of collapse inside the network, we identify novel properties of bothrobust and non-robust machine learning models, and show that earlier, unlikelater layers maintain reliable simplices on perturbed data.</description><author>Jingtong Su, Ya Shi Zhang, Nikolaos Tsilivis, Julia Kempe</author><pubDate>Mon, 13 Nov 2023 16:18:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07444v1</guid></item><item><title>Investigating Multi-Pivot Ensembling with Massively Multilingual Machine Translation Models</title><link>http://arxiv.org/abs/2311.07439v1</link><description>Massively multilingual machine translation models allow for the translationof a large number of languages with a single model, but have limitedperformance on low- and very-low-resource translation directions. Pivoting viahigh-resource languages remains a strong strategy for low-resource directions,and in this paper we revisit ways of pivoting through multiple languages.Previous work has used a simple averaging of probability distributions frommultiple paths, but we find that this performs worse than using a single pivot,and exacerbates the hallucination problem because the same hallucinations canbe probable across different paths. As an alternative, we propose MaxEns, acombination strategy that is biased towards the most confident predictions,hypothesising that confident predictions are less prone to be hallucinations.We evaluate different strategies on the FLORES benchmark for 20 low-resourcelanguage directions, demonstrating that MaxEns improves translation quality forlow-resource languages while reducing hallucination in translations, comparedto both direct translation and an averaging approach. On average, multi-pivotstrategies still lag behind using English as a single pivot language, raisingthe question of how to identify the best pivoting strategy for a giventranslation direction.</description><author>Alireza Mohammadshahi, Jannis Vamvas, Rico Sennrich</author><pubDate>Mon, 13 Nov 2023 16:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07439v1</guid></item><item><title>Hardest Monotone Functions for Evolutionary Algorithms</title><link>http://arxiv.org/abs/2311.07438v1</link><description>The study of hardest and easiest fitness landscapes is an active area ofresearch. Recently, Kaufmann, Larcher, Lengler and Zou conjectured that for theself-adjusting $(1,\lambda)$-EA, Adversarial Dynamic BinVal (ADBV) is thehardest dynamic monotone function to optimize. We introduce the functionSwitching Dynamic BinVal (SDBV) which coincides with ADBV whenever the numberof remaining zeros in the search point is strictly less than $n/2$, where $n$denotes the dimension of the search space. We show, using a combinatorialargument, that for the $(1+1)$-EA with any mutation rate $p \in [0,1]$, SDBV isdrift-minimizing among the class of dynamic monotone functions. Ourconstruction provides the first explicit example of an instance of thepartially-ordered evolutionary algorithm (PO-EA) model with parameterizedpessimism introduced by Colin, Doerr and F\'erey, building on work of Jansen.We further show that the $(1+1)$-EA optimizes SDBV in $\Theta(n^{3/2})$generations. Our simulations demonstrate matching runtimes for both static andself-adjusting $(1,\lambda)$ and $(1+\lambda)$-EA. We further show, using anexample of fixed dimension, that drift-minimization does not equal maximalruntime.</description><author>Marc Kaufmann, Maxime Larcher, Johannes Lengler, Oliver Sieberling</author><pubDate>Mon, 13 Nov 2023 16:13:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07438v1</guid></item><item><title>Integrating Pre-trained Language Model into Neural Machine Translation</title><link>http://arxiv.org/abs/2310.19680v2</link><description>Neural Machine Translation (NMT) has become a significant technology innatural language processing through extensive research and development.However, the deficiency of high-quality bilingual language pair data stillposes a major challenge to improving NMT performance. Recent studies areexploring the use of contextual information from pre-trained language model(PLM) to address this problem. Yet, the issue of incompatibility between PLMand NMT model remains unresolved. This study proposes a PLM-integrated NMT(PiNMT) model to overcome the identified problems. The PiNMT model consists ofthree critical components, PLM Multi Layer Converter, Embedding Fusion, andCosine Alignment, each playing a vital role in providing effective PLMinformation to NMT. Furthermore, two training strategies, Separate LearningRates and Dual Step Training, are also introduced in this paper. Byimplementing the proposed PiNMT model and training strategy, we achievedstate-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset.This study's outcomes are noteworthy as they demonstrate a novel approach forefficiently integrating PLM with NMT to overcome incompatibility and enhanceperformance.</description><author>Soon-Jae Hwang, Chang-Sung Jeong</author><pubDate>Mon, 13 Nov 2023 16:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19680v2</guid></item><item><title>milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing</title><link>http://arxiv.org/abs/2306.17010v3</link><description>Approaching the era of ubiquitous computing, human motion sensing plays acrucial role in smart systems for decision making, user interaction, andpersonalized services. Extensive research has been conducted on human tracking,pose estimation, gesture recognition, and activity recognition, which arepredominantly based on cameras in traditional methods. However, the intrusivenature of cameras limits their use in smart home applications. To address this,mmWave radars have gained popularity due to their privacy-friendly features. Inthis work, we propose milliFlow, a novel deep learning method for scene flowestimation as a complementary motion information for mmWave point cloud,serving as an intermediate level of features and directly benefiting downstreamhuman motion sensing tasks. Experimental results demonstrate the superiorperformance of our method with an average 3D endpoint error of 4.6cm,significantly surpassing the competing approaches. Furthermore, byincorporating scene flow information, we achieve remarkable improvements inhuman activity recognition, human parsing, and human body part tracking. Tofoster further research in this area, we will provide our codebase and datasetfor open access upon acceptance.</description><author>Fangqiang Ding, Zhen Luo, Peijun Zhao, Chris Xiaoxuan Lu</author><pubDate>Mon, 13 Nov 2023 16:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17010v3</guid></item><item><title>Supersampling of Data from Structured-light Scanner with Deep Learning</title><link>http://arxiv.org/abs/2311.07432v1</link><description>This paper focuses on increasing the resolution of depth maps obtained from3D cameras using structured light technology. Two deep learning models FDSR andDKN are modified to work with high-resolution data, and data pre-processingtechniques are implemented for stable training. The models are trained on ourcustom dataset of 1200 3D scans. The resulting high-resolution depth maps areevaluated using qualitative and quantitative metrics. The approach for depthmap upsampling offers benefits such as reducing the processing time of apipeline by first downsampling a high-resolution depth map, performing variousprocessing steps at the lower resolution and upsampling the resulting depth mapor increasing the resolution of a point cloud captured in lower resolution by acheaper device. The experiments demonstrate that the FDSR model excels in termsof faster processing time, making it a suitable choice for applications wherespeed is crucial. On the other hand, the DKN model provides results with higherprecision, making it more suitable for applications that prioritize accuracy.</description><author>Martin Melicherčík, Lukáš Gajdošech, Viktor Kocur, Martin Madaras</author><pubDate>Mon, 13 Nov 2023 16:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07432v1</guid></item><item><title>Controlled Text Generation for Black-box Language Models via Score-based Progressive Editor</title><link>http://arxiv.org/abs/2311.07430v1</link><description>Despite recent progress in language models, generating constrained text forspecific domains remains a challenge, particularly when utilizing black-boxmodels that lack domain-specific knowledge. In this paper, we introduce ScoPE(Score-based Progressive Editor) generation, a novel approach for controlledtext generation for black-box language models. We employ ScoPE to facilitatetext generation in the target domain by integrating it with language modelsthrough a cascading approach. Trained to enhance the target domain score of theedited text, ScoPE progressively edits intermediate output discrete tokens toalign with the target attributes throughout the auto-regressive generationprocess of the language model. This iterative process guides subsequent stepsto produce desired output texts for the target domain. Our experimental resultson diverse controlled generations demonstrate that ScoPE effectivelyfacilitates controlled text generation for black-box language models in bothin-domain and out-of-domain conditions, which is challenging for existingmethods.</description><author>Sangwon Yu, Changmin Lee, Hojin Lee, Sungroh Yoon</author><pubDate>Mon, 13 Nov 2023 16:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07430v1</guid></item><item><title>ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs</title><link>http://arxiv.org/abs/2311.02775v2</link><description>Responding to the thousands of student questions on online QA platforms eachsemester has a considerable human cost, particularly in computing courses withrapidly growing enrollments. To address the challenges of scalable andintelligent question-answering (QA), we introduce an innovative solution thatleverages open-source Large Language Models (LLMs) from the LLaMA-2 family toensure data privacy. Our approach combines augmentation techniques such asretrieval augmented generation (RAG), supervised fine-tuning (SFT), andlearning from human preferences data using Direct Preference Optimization(DPO). Through extensive experimentation on a Piazza dataset from anintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs ofpreference data, we demonstrate a significant 30% improvement in the quality ofanswers, with RAG being a particularly impactful addition. Our contributionsinclude the development of a novel architecture for educational QA, extensiveevaluations of LLM performance utilizing both human assessments and LLM-basedmetrics, and insights into the challenges and future directions of educationaldata processing. This work paves the way for the development of CHATA, anintelligent QA assistant customizable for courses with an online QA platform</description><author>Yann Hicke, Anmol Agarwal, Qianou Ma, Paul Denny</author><pubDate>Mon, 13 Nov 2023 16:03:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02775v2</guid></item><item><title>Boolean Variation and Boolean Logic BackPropagation</title><link>http://arxiv.org/abs/2311.07427v1</link><description>The notion of variation is introduced for the Boolean set and based on whichBoolean logic backpropagation principle is developed. Using this concept, deepmodels can be built with weights and activations being Boolean numbers andoperated with Boolean logic instead of real arithmetic. In particular, Booleandeep models can be trained directly in the Boolean domain without latentweights. No gradient but logic is synthesized and backpropagated throughlayers.</description><author>Van Minh Nguyen</author><pubDate>Mon, 13 Nov 2023 16:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07427v1</guid></item><item><title>Optimising Human-AI Collaboration by Learning Convincing Explanations</title><link>http://arxiv.org/abs/2311.07426v1</link><description>Machine learning models are being increasingly deployed to take, or assist intaking, complicated and high-impact decisions, from quasi-autonomous vehiclesto clinical decision support systems. This poses challenges, particularly whenmodels have hard-to-detect failure modes and are able to take actions withoutoversight. In order to handle this challenge, we propose a method for acollaborative system that remains safe by having a human ultimately makingdecisions, while giving the model the best opportunity to convince and debatethem with interpretable explanations. However, the most helpful explanationvaries among individuals and may be inconsistent across stated preferences. Tothis end we develop an algorithm, Ardent, to efficiently learn a rankingthrough interaction and best assist humans complete a task. By utilising acollaborative approach, we can ensure safety and improve performance whileaddressing transparency and accountability concerns. Ardent enables efficientand effective decision-making by adapting to individual preferences forexplanations, which we validate through extensive simulations alongside a userstudy involving a challenging image classification task, demonstratingconsistent improvement over competing systems.</description><author>Alex J. Chan, Alihan Huyuk, Mihaela van der Schaar</author><pubDate>Mon, 13 Nov 2023 16:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07426v1</guid></item><item><title>Hallucination Augmented Recitations for Language Models</title><link>http://arxiv.org/abs/2311.07424v1</link><description>Attribution is a key concept in large language models (LLMs) as it enablescontrol over information sources and enhances the factuality of LLMs. Whileexisting approaches utilize open book question answering to improveattribution, factual datasets may reward language models to recall facts thatthey already know from their pretraining data, not attribution. In contrast,counterfactual open book QA datasets would further improve attribution becausethe answer could only be grounded in the given text. We propose HallucinationAugmented Recitations (HAR) for creating counterfactual datasets by utilizinghallucination in LLMs to improve attribution. For open book QA as a case study,we demonstrate that models finetuned with our counterfactual datasets improvetext grounding, leading to better open book QA performance, with up to an 8.0%increase in F1 score. Our counterfactual dataset leads to significantly betterperformance than using humanannotated factual datasets, even with 4x smallerdatasets and 4x smaller models. We observe that improvements are consistentacross various model sizes and datasets, including multi-hop, biomedical, andadversarial QA datasets.</description><author>Abdullatif Köksal, Renat Aksitov, Chung-Ching Chang</author><pubDate>Mon, 13 Nov 2023 15:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07424v1</guid></item><item><title>Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning</title><link>http://arxiv.org/abs/2308.11630v2</link><description>We present and experimentally evaluate using transfer learning to addressexperimental data scarcity when training neural network (NN) models forMach-Zehnder interferometer mesh-based optical matrix multipliers. Our approachinvolves pre-training the model using synthetic data generated from a lessaccurate analytical model and fine-tuning with experimental data. Ourinvestigation demonstrates that this method yields significant reductions inmodeling errors compared to using an analytical model, or a standalone NN modelwhen training data is limited. Utilizing regularization techniques and ensembleaveraging, we achieve &lt; 1 dB root-mean-square error on the matrix weightsimplemented by a 3x3 photonic chip while using only 25% of the available data.</description><author>Ali Cem, Ognjen Jovanovic, Siqi Yan, Yunhong Ding, Darko Zibar, Francesco Da Ros</author><pubDate>Mon, 13 Nov 2023 15:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11630v2</guid></item><item><title>Robust semi-supervised segmentation with timestep ensembling diffusion models</title><link>http://arxiv.org/abs/2311.07421v1</link><description>Medical image segmentation is a challenging task, made more difficult by manydatasets' limited size and annotations. Denoising diffusion probabilisticmodels (DDPM) have recently shown promise in modelling the distribution ofnatural images and were successfully applied to various medical imaging tasks.This work focuses on semi-supervised image segmentation using diffusion models,particularly addressing domain generalisation. Firstly, we demonstrate thatsmaller diffusion steps generate latent representations that are more robustfor downstream tasks than larger steps. Secondly, we use this insight topropose an improved esembling scheme that leverages information-dense smallsteps and the regularising effect of larger steps to generate predictions. Ourmodel shows significantly better performance in domain-shifted settings whileretaining competitive performance in-domain. Overall, this work highlights thepotential of DDPMs for semi-supervised medical image segmentation and providesinsights into optimising their performance under domain shift.</description><author>Margherita Rosnati, Melanie Roschewitz, Ben Glocker</author><pubDate>Mon, 13 Nov 2023 15:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07421v1</guid></item><item><title>Speech-based Slot Filling using Large Language Models</title><link>http://arxiv.org/abs/2311.07418v1</link><description>Recently, advancements in large language models (LLMs) have shown anunprecedented ability across various language tasks. This paper investigatesthe potential application of LLMs to slot filling with noisy ASRtranscriptions, via both in-context learning and task-specific fine-tuning.Dedicated prompt designs and fine-tuning approaches are proposed to improve therobustness of LLMs for slot filling with noisy ASR transcriptions. Moreover, alinearised knowledge injection (LKI) scheme is also proposed to integratedynamic external knowledge into LLMs. Experiments were performed on SLURP toquantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B andVicuna-13B (v1.1 and v1.5) with different ASR error rates. The use of theproposed fine-tuning together with the LKI scheme for LLaMA-13B achieved an8.3% absolute SLU-F1 improvement compared to the strong Flan-T5-base baselinesystem on a limited data setup.</description><author>Guangzhi Sun, Shutong Feng, Dongcheng Jiang, Chao Zhang, Milica Gašić, Philip C. Woodland</author><pubDate>Mon, 13 Nov 2023 15:54:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07418v1</guid></item><item><title>Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration</title><link>http://arxiv.org/abs/2311.07417v1</link><description>As the capacity of deep neural networks (DNNs) increases, their need for hugeamounts of data significantly grows. A common practice is to outsource thetraining process or collect more data over the Internet, which introduces therisks of a backdoored DNN. A backdoored DNN shows normal behavior on clean datawhile behaving maliciously once a trigger is injected into a sample at the testtime. In such cases, the defender faces multiple difficulties. First, theavailable clean dataset may not be sufficient for fine-tuning and recoveringthe backdoored DNN. Second, it is impossible to recover the trigger in manyreal-world applications without information about it. In this paper, weformulate some characteristics of poisoned neurons. This backdoorsuspiciousness score can rank network neurons according to their activationvalues, weights, and their relationship with other neurons in the same layer.Our experiments indicate the proposed method decreases the chance of attacksbeing successful by more than 50% with a tiny clean dataset, i.e., ten cleansamples for the CIFAR-10 dataset, without significantly deteriorating themodel's performance. Moreover, the proposed method runs three times as fast asbaselines.</description><author>Soroush Hashemifar, Saeed Parsa, Morteza Zakeri-Nasrabadi</author><pubDate>Mon, 13 Nov 2023 15:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07417v1</guid></item><item><title>Three-dimensional granular flow simulation using graph neural network-based learned simulator</title><link>http://arxiv.org/abs/2311.07416v1</link><description>Reliable evaluations of geotechnical hazards like landslides and debris flowrequire accurate simulation of granular flow dynamics. Traditional numericalmethods can simulate the complex behaviors of such flows that involvesolid-like to fluid-like transitions, but they are computationally intractablewhen simulating large-scale systems. Surrogate models based on statistical ormachine learning methods are a viable alternative, but they are typicallyempirical and rely on a confined set of parameters in evaluating associatedrisks. Due to their permutation-dependent learning, conventional machinelearning models require an unreasonably large amount of training data forbuilding generalizable surrogate models. We employ a graph neural network(GNN), a novel deep learning technique, to develop a GNN-based simulator (GNS)for granular flows to address these issues. Graphs represent the state ofgranular flows and interactions, like the exchange of energy and momentumbetween grains, and GNN learns the local interaction law. GNS takes the currentstate of the granular flow and estimates the next state using Euler explicitintegration. We train GNS on a limited set of granular flow trajectories andevaluate its performance in a three-dimensional granular column collapsedomain. GNS successfully reproduces the overall behaviors of column collapseswith various aspect ratios that were not encountered during training. Thecomputation speed of GNS outperforms high-fidelity numerical simulators by 300times.</description><author>Yongjin Choi, Krishna Kumar</author><pubDate>Mon, 13 Nov 2023 15:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07416v1</guid></item><item><title>FIRST: A Million-Entry Dataset for Text-Driven Fashion Synthesis and Design</title><link>http://arxiv.org/abs/2311.07414v1</link><description>Text-driven fashion synthesis and design is an extremely valuable part ofartificial intelligence generative content(AIGC), which has the potential topropel a tremendous revolution in the traditional fashion industry. To advancethe research on text-driven fashion synthesis and design, we introduce a newdataset comprising a million high-resolution fashion images with richstructured textual(FIRST) descriptions. In the FIRST, there is a wide range ofattire categories and each image-paired textual description is organized atmultiple hierarchical levels. Experiments on prevalent generative modelstrained over FISRT show the necessity of FIRST. We invite the community tofurther develop more intelligent fashion synthesis and design systems that makefashion design more creative and imaginative based on our dataset. The datasetwill be released soon.</description><author>Zhen Huang, Yihao Li, Dong Pei, Jiapeng Zhou, Xuliang Ning, Jianlin Han, Xiaoguang Han, Xuejun Chen</author><pubDate>Mon, 13 Nov 2023 15:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07414v1</guid></item><item><title>A Large Deviations Perspective on Policy Gradient Algorithms</title><link>http://arxiv.org/abs/2311.07411v1</link><description>We derive the first large deviation rate function for the stochastic iteratesgenerated by policy gradient methods with a softmax parametrization and anentropy regularized objective. Leveraging the contraction principle from largedeviations theory, we also develop a general recipe for deriving exponentialconvergence rates for a wide spectrum of other policy parametrizations. Thisapproach unifies several results from the literature and simplifies existingproof techniques.</description><author>Wouter Jongeneel, Mengmeng Li, Daniel Kuhn</author><pubDate>Mon, 13 Nov 2023 15:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07411v1</guid></item><item><title>Towards Automatic Honey Bee Flower-Patch Assays with Paint Marking Re-Identification</title><link>http://arxiv.org/abs/2311.07407v1</link><description>In this paper, we show that paint markings are a feasible approach toautomatize the analysis of behavioral assays involving honey bees in the fieldwhere marking has to be as lightweight as possible. We contribute a noveldataset for bees re-identification with paint-markings with 4392 images and 27identities. Contrastive learning with a ResNet backbone and triplet loss led toidentity representation features with almost perfect recognition in closedsetting where identities are known in advance. Diverse experiments evaluate thecapability to generalize to separate IDs, and show the impact of usingdifferent body parts for identification, such as using the unmarked abdomenonly. In addition, we show the potential to fully automate the visit detectionand provide preliminary results of compute time for future real-time deploymentin the field on an edge device.</description><author>Luke Meyers, Josué Rodríguez Cordero, Carlos Corrada Bravo, Fanfan Noel, José Agosto-Rivera, Tugrul Giray, Rémi Mégret</author><pubDate>Mon, 13 Nov 2023 15:41:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07407v1</guid></item><item><title>Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization</title><link>http://arxiv.org/abs/2311.01544v2</link><description>Large Language Models (LLMs) have reshaped natural language processing withtheir impressive capabilities. Their ever-increasing size, however, raisedconcerns about their effective deployment and the need for LLM compressions.This study introduces the Divergent Token metrics (DTMs), a novel approach forassessing compressed LLMs, addressing the limitations of traditional perplexityor accuracy measures that fail to accurately reflect text generation quality.DTMs focus on token divergence, that allow deeper insights into the subtletiesof model compression, i.p. when evaluating component's impacts individually.Utilizing the First Divergent Token metric (FDTM) in model sparsificationreveals that a quarter of all attention components can be pruned beyond 90% onthe Llama-2 model family, still keeping SOTA performance. For quantization FDTMsuggests that over 80% of parameters can naively be transformed to int8 withoutspecial outlier management. These evaluations indicate the necessity ofchoosing appropriate compressions for parameters individually-and that FDTM canidentify those-while standard metrics result in deteriorated outcomes.</description><author>Björn Deiseroth, Max Meuer, Nikolas Gritsch, Constantin Eichenberg, Patrick Schramowski, Matthias Aßenmacher, Kristian Kersting</author><pubDate>Mon, 13 Nov 2023 15:33:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01544v2</guid></item><item><title>Processing and Segmentation of Human Teeth from 2D Images using Weakly Supervised Learning</title><link>http://arxiv.org/abs/2311.07398v1</link><description>Teeth segmentation is an essential task in dental image analysis for accuratediagnosis and treatment planning. While supervised deep learning methods can beutilized for teeth segmentation, they often require extensive manual annotationof segmentation masks, which is time-consuming and costly. In this research, wepropose a weakly supervised approach for teeth segmentation that reduces theneed for manual annotation. Our method utilizes the output heatmaps andintermediate feature maps from a keypoint detection network to guide thesegmentation process. We introduce the TriDental dataset, consisting of 3000oral cavity images annotated with teeth keypoints, to train a teeth keypointdetection network. We combine feature maps from different layers of thekeypoint detection network, enabling accurate teeth segmentation withoutexplicit segmentation annotations. The detected keypoints are also used forfurther refinement of the segmentation masks. Experimental results on theTriDental dataset demonstrate the superiority of our approach in terms ofaccuracy and robustness compared to state-of-the-art segmentation methods. Ourmethod offers a cost-effective and efficient solution for teeth segmentation inreal-world dental applications, eliminating the need for extensive manualannotation efforts.</description><author>Tomáš Kunzo, Viktor Kocur, Lukáš Gajdošech, Martin Madaras</author><pubDate>Mon, 13 Nov 2023 15:25:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07398v1</guid></item></channel></rss>